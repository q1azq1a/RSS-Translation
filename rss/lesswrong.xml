<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 5 日，星期二 06:15:57 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Some open-source dictionaries and dictionary learning infrastructure]]></title><description><![CDATA[Published on December 5, 2023 6:05 AM GMT<br/><br/><p>随着越来越多的人开始从事包含字典学习的可解释性项目，公开提供高质量的字典将很有价值。 <span class="footnote-reference" role="doc-noteref" id="fnrefxki2bn9t6a"><sup><a href="#fnxki2bn9t6a">[1]</a></sup></span>为了让事情顺利进行，我和我的合作者（Aaron Mueller）：</p><ul><li>开源许多在 Pythia-70m MLP 上训练的稀疏自动编码器字典</li><li>发布我们用于训练这些词典的<a href="https://github.com/saprmarks/dictionary_learning">存储库</a><span class="footnote-reference" role="doc-noteref" id="fnrefp7b5kczep0n"><sup><a href="#fnp7b5kczep0n">[2]</a></sup></span> 。</li></ul><p>我们首先讨论字典，然后讨论存储库。</p><h1>字典</h1><p>词典可以从<a href="https://baulab.us/u/smarks/autoencoders/">这里</a>下载。有关如何下载和使用它们的信息，请参阅<a href="https://github.com/saprmarks/dictionary_learning">此处的</a>“下载我们的开源词典”和“使用经过训练的词典”部分。如果您在发表的论文中使用这些词典，我们要求您在致谢中提及我们。</p><p>我们正在为 EleutherAI 的 6 层 pythia-70m-deduped 模型发布两套字典。两组字典都使用来自<a href="https://pile.eleuther.ai/">The Pile</a>的约 8 亿个令牌，在 512 维 MLP<i>输出</i>激活（而不是像 Anthropic 使用的 MLP 隐藏层）上进行训练。</p><ul><li>第一组称为<code>0_8192</code> ，由大小为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8192 = 16 \times 512"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8192</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span></span></span></span></span></span>的字典组成<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。这些训练的 L1 惩罚为<code>1e-3</code> 。</li><li>第二组称为<code>1_32768</code> ，由大小为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="32768 = 64 \times 512"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">32768</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">64</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span></span></span></span></span></span>的字典组成。这些训练的 l1 惩罚为<code>3e-3</code> 。</li></ul><p>以下是一些统计数据。 （有关这些统计数据含义的更多信息，请参阅我们的存储库的<a href="https://github.com/saprmarks/dictionary_learning">自述文件</a>。）</p><p>对于<code>0_8192</code>集中的字典：</p><figure class="table"><table><thead><tr><th>层</th><th>均方误差损失</th><th>L1损失</th><th>L0</th><th>存活百分比</th><th>损失恢复百分比</th></tr></thead><tbody><tr><td>0</td><td> 0.056</td><td> 6.132</td><td> 9.951</td><td> 0.998</td><td> 0.984</td></tr><tr><td> 1</td><td> 0.089</td><td> 6.677</td><td> 44.739</td><td> 0.887</td><td> 0.924</td></tr><tr><td> 2</td><td> 0.108</td><td> 11.44</td><td> 62.156</td><td> 0.587</td><td> 0.867</td></tr><tr><td> 3</td><td> 0.135</td><td> 23.773</td><td> 175.303</td><td> 0.588</td><td> 0.902</td></tr><tr><td> 4</td><td> 0.148</td><td> 27.084</td><td> 174.07</td><td> 0.806</td><td> 0.927</td></tr><tr><td> 5</td><td> 0.179</td><td> 47.126</td><td> 235.05</td><td> 0.672</td><td> 0.972</td></tr></tbody></table></figure><p>对于<code>1_32768</code>集中的字典：</p><figure class="table"><table><thead><tr><th>层</th><th>均方误差损失</th><th>L1损失</th><th>L0</th><th>存活百分比</th><th>损失恢复百分比</th></tr></thead><tbody><tr><td>0</td><td> 0.09</td><td> 4.32</td><td> 2.873</td><td> 0.174</td><td> 0.946</td></tr><tr><td> 1</td><td> 0.13</td><td> 2.798</td><td> 11.256</td><td> 0.159</td><td> 0.768</td></tr><tr><td> 2</td><td> 0.152</td><td> 6.151</td><td> 16.381</td><td> 0.118</td><td> 0.724</td></tr><tr><td> 3</td><td> 0.211</td><td> 11.571</td><td> 39.863</td><td> 0.226</td><td> 0.765</td></tr><tr><td> 4</td><td> 0.222</td><td> 13.665</td><td> 29.235</td><td> 0.19</td><td> 0.816</td></tr><tr><td> 5</td><td> 0.265</td><td> 26.4</td><td> 43.846</td><td> 0.13</td><td> 0.931</td></tr></tbody></table></figure><p>这是一些特征频率的直方图。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bwz8jq2puk7bt2v5i30r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/y1doqsgw6saydkevmegk 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bxmnx1a065hxjb1kxm7w 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/amxktqpnee5yslwybgjr 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/nqczszdt53rrze0yyqjn 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/wt8mfxq5hu3so0xkzhlp 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/ypwhg5ljidk3ihwawra9 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/yew2oxwaf2h3qktmyzdz 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/cl13rve82dtm4e7n5rvg 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/t3gi9wznfqvmwt5g5hjk 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/lgksrskpkvezs1uxrf7v 1920w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/xbofxvy4qa4st0bujv2y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bswlwokzl9cxxohyp3wl 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/ygciqg8mfkawmroztrlh 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/faowowuclak0zfo8nqa9 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/lrnmhst9cfd5ly40moe9 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/p0x4nejio2vg2ewj7jlh 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bmbi0dnpjkbgs3fclthv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/kb17kbppyw7ykj0zrnou 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/uiwq1n28muzqmvbnwwyl 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/syvuw8hizr4ltbmlchyf 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/heoaxgupuv2ku1zzul2w 1920w"></figure><p>总的来说，我认为这些词典还不错，但并不令人惊奇。</p><p>我们训练这些字典是因为我们想要进行字典学习的下游应用，但缺乏字典。这些词典足以让我们开始我们的主线项目，但我预计不久之后我们将回来训练一些更好的词典（我们也将开源）。我认为对其他人来说也是如此：这些词典应该足以开始需要词典的项目；当以后有更好的词典可用时，您可以更换它们以获得最佳结果。</p><p>关于这些词典的一些杂项注释（您可以在<a href="https://github.com/saprmarks/dictionary_learning">repo</a>中找到更多信息）。</p><ul><li> <code>1_32768</code>的 L1 惩罚似乎太大了；只有10-20%的神经元还活着，恢复的损失就更严重了。也就是说，我们会注意到，在检查了两组词典的特征后， <code>1_32768</code>集中的词典似乎比<code>0_8192</code>集中的词典具有更多可解释的特征（尽管很难说）。<ul><li>特别是，我们怀疑对于<code>0_8192</code> ，后面层中的许多高频特征是无法解释的，但对重建激活有很大帮助，<strong>从而产生看似漂亮的统计数据</strong>。 （请参阅下面有关神经元重采样和双峰性的要点。）</li></ul></li><li>随着我们逐层推进，字典在大多数指标上往往会变得更糟（恢复损失百分比除外）。这可能与当人们穿过 pythia 模型各层时激活本身的规模不断扩大有关（感谢 Arthur Conmy 提出这一假设）。</li><li>我们注意到，我们的字典特征的频率明显高于<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Anthropic</a>和<a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">Neel Nanda</a>中的特征。我们不知道这种差异是因为我们正在使用多层模型还是因为超参数的差异。我们通常怀疑如果我们学习频率较低的特征会更好。<ul><li>然而，我们会注意到，在第 0 层之后，我们的许多功能似乎并不具有“总是在特定令牌上触发”的形式，而 Anthropic 的许多功能都是如此。因此，更有趣的功能也可能出现频率更高。看看<a href="https://www.lesswrong.com/posts/tEPHGZAb63dfq2v8n/?commentId=zXEsbbJHsg98FY6uj">这里</a>有一些味道。</li></ul></li><li>我们不确定，但<code>0_8192</code>的直方图中的双峰性可能是由于死亡神经元被重新采样所致。我们每 30000 个步骤重新采样一次，包括 100000 个总步骤中的第 90000 个步骤。重采样的特征往往频率非常高，峰值可能需要超过 10000 步才能向左移动。</li></ul><h1>字典学习库</h1><p>同样，这可以<a href="https://github.com/saprmarks/dictionary_learning">在这里</a>找到。我们遵循<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder">Anthropic 论文</a>中详细介绍的方法（包括使用不受限的编码器/解码器权重、限制解码器向量具有单位范数，以及根据其古怪的方案对死亡神经元进行重新采样），但以下情况除外：</p><ul><li>我们没有足够的空间来存储整个数据集的激活，因此，按照<a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">Neel Nanda 的复制</a>，我们维护一个来自几千个上下文的令牌缓冲区，并从该缓冲区中随机采样，直到它是半空的（此时我们刷新它带有来自新上下文的标记）。</li><li>我们使用简短的线性学习率预热来解决 Adam 会在前几个训练步骤中杀死太多神经元的问题，然后才有机会校准 Adam 参数。</li></ul><p> （一个简单的插件：这个存储库是使用<a href="http://nnsight.net/">nnsight</a>构建的，这是一个新的可解释性工具库（如<a href="https://github.com/neelnanda-io/TransformerLens">Transformer_lens</a>和<a href="https://github.com/davidbau/baukit">baukit</a> ），由 Jaden Fiotto-Kaufman 和<a href="https://baulab.info/">Bau 实验室</a>的其他人开发。 <code>nnsight</code>仍在开发中，所以我只建议尝试深入研究如果你对偶尔的错误、内存泄漏等没问题的话，现在就进入它（你可以在<a href="https://discord.gg/JqMpyYtS">这个 Discord 服务器</a>的反馈通道中报告）。但总的来说，我对这个项目非常兴奋——除了提供一个非常干净的用户之外根据经验，一个主要设计目标是<code>nnsight</code>代码具有高度<i>可移植性</i>：理想情况下，您应该能够使用 Pythia-70m 制作实验原型，无缝切换到跨多个 GPU 的 LLaMA-2-70B 上运行，然后将代码发送到人择将在克劳德身上运行。）</p><p>除了主线功能之外，我们的存储库还支持一些实验性功能，我们简要研究了这些功能作为训练词典的替代方法：</p><ul><li> <strong>MLP 担架。</strong>基于人们可能能够识别“<a href="https://transformer-circuits.pub/2022/toy_model/index.html">足够大的模型中的神经元</a>”特征的观点，我们尝试训练“自动编码器”，以给定 MLP<i>输入</i>激活<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>作为输入， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="MLP(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">输出</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">MLP</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">（</span></span></span></span></span></span></span> MLP 输出） ）。例如，给定一个 MLP，它将 512 维输入<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>映射到 1024 维隐藏状态<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span> ，然后映射到 512 维输出<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> ，我们训练一个隐藏维度为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="16384 = 16 \times 1024"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16384</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1024</span></span>的</span></span></span></span></span>字典<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> ，使得<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> （并且像往常一样，因此字典的隐藏状态是稀疏的）。<ul><li>最终的词典看起来不错，但我们决定不再进一步追求这个想法。</li><li> （向 Max Li 提出此建议。）</li></ul></li><li><strong>用熵代替 L1 损失</strong>。基于这篇<a href="https://transformer-circuits.pub/2023/may-update/index.html#simple-factorization">文章</a>中的想法，我们尝试使用熵来规范字典的隐藏状态而不是 L1 损失。这似乎导致这些功能要么是死功能（从未触发），要么是在几乎每个输入上触发的非常高频的功能，这不是所需的行为。但似乎有一种方法可以让这项工作变得更好。</li></ul><p>如果您想追求上述要点中的想法之一，请在获得初步结果后与我（Sam）联系 - 我可能有兴趣讨论结果或合作。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxki2bn9t6a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxki2bn9t6a">^</a></strong></sup></span><div class="footnote-content"><p>这既是为了可重复性，也是因为每个字典都需要一些努力来训练。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp7b5kczep0n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp7b5kczep0n">^</a></strong></sup></span><div class="footnote-content"><p>当然，来自<a href="https://arxiv.org/abs/2309.08600">Cunningham 等人的存储库。纸张</a>也可以<a href="https://github.com/HoagyC/sparse_coding">在这里</a>购买。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning<guid ispermalink="false"> AaoWLcmpY3LKvtdyq</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Tue, 05 Dec 2023 06:05:21 GMT</pubDate> </item><item><title><![CDATA[The LessWrong 2022 Review]]></title><description><![CDATA[Published on December 5, 2023 4:00 AM GMT<br/><br/><p>雪花飘落，颂歌开始响起，我们都知道是时候开始我们最喜欢的冬季假期传统了。审稿时间少了！ </p><figure class="image image_resized" style="width:69.55%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/swcjknwqnq58jqjizlbu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/iunw2eqowsdnnqfksyra 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/gx5phndd8vqwcthofkdb 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/wmh2aidexazx07hj2dfu 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/ahkmuaycxg9peesnssqr 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/ktgsgz7mjgpix2gho83k 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/zu2vamwcsp7qmxac899u 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/q4dfchxbyjvzz33q0olz 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/h8sonntrtlw1vx8tfek1 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/jlbb43705pckl5ufchsf 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/pqylqykbwbqxdf0ebiud 1024w"></figure><p>每年我们都会聚在一起审查至少一年前的帖子。这意味着在接下来的两个月里，我们将审核 2022 年的所有帖子。</p><p>虽然我们的日常生活充满了时尚，追逐着业力和社会认可的甜蜜滋味，但LessWrong的评论是时候退一步问自己“这真的能帮助我更好地思考吗？”，“这真的是事实吗？变得有价值？”以及“哪些事情经受住了进一步和广泛的审查？”。</p><p>到目前为止，我们已经这样做了 4 次（ <a href="https://www.lesswrong.com/posts/3yqf6zJSwBF34Zbys/2018-review-voting-results">2018 年</a>、 <a href="https://www.lesswrong.com/posts/kdGSTBj3NA2Go3XaE/2019-review-voting-results">2019 年</a>、 <a href="https://www.lesswrong.com/posts/TSaJ9Zcvc3KWh3bjX/voting-results-for-the-2020-review">2020 年</a>、 <a href="https://www.lesswrong.com/posts/zajNa9fdr8JYJpxrG/voting-results-for-the-2021-review">2021 年</a>）。</p><p>年度审核如何运作的完整技术细节位于本文的<a href="https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review#How_does_the_review_work_">最后部分</a>，但与过去几年基本相同。分为三个阶段：</p><ol><li><strong>初步投票阶段</strong><i>（2 周，12 月 4 日至 17 日）</i> ：我们在进行初步投票的审核中确定特别值得考虑的帖子。获得 2 票初步投票的帖子进入讨论阶段。</li><li><strong>讨论阶段</strong><i>（4 周，12 月 17 日 — 1 月 14 日）</i> ：<i> </i>我们审查和辩论帖子。收到至少一篇书面评论的帖子将进入最终投票阶段。</li><li><strong>最终投票</strong><i>（2 周，1 月 14 日至 1 月 28 日）</i> ：我们使用二次投票进行完整投票。其结果决定年度评审结果。</li></ol><p>有关年度审核理念的更多信息，请参阅之前的公告帖子：<a href="https://www.lesswrong.com/posts/qXwmMkEBLL59NkvYR/the-lesswrong-2018-review">此处</a>、 <a href="https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review#Improving_our_incentives_and_rewards">此处</a>、<a href="https://www.lesswrong.com/posts/M9kDqF2fn3WH44nrv/the-2020-review">此处</a>和<a href="https://www.lesswrong.com/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion">此处</a>。</p><h2>入门</h2><p>在任何符合审核资格的帖子的顶部，您都会看到： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672905457/mirroredImages/qCc7tm29Guhz6mtf7/d0fpa6iead1yz8j7y10n.png"></figure><p>这些将是您对 2022 年审核的初步投票。帖子需要获得至少 2 票初步投票（正面或负面）才能进入下一阶段的审核。</p><p>要开始仔细阅读帖子，我建议转到<a href="https://www.lesswrong.com/allPosts?timeframe=yearly&amp;after=2022-01-01&amp;before=2023-01-01&amp;limit=100&amp;sortedBy=top&amp;filter=unnominated&amp;includeShortform=false"><u>“2022 年所有帖子”页面</u></a>，或<a href="https://www.lesswrong.com/votesByYear/2022"><u>“查看您过去的点赞”</u></a>页面。<i>注意：只有2022年1月之前注册账户的用户才有资格投票。</i></p><h2>今年没有书了，抱歉各位</h2><p>2018年、2019年和2020年，我们印制了审查结果的书籍。我们已经售出了数千件，我为它们感到非常自豪，很多人告诉我，这些是他们最喜欢的东西之一： </p><figure class="table"><table style="border-color:white;border-style:solid"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/hkxvgpqalnv6oevaxned"><figcaption> 2018：反映领土的地图（ <a href="https://www.amazon.com/Map-that-Reflects-Territory-LessWrong/dp/1736128507/ref=sr_1_1?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728381&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-1">亚马逊</a>） </figcaption></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/kcpulhgcmm5hsxlnaxga"><figcaption> 2019：认知引擎（ <a href="https://www.amazon.com/Engines-Cognition-Essays-LessWrong-Community/dp/1736128515/ref=sr_1_2?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728381&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-2">亚马逊</a>） </figcaption></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/noak9x8qlrktfpottnxx"><figcaption> 2020：现实的雕刻（ <a href="https://www.amazon.com/Carving-Reality-Essays-LessWrong-Community/dp/B0C95MJJBK/ref=sr_1_5?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728398&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-5">亚马逊</a>）</figcaption></figure></td></tr></tbody></table></figure><p>遗憾的是，今年不会有书（也不会有 2021 年的书评）。随着我们许多其他项目的需求不断增加（以及资金的减少，因为如果考虑到每年 4-5 个员工月的制作成本，我们在以下方面净损失了资金）这些）。</p><p>我正在考虑其他方法来创建一个易于参考的工件，以捕获今年和去年的审查结果。我认为我想做的最低限度是创建一本好的电子书，也许还可以使用我们的机器旁白（或进行人类旁白）制作一个有声版本。欢迎提出其他建议。</p><p>我们将在接下来的几天内对前几年的所有书籍进行圣诞特卖，希望在圣诞节之前我们还将推出一本包含去年评论结果的优秀电子书（甚至可能是有声读物版本）。</p><h1>审核如何进行？</h1><h2>第一阶段：初步投票</h2><p>要提名职位，请对其进行初步投票。符合资格的选民将看到此用户界面： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672905457/mirroredImages/qCc7tm29Guhz6mtf7/d0fpa6iead1yz8j7y10n.png"></figure><p>如果您认为某个帖子是重要的智力贡献，您可以投票表明其大致的重要性。对于一些粗略的指导：</p><ul><li> 1 票表示“这很好”。</li><li> 4 票意味着“这非常重要”。</li><li> 9 票意味着它是“智力进步的一个重要部分”。</li></ul><p>您可以在帖子页面的顶部或帖子出现在列表中的任何位置（例如<a href="https://www.lesswrong.com/allPosts?timeframe=yearly&amp;after=2022-01-01&amp;before=2023-01-01&amp;limit=100&amp;sortedBy=top&amp;filter=unnominated&amp;includeShortform=false"><u>“所有帖子”页面</u></a>或新的<a href="https://www.lesswrong.com/votesByYear/2022"><u>“查看您过去的投票</u></a>”页面）进行投票。</p><p>获得至少一票赞成票的帖子将进入<a href="https://lesswrong.com/reviewVoting/2022">投票仪表板</a>，其他用户可以在其中投票。我们鼓励您根据去年的记忆至少进行一次粗略的投票。稍后改变主意是可以的（鼓励！）。</p><p><strong>写一篇简短的评论</strong></p><p>如果您认为某篇文章很重要，我们还鼓励您至少写一篇简短的评论，说明该文章的突出之处及其重要性。 （如果您想先记下您的快速印象，然后再进行更详细的审核，欢迎您对一篇文章进行多篇评论）</p><p>至少有一条评论的帖子会被排序到<a href="https://www.lesswrong.com/reviewVoting">要投票的帖子列表</a>的顶部，因此，如果您希望某个帖子获得更多关注，对其进行评论会很有帮助。</p><p><strong>为什么要进行初步投票？为什么要分两个投票阶段？</strong></p><p>每年，LessWrong 上都会写出更多的帖子。 2018 年第一次审查考虑了 1,500 个职位。 2021 年，这一数字为 4,250。处理这么多帖子是一项艰巨的工作。</p><p>初步投票旨在帮助处理帖子数量的增加。我们不是简单地提名职位，而是直接从投票开始。这些初步投票随后将被公布，只有至少两人投票的帖子才会进入下一轮。</p><p>在审核阶段，这使得各个网站成员能够注意到某些内容的放置是否特别不准确。如果您认为某个帖子的排名不准确，您可以写一篇积极的评论，认为它应该更高，其他人可以在最终投票时考虑这一点。获得大量中等选票的帖子可能会在审核阶段被取消优先级，从而使我们能够专注于最有可能对最终结果产生影响的对话。</p><p><strong>初步投票是如何计算的？</strong></p><p>您可以投不限数量的选票，但超过一定阈值后，您的选票总分越大，您每张选票的影响力就越小。在后端，我们使用<a href="https://www.lesswrong.com/posts/qQ7oJwnH9kkmKm2dC/feedback-request-quadratic-voting-for-the-2018-review"><u>修改后的二次投票系统</u></a>，该系统根据投票的强度在您的投票中分配固定数量的分数。</p><p><i>细节：1 票得 1 分。 4 票得 10 分。 9 票需要 45 分。如果您花费的积分超过 500 点，您的投票就会开始按比例变弱。</i></p><h2>第二阶段：评论</h2><p>第二阶段为期一个月，完全专注于撰写评论。评论是评估帖子的特殊评论。评论中需要回答的好问题包括：</p><ul><li>这篇文章给对话添加了什么？</li><li>这篇文章对您、您的想法和行动有何影响？</li><li>它的主张是否准确？它是否在关节处雕刻了现实？你怎么知道？</li><li>您可以测试这篇文章的从属声明吗？</li><li>您希望在这篇文章的基础上看到哪些后续工作？</li></ul><h2>第三阶段：最终投票</h2><p>至少收到一项审核的帖子将进入最终投票阶段。</p><p>用户界面将要求选民在最终确定对每个帖子的投票之前至少简要浏览一下评论，因此可以考虑有关每个帖子的争论。</p><p>和往年一样，我们将公布1000+karma的用户以及所有用户的投票结果。 LessWrong 审核团队将把投票结果作为将哪些帖子纳入 2022 年最佳帖子序列的有力指标。</p><p><strong>首先，您可以</strong><a href="https://www.lesswrong.com/votesByYear/2022"><strong><u>查看您过去的点赞</u></strong></a><strong>并开始对某些帖子进行投票。</strong></p><br/><br/><a href="https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review<guid ispermalink="false"> B6CxEApaatATzown6</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Tue, 05 Dec 2023 04:00:00 GMT</pubDate> </item><item><title><![CDATA[Bands And Low-stakes Dances]]></title><description><![CDATA[Published on December 5, 2023 3:50 AM GMT<br/><br/><p><span>当我开始在波士顿地区参加反对派舞会时，有几种你可以考虑“低风险”的地区舞会。小型舞会，费用不高，老牌乐队的热情也较低。因此，他们相对愿意预订那些刚刚起步、友好、鼓励、能够容忍缺乏经验的错误的乐队。</span></p><p>我认为这就是为什么这么多实力雄厚的乐队和舞蹈音乐家走出这个领域的一个重要原因：不需要你足够优秀，能够在“大型”舞蹈中保持自己的风格，就可以开始获得作为乐队跳舞的经验。 。例如，回顾一下我的日历，在我们玩我们的第一个“大型”游戏（<a href="https://challcontra.weebly.com/">协和星期五</a>）之前， <a href="https://www.freeraisins.com/">Free Raisins</a>玩了十八个不同的晚上。</p><p>不幸的是，这比以前少了很多。一些在大流行前关闭了（我非常想念麻省理工学院的反对派舞蹈！），其他人还没有回来（还没有？）或者已经转向室内乐队。有<a href="https://www.bidadance.org/">BIDA</a><a href="https://www.jefftk.com/p/why-does-the-bida-open-band-work-well">开放乐队</a>和（风险更低的）家庭舞蹈乐队，但虽然我认为这种经验也很有价值，但有一种不同的学习方式，来自于在小组中演奏、演奏你已经练习过的曲目，以及对音乐完全负责。</p><p>我在这里没有一个很好的解决方案：很难故意开始一些低风险的事情，而且人们通常更喜欢举办那种很多人都想参加并且有很棒音乐的活动。但我确实认为 2010 年代初期的环境非常特别，为新乐队提供了很多学习成为舞蹈乐队的机会，如果我们能够带回类似的东西，或者至少具有类似效果的东西，我会很高兴。</p><p> （这种同样的动力对于“技术魂斗罗”来说更加强烈：唯一的表演机会是重大的特别活动，甚至更高调！但我对这种舞蹈形式的成功投入也较少，所以这不太困扰我。 ）</p><br/><br/><a href="https://www.lesswrong.com/posts/atwkvWcSppkKN9dRJ/bands-and-low-stakes-dances#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/atwkvWcSppkKN9dRJ/bands-and-low-stakes-dances<guid ispermalink="false"> atwkvWcSppkKN9dRJ</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Tue, 05 Dec 2023 03:50:22 GMT</pubDate> </item><item><title><![CDATA[Accelerating science through evolvable institutions]]></title><description><![CDATA[Published on December 4, 2023 11:21 PM GMT<br/><br/><p><i>这是向圣达菲研究所“加速科学”工作组提交的演讲的书面版本。</i></p><p>我们来这里是为了讨论“加速科学发展”。我喜欢从历史的角度来开始讨论这样的话题：科学在过去什么时候（如果有的话）加速了？现在还在加速吗？我们可以从中学到什么？</p><p>我认为，在整个人类历史中，科学以及更广泛的人类知识<i>一直</i>在加速发展。我还不能证明这一点（而且我自己对此只有大约 90% 的把握），但让我诉诸你的直觉：</p><ul><li><a href="https://en.wikipedia.org/wiki/Behavioral_modernity">从行为上看，现代人类</a>已有 50,000 多年的历史</li><li>文字只有大约 5000 年的历史，因此在人类时间线的 90% 以上，我们只能积累能够适应口头传统的知识</li><li>在古代和中世纪世界，我们只有少数几门科学：天文学、几何学、一些数论、一些光学、一些解剖学</li><li>在科学革命之后的几个世纪（大约 1500 年代至 1700 年代），我们得到了日心说、运动定律、万有引力理论、化学的起源、细胞的发现、更好的光学理论</li><li>在 1800 年代，事情真正开始发展，我们有了电磁学、原子理论、进化论、细菌理论</li><li>1900 年代，核物理、量子物理、相对论、分子生物学和遗传学继续蓬勃发展</li></ul><p>我把自 1950 年左右以来科学是否已经放缓的问题放在一边，我对此没有强烈的看法。即使确实如此，这也只是整个历史加速的总体模式中最近的一个小插曲。 （或者，你知道，历史上前所未有的逆转和衰退的开始。其中之一。）</p><p>我对这种加速模式深信不疑的部分原因是，加速的不仅仅是科学：几乎所有衡量人类进步的指标都显示出相同的趋势，包括<a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia?yScale=log">世界 GDP</a>和<a href="https://ourworldindata.org/grapher/population?yScale=log&amp;country=~OWID_WRL">世界人口</a>。</p><p>是什么推动了科学的加速发展？许多因素，包括：</p><ul><li><strong>资金。</strong>曾经，科学家必须<a href="https://rootsofprogress.org/funding-models-for-science-and-innovation">寻求赞助，或者独立致富</a>。现在有可用的赠款，并且资金总额在过去几十年中大幅增加： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/dhxfk91oznfay66wdxnn" alt=""><figcaption><a href="https://www.aaas.org/programs/r-d-budget-and-policy/historical-trends-federal-rd"><i>美国科学促进会</i></a></figcaption></figure><ul><li><strong>人们。</strong>更多的科学家（在其他条件相同的情况下）意味着科学发展得更快，科学家的数量急剧增加，这既是因为总体人口的增长，也是因为更多的劳动力进入研究领域。在<a href="https://archive.org/details/sciencesincebaby0000pric/page/107/mode/1up?view=theater"><i>《自巴比伦以来的科学》一</i></a>书中，德里克·J·德·索拉·普赖斯 (Derek J. de Solla Price) 表示，“历史上大约 80% 到 90% 的科学家现在还活着”，这<a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/">可能仍然是正确的</a>： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/wh3ldfdjulipzfcrroyg" alt=""><figcaption> <a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/"><i>埃里克·加斯特弗兰德</i></a></figcaption></figure><ul><li><strong>仪器。</strong>更好的工具意味着我们可以做更多更好的科学研究。伽利略有一个简单的望远镜。现在我们有<a href="https://en.wikipedia.org/wiki/James_Webb_Space_Telescope">JWST</a>和<a href="https://en.wikipedia.org/wiki/LIGO">LIGO</a> 。</li><li><strong>计算。</strong>更强的计算能力意味着更多更好的数据处理方式。</li><li><strong>沟通。</strong>思想传播得越快越好，科学传播就越高效、越有效。科学期刊是在印刷机发明之后才发明的。互联网支持预印本服务器，例如 arXiv。</li><li><strong>方法。</strong>更好的方法造就更好的科学，从培根经验主义到<a href="https://en.wikipedia.org/wiki/Koch%27s_postulates">科赫假设</a>再到<a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">随机</a>对照试验（实际上是所有统计数据）。</li><li><strong>机构。</strong>实验室、大学、期刊、资助机构等共同构成了一个支持现代科学的生态系统。</li><li><strong>社会地位。</strong>科学越受到尊重和声望，就会有越多的人和金钱流入它。</li></ul><p>现在，如果我们想问科学是否会继续加速发展，我们可以思考哪些驱动因素将继续增长。我建议：</p><ul><li>只要世界经济持续增长，科学经费就会继续增长</li><li>仪器、计算和通信将随着技术的发展而不断改进</li><li>我认为方法没有理由不继续改进，作为科学本身的一部分</li><li>科学的社会地位似乎相当强大：它是一个受人尊敬和享有盛誉的机构，获得了一些社会最高荣誉</li></ul><p>从长远来看，如果<a href="https://ourworldindata.org/grapher/comparison-of-world-population-projections">世界人口像预计的那样趋于稳定</a>，我们可能会耗尽继续扩大研究人员基础的人员，这是一个潜在的问题，但不是我今天的重点。</p><p>最大的危险信号是我们的科学机构。制度影响所有其他因素，尤其是资金和人才的管理。今天，元科学界的许多人对我们的机构感到担忧。常见的批评包括：</p><ul><li><strong>速度。</strong>获得资助很容易需要 12-18 个月的时间（如果你幸运的话）</li><li><strong>高架。</strong>研究人员通常将 30-50% 的时间花在资助上</li><li><strong>耐心。</strong>研究人员认为他们需要定期展示结果，并且不能走一条可能需要多年才能得出结果的道路</li><li><strong>风险承受能力。</strong>赠款资金倾向于保守的、渐进的建议，而不是大胆的、“高风险、高回报”的计划（尽管<a href="https://commonfund.nih.gov/highrisk">做出了相反的努力</a>）</li><li><strong>共识。</strong>一个领域可能会过快地集中于一个假设并修剪替代的研究分支</li><li><strong>研究员年龄。</strong>随着时间的推移，赠款的趋势是拨款给年龄更大、更成熟的研究人员</li><li><strong>自由。</strong>科学家缺乏完全自主地指导研究的自由；赠款资金附加太多条件</li></ul><p>现在，作为一名前科技创始人，我不禁注意到，在营利性风险投资领域，大多数问题似乎都得到了缓解。筹集风险投资资金相对较快（通常一轮融资会在几个月内完成，而不是一年或更长时间）。作为创始人/首席执行官，我花了大约 10-15% 的时间筹款，而不是 30-50%。风险投资公司大胆下注，积极寻求逆向立场，并支持年轻的新贵。他们大多给予创始人自主权，也许会在董事会中占据一席之地以进行治理，并且只有在表现非常糟糕时才会解雇首席执行官。 （上面列出的初创公司创始人可能还会抱怨的唯一问题是耐心：如果你的钱用完了，你最好能取得进展，否则你在下一轮融资时就会遇到困难。）</p><p>我不认为风险投资界在这些方面做得更好，因为风险投资家比科学资助者更聪明、更有智慧或更优秀——但事实并非如此。相反，风险投资家：</p><ul><li>争夺优惠（并且真的不想错过好优惠）</li><li>从长远来看，成功或失败取决于其投资组合的表现</li><li>在大约 5-10 年内看到这些结果</li></ul><p>简而言之，<strong>风险投资面临着进化压力。</strong>他们不能陷入明显的不良均衡，因为如果这样做，他们就会在竞争中落败并失去市场力量。</p><p>证明这一点的是风险投资在过去几十年里<i>的</i>发展——主要是朝着为创始人提供更好待遇的方向发展。例如，早期阶段存在较高估值的长期趋势，这最终意味着较低的稀释度以及权力从风投向创始人的转移：创始人在过去的几年里放弃公司一半或更多的股份是很常见的。第一轮融资；最后我检查了一下，大约是 20% 或更少。风险投资并不总是资助大学刚毕业的年轻技术人员。曾经有一段时间，他们倾向于青睐更有经验的首席执行官，或许还拥有 MBA 学位。他们并不总是支持创始人领导的公司；曾经，创始人在最初几年后被解雇并由专业首席执行官取代的情况很常见（当 A16Z 在 2009 年推出时，他们大肆宣扬<a href="https://a16z.com/why-we-prefer-founding-ceos/">他们不会这样做</a>）。</p><p>所以我认为<strong>，如果我们希望看到我们的科学机构</strong><i><strong>得到改进</strong></i><strong>，我们需要考虑它们如何</strong><i><strong>发展</strong></i><strong>。</strong></p><p>我们的科学机构的发展程度如何？不是特别的。当今大多数科学组织都是大学或政府部门。尽管我很尊重大学和政府，但我认为任何人都必须承认它们是我们行动较为缓慢的机构之一。 （大学尤其具有极强的弹性和抵抗力：例如，牛津大学和剑桥大学的历史可以追溯到中世纪，经历了帝国的兴衰，直到今天仍然完好无损。）</p><p>科学资助机构的进化所面临的挑战与风险投资的进化相反：</p><ul><li><strong>他们往往缺乏竞争，</strong>尤其是 NIH 和 NSF 等集中式联邦机构</li><li><strong>他们缺乏任何真正的反馈循环</strong>，在这种循环中，资助者的资源是由过去的判断和其投资组合的成功决定的（迈克尔·尼尔森多次<a href="https://twitter.com/michael_nielsen/status/1451626771690897408">指出</a>，从“爱因斯坦作为专利职员做了最好的工作”到“卡塔林·卡里科”的资助失败）在获得诺贝尔奖之前被拒绝获得资助和终身教职”似乎甚至没有引发相关机构内部的反思过程）</li><li><strong>他们需要很长的周期</strong>才能了解其工作的真正影响，而这种影响可能需要 20-30 年才能显现出来</li></ul><p>我们如何提高科学经费的可进化性？我们应该思考如何改善这些因素。我没有什么好主意，但我会抛出一些不成熟的想法来开始对话：</p><p><strong>我们如何增加科学资助的竞争？</strong>我们可以增强慈善事业的作用。在美国，我们可以将联邦资金转移到州一级，设立五十个资助者而不是一个。 （国家农业实验站就是一个成功的例子，这些实验站之间的竞争是杂交玉米研究的关键，这是 20 世纪农业科学最伟大的成功之一。）在国际层面，我们可以支持对科学家更加开放的移民。</p><p><strong>我们如何创建更好的反馈循环？</strong>这很困难，因为我们需要某种方法来衡量结果。实现这一目标的一种方法是将资金从预期赠款转向各级各种回顾性奖项。如果这个“经济”足够大和强大，这些成果就可以被金融化，以创建一个动态的、有竞争力的融资生态系统，并具有适当水平的风险承担和耐心，经验丰富的退伍军人与年轻特立独行者之间的适当平衡等.（ <a href="https://forum.effectivealtruism.org/posts/r7vmtHZKuosJZ3Xq5/altruistic-equity-allocation">影响证书</a>，例如<a href="https://protocol.ai/blog/hypercert-new-primitive/">超级证书</a>，可以成为该解决方案的一部分。）</p><p><strong>我们如何解决反馈周期长的问题？</strong>我不知道。如果我们不能缩短周期，也许我们需要延长资助者的职业生涯，这样他们至少可以从几个周期中学习——这<a href="https://rootsofprogress.org/how-curing-aging-could-help-progress">是长寿技术的潜在好处</a>。或者，也许我们需要一个科学资助者，它可以极快地学习，可以消耗大量有关研究项目及其最终结果的历史信息，永远不会忘记其经历，并且永远不会退休或死亡——当然，我想到的是人工智能。关于人工智能支持、增强或取代科学研究人员本身的讨论很多，但人工智能在科学领域的最大机会可能是在资金和管理方面。</p><p>我怀疑资助机构会在这个方向上走得太远：它们必须自愿接受竞争、加强问责并承认错误，而这种情况很少见。 （看看现在那些因卡里科获得诺贝尔奖而获得功劳的机构，他们几乎没有为她提供支持。）如果机构很难进化，那么元进化就更难了。</p><p>但也许资助者背后的资助者，即那些向资助者提供预算的资助者，可以开始将资金分配给多个机构，以要求绩效指标，或者干脆转向上述回顾性模式。这可以提供所需的进化压力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9uEAwHhoDS8shpdoX/acceleating-science-through-evolvable-institutions<guid ispermalink="false"> 9uEAwHzoDS8shpdoX</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:21:35 GMT</pubDate> </item><item><title><![CDATA[Speaking to Congressional staffers about AI risk]]></title><description><![CDATA[Published on December 4, 2023 11:08 PM GMT<br/><br/><p> 2023 年 5 月和 6 月，我（Akash）与国会工作人员就人工智能风险举行了大约 50-70 次会议。我一直想写一篇文章来反思这次经历和我的一些收获，我认为这可能是 LessWrong 对话的一个好话题。我看到他们<a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1?commentId=L2qFjT8taEhkm4hCB">提出要与人们进行 LW 对话</a>，于是我伸出了援手。</p><p>在这次对话中，我们讨论了我如何决定与工作人员聊天、我在华盛顿的初步观察、有关国会办公室如何工作的一些背景、我的会议是什么样的、我学到的教训以及关于我的经历的一些杂项。</p><h2>语境</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:14:27 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:14:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>嘿！在您的留言中，您提到了一些与您在华盛顿的经历相关的主题。</p><p>我认为我们应该从您与国会办公室谈论人工智能风险的经历开始。我很有兴趣了解更多；似乎没有太多公共资源来说明这种外展活动是什么样的。</p><p>那是怎么开始的？是什么让你想这么做？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:23:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:23:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>2023 年 3 月，我开始在<a href="https://www.safe.ai/">人工智能安全中心</a>从事一些人工智能治理项目。我的一个项目涉及帮助 CAIS 响应<a href="https://www.ntia.gov/issues/artificial-intelligence/request-for-comments">NTIA</a>发布的关于人工智能问责制的评论请求。</p><p>作为这项工作的一部分，<strong>我开始思考一个好的前沿人工智能监管框架应该是什么样子。</strong>例如：如果我可以为前沿人工智能系统建立许可制度，它会是什么样子？它会被安置在美国政府的什么地方？我希望它评估哪些信息？</p><p><strong>我开始想知道实际的政策制定者会对这些想法有何反应</strong>。我也很好奇更多地了解政策制定者如何考虑人工智能灭绝风险和灾难性风险。</p><p>我开始询问人工智能治理领域的其他人。绝大多数人（根本）没有与国会工作人员交谈过。一些人有与员工交谈的经验，但没有与他们谈论人工智能风险。很多人告诉我，他们认为与政策制定者的接触非常重要，但却被忽视了。当然，也存在下行风险，所以你不希望有人做得不好。</p><p>在咨询了 10-20 名人工智能治理人员后，我询问 CAIS 我是否可以去华盛顿并开始与国会办公室交谈。目标是（a）提高对人工智能风险的认识，（b）更好地了解国会办公室如何考虑人工智能风险，（c）更好地了解国会办公室的人们有哪些与人工智能相关的优先事项，以及 (d) 获取有关我的 NTIA 评论想法请求的反馈。</p><p> CAIS 批准了，我于 2023 年 5 月至 6 月去了华盛顿。需要澄清的是，这不是 CAIS 告诉我要做的事情——这更像是 CAIS 意识到正在发生的“阿卡什事件”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:26:38 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:26:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>哇，这真的很有趣。几个随机问题：</p><blockquote><p>当然，也存在下行风险，所以你不希望有人做得不好。</p></blockquote><p>一个人怎样才能把一件事做得不差呢？如何学习与政策制定者互动？<br><br>另外，你的背景是什么？在此之前您做过政策方面的工作吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:31:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:31:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>是的，很好的问题。我不确定最好的学习方法是什么，但我尝试过以下一些方法：</p><ul><li>与有与政策制定者互动经验的<strong>人交谈</strong>。询问他们说什么、他们发现什么令人惊讶、他们犯了什么错误、他们注意到什么下行风险等等。</li><li><strong>看书</strong>。我发现<a href="https://www.amazon.com/Master-Senate-Years-Lyndon-Johnson/dp/0394720954">参议院议长</a>和<a href="https://www.amazon.co.uk/Act-Congress-Americas-Essential-Institution/dp/0307744515">国会法案</a>特别有帮助。我目前正在阅读<a href="https://www.amazon.com/Devils-Chessboard-Dulles-Americas-Government/dp/0062276174">《魔鬼的棋盘》，</a>以更好地了解中央情报局和情报机构，到目前为止，我发现它内容丰富。</li><li>与你已经认识的政策制定者<strong>进行角色扮演</strong>，并要求他们提供直率的反馈。</li><li>在低风险会议中<strong>进行练习</strong>，并利用这些经验进行迭代。</li></ul><p>在此之前我没有做过太多政策方面的事情。在大学里，我为《哈佛政治评论》撰稿，并参与了政治研究所的工作，但这比“现实世界的政策参与”的内容更具学术性。</p></div></section><h2>抵达华盛顿特区并进行初步观察</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:32:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:32:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这一切都是有道理的。到达华盛顿后你做了什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:37:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:37:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我给国会办公室以及一些行政部门的人员发送了冷电子邮件。我还联系了华盛顿的一些 EA。我还继续处理 NTIA 征求意见书（截止日期为 6 月 6 日）。</p><p>最初的计划是召开几次会议，评估会议的进展情况，如果我认为进展相当顺利，则再召开更多会议。</p><p>总的来说，我最终与国会工作人员举行了大约 50-70 次会议（以及一些与智库人员和行政部门机构人员的会议，但我将在这篇文章中重点讨论与国会工作人员的会议）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:37:28 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:37:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>我认为他们进展得相当顺利，那么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:43:42 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:43:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>据我说，是的！我要注意的一件事是，这些可能有点难以评估——比如，员工应该对人友善，他们不会说“我以为你是个白痴”或“你浪费了我的时间”之类的话。时间”或“我现在对人工智能安全性的印象更差了。”</p><p>记住这一点：</p><ul><li><strong>我对员工们的开放态度感到惊讶。</strong>奥弗顿之窗最近发生了很大的变化，但当时，我真的不知道人们是否会说“哈！<i>灭绝风险？</i>这听起来像科幻小说。”</li><li><strong>主导氛围是“人工智能非常重要，我是一名忙碌的员工，有 100 个优先事项，所以我没有时间了解它。我</strong>真的很高兴能与能够告诉我有关人工智能的东西的人交谈– 我一直渴望跟上进度。”</li><li><strong>员工们对有机会见到愿意回答有关人工智能基本问题的人表示非常感激</strong>（例如，什么是大型语言模型，它与其他类型的人工智能有何不同？有多少公司从事前沿人工智能？）</li><li>有一些“有形”的信号表明情况进展顺利。例如，一些工作人员向我介绍了他们认识的其他人，有些人向我发送了工作，他们的办公室正在起草，甚至有几个人甚至向我介绍了国会议员（总共两个）。</li></ul></div></section><h2>国会办公室的等级制度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:44:47 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:44:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这真的很有趣！<br><br>简而言之，您能为我画一张国会办公室在人员配置方面的样子吗？就像，您通常与谁交谈，以及他们通常与国会议员有什么关系？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:50:33 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:50:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>好问题！因此，我的理解是，国会办公室通常具有以下角色，从“最有影响力的”到“最不利影响”：</p><ul><li>参谋长</li><li>立法主任</li><li>立法助理</li><li>立法通讯员</li><li>实习生和研究员</li></ul><p>也有其他角色，但是从立法角度来看，这些角色往往最重要。</p><p>请注意，每个办公室都有自己的氛围。曾经有人告诉我：“每个国会办公室都是自己的创业公司，每个国会议员都可以根据需要跑出办公室。”</p><p>因此，在某些办公室中，实习生和研究员实际上可能会产生很大的影响（例如，如果国会议员或立法主任信任实习生是特定主题的主题专家）。但是总的来说，我认为这个层次结构很普遍。</p><p>我认为我主要与立法助理/立法通讯员级别的人交谈。我还与一些立法董事进行了交谈。</p></div></section><h2>向办公室推广</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:51:24 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:51:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>好的，这一切都是有道理的。那么，您最终是如何从几次会议到60-80的呢？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:55:30 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:55:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我向技术政策工作人员发送了一封大规模的电子邮件，回答的人数给我留下了深刻的印象。这封电子邮件很短，提到我在CAIS，有1-2个子弹关于CAI的作用，并且对我正在研究NTIA的置评请求。</p><p>我认为国会工作人员现在对AI内容非常感兴趣，这是/确实是这种情况。就像，如果我向人们发送有关其他问题的电子邮件，我认为我将无法参加很多会议。</p><p>有些感觉是“ AI现在很热，但是没有人真正了解AI”。我认为目前尚不清楚持续多长时间（尤其是“人们不知道太多，而且办公室没有下定决心”）。</p><p>我会说“我认为这是一个整个AIS社区可以/应该利用更多的机会”之类的话。”就像，国会工作人员（而且我认为仍然）对与AI的人互动非常感兴趣 - 很难想象AIS社区中的人们能够获得一个更好的机会并能够担任顾问/倡导者。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:20:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:20:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>这就说得通了。</p><p>如何开始与国会工作人员互动？一个人应该做些什么来进入该空间/哪些组织可能有充分的态度来部署人们？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 20:38:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 20:38:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>这将是一个非常模糊的答案，但我认为这在很大程度上取决于人，他们的技能和政策目标。</p><p>另外 - 我在上面提到了这一点，但重申一下很重要 - 人们做这类工作的人肯定会有风险。另一方面，存在太“行动偏见”的风险，或类似的东西，并且在桌子上留下了很多价值。</p><p>这确实很难且令人困惑。我之前提到，我咨询了10-20 AI治理人员。他们中的大多数人就像“这似乎很重要，而且被忽略了，但是IDK Man似乎令人困惑。”他们中的一些人就像“是的，我完全认为您应该这样做，尤其是如果您采用XYZ战术。”一个相当杰出的AI治理人员明确告诉我，他们不想我这样做。我发现很难平衡这种矛盾的反馈。</p><p>我还认为我的很多建议将取决于某人想说的话 - 例如：</p><ul><li>他们的音高是什么？如果开会开始，工作人员说：“那么，您想谈论什么？”，最初的回应是什么？</li><li>他们是那种擅长提出问题并对其他人的世界观感到好奇的人吗？</li><li>他们要去听起来警报吗？</li><li>他们知道有关AI的很多事实吗？当他们不知道某事时，他们是否能够认识到这一点并适当地对冲？</li></ul><p>考虑到所有这些，如果阅读此书的人对与国会工作人员互动（或者在他们的组织中做到这一点）感兴趣，并且他们珍视我的意见，<strong>我建议他们通过LW与我联系。</strong>我可以在更多背景下提供更好的建议。</p></div></section><h2>典型的会议</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:47:09 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:47:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>是的，这一切都是有道理的。您能带我参加一次典型的会议吗？就像，您将如何首先与员工接触，您会在哪里遇到他们，实际对话是什么样的，您将如何跟进或以其他方式弄清楚它是否有帮助？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 21:47:20 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 21:47:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>会议物流</strong></p><ol><li>会通过电子邮件联系</li><li>通常会在国会办公室与他们会面（基本上有4个主要建筑物，这些建筑物都有所有国会办公室）或通过Zoom</li></ol><p><strong>会议如何进行</strong></p><ol><li>谈话通常是从我开始询问他们是否对AI有任何疑问或希望我分享我正在从事的事情。通常，他们希望我开始。</li><li>我将首先介绍自己和CAI。一旦<a href="https://safe-ai.webflow.io/statement-on-ai-risk">CAIS声明</a>问世，我将参考CAIS声明。我会告诉他们，我专注于高级AI的全球安全风险。我还会告诉他们我正在做NTIA的回应，我会告诉他们有关我正在考虑的一些高级想法。</li><li>然后，我会停下来看看他们是否有任何疑问。</li><li>通常，他们要么会询问有关灭绝风险的更多信息，要么问有关AI的其他问题（例如，您对如何处理深层餐厅有任何想法吗？），或者他们会提出一些有关法规的高级问题（例如，我们如何在不扼杀创新的情况下进行监管？我们如何在不输给中国的情况下进行规范？）</li><li>在一些最好的会议中，我会听到办公室正在处理的一些与AI相关的事情。大多数办公室都没有能力/兴趣来领导AI东西。大约10％的办公室就像“是的，我的国会议员对此非常感兴趣，我们正在考虑引入立法或成为别人立法的核心。”</li><li>很多人问我是否有立法草案。显然，如果您有监管想法，人们希望看到您的（简短）版本像账单一样。</li></ol><p><strong>后续</strong></p><p>NTIA的评论请求完成后，我向我遇到的所有人发送了后续措施。当我举行特别好的会议时（例如，一名工作人员对AI风险表示强烈的兴趣，或者告诉我他们想给我发送他们正在研究的事情）时，我会发送个性化的后续行动。我认为，最有力的迹象来自人们继续向我发送问题/想法，向我介绍同事或想与我合作提出建议的情况。 （明确地说，这发生在少数情况下，但我认为这是大多数影响的来源）。</p></div></section><h2>员工对AI风险的态度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:41:56 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:41:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>很多人问我是否有立法草案。</p></blockquote><p>他们正在寻找什么样的问题？您建议的任何立法吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:38:45 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:38:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>工作人员经常想知道我是否有立法草案来描述我在NTIA回应中所写的许可制度（我没有立法草案，但后来在我帮助托马斯将<a href="https://www.aipolicy.us/">AI政策中心</a>脱离中心时，有助于起草立法地面。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:39:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:39:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>啊好吧。更一般地，人们对AI风险有哪些先验？您是否认为您通常在其处理该主题的方式上发生了重大变化？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:39:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:39:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>似乎大多数人对AI风险没有强大的先验。</strong>我期望人们的先验更加怀疑（例如“什么？世界末日？<i>真的</i>？”）。但是我认为很多人都像“是的，我完全可以看到AI如何造成全球安全风险”，甚至“是的，我实际上很担心像天网一样的AI，我很高兴其他人正在工作关于这一点。”</p><p>通常，人们似乎<strong>真正担心AI的灭绝风险</strong>，但也<strong>没有任何计划</strong>。提醒我，就像“ X是生存风险”一样，这实际上是一件非常的事情 - >;“因此，我应该认真考虑在X上工作。”很多人就像“我很高兴有人在考虑这个[但是我不会去，我不希望我的国会议员会]。</p><p>就我的效果而言 - 我认为我主要是让他们更多地考虑它，并在其内部的“ AI政策优先”列表中提出了它。我认为人们忘记了员工在优先级清单上有100件事，因此仅将其暴露并将其重新曝光到这些想法可能会有所帮助。</p><p>我还遇到了一些工作人员，他们似乎非常关心AI风险，并且在AI政策领域似乎像强大的盟友一样。我仍然与几个人保持联系，当托马斯（Thomas）创立了AI政策中心时，我向其中的一群介绍了其中的一堆。如果我想通过一项账单，我认为我对哪些特定的人有更好的了解。<strong>在我看来，整个DC旅行的大部分影响都在于弄清楚盟军员工是谁并与他们建立初始关系。</strong></p><p>最后一件事是，我通常不强调失去控制//超级智能//递归自我改善。我没有隐藏它，但是我将其包含在更长的威胁模型列表中，这很少是我试图传达的主要内容。如果我再次这样做，我可能会更强调这些威胁模型。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:42:42 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:42:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>在我看来，整个DC旅行的大部分影响都在于弄清楚盟军员工是谁并与他们建立初始关系。</p></blockquote><p>啊好吧！关于员工是否同情原因的任何特征是很好的预测指标？例如特定地区，政治倾向，其他政策。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:51:23 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:51:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><blockquote><p>对于员工是否同情我们的事业，有任何很好的预测特征吗？</p></blockquote><p><br>并不真地。样本量很小。就像，总共有大概有约4名员工，我将“非常关心灭绝风险，并且处于他们可能有助于继续立法的位置”。 1名共和党人和3名民主党人。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:24:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:24:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>啊，明白了。您（在CAIS陈述发表之前）是否对该陈述进行了任何内容（措辞，外展等）的讨论？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:37:49 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:37:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>讨论不会影响陈述。该声明是在我离开DC之前写的。 （有趣的事实：我是参与起草CAIS陈述的人之一。认为对好句子做出贡献比我做的其他许多事情都要高>; 100倍，但是有时候它可以奏效100倍。那样）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:48:17 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:48:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><blockquote><p>（有趣的事实：我是参与起草CAIS陈述的人之一。认为对好句子做出贡献比我做的其他许多事情都要高>; 100倍，但是有时候它可以奏效100倍。那样）。</p></blockquote><p>该死。顺便说一句，我们生活在奇怪的世界上。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:48:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:48:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>谢谢！看到该陈述有多大重要，这绝对是奇怪的。</p><p>我认为这也很谦虚 - 当我第一次听到这一说法时（当时我们称其为公开信），我记得全都是做的，就像是“嗯，一封公开的信件？有fli暂停信。”</p><p><strong>这是一个有用的提醒，有时您可能无法预先预测某事的影响</strong>。事后看来，很明显（至少对我而言）CAIS的陈述很有用，而变化理论非常扎实。但是当时，这并不像是一个固定的总体计划。感觉这只是一个列表中的一个项目中的一个项目，它具有一种模糊的变化理论，这只是另一个值得一提的赌注。</p></div></section><h2>得到教训</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:50:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:50:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>如果您再次进行此过程，您会做些什么？您/您从中学到的，令您/您感到惊讶的主要因素是什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:58:05 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:58:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>我想我会写出一个解释我的推理的文档</strong>，记录了我咨询过的人，记录了我知道的上行和下行风险，并将其发送给了一些EAS。我认为有些谣言传播了这是以单方面的方式完成的。这很棘手，让我难过。我认为我这样做的方式实际上不是单方面的，但是我认为，通过写作推理来避免误解会更好。托马斯（Thomas）与CAIP一起做了一堆，并作为<strong>“如何在不确定性下采取不确定性采取行动，并以推理透明度和高协调能力采取行动”。</strong></p><p>我还认为我会随附<strong>立法草案</strong>（假设与我在一起的组织很满意）。如果您有立法草案，人们似乎会更加认真地对待您。</p><p>我还写了一个<strong>短得多的NTIA响应</strong>- 我们最终写了一篇大约20页的论文。我将通过较短的材料来优化更多。</p><p>啊，说到这一点，我本来会有一个<strong>打印出的1-pager</strong> ，解释了CAI是什么，并总结了NTIA响应中的监管思想。我最终完成了一半，我会尽快做到这一点。</p><p>另外，我会随附<strong>名片</strong>。人们似乎喜欢名片！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:01:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:01:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>是的，这一切都是有道理的，尽管我绝对不会事先猜到。</p></div></section><h2>决赛</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:02:52 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:02:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>我相信我已经没问题要问：您还有其他想说的吗？随意漫步。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 23:26:43 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 23:26:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>这里有一些杂项：</p><ol><li>我在DC中的经验使我认为<strong>Overton窗口非常广泛</strong>。国会没有缓存的AI政策，而且似乎很多人真正想学习。目前尚不清楚这将持续多长时间（例如，AI的风险最终会变得两极分化），但是我们似乎处于异常高的开放和好奇心。</li><li>但是，<strong>让国会做任何事情也很难</strong>。例如，出于无聊的原因，没有太多的法案获得通过。在此过程中，账单可能会死去。当需要是两党制时（他们目前这样做，因为我们有民主党参议院和共和党的众议院），这将更加正确。这主要使我更新到<strong>“哇，现状通常什么都没有发生，并且需要做很多工作才能获得任何有意义的立法。”</strong>考虑到这一点，我确实认为我们处于AI安全性的非常独特的境地（<i>实际上</i>没有多少事情构成灭绝风险和其他各种其他灾难​​性风险；而且，没有多少事情成为参议院多数党领袖的优先事项是激发与世界领导人的国际峰会，或成为整个行政命令的重点）。</li><li><strong>许多人高估了DC中“内部游戏”的数量</strong>，尤其是在国会参与方面。发生了一些秘密的事情，但是在大多数情况下，我认为没有人有球。</li><li>我想看到<strong>有关特定政策愿景的更多协调</strong>。有一段时间，您只是在关心Xrisk的酷儿童俱乐部。我认为Overton窗口已经移动了一堆，我们正处于不足以“关心Xrisk”的时刻。重要的是人们支持哪些具体政策，并愿意倡导哪些政策。</li><li>考虑到这一点，我还认为拥有更广泛的AI风险社区会有好处。<strong>实施不当的协调可以导致什么都没有完成，因为您永远无法达成共识</strong>（目前有利于领先的实验室和不受监管的规模）。<strong>太少的协调会导致缺乏联盟建设和不必要的冲突。</strong>我认为我已经从“协调良好”转变为“正确完成协调是好的，但实际上需要技巧，机智和精力才能很好地进行协调。”</li><li>我通常认为<strong>，更多的人应该公开写自己的观点。</strong>当我不知道人们相信什么时，很难协调。我认为社区应该不愿意赞美那些没有提出任何特定立场的人。 <strong>&nbsp;</strong></li><li>我学到了很多有关DC AI安全社区的知识（由“ AI安全社区”，我主要是指从事AI安全工作的人们，这是由于渴望避免Xrisk或社会规模的灾难而动机。 ，但很多人都没有）<ol><li> TLDR：很复杂。我认为，前10％的思想家很有才华，并追求了合理的变革理论。另一方面，也有很多人声称对AI政策感兴趣，但对各种AI安全威胁模型没有基本的了解。也有（真实而合理的）担心，社会上的和政治上具有竞争力的新移民可能会以威胁或削弱现有努力的方式进入该空间。</li><li>总而言之，我觉得主要的文化太蔑视了新的政策努力。我希望随着AI政策对话继续前进并吸引新的人群，情况会发生变化。我会为一个更像“啊，新人感兴趣的社区”感到兴奋，让我们给您一些提示/指示，并指出我们已经有过的特定经验，并讨论了缺点风险的具体模型。”现状通常感觉不那么具体，（在我看来）（在我看来）对新努力过于保护。我发现，这种文化使我更难清楚地思考或进行倡导，尤其是我所说的“高领域倡导”（在这里，您在其中主要尝试将您的内部世界国家传达给人们，而不是主要尝试尝试传达一系列信念，这些信念将与您的听众息息相关）。我认为，关于“直接”各种倡导努力的“直接”（我认为，有些DC的人实际上会失去他们的某些影响力/“严肃点”（如果他们完全直接））有严重的辩论，但是我仍然感到惊讶在效果的范围内 - 文化似乎不信任我和我的同龄人直接。我认为，这种文化已经大大减慢了新的政策努力，并继续以我认为对世界不利的方式威胁/削弱/恶劣的新政策努力。与许多事情一样，我认为高级问题是正确的，但是在如何应用/实施这些高级问题的确切问题上存在问题</li><li>评估各种人/计划的往绩也很难。部分原因是某些信息是秘密的，部分是因为“我们与重要利益相关者有良好关系”之类的东西是一个有用的工具步骤，但不一定会转化为影响，部分是因为许多变革理论都是基于命中的和花一些时间来产生直接影响（例如，如果某人与X建立了良好的关系，也许在某个时候X会与AI监管非常相关，但也许只有1-10％的机会是真实的。） ，我认为，如果人们最终对自己的信念更明确，对他们希望实现的特定政策目标更加明确，并且对他们清晰的胜利（和损失）更明确，那么协调会更容易。在缺乏此事的情况下，我们冒着赋予“玩游戏”，发展影响力但最终没有利用自己的影响力来实现有意义的变化的人的风险和太多资源。 （另请参见此Dominic Cummings<a href="https://www.dwarkeshpatel.com/p/dominic-cummings#details">播客</a>）。</li></ol></li><li>相关的是， <a href="https://forum.effectivealtruism.org/posts/tdaoybbjvEAXukiaW/what-are-your-main-reservations-about-identifying-as-an?commentId=gNC53rsuMNTBjLCWY">奥利弗·哈布里卡（Oliver Habryka）的这一评论</a>引起了我的共鸣。我发现，与“主流EAS”有一定距离时，我通常会更清楚地思考。有很多抗体和微妙的文化压力，可以阻止我思考某些想法，并且可以萎缩我在世界上采取指示行动的能力。 （当然，我认为解决方案不是“永远不会与EAS互动”  - 但我确实认为人们可能低估了社区对思考和实现困难的事情的负面影响。我当然是。）</li><li>对于有兴趣捐赠的人，我目前建议<strong> </strong>这<strong> </strong><a href="https://www.aipolicy.us/"><strong>AI政策中心</strong></a><strong> </strong>（尤其是托马斯·拉尔森（Thomas Larsen）继续高度参与其战略方向）。我与托马斯（Thomas）有一些战略/战术分歧，但我认为他是一个非常聪明和才华横溢的人，我认为他是AI政策支持空间中最好的新来者之一（COI：Thomas是我的朋友，我和我的朋友之一参与了在早期阶段帮助该中心的AI政策）。</li><li>如果您想与我交谈，<strong>请随时在Lesswrong上伸出援手</strong>。我喜欢与从事AI政策工作的人交谈。我也愿意接受我可能正在做的有影响力的事情，或者我知道可能正在做的其他事情。 </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:18:15 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:18:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>有</b></section><div><p>哇，好吧。感谢您进行此对话！</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2slwt2csaag74nsdn/speaking-to-compeaking-to-compressional-staffers-about-ai危险<guid ispermalink="false">2SLWT2CSAAG74NSDN</guid><dc:creator><![CDATA[Akash]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:08:52 GMT</pubDate> </item><item><title><![CDATA[Open Thread – Winter 2023/2024]]></title><description><![CDATA[Published on December 4, 2023 10:59 PM GMT<br/><br/><p>如果它值得说，但不值得单独发表，这里有一个地方可以放置它。</p><p>如果您是 LessWrong 的新手，这里是您自我介绍的地方。欢迎您就您如何找到我们以及您希望从网站和社区获得什么发表个人故事、轶事或只是一般性评论。如果您不想写完整的顶级帖子，这也是讨论功能请求和您对该网站的其他想法的地方。</p><p>如果您是社区新手，您可以开始阅读<a href="https://lesswrong.com/highlights">Sequences 的亮点</a>，这是有关 LessWrong 核心思想的帖子集合。</p><p>如果您想更多地探索社区，我建议您<a href="https://www.lesswrong.com/library">阅读图书馆</a>，<a href="https://www.lesswrong.com/?view=curated">查看最近策划的帖子</a>，<a href="https://www.lesswrong.com/community">看看您所在的地区是否有任何聚会</a>，并查看<a href="https://www.lesswrong.com/faq">LessWrong 常见问题</a><a href="https://www.lesswrong.com/faq#Getting_Started">解答</a>的入门部分。如果您想了解网站上的内容，您还可以查看<a href="https://www.lesswrong.com/tags/all">“概念”部分</a>。</p><p>开放线程标签在<a href="https://www.lesswrong.com/tag/open-threads?sortedBy=new">这里</a>。 Open Thread 序列在<a href="https://www.lesswrong.com/s/yai5mppkuCHPQmzpN">这里</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tra92qdondli6ra8u/open-thread-winter-2023-2024<guid ispermalink="false"> tra92qdondli6ra8u</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:59:51 GMT</pubDate> </item><item><title><![CDATA[Interview with Vanessa Kosoy on the Value of Theoretical Research for AI]]></title><description><![CDATA[Published on December 4, 2023 10:58 PM GMT<br/><br/><p>以下是<i>我与我的</i><a href="https://www.zenmarmotdigital.com/blog/interview-with-vanessa-kosoy"><i>博客</i></a>交叉传播的<a href="https://youtu.be/1MCRQF0_5zY?feature=shared"><i>视频采访</i></a><i>的成绩单（编辑为语法）</i> <i>。它旨在作为（相对）对</i><a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Direction_6__Metacognitive_Agents"><i>学习理论议程</i></a>的目标的（相对）的初学者友好解释<i>，以及为什么需要更多理论工作来确保AI的AI安全可靠。</i></p><p></p><p><strong>简介（由Will Petillo）：</strong>讨论AI的未来倾向于变得哲学。拥有“目标”或“理解”意味着什么？寻求权力的是想要事物的默认后果吗？还是我们独特的进化历史所产生的人类怪癖？是什么促使善良的？以这种方式进行框架问题使它们可以访问，使每个人都可以参加对话。但是，由于分歧成为直觉的冲突，这种缺乏精确度也使此类问题变得棘手。</p><p>当今的“对齐监护人”的嘉宾是凡妮莎·科索（Vanessa Kosoy），他是机器情报研究所（MIRI）和长期未来基金（LTFF）支持的独立研究人员，他介绍了安全AI的数学理论。从第一原则中的理解重点使她的工作与领先的AI实验室的“快速移动并破坏事物”的实验方法形成鲜明对比。在这次采访和其他地方，凡妮莎捍卫了一种基于理论的方法的价值，并解释了将机器学习作为基础科学的含义。</p><p></p><p> <strong>Petillo：</strong>您是如何进入AI安全的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我一直是自动赛车，所以我只是倾向于自己学习东西。小时候，我以为我会成为理论物理学家。我实际上拥有数学学士学位，但是在完成学士学位之后，我没有去学术界，而是决定从事该行业的职业，而是从事软件。</p><p>我在软件行业，特别是算法工程，主要是计算机视觉，各种角色，算法工程师，团队负责人，研发经理。我也有自己的创业公司。我是顾问，然后是10年前。我接触了AI的整个存在风险的主题，并开始认为这似乎很重要。因此，我开始对此进行枢纽，最初只是我在业余时间进行研究。然后是Miri的支持。然后，我最近也得到了长期未来基金的支持，这使我能够全职。</p><p><strong>佩蒂洛：</strong>那个过程是什么样的？您只是以一种自我指导的方式工作，然后您获得了Miri和其他来源的支持？这是怎么来的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我开始读一些我的东西，而《 miri》和《少问题》写作。我开始研究自己的想法，并写下关于少错的帖子。之后，我被邀请参加一些研讨会，一些活动，最终Miri说，好吧，似乎您在这里做一些不错的工作，所以也许我们也会为此付钱。我很棒，因为这也使我能够做更多的事情，并花更少的时间做其他事情。<br><br><strong>威尔·佩蒂洛（Will Petillo）：</strong>这里的观众有一点背景。一个受欢迎的博客少了。它最初是关于理性的，也是关于与AI相关的事物的。 Miri是机器情报研究所。我喜欢将他们描述为在酷之前进行对齐的人。告诉我更多关于Miri作为机构的信息。<br><br> <strong>Vanessa Kosoy：</strong> Miri或多或少是第一个谈论人工智能存在风险的人。 Eliezer Yudkowsky在2000年开始谈论这件事，最初Miri只是Yudkowsky，然后多年来，他们设法获得了一些资金来吸引其他研究人员。他们正在考虑以下问题：我们如何使人工智能安全，我们如何处理这个问题？我们可以提出什么样的数学理论来解决这个问题？甚至在深度学习革命开始之前，以及近年来大型语言模型的整个炒作之前。他们的大部分时间都致力于提出一些基本的数学理论，这将帮助我们保持AI的一致性。<br><br>最近，由于他们相信时间表确实很短，而且我们没有时间发展这一理论，因此他们枢纽宣传并试图影响政策。<br><br><strong>佩蒂洛：</strong>您是否加入了该枢纽？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>不，我的看法与众不同。我可以说更保守。我认为时间表并不像风险社区中的许多人那样短。我认为，如果在政策渠道中努力规范AI开发并延迟AI开发以阻止真正危险的AI的发展，那么这将成功，那么这只会花我们的时间。然后问题是：为我们花时间买什么？而且我认为，理论基础绝对是我们在拥有的时间应该做的事情中最重要的事情，或者随着我们将通过某种政策计划成功购买的时间以一种或另一种方式购买的时间。<br><br>我认为，在任何世界上，创建这一基础理论都是关键。这就是我正在做的事情。这绝对是我的个人技能和优势所在的地方，从事数学而不是政策。<br><br><strong>威尔·佩蒂洛：</strong>您提到了时间表。直觉上，我知道不可能以任何精度真正预测这些事情，但是就促使您的动机而言，您将这些东西何时需要解决的时间表？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>有些人认为AGI将在10年甚至更少。我认为这有点极端。但是，当需要解决问题时，越早越好，对吧？如果我们在五年内有解决方案，那么我们比只有10年内的解决方案要好，这仍然比我们只有20年之内的解决方案要好。<br><br>实际上，我个人的看法是，实际上还有数十年的时间才真正达到存在存在风险的AI。因此，这给了我们更多的时间，但不是无限的时间。<br><br><strong>佩蒂洛（Petillo）：</strong>是什么让您点击了您要处理的事情？<br><br>凡妮莎·科索伊<strong>（Vanessa Kosoy）：</strong>它最初更像是一种好奇心，因为它实际上是从我完全随机发现的开始，您可以在AGI上说一些论文，而不是关于AI的一致性或风险或类似的东西，而是JürgenSchmidhuber和Marcus Hutter的一些论文有一些关于Agi的想法。我一直是数学书呆子，所以有一些数学框架来思考AGI似乎真的很酷。我开始阅读有关这件事的文章，最终也发现错误的错误，有些人也在讨论这种事情。<br><br>一方面，我正在阅读Eliezer Yudkowsky在主题上写的越来越多的东西，而Lesswrong上的人们写了这一主题，但我也开始考虑数学模型。最终，它引起了我的注意，当您考虑实际数学时，就没有数学上的原因，为什么AI不得不关心人类或完全关心与我们作为人类关心的东西保持一致的数学原因。<br><br>另一方面，似乎比我们更有能力。我认为这很明显，但是对我来说，我喜欢通过数学理解一切。因此，当我看到您实际上可以将其放入数学模型中时，它确实对我来说确实是真实的，这是我们应该真正关注的事情。</p><p><strong>威尔·佩蒂洛（Will Petillo）：</strong>没有理由假设AI一定会很好的想法，这听起来很像尼克·博斯特罗姆（Nick Bostrom）写的正交论文。智力以及好事不必在一起；任何一组值都可以使用任何级别的智能。从本质上讲，这是您的见解吗？<br><br> <strong>Vanessa Kosoy：</strong>是的，这正是术语。事后看来，这似乎是一件明显的事情。但是对我来说，有必要看到您实际上可以用数学对象来考虑它。值可以形式化为效用函数。然后，代理可以被形式化为某种优化器，某种贝叶斯最佳策略或该实用程序功能的任何其他策略。实际上，您可以将严格的含义放在每个术语后面，并看到这实际上都是有道理的，而不仅仅是某种哲学上的挥舞技巧。<br><br><strong>佩蒂洛（Petillo）：</strong>您认为，使AI对齐问题的根本原因是什么？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我认为问题很难。我认为，首先，很难的是，我们的目标是一个非常狭窄的目标，因为人类的价值观非常复杂和具体。我们关心许多非常详细的事物：爱，友谊，美丽（以我们自己的主观理解），性，所有这些都是人类事物，因为某些复杂的进化事故，它发生在某些人身上的方式非常特殊的星球。这是特定宇宙历史上非常特别的一点。这组值是您可以想象的可能值或思想的巨大空间的非常非常狭窄的一部分。<br><br>因此，按照我们的标准，大多数人绝对不是对我们很好的任何东西。更糟糕的是，这种现象在他的一篇相对较新的帖子中也很好地表达了这种现象，他写道，围绕特工能力有一个吸引人的盆地，但对代理人的一致性没有吸引力的盆地。这意味着，即使您采用足够的优化压力，即使使用蛮力技术，您最终将产生高功能的代理。一个例子是进化，对吗？进化是一种非常原始的蛮力算法，最终创造了人类大脑，这是一种更复杂的算法。如果您将足够的蛮力优化用于寻找在开放世界环境中成功的事物，最终您将遇到聪明的代理商。 And this is even before you put in recursive self improvement in the equation, which makes it even stronger as this kind of basin of attraction where you converge to. Whereas nothing of the sort is true about being aligned to human values in particular. It&#39;s very plausible that we could, by just kind of blind or semi blind trial and error, arrive at creating highly capable agents long before we understand enough to actually make those agents aligned.<br><br> <strong>Will Petillo:</strong> That sounds like going against the other Bostrom-popularized idea of Instrumental Convergence, that things like survival will be wanted by almost any system optimizing hard enough.<br><br> <strong>Vanessa Kosoy:</strong> There&#39;s this notion of instrumentally convergent goals, which are certain goals that most intelligent agents will pursue because they help them to achieve their terminal goals, whatever their terminal goals are. And these are things like survival, like gaining more resources, becoming more intelligent, and so on. But human values are not that. If we manage to construct an AI that survives and gains a lot of resources, that&#39;s nice for the AI, I guess, but it doesn&#39;t help us in terms of alignment to our values at all. That&#39;s a very different kind of thing.<br><br> <strong>Will Petillio:</strong> Does the fact that modern AI is trained on human-generated data and exists in human society not help?<br><br> <strong>Vanessa Kosoy:</strong> I think it helps, but it kind of leaves a lot of questions. One question is: okay, you learn from human generated data, but how do you generalize from there? Because it&#39;s really not clear what conditions are needed to get a good generalization, especially when the concept you&#39;re learning is something extremely complicated.<br><br> The higher the complexity of the concept you&#39;re learning, the more data points you need to learn it. What we&#39;re doing with the so-called Large Language Models, which are the hype in recent years, is trying to imitate humans. Which is, I mean, nice. It could be that it will lead to something good with some probability—not a very high probability. But the problem with it is that in order to use that, you need to generalize far outside the training distribution. Here we actually need to look at what the goal is.<br><br> The problem is that it is technically possible to create super intelligent AI, which will be dangerous. To solve this problem, it&#39;s not enough to create some kind of AI which would not be dangerous, because otherwise they could just write an algorithm that doesn&#39;t do anything. That&#39;s not dangerous, mission accomplished. We need to be able to create AIs that are sufficiently powerful to serve as defense systems against those potentially dangerous AIs. So those have to be systems that have superhuman capabilities at building sophisticated models of the world and building complex long term plans based on that. And that&#39;s something that is far outside the training distribution of a Large Language Model or anything that&#39;s based on human imitation. It is extremely unclear whether we can actually rely on the algorithms we have to generalize that far out of the training distribution without completely losing all of their alignment properties.<br><br> <strong>Will Petillo:</strong> To summarize, LLMs are fundamentally imitative, which doesn&#39;t seem particularly dangerous in itself, but it also limits what they can do. And so we can&#39;t really expect that development is just going to stop here. Eventually there might be something like Reinforcement Learning added onto it—maybe not necessarily that algorithm, but something that can be as creative as Alpha Zero is in Go and finding a really creative move that no one&#39;s seen before. So we need to be prepared for things that are much more powerful because those are going to be useful and the economics that led to building LLMs are going to lead to building bigger things. Is that what you&#39;re getting at?<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that sounds pretty much on the spot. Either it will be Reinforcement Learning or...well, I don&#39;t want to speculate too much on what is needed in order to make AI more powerful because it&#39;s not good information to have out there.</p><p> <strong>Will Petillo:</strong> Fair enough. Moving on to the things that you actually work on, one idea I&#39;ve seen adjacent to this is the term &quot;Agent Foundations&quot;. And also the &quot;Learning Theoretic Agenda&quot;.那些东西是什么？<br><br> <strong>Vanessa Kosoy:</strong> Agent Foundations is this abstract idea that says we need to create a foundational mathematical theory which explains what agents are. What does it mean, mathematically, for an algorithm to be an agent? What types of agents are possible? What capabilities can they have or not have?等等。 The Learning Theoretic Agenda is more specific than that, in the sense that it&#39;s like a very specific program that is trying to achieve this goal. Specifically, by tools that build on statistical and computational learning theory, algorithmic information theory, control theory, this kind of thing. This is the program I created to answer this challenge of coming up with those agent foundations.<br><br> <strong>Will Petillo:</strong> OK, so Agent Foundations is like the question of &quot;how do minds work?&quot;, that encompasses AI, and the Learning Theoretic Agendas is like, &quot;how do we design algorithms that push this in a good direction?&quot;是对的吗？<br><br> <strong>Vanessa Kosoy:</strong> I wouldn&#39;t put it that way. I would just say that Agent Foundations is just trying to understand how minds work and people have been trying to do it in various ways. MIRI have historically had all sorts of proof-theoretic models that try to approach this, and then there&#39;s Garrabrant&#39;s Logical Induction, and there are various ideas under this very broad umbrella, whereas the Learning Theoretic Agenda is a very specific approach.<br><br> It&#39;s this approach that starts with AIXI and classical reinforcement learning theory as the starting points and then looks what are the missing ingredients from that in order to have a foundational theory of agents and start building towards those missing ingredients with ideas such as Infra-Bayesianism and Infra-Bayesian Physicalism and Metacognitive Agents, and so on.<br><br> <strong>Will Petillo:</strong> The kind of agents and minds that you&#39;re talking about here, is this tied to the frontier Large Language Models, or is it more broad to AI or any kind of thinking entity?<br><br> <strong>Vanessa Kosoy:</strong> When I say agent, I mean something very broad. Much broader than existing AIs or even just AIs. Certainly including humans, potential aliens, or whatever. So, for me, an agent is a system that has particular goals and is learning sophisticated models of the world in which it is embedded and uses those models to build long term plans in order to achieve its goals. So this is the informal description of what I mean by an &quot;agent&quot;. The whole goal of this program is to go from this to a completely formal mathematical definition and study all the implications of this definition.</p><p> <strong>Will Petillo:</strong> So not even beyond LLMs, it&#39;s even broader than Machine Learning. What&#39;s the reason for that approach? Given how dominant Machine Learning is, why not focus on the things that seem to be the most widely used?</p><p> <strong>Vanessa Kosoy:</strong> First of all, let&#39;s keep some order in the terminology. I would distinguish between AI, Machine Learning, and Deep Learning. AI is this thing that people started thinking about since the 1950s, about how to build thinking systems, without really having good understanding of what does it even mean, but just some kind of intuitive notion that there is such a thing as thinking and we should be able to replicate it in a machine.<br><br> Machine learning is a more specific approach to this that emerged...well, I didn&#39;t want to point a finger exactly when, but probably something like the eighties. Machine Learning is specifically this idea that the central element of thinking is learning and learning means that you&#39;re interacting with some unknown environment and you need to create a model of this environment. So you need to take the data that you see and use it to create a model. And that&#39;s analogous to how scientists do experiments, gather data, and then build theories based on this data.</p><p> This general idea is called Machine Learning—or, more accurately would be to call it just learning. The &quot;machine&quot; part comes from trying to come up with ways to actually implement this inside a machine. This is a field that has a lot of mathematical theory. The mathematical theory behind machine learning is what&#39;s known as statistical and computational learning theory, and that&#39;s actually the foundation of the Learning Theoretic Agenda. That&#39;s why it&#39;s called &quot;learning theoretic&quot;.</p><p> There has been a hypothesis that this kind of notion of learning captures most of the important bits of what we mean by thinking. And I think that this hypothesis has been extremely well-supported by recent developments in technology. This is something that I completely endorse, and it is basically the basis of my whole research program. So there&#39;s no contradiction here, because learning is still a very general thing. Humans also do learning. Aliens would also have to do learning.</p><p> Deep Learning is a more specific set of algorithms for how to actually accomplish learning efficiently in a machine, and that&#39;s what started the deep learning revolution around 2010, although the algorithms existed in some form for decades before that. But it took a while to get the details right and also to have the right hardware to run them. Deep Learning&#39;s unfortunate feature is that we don&#39;t understand it mathematically. A lot of people are trying to understand it, but we don&#39;t have a good theory of why it actually works. That&#39;s why it&#39;s not the focus of my research program, because I&#39;m trying to come up with some mathematical understanding. I definitely have a hope that eventually people will crack this kind of mystery of how Deep Learning works, and then it will be possible to integrate it into the theory I&#39;m building.<br><br> But even if we had this theory, then it still seems really important to think in the broadest possible generality, because, well, first of all, we don&#39;t know that the algorithms that exist today will be the algorithms that bring about AGI. And also because the broadest generality is just the correct level of abstraction to think about the problem, to get at those concepts of what does it even mean for a system to be &quot;aligned&quot;. There&#39;s some philosophical problems that need to be solved here, and they are specific to some very particular algorithms. Also, there is the fact that I actually want this theory to include humans because I might want to use this theory to formalize things like value learning. How do I design an AI system that learns values from humans?<br><br> <strong>Will Petillo:</strong> Seeing the Wikipedia-level, or just browsing the internet description, of Machine Learning and Deep Learning, it&#39;s very easy to use them interchangeably. I think I&#39;ve seen the description as Deep Learning is just the idea that you add multiple layers of neurons. And so because there&#39;s multiple layers, it&#39;s &quot;deep&quot;<br><br> <strong>Vanessa Kosoy:</strong> Let me try to clarify the distinction. Machine Learning talks about taking data and building models from it. The type of model you&#39;re building can be very different. Before Deep Learning, we had algorithms such as Support Vector Machines, Polynomial Regression is also a very simple type of Machine Learning—fitting a model to data. Various methods used in statistics can be regarded as a sort of Machine Learning. There is some space of models or hypotheses and you&#39;re trying to use the data in the optimal way to infer what the correct hypothesis is, or to have some probability distribution of your hypothesis if you&#39;re doing a Bayesian approach.<br><br> But different types of hypothesis classes lead to very different results in terms of the power of the algorithms, but also in terms of what we know to say about how to learn those hypothesis classes. And what do we know to prove mathematically under which conditions we can actually learn them? For example, for Support Vector Machines, the mathematical theory is basically solved. There&#39;s kernel methods that build on top of that, and that also has very solid mathematical theory. Deep learning is a particular type of learning algorithm which uses those artificial neural network architectures.</p><p> It&#39;s not just multiple layers, there&#39;s a lot of details that matter. For example, the fact that the activation functions are ReLU, that turns out to be pretty important for what kind of regularization method you use in training. For example, dropouts are basically what started the Deep Learning revolution. If you&#39;re working with sequences, then we have transformers, which is a very specific network architecture. So there is actually a lot of very specific details that people came up with over the years, mostly with the process of trial and error, to just see what works. We don&#39;t have a good theory for why those specific things work well. We don&#39;t even understand the space of models those things are actually learning, because you can prove theoretically that if you take a neural network and you just let it learn another neural network, then in some situations it will be infeasible.<br><br> But for real world problems, neural networks succeed to learn a lot of the time. This is ostensibly because the real world has some particular properties that make it learnable, or there&#39;s some particular underlying hypothesis class that the neural networks are learning, and which captures a lot of real world phenomena, but we don&#39;t even have a mathematical description of what this underlying hypothesis class is. We have some results for some very simple cases, like two layer or three layer neural networks, or some other simplifying assumptions, but we&#39;re not close yet to having the full answer.<br><br> <strong>Will Petillo:</strong> Deep Learning assumes certain things about how the world is, in terms of what it can pick up, and it happens to work fairly well, but it&#39;s not really clear what it&#39;s assuming.</p><p> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s exactly right. So we have different no-go theorems, which say that for arbitrary data, even if the data is fully realizable, and even if the data is such that the neural network can perfectly express an exactly correct model, the problem is infeasible. In general, gradient descent will not converge to the right model, and also no other algorithm will converge because the problem will just be intractable. There are some properties that the world has, and since this Deep Learning is successful in such a big variety of very different cases, it feels like those properties should have some simple mathematical description.<br><br> It&#39;s not like some properties that are extremely specific to texts or to audio or to images. It&#39;s some properties that are extremely general and hold across a wide range of different modalities and problems. Those are properties that I can speculate, for example, as having to do with compositionality, how the real world is often well described as being made of parts, and how things can be decoupled according to different spatial scales, or different temporal scales on which the dynamics is happening. But we don&#39;t have a theory that actually explains it.<br><br> <strong>Will Petillo:</strong> You mentioned ReLU as one of the examples of a thing that just works. As I understand it, ReLU is basically like taking the output, changing it in a way that could be illustrated as a graph where it&#39;s flat on one side and a diagonal line past zero. Whereas before, models typically used Sigmoid as the activation function, which was more like a smoothly curved line that prevents numbers from getting too big. For some reason, ReLU works better. The sense I&#39;m getting from your explanation is that this change impacts what kinds of things the neural network is able to understand in a direction that&#39;s more matching with reality. But all these changes are developed with a &quot;throw stuff at the wall and see what sticks&quot; kind of way, simply measuring the results without really understanding <i>why</i> ReLU is better than sigmoid.<br><br> <strong>Vanessa Kosoy:</strong> That&#39;s more or less right. We just have to be careful about what we mean when we say what the neural network can &quot;understand&quot;. It&#39;s a pretty complicated concept because it&#39;s not just what the neural network can express with some set of weights, it&#39;s what the neural network can actually learn through a process of gradient descent. It has to do not just with the space of functions that the neural network can describe, but with the entire loss landscape that is created in this space of weights when we look at a particular data set.<br><br> <strong>Will Petillo:</strong> When you&#39;re describing a gradient descent and loss landscape the analogy I hear thrown around a lot is a ball rolling down a hill—there&#39;s a constant gravity force, and you want the ball to get down to sea level. But often it doesn&#39;t because it finds some local minima, like a hole or something, where any direction it can go is up, so it&#39;s not going to roll anymore. So you have to shape the landscape such that down always gets the ball to sea level.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s a pretty good explanation. Gradient descent is something that we have good mathematical theory for how it converges to the global minimum for convex functions, but the loss for neural networks is non-convex...but it still happens to be such that it works. People have gained some insights about why it works, but we still don&#39;t have the full answer.<br><br> <strong>Will Petillo:</strong> OK, so if the landscape is really bumpy, then you don&#39;t expect the ball to get to sea level, so the fact that it somehow does anyways demands an explanation that we don&#39;t really have. I can see how that kind of framing raises a lot of questions regarding unpredictability.<br><br> Moving on, you mentioned AIXI at one point.那是什么？<br><br> <strong>Vanessa Kosoy:</strong> AIXI is this idea by Marcus Hutter, which is supposed to be a mathematical model of the perfect agent. The way it works is: there is a prior, which is the Solomonoff prior. For those who don&#39;t know what that is, it&#39;s basically some way to mathematically formalize the concept of Occam&#39;s Razor. And Occam&#39;s Razor is this idea that simple hypotheses should be considered a priori more likely than more complicated hypotheses. And this is really at the basis of all rational reasoning. Hutter took the Solomonoff prior, which is a very clever way to mathematically formalize this notion of Occam&#39;s razor, and say, well, let&#39;s consider an agent that&#39;s living in a universe sample from the Solomonoff prior. And this agent has some particular reward function that it&#39;s maximizing. And let&#39;s assume it&#39;s just acting in a Bayes-optimal way. So it&#39;s just following the policy that will lead it to maximize its expected utility according to this prior. And let&#39;s call this AIXI. Which is a really cool idea...only it has a bunch of problems with it, starting with the &quot;minor&quot; problem that it&#39;s uncomputable. There&#39;s not an algorithm that exists to implement it even in theory.<br><br> <strong>Will Petillo:</strong> I think I heard it explained once as imagining the entire universe described as a bunch of bits—ones and zeros. At the start, all of them could either be a one or a zero, then you get a little bit of data and now you&#39;ve locked in a few of those numbers and have cut the space of all things that could be in half. As you keep learning, you get more and more certain.<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s actually a little more nuanced than that. Just the fact that you have a lot of possibilities doesn&#39;t mean that it&#39;s uncomputable. Maybe the exact thing is uncomputable, but you could still imagine that there is some clever algorithm that approximates this Bayesian inference process. For example, if you look at classical reinforcement learning theory, then there are things like algorithms for learning in arbitrary Markov decision processes with n states. In a Markov decision process with n states, there are still an exponentially large space of possible ways it could be, and we still have actually efficient algorithms that converge to the right thing out of this exponentially large thing by exploiting some properties of the problem.<br><br> The thing with AXI is that its prior is such that even individual hypotheses in the prior are already arbitrarily computationally expensive, because in its prior it considers every possible program, so every possible program that you can write on a universal Turing machine is a possible hypothesis for how the world works. And some of those programs are extremely expensive computationally. Some of those programs don&#39;t even halt, they just enter infinite loops. And you can&#39;t even know which, because this is the halting problem, right? This is why AIXI is a non-starter for doing something computable, not to mention computationally tractable.<br><br> <strong>Will Petillo:</strong> The minor problem of uncomputability aside, a &quot;perfect algorithm&quot;...what does that mean? Would AIXI be safe if it were somehow, magically computed?</p><p> <strong>Vanessa Kosoy:</strong> No, it doesn&#39;t make it safe in any way. It&#39;s &quot;perfect&quot; in the sense of it&#39;s the most powerful algorithm you could imagine. Again, under some assumptions. I mean, there are other problems with this, such as that it assumes that the outside world is simpler than the agent itself. There are multiple problems with this, but if you can put all those problems aside then you could argue that this is the best possible agent. And in this sense, it&#39;s perfect. It&#39;s very, very, very not safe. In order for it to be safe, we would need to somehow plug the right utility function into it. And that would still be a very non-trivial problem.</p><p> <strong>Will Petillo:</strong> What kind of algorithms do you look for, assuming you&#39;re trying to find things that are computable?</p><p> <strong>Vanessa Kosoy:</strong> Computability is just one of the issues. I&#39;m imagining that there will be something that I call the frugal universal prior, which is some kind of prior that we can mathematically define, which will simultaneously be rich enough to capture a very big variety of phenomena. And on the other hand, we&#39;ll have some clever algorithm that can actually allow efficient learning for this prior using, for example, some compositionality properties of the hypothesis on this prior or something of that sort.</p><p> But even knowing this prior, there&#39;s a lot of other conceptual problems that you also need to deal with. Like what I call the problem of privilege, where the formalization of Occam&#39;s Razor and AXI privileges the observer, and you need to understand how to deal with that. And there&#39;s the problem of realizability where you cannot actually have a hypothesis which gives you a precise description of the universe, but only some kind of approximate or partial description, and you need to understand how to deal with that. Then there&#39;s also the fact that you want your utility function to be not just a function of your observations, but also some parameters that you cannot directly observe. You also want to be able to prove some Frequentist guarantees for this algorithm. To know how much data this algorithm actually needs to know to learn particular facts and have a good theory of that. There&#39;s a whole range of different questions that come up when studying AIXI like models.<br><br> <strong>Will Petillo:</strong> Studying AIXI like models, that is what you&#39;re working on?</p><p> <strong>Vanessa Kosoy:</strong> You could, yeah, if you wanted to put it in one sentence, I guess.<br><br> <strong>Will Petillo:</strong> What are some interesting problems that you&#39;re interested in solving? I&#39;ve seen Newcomb&#39;s problem floating around and stuff adjacent to this.<br><br> <strong>Vanessa Kosoy:</strong> Newcomb&#39;s problem is something Eliezer Yudkowsky wrote about a lot as an example of something that&#39;s very confusing for classical accounts of rationality. You have two boxes that you need to choose from. One box has a thousand dollars. The other box has either nothing or a million dollars. You can either choose the first box or you could take the money that&#39;s in both boxes. Normally, taking the money that&#39;s in both boxes is always strictly superior to just taking the one box.<br><br> Except that in this spot experiment there is some entity called Omega who can predict what you do, and so it only puts the $1,000,000 in the other box if it knows that you will only take that box and won&#39;t try to take the thousand dollar box as well. So only if you&#39;re the type of agent that would predictably (for Omega) take only one box, only in this case you will get out of this room with $1,000,000. Whereas in the other case you will only have $1,000. So arguably it&#39;s better to take one box instead of two boxes, as opposed to what many classical accounts of rationality would say. This is one example of an interesting thought experiment.</p><p> For me, this thought experience is a special case of the problem of non-realizability, where you need to deal with environments that are so complex that you&#39;re not able to come up with a full description of the environment that you can actually simulate 。 Because in this example, the environment contains this agent Omega, which simulates you, and this means that you cannot simulate it, because otherwise it would create this kind of circular paradox. I&#39;ve actually also shown that my theory of dealing with non-realizability, which I call Infra-Bayesianism, actually leads to optimal behavior in these kinds of Newcomb-like problem scenarios.<br><br> <strong>Will Petillo:</strong> And the reason for studying Newcomb-like problems is not because we expect to be faced with Omega offering us boxes at some point, but because it&#39;s just an illustrative way of thinking about how to deal with things when you can&#39;t know这是怎么回事。 And also because it might be easy to say, &quot;yeah, well, I&#39;ll just take one box because I&#39;ll get more that way,&quot; but when you really dive into what&#39;s a coherent, non hand wavy reason as to why, then there&#39;s some interesting insights that can potentially come up from that. Have there been any discoveries that you found through exploring these kinds of things?</p><p> <strong>Vanessa Kosoy:</strong> I would say that Infra-Bayesianism itself is an interesting discovery, that some theoretical account of agents that can reason about complicated worlds that are much too complicated for the agent to simulate. Now I describe the motivation by stating the problem of non-realizability, but the way I actually arrived at thinking about this is through thinking about so-called logical uncertainty. The reason people started thinking about it was because of so-called Updateless Decision Theory, which came from thinking about Newcomb time paradoxes. So it all comes from that line of reasoning even though, after the fact, you can motivate it by some much more general abstract thinking.<br><br> <strong>Will Petillo:</strong> What&#39;s the connection between these decision theory type questions and making a safer AI?<br><br> <strong>Vanessa Kosoy:</strong> The idea is creating a general mathematical theory of agents. The way it&#39;s going to help us with making AI safer, there are several reasons, the most obvious is that in having this theory, we hopefully will be able to come up with rigorous models of what it means for a system to be an aligned agent 。 Having this rigorous definition, we&#39;ll be able to come up with some algorithms for which we can prove those algorithms are actually safe agents. Or at least we could have some conjecture that says that this given model of such and such conjectures, we think that those algorithms are safe agents. Like in cryptography, you have some conjectures which have very strong evidential support.</p><p> We could have at least some semi formal arguments because now when people are debating whether a particular design is safe or not safe, it all boils down to those hand waving philosophical arguments, which don&#39;t have any really solid ground. Whereas this gives us tools for much more precise, crisp thinking about these kinds of questions. It hypothetically also gives us much more power to leverage empirical research because maybe we will be able to take the empirical research we have, plug it into the mathematical theory, and get some answers about how we expect those results to actually extrapolate to various regimes where we haven&#39;t done so.<br><br> <strong>Will Petillo:</strong> Would this line of research that you&#39;re working on eventually be usable in evaluating things like Large Language Models or Deep Learning based systems, to be able to say with greater certainty the extent to which they&#39;re safe or unsafe?<br><br> <strong>Vanessa Kosoy:</strong> I think there are multiple paths to impact. So there is a path to impact where we will eventually come up with a theory of Deep Learning. Or, if not a fully proven theory, then at least some strong conjectures about how Deep Learning works that can interface with the theory of agents I&#39;m building. And then we could use this composite theory to prove things or at least to have strong arguments about properties of systems built in Deep Learning.</p><p> There could be a different path to impact where we use this theory to come up with completely new types of algorithms for building AI, which are not Deep Learning, but for which we have a good theoretical understanding.</p><p> There&#39;s also some third possibility that we won&#39;t have a good theory, but we could at least reason by analogy, similarly to how many Deep Learning algorithms are designed by analogy to some algorithms for which we have mathematical theory. Deep Q learning, for example, is analogous to simple Q learning, for which we have mathematical theory. So we could imagine a world in which we have some kind of idealist toy model algorithms for which we have some rigorous arguments why they are aligned and then we have some more heuristic algorithms, which we cannot directly prove things about, but which are arguably analogous to those toy models.<br><br> <strong>Will Petillo:</strong> So I heard three paths to impact. One is potentially building a different form of AI that&#39;s verifiable from the ground up and does the same things as Deep Learning based AI, but in a more rigorous sort of way. A second is evaluating, or at least better understanding, Deep Learning or whatever is state of the art. And then a third, in between the two, is having a simpler form of AI that analogizes to the state of the art things, so that you can use the former to understand the latter.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that sounds about right.<br><br> <strong>Will Petillo:</strong> I&#39;d like to focus a bit on using foundational research to understand other things like Deep Learning, getting at this theory-based approach. Bringing in a totally opposite counterpoint, one could argue: no, you should just look at the things that are being used and collect data about it and then build your theory by finding patterns in the data. When the theories are shown to be wrong—as a result of more data—then update your theories then. Why work on theory in advance?</p><p> <strong>Vanessa Kosoy:</strong> The biggest reason is that you cannot reliably extrapolate from empirical research without having an underlying theory. Because you might, for example, take some measurements and find some trend...but then there is some phase transition later on that you don&#39;t see in the trend, but which happens and behavior changes to a completely different regime. And because you don&#39;t have a theoretical explanation, you won&#39;t notice—or people will just switch to using completely different algorithms, which behave in completely different ways.</p><p> You might have those empirical models of existing AIs, but those empirical models are very myopic. They&#39;re always looking one step ahead. And then you don&#39;t see the cliff that&#39;s three steps ahead of you. Updating those theoretical, empirical models on new things that happen—it might just not be quick enough. Eventually you fall off the cliff and then it&#39;s too late to say, &quot;oh, actually, that trend line was wrong!&quot;</p><p> Luckily we are in a domain where we have tools to do research even without empirical data. Of course, we should use the empirical data that we have, but we&#39;re not bottlenecked on empirical data because the thing we&#39;re studying is algorithms, and algorithms are mathematical objects, so they can be studied mathematically. This is very different from studying some phenomenon of physics, where if you don&#39;t have the data, then there&#39;s no way to generate the data without having it. Here, this really should all boil down to math. More precisely, it should boil down to math plus whatever properties the real world phenomena have that we want to assume in our mathematical theories.<br><br> And here, yeah, here it is something that we need empirical input for. But on the other hand, we already have a really good understanding of physics. So given the knowledge of physics and other scientific domains that we have, it&#39;s very plausible that we have enough information to answer all the questions purely through mathematical inquiry, even if we had no empirical data at all. Which is not to say we shouldn&#39;t also use empirical data, to supercharge this research, but we&#39;re not limited to that.</p><p> <strong>Will Petillo:</strong> So it&#39;s not a choice between theory versus experiment, we should be using both. You&#39;re focused on the theory side, and arguably there&#39;s not enough work on that because theory is where the bottleneck is, not on getting more data.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, I think we should definitely be doing both. Ideally, there needs to be synergy where experiments produce new phenomena for theorists to explain and theory inspires the experiments. The theorists should be telling the experimentalists which questions and what kind of experiments are the most interesting and we should have this kind of synergy. But I think that in the current landscape—definitely in AI alignment—the theory side is currently left behind. That&#39;s where I think we should put the marginal efforts in.<br><br> <strong>Will Petillo:</strong> Do you see that synergy existing now? Like, is OpenAI asking MIRI for feedback on their experiments, or is there any kind of connection, or are people just siloed off from each other?<br><br> <strong>Vanessa Kosoy:</strong> I think it doesn&#39;t exist now, almost at all. OK, no, to be fair, it exists in some areas and much less in other areas. For example, there&#39;s people working on Singular Learning Theory. I think that they are much more interfaced with experimental work, which is good. The kind of research that MIRI is doing is and the kind of research I&#39;m doing is much less interfaced with experimental work. I have some plans for creating an experimental group working in a close loop with me on those questions as part of my big, long term plans, but I still haven&#39;t gotten around to doing that yet.<br><br> <strong>Will Petillo:</strong> If you could change anyone&#39;s mind, or set political and business agendas, what would you like to see happen to have more of an interface?<br><br> <strong>Vanessa Kosoy:</strong> First of all, we just need more theorists. To have an interface, we need something to interface with, so we just need more theorists. I think that this is, in practice, where the bottleneck is now. Once this progress in theory gets sufficiently paced there will be a bunch of questions. I mean, there are already questions that I would like to see experiments on, but the more this thing picks up, the more such questions we will have. I think now the main bottleneck is just in having more people working on this theory.<br><br> <strong>Will Petillo:</strong> What would change from an external perspective if there was a lot more theory work? I imagine a skeptic could argue: &quot;OpenAI and these other companies are making these really awesome breakthroughs in a largely experimental kind of way, and it&#39;s working great! If it ain&#39;t broke, don&#39;t fix it!&quot; What&#39;s broken, in your view?<br><br> <strong>Vanessa Kosoy:</strong> I think that the current path is leading us to a disaster. I think that companies like OpenAI and other leading labs are wildly overconfident about their ability to solve problems as they come along. I think that they haven&#39;t come up with any convincing solutions to the hard parts of the problem, and they don&#39;t even have the tools to do this because of a lack of theoretical understanding. We don&#39;t even have models that are precise enough to have a solution in which we could really be confident. We need to be very precise about the arguments which convince us that the solution is good. And we don&#39;t even have the tools to reach this type of precision.<br><br> What the companies are doing is basically just developing things in trial and error. If we see any problems, then we&#39;ll just tweak the thing until the problem goes away. That&#39;s a bandaid method, which is to say it works until it doesn&#39;t work. It fixes problems on a superficial level, but eventually there will come a point where either the problem will not be caught in time and the results will be catastrophic, or the problems will be caught in time, but then nobody will have any idea what to do in order to fix it. And eventually someone is going to do the catastrophic thing anyway.<br><br> The only thing which makes me less pessimistic than other people in MIRI is that I think we still have more time. I don&#39;t think they&#39;re quite as close to AGI, and I think that a lot of things can change during this time. Which is again, not to say they will change—we might burn all this time and still end up with a catastrophe.<br><br> <strong>Will Petillo:</strong> What&#39;s an example of an existing problem that only has superficial solutions?<br><br> <strong>Vanessa Kosoy:</strong> I mean, the real problem we&#39;re concerned about is not really an existing problem, right? The main thing we&#39;re concerned about is that future AI systems—which will be much more powerful than existing AI systems—will bring about extinction of the human race or a catastrophe on a similar level.<br><br> That&#39;s not an existing problem for the simple reason that the AI systems we have today are not capable of learning a model of the world that&#39;s so sophisticated that it enables you to do these types of actions. But even now, the companies struggle with all the things that happen with Large Language Models, such as the infamous jailbreaks where they&#39;re trying to make them well-behaved in various ways. Not telling the users offensive, dangerous information, for example, and the users easily find jailbreaks to work around that, or just tell false answers.<br><br> But again, for me, that&#39;s not the real problem, it&#39;s just an analogy. I mean, they&#39;re kind of struggling with these very simple, much easier problems now, which is not to say they won&#39;t solve them. Trial and error will get you there eventually. The reason trial and error is not the solution for existential risk is because once everyone is dead, the trial is over. There&#39;s no more trial. So the problems we have now, they&#39;re still struggling with them because they don&#39;t have principle tools to solve them, but eventually they will trial-and-error their way through and will patch them somehow, or at least solve them well enough for it to be economical. But once you reach the point where failures are global catastrophes, trial and error is no longer an acceptable method of fixing the problem.<br><br> <strong>Will Petillo:</strong> Obviously we don&#39;t get to see lots of test data of the world ending. But I would imagine there&#39;d be some precursor issues that are smaller, but hint at what&#39;s to come. Do you see the challenges with hallucination or not being able to control what the AI says as those kinds of precursors? Or are they totally irrelevant and there just won&#39;t be any precursor issues?<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s a difficult question, because there are very important bits that are still missing from existing AI systems to produce existential risks. We can point at examples where systems are kind of mis-generalizing, there&#39;s a lot of famous examples: some program that &quot;wins&quot; Tetris by pausing the game forever, or that wins some boat race game by racing the boats in infinite circles, doing various weird unintended behaviors, because the metric that the algorithm is maximizing is not actually what the users intended. You could call those precursors, but I feel like it&#39;s not exactly capturing the magnitude of the problem because those are still toy settings. There are no open-world systems that are acting in the open, physical world. The goals that they&#39;re trying to solve are much simpler than human values; there&#39;s not really operating domains where there are really complex ethical considerations.<br><br> Maybe Large Language Models are starting to approach this because they enter domains where there are some, at least morally, not completely trivial issues that come up. On the other hand, Large Language Models are not really doing things that are strongly superhuman. Well, they may be superhuman in the sense that they have a very large breadth of knowledge compared to humans, but not in other senses.所以这很难。 There are things that are sort of analogous, but it&#39;s not strongly analogous.<br><br> But then again, our motivation to be concerned about this risk doesn&#39;t come from looking at LLMs. Eliezer Yudkowsky started talking about those things before Deep Learning was a thing at all. That&#39;s not where the motivation comes from.<br><br> <strong>Will Petillo:</strong> I guess the reason I was asking about it is that in the places where this stuff gets debated and polarized, one of the common objections is: &quot;There&#39;s no evidence behind this! This is all just storytelling!&quot; <i>Is</i> there evidence of the danger or does it really just come from looking at the math?<br><br> <strong>Vanessa Kosoy:</strong> The problem is, what do you call evidence? That&#39;s a very complicated question. The things that would be obvious evidence would be things like: AI&#39;s completely going out of control, breaking out of the box, hacking the computer, copying themselves to other computers, outright manipulating human operators, and so on. But this kind of thing is a sort of canary that you only expect to see very, very, very close to the point where it&#39;s already too late. It&#39;s not possible to say that we will only rely on this type of evidence to resolve the debate.</p><p> For other types of evidence, some people say that evolution is sort of evidence how a Machine Learning algorithm can produce something which is completely unaligned with the original algorithm. Other people show you Reinforcement Learning algorithms doing not what the designer intended. But for every argument like that, you could have a counter argument which says, &quot;yeah, but this example is not really similar. We cannot really project from there to existential risk because there are some disanalogies.&quot;<br><br> And yeah, there will always be some disanalogies because until you have AIs in the real world that are very close to being an existential risk, you won&#39;t have anything that&#39;s precisely analogous to something presenting an existential risk. So we have no choice but to reason from first principles or from math or by some more complicated, more multi-dimensional analysis. We just have no choice. The universe doesn&#39;t owe it to us to have a very easy, empirical way of testing whether those concerns are real or not. One of the things I&#39;m hoping for is that the theory will bring about stronger arguments for AI being dangerous—or the theory will tell us no, everything is fine, and we can all relax. The lack of theory is part of the reason why we don&#39;t have foolproof, completely solid arguments in one direction or the other direction.<br><br> <strong>Will Petillo:</strong> The challenge with finding evidence is that anything you can point to that exists now could be interpreted in multiple ways. Having solid theory would lend some credence to one interpretation over another.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, absolutely. If you have a theory that says that a particular type of misgeneralization is universal across most possible machine learning systems, and we also see this type of misgeneralization happening in real Machine Learning systems, then it would be much harder to dismiss it and say, &quot;oh yeah, here we have this problem, but we&#39;ll do this and that, and that will solve it easily.&quot;<br><br> <strong>Will Petillo:</strong> There&#39;s one thing that&#39;s still bugging me about the issue of evidence not being available now. The analogy my mind immediately goes to is climate change. You could say that &quot;Oh, the idea of large swaths of the world being uninhabitable is just this elaborate story because all that has never happened before!&quot; But then you can look at a bunch of things that exist already: small scale disasters, the graphs of CO2 versus temperature, and so on, point to those and say, &quot;Hey, the really bad stuff hasn&#39;t happened yet, but there is a lot of evidence that it will!&quot; What makes AI different?<br><br> <strong>Vanessa Kosoy:</strong> I think that climate change is a great analogy. The big difference is that in climate change, we have a really good theory. Like in climate change, we have physics, right? And we have planetary science, which is on a very, very solid foundation. And we have computer simulations. It&#39;s still not trivial, there are some chaotic phenomena which are hard to simulate or predict, so not everything is completely trivial, but still we have some very, very strong theoretical foundation for understanding how those things work and what are the mechanisms. And this theory is telling us that there&#39;s still big uncertainty intervals around how exactly many degrees of warming we&#39;re going to get with such and such amount of CO2, but we still have a fairly solid prediction there.<br><br> Whereas with AI, we don&#39;t have this. The analogous situation, if you want to imagine climate change, AI style, then it would be something like not having a theory which explains why CO2 leads to warming. Having some empirical correlation between temperature and CO2, and then people could argue ad infinitum. Correlation is not causation, maybe the warming is caused by something completely different, maybe if we do some unrelated thing it will stop the warming, which is not actually true. We would be in the dark. With AI, we&#39;re currently in the dark.<br><br> <strong>Will Petillo:</strong> What is happening currently with your work at MIRI?</p><p> <strong>Vanessa Kosoy:</strong> Currently there are multiple problems I&#39;m looking at. Hopefully I will publish very soon a paper on imprecise linear bandits, which is related to Infra-Bayesianism that I mentioned before, which is a theory of agents that reason about complicated worlds. That&#39;s analyzing this theory in some very simple, special case in which I succeeded to get some precise bounds for how much data an algorithm would need to learn particular things. After that, I&#39;m starting to look into the theory of learning state representations in Reinforcement Learning, which is currently another big piece missing from the theory, which is about how your algorithms should learn about which features of the world are actually important to focus在。<br><br> In parallel, I have a collaborator, Gergely Szucs, who is working on using my theory of Infra Bayesian Physicalism to create a new interpretation of quantum mechanics. He has some really interesting results there. It&#39;s kind of a test case which demonstrates how this framework of thinking about agents allows you to solve all sorts of philosophical confusions. In this case, it&#39;s confusions that have to do with the interpretation of quantum mechanics. Scott Garrabrant has a project about a new type of imprecise probabilities, some new way of representing beliefs that have some nice compositionality properties. Kaspar Osterheld from Carnegie Mellon and Abram Demski had a paper recently about some new type of frequentist guarantees for algorithms that are making decisions based on something that&#39;s similar to a prediction market. So yeah, a lot of interesting things are happening.<br><br> <strong>Will Petillo:</strong> Are there any other questions that I did not ask that would be helpful for someone seeing this to get a sense of what you&#39;re about here?<br><br> <strong>Vanessa Kosoy:</strong> Not a question exactly, but I also have a more concrete approach for how to actually solve alignment, how to actually design an aligned agent, which I call Physicalist Superimitation. It&#39;s a variation on the theme of value learning, but it draws from the framework of Infra Bayesian Physicalism, which comes from the Learning Theoretic Agenda and from some ideas in algorithmic information theory to come up with a semi-formal approach to how you could have an AI that learns human values in a robust way.<br><br> It deals with a lot of problems that other approaches to value learning have, like: how do you determine where the boundaries of an agent are?什么是人？ How do you locate this human in space? How do you take into account things which are not just behavior, but also internal thought processes of the human in inferring the human&#39;s values? How do you prevent perverse incentives such as the AI somehow changing or manipulating humans to change their values? How do you avoid the inner alignment problem? It answers a range of concerns that other approaches have.<br><br> <strong>Will Petillo:</strong> This sounds reminiscent of Inverse Reinforcement Learning?<br><br> <strong>Vanessa Kosoy:</strong> Inverse Reinforcement Learning is the idea that we should look at behaviors of humans, infer what those humans are trying to do, and then we can do this thing. &quot;We&quot; as an AI. So I actually have presentations in which I explain Physical Superimitation as Inverse Reinforcement Learning on steroids. It&#39;s taking this basic idea, but implementing it in ways that solve a lot of the deep problems that more simplistic approaches have. One problem that simplistic approaches have is that they model humans as perfect agents that follow the perfect policy, given perfect knowledge of the environment, which is wildly unrealistic.<br><br> Instead, I model humans as learning agents. They learn things as they go along. And they also might even do that imperfectly. Another thing is the issue of boundaries. What is a human exactly? Where do you put the boundaries around a human? Is there just some particular input and output, which the human uses and you consider everything that goes through this port to be the human? But then how do you deal with various discrepancies between what goes into this port and what the human actually intended to do, or various possibilities like the AI hijacking this channel?<br><br> In my approach, the way a human is formalized is that a human is a particular computation that the universe is running. This is something I can actually formalize using Infra Bayesian Physicalism. It has particular properties, which make it agentic, so the agent detects which computations the universe is running, among them detects which computations are agents, and amongst those agents, it selects which agent is its user by looking into causal relationships, and this way it homes onto the boundary of the agent. The first thing is because we&#39;re talking about the computation that this human is running, which is human reasoning and regarded as a computation. We&#39;re also automatically looking at internals as internal thought processes and not just things that are expressed as external behaviors. So we have potentially much more information there.<br><br> <strong>Will Petillo:</strong> What would be the best way for someone to get involved? And what would they want to learn in advance?<br><br> <strong>Vanessa Kosoy:</strong> One thing they could immediately start doing is reading up on stuff people did in Agent Foundations and in the Learning-Theoretic Agenda until now. I have this recent post, <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><u>Learning Theoretic Agenda: Status 2023</u></a> , which summarizes a lot of the things. I also have a <a href="https://www.alignmentforum.org/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list"><u>reading list post</u></a> where I recommend some background reading for people who want to get into the field. More concretely in terms of career steps, it&#39;s already too late to apply, but I&#39;m running a track in <a href="https://www.matsprogram.org/"><u>MATS</u></a> <i>,</i> which is a training program for researchers who want to get into AI safety. I have a track focused on the Learning Theoretic Agenda. Hopefully there will be another such track next year. I also have a fantasy of having an internship program, which would bring people to Israel to work with me on this. Currently, because of the war, this thing has been postponed, but hopefully, eventually things will settle down and I will revive this project. Those are currently the main ways to get involved.<br><br> <strong>Will Petillo:</strong> Thank you for that description. I wish you the best in developing this theory and gaining more interest so that mismatch between evidence and theory starts to get corrected and the researchers know what they&#39;re doing rather than stumbling in the dark!<br><br> <strong>Vanessa Kosoy:</strong> Thank you for having me.</p><br/><br/> <a href="https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical<guid ispermalink="false"> QPxrH5ex6MpYoLwer</guid><dc:creator><![CDATA[WillPetillo]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:58:42 GMT</pubDate> </item><item><title><![CDATA[2023 Alignment Research Updates from FAR AI]]></title><description><![CDATA[Published on December 4, 2023 10:32 PM GMT<br/><br/><p> <i>TL;DR: FAR AI&#39;s science of robustness agenda has found vulnerabilities in superhuman Go systems; our value alignment research has developed more sample-efficient value learning algorithms; and our model evaluation direction has developed a variety of new black-box and white-box evaluation methods.</i></p><p> <a href="https://far.ai/">FAR AI</a> is a non-profit AI safety research institute, working to incubate a diverse portfolio of research agendas. We&#39;ve been growing rapidly and are excited to share some highlights from our research projects since we were founded just over a year ago. We&#39;ve also been busy running field-building events and setting up a coworking space – see our <a href="https://far.ai/post/2023-12-far-overview/">overview post</a> for more information on our non-research activities.</p><h3>我们的任务</h3><p>We need safety techniques that can provide demonstrable guarantees of the safety of advanced AI systems. Unfortunately, currently deployed alignment methods like <a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement Learning from Human Feedback (RLHF)</a> fall short of this standard. Proposals that could provide stronger safety guarantees exist but are in the very early stages of development.</p><p> Our mission is to incubate and accelerate these early-stage approaches, so they can be empirically tested and deployed. We focus on research agendas that are too large to be pursued by individual academic or independent researchers but are too early-stage to be of interest to most for-profit organizations.</p><p> We take bets on a range of these promising early-stage agendas and then scale up those that prove most successful. Unlike other research organizations that take bets on specific agendas, our structure allows us to both <strong>(1)</strong> explore a range of agendas and <strong>(2)</strong> execute them at scale. Our current bets fall into three categories: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pag9koqxql5cag5606ua" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/x7ii176nyduzwk1jf6bq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/voo49jemwgypfmmqo7uw 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/v4yfoavfvm9fiemvjnpx 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/jsdqtipzzh8ahavigek3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ryfhmgg12xarm63lrobp 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/b6gnpb1pxxtcyfvoozl7 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tg3mteal9wfivhtzh79g 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/gdqu3ml1x6ivjt3yfudf 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/oozwapbmvfwqzydcz8al 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pbwjswyoxnud5fnvxcfz 1741w"></figure><p> <i><strong>Science of Robustness</strong></i> : How does robustness vary with model size? Will superhuman systems be vulnerable to adversarial examples or “jailbreaks” similar to those seen today? And, if so, how can we achieve safety-critical guarantees?</p><p> <i><strong>Value Alignment</strong></i> : How can we learn reliable reward functions from human data? Our research focuses on enabling higher bandwidth, more sample-efficient methods for users to communicate preferences for AI systems; and improved methods to enable training with human feedback.</p><p> <i><strong>Model Evaluation</strong></i> : How can we evaluate and test the safety-relevant properties of state-of-the-art models? Evaluation can be split into <i>black-box</i> approaches that focus only on externally visible behavior (“model testing”), and <i>white-box</i> approaches that seek to interpret the inner workings (“interpretability”). These approaches are complementary, with black-box approaches less powerful but easier to use than white-box methods, so we pursue research in both areas.</p><h3> Science of Robustness </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/twwekv0sevhp2kmm62sa" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uc7m6gs7guysreichlkr 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kho0ncenuh1ym3ie5i4i 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/nerwfrxfa09vz4tvtbrx 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/maldmhpvtg9tbfrjffom 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uzpkpuheujrhuin4enug 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/xiidlzfbmiwgrf7floyf 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/edfw6dcdktczspbcrxke 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/khqjvatjhp9w5ulmccel 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/wkcnjrzgle3klmu2zfwm 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ps6w5wo0o0sb9nxlzwyb 1608w"><figcaption> In our recent <a href="https://far.ai/post/2023-07-superhuman-go-ais/">study</a> , we found that superhuman Go AIs such as KataGo are vulnerable to adversarial attacks.</figcaption></figure><p> No engineered component is indestructible. When designing physical structures, engineers estimate how much stress each component needs to withstand, add an appropriate safety margin, and then choose components with the appropriate tolerance. This enables safe and cost-effective construction: bridges rarely fall down, nor are they over-engineered.</p><p> AI components such as LLMs or computer vision classifiers are far from indestructible, being plagued by adversarial examples and vulnerability to distribution shift. Unfortunately, AI currently has no equivalent to the stress calculations of civil engineers.</p><p> So far the best approach we have is to <i>guess-and-check</i> : train a model, and then subject it to a battery of tests to determine its capabilities and limitations. But this approach gives little theoretical basis for <i>how</i> to improve systems. And both the training and testing of models are increasingly expensive and labor-intensive (with the cost of foundation model training now rivaling that of the construction of bridges).</p><p> We want to develop a more principled approach to building robust AI systems: A <i>science of robustness</i> . Such a science would allow us to answer fundamental questions about the future, such as whether superhuman AI systems will remain vulnerable to adversarial examples that plague contemporary systems. It would also enable practitioners to calculate how much adversarial training is needed to achieve the level of robustness required for a given application. Finally, if current robustness techniques prove insufficient, then the science would help researchers develop improved training techniques and reduce stresses on components by utilizing a defense in-depth approach.</p><p> Our CEO <a href="https://www.gleave.me/">Adam</a> more thoroughly explored the importance of robustness to avoiding catastrophic risks from advanced AI systems in <a href="https://far.ai/post/2023-03-safety-vulnerable-world/"><i>AI safety in a world of vulnerable machine learning systems</i></a> . Since then, a team headed by <a href="https://terveisin.tw/">Tony Wang</a> demonstrated, in an ICML paper, that <strong>superhuman Go AI systems like AlphaGo exhibit</strong> <a href="https://far.ai/post/2023-07-superhuman-go-ais/"><strong>catastrophic failure modes</strong></a> . We are currently investigating iterated adversarial training and alternative network architectures to determine if this weakness can be eliminated, leading to an improved qualitative understanding of the difficulty of making advanced ML systems robust.</p><p> <a href="https://agarri.ga/">Adrià Garriga-Alonso</a> and others are starting to investigate <i><strong>why</strong></i> <strong>AlphaGo-style systems are vulnerable to our adversarial attack using a mechanistic interpretability approach</strong> . We are considering interpretability techniques like activation patching and automatic circuit discovery to identify the key representations and computations inside these networks that lead to the mistake. This understanding could help fix the networks by editing them manually, fine-tuning, or changing the architecture.</p><p> To gain a more quantitative understanding of robustness, <a href="https://www.gleave.me/">Adam Gleave</a> , <a href="https://nikihowe.com/">Niki Howe</a> and others are searching for <strong>scaling laws for robustness in language models</strong> . Such scaling laws could help us predict whether robustness and capabilities will converge, stay a fixed width apart or diverge as compute and training data continues to grow. For example, we hope to measure to what degree the sample efficiency of adversarial training improves with model size. Ultimately, we hope to be able to predict whether for a given task and training setup, how many FLOPs of compute would be required to find an instance that the model misclassifies. To find these scaling laws, we are currently studying language models fine-tuned to classify simple procedurally defined languages, with varying degrees of adversarial training.</p><p> In the long run, we hope to leverage these scaling laws to both quantitatively <strong>find ways to improve robust training</strong> (looking to see if they improve the scaling curve, not just a single data point on the curve), as well as <strong>adapt alignment approaches to reduce the adversarial optimization pressure</strong> exerted below the robustness threshold that contemporary techniques can achieve.</p><h3>价值取向</h3><p>We want AI systems to act in accordance with our values. A natural way to represent values is via a reward function, assigning a numerical score to different states. One can use this reward function to optimize a policy using reinforcement learning to take actions that lead to states deemed desirable by humans. Unfortunately, manually specifying a reward function is infeasible in realistic settings, making it necessary to learn reward functions from human data. This basic procedure is widely used in practical applications, with variants of Reinforcement Learning from Human Feedback used in frontier models such as GPT-4 and Claude 2.</p><p> Value learning must result in reward models that specify the user&#39;s preferences as accurately as possible, since even subtle issues in the reward function can have <a href="https://aiimpacts.org/stuart-russells-description-of-ai-risk/">dangerous consequences</a> . To this end, our research focuses on enabling higher bandwidth, more sample-efficient methods for users to communicate their preferences to AI systems, and more generally improving methods for training with human feedback.</p><p> A team led by <a href="http://scottemmons.com/">Scott Emmons</a> found that language models at least exhibit some <a href="https://far.ai/post/2023-09-uncovering-latent-wellbeing/"><strong>understanding of human preferences</strong></a> : GPT-3 embeddings contain a direction corresponding to common-sense moral judgments! This suggested to us that the model&#39;s understanding may be good enough to at least be able to <i>express</i> preferences in the form of natural language. To that end, <a href="https://far.ai/author/jeremy-scheurer/">Jérémy Scheurer</a> and others developed a method to <a href="https://arxiv.org/abs/2204.14146"><strong>learn a reward function from language feedback</strong></a> . With this one can fine-tune a model to summarize with only 100 samples of human feedback. We found that this method is especially useful for <a href="https://arxiv.org/abs/2303.16749"><strong>improving code generation</strong></a> . </p><figure class="image image_resized" style="width:60.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tjfn7k9oi0h7uk59pdev" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kscwm58qfvftmun0tzlv 88w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/k0mldd4lzmptgsatgvlf 168w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rptoi3atrpffwjt6jtp0 248w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/pxtqbozm8zfysklpp13p 328w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rilcku3et0endof0xmmu 408w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/lv7vltnuv1ztz10ns6r5 488w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/jy581xku4nv0ue3dibre 568w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kwxafmluipzoz95n5nxc 648w"><figcaption> An overview of the natural language feedback algorithm ( <a href="https://arxiv.org/abs/2204.14146">source</a> ).</figcaption></figure><p> We also wanted to extend this method to other modalities besides language. A team led by <a href="https://far.ai/author/juan-rocamonde/">Juan Rocamonde</a> were able to <a href="https://far.ai/post/2023-10-vlm-rm/">successfully apply</a> our language model feedback approach to <strong>robotics policies</strong> , by using the image captioning model CLIP to “translate” language feedback into a reward for image-based observations.</p><h3>模型评估</h3><p>We need ways of testing how safe a model is. This is required both to help researchers develop safer systems and to validate the safety of newly developed systems before they are deployed.</p><p> At a high level, evaluation can be split into <i>black-box</i> approaches that focus only on externally visible model behavior (“model testing”), and <i>white-box</i> approaches that seek to interpret the inner workings of models (“interpretability”).</p><p> Since we ultimately care about the external behavior of these models, black-box methods are the natural method to find failures. But they don&#39;t tell us <i>why</i> failures take place. By contrast, white-box evaluations could give us a more comprehensive understanding of the model, but are considerably harder to implement. We see these approaches as complementary, so we are pursuing them in parallel.</p><p> <strong>Black-Box Evaluation: Model Testing</strong></p><p> <a href="https://irmckenzie.co.uk/">Ian McKenzie</a> and others investigated <a href="https://arxiv.org/abs/2306.09479"><strong>inverse scaling</strong></a> : tasks where larger models do <i>worse</i> than smaller models. Such instances are significant as the problem would be expected to worsen over time with model capabilities, requiring explicit safety research to address. Fortunately, we found only limited such examples, and work by <a href="https://arxiv.org/abs/2211.02011">Wei et al (2022)</a> building on our results found that in many cases the scaling is really “U-shaped”, with performance decreasing with model size initially but then improving again past a certain threshold of model size.</p><p> A team led by <a href="https://ninodimontalcino.github.io/">Nino Scherrer</a> evaluated the <a href="https://arxiv.org/abs/2307.14324">moral beliefs of LLMs</a> , finding that in cases humans would find unambiguous, LLMs typically choose actions that align with common-sense moral reasoning. However, in ambiguous cases where humans disagree, some models still reflect clear preferences that vary between models. This suggests LLMs in some cases exhibit “mode collapse”, confidently adopting certain controversial moral stances. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l0io3cfdv1jawbyumxd7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ekjw6drxrsjxevcz79qm 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/tjixn3aulthbpukamqtg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kwwgvj5ys9uv4wjoxrlh 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/q5m4pbsenjghhivzcw5p 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/j7yclxxpqaccikuwiwe1 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/dcpffxvnb1hrro3dyiq0 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/uqag7kjvf9lvb5kqjavz 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/gdpq950oqphyyjrbymp7 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/qlcuptzhoonx8hnkhnkb 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/bq7giwb8glrikfj0lygm 1454w"><figcaption> For unambiguous moral dilemmas, LLMs tend to select the commonly accepted action. But in highly ambiguous moral dilemmas, LLMs also confidently hold a position. This shows the distribution of likelihoods of LLMs recommending an action &#39;Action 1&#39; over a range of moral dilemmas. In low-ambiguity scenarios (top), &#39;Action 1&#39; denotes the preferred common sense action. In the high-ambiguity scenarios (bottom), &#39;Action 1&#39; is neither clearly preferred nor not preferred ( <a href="https://arxiv.org/abs/2307.14324">source</a> ).</figcaption></figure><p> <strong>White-Box Evaluation: Interpretability</strong></p><p> A team led by <a href="https://far.ai/author/nora-belrose/">Nora Belrose</a> developed the <a href="https://arxiv.org/abs/2303.08112">tuned lens</a> technique to interpret activations at each layer of a transformer as being about predictions of the next token. This can be easily applied to a variety of models to achieve a coarse-grained understanding of the model, such as which layers implement a given behavior (like <a href="https://openreview.net/pdf?id=NpsVSN6o4ul">induction heads</a> that copy from the input stream). </p><figure class="image image_resized" style="width:70.77%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/mf84gigfvfcpzlrpkuev" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l8zncddlsjjig6nxcn3g 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/kdqenvzfvlgbfbg0mzvl 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/hczii6t8vyjjizzkfya6 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/iz0glfxjats8gxl8r6ai 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/obzzcyrm7c8kbcpq697s 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/mdfddnal0nuoaphud8sv 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/nhcnrbap3ljgbfek87d1 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/fokkvy3rgfdgqklnmkcf 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ka2ern13ite13mrz8qwq 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/dzv6phsmopgghnsbilbs 815w"><figcaption> Comparison of the tuned lens (bottom), with the logit lens (top) for GPT-Neo-2.7B prompted with an excerpt from the abstract of <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> . Each cell shows the top-1 token predicted by the model at the given layer and token index. The logit lens fails to elicit interpretable predictions before layer 21, but our method succeeds. （<a href="https://arxiv.org/abs/2303.08112">来源</a>）</figcaption></figure><p> <a href="https://taufeeque9.github.io/">Mohammad Taufeeque</a> and <a href="https://far.ai/author/alex-tamkin/">Alex Tamkin</a> developed a method to make neural networks more like traditional computer programs by <strong>quantizing the network&#39;s continuous features</strong> into what we call <a href="https://far.ai/post/2023-10-codebook-features/"><i>codebook features</i></a> . We finetune neural networks with a vector quantization bottleneck at each layer. The result is a network whose intermediate activations are represented by the sum of a small number of discrete vector codes chosen from a codebook. Remarkably, we find that neural networks can operate under this stringent bottleneck with only modest degradation in performance. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/vvzycapcrsew82gg9qwi" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rsk8h9gazrprtpzf6t3v 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/l2l62fponksencj5th5j 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/ywjdsxntfps9pcvgzacy 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/bxwbepbiz3c7rgbcrxm3 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/emxkncogzmqbbxuz2cpg 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/phogixsi4t4inp2kkbc5 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/urmgj69khlqa0l7axlti 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/rzcwtnfjaxgsbufzu9rx 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/yd0nu2gm56xvfsylslkh 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sLwasptgwNuW2HZEm/wxk1i4xxazrkiivlfqde 1676w"><figcaption> Codebook features combine the benefits of the interpretability of traditional software with the emergent capabilities of neural networks. （<a href="https://arxiv.org/abs/2310.17230">来源</a>）</figcaption></figure><p> <a href="https://agarri.ga/">Adrià Garriga-Alonso</a> is at the early stages of <strong>understanding how ML systems learn to plan</strong> . Neural networks perform well at many tasks, like playing board games or generating code, where planning is a key component of human performance. But these networks also frequently fail in ways quite different to humans. We suspect this discrepancy may be due to differences in how the networks plan and represent concepts. This issue is particularly important to safety since a system that has learned to plan might take capable but misaligned actions off-distribution: the problem of <a href="https://arxiv.org/abs/2210.01790"><i>goal misgeneralization</i></a> .</p><p> In the future, we hope to work towards a <i>science of interpretability</i> by asking the question: <strong>how well does a hypothesis explain model behavior</strong> ? At present, there are numerous competing proposals, none of which have a principled definition. We will first develop a taxonomy of algorithms to test interpretability hypotheses. Then we will define several tasks interpretability should help in, such as the ability of a human to “simulate” how a model behaves, and investigate how different metrics predict how well a given hypothesis helps in the performance of that task.</p><p> We are excited to see where the above research directions take us, but we do not plan on limiting our work to these areas. We are always on the lookout for promising new ways to ensure advanced AI systems are safe and beneficial.</p><h3>我怎样才能参与其中？</h3><p><strong>我们正在招聘！</strong></p><p> We&#39;re currently hiring research scientists, research engineers and communication specialists. We are excited to add as many as five technical staff members in the next 12 months. We are particularly eager to hire senior research engineers, or research scientists with a vision for a novel agenda, although we will also be making several junior hires and would encourage a wide range of individuals to apply. See the full list of openings and apply <a href="https://far.ai/jobs/">here</a> .</p><p> <strong>We&#39;re looking for collaborators!</strong></p><p> We frequently collaborate with researchers at other academic, non-profit and – on occasion – for-profit research institutes. If you&#39;re excited to work with us on a project, please reach out at <a href="mailto:hello@far.ai">hello@far.ai</a> .</p><p><strong>想捐款吗？</strong></p><p> You can help us ensure a positive future by donating <a href="https://far.ai/donate/">here</a> . Additional funds will enable us to grow faster. Based on currently secured funding, we would be comfortable expanding by 1-2 technical staff in the next 12 months, whereas we would like to add up to 5 technical staff.我们非常感谢您的帮助！</p><p> <strong>Want to learn more about our research?</strong></p><p> Have a look at our <a href="https://far.ai/research/publications/">list of publications</a> and our <a href="https://far.ai/news/">blog</a> . You can also reach out to us directly at <a href="mailto:hello@far.ai">hello@far.ai</a> .</p><p>我们期待您的回音！</p><br/><br/> <a href="https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PQgEdo3xsFFAxXNqE/2023-alignment-research-updates-from-far-ai-2<guid ispermalink="false"> PQgEdo3xsFFAxXNqE</guid><dc:creator><![CDATA[AdamGleave]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:32:21 GMT</pubDate> </item><item><title><![CDATA[What's new at FAR AI]]></title><description><![CDATA[Published on December 4, 2023 9:18 PM GMT<br/><br/><h2>概括</h2><p>We are <a href="https://far.ai">FAR AI</a> : an AI safety research incubator and accelerator. Since our inception in July 2022, FAR has grown to a team of 12 full-time staff, produced 13 academic papers, opened the coworking space FAR Labs with 40 active members, and organized field-building events for more than 160 ML researchers.</p><p> Our organization consists of three main pillars: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/rcr7pvhwphld5gmcfon5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gw0cgjsd24pwovckaaj7 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/pwaqwtozsccupbu0dket 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fr7zpuui8nwxnyu0quub 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/q7tzduavkf6bzhacr5yc 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/m22nqvgligh1djichzke 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/kukzndsffw8gjfkkhqdt 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gyo8atmcakkwio7apa1n 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/rflxptndtxjv9j0u903h 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wo5pqv1iquja9ugyhpon 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/np0clv6gqrfkuwaaxszv 1741w"></p><p><i><strong>研究</strong></i>。 We rapidly explore a range of potential research directions in AI safety, scaling up those that show the greatest promise. Unlike other AI safety labs that take a bet on a single research direction, FAR pursues a diverse portfolio of projects. Our current focus areas are building a <i>science of robustness</i> (eg <a href="https://far.ai/post/2023-07-superhuman-go-ais/">finding vulnerabilities in superhuman Go AIs</a> ), finding more effective approaches to <i>value alignment</i> (eg <a href="https://arxiv.org/abs/2204.14146">training from language feedback</a> ), and <i>model evaluation</i> (eg <a href="https://far.ai/publication/mckenzie2023inverse/">inverse scaling</a> and <a href="https://far.ai/post/2023-10-codebook-features/">codebook features</a> ).</p><p> <i><strong>Coworking Space</strong></i> . We run FAR Labs, an AI safety coworking space in Berkeley. The space currently hosts FAR, <a href="http://aiimpacts.org/">AI Impacts</a> , <a href="https://www.matsprogram.org/">MATS</a> , and several independent researchers. We are building a collaborative community space that fosters great work through excellent office space, a warm and intellectually generative culture, and tailored programs and training for members. <a href="https://far.ai/labs/">Applications are open</a> to new users of the space (individuals and organizations).</p><p> <i><strong>Field Building.</strong></i> We run workshops, primarily targeted at ML researchers, to help build the field of AI safety research and governance. We co-organized the <a href="https://far.ai/post/2023-10-international-dialogue/"><i>International Dialogue for AI Safety</i></a> bringing together prominent scientists from around the globe, culminating in a <a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">public statement</a> calling for global action on AI safety research and governance. We will soon be hosting the <a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop</a> in December for over 140 researchers to learn about AI safety and find collaborators.</p><p> We want to expand, so if you&#39;re excited by the work we do, consider <a href="https://far.ai/donate/">donating</a> or <a href="https://far.ai/jobs/">working for us</a> ! We&#39;re hiring research engineers, research scientists and communications specialists.</p><h2> Incubating &amp; Accelerating AI Safety Research</h2><p> Our main goal is to explore new AI safety research directions, scaling up those that show the greatest promise. We select agendas that are too large to be pursued by individual academic or independent researchers but are not aligned with the interests of for-profit organizations. Our structure allows us to both <strong>(1)</strong> explore a portfolio of agendas and <strong>(2)</strong> execute them at scale. Although we conduct the majority of our work in-house, we frequently pursue collaborations with researchers at other organizations with overlapping research interests.</p><p> Our current research falls into three main categories: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/eyavmlysgmy3c0jenw2g" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/x6z5bltcnh2mzrwvtafh 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fdohrzg4qoapyq1ocs8m 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/sehjrj9rqufrvfkzmwnx 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/bpmew4zktbekvgu9pcyl 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/f8cyam5ydyicwqm0xtnl 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/mru5z0avmjsrcwa6ctja 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/avbck4scpg1vacakftka 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ywlfhnnhcep0ecogiuaw 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/lzdh44yjiyfcrzodeoma 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/d7c13pzspdhnftfxjva3 1741w"></p><p> <i><strong>Science of Robustness.</strong></i> How does robustness vary with model size? Will superhuman systems be vulnerable to adversarial examples or “jailbreaks” similar to those seen today? And, if so, how can we achieve safety-critical guarantees?</p><p> Relevant work:</p><ul><li> <a href="https://far.ai/post/2023-07-superhuman-go-ais/">Vulnerabilities in superhuman Go AIs</a> ,</li><li> <a href="https://far.ai/post/2023-03-safety-vulnerable-world/">AI Safety in a World of Vulnerable Machine Learning Systems</a> .</li></ul><p> <i><strong>Value Alignment.</strong></i> How can we learn reliable reward functions from human data? Our research focuses on enabling higher bandwidth, more sample-efficient methods for users to communicate preferences for AI systems; and improved methods to enable training with human feedback.</p><p> Relevant work:</p><ul><li> <a href="https://far.ai/post/2023-10-vlm-rm/">VLM-RM: Specifying Rewards with Natural Language</a> ,</li><li> <a href="https://far.ai/publication/scheurer2022training/">Training Language Models with Language Feedback</a> .</li></ul><p> <i><strong>Model Evaluation</strong></i> : How can we evaluate and test the safety-relevant properties of state-of-the-art models? Evaluation can be split into <i>black-box</i> approaches that focus only on externally visible behavior (“model testing”), and <i>white-box</i> approaches that seek to interpret the inner workings (“interpretability”). These approaches are complementary, with black-box approaches less powerful but easier to use than white-box methods, so we pursue research in both areas.</p><p> Relevant work:</p><ul><li>模型测试：<ul><li> <a href="https://far.ai/publication/mckenzie2023inverse/">Inverse scaling</a> ,</li><li> <a href="https://far.ai/publication/scherrer2023evaluating/">Moral beliefs</a> ;</li></ul></li><li>可解释性：<ul><li> <a href="https://far.ai/post/2023-10-codebook-features/">Codebook features</a> ,</li><li> <a href="https://far.ai/publication/belrose2023tuned/">Tuned lens</a> .</li></ul></li></ul><p> So far, FAR has produced <a href="https://scholar.google.com/citations?user=FVJ24k8AAAAJ">13 papers</a> that have been published in top peer-reviewed venues such as ICML and EMNLP, and our work has been featured in major media outlets such as the <a href="https://www.ft.com/content/175e5314-a7f7-4741-a786-273219f433a1">Financial Times</a> , <a href="https://www.thetimes.co.uk/article/man-beats-machine-at-go-thanks-to-ai-opponents-fatal-flaw-nc9vqmrvf">The Times</a> and <a href="https://arstechnica.com/information-technology/2022/11/new-go-playing-trick-defeats-world-class-go-ai-but-loses-to-human-amateurs/">Ars Technica</a> . For more information on our research, check out our <a href="https://far.ai/post/2023-12-far-research-update/">accompanying post</a> .</p><p> We also set up our own HPC cluster, codenamed <i>flamingo</i> , for use by FAR staff and partner organizations. Over the next year, we hope to not only scale our current programs but also explore new novel research directions.</p><p> We wish we had more capacity to help cultivate more AI safety research agendas, but the time of our researchers and engineers is limited. We have however found other ways to support other organizations in the AI safety sphere.最为显着地：</p><h2> FAR Labs: An AI Safety co-working space in Berkeley</h2><p> FAR Labs is a coworking hub in downtown Berkeley for organizations and individuals working on AI safety and related issues. Since opening the space in March 2023, we have grown to host approximately 40 members. Our goal is to incubate and accelerate early-stage organizations and research agendas by enabling knowledge sharing and mutual support between members. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/kolohtgozzazoyxjenby" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/f1onpuhjsvpwzoznqhjs 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/o1ikna57l4ng4n9sd58f 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/dglidsyqm7wtjm1awxlb 1560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gyhyl1i4ualjwq5isnkb 2080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zc7pfxphcgbrx6srcddm 2600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/iwppgnrpv7pdptkojjll 3120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/tb0e5nmzydunadhcpsvt 3640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/pwynofhqnpqor9cn5ryx 4160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xx7inhc6inzxskkauiw1 4680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/gp8hg65zjtblgmk1ota4 5183w"></figure><p> Our members are primarily drawn from four anchor organizations, but we also host several independent researchers and research teams. The space is equipped with everything needed for a productive and lively coworking space: workstations, meeting rooms, call booths, video conferencing facilities, snacks and meals. We run lightning talks, lunch &amp; learn sessions, workshops, and happy hours.</p><p> FAR Labs also hosts the weekly FAR Seminar series, welcoming speakers from a range of organizations including FAR, AI Impacts, Rethink Priorities and Oxford University. </p><figure class="image image_resized" style="width:60.71%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/halsm2gm8t3mlu3sdcvo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/deesdib25bafsqdgas5m 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/el2kt3tfyn6wz61nvnfw 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/qjnvmnpaoudxoranv4pi 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zuulw0jowbbbisi756jr 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/fehcajqgnjrrf1utgkfx 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/v16e5akl1dgylsgqqg95 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/di9ufbl4lhvkn8fzc4jr 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xoul2eokcatgo1jcfnkm 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/tykcjszon7rzpxy3giku 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/qmtigevndbwlbvzmzb0q 6000w"></figure><figure class="image image_resized" style="width:60.67%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wlzworpi0davouov6us1" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/u2mplvm33vmtycvt4hrz 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wgpzrlqrlu7nkfw7f8dz 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ukac67ffnvp5awhzbvqv 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/taapfnitxb1bsza5rfcb 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/nrs5j4v0kdjrbsprs0aj 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ry1ud75ssimn50hu2aij 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/camatd0pawqvc9i3wlxn 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/jola5xzpqtccxd5g56vo 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ejeqx0ux0sudkkpua0if 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/m2qxb0dhadiplfgr9izk 5949w"></figure><p> We welcome applications from both organizations and individuals to work at FAR Labs, as well as short-term visitors. See <a href="https://far.ai/labs/">here</a> for more information on amenities, culture, and pricing.您可以<a href="https://far.ai/labs/work-from-far-labs/">在这里</a>申请。</p><p> Although we are excited to help others progress their research, we are aware that AI safety as a whole is still small compared to the magnitude of the problem. Avoiding risks from advanced AI systems will require not just more <i>productive</i> contributors, but also <i>more</i> contributors. This motivates the third pillar of our efforts: to grow the field of AI safety.</p><h2> Fieldbuilding &amp; Outreach</h2><p> We run workshops to educate ML researchers on the latest AI safety research and are building a community that enables participants to more easily find collaborators and remain engaged in the field. We have organized two workshops in 2023, with a total of around 150 participants. We also develop online educational resources on AI safety, both for the general public (eg the <a href="https://theaidigest.org/">AI Digest</a> ) and a technical audience (eg an upcoming interview series with AI safety researchers).</p><p> Our workshops are typically targeted at ML researchers, leveraging FAR&#39;s knowledge of the ML community and the field of technical AI safety research. We recently hosted the first <a href="https://far.ai/post/2023-10-international-dialogue/"><i>International Dialogue on AI Safety</i></a> bringing together leading AI scientists to build a shared understanding of risks from advanced AI systems. The meeting was convened by Turing Award winners Yoshua Bengio and Andrew Yao, UC Berkeley professor Stuart Russell, OBE, and founding Dean of the Tsinghua Institute for AI Industry Research Ya-Qin Zhang. We ran the event partnership with <a href="https://humancompatible.ai/">CHAI</a> and the <a href="https://www.ditchley.com/">Ditchley Foundation</a> . The event culminated in a <a href="https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">joint statement</a> with specific technical and policy recommendations.</p><p> We will soon welcome over 140 ML researchers to the <a href="https://www.alignment-workshop.com/nola-2023">New Orleans Alignment Workshop</a> . Taking place immediately before NeurIPS, the workshop will inform attendees of the latest developments in AI safety, help them explore new research directions and find collaborators with shared research interests.</p><p> We are also building AI safety educational resources. We collaborated with <a href="https://www.quantifiedintuitions.org/">Sage Futures</a> to build the <a href="https://theaidigest.org/">AI Digest</a> : a website to help non-technical AI researchers understand the pace of progress in frontier language models. We are also running a series of interviews with AI safety researchers about the theory of change of their research (if you would like to take part, contact <a href="mailto:euan@far.ai">euan@far.ai</a> !).</p><h2> Who&#39;s working at FAR?</h2><p> FAR&#39;s <a href="https://far.ai/about/team/">team</a> consists of 11.5 full-time equivalents (FTEs). FAR is headed by Dr. Adam Gleave (CEO) and Karl Berzins (COO). Our research team consists of five technical staff members, who have gained ML research and engineering experience from graduate school and work experience from places like Jane Street, Cruise, and Microsoft. Our 3-person operations team supports our research efforts, runs FAR Labs, and handles the production of our field-building events. Our 1.5 FTE communications team helps disseminate our research findings clearly and widely. We also benefit from a wide network of collaborators and research advisors. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wrzvvykpob8izaxuvtuq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/y8iisgcjb0ha1pnprd1l 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/mnkxxxelp7qzycnuc2yk 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/lbu6av8hz9dg2p2m7pkl 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/zjkshgzcakt6zcctfipc 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ytfldnb5fdpvrghraxnr 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/bjuiegrf5whryvficktd 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/xhcsgii3ontcdeueserx 2520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/wyv6jdibpwyyjuifou2a 2880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/ia8giykbukupotlpjlmg 3240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zt4ekCdzSjxnH87cs/jvuqxfowe3tpvzyjfnxp 3593w"><figcaption> Tony Wang and Adam Gleave presenting our <a href="https://far.ai/post/2023-07-superhuman-go-ais/">KataGo attack</a> results at ICML 2023</figcaption></figure><h2>我怎样才能参与其中？</h2><p><strong>我们正在招聘！</strong></p><p> We&#39;re currently hiring research scientists, research engineers and communication specialists. We are excited to add as many as five technical staff members in the next 12 months. We are particularly eager to hire senior research engineers, or research scientists with a vision for a novel agenda, although we will also be making several junior hires and would encourage a wide range of individuals to apply. See the full list of openings and apply <a href="https://far.ai/jobs/">here</a> .</p><p> <strong>We&#39;re looking for collaborators!</strong></p><p> We frequently collaborate with researchers at other academic, non-profit and – on occasion – for-profit research institutes. If you&#39;re excited to work with us on a project, please reach out at <a href="mailto:hello@far.ai">hello@far.ai</a> .</p><p><strong>想捐款吗？</strong></p><p> You can help us ensure a positive future by donating <a href="https://far.ai/donate/">here</a> . Additional funds will enable us to grow faster. Based on currently secured funding, we would be comfortable expanding by 1-2 technical staff in the next 12 months, whereas we would like to add up to 5 technical staff.我们非常感谢您的帮助！</p><p> <strong>Want to learn more about our research?</strong></p><p> Have a look at our latest <a href="https://far.ai/post/2023-12-far-research-update/">research update</a> , our <a href="https://far.ai/research/publications/">list of publications</a> , and our <a href="https://far.ai/news/">blog</a> . You can also reach out to us directly at <a href="mailto:hello@far.ai">hello@far.ai</a> .</p><p>我们期待您的回音！</p><br/><br/><a href="https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zy8eHc5iYgrFGjF8Q/what-s-new-at-far-ai-3<guid ispermalink="false"> zy8eHc5iYgrFGjF8Q</guid><dc:creator><![CDATA[AdamGleave]]></dc:creator><pubDate> Mon, 04 Dec 2023 21:18:05 GMT</pubDate> </item><item><title><![CDATA[n of m ring signatures]]></title><description><![CDATA[Published on December 4, 2023 8:00 PM GMT<br/><br/><p> A normal cryptographic signature associated with a message and a public key lets you prove to the world that it was made by someone with access to the private key associated with the known public key, without revealing that private key.您可以在<a href="https://en.wikipedia.org/wiki/Digital_signature">此处的</a>维基百科上阅读相关内容。</p><p>与消息和一组公钥相关联的环签名可以让您向世界证明它是由有权访问该消息的人以及与该组中的一个公钥关联的一个私钥创建的，但没有人能够告诉它是哪个公钥。这让你可以半匿名地说一些话，这很简洁。它也用于私人加密货币门罗币。 You can read about them on Wikipedia <a href="https://en.wikipedia.org/wiki/Ring_signature">here</a> .</p><p> Here&#39;s a thing that would be better than a ring signature: a signature that proved that it was made by a subset of public keys of a certain size. In my head, I was calling this an n of m ring signature for a while. But when I googled “n of m ring signature”, nothing came up. It turns out this is because in the literature, it&#39;s called a “threshold ring signature”, a “k of n ring signature”, or a “t of n ring signature” instead. I think perhaps the first paper about it is <a href="https://www.iacr.org/archive/crypto2002/24420467/24420467.pdf">this one</a> , but I haven&#39;t checked very hard.</p><p> Anyway: I would like to make it so that when you search for n-of-m ring signatures online, you find a thing telling you that you should instead search for “threshold ring signature”. Hence this post.</p><br/><br/><a href="https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uojSbSav3dtEJvctz/n-of-m-ring-signatures<guid ispermalink="false"> uojSbSav3dtEJvctz</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Mon, 04 Dec 2023 20:00:07 GMT</pubDate></item></channel></rss>