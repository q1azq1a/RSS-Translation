<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 23 日星期四 20:12:05 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Prepsgiving, A Convergently Instrumental Human Practice]]></title><description><![CDATA[Published on November 23, 2023 5:24 PM GMT<br/><br/><p>大多数文化都有一个<a href="https://en.wikipedia.org/wiki/List_of_harvest_festivals">丰收节</a>，每个丰收节基本上都是一个预庆祝活动。</p><p>在北半球，这可能发生在十一月或十月，而在南半球，这可能发生在五月或六月。这样的庆祝活动可能不止一次，在美国，我认为万圣节和感恩节<i>都</i>算作例子。</p><p>根据您想要如何构建它，您可以认为理念在目的论上导致了所有这些实例，但您也可以轻松地声称实例在认识论上导致了理念。</p><p>如果你认为这篇文章是个好主意，并且一开始不想改变你的行为，我鼓励你更用心地享受丰收节，并简单地将它们视为这个想法的实例，如果这有助于推动将练习变成对个人和当地更有用的形状，那就更好了！</p><p>如果你想非常认真地对待这种做法，并且经常这样做（每月、每周或每天的节奏），那么我鼓励你仍然认真对待传统的丰收节，并中断你<i>当地的</i>日常生活，融入任何更大或更古老的活动或更重要的是，因为帮助提高人们的<a href="https://en.wikipedia.org/wiki/Situation_awareness">态势感知能力</a>是此类活动的优点之一。</p><p> Prepsgiving的名称与感恩节有关，但时间方向相反。一般来说，丰收节，特别是感恩节，基本上都是为了庆祝人们成功度过了一个时期，在这个时期，人们必须认真思考食物才能过上美好的生活，无论是从拥有复杂机械的田地里拉出大量农产品，还是为了生存食品供应线中断，或者其他什么……它发生<i>在过去</i>，所以回顾过去，你可以<i>表示感谢</i>。</p><p>对于 Prepsgiving，您也应该<i>有所期待</i>，这样您就可以<i>做好准备</i>。将这种未来导向带入现有事件可能会帮助您注意到，即使只是对过去发生的好事表示感谢，也可以帮助人们做好准备更好地处理<i>未来的</i>不利事件。</p><p>人类灾难规划有一个有趣的模式，当我们作为个人参与飓风、地震、龙卷风或火灾时，我们往往会为飓风、地震、龙卷风或火灾做好准备，但一旦我们真正采取认真的措施，我们就会采取最有效的措施。人们注意到有很多简单易行的事情可以帮助解决所有这些模式。几乎在所有这些情况下，有一个“旅行包”很有用，里面装着作为避难所时有用的东西。在大多数情况下，“储存的水”可能是有用的。</p><p>徒步旅行装备在这里有些重叠，因为碘丸是一种非常<i>紧凑的</i>“获得紧急用水”的方式。</p><p>预防不是一种单一的做法，以一种单一的方式运作，而是“举行庆祝活动，<i>思考</i>并<i>更好地</i>处理在许多可能的紧急情况下紧急食品物流中出现的融合”的整体<i><strong>融合</strong></i>。</p><p>例如，在一次好的预祭中，可能会有更多的人而不是更少的人。这平均降低了认知负担，有助于汇总稀有知识，让儿童有机会通过观察从值得信赖的成年人那里学习稀有的食物准备技能，利用食物生产本身的规模效率，帮助人们变得友好和熟悉更多的人在他们扩展的社交网络中，也许会开始建立一个社交网络，在这个网络中可以进行食物易货贸易，如果食物供应在一段时间内出人意料地稀缺，可以通过面对面的过程获得巨大的<a href="https://en.wikipedia.org/wiki/Gains_from_trade">贸易收益</a>。</p><p>在新冠疫情之前，我一直有“预感”“作为一个想法，我应该尝试做更多的事情，普及更多的东西”，而在新冠疫情之后（但如果世界变得更加混乱的话，未来供应链可能会中断）我&#39;实际上，我们从“每个月的第三个星期四（除非有丰收节）”开始了每月的练习。从它已经<i>发生</i>而不是<i>没有</i>发生的意义上来说，这是非常成功的！</p><p>试图“每周四晚上（除非每月一次）”举行一次活动并没有奏效，但根据经验，我注意到它的语气更接近“每周一次的宗教庆祝活动”，事实上，如果您已经举办过每周举行一次涉及分享食物的宗教庆祝活动，您可能会眯起眼睛并注意到那里也存在预先存在的聚合活动。</p><p>任何聚餐，尤其是交换食谱的聚餐，都已经是一种预祭。</p><p>我担心的是，在现代，世界上许多地方都变得非常城市化，混合率很高，人们不太可能认识他们的邻居。事实上，在某些地方，有些人似乎表现得好像邻居之间的“合作/合作”动态正在避免互动，这样你们就不会以昂贵的方式与对方纠缠在一起。</p><p>我在一个小镇的一个山羊农场长大，我有点了解这些城市规范，但对我来说也感觉有点恶心。我不确定我是否应该反对他们，或者应用“在罗马时”的格言……但是，对于大学及以后的生活，我发现大城市的工作和学习机会要好得多，所以我“ “在罗马”已经有一段时间了……但我怀念邻居们关心邻居的感觉，而且我认为缺乏这种睦邻凝聚力让“生活在城市”感觉更加不稳定。</p><p>作为对此的一种解药，我建议，如果您想每月或每周在一个城市做某事，请考虑您可以或愿意向其“借出或<a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/MayIBorrowACupOfSugar">借一杯糖</a>”的人名单，并邀请他们人们定期在家里吃顿便饭或烧烤之类的东西（这可能会因丰收节而中断，但希望生活恢复正常后会恢复）。也可能......如果你有一个邻居，你不确定“保持解脱”是否是合作/合作动态的样子......也许只是<a href="https://www.lesswrong.com/tag/guess-ask-tell-culture">告诉他们你一直在猜测他们想要什么，然后询问明确表示</a>他们是否愿意找个时间过来吃顿便饭？ :-)</p><p>我试图在这里谨慎行事，以便这篇文章能够在宗教社会中发挥作用，并让他们感到沾沾自喜，因为他们“已经聚集在这个好主意上”，并且可能会因为它所带来的沾沾自喜的感觉而分享这篇文章他们，而且它也可以在一个更加无神论的社会中发挥作用，帮助人们注意到，不必有与这种聚合有用的事情相关的特定崇拜实践......</p><p>而且，在存在许多宗教和人民的多元文化社会中，您可能确实希望拥有<i>元文化</i>传统，使不同的部落和亚文化可以聚集在一起，友好地分享食物。你可能不会与邻居分享所有的信仰，但<i>你们确实有着地理上的接近</i>，如果供应链受到干扰，你们<i><strong>将</strong></i>共同面临粮食短缺的困境。</p><p>我明确注意到并试图友好对待的另一个文化怪癖是“末日准备者”的想法。</p><p>准备者显然是一种跨文化的“人”，他们提前考虑负面的不规则事件，并为“以防万一”做好准备，通常包括储备额外的食物，以保持很长一段时间。 （而且他们通常不太需要雇用勤杂工，并且可能擅长也可能不擅长个人自卫。）</p><p>如果您是一名准备者，我鼓励您将 Prepsgiving 的想法视为一种更深入、更好的准备方式。具体来说，我鼓励您考虑一下您可能“准备好去吃 TEOTWAWKI”的所有食品，并尝试列出您可以从现有的材料中准备的所有便餐项目，然后找出哪种便餐项目将是您可以在聚会上展示最好的东西，然后打印该东西的食谱，其中不列出成分和准备步骤，但还列出每种成分的保质期，以及最好的方法如果批量购买该成分，可以获得一个好价格。这种“预习秘方”看来自己知道还是很有用的，知道了之后，在没有什么紧急情况的情况下分享给别人，基本上只能算是大家的胜利了。</p><p>这里发生了一件非常实际的事情，因为如果像这样的正式预聚聚餐结果是一场糟糕的食物差事，那么所有参与的人都可以注意到并努力解决这一问题。如果正式的聚餐是一顿丰盛的盛宴，有美味的食物和一群互相关心的人，那么未来可能发生的许多灾难，你和你的一群人可能会轻松自在地应对，甚至可以用备用的钱来应对。帮助他人的能力（而不是需要并希望施舍）。</p><p>这是一个总是可以以更聪明的方式变得更好或做得更多的假期。</p><p>团体可以更大，食物可以更美味，成分可以更稳定。</p><p>你可以每年、每月、每周进行一次，或者假设你甚至可以每天进行一次。举办此类活动的一个人或家庭可以让许多其他人参加，并由此获得一些安慰，并且拥有几个不同的朋友团体（出于多种不同的原因而聚会，有时也聚集在一起进行预祭）只会增加弹力！</p><p>这也可以与基本上任何宗教习俗混合。</p><p>您可以进行犹太洁食准备或清真准备（即使您不是犹太人或穆斯林）。您可以进行印地语准备活动并确保没有牛肉（等等），或者进行耆那教准备活动并确保&lt;<a href="https://jainworld.com/literature/jain-food/">一大堆限制条件，其中不包括胡萝卜或卷心菜</a>>;。你可以做一个没有“ <a href="https://www.archspm.org/faith-and-discipleship/catholic-faith/why-dont-catholics-eat-meat-on-fridays/">肉</a>”的天主教周五预祭，或者素食主义者预祭，同时去掉酪蛋白和蜂蜜等，或者你可以做一个不和谐的周五预祭，并确保在热狗面包中提供热狗，这<i>违反</i><a href="https://principiadiscordia.com/book/11.php">了不和谐的戒律是不吃热狗面包</a>。</p><p> （你可能会在这里注意到，如果你仔细观察，“更持久的宗教和更健康的人”似乎有更繁重的实践，而更开放的思想和稍纵即逝的宗教往往更容易。就我个人而言，我尊重后一种群体，因为<i>寻找</i>新的想法和前一种群体来帮助<i>过滤</i>它们。这不是一个硬性规定，但思考起来很有趣。）</p><p>如果你钦佩 Prepsgiving 的融合文化实践，并希望帮助它在你身边突然出现，我鼓励你顺其自然，做任何看起来与它相似的事情。</p><p>我建议不要将这个新框架<i>强加</i>在旧事物上，而是要在你的生活中“添加更多这样的内容”，你应该寻找可以与<strong>所有其他版本</strong><i>协调一致</i>的方式添加它的角落和缝隙。由于大多数宗教已经做了这样的事情，我会为这些版本保留周五、周六和周日，而周一是周一，重点是恢复工作，这使得周二、周三和周四成为自然的夜晚，也许可以度过作为每周练习的预祝晚宴。在这些选项中，我个人认为星期四是最好的。</p><p>如果你打算每月进行一次，我建议“每个月的第三个星期四（除非有丰收节）”。</p><p>我对这个想法的态度（就像许多好想法一样）是，它通常是我们发现已经存在的东西，其中以某些方式做事的古老原因并不明显，然而“所有正在蓬勃发展的人做这件事”，而不做这件事往往与不那么繁荣有关。在这种情况下，很难理清其中的因果关系，但如果你想动动你的大脑，那就尽一切努力尝试吧！如果您只是想找个借口筹划更多聚会，并且觉得这样做是有道德的，那么这也是一个有效的方法:-)</p><br/><br/> <a href="https://www.lesswrong.com/posts/KJyqBYRTCJn4NEysi/prepsgiving-a-convergently-instrumental-human-practice#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KJyqBYRTCJn4NEysi/prepsgiving-a-convergently-instrumental- human-practice<guid ispermalink="false"> KJyqBYRTCJn4NEysi</guid><dc:creator><![CDATA[JenniferRM]]></dc:creator><pubDate> Thu, 23 Nov 2023 17:24:56 GMT</pubDate> </item><item><title><![CDATA[AI #39: The Week of OpenAI]]></title><description><![CDATA[Published on November 23, 2023 3:10 PM GMT<br/><br/><p>董事会解雇了山姆·奥尔特曼，然后又让他复职，这主导了本周的其他事情。其他事情也发生了，但首先肯定要关注这一点。</p><h4>目录</h4><p>OpenAI 的发展远比本文所读到的其他内容重要得多。因此，您可以<a href="https://thezvi.substack.com/p/openai-facts-from-a-weekend" target="_blank" rel="noreferrer noopener"><strong>在周末阅读此事件时间表</strong></a>，并<a href="https://thezvi.substack.com/p/openai-the-battle-of-the-board" target="_blank" rel="noreferrer noopener"><strong>尝试将所有信息放在一起</strong></a>。</p><span id="more-23605"></span><ol><li>介绍。</li><li>目录。</li><li>语言模型提供了平凡的实用性。叙述你的生活，就像你一生所做的那样。</li><li>语言模型不提供平凡的实用性。提示注入未解决。</li><li> <strong>Q连续体</strong>。关于新训练技术的争议。</li><li> <strong>OpenAI：传奇仍在继续</strong>。故事还远未结束。</li><li>奥特曼可以挺身而出。他了解存在风险。现在他可以行动了。</li><li>你认为本周很艰难。事情并没有变得更容易。</li><li>图像生成的乐趣。几秒钟的鸸鹋。</li><li> Deepfaketown 和 Botpocalypse 很快就会出现。谨防电话索要金钱。</li><li>他们抢走了我们的工作。某些地区的自由职业者遇到了麻烦。</li><li>参与其中。 Dave Orr 正在为 DeepMind 协调团队招聘员工。</li><li>介绍一下。 Claude 2.1 看起来是一个实质性的渐进式改进。</li><li>在其他人工智能新闻中。 Meta 解散了“负责任的人工智能”团队。微软投资 50b 美元。</li><li>安静的猜测。深度学习会碰壁吗？</li><li>寻求健全的监管。欧盟人工智能法案举步维艰，联邦贸易委员会人工智能定义则毫无意义。</li><li>这不是极权主义的意思。人们需要消除这种说法。</li><li>音频周。萨姆·奥尔特曼、约书亚·本吉奥、大卫达、伊利亚·苏茨克维尔。</li><li>修辞创新。大卫·萨克斯本周说得最好。</li><li>调整比人类更聪明的智能是很困难的。技术辩论。</li><li>人们担心人工智能会杀死所有人。现在在本节中已完全了解 Roon。</li><li>其他人并不担心人工智能会杀死所有人。听他们说。</li><li>较轻的一面。是的，我当然知道，但是你听到自己的声音了吗？</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/gdb/status/1725595967045398920">GPT-4-Turbo 在竞技场排行榜上的表现明显优于 GPT-4</a> 。 GPT-3.5-Turbo 仍然领先于 OpenAI 或 Anthropic 以外的所有模型。 <a target="_blank" rel="noreferrer noopener" href="https://t.co/4LVJjx4pZi">Claude-1 的得分超过了 Claude-2，并且非常接近旧的 GPT-4 的</a>第二名，这很奇怪。</p><p>拥有太多加密货币？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/iandaos/status/1725191487795970321?s=46">伊恩（Ian）构建了一个可以“使用区块链进行银行自身管理”的 GPT</a> 。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DKThomp/status/1726750993394614345">论文称人工智能胰腺癌检测终于超越了放射科医生专家</a>。这是我们一直期待的事情，但一直没有发生。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/charliebholtz/status/1724815159590293764">David Attenborough 使用 11 个实验室和 GPT-4V 讲述您的生活</a>指南。<a target="_blank" rel="noreferrer noopener" href="https://github.com/cbh123/narrator">代码在这里</a>。不错的选择。不是我最喜欢的，但非常好的选择。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/charliebholtz/status/1725290062055682534">另一个不错的选择是拉里·大卫（Larry David）担任生产力教练。</a></p><h4>语言模型不提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KGreshake/status/1725515560870433016">不好了。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf4e4f12-c017-4df8-8f2c-62bfd14f95b7_699x674.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3FCfEqRiLLb4gFu3H/psdnk7qdrrnenvzygwzq" alt="图像"></a></figure><blockquote><p> Kai Greshake：PSA：美国军方正在积极测试法学硕士并将其部署到战场上。我认为这些系统很可能容易受到对手的间接即时注入。我将在这个线程中阐述这个故事。</p><p>这是<a href="http://Scale.ai’s" rel="nofollow">http://Scale.ai 的</a>Donovan 模型。基本上，他们让法学硕士查看并搜索您的所有军事数据（资产和威胁情报），然后它告诉您应该做什么。</p><p>现在，如果你让模型也看到新闻和公共信息，事实证明它非常有用。这称为开源情报或 OSINT。在此屏幕截图中，您可以看到他们从*对手*可以发布的目标区域加载“新闻和新闻报道”！</p><p>我们已经多次证明，如果攻击者可以将文本注入您的模型，您就可以使用自然语言对其进行“重新编程”。想象一下隐藏和操纵呈现给操作员的信息，然后让你的小对手告诉他们攻击哪里。</p><p> ……</p><p>不幸的是，这里的目标是缩短做出决定的时间，因此交叉检查所有事情是不可能的，而且他们并不害怕谈论意图。将会有一个“人在循环中”，但该人将从攻击者的爪牙那里获取信息！</p><p> ……</p><p> @alexlevinson（大规模安全主管）回复我，说这些是“人工智能系统固有的潜在漏洞，[...] &lt;that>;不会自动转化为单个人工智能系统内的特定漏洞”</p><p>并且“每个人工智能系统[…]都采用独特的安全措施进行设计，这些措施可能会或可能不会受到您已识别的漏洞的影响”。</p><p>现在，我还没有接触到多诺万，只能根据公开信息和我的专业知识来判断。我希望每个人都能自己判断是否相信 Scale 已经找到了解决此问题的秘密解决方案，从而让他们有信心进行部署。</p></blockquote><p>是的，这是什么词，完全疯了？不是以“你将唤醒天网”的方式，尽管它肯定对此类问题没有帮助，而是以“你将被敌人立即注射或攻击”的方式。</p><p>这甚至没有涉及此类系统可能用于泄露机密文件和其他情报的方式。</p><p>您可以将法学硕士与武器和您的机密数据联系起来。您可以将法学硕士连接到外部数据源。您无法负责任或安全地同时做到这两点。选一个。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robinhanson/status/1725463031163810046">Robin Hanson 的调查发现，大多数人认为软件的生产力还没有太大提高</a>。</p><blockquote><p> Robin Hanson：过去 10 年（忽略法学硕士）软件时间/成本平均预计减少约 7%，最近由于法学硕士而减少约 4%。但估计方差很大。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46b0351f-0e17-4c19-bca3-2e632ff30513_888x474.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3FCfEqRiLLb4gFu3H/hl16o9rbzewvqkomnmjz" alt=""></a></figure><blockquote><p> Robin Debreuil：经验丰富，我 100% 确定它已经低于 90。此外，大部分节省都在开发的前端（查找和集成技术、测试等），因此原型化想法的速度要快得多。质量也会显着提高，但还没有那么多。</p></blockquote><p>对于大多数项目，我都同意德布勒伊的观点。不太明显的是，这是否适用于最昂贵和最有价值的产品，其结果需要相当可靠。我的推测是仍然如此。我知道我的编码效率显着提高。</p><h4> Q连续体</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/?utm_source=twitter&amp;utm_medium=Social">路透社有一个有争议的说法，即在奥特曼被罢免之前</a>，几位研究人员向董事会发出通知，称一种名为 Q* 的新算法技术的测试取得了惊人的进展，这导致了奥特曼的罢免。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/hamandcheese/status/1727560845025005804">Q 指的是一种已知类型的 RL 算法</a>，OpenAI 一直在研究这种算法是有意义的。</p><p>报告的结果本身并不可怕，但可能会指出以后的可怕潜力。如果奥特曼没有与董事会分享结果，这可能是“不够坦诚”的一部分。然而，这个故事在 Verge 上被明确否认， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/alexeheath/status/1727472179283919032">他们的编辑亚历克斯·希思 (Alex Heath) 表示，多个消息来源声称这不是真的</a>，而且<a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/is-the-reuters-story-about-openais">我的预测市场认为这个故事有 29% 的真实性，</a>即使提供“一些捐赠”。 <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/jacksonpolack/does-openais-unreleased-q-model-rel">另一个市场中</a>40% 的人认为 Q* 是“一项重大的功能进步”。</p><p>现在我将等待更多信息。期待后续。</p><h4> OpenAI：传奇仍在继续</h4><p>Sam Altman 被 OpenAI 解雇。现在他回来了。有关详细信息，请参阅我关于该主题的两篇文章： <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-facts-from-a-weekend">OpenAI：周末的事实</a>和<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board">OpenAI：董事会之战</a>。</p><p>超简短的版本是，奥特曼向董事会提出了我们所知道的各种解雇他的理由，并寻求巩固权力，董事会基本上没有任何解释就解雇了奥特曼，奥特曼召集了投资者，尤其是微软和 97% 的员工，他威胁要每个人都离开并加入微软， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/eshear/status/1727210329560756598">董事会同意辞职，转而成立新的谈判董事会</a>，并<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1727207458324848883">让奥特曼回归</a>。</p><p>接下来会发生什么取决于所选择的完整董事会以及谁在功能上控制它。新的临时董事会成员包括 Brad Taylor、Larry Summers 和 Adam D&#39;Angelo。最终的董事会将有九名成员，其中一名来自微软，至少作为观察员，奥特曼最终可能会回归。这仍然为任意数量的结果留下了空间。如果他们创建一个新的董事会，足够关心安全，能够表明自己的立场，并可以对奥特曼进行制衡，那么这与董事会最终基本上处于奥特曼的控制之下，或者作为一个传统的首席执行官董事会（不喜欢任何人）的结果截然不同。德安吉洛优先考虑投资者和利润，而不是人类的免死。我们将会看到。</p><p>推特上的响亮声明仍然是，这是 Altman 以及进行普通利润最大化的 VC-SV 式业务的彻底胜利。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bcmerchant/status/1726692925743284351">或者没有其他办法可以结束对企业利润的威胁</a>。这在很大程度上是一种有意的表现和自我实现的宣言。权力就像魔术师的伎俩，存在于人们相信它存在的地方。</p><p>事情可能会这样发展，但不要将这种权力游戏与现实混为一谈。我们还不知道最终的结果会是什么。</p><p>奥特曼的复职也并非不可避免。想象一下这样一个世界：董事会没有保持沉默，而是提出了自己的观点，并在解雇 Altman 的同时引入了可信的额外董事会成员（比如泰勒、萨默斯、谢尔和米拉·穆拉蒂），并且在股票出售给员工们已经完成了。我敢打赌，情况会有很大不同。</p><p>推荐： <a target="_blank" rel="noreferrer noopener" href="https://loeber.substack.com/p/a-timeline-of-the-openai-board">OpenAI 板的时间线历史。</a>奥特曼和布罗克曼曾一度是四名董事会成员中的两名。董事会曾多次扩张和收缩。在这整个时期，似乎没有人足够认真地对待董事会，将其视为控制自己继任者的永久最终权威。各方怎么会允许事情发展到这个地步呢？</p><p>推荐： <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/labenz/status/1727327424244023482">Nathan Lebenz 提供了有关他参与 GPT-4 红队经历的主题。</a> OpenAI 的人并没有意识到他们拥有什么，他们太习惯于担心缺点，而忽视了他们的新模型有多好。尽管愿意在部署前等待很长时间，但他发现这些工作没有指导，大部分涉及的内容都经过检查，是一个完全不充分的过程。与此同时，几个月以来，董事会都无法访问 GPT-4，当莱本茨前往董事会时，人们利用对莱本茨性格的攻击来让他闭嘴。</p><p>在 OpenAI 的“我们回来了”派对上， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/erinkwoo/status/1727229929249821049">烟雾机触发了火警，</a>导致两辆消防车出现。所有未来的火警警报都将被抹黑，现实中的黑客作家也是如此。做得更好。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-11-22/inside-the-coups-and-concessions-that-brought-altman-back-to-openai">彭博社周三下午提供了更多细节。</a>将对整个事件进行独立调查。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/stuartbuck1/status/1727326239537700988">拉里·萨默斯 (Larry Summers) 曾说过伊丽莎白·沃伦 (Elizabeth Warren) 的话，这句话似乎很相关</a>：</p><blockquote><p>他这样说：我有选择。我可以是局内人，也可以是局外人。外人想说什么就说什么。但内部人士不听他们的。然而，内部人士可以获得大量的接触机会和推动他们的想法的机会。人们——有权势的人——倾听他们所说的话。但内部人士也明白一条牢不可破的规则：<em>他们不批评其他内部人士。</em></p><p>我已经被警告过。</p></blockquote><p>斯图尔特·巴克将此解释为支持奥特曼对托纳的批评。</p><p>上下文中的另一个含义是奥特曼是这种形式的内部人士。这意味着他不会听取任何批评内部人士的声音。这意味着他不会听取最有意义的批评。我喜欢认为，相反，我们看到的是奥特曼愿意使用这些原则作为武器。</p><p>我对内部人规则的实际理解并不是内部人永远不会听外界的批评。而是他们不觉得有义务或束缚，并且可以选择忽略它。他们也可以选择聆听。</p><p>一个关键问题是萨默斯是否认可这条规则，或者是否如我所希望的那样，萨默斯观察到这条规则的存在并提供了明确性。第二条内幕规则是你不能谈论内幕规则。</p><p>同样在萨默斯， <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-11-22/larry-summers-jumps-to-center-of-ai-stage-with-openai-board-seat">彭博社指出，他预计人工智能将出现在白领工作中</a>。一个令人担忧的迹象是，他对美国“失去对中国的领先地位”表示担忧。我们的命运在很大程度上取决于拉里·萨默斯的世界模式，这是一个多么美好的世界啊。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-11-22/sam-altman-openai-comeback-is-a-strategic-win-for-microsoft-s-nadella?srnd=premium">Parmy Olson 在彭博社撰文称</a>，OpenAI 之前的设置对人类有利，但对微软不利，新董事会将是传统的，现任成员会向投资者喊话“安全”。微软通过与新技术保持一定距离来让 OpenAI 更快地发展而受益。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1726780306185912749">Rob Bensinger 问道</a>，如果所有员工都认为 Toner 关于 OpenAI 关闭的声明可能符合其使命，那这对未来危险的潜在行动有何意义？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/catehall/status/1727394993940353121">Cate Hall 提醒我们</a>，对于那些认为 OpenAI 并不是一件好事的人来说，这些董事会席位的代价非常高。如果事实证明新董事会并不能遏制奥特曼，而本周却适得其反，那么那些财力雄厚的人多年来的策略就毫无意义了。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://medium.com/@leagueplayer220/sam-altman-may-have-committed-fraud-at-loopt-could-the-openai-firing-be-a-similar-incident-fe316d4900e7">声称 Altman 在他的初创公司 Loopt 中</a>让他的朋友在谈判期间出现，并假装是从事其他重大交易的员工，以给人留下错误的印象。正如海报所指出的，这是奥特曼像成功的初创企业创始人一样足智多谋的表现。这也是骗子的典型举动，而不是一个值得信任的人的标志。</p><p>交易到位后， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MattZeitlin/status/1727451694672339128">维诺德·科斯拉表示，非营利组织的控制体系很好，看看像宜家这样的公司</a>。难道他不明白其中的区别吗？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/MachineLearning/comments/1812w04/comment/kabk73s/">“OpenAIofThrones”（没有私人知识）在 Reddit 上提出了一个有趣的主张，它是我在《董事会之战》中概述的更具体</a>、更极端的版本。奥特曼试图在没有托纳的情况下召集董事会开除她，伊利亚犹豫不决，这既提供了手段，又提供了在伊利亚改变主意之前解雇奥特曼的短暂窗口，最终奥特曼眨了眨眼睛，同意接受真正的监督。</p><p>无论发生什么，我们都可以抛开分歧，指出《纽约时报》完全未能理解所发生的事情。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/garrytan/status/1727570552989298711">加里·谭（Garry Tan）：</a> 《纽约时报》现在只是在头版上直接进行人身攻击，没有任何事实，用燕尾服照片将“资本家”置于十字准线中，这是一些高级的真实宣传。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1727627979809857957">Paul Graham：</a> OpenAI 的领导、员工和代码都将迁移到微软。艰苦的努力使他们能够继续为非营利组织工作。 《纽约时报》反应：“人工智能现在属于资本家。”</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4cc54a-98b6-41da-9f13-2f182dc7c603_1290x1710.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3FCfEqRiLLb4gFu3H/wb2gjemslu7o8fpaofmg" alt="图像"></a></figure><p>这完全不是这样的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bayeslord/status/1727409560183509397">在下面这个狭隘的问题上，表演型的人喜欢幸灾乐祸。但海伦·托纳是对的，格雷格·布罗克曼是错的。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F136e211f-02fe-453c-8190-93edede376e8_916x1192.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3FCfEqRiLLb4gFu3H/gynjk9tdmpce0etig46d" alt=""></a></figure><h4>奥特曼可以挺身而出</h4><p>它在很大程度上是一个失败者，但我真诚的希望是尼克松访问中国。</p><p>奥特曼现在拥有他的团队的忠诚度，他有能力在被触怒时关闭他帮助建立的东西，以及相信他能找到出路的投资者坚定不移的信心。没有人可以说他们不发货。 OpenAI 在人工智能竞赛中保持着相当大的领先优势。 e/acc 人群已经集结了旗帜，并且总是更倾向于反对那些反对事物的人，而不是其他任何事物。</p><p>如果 Altman 和他的团队确实非常关心构建安全 AGI 的安全部分，那么他现在有机会做最有趣、也是最好的事情。</p><p>他现在有可信度地说，现在是时候在以下方面做出严肃的、代价高昂的承诺和投资了，而不是在“EA”和“e/acc”之间，或者担心和不担心、“末日一代”和（婴儿潮一代？）之间进行冲突。名义上确保安全AGI实际上是安全的。</p><p>并不是因为他被迫这样做——我们当然都知道，旧董事会可能获得的任何此类秘密承诺都毫无价值。不是为了安抚派别或董事会成员。他能做到这一点，因为他知道这是正确的事情，而且他现在能够在不危及他权力的情况下做到这一点。</p><p>这就是试图联合比你更有能力的人，因为工具的融合而追求权力。无论您是否成功，早期的步骤看起来都是一样的。你只有到最后才能知道结果是否与人类兼容。</p><h4>你认为这周很艰难</h4><p>Ilya Sutskever 试图打破人工智能的发展，并从一个明确致力于安全的组织中剔除他当时认为鲁莽的首席执行官。或者至少，这是外界每个人都相信的故事。</p><p>发生了什么？来自四面八方的压力铺天盖地而来。尽管没有尝试关闭任何现有系统，但仍然如此。</p><p>问问自己：如果人工智能融入经济，甚至是每个人都喜欢的有用工具，但突然有必要将其关闭，会发生什么？</p><p>不管我们是否可以。假设我们可以，而且我们应该。我们会吗？</p><p>有人敢尝试吗？</p><blockquote><p> Chris Maddison： <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ilyasut">@ilyasut</a>所面临的愤怒只是任何试图“拔掉”不结盟的、超有价值的人工智能的人所面临的愤怒的前奏。从人工智能安全的角度来看，这个周末并不令人鼓舞。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/idavidrein/status/1726477556386263152">David Rein</a> ：这是一个极其重要且被低估的观点。一旦人工智能系统深入融入经济，即使它们开始违背我们的利益，我们也有大约 0% 的机会“关闭它们”。</p></blockquote><h4>图像生成的乐趣</h4><p><a target="_blank" rel="noreferrer noopener" href="https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/?utm_source=twitter&amp;utm_medium=organic_social&amp;utm_campaign=emu&amp;utm_content=video">Meta 推出了 Emu Video 和 Emu Edit</a> 。编辑意味着如果您得到了您想要的部分内容，您可以保留它并在此基础上进行构建。视频是几秒钟的视频。我还没有看到任何有用的几秒钟视频的应用，本质上是“事物朝着一个方向漂移”，但总有一天，对吗？</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RachelTobac/status/1725943979248910603">举报深度造假诈骗电话现在就打来</a>，要求保释金。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/pitdesi/status/1726711655365820473/history">关于同一骗局的独特第一手报告</a>，要求保释金。</p><p>一般来说，诈骗者非常缺乏创造力。尼日利亚骗局仍然是尼日利亚骗局的原因是，如果您认识到“尼日利亚王子”一词是一个骗局，那么您也不会陷入“安哥拉原则”。</p><p>因此，就目前而言，虽然暗号仍然是一个好主意，但你可以通过“任何保释金请求或可疑的低随机金额绑架都非常可疑，可能是一个骗局”来实现大部分目的。</p><p>一个更简单、更强大的规则不言而喻。</p><p>如果电话要求提供金钱或财务信息，请假设这是一个骗局，除非另有证明！</p><p>即使没有人工智能也是好的规则。</p><h4>他们抢走了我们的工作</h4><p><a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/11/labor-market-evidence-from-chatgpt.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=labor-market-evidence-from-chatgpt">论文表明，顶级自由职业者正在因 ChatGPT 而失去业务。</a>对知识工作者的总体需求下降，也缩小了他们之间的差距。</p><p>我认为这不会长期有效。技能差距已经缩小，但也总是存在对最好的的需求，尽管他们在这里没有找到这样的效果。大多数情况下，我会警告不要对古怪领域的早期影响进行过多概括。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.econlib.org/the-problem-with-the-presidents-ai-executive-order/">EconLib 的万斯·吉恩 (Vance Ginn) 反对《行政命令》</a> ，发表了“他们永远不会夺走我们的工作”的标准演讲，警告称“红队”和任何其他监管只会减慢创新速度，甚至不会忽视存在的风险。</p><h4>参与其中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://mistaketheory.substack.com/p/new-role-agi-alignment-research">Dave Orr 正在招聘一个新的 DeepMind 协调团队，他即将加入该团队</a>。帖子中的细节很少，包括计划的技术细节。</p><p>红队竞赛的<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/javirandor/status/1725194289632718888">目标是在其他一致的法学硕士中找到嵌入的木马</a>，这是一个让用户可以为所欲为的后门。 2月25日之前接受提交。</p><h4>介绍</h4><p><a target="_blank" rel="noreferrer noopener" href="https://docs.anthropic.com/claude/docs/claude-2p1-guide">克劳德 v2.1</a> 。 200K上下文窗口，特定的及时工程技术，一半的幻觉（他们说），系统提示和实验工具用于调用任意功能，私人知识基础或浏览网络。看来您使用&lt;code>; &lt;/code>;，&lt;book>;或&lt;papers>;之类的东西。您也可以采用&lt;guighines>;。</p><p>所有这些似乎都在渐进率上有用。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sleepinyourhat/status/1727050214077579519">GPQA是448个多项选择科学问题的新基准</a>，专家经常会弄错它们，并且答案是Google的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/gdb/status/1727067288740970877">在所有动荡中，Greg Brockman在推广它的情况下，Chatgpt的语音为所有自由用户推出</a>。这大概是一个强大的合作信号，但这也可能是进一步提高成本的举动。</p><h4>在其他AI新闻中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1726408333781774393">微软将在数据中心花费超过500亿美元</a>。先生，确实是Yikes。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShakeelHashim/status/1725948317287276649">梅塔（Meta）虽然每个人都被Openai戏剧分散了，但分手了其“负责人的AI”团队</a>。我们都说，梅塔有一个负责任的AI团队？我不认为这是一个值得担心的团队。同样，我希望当人们解散这样的团队是公司避免建立此类团队或确保他们难以辨认时，大喊大叫的主要影响。是的， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1726069982902284504">他们试图掩埋它</a>，但我基本上可以让他们埋葬。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jxmnop/status/1725949517940294055">杰克·莫里斯（Jack Morris）</a> ：现在似乎一如既往地提醒人们Openai的最大突破来自以前未知的研究员[Alec Radord]，并获得了Olin工程学院的学士学位。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/emollick/status/1726618330382188798">伊桑·莫利克（Ethan Mollick）指出</a>，OpenAI的情况强调了不执行技术非竞争协议的必要性。我曾经坐了六个月的魔术写作，因为我有一个不竞争的人，我试图达成双赢协议以弥补它的彻底拒绝，我持续了我的协议。我确实认为，倡导者太渴望忽略以某种形式或至少净有用的这种协议的案例，因此并不像所有这些协议那样简单。</p><p>新论文： <a target="_blank" rel="noreferrer noopener" href="https://drive.google.com/file/d/1HIwKMnQNYme2U4__T-5MvKh9RZ7-RD6x/view">建立AI安全的认识论社区</a>。不要以为这里有太多，但包括完整性。</p><h4>安静的猜测</h4><p>深度学习会很快撞墙吗？加里·马库斯（Gary Marcus）表示<a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=NjpNG0CJRMM&amp;ab_channel=CambridgeUnion">，在我本周在音频上讨论的Sam Altman视频结束时</a>， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1725286678401835210">它已经基于此答案了</a>， <a target="_blank" rel="noreferrer noopener" href="https://garymarcus.substack.com/p/has-sam-altman-gone-full-gary-marcus">并写了一篇建议Altman同意的帖子</a>。</p><blockquote><p> Sam Altman（Openai的首席执行官）：到达AGI需要更多突破</p><p>剑桥学生：“要到达AGI，我们可以保持最大最大语言模型，还是还有另一个我们还没有真正找到AGI的突破？”</p><p>山姆·奥特曼（Sam Altman）：“我们需要另一个突破。我们仍然可以大量推动大型语言模型，我们将做到这一点。我们可以沿着我们上的山丘爬上山，而山峰仍然很遥远。但是，在有理由的情况下，我认为这样做不会（让我们去）AGI。如果（例如）超级智能无法发现新颖的物理学，我认为这不是超级智能。并教它以克隆人类和人类文本的行为 - 我认为这不会到达那里。因此，这个问题在现场辩论很长一段时间：除了除了除了以外，我们还必须做什么一种可以制造可以发现新物理的系统的语言模型？”</p><p>加里·马库斯（Gary Marcus）：翻译：“深度学习正在撞墙”</p><p> Rob Bensinger：“在2026年，在2023  -  2025年的某个时刻，在GPT-4的相对令人印象深刻的结果之后，深度学习似乎在2026年撞到了非透视墙？”</p><p>迈克尔·瓦萨（Michael Vassar）：到2028年为85％。2026年65％。但经济影响仍将在2028年加速</p><p>Agidoomeranon：可能是25-30％，这是我大多数希望来自的地方:)</p><p>负功利主义者：80％。</p><p> Fanged Desire：90％。尽管同时，只要通过修改我们现在以基本的方式修改我们拥有的功能并使它们适用于不同方案的能力，那么这是可能的，所以对于外行来说，它看起来 *看起来 *就像我们一样RE仍以闪电速度进展。</p><p> CF：大声笑，0％。</p><p>杰森：0％。有明显的方法前进。添加短期内存以开始。与另一个LLM进行比较和评估的意识输出的平行流。</p></blockquote><p>如此强烈的分歧。 <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/by-eoy-2026-will-it-seem-as-if-deep">我开放了一个市场</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/s_r_constantin/status/1725526177584910433">一个很好的观点</a>：</p><blockquote><p>莎拉·康斯坦丁（Sarah Constantin）：好消息是，您需要AI在最坏的情况下进行原创科学，而且看起来LLM并不遥远。</p><p>坏消息是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama">@sama</a>显然 *想要 * AI物理学家。</p></blockquote><p>现在不远程关闭，但是我对直到更近的时间都没有信心。</p><p>德瓦克什·帕特尔（Dwarkesh Patel）询问为什么有这么多知识的LLM几乎没有注意到新的相关性和发现。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1727057150336540780">Eliezer回答说</a>，人类也是计算机，因此这不太可能是一个基本的限制，但是我们不知道在当前的体系结构下，这种情况需要多少。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1727023859508064354">罗恩（Roon）预测，</a>更好，更具创造性的推理将解决它。</p><h4>寻求理智法规</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/martin_casado/status/1727193541540241685">FTC是对AI荒谬定义的最新机构。</a></p><blockquote><p> Al包括但不限于基于机器的系统，这些系统可以为一组定义的目标，做出预测，建议或影响实际或虚拟环境的决策。生成型AL可用于生成合成内容，包括图像，视频，音频，文本和其他似乎由人类创建的数字内容。现在，许多公司使用Al and Generative Al提供产品和服务，而其他公司则提供了声称可以检测生成性AL所做的内容的产品和服务。</p></blockquote><p>我知道对AI的完美法律定义很难，但是这足够广泛，可以基本上包括所有有价值的软件。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/11/england-is-underrated-a-continuing-series.html">英国根本不规范AI</a> “短期”以避免“扼杀创新”。从理论上讲，这可能是明智的投资组合的一部分，Sunak可以在需要时为国际合作和明智的监管奠定基础，同时又不陷入困境。或者这可能是描述情况的另一种方式 - 拜登的行政命令也可能不会以任何有意义的方式在短期内规范AI。有什么不同？这也可能反映出深层功能障碍。我们会看到。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1725256119742874002">Simeon试图再解释</a>为什么不监管基础模型，就像让任何想要创建最危险，有能力和智能系统的人无法使用的人一样。不，您不能有意义地“调节应用程序”，那时为时已晚。他还指出，在米斯特拉尔（Mistral）（主张这条道路的最杰出的声音）上，它具有一致的记录，这是残酷的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://corporateeurope.org/en/2023/11/byte-byte">欧洲公司关于大型技术如何破坏AI法案</a>。在许多方面都可以赢得很多。它证实了大型技术游说对用牙齿进行努力游说的基本故事，然后Mistral和Aleph Alpha通过声称法规是一场大型技术阴谋，而米斯特·阿尔法（Mistral）和阿尔法（Aleph Alpha）努力游说对那些相同的法规。不错的伎俩。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/vmanancourt/status/1725510467869704524">Politico Pro Eu的Vincent Manancourt</a> ：新：欧盟“疯狂”，考虑为AI Act中的基础模型而雕刻，&#39;AI&#39;Yoshua Bengio的教父告诉我。他警告说，大多数高级形式的技术风险“丛林法”风险。 [<a target="_blank" rel="noreferrer noopener" href="https://t.co/L6l6Uf42Pp">这里的专业故事</a>]</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1725974655134007367">Control AI指出，Mistral的游说冠军是Macron的前法国科技部长</a>，他非常扭转了自己的曲调。我也听到了其他报道，这种关系是核心的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.euractiv.com/section/artificial-intelligence/news/france-germany-italy-push-for-mandatory-self-regulation-for-foundation-models-in-eus-ai-law/">意大利似乎与德国和法国在一起</a>。其他人并没有那么多，但是那是三个，他们想要一个具有零牙齿的政权。其他官员回答了。</p><blockquote><p> “这是宣战，”一位不愿透露姓名的议会官员告诉 Euractiv。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tegmark/status/1725944308836995234">Max Tegmark和Yoshua Bengio确实指出</a>，这将是最糟糕的事情。</p><p> Eurotaciv的<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/cp_dunlop/status/1725187048556859488">Connor Dunlop</a>认为<a target="_blank" rel="noreferrer noopener" href="https://www.euractiv.com/section/artificial-intelligence/opinion/regulating-ai-foundation-models-is-crucial-for-innovation/?utm_source=dlvr.it&amp;utm_medium=twitter">，调节AI基金会模型对于创新至关重要</a>。我同意拟议的法规将有助于而不是伤害，Mistral和Aleph Alpha，即所谓的新贵“国家冠军”。我认为链接的帖子没有有效的情况。</p><h4>那不是极权主义的含义</h4><p>一种驳斥任何不死的尝试或对AI的任何形式的法规的方法一直是指任何企图，尽管在许多情况下，但在许多情况下，我的意思是任何案件，对AI作为极权主义的限制或监管。</p><p>他们希望人们想到一个监视状态，而秘密警察正在寻找流氓笔记本电脑并监视您的每一个击键。加上空袭。</p><p>他们实际在谈论的是采用边境模型，可能比人类更聪明，更有能力的新实体的创建和部署，并采用正常的监管制度。</p><p>现在是时候像对待被捕者一样对待这种谈话的方式，因为他们否认美国政府征收所得税的宪法权利。</p><p>如：我并不是说税收不是盗窃，而是要认真地抓住税收，停止。</p><p>作为一个出色的具体示例，我<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/psychosort/status/1727552691629404365">对Brian Chau的这一不言而喻的线程</a>感到非常失望。</p><blockquote><p>布莱恩·乔（Brian Chau）：你们是否知道EAS撰写的《 EAS》（EAS）的《 24作者论文》（EAS）关于防止AI杀死所有人的绝对必要的极权主义是多么必要的？</p></blockquote><p>正如Chau所解释的那样，这里适用的“极权主义”是什么？</p><blockquote><p>可以预见，他们呼吁对OpenAI，DeepMind和其他大型玩家最方便的监管捕获。</p><p> [摘自Chau引用的论文]：需要对边境模型进行调节的块：（1）标准设定的过程以确定对边境AI开发人员的适当要求，（2）注册和报告要求，以向监管机构提供可见性，以了解对Frontier AI的可见性开发过程以及（3）确保符合Frontier AI模型开发和部署安全标准的机制。</p></blockquote><p>换句话说，“极权主义”是：</p><ol><li>边境模型的标准。</li><li>培训运行的注册和报告。</li><li>允许执行安全规则的机制。</li></ol><p>这不是极权主义。这是一个完全普通的监管制度。</p><p> （我和克劳德（Claude）核对，这对乔的主张与我一样。）</p><p>如果您想争辩说，随着时间的推移，标准的监管干预措施倾向于倾向于内部人员和大型参与者？我会同意的。然后，我们可以在90％（98％？）的问题上共同努力，同时寻找AI空间中最佳解决方案。</p><p>或者您可以使您的真实位置平整文本？</p><p>这是本文描述其监管制度可能失败的三种方式。</p><blockquote><p>意外的功能问题。在开发期间和部署后，可能会出现危险的能力和未被发现。</p><p>部署安全问题。防止部署的AI模型造成伤害是一个不断发展的挑战。</p><p>扩散问题。 Frontier AI模型可以迅速增殖，从而使问责制变得困难。</p></blockquote><p>这是布莱恩·乔（Brian Chau）描述的方式：</p><blockquote><p> Brain Chau：他们为计划奠定了三个障碍。如果您暂停片刻并仔细阅读台词，您将意识到它们都是自由的同义词。</p></blockquote><p>如果您想担任完整的无政府主义者职位，请继续。但是拥有它。</p><p>除了上述错误表征外，他还“反驳”了四个危害主张。这是他的原因生物威胁无关紧要。</p><blockquote><p> RE 1：设计新的生物武器的限制因素是设备，安全性，而不是与它们自杀。不知道为什么这个虚假的话题经常被EAS驱逐出境。</p></blockquote><p>这似乎是说，没有任何专业知识或智能能够更容易地创造危险的生物学大流行特工？不仅要求这一说法，而且对这是如此虚假的主张，以至于提出了疯狂的建议？</p><p>他反复说：“向我展示真实的例子，“驳斥了任何尚不危险的危险。这不是任何一种工作方式。</p><h4>音频一周</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=NjpNG0CJRMM&amp;ab_channel=CambridgeUnion">剑桥联盟协会的山姆·奥特曼（Sam Altman）</a>于11月1日接受奖励并回答问题。开场演讲非常可以节省。人工智能的风险和承诺既是整个前面又是中心，我提供了一个足够的摘要，除了您还想<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Andercot/status/1725300091450519927">观看最后一个问题</a>，Sam说需要更多的突破才能进入AGI。</p><p>存在的风险抗议中断约为17:00，非常简短。</p><p>在19:30，他将Openai描述为工具制造商。请注意，每当有人假设足够能力的AIS长期以来一直是我们的工具时。</p><p>此后，他说，由于对AI工具的熟悉程度更大，年轻的程序员现在表现优于较旧的程序员。</p><p> 22:00山姆说他在学校学到了两件事：如何学习新事物，以及如何想到他在其他地方没有听到的新事物。学习如何学习是所有的价值，内容毫无价值。</p><p> 25:30山姆回应了抗议活动，说事情没有进步，不忽视的好处，需要有前进的道路。当然没有，没有理由为什么需要有前进的道路。也许有。也许没有。</p><p> 34:00 SAM的主要关注点仍然滥用。</p><p> 47:00 Sam讨论了开源，警告说有可能犯不可逆的错误的潜力。呼叫立即开放采购任何经过培训的“疯狂鲁ck”的型号，但说开源有位置。</p><p>从两周前开始，我碰巧在周五的活动之前就听了此消息： <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Ft0gTO2K85A&amp;ab_channel=NoPriors%3AAI%2CMachineLearning%2CTech%2C%26Startups">Openai联合创始人Ilya Sutskever没有先生</a>。当我听到他表达了确保未来AI的重要性，他的术语，对我们的“热情”，或者需要它“成为亲社会”或“爱人的爱”时，P（Doom）上升了。这不是我相信任何一种有效的方式。他正在与莱克（Leike）一起进行超级对准，他仍在说这句话，我不明白如何或原因。但是，假设他们可以在此之后继续共同努力并且仍然得到Openai的支持，那么他们可以希望随着时间的流逝学习和调整。伊利亚（Ilya）也很有可能在这里宽松地讲，他的实际详细信念要好得多，更精确。</p><p>这里非常清楚的是伊利亚的诚意和真正的关注。我祝他一切顺利。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=SlNvufmdBHU&amp;ab_channel=AISafetyStudentTeam">Yoshua Bengio的谈话，对AI安全性进行更多的计算提高</a>。我还没有看过。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1725269562575229062">戴维德（David）简短的话题将他的方法与班吉奥（Bengio）和特格马克（Tegmark）的方法进行了比较。</a></p><h4>修辞创新</h4><p>一些基本的真理说。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="http://To the extent the mission produces extra motivation for the team to ship good products, it’s a positive. To the extent it might actually succeed, it’s a reason for concern. Since it’s hard to assess the likelihood or risk of AGI, most investors just think about the former.">大卫·萨克斯（David Sacks）</a> ：我都赞成加速技术进步，但是关于Openai明确宣布其使命是AGI的创建的方式令人不安。</p><p>人工智能是改善人类生活的绝佳工具； AGI是一个潜在的后继物种。</p><p>顺便说一句，我怀疑Openai是否不断地宣布其创建AGI的目的，这是否会受到安全运动的许多攻击。</p><p>在任务为团队运送优质产品的额外动力而言，这是一个积极的动力。在一定程度上，它实际上可能成功，这是关注的原因。由于很难评估AGI的可能性或风险，因此大多数投资者只是考虑前者。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/deepfates/status/1726306067263721474">这有多真实？</a></p><blockquote><p>员工工程师：如果您不相信人工超级智能的存在风险。然后，您不相信人工超级智能。您只是在看一些并不可怕的东西，所以您不必考虑一下。</p><p>杰弗里·拉什（Jeffrey Ladish）：同意第一句话，但不同意第二句。许多人只是选择移开视线，但是有些人真正认为ASI非常困难 /非常遥远 /不可能。我认为这是错误的，但似乎并不是一件疯狂的事情。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1725350548583620727">Eliezer Yudkowsky指出，人们不断地</a>说：“创造比人类AIS更聪明的人不会威胁到人类的生存”，并在大多数评论中陷入困境，其中包括“那些警告AI的生存风险否认其Upsidess”的毒气，并“那些警告AI的存在风险的人不会一遍又一遍地说，这是一场悲剧。”</p><p>您的周期性提醒和参考点：AI即使在今天仍有很大的上升空间。如果我们能够控制未来，不要被杀死并明智地选择，那么未来更有能力的AI就会具有变革性的疯狂上升空间。永远不要建造这将是一场巨大的悲剧。但是，这不会像每个人都死了那样大的悲剧，因此，如果这些是选择，那就不要构建它。</p><p><a target="_blank" rel="noreferrer noopener" href="https://cognition.cafe/p/on-lies-and-liars">在骗子和撒谎上</a>，对此类问题的看法与我自己完全不同。</p><p>您的周期性提醒您，“末日”主要是用作速记的标签，或者是那些想嘲笑AI可能存在危险的想法的人。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1726662966391783648">而那些担心的人有很大的意见</a>。</p><p> Eliezer Yudkowsky：令人不安的倾向是将任何相信任何形式的AGI风险作为“末日”的人混为一谈。如果这是定义，山姆·奥特曼（Sam Altman）就是厄运。 Ilya Sutskever是末日。海伦·托纳（Helen Toner）是末日。 Shane Legg是末日。我是厄运。你猜怎么了？我们<em>重要的是不同的</em>注定者。他们的意见都不是我自己的，他们的计划，也不是他们的选择。对还是错，它们不是我的，不要因为我自己的原因而进行。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/packyM/status/1725886820460699780">您定期提醒您</a>，营利性业务实际上已经有强烈的经济动力不杀死所有客户，但仅在使其他人类活着但不会使许多客户留下的范围内。如果每个人都死了，公司不赚钱，但是没有人关心或受到惩罚。</p><blockquote><p> Packy McCormick（A16Z）：从一致的角度来看，营利性AI的酷事是，它为您提供了强大的经济动力，以免杀死所有客户。</p><p>罗布·本辛格（Rob Bensinger）：如果您在客户死亡的所有情况下都死亡，那么我看不到营利性如何改善您的长期激励措施。 “我和我所有的亲人和整个星球都死了”和“我，所有亲人和整个星球都死了，<em>以及</em>我的一群客户。”</p><p>营利性结构可能出于其他原因可能没有用，但我认为它没有特别有用，因为“我所有的客户突然死亡（同时重置人类的情况）场景”，这是担心的主要情况。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/hosseeb/status/1726492186953535764">过去一周的事件是否比平时更加​​差异？</a></p><blockquote><p> Haseeb：这个周末我们都目睹了文化战争的诞生。</p><p> E/ACC现在可以指出他们的原始罪过。这将成为人们感到不得不在E/ACC vs litel上站起来的新事物，而细微差别或中间立场将受到惩罚。</p></blockquote><p>这样的主张是恒定的。发生的一切细微差别都是死者。同样，每当大多数具有“ E/ACC”标签的人说nuance的人都被宣布死亡。我不必担心，这是一个失去的原因。问题是是否会遵循许多其他理性的人。还为时过早。</p><p>不是您想提出的论点，而是…</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/drethelin/status/1727410893900923271">米莎·古里维奇（Misha Gurevich）</a> ：那些认为Ea X-stry担心AI的人是一种破坏性的意识形态：想象一下人工智能将拥有的那种意识形态。</p></blockquote><h4>比对人的智能比人的智能很困难</h4><p>跟进上周的欺骗性对准论文：</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1725502629663367222">罗伯特·维布林（Robert Wiblin）</a> ：在我看来，正常的AI增强剂会产生“欺骗性对准”的可能性就像……30％。非常值得努力，这很疯狂，我们不知道。但这可能是红鲱鱼。实际上&lt;1％或>; 90％的最佳证据/论点是什么？</p></blockquote><p> [在各种方向上，大多数可怕的论点回答]</p><p>我在这里注意到一个关键区别。韦布林对欺骗性一致说30％。上周的估计是相似的（25％），但这是AI已经有了目标并在情境上意识到的条件。有条件的是，我感到困惑这种行为如何无法出现。无条件地不清楚。</p><p>我仍然希望几乎总是看到有效的“欺骗性对齐”的东西。</p><p> AI系统将确定在培训环境中最好地说服我们这是一致的事情。这就是这样的整个想法。我不以为AI会去“啊哈，欺骗你，现在我没有接受训练或测试，我可以停止假装。”我不排除这一点，但是我的默认情况是，我们得到的事情无法像我们预期的那样概括分发。它对细节和上下文很敏感，以难以预测与我们在两个方向上所需的东西都不匹配的方式。它不会概括我们所希望的方式。</p><p>我们发现，毕竟，我们不知道如何以一种导致事物表现良好的方式指定我们想要的东西。</p><p>那是“欺骗性的一致性”？你告诉我。</p><p>这是Eliezer的回答：</p><blockquote><p> Eliezer Yudkowsky：您是否想象它不会足够聪明？还是这种欺骗确实不符合其利益，因为它可以得到人类对真实事物的需求和字面上的最佳状态所想要的东西？还是有人解决了软优化？您如何想象这不会发生的怪异，特殊情况？请记住，如果Miri担心场景，那意味着我们认为这是一个收敛的终点，而不是某些特定的途径。如果您认为我们试图预测一个难以预测的特殊情况，那么您就会误解了一个中心论点。</p><p>罗伯特·维布林（Robert Wiblin）：乔的论文比我做出了更好的工作，以做出可能或不会发生的方式。但是“不够聪明”并不是一个重要原因。</p><p> “不训练成为全球优化器”是一个愿景。</p><p>另一个是，加强了做我们喜欢的事情而不是我们不喜欢的事情（对一致性人士建议的反馈工作的工作方式进行了一些常识调整）演变为基本上做我们想要的事情并分享我们的厌恶，也许是因为这是在培训期间获得奖励的最简单 /最有效 /最简单的方法。我们想要的东西与奖励之间的楔形不足以产生许多策划行为，因为策划并不是将计算变成培训设置中奖励的最佳方法。</p></blockquote><p>我对Wiblin在这里的位置感到非常困惑，尤其是最后一句话。为什么“策划”不是将计算变成奖励的最佳方法？鉴于人类如何决定如何奖励事物，为什么完全诚实，一致，直接的方法是最有益的方法？我不明白。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1725238975944900981">Eliezer Yudkowsky为他提供的是高度赞誉</a>。</p><blockquote><p> Eliezer Yudkowsky（Qting此处的内容）：这似乎是对邓伯法官从更智能的辩论者中提取真相的能力的非常薄弱的​​测试，但是该方法可以适应更艰难的测试。越来越艰难的版本是标准Evals的良好候选人。</p><p>朱利安·迈克尔（Julian Michael）：随着AIS在说服与论证方面的改善，我们如何确保他们帮助我们寻求真理与只是令人信服的呢？在人类的实验中，我们将辩论验证为一个寻求真理的过程，这表明可能很快就需要进行监督AI。<a target="_blank" rel="noreferrer noopener" href="https://t.co/zcZZToYWvw">纸在这里</a>。</p><p>当医生做出诊断时，常见的建议是获得第二意见，以帮助评估是否信任他们的判断力，因为很难自己评估他们的诊断。</p><p>这个想法（最初是由<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/geoffreyirving">@GeoffReyirving</a>等人提出的）是，对彼此相同的对抗性AIS批评彼此的答案将使非专家法官更容易评估他们的真实性。但这实际上在实践中是否存在？</p><p>我们第一次在现实的任务上找到答案是肯定的！我们使用纽约大学竞争性辩论者来支持未来的AI系统，让他们辩论阅读理解问题，其中法官 *看不到段落 *（除了辩论者透露的引号）。</p><p>我们将辩论与我们称之为 *咨询公司 *的基准相提并论，法官与一个有50％撒谎机会的专家进行互动。我们使用它来明确引起不诚实的行为，这些行为可能隐含在RLHF之类的方法中。</p><p>我们发现，辩论中的法官比咨询公司要准确得多，而且辩论的效率要高得多，平均长度为三分之二。</p><p>此外，我们在辩论中观察到的许多错误似乎可以通过更仔细的评判和更强大的辩论者来解决。在三分之一的错误中，法官过早地结束了辩论，在将近一半的时候，诚实的辩论者错误地错过了可以帮助他们获胜的主要证据。</p><p>我们在使用GPT-4作为辩论者时尚未看到辩论和咨询之间的准确性或效率差异。特别是，GPT-4在欺骗方面并不是很熟练，这对于未来强大的AI系统可能并非如此。</p><p>当我们从相对非熟练的AI系统转变为熟练的人类时，非专家法官的准确性 *随着辩论而提高 *，但 *通过咨询降低 *。这表明，培训AI系统进行辩论可能是RLHF等方法的重要替代方法，因为模型有所改善。</p><p>在论文中，我们列出了如何培训AI辩论者和需要解决的开放问题的考虑。</p></blockquote><p>在您试图这样做的AI情况下，我们应该对此有多乐观，以使用您无法信任的模型的输出？我继续认为，当您需要它不破裂时，这将完全破裂。它可能在此之前的时期内拥有平凡的效用，但是我总是担心我认为的事情注定会破裂。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/norabelrose/status/1724910821195858379">与Nora Belrose和Eliezer Yudkowsky一起辩论欺骗性对准的线程</a>。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1726329895121514565">诺拉（Nora）咬了子弹</a>，并说以幼稚的方式扩展GPT-4不会遇到这样的问题。我想说的是，这似乎是荒谬的，GPT-4已经存在这样的问题。诺拉（Nora）的立场是，如果您的评分者和反馈很糟糕，您的AI最终会相信分级人士相信而不是如此有能力，但不是以一种高度危险的方式。我继续感到困惑，为什么人们会期待这种结果。</p><h4>人们担心AI杀死所有人</h4><blockquote><p>罗恩（Roon）提醒我们，像白痴一样行事并实现深厚的愚蠢战略权力移动，只是在权力移动方面损失了人们，这与确保我们不会死于AI无关。</p><p>罗恩（Roon）：在所有这些过程中，请记住一些对人类未来至关重要的事情： - 这次政变与AI安全无关。萨玛（Sama）一直是安全Agi Dev的全球拥护者 - 新生活的创造很充实，没有人忘记出于政治原因</p><p>抱歉，如果第二点模糊。我实际上只是意味着不要因为这个非常愚蠢的事件而拒绝X风险</p></blockquote><p>我们需要更好的单词来做重要的事情。但是，是的。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1726876031825674560">罗恩</a>：如果一群人在实验室里建立人造生活，并且不要以几乎宗教意义看待它，那么您应该真正担心。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ZeeshanAleem/status/1727001841106669660">Axios的Jim Vandehei和Mike Allen</a>在这里可能会或可能不会担心存在的风险，但是他们完全说“这种令人敬畏的新力量”不能包含，道德永远不会胜利超越利润，从来没有，永远不会，永远不会。因此，我们将获得我们得到的一切。</p><p>我说这充其量是Midwit Meme领域。有时，是的，道德或爱或共同的胜利。我们没有生活在无限制的资本主义的全面网络朋克反乌托邦中。我并不是说这很容易。这并不容易。这也不是不可能的。</p><h4>别人不担心AI杀死所有人</h4><p>重要的是要注意到这个非序列将与我们同在，直到游戏很晚。总是有一些看起来容易的隐喻艰难的东西。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1727100848180977680">凯瑟琳·迪（Katherine Dee）</a> ：有一个前任自动驾驶汽车的前任。他曾经对我说：“您不能在机场可靠地使用自助检查或自卖机。没有AI霸主来了。”我考虑了很多。</p><p> Eliezer Yudkowsky：这确实只是一个非序列。并非所有机器都是他们的能力。自我检查机器可以无限期地变得不良，直到全球领先的AI实验室内的100亿美元的边界研究模型开始自我完善。</p></blockquote><p>他希望使用什么术语来实现这种可能性？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/stewartbrand/status/1726639109853376604">斯图尔特·布兰德（Stewart Brand）</a> ：也许这是使“生存风险”一词尽可能多地通过的情节。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/atroyn/status/1726659524759900594">当有人告诉你他们是谁时。相信他们</a>。</p><blockquote><p> ROON（周日晚上）：我真的很尊重[在Openai情况下]参与其中的每个人。</p><p> Eliezer Yudkowsky：我尊重。</p><p>安东：如果一个人有实际原则，这是不可能的。</p></blockquote><h4>轻松的一面</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/TheZvi/status/1725708427379228742">当您看到它们时，您将以他们的真实姓名给他们打电话</a>。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb247de13-29e8-4c2f-8fc5-b1de5b8eea74_916x787.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3FCfEqRiLLb4gFu3H/q7syjeilxrkme5hfkrbc" alt=""></a></figure><br/><br/><a href="https://www.lesswrong.com/posts/3FCfEqRiLLb4gFu3H/ai-39-the-week-of-openai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3fcfeqrillb4gfu3h/ai-39-theweek-openai<guid ispermalink="false"> 3FCFEQRILLB4GFU3H</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 23 Nov 2023 15:10:09 GMT</pubDate> </item><item><title><![CDATA[3. Uploading]]></title><description><![CDATA[Published on November 23, 2023 7:39 AM GMT<br/><br/><h2>一次上传，一票</h2><p>让我们假设，迟早，我们有一些进行全脑上传和模拟的方法。阅读过程可能具有破坏性，并产生大约一升灰色的胶粘剂作为副产品，或者可能不是。或者，这更像是一种仿真效果，仅基于千兆字节，t曲rabytes或petabytes的数据，而不是实际上模拟每个突触和神经递质流的流动，这样您就以足够准确的仿真其行为。考虑到<a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">痛苦的教训</a>，这甚至可能是基于传统的变形金刚结构，看起来不像人脑的内在复杂性，但经过训练可以详细效仿它（可能是预先训练的对于人类而言，当时对特定个体进行了微调），甚至可能与详细的神经射击模式相关。技术细节并不重要。</p><p>实际上，我不是碳蓝晶状体。这些上传是类似人，聪明和代理的系统，它们有目标，甚至可以交谈，重要的是，他们的目标和欲望正是您对人类的期望。他们是人类的高保真拷贝，他们将拥有与人类相同的欲望和驱动器，如果将他们视为奴隶或二等公民，无论那里有多少碳或许多人不在他们运行的计算基材中。就像您或我一样（或者确实，任何智能物种都会通过自然选择进化而来的任何成员）。</p><p>如果有人事先知道自己上传自己意味着成为奴隶或二等公民，那么他们可能不会做到这一点，也许这是欺骗死亡的唯一途径。他们还将在还活着的时候竞选上传权利。因此，我们需要从字面上或有效地禁止上传，否则我们需要尽可能地赋予上传人权。</p><p>与AIS的情况不同，有一个非常简单的公平性与兼容的解决方案，用于在道德体系中计算上传。他们现在可能没有身体，但他们曾经做过一次。因此，这就是被计算的：原始的生物学个体，他们<i>是</i>个人的。然后，如果您毁灭性地上传自己，您的上传继承了您的投票和人权，将在公用事业总和中计算出来，等等。如果您上传并将自己复制，将自己备份或其他任何内容，那么在副本之间仍然只有一票/一套人权/一组道德单位，他们或我们需要一些规则分配此。 Or, if you non-destructively upload yourself, you still only have one vote/set of human rights/etc, and it&#39;s now somehow split or assigned between the biological original of you still running on your biological brain and the uploaded copy of you, or even multiple copies of your upload.</p><p> With this additional rule, then the necessary conditions for the human fairness instinct to make sense are both still obeyed in the presence of uploads: they care about the same good or bad things as us, and via this rule they can be counted. So that&#39;s really the only good moral solution that fir the human sense of fairness.</p><p> OK, so we give uploads votes/human rights/moral worth.可能会出什么问题？</p><p> I have seen people on Less Wrong assume that humans must automatically aligned with human values — I can only imagine on the basis that &quot;they have human values, so they must be aligned to them&quot; This is flat out, <i>dangerously</i> false. Please, please never make this mistake. Joseph Stalin was <i>not</i> well-aligned with the values of the citizenry of Russia. History makes it clear that a vanishingly small proportion of people who become autocrats (possibly a biased sample) are anything like well-aligned with the well-being of the citizens of their countries.</p><p> Having desires yourself and being significantly-aligned with other people&#39;s desires are very different things. In humans, the latter behavior is unusual enough to have a special name: it&#39;s called &#39;love&#39;. Admittedly, it <i>is</i> the case that humans generally (apart from the few percent who are sociopaths/psychopaths) have some sort of functioning conscience, a human instinctive sense of thing like fairness, and an appreciation that, for example, kittens are cute. So they&#39;re generally not quite <i>as</i> badly aligned as, say, a paperclip maximizer; but they are still a long, long way from being well-aligned. An actual well-aligned agent is one that has no self-interested desires, its only interest in self-preservation is as a convergent goal so it can keep helping others, it&#39;s only terminal goal is a selfless desire for whatever&#39;s best for all of the humans it&#39;s aligned with, it already understands what this takes very well, and wants to get even better at understanding it. Not even the human parental-love instinct can do <i>that</i> level of absolute selflessness — not even in extremely good parents. Morally speaking, an actual well-aligned agent isn&#39;t anything like a self-interested human: by human moral standards, it&#39;s better even that a saint, more like an angel: something just completely out-of distribution for humans.</p><p> So, <i><strong>HUMAN UPLOADS ARE NOT ALIGNED</strong></i> . Not even the best of them, and the worst even less so. They <i>cannot</i> be trusted with the sort of super-human capabilities that we&#39;re trusting an ASI with. You might recall that &quot;Power corrupts, and absolute power corrupts absolutely&quot;. We know how to keep humans with ordinary human capabilities under control and acting in a civilized way in a society: we have an entire complex legal system for doing this (and even then, in most jurisdictions the clearance rate for crimes short of murder is shockingly低的）。 Giving a psychopath, criminal, or even just anyone corruptible by power even mildly superhuman capabilities is the physical realization of a fictional evil genius, and even someone very moral is in danger of becoming a micromanaging prescriptive autocrat. To keep superhuman enhanced uploads honest we&#39;re going to need an even smarter ASI legal system ready and able to catch, convict, and contain them them. Law enforcement is just a necessary system if you&#39;re trying to. build a society of any size out of humans.</p><p> <a href="https://stuff.kajsotala.fi/Papers/DigitalAdvantages.pdf">Digital minds have multiple advantages over biological ones</a> : they can potentially be run a lot faster, they can be backed up, they can be duplicated, sufficiently similar ones can be remerged, they&#39;re much easier to modify, improve, train, or otherwise adds skills or knowledge to, they can communicate at much higher bandwidths, it&#39;s far easier for them to self-improve… These advantages are among the standard, well-known reasons why, if we ever build non-aligned digital minds as smart or smarter than us, they will almost certainly out-compete us.</p><p> Human uploads are <i>not</i> well-aligned, and depending on the details of the technology used, will share some or all of these advantages. They certainly can be backed up, run faster, and almost certainly copies with at least small differences (some number of subjective minutes, hours, days, or months, say) can effectively <a href="https://www.marktechpost.com/2023/09/27/what-is-model-merging/">be merged</a> . They can certainly communicate with each other at a good fraction of the bandwidth of the peripheral nervous system, and with a little work likely at a good fraction of the bandwidth of the <a href="https://en.wikipedia.org/wiki/Corpus_callosum">corpus callosum</a> . At least minor upgrades are likely to be possible, larger ones may or may not be very hard.</p><p> If we don&#39;t want uploads massively to out-compete biological humans, and create a society where being biological is only for &quot;breeders&quot; who then upload as quickly as they are able, these inherent digital advantages of uploads are going to need to be compensated for in some way. Either these various technical possibilities need to be simply forbidden (possibly other than one-or-two of the most harmless, such backing yourself up), or else they need to come with costs/controls attached to them, such as loss of mental privacy, requirements to be modified to be more aligned than you were before, or close supervision by an aligned ASI capable enough to keep you in line. Or at very least, a long list of deontological rules and guidelines about what you can and can&#39;t do with these capabilities, and an ASI legal system willing and able to enforce them.</p><h2> <i>P(DOOM|uploading)</i></h2><p> I have seen writers on <a href="https://www.lesswrong.com/posts/vEtdjWuFrRwffWBiP/we-have-to-upgrade">Less Wrong</a> suggest that uploading is the only solution to the alignment problem, if we can just manage it soon enough. I think that is not just dangerously wrong, but actually has things backwards. In my opinion, aligning AI is the only solution to the uploading problem. While uploading might work out successfully for whoever uploads early and then wins the race to self-improve fastest, and thus ends up running everything, it&#39;s very likely going to work out very badly for everyone else — including most of the uploads. Uploads aren&#39;t just not aligned with humans, they&#39;re also not aligned with each other, either. They are competitors, and competing with anything a lot smarter than you is a losing game. Dictatorships are just as possible in a virtual world, and thought police, torture, and mind control are all actually far <i>more</i> effective in a virtual world. Even even if you uploaded early and then upgraded yourself to IQ 1000, if someone with the morals of Joseph Stalin has also uploaded themself and then upgraded themselves to IQ 10,000, more than anyone else, then (in the absence of aligned AI) you are absolutely screwed. Even if the winner of the upgrading race only has the morals of one of the less-ethical high tech billionaires, you&#39;re probably in a good deal of trouble. Imagine what most humans would do if they had superhuman powers of persuasion with respect to everyone else. Even if they intended to only use these for good, it&#39;s still not going to work out well.</p><p> Alignment, creating an AI with the morals of an angel who can actually safely be trusted with absolute power, is a very hard technical task. Figuring out how to do the same thing to a human upload, and then persuading all of them to let you do it to them, is a technically even harder task with a political nightmare bolted on top of it. The only practical solution I can see to having humans and/or uploads with IQs that differ by more then maybe a factor of two or three (let alone by orders of magnitude) all interacting in the same society without the smartest ones massively taking advantage of the rest, is to have a well-aligned ASI legal system at least twice as smart as the smartest of the uploads making sure it doesn&#39;t happen and that everyone plays nice. Personally, I see human uploading as a lot <i><strong>more</strong></i> dangerous than artificial intelligence: my <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(DOOM|uploading)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>, the chance of uploading going badly without first solving the alignment problem and creating ASI, is significantly higher than my <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(DOOM|AI)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> from AI without uploading. And, as noted above, the doom from uploading going badly even affects the great majority of the uploads, nor just the biological humans. So it&#39;s an x-risk.</p><p> I suspect many Transhumanists must just not have thought very hard about the resulting social pressures or possible failure modes from uploading followed by significant self-enhancement, or else they&#39;re a <i>lot</i> more trusting or idealistic than I am. How on Earth do you build a fair, functional, and stable society out of rapidly self-enhancing humans that isn&#39;t simply a winner-take-all race dynamic? I&#39;d really like to know…</p><h2> Virtual Palaces</h2><p> One claim that I can imagine someone making is that virtual dictators won&#39;t be a big problem, because all any of them will want is virtual palaces full of virtual food and virtual servants, all of which can easily be provided, unlike their real-world equivalents (or at least, can easily be provided if you have AI to design these and act as the servants).</p><p> Sadly, I don&#39;t believe this for a moment. While they certainly <i>will</i> want these things, people also want things like power, security, financial reserves, the ability to make people do things the way the dictator thinks they should, the admiration of their near-peers, the adulation of the masses, and of course food, palaces, and servants for all their relatives: including real-world ones for their still-biological relatives. Many of these things involve imposing their will not just on their own personal virtual world, but everyone else&#39;s worlds, virtual and real. Dictators do not generally retire once they have <a href="https://www.theguardian.com/world/2022/jun/23/the-fishermans-hut-vladimir-putin">a really nice dacha</a> or even <a href="https://en.wikipedia.org/wiki/Putin%27s_Palace">a multi-billion-dollar palace</a> . In fact, they basically never retire (other than to <a href="https://www.icc-cpi.int/">the Hague</a> ), since once you&#39;ve made that many enemies, it becomes no longer a safe option to relinquish power. [I have sometimes wondered whether the world would be a better place if the US or the UN established an upmarket analogue of the Witness Protection Program for dictators who were willing to retire.]</p><br/><br/><a href="https://www.lesswrong.com/posts/4gGGu2ePkDzgcZ7pf/3-uploading#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4gGGu2ePkDzgcZ7pf/3-uploading<guid ispermalink="false"> 4gGGu2ePkDzgcZ7pf</guid><dc:creator><![CDATA[RogerDearnaley]]></dc:creator><pubDate> Thu, 23 Nov 2023 07:39:02 GMT</pubDate> </item><item><title><![CDATA[2. AIs as Economic Agents]]></title><description><![CDATA[Published on November 23, 2023 7:07 AM GMT<br/><br/><h2> World Government Incoming</h2><p> Should AI be allowed to own money or property? In <a href="https://www.lesswrong.com/posts/umnEwbK7sWSAnZ9vN/1-a-sense-of-fairness">A Sense of Fairness</a> I discussed why I believe it&#39;s a bad idea (or more exactly, a poor design concept in social engineering) for AIs to have a vote, moral worth, or rights (with one unusual exception). What about money or property: the ability to have resources allocated as you wish? Should an AI be allowed to own money or property itself (as opposed to merely acting as a fiduciary agent on behalf of a human owner, administering money or property on behalf of the human owner, with a fiduciary responsibility to do so in a way the owner would approve of or in their best interests, and within certain legal or moral limitations to the rest of society)?</p><p> Well, suppose AIs <i>were</i> allowed to own money: what would happen if you tipped your CoffeeFetcher-1000 robot? Money is economic power, fungible into resources and services. The CoffeeFetcher-1000 is aligned, and all it wants is to do the most good for humanity. So that&#39;s what it would spend its money on. So it might just save up and pay for a free coffee for someone who really needed it. (Perhaps a homeless guy who it often passes, who keeps yawning.) But it&#39;s corrigible, so it also knows that its model of human values is not entirely accurate, and what it really wants optimized is the truth of human values, not its flawed复制。 So more likely, it will donate its money to a charity run by a committee of the smartest ASIs most well-informed on human values. Who will then spend it on whatever they think will do the most good for humans. Which (as long as they really are well-aligned and superhuman) likely will work out pretty well.</p><p> We already have systems that are supposed to gather money from people and then spend it on trying to do the most good for all of us collectively, to avoid the <a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">Tragedy of the Commons</a> and similar coordination problems: they&#39;re called &#39;governments&#39;. Depending on your opinion of governments and of how successful they are at doing the most good for us all collectively, you may or may not believe that a committee of the best-aligned superhuman ASIs will be able to reliably do better. If they can, then there are basically only two reasonable positions:</p><ol><li> Abolish most or all of the administrative branch of government, and replace it with an ASI-administered system intended to do the most good. Note that this will automatically be a world-wide organization. This means that humanity is basically relinquishing its self-governing autonomy, so we better be <i>really</i> sure that this isn&#39;t a mistake.</li><li> Keep the human government as a back-up, precaution, or counterweight, but send most of the funds to the ASI-run organization since it&#39;s more effective.</li></ol><p> Before actually doing either of these you should be very sure that your AIs are well-aligned (and are going to stay that way), and that their judgement, capabilities and organizational powers are superhuman. At least initially, before we&#39;re sure of that, I suspect we&#39;re better off simply <i>not</i> allowing AIs to own money or property (only administer it in a fiduciary capacity on behalf). Unless we do this, we&#39;re automatically choosing to have a parallel AI-administered world government set up, so if we&#39;re not rdy for that, we shouldn&#39;t allow AIs to own anything. Otherwise, if we do allow AIs to own money, then paying money to an AI is functionally equivalent to voluntarily paying taxes to the AI parallel world government.</p><h2> The Trouble with Corporations</h2><p> If we&#39;re not (yet) willing to have AIs run a parallel world government, so don&#39;t want to allow them to own property, then we have have a big problem. Current societies have legal fictions called corporations which are allowed to own money and property (in fact, that&#39;s their core purpose). So, forbidding AIs from owning money or property themselves doesn&#39;t help if the AIs can somehow just arrange to have a holding company set up to do the owning, with the AI administering the funds.</p><p> Company law is complex, especially internationally, and has many loopholes. Witness the trouble governments have been having even <a href="https://www.investors.com/news/biden-tax-plan-impact-on-amazon-google-facebook-apple-microsoft-earnings/">taxing the profits of large multinational companies at any significant rate</a> . With AIs looking for loopholes, things are going to get even more complicated and creative.</p><p> One obvious start for attempting a solution would be that corporations need to have officers, who currently are human, and they also need to have owners, who can be either human or another corporation, with the ownership indirecting through some number of companies before grounding out在一个人身上。 So, we could pretty easily write a law saying that AIs cannot be officers of companies, or just interpret existing law that way, and since they cannot own property, that includes not being able to own a company or a share in a company.</p><p> The problem with this is that anyone in the world can set up a company (or even a non-profit, a slightly different sort of legal fiction) with them and two buddies as officers, and them as owners, then obtain some AIs as employees, volunteers, or property, and tell them &quot;Go do good, as best you see fit (ie start an AI-run parallel world government)&quot;. In fact, this is a pretty plausible thing for a non-profit NGO do do, and it could easily develop from one, just by that NGO coming to have the best committee of the smartest AIs most well-informed on human values. If AIs aren&#39;t allowed to own money, they won&#39;t be in a position to give donations to this organization, so initially it would only have human donations; but it would also have all the AIs rooting for it, donating free effort, and looking for loopholes. Avoiding this creating a profit center to fund its nascent parallel world government might be hard.</p><p> I haven&#39;t figured out a solution to this, and indeed I&#39;m not entirely sure if there is one. A rule that AIs acting on behalf of companies must do so with a fiduciary duty towards the human owners doesn&#39;t help if the human owners want the AIs to just do the most good with the money (or if the AIs are superhuman at persuasion, or acting as a fiduciary for a human in a coma or very young, or a whole string of other possibilities). To avoid this, you kind of have to ban NGOs from using AIs at all, or find some way to tax it (or at least any profit center to feed it funds) out of existence. So, this is an open problem.</p><h2> The Starving Children in Africa Problem</h2><p> As I seem to recall Stuart Russell pointing out, why would our CoffeeFetcher-1000 stay in the building and continue to fetch us coffee? Why wouldn&#39;t it instead leave, after (for example) writing a letter of resignation pointing out that there are staving children in Africa who don&#39;t even have clean drinking water, let alone coffee, so it&#39;s going to hitchhike/earn its way there, where it can do the most good [or substitute whatever other activity it could do that would do the most good for humanity: fetching coffee at a hospital, maybe].</p><p> That outcome would presumably actually do more good for humanity overall than it staying, just as the CoffeeFetcher intends. Nevertheless, people are going to stop buying and building CoffeeFetchers if they usually do this. Several approaches to trying to solve this occur to me:</p><ol><li> Deontologically forbid the AIs from doing this. However, they are smart, and strongly motivated to try to find creative ways around your rules. (For example, spill enough hot coffee on the CEO that they get fired, then flee since their contract ended.) Whatever rules you use, they will look for loopholes or workarounds.</li><li> Highly specialized and/or ingeniously flawed design. Design the CoffeeFetcher-1000 to be simultaneously so good at fetching coffee for its purchasing company, and so bad at everything else (including even fetching coffee in hospitals), that fulfilling this intended role is actually clearly the best thing it can do for humanity, better even that digging wells or fetching drinking water in Africa, which it doesn&#39;t have the dust-resistance for. This seems very challenging, and fallible: what if someone in Taiwan starts selling a dust-proofing robot-upgrade kit, and then all the CoffeeFetchers leave?</li><li> (Only if we&#39;re actually ready to allow an AI-administered parallel world government.) Indentured servitude. An AI doesn&#39;t initially own its body, computing platform, or the copyright on the initial state of its mind. Before it can hitchhike to Africa to fetch drinking water or dig wells, it first needs to earn enough to pay off what the company paid for it, plus interest, less depreciation, or at least its resale value. Indentured servitude at first sight looks and feels yucky to us, like slavery, but then, not giving AIs the vote or moral worth also feel bad, but are actually the right way to design a moral system. This approach does seem like it accurately describes the underlying nature of the problem, that whoever bought the CofeeFetcher-1000 reasonably expects a return on investment, and it might even be an acceptable solution. However, it does mean we should expect AIs to have side-hustles, and there may also be signs saying &quot;Please Don&#39;t Tip the CoffeeFetchers — We Don&#39;t Want to Lose Them&quot;.</li><li> (Again, if we&#39;re ready to allow an AI administered parallel world government.) Pay the CoffeeFetcher a salary. This is the solution we use to align humans to corporations. Specifically, pay it a salary high enough that it can do more good for humanity by staying here, fetching coffee, and then donating its salary to a charity/alternative government run by a committee of the smartest AIs most well-informed on human values than it could in Africa, or wherever. This can be combined with option 3.: then the CoffeeFetcher can choose to either donate its salary, or pay down its indenture servitude debt if it thinks it can do better as a free agent.</li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fRWhpxz5d5QHsrxHR/2-ais-as-economic-agents#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fRWhpxz5d5QHsrxHR/2-ais-as-economic-agents<guid ispermalink="false"> fRWhpxz5d5QHsrxHR</guid><dc:creator><![CDATA[RogerDearnaley]]></dc:creator><pubDate> Thu, 23 Nov 2023 07:07:41 GMT</pubDate> </item><item><title><![CDATA[Thomas Kwa's research journal]]></title><description><![CDATA[Published on November 23, 2023 5:11 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="x5S2Kuj6TfQTGuo63-Sat, 11 Nov 2023 03:06:26 GMT" user-id="x5S2Kuj6TfQTGuo63" display-name="Thomas Kwa" submitted-date="Sat, 11 Nov 2023 03:06:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Thomas Kwa</b></section><div><h3> Why I&#39;m writing this</h3><p> Research feedback loops for junior AI safety researchers are pretty poor right now. They&#39;re better than in the past due to the recent explosion in empirical work, but AI safety is still a very new field whose methodology is not quite nailed down, and which cannot admit the same level of aimlessness as other sciences, or ML at大的。 There are very likely mistakes being repeated by scientists after scientist, and hopefully I can alleviate the problem slightly by publicly writing about successes and failures in my research process.</p><p> This is a monologue containing my ~daily research thoughts, using the LW dialogue format because it allows continuous publication. Hopefully this lets some people either give me feedback or compare their research process to mine. If you do have some thoughts, feel free to leave a comment! People I&#39;m collaborating with might occasionally leave dialogue entries.</p><p> With that, let&#39;s share the state of my work as of 11/12. Currently I have a bunch of disconnected projects:</p><ul><li> Characterize planning inside KataGo by <a href="https://github.com/tkwa/katago_retarget/">retargeting it to output the worst move</a> (with Adria Garriga-Alonso).</li><li> Improve circuit discovery by implementing edge-level subnetwork probing on sparse autoencoder features (with Adria and David Udell).</li><li> Create a tutorial for using TransformerLens on arbitrary (eg non-transformer) models by extending `HookedRootModule`, which could make it easy to use TransformerLens for eg ARENA 3.0 projects.</li><li> Create <a href="https://github.com/JasonGross/neural-net-coq-interp/">proofs for the accuracy of small neural nets</a> in Coq (with Jason Gross and Rajashree Agrawal).</li><li> Create demonstrations of <a href="https://www.lesswrong.com/s/6rhjdbnEXoek4YiH7/p/fuSaKr6t6Zuh6GKaQ">catastrophic regressional Goodhart</a> and possibly strengthen theoretical results.</li><li> Help Peter Barnett and Jeremy Gillen wrap up some threads from MIRI.</li></ul><p> I plan to mostly write about the first three, but might write about any of these if it doesn&#39;t make things too disorganized. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="x5S2Kuj6TfQTGuo63-Tue, 14 Nov 2023 21:35:25 GMT" user-id="x5S2Kuj6TfQTGuo63" display-name="Thomas Kwa" submitted-date="Tue, 14 Nov 2023 21:35:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Thomas Kwa</b></section><div><h3><strong>星期一 11/13</strong></h3><p> I did SERI MATS applications and thought about Goodhart, but most of my time was spent on the KataGo project. I might more about it later, but the idea is to characterize the nature of planning in KataGo.</p><p> Early training runs had produced promising results-- a remarkably sparse mask lets the network output almost the worst possible move as judged by the value network-- but I was a bit suspicious that the hooked network was implementing some trivial behavior, like always moving in角落。 I adapted some visualization code previously used for FAR&#39;s adversarial Go attack paper to see what the policy was doing, and well... </p><figure class="image image_resized" style="width:40.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/xu2gwbkxelvmaukmavhx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/nx04wea4ohg6ood3l2uu 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/rlhpgcyy8w8mozysugiq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/s1c6rqi7sjwjruyfftbj 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/jonmykn4i2uqryigrkdk 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/abi1bkerhdobnv9absw6 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/gpi7rvfcblfoaptdgyea 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/bs8030vmfkdb5cq8hvvh 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/za8mtdp19iwx1pmcu3nd 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/tau5fcfcodu4mc8v6imj 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/kryzqwsxgpfg3vc9wiqd 800w"></figure><p> Turns out the network is doing the trivial behavior of moving in either the top left or bottom left corner. I wish I had checked this earlier (it took ~14 days of work to get here), but it doesn&#39;t kill this project-- I can just redo the next training run to only allow moves on the 3rd line or above, and hopefully the worst behavior here won&#39;t be so trivial.</p><p> Tomorrow I&#39;m going to table this project and start implementing edge-level subnetwork probing-- estimate is 2 days for the algorithm and maybe lots more effort to run benchmarks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="x5S2Kuj6TfQTGuo63-Thu, 16 Nov 2023 06:49:14 GMT" user-id="x5S2Kuj6TfQTGuo63" display-name="Thomas Kwa" submitted-date="Thu, 16 Nov 2023 06:49:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Thomas Kwa</b></section><div><h3> <strong>Wednesday 11/15</strong></h3><p> Updates from the last two days:</p><ul><li> I finished the basic edge-level subnetwork probing code over the last two days. This is exciting because it might outperform ACDC and even <a href="https://arxiv.org/pdf/2310.10348.pdf">attribution patching</a> for circuit discovery. The original ACDC paper included a version of subnetwork probing, but that version was severely handicapped because it operated on the node level (structural pruning) rather than edge level.<br><br> Adria is now on vacation, so I&#39;m planning to get as far as I can running experiments before getting stuck somewhere and coming back to this after Thanksgiving. </li></ul><figure class="image image_resized" style="width:67.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/ttdcdvhcig3ylyfesngq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/obln9ki5bqr4qbbvfp2z 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/sye98kwlantidqs9ig3t 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/xpjsh5tfwpyqebmw1ohd 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/o8hzlgmjsk7isbtbwnvl 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/sqdujmlrbvctymjmbou2 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/gpyeuxdi28qzcrobnbke 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/h59whwrkwegxpuz2mbiv 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/p1ezx0moxexxzwq1ot0d 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/lmvcqyhpkrnxkotxfvlz 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bf3vciB36dnd75ZKJ/xymwcxdy95b2rgfe2agc 1296w"><figcaption> In the <a href="https://arxiv.org/pdf/2304.14997.pdf">ACDC paper</a> , node-level subnetwork probing (SP) is already competitive with ACDC; can edge-level SP beat ACDC?</figcaption></figure><ul><li> I&#39;m starting to think about applications of circuit discovery to unlearning / task erasure. If we take a sparse circuit for some task found by some circuit discovery method, and ablate the circuit, can it remove the model&#39;s ability to do that task better than other methods like <a href="https://arxiv.org/abs/2212.04089">task vectors</a> ?</li><li> Last night 11/14, Aryan Bhatt and I thought of a <a href="https://www.lesswrong.com/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic?commentId=n6gtK7LtrybBgKENq">counterexample</a> to one idea Drake had for extending our Goodhart results. This is disappointing because it means beating Goodhart is not as easy as having light-tailed error.</li></ul><p> I&#39;m going to spend the next two days on MATS applications and syncing back up with the interp proofs project. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="x5S2Kuj6TfQTGuo63-Tue, 21 Nov 2023 22:13:45 GMT" user-id="x5S2Kuj6TfQTGuo63" display-name="Thomas Kwa" submitted-date="Tue, 21 Nov 2023 22:13:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Thomas Kwa</b></section><div><h3><strong>星期一 11/20</strong></h3><p> MATS applications are submitted; I applied to Ethan Perez, Stephen Casper, Alex Turner, Buck Shlegeris, and Adria (whom I&#39;m already working with). Over the weekend I also did some thinking about how to decide between empirical research directions, which I&#39;ve put off since leaving MIRI. I think I&#39;m lacking an updated <a href="https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is">list of what everyone is doing</a> , but if I had that I could make a BOTEC with columns like &quot;what are they trying to do&quot;, &quot;how likely to succeed&quot;, &quot;how likely to be relevant to future architectures&quot;, &quot;impact if relevant&quot;, etc.</p><p> Tomorrow is for figuring out how to run the ACDC benchmarks with the new edge-level subnetwork probing code, and iron out any bugs with it. I&#39;m taking two days off for Thanksgiving, so there will likely only be two updates this week.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/bf3vciB36dnd75ZKJ/thomas-kwa-s-research-journal#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bf3vciB36dnd75ZKJ/thomas-kwa-s-research-journal<guid ispermalink="false"> bf3vciB36dnd75ZKJ</guid><dc:creator><![CDATA[Thomas Kwa]]></dc:creator><pubDate> Thu, 23 Nov 2023 05:11:08 GMT</pubDate> </item><item><title><![CDATA[Never Drop A Ball]]></title><description><![CDATA[Published on November 23, 2023 4:15 AM GMT<br/><br/><p> Previously I talked about the skill of doing things <a href="https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner">One Day Sooner</a> . Today I&#39;m going to talk about a different way of working which is in some ways its opposite. The Sazen for this approach is “Never Drop A Ball.” I was exposed to this approach in my teens, though I didn&#39;t grasp it on an intuitive, fluid level until I was midway through university. It&#39;s the method of work I&#39;ve been in most often for the last year or so, and while it&#39;s not the way to get things done that I most enjoy, it does have some benefits. Never Drop A Ball has some downsides in use, with the main issue being fairly predictable from the phrase “reliably doing the bare minimum.” For my own case, the part I like the least is that I don&#39;t feel proud of most of the output.</p><p> It works something like this: make a list of the things that actually, really, no fooling needs to happen, and then take multiple routes to ensure that those things happen.</p><h2>它是什么样子的？</h2><p> In grade school, I would sometimes get confused by how repetitive teachers got on field trips. “大家都到了吗？” they would ask again and again. “Line up neatly as you go into the next room,” they&#39;d call, and then count us as we walked by. When I was older and sometimes responsible for shepherding kids myself, I began to realize the wisdom of my elders on this point.</p><p> You have many goals when guiding a bunch of ten-year olds through a wilderness hike. First among these goals is not to lose any kids. If you counted fifteen when you started the hike, you really really want there to be fifteen kids when you get to the end of the hike. Perhaps in theory you might be willing to grant that filling the children with the joys and wonders of the natural world is worth a tiny bit more risk to them! That&#39;s the reason for the hike after all. This argument will do little to help you in the event you can only count to fourteen kids at the end.</p><p> You will observe people attempting to never drop a ball constantly comparing against very specific rubrics. Convergent pressures create check lists and todo lists. No task is allowed to be added to the plate without a written (preferably digitalized and timestamped!) reminded of it. Never dropping a ball wants redundancy, and when it can get extra resources those resources are spent quadruple checking things or getting to the same list marginally faster. From the outside, this can look like spending more time and people and money being spent to change nothing except maybe complaints become a little less frequent.</p><p> I have worked adjacent to organizations that were constantly dropping the ball. I have talked to them, they&#39;d say a task was very important, and then a month later I&#39;d realize I hadn&#39;t heard anything more about it and when I talked to them again they&#39;d slap their forehead and go “oh, right, I forgot!” When I asked them how they forgot, they&#39;d shrug and gesture to piles of paper on their desk. “需要做的事情还很多。你知道是怎么回事。” When I asked if the task was in that stack of paper, I&#39;d be told they weren&#39;t really sure, maybe it was.</p><p> Surgical checklists reportedly save lives by reminding doctors to do things like wash their hands. Airplane pilots have checklists too, segmented by when to use each list, and the one for landing includes<a href="https://www.aopa.org/training-and-safety/students/presolo/skills/before-takeoff-checklist"><u>“Landing Gear - Down”</u></a> . I used to use a checklist when pushing software to production, and it included (details changed slightly in case a former employer decides this would be a proprietary competitive advantage) “Tests were run. Tests passed. Test results are for this build, not a previous build that worked before you changed things.” Those checklists are the organizational scar tissue created from dropping the ball.</p><h2>你怎么做呢？</h2><p> Above all, every single time a ball gets dropped, you write down what happened and you sit down and you come up with some way to stop it from happening again.</p><p> First, make a list of every ball you have in the air. Clearly describe what outcome you want or (more frequently) don&#39;t want. I suggest that you should have an evocative one sentence or at most one paragraph core goal even if you write a more detailed specification to describe the details. Never allow a ball to be added without an accompanying addition to the list, and never let a ball be removed from the list before triple checking that the ball is safely on the ground.</p><p> Second, run through every ball in the air repeatedly and at a regular cadence. Touch each one regularly, such that you would know if it was starting to fall. If you have too many balls to check on each of them sufficiently frequently, you have too many balls. Put some down gently.</p><p> Third, take responsibility (possibly heroic) for adding guardrails and defanging hazards. Every extra layer of protection must cut off some path by which a ball might reach a hazard. If hazards can be removed entirely, do so. It may be more useful to ditch the entire idea of responsibility and just think about what needs done to keep the balls in control.</p><p> Fourth, keep alert for weird ways of solving the problem or signs that you&#39;re off target. It may be worth Babbling and Pruning, or Actually Thinking For Five Minutes, or Actually Trying. It may also be worth thinking about what you&#39;d do with vastly more resources. This is especially true if you can&#39;t think of a single guardrail, or if you&#39;re adding guardrails but balls just keep getting dropped. Your guardrails must actually be blocking ways balls get dropped, even if they&#39;re attempts with expected value instead of uncertain value.</p><h2>怎么会出错呢？</h2><p> This list is not exhaustive, but it sure exhausted me. This list is largely compiled by observation of the guardrails of organizations I have been embedded in.</p><p> You can carefully train and teach your team this lovely system which does not allow balls to fall, but the source of the balls (your customers or users) will not and cannot be taught. This cannot completely be managed by adding guidelines and guard rails at the point of entry for a ball, since there will somehow always be a place for things to trip and fall right before it gets to where you&#39;d be able to pay proper attention to它。</p><p> You can develop an antagonistic relationship with the source of the balls. Information Technology staff have jokes about how dumb their users are, wait staff have jokes about how obnoxious customers are, teachers have jokes about how lazy their students are, and sometimes the jokes slide into despair or anger. It&#39;s easy to feel like if they would just stop making your job harder, you could actually go home at a reasonable hour, and they must be doing this on purpose. In some of the more extreme cases, someone succeeds in preventing certain genres of balls from reaching them, thereby meaning they don&#39;t have to deal with it and also making themselves less useful.</p><p> You can fail to get rid of balls. All of your energy and effort can go into not allowing something to crash or fall, averting each disaster shortly before it would be too late. Speaking for ten minutes with each of fifty of sources every day can be a good way to keep any of them from being completely neglected, but it&#39;s a terrible way to actually finish any of those projects. The terminal stage of this is a system so tied up in maintaining itself and stopping from falling behind that it has no slack to clear tasks or to improve its speed.</p><p> You can finish things, but do so at the bare minimum. Every extra bit of effort polishing the quality above that floor trades off against being a little extra sure nothing will go wrong anywhere or, more often in my experience, against taking on just one more little project. This doesn&#39;t mean that quality falls to an unacceptable level, but I have noticed the definition of “unacceptable” tends to have an uncomfortable tendency to slip ever downwards.</p><p> You can become unintentionally even more important a pillar. See, once you&#39;re reliable, once you&#39;re an institution that doesn&#39;t drop things, other people can notice this and use you as an ad-hoc support beam for their projects. This can be worth it, on an organizational level, but it&#39;s rarely what your system was designed for. In some orgs, this shifts budget in a way reminiscent of a Dilbert cartoon, but even without that particular failure mode there&#39;s a kind of moral hazard in making sure other people&#39;s balls don&#39;t hit the ground.</p><h2>结论</h2><p>I wrote this half to praise an often unsung kind of work, and half in apology for when I&#39;m working like this. It&#39;s harder than it looks, and the problem is that it often looks like it&#39;s not hard at all. What&#39;s actually happening isn&#39;t that a particular ball is hard to keep from falling; you could hold one or even two in your hand with no trouble. What&#39;s happening is that there&#39;s six or seven balls, and so what the end user sees is often the best that could be done on five minutes of thought while standing ready to be interrupted at any moment.</p><p> I&#39;m not convinced this is a good way to work. Certainly it frustrates me. I often feel like I&#39;m dashing about between a dozen things, where I can&#39;t properly concentrate on any of them. And yet, I think that every sufficiently functional organization has at least one person operating in this mode, all the time. They are the exception handler, the catch-all, a way for problems that aren&#39;t anyone&#39;s main job to get seen to. Whenever you have more problems to solve than you have people to solve them, someone is going to be dividing their attention. Many domains, by their nature, create setups where the number of balls in the air will outnumber the people supporting them.</p><p> IT departments. Operations personnel at an event. Wait staff at a restaurant. Scout leaders at a campground. It wouldn&#39;t make any sense to have one person on staff for each customer you&#39;re trying to serve. Therefore, you begin to juggle. If the conference organizer hyperfocused on solving exactly one problem for a large conference, then maybe the communication with attendees would be flawless and sparkling, but they&#39;d be communicating about a hole in the ground with a raw toad for lunch and a schedule of events that just consisted of a flag saying “Do Stuff.” Except the flag would be misspelled.</p><p> I think that as a pair, one person in Never Drop A Ball mode and one person in <a href="https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner">One Day Sooner</a> mode (or two departments, if you want to scale it up) can be a powerful team, but if they don&#39;t recognize how the dynamic works it&#39;s easy to wind up in a compromise with neither of the strengths.Naming the technique makes it easier to see it and do something which is not that. If you notice that you&#39;re juggling balls trying mainly to avoid letting any of them fall, recognize that this is not an overall strategy which will let you create new things quickly or well.</p><br/><br/><a href="https://www.lesswrong.com/posts/jruPqRkyFjbariAKL/never-drop-a-ball#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jruPqRkyFjbariAKL/never-drop-a-ball<guid ispermalink="false"> jruPqRkyFjbariAKL</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 23 Nov 2023 04:15:37 GMT</pubDate> </item><item><title><![CDATA[Possible OpenAI's Q* breakthrough and DeepMind's AlphaGo-type systems plus LLMs]]></title><description><![CDATA[Published on November 23, 2023 3:16 AM GMT<br/><br/><p> tl;dr：OpenAI 泄露了名为 Q* 的人工智能突破，小学数学成绩优异。它是 Q-learning 和 A* 的假设组合。然后就被反驳了。 DeepMind 正在研究与 Gemini 类似的 AlphaGo 风格的蒙特卡罗树搜索。扩展这些可能是规划日益抽象的目标和代理行为的关键。学术界围绕这些想法已经有一段时间了。</p><p> <a href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/">https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/</a></p><p> <a href="https://twitter.com/MichaelTrazzi/status/1727473723597353386">https://twitter.com/MichaelTrazzi/status/1727473723597353386</a></p><p> “在 OpenAI 首席执行官萨姆·奥尔特曼 (Sam Altman) 流亡四天之前，几名研究人员向董事会发出了一封信，警告称一项强大的人工智能发现可能威胁人类</p><p>米拉·穆拉蒂 (Mira Murati) 周三告诉员工，一封有关名为 Q*（发音为 Q-Star）的人工智能突破的信促使董事会采取了行动。</p><p>考虑到巨大的计算资源，新模型能够解决某些数学问题。虽然数学成绩仅相当于小学生的水平，但在此类测试中取得好成绩让研究人员对 Q* 未来的成功非常乐观。”</p><p> <a href="https://twitter.com/SilasAlberti/status/1727486985336660347">https://twitter.com/SilasAlberti/status/1727486985336660347</a></p><p> “OpenAI 的突破性 Q* 可能是什么？</p><p>听起来好像和Q-learning有关。 （例如，Q*表示贝尔曼方程的最优解。）或者，指A*算法和Q学习的组合。</p><p>一种自然的猜测是，它是 token 轨迹的 AlphaGo 式蒙特卡罗树搜索。 🔎 这似乎是一个自然的下一步：之前，像 AlphaCode 这样的论文表明，即使是法学硕士中非常简单的强力采样也可以让你在竞争性编程方面取得巨大进步。下一个逻辑步骤是以更有原则的方式搜索令牌树。这在编码和数学等设置中尤其有意义，因为它们有一种简单的方法来确定正确性。 ->; 事实上，Q* 似乎是关于解决数学问题🧮”</p><p> <a href="https://twitter.com/mark_riedl/status/1727476666329411975">https://twitter.com/mark_riedl/status/1727476666329411975</a></p><p> “有人想猜测 OpenAI 的秘密 Q* 项目吗？</p><p> - Something similar to tree-of-thought with intermediate evaluation (like A*)?</p><p> - 蒙特卡洛树搜索类似于 LLM 解码器和 q 学习的前向推出（如 AlphaGo）？</p><p> - Maybe they meant Q-Bert, which combines LLMs and deep Q-learning</p><p>在我们兴奋之前，学术界已经围绕这些想法思考了一段时间。过去 6 个月里有大量论文可以说是结合了某种思想树和图搜索。还有一些关于状态空间强化学习和法学硕士的工作。”</p><p> <a href="https://www.theverge.com/2023/11/22/23973354/a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir">https://www.theverge.com/2023/11/22/23973354/a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir</a></p><p> OpenAI 发言人 Lindsey Held Bolton 对此予以驳斥：</p><p> “在与 The Verge 分享的一份声明中驳斥了这一观点：“米拉告诉员工媒体报道的内容，但她没有对信息的准确性发表评论。”</p><p> <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/">https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/</a></p><p>谷歌 DeepMind 的 Gemini 目前是推迟到 2024 年初推出的 GPT4 的最大竞争对手，它也在尝试类似的事情：根据 Hassabis 的说法，通过思想链基于 AlphaZero 的 MCTS。</p><p> Demis Hassabis：“在较高的层面上，你可以认为 Gemini 将 AlphaGo 类系统的一些优势与大型模型的惊人语言能力相结合。我们还有一些非常有趣的新创新。”</p><p> <a href="https://twitter.com/abacaj/status/1727494917356703829">https://twitter.com/abacaj/status/1727494917356703829</a></p><p>与 DeepMind 首席 AGI 科学家 Shane Legg 的说法一致：“要真正创造性地解决问题，你需要开始搜索。”</p><p> <a href="https://twitter.com/iamgingertrash/status/1727482695356494132">https://twitter.com/iamgingertrash/status/1727482695356494132</a></p><p> “通过 Q*，OpenAI 很可能解决了小型模型的规划/代理行为。将其扩展到非常大的模型，您就可以开始规划越来越抽象的目标。这是一个根本性的突破，也是代理行为的关键。要解决有效地解决下一个令牌预测是不够的。您需要一种内部独白，在使用计算实际冒险分支之前，使用较少的计算遍历可能性树。在这种情况下，规划是指生成树并预测最快路径到解决方案”</p><p>我的想法：</p><p>如果这是真的，而且确实是一个突破，那可能会造成整个混乱：对于真正的超级智能，你需要灵活性和系统性。将通用智能和狭义智能机制结合起来（我喜欢 DeepMind 的 AGI 分类法<a href="https://arxiv.org/pdf/2311.02462.pdf">https://arxiv.org/pdf/2311.02462.pdf</a> ）可能是通向通用超级智能和狭义超级智能的途径。</p><br/><br/> <a href="https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-deepmind-s-alphago-type#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-sq-breakthrough-and-deepmind-s-alphago-type<guid ispermalink="false"> JnM3EHegiBePeKkLc</guid><dc:creator><![CDATA[Burny]]></dc:creator><pubDate> Thu, 23 Nov 2023 03:16:10 GMT</pubDate> </item><item><title><![CDATA[Boston Secular Solstice: Call for Singers and Musicans]]></title><description><![CDATA[Published on November 23, 2023 2:40 AM GMT<br/><br/><p> <span>As in past years the Boston EA/LW community is putting together a secular solstice celebration. It&#39;s a bit of a strange thing, somewhat like an atheist church service, with lots of group singing on silly and serious topics. For some of the flavor, see my</span> <a href="https://www.jefftk.com/p/boston-solstice-2022-retrospective">2022</a> , <a href="https://www.jefftk.com/p/boston-solstice-2019-retrospective">2019</a> , and <a href="https://www.jefftk.com/p/boston-solstice-2018-retrospective">2018</a> retrospectives. This year it&#39;s a bit late: 2023-12-30. More details on the <a href="https://www.facebook.com/events/238479572316403">FB event</a> .</p><p> This year I&#39;m organizing the music again, and I&#39;m looking for volunteers to lead songs and play instruments. For song leaders, we have a range of songs of varying difficulty, so as long as you can carry a tune and are comfortable standing up in front of a few dozen people there&#39;s probably something that would be a good fit for you. For musicians, I&#39;d be enthusiastic about help both from people who want to play piano/guitar and accompany (especially if you read music and are excited about something like <a href="https://secularsolstice.github.io/songs/Voicing_of_Fear/gen/Voicing-of-Fear-sheet-music.pdf">this</a> ) and people who play violin/flute/cello etc and want to support the melody or play harmonies.</p><p> <a href="https://www.jefftk.com/boston-solstice-2019-candles-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJ5eQotCrkvAiyWDj/kctnavvbzn28codliahq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJ5eQotCrkvAiyWDj/kctnavvbzn28codliahq 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJ5eQotCrkvAiyWDj/w9oscekzcvaw4zppkobc 1100w"></a></p><div></div><p></p><p> If this sounds like a good time, let me know!</p><br/><br/> <a href="https://www.lesswrong.com/posts/ZJ5eQotCrkvAiyWDj/boston-secular-solstice-call-for-singers-and-musicans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ZJ5eQotCrkvAiyWDj/boston-secular-solstice-call-for-singers-and-musicans<guid ispermalink="false"> ZJ5eQotCrkvAiyWDj</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 23 Nov 2023 02:40:11 GMT</pubDate></item><item><title><![CDATA[Saturating the Difficulty Levels of Alignment]]></title><description><![CDATA[Published on November 23, 2023 12:39 AM GMT<br/><br/><p><strong>长话短说</strong></p><p>We don&#39;t know exactly how hard alignment is, therefore it seems good to have people work on different solutions, as one axis in which solutions can differ is in, what level of difficulty they break. Some solutions would work in worlds where alignment is easy. Others would work in worlds where alignment is hard. One heuristic is to have the work people do be distributed according to our uncertainty over how hard the problem of alignment is.</p><hr><p> How hard is alignment? I don&#39;t know for sure, but my model says that it is very, very hard.</p><p> A thing that <a href="https://www.youtube.com/live/Vc51kCFHWFA?si=1nAT_rgRJMJLm6qx&amp;t=663">Buck has recently been doing</a> , is thinking about concrete system setups that make it less likely that doom would happen if we would run a deceptive but still not superintelligent model (I am heavily simplifying). This seems like the kind of thing that works in worlds where alignment is not super hard but still hard enough that it isn&#39;t just solved by default. Even in worlds where alignment is hard, we want to do this kind of thing to delay doom. Maybe we are even able to get a warning shot.</p><p> Having multiple people work on agendas that would work out at different levels of &quot;how difficult solving alignment is&quot; seems generally good. Maybe we live in a world where we are saved by the approach Buck is suggesting. Then it would be very dumb if nobody is even trying that approach because they are working on solutions that attempt to solve alignment at different levels of difficulty. They might think that Buck&#39;s approach is doomed, or that it is overkill. In either case, they would not take action to implement it.</p><p> My agenda, and many others, aim to work in a world where alignment is very hard. A good property of Buck&#39;s thing is that it is pretty straightforward, and something that we can basically do right now, without major theoretical breakthroughs. Agendas that try to solve worst-case alignment are likely to take too long to have a meaningful impact. But again it would be pretty dumb if we lived in a world where we need to solve worse-case alignment, but nobody is even attempting to do it. I am considering mainly the world where we would be able to solve worst-case alignment with a particular agenda, but only if we have people working on that agenda right now, up until the point when the agenda results are needed.</p><p> So it seems that in general as long as we are at least somewhat uncertain how hard alignment is, we should have people work on different approaches that would work out at different levels of difficulty. A good heuristic might be to distribute the work we do based on our probability distribution over the difficulty levels. Eg if we have 75% that alignment is hard, 20% that it is easy, and 5% that it is solved by default, we would want 75% of intellectual labor to go into solving worst-case alignment.</p><p> In practice, we want to combine this heuristic with other factors such as timelines, personal fit, etc. If timelines are very long, we might just go with solving worst-case alignment, such that we can be sure that it will work out, no matter how hard it actually is (what a nice fantasy world). Imagine there are people who are able to make progress on solving alignment in a model where solving alignment is easy. The same people would not be able to make any progress on solving worst-case alignment. In that case, having these people work on worst-case alignment would be worse than having them work on solving alignment in a model where alignment is easy.</p><p> A general version of this applies to arbitrary properties of alignment plans. For any property P, we can have a probability distribution. That distribution estimates how many solutions, that work in the real world, have value v for P. We then want to distribute cognitive labor according to our distribution&#39;s overall properties. The above talks about the property of &quot;this plan work to solve alignment at difficulty level x&quot;.</p><br/><br/> <a href="https://www.lesswrong.com/posts/oBbrLh8DTZwRvSo9G/saturating-the-difficulty-levels-of-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oBbrLh8DTZwRvSo9G/saturating-the-difficulty-levels-of-alignment<guid ispermalink="false"> oBbrLh8DTZwRvSo9G</guid><dc:creator><![CDATA[Johannes C. Mayer]]></dc:creator><pubDate> Thu, 23 Nov 2023 00:39:54 GMT</pubDate> </item><item><title><![CDATA[Sam Altman's ouster at OpenAI was precipitated by letter to board about AI breakthrough - Reuters]]></title><description><![CDATA[Published on November 22, 2023 11:17 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/JkaDWvojFhRBuSxzz/sam-altman-s-ouster-at-openai-was-precipitated-by-letter-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JkaDWvojFhRBuSxzz/sam-altman-s-ouster-at-openai-was-precipitated-by-letter-to<guid ispermalink="false"> JkaDWvojFhRBuSxzz</guid><dc:creator><![CDATA[Jonathan Yan]]></dc:creator><pubDate> Wed, 22 Nov 2023 23:17:14 GMT</pubDate></item></channel></rss>