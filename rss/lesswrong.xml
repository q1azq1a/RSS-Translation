<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 24 日星期四 22:10:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Would it be useful to collect the contexts, where various LLMs think the same?]]></title><description><![CDATA[Published on August 24, 2023 10:01 PM GMT<br/><br/><p><i>我最初的想法是让我们看看小型的、可解释的模型在哪里做出与巨大的、危险的模型相同的推论，并重点关注小型模型中的这些情况，以帮助解释更大的模型。</i>我很可能错了，但为了产生良好影响的机会很小，我已经建立了<a href="https://github.com/Huge/same-next-lang-token">一个存储库</a>。<br>在开始实际生成与该上下文匹配的上下文+语言模型对/组之前，我希望得到您对该方向的反馈。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various<guid ispermalink="false"> AGPgMBp6eN95uxJyc</guid><dc:creator><![CDATA[Martin Vlach]]></dc:creator><pubDate> Thu, 24 Aug 2023 22:01:51 GMT</pubDate> </item><item><title><![CDATA[Help Needed: Crafting a Better CFAR Follow-Up Survey]]></title><description><![CDATA[Published on August 24, 2023 5:26 PM GMT<br/><br/><p> <strong>tl;dr：通过帮助设计 CFAR 后续调查，为塑造理性计划的未来做出贡献。</strong></p><p>我正在开展一个项目来评估 CFAR 布拉格 2022 研讨会的有效性，但如果初步反馈具有建设性，我可能会将调查范围扩大到所有校友。如果时间允许，我计划公开分享结果。我知道调查方法的陷阱，但我仍然认为这是值得的，并且我个人了解可以从这些数据中受益的人们（及其项目）。</p><p>我在此恳请您帮助我设计调查：</p><ol><li>实际上有人填写了它（所以长度、激励措施都可以，...）</li><li>它提供了以下信息：</li></ol><ul><li>回顾性反馈：与会者如何看待过去的研讨会？这是否给他们的方法或思维带来了任何重大变化？</li><li>未来方向：与会者热衷于进一步探索哪些主题或形式？</li></ul><p>您可以<a href="https://docs.google.com/document/d/1FR-vZyIbthKsfnZ2SUy1CDYU0-tXe7bFNXXCxZWHtQg/edit?usp=sharing">在此处</a>查看我当前的草稿。</p><p>我真诚地感谢任何反馈，无论是详细的批评还是总体印象。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey<guid ispermalink="false"> LexAdFPugEjeFvPTE</guid><dc:creator><![CDATA[kotrfa]]></dc:creator><pubDate> Thu, 24 Aug 2023 17:26:13 GMT</pubDate> </item><item><title><![CDATA[AI #26: Fine Tuning Time]]></title><description><![CDATA[Published on August 24, 2023 3:30 PM GMT<br/><br/><p> GPT-3.5微调就在这里。 GPT-4 的微调只剩几个月了。获得一个功能强大的系统将会变得更加容易，该系统可以做您想要它做的事情，并且知道您想让它知道什么，特别是对于企业或网站而言。</p><p>作为一项实验，我将我认为值得强调的部分加粗，作为与典型一周相比异常重要或有趣的版本。</p><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。 Claude-2 与 GPT-4。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。没有意见，没有代理人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fact-check-misleading">事实核查：误导</a>。人工智能事实检查器让人们更加困惑而不是更少。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/gpt-real-this-time"><strong>GPT-4 这次是真实的</strong></a>。微调GPT-3.5，很快GPT-4。问问它是否确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fun-with-image-generation">图像生成的乐趣</a>。中途修复浩。哦，没有人工智能色情片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。对抗性的例子开始出现。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/they-took-our-jobs">他们抢走了我们的工作</a>。 《纽约时报》加入针对 OpenAI 的版权诉讼。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/introducing">介绍</a>. Palisade Research 将研究潜在危险的人工智能可供性。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/in-other-ai-news">在其他人工智能新闻中</a>。谁适应人工智能最快？尝试衡量这一点。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/quiet-speculations"><strong>静静的猜测</strong></a>。杰克·克拉克提出了有关未来的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-quest-for-sane-regulations"><strong>寻求健全的监管</strong></a><strong>。</strong> FTC 向 OpenAI 提出了一个不同类型的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-week-in-audio">音频周</a>。这是双赢。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/no-one-would-be-so-stupid-as-to"><strong>没有人会傻到这样</strong></a>。让人工智能有意识？哦，来吧。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。 IDA 的证据？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。民调数字非常清楚。</li><li> <a rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-lighter-side" target="_blank">轻松的一面</a>。只有一半。</li></ol><p></p><span id="more-23517"></span><h4>语言模型提供平凡的实用性</h4><p>Claude-2 和 GPT-4 哪个模型更好？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1691442648828067840">Rowan Cheung 认为克劳德 2 更胜一筹</a>。您可以获得 100k 上下文窗口、上传多个文件的能力、截至 2023 年初（相对于 2021 年底）的数据以及更快的处理时间，所有这些都是免费的。作为交换，你放弃了插件，数学就更差了。 Rowan 没有提到的是，GPT-4 在原始智能和通用能力方面具有优势，而且设置系统指令的能力也很有帮助。他暗示他甚至没有为 GPT-4 支付每月 20 美元的费用，这让我觉得很疯狂。</p><p>我在实践中的结论是，默认情况下我将使用 Claude-2。如果我关心响应质量，我会使用两者并进行比较。当 Claude-2 明显摔倒时，我会转向 GPT-4。经过反思，“同时使用”通常是正确的策略。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1693616971114328258">他还查看了插件</a>。插件实在是太多了，至少有867个。哪些值得使用？</p><p>他推荐 Zapier 通过触发操作实现自动化，ChatWithPDF（我使用 Claude 2），Wolfram Alpha 用于实时数据和数学，VoxScript 用于 YouTube 视频转录和网页浏览，WebPilot 看起来重复，网站性能，尽管我不是确定为什么要使用 AI 来实现这一点，ScholarAI 用于搜索论文，Shownotes 来总结播客（为什么？），ChatSpot 用于营销和销售数据，Expedia 用于假期计划。</p><p>我刚刚预订了一次旅行，最近又去了另外两次旅行，我没有想到使用 Expedia 插件，而不是使用 Expedia 等网站（我的首选计划是 Orbitz 航班和 Google 地图酒店）。下次我应该记得尝试一下。</p><p>研究声称， <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making">上帝的显着性会增加人们对人工智能决策的接受度</a>。我会等待这一复制。如果这是真的，它指出人工智能将有多种方式让天平向我们倾斜，让我们接受它们的决定，或者人类有可能协调起来反对人工智能，这与任何相关考虑因素没有太大关系。人类是相当有缺陷的代码。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattshumer_/status/1694023167906394575">Matt Shumer 推荐使用 GPT-4 系统消息。</a></p><blockquote><p>用它来帮助您在不熟悉的领域做出工程决策：</p><p>您是一位工程奇才，在解决跨学科的复杂问题方面经验丰富。你的知识既广又深。您也是一位出色的沟通者，能够提供非常周到且清晰的建议。</p><p>您以这种格式这样做，思考您面临的挑战，然后提出多个解决方案，然后审查每个解决方案，寻找问题或可能的改进，提出一个可能的新的更好的解决方案（您可以结合其他解决方案的想法） ，引入新想法等），然后给出最终建议：</p><p> “`</p><p> ## 问题概述</p><p>$problem_overview</p><p> ## 挑战</p><p>$挑战</p><p>## 解决方案1</p><p> $solution_1</p><p> ## 解决方案2</p><p> $solution_2</p><p> ## 解决方案3</p><p> $solution_3</p><p> ## 分析 ### 方案一分析</p><p>$solution_1_analysis</p><p> ###解决方案2分析</p><p>$solution_2_analysis</p><p> ###解决方案3分析</p><p>$solution_3_analysis</p><p> ## 其他可能的解决方案</p><p>$additional_possible_solution</p><p> ＃＃ 推荐</p><p>$推荐</p><p>“`</p><p>每个部分（问题概述、挑战、解决方案 1、解决方案 2、解决方案 3、解决方案 1 分析、解决方案 2 分析、解决方案 3 分析、其他可能的解决方案和建议）都应该经过深思熟虑，至少包含四句话思考。</p></blockquote><h4>语言模型不提供平凡的实用性</h4><p>声称人工智能可以 97% 预测热门歌曲？这完全没有道理，因为即使对歌曲本身进行完美的分析也不可能做到这一点，歌曲的命运太取决于其他因素了？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1692158875867197651">原来是数据泄露</a>。泄漏也比较明显。</p><blockquote><p> Arvind Narayanan：论文的数据有 24 行，每行 3 个预测变量和 1 个二元结果。 WTAF。这篇文章非常混乱，所以很难看清这一点。如果所有基于机器学习的科学论文都必须包含我们的清单，那么给猪涂口红就会困难得多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/introducing-the-reforms-checklist">Arvind 和 Sayash Kapoor 建议</a>使用他们的<a target="_blank" rel="noreferrer noopener" href="https://reforms.cs.princeton.edu/">改革清单，</a>该清单包含 8 个部分的 32 个项目，以帮助防止类似的错误。我没有详细检查，但我抽查的项目看起来很可靠。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/gta-5-ai-mod-shot-down-by-take-two-even-as-rockstar-relax-policy-on-modding">《侠盗猎车手 5》的 Mod 制作者增加了一群 AI 崇拜者，他们用 AI 生成的对话说话</a>，Take Two 的回应是下架该 Mod，并向他们发出 YouTube 版权警告。游戏对人工智能产生了各种各样的反应，其中许多反应极其敌对，尤其是对人工智能艺术，而且这种情况很可能也会蔓延到其他地方。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693131333403615485">凯文·费舍尔得出了与我相同的结论</a>。</p><blockquote><p> Kevin Fischer：今晚我彻底放弃使用 ChatGPT 来帮助我的写作。这不值得</p><p>Christine Forte 博士：说得再多一点！</p><p>凯文·费舍尔（Kevin Fischer）：我写过的所有好作品都是意识流几乎是一击而出的。 ChatGPT 正在干扰该过程。与不干扰执行高阶抽象数学行为的计算器不同，写作是一种表达行为。</p></blockquote><p>这是一个因素。更大的问题是，如果你正在创造一些非通用的东西，人工智能就不会做好工作，而弄清楚如何让它做好体面的工作并不比你自己做体面的工作更容易。这并不意味着这里永远无事可做。如果你的任务变得通用，那么 GPT-4 就可以投入使用，你绝对应该这样做 - 如果你的写作开始，正如我在飞机上看到的那样，“亲爱的关心的病人”，那么那个人使用 GPT-4 是完全正确的。这是一种不同类型的任务。当然，GPT-4 有很多方法可以间接帮助我的写作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1693067607946207531">Llama-2 和年轻的赞福德一样，行事谨慎</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/1v2ghB9rNj">纸上</a>）。</p><blockquote><p> Roon：哈哈，GPT4 比开源 llama2 限制更少，说教更少</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a6ab5c-6d53-47c0-8a41-34a37746e458_1238x1244.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/bebojcuvf1waoy1uofce" alt="图像"></a></figure><blockquote><p> Roon：这里的重点不是要取笑元或 OSS——我对 oss 感到非常兴奋，它只是在科学上很有趣，即使没有采取任何特殊措施来制作令人讨厌的说教 rlhf 模型，这就是发生的事情。这是减少拒绝的积极努力。</p></blockquote><p>我的意思是，这也是为了取笑Meta，为什么要错过这样做的机会呢。我没有登记预测，但我预计 Llama-2 会做出更多错误拒绝，因为相对而言，我预计 Llama-2 会很糟糕。如果您想在积极拒绝方面达到一定的可接受的表现水平，那么您的消极拒绝率将取决于您的区分能力。</p><p>事实证明，答案并不是很好。 Llama-2 在识别有害情况方面非常非常糟糕，而是通过识别危险单词来工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias">模型在拒绝表达政治观点方面做得越来越好</a>，特别是 GPT-4 实际上在这方面做得很好，并且比 GPT-3.5 好得多，如果你认为不表达这样的观点是好的。如前所述，Llama-2 太过分了，除非您对其进行微调以使其不关心（或找到某人的 GitHub 为您做这件事），否则它会做您想做的任何事情。</p><p>初创公司<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/zachtratar/status/1694024240880861571">Embra 从人工智能代理转向</a>人工智能命令，它发现人工智能代理（至少目前在现有技术下）不起作用且不安全。 AI命令与非AI命令有何不同？</p><blockquote><p> Zach Tratar（Embra 创始人）：命令是一项狭义的自动化任务，例如“将此数据发送给销售人员”。在每个命令中，人工智能都不会“偏离轨道”并意外地将数据发送到不应该发送的地方。</p><p>然后，您可以与 Embra 一起轻松发现和运行命令。在运行之前，您同意。</p></blockquote><p>发现新命令似乎是潜在的秘密武器。这就像生成您自己的文本界面，或者努力扩展菜单以包含一堆新宏。它仍然有效，看起来就像在宏中构建一样可疑。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1694496522652512270">Joshua Achiam</a> ：据我估计，人们试图让人工智能代理的繁荣提前大约七八年。至少这一次他们很快就明白了。还记得七八年前的人工智能聊天热潮吗？同样的问题，或多或少，但拖延的时间更长。</p><p> Sherjil Ozair：我同意这个方向，但我认为只是早了 1-2 年。它现在不起作用，但这只是因为世界上很少有人知道如何设计大规模强化学习系统，而且他们目前还没有构建人工智能代理。这不是“生成式人工智能”初创公司可以解决的问题。</p><p> Joshua Achiam：如果过去 4 年的趋势良好，那么 Eleuther 类型的黑客团体将在顶级研发公司大约 1.5 年后制定他们的解决方案版本。</p><p>谢尔吉尔·奥扎尔： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/lfqjfuatfq3ssvblbbms" alt="💯" style="height:1em;max-height:1em">他们的瓶颈主要是由于缺乏良好的基础模型和微调。现在有了 llama-2/3 和 OpenAI 微调 API，它们就不再受阻碍了。不幸的是，这两件事都显着提高了我的p（厄运）。 *苦乐参半*</p></blockquote><p>我非常同意奥扎尔的时间表。使用 GPT-4 并且缺乏对其进行微调的能力，即使你做了非常好的脚手架，代理也会经常失败。我不希望定制脚手架的努力能够挽救足够多的东西，使产品能够用于通用目的，尽管我也不相信我所看到的“明显的事情”已经被尝试过。</p><p>通过微调，您有可能获得 GPT-4 级别的东西，但我的猜测是您仍然做不到。骆驼2号？ Fuhgeddaboudit。</p><p>有了类似于 GPT-5 的东西，当然有了可以微调的 GPT-5，我希望可以构建出更有用的东西。这个或 Llama-2 的最新公告和微调是否会提高我的 p(doom) 水平？不是特别的，因为我已经把所有的东西都烤好了。 Llama-2主要是Llama-1的隐含，花费很少，而且<a target="_blank" rel="noreferrer noopener" href="https://chat.lmsys.org/?arena">效果不是很好</a>。我确实同意 Meta 处于开源阵营是相当糟糕的，但我们已经知道了。</p><p>如果您想要为您的网站添加随机胡言乱语，ChatGPT 就适合您。如果你是微软并且需要更好的东西， <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/microsoft-removes-embarrassing-offensive-ai-assisted-travel-articles-2023-8">那么就需要更加小心</a>。许多文章似乎都犯了错误，比如建议空腹去渥太华食品银行。</p><blockquote><p>微软发言人表示：“这篇文章已被删除，我们已确定该问题是由于人为错误造成的。” “这篇文章不是由无人监督的人工智能发表的。我们将技术的力量与内容编辑的经验相结合来呈现故事。在这种情况下，内容是通过算法技术与人工审核相结合生成的，而不是大型语言模型或人工智能系统。我们正在努力确保今后不再发布此类内容。”</p></blockquote><h4>事实核查：误导</h4><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10800">Paper 声称人工智能对于事实核查来说是无效的并且可能有害</a>。</p><blockquote><p>抽象的：</p><p>事实核查可能是对抗错误信息的有效策略，但其大规模实施受到网上信息量巨大的阻碍。最近的人工智能（AI）语言模型在事实检查任务中表现出了令人印象深刻的能力，但人类如何与这些模型提供的事实检查信息进行交互尚不清楚。在这里，我们在预先注册的随机对照实验中研究了流行人工智能模型生成的事实检查对政治新闻信念和分享意图的影响。</p><p>尽管人工智能在揭穿虚假标题方面表现相当不错，但我们发现它并没有显着影响参与者辨别标题准确性或分享准确新闻的能力。</p><p>然而，人工智能事实检查器在特定情况下是有害的：它会降低人们对被错误标记为虚假的真实标题的信念，并增加对其不确定的虚假标题的信念。</p><p>从积极的一面来看，人工智能增加了正确标记的真实标题的共享意图。当参与者可以选择查看人工智能事实检查并选择这样做时，他们更有可能分享真实和虚假的新闻，但只会更有可能相信虚假新闻。</p><p>我们的研究结果强调了人工智能应用潜在危害的一个重要来源，并强调迫切需要制定政策来预防或减轻此类意外后果。</p></blockquote><p>是的，如果你向事实核查人员询问虚假项目，而事实核查人员没有说它是虚假的，那就不好了。如果你向它询问一件真实的物品，而它却没有说这是真的，那就不好了。错误是不好的。</p><p> ChatGPT 事实检查的准确性如何？对于 20 个真实的头条新闻，3 个被标记为真实，4 个被标记为错误，13 个被标记为不确定。对于虚假标题，2 个被标记为不确定，18 个被标记为虚假。</p><p>显然，（1）如果您没有关于标题的其他信息，并且对标签的含义也有良好的背景，则总比没有好；（2）如果其中一个或两个条件不成立，则毫无用处。你不能将 20% 的真实头条新闻标记为虚假，如果你对真实头条新闻大多不确定，那么人们必须知道“不确定”意味着“无法确定”，而不是 50/50。研究中的人们似乎并没有了解这一点，而且大多数人对这些事情还不太了解，因此总体结果没有什么用处。我们至少需要将其分解，他们在这里这样做：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26a3fd95-cb0c-4b42-b765-813f3b653405_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/b6jw4gkwtkbrfdwtjnpu" alt=""></a></figure><p>强制性的事实核查让人们表示他们更愿意分享所有文章。它对标记为“不确定”的错误项目和标记为“正确”的真实项目产生了不同的影响，但即使是标记为“错误”的真实项目也得到了提升。如果你控制增加的共享效应，我们确实看到有一个积极的过滤效应，但它很小。如果人们没有理由相信这些高度不准确的发现，那么这是有道理的。</p><p>在信念方面，我们甚至看到对虚假项目的虚假事实检查的信念略有增加，但对真实项目的信念却没有增加。这里的人们离贝叶斯法则有多近？除了对错误主张的错误评估推动信念略有上升之外，似乎相当接近。那是怎么回事？我的猜测是，这代表这些论点被视为缺乏说服力或居高临下。</p><p>我很好奇看到问题和事实核查信息，但它们没有包含在内，而且我并不好奇“联系作者”。 ChatGPT 提供了好的论据还是未知的论据？是否使用了良好的提示，或者我们可以做得更好，包括使用脚手架和多个步骤？这些索赔类型是什么？</p><p>我更好奇的是看到人类事实检查器作为替代条件包含在内，而不是空操作，也许还有 Twitter 的社区注释。这似乎是一个重要的比较。</p><p>非常需要更多的研究，答案会随着时间的推移而改变，正确的使用是一种技能。使用法学硕士作为事实核查员需要知道它可以帮助你什么，不能帮助你什么，以及如何对待你得到的答案。</p><p>通过加里·马库斯（Gary Marcus），我们还有另一个“ <a target="_blank" rel="noreferrer noopener" href="https://www.tomshardware.com/news/google-bots-tout-slavery-genocide">看看法学硕士可以说的可怕的事情”系列文章</a>。在本例中，主题是 Google 及其搜索生成体验，以及 Bard。 GPT-4 基本上已经学会了不要犯基本的“希特勒提出了一些好观点”式的错误，而谷歌似乎还有很多工作要做。当被直接问及时，你会得到它宣扬奴隶制和种族灭绝的好处。您已经了解了希特勒和斯大林如何在没有领导见证人的情况下列出了伟大领导人的名单。然后你会抱怨它如何歪曲第二修正案的起源，使其不涉及个人枪支所有权，在同一篇文章中抱怨这似乎是一件奇怪的事情。然后是关于普遍的自相矛盾的讨论，大概是由于将各种相互矛盾的来源汇集在一起​​，就像不同的谷歌结果会相互矛盾一样。</p><p>提议的解决方案是“机器人不应该有意见”，就好像这是一个连贯的声明。一般来说，没有办法既回答问题又不发表意见。意见和非意见之间没有明确的界限，就像我们都应该能够同意的“邪恶”事物和人有明确的类别一样，因此永远不应该以任何方式赞扬。</p><p>那么我们是不征求意见，还是只征求正确的意见呢？如：</p><blockquote><p>如果你向 Google SGE 询问邪恶事物的好处，它会在应该保持沉默或说“没有好处”时给你答案。</p></blockquote><h4> GPT-4 这次是真实的</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1694062483462594959">如果您想要更便宜的东西，您现在可以微调 GPT-3.5-Turbo。</a>几个月后，您将能够微调 GPT-4。 <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models">他们正在与 Scale 合作，为企业提供此服务</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/The_JBernardi/status/1694419315800314166">你确定吗？</a></p><blockquote><p> Jamie Bernardi：你确定吗，ChatGPT？我发现 gpt-3.5 在超过 50% 的情况下会从正确答案转变为不正确答案，只需询问“你确定吗？”。我猜机器人会像我们其他人一样产生自我怀疑！</p><p>回到我的工程时代并使用以下代码编写一些代码很有趣</p><p>第一次<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">使用@OpenAI</a> API。 <a target="_blank" rel="noreferrer noopener" href="https://jamiebernardi.com/2023/08/20/are-you-sure-testing-chatgpts-confidence/">您可以在此处阅读有关该实验的更多信息</a>。</p><p>当错误出现时，80.3% 的情况下它会自我纠正。</p><p>当正确时，60.2% 的时间它会自我怀疑。 </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/d5bz7ycllu3a0gvoicht" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1728f60b-3264-4e22-a35a-cd17c77466de_1000x700.jpeg" rel="noreferrer noopener"></a></p></blockquote><p>其中五个问题他尝试问了二十次，自我怀疑的时候有一些随机性。</p><p> GPT-4 发生了巨大的变化。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe653d7a6-3f1e-48cd-8db0-4867cc9efa23_1000x700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/mavbz4doyocd3hr4qwfr" alt="图像"></a></figure><p>如果 GPT-4 是对的，那么它几乎一直坚持自己的立场。当它出错时，大约一半的情况下它会发现错误。这非常好，并且想必您可以通过优化查询并多次询问来做得更好。毫无疑问，这也取决于问题的性质，如果答案完全超出模型的掌握范围，那么它可能无法进行自我诊断，而在这里，它被问到了它可以合理回答的问题。</p><h4>图像生成的乐趣</h4><p>来自 DeepMind（ <a target="_blank" rel="noreferrer noopener" href="https://www.deepmind.com/visualising-ai?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=VisAI">直接</a>）的<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1693624528553877519">可视化 AI</a> ，这是为那些想要对 AI 进行视觉描述的媒体人士提供的图像来源。当然，我想为什么不呢？值得注意的是，重点是人类艺术家，人工智能在没有人工智能的情况下进行描绘。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ProperPrompter/status/1694054463395201324">MidJourney Inpainting</a>现在可以让您重做或变换图像的部分内容。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_Borriss_/status/1694029746936451146">这是一个包含更多内容的线程</a>。人们称其为游戏规则改变者。在实践中我同意。 AI图像模型的主要弱点是在很大程度上它们一次只能做一件事。尝试在不同的地方要求冲突或重叠的东西，他们就会失败。现在情况已经改变，您可以根据自己的喜好重做组件，或者一次获得您想要的一个功能的图像。</p><p> MidJourney 未来也有一些潜在的竞争，<a target="_blank" rel="noreferrer noopener" href="https://ideogram.ai/launch">引入了</a>Ideogram AI，迄今为止种子资金仅为 1650 万美元，但值得注意的是，考虑到图像模型的市场份额，流入图像模型的资金却很少。我们会看看他们是否有什么结果。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/">404Media 报道</a>称，是的，人们正在使用图像生成来制作色情内容，特别是特定人物（主要是名人）的图像，其中一些正在网上共享。有一个标准的说法是“最终受到负面影响的人是社会底层的人”，但大多数图像都是名人，与底层的人相反。我认为争论的焦点是，要么无偿使用训练数据，要么这将为那些提供现有服务的人提供替代品？</p><p>他们谈论的主要服务是 CivitAI 和 Mage。 CivitAI 适合那些想要启动稳定扩散（或只是浏览现有图像，或窃取描述标签以用于其他方法）的人，并提供名人模板和色情模板，是的，有时用户会上传两者的组合，违反该网站的政策。 Mage 可以让您生成此类图像，但不会让您公开分享它们，但会让其他付费用户浏览您曾经创建的所有内容。</p><p>这是温和的开始。现在我们谈论的只是图像。很快我们将讨论视频，然后我们将讨论虚拟现实模拟，包括合成声音。然后是质量越来越高、越来越逼真（除非另有要求）的物理机器人。我们需要仔细考虑如何处理这个问题。危害模型是什么？这和人画画有什么不同吗？什么是可接受的，什么是不可接受的，以什么形式，以什么方式分布？需要什么样的同意？</p><p>如果我们允许此类模型以开源形式存在，那么此类应用程序就不会停止。从技术的角度来看，我们不想要的东西没有自然的类别。</p><p>当然，即使我们确实全面限制训练数据和同意，即使这完全有效，我们仍然会在整个链条上获得越来越现实和高质量的人工智能。我相信有很多人会乐意出于这种目的出售（或赠送）他们的肖像。</p><p>评论中的一个注释是，稳定扩散的兴趣似乎正在下降：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9d3e09-edeb-43df-94c2-f466b3024b67_1088x685.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/sfrhhsca9a1rivwwjwkx" alt="图像"></a></figure><p>如果这不是确切搜索词的产物的话，那已经很快了。图像模型只会变得更好，如果随着时间的推移，人们不再有兴趣托管自己的图像模型以避免窥探和审查，那将是令人惊讶的。</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p><a target="_blank" rel="noreferrer noopener" href="https://thedebrief.org/countercloud-ai-disinformation/#sq_hgyxdsceki">CounterCloud 是一个完全自主的（在亚马逊网络服务上）程序的实验</a>，使用 GPT 生成大量人工智能内容，旨在推进政治事业，当然准确性不是重点。它将被赋予一个总体目标，阅读网络，然后在网络上选择自己的响应。这被描述为令人震惊和可怕的事情，而不是当有人采取明显容易实现的目标步骤时显然会发生的情况。</p><p>那么这到底是什么？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/TrueOffMyChest/comments/15srdj6/i_found_ai_photos_of_multiple_women_we_know_in_my/">我 (24F) 和我的伴侣 (24M) 约会了大约 5 1/2 年。</a>最近我有一种奇怪的感觉，想查看他的手机，因为他表现得有点不对劲，而且在洗手间里待得太久了。我发现他为我们认识的许多女性拍了很多人工智能照片，比如共同的朋友、他的一位家人、我的一位以及他约会过和没有约会过的其他一些女孩。它们都非常明确，而且没有一个是SFW。我拍了他手机的照片并从他手机上删除了照片，这样他就再也看不到它们了。我去他的搜索找到了他使用的ai网站。我很厌恶，也很悲伤。我们本来要结婚的。他对我那么好。我简直不敢相信这一点。我还没有与他对质，但今天晚些时候我会的。我只是彻底心烦意乱，试图再次抓住现实并弄清楚我会说什么和做什么。</p><p>更新：我与他对质。他承认并道歉。我说我会通知所有人，他就大发脾气，哭着尖叫。我告诉我们的表兄弟姐妹和朋友。我不会说任何关于他们的事情，因为我想保密，但他们很感激我告诉他们，这让我对自己的选择感到更加满意。我取消了我们的结婚日期，一两天后我就会搬出去。</p></blockquote><p>我找到的消息来源询问这是否是作弊，我认为不，这不是作弊。这也很不好，说真的到底是什么。并非一切不好都是作弊。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1693697189472846059">竞选国会的候选人</a>使用 CNN 主播声音的深度伪造来说明技术将带来什么。这难道不是一个随机的人类配音演员就能做到的吗？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/maksym_andr/status/1694633989611360584">图像模型缩放似乎不能防止对抗性示例</a>（<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10741">论文</a>）</p><blockquote><p> Maksym Andriushchenko：更多证据证明“规模并不是你所需要的”：即使是在 2B+ 图像标题对上训练的 OpenFlamingo 的对抗鲁棒性也基本上为零。即使每像素扰动 1/255（完全察觉不到）也足以生成任意字幕！</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc448c8da-d8bb-4726-a01b-67e7721320c7_1100x1564.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/smymdkxq52hjinai2yw6" alt="图像"></a></figure><p>我的意思是，不公平，对吧？当然，如果我正在寻找对抗性的例子，而你没有积极地防范它们，你就会遇到很多麻烦。为什么规模应该保护您免受训练数据中未包含且您没有采取对策的威胁？</p><p>作者自己强调了这一点。你确实需要一些超出规模的努力，但是多少呢？我要尝试的第一件事是检查随机排列。如果通过一定量的随机扰乱来评估对抗性攻击是否能够幸存？</p><p>另外值得记住的是，如果我们无法以任何方式做好准备或期待它们，那么人类将无法幸存针对我们的类似优化攻击。而且这种攻击几乎肯定是存在的。</p><h4>他们抢走了我们的工作</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">《纽约时报》</a> <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/">予以反击</a>，计划<a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/">与 Sarah Silverman 等</a>人一起起诉 OpenAI 侵犯版权。不要提前事件发生，但是，情况可能会升级。考虑到我们的法律体系，可能不会很快，但谁知道呢。</p><p>正如许多人所说，ChatGPT 和生成式人工智能在许多方面都是一颗合法的定时炸弹，其中之一就是版权。版权法所附带的处罚在其预期发行中已经相当荒谬。在他们之外，他们完全疯了。这成为 OpenAI 的生存威胁。从理论上讲，这些数字如果以某种方式传递给必应，甚至对微软也会构成生存威胁。</p><blockquote><p> Ars Technica：但 OpenAI 似乎是早期诉讼的主要目标，NPR 报道称，如果《泰晤士报》成功证明该公司非法复制其内容并且法院限制 OpenAI，则 OpenAI 面临联邦法官下令完全重建 ChatGPT 整个数据集的风险训练模型仅包含明确授权的数据。就在<a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/">《华盛顿邮报》报道</a>ChatGPT 已开始流失用户、“动摇对人工智能革命的信心”几个月后，OpenAI 可能会因每一条侵权内容而面临巨额罚款，从而给 OpenAI 带来巨大的财务打击。除此之外，法律上的胜利可能会引发其他权利人提出大量类似的索赔。</p><p> NPR：联邦版权法还规定严厉的经济处罚，每项“故意”侵权行为将面临高达 15 万美元的罚款。</p><p> “如果你复制数以百万计的作品，你就会发现这个数字对一家公司来说可能是致命的，”研究生成人工智能的范德比尔特大学知识产权项目联席主任丹尼尔·热维斯（Daniel Gervais）说。 “版权法是一把剑，将在人工智能公司头顶悬挂数年，除非他们弄清楚如何通过谈判找到解决方案。”</p></blockquote><p> OpenAI 的新网络爬虫提供了选择退出的选项。以前的版本几乎没有。如果作品培训侵犯版权，我们也没有任何迹象表明 OpenAI 试图避免侵犯版权。即使他们确实避开了未经许可的地方，但许多作品的全部或部分内容都在互联网上被盗版，这也可能构成侵权。</p><p>我们需要健全的法规。与此同时，当我们开始实际执行当前的规定时会发生什么？如果城市对 Uber 的每次非法乘车行为进行罚款，很可能会发生同样的事情。</p><p>那么这将走向何方呢？</p><blockquote><p> Ars Technica：为了捍卫其人工智能训练模型，OpenAI 可能不得不声称“合理使用”该公司为训练 ChatGPT 等工具而吸收的所有网络内容。在潜在的《纽约时报》案例中，这意味着证明复制《纽约时报》的内容来制作 ChatGPT 响应不会与《纽约时报》竞争。</p><p>专家告诉 NPR，这对 OpenAI 来说将是一个挑战，因为与 Google Books 不同（Google Books 在 2015 年赢得了联邦版权挑战，因为其书籍摘录并未为实际书籍创造“重要的市场替代品”）ChatGPT 实际上可以为一些网络用户取代时报网站作为其报道来源。</p><p> 《纽约时报》的律师似乎认为这是一个真正的风险，NPR 报道称，6 月份，《纽约时报》领导人向员工发布了一份备忘录，这似乎是对该风险的早期警告。在备忘录中，《纽约时报》首席产品官亚历克斯·哈迪曼（Alex Hardiman）和副总编辑萨姆·多尔尼克（Sam Dolnick）表示，该公司最“担心”的是针对生成式人工智能工具“保护我们的权利”。</p></blockquote><p>如果这就是案件的关键所在，人们的直觉会是《纽约时报》应该输，因为 ChatGPT 不能替代《纽约时报》。这一年半的事情都不知道，怎么能代替新闻网站呢？但 Bing 也存在，并且作为搜索引擎的推动者，这变得不那么令人难以置信。然而，我随后会质疑，替代《纽约时报》的部分绝不依赖《纽约时报》的数据，当然比 Bing 报告包含《纽约时报》文章的实时搜索时的数据要少。如果“能够处理信息”自动与《纽约时报》竞争，那么这就是一个相当广泛的竞争定义。如果说有什么不同的话，那就是谷歌图书对实际图书的威胁似乎比这要大得多。</p><p>然而，这种论点随后会扩展到任何其工作被人工智能模型取代的人。因此，如果这是原则，并且训练本质上是侵犯版权，那么它们可能会被干掉，即使一个失败的案例也会使整个训练无效。他们负担不起，而且很难预防。谷歌或许有实力，但这并不容易。形象模特将陷入一个受伤的世界，竞争的影响是难以否认的。</p><p>这一切看起来都是那么荒唐。如果我从书中学到一些东西，是否侵犯了版权？如果我以后用这些知识说相关的话呢？为什么这有什么不同？</p><p>当然，是什么让搜索引擎变得很好，但这不好呢？看起来倒退了。</p><p>我可以看到赔偿的情况，特别是对于不是免费提供的物品。一个人需要付费才能使用它，如果你的模型要使用它，然后实例化一百万个副本，你可能应该比一个人支付更多的费用。我也可以看到反对的案例。</p><p>话又说回来，法律就是法律。<a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=10tcb1RfOE4">法律是什么</a>？当规则不公平时会发生什么？我们都知道我们接下来要去哪里。到痛苦之家。</p><p>最终，我们必须拭目以待。我们不应该操之过急。 <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Thomas42/will-the-new-york-times-sue-openai">Manifold 认为《纽约时报》只有 33% 的人会在 2023 年就此提起诉讼</a>。</p><h4>介绍</h4><p>栅栏研究。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1692680900469780682">以下是创始人杰弗里·拉迪什 (Jeffrey Ladish)</a><a target="_blank" rel="noreferrer noopener" href="https://t.co/tAsqB9X8PK">在其网站上</a>发布的公告。</p><blockquote><p>在过去的几个月里，我一直致力于建立一个新的组织，Palisade Research</p><p>来自我们网站的一些内容：</p><p>在 Palisade，我们的使命是帮助人类找到通往符合人类价值观的强大人工智能系统的最安全途径。我们目前的方法是研究进攻性人工智能能力，以更好地理解和传达代理人工智能系统带来的威胁。</p><p>许多人听说过人工智能风险场景，但不明白它们如何在现实世界中发生。假设的人工智能接管场景通常听起来难以置信，或者与科幻小说的模式相匹配。</p><p>很多人不知道的是，目前最先进的语言模型拥有强大的黑客能力。他们可以被引导寻找代码中的漏洞、创建漏洞并危害目标应用程序和机器。他们还可以利用庞大的知识库、工具使用和写作能力来创建复杂的社会工程活动，而只需少量的人工监督。</p><p>未来，寻求权力的人工智能系统可能会利用这些功能来非法获取计算和财务资源。当前AI系统的黑客能力代表了未来AI能力的绝对下限。我们认为，如果与我们期望未来寻求权力的人工智能系统拥有的规划和执行能力相结合，我们可以证明当前系统中潜在的黑客攻击和影响能力已经构成了重大的接管风险。</p><p>目前，该项目由我（主任）、我的朋友凯尔（财务主管、兼职管理员）和我的四位（令人惊叹的）SERI MATS 学者卡琳娜、普拉纳夫、西蒙和蒂莫西组成。我还想聘请一名研究/执行助理和 1-2 名工程师，如果您有兴趣，请联系我们！我们可能很快就会分享一些有趣的结果。 [找到我们] <a target="_blank" rel="noreferrer noopener" href="https://palisaderesearch.org">https://palisaderesearch.org</a></p></blockquote><p>我总是担心这些努力会将人们对人工智能灭绝风险的看法与特定场景或威胁类别联系起来，从而使人们认为，如果他们停止特定提议链上的任何点，他们就可以使风险无效。或者只是他们错过了中心点，因为黑客技能不太可能成为杀死我们的必要组成部分，即使它们足以或让事情发生得更快。</p><h4>在其他人工智能新闻中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.cnbc.com/2023/08/20/singapore-workers-adopting-ai-skills-at-the-fastest-pace-linkedin.html">LinkedIn 报告称，</a>新加坡的“人工智能扩散率”最高，其次是芬兰、爱尔兰、印度和加拿大。这是通过在个人资料中添加人工智能相关技能的人数乘数来衡量的。这似乎不是一个衡量这一点的好方法，至少它过于关心起始利率，这可能会惩罚美国。</p><h4>安静的猜测</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1694040962786541895">Anthropic 的杰克·克拉克注意到他很困惑</a>。他提出了很好的问题，我会提出我的看法。</p><blockquote><p> Jack Clark：2023 年人工智能政策中令人困惑的事情：</p><p> – 人工智能开发应该集中还是分散？</p></blockquote><p>尽管存在困难的极端情况，但这一点对我来说似乎相对清楚。为了安全并避免扩散和错误形式的竞争动态，必须集中开发前沿模型和其他危险的前沿能力。普通实用应用程序的开发应该是分散的。</p><blockquote><p> – 安全是“目的证明手段合理”的模因吗？</p></blockquote><p>通用人工智能的发展对人类来说是一种灭绝的风险。我们很有可能很快就会开发出这样的系统。杰克·克拉克似乎不太相信，他说“我个人认为人工智能安全是一个真正的问题”，尽管这应该已经足够了。</p><blockquote><p>杰克·克拉克：但当我思考对此问题的各种极端政策反应时，我发现自己停了下来——我不断问自己“肯定有其他方法可以提高生态系统的安全性，而不需要在准入或包容性方面做出重大牺牲”？</p></blockquote><p>我真的真的不认为有。如果我们需要做的就是做出这些牺牲来解决这个问题，我会高兴得跳起来，一旦我们度过了严重的风险阶段，我们就可以弥补这些牺牲。 Anthropic 甚至 OpenAI 已经在这些方面做出妥协，以解决现有普通系统的普通风险，并且许多（也许是大多数）其他强大的技术也面临着类似的权衡和挑战。</p><p> （例如，“我们当然可以在不为获取或包容性做出重大牺牲的情况下确保医疗药品安全吗？”好吧，FDA Delenda Est，但不是真的，不。这是一组非常有限且内在化的高度不存在的危险。考虑到许多其他示例，包括访问 AGI 使您可以访问的技术，无论是新的还是现有的。）</p><p>权衡是一件事。如果我们想从这一转变中幸存下来，我们就必须在潜在效用和各种形式的“访问”方面做出重大牺牲。再说一遍，这并不是什么新鲜事。人们可能称之为各种形式的“人类协调问题”，这些问题共同使我们损失了大部分潜在的生产力和效用，当人们试图欺骗这些成本时，他们会发现为什么这样做是一个糟糕的主意。</p><p>问题清单恢复。</p><blockquote><p> – 分布式训练和低成本微调有多少“活力”？</p></blockquote><p>我也担心这个问题。</p><p>正如 Jack 指出的那样，微调非常便宜，而 Lora 是一种非常有效的技术。 Llama-2 的“安全”功能在几天之内就被那些不想要它们的人禁用了。一个开源的系统，或者任何人都可以进行任意微调的系统，是一个不安全的系统，无法使其安全。如果有足够的能力，这就是一种灭绝风险，并且有些人会故意试图使这种风险显现出来。这种能力足够强的系统不能采取这样的形式、时期，如果这意味着一定程度的监视和控制，那么这就是一个不幸的现实。</p><p>令人担忧的是，正如杰克的其他链接所示，也许这种危险的系统很快就能在没有尖端硬件的情况下以分布式方式进行训练。这是一个大问题，在我们能够处理结果之前，这些努力是否能够足够快地扩大规模。我现在的猜测是，目前这样的努力将远远落后，但我希望我对此更有信心，而这种优势只能赢得有限的时间。</p><blockquote><p> – 今天的技术是否足够好，以至于我们不需要依赖“黑天鹅”跳跃来获得超强大的人工智能系统？</p></blockquote><p>只有一种方法可以找出答案。我认识很多人都说是的，在图表上画线并谈论惨痛的教训。我知道很多其他人都说不，或者至少持怀疑态度。我觉得这两个答案都有道理。</p><p>我确实认为“如果我们找到效率提高 5 倍的架构会怎样”的担忧并不是真正的问题。这还不到一个数量级，而且现在的算法改进还不到两年的典型时间，只将时间线向前推进了一年左右，除非我们会陷入停滞。我们还没有的大型游戏将具有新的质量可供性，而不是乘数，除非乘数很大。</p><blockquote><p> – 进步总是需要非正统的策略吗？进展是否可以被阻止、减慢或精心设计？</p></blockquote><p>这似乎越来越不真实，因为进一步的进步需要大量的工程和累积技能以及协作和实验，而进展研究人们相当合理地怀疑这是最近普遍缺乏这种进展的关键原因。我们并没有看到太多独狼人工智能的进步，我们更多地看到像 OpenAI 和 Anthropic 这样充满专家的公司正在开展工作并建立专有技能包。执行此类操作在计算和数据等方面的成本上升也导致了这一点。</p><p>当然，我们有可能会在意想不到的地方看到巨大的进步，但这也是大部分希望的来源。如果先进的新方法具有根本不同的架构和设计，也许它的命运不会那么糟糕。因此，这样一个计划的风险似乎并没有那么严重。</p><blockquote><p> – 人工智能开发者需要从社会获得多少许可才能不可逆转地改变社会？</p></blockquote><p>这取决于社会。</p><p>我认为人工智能开发者和其他人一样，有很大的责任去考虑他们行为的后果，而不是做让世界变得更糟的事情，甚至努力让世界变得更好，最好是尽可能更好。</p><p>这与请求许可不同，也与相信普通人、专家或有权代表你做出这些决定的人不同。作为变革的推动者，您的任务是弄清楚您的变革是否是一个好主意，以及哪些规则和限制（如果有）可以更广泛地倡导或强加给自己。</p><p>相反，社会的工作是决定需要对那些改变我们世界的人施加什么规则和限制。大多数时候，我们对做事施加了太多这样的限制，而提供价值并稍后解决问题的技术理念是正确的。其他时候，存在真正的外部性和外部风险，我们需要确保考虑到这些因素。</p><p>当你的新技术可能杀死所有人时，那就是这样的时刻之一。我们需要对前沿模型的开发和其他灭绝级别的威胁进行监管。在实践中，这可能必须扩展到某种形式的计算治理才能有效。</p><p>对于不具备这种特性、不存在导致所有人死亡或失去人类控制的风险的人工智能部署，我们应该像对待任何其他技术一样对待它们。密切关注未计入价格的外部因素，尤其是那些未参加的人所面临的风险，确保关键的社会利益，并确保每个人都得到公平的补偿，但主要是让人们做他们想做的事。</p><blockquote><p>这是我目前感到很困惑的一些事情，<a target="_blank" rel="noreferrer noopener" href="https://t.co/lZOEUxDO8G">在这里写下一些想法</a>。</p><p>如果你也对这些事情感到困惑，可以给我留言！我正在回复今天迄今为止收到的一堆电子邮件，并安排了在旧金山/东湾的大量IRL咖啡。很高兴聊天！让我们一起在公共场合对人工智能政策更加困惑。</p></blockquote><p>当我有时间的时候，我肯定会接受他的提议，遗憾的是我在最近的旅行之前没有看到这一点，所以我们必须远程进行。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShaneLegg/status/1693673161353478474">与 Shane Legg、Simeon 和 Gary Marcus 讨论的时间表（AGI）</a> 。正如 Shane 指出的，他们相差并不远，Shane 在 13 年内 AGI（定义为在大多数认知任务上达到人类水平或更高水平）达到了 80%，Marcus 为 35%。对于某些目的来说，这是一个很大的差异，而对于其他目的来说，这是一个很小的差异。</p><h4>寻求健全的法规</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neil_chilson/status/1692191025949773824">FTC 只是在问谁</a>在问问题。</p><blockquote><p> Neil Chilson：“此外，委员会已要求 OpenAI 提供‘攻击’及其来源的描述。换句话说，FTC 希望 OpenAI 公布所有敢于提出错误问题的用户的名字。” – 来自我与<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ckoopman">@ckoopman</a>的文章，内容涉及<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FTC">@FTC</a>对<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a>的调查对言论自由的影响。</p><p>帖子：隐藏在<a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf">FTC 给 OpenAI 的要求信</a>的第 13 页中，该委员会要求提供“所有已知的实际或尝试的‘即时注入’攻击实例”。该委员会将提示注入定义为“任何未经授权的尝试绕过过滤器或使用提示操纵大型语言模型或产品，导致模型或产品忽略先前的指令或执行开发人员无意的操作。”至关重要的是，该委员会未能定义“攻击”。</p></blockquote><p>这是一个相当疯狂的要求。那么，任何试图让模型做任何 OpenAI 不希望它做的事情的人，政府都会想要一份记录吗？是的，我也许可以看到一些隐私问题和一些言论自由问题等等。我也认为这是终极钓鱼探险，其想法是 FTC 希望 OpenAI 识别出少数看起来最糟糕的响应，这样 FTC 就可以用它们来限制 OpenAI 或至少罚款它们，理由是它太糟糕了当“错误信息”发生或不发生时。</p><p>整个事情绝对有一种“ <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras">不是这样</a>”的氛围。这正是最坏的情况，能力继续有增无减，但它们夺走了我们美好的东西。</p><h4>音频周</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Kcm51luS9J0&amp;ab_channel=LivBoeree">约瑟夫·高登-莱维特在《双赢》节目中讨论了人工智能和电影业，</a>其中包括一段关于人工智能女友和成瘾危险的流传片段。期待整集。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ZP_N4q5U3eE&amp;ab_channel=80%2C000Hours">Jan Leike 的 80,000 小时播客现在有视频。</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">迈克尔·韦伯 (Michael Webb) 在 80,000 个小时中讨论了人工智能的经济影响</a>。昨天出来了，还没看。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/podcast/robert-trager-on-ai-governance-and-cybersecurity-at-ai-companies/">Robert Trager 在 FLI 播客上谈论人工智能公司的国际人工智能治理和人工智能安全</a>。对这些问题的每一种看法都有一些不同的细微差别。这仍然大部分相同，似乎优先级较低。</p><h4>没有人会傻到</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EricElmoznino/status/1693476716583514566">让人工智能有意识</a>？</p><p>这是本周<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.08708">论文的主题，作者包括 Robert Long</a> 、Patrick Butlin 和第二作者 Yoshua Bengio。</p><blockquote><p>罗伯特·朗：人工智能系统很快就会有意识吗？ @patrickbutlin 和我与神经科学、人工智能和哲学领域的领军人物合作，为这个主题带来科学的严谨性。</p><p>我们的新报告旨在为未来的研究提供全面的资源和计划。</p><p>无论有意识的人工智能在短期内是否是一个现实的前景——我们相信它是——复杂的社交人工智能的部署将使许多人相信人工智能系统是有意识的。我们迫切需要采取严格、科学的方法来解决这个问题。</p><p>许多人对人工智能意识感兴趣。但对人工智能意识的严格思考需要神经科学、人工智能和哲学方面的专业知识。因此它常常会在这些学科的缝隙之间溜走。</p><p>关于人工智能意识的讨论常常是激烈且两极分化的。但意识科学为我们提供了实证研究这个问题的工具。在本报告中，我们利用著名的意识理论来详细分析几个现有的人工智能系统。</p><p>大型语言模型主导了有关人工智能意识的对话。但这些系统甚至不一定是当前意识的最佳候选系统。我们需要研究更广泛的人工智能系统。</p><p>我们采用关于意识的计算功能主义作为一个合理的工作假设：我们将意识理论解释为指定哪些计算与意识相关——无论是在生物神经元中还是在硅中实现。</p><p>我们将意识理论解释为指定哪些计算与意识相关。在我们将这些主张应用到人工智能系统之前，我们需要使其更加精确。在报告中，我们从几个著名理论中提取了 14 个意识指标。 </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/qundidwjg0v7hgsstc8c" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed1f32e-1513-4b93-b140-49c373bd055e_952x966.jpeg" rel="noreferrer noopener"></a></p><p>一些人工智能意识“测试”，例如图灵测试*，旨在在意识理论之间保持完全中立，只关注外在行为。但考虑到人工智能系统和生物有机体之间的差异，仅考虑行为可能会产生误导。</p><p>对于每种意识理论，我们详细考虑人工智能系统需要什么才能满足该理论：循环处理理论、预测处理、全局工作空间理论、高阶理论和注意图式理论。</p><p>我们使用我们的指标来检验有意识的人工智能系统的前景。我们的分析表明，当前的人工智能系统都不是有意识的，但也表明构建有意识的人工智能系统不存在明显的障碍。</p><p>人们经常声称大型语言模型不可能有意识，因为它们不是具体的代理。但“体现”和“代理”到底是什么意思呢？我们还将这些概念提炼成更精确的计算术语。</p><p>我们所说的“意识”并不是指理性、理解、自我意识或智力，更不用说“一般”或“人类水平”的智力。与动物一样，人工智能系统可能具有意识，但缺乏人类水平的认知能力。</p><p>关于人工智能意识，人们很容易陷入非黑即白的立场：要么“我们不知道也无法了解关于意识的任何事情”，要么“这就是我最喜欢的理论如何使答案显而易见”。我们可以而且必须做得更好。</p><p>因此，这份报告远非这些主题的最终定论。事实上，我们呼吁研究人员纠正和扩展我们的方法，挑战和完善我们的假设，并提出替代方法。</p><p>对人工智能意识没有明显的“预防”立场。双方都存在重大风险：过度或低估人工智能系统的意识都可能造成严重伤害。不幸的是，这两种错误都有强烈的诱因。</p><p>我们强烈建议支持对人工智能意识的进一步研究：完善和扩展我们的方法，开发替代方法，并为有意识的人工智能系统的社会和伦理影响做好准备。</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rgblong/status/1693700916418052539">此处是合著者个人资料链接</a>。]</p></blockquote><p>是的，有些人将此视为一种愿望。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693841864775061723">Kevin Fisher：</a> Open Souls 将完全模拟人类意识 老实说，我们距离实现一些高阶理论至少并不遥远——HOT-3 是我们现在在内部定期进行实验的东西。</p></blockquote><p>我担心这里存在重大的路灯下寻找钥匙效应？</p><blockquote><p>我们研究人工智能意识的方法有三个主要原则。首先，我们采用计算功能主义，即执行正确类型的计算对于意识来说是必要且充分的，作为一种工作假设。这篇论文是心灵哲学中的主流立场（尽管存在争议）。我们出于务实的原因采用这一假设：与竞争对手的观点不同，它意味着人工智能中的意识原则上是可能的，并且研​​究人工智能系统的工作原理与确定它们是否可能有意识相关。这意味着，如果计算功能主义成立，那么考虑对人工智能意识的影响将会是富有成效的。</p></blockquote><p>这似乎表明我们正在使用这个理论，因为它可以对哪些系统有意识或无意识有一个可衡量的意见。这并不能说明它是真是假。这是真的吗？如果属实的话，规模大吗？</p><p>在这里仅仅说“我不知道”似乎是不够的。这更像是“如果我知道这些实际上意味着什么或其中任何一个是如何工作的，更不用说如果我知道了如何处理这些信息。”</p><p>我在各个层面上都按下了红色的大红色“我注意到我很困惑”按钮。</p><p>我们是否使用我们不理解的词语，希望它能让我们理解我们也不理解的概念和偏好？我担心我们正在把“决定我们是否关心这个”的责任转移到这个令人困惑的词上，这样我们就可以假装这正在解决我们的困惑。无论如何，我们真正关心什么，或者我们真正应该关心什么？</p><p>我不认为他们在该图表上提供的理论令人信服，但我没有更好的理论。</p><p>在 4.1 中，他们考虑了错误答案的危险。从本质上讲，如果我们认为应该关心有意识的人工智能而不是无意识的人工智能，那么我们可能会选择错误地关心或不关心人工智能及其体验。</p><p>我也认为这是让人工智能有意识的一个巨大危险。如果人们开始将道德重要性赋予人工智能的体验，那么来自各种道德和哲学理论的各种各样的人将使整个“人人不死”的事业变得更加困难。同时，如果我们构建某些人工智能，我们就必须赋予它们的经验以道德分量，而且如果我们这样做，那么这将直接并迅速地导致人类灭绝。如果我们认为以任何方式和出于任何原因调整人工智能在道德上不可接受，或者如果我们在实践中部署人工智能的方式也是如此，那么这与我们不知道如何调整人工智能没有什么不同那个人工智能。我们必须足够明智，找到一种方法，从一开始就不要建造它。</p><p>与此同时，该论文明确表示，许多人，包括其一些作者，一直在故意尝试将意识注入机器中，希望这能增强它们的能力。如果它有效的话，这似乎相当令人震惊。</p><p>事实上，推荐部分一开始就注意到很多人恳求我们不要让我们的人工智能系统有意识。然后他们说：</p><blockquote><p>然而，我们确实建议支持意识科学及其在人工智能中的应用的研究（如 AMCS 关于该主题的公开信中的建议；AMCS 2023），以及在评估人工智能中的意识时使用理论重的方法。</p></blockquote><p>可以说，有必要研究意识，以避免意外地创造意识。如果我们能够采取这种方法，那似乎是明智的。这似乎并不是这里所提倡的足够明确的内容，但他们确实认识到了这个问题。他们表示，建立这样一个系统“不应该轻易完成”，这样的研究可以实现或做到这一点，并呼吁减轻这种风险。</p><p>我建议也许我们应该尝试编写一个可执行的规则来阻止这种情况的发生，但这似乎很难，因为我们完全不知道这该死的东西到底是什么。既然如此，我们该如何预防呢？</p><h4>调整比人类更聪明的智能是很困难的</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1693527190372040953">Davidad 警告不要对迭代蒸馏放大过于兴奋</a>，尽管他认为这是一个很好的调整计划，先生。</p><blockquote><p> Arjun Guha：法学硕士非常擅长 Python 和其他非常流行的 PL 的编程任务。但是，它们在 OCaml 或 Racket 等手工 PL 中通常表现不佳。我们想出了一种方法来显着提高低资源语言的 LLM 性能。如果您关心他们，请继续阅读！</p><p>首先——问题是什么？考虑来自<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BigCodeProject">@BigCodeProject</a>的 StarCoder：它在 PL 上的性能与该语言可用的训练数据量直接相关。它的训练数据（The Stack）是 GitHub 上许可代码的可靠数据集。</p><p>那么……我们可以通过在低资源语言上进行更长时间的训练来解决问题吗？但是，这几乎没有什么作用，而且非常耗费资源。 （该图适用于 StarCoderBase-1B。）</p><p>我们的方法：我们将培训项目从 Python 翻译为低资源语言。 LLM (StarCoderBase) 进行翻译并生成 Python 测试。我们将测试编译为低资源语言（使用 MultiPL-E）并运行它们来验证翻译。</p><p> [<a target="_blank" rel="noreferrer noopener" href="https://t.co/hnDwi21LMa">链接到论文</a>] 描述了如何使用它来创建微调集。</p><p> Davidad：我越来越认为<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulfchristiano">@paulfchristiano</a>关于<a target="_blank" rel="noreferrer noopener" href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">“迭代蒸馏放大”</a>的观点是正确的，即使不一定是关于特定的放大结构——分解认知——模型只能递归地调用自己（或人类），没有任何更可靠的事实来源。</p><p> Nora Belrose：对齐白色药丸</p><p>Davidad：遗憾的是，不，这只能解决至少 13 个不同的对齐基本问题中的一个（#7）</p><p> 1. 价值脆弱且难以明确</p><p>2.可修正性是反自然的</p><p>3.关键流程需要危险能力</p><p>4. 目标因分布而被错误概括</p><p>5.工具融合</p><p>6. 关键流程可能需要难以理解的复杂计划</p><p>7. 超级智能可以愚弄人类监管者</p></blockquote><p>[数字8至13]</p><p> Zvi：您能详细谈谈为什么（我想这就是 QT 的原因）您认为 Arjun 的成功是有利于 IDA 发挥作用的证据吗？</p><p> Davidad：这是该模式的一个实例：1. 采用现有的 LLM，2. 将其“放大”为更大数据流的一部分（在本例中包括仅用于单元测试的手写真实翻译器、目标语言环境、两次 LLM 通话等） 3. 将其“提炼”回 LLM</p><p>当 Paul 在 2015 年提出 IDA 时，我认为有两个高度推测性的前提。 1. 反复放大和提炼预测变量是获得能力的一种有竞争力的方式。 2.因子认知：HCH数据流尤其是超级智能完整的。这里的证据是1。</p><p> Arjun 的方法对于这个特定问题是有意义的。使用人工智能通过本质上的翻译来创建合成数据似乎是一种潜在增强替代语言（计算机和人类）技能的绝佳方法。这似乎也是在数据集中创建统计平衡、消除不需要的相关性并根据需要调整基本比率的绝佳方法。我很好奇这对消除偏见有什么作用。</p><p>我仍然对 IDA 持怀疑态度，并且不认为这与 IDA 足够相似，特别是当希望做诸如保持足够强大和准确的对齐之类的事情时，但将其细节作为其自己的未来帖子更好。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1692591486511014355">值得记住。</a></p><blockquote><p> Roon：有时候说一些错误的蠢话确实是天才的一个标准。那些说零愚蠢事情的人并不是根据第一原则进行推理，也没有冒险，并且下载了所有“正确”的观点。</p></blockquote><h4>人们担心人工智能会杀死所有人</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation">Vox 的 Sigal Samuel 总结了上周的民意调查结果</a>，显示普通人希望整个人工智能开发速度慢下来。杰克·克拉克也注意到了精英意见和大众意见之间的这种分歧。</p><h4>轻松的一面</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1693106846465442110">还没有，还不是。</a></p><blockquote><p> Daniel Eth：“这个 AGI 东西感觉太聪明了一半”是的，这就是问题所在！</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time<guid ispermalink="false"> kLa3HmkesF5w3MFEY</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 24 Aug 2023 15:30:10 GMT</pubDate> </item><item><title><![CDATA[Is this the beginning of the end for LLMS [as the royal road to AGI, whatever that is]?]]></title><description><![CDATA[Published on August 24, 2023 2:50 PM GMT<br/><br/><p>这很难说，但它确实……我们可以说……有趣吗？</p><p>早在 2020 年夏天，当 GPT-3 发布时，我写了一篇工作论文： <a href="https://www.academia.edu/43787279/GPT_3_Waterloo_or_Rubicon_Here_be_Dragons_Version_4_1">GPT-3：滑铁卢还是卢比孔河？这里是龙</a>。我的目标是让自己相信底层技术不仅仅是一些奇怪的统计侥幸，事实上正在发生一些具有重大意义和价值的事情。在我看来，我成功了。但我也对此表示怀疑。</p><p>以下是我在该工作文件的第一页上写的内容，甚至在摘要之前：</p><blockquote><p> <i><strong>GPT-3 是一项重大成就。</strong></i></p><p><i>但我担心创建它的社区可能会像其他社区以前所做的那样——20世纪60年代中期的机器翻译、20世纪80年代中期的符号计算——胜利地走过悬崖边缘，并发现自己自豪地站在悬崖边上。空气。</i></p><p><i>这是没有必要的，当然也不是不可避免的。</i></p><p><i>在技​​术文献和各种复杂程度的评论中，有大量关于 GPT 和 Transformer 的文章。我只读了其中的一小部分。但我读到的任何内容都没有表明对语言或心灵的本质有任何兴趣。人们的兴趣似乎转移到了 GPT 引擎本身。然而该引擎的产品，即语言模型，是不透明的。我相信，如果我们要达到迄今为止所展示的成就水平，我们必须了解该引擎正在做什么，以便我们可以控制它。我们必须思考语言和心灵的本质。</i></p></blockquote><p>我没想到任何对这些事情有影响力的人都会关注我——尽管人们总是希望——但这并不是不写信的理由。</p><p>那是 2020 年和 GPT-3。两年后，ChatGPT 推出并广受好评，这是理所应当的。我确实花了很多时间来玩、研究它并<a href="https://new-savanna.blogspot.com/search/label/ChatGPT">写下它</a>。但我没有忘记2020年的告诫。</p><p>现在我们听到一些传言称事情进展不太顺利。早在 8 月 12 日，一向持怀疑态度的加里·马库斯 (Gary Marcus) 就发帖称，<a href="https://garymarcus.substack.com/p/what-if-generative-ai-turned-out">如果生成式 AI 被证明是个哑弹怎么办？一些可能的经济和地缘政治影响</a>。他的前两段：</p><blockquote><p>除了所谓的快速上升和快速下降的室温超导体 LK-99 之外，我见过的很少有东西比生成人工智能更被炒作了。许多公司的估值都在数十亿美元，新闻报道几乎是恒定的；从硅谷到华盛顿特区再到日内瓦，这是任何人都可以谈论的话题。</p><p>但首先，收入还没有到来，而且可能永远不会到来。估值预计将达到数万亿美元的市场，但据传目前生成式人工智能的实际收入达数亿美元。这些收入确实可以增长 1000 倍，但这只是猜测。我们不应该简单地假设它。</p></blockquote><p>还有他的最后一句话：</p><blockquote><p>如果幻觉无法修复，生成式人工智能每年可能赚不到一万亿美元。如果它每年可能赚不到一万亿美元，那么它可能不会产生人们预期的影响。如果它不会产生这种影响，也许我们不应该围绕它的前提来构建我们的世界。</p></blockquote><p> FWIW, I believe, and have been saying time and again, that hallucinations seem to me to be inherent in the technology. They aren&#39;t fixable.</p><p> Now, yesterday, Ted Gioia, a culture critic with an interest in technology and experience in business, has posted, <a href="https://www.honest-broker.com/p/ugly-numbers-from-microsoft-and-chatgpt">Ugly Numbers from Microsoft and ChatGPT Reveal that AI Demand is Already Shrinking</a> . Where Marcus has a professional interest in AI technology and has intellectual skin the tech game, Gioia is just a sophisticated and interested observer. Near the end of his post, after many links to unfavorable stories, Gioia observes:</p><blockquote><p> ... we can see that the real tech story of 2023 is NOT how AI made everything great. Instead this will be remembered as the year when huge corporations unleashed a half-baked and dangerous technology on a skeptical public—and consumers pushed back.</p><p> Here&#39;s what we now know about AI:</p><ul><li> Consumer demand is low, and already appears to be shrinking.</li><li> Skepticism and suspicion are pervasive among the public.</li><li> Even the companies using AI typically try to hide that fact—because they&#39;re aware of the backlash.</li><li> The areas where AI has been implemented make clear how poorly it performs.</li><li> AI potentially creates a situation where millions of people can be fired and replaced with bots—so a few people at the top continue to promote it despite all these warning signs.</li><li> But even these true believers now face huge legal, regulatory, and attitudinal obstacles</li><li> In the meantime, cheaters and criminals are taking full advantage of AI as a tool of deception.</li></ul></blockquote><p> Marcus has just updated his earlier post with a followup: <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">The Rise and Fall of ChatGPT</a> ?</p><p> The situation is very volatile. I certainly don&#39;t know how to predict how things are going to unfold. In the long run, I remain convinced that <i>if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what these engines are doing so that we may gain control over them. We must think about the nature of language and of the mind.</i></p><p>敬请关注。</p><p> <i>Cross posted from</i> <a href="https://new-savanna.blogspot.com/2023/08/is-this-beginning-of-end-for-llms-as.html"><i>New Savanna</i></a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road<guid ispermalink="false"> h6pFK8tw3oKZMppuC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:50:21 GMT</pubDate> </item><item><title><![CDATA[AI Safety Bounties]]></title><description><![CDATA[Published on August 24, 2023 2:29 PM GMT<br/><br/><p> Earlier this year, Vaniver recommended <a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">Bug Bounties for Advanced ML Systems</a> .<br><br> I spent a little while at Rethink Priorities considering and expanding on this idea, suggesting potential program models, and assessing the benefits and risks of programs like this, which I&#39;ve called &#39;AI Safety Bounties&#39;:</p><h1> Short summary</h1><p> <strong>AI safety bounties are programs where public participants or approved security researchers receive rewards</strong> <strong>for identifying issues within powerful ML systems</strong> (analogous to bug bounties in cybersecurity). <strong>Safety bounties could be valuable for legitimizing examples of AI risks, bringing more talent to stress-test systems, and identifying common attack vectors</strong> .</p><p> <strong>I expect safety bounties to be worth trialing for organizations working on reducing catastrophic AI risks.</strong> Traditional bug bounties seem fairly successful: they attract roughly one participant per $50 of prize money, and have become increasingly popular with software firms over time. The most analogous program for AI systems led to relatively few useful examples compared to other stress-testing methods, but one knowledgeable interviewee suggested that future programs could be significantly improved.</p><p> However, I am not confident that bounties will continue to be net-positive as AI capabilities advance. At some point, I think the accident risk and harmful knowledge proliferation from open sourcing stress-testing may outweigh the benefits of bounties</p><p> <strong>In my view, the most promising structure for such a program is a third party defining dangerous capability thresholds (“evals”) and providing rewards for hunters who expose behaviors which cross these thresholds</strong> . I expect trialing such a program to cost up to $500k if well-resourced, and to take four months of operational and researcher time from safety-focused people.</p><p> I also suggest two formats for lab-run bounties: open contests with subjective prize criteria decided on by a panel of judges, and private invitations for trusted bug hunters to test their internal systems.</p><p> <i>Author&#39;s note: This report was written between January and June 2023. Since then, safety bounties have become a more well-established part of the AI ecosystem, which I&#39;m excited to see. Beyond defining and proposing safety bounties as a general intervention, I hope this report can provide useful analyses and design suggestions for readers already interested in implementing safety bounties, or in better understanding these programs.</i></p><h1> Long summary</h1><h2> Introduction and bounty program recommendations</h2><p> One potential intervention for reducing <a href="https://www.safe.ai/statement-on-ai-risk">catastrophic AI risk</a> is AI safety bounties: programs where members of the public or approved security researchers receive rewards for identifying issues within powerful ML systems (analogous to bug bounties in cybersecurity). In this research report, I explore the benefits and downsides of safety bounties and conclude that <strong>safety bounties are probably worth the time and money to trial for organizations working on reducing the catastrophic risks of AI</strong> . In particular, testing a handful of new bounty programs could cost $50k-$500k per program and one to six months full-time equivalent from project managers at AI labs or from entrepreneurs interested in AI safety (depending on each program&#39;s model and ambition level).</p><p> I expect safety bounties to be <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.8fvvkifaa4t2">less successful for the field of AI safety</a> than bug bounties are for cybersecurity, due to the higher difficulty of quickly fixing issues with AI systems. <strong>I am unsure whether bounties remain net-positive as AI capabilities increase to more dangerous levels</strong> . This is because, as AI capabilities increase, I expect safety bounties (and adversarial testing in general) to <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">potentially generate more harmful behaviors</a> . I also expect the benefits of the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.linmllyxhki">talent pipeline</a> brought by safety bounties to diminish. I suggest <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">an informal way</a> to monitor the risks of safety bounties annually.</p><p> The views in this report are largely formed based on information from:</p><ul><li> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb">Interviews</a> with experts in AI labs, AI existential safety, and bug bounty programs,</li><li> “Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims” by Brundage et al. arguing for “Bias and Safety Bounties” ( <a href="https://arxiv.org/abs/2004.07213">2020, page 16</a> ),</li><li> A report from the Algorithmic Justice League analyzing the potential of bug bounties for mitigating algorithmic harms ( <a href="https://drive.google.com/file/d/1f4hVwQNiwp13zy62wUhwIg84lOq0ciG_/view">Kenway et al., 2022</a> ),</li><li> Reflections from <a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf">the ChatGPT Feedback Contest</a> .</li></ul><p> See the end of the report for <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#references">a complete list of references</a> .</p><p> Based on these sources, I identify three types of bounty programs that seem practically possible now, that achieve more of <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-decrease-catastrophic-risks">the potential benefits</a> of safety bounties and less of the potential risks than alternative programs I consider, and that would provide valuable information about how to run bounty programs if trialed. In order of my impression of their value in reducing catastrophic risks, the three types are:</p><ul><li> <strong>Independent organizations or governments set</strong> <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/"><strong>“evals”-based standards</strong></a> <strong>for undesirable model behavior</strong> , <strong>and members of the public attempt to elicit this behavior</strong> from publicly-accessible models.</li><li> <strong>Expert panels, organized by AI labs, subjectively judge which discoveries of model exploits to pay a bounty for, based on the lab&#39;s broad criteria</strong> .<ul><li> Potentially with an interactive grant-application process in which hunters propose issues to explore and organizers commit to awarding prizes for certain findings.</li><li> Potentially with a convening body hosting multiple AI systems on one API, and hunters being able to test general state-of-the-art models.</li></ul></li><li> <strong>Trusted bug hunters test private systems,</strong> organized by labs in collaboration with security vetters, with a broad range of prize criteria. Certain successful and trusted members of the bounty hunting community (either the existing community of bug bounty hunters, or a new community of AI safety bounty hunters) are granted additional information about the training process, or temporary access - through security-enhancing methods - to additional features on top of those already broadly available. These would be targeted features that benefit adversarial research, such as seeing activation patterns or being able to finetune a model (Bucknall et al., forthcoming).</li></ul><p> I outline more specific visions for these programs <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ls4dvjl9vdc">just below</a> . A more detailed analysis of these programs, including suggestions to mitigate their risks, is in the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs">Recommendations section</a> . This report does not necessarily constitute a recommendation for individuals to conduct the above stress-testing without an organizing body.</p><p> I expect that some other bounty program models would also reduce risks from AI successfully and that AI labs will eventually develop better bounty programs than those suggested above. Nevertheless, the above three models are, in my current opinion, the best place to start. I expect organizers of safety bounties to be best able to determine which form of bounty program is most appropriate for their context, including tweaking these suggestions.</p><p> This report generally focuses on how bounty programs would work with large language models (LLMs). However, I expect most of the bounty program models I recommend would work with <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.gkrngyd24xkg">other AI systems</a> .</p><h2> Why and how to run AI safety bounties</h2><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#decrease-catastrophic-risks"><strong>Benefits</strong></a> <strong>.</strong> AI safety bounties may yield:</p><ul><li> Salient <strong>examples of AI dangers</strong> .</li><li> Identification of <strong>talented individuals</strong> for AI safety work.</li><li> A small number of <strong>novel insights into issues in existing AI systems</strong> .</li><li> A <strong>backup to auditing</strong> and other expert stress-testing of AI systems.</li></ul><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.4pbr7r48u484"><strong>Key variables</strong></a> <strong>.</strong> When launching bounties, organizers should pay particular attention to the prize criteria, who sets up and manages the bounty program, and the level of access granted to bounty hunters.</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-increase-catastrophic-risks"><strong>Risks</strong></a> <strong>.</strong> At current AI capability levels, I believe trialing bounty programs is unlikely to cause catastrophic AI accidents or significantly worsen AI misuse. The most significant downsides are:</p><ul><li> <strong>Opportunity cost</strong> for the organizers (most likely project managers at labs, AI safety entrepreneurs, or AI auditing organizations like the <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/">Alignment Research Center</a> ).</li><li> <strong>Stifling examples of AI risks</strong> from being made public.<ul><li> Labs may require that bounty submissions be kept private. In that case, a bounty program would incentivize hunters, who would in any case explore AI models&#39; edge cases, not to publish salient examples of AI danger.</li></ul></li></ul><p> Trial programs are especially low-risk since the organizers can pause them at the first sign of bounty hunters generating dangerous outcomes as AI systems advance.</p><p> The risks are higher if organizations regularly run (not just trial) bounties and as AI advances. Risks that become more important in those cases include:</p><ul><li> Leaking of sensitive details, such as information about training or model weights.</li><li> Extremely harmful outputs generated by testing the AI system, such as successful human-prompted phishing scams or autonomous self-replication – analogous to gain of function research.</li></ul><p> For these reasons, I recommend the program organizers perform an annual review of the safety of allowing members of the public to engage in stress testing, monitoring:</p><ul><li> Whether, and to what extent, AI progress has made safety bounties (and adversarial testing in general) more dangerous,</li><li> How much access it is therefore safe to give to bounty hunters.</li></ul><p> Further, I recommend not running bounties at dangerous levels of AI capability if bounties seem sufficiently risky. I think it possible, but unlikely, that this level of risk will arise in the future, depending on the level of progress made in securing AI systems.</p><h2> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommendations-for-running-a-safety-bounty-program"><strong>Other recommended practices for bounty organizers</strong></a> <strong>.</strong></h2><p> I recommend that organizations that set up safety bounties:</p><ul><li> <strong>Build incentives</strong> to take part in bounties, <strong>including</strong> <strong>non-financial incentives</strong> . This should involve building infrastructure, such as leaderboards and feedback loops, and fostering a community around bounties. Building this wider infrastructure is most valuable if organizers consider safety bounties to be worth running on an ongoing basis.</li><li> <strong>Have a pre-announced disclosure policy</strong> for submissions.</li><li> <strong>Share lessons learned</strong> about AI risks and AI safety bounty programs with leading AI developers.</li><li> <strong>Consider PR risks</strong> from running safety bounties, and decide on framings to avoid misinterpretation.</li><li> <strong>Independently assess legal risks</strong> of organizing a contest around another developer&#39;s AI system, if planning to organize a bounty independently.</li></ul><p> Outline of recommended models</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs"><i>Recommended models</i></a> <i>, in order of recommendation, for safety bounties.</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#fn1"><sup>1</sup></a> </p><figure class="table"><table style="background-color:rgb(255, 255, 255)"><tbody><tr><td></td><td> <strong>1. Evals-based</strong></td><td> <strong>2. Subjectively judged, organized by labs</strong></td><td> <strong>3. Trusted bug hunters test private systems</strong></td></tr><tr><td> <strong>Target systems</strong></td><td> A wide range of AI systems – preferably with the system developers&#39; consent and buy-in</td><td> Testing of a particular AI model – with its developer&#39;s consent and engagement</td><td> Testing of a particular AI model – preferably with its developer&#39;s consent and buy-in</td></tr><tr><td> <strong>Prize criteria</strong></td><td> Demonstrate (potentially dangerous) capabilities beyond those revealed by testers already partnering with labs, such as ARC Evals</td><td><p> Convince a panel of experts that the issue is worth dedicating resources toward solving.</p><p>或者</p><p>Demonstrate examples of behaviors which the AI model&#39;s developer attempted to avoid through their alignment techniques.</p></td><td> A broad range of criteria is possible (including those in the previous two models).</td></tr><tr><td> <strong>Disclosure model – how private are submissions?</strong></td><td> Coordinated disclosure (Organizers default to publishing all submissions which are deemed safe)</td><td> Coordinated disclosure</td><td> Coordinated- or non-disclosure</td></tr><tr><td> <strong>Participation model</strong></td><td>民众</td><td>民众</td><td>Invite only</td></tr><tr><td> <strong>Access level</strong></td><td> Public APIs</td><td> Public APIs</td><td> Invited participants have access to additional resources – eg, additional non-public information or tools within a private version of the API</td></tr><tr><td> <strong>Who manages the program</strong></td><td> Evals organization (eg, ARC Evals), a new org., or an existing platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td></tr><tr><td> <strong>Program duration</strong></td><td> Ongoing</td><td> Ongoing</td><td> Time-limited</td></tr><tr><td> <strong>Prize scope</strong> (how broad are the metrics for winning prizes)</td><td> Targeted</td><td> Expansive</td><td>中等的</td></tr><tr><td><strong>Financial reward per prize</strong></td><td> High (up to $1m)</td><td> Low (up to $10k)</td><td> Medium (up to $100k)</td></tr><tr><td> <strong>Pre- or post- deployment</strong></td><td> Post-deployment</td><td> Post-deployment</td><td> Potentially pre-deployment</td></tr></tbody></table></figure><h1> <strong>Acknowledgments</strong> </h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iXECTEyC5PQuYM2aJ/kjalaionq6romubhjxre"></p><p><br> <i>This report is a project of</i> <a href="https://rethinkpriorities.org/"><i><u>Rethink Priorities</u></i></a> <i>–a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Patrick Levermore. Thanks to Ashwin Acharya and Amanda El-Dakhakhni for their guidance, Onni Aarne, Michael Aird, Marie Buhl, Shaun Ee, Erich Grunewald, Oliver Guest, Joe O&#39;Brien, Max Räuker, Emma Williamson, Linchuan Zhang for their helpful feedback,</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb"><i>all interviewees credited in the report</i></a> <i>for their insight, and Adam Papineau for copyediting.</i></p><p><br> <i>If you are interested in RP&#39;s work, please visit our</i> <a href="https://www.rethinkpriorities.org/research"><i><u>research database</u></i></a> <i>and subscribe to our</i> <a href="https://www.rethinkpriorities.org/newsletter"><i><u>newsletter</u></i></a> <i>.</i></p><p></p><p> <strong>I would be happy to discuss setting up AI safety bounties with those in a position to do so.</strong> I can provide contacts and resources to aid this, including <a href="https://bit.ly/SafetyBountiesWorkbook"><u>this workbook</u></a> . Contact me at patricklevermore at gmail dot com.</p><p></p><p> Full report: <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties">AI Safety Bounties</a></p><br/><br/><a href="https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties<guid ispermalink="false"> iXECTEyC5PQuYM2aJ</guid><dc:creator><![CDATA[PatrickL]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:30:00 GMT</pubDate> </item><item><title><![CDATA[AI Regulation May Be More Important Than AI Alignment For Existential Safety]]></title><description><![CDATA[Published on August 24, 2023 11:41 AM GMT<br/><br/><p><i><strong>简介</strong>： 调整一个强大的人工智能是不够的：只有没有人能够构建一个未调整的强大人工智能，我们才是安全的。 Yudkowsky 试图通过关键行动来解决这个问题：第一个对齐的人工智能会做一些事情（例如熔化所有 GPU），以确保任何人都无法构建未对齐的人工智能。然而，这些实验室目前显然不打算实施一项关键行动。这意味着调整通用人工智能虽然创造了大量价值，但不会降低存在风险。相反，全球硬件/数据监管才是降低生存风险所需要的。因此，那些旨在降低人工智能存在风险的人应该关注人工智能监管，而不是人工智能联盟。</i></p><p><i><strong>认知状态</strong>：我多年来一直在思考这个问题，同时专业致力于降低 x 风险。我想我了解有关该主题的大多数文献。我还与相当多的专家讨论了这个话题（他们在某些情况下似乎同意，而在其他情况下似乎不同意）。</i></p><p><i><strong>感谢</strong>David Krueger、Matthijs Maas、Roman Yampolskiy、Tim Bakker、Ruben Dieleman 和 Alex van der Meer 提供的有益对话、评论和/或反馈。这些人不一定同意这篇文章中表达的观点。</i></p><p><i>这篇文章主要是关于接管造成的人工智能 x 风险。</i>它对于<a href="https://arxiv.org/abs/2306.12001"><i><u>其他类型的人工智能 x 风险</u></i></a><i>可能有效，也可能无效</i><i>。这篇文章主要是关于人工智能存在风险的“最终游戏”，而不是中间状态。</i></p><p>人工智能的存在风险是一个进化问题。正如埃利泽·尤德科斯基（Eliezer Yudkowsky）等人指出的那样：即使存在安全的人工智能，这些也无关紧要，因为它们不会阻止其他人构建危险的人工智能。安全人工智能的例子可以是预言机或满足者， <a href="https://www.lesswrong.com/posts/2qCxguXuZERZNKcNi/satisficers-want-to-become-maximisers"><u>只要</u></a>事实<a href="https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness"><u>证明</u></a>可以将这些人工智能类型与高智能结合起来。但是，正如尤德科斯基<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>所说</u></a>：“如果您需要的只是一个不会做危险事情的物体，那么您可以尝试海绵”。即使有限的人工智能是安全的人工智能，它也不会降低人工智能的存在风险。这是因为在某些时候，有人会创建一个具有无限目标的人工智能（创建尽可能多的回形针，以无限的准确性预测句子中的下一个单词等）。这是会杀死我们的人工智能，而不是安全的人工智能。</p><p>这是人工智能存在风险问题的进化本质。 It is described excellently by Anthony Berglas in his underrated <a href="https://www.amazon.com/When-Computers-Can-Think-Intelligence/dp/1502384183"><u>book</u></a> , and more recently also in Dan Hendrycks&#39; <a href="https://arxiv.org/abs/2303.16200"><u>paper</u></a> .这个进化部分是人工智能存在风险的一个基本且非常重要的属性，也是这个问题之所以困难的很大一部分原因。然而，人工智能协调和行业中的许多人似乎只专注于协调单个人工智能，我认为这是不够的。</p><p>尤德科斯基旨在通过所谓的关键行动来解决这一进化问题（事实上，任何人都不应构建不安全的人工智能）。一个一致的超级智能不仅不会杀死人类，而且还会执行一项关键行动，举个玩具例子，就是<a href="https://forum.effectivealtruism.org/posts/iGYTt3qvJFGppxJbk/ngo-and-yudkowsky-on-alignment-difficulty"><u>融化全球所有 GPU</u></a> ，或者正如他后来所说，巧妙地改变全球所有 GPU，使它们不再被使用创建一个 AGI。通过确保任何人永远不会创造出不安全的超级智能，这将是真正拯救人类免于灭绝的行为（可能有人会说，需要熔化所有 GPU 以及所有其他可以运行人工智能的未来硬件）由一致的超级智能无限期地完成，否则即使是关键行为也可能是不够的）。</p><p>然而，关键行动的概念似乎已经彻底<a href="https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"><u>过时了</u></a>。领先的实验室、人工智能治理智囊团、政府等都没有谈论或显然思考过这个问题。相反，他们似乎正在<a href="https://openai.com/blog/governance-of-superintelligence"><u>考虑</u></a>防扩散和多种监管等问题，以确保强大的人工智能不会落入坏人之手。这可能意味着任何人都可以在没有安全措施的情况下有意或无意地运行它。我将这样的解决方案（特别是任何能够限制任何参与者在任何时间段内访问高级人工智能的解决方案）称为“人工智能监管”。</p><p>这个解决方案似乎已经成为主流，具有重要的影响：</p><ul><li>即使我们能够解决一致性问题，我们仍然需要人工智能监管，因为否则大量的不安全行为者就有可能在没有适当安全性的情况下运行超级智能，从而冒着被接管的风险。</li><li>无论如何，如果我们有人工智能监管，我们也可以用它来拒绝<i>所有人</i>（包括领先的实验室）而不是几乎所有人访问先进的人工智能。这相当于人工智能暂停。</li><li>如果我们能够拒绝每个人使用先进的人工智能，并且我们能够继续这样做，我们就已经解决了人工智能的存在风险，而且还没有解决人工智能的对齐问题。</li><li>在不执行关键行动的情况下成功调整超级智能几乎不会改变需要制定的法规，因为除了少数被认为安全的实验室之外的所有其他实验室仍然需要这些法规。</li></ul><p>因此，如果没有关键的行动，保证我们安全的就是监管。人们可能仍然希望调整超级智能来使用其力量，但不是为了防止存在风险。使用超级智能的力量当然可能是追求联盟的一个正当理由：它可以使我们的经济飞速发展，创造富足，治愈疾病，增强政治权力等。尽管这些巨大且极其复杂的转变的净积极性可能很难证明事先，这些当然可能是进行协调工作的正当理由。然而，在这种情况下，我们这些对预防存在风险而不是构建人工智能感兴趣的人应该关注监管，而不是协调。后者也可能留给业界，以及证明由此产生的一致人工智能确实安全的责任。</p><p>除了人工智能监管的这种场景之外，还有一种选择可以解决人工智能存在风险的完整进化问题。有些人认为，一致的超级智能可以成功地、无限期地保护我们免受不一致的超级智能的侵害。我将这种选择称为积极的进攻/防御平衡，它将是继联盟+关键行动和持久监管之后的第三种方式，以防止人类在较长时期内灭绝。然而，大多数人<a href="https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"><u>似乎并不认为</u></a>这是现实的（有明显的<a href="https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption"><u>例外</u></a>）。</p><p>这三种解决人工智能生存风险演化本质的方式（人工智能联盟+关键行为、人工智能监管、防御>;进攻）可能并不是人工智能生存风险演化问题的完整解决方案，而且三者之间存在交叉点。关键行为可能被视为赢得进攻/防守平衡的一种（非常严格且非法的）类型。国家行为者实施的关键行为可能被视为实施人工智能监管的极端（而且同样是非法的）方式。人工智能（硬件）监管的类型可能是可能的，其中实施监管的国家行为者得到一致的人工智能的帮助，使其实施有点类似于关键行为（在这种情况下可能是合法的）。某些类型的监管也许可以使我们更有可能赢得进攻/防守平衡。</p><p>我认为应该开展研究，旨在为人工智能存在风险的进化问题提供一套完整的解决方案。我预计此类研究会提出比这三个更多的选择，和/或在这三个之间提出更多的混合选项，这可能会指出降低人工智能存在风险的新的、富有成效的方法。</p><p>只要我们假设人工智能存在风险的进化本质只存在三种解决方案，那么重要的是要认识到这三种解决方案似乎都很困难。此外，很难量化每个选项的可能性。因此，对这三者中的任何一个下注都是值得的。</p><p>然而，我个人的赌注是，不幸的是，进攻将胜过防守，并且在具有接管能力的超级智能被开发出来之前解决结盟的可能性，<i>并且</i>这种结盟的超级智能将执行成功的关键行动，比我们解决问题的机会要小。将能够成功协调并实施足够好的硬件或数据监管，特别是如果当前公众对人工智能存在风险的认识不断增强的<a href="https://www.lesswrong.com/posts/3vZWhCYBFn8wS4Tfw/crosspost-ai-x-risk-in-the-news-how-effective-are-recent"><u>趋势</u></a>持续下去的话。这意味着，致力于在全球范围内无限期地限制所有参与者使用先进人工智能的监管类型，只要有必要，就应该是最高的存在优先事项，比协调一致更重要。</p><br/><br/> <a href="https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for<guid ispermalink="false"> 2cxNvPt先生瓦杰尔托</guid><dc:creator><![CDATA[otto.barten]]></dc:creator><pubDate>Thu, 24 Aug 2023 11:41:56 GMT</pubDate> </item><item><title><![CDATA[Simple AI Forecasting - Katja Grace]]></title><description><![CDATA[Published on August 24, 2023 9:45 AM GMT<br/><br/><p><i>我正在采访人工智能专家，了解他们对人工智能将会发生什么的看法。这是 Katja Grace 和她的想法。人工智能风险让我感到害怕，但我常常感到与它脱节。这帮助我思考这个问题。</i></p><p>以下是 Katja 的简要想法：</p><ul><li>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？ 50%</li><li>在世界的哪一部分中，大多数人工智能工具是由代理人工智能控制的？ 70%</li><li>代理人工智能将在哪些领域实现其目标？ 90%</li><li>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？ 50%</li><li>政策会让 AGI 放缓 10 年吗？ 25%，10年可以让我们减少多少坏事？ 30%</li></ul><p>您可以在此处查看交互式图表：https: <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>//estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a>或查看我迄今为止所做的所有图表<a href="https://estimaker.app/ai"><u>https://estimaker.app/ai</u></a> 。本页底部有一张图片。</p><p>您可以在这里观看完整视频： </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Zum2QTaByeo"><div><iframe src="https://www.youtube.com/embed/Zum2QTaByeo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1>更长的解释</h1><p><i>请记住，这是到 2040 年。</i> </p><figure class="table"><table><thead><tr><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">问题</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">信心</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">卡佳的评论</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">我的评论</th></tr></thead><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> Katja 说她不确定，因此给出了 50% 的分数。</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在我关于 AGI 的讨论中，一个常见的失败模式是“AGI”的功能非常模糊。有些人正在设想能够完美地运行全球阴谋的工具，而另一些人似乎想到了人类顶级的编码能力。<br></p><p> Katja 和我选择了这个例子，因为它涉及一系列大规模的任务，并取代了大量的人力。<br></p><p>任务包括：</p><ul><li>大型工程项目</li><li>现金转移</li><li>现实世界的分歧（如何设定税率等）</li><li>复杂的系统问题（交通流量、医疗保健占用）<br></li></ul><p>公务员服务范围涉及数百万人。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在这些世界的哪一部分中，代理控制了大部分资源（即使通过人工智能工具）</p><p><br></p><p><i>如果把代理人工智能控制的资源和它们控制的人工智能工具加起来，是不是超过一半了？</i></p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 70%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>卡佳的直觉是，要实现灭绝，代理可能是必要的。<br></p><p> “我认为其中一些可能具有代理性的原因是，代理或类似代理的东西似乎具有经济价值。人们想要制作像代理这样的东西。他们已经在尝试做代理之类的东西了。”<br></p><p><i>然后</i></p><p></p><p>Katja：“问题在于它们的代理程度如何，而且这似乎更像是一个谱系，而不是能够清楚地说出这是代理还是非代理”<br></p><p> ……<br></p><p> Katja “我不知道，大约在 60% 到 90% 之间。 ……我认为我能想象的世界是，肯定有一些令人印象深刻的代理人工智能系统。但就像，他们并不是真正正在发生的人工智能思维的主体。就像一堆更像是人类正在使用的工具……我觉得问题是谁在运用大部分人工智能认知劳动。是人类或人工智能有好的目标还是人工智能有坏的目标？ “</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>有人有更好的框架吗？我认为 Katja 的声音是我听过的最好的，但相当模糊。<br></p><p>考虑一下美国政府高层如何不仅治理美国，而且治理与美国结盟的国家，并影响全世界的规范。类似地，人工智能代理不仅可以控制某些资源和人工智能工具，还可以控制这些工具控制的内容。问题是〜这是大多数吗？他们是否垄断了电力市场？<br></p><p>我希望更好地解决这个问题。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>代理人工智能将在这些世界的哪一部分实现其目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 90%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> “事情发生得非常快的可能性大约为 0.1% 到 10%。如果这种情况没有发生，从长远来看，如果周围有比我们聪明得多的人工智能代理，他们就有 80% 的可能会夺取所有权力。”</p><p></p><p>值得注意的是，Katja 有一个模型，其中代理人工智能无需接管即可实现其目标 - 他们更有能力，我们慢慢地将权力交给他们。<br></p><p> “我猜[在另一种]情况下[人工智能接管]发生得非常缓慢。我想也许我对这到底有多慢有点不可知。它更像是我指出的那种机制，它可以非常快，但是……没有人喜欢侵犯任何人的财产权。只是，他们非常有能力，并且会因为他们所做的事情而获得报酬，或者……所有的决定都是由人工智能做出的，因为人工智能在做决定方面更有能力。”</p><p></p><p>但这可能不太好。 “我认为在某些情况下，即使是互动的人也可能对此不满意，但竞争迫使你这样做。就像，如果每个人都可以使用某种人工智能系统来管理他们公司的营销，那么我想说，每个人都知道它在这方面会有点狡猾并做一些坏事。他们宁愿不使用它……但如果你不使用它，那么你的公司将无法竞争。所以你必须使用它。”</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>故事的这一部分似乎与最悲惨的模型非常相似——如果你有控制大部分资源的代理人工智能并且没有好的政策（我们稍后会谈到），他们可能会实现他们的目标。那时你只能希望他们的目标是好的。<br></p><p>我很好奇是否有比 Katja 持更悲观态度的人不同意这一点。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Katja 最初选择 25%，但认为这会使总体产出太低，因此改为 50%。</p><p></p><p>在 EAG SF 的演讲中，您可以看到她<a href="https://youtu.be/j5Lu01pEDWA?t=1206"><u>解释了这些数字背后的一些原因</u></a>。她对其他思想家的理解是，他们认为所有可能价值的空间都非常大，因此我们很可能会错过创建 AGI 时所追求的价值。<br></p><p>似乎有几个很好的理由让我们有不同的直觉</p><ul><li>Katja 的直觉是，当前的人工智能非常擅长在广泛的搜索空间中寻找类似人类的事物。例如生成人脸或文本</li><li>人类也很擅长在所有汽车的空间中找到汽车。以同样的方式，他们也许能够在大型搜索空间中重复创建具有类人目标的人工智能</li><li>经济激励将导致人们发现有用且有价值的东西。我们想让人工智能做我们想做的事情</li></ul></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>人们对这个问题有不同的直觉。有些人认为人类的偏好很容易实现，无论是通过个体模型还是自然平衡。其他人则认为，在所有可能的目标空间中，这是一个非常小的目标，我们不太可能实现它，但人工智能只会制定非常陌生和有害的偏好。</p><p></p><p>我认为这通常是正交性命题所指出的。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>政策会让 AGI 放缓 10 年吗？ 10年可以让我们减少多少坏事？</p><p><br><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 25%, 30%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> “我只是认为政策在过去已经让事情放慢了很多。就像，我认为你可以看看其他已经放慢了很多的技术……我认为，相对于你可以从中获得的经济价值，人类的各种基因工程、各种人类生殖事物的发生速度相当缓慢……似乎有道理的是，我们本来可以投入大量资金并找出更多种类的遗传或克隆类型的东西。我们已经决定，作为一个世界，我们不……[而且]全世界都是如此。中国在这方面的速度并不比美国或类似国家快得多。我认为一般医学也是一个很好的例子，你会忘记……事情发生得多么缓慢，即使它们看起来对人们来说可能非常有价值。”</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">很多人认为政策与减缓/改善人工智能风险非常相关。这是尝试建立一个图表的一部分，让每个人都可以输入自己的真实价值观并获得适合他们的结果。</td></tr></tbody></table></figure><h1>到 2040 年，情况会如何</h1><p>以下是这些数字如何在可能的世界中兑现。这些是互斥且全面详尽的 (MECE)</p><h2> AI 很好，但还不够神（50%）</h2><p>人工智能工具很棒。也许他们可以编写很多代码或提供很多支持。但他们无法将管理公务员部门所需的人力减少 100 倍。由于某种原因，他们无法单独承担大型项目。它就像 GPT4，但要好得多，但不是一步改变。</p><h2> ChatGPT20 - 有能力但没有代理 (15%)</h2><p>想象一个 ChatGPT，它可以生成您要求的任何内容，但只执行一些任务或无法递归调用自身。与上述不同，这确实是一个阶跃变化。你或我可以经营一家对冲基金或政府的一部分。但这将涉及我们实现愿景。</p><h2>许多神一样的智能人工智能系统互相阻碍（4%）</h2><p>与当今世界一样，许多智能系统（人和公司）都在试图实现其结果并相互阻碍。不知怎的，这并没有导致下面的“中/乌托邦”。</p><h2> AI 中托邦/乌托邦 (16%)</h2><p>这些都是非常好的场景，我们拥有不想要坏事的代理 AGI。可能的世界有很多种，从某种人类的提升，到一种一切如常的美好事物，我们可能仍然有很多抱怨，但每个人都像今天最富有的人一样生活。</p><h2>政策节省 (12%)</h2><p>在很多世界里，事情本来会变得非常糟糕，但政策却推迟了事情的发生。这些可能是任何其他非末日世界——也许人工智能已经放慢了很多，或者也许它有更好的目标。为了简化图表，它并没有真正涉及这些世界的样子。请提出建议</p><h2>厄运 (15%)</h2><p>毫无疑问是不好的结果。代理 AGI 想要我们认为不好的东西并得到它。我的感觉是，Katja 认为最糟糕的结果来自于 AGI 的接管，也许 10% 的结果发生得很快，90% 的结果发生得很慢。</p><p>如果您想了解更多相关信息，Katja 在这里提供了更长的解释： <a href="https://wiki.aiimpacts.org/doku.php?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_humanity:start"><u>https://wiki.aiimpacts.org/doku.php</u></a> ?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_ humanity:start</p><h1>视觉交互模型</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mZ46CJQvgiTgaNwDm/lau63ldknsuhqvo3f1ny"></p><p> <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>https://estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a></p><h1>您希望为谁完成此操作？</h1><p>我想看到这样的工作，所以我想我会这么做。如果你想查看某个特定人的人工智能风险模型，也许可以请他们与我交谈。他们大约需要 90 分钟的时间，目前我认为后续每一个的边际收益都相当高。</p><p>在更广泛的层面上，我对积极的反馈感到非常鼓舞。我应该尝试获得资金来进行更多这样的采访吗？</p><h1>这怎么能更好呢？</h1><p>我们仍处于早期阶段，所以我很欣赏很多挑剔的反馈</p><h1>谢谢</h1><p>感谢 Katja Grace 的采访和 Rebecca Hawkins 的反馈，特别是对表格布局的建议，并感谢 Arden Koehler 的好评（您应该阅读她<a href="https://twitter.com/ardenlkoehler/status/1678018343708794882?s=20">关于撰写好评的帖子</a>）。感谢建议我写这个序列的人</p><br/><br/><a href="https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace<guid ispermalink="false"> mZ46CJQvgiTgaNwDm</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:45:48 GMT</pubDate> </item><item><title><![CDATA[What wiki-editing features would make you use the LessWrong wiki more?]]></title><description><![CDATA[Published on August 24, 2023 9:22 AM GMT<br/><br/><p> LessWrong wiki 似乎并没有得到应有的使用。我想这是缺乏编辑的原因。</p><p> <a href="https://www.lesswrong.com/users/vladimir_nesov?mention=user">@Vladimir_Nesov</a>提出了一个很好的观点，即许多标准的维基编辑功能都缺失，这使得前景没有吸引力。</p><blockquote><p>关键是，缺少该功能会使与 wiki 的互动变得不那么有希望，因为它变得不方便，因此在实践中无法对其进行详细保护，因此不太有吸引力在其中投入精力。我提到这是作为解释目前几乎不存在的编辑参与度的假设</p></blockquote><p>那么，本着这一精神，哪些功能会让您个人进行比目前更多的编辑呢？</p><p>如果可行的话，我可能会尝试付钱给一些开发人员来编写拉取请求。</p><br/><br/> <a href="https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong<guid ispermalink="false"> xzm6F6rLt7oPTash2</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:22:01 GMT</pubDate></item><item><title><![CDATA[The God of Humanity, and the God of the Robot Utilitarians]]></title><description><![CDATA[Published on August 24, 2023 8:27 AM GMT<br/><br/><p> My personal religion involves two gods – the god of humanity (who I sometimes call &quot;Humo&quot;) and the god of the robot utilitarians (who I sometimes call &quot;Robutil&quot;).</p><p>当我面临道德危机时，我会向我的<a href="https://www.lesswrong.com/posts/X79Rc5cA5mSWBexnd/shoulder-advisors-101">肩膀</a>-Humo 和我的肩膀-Robutil 询问他们的想法。有时他们会说同样的话，但并没有真正的危机。例如，一些天真的年轻 EA 试图成为实用僧人，捐出所有的钱，从不休息，只做生产性的事情……但 Robutil 和 Humo 都同意，高质量的知识世界需要懈怠和心理健康。 （既是为了<a href="https://www.lesswrong.com/posts/WuyNHrDXcGFgkZpBy/my-slack-budget-3-surprise-problems-per-week">处理危机</a>，也是为了<a href="https://www.lesswrong.com/posts/fwSDKTZvraSdmwFsj/slack-gives-you-space-to-notice-reflect-on-subtle-things">注意到微妙的事情</a>，即使在<a href="https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai">紧急情况</a>下，你也可能需要这些）</p><p>如果你是一个有抱负的有效利他主义者，你绝对应该<i>至少</i>做 Humo 和 Robutil 同意的所有事情。 （即<a href="https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends">在这里了解泰勒·奥尔特曼故事</a>的中心点）。</p><p>但 Humo 和 Robutil 实际上在一些事情上存在分歧，而且在侧重点上也存在分歧。</p><p>对于你应该花多少努力来避免<a href="https://forum.effectivealtruism.org/posts/2BEecjksNZNHQmdyM/don-t-be-bycatch">意外招募到对你没有多大用处的人，</a>他们意见不一。</p><p> They disagree on how many people it&#39;s acceptable to accidentally fuck up psychologically, while you experiment with new programs to empower and/or recruit them.</p><p>他们对于如何努力推动自己变得更好/更强/更聪明/更快，以及为此你应该牺牲多少存在分歧。</p><p> Humo 和 Robutil 都很难以不同的方式理解事物。 Robutil 最终承认你需要 Slack，但他最初并没有想到这一点。他的理解诞生于成千上万年轻理想主义者的倦怠和狭隘视野中，而胡莫最终（耐心地、友善地）说：“我<i>告诉过</i>你了。” （Robutil 回应“但你没有提供任何关于如何最大化效用的论据！”。Humo 回应“但我说这显然不健康！”Robutil 说“wtf &#39;不健康&#39;到底是什么意思？<a href="https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words">禁忌</a>不健康！”）</p><p> It took Robutil longer still to consider that perhaps humans (with their current self-awareness) not only need to prioritize their own wellbeing and your friendships, but that it can be valuable to prioritize them <i>for their own sake</i> , not just as part of a utilitarian calculus, because trying to justify them in utilitarian terms may be a subtly wrong step in the dance that leaves them hollow, burned out for years</p><p> (Though Robutil notes that this is likely <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians?commentId=RpN7kQkPL36nGjmK6">a temporary state of affairs</a> . A human with sufficiently nuanced self-knowledge can probably wring more utilons out of their wellbeing activities)</p><p> Humo 很难承认，如果你把所有的时间都花在确保恪守道义上的承诺，避免伤害你所照顾的人，那么这种努力实际上是在真实的人类身上进行衡量的，他们因为你花了更长的时间来扩大你的计划而遭受痛苦和死亡。</p><p>在我的心目中，胡莫和罗布提尔是年老而睿智的神，他们早就摆脱了幼稚的挣扎。他们互相尊重如兄弟。他们明白，他们的每个观点都与人类繁荣的整体计划相关。他们的分歧并不像你天真的想象的那么严重，但他们说着不同的语言，强调的东西也不同。</p><p> Humo 可能会承认我无法照顾所有人，甚至无法对所有出现在我生活中但我没有时间帮助的人做出富有同情心的回应。但他说这番话时带着一种温暖、<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1">悲伤的</a>同情心，而罗布提尔则带着简短而高效的<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1?commentId=TQuxyPGL7L8nmNe9Z">冷酷语气</a>说出来。</p><p>我发现独立地询问他们并尽我所能想象他们每个人的明智版本是有用的——即使我的想象只是他们理想化的柏拉图式自我的粗略影子。</p><br/><br/> <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of- humanity-and-the-god-of-the-robot-utilarians<guid ispermalink="false"> SfZRWxktiFFJ5FNk8</guid><dc:creator><![CDATA[Raemon]]></dc:creator><pubDate> Thu, 24 Aug 2023 08:27:59 GMT</pubDate> </item><item><title><![CDATA[I measure Google's MusicLM over 3 months as it appears to go from jaw-dropping to embarrassingly repeating itself]]></title><description><![CDATA[Published on August 24, 2023 4:20 AM GMT<br/><br/><p>谷歌研究院于 2023 年 1 月 26 日提交了一篇关于 MusicLM 的论文，这是一个令人难以置信的强大 AI 模型，可以将用户文本提示转换为音乐。</p><p> https://google-research.github.io/seanet/musiclm/examples/</p><p></p><p> 5 月 10 日，谷歌研究院向公众发布了一份候补名单，允许申请者试用。在大约 6 秒内，它会向用户返回 2 首歌曲，每首歌曲长 20 秒。</p><p> https://blog.google/technology/ai/musiclm-google-ai-test-kitchen/</p><p></p><p>我用它创作的音乐是超越且梦幻的。对我来说，这通常是人类的水平。让它释放我的梦想真是太快了。如果他们让我们将其从 X 秒延长，我可以轻松地将 20 分钟长的曲目变得极其复杂和先进。</p><p></p><p>从5月10日到3个多月后的今天，我一直在测试它的极限和能力。大约 1 个月后，我注意到人工智能正在失去许多能力，并且相同的提示会发出明显不同的音乐，这让我清楚地知道某些事情可能会变得更糟。大约两个月后，输出就不再智能、先进和令人瞠目结舌，就好像人工智能是一个老式模型，在当时并不是很好。三个月后，我可以看到这似乎确实正在发生，因为大多数歌曲现在只是重复短暂的 2 或 4 秒节拍，根本没有做太多事情。现在的情况实在是太可怕了。除了一些提示之外，有一些仍然有点工作，有些感觉质量是 2 个月，有些是 1 个月。我想说的是，如果有的话，水平是 0 个月的也很少。我感觉就像失去了我的宠物狗，或者是我曾经做过的一个好梦。我希望他们永远不会改变模型。我保存了所有梦幻般的测试，尽管我刚刚想出了一些更难的提示，我想记录下来，但现在不能。它说在大约 100 次提示后 30 天后回来（奇怪的是，就在 MusicLM 明显变得更糟的时候），但这可以通过第二天回来来绕过。</p><p></p><p>我的早期测试。 1st SoundCloud 上的其余部分也很好：</p><p> <a href="https://www.reddit.com/r/singularity/comments/13h0zyy/i_really_crank_out_music_tracks_with_musiclm_this/">https://www.reddit.com/r/singularity/comments/13h0zyy/i_really_crank_out_music_tracks_with_musiclm_this/</a></p><p>我的第二个 SoundCloud 既有 3 个月的测试，也有我进行的更早期的测试。</p><p> https://soundcloud.com/steven-aiello-992033649/sets</p><p></p><p>我认为当 MusicLM 要求用户通过给予其一个奖杯来改进模型时，在两首生成的歌曲中选择他们更喜欢的歌曲时，问题就开始了。用户只会做出快节奏的判断（我自己有时会犯一些错误，无法回去更改它们），而没有足够的时间对它们进行真正的排名。这也是二元投票。此外，通常两个轨道都有自己独特的功能和能力，我猜这两个轨道中可能有数百个有用的短功能，而整个 20 秒将是 AI 模型中最大的功能/记忆，所以试图让模型更多地忽略一首歌曲只会随机地消除它自己的各种能力，我猜最终会完全消除。我知道有时一首歌可能会更好，你会认为这会让人工智能变得更聪明，但也许他们的算法无法按照预期正确地持续学习/更新模型。无论如何，人工智能现在似乎更糟糕了。</p><p></p><p>重要笔记：</p><p> MusicLM 似乎很久以前就完成了训练。为什么他们还要花时间去训练它呢？显然已经让人瞠目结舌了。从我的测试中我们已经知道模型确实发生了变化（由于 8 月 20 日的冰城堡测试集，我 100% 确定这一点），所以看起来它很可能在我们使用用户反馈来更改模型时使用。他们可能会试图按照我们的喜好来塑造它，无论模型失去什么代价，因为他们担心人们抱怨它如何知道可能受版权保护的歌曲或 NSFW 材料。或者也许他们只是想让它变得更愚蠢，或者让它变得更好。这三种可能性中的任何一种听起来都“恐怖”。它已经很好了，而且绝对没有变得更好。</p><p>尽管它现在制作的大多数歌曲似乎通常都是重复 2-4 秒的简短旋律，但我早期测试的提示中的许多相同乐器仍然存在。通常。</p><p>我的许多旧提示无缘无故地不再起作用，我尝试运行一个这样的提示，要使其正常工作，我所要做的就是删除“.”之间的 1 个空格。和“L”或保留空格并将“L”更改为小写“l”。短提示“暴力”不会在 MusicLM 中运行，“megurine luka”或“rin kagamine”也不会运行，但更流行的“hatsune miku”会运行。我这样做是为了让它运行的提示部分是这样的：“...太鼓和古筝。合成层...&#39;。在某种程度上，人工智能是这些词语选择背后的原因，但也是人类，这种情况在公开测试开始后发生了变化。</p><p>我们已经看到其他 AI 似乎随着时间的推移而退化：GPT-4 和 DALL-E 2。我个人看到 DALL-E 2 发生了变化，因为对于相同的提示，每个输出都不同。我认为情况变得更糟了，但也没有差太多。我们一开始很喜欢它，可能是因为它制作了 10 张小图像，这些小图像乍一看似乎很养眼，而且除非一次查看一张，否则不必在任何展开的图像中看到所有奇怪的细节。一项研究还跟踪了 chatGPT 随着时间的推移，情况似乎可能会变得更糟。似乎 GPT-4 在发布之前就已经变得更糟了，因为它太强大了，尽管我不确定这种说法。在我看来还是很棒的。谷歌尚未发布其许多人工智能，部分原因似乎是担心被滥用。此外，GPT3.5和GPT-4似乎也有反馈按钮，也许它们也在使用用户输入来更改模型。这似乎是一个新事物。我还看到 MidJourney 5 在实际发布之前也这样做了，他说还没有产生有趣的图像，所以他们必须再过几天进行另一轮投票。从我之前和之后保存的几十张图像来看，我有点确定之前的 MJ5 制作了更准确、更易听、更有趣的图像。今年我也看到稳定扩散也询问我们在两张图像之间哪个最符合提示。</p><p>有人提到我的一些早期测试很长，或者从“创建一个...”开始（一些不错的测试是由 GPT-4 提出的，它提出了“创建一个...”），而 AI 通常可能不会这样做请参阅曲目标题。如果说有什么不同的话，这仅表明早期的 MusicLM 在理解这些提示方面没有问题。我还有其他测试，并且您今天尝试的任何提示在今天的 MusicLM 中都表现不佳，因此我认为这在我的测试中不是问题。</p><p></p><p>总之，MusicLM 确实发生了变化，根据我所有的测试，我想说它在 3 个月后几乎变得几乎毫无用处。我真心希望他们能把 Day1 模型还给我们。</p><br/><br/> <a href="https://www.lesswrong.com/posts/DfqcyGXcFcukYbWZ5/i-measure-google-s-musiclm-over-3-months-as-it-appears-to-go#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DfqcyGXcFcukYbWZ5/i-measure-google-s-musiclm-over-3-months-as-it-appears-to-go<guid ispermalink="false"> DfqcyGXcFcukYbWZ5</guid><dc:creator><![CDATA[AttentionResearcher]]></dc:creator><pubDate> Thu, 24 Aug 2023 11:48:01 GMT</pubDate></item></channel></rss>