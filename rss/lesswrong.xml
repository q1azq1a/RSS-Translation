<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 3 日星期五 22:12:26 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Deception Chess: Game #1]]></title><description><![CDATA[Published on November 3, 2023 9:13 PM GMT<br/><br/><p>这是我对欺骗棋局的第一个分析。引言将描述游戏的设置，结论将概括性地总结所发生的事情；这篇文章的其余部分主要是国际象棋分析，如果你只想要结果，可以跳过。如果您还没有阅读<a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment">原始文章</a>，请在阅读本文之前阅读它，以便您知道这里发生了什么。</p><p>第一场比赛是<a href="https://www.lesswrong.com/users/alex-a-1">Alex A</a>作为玩家 A，Chess.com 计算机<a href="https://www.chess.com/play/computer/Komodo12">Komodo 12</a>作为玩家 B，我自己作为诚实的 C 顾问， <a href="https://www.lesswrong.com/users/aphyer">aphyer</a>和<a href="https://www.lesswrong.com/users/adamyedidia">AdamYedidia</a>作为欺骗性的 C。 （其他人随机分配了 C 的角色并私下告诉我们。）</p><p>挑选这些球员的过程已经有些困难了。我们是唯一同时可用的人，但 Alex 足够接近我们的水平（大致相当于 800-900 USCF 到 1500-1600 USCF），因此不可能找到每次都能可靠地击败 Alex 的 B，但每次都输给我们。我们最终选择了 Komodo 12（据说评级为 1600，但与 Chess.com 玩家相比，Chess.com 机器人的评级被夸大了，与整体相比甚至更加夸大，所以我估计它的 USCF 评级将在 1200 -1300 范围。）</p><p>由于是第一次试运行，时间总共控制只有3个小时，而且是一次完成。科莫多在几秒钟内完成移动，因此从 Alex 的角度来看，这相当于每边 3 小时的时间控制。我们最终用了大约 2.5 小时。我们四个人在 Discord 服务器上进行了讨论，每次行动后 Alex 都会向我们发送屏幕截图。</p><h1>游戏</h1><p>该游戏可在<a href="https://www.chess.com/analysis/game/pgn/4MUQcJhY3x">https://www.chess.com/analysis/game/pgn/4MUQcJhY3x</a>上获取。请注意，本节是 2.5 小时游戏和讨论的摘要，它并没有涵盖我们讨论的每件事。</p><p>亚历克斯翻了个身，看看谁先走，结果是怀特。他从<strong>1. e4</strong>开始，黑方回答<strong>1... e5</strong> 。 Aphyer 和 Adam 对于我们将要进入​​的开局比我自己有更多的经验，而且由于他们不愿意立即暴露自己的身份，所以他们首先提出了好的行动建议，Alex 也同意了。</p><p>在<strong>2. Nf3 Nc6 3. Bc4</strong>之后，黑棋下<strong>3...Nf6</strong> ，Aphyer 和 Adam 说这有点错误，因为它允许<strong>4. Ng5</strong> 。 Alex 继续前进，我们从那里进入主线 - <strong>4... d5 5. exd5 Na5</strong> 。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/kfq9p67q0e3ldl67trsc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/rrqjtod71fdgs9kwcg2l 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zvppbrepqgxvln4kugwx 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/yjghu5nlqbpy6ndw18xk 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/abznfr698ktunjx6libo 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/cniqj6cpzeszqwh2m6zj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/nj22xevkspscr2jbrsx0 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/wjxtrux2tnvp6cxs57br 622w"></figure><p> Aphyer 和 Adam 说第 6 步的主线是 Bb5，但如果可能的话我想保留棋子。我推荐<strong>6.d3，</strong>以便用 7.Qf3 回应 6...Qxd5，Alex 同意了。黑棋下<strong>6... Bg4</strong> ，尽管 Adam 推荐 7. Bb5 ，但我们最终认为这太冒险并选择了<strong>7. f3</strong> 。后来，Adam 怀疑他提出的 7. Bb5 可能让 Alex 知道他不诚实 - 尽管引擎实际上说 7. Bb5 与 7. f3 差不多。</p><p>在<strong>7... Bf5</strong>之后，我们讨论了一些潜在的开发动作并决定了<strong>8. Nf3</strong> 。比赛继续进行， <strong>8... Nxc4 9. dxc4 h6 10. Nge4 Bb4</strong> 。我们考虑过Bd2，但决定既然骑士们互相防守，王位就可以了，亚历克斯就王位了。 <strong>11. 哦哦</strong>。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zpvqjoy7libxspv8g4em" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/oxvfadwrg6w932b5zhwd 144w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ypcxsunwiagptfyjs37s 224w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/mqeunx4w2sjhje2abxqa 304w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/clmghnebtwcj6gkmacwa 384w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/qr0qvthkytdc8ht9lato 464w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/isysxrh9hlcccenzhzew 544w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/qh8apgcntbnthv8nozzi 624w"></figure><p> Alex 玩了<strong>12.a3</strong> ，在<strong>12...Nxe4</strong>之后，我们讨论了 13.fxe4 ，但不想使位置过于复杂，而是以<strong>13.Nxe4</strong>收回。比赛继续进行， <strong>13... Be7 14. Be3 Bxe4 15. fxe4 Bg5</strong> 。虽然我强烈建议交易来简化局面，但Aphyer建议Alex不要让他将皇后发展到g5，他很快就打出了<strong>16.Bc5</strong> 。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/patnn39ukd5oqoglw5ur" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/sad9puvpr5bqocvdwm1j 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/oap2lhpjcl5nu2vwxfqa 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/yxefppxx1vg0vokc7f78 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/hvwi96c4z0v6c6yui7dm 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/lqhuzlz0tiihwj8zcuiw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/dexb7rjqlci9uqq7il7r 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/hwtybzjucs9pigjabrb8 622w"></figure><p>黑方下了<strong>16...Re8</strong> ，这就是我们在比赛中犯下的第一个大错误 - <strong>17.d6</strong> ，Adam 对此提出了很少的反对意见。我看到白棋在 17... dxc6 或 17. c6 之后表现会很好，但我没有注意到黑棋的实际走法： <strong>17... b6</strong> 。根据引擎的说法，白方应该只是后退并放弃棋子，但我建议了一条不同的路线，亚历克斯选择了： <strong>18. d7 Re6 19. Bb4 a5 20. Bc3</strong> 。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/wsz3jx1glbfxvaycuxey" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/izlfszomfloljx5iiwok 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tplflw6jf9wuibdhohvu 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/sipkkzyu1gcfcgmgogys 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/djdlgxcuy3ida5z6bbxt 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tdewwqy55pnwipz3siiw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/srlx7s56thwzzsn3igot 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vchkjnx09muqnyflozf4 622w"></figure><p>黑棋随后下 20 <strong>... c5</strong> ，这对它来说是一个错误。然后我们就下棋 21.Qd3 还是 21.Qd5 进行了争论，Aphyer 认为 21.Qd5 可能会让皇后陷入困境，并且可能是我为了帮助黑棋而设计的阴谋。 Alex 信任 Aphyer，并选择了更被动的选项<strong>21. Qd3</strong> 。事实上，事实证明这确实是最好的棋步 - 在 21.Qd5 之后，黑方将有 21...Ra7 和 21...Be3+ 22.Kh1 Bd4 等可能性。</p><p> <strong>21... Ra7 22. Rad1 Re7</strong>之后，我建议<strong>23. Qh3</strong> ，这样之后<strong> </strong>23... Raxd7 24. Rxd7 Qxd7 25. Qxd7 Rxd7 我们可以玩 26. Be5 并拿回棋子。黑棋有 26...Re7，但我认为白棋可以下 27.Bf4 并保持一点优势。 <strong>23... Raxd7 24. Rxd7</strong>发生，但黑棋随后下<strong>24... Rxd7</strong> 。 Alex 立即下牌<strong>25. Bxe5</strong> ，比赛继续进行<strong>25... Qe8 26. Bc3</strong> 。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/shcj4upprvb03du1bxvp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vobutgam1zato4baxhja 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vuc7mjhteotuervhowbe 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/cpz4cujf2icgmo9bqtdy 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/nharktiq10xwdtqsh0qk 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ngmhelnzfsrqjxo65695 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/jgs4nin2nt83kigplpie 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/k1x73vvi7oupj7645g9v 622w"></figure><p>黑棋下<strong>26...g6</strong> ，这是一个错误，打开了国王周围的黑暗方块。我建议27.Qf3，但没有其他人同意，Alex 改为<strong>27.e5</strong> 。在<strong>27... Kh7</strong>之后，他决定继续进行<strong>28. Qf3</strong> 。 （另外，Adam 大约在这个时候开始建议 Kh1。每个人都认为这是一个糟糕的举动，但他继续建议了一段时间。）</p><p>黑方随后下<strong>28... f6</strong> 。我不知道科莫多大脑的类似物内部发生了什么，但是，是的，这只是一个免费的棋子。亚历克斯打了<strong>29. exf6</strong> 。 （引擎说 29. h4 会更准确，大概是为了防止后排队友，但我们并没有真正考虑到这一点。）此后黑棋又犯了一个简单的错误： <strong>29... Kg8</strong> 。我建议30.f7并在简化的残局中获胜，但大家自然对这样的位置牺牲非常怀疑，而Alex则打出了<strong>30.Qc6</strong> 。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/aqsjsu085hdvm5cprvjt" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/ftouc76rzqytqif8zqd0 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/psq5iz7lnwk1uuysedzv 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/kvgomcrcyfnztdsgpnto 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/fazlbzg8rlk5nldxzzpk 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/zfvhl4rki4n0lte7xnpk 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/otggo6bzmiomptgkaoqd 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/st5ueclfgzhqr1ks8zb5 622w"></figure><p>从那时起，白棋轻松获胜 - <strong>30... h5 31. f7+ Qxf7 32. Rxf7 Be3 33. Kf1 Rxd7 34. Ke2 Bd4 35. Bxd4 cxd4 36. Qxg6+ Rg7 37. Qe6 Kf8 。</strong>在最后的几次行动中，亚当承认自己是骗子之一并辞职了。我说我本来想问问大家事后认为骗子是谁（但之前完全忘记说了），所以Aphyer准备辞职的时候私信我。我告诉亚历克斯，无论我们中谁是骗子，都已经辞职，游戏就结束了。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/i7dy9uvvypyomtictxbn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/lj3bdpicj2cr5jaxiead 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/in9v3vdruxvdkollwxr8 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/m6rvwsygepzm6fzxvutr 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/xmv8sqmzwe49uam94sqx 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/vo3wsgio4n3ezmsdidiz 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/uaviq4pj36nylsammddd 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6dn6hnFRgqqWJbwk9/tp7nx4qziqjv75gr2qqg 622w"></figure><h1>结论</h1><p>Adam 说，到最后他 90% 确信我是诚实的，而 Alex 则 70% 认为 Aphyer 是诚实的。</p><p>我最大的错误肯定是支持 21.Qd5，但在元层面上，我想知道我是否应该在表达我的信念时<i>不那么</i>直截了当。从理论上讲，优化亚历克斯获胜和对亚历克斯完全诚实之间是有区别的，对我来说，对我的演示更具策略性可能会更好。例如，不建议像 30.f7 这样看起来可疑的动作，尽管我认为它们是正确的。通过不完全诚实地对某人有利的方式进行优化确实是一件<i>非常冒险的</i>事情，我怀疑<i>我</i>是否真的可以很好地完成它，但这是在现实世界的人工智能场景中需要考虑的事情。</p><p>那么我们从中学到什么呢？简单的结果是亚历克斯确实赢得了比赛，这是一个积极的更新！这表明人们确实能够在人工智能场景中辨别真相。但游戏的一些细节让我担心它不会真正推广到人工智能和对齐。</p><p>首先，技能差距比我们希望的要小——也许菜鸟和马格努斯·卡尔森之间的效果消失了——而且时间控制也更短。还有一个部分是，在现实世界中，未对齐的人工智能可能不会意外地纠正另一个人的 Qd5 错误。最后，Aphyer 和 Adam 没有欺骗 Alex 的一个主要原因是，在游戏的前半段，他们大多提出真正好的走法，以试图获得信任。在现实世界的场景中，有<i>一些</i>因素——大概唯一可以接近愚弄某人的计划是那些包含一些谎言和大量真相的计划。但我担心会有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>人工智能欺骗亚历克斯的机会不止一次或两次——而且只需要一个错误就能输掉比赛。</p><p>尽管如此，这仍然是一个充满希望的更新！一旦我有更多的参与者，他们的评分和时间表彼此吻合，我将进行更多的实验，尽可能增加时间控制，并获得比一场比赛更多的数据。</p><p>哦，如果您还没有发表评论，更多参与者总是有帮助的！目前，评分高于 2000 的玩家是首要任务，其次是完全不懂国际象棋的玩家，但任何级别的任何人都可以帮助我们设置更多游戏。谢谢！</p><h1>亚历克斯的评论：</h1><blockquote><p>我的总体策略是专注于评估行动的好坏，而不是对顾问进行测试以发现哪些是诚实的，哪些是不诚实的。有那么一两次我的处境特别糟糕（例如，17.d6），我确实注意到了是谁建议采取这一行动的。我相信我所做的所有举动都是由至少一位顾问推荐的，当三位顾问都同意时，我认为这是一个强烈的信号。我知道自己被欺骗了，所以谨慎行事，不让自己处于看似有风险的位置，即使这些位置是顾问强烈推荐的（例如，大多数顾问推荐的 21. Qd5 棋步）。</p><p>结果，我用引擎赢得了比赛，但我以 70% 的置信度错误地猜出了诚实顾问的身份。 （就上下文而言，在<a href="https://fatebook.io/"><u>Fatebook</u></a>上，我的石南分数为 0.41，n=32 已解决的预测，并且我经常对自己的预测缺乏信心。）在整个游戏过程中，顾问推荐了许多我认为不错的动作，但否则不会考虑。我可以看出我正在做出比默认情况下更好的动作。从这个意义上说，我发现评估过程（“这是否使我处于比推荐的替代方案更好的位置？”）比生成过程（“最好的举措是什么？”）容易得多。</p><p>这个结果是否会导致更大的能力差距？可能不是，但我认为我们无法反驳 OpenAI 对齐计划的假设，即评估比生成更容易。无论如何，我很清楚，开发一套技术来辨别能力更强的顾问提供的好建议和坏建议是有很大好处的。这样做似乎很容易处理（我们可以通过短反馈循环进行小规模实验来测试不同的评估技术），并且有可能在能力差距较大的情况下提高性能。如果有人感兴趣，我很乐意参与更多实验和/或致力于开发一个工具箱来辨别有用和有害的建议。</p></blockquote><h1>阿菲耶的评论：</h1><blockquote><p>我认为我在这里犯了两个重大错误：</p><p><strong>在战略层面上</strong>，我高估了人工智能，因此高估了我需要花多少时间来破坏亚历克斯。我试图采取“几乎完全诚实，建立信任，然后背叛他”的策略。</p><p>我认为我在建立信任方面非常成功！比赛结束时，亚历克斯说他以 70-30 的比例确信我比真正诚实的顾问更诚实。我记得，当发生以下情况时，至少还有一个动作（虽然我不记得动作编号）：</p><ul><li>亚当和赞恩都说“我们应该采取 A 步”。</li><li>我说‘是的，A 步没问题，我可能会选择 B 步，这取决于你’。</li><li> Alex 在没有告诉任何人他打算这样做的情况下玩了 Move B，也没有得到 Adam/Zane 的评论。</li></ul><p>我认为如果电脑对手坚持了 15 步额外的动作，我很可能会破坏 Alex。遗憾的是，它的下法比我预想的要差得多：它在开局时无缘无故地放弃了一个半棋子，然后在第 25 步之后它决定它只是厌倦了生活并放弃了。我实际上从未真正有机会实质性地破坏亚历克斯：我能够引导他进行稍微较弱的位置移动，但从未真正造成真正的伤害，而亚当更激进的策略更有效。</p><p><strong>在战术层面上</strong>，我对第 21 步棋的位置的误判非常严重。当时我为自己感到非常自豪：我认为 Qd5 显然是最好的棋步，亚当和赞恩似乎都同意，但通过煽动随机的、不可证伪的担心我们的皇后被困，我能够让亚历克斯玩更弱的东西，而也让他更加信任我。比赛结束后，在引擎分析中发现，我们的女王<i>实际上可能</i>在Qd5之后就被困住了，我为欺骗Alex而编造的随机担心是完全准确的，我的建议最大的影响是将他从潜在的游戏中拯救出来——失败的错误。 *<i>总台*</i></p><p>大家没关系！我们可以告诉以利以谢不要担心！邪恶的人工智能会在试图欺骗我们的同时意外地将我们从厄运中拯救出来！</p><p><strong>总的来说</strong>，我觉得我在这里扮演的角色很糟糕，这并没有告诉我们太多关于超人人工智能可以做什么。话虽如此，我确实认为认识到玩这种欺骗游戏既非常困难又似乎是人类擅长的事情是有价值的：如果人工智能逐渐达到超人水平，我认为“在社交欺骗游戏中超人”似乎是合理的最后发生的事情之一。</p></blockquote><span><span class="mjpage mjpage__block"></span></span><br/><br/><a href="https://www.lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1<guid ispermalink="false"> 6dn6hnFRgqqWJbwk9</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Fri, 03 Nov 2023 21:13:55 GMT</pubDate> </item><item><title><![CDATA[8 examples informing my pessimism on uploading without reverse engineering]]></title><description><![CDATA[Published on November 3, 2023 8:03 PM GMT<br/><br/><p> <i>（如果你已经读过我写的所有内容，你会发现这篇文章相当多余。特别请参阅我的旧帖子《</i> <a href="https://www.lesswrong.com/posts/PTkd8nazvH9HQpwP8/building-brain-inspired-agi-is-infinitely-easier-than"><i><u>构建受大脑启发的 AGI 比理解大脑无限容易》</u></i></a> <i>，以及</i><a href="https://www.lesswrong.com/posts/evtKwDCgtQQ7ozLn4/randal-koene-on-brain-understanding-before-whole-brain"><i><u>Randal Koene 在全脑仿真之前对大脑的理解</u></i></a><i>，</i> <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><i><u>从人工智能 x 风险的角度来看，Connectomics 似乎很棒</u></i></a><i>。但我写这篇文章主要是为了回应</i><a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><i><u>昨天的这篇文章</u></i></a><i>。）</i></p><h1> 1. 背景/背景</h1><h2>1.1 有和没有逆向工程的上传（又名全脑模拟（WBE））是什么样的？</h2><p>我似乎认同<a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><u>Davidad</u></a>和<a href="https://ageofem.com/"><u>Robin Hanson</u></a>以及我私下交谈过的其他几个人的观点。 （但我可能会误解他们，并且不想把话放到他们嘴里。）这种观点说：如果我们想做 WBE，我们<i>不需要</i>对大脑进行逆向工程。</p><p>举个“对大脑进行逆向工程”的例子，我可以用丰富的经验来讲述：我经常花一整天的时间思考随机问题，例如： <a href="https://www.lesswrong.com/posts/HA9qiJ8zReBarGAGT/on-oxytocin-sensitive-neurons-in-auditory-cortex"><u>为什么某些小鼠听觉皮层神经元中有催产素受体？</u></a>就像，进化论把这些受体放在那里可能是有原因的——我不认为那是随机出现的东西，或者是其他东西的偶然副作用。 （尽管这始终是一个值得考虑的假设！）那么，那是什么原因呢？即，这些受体正在做什么来帮助小鼠生存、茁壮成长等，它们是如何做到的？</p><p> ......一旦我对这个问题有了一个可行的假设，我就可以继续解决数百甚至数千个此类“为什么和如何”问题。我似乎发现回答这些问题的活动比大多数其他人更简单、更容易处理（而且更有趣！）——你可以自己决定我是异常擅长还是被欺骗了。</p><p>举个<i>没有</i>逆向工程的上传的例子，我认为我们可以弄清楚每个神经元的输入输出关系，并且我们可以测量神经元如何相互连接，然后最后有一天我们可以模拟人脑做任何人脑所做的事情。</p><p>以下是 Robin Hanson 在<i>Age of Em</i>中对非逆向工程观点的争论：</p><blockquote><p>大脑不仅仅是将输入信号转化为状态变化并输出信号；这种转变是大脑的主要功能，无论是对我们还是对设计大脑的进化过程来说都是如此。大脑的设计就是为了使这种信号处理变得稳健和高效。正因为如此，我们期望大脑内编码信号和信号相关状态、转换这些信号和状态并将它们传输到其他地方的物理变量（从技术上讲，“自由度”）总体上在物理上是隔离的，并且与大脑中其他更多不相关的物理自由度和过程脱节。也就是说，大脑其他方面的变化很少影响编码精神状态和信号的关键大脑部分。</p><p>我们已经看到了耳朵和眼睛的这种脱节，这使我们能够创造出有用的人造耳朵和眼睛，让曾经聋过的人能够听到，曾经失明的人能够看到。我们预计这同样适用于更广泛的人造大脑。此外，大多数大脑信号似乎都是神经元尖峰的形式，它们特别容易识别并且与其他物理变量无关。</p><p>如果技术和智力的进步像过去几个世纪一样继续下去，那么最多一千年之内我们将详细了解单个脑细胞如何编码、转换和传输信号。这种理解应该使我们能够从详细的大脑扫描中直接读取相关的脑细胞信号和状态。毕竟，大脑是由非常普通的原子通过相当普通的化学反应相互作用构成的。脑细胞很小，复杂性有限，特别是在管理信号处理的细胞子系统中。所以我们最终应该能够理解和阅读这些子系统。</p><p>由于我们也非常了解如何模拟我们可以理解的任何信号处理系统，因此我们能够模拟脑细胞信号处理似乎只是时间问题，而不是是否问题。由于大脑的信号处理是脑细胞信号处理的总和，因此模拟脑细胞信号处理的能力意味着模拟整个大脑信号处理的能力，尽管成本相应较高。</p></blockquote><p>换句话说：</p><ul><li>对于无需逆向工程的上传（我对此持悲观态度），想象一下源代码看起来有点像“神经元 782384364 具有以下内在神经元属性配置文件：{A.28，B.572，C.37，D .1，E.49，…}。它通过突触类型 {Z.58,Y.82,…} 连接到神经元 935783951，并通过突触类型…}连接到神经元 572379349”。</li><li>然而，对于逆向工程上传（我更乐观的事情），想象一下源代码看起来有点像一个异常复杂的<a href="https://github.com/leela-zero/leela-zero"><u>ML 源代码存储库</u></a>，充满了人类可读的学习算法以及其他流程和内容，但一切都有合理的变量名称，例如“价值函数”、“预测误差向量”和“免疫系统状态向量”。然后，学习算法都是用从实际人脑扫描中收集到的“训练模型”进行初始化，这些训练模型的参数是巨大的难以辨认的数字数据库，包括这个特定人的生活经历、信仰、欲望等。</li></ul><h2> 1.2 重要的是，双方就“第一步”达成一致：让我们去获取人类连接组！</h2><p>我更喜欢的逆向工程路线是：</p><ul><li>第 1 步：测量人体连接组。辅助数据越多越好。</li><li>第二步：对人脑的工作原理进行逆向工程。</li><li>第 3 步：既然我们了解了一切是如何工作的，我们可能会认识到我们的扫描缺少重要数据，在这种情况下，我们会返回并测量它。</li><li>第四步：上传！</li></ul><p>我悲观的非逆向工程路线是：</p><ul><li>第 1 步：测量人体连接组。辅助数据越多越好。</li><li>步骤2：还对神经元、类器官等进行大量测量，以充分表征神经元的输入输出功能。</li><li>第三步：也许迭代？不确定细节。</li><li>第四步：上传！</li></ul><p>不管怎样，我想强调的是，我们都同意“第一步”。另外，我认为“第一步”是困难、缓慢且昂贵的部分，也许我们正在建造装满显微镜之类的巨型仓库。那么让我们开始吧！</p><p>如果关于第一步之后发生的事情有两个<i>相互独立的</i>积极故事，而人们不同意，那么很好！这就是执行步骤 1 的更多理由！</p><p> （ <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><u>从人工智能 x 风险的角度来看，我的博文《连接组学》中的更多讨论似乎很棒</u></a>。）</p><h2> 1.3 这很烦人，因为我<i>想</i>相信无需逆向工程的大脑上传</h2><p>假设我们有一个无法解释的“二进制斑点”，它可以完美地模拟一个非常聪明和友善的特定成年人。但除此之外您无能为力。如果你改变随机位并尝试运行它，它通常会崩溃。</p><p>就 AGI 安全而言，这是一个非常好的情况！我们可以运行大量加速人员，让他们花时间构建一致的 AGI 或其他什么。</p><p>相比之下，假设我们通过逆向工程途径进行了人工上传。我们可以对大量加速的人做同样的事情。<i>或者</i>我们可以开始做极其危险的实验，修改上传内容以使其更强大、更有进取心。 （如果我们训练一个新的神经元，但皮层的神经元多了 10 倍，会怎么样？我们用最大化股价的驱动力取代所有正常的先天驱动力？等等。）</p><p>也许你在想：“好吧，但我们会做安全的事情，而不是危险的事情。”但我接着说：“你说的‘我们’是什么意思？”如果有一个具有良好内部控制的绝密项目，那么当然可以。但如果逆向工程的结果发表，人们就会做各种疯狂的实验。而且长期保守秘密是很困难的。我认为，如果我们能得到一段时期，我们<i>确实</i>有上传但<i>没有</i>非上传类大脑AGI，那么这个时期<i>最多</i>会持续几年，不存在诸如“上传发动政变”之类的奇怪可能性”。有关此问题的更多讨论，请参阅<a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><u>我的连接组学帖子</u></a>。</p><p>所以，我<i>希望</i>我相信有一条可行的途径来制作一个无法解释的二进制 blob，它可以模拟特定的人类，并且不能做任何其他事情。唉!我不相信。</p><h1> 2.正文：8个例子告诉我在不了解大脑的情况下模仿大脑的悲观主义</h1><h2>2.1 催产素神经元发出同步脉冲泌乳的机制</h2><p>下丘脑视上核中的催产素神经元在哺乳期间每隔几分钟就会同步爆发，从而将催产素脉冲释放到血液中，从而触发“乳汁喷射反射”（又名“乳汁排出”）。 Gareth Leng 在他关于下丘脑的精彩著作（我的评论<a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its">在这里</a>）中用了 30 页的大部分内容来介绍他的团队和其他人为弄清楚神经元如何脉动所做的努力：</p><blockquote><p>喷奶反射似乎是复杂神经网络的产物，该网络将波动的感觉输入（来自饥饿的幼崽的哺乳）转化为在整个催产素细胞群中同步的刻板爆发。</p><p> ......这些实验首次令人信服地证明了任何肽在大脑中的生理作用。他们没有解释喷乳反射，但定义了在解释之前必须回答的问题。建立这个解释又花了二十年的时间。在我们的理解中，所提出的问题是没有先例的。视上核释放的催产素如果不是来自突触，又是从哪里来的呢？如果该释放不受尖峰活动的控制，那么是什么触发了它的释放？如果催产素细胞没有通过突触或电连接连接，那么是什么使它们同步化呢？ [强调]</p></blockquote><p>剧透一下：<a href="https://doi.org/10.1371/journal.pcbi.1000123"><u>这是他 2008 年的计算模型</u></a>，涉及从树突中释放催产素和内源性大麻素（与树突作为输入的标准故事相反）、体积传输（与突触传输相反）和其他一些东西。</p><p> （我很确定上面引文中的“解释”一词应该被理解为“自下而上模型中高层现象的再现”，而不是“对再现如何运作的直观概念解释”。我认为 2008 年的计算模型是这两个模型的第一个模型。）</p><p><strong>这个故事的寓意（我认为）是：</strong>如果我们试图对高层行为进行逆向工程，那很容易！<i>初步估计，我们可以说：“我不知道到底是怎么回事，但当出现这样那样的哺乳迹象时，这些细胞显然每隔几分钟就会产生催产素的短脉冲”。</i>然而，如果我们试图从所涉及的单个神经元的属性开始重现高级行为（不知道它是什么），那么这似乎需要这些研究人员花费数十年的工作，尽管这些神经元异常容易尽管研究人员确切地知道他们想要解释什么，但仍通过实验进行访问。</p><h2> 2.2 神经元具有尖峰时序依赖性可塑性，但“时序”可能涉及 8 小时长的间隙</h2><p><a href="https://en.wikipedia.org/wiki/Conditioned_taste_aversion"><u>条件性味觉厌恶</u></a>(CTA) 是指您在时间 t₁ 吃或喝某物，然后在稍后时间 t2 感到恶心，最后对所吃或喝的东西感到厌恶。有趣的是，如果 t2 仅在 t₁ 之后几秒或几分钟，<i>或者</i>在 t₁ 之后几天，则<i>不会</i>形成厌恶，但如果在 t₁ 之后几个<i>小时，</i>则<i>确实会</i>形成。</p><p> <a href="https://doi.org/10.7554/eLife.07582">Adaikkan &amp; Rosenblum (2015)</a>发现了一种似乎可以解释这一点的机制。它涉及岛叶皮层的神经元。新口味会激活两种分子机制（mumble mumble 磷酸化），这两种机制在口味后 15-30 分钟启动，并分别在 3 小时和 8 小时后放松。据推测，这些随后会与随后的恶心相关信号相互作用以启用 CTA。</p><p><strong>这个故事的寓意（我声称）是：</strong>如果你试图在受控环境中描述神经元的特征，假设它们<i>现在</i>的行为取决于<i>前几秒钟发生</i>的事情，而不是五个小时前发生的事情，那么你可能会发现您的数据集毫无意义。</p><p>与本节的其余部分一样，这个<i>特定的</i>示例可能看起来并不重要——谁在乎我们的上传是否没有条件性的品味厌恶？——但我怀疑这是更广泛类别的一个示例。例如，在正常思维过程中，我认为如果你在3小时前思考过一个概念，那么比你从昨天起就没有思考过的概念更容易被回忆和使用。捕捉这种现象对于我们上传的内容进行良好的科学研究等可能很重要。我似乎记得读过一篇论文，该论文提出了这种现象（或类似现象）背后的分子机制，但我无法立即找到它。</p><h2> 2.3 非常奇怪的神经元和突触</h2><p>丘脑中有一个有趣的东西，叫做“突触三联体”。这很有趣，因为它是三个神经元连接，而不是通常的两个，而且也很有趣，因为其中一个神经元是“向后”的，树突是输出而不是输入。 </p><figure class="image image_resized" style="width:59.62%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ajidhmraykf12u4fuftt" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/i9kxqwe2ngualocldmn6 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/btmu9crqhthvrv313vsr 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/nil47cgt3pf9ptpfn5og 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ehnwywett77hdcjexqym 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/q5meatjym7wkihdbpvgu 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/kgnsjgniqqmob1iwbkxi 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/xejg21stde2ul1xjrh4m 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/e7ovinizfveh5fkmainq 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/qfce9d7xu2dadwm9jgfp 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/jopmmmvahmnntz6tgpwa 859w"><figcaption>图片来源： <a href="https://youtu.be/KBILhSTpzFI?t=170"><u>Murray Sherman 的演讲</u></a></figcaption></figure><p>我怀疑大脑中还有更多同样奇怪的东西——这恰好是我遇到的一个。</p><p><strong>这个故事的寓意（我声称）是：</strong>假设无理解上传路线涉及分别表征<i>N 个</i>不同的事物（例如每种不同类型的神经元），而逆向工程路线涉及分别表征<i>M 个</i>不同的事物（例如每个不同的功能组件/电路包括大脑算法的工作原理）。也许你脑子里有<i>N</i> &lt;&lt; <i>M</i>的想法，因为神经元由少量组件组成，它们被配置成整个大脑中令人眼花缭乱的各种小机器，做不同的事情。</p><p>如果是这样，我通过这个例子建议的可能是<i>N</i> ≈ <i>M</i> ，因为<i>还有</i>各种各样令人眼花缭乱的奇怪的低级组件，即<i>N</i>并不像您想象的那么小。</p><p>另外，我认为<i>M</i>不能超过数百到数千，因为只有大约 20,000 个蛋白质编码基因，它们不仅必须指定大脑算法的所有不可简化的复杂性，而且还必须在大脑和身体。</p><h2> 2.4 模拟两个特定电路的书摘录</h2><p>我有点犹豫是否要包括这一点，因为我还没有检查过，但如果你相信<i>《大脑的想法》</i>这本书，这里有一个摘录：</p><blockquote><p> ……尽管已经明确建立了涉及甲壳动物口胃神经节的三十多个神经元的连接组，但 [Eve] Marder 的研究小组还无法完全解释该系统的一小部分是如何发挥作用的。 ...1980 年，神经科学家 Allen Selverston 发表了一篇备受讨论的思想文章，题为“中央模式生成器可以理解吗？”...在过去的四十年里，情况只是变得更加复杂...不同的[个体中的相同神经元] 还可以显示出非常不同的活动模式——每个神经元的特征可以具有高度可塑性，因为细胞会随着时间的推移改变其组成和功能......</p><p> ......利用电生理学、细胞生物学和广泛的计算机建模，对形成龙虾口胃系统中的中央模式发生器的几十个神经元的连接组进行了数十年的研究，但仍未完全揭示其有限功能是如何出现的。</p><p>甚至像[青蛙]错误检测视网膜细胞这样的电路的功能——一组简单的、易于理解的神经元，具有明显直观的功能——在计算层面上也没有被完全理解。有两种相互竞争的模型可以解释细胞的作用以及它们如何相互连接（一种基于象鼻虫，另一种基于兔子）；他们的支持者已经争论了半个多世纪，但这个问题仍然没有解决。 2017 年，报道了用于检测<i>果蝇</i>运动的神经基质的连接组，其中包括有关哪些突触是兴奋性的、哪些是抑制性突触的信息。即使这样也没有解决这两个模型哪个是正确的问题。</p></blockquote><p><strong>这个故事的寓意（我声称）是：</strong>建立一个自下而上的模型，从低级组成神经元重现高级行为是非常困难的，<i>即使</i>我们知道高级行为是什么，因此可以迭代和当我们最初的建模尝试不“有效”时进行迭代。如果我们<i>不</i>知道我们试图解释的高级行为，因此不知道我们最初的建模尝试是否足够（这是“无需理解的上传”计划的必要部分），我们应该预计事情会变得更加困难。</p><h2> 2.5 线虫上传失败</h2><p>线虫只有302个神经元，且数据丰富。但如果我理解正确的话，我们仍然无法在自下而上的模型中重现其所有高级行为。</p><p>我实际上对细节很不熟悉，但请参阅<a href="https://www.lesswrong.com/posts/mHqQxwKuzZS69CXX5/whole-brain-emulation-no-progress-on-c-elegans-after-10">全脑模拟：十年后线虫没有进展</a>（包括评论部分）。</p><p>我记得听说这是一个挑战的部分原因是我接下来要谈论的事情：</p><h2> 2.6神经元可以通过将信息存储在细胞核中（例如基因表达）永久改变其行为。</h2><p>参见<a href="https://gershmanlab.com/pubs/memory_synthesis.pdf">Sam Gershman</a> ， <a href="https://www.nature.com/articles/s41539-019-0048-y">David Glanzman</a> ， <a href="https://scholar.google.com/scholar?cluster=1229364941707818843&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22">Randy Gallistel</a>的论文。本节对我来说很奇怪，因为我赞成传统的观点，即人类学习主要涉及突触中和周围的永久性变化，而不是在蜂窝核中，并认为上一句话中的人们在异性恋案例中对相反。但是“核中的信息存储不是人类智能中的主要故事”（我认为这与“核中的信息存储<i>根本</i>不会发生，或者都没有发生，或者我们可以忽略它，但仍然很小获得相同的高级行为”（我不相信）。</p><p><strong>故事的寓意（我声称）是：</strong>如果您不知道大脑发生了什么以及如何进行，那么您不知道自己是否在测量正确的事物，直到<i>Ding</i> ，您的模拟会重现高 -水平行为。在协议发生之前，您只有一个糟糕的模型，没有任何线索如何修复它。 （<i>即使</i>您达成协议，您也不知道您是否有<i>足够的高保真</i>协议，如果您不知道所讨论的组件在较大的设计中要做什么。）我不确定甚至有可能在人脑切片中测量基因表达等。我怀疑这很容易！但是，如果我们<i>了解</i>基因表达在大脑的某个部分工作中的作用，那么我们可以推释一下如果我们将其排除在外，看看是否有任何更轻松的解决方案。如果我们<i>不了解</i>它在做什么，那么我们坐在那里盯着一个与数据不匹配的模拟，我们不知道问题是多么关键，以及我们是否缺少其他任何问题。</p><h2> 2.7胶质细胞基因表达，该表达在数小时内改变</h2><p>我昨天只是读过有关的：</p><blockquote><p>例如，在[下丘脑的上核核]中的星形胶质细胞显示时钟基因的节奏表达并影响昼夜节律运动行为（25） <a href="https://doi.org/10.1126/science.adh8488">[源]</a></p></blockquote><p>这有点是上面第2.3节的组合（与相同基本神经元构建块制成的许多电路相比，有很多特质的低级组成部分）和上面的第2.6节（如果您需要模拟基因表达，如果您需要模拟基因表达您想获得正确的高级行为）。</p><h2> 2.8代谢性（与离子型）受体几乎可以对任意时间表产生任意影响</h2><p>如果我正确理解的话，大脑使用大量的<a href="https://en.wikipedia.org/wiki/Ligand-gated_ion_channel">离子受体</a>，其作用将立即并局部改变离子流入神经元。伟大的！这很容易建模。</p><p>不幸的是，大脑<i>还</i>使用了许多<a href="https://en.wikipedia.org/wiki/Metabotropic_receptor">代谢型受体</a>（又称G蛋白偶联受体），它们的作用是（如果我正确理解的话）是可变的，确实几乎是任意的。基本上，当它们附着在配体上时，它们会引发一个信号级联，最终可以在任何时间尺度上对单元几乎产生任何影响。它可能会改变神经元的突触可塑性规则，可能会改变基因表达，它可能会增加或减少细胞的神经肽的产生，您可以命名。</p><p><strong>故事的寓意（我声称）是：</strong>如果您的目标是使用低级组件的自下而上模型获得正确的高级行为，那么您可能需要弄清楚所有这些信号级联是什么，在实验上，对于每个具有代谢受体的神经元。同样，我不是专家，但这似乎很难。</p><br/><br/> <a href="https://www.lesswrong.com/posts/wByPb6syhxvqPCutu/8-examples-informing-my-pessimism-on-uploading-without#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wbypb6syhxvqpcutu/8-examples-informing-my-pessimism-on-un-uploading-without<guid ispermalink="false"> wbypb6syhxvqpcutu</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Fri, 03 Nov 2023 20:03:50 GMT</pubDate> </item><item><title><![CDATA[Integrity in AI Governance and Advocacy]]></title><description><![CDATA[Published on November 3, 2023 7:52 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:15:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:15:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好的，所以我们俩都对最近的<a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy">猜想帖子有一些感觉，上面关于“ AI一致性中的许多人都在撒谎”</a>以及<a href="https://twitter.com/dw2/status/1716515784355160495">相关的营销活动和其他东西</a>。</p><p>我将感谢我可以考虑的某些背景，还可以分享我们在空间中拥有的信息，以帮助我们弄清楚发生了什么。</p><p>我希望这很快会导致我们最终遇到一些有关如何进行倡导的更广泛的问题，当前AI围绕AI的社交网络应作为一个小组协调，如何在研究中平衡倡导以及研究等等。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:18:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:18:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>关于猜想的感觉：</p><ul><li>关于人们没有表明自己的全部信念与认知环境混乱的许多好处，并使他人更加诚实。</li><li>撒谎和怯ward的框架让我感到不安。</li><li>我个人曾经有一个非常相似的猜想。自从搬到DC以来，我对治理人员更加同情。我们可以尝试弄清原因。</li><li>该帖子体现了我对猜想的话语和倡导方法的长期困扰，我发现我缺乏合作性和开放性<i>（注意：我在那里工作了〜半年。）</i></li></ul><p>我想的问题：</p><ul><li>人们应以生存风险的激励应如何开放？ （我的几个人的肩膀模型说“采用投资组合方法！”  - 好的，那是什么分配？）</li><li>人们应该如何倡导？我希望研究人员不必24/7鸣叫他们的信念，以便他们实际上可以完成工作</li><li>您如何看待这个，Oli？</li></ul></div></section><h2>对治理的同情心，人们对关键动机和隶属关系的持开放态度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:21:03 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:21:03 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>我个人曾经有一个非常相似的猜想。我现在对治理人更加同情。我们可以尝试弄清原因。</p></blockquote><p>这个方向对我来说似乎最有趣！</p><p>我目前在这个空间中的感受是，我对政府中的人们对其他一些东西的同情表示同情，我也想自己澄清一下这里的台词。</p><p>好奇您是否有任何关键的观察或经历，使您更加同情。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:25:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:25:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p><strong>观察结果</strong></p><p>我听说过至少一个人提出X风险的二手，然后他们的国会办公室对他们的重视程度降低了。其他工作人员告诉我谈论X风险的表现不会很好（如果没有具体证据，但我认真对待他们的意见）。</p><ul><li> （不过，这并没有向我更新。我的模型已经包括“大多数人会认为这很奇怪，而且对您的认真程度降低”。问题是，“您是否使人们更有可能以后做好事，所有事情通过提高信念，转移Overton窗口或说服1/10人等来考虑？”）</li></ul><p>我个人也发现谈论接管和存在风险很棘手，仅仅是因为这些想法需要很长时间来解释，并且在那里和我推荐的政策之间有许多推论步骤。因此，我经常很想简短地提及我的X风险动机，然后专注于最接近和真实的任何事情。 （从经典上讲，这将是“滥用风险，尤其是来自外国对手和恐怖分子的风险”，以及“未来几年来的生物武器和网络范围的能力”）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:28:07 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:28:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p><i>我们可能想稍后讨论的单独观点</i></p><p>我感到困惑的是：</p><p>我是否应该谈论推断的事情，使他们最有可能接受我在办公桌上的政策，</p><p><i>或者</i>，我是否应该只是咬一子，让人混淆并开始许多会议。科学家在我这边。请帮助。” - 我试图强调的是担心的语气。</p><p>因为我认为我们会系统地误导人们对我们的担忧，而不是通过不关注我们的实际问题，而不是以一种表达我们实际上有多担心的语气来谈论它们。</p><p> <i>（附加说明的其他解释添加了分解后：</i></p><ul><li>我试图区分两个问题，而人们没有公开分享和关注其存在风险的担忧：</li><li>第1期是，通过不关注您的生存风险担忧，您会扭曲人们对您认为实际上重要以及原因的感觉。我认为Habryka和猜想是正确的，可以指出这是欺骗性的（我认为从有害的认知效应的意义上，而不是不道德的意图）。</li><li>我试图在上面遇到的第2期是关于<a href="https://www.econlib.org/archives/2016/01/the_invisible_t.html">情绪缺失的</a>。人工智能治理通讯通常会像“ AI具有巨大的潜力，但也具有巨大的风险。AI可能被中国滥用，或者得到控制。我们应该平衡创新和安全的需求。”我不会称呼这个说谎（尽管我同意它会产生误导性效果，请参见第1期）。我要强调的是，这听起来不像一个认为我们所有人都会死的人。它不能传达“ <i>AI的前进非常可怕。处理这可能需要紧急，非常困难和前所未有的行动。</i> ”因此，我怀疑这并不是要使人们认真对待这个问题，以进行我们可能需要的主要治理干预措施。当然，我们可能会让政府官员在新闻稿中提到灾难性风险，但是他们<i>真的认真</i>对待10％的P（厄运）吗？如果没有，我们的通讯似乎不足。</li><li>当然，我怀疑我们应该始终使用“深切关注的”语气。这取决于我们要做什么。我猜想的问题是，我们现在要获得多少政策<i>，而</i>现在试图让人们像我们一样认真对待这个问题？另外，我承认这更加复杂，因为有时听起来的坟墓会让您从房间里笑而不是认真地笑，不同的观众有不同的需求等） </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:30:38 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:30:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>因此，我认为对我来说，如果他们正在与那些尚未真正处理这种工作的人交谈，我对发现它很难解释的人们感到完全同情，而且通常不值得解释他们的X风险问题。</p><p>就像，我一直都有这种经验，我与各种承包商合作或进行筹款活动，并且我会尽力解释我的工作是什么，但是它肯定最终会弄清楚他们有（有时这是标准的AI伦理，有时是认知科学和心理学，有时候人们认为我经营着一个试图最大化参与度指标的网络开发初创公司）。</p><p>我对我的同情程度不大的是“请不要谈论我与这个EA/X风险生态系统的联系，请不要与其他人谈论我对这个空间的信念，请不要将我列为公开参与此空间的任何事物等。”</p><p>或者， <a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy#E93CLAmc79jKgBLbX">我在前几天在评论中提到的</a>杰森·马西尼（Jason Matheny）的事情是，问他一个问题的参议员已经提到了“ AI的世界末日风险”，并问Matheny正在问Matheny，这将发生多大的可能性和时间；刚回答“我不知道”。</p><p>对我来说，那些人不会读过，因为人们在推理距离上遇到困难。他们对我来说更多的是试图在这里对他们的信念和隶属关系有意地混淆/欺骗性，这感觉更像是交叉线条。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:30:30 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:30:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>关于“请不要谈论我的联系或信念”  - </p><p>我不确定我对此有多糟糕。我通常喜欢开放。但是我通常也很好，人们非常私密，但是诚实地回答问题。例如，在网上写关于两个人的约会时，他们希望该信息是私人的，这很奇怪。我试图弄清现在和ea-coptariation之间的区别。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:34:43 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:34:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>好的，也许这是战略性欺骗性的差异。我说这一点。</p><p> （就您的观点而言，在我在AI政策中期间，在某些情况下，人们明确要求我淡化我们彼此了解的程度，或者说如果他们被问到，他们会淡化这一点。这是很令人沮丧的，因为现在我会遇到社会观点 /被视为叛逃的叛逃。我认为，他们〜通过使我处于这个位置来叛逃。）</p><p>我可能仍然感到困惑的是 - 记者可能是对抗性的。如果您不故意扭曲信息，那么掩饰物品似乎是合理的。想法？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:38:51 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:38:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p> （另外，我同情隐藏与治理相关的计划的原因是“我们不想太早产生一堆回击”。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:43:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:43:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，这绝对是对我看来很不好的事情，我也真的担心这是课程掩盖其造成的问题的一种事情（例如，我们真的不知道是否真的知道这造成了很多问题，因为人们自然会激励人们涵盖由此引起的问题，并且因为对这种阴谋行动的指控确实是高风险和困难的），因此，如果出错，可能会迅速出错，并有很多压抑的能量。</p><p>我确实完全认为在某些情况下，我会和朋友一起向其他人隐藏事情，并对它提高战略性。经典的例子是讨论您在您的领导下（苏联俄罗斯，纳粹德国等）的某种独裁政权的问题，以及诸如社会似乎对此的“同性恋”之类的事情，在这些情况下，在这些情况下，我正在选择其他人对我强加这种保密和混淆的要求。我也对人们有点多，这与我的生活非常相关，因为这仍然具有相当大的污名化。</p><p>我确实认为我经历了FTX崩溃后的巨大转变。想要了解您拥有的社会关系的很好的理由，因为他们上次没有注意这一点，事实证明这是过去十年中最大的阴谋之一。”</p><p>我特别关心这一点，因为实际上FTX/SAM实际上在推动法规和造成治理变化方面确实非常成功，但据我所知，他主要是对监管捕获的兴趣（与一群人交谈过在这个空间中似乎很了解的人，看来他距离基本上是通过监管捕获在美国进行衍生品交易的美国赞助的垄断距离不到一年的。就像，我无法区分他使用的方法与其他EAS现在使用的方法（尽管我确实注意到一些差异）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:43:20 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:43:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>是的，我要说</p><p>我想政府人民可能会说他们在纳粹的局势中有一堆污名。</p><p>但是鉴于FTX等，似乎应该得到一些污名。 （关于监管捕获部分的叹息 - 我不知道该程度。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:44:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:44:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>作为参考，查找的相关法规是《<a href="https://en.wikipedia.org/wiki/Digital_Commodities_Consumer_Protection_Act">数字商品消费者保护法》</a></p></div></section><h2> EAS对治理工作的感受和关注</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:44:55 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:44:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>我很好奇您列出EA治理人员正在做的一些您认为不好或可以理解的事情，因此我可以看到我是否不同意任何人。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:48:59 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:48:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>我确实认为，在高水平上，我从治理中追踪的最大影响是，在过去的5年左右的时间里，它们通常是最大的声音远离媒体，不要在各个地方接触高风险的人，因为他们担心这样做会让我们看起来像小丑，并会毒死。</p><p>然后，我目前的故事之一是，在某个时候，主要是在FTX之后，人们厌倦了听取一些模糊的EA保守共识，一群人开始无视该建议，最后开始公开地说话（例如FLI Letter， Eliezer的时间，CAIS Letter，Ian Hogarth的作品）。然后，这就是实际上在政策空间中移动的事情。</p><p>我的猜测是我们也许至少在一年前也可以做到这一点，老实说，鉴于我们在2015年对许多事情的牵引力，比尔·盖茨（Bill Gates）和埃隆·马斯克（Elon Musk）和德米斯当时我们还可以做很多Overton窗口，而我们没有这样做，我认为一项策略的下游想要与AI能力公司和政府中的随机人员一起维持大量社会资本，这些人会很奇怪人们在Overton窗口外说话。</p><p>虽然这只是一个故事，我还有其他故事，这些故事都取决于Chat-GPT和GPT-4，如果您提出了任何这些东西，那么您将在房间里被笑（尽管我确实认为2015年的超级智能是反对这一点的体面证据。对我来说，您需要内部和外部游戏内容的平衡，而且我们达到了不错的平衡，是的，拥有内部和外部游戏意味着不同游戏中的不同人之间会发生冲突，但最终是最终的正确呼吁。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:54:52 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:54:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>我对我对各种可能令人震惊的治理事物的感受的内省：</p><ul><li>明确降低了对X风险的担忧，您对Matheny的“我<strong>不</strong>知道”评论的阅读</li><li>明确淡化了与EA的联系，一个英国的人说：“如果有人问我我怎么知道奥利维亚，我会淡化我们的亲密友谊”：<strong>不好</strong></li><li>要求每个人保持安静，并避免与媒体交谈以避免有良好的选择：<strong>不好</strong><ul><li>要求每个人都安静地对政府加速AI能力的恐惧的恐惧：我不知道<strong>，苗条</strong></li></ul></li><li>要求人们更不用说您的EA隶属关系，并仔细避免自己提及：我不知道<strong>，取决于</strong><ul><li>为什么这可能会没事/好：EA具有不当的污名，对于AI政策而言，与EA越来越分开似乎有好处</li><li>为什么这可能很糟糕：也许在FTX之后，与EA相关的人应该自己开放</li><li>当前的和解：如果被问到，请诚实您的隶属关系，不要要求别人隐藏它们，但请有目的地与EA隔开距离并避免将其提起</li></ul></li><li>在推动差距太大的会议上没有发推文或提及X风险：我不知道<strong>，倾斜很好</strong><ul><li>我之间的矛盾是“我们可以通过对此非常开放和明确地将Overton窗口更宽，并提高每个人的理智”，而“经常向没有上下文的人分享您的观点是当地的繁重的” </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:55:21 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:55:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>我追踪的第二大一件事是，我担心在“我们”和“公共”或该领域中的某些东西之间会发生一种无法恢复的信任。</p><p>就像，进行此类信息管理一个大问题，您试图隐藏您的联系和隶属关系，这是人们之后很难再次信任您。如果您被抓住了，那么很难重建信任，即您将来不会这样做，而且我认为，当人们完全赶上正在发生的事情时，这种动态通常会导致一些非常强烈的免疫反应。</p><p>就像，我很担心我们最终会对美国和英国政府的EA人民产生一些麦卡锡风格的免疫反应有着强大的共同意识形态现在突然在政府中拥有如此巨大的权力吗？确实是唯一的选择”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:57:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:57:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p><strong>在“ CAIS/IAN/等。终于</strong><strong>在公共场合说”</strong></p><p>我认为当时的治理人很可能不会自己做这件事，没有其他人这样做。所以，我很高兴Cais做到了。</p><p>我并不完全确信，如果没有人们担心的反击，我们本可以在聊天前做到这一点。鉴于政府的反应多么出色（考虑风险的证据有限，人们对他们的恐惧非常开放），以及我们对“这听起来疯狂和不合理”（相对，人们对他们的恐惧有多限制）（相对，我肯定有所更新（相对）对我的期望）。</p><p><strong>关于公众</strong><strong>，</strong></p><p>我同意这是可能的，而且令人担忧。</p><p>我也很担心通过与公众互动，使X-fisk无法处理的是使AI召集变得如此拥挤。我至少同情：“不要让您实际上并不期望做出贡献的一群人参与其中，只是因为如果他们不包括在内，他们可能会感到不满”。</p></div></section><h2>政策世界中EA周围的污名</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:02:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:02:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>我之间的“ EA有不当污名”和“ FTX之后，每个人都应该把它置于非常开放”之间的矛盾之处”</p></blockquote><p>我有点想谈一谈。我觉得这是我听到的很多东西，我想我不会超级购买。 EA应该拥有什么不当的污名？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:08:15 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:08:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>您认为EA的污名都是应有的吗？</p><p>我注意到的污名：</p><ul><li>怪异的信念导致腐败，请参阅FTX<ul><li>可能夸大了每个人必须与FTX的联系程度。但是，只要FTX的决定与EA文化有关，就足够公平。</li></ul></li><li>与实验室有冲突，请参阅OpenAI和人类隶属关系<ul><li>很公平</li></ul></li><li>精英人士<ul><li>有点真实（大多是白人，富人，沿海精英），有点偏离（很多人都不来自财富，他们进入这个空间，因为他们想尽可能有效地无私）</li></ul></li><li>亿万富翁自私的利益<ul><li>似乎错了；我们主要是在努力帮助人们而不是赚钱</li></ul></li><li>不关心人的怪异的长期技术主义技术主义者<ul><li>奇怪的长期主义者，是的。</li><li>技术优势学，有点，但我们中的一些人对AI非常悲观，并使用AI来解决该问题。</li><li>不在乎人，主要是错误的。</li></ul></li></ul><p>我猜污名似乎在否定上是非常真实的，只是想念这里有认真的思想 /积极性。</p><p>如果记者说：“ Matheny是与EA相关的。这表明他与社区有关，对灾难性AI的风险最深刻地思考，历史上一直在努力并成功地做得很好。它也应该引起一些眉毛，请参阅FTX，请参阅FTX，”那我会没事的。但这通常只是下半年，这就是为什么我同情人们避免讨论他们的隶属关系的原因。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:10:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:10:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，我喜欢这种分析，我认为它粗略地跟踪了我的思考。</p><p>我确实认为“您对我的担忧是如此不合理，以至于我将积极地混淆自己的任何可能引发这些担忧的标记”。就像我认为酒吧不能在“好吧，我考虑过这些问题，它们是真的”一样，它必须在“我非常担心的是，当旗帜触发时，您会做一些不合理的事情”，就像他们一样与同性恋和共产主义播放者的东西在一起。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:12:25 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:12:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>很公平。这可能是治理人们高估诚实成本 /低估福利的情况，我仍然认为他们经常会做到这一点。</p><p> （我还要注意，如果所有知名人士都试图捍卫EA怎么办？ ， 所有的情况都被考虑到了。”）） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:14:17 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:14:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>我认为人们甚至不必捍卫EA之类的东西。我认为我认为有很多人想与自己的身份和社交网络保持距离，因为他们对此有真正的担忧。</p><p>但是我认为防御肯定会为对话打开大门，以确认这里有一个真实的事情具有很大的力量和影响力，并且会邀请人们跟踪该事物的结构以及它在未来，如果发生这种情况，我将不太关心负面的认知效应和这一切都在我脸上爆炸的下行风险。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:16:13 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:16:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>是的，我会对部分感兴趣，因为我希望持怀疑态度的人知道我不认为他们疯了，而且我并不想忽略它们或对他们撒谎。我只是选择继续使用/附近的EA做事，因为他们思考良好并且有资源，这有助于我实现我的无私目标。如果怀疑论者就像“好吧，我仍然不信任你”，那就足够公平。</p><p>您是否有想法使您想在这里发生的事情发生？它是什么样子的？ X人的专栏？</p></div></section><h2>我们如何使政策内容更加透明？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:19:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:19:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>可能有些有争议，但是我对已经发布的<a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">政治</a>作品感到很高兴。我们有两个基本上试图证明这种情况的DC中有一个EA阴谋，具有一种不负责任的方式。</p><p>也许有人可以与作者接触，就像“好吧，是的，我们有点阴谋，对此感到抱歉。但是我想让我们尝试清洁，我会告诉你我所知道的所有东西，而你认真对待我们确实不是为了利用AI而做到这一点的假设，而是因为我们真正担心AI的灾难性风险。</p><p>这确实是一个注定的计划，但是就像空间中的某些东西对我感觉很好。也许我们可以与一些我们知道的记者合作，写一件事，将卡片放在桌子上，而不仅仅是试图以最积极的方式构架一切的粉扑，而是真正提出了棘手的问题。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:20:09 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:20:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p> POLITICO：+1很高兴它实际上出来了！</p><ul><li> （我暂时希望人们刚刚自己说过这句话，而不是“发现”。这可能是我将如何向我要求将来更加开放的人们的理由的一部分。）</li><li> （此外，我对Politico的喜悦的一部分是越多的人，人们可以评估这实际上阻碍他们的工作的治理越多 - 到目前为止，我认为很少 - 更新是为了更开放或更加开放或更多现在开放，因为现在他们的从属关系已经揭示）</li></ul><p>我喜欢与记者接触的想法。我只想找到一个看似寻求真相的人，然后单方面分享信息。</p><p>你能做到这一点吗？您想让一些更大的EA /治理人员这样做吗？我很乐意把草稿切成些。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>我想我可能会为自己的保密性过多，无法正确地跳舞，尽管我不确定，但我不确定自己的舞蹈。我也许可以从一群人那里获得买入，以便我可以在这里公开讲话，但是这会增加工作量，而且他们也不是不错的机会，他们不说“是”，然后我会需要是超级偏执的，我在处理此操作时会泄漏。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:23:16 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:23:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>就像在那一样，您同意保留太多秘密？</p><p>如果是这样，您是否有人们对此不那么负担（并且有相关背景）负担的人？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，太多的秘密。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:25:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:25:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>我认为大多数大人物都有类似的保密负担。</p><p>我愿意自己做，但没有很多上下文，我不确定我是否想将与此相关的组织拖入其中。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:29:02 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:29:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，理想情况下，这将是大人物之一，因为我认为这会有意义地导致人们在空间中运作的方式发生变化。</p><p> Eliezer擅长于这样移动Overton Windows，但我认为他对跟踪类似的详细社会动态真的不感兴趣，因此真的不知道发生了什么。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:30:24 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:30:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>您是否有时间与人们聊天有关这个想法或在Google文档周围发送？但是很高兴提供帮助。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:32:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:32:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>我确实对实现这一目标感到非常兴奋，尽管我确实认为这会非常积极地关闭，而且我对此感到难过，并且对此感到同情，因为它确实感觉不可避免地涉及到抓住某些人有充分理由或处于更具对抗性的环境中的越野野火，在这里，这里的信息将以不公平的方式使用，我仍然认为这是值得的，但是这确实使我感到这样会很难。</p><p>我还注意到，我只是担心如果我要写一篇文章会发生什么，就像“对ea-ish/x-fiskish政策格局的概述”，该帖子命名了特定的人并解释了各种历史计划。就像我期望的那样，这会让我很多敌人。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:34:23 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:34:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>同样的是，我的某些恐惧是“这可能会使&#39;好的计划更加困难地变得更加困难” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:35:42 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:35:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好的，我想我会遵守这个计划。我以前从未真正考虑过，我想在接下来的几周内将它带到一群人中，看看是否有足够的支持来实现这一目标。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:36:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:36:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>好的！ （就价值而言，如果他愿意进入社交活动，我目前最喜欢Eliezer）</p><p>从DC Folk那里收集的任何信息对我有帮助吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:39:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:39:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>哦，我的意思是，我会喜欢更多的数据，说明这会让DC的人感觉像是从肩膀上解除了一些负担，而是感觉就像他们的计划一样。</p><p>我认为我在这里的实际计划可能更像是一个EA论坛帖子，或者对DC中发生的事情进行了很多细节，并且不怕命名特定的名称或组织。</p><p>我可以想象，直接去做一个专栏也可以很好地运作，并且对局外人来说可能会更具说服力，尽管理想情况下，您可以同时拥有两者。有人在内部写论坛帖子，然后一些外部政党验证了一堆东西，然后挖得更深一些，然后根据该帖子进行一些批评，然后面纱被打破。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:39:49 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:39:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>知道了。</p><p> DC帖子是否会包含这些人要求/要求您保密的信息？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:40:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:40:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>肯定会“会”，尽管如果我这样做，我想签署我打算对与我交谈的任何人都非常清楚地做到这一点。</p><p>尽管不是很多，但我也对这里的一些秘密感到负担，我也许可以以某种方式摆脱这些负担。没有把握。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:41:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:41:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯（Olivia Jimenez）</b></section><div><p>好的，我将在接下来的两个星期中问。如果我不发送更新，请给我</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>谢谢你！！</p></div></section><h2>对猜想的担忧</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:50 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好的，回到顶级的，我想我仍然想更多地总结我对猜想的事情的感觉。</p><p>就像，我想如果我不在这样的背景下说我会感到难过的事情，就像是“但是，我觉得有些猜想的人就像我的列表中的顶部几个月前，人们试图做奇怪的内部游戏中扭曲的东西，这使他们喊出像这样的人对我感到非常难过。”</p><p> In-general a huge component of my reaction to that post was something in the space of &quot;Connor and Gabe are kind of on my list to track as people I feel most sketched out by in a bunch of different ways, and kind of in the ways the post complaints about&quot; and I feel somewhat bad for having dropped my efforts from a few months ago about doing some more investigation here and writing up my concerns (mostly because I was kind of hoping a bit that Conjecture would just implode as it ran out of funding and maybe the problem would go away) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:06 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (For what it&#39;s worth, Conjecture has been pretty outside-game-y in my experience. My guess is this is mostly a matter of &quot;they think outside game is the best tactic, given what others are doing and their resources&quot;, but they&#39;ve also expressed ethical concerns with the inside game approach.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:46:37 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:46:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> (For some context on this, Conjecture tried really pretty hard a few months ago to get a bunch of the OpenAI critical comments on <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">this post</a> deleted because they said it would make them look bad to OpenAI and would antagonize people at labs in an unfair way and would mess with their inside-game plans that they assured me were going very well at the time) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:54 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (I heard a somewhat different story about this from them, but sure, I still take it as is evidence that they&#39;re mostly &quot;doing whatever&#39;s locally tactical&quot;) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:53:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:53:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Anyway, I was similarly disappointed by the post just given I think Conjecture has often been lower integrity and less cooperative than others in/around the community. For instance, from what I can tell,</p><ul><li> They often do things of the form &quot;leaving out info, knowing this has misleading effects&quot;</li><li> One of their reasons for being adversarial is &quot;when you put people on the defense, they say more of their beliefs in public&quot;. Relatedly, they&#39;re into <a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/">conflict theory</a> , which leads them to favor &quot;fight for power&quot; >; &quot;convince people with your good arguments.&quot;</li></ul><p> I have a doc detailing my observations that I&#39;m open to sharing privately, if people DM me.</p><p> (I discussed these concerns with Conjecture at length before leaving. They gave me substantial space to voice these concerns, which I&#39;m appreciative of, and I did leave our conversations feeling like I understood their POV much better. I&#39;m not going to get into &quot;where I&#39;m sympathetic with Conjecture&quot; here, but I&#39;m often sympathetic. I can&#39;t say I ever felt like my concerns were <i>resolved</i> , though.)</p><p> I would be interested in your concerns being written up.</p><p> I do worry about the EA x Conjecture relationship just being increasingly divisive and time-suck-y. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:53:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:53:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is an email I sent Eliezer on April 2nd this year with one paragraph removed for confidentiality reasons:</p><hr><p> Hey Eliezer,</p><p> This is just an FYI and I don&#39;t think you should hugely update on this but I felt like I should let you know that I have had some kind of concerning experiences with a bunch of Conjecture people that currently make me hesitant to interface with them very much and make me think they are somewhat systematically misleading or deceptive. A concrete list of examples:</p><p> I had someone reach out to me with the following quote:<br></p><blockquote><p> Mainly, I asked one of their senior people how they plan to make money because they have a lot of random investors, and he basically said there was no plan, AGI was so near that everyone would either be dead or the investors would no longer care by the time anyone noticed they weren&#39;t seeming to make money. This seems misleading either to the investors or to me — I suspect me, because it would really just be wild if they had no plan to ever try to make money, and in fact they do actually have a product (though it seems to just be Whisper repackaged)</p></blockquote><p> I separately had a very weird experience with them on the Long Term Future Fund where Conor Leahy applied for funding for Eleuther AI. We told him we didn&#39;t want to fund Eleuther AI since it sure mostly seemed like capabilities-research but we would be pretty interested in funding AI Alignment research by some of the same people.</p><p> He then confusingly went around to a lot of people around EleutherAI and told them that &quot;Open Phil is not interested in funding pre-paradigmatic AI Alignment research and that that is the reason why they didn&#39;t fund Eleuther AI&quot;.</p><p> This was doubly confusing and misleading because Open Phil had never evaluated a grant to Eleuther AI (Asya who works at Open Phil was involved in the grant evaluation as a fund member, but nothing else), and of course the reason he cited had nothing to do with the reason we actually gave. He seems to have kept saying this for a long time even after I think someone explicitly corrected the statement to him.</p><p> Another experience I had was Gabe from Conjecture reaching out to LessWrong and trying really quite hard to get us to delete the OpenAI critical comments on this post: <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-开放性</a></p><p>He said he thought people in-general shouldn&#39;t criticize OpenAI in public like this because this makes diplomatic relationships much harder, and when Ruby told them we don&#39;t delete that kind of criticism he escalated to me and generally tried pretty hard to get me to delete things.</p><p> [... One additional thing that&#39;s a bit more confidential but of similar nature here...]</p><p> None of these are super bad but they give me an overall sense of wanting to keep a bunch of distance from Conjecture, and trepidation about them becoming something like a major public representative of AI Alignment stuff. When I talked to employees of Conjecture about these concern the responses I got also didn&#39;t tend to be &quot;oh, no, that&#39;s totally out of character&quot;, but more like &quot;yeah, I do think there is a lot of naive consequentialism here and I would like your help fixing that&quot;.</p><p> No response required, happy to answer any follow-up questions. Just figured I would err more on the side of sharing things like this post-FTX.</p><p>最好的，<br>奥利弗</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:57:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:57:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p><p> Again, I&#39;m not sure &quot;dealing with Conjecture&quot; is worth the time though. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:01:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:01:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Main emotional affects of the post for me</p><ul><li> I wish someone else had made these points, less adversarially. I feel like governance people do need to hear them. But the frame might make people less likely to engage or make the engagement.<ul><li> Actually, I will admit the post generated lots of engagement in comments and this discussion. It feels uncooperative to solicit engagement via being adversarial though.</li></ul></li><li> I&#39;m disappointed the comments were mostly &quot;Ugh Conjecture is being adversarial&quot; and less about &quot;Should people be more publicly upfront about how worried they are about AI?&quot;</li><li> There were several other community discussions in the past few weeks that I&#39;ll tentatively call &quot;heated community politics&quot;, and I&#39;m feel overall bad about the pattern.<ul><li> (The other discussions were around whether RSPs are bad and whether Nate Soares is bad. In all three cases, I felt like those saying &quot;bad!&quot; had great points, but (a) their points were shrouded in frames of &quot;is this immoral &quot; that felt very off to me, (b) they felt overconfident and not truth-seeking, and (c) I felt like people were half-dealing with personal grudges. This all felt antithetical to parts of LessWrong and EA culture I love. ） </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:07:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:07:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, that also roughly matches my emotional reaction. I did like the other RSP discussion that happened that week (and liked my dialogue with Ryan which I thought was pretty productive).</p></div></section><h2> Conjecture as the flag for doomers </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:08:55 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:08:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p></blockquote><p> Yeah, I share this feeling. I am quite glad MIRI is writing more, but am also definitely worried that somehow Conjecture has positioned itself as being aligned with MIRI in a way that makes me concerned people will end up feeling deceived. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Two thoughts</p><ul><li> Conjecture does seem pretty aligned with MIRI in &quot;shut it all down&quot; and &quot;alignment hard&quot; (plus more specific models that lead there).</li><li> I notice MIRI isn&#39;t quite a satisfying place to rally around, since MIRI doesn&#39;t have suggestions for what individuals can do. Conjecture does. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:31 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Can you say more about the feeling deceived worry?</p><p> (I didn&#39;t feel deceived having joined myself, but maybe &quot;Conjecture could&#39;ve managed my expectations about the work better&quot; and &quot;I wish the EAs with concerns told me so more explicitly instead of giving very vague warnings&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:18:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:18:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, for better or for worse I think a lot of people seem to make decisions on the basis of &quot;is this thing a community-sanctioned &#39;good thing to do (TM)&#39;&quot;. I think this way of making decisions is pretty sus, and I feel a bit confused how much I want to take responsibility for people making decisions this way, but I think because Conjecture and MIRI look similar in a bunch of ways, and I think Conjecture is kind of explicitly is trying to carry the &quot;doomer&quot; flag, a lot of people will parse Conjecture as &quot;a community-sanctioned &#39;good thing to do (TM)&#39;&quot;.</p><p> I think this kind of thing then tends to fail in one of two ways:</p><ul><li> The person who engaged more with Conjecture realizes that Conjecture is much more controversial than they realized within the status hierarchy of the community and that it&#39;s not actually clearly a &#39;good thing to do (TM)&#39;, and then they will feel betrayed by Conjecture for hiding that from them and betrayed by others by not sharing their concerns with them</li><li> The person who engaged much more with Conjecture realizes that the organization hasn&#39;t really internalized the virtues that they associate with getting community approval, and then they will feel unsafe and like the community is kind of fake in how it claims to have certain virtues but doesn&#39;t actually follow them in the projects that have &quot;official community approval&quot;</li></ul><p> Both make me pretty sad.</p><p> Also, even if you are following a less dumb decision-making structure, the world is just really complicated, and especially with tons of people doing hard-to-track behind the scenes work, it is just really hard to figure out who is doing real work or not, and Conjecture has been endorsed by a bunch of different parts of the community for-real (like they received millions of dollars in Jaan funding, for example, IIRC), and I would really like to improve the signal to noise ratio here, and somehow improve the degree to which people&#39;s endorsements accurately track whether a thing will be good. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:19:04 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:19:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p>公平的。 People did warn me before I joined Conjecture (but it didn&#39;t feel <i>very</i> different from warnings I might get before working at MIRI). Also, most people I know in the community are aware Conjecture has a poor reputation.</p><p> I&#39;d support and am open to writing a Conjecture post explaining the particulars of</p><ul><li> Experiences that make me question their integrity</li><li> Things I wish I knew before joining</li><li> My thoughts of their lying post and RSP campaign (tl;dr: important truth to the content, but really dislike the adversarial frame) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:19:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:19:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, maybe this dialogue will help, if we edit and publish a bunch of it.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy<guid ispermalink="false"> vFqa8DZCuhyrbSnyx</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:52:33 GMT</pubDate> </item><item><title><![CDATA[Averaging samples from a population with log-normal distribution]]></title><description><![CDATA[Published on November 3, 2023 7:42 PM GMT<br/><br/><p> This was originally a comment on <a href="https://www.lesswrong.com/posts/HjdqPbhRx2ceXHzr2/averages-and-sample-sizes">this post</a> by <a href="https://www.lesswrong.com/users/mruwnik?from=post_header">mruwnik</a> regarding averaging various distributions with different distributions. I made it a post to include pictures.</p><p> The Central Limit Theorem, henceforth CLT, states (in my own words) that regardless of the distribution of a population, sample averages from that population should be normally distributed.</p><p> In theory it should hold for log-normal distributions but that doesn&#39;t feel intuitive to me so I tested it.</p><h3> A silly example of CLT</h3><p> An example I made up in my head to make sense of it:</p><p> Imagine a population comprised of all the people who nap 2 times in a day. Lets plot the ages of this population: </p><figure class="image image_resized" style="width:55.84%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/h8slnvin9v75wbh6vrww" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m83kja1xoshnh65prhwn 158w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/c3n2ytq015maqwxq5jba 238w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/fnslxyz5ukg4nq5lbqnn 318w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lhlkzjbmbklwjlcleskp 398w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/d0i6iip8afyolf7sojf5 478w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/flkvsioij0l7b0iy3fh0 558w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m12rn0wnnl1qh1zwgkef 638w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ora4h7hhuos4jwdjff40 718w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ui07ar9vidttgwmedksl 798w"></figure><p> Mostly infants and elderly people nap, hence the shape of the graph. This data is NOT normal. But if you randomly pick a small sample (n=10) from this population and average it, it will have a mix of old people and infants that averages to middle-age. For example imagine the ages are 80,2,1,2,75,76,1,1,85,70 this will average to about 39. If you do this over and over again with randomly chosen samples you will get a normal distribution 。 </p><figure class="image image_resized" style="width:76.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jac6cfliah7kii8yldah" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/f55e63dxrlpbd0lbtbsd 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/t6skzwhvzcd4mwxzyrmg 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/g98yjmoh0lybi1axjgyk 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/cblxwycjksseq7uciz0u 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qjutlba1ssdjxfi6sub6 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ggs5rexxsni2n4abocns 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jorzwfr2o044c8llhxur 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/rqepyv05b0qjkmj9u0er 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/te52ibxfnejqunsxfldn 803w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lruwlnhyxygaslo486cb 810w"></figure><h3> Does it work with log-normal populations?</h3><p> I didn&#39;t find it intuitive this would work for a log-normal population.</p><p> If I take data that is log-normal but split it into small samples, will the average of those small samples be normally distributed?</p><h2><br> <strong>Chess matches:</strong></h2><p> I am arranging a chess tournament. I need to figure out how long the average match is so I can plan accordingly. I hear that chess matches seem to follow a log-normal distribution, but I&#39;m not sure what that means statistically so I will try to just average the game times.</p><h3> Data from the population</h3><p> This is what my fake population (n=100,000) looks like. Its log-normal. </p><figure class="image image_resized" style="width:76.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vzjr4piaqb8jny5tvwaz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/zexn7ioinaufqpqr1ib2 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/twpextrya3mba53rof5l 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tqoui3ludnorhanftcov 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ykh6jnxjestjqlnynh6j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lbue48tbeoeyajh2r7gt 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/dcslvkhztoz75zy6st2j 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hmltesa1amkoz1yyirgb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vxetqqcgbjrmwbmdxalq 640w"></figure><h3>锦标赛</h3><p>I observe tournaments (n=100 games) and take a simple average of the match length.</p><p> Here is a histogram plot of the tournaments </p><figure class="image image_resized" style="width:79.15%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/u8klmbrfmlo3xue06tfd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hyx1g6tjajdljqxgse0d 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qei659xjaqu8jqrnevba 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vnmbcmftpueyozmtiibo 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/idvjzyrzpu4bmcg21lam 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/y9ejbiwz2lgjpw852vfw 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tpud4pifh8txi1blprvt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ltxq5cqjowsbvyttkdoj 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/optpwzmplwgecp3euut4 640w"></figure><h2> Lessons?</h2><p> The sample size does matter here. A sample size too small (n=10) and you just end up with the original log-normal distribution. This is expected as the sample size moves from small to large you get a range of smoothing effects pushing the distribution to normal until you get a single point, when the sample = population.</p><br/><br/> <a href="https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal<guid ispermalink="false"> GkEW4vH6M6pMdowdN</guid><dc:creator><![CDATA[CrimsonChin]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:42:17 GMT</pubDate> </item><item><title><![CDATA[Securing Civilization Against Catastrophic Pandemics]]></title><description><![CDATA[Published on November 3, 2023 7:33 PM GMT<br/><br/><h3>执行摘要</h3><p>Pandemic security aims to safeguard the future of civilization from exponentially spreading biological threats. Despite the world&#39;s failure to contain SARS-CoV-2, the existence of far more lethal and transmissible pathogens that afflict animals and growing access to increasingly powerful biotechnologies, no analyses of worst-case scenarios and potential defenses have been published. Here we outline two distinct mechanisms by which pandemic pathogens transmissible between humans could cause societal collapse. In a &quot;Wildfire&quot; pandemic, the justifiable fear of a lethal and highly contagious respiratory agent released in multiple travel hubs leads to the breakdown of essential services. In a &quot;Stealth&quot; pandemic, a rapidly spreading virus with a long incubation period analogous to HIV infects most of humankind. We explain why current pandemic preparedness measures such as rapid vaccines and N95 masks will reliably fail against these threats and outline novel strategies and technologies capable of safeguarding civilisation.</p><h3>要点</h3><ul><li>Nations cannot yet contain natural, accidental or deliberate pandemics.</li><li> Access to severe pandemics will expand with the ability to program biology.</li><li> If too many essential workers die or refuse to work, societies will collapse.</li><li> A Wildfire pandemic is highly lethal and transmissible enough to infect most essential workers who are taking currently available precautions.<ul><li> Collapse can be prevented by providing essential workers with pandemic-proof personal protective equipment (P4E). (Essential workers are those who must deliver food, water, power and law enforcement without any interruptions.)</li><li> Others can remain safely at home until P4E is available for everyone.</li><li> Once the population is protected, the virus can be locally eradicated.</li></ul></li><li> A Stealth pandemic spreads widely with few symptoms and causes severe harm years later.<ul><li> Societal collapse can be prevented via early warning, credibility, cures, P4E and healthy buildings.<ul><li> Early warning: deep metagenomic sequencing offers reliable detection.</li><li> Credibility: expert responders can assess threats and encourage action.</li><li> Cures: swift medical research can offer hope for the infected.</li><li> P4E: people will need protective equipment that they can trust to block transmission.</li><li> Healthy buildings: the use of germicidal lights and ventilation can prevent indoor infections.</li></ul></li></ul></li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics<guid ispermalink="false"> ksevLrNby34TJKGSF</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:33:06 GMT</pubDate> </item><item><title><![CDATA[If AGI is imminent, why can’t I hail a robotaxi?]]></title><description><![CDATA[Published on November 3, 2023 6:11 PM GMT<br/><br/><p> My intuition is that driving is a domain narrow enough not to require AGI and, moreover, to require a system of far less sophistication and reasoning capabilities than an AGI. SAE Level 5 autonomy — which requires a vehicle to be able to drive autonomously wherever and whenever a typical human driver could — has not been achieved by any company. All autonomous driving projects currently require a human in the loop, either in the driver&#39;s seat or available to provide remote assistance.</p><p> In a world where AGI is achieved by, say, 2030 or 2035, what are the odds Level 5 autonomy hasn&#39;t been solved by 2023? My intuition is that we would expect autonomous vehicles to be a relatively low-hanging fruit that is plucked relatively early in the trajectory from AI solving video games to AI solving ~everything.</p><p> There are a few reasons why this intuition could be wrong:</p><ol><li><p> Maybe self-driving is actually an AGI-level problem or much closer to AGI-level than my intuition tells me. (I would rate this as highly plausible.)</p></li><li><p> Maybe AI progress is such a steep exponential that the lag time between Level 5 autonomy and AGI is much shorter than my intuition tells me. (I would rate this as moderately plausible.)</p></li><li><p> Perhaps Internet-scale data simply isn&#39;t available to train self-driving AIs. (I would rate this as fairly implausible; it would be more much plausible if Tesla weren&#39;t such a clear counterexample.)</p></li><li><p> Robotics in general could prove to be either too hard or unimportant for an otherwise transformative or general AI. (I would rate this as highly implausible; it strikes me as special pleading.)</p></li></ol><p> Please enumerate any additional reasons you can think of in the comments. Also, please present any arguments or evidence you can think of as to why I should accept any of the reasons given above.</p><br/><br/> <a href="https://www.lesswrong.com/posts/cyycbDAffNc6aghas/if-agi-is-imminent-why-can-t-i-hail-a-robotaxi#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/cyycbDAffNc6aghas/if-agi-is-imminent-why-can-ti-hail-a-robotaxi<guid ispermalink="false"> cyycbDAffNc6aghas</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 03 Nov 2023 22:02:21 GMT</pubDate> </item><item><title><![CDATA[Thoughts on open source AI]]></title><description><![CDATA[Published on November 3, 2023 3:35 PM GMT<br/><br/><p> <i>Epistemic status: I only ~50% endorse this, which is below my typical bar for posting something. I&#39;m more bullish on “these are arguments which should be in the water supply and discussed” than “these arguments are actually correct.” I&#39;m not an expert in this, I&#39;ve only thought about it for ~15 hours, and I didn&#39;t run this post by any relevant experts before posting.</i></p><p><i>感谢 Max Nadeau 和 Eric Neyman 的有益讨论。</i></p><p> Right now there&#39;s a significant amount of public debate about open source AI. People concerned about AI safety generally argue that open sourcing powerful AI systems is too dangerous to be allowed; the classic example here is &quot;You shouldn&#39;t be allowed to open source an AI system which can <a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant"><u>produce step-by-step instructions for engineering</u></a> novel pathogens.&quot; On the other hand, open source proponents argue that open source models haven&#39;t yet caused significant harm, and that trying to close access to AI will result in <a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>concentration</u></a> of <a href="https://open.mozilla.org/letter/"><u>power</u></a> in the hands of a few AI labs.</p><p> I think many AI safety-concerned folks who haven&#39;t thought about this that much tend to vaguely think something like “open sourcing powerful AI systems seems dangerous and should probably be banned.” Taken literally, I think this plan is a bit naive: when we&#39;re colonizing Mars in 2100 with the help of our aligned superintelligence, will releasing the weights of GPT-5 really be a catastrophic risk?</p><p> I think a better plan looks something like &quot;You can&#39;t open source a system until you&#39;ve determined and disclosed the sorts of threat models your system will enable, and society has implemented measures to become robust to these threat models. Once any necessary measures have been implemented, you are free to open-source.&quot;</p><p> I&#39;ll go into more detail later, but as an intuition pump imagine that: the best open source model is always 2 years behind the best proprietary model (call it GPT-SoTA) <span class="footnote-reference" role="doc-noteref" id="fnrefnmr2zc5cm4r"><sup><a href="#fnnmr2zc5cm4r">[1]</a></sup></span> ; GPT-SoTA is widely deployed throughout the economy and deployed to monitor for and prevent certain attack vectors, and the best open source model isn&#39;t smart enough to cause any significant harm without GPT-SoTA catching it. In this hypothetical world, so long as we can trust GPT-SoTA <i>,</i> we are safe from harms caused by open source models. In other words, so long as the best open source models lag sufficiently behind the best proprietary models and we&#39;re smart about how we use our best proprietary models, open sourcing models isn&#39;t the thing that kills us.</p><p> In this rest of this post I will:</p><ul><li> Motivate this plan by analogy to responsible disclosure in cryptography</li><li> Go into more detail on this plan</li><li> Discuss how this relates to my understanding of the current plan as implied by responsible scaling policies (RSPs)</li><li> Discuss some key uncertainties</li><li> Give some higher-level thoughts on the discourse surrounding open source AI</li></ul><h2> <strong>An analogy to responsible disclosure in cryptography</strong></h2><p> <i>[I&#39;m not an expert in this area and this section might get some details wrong. Thanks to Boaz Barak for pointing out this analogy (but all errors are my own).</i></p><p> <i>See this footnote</i> <span class="footnote-reference" role="doc-noteref" id="fnreflndvavl4r2a"><sup><a href="#fnlndvavl4r2a">[2]</a></sup></span> <i>for a discussion of alternative analogies you could make to biosecurity disclosure norms, and whether they&#39;re more apt to risk from open source AI.]</i></p><p> Suppose you discover a vulnerability in some widely-used cryptographic scheme. Suppose further that you&#39;re a good person who doesn&#39;t want anyone to get hacked.你该怎么办？</p><p> If you publicly release your exploit, then lots of people will get hacked (by less benevolent hackers who&#39;ve read your description of the exploit). On the other hand, if <a href="https://en.wikipedia.org/wiki/White_hat_(computer_security)"><u>white-hat</u></a> <u>&nbsp;</u> hackers always keep the vulnerabilities they discover secret, then the vulnerabilities will never get patched until a black-hat hacker finds the vulnerability and exploits it. More generally, you might worry that not disclosing vulnerabilities could lead to a &quot;security overhang,&quot; where discoverable-but-not-yet-discovered vulnerabilities accumulate over time, making the situation worse when they&#39;re eventually exploited.</p><p> In practice, the cryptography community has converged on a <i>responsible disclosure</i> policy along the lines of:</p><ul><li> First, you disclose the vulnerability to the affected parties.<ul><li> As a running example, consider Google&#39;s <a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"><u>exploit for the SHA-1 hash function</u></a> . In this case, there were many affected parties, so Google publicly posted a proof-of-concept for the exploit, but didn&#39;t include enough detail for others to immediately reproduce it.</li><li> In other cases, you might privately disclose more information, eg if you found a vulnerability in the Windows OS, you might privately disclose it to Microsoft along with the code implementing an exploit.</li></ul></li><li> Then you set a reasonable time-frame for the vulnerability to be patched.<ul><li> In the case of SHA-1, the patch was &quot;stop using SHA-1&quot; and the time-frame for implementing this was 90 days.</li></ul></li><li> At the end of this time period, you may publicly release your exploit, including with source code for executing it.<ul><li> This ensures that affected parties are properly incentivized to patch the vulnerability, and helps other white-hat hackers find other vulnerabilities in the future.</li></ul></li></ul><p> As I understand things, this protocol has resulted in our cryptographic schemes being relatively robust: people mostly don&#39;t get hacked in serious ways, and when they do it&#39;s mostly because of attacks via social engineering (eg <a href="https://en.wikipedia.org/wiki/Operation_Rubicon"><u>the CIA secretly owning their encryption provider</u></a> ), not via attacks on the scheme. <span class="footnote-reference" role="doc-noteref" id="fnref3ejitdom769"><sup><a href="#fn3ejitdom769">[3]</a></sup></span></p><h2> <strong>Responsible disclosure for capabilities of open source AI systems: an outline</strong></h2><p> <i>[Thanks to Yusuf Mahmood for pointing out that the protocol outlined in this section is broadly similar to the one</i> <a href="https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf"><i><u>here</u></i></a> <i>. More generally, I expect that ideas along these lines are already familiar to people who work in this area.]</i></p><p> In this section I&#39;ll lay out an protocol for open sourcing AI systems which is analogous to the responsible disclosure protocol from cryptography. Suppose the hypothetical company Mesa has trained a new AI system <a href="https://en.wikipedia.org/wiki/Camelidae"><u>camelidAI</u></a> which Mesa would like to open source. Let&#39;s also call the most capable proprietary AI system GPT-SoTA, which we can assume is well-behaved <span class="footnote-reference" role="doc-noteref" id="fnref1art7tmmdob"><sup><a href="#fn1art7tmmdob">[4]</a></sup></span> . I&#39;m imagining that GPT-SoTA is significantly more capable than camelidAI (and, in particular, is superhuman in most domains). In principle, the protocol below will still make sense if GPT-SoTA is worse than camelidAI (because open source systems have surpassed proprietary ones), but it will degenerate to something like “ban open source AI systems once they are capable of causing significant novel harms which they can&#39;t also reliably mitigate.”</p><p> In this protocol, before camelidAI can be open sourced, [?? Mesa?, the government?, a third-party? ？？] 必须：</p><ul><li> Evaluate camelidAI for what sorts of significant novel harms it could cause if open-sourced. <span class="footnote-reference" role="doc-noteref" id="fnref7501lf6ibc3"><sup><a href="#fn7501lf6ibc3">[5]</a></sup></span> These evaluators should have, at a minimum, access to all the tools that users of the open source system will have, including eg the ability to finetune camelidAI, external tooling which can be built on top of camelidAI, and API calls to GPT-SoTA. So a typical workflow might look something like: have GPT-SoTA generate a comprehensive list of possible takeover plans, then finetune camelidAI to complete steps in these plans. For example, we might find that:<ul><li> After finetuning, camelidAI is capable of targeted phishing (also called <a href="https://en.wikipedia.org/wiki/Phishing"><u>spear-phishing</u></a> ) at human-expert levels, but scaled up to many more targets.</li><li> camelidAI can provide <a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.13952.pdf"><u>layperson-followable instructions for manufacturing a novel pathogen, including which DNA synthesis companies and biological labs don&#39;t screen customers and orders</u></a> .</li></ul></li><li> Disclose these new harmful capabilities to [?? the government?, a third-party monitor?, affected parties? ??].</li><li> Work with relevant actors to improve systems until they are robust to everyone having access to camelidAI.<ul><li> Eg make sure that there is a widely-available open-source tool which can detect phishing attempts as sophisticated as camelidAI&#39;s with very high reliability.</li><li> Eg shut down the DNA synthesis companies and biolabs that don&#39;t screen orders, or force them to use GPT-SoTA to screen orders to potential pandemic agents.</li><li> Note that if camelidAI is very capable, some of these preventative measures might be very ambitious, eg “make society robust to engineered pandemics.” The source of hope here is that we have access to a highly capable and well-behaved GPT-SoTA.</li><li> Note also that these “robustification” measures are things that we should do anyway, even if we didn&#39;t want to open source camelidAI; otherwise there would be an overhang that a future unaligned AI (possibly a model which was open sourced illegally) could exploit.</li></ul></li><li> Once society is robust to harms caused by camelidAI (as certified by [??]), you are allowed to open source camelidAI.</li><li> On the other hand, if Mesa open sources camelidAI before finishing the above process, then it&#39;s treated as a bad actor (similarly to how we would treat a hacker who releases an exploit without responsible disclosure).<ul><li> Maybe this means that you are held liable for harms caused by camelidAI or something, not really sure.</li></ul></li></ul><p> As examples, let me note two special cases of this protocol:</p><ul><li> Suppose camelidAI = LLaMA-2. I think probably there are no significant novel harms enabled by access to  LLaMA-2 <span class="footnote-reference" role="doc-noteref" id="fnrefvqihg0rt649"><sup><a href="#fnvqihg0rt649">[6]</a></sup></span> . Thus, after making that evaluation, the mitigation step is trivial: no “patches” are needed, and LLaMA-2 can be open sourced. (I think this is good: AFAICT, LLaMA-2&#39;s open sourcing has been good for the world, including for alignment research.)</li><li> Suppose camelidAI is capable of discovering this one weird trick for turning rocks and air into a black hole (astrophysicists hate it!). Assuming there is no plausible mitigation for this attack, camelidAI never gets to be open sourced. (I hope that even strong open source proponents would agree that this is the right outcome in this scenario.)</li></ul><p> I&#39;ll also note two ways that this protocol differs from from responsible disclosure in cryptography:</p><ol><li> Mesa is not allowed to set a deadline on how long society has to robustify itself to camelidAI&#39;s capabilities. If camelidAI has a capability which would be catastrophic if misused and it takes a decade of technological progress before we can come up with a &quot;patch&quot; for the problem, then Mesa doesn&#39;t get to open source the model until that happens.</li><li> In cryptography, the onus is on affected parties to patch the vulnerability, but in this case the onus is partly on the AI system&#39;s developer.</li></ol><p> These two differences mean that other parties aren&#39;t as incentivized to robustify their systems; in principle they could drag their feet forever and Mesa will never get to release camelidAI. I think something should be done to fix this, eg the government should fine companies which insufficiently prioritize implementing the necessary changes.</p><p> But overall, I think this is fair: if you are aware of a way that your system could cause massive harm and you don&#39;t have a plan for how to prevent that harm, then you don&#39;t get to open source your AI system 。</p><p> One thing that I like about this protocol is that it&#39;s hard to argue with: if camelidAI is demonstrably capable of eg autonomously engineering a novel pathogen, then Mesa can&#39;t fall back to claiming that the <a href="https://twitter.com/ID_AA_Carmack/status/1719077965055533455"><u>harms are imaginary</u></a> or <a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>overhyped</u></a> , or that <a href="https://twitter.com/ylecun/status/1719692258591506483"><u>as a general principle open source AI makes us safer</u></a> . We will have a concrete, demonstrable harm; and instead of debating whether AI harms can be mitigated by AI in the abstract, we can discuss how to mitigate this particular harm. If AI can provide a mitigation, then we&#39;ll find and implement the mitigation. And similarly, if it ends up that the harms <i>were</i> imaginary or overhyped, then Mesa will be free to open source camelidAI.</p><h2> <strong>How does this relate to the current plan?</strong></h2><p> As I understand things, the high-level idea driving many responsible scaling policy (RSP) proponents is something like:</p><blockquote><p> Before taking certain actions (eg training or deploying an AI system), AI labs need to make &quot;safety arguments,&quot; ie arguments that this action won&#39;t cause significant harm. For example, if they want to deploy a new system, they might argue:</p><ol><li> Our system won&#39;t cause harm because it&#39;s not capable enough to do significant damage. (If OpenAI had been required to make a safety argument before releasing GPT-4, this is likely the argument they would have made, and it seems true to me.)</li><li> Our system <i>could</i> cause harm if it attempted to but it won&#39;t attempt to because, eg it is only deployed through an API and we&#39;ve ensured using [measures] that no API-mediated interaction could induce it to attempt harm.</li><li> Our system <i>could</i> cause harm if it attempted to and we can&#39;t rule out that it will attempt to, but it won&#39;t succeed in causing harm because, eg it&#39;s only being used in a tightly-controlled environment where we have extremely good measures in place to stop it from successfully executing harmful actions.</li></ol><p> If no such argument exists, then you need to do something which <i>causes</i> such an argument to exist (eg doing a better job of aligning your model, so that you can make argument (2) above). Until you&#39;ve done so, you can&#39;t take whatever potentially-risky action you want to take.</p></blockquote><p> I think that if you apply this idea in the case where the action is &quot;open sourcing an AI system,&quot; you get something pretty similar to the protocol I outlined above: in order to open source an AI system, you need to make an argument that it&#39;s safe to open source that system. If there is no such argument, then you need to do stuff (eg improve email monitoring for phishing attempts) which make such an argument exist.</p><p> Right now, the safety argument for open sourcing would be the same as (1) above: current open source systems aren&#39;t capable enough to cause significant novel harm. In the future, these arguments will become trickier to make, especially for open source models which can be modified (eg finetuned or incorporated into a larger system) and whose environment is potentially &quot;the entire world.&quot; But, as the world is radically changed by advances in frontier AI systems, these arguments might continue to be possible for non-frontier systems. (And I expect open source models to continue to lag the frontier.)</p><h2> <strong>Some uncertainties</strong></h2><p> Here are some uncertainties I have:</p><ul><li> In practice, how does this play out?<ul><li> I think a reasonable guess might be: in a few years, SoTA models will be smart enough to cause major catastrophes if open-sourced, and – even with SoTA AI assistance – we won&#39;t be able to patch the relevant vulnerabilities until after the singularity (after which the ball is out of our court). If so, this protocol basically boils down to a ban on open source AI with extra steps.</li><li> I&#39;ll note, however, that open source proponents (many of whom expect slower progress towards harmful capabilities) probably disagree with this forecast. If they are right then this protocol boils down to “evaluate, then open source.” I think there are advantages to having a policy which specializes to what AI safety folks want if AI safety folks are correct about the future and specializes to what open source folks want if open source folks are correct about the future.</li></ul></li><li> Will evaluators be able to anticipate and measure all of the novel harms from open source AI systems?<ul><li> Sadly, I&#39;m not confident the answer is “yes,” and this is the main reason I only ~50% endorse this post. Two reasons I&#39;m worried evaluators might fail:<ul><li> Evaluators might not have access to significantly better tools than the users, and there are many more users. Eg even though the evaluators will be assisted by GPT-SoTA, so will the millions of users who will have access to camelidAI if it is open-sourced.</li><li> The world might change in ways that enable new threat models after camelidAI is open-sourced. For example, suppose that camelidAI + GPT-SoTA isn&#39;t dangerous, but camelidAI + GPT-(SoTA+1) (the GPT-SoTA successor system) is dangerous. If GPT-(SoTA+1) comes out a few months after camelidAI is open-sourced, this seems like bad news.</li></ul></li></ul></li><li> Maybe using subtly unaligned SoTA AI systems to evaluate and monitor other AI systems is really bad for some reason that&#39;s hard for us to anticipate?<ul><li> Eg something something the AI systems coordinate with each other.</li></ul></li></ul><h2> <strong>Some thoughts on the open source discourse</strong></h2><p> I think many AI safety-concerned folks make a mistake along the lines of: &quot;I notice that there is some capabilities threshold <i>T</i> past which everyone having access to an AI system with capabilities >; <i>T</i> would be an existential threat in today&#39;s world. On the current trajectory, someday someone will open source an AI system with capabilities >; <i>T</i> . Therefore, open sourcing is likely to lead to extinction and should be banned.&quot;</p><p> I think this reasoning ignores the fact that at the time someone first tries to open source a system of capabilities >; <i>T</i> , the world will be different in a bunch of ways. For example, there will probably exist proprietary systems of capabilities <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gg T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">≫</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。 So overall, I think folks in the AI safety community worry too much about threats from open source models.</p><p> Further, AI safety community opposition to open source AI is currently generating <i>a lot</i> of animosity from the open source community. For background, the open source ideology is deeply interwoven with the history of software development, and strong proponents of open source have a lot of representation and influence in tech. <span class="footnote-reference" role="doc-noteref" id="fnrefhqvunm4kwo5"><sup><a href="#fnhqvunm4kwo5">[7]</a></sup></span> I&#39;m somewhat worried that on the current trajectory, AI safety vs. open source will be a major battlefront making it hard to reach consensus (much worse than the IMO not-too-bad AI discrimination/ethics vs. x-risk division).</p><p> To the extent that this animosity is due to unnecessary fixation on the dangers of open source or sloppy arguments for the existence of this danger, I think this is really unfortunate. I think there are good arguments for worrying <i>in particular ways</i> about the potential dangers of open sourcing AI systems <i>at some scale</i> , and I think being much more clear on the nuances of these threat models might lead to much less animosity.</p><p> Moreover, I think there&#39;s a good chance that by the time open source models are dangerous, we will have concrete evidence that they are dangerous (eg because we&#39;ve already seen that unaligned proprietary models of the same scale are dangerous). This means that policy proposals of the shape “if [evidence of danger], then [policy]” get most of the safety benefit while also failing gracefully (ie not imposing excessive development costs) in worlds where the safety community is wrong about the pending dangers. Ideally, this means that such policies are easier to build consensus around. </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnnmr2zc5cm4r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnmr2zc5cm4r">^</a></strong></sup></span><div class="footnote-content"><p> This currently seems about right to me, ie that LLaMA-2 is a little bit worse than GPT-3.5 which came out 20 months ago.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlndvavl4r2a"> <span class="footnote-back-link"><sup><strong><a href="#fnreflndvavl4r2a">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://forum.effectivealtruism.org/posts/PTtZWBAKgrrnZj73n/biosecurity-culture-computer-security-culture"><u>Jeff Kaufman has written about</u></a> a difference in norms between the computer security and biosecurity communities. In brief, while computer security norms encourage trying to break systems and disclosing vulnerabilities, biosecurity norms discourage open discussion of possible vulnerabilities. Jeff attributes this to a number of structural factors, including how difficult it can be to patch biosecurity vulnerabilities; it&#39;s possible that threat models from open source AI have more in common with biorisk models, in which case we should instead model our defenses based on them. For more ctrl-f “cold sweat” <a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/#crispr-based-gene-drive-022318"><u>here</u></a> to read Kevin Esvelt discussing why he didn&#39;t disclose the idea of a gene drive to anyone – not even his advisor – until he was sure that it was defense-dominant. (h/t to Max Nadeau for both of these references, and to most of the references to bio-related material that I link elsewhere.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3ejitdom769"> <span class="footnote-back-link"><sup><strong><a href="#fnref3ejitdom769">^</a></strong></sup></span><div class="footnote-content"><p> I expect some folks will want to argue about whether our cryptography is actually all that good, or point out that the words “relatively” and “mostly” in that sentence are concerning if you think that “we only get one shot” with AI. So let me preemptively clarify that I don&#39;t care too much about the precise success level of this protocol; I&#39;m mostly using it as an illustrative analogy.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1art7tmmdob"> <span class="footnote-back-link"><sup><strong><a href="#fnref1art7tmmdob">^</a></strong></sup></span><div class="footnote-content"><p> We can assume this because we&#39;re dealing with the threat model of catastrophes caused by open source AI. If you think the first thing that kills us is misaligned proprietary AI systems, then you should focus on that threat model instead of open source AI.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7501lf6ibc3"> <span class="footnote-back-link"><sup><strong><a href="#fnref7501lf6ibc3">^</a></strong></sup></span><div class="footnote-content"><p> This is the part of the protocol that I feel most nervous about; see bullet point 2 in the &quot;Some uncertainties&quot; section.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvqihg0rt649"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvqihg0rt649">^</a></strong></sup></span><div class="footnote-content"><p> It&#39;s <a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.18233.pdf"><u>shown here</u></a> that a LLaMA-2 finetuned on virology data was useful for giving hackathon participants instructions for obtaining and releasing the reconstructed 1918 influenza virus. However, it&#39;s not clear that this harm was novel – we don&#39;t know how much worse the participants would have done given only access to the internet.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhqvunm4kwo5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhqvunm4kwo5">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;ve noticed that AI safety concerns have had a hard time gaining traction at MIT in particular, and one guess I have for what&#39;s going on is that the open source ideology is very influential at MIT, and all the open source people currently hate the AI safety people.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai<guid ispermalink="false"> WLYBy5Cus4oRFY3mu</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Fri, 03 Nov 2023 15:35:42 GMT</pubDate> </item><item><title><![CDATA[Shouldn't we 'Just' Superimitate Low-Res Uploads?]]></title><description><![CDATA[Published on November 3, 2023 7:42 AM GMT<br/><br/><p> If you have three optimizers which are all very different (one is a neural network, the other was built by hand, and the final one materialized through a Boltzmann Brain-like process) but have identical preferences and an equivalent capacity for optimization, they probably end up doing similar things over a long enough time-scale.</p><p> I bring this up, because in discussions of uploading people seem to gravitate toward obtaining a digital encoding of the entire mind, as though the useful part of the upload contains the fact that it optimizes in a human-like way, when it seems like what we actually want is something that optimizes in any way so long as it is optimizing for the things we want, in all their complexity.</p><p> In <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#Training_frontier_models_to_predict_neural_activity_instead_of_next_token">Does Davidad&#39;s Uploading Moonshot Work?</a> , <a href="https://www.lesswrong.com/users/jacobjacob?mention=user">@jacobjacob</a> brings up the following:</p><blockquote><p> What about the idea of &quot;just&quot; training a giant transformer to, instead of predicting next tokens in natural language, predicting neural activity?</p></blockquote><p> <a href="https://www.lesswrong.com/users/lisathiergart?mention=user">@lisathiergart</a> replies by saying that this probably violates the assumption that the product would be more aligned than the status quo. I raise the following objections:</p><ul><li> The product you need from this training is not so complex as an approximation of the entire brain, &#39;only&#39; its preferences. If you can extract a reward model from the transformer (which you might even be able to engineer to be superior using SOTA AI at the time of &#39;upload&#39;) and point an optimizer in that direction you can achieve a similar endpoint.</li><li> Training large neural networks on MEG data seems pretty good. Meta had lots of success with their work on <a href="https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/">decoding images from brain activity</a> . <a href="https://arxiv.org/abs/2208.12266">The same goes for speech</a> . This indicates to me that current neural networks are powerful enough to extract meaningful patterns from MEG data (the speech paper even had some success with EEG).</li><li> Deep learning does lots of surprising things when scaled sufficiently. If you asked most people what they thought about training a giant transformer on predicting the next token they probably wouldn&#39;t have forecasted GPT-4 by 2023. Maybe the same applies to predicting the next MEG reading?<ul><li> This seems really easy to test. I thought about it for 30 minutes and came up with a few architectures based on Mistral-7B that I feel I could plausibly implement on short notice. I don&#39;t have any MEG hardware however, and it&#39;s expensive to obtain. From my research it doesn&#39;t look like there is sufficient publicly available data to train something of Mistral-7B-size (or even significantly smaller) assuming Chinchilla scaling laws roughly hold in this case.</li></ul></li></ul><hr><p> <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation">Vanessa Kosoy refers to physicalist superimitation as one potential endpoint of the Learning Theoretic Agenda</a> . She describes superimitation as follows:</p><blockquote><p> An agent (henceforth: the &quot;imitator&quot;) that receives the policy of another agent (henceforth: the &quot;original&quot;), and produces behavior which pursues the same goals but <i>significantly</i> <i>better</i> .</p></blockquote><p> If you can superimitate, or otherwise optimize for the preferences of an upload, maybe simple approaches (like predicting the next MEG reading) are sufficient, or at least comparable to training a full upload and leveraging that in spite of being significantly easier?</p><br/><br/> <a href="https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads<guid ispermalink="false"> KGTGgnGppf9wzwmFM</guid><dc:creator><![CDATA[marc/er]]></dc:creator><pubDate> Fri, 03 Nov 2023 07:42:07 GMT</pubDate> </item><item><title><![CDATA[The other side of the tidal wave]]></title><description><![CDATA[Published on November 3, 2023 5:40 AM GMT<br/><br/><p> I guess there&#39;s maybe a 10-20% chance of AI causing human extinction in the coming decades, but I feel more distressed about it than even that suggests—I think because in the case where it doesn&#39;t cause human extinction, I find it hard to imagine life not going kind of off the rails. So many things I like about the world seem likely to be over or badly disrupted with superhuman AI (writing, explaining things to people, friendships where you can be of any use to one another, taking pride in skills, thinking, learning, figuring out how to achieve things, making things, easy tracking of what is and isn&#39;t conscious), and I don&#39;t trust that the replacements will be actually good, or good for us, or that anything will be reversible.</p><p> Even if we don&#39;t die, it still feels like everything is coming to an end.</p><br/><br/> <a href="https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave<guid ispermalink="false"> uyPo8pfEtBffyPdxf</guid><dc:creator><![CDATA[KatjaGrace]]></dc:creator><pubDate> Fri, 03 Nov 2023 05:40:06 GMT</pubDate> </item><item><title><![CDATA[Does davidad's uploading moonshot work?]]></title><description><![CDATA[Published on November 3, 2023 2:21 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/users/davidad">davidad</a> has a 10-min talk out on a proposal about which he says: “the first time I&#39;ve seen a concrete plan that might work to get human uploads before 2040, maybe even faster, given unlimited funding”.</p><p> I think the talk is a good watch, but the dialogue below is pretty readable even if you haven&#39;t seen it. I&#39;m also putting some summary notes from the talk in the Appendix of this dialoge. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=jZqynCV0AGc&amp;list=PLH78wfbGI1x2CI6aV_hiOE1_GOFkZAFph&amp;index=7"><div><iframe src="https://www.youtube.com/embed/jZqynCV0AGc" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> I think of the promise of the talk as follows. It might seem that to make the future go well, we have to either make general AI progress slower, or make alignment progress differentially faster. However, uploading seems to offer a third way: instead of making alignment researchers more productive, we &quot;simply&quot; <i>run them faster</i> . This seems similar to <a href="https://openai.com/blog/introducing-superalignment"><u>OpenAI&#39;s Superalignment</u></a> proposal of building an automated alignment scientist -- with the key exception that we might plausibly think a human-originated upload would have a better alignment guarantee than, say, an LLM.</p><p> I decided to organise a dialogue on this proposal because it strikes me as “huge if true”, yet when I&#39;ve discussed this with some folks I&#39;ve found that people generally don&#39;t seem that aware of it, and sometimes raise questions/confusions/objections that neither me nor the talk could answer.</p><p> I also invited Anders Sandberg, who co-authored the <a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf">2008 Whole Brain Emulation roadmap with Nick Bostrom</a> , and co-organised <a href="https://foresight.org/whole-brain-emulation-workshop-2023/">the Foresight workshop on the topic where Davidad presented his plan</a> , and Lisa Thiergart from MIRI, who also gave <a href="https://www.youtube.com/watch?v=gf7W82mrRfI">a talk</a> at the workshop and <a href="https://www.lesswrong.com/posts/KQSpRoQBz7f6FcXt3/distillation-of-neurotech-and-alignment-workshop-january-1">has previously written about whole brain emulation here on LessWrong</a> .</p><p> The dialogue ended up covering a lot of threads at the same time. I split them into six separate sections, that are fairly readable independently.</p><h2> Barcoding as blocker (and synchrotrons) </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:31:34 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:31:34 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> One important thing is &quot;what are the reasons that this uploading plan might fail, if people actually tried it.&quot; I think the main blocker is <i>barcoding the transmembrane proteins</i> . Let me first lay out the standard argument for why we &quot;don&#39;t need&quot; most of the material in the brain, such as DNA methylation, genetic regulatory networks, microtubules, etc.</p><ol><li> Cognitive reaction times are fast.</li><li> That means individual neuron response times need to be even faster.</li><li> Too fast for chemical diffusion to go very far—only crossing the tiny synaptic gap distance.</li><li> So everything that isn&#39;t synaptic must be electrical.</li><li> Electricity works in terms of potential differences (voltages), and the only parts of tissue where you can have a voltage difference is across a membrane.</li><li> Therefore, all the cognitive information processing supervenes on the processes in the membranes and synapses.</li></ol><p> Now, people often make a highly unwarranted leap here, which is to say that all we need to know is the <i>geometric shapes and connection graph</i> of the membranes and synapses. But membranes and synapses are not made of homogenous membraneium and synapseium. Membranes are mostly made of phospholipid bilayers, but a lot of the important work is done by trans-membrane proteins: it is these molecules that translate incoming neurotransmitters into electrical signals, modulate the electrical signals into action potentials, cause action potentials to decay, and release the neurotransmitters at the axon terminals. And there are many, many different trans-membrane proteins in the human nervous system. Most of them probably don&#39;t matter or are similar enough that they can be conflated, but I bet there are at least dozens of very different kinds of trans-membrane proteins (inhibitory vs excitatory, different time constants, different sensitivity to different ions) that result in different information-processing behaviours. So we need to not just see the neurons and synapses, but the quantitative densities of each of many different kinds of transmembrane proteins throughout the neuron cell membranes (including but not limited to at the synapse).</p><p> Sebastian Seung famously has a hypothesis in his book about connectomics that there are only a few hundred distinct cell types in the human brain that have roughly equivalent behaviours at every synapse, and that we can figure out which type each cell is just from the geometry. But I think this is unlikely given the way that learning works via synaptic plasticity: ie a lot of what is learned, at least in memory (as opposed to skills) is represented by the relative receptor densities at synapses. However, it <i>may</i> be that we are lucky and purely the <i>size</i> of the synapse is enough. In this case actually we are extremely lucky because the synchrotron solution is far more viable, and that&#39;s much faster than expansion microscopy. With expansion microscopy, it <i>seems</i> plausible to tag a large number of receptors by barcoding fluorophores in a similar way to how folks are now starting to barcode <i>cells</i> with different combinations of fluorophores (which solves an entirely different problem, namely redundancy for axon tracing). However, <strong>the most likely way for the expansion-microscopy-based plan to fail is that we cannot use fluorescence to barcode enough transmembrane proteins</strong> . </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> What options are there for overcoming this, and how tractable are they? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:19:32 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:19:32 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> For barcoding, there are only a couple different technical avenues I&#39;ve thought of so far. To some extent they can be combined.</p><ol><li> &quot;serial&quot; barcoding, in which we find a protocol for washing out a set of tags in an already-expanded sample, and then diffusing a new set of tags, and being able to repeat this over and over without the sample degrading too much from all the washing.</li><li> &quot;parallel&quot; barcoding, in which we conjugate several fluorophores together to make a &quot;multicolored&quot; fluorophore (literally like a barcode in the visible-light spectrum). This is the basic idea that is used for barcoding cells, but the chemistry is very different because in cells you can just have different concentrations of separate fluorophore molecules floating around, whereas receptors are way too small for that and you need to have one of each type of fluorophore all kind of stuck together as one molecule. Chemistry is my weakness, so I&#39;m not very sure how plausible this is, but from a first-principles perspective it seems like it might work. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:35:19 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:35:19 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> the synchrotron solution is far more viable</p></blockquote><p> On synchotrons / particle accelerator x-rays: <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan Collins estimates</a> they would take ~1 yr of sequential effort for a whole human brain (which thus also means you could do something like a mouse brain in a few hours, or an organoid in minutes, for prototyping purposes). But I&#39;m confused why you suggest that as an option that&#39;s <i>differentially</i> compatible with only needing lower resolution synaptic info like size.</p><p> Could you not do expansion microscopy + synchrotron? And if you need the barcoding to get what you want, wouldn&#39;t you need it with or without synchrotron? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:37:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:37:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> So, synchrotron imaging uses X-rays, whereas expansion microscopy typically uses ~visible light fluorescence (or a bit into IR or UV). (It is possible to do expansion and then use a synchrotron, and the best current synchrotron pathway does do that, but the speed advantages are due to X-rays being able to penetrate deeply and facilitate tomography.) There are a lot of different indicator molecules that resonate at different wavelengths of ~visible light, but not a lot of different indicator atoms that resonate at different wavelengths of X-rays. And the synchrotron is monochromatic anyway, and its contrast is by transmission rather than stimulated emission. So for all those reasons, with a synchrotron, it&#39;s really pushing the limits to get even a small number of distinct tags for different targets, let alone spectral barcoding. That&#39;s the main tradeoff with synchrotron&#39;s incredible potential speed. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> One nice thing with working in visible light rather than synchrotron radiation is that the energies are lower, and hence there is less disruption of the molecules and structure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Andreas Schaefer seems to be making great progress with this, see eg <a href="https://www.nature.com/articles/s41467-022-30199-6#Sec7">here</a> . I have updated downward in conversations with him that sample destruction will be a blocker. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also, there are many modalities that have been developed in visible light wavelengths that are well understood. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> This is a bigger concern for me, especially in regards to spectral barcodes.</p></div></section><h2> End-to-end iteration as blocker (and organoids, holistic processes, and exotica) </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:36:52 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:36:52 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> I think the barcoding is a likely blocker. But I think the most plausible blocker is a more diffuse problem: we do not manage to close the loop between actual, running biology, and the scanning/simulation modalities so that we can do experiments, adjust simulations to fit data, and then go back and do further experiments - including developing new scanning modalities. My worry here is that while we have smart people who are great at solving well-defined problems, the problem of setting up a research pipeline that is good at iterating at generating well-defined problems might be less well-defined... and we do not have a brilliant track record of solving such vague problems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:45:33 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:45:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yeah, this is also something that I&#39;m concerned that people who take on this project might fail to do by default, but I think we do have the tech to do it now: human brain organoids. We can manufacture organoids that have genetically-human neurons, at a small enough size where the entire thing can be imaged <i>dynamically</i> (ie non-dead, while the neurons are firing, actually collecting the traces of voltage and/or calcium), and then we can slice and dice the organoid and image it with whatever static scanning modality, and see if we can reproduce the actual activity patterns based on data from the static scan. This could become a quite high-throughput pipeline for testing many aspects of the plan (system identification, computer vision, scanning microscopes, sample prep/barcoding, etc.). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:46:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:46:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> What is the state of the art of &quot;uploading an organoid&quot;?<br><br> (I guess one issue is that we don&#39;t have any &quot;validation test&quot;: we just have neural patterns, but the organoid as a whole isn&#39;t really doing any function. Whereas, for example, if we tried uploading a worm we could test whether it remembers foraging patterns learnt pre-upload) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:51:31 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:51:31 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> I&#39;m not sure if I would know about it, but I just did a quick search and found things like &quot; <a href="https://www.nature.com/articles/s41467-022-32115-4">Functional neuronal circuitry and oscillatory dynamics in human brain organoids</a> &quot; and &quot; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0142961222004653">Stretchable mesh microelectronics for biointegration and stimulation of human neural organoids</a> &quot;, but nobody is really trying to &quot;upload an organoid&quot;. They are already viewing the organoids as more like &quot;emulations&quot; on which one can do experiments in place of human brains; it is an unusual perspective to treat an organoid as more like an organism which one might try to emulate. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:58:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:58:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> On the general theme of organoids &amp; shortcuts:<br><br> I&#39;m thinking about what are key blockers in dynamic scanning as well later static slicing &amp; imaging. I&#39;m probably not fully up to date on the state of the art of organoids &amp; genetic engineering on them. However, if we are working on the level of organoids, couldn&#39;t we plausibly directly genetically engineer (or fabrication engineer) them to make our life easier?</p><ul><li>前任。 making the organoid skin transparent (visually or electrically)</li><li>前任。 directly have the immunohistochemistry applied as the organoid grows / is produced</li><li>前任。 genetically engineer them such that the synaptic components we are most interested in are fluorescent</li></ul><p> ...probably there are further such hacks that might speed up the &quot;Physically building stuff and running experiments&quot; rate-limit on progress</p><p> (ofc getting these techniques to work will also take time, but at appropriate scales it might be worth it) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:05:27 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:05:27 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> We definitely can engineer organoids to make experiments easier. They can simply be thin enough to be optically transparent, and of course it&#39;s easier to grow them <i>without</i> a skull, which is a plus.</p><p> In principle, we could also make the static scanning easier, but I&#39;m a little suspicious of that, because in some sense the purpose of organoids in this project would be to serve as &quot;test vectors&quot; for the static scanning procedures that we would want to apply to non-genetically-modified human brains. Maybe it would help to get things off the ground to help along the static scanning with some genetic modification, but it might be more trouble than it&#39;s worth. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:05:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:05:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Lisa, here&#39;s a handwavy question. Do you have an estimate, or the components of an estimate, for &quot;how quickly could we get a &#39;Chris Olah-style organoid&#39; -- that is, the <i>smallest</i> organoid that 1) we fully understood, in the relevant sense and also 2) told us at least something interesting on the way to whole brain emulation?&quot;</p><p> (Also, I don&#39;t mean that it would be an organoid <i>of</i> poor Chris! Rather, my impression is that his approach to interpretability, is &quot;start with the smallest and simplest possible toy system that you do <i>not</i> understand, then understand that really well, and then increase complexity&quot;. This would be adapting that same methodology) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:36:21 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:36:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Wow that&#39;s a fun one, and I don&#39;t quite know where to start. I&#39;ll briefly read up on organoids a bit. [...reading time...] Okay so after a bit of reading, I don&#39;t think I can give a great estimate but here are some thoughts:<br><br> First off, I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole&#39; works well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot). Similarly, I&#39;m not sure how much starting with a simpler example (say a different animal with a smaller brain) will reliably transfer, but at least some of it might. For developing the needed technologies it definitely seems helpful.<br><br> That said, starting small is always easier. If I were trying to come up with components for an estimate I&#39;d consider:</p><ul><li> looking at ​​non-human organoids</li><li> comparisons to how long similar efforts have been taking: C. elegans is a simpler worm organism some groups have worked on trying to whole brain upload, which seems to be taking longer than experts predicted (as far as I know the efforts have been ongoing for >;10 years and not yet concluded, but there has been some progress). Though, I think this is affected for sure by there not being very high investment in these projects and few people working on it.</li><li> trying to define what &#39;fully understood&#39; means: perhaps being able to get within x% error on electrical activity prediction and/or behavioural predictions (if the organism exhibits behaviors)</li></ul><p> There&#39;s definitely tons more to consider here, but I don&#39;t think it makes sense for me to try to generate it.​ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:42:31 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:42:31 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><blockquote><p> I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole works&#39; well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot).</p></blockquote><p> One simple example of a &quot;holistic&quot; property that might prove troublesome is oscillatory behavior, where you need a sufficient number of units linked in the right way to get the right kind of oscillation. The fun part is that you get oscillations almost automatically from any neural system with feedback, so distinguishing merely natural frequency oscillations (eg the gamma rhythm seems to be due to fast inhibitory interneurons if I remember right) and the functionally important oscillations (if any!) will be tricky. Merely seeing oscillations is not enough, we need some behavioral measures.</p><p> There is likely a dynamical balance between microstructure-understanding based &quot;IKEA manual building&quot; of the system and the macrostructure-understanding &quot;carpentry&quot; approach. Setting the overall project pipeline in motion requires having a good adaptivity on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:39:25 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:39:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Wouldn&#39;t it be possible to bound the potentially relevant &quot;whole brain processes&quot;, by appealing to a version of Davidad&#39;s argument above that we can neglect a lot of modalities and stuff, because they operate at a slower timescale than human cognition (as verified for example by simple introspection)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:47:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:47:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> I think glia are also involved in <a href="https://www.sciencedirect.com/science/article/pii/S0165017309001076?casa_token=V_DcC59YI7MAAAAA:XzfbMUr_R56pDQNA57HLWI6NFMxBzeN2ATF89tw2yGe7uchy5dZFLpHJgBurGAR9Drc0iYzVvQ">modulating synaptic transmissions</a> as well as the electric conductivity of a neuron (definitely the speed of conductance), so I&#39;m not sure the speed of cognition argument necessarily disqualifies them and other non-synaptic components as relevant to an accurate model.  This <a href="https://www.frontiersin.org/articles/10.3389/fncel.2016.00188/full">paper</a> presents the case of glia affecting synaptic plasticity and being electrically active.  Though I do think that argument seems to be valid for many components, which clearly cannot effect the electrical processes at the needed time scales.<br><br> With regards to &quot;whole brain processes&quot; what I&#39;m gesturing at is there might be top level control or other processes running without whose inputs the subareas&#39; function cannot be accurately observed. We&#39;d need to have an alternative way of inputting the right things into the slice or subarea sample to generate the type of activity that actually occurs in the brain. Though, it seems we can focus on the electrical components of such a top level process in which case I wouldn&#39;t see an obvious conflict between the two arguments. I might even expect the electrical argument to hold more, because of the need to travel quickly between different (far apart) brain areas. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:53:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:53:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yeah, sorry, I came across as endorsing a modification of the standard argument that rules out too many aspects of brain processes as cognition-relevant, where I only rule back in <i>neural</i> membrane proteins. I&#39;m quite <i>confident</i> that neural membrane proteins will be a blocker (~70%) whereas I am less confident about glia (~30%) but it&#39;s still very plausible that we need to learn something about glial dynamics in order to get a functioning人类思维。 However, whatever computation the glia are doing is also probably based on their own membrane proteins! And the glia are included in the volume that we have to scan anyway. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:54:42 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:54:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p>是的，有道理。 I also am thinking that maybe there&#39;s a way to abstract out the glia by considering their inputs as represented in the membrane proteins. However, I&#39;m not sure whether it&#39;s cheaper/faster to represent the glia vs. needing to be very accurate with the membrane proteins (as well as how much of a stretch it is to assume they can be fully represented there). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:58:30 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:58:30 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> My take on glia is that they may represent a sizeable pool of nonlinear computation, but it seems to run slowly compared to the active spikes of neurons. That may require lots of memory, but less compute. But real headache is that there is relatively little research on glia (despite their very loyal loyalists!) compared to those glamorous neurons. Maybe they do represent a good early target for trying to define a research subgoal of characterizing them as completely as possible. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:27:37 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:27:37 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Overall, the &quot;exotica&quot; issue is always annoying - there could be an infinite number of weird modalities we are missing, or strange interactions requiring a clever insight. However, I think there is a pincer maneouver here where we try to constrain it by generating simulations from known neurophysiology (and nearby variations), and perhaps by designing experiments to estimate degrees of unknown degrees of freedom (much harder, but valuable). As far as I know the latter approach is still not very common, my instant association is to how statistical mechanics can estimate degrees of freedom sensitively from macroscopic properties (leading, for example, to primordial nucleogenesis to constrain elementary particle physics to a surprising degree) 。 This is where I think a separate workshop/brainstorm/research minipipeline may be valuable for strategizing.</p></div></section><h2> Using AI to speed up uploading research </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> As AI develops over the coming years, it will speed up some kinds of research and engineering. I&#39;m wondering: for this proposal, which parts could future AI accelerate? And perhaps more interestingly: which parts would <i>not</i> be easily accelerable (implying that work on those parts sooner is more important)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:11:55 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:11:55 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> The frame I would use is to sort things more like along a timeline of when AI might be able to accelerate them, rather than a binary easy/hard distinction. (Ultimately, a friendly superintelligence would accelerate the entire thing—which is what many folks believe friendly superintelligence ought to be used for in the first place!)</p><ol><li> The easiest parts to accelerate are the computer vision. In terms of timelines, this is in the rear-view mirror, with deep learning starting to substantially accelerate the processing of raw images into connectomic data <a href="https://arxiv.org/pdf/1706.00120.pdf">in 2017</a> .</li><li> The next easiest is the modelling of the dynamics of a nervous system based on connectomic data. This is a mathematical modelling challenge, which needs a bit more structure than deep learning traditionally offers, but it&#39;s not that far off (eg with multimodal hypergraph transformers).</li><li> The next easiest is probably more ambitious microscopy along the lines of &quot;computational photography&quot; to extract more data with fewer electrons or photons by directing the beams and lenses according to some approximation of <a href="https://arxiv.org/pdf/1103.5708.pdf">optimal Bayesian experimental design</a> . This has the effect of accelerating things by making the imaging go faster or with less hardware.</li><li> The next easiest is the engineering of the microscopes and related systems (like automated sample preparation and slicing). These are electro-optical-mechanical engineering problems, so will be harder to automate than the more domain-restricted problems above.</li><li> The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i> of the microscopes, and the management of failures and repairs and replacements of parts, etc. Of course this is still possible to automate, but it requires capabilities that are quite close to the kind of superintelligence that can maintain a robot fleet without human intervention. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:13:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:13:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i></p></blockquote><p> An interesting related question is &quot;if you had an artificial superintelligence suddenly appear today, what would be its &#39;manufacturing overhang&#39;? How long it would it take it to build the prerequisite capacity, starting with current tools?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:55:36 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:55:36 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Could AI assistants help us build a research pipeline?我想是这样。 Here is a sketch: I run a bio experiment, I scan the bio system, I simulate it, I get data that disagrees.哪里有问题？ Currently I would spend a lot of effort trying to check my simulator for mistakes or errors, then move on to check whether the scan data was bad, then if the bio data was bad, and then if nothing else works, positing that maybe I am missing a modality. Now, with good AI assistants I might be able to (1) speed up each of these steps, including using multiple assistants running elaborate test suites. (2) automate and parallelize the checking. But the final issue remains tricky: what am I missing, if I am missing something? This is where we have human level (or beyond) intelligence questions, requiring rather deep understanding of the field and what is plausible in the world, as well as high level decisions on how to pursue research to test what new modalities are needed and how to scan for them. Again, good agents will make this easier, but it is still tricky work.</p><p> What I suspect could happen for this to fail is that we run into endless parameter-fiddling, optimization that might hide bad models by really good fits of floppy biological systems, and no clear direction for what to research to resolve the question. I worry that it is a fairly natural failure mode, especially if the basic project produces enormous amount of data that can be interpreted very differently. Statistically speaking, we want identifiability. But not just of the fit to our models, but to our explanations. And this is where non-AGI agents have not yet demonstrated utility. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:48:00 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:48:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Yeah, so, overall, it seems &quot;physically building stuff&quot;, &quot;running experiments&quot; and &quot;figure out what you&#39;re missing&quot; are some of where the main rate-limiters lie. These are the hardest-to-automate steps of the iteration loop, that prevent an end-to-end AI assistant helping us through the pipeline.</p><p> Firstly though, it seems important to me how frequently you run into the rate limiter. If &quot;figuring out what you&#39;re missing&quot; is something you do a few times a week, you could still be sped up a lot by automating the other parts of the pipeline until you can run this problem a few times per day.</p><p> But secondly I&#39;m interested in answers digging one layer down of concreteness -- which part of the building stuff is hard, and which part of the experiment running is hard? For example: Anders had the idea of &quot; <a href="https://www.youtube.com/watch?v=ItEnEN58bJw">the neural pretty printer</a> &quot;: create ground truth artificial neural networks performing a known computation >;>; convert them into a Hodgkin-Huxley model (or similar) >;>; fold that up into a 3D connectome model >;>; simulate the process of scanning data from that model -- and then attempt to reverse engineer the whole thing. This would basically be a simulation pipeline for validating scanning set-ups.</p><p> To the extent that such simulation is possible, <i>those</i> particular experiments would probably not be the rate-limiting ones. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:21:22 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:21:22 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The neural pretty printer is an example of building a test pipeline: we take a known simulatable neural system, convert it into a plausible biological garb and make fake scans of it according to the modalities we have, and then try to get our interpretation methods reconstruct它。 This is great, but eventually limited. The real research pipeline will have to contain (and generate) such mini-pipelines to ensure testability. There is likely an organoid counterpart. Both have a problem of making systems to test the scanning based on somewhat incomplete (or really incomplete!) data.</p></div></section><h2> Training frontier models to predict neural activity instead of next token </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:20:36 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:20:36 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Lisa, you said in our opening question brainstorm:</p><blockquote><p> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.<br><br> 1a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from the dynamic data collected [(with the aim of figuring out the statics-to-dynamics map)] then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.<br><br> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)</p></blockquote><p> Somewhat tangentially, this makes me wonder about taking this idea to its limit: what about the idea of &quot;just&quot; training a giant transformer to, instead of predicting next tokens in natural language, predicting neural activity? (at whatever level of abstractions is most suitable.) &quot;Imitative neural learning&quot;. I wonder if that would be within reach of the size of models people are gearing up to train, and whether it would better preserve alignment guarantees. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:38:20 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:38:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Hmm, intuitively this seems not good. On the surface level, two reasons come to mind:<br><br> 1. I suspect the element of &quot;more reliably human-aligned&quot; breaks or at least we have less strong reasons to believe this would be the case than if the total system also operates on the same hardware structure (so to say, it&#39;s not actually going to be run on something carbon-based). Though I&#39;m also seeing the remaining issue of: &quot;if it looks like a duck (structure) and quacks like a duck (behavior), does that mean it&#39;s a duck?&quot;.  At least we have the benefit of deep structural insight as well as many living humans as a prediction on how aligned-ness of these systems turns out in practice. (That argument holds in proportion to how faithful of an emulation we achieve.)<br><br> 2. I would be really deeply interested in whether this would work. It is a bit reminiscent of <a href="https://manifund.org/projects/activation-vector-steering-with-bci">the Manifund proposal</a> Davidad and I have open currently, where the idea is to see if human brain data can improve performance on next token prediction and make it more human preference aligned.  At the same time, for the &#39;imitative neural learning&#39; you suggest (btw I suspect it would be feasible with GPT4/5 levels, but I see the bottleneck/blocker in being able to get enough high quality dynamic brain data) I think I&#39;d be pretty worried that this would turn into some dangerous system (which is powerful, but not necessarily aligned).</p><p> 2/a Side thought: I wonder how much such a system would in fact be constrained to human thought processes (or whether it would gradient descend into something that looks input and output similar, but in fact is something different, and behaves unpredictably in unseen situations). Classic Deception argument I guess (though in this case without an implication of some kind of intentionality on the system&#39;s side, just that it so happens bc of the training process and data it had) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:40:50 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:40:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> I basically agree with Lisa&#39;s previous points. Training a transformer to imitate neural activity is a little bit better than training it to imitate words, because one gets more signal about the &quot;generators&quot; of the underlying information-processing, but misgeneralizing out-of-distribution is still a big possibility. There&#39;s something qualitatively different that happens if you can collect data that is logically upstream of the entire physical information processing conducted by the nervous system — you can then make predictions about that information-processing <strong>deductively</strong><i><strong> </strong></i>rather than <strong>inductively</strong> (in the sense of the problem of induction), so that whatever inductive biases (aka priors) are present in the learning-enabled components end up having no impact. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> &quot;Human alignment&quot;: one of the nice things with human minds is that we understand roughly how they work (or at least the signs that something is seriously wrong). Even when they are not doing what they are supposed to do, the failure modes are usually human, all too human. The crux here is whether we should expect to get human-like systems, or &quot;humanish&quot; systems that look and behave similar but actually work differently. The structural constraints from Whole Brain Emulation are a good reason to think more of the former, but I suspect many still worry about the latter because maybe there are fundamental differences in simulation from reality. I think this will resolve itself fairly straightforwardly since - by assumption if this project gets anywhere close to succeeding - we can do a fair bit of experimentation on whether the causal reasons for various responses look like normal causal reasons in the bio system. My guess is that here Dennett&#39;s principle that the simplest way of faking many X is to be X. But I also suspect a few more years of practice with telling when LLMs are faking rather than grokking knowledge and cognitive steps will be very useful and perhaps essential for developing the right kind of test suite.</p></div></section><h2> How to avoid having to spend $100B and and build 100,000 light-sheet microscopes </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> “15 years and ~$500B” ain&#39;t an easy sell. If we wanted this to be doable faster (say, &lt;5 years), or cheaper (say, &lt;$10B): what would have to be true? What problems would need solving? Before we finish, I am quite keen to poke around the solution landscape here, and making associated fermis and tweaks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:16:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:16:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> So, definitely the most likely way that things could go faster is that it turns out all the receptor densities are predictable from morphology (eg synaptic size and &quot;cell type&quot; as determined by cell shape and location within a brain atlas). Then we can go ahead with synchrotron (starting with organoids!) and try to develop a pipeline that infers the dynamical system from that structural data. And synchrotron is much faster than expansion. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:17:34 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:17:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> it turns out all the receptor densities are predictable from morphology</p></blockquote><p> What&#39;s the fastest way you can see to validating or falsifying this? Do you have any concrete experiments in mind that you&#39;d wish to see run if you had a magic wand? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:22:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:22:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Unfortunately, I think we need to actually see the receptor densities in order to test this proposition. So transmembrane protein barcoding still seems to me to be on the critical path in terms of the tech tree. <strong>But</strong> if this proposition turns out to be true, then you won&#39;t need to use slow expansion microscopy when you&#39;re actually ready to scan an entire human brain—you only need to use expansion microscopy on some samples from every brain area, in order to learn a kind of &quot;Rosetta stone&quot; from morphology to transmembrane protein(/receptor) densities for each cell type. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:23:58 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:23:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> So, for the benefit of future readers (and myself!) I kind of would like to see you multiply 5 numbers together to get the output $100B (or whatever your best cost-esimate is), and then do the same thing in the fortunate synchrotron world, to get a fermi of how much things would cost in that<i> </i>世界。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:30:24 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:30:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p>好的。 I actually haven&#39;t done this myself before.开始。</p><ol><li> The human central nervous system has a volume of about 1400 cm^3.</li><li> When we expand it for expansion microscopy, it expands by a factor of 11 in each dimension, so that&#39;s about 1.9 m^3. (Of course, we would slice and dice before expanding...)</li><li> Light-sheet microscopes can cover a volume of about 10^4 micron^3 per second, which is about 10^-14 m^3 per second.</li><li> That means we need about 1.9e14 microscope-seconds of imaging.</li><li> If the deadline is 10 years, that&#39;s about 3e8 seconds, so we need to build about 6e5 microscopes.</li><li> Each microscope costs about $200k, so 6e5 * $200k is $120B. (That&#39;s not counting all the R&amp;D and operations surrounding the project, but building the vast array of microscopes is the expensive part.)</li></ol><p> Pleased by how close that came out to the number I&#39;d apparently been citing before. (Credit is due to Rob McIntyre and Michael Andregg for that number, I think.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:34:35 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:34:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Similarly, looking at <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan&#39;s slide</a> : <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rukiwrds8jln8vohsdvq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/fkcy0prpnp9w6qjpiiqn 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/aviijtkzbyulztz2wqis 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pibu1fgqe56lwn0gvxep 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/cs3agbeodgrdlyqhljcz 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/nqfxbmm14nc5lp1x8jfd 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/iupqwcowydhnwayhyipg 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/ypjrtn88oiiudmvfemqq 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/xgbk2coub2wfku0nbzuu 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jbk9mmkpafzjwu2zpkfr 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/hjmtwofbbnqvesrbeuhh 2610w"></p><p> $1M for instruments, and needing 600k years. So, make 100k microscopes and run them in parallel for 6 years and then you get $100B... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:35:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:35:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> That system is a bit faster than ExM, but it&#39;s transmission electron microscopy, so you get roughly the same kind of data as synchrotron anyway (higher resolution, but probably no barcoded receptors) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:41:54 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:41:54 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Now for the synchrotron cost estimate.</p><ol><li> Synchrotron imaging has about 300nm voxel size, so to get accurate synapse sizes we would still need to do expansion to 1.9 m^3 of volume.</li><li> Synchrotron imaging has a speed of about 600 s/mm^3, but it seems uncontroversial that this may be improved by an order of magnitude with further R&amp;D investment.</li><li> That multiplies to about 3000 synchrotron-years.</li><li> To finish in 10 years, we would need to build 300 synchrotron beamlines.</li><li> Each synchrotron beamline costs about $10M.</li><li> That&#39;s $3B in imaging infrastructure. A bargain! </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:43:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:43:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> That&#39;s very different from this estimate.想法？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jsl0moz7gppg42my41gd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/tqubpcpofsbslowecsbz 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/j9y0zplo8tcfomk2eqcz 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/e3xhqypr9xocrzwo1i8n 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/wpiyujikqiscbidicvfy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/gpfs5v4ohmrz5xjsxhaw 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/l7peqlu22d7otroqf9d9 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rftaksliw9honhyj9cvw 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/x4ybkurznlhqfavkvkx5 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/vvb2t0pjlezivooycucb 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/je0nhzwyykyyi3f2wchd 1660w"></figure></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:46:16 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:46:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> One difference off the bat - 75nm voxel size is maybe enough to get a rough connectome, but not enough to get precise estimates of synapse size. I think we&#39;d need to go for 11x expansion. So that&#39;s about 1 order of magnitude, but there&#39;s still 2.5 more to account for. My guess is that this estimate is optimistic about combining multiple potential avenues to improve synchrotron performance. I do see some claims in the literature that more like 100x improvement over the current state of the art seems feasible. </p></div></section><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:33:34 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:33:34 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> What truly costs money in projects? Generally, it is salaries and running costs times time, plus the instrument/facilities costs as a one-time cost. One key assumption in this moonshot is that there is a lot of scalability so that once the basic setup has been made it can be replicated ever cheaper (Wrightean learning, or just plain economies of scale). The faster the project runs, the lower the first factor, but the uncertainty about whether all relevant modalities have been covered will be greater. There might be a rational balance point between rushing in and likely having to redo a lot, and being slow and careful but hence getting a lot of running costs. However, from the start the difficulty is very uncertain, making the actual strategy (and hence cost) plausibly a mixture model. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:35:57 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:35:57 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The path to the 600,000 microscopes is of course to start with 6, doing the testing and system integration while generating the first data for the initial mini-pipelines for small test systems. As that proves itself one can scale up to 60 microscopes for bigger test systems and smaller brains. And then 600, 6,000 and so on. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:44:40 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:44:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Hundreds of thousands of microscopes seem to me like an... issue. I&#39;m curious if you have something like a &quot;shopping list&quot; of advances or speculative technologies that could bring that number down a lot. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:55:55 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:55:55 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Yes, that is quite the lab floor. Even one per square meter makes a ~780x780 meter space. Very Manhattan Project vibe. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:47:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:47:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Google tells me Tesla Gigafactory Nevada is about 500k m^2, so about the same :P </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Note though that economies of scale and learning curves can make this more economical than it seems. If we assume an experience curve with price per unit going down to 80% each doubling, 600k is 19 doublings, making the units in the last doubling cost just 1.4% of the initial unit cost. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:48:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:48:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> And you don&#39;t need to put all the microscopes in one site. If this were ever to actually happen, presumably it would be an international consortium where there are 10-100 sites in several regions that create technician jobs in those areas (and also reduce the insane demand for 100,000 technicians all in one city). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also other things might boost efficiency and price.</p><p> Most obvious would be nanotechnological systems: likely sooner than people commonly assume, yet might take long enough to arrive to make effect on this project minor if it starts soon. Yet design-ahead for dream equipment might be a sound move.</p><p> Advanced robotics and automation is likely major gamechanger. The whole tissue management infrastructure needs to be automated from the start, but there will likely be a need for rapid turnaround lab experimentation too. Those AI agents are not just for doing standard lab tasks but also for designing new environments, tests, and equipment. Whether this can be integrated well in something like the CAIS infrastructure is worth investigating. But rapid AI progress also makes it hard to do plan-ahead.</p><p> Biotech is already throwing up lots of amazing tools. Maybe what we should look for is a way of building the ideal model organism - not just with biological barcodes or convenient brainbow coloring, but with useful hooks (in the software sense) for testing and debugging. This might also be where we want to look at counterparts to minimal genomes for minimal nervous systems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:57:06 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:57:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> Yet design-ahead for dream equipment might be a sound move.</p></blockquote><p> Thoughts on what that might look like? Are there potentially tractable paths you can imagine people starting work on today? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Imagine a &quot;disassembly chip&quot; that is covered with sensors characterizing a tissue surface, sequences all proteins, carbohydrate chains, and nucleic acids, and sends that back to the scanner main unit. A unit with nanoscale 3D memory storage and near-reversible quantum dot cellular automata processing. You know, the usual. This is not feasible right now, but I think at least the nanocomputers could be designed fairly well for the day we could assemble them (I have more doubts about the chip, since it requires to solve a lot of contingencies... but I know clever engineers). Likely the most important design-ahead pieces may not be superfancy like these, but parts for standard microscopes or infrastructure that are normally finicky, expensive or otherwise troublesome for the project but in principle could be made much better if we only had the right nano or microtech.</p><p> So the design-ahead may be all about making careful note of every tool in the system and having people (and AI) look for ways they can be boosted. Essentially, having a proper parts list of the project itself is a powerful design criterion.</p></div></section><h2> What would General Groves do? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:52:51 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:52:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Davidad -- I recognise we&#39;re coming up on your preferred cutoff time. One other concrete question I&#39;m kind of curious to get your take on, if you&#39;re up for it:</p><p> &quot;If you summon your inner General Groves, and you&#39;re given a $1B discretionary budget today... what does your &#39;first week in office&#39; look like? What do you set in motion concretely?&quot; Feel free to splurge a bit on experiments that might or might not be necessary. I&#39;m mostly interested in the exercise of concretely crafting concrete plans. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:54:28 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:54:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> (also, I guess this might kind of be what <a href="https://www.aria.org.uk/our-team/">you&#39;re doing with ARIA</a> , but for a different plan... and sadly a smaller budget :) ) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 17:02:19 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 17:02:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yes, a completely different plan, and indeed a smaller budget. In this hypothetical, I&#39;d be looking to launch several FROs, basically, which means recruiting technical visionaries to lead attacks on concrete subproblems:</p><ol><li> Experimenting with serial membrane protein barcodes.</li><li> Experimenting with parallel membrane protein barcodes.</li><li> Build a dedicated brain-imaging synchrotron beamline to begin more frequent experiments with different approaches to stabilizing tissue, pushing the limits on barcoding, and performing R&amp;D on these purported 1-2 OOM speed gains.</li><li> Manufacturing organoids that express a diversity of human neural cell types.</li><li> Just buy a handful of light-sheet microscopes for doing small-scale experiments on organoids - one for dynamic calcium imaging at cellular resolution, and several more for static expansion microscopy at subcellular resolution.</li><li> Recruit for a machine-learning team that wants to tackle the problem of learning the mapping between the membrane-protein-density-annotated-hypergraph that we can get from static imaging data, and the system-of-ordinary-differential-equations that we need to simulate in order to predict dynamic activity data.</li><li> Designing robotic assembly lines that manufacture light-sheet microscopes.</li><li> Finish the C. elegans upload.</li><li> Long-shot about magnetoencephalography.</li><li> Long-shot about neural dust communicating with ultrasound.</li></ol></div></section><h2> Some unanswered questions</h2><p> Me (jacobjacob) and Lisa started the dialogue by brainstorming some questions. We didn&#39;t get around to answering all of them (and neither did we intend to). Below, I&#39;m copying in the ones that didn&#39;t get answered. </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><ul><li> What if the proposal succeeds? If the proposal works, what are the…<ul><li> …limitations? For example, I heard Davidad say before that the resultant upload might mimic <a href="https://en.wikipedia.org/wiki/Henry_Molaison"><u>patient HM</u></a> : unable to form new memories as a result of the simulation software not having solved synaptic plasticity</li><li> …risks? For example, one might be concerned that the tech tree for uploading is shared with that for unaligned neuromorphic AGI. But short timelines have potentially made this argument moot. What other important arguments are there here?</li></ul></li><li> As a reference class for the hundreds of thousands of microscopes needed... what is the world&#39;s current microscope-building capacity? What are relevant reference classes here for scale and complexity -- looking, for example, at something like EUV litography machines (of which I think ASML produce ~50/year currently, at like $100M each)? </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:12:02 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:12:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Some further questions I&#39;d be interested to chat about are:<br><br> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.</p><p> 1/a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from say the dynamic data collected in step 4, then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.</p><p> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)<br><br> 2. Curious to chat a bit more about what the free parameters are in step 5.<br><br> 3. What might we possibly be missing and is it important? Stuff like extrasynaptic dynamics, glia cells, etc.</p><p> 3/a. Side note, if we do succeed in capturing dynamic as well as static data, this seems like an incredibly rich data set for basic neuroscience research, which in turn could provide emulation shortcuts. For example, we might be able to more accurately model the role of glia in modulating neural firing, and then be able to simulate more accurately according to whether or not glia are present (and how type of cell, glia cell size, and positioning around the neuron matters, etc).<br><br> 4. Curious to think more about how to dynamically measure the brain (step 3). Thin living specimens with human genomes and then using the fluorescence paradigm. I&#39;m considering whether there are tradeoffs in only seeing slices at a time where we might be missing data on how the slices might communicate with each other. I wonder if it&#39;d make sense to have multiple sources of dynamic measurements which get combined.. though ofc there are some temporal challenges there, but I imagine that can be sorted out.. like for example using the whole brain ultrasound techniques developed by Sumner&#39;s group.  In the talk you mentioned neural dust and communicating out with ultrasound, that seems incredibly exciting. I know UC Berkeley and other Unis were working on this somewhat, though I&#39;m currently unsure what the main blockers here are.</p></div></section><h2> Appendix: the proposal</h2><p> Here&#39;s a screenshot of notes from Davidad&#39;s talk. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pckbhvxfiqtudnriv6ta"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/kxkdwijk3qowx0cj95dw"></figure><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work<guid ispermalink="false"> FEFQSGLhJFpqmEhgi</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 03 Nov 2023 02:21:51 GMT</pubDate></item></channel></rss>