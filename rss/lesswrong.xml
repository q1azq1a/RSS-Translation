<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 19 日，星期日 04:14:27 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Altman firing retaliation incoming?]]></title><description><![CDATA[Published on November 19, 2023 12:10 AM GMT<br/><br/><p> “匿名消息人士”向记者坚称 OpenAI 员工正在计划一场“反政变”以恢复 Altman 的职位，有些人甚至声称计划推翻董事会。</p><p>这似乎是投资者甚至大型科技公司的一项策略，目的是创造一个自我实现的预言，以创建一个 OpenAI 员工联盟，而此前还没有这样的联盟。</p><p>这里发生的事情充满了权势人物的廉价而轻松的举动。值得注意的是，人工智能投资公司和大型科技公司在权力动态方面经验丰富、经验丰富，甚至有可能利用人工智能与用户数据的结合进行<a href="https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated#:~:text=When%20Anthropic%20grabs,of%20human%20psychology.">充分的心理研究，以在非常规环境中运用大量的操纵能力</a>，这可能已经是远不如面对面的对话，但可能仍然仅限于通过社交媒体等数字环境进行操纵。</p><p>像微软这样的公司也与美国 Natsec 社区有联系，并且也存在来自那里的潜在风险（我对美国 Natsec 社区的模型是，他们可能仍然对人工智能安全感到困惑或不感兴趣，但可能根本不会对任何人工智能安全感到困惑或不感兴趣）更长，并且<a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for">可能对使用人工智能和人工智能行业促进现代信息战非常感兴趣和熟悉</a>）。随机投资者的反击似乎是目前最好的解释，我只是认为这值得一提；众所周知，像微软这样的公司是你最好不要惹的力量。</p><p>如果这种情况真的发生了，如果人工智能安全真的对人工智能行业产生了巨大的影响，那么了解这些事情就很重要了。</p><p>这些文章中的大多数都是付费的，所以我不得不将它们粘贴到与<a href="https://www.lesswrong.com/posts/eHFo7nwLYDzpuamRM/sam-altman-fired-from-openai">Altman 主要讨论帖子</a>不同的单独帖子中，而且似乎有各种各样的人在各种各样的地方应该尽快得到通知。</p><p></p><p> <a href="https://www.forbes.com/sites/alexkonrad/2023/11/18/openai-investors-scramble-to-reinstate-sam-altman-as-ceo/?sh=10fea87660da"><strong>福布斯：OpenAI 投资者计划在最后一刻与微软合作，恢复 Sam Altman 首席执行官职务</strong></a>（太平洋标准时间下午 2:50，付费）</p><blockquote><p> OpenAI 董事会以令人震惊的方式<a href="https://www.forbes.com/sites/davidjeans/2023/11/17/sam-altman-out-at-openai/?sh=6f3c03c54691">解雇了前首席执行官萨姆·奥尔特曼 (Sam Altman)</a> ，一天后，该公司的<a href="https://www.forbes.com/sites/alexkonrad/2023/11/17/openai-investors-blindsided-by-sam-altmans-firing/?sh=6aed1f6f3fc9">投资者</a>正在密谋如何让他复职，这将是一场更令人惊讶的反击。</p><p>四位消息人士告诉<i>《福布斯》</i> ，在 OpenAI 的营利性实体中持有职位的风险投资公司已讨论与微软和该公司的高级员工合作，以召回 Altman，尽管他已向一些人表示他打算创办一家新的初创公司。</p><p>目前尚不清楚这些公司是否能够施加足够的压力来完成这一举措，并以足够快的速度让奥特曼保持兴趣。</p><p>一位消息人士告诉<i>《福布斯》</i> ，该剧本很简单：让 OpenAI 的新管理层（在代理首席执行官 Mira Murati 和其余董事会的领导下）承认，由于高级研究人员的大规模反抗、扣留微软的云计算积分以及投资者的潜在诉讼。面对这样的组合，人们的想法是管理层必须接受奥特曼的回归，这可能会导致那些被认为推动奥特曼下台的人随后离职，其中包括联合创始人伊利亚·苏茨克韦尔(Ilya Sutskever)和董事会董事、Quora 首席执行官亚当·德安吉洛(Adam D&#39;Angelo)。</p><p>两位消息人士称，如果这一努力未能及时落实，Altman 和 OpenAI 前总裁格雷格·布罗克曼 (Greg Brockman) 将准备为一家新初创公司筹集资金。 “如果他们不能尽快解决这个问题，他们就会继续与 Newco 合作，”一位消息人士补充道。</p><p>截至发稿时，OpenAI 尚未回应置评请求。微软拒绝置评。</p><p>周六早些时候，《The Information》 <a href="https://www.theinformation.com/articles/openai-co-founder-altman-plans-new-venture?utm_campaign=OpenAI+Crisis+Update&amp;utm_content=2279&amp;utm_medium=email&amp;utm_source=cio&amp;utm_term=1650">报道</a>称 Altman 已经与投资者会面，为此类项目筹集资金。一位与奥特曼关系密切的消息人士表示，这两种选择仍然是可能的。 “我认为他确实想要最好的结果，”该人士表示。 “他不想看到生命被毁。”</p><p>任何恢复尝试的关键参与者都将是 OpenAI 的主要合作伙伴微软，该公司已向该公司注资 100 亿美元。据彭博社<a href="https://www.bloomberg.com/news/articles/2023-11-18/openai-altman-ouster-followed-debates-between-altman-board?sref=YK080Hgh#xj4y7vzkg">报道</a>，首席执行官萨蒂亚·纳德拉 (Satya Nadella) 对此次罢免感到惊讶和“愤怒”。根据 Semafor 的<a href="https://www.semafor.com/article/11/18/2023/openai-has-received-just-a-fraction-of-microsofts-10-billion-investment">报告</a>，微软仅向 OpenAI 发送了上述金额的一小部分。一位了解微软想法的消息人士表示，该公司希望看到关键合作伙伴的稳定性。</p><p> The Verge 周六<a href="https://www.theverge.com/2023/11/18/23967199/breaking-openai-board-in-discussions-with-sam-altman-to-return-as-ceo">报道</a>称，OpenAI 董事会正在讨论让 Altman 回归的事宜。目前尚不清楚此类讨论是否是投资者压力的直接结果。</p></blockquote><p> <a href="https://www.wsj.com/tech/openai-trying-to-get-sam-altman-back-4b728049?mod=hp_lead_pos1"><strong>华尔街日报：OpenAI 投资者在突然解雇后试图让 Sam Altman 重新担任首席执行官</strong></a>（太平洋标准时间下午 3:28，付费）</p><blockquote><p>知情人士称，OpenAI 的投资者正在努力召回 <a href="https://www.wsj.com/tech/sam-altman-departs-open-ai-mira-murati-interim-ceo-41f6d51e"><u>周五被罢免的首席执行官</u></a>萨姆·奥尔特曼 (Sam Altman)，这是 ChatGPT 背后的人工智能公司一系列快速发展的事件的最新进展。</p><p>知情人士称，奥特曼正在考虑回归，但已告诉投资者他想要一个新的董事会。知情人士称，他还讨论了创办一家公司，聘请前 OpenAI 员工，并正在这两种选择之间做出选择。</p><p>知情人士称，预计奥特曼将很快在这两种选择之间做出决定。 OpenAI 的主要股东，包括<a href="https://www.wsj.com/market-data/quotes/MSFT"><u>微软</u></a><u> </u>和风险投资公司 Thrive Capital 正在帮助协调恢复奥特曼的工作。微软向 OpenAI 投资了 130 亿美元，并且是其主要的财务支持者。 Thrive Capital是该公司第二大股东。</p><p>知情人士称，该公司的其他投资者也支持这些努力。</p><p>此次谈判正值 OpenAI 董事会突然决定与 Altman 分道扬镳，理由是他在沟通中缺乏坦诚，并降职其总裁兼联合创始人格雷格·布罗克曼 (Greg Brockman)，导致他辞职后，该公司陷入混乱。</p><p>奥特曼被解雇的确切原因尚不清楚。但据知情人士透露，几周来，OpenAI 商业产品的快速扩张引发了紧张局势，一些董事会成员认为这违反了公司开发安全人工智能的最初章程。</p></blockquote><p> <a href="https://www.theverge.com/2023/11/18/23967199/breaking-openai-board-in-discussions-with-sam-altman-to-return-as-ceo">The Verge：OpenAI 董事会正在与 Sam Altman 讨论重新担任首席执行官</a>（太平洋标准时间下午 2:44，非付费）</p><blockquote><p>据多位知情人士透露，OpenAI 董事会正在与 Sam Altman 讨论重返公司担任首席执行官的事宜。其中一位人士表示，奥特曼<a href="https://www.theverge.com/2023/11/17/23965982/openai-ceo-sam-altman-fired">周五在没有任何通知的情况下被董事会突然解雇</a>，他对回归感到“矛盾”，并希望进行重大的治理变革。</p><p> Altman 在被赶下台后仅一天就与公司进行了会谈，这表明 OpenAI 在没有他的情况下正处于自由落体状态。 OpenAI 总裁兼前董事会主席格雷格·布罗克曼 (Greg Brockman) 被解雇数小时后辞职，两人一直在与朋友和投资者讨论创办另一家公司的事宜。周五，一批高级研究人员<a href="https://www.theinformation.com/articles/three-senior-openai-researchers-resign-as-crisis-deepens?rc=k5vrz1">也辞职了</a>，接近 OpenAI 的人士表示，更多的离职人员正在酝酿之中。</p><p>奥特曼对于回归“矛盾”</p><p> OpenAI 最大的投资者微软在 Altman 被解雇后不久发表声明称，该公司“仍然致力于”与这家人工智能公司的合作关系。然而，OpenAI 的投资者没有收到提前警告，也没有机会对董事会罢免 Altman 的决定发表意见。作为公司的代言人和人工智能领域最杰出的代言人，在<a href="https://www.theverge.com/23610427/chatbots-chatgpt-new-bing-google-bard-conversational-ai">竞争对手竞相追赶空前</a>崛起的 ChatGPT 之际，他的下台让 OpenAI 的未来陷入不确定性。</p><p> OpenAI 的发言人没有回应有关 Altman 与董事会讨论回归的置评请求。微软发言人拒绝置评。</p><p> OpenAI 目前的董事会由首席科学家 Ilya Sutskever、Quora 首席执行官 Adam D&#39;Angelo、前 GeoSim Systems 首席执行官 Tasha McCauley 以及乔治城安全与新兴技术中心战略总监 Helen Toner 组成。与传统公司不同，董事会的任务不是最大化股东价值，而且他们都不持有 OpenAI 的股权。相反，他们宣称的使命是确保创建“广泛有益的”通用人工智能（AGI）。</p><p>据多个消息来源称，Sutskever 也是 OpenAI 的联合创始人并领导其研究人员，<a href="https://www.theverge.com/2023/11/17/23966446/what-happened-to-sam-altman-open-ai">他在本周罢免 Altman 的过程中发挥了重要作用</a>。消息人士称，他在政变中所扮演的角色表明公司研究部门和产品部门之间存在权力斗争</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/TWvLLuRmRxNrrbcHg/altman-firing-retaliation-incoming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TWvLLuRmRxNrrbcHg/altman-firing-retaliation-incoming<guid ispermalink="false"> TWvLLuRmRxNrrbcHg</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sun, 19 Nov 2023 00:10:16 GMT</pubDate> </item><item><title><![CDATA[When Will AIs Develop Long-Term Planning?]]></title><description><![CDATA[Published on November 19, 2023 12:08 AM GMT<br/><br/><p> 【我写这篇文章主要是为了澄清我的想法。我不清楚这对读者是否有价值。 ]</p><p>我预计十年内，人工智能将能够完成目前人类90%的工作。我并不是说 90% 的人类都会被淘汰。我的意思是，普通工人可以将 90% 的任务委托给 AGI。</p><p>我对这对于人工智能的长期规划和战略意味着什么感到困惑，如果人工智能的协调不善，就会使人工智能造成大规模的伤害。</p><p>实现长期目标的能力对于人工智能来说很难发展吗？</p><p>我指的长期目标既需要长期的视野，又需要一定的能力来预测多个干预步骤的结果。</p><h3>进化论的证据</h3><p>进化论提供了一些证据，证明这很困难。</p><p>对于大多数物种来说，做任何需要提前几天计划的事情似乎并不常见。我能找到的多月计划的主要例子似乎足够专业，以至于它们可能涉及无法适应新任务的本能：海狸建造水坝和松鼠储存食物。</p><p>人类的成功表明，更普遍的长期规划能力是有价值的。所以可能存在一些选择压力。进化到达到人类规划水平所需的时间表明它相对困难。</p><p>人类婴儿有能力发展长期规划能力。似乎他们会从出生时拥有这些计划能力中受益，但他们需要数年时间才能发展。根据 ChatGPT 的说法：</p><blockquote><p>幼儿期（3-6 岁）：随着孩子们开始发展出更好的记忆力和预测未来的能力，他们对时间的理解也开始萌芽。然而，他们对更长时期的把握仍然不成熟。他们可能理解“明天”，但对“下周”或“下个月”的概念感到困惑。</p></blockquote><blockquote><p>童年中期（7-10岁）：在这个阶段，孩子对时间的理解变得更加复杂，他们开始发展延迟满足和超前思考的能力。例如，他们可能会省钱购买想要的玩具，或者了解现在学习以便在以后的考试中取得好成绩的想法。然而，他们制定长期计划（例如，未来数月或数年）的能力仍然有限。</p></blockquote><p>这一证据表明，人工智能可能需要更长的训练时间，或者与世界进行更多样化的互动，这超出了我在 10 年内的预期。</p><h3>规划的障碍</h3><p>我问 ChatGPT 开发能够进行长期规划的人工智能有哪些障碍。它的答案包括时间信用分配、环境复杂性、利用与探索困境以及反馈延迟。</p><p>我将以不同的方式给出我的答案：很难开发出足够通用的休闲模型来处理各种场景。</p><h3>人工智能会有所不同吗？</h3><p>通过观察大型数据集中的相关性可以获得很多知识。当前的人工智能培训几乎完全集中于此。</p><p>相比之下，人类的童年涉及对儿童环境的一些积极干预。我希望这能为构建因果模型提供更好的证据。</p><p>这意味着将法学硕士扩大到大致人类水平将使人工智能在因果建模方面的能力相对较弱，因此规划能力也相对较弱。</p><p>然而，我并不认为人工智能的进步只会扩大法学硕士的规模。机器人技术似乎可能变得重要。机器人将接受培训，使他们能够开发出比相对聪明的法学硕士更复杂的因果模型。</p><p>机器人会成为人工智能的一个独立分支，还是会与法学硕士知识相结合？我希望至少能进行一些集成，哪怕只是为了让它们易于通过自然语言进行指导。我不清楚是否会有强烈的动机来不断更新具有最强大的法学硕士类型知识的机器人。</p><p>机器人会被训练成拥有良好的人类因果模型吗？我可以想象答案是否定的，因为对人类进行建模很困难，而且将制造工厂设计为仅限机器人的环境相对简单。我对这个预测的信心相当低。</p><p>默认情况下，机器人的因果模型将变得多么通用？</p><h3>迄今为止最好的人工智能规划？</h3><p>我寻找人工智能长期规划的好例子。</p><p> OpenAI 的<a href="https://openai.com/research/vpt">Minecraft 游戏系统</a>看起来比较令人印象深刻。它在制作钻石镐方面的表现大致达到了人类水平。人类专家通常需要 20 分钟和 24,000 次操作才能完成这一任务。</p><p>但人工智能独立学习了多少规划呢？少于摘要所暗示的。该任务需要按顺序收集其他 11 件物品。看起来他们训练人工智能时对每件物品都给予奖励，因此在训练的任何一个阶段，它只是找出如何以熟悉的顺序收集一件新物品。</p><p>他们能够做到这一点听起来仍然令人印象深刻，但这可能与我所说的长期规划并不接近。这项研究本来可以受益于长期规划。他们未能实现这一目标是另一个小证据，表明长期规划是困难的。</p><p>另一个 Minecraft 系统<a href="https://arxiv.org/abs/2305.16291">Voyager</a>通过为其想要执行的每个任务编写代码块来玩 Minecraft。当执行由多个子任务组成的任务时，它可以重用已经编写的函数来执行这些子任务。我在这里看到了一些令人印象深刻的搜索和构图，但没有太多计划。</p><p>如果我发挥我的想象力，我可以看到这种方法有朝一日有可能带来人类水平或更好的规划。但就目前而言，人工智能似乎正在以两岁人类的水平进行规划，而在其他推理能力上则更接近四岁儿童。我预计这种相对成熟的情况会持续一段时间。</p><h3> LeCun 的 JEPA 模型</h3><p>Yann LeCun 有一个制定人类级规划的策略，在<a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">《走向自主机器智能之路》</a>中概述：</p><blockquote><p>人类和许多动物能够构想出多层次的抽象，通过将复杂的动作分解为较低层次的动作序列，可以进行长期预测和长期规划。</p></blockquote><blockquote><p> JEPA 学习抽象的能力表明了该架构的扩展，可以处理多个时间尺度和多个抽象级别的预测。直观上，低级表示包含大量有关输入的细节，可用于短期预测。但可能很难以相同的细节水平做出准确的长期预测。相反，高级抽象表示可以实现长期预测，但代价是消除大量细节。</p></blockquote><p> LeCun 很可能拥有人类长期规划的最佳方法之一。如果是这样，他认为人类水平的人工智能还有很长的路要走，这在某种程度上证明了规划的发展将会缓慢。</p><h3>结论</h3><p>这种分析不可避免地存在不确定性。可能有一些简单的技巧可以让人工智能规划比人类规划更好。但这似乎是我能做的最好的分析。</p><p>我倾向于期待一个不平凡的时期，在这个时期，人工智能大多具有人类水平的能力，但对于流氓人工智能而言，目光短浅，不会成为一个主要问题。</p><p>因此，我预计最严重的人工智能风险将比我根据智商测试进行预测的时间晚几年。</p><p>我对未来一年多的时间抱有一定的希望，希望人工智能助手能够为加快安全研究做出相当大的贡献。</p><br/><br/> <a href="https://www.lesswrong.com/posts/8tuzCv9ujgoTPcgiA/when-will-ais-develop-long-term-planning#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8tuzCv9ujgoTPcgiA/when-will-ais-develop-long-term-planning<guid ispermalink="false"> 8tuzCv9ujgoTPcgiA</guid><dc:creator><![CDATA[PeterMcCluskey]]></dc:creator><pubDate> Sun, 19 Nov 2023 00:08:19 GMT</pubDate> </item><item><title><![CDATA[Superalignment]]></title><description><![CDATA[Published on November 18, 2023 10:37 PM GMT<br/><br/><p> <strong>OpenAI 宣布了他们打算使用的方法，以确保人类比他们更聪明地控制人工智能：</strong></p><p>我们的目标是建立一个大致达到人类水平的自动对齐研究人员。然后，我们可以使用大量计算来扩展我们的工作，并迭代地调整超级智能。</p><p>为了对齐第一个自动对齐研究人员，我们需要 1) 开发可扩展的训练方法，2) 验证生成的模型，3) 对整个对齐管道进行压力测试：</p><ol><li>为了针对人类难以评估的任务提供训练信号，我们可以利用人工智能系统来协助评估其他人工智能系统（可扩展监督）。此外，我们希望了解和控制我们的模型如何将我们的监督推广到我们无法监督的任务（泛化）。</li><li>为了验证我们系统的一致性，我们自动搜索有问题的行为（鲁棒性）和有问题的内部结构（自动可解释性）。</li><li>最后，我们可以通过故意训练未对准的模型来测试我们的整个流程，并确认我们的技术可以检测到最严重的未对准类型（对抗性测试）。</li></ol><br/><br/><a href="https://www.lesswrong.com/posts/TjmrgHPvb36atGWAP/superalignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TjmrgHPvb36atGWAP/superalignment<guid ispermalink="false"> TjmrgHPvb36atGWAP</guid><dc:creator><![CDATA[Douglas_Reay]]></dc:creator><pubDate> Sat, 18 Nov 2023 22:37:41 GMT</pubDate> </item><item><title><![CDATA[Predictable Defect-Cooperate?]]></title><description><![CDATA[Published on November 18, 2023 3:38 PM GMT<br/><br/><p><i>认知状态：我认为这里写的所有内容都非常明显，但我在其他地方没有看到过。如果您能提供有关该主题的资源，那就太酷了！</i></p><p><i>写作理由：我曾经在推特上看到过关于多个超级智能如何可预测地最终达到缺陷-缺陷平衡的相当混乱的讨论，我怀疑如果我能加入这个玩具示例，讨论会更好。</i></p><p>如果 Agent 与<a href="https://arxiv.org/abs/1401.5577">PrudentBot</a>合作且不与 DefectBot 合作，则 PrudentBot 与已知源代码的 Agent 合作。它是不可利用的，并且不会留下大量的实用性。但我们能做得更好吗？我们如何才能形式化“两个智能体都明白什么是程序均衡，但他们可以预见地最终会陷入缺陷合作的情况，因为一个智能体非常聪明”的概念？</p><p>让我们从玩具模型开始吧。想象一下，您要与 PrudentBot 或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">CooperateBot</span></span></span></span></span></span></span>对抗<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1-p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>每个概率。支付矩阵为 5;5, 10;0, 2;2。机器人不能直接和你玩，但你可以编写程序来玩。您的目标是获得最大的期望值。</p><p>如果你合作，你总是会得到 5 个，所以如果你期望得到超过 5 个，你应该背叛：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2p + 10 - 10p > 5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span></p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p < 5/8"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span></p><p>因此，我们的UncertainBot应该采用概率分布，判断遇到PrudentBot的概率是否小于5/8并且缺陷，否则合作。 PrudentBot 和 DefectBot 的混合也是如此：如果你背叛，你肯定会得到 2，所以</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="5p + 0(1 - p) > 2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p > 2/5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span></p><p>我们可以发明更好版本的 DefectBot 吗？我们可以想象 TraitorBot，它采用 UncertainBot 的信念状态并预测它是否可以逃脱叛逃并以其他方式进行合作。根据之前对 PrudentBot 和 DefectBot 混合的分析，很明显，如果 PrudentBot 的概率高于 2/5，则 TraitorBot 会缺陷，<i>否则</i>进行合作，产生的效用严格不低于 Cooperate;Cooperate 的效用。</p><p>这样的设置提供了<i>令人惊奇的</i>探索可能性。</p><p>探索足够聪明的代理人之间如何发生背叛的可能性：</p><ul><li>首先，TraitorBot 可以通过不领先于 UncertainBot 来获胜。</li><li>其次，在现实世界中，我们没有写有“缺陷/合作”的按钮。如果我们试图决定是否建立由超级智能设计的纳米技术，你知道超级智能的确切行动 - 你只是不知道这个行动是合作还是背叛。</li><li>第三，这里的TraitorBot是“按标签”定义的。如果我们在 UncertainBot 中存在具有不同概率的 TraitorBot1 和 TraitorBot2，那么当两个相同的算法由于另一个算法中的错误表示而得到不同的结果时，我们可能会得到奇怪的动态。另一方面，可能存在多个级别的欺骗，并且尚不清楚如何实施它们。我的猜测是，欺骗的程度取决于欺骗者执行了多少推理步骤。</li></ul><p>重要的理论时刻：</p><ul><li>目前尚不清楚如何协调 Löb 定理与 UncertainBot。当我试图写一些类似“可以证明，如果 UncertainBot 在其信念状态下存在缺陷，那么 PrudentBot 也存在缺陷，因此，UncertainBot 会合作，而 PrudentBot 可证明会合作”，这伤害了我的大脑。我怀疑这是“逻辑不确定性”之一。</li><li>将 UncertainBot 与 TraitorBot 合并在一个实体内，即在对其他实体的概率信念具有概率信念的事物内部，可以预测它是否可以在给定其他实体概率信念的情况下逃脱叛逃，这将是一件好事……我不知道如何做到这一点以这种程度的自我参照来工作。</li></ul><p>在完美的理想发展中，我希望在囚徒困境中有一种欺骗理论，它可以向我们展示在什么条件下，聪明的代理人可以逃脱对不太聪明的代理人的背叛，以及我们是否可以首先防止这种情况的出现。</p><br/><br/> <a href="https://www.lesswrong.com/posts/FZfnK9A3DpdbBjkyh/predictable-defect-cooperate#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FZfnK9A3DpdbBjkyh/predictable-defect-cooperate<guid ispermalink="false"> FZfnK9A3DpdbBjkyh</guid><dc:creator><![CDATA[quetzal_rainbow]]></dc:creator><pubDate> Sat, 18 Nov 2023 15:38:44 GMT</pubDate> </item><item><title><![CDATA[I think I'm just confused.  Once a model exists, how do you "red-team" it to see whether it's safe.  Isn't it already dangerous?]]></title><description><![CDATA[Published on November 18, 2023 2:16 PM GMT<br/><br/><p>我想你明白了，但是说 openAI“训练”了 GPT-5，结果证明它非常危险，它可以说服任何人任何事情，并且想要毁灭世界。</p><p>我们已经搞砸了，对吧？谁在乎他们是否决定不向公众发布它？或者就像他们现在不能“RLHF”一样，对吧？它已经存在存在危险了吗？</p><p>我想也许我只是不明白它是如何工作的。那么，如果他们“训练”GPT-5，这是否意味着他们在训练完成之前根本不知道它会说什么或是什么样子？然后他们就会说“嘿，怎么了？”他们发现了吗？</p><br/><br/> <a href="https://www.lesswrong.com/posts/m4QpTeHfYFKTNRhnr/i-think-i-m-just-confused-once-a-model-exists-how-do-you-red#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/m4QpTeHfYFKTNRhnr/i-think-im-just-confused-once-a-model-exists-how-do-you-red<guid ispermalink="false"> m4QpTeHfYFKTNRhnr</guid><dc:creator><![CDATA[FTPickle]]></dc:creator><pubDate> Sat, 18 Nov 2023 14:16:26 GMT</pubDate> </item><item><title><![CDATA[AI Safety Camp 2024]]></title><description><![CDATA[Published on November 18, 2023 10:37 AM GMT<br/><br/><p> <strong>AI 安全营</strong>将您与研究负责人联系起来，共同开展一个项目，看看您的工作可以在哪些方面帮助确保未来 AI 的安全。</p><p> 12 月 1 日之前<a href="https://airtable.com/appi7jDH1gAfAZDyC/shrwKt5p0TKG86j9G">申请</a>，2024 年 1 月至 4 月在线合作。</p><p></p><p>我们重视多元化的背景。许多角色（但<u>并非所有角色）都</u>需要以下其中之一的知识：人工智能安全、数学或机器学习。</p><p><strong>各种项目要求的一些技能：</strong></p><ul><li>艺术、设计、摄影</li><li>人文学者</li><li>沟通</li><li>营销/公关</li><li>法律专业知识</li><li>项目管理</li><li>可解释性方法</li><li>使用法学硕士</li><li>编码</li><li>数学</li><li>经济学</li><li>网络安全</li><li>阅读科学论文</li><li>了解科学方法</li><li>独立思考和工作</li><li>熟悉人工智能风险研究领域</li></ul><p></p><h2>项目</h2><p><strong>不要构建无法控制的人工智能</strong><br>限制企业不计后果地扩展机器学习模型的训练和使用的项目。给定可控极限。<br></p><ul><li> 1. <a href="https://docs.google.com/document/d/1ROdSPA5TaJDe18pK59YUyEnv0qqUqkEbrUezLamcn54/edit?usp=sharing">为基于基础模型的人工智能产品实现现实的 ODD</a><br> 2. <a href="https://docs.google.com/document/d/1KeO_zaTDMSGKKYHW_TiAQdvrvvJkKy5tCINV8zE7oM8/edit">Luddite Pro：精炼勒德分子的信息</a><br>3.<a href="https://docs.google.com/document/d/1fblXOOz4KLn8EA1omhrwdIUJYB9NJJ7LxUdRPGDWvXo/edit">限制人工智能数据洗钱的律师（和程序员）</a><br> 4. <a href="https://docs.google.com/document/d/1vZMMr7gSNNmlVI1C3ouagcbOKTcRmVfQ0L05DbCZf6Q/edit?usp=sharing">评估国会 AIS 信息传递活动的潜力</a></li></ul><p><strong>其他一切</strong><br>其他多样化的项目，包括符合人类价值观的 AGI 技术控制。<br></p><ul><li>机甲解释员<br>5. <a href="https://docs.google.com/document/d/1pAEnfUza987OMmt8fnXuOpGO9MykoVgDLcT3V48IkaY/edit?usp=sharing">语言模型轨迹建模</a><br>6. <a href="https://docs.google.com/document/d/1jce3f64Fz7PXmdCEyd9i0PTmcFaiP1pZdcBn5ye5sxY/edit?usp=sharing">实现雄心勃勃的机械解释</a><br>7.<a href="https://docs.google.com/document/d/19Lu2NDVJN-7ZdA-cQuLcqTuWJSr7SA88ZLAM8H7Qgzs/edit">探索代理的玩具模型</a><br>8. <a href="https://docs.google.com/document/d/1DbRHPBlFUFojiEEGJnI0ghNOWnE5bOZBNXC1fC51T9I/edit#heading=h.jepgk0u8wx8p">高水平的机械解释和激活工程库</a><br>9. <a href="https://docs.google.com/document/d/1qs0v6emq3Sn1UbDbstmsz7rzT38qklvIl4q3gw9lvaU/edit#heading=h.9lmc73wscx1r">脱离情境学习的可解释性</a><br>10. <a href="https://unsearch-ai.notion.site/AISC-2024-Research-Proposal-5adbd5918fe443c491a0a5b4252113fc">理解 Transformer 中的搜索和目标表示</a></li><li>评估和指导模型<br>11. <a href="https://docs.google.com/document/d/1N0Kcvu3aNTUWTXMi3rE9PrrRVcF4soR6ucgq4ej0g8I/edit#heading=h.9lmc73wscx1r">稳定反射率的基准</a><br>12. <a href="https://docs.google.com/document/d/1WARwqgH9UAIri2I9215OoLZPTJdq4evLPN2wgs0hP3Y/edit#heading=h.9lmc73wscx1r">SADDER：用于检测极端风险的态势感知数据集</a><br>13. <a href="https://docs.google.com/document/d/1myBsd-LqTaEWOqZwqOX3UBphOGiEdlANDtOA1a-lFFg/edit#heading=h.9lmc73wscx1r">TinyEvals：语言模型如何说连贯的英语？</a><br> 14. <a href="https://docs.google.com/document/d/1hLdNZhzQgGSRDIdulHi2lZjM159iyYcZ5PFJNAPma1s/edit#heading=h.9lmc73wscx1r">评估一致性评估</a><br>15. <a href="https://docs.google.com/document/d/1P-SrvH9V8IGa_rP_c7uNkobIcQ5LXWrLouJER2NnAE4/edit#heading=h.9lmc73wscx1r">评估和引导法学硕士实现忠实推理的管道</a><br>16.<a href="https://docs.google.com/document/d/1VYWKz_Oly6vtJKaA4iXWzKLDpNhI7pJ_h-3FXw08JHQ/edit#">通过添加具有潜在道德效价的激活向量来指导法学硕士</a></li><li>代理基金会<br>17. <a href="https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit#heading=h.zat5vvnwtl0j">高驱动空间</a><br>18. <a href="https://docs.google.com/document/d/1VGmOim2pGm8NZMFQSaBpEXXI9AcmRcai1viq33_njQs/edit#heading=h.9lmc73wscx1r">充分优化是否意味着代理结构？</a><br> 19.<a href="https://docs.google.com/document/d/1Xqmh9By3yZGVWLgvHWxl4saw2u4S71GouOU_ms9UoqY/edit">在原始字节流中发现代理</a><br>20. <a href="https://docs.google.com/document/d/1GoXaYtUyanRrkBcjAknafqdKDCyTzzgxpckYMBtgPkQ/edit#heading=h.9lmc73wscx1r">科学算法</a></li><li>各种对齐方法<br>21. <a href="https://docs.google.com/document/d/1JhmK31IwYGcwqX0nKmxKsbmTh_DX3o1OoW7NJmhVbIw/edit#heading=h.l9ot7n22b6ry">SatisfIA——既能满足又不会过度的人工智能</a><br>22. <a href="https://docs.google.com/document/d/1cXU-DoE2O2vLhVBRWYFBcXyyOuvwVQx97wwtMroJzZU/edit#heading=h.9lmc73wscx1r">自动化比对研究有多大前景？ （文献综述）</a><br> 23. <a href="https://docs.google.com/document/d/1NMRmB9NL_Sv_u8H4Fmn6mwCRHW4Pbbje-OvHQrrm0Co/edit#heading=h.1443xgz5n5cm">AI价值调整的个性化微调代币</a><br>24.<a href="https://docs.google.com/document/d/1fMropF42vJLyKsm99XLk1UK8iCnlrjs6NhryUUCr9IM/edit">自我他人重叠@AE Studio</a><br> 25. <a href="https://docs.google.com/document/d/1wuHalfHjAmFA-RKYcq3MBptFYLPvoMdy9KTPelyZ67A/edit#heading=h.9lmc73wscx1r">法学硕士中的非对称控制：模型编辑和控制，抵制不对齐的控制</a><br>26.<a href="https://docs.google.com/document/d/1BLpAMmrhkl3EQY18AFX5rBIwNRBX4HMOggEz-oZPlpE/edit">应对辩论中的关键挑战</a></li><li>其他<br>27. <a href="https://docs.google.com/document/d/13Jv8HV3D40Ig-lgj2eO7AHU4hPU3eNWuQR2FmoRF5W4/edit#heading=h.9lmc73wscx1r">人工智能驱动的经济安全网：限制AGI部署对宏观经济的干扰</a><br>28. <a href="https://docs.google.com/document/d/1VvoE1u0Mmifol5TdlQfsMXviIF0bug5CpQ5hsLEg9Fs/edit#heading=h.9lmc73wscx1r">基于策略访问强大的模型</a><br>29. <a href="https://docs.google.com/document/d/133ZqQDGtm3ZY4JqSYUfGvTPtxDLDCl8VDb6iUeOS1Do/edit#heading=h.9lmc73wscx1r">组织下一次虚拟人工智能安全会议</a></li></ul><p>请在撰写您的申请时牢记您最喜欢的项目的研究负责人。研究负责人将直接审查本轮的申请。我们的组织者只会在项目收到大量申请时才会提供帮助。</p><p></p><p><a href="https://airtable.com/appi7jDH1gAfAZDyC/shrwKt5p0TKG86j9G"><strong>现在申请</strong></a></p><p></p><h2>申请如果您...</h2><ol><li>想要考虑并尝试帮助确保未来人工智能安全运行的角色；</li><li>能够解释您为什么以及如何为一个或多个项目做出贡献；</li><li>之前研究过某个主题或接受过可以促进新团队进步的技能培训；</li><li>从<strong>2024 年 1 月到 4 月，</strong>可以参加每周的团队通话并每周抽出 5 小时的工作时间。</li></ol><p></p><h2>时间线</h2><p><strong>应用领域</strong></p><p><u>12 月 1 日之前</u>：申请。填写<a href="https://docs.google.com/document/d/12eA66mMxPSOn7FlZ4WG_gk7ajf14atnr_ZMYcxsRkFc/copy">问题文档</a>并通过<a href="https://airtable.com/appi7jDH1gAfAZDyC/shrwKt5p0TKG86j9G">表格</a>提交。</p><p> <u>12 月 1 日至 22 日</u>：面试。您可能会收到一封面试电子邮件，来自您申请的项目的一位或多位研究负责人。</p><p> <u>12 月 28 日之前</u>：最终决定。你肯定会知道你是否被录取。希望我们能早点告诉您，但我们小指发誓我们会在 12 月 28 日之前告诉您。<br></p><p><strong>程序</strong></p><p><u>1 月 13 日至 14 日</u>：周末开幕。与队友的第一次会面和一对一的聊天。</p><p> <u>1 月 15 日至 4 月 28 日</u>：研究正在进行中。团队每周举行一次会议，并在自己的工作时间内进行计划。</p><p> <u>4 月 25 日至 28 日</u>：最终演示持续四天。</p><p><strong>然后</strong></p><p><u>只要你愿意</u>：有些团队在 AISC 正式结束后仍继续合作。</p><p>当您开始项目时，我们建议您不要做出超出项目官方期限的任何承诺。然而，如果您发现你们作为一个团队合作得很好，我们鼓励您即使在 AISC 正式结束后也继续前进。 </p><p></p><p><br><img style="width:900px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FsYbie3qkqj84D98c/hazybxlx5lw0wlydmqr1" alt=""><i>第一个虚拟版本——自发的拼贴画</i><br></p><p></p><h2>团队架构</h2><p>每个团队都会有：</p><ul><li>一名研究主管 (RL)</li><li>一名团队协调员 (TC)</li><li>其他团队成员</li></ul><p>所有团队成员每周至少要在该项目上工作 5 个小时（对于特定项目，这个数字可能会更高），其中包括参加每周的团队会议，并定期与其他团队成员就他们的工作进行沟通。</p><p><strong>研究主管 (RL)</strong></p><p> RL 是研究提案背后的人。他们将指导研究项目，并跟踪相关的里程碑。当事情不可避免地不按计划进行时（这毕竟是研究），RL 负责制定新的路线。</p><p> RL 是研究团队的一部分，将像团队中的其他人一样为研究做出贡献。</p><p><strong>团队协调员（TC）</strong></p><p> TC 是团队的运营人员。如果您是 TC，那么您负责确保安排会议、检查个人的任务进度等。</p><p> TC 的作用很重要，但预计不会花费太多时间（项目管理重的团队除外）。大多数时候，TC 会像普通团队成员一样为研究做出贡献，与团队中的其他人一样。</p><p>每个项目提案都会说明是否正在寻找像您这样的人来担任这个角色。</p><p><strong>其他团队成员</strong></p><p>其他团队成员将在 RL 和 TC 的指导下开展该项目。将根据相关技能、理解和承诺来选择团队成员，为研究项目做出贡献。</p><p></p><p><a href="https://airtable.com/appi7jDH1gAfAZDyC/shrwKt5p0TKG86j9G"><strong>现在申请</strong></a></p><p></p><h2><br>问题？</h2><p>查看我们的<a href="https://aisafety.camp/faq">常见问题</a>，以便您能在那里找到答案。</p><ul><li>有关项目的问题，请联系研究负责人。在他们的<a href="https://aisafety.camp/#Projects">项目文档</a>底部找到他们的联系信息。<br></li><li>如果您对训练营有一般疑问，或者您无法联系到具体的研究负责人，请发送电子邮件至<a href="mailto:contact@aisafety.camp">contact@aisafety.camp</a> 。<br>组织者可能需要 5 天的时间才能回复。</li></ul><p></p><h2>我们正在筹款！</h2><p>由于我们不得不冻结工资，因此本轮组织者自愿参加。这是不可持续的。为了举办下一版，请考虑<a href="https://donorbox.org/ai-safety-camp">捐款</a>。如需更多金额，请随时发送电子邮件至<a href="mailto:remmelt@aisafety.camp">Remmelt</a> 。</p><p></p><p><a href="https://donorbox.org/ai-safety-camp"><strong>捐</strong></a></p><br/><br/><a href="https://www.lesswrong.com/posts/FsYbie3qkqj84D98c/ai-safety-camp-2024#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FsYbie3qkqj84D98c/ai-safety-camp-2024<guid ispermalink="false"> FsYbie3qkqj84D98c</guid><dc:creator><![CDATA[Linda Linsefors]]></dc:creator><pubDate> Sat, 18 Nov 2023 10:37:02 GMT</pubDate> </item><item><title><![CDATA[Post-EAG Music Party]]></title><description><![CDATA[Published on November 18, 2023 3:00 AM GMT<br/><br/><p><span>今年秋季</span><a href="https://www.effectivealtruism.org/ea-global/events/ea-global-boston-2023">EA 全球</a>会议回到波士顿，这是我自<a href="https://www.effectivealtruism.org/ea-global/events/ea-global-2017-boston">2017 年</a>以来第一次参加。我们一楼的租户<a href="https://www.jefftk.com/p/downstairs-opening-2br-apartment">最近</a>搬出了，我们的新租户还没有搬进来：这是举办余兴派对的好机会！</p><p>我决定在楼上的公寓举办一场聚会，举办棋盘游戏和安静的谈话，并在楼下空荡荡的公寓里播放音乐。我整个晚上都在楼下度过，所以我不知道比赛进展如何，但我对音乐感到非常满意！</p><p>由于人们来自外地，可能没有自己的乐器，所以我在房间里设置了一系列选项：键盘、脚鼓、电贝司、电曼陀林、（古典）吉他、小提琴、摇床、大号、男中音、小号、口哨等。朋友带来了长笛和钢弦吉他。这可能有点过分了：大多数人对键盘、钢弦、贝斯、曼陀林和鼓感兴趣。</p><p> <a href="https://www.jefftk.com/post-eag-music-party-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HJZ7j3NHwGvB2iGNB/fqltt9d0nr5srurtnyfp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HJZ7j3NHwGvB2iGNB/fqltt9d0nr5srurtnyfp 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HJZ7j3NHwGvB2iGNB/flhduxppuki8bxw8dml0 1100w"></a></p><div></div><p></p><p>正如这种音乐家聚会所常见的那样，我事先并不知道我们最终会演奏什么样的音乐。我记得的一些事情：</p><p></p><ul><li><p>简单的小提琴曲调：Angeline the Baker、Sandy Boys。有几个人认识他们，其他人可以接他们，其他人则觉得有点尴尬而被排除在外。</p></li><li><p>世俗至日正典： <a href="https://humanistculture.bandcamp.com/track/the-circle-scratch-recording">《The Circle</a> 》、《 <a href="https://www.jefftk.com/p/hamephorash">haMephorash》</a> 、《Still Alive》、 <a href="https://humanistculture.bandcamp.com/track/brighter-than-today">《Brighter than Today》（波士顿版）</a> 、 <a href="https://www.jefftk.com/p/simplified-level-up">《Level Up》（简化版）</a> 、 <a href="https://genius.com/Tom-lehrer-we-will-all-go-together-when-we-go-lyrics">《We Will All Go Together》</a> 、《 <a href="https://www.jefftk.com/p/resetting-somebody-will-v2">Somebody Will》</a> （未简化版）、 <a href="https://www.echoschildren.org/CDlyrics/WORDGOD.HTML">《God Write the World》</a> 、《 <a href="https://vixyandtony.bandcamp.com/track/uplift">Uplift》</a> 、 <a href="https://genius.com/Julia-ecklar-and-leslie-fish-hymn-to-breaking-strain-lyrics">《Break Strain 赞美诗</a>》 。也许还有其他一些？当我们演奏《比今天更光明》时，一群人中途下楼加入唱歌。</p></li><li><p>舞曲：我们创作了一些摇摆乐、华尔兹和波尔卡舞的音乐。摇摆舞和华尔兹没有被教授或召唤；我教了一套（三对）克里套装。</p></li><li><p>跟唱：在手机上查找歌词和《Rise up Singing》续集的结合。我至少记得《Viva la Vida》、《I Want You Back》、《Let it Go》和《House of the Rising Sun》，但还有很多。他们中的一些人崩溃了（很难判断你是否有一个足够了解一首歌的人来领导它，有时你认为你了解一首歌，但你实际上只知道副歌）但大多数情况下他们进展顺利并且是乐趣。</p></li></ul><p>大概 80% 的歌曲有歌词，20% 是器乐？我很高兴很多人尝试了<a href="https://www.jefftk.com/p/introduction-to-heel-toe-drumming">脚鼓</a>，并且总体上对人们的音乐水平以及他们尝试走出自己的舒适区的意愿感到非常满意。</p><p>我们走了大约四个小时，有些人一直待在那里，有些人则进进出出。平均一次可能有 15 个人？我度过了一个非常美好的夜晚，感觉这是从充满相对激烈的一对一对话的周末中放松下来的好方法。</p><br/><br/><a href="https://www.lesswrong.com/posts/HJZ7j3NHwGvB2iGNB/post-eag-music-party#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HJZ7j3NHwGvB2iGNB/post-eag-music-party<guid ispermalink="false"> HJZ7j3NHwGvB2iGNB</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 18 Nov 2023 03:00:07 GMT</pubDate> </item><item><title><![CDATA[Letter to a Sonoma County Jail Cell]]></title><description><![CDATA[Published on November 18, 2023 2:24 AM GMT<br/><br/><p>我不知道这是否会在 LessWrong 上获得成功。它不是用首选的中性语气写的。但这似乎值得说，并且我认为值得在这里分享。</p><br/><br/> <a href="https://www.lesswrong.com/posts/FxAvKcrWRDvBBQm3d/letter-to-a-sonoma-county-jail-cell#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FxAvKcrWRDvBBQm3d/letter-to-a-sonoma-county-jail-cell<guid ispermalink="false"> FxAvKcrWRDvBBQm3d</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Sat, 18 Nov 2023 02:24:11 GMT</pubDate> </item><item><title><![CDATA[Sam Altman fired from OpenAI]]></title><description><![CDATA[Published on November 17, 2023 8:42 PM GMT<br/><br/><p>基本上只是标题，请参阅 OAI 博客文章了解更多详细信息。</p><blockquote><p>奥特曼先生的离职是在董事会经过慎重审查之后得出的结论，他在与董事会的沟通中始终不坦诚，阻碍了董事会履行职责的能力。董事会不再对他继续领导 OpenAI 的能力充满信心。</p><p>董事会在一份声明中表示：“OpenAI 的构建是为了推进我们的使命：确保通用人工智能造福全人类。董事会仍然全力致力于实现这一使命。我们感谢 Sam 对 OpenAI 的创立和发展做出的许多贡献。与此同时，我们相信，随着我们的前进，新的领导层是必要的。作为公司研究、产品和安全职能部门的领导者，米拉非常有资格担任临时首席执行官。我们对她在这一过渡时期领导 OpenAI 的能力充满信心。”</p></blockquote><hr><p>编辑：</p><p>此外，格雷格·布罗克曼 (Greg Brockman) 将从董事会席位上辞职：</p><blockquote><p>作为此次过渡的一部分，格雷格·布罗克曼 (Greg Brockman) 将辞去董事会主席职务，并继续担任公司职务，向首席执行官汇报。</p></blockquote><p>其余董事会成员为：</p><blockquote><p> OpenAI 首席科学家 Ilya Sutskever、独立董事 Quora 首席执行官 Adam D&#39;Angelo、技术企业家 Tasha McCauley 以及乔治城安全与新兴技术中心的 Helen Toner。</p></blockquote><hr><p>编辑2：</p><p>萨姆·奥尔特曼 (Sam Altman)<a href="https://twitter.com/sama/status/1725631621511184771">发推文如下</a>。</p><blockquote><p>我很喜欢在 openai 的时光。这对我个人来说是变革性的，希望对世界也有一点变革。最重要的是，我喜欢与这些才华横溢的人一起工作。</p><p>稍后将有更多关于接下来的事情要说。 </p><p><img style="width:1.2em" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eHFo7nwLYDzpuamRM/hxqbjl4dgtfoz7svch5d" alt="🫡"></p></blockquote><p>格雷格·布罗克曼<a href="https://twitter.com/gdb/status/1725667410387378559">也已辞职</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/eHFo7nwLYDzpuamRM/sam-altman-fired-from-openai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eHFo7nwLYDzpuamRM/sam-altman-fired-from-openai<guid ispermalink="false"> eHFo7nwLYDzpuamRM</guid><dc:creator><![CDATA[LawrenceC]]></dc:creator><pubDate> Fri, 17 Nov 2023 20:42:31 GMT</pubDate> </item><item><title><![CDATA[On the lethality of biased human reward ratings]]></title><description><![CDATA[Published on November 17, 2023 6:59 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Fri, 13 Oct 2023 23:56:45 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Fri, 13 Oct 2023 23:56:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>我正在仔细阅读<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">死亡清单</a>并考虑我对每一点的看法。<br><br>我想我非常不明白#20，我想也许你可以解释一下我错过了什么？</p><blockquote><p> <strong>20</strong> .人类操作员容易犯错、易损坏且容易被操纵。<strong>人类评估者会犯系统性错误——有规律的、可简洁描述的、可预测的错误</strong>。<i>忠实地</i>从“人类反馈”中学习函数就是（从我们外部的角度来看）学习对人类偏好的不忠实描述，其中的错误不是随机的（从我们希望传递的外部角度来看）。如果你完美地学习并完美地最大化人类操作员分配的奖励<i>的参考</i>，那就会杀死他们。这是关于领土，而不是地图的事实，关于环境，而不是优化器的事实，人类答案的<i>最佳预测</i>解释是预测我们反应中的系统错误，因此是一个正确预测更高分数的心理学概念这将被分配给人为错误产生的案例。</p></blockquote><p>我想我不明白这一点。</p><p> （也许有帮助的一件具体事情就是举一些例子。）</p><hr><p>这可能指向的一件事是我称之为“动态反馈方案”的问题，例如 RLHF。动态反馈方案的关键特征是，人工智能系统生成输出，人类评估者为其提供反馈，以强化良好的输出并反强化不良的输出。</p><p>此类方案的问题在于，对于人类评估者看来不错但实际上很糟糕的输出存在逆向选择。这意味着，从长远来看，你正在强化最初的意外失实陈述，并将其塑造成越来越复杂的欺骗（因为你反强化了所有被发现的失实陈述案例，并强化了所有未被发现的失实陈述案例） t）。</p><p>这似乎非常糟糕，因为我们最终没有进入一个所有指标看起来都很棒，但潜在现实却很糟糕或空洞的世界，正如保罗在<a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like">《失败是什么样子》第一部分中所描述的那样</a>。</p><p>看起来也许你可以通过静态反馈机制来避免这种情况，你可以对结果进行一堆描述，可能是程序生成的，可能来自小说，可能来自新闻报道，等等，然后让人们根据结果的好坏程度对这些结果进行评分，构建可用于训练的奖励模型。只要评级没有反馈到生成器中，就没有太多系统性的激励来训练欺骗。</p><p> ……其实，转念一想，我想这只是把问题倒退了一步。现在你有了一个奖励模型，它可以向你正在训练的某些人工智能系统提供反馈。人工智能系统将学习以与人类博弈相同的方式对抗奖励模型。</p><p>这似乎是一个真正的问题，但它似乎也不是列表中这一点试图解决的问题。它似乎在说更像是“奖励模型将会是<i>错误的</i>，因为人类评级会存在系统性偏差。”</p><p>公平地说，这似乎是真的，但我不明白为什么这是<i>致命的</i>。奖励模型似乎在某些地方是错误的，我们会在这些地方失去价值。但为什么奖励模型需要在所有领域都是精确的、高保真度的表示，以免杀死我们呢？为什么奖励模型在可预测的方向上稍微偏离了一点点，就会带来灾难性的后果？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sat, 14 Oct 2023 07:47:41 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sat, 14 Oct 2023 07:47:41 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>首先要做的事情是：</p><ul><li>你所说的“动态反馈方案”问题确实是一个致命的问题，我认为它与尤德科夫斯基的#20 不太一样，正如你所说。</li><li> “人类评级中将会存在系统性偏差”……从技术上讲是正确的，但我认为这是一种误导性的思考方式，因为“偏差”一词通常表明数据大致正确，但略有偏差。这里的问题是，可以预见的是，在许多政权中，人类的评级与人类实际想要的<i>相差甚远</i>。<ul><li> （此处相关的更一般原则：Goodheart 是关于泛化，而不是近似。近似不存在 Goodheart 问题，只要近似<i>在任何地方</i>都是准确的。）</li></ul></li><li>因此奖励模型<i>不需要</i>是精确的、高保真度的表示。近似可以，“稍微偏离”也可以，但它需要<i>在任何地方</i>都是近似正确的。<ul><li> （实际上这里还有一些进一步的漏洞 - 特别是在近似值和“实际”价值函数都分配非常低的奖励/效用的地方，近似值<i>有时</i>可能会更加错误，这取决于我们所处的环境类型以及如何优化器有能力。）</li><li> （我们还可以讨论在保持“正确性”的同时可以应用什么样的转换，但我认为这在这一点上无关紧要。只是想指出那里也存在一定程度的自由度.)</li></ul></li></ul><p>我希望我们主要想讨论一些例子，在这些例子中，人类的评级与人类真正想要的相差甚远。在此之前，上述要点是否有意义（就它们看起来相关而言），以及在举例之前我们应该触及任何其他高级要点吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sat, 14 Oct 2023 18:37:58 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sat, 14 Oct 2023 18:37:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>我希望我们主要想讨论一些例子，在这些例子中，人类的评级与人类真正想要的相差甚远。</p></blockquote><p>这是正确的！<br><br>我不确定每一点有多重要，或者我们是否需要深入探讨高层次问题，但以下是我的回答： </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sat, 14 Oct 2023 18:38:02 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sat, 14 Oct 2023 18:38:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>你所说的“动态反馈方案”问题确实是一个致命的问题，我认为它与尤德科夫斯基的#20 不太一样，正如你所说。</p></blockquote><p>我不太清楚为什么这个问题本身是致命的。<br><br>我想，如果你训练一个系统想要做看起来不错的事情，而牺牲实际上好的事情，那么你正在训练它，例如，杀死所有可能干扰其运行的人，然后欺骗传感器让这些人类看起来还活着并且幸福。就像，这就是优化“看起来你表现得很好”的预期值的行为。</p><p>根据我现在的内心观点，这个论点感觉像是“老师会说的话”，而不是“这显然是正确的”。</p><p>为自己充实一下：训练一些东西来关心它的输出在[某些特定的非全知观察者]看来是什么样子是一个严重的失败，因为在高能力水平上，最大化该目标的明显策略是抓住传感器并努力优化它们所看到的内容，并控制宇宙的其余部分，以便没有其他东西影响传感器所看到的内容。</p><p>但是，当您使用 RLHF 进行训练时，您将强化“做看起来好的事情”和“做实际上好的事情”的混合。<i>一些</i>“做真正好的事情”将成为人工智能激励系统的动力，这似乎与控制整个世界来欺骗某些传感器等无情的超级恶棍计划相抵触。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sat, 14 Oct 2023 19:14:50 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sat, 14 Oct 2023 19:14:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>（此处相关的更一般原则：Goodheart 是关于泛化，而不是近似。近似不存在 Goodheart 问题，只要近似<i>在任何地方</i>都是准确的。）</p></blockquote><p>是的，你以前对我说过这句话，但我还没有真正理解它。</p><p>看来古德哈特的很多内容都是关于近似值的！</p><p>就像我 20 岁时，我决定以优化“每天阅读的页数”作为衡量标准，这可以预见地导致我转向阅读更轻松、阅读速度更快的书籍。这似乎是古德哈特的一个例子，与概括无关。该指标并没有完美地捕捉到我所关心的内容，因此当我优化它时，我得到的东西很少。但我不会将其描述为“这个指标未能推广到轻量级、易于阅读的书籍的边缘情况领域”。你会？</p><blockquote><p>近似值不存在 Goodheart 问题，只要近似值处处准确<i>即可</i>。</p></blockquote><p>这句话让人觉得你所说的“近似”是指非常精确的东西。<br><br>就像如果你有 af(x) 函数，并且你有另一个函数 a(x) 近似它，你可以很舒服地将它称为近似值，如果 a(x) +/- C = f(x)，对于某些“合理-大小”常数 C，或类似的东西。<br><br>但我明白，危险的古德哈特至少不是当你的模型稍微偏离时，而是当你的模型在域的某些区域严重偏离地面事实时，因为没有足够的数据点确定该区域的模型。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sat, 14 Oct 2023 19:21:00 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sat, 14 Oct 2023 19:21:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>因此奖励模型<i>不需要</i>是精确的、高保真度的表示。近似可以，“稍微偏离”也可以，但它需要<i>在任何地方</i>都是近似正确的。</p></blockquote><p>至少在精神上这似乎是正确的，尽管我不知道这是否是真的。就像有些情况不太可能被观察到一样，因此值的近似值如何推广并不重要。</p><p>但是，是的，开发强大的 AGI 的一个关键点是，在重大能力提升带来以前不可用（或什至设想）的新选项之后，你无法预测它/我们将陷入什么样的疯狂情况。我们需要人工智能的激励系统在那些对我们来说非常奇怪和预先不可预测的情况下正确地概括（匹配我们实际想要的）。<br><br>总结起来就是“我们需要模型在任何地方都能大致正确地概括”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sat, 14 Oct 2023 19:30:18 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sat, 14 Oct 2023 19:30:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>这是一些吹毛求疵的说法，但我有一个基本的想法，那就是“让你的人工智能模型近似于好的东西可能没问题。但是，如果近似值在某些地区与真实情况大幅波动，那就是一个大问题了。”行动/结果的空间。” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sun, 15 Oct 2023 19:43:56 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sun, 15 Oct 2023 19:43:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>好吧，我们将从一些例子开始，这些例子<i>不是</i>核心的“杀死我们的事物”，而是更熟悉的日常事物，旨在建立一些背景直觉。特别是，我试图建立的直觉是，人类给出的评级对于我们想要的东西来说是一个非常糟糕的代理，并且我们已经可以在日常生活中看到很多这方面的证据（在数量上比对齐问题更小）生活。</p><p>让我们从一本教科书开始：<a href="https://www.amazon.com/Well-Being-Foundations-Psychology-Daniel-Kahneman/dp/0871544237">幸福：享乐心理学的基础</a>。整个前五章（共 28 章）集中在标题为“我们如何知道谁幸福？概念和方法论问题”的部分中。我已经有一段时间没有读过这本书了，但我记得的要点是：我们可以通过多种不同的方式来衡量幸福感，但它们之间的相关性并不那么好。 （不确定以下哪个例子是我从教科书中得到的，还是从其他地方得到的，但这应该给出一般的格式塔印象……）询问人们他们在一项活动中有多快乐，这与他们记得之后的快乐程度不太相符-事实，或者他们事先预测有多幸福。当问及不同类型的幸福时，例如当下的享受或长期的满足，人们会给出截然不同的答案。 “复杂的感觉”是一件事 - 例如（不是来自书中的心理模型）人们有不同的部分，一部分可能对某件事感到高兴，而另一部分可能对同一件事感到不高兴。然后就是“想要想要”的整个现象，以及“我想要什么”和“根据其他人或我自己‘我应该想要’的东西”之间的关系。当然，人们对于哪些事情会或不会<i>让</i>他们快乐的理解通常非常糟糕。</p><p>我预计，如果你对人类的评级进行“一点点”优化（在“大量优化”涉及后奇点疯狂的东西的范围内），这些问题将会成为一个大问题。再说一次，这并不一定会导致人类灭绝，但我想它会导致像<a href="https://www.imdb.com/title/tt4955642/">《好地方》</a>这样的事情。 （有人可能会合理地回答：等等，这不只是对当今经济的描述吗？对此我要说的是：事实上，现代经济在某种程度上对人类的评级/预测幸福/记忆幸福/等施加了温和的优化压力这使大多数人明显“富裕”，但在大多数情况下往往让人们在当下不那么快乐，并且从长远来看也不太满意。）</p><p>此类事情的一些具体例子：</p><ul><li>我去主题公园。之后，我记得各种很酷的时刻（例如在过山车上），以及排长队等待。虽然排队占了公园 95% 的时间，但它们却占据了我记忆的 30%。</li><li>人们对性的感受往往是一团糟：（1）他们中的一部分人想要/不想要直接的体验，（2）他们中的一部分人想要/不想要一段关系的体验，或者调情或任何与性有关的事情，（3）他们中的一部分人不想要/不想要关于他们的性取向的某种身份/形象，（4）他们中的一部分人想要（或想要不-）想要）性，（5）他们中的一部分主要关心别人对自己性活动的看法，（6...）等。</li><li>饥饿是一种现象，很多人在饥饿时并没有意识到。</li><li> IIRC，事实证明，与人们的预期相比，每日通勤时间对人们幸福感的影响大得离谱。</li><li>另一方面，IIRC 认为，亲人去世或严重受伤等事情对长期幸福的影响通常比人们预期的要小得多。</li></ul><p>顺便说一句：在某个时候，我希望在 LW 上看到应用乐趣理论序列。前面的大部分内容可能会集中在“如何使你对让你快乐的事情的理解与实际让你快乐的事情相匹配”，即避免上述的陷阱。</p><p>好吧，接下来是一些更强的优化如何出错的例子...... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Sun, 15 Oct 2023 22:15:51 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Sun, 15 Oct 2023 22:15:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>[我的猜测是，我在这里带来了一堆单独的混乱，我们将不得不一次处理一个。也许我在这里的回应与最初的讨论有所偏差，也许我们想单独处理。 ]<br><br>因此，“幸福”是一个直观的概念（并且与什么构成美好世界的问题高度相关），不幸的是，即使是少量的压力/分析，它也会崩溃。</p><p>从表面上看，我们似乎必须研究很多哲学（包括作为“哲学”一部分的经验科学）才能有一个幸福的概念，或者可能是一系列更清晰定义的概念，以及它们之间的关系和关系。相对价值权重，或者更不直观的东西，我们可以依靠它来对美好世界有一个<i>清晰的</i>概念。</p><p>但我们需要那个吗？<br><br>假设我只是指出我对幸福的民间概念，通过向强大的人工智能提供一万亿个我称之为“幸福”的情况的例子，包括野餐、去水上乐园、去露营（并享受它）以及努力工作项目，和朋友一起看电影，在下雨天读书等等（包括一千个我现在还不够聪明想出的边缘案例，以及一些附近的例子，比如“去露营和讨厌它”）。人工智能是否能够发现这些共性并学习出我们可以使用的“相当不错”的幸福概念？</p><p>它不会准确地了解我对幸福的概念。但正如您所指出的，这从一开始就不是一个连贯的目标。我<i>没有</i>一个精确的幸福概念来尝试精确匹配。我实际上拥有的是一个概念的模糊云，由于它的模糊性，它非常适合人工智能可能生成的<i>一堆</i>可能的概念。</p><p></p><p> ...现在，我猜你会说，如果你尝试在“相当不错”的概念上努力优化，我们会一直努力，直到所有实际的优点都被耗尽。</p><p>我不确定这是否属实。我们最终得到的将是高度优化的，所以这会很奇怪，但我没有清晰的直觉来判断结果是否仍然对我来说是好的。</p><p>似乎一万亿个数据点就足够了，即使您进入一个全新的分布，剩下的任何自由度对于您想要三角测量的概念来说都不是核心。</p><p>例如，如果你给人工智能一万亿个快乐人类的例子，它会学习一个价值概念，从而决定如果人类是仿真的话会更好，我会说“是的，看起来不错。” ems与生物人类有很大不同，但这种差异与他们的生命价值正交（我认为）。享受乐趣的人就是享受乐趣的人，无论他们的底质如何。<br><br>然而，如果人工智能学习了一个价值概念，当它高度优化时，会创造出一群僵尸人类，假装享受乐趣，但没有人“在家”享受它，我会感到恐惧。与底层的轴不同，意识与非意识的轴与价值<i>极其</i>相关。<br><br>如果你有足够的数据点，并将它们输入到一个非常智能的深度学习 AGI 分类器中，那么这些数据点似乎可能会三角化出一个“相当不错”的概念，而该概念不具有任何与价值相关的自由度。所有与价值相关的轴，所有如果在超级优化中被破坏而让我们感到震惊的地方，都包含在 AGI 的价值概念中。</p><p>即使我们自己的价值概念相当模糊和不明确，这仍然是正确的。<br><br><br>就像隐喻一样，我们似乎并没有试图瞄准价值空间中的某个点。我们正在尝试绑定一个卷。如果您有足够的数据点，您可以在每个重要维度上限制体积。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sun, 15 Oct 2023 23:16:30 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sun, 15 Oct 2023 23:16:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>好的，这涉及到一些有趣的点，让我们在讨论更致命的故障模式之前深入研究它们。</p><blockquote><p>假设我只是指出我对幸福的民间概念，通过给强大的人工智能提供一万亿个我称之为“幸福”的情况的例子，包括野餐、去水上乐园、露营、努力完成一个项目，以及观看和朋友一起看电影，在雨天读书等等（包括一千个我现在还不够聪明想出的边缘情况）。人工智能是否能够发现这些共性并学习出我们可以使用的“相当不错”的幸福概念？</p></blockquote><p>这需要一些设置。</p><p>想象一下，我们训练人工智能的方式是使其内部认知通常围绕与人类基本相似的概念构建。在这个人工智能的内部，有一些与人类概念基本相似的结构，可以“指向”（大致相当于编程语言中的指针），这意味着它们是可以传递到内部的东西。优化过程（例如规划），或内部推理过程（例如学习有关概念的新事物），或内部沟通过程（例如将人类单词映射到概念）等。然后我们可能进一步想象我们可以摆弄人工智能思维的内部将该概念设置为驱动人工智能行动的某些规划过程的目标，从而使人工智能与该概念“对齐”。</p><p>当我谈论人工智能与某个概念“一致”意味着什么时，这大致就是我想到的心理模型。 （请注意，我不一定认为这是对人工智能内部结构的很好描述；这只是我所说的“对齐”的意思最明显的设置。）</p><p>考虑到这种心理模型，回到这个问题：如果我们给人工智能一万亿个你称之为快乐的情况的例子，人工智能是否会发现共性并学习一个“相当不错”的幸福概念，我们可以使用它？嗯，我绝对想象人工智能最终会围绕大致类似于人类的幸福概念（或多个概念）构建其一些认知。 （我们不需要一万亿个例子，甚至根本不需要任何标记的例子 - 无监督训练就可以很好地实现这个目标。）但这并不意味着任何内部规划过程都使用这个概念作为目标；人工智能不一定<i>符合</i>这个概念。</p><p>因此，对于诸如“人工智能是否能学习类似人类的幸福概念”之类的问题，我们需要澄清我们是否在问：</p><ul><li>人工智能的某些内部认知是否是围绕类似人类的幸福概念构建的，特别是以支持类似内部指针之类的方式构建的？</li><li>是否有一个内部规划/搜索过程以概念为目标，然后相应地驱动人工智能的行为？</li></ul><p>我猜前者是“是”，后者是“否”。 （由于讨论是从关于以利以谢的一项主张的问题开始的，我将在这里指出，我认为以利以谢会对两者都说“不”，这使得整个问题变得更加困难。）</p><blockquote><p>我<i>没有</i>一个精确的幸福概念来尝试精确匹配。我实际上拥有的是一个概念的模糊云，由于它的模糊性，它非常适合人工智能可能生成的<i>一堆</i>可能的概念。</p></blockquote><p>我直觉地认为这两句话有一个错误，有点像以利以谢的类比“认为一把未知的钥匙与一把未知的锁相匹配”。让我尝试稍微解释一下这种直觉。</p><p> （至少）在两种意义上人们可以拥有“概念的模糊云”。首先是统计意义上的集群；例如，您可以想象高斯聚类的混合。在这种情况下，存在“模糊云”，即簇在特征空间中没有离散边界，但仍然存在清晰明确的簇（即每个簇的均值和方差是精确可估计的） 。我可以谈论集群，而且我所说的内容没有任何歧义。当谈到概念时，这就是我所说的“普通情况”。但在这种情况下，我们谈论的是第二种“概念的模糊云”——并不是有一个清晰的集群，而是根本没有集群，有一堆不同的集群它们本身并不一定形成一个巨型集群，而且我们正在谈论哪一个或我们想要谈论哪一个都是不明确的。</p><p>错误在于从“我们不确定我们在谈论哪件事或我们想谈论哪件事”到“因此人工智能可能会抓住我们可能正在谈论的任何事情，并且这将符合我们的意思”。想象一下，爱丽丝说“我想要一个 flargle。也许 flargle 意味着一把椅子，也许是矮牵牛，也许是一座火山，或者也许是一条 50 岁的发情蓝鲸，不确定。”然后鲍勃回答“太好了，这是一朵矮牵牛。”。就像，[爱丽丝不知道她想要这四件事中的哪一件]这一事实并不意味着[通过给她四件事中的一件，鲍勃给了她她想要的东西]。鲍勃实际上给了她一个“也许她想要但也许不是”的东西。</p><blockquote><p> ...现在，我猜你会说，如果你尝试在“相当不错”的概念上努力优化，我们会一直努力，直到所有实际的优点都被耗尽。</p></blockquote><p>如果你真的设法将人工智能与所讨论的概念（在上述意义上）保持一致，我实际上认为结果<i>可能</i>会很好。其他各种问题也随之而来，但在我看来，没有一个问题那么困难或那么重要。</p><p>问题在于使人工智能与相关概念保持一致。如果我们只是针对某种人类评级来优化人工智能，并在训练中投入相当大的优化压力，那么我不认为它最终会与这些评级所代表的概念保持一致。我预计它最终会与评级过程本身（人工智能的概念）保持一致。</p><p> （同样，埃利泽在这里会说一些不同的话 - IIUC 他会说人工智能最终可能会与某些外星概念保持一致。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sun, 15 Oct 2023 23:39:44 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sun, 15 Oct 2023 23:39:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>在这一点上，我将在讨论开始时指出有关以利以谢主张的一些重要内容：</p><blockquote><p> 20. 人类操作员容易犯错、容易损坏、容易被操纵。<strong>人类评估者会犯系统性错误——有规律的、可简洁描述的、可预测的错误</strong>。<i>忠实地</i>从“人类反馈”中学习函数就是（从我们外部的角度来看）学习对人类偏好的不忠实描述，其中的错误不是随机的（从我们希望传递的外部角度来看）。如果你完美地学习并完美地最大化人类操作员分配的奖励<i>的参考</i>，那就会杀死他们。这是关于领土，而不是地图的事实，关于环境，而不是优化器的事实，人类答案的<i>最佳预测</i>解释是预测我们反应中的系统错误，因此是一个正确预测更高分数的心理学概念这将被分配给人为错误产生的案例。</p></blockquote><p>请注意，这一说法实际上与以下说法完全兼容：如果你在一堆标记为“快乐”和“不快乐”的人类示例上训练人工智能，并要求它提供更多“快乐”，那就行得通！ （显然，Eliezer 并不期望这能起作用，但问题 20 本身是不够的。）Eliezer 在这里说，如果你真的努力优化人类分配的奖励，那么人类最终会死掉。这种说法与以下问题是分开的：在一堆“快乐”和“不快乐”的人类标签示例上训练的人工智能是否实际上最终会针对“快乐”/“不快乐”标签进行努力优化。</p><p> （例如，我目前最好的亚历克斯·特纳模型是这样的：“也许人工智能的一些内部认知最终会围绕预期的幸福概念进行构建，并且内部错位会对我们有利，以这样的方式人工智能的内部搜索/规划和/或行为启发法也可能最终指向预期的“幸福”概念，而不是“快乐”/“不快乐”标签或一些外来概念”。这将是“最简单的版本”<a href="https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default">默认对齐</a>”的故事。重点是，埃利泽的上述主张实际上与所有这些都是正交的，因为它说，<i>假设</i>人工智能最终为“快乐”/“不快乐”标签进行了努力优化，人类就会死亡。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sun, 15 Oct 2023 23:42:18 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sun, 15 Oct 2023 23:42:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>现在来谈谈更致命的问题。我现在假设我们有一个相当强大的人工智能，可以直接优化人类的评分。</p><p> ...实际上，我认为您可能已经知道这是如何出错的？想要给出答案和/或出价将对话引向您所困惑的更核心的方向吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Tue, 31 Oct 2023 18:15:34 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Tue, 31 Oct 2023 18:15:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>[我对这个回应的总体感觉是……我一定是错过了重点。]</p><blockquote><p>考虑到这种心理模型，回到这个问题：如果我们给人工智能一万亿个你称之为快乐的情况的例子，人工智能是否会发现共性并学习一个“相当不错”的幸福概念，我们可以使用它？嗯，我绝对想象人工智能最终会围绕大致类似于人类的幸福概念（或多个概念）构建其一些认知。 （我们不需要一万亿个例子，甚至根本不需要任何标记的例子 - 无监督训练就可以很好地实现这个目标。）但这并不意味着任何内部规划过程都使用这个概念作为目标；人工智能不一定<i>符合</i>这个概念。</p><p>因此，对于诸如“人工智能是否能学习类似人类的幸福概念”之类的问题，我们需要澄清我们是否在问：</p><ul><li>人工智能的某些内部认知是否是围绕类似人类的幸福概念构建的，特别是以支持类似内部指针之类的方式构建的？</li><li>是否有一个内部规划/搜索过程以概念为目标，然后相应地驱动人工智能的行为？</li></ul></blockquote><p>正确的。在我看来，这听起来像是经典的内部对齐和外部对齐的区别。人工智能可以学习一些人类本体概念，并对其进行推理，但这与“这些概念是否进入人工智能的动机系统？”是一个非常不同的问题。<br></p><blockquote><p> （至少）在两种意义上人们可以拥有“概念的模糊云”。首先是统计意义上的集群；例如，您可以想象高斯聚类的混合。在这种情况下，存在“模糊云”，即簇在特征空间中没有离散边界，但仍然存在清晰明确的簇（即每个簇的均值和方差是精确可估计的） 。我可以谈论集群，而且我所说的内容没有任何歧义。当谈到概念时，这就是我所说的“普通情况”。但在这种情况下，我们谈论的是第二种“概念的模糊云”——并不是有一个清晰的集群，而是根本没有集群，有一堆不同的集群它们本身并不一定形成一个巨型集群，而且我们正在谈论哪一个或我们想要谈论哪一个都是不明确的。</p></blockquote><p><strong>还有一种中间状态，其中有许多内部紧密的集群，彼此之间形成松散的集群。也就是说，有许多簇比随机选择的概念列表具有更多的重叠。</strong><br><br><strong>我不知道这是否很棘手，但这就是我对“幸福”“概念”的猜测。子组件并非</strong><i><strong>完全</strong></i>不<strong>相关。那里有值得学习的地方。</strong><br><br> （如果我在这里思考的方式出于某种原因是数学胡言乱语，请告诉我。）</p><blockquote><p>想象一下，爱丽丝说“我想要一个 flargle。也许 flargle 意味着一把椅子，也许是矮牵牛，也许是一座火山，或者也许是一条 50 岁的发情蓝鲸，不确定。”然后鲍勃回答“太好了，这是一朵矮牵牛。”。就像，[爱丽丝不知道她想要这四件事中的哪一件]这一事实并不意味着[通过给她四件事中的一件，鲍勃给了她她想要的东西]。鲍勃实际上给了她一个“也许她想要但也许不是”的东西。</p></blockquote><p>我相信这个例子不会以爱丽丝得到她想要的东西而告终，但我不确定我是否相信它很好地映射到我们正在幸福地谈论的案例。如果爱丽丝只是说“我想要玩玩”，她就不会得到她想要的东西。但在训练人工智能时，我们给予它的比特数远多于单一的无根据的标签。这看起来更像是爱丽丝和鲍勃要玩一百万轮包含 20 个问题的游戏，或者是冷热游戏，这与给出一根不接地的字符串有很大不同。</p><p> （虽然我认为也许你试图在这里提出一个精确的观点，我将直接讨论它是如何应用的。）</p><blockquote><p>如果你真的设法将人工智能与所讨论的概念（在上述意义上）保持一致，我实际上认为结果<i>可能</i>会很好。其他各种问题也随之而来，但在我看来，没有一个问题那么困难或那么重要。</p><p>问题在于使人工智能与相关概念保持一致。如果我们只是针对某种人类评级来优化人工智能，并在训练中投入相当大的优化压力，那么我不认为它最终会与这些评级所代表的概念保持一致。我预计它最终会与评级过程本身（人工智能的概念）保持一致。</p></blockquote><p>听起来你在这里说的问题主要是内部对齐？<br></p><blockquote><p>我预计它最终会与评级过程本身（人工智能的概念）保持一致。</p></blockquote><p>我想我不明白这句话。与评级过程（人工智能的概念）大致一致的具体例子是什么？</p><p>这是否意味着类似以下内容...？</p><ul><li>人工智能确实学习/弄清楚了“人类幸福”的真实/合理概念（即使它是一种拼凑在一起的临时概念）。</li><li>它还学习预测评级过程的输出。</li><li>它最终是由第二件事而不是第一件事驱动的。</li></ul><p>我想我在这里遗漏了一些东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Tue, 31 Oct 2023 20:48:40 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Tue, 31 Oct 2023 20:48:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><blockquote><p>在我看来，这听起来像是经典的内部对齐和外部对齐的区别。人工智能可以学习一些人类本体概念，并对其进行推理，但这与“这些概念是否进入人工智能的动机系统？”是一个非常不同的问题。</p></blockquote><p>您已经正确地总结了这个想法，但这是与内部/外部对齐完全不同的因式分解。内部/外部是关于“我构造一个反馈信号（人工智能外部），该信号通过&lt;我想要的>;最大化”与“人工智能最终（内部）针对&lt;我想要的>;进行优化”之间的分歧。我指出的区别完全是关于人工智能内部的两个不同的事物：“人工智能围绕&lt;我想要的>;的概念构建其内部认知”，与“人工智能最终（内部）优化&lt;我想要的东西>;”。</p><p>回到这一部分：</p><blockquote><p>问题在于使人工智能与相关概念保持一致。如果我们只是针对某种人类评级来优化人工智能，并在训练中投入相当大的优化压力，那么我不认为它最终会与这些评级所代表的概念保持一致。我预计它最终会与评级过程本身（人工智能的概念）保持一致。</p></blockquote><p>我并不是说问题主要在于内在的对齐。 （有点相反，如果有人试图将其硬塞到内部/外部框架中，但整个内部/外部对齐二分法并不是理解这里所提出的观点的最短路径。）</p><blockquote><p>这是否意味着类似以下内容...？</p><ul><li>人工智能确实学习/弄清楚了“人类幸福”的真实/合理概念（即使它是一种拼凑在一起的临时概念）。</li><li>它还学习预测评级过程的输出。</li><li>它最终是由第二件事而不是第一件事驱动的。</li></ul></blockquote><p>这是完全正确的想法。它最终会被第二件事而不是第一件事所激励的明显原因是，第二件事是实际奖励的东西 - 因此，在训练期间两者不同的任何情况下，人工智能将通过追求（其概念）获得更高的奖励的高收视率而不是追求“人类幸福”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 17:13:30 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 17:13:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>这是完全正确的想法。它最终会被第二件事而不是第一件事所激励的明显原因是，第二件事是实际奖励的东西 - 因此，在训练期间两者不同的任何情况下，人工智能将通过追求（其概念）获得更高的奖励的）高收视率而不是追求（其概念）“人类幸福”。</p></blockquote><p>我认为它最终会与评级过程的预测保持一致，而不是对评级过程试图指出的事物的预测（即使在它可以清楚地看到评级过程旨在模拟什么之后）人类想要的，并且<i>可以</i>直接优化）。<br><br>不过，这让我回到了我最初的问题。有那么糟糕吗？我们是否有理由认为评级过程会在某个地方出现严重偏差？ （也许你正在为此努力。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 17:26:58 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 17:26:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p></p><blockquote><p>想象一下，我们训练人工智能的方式是使其内部认知通常围绕与人类基本相似的概念构建。在这个人工智能的内部，有一些与人类概念基本相似的结构，可以“指向”（大致相当于编程语言中的指针），这意味着它们是可以传递到内部的东西。优化过程（例如规划），或内部推理过程（例如学习有关概念的新事物），或内部沟通过程（例如将人类单词映射到概念）等。然后我们可能进一步想象我们可以摆弄人工智能思维的内部将该概念设置为驱动人工智能行动的某些规划过程的目标，从而使人工智能与该概念“对齐”。</p><p>当我谈论人工智能与某个概念“一致”意味着什么时，这大致就是我想到的心理模型。 （请注意，我不一定认为这是对人工智能内部结构的很好描述；这只是我所说的“对齐”的意思最明显的设置。）</p><p>考虑到这种心理模型，回到这个问题：如果我们给人工智能一万亿个你称之为快乐的情况的例子，人工智能是否会发现共性并学习一个“相当不错”的幸福概念，我们可以使用它？嗯，我绝对想象人工智能最终会围绕大致类似于人类的幸福概念（或多个概念）构建其一些认知。 （我们不需要一万亿个例子，甚至根本不需要任何标记的例子 - 无监督训练就可以很好地实现这个目标。）但这并不意味着任何内部规划过程都使用这个概念作为目标；人工智能不一定<i>符合</i>这个概念。<br><br>因此，对于诸如“人工智能是否能学习类似人类的幸福概念”之类的问题，我们需要澄清我们是否在问：</p><ul><li>人工智能的某些内部认知是否是围绕类似人类的幸福概念构建的，特别是以支持类似内部指针之类的方式构建的？</li><li>是否有一个内部规划/搜索过程以概念为目标，然后相应地驱动人工智能的行为？</li></ul><p>我猜前者是“是”，后者是“否”。 （由于讨论是从关于以利以谢的一项主张的问题开始的，我将在这里指出，我认为以利以谢会对两者都说“不”，这使得整个问题变得更加困难。）</p></blockquote><p>我重读了这一点。</p><p>只是澄清一下，“类人的幸福概念”<i>并不是</i>指“评级过程的预测”。你的意思是，“考虑到伊莱还没有解决他对此的哲学困惑，当他说‘幸福’时，他的大致意思是”，是吗？<br><br>我不完全确定为什么你认为人类的概念进入了认知，但没有进入动机。<br><br>我对你的猜测是...</p><ol><li>你认为存在自然的抽象，因此人类的幸福概念是趋同的。除非你故意做一些奇怪的事情，否则人工智能观察世界并在关节处雕刻现实的发展将接近与人类相同的概念，因为它只是一个对世界进行建模的富有成效的概念。</li><li>但激励系统是由评级过程决定的，无论系统学习了哪些其他概念。</li></ol><p>是这样吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Thu, 02 Nov 2023 17:33:22 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Thu, 02 Nov 2023 17:33:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><blockquote><p>只是澄清一下，“类人的幸福概念”<i>并不是</i>指“评级过程的预测”。你的意思是，“考虑到伊莱还没有解决他对此的哲学困惑，当他说‘幸福’时，他的大致意思是”，是吗？</p></blockquote><p>是的。</p><blockquote><p>我对你的猜测是...</p><ol><li>你认为存在自然的抽象，因此人类的幸福概念是趋同的。除非你故意做一些奇怪的事情，否则人工智能观察世界并在关节处雕刻现实的发展将接近与人类相同的概念，因为它只是一个对世界进行建模的富有成效的概念。</li><li>但激励系统是由评级过程决定的，无论系统学习了哪些其他概念。</li></ol><p>是这样吗？</p></blockquote><p>另外，是的，关于抽象“幸福”的自然程度的模不确定性尤其如此（根据我们上面关于它是否自然是一个“集群”/“巨型集群”的讨论）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 17:36:43 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 17:36:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>[竖起大拇指] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 17:38:02 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 17:38:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>我们关心的自然抽象事物越少，我们的工作就越困难。如果我们的概念不自然，除了让它们进入人工智能动机之外，我们还必须让它们进入人工智能认知。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Thu, 02 Nov 2023 17:56:37 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Thu, 02 Nov 2023 17:56:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><blockquote><p>不过，这让我回到了我最初的问题。有那么糟糕吗？我们是否有理由认为评级过程会在某个地方出现严重偏差？ （也许你正在为此努力。）</p></blockquote><p>太好了，听起来我们已经准备好返回主线程了。</p><p>到目前为止的心智模型总结：</p><ul><li>我们有一个人工智能，它开发一些“内部概念”，围绕这些概念构建其认知（这些概念可能与人类概念相当匹配，也可能不匹配；这就是自然抽象假设部分）。</li><li>训练将（根据这个特定心理模型的假设）诱导人工智能优化其内部概念（的某些功能）。</li><li>只要人工智能在训练期间针对[其内部概念][产生人类评级的过程]进行优化，它就会在训练中获得比针对[其内部概念]进行优化时更高的奖励。 ] [人类的幸福感，或者收视率应该代表的其他东西]。<i>训练过程中</i>两者之间的差异是因为人类的评级对于人类真正想要的东西来说是一个糟糕的代表（正如我们上面所讨论的）。</li></ul><p> ...但是现在，在我们的心理模型中，人工智能完成了训练并得到部署。也许它已经相当强大，或者也许它开始自我改进和/或建立继任者。重点是，它仍在针对[其内部概念][产生人类评级的过程]进行优化，但现在它已经面世，它可以对该概念施加更多的优化压力。</p><p>因此，举例来说，也许[人工智能的内部概念][产生人类评级的过程]可以归结为[它的模型]“一个假设的人类会看一些快照在某某时间、某某地点拍摄的世界，然后根据他们所看到的内容竖起大拇指/竖起大拇指”。然后，人工智能要做的显而易见的事情就是非常努力地优化假设的相机在这些地点和时间所看到的内容，并将宇宙的其余部分变成&lt;无论什么>;，以便非常努力地优化这些快照。</p><p>或者，也许[人工智能的内部概念][产生人类评级的过程]最终指向某处建筑物中的实际物理评级者[其模型]。然后，人工智能要做的显而易见的事情就是将这些评分者锁在机械服中，让他们的手指总是按下竖起大拇指按钮。</p><p>或者，如果我们比这更幸运的话，[人工智能的内部概念][产生人类评级的过程]最终会指向[其模型]软件中记录的位置按下“竖起大拇指”/“竖起大拇指”，然后人工智能就会接管评级软件，并用“竖起大拇指”填充数据库。 （……然后也许会用充满赞许标记的 MySQL 数据库来平铺宇宙，具体取决于人工智能的内部概念如何概括。）</p><p>这些例子有意义吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 19:09:50 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 19:09:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>是的，所有这些例子从表面上看都是有道理的。这些都是经典的奖励错误指定人工智能风险故事。</p><p> [我将在这里喋喋不休地试图阐明我的问题/不确定性。]<br><br>但因为它们是典型的人工智能风险故事，我预计这些场景会受到训练过程的惩罚。</p><p>评级过程的一部分将是“抓住评级者，让他们穿上特殊的‘竖起大拇指’套装……这非常非常糟糕。”在模拟中，这样的行为会受到很多惩罚。如果它确实发生了同样的事情，那就意味着我们的训练过程<i>根本</i>不起作用。<br><br>我们塑造了人工智能的动机系统，使其完全与其评级过程的概念保持一致，并与评级过程的参照物保持 0% 的一致。</p><p>这现实吗？似乎在人工智能系统训练的早期，它还没有一个清晰的评级过程模型，它的动机将以一种更加临时的方式形成：个别事物有好有坏，并且分散，从这些个体实例中概括出更深层次原则的半成功尝试。<br><br>在训练过程的后期，它可能会获得评级过程的详细模型，并且与评级过程的详细模型一致的内部过程会得到强化，超越竞争的内部冲动，例如“不要伤害人类”。 ..我认为，也许在我的人类中心主义中，这是一个更容易、更自然的假设，因此在人工智能有足够能力建立评级过程的详细模型之前，在训练的早期具有更大的影响力。</p><p>当人工智能被部署时，它的动机系统中就<i>没有</i>留下那种更早、更简单的推理元素了吗？<br><br> ...或者我猜，也许有，但我们只是走进了一个最近的畅通无阻的策略问题，其中人工智能没有做我们专门训练它不做的任何事情，但它做了下一个大多数[评级过程的概念]优化策略没有经过专门训练。</p><p> ...</p><p>好的。我的心理模型有一个有趣的特点，那就是事情可能会很好，这既取决于人工智能的泛化，又不泛化<i>太多。</i></p><p>就像，一方面，A，我期望人工智能的动机系统能够概括为……</p><ul><li> “用刀刺人是非常糟糕的”</li><li> “向人类开枪是非常糟糕的。”</li><li> “把人冻在碳酸盐里是非常糟糕的”</li></ul><p> ...到</p><ul><li>“侵犯人类的身体自主权是非常糟糕的。”</li></ul><p>但另一方面，B，我并不期望人工智能的动机系统能够泛化到将所有数据点泛化到生成它们的评级过程模型中，并坚持这一点，以牺牲任何“天真的”为代价。 “当原始读数与评级过程模型的预测不同时，读取任何特定数据点。</p><p>如果你没有至少像A一样多的泛化能力，你的人工智能是危险的，因为（例如）它会了解到你可以用松木柄的钢锯齿刀刺入人类的胸部，但会认为将它们刺入胸部使用钢制、带有桦木手柄的锯齿刀是获得想要的东西的聪明方法。</p><p>但是，如果您得到与 B 一样多的概括，您就不再拥有希望从“天真的”数据点读取中获得的任何安全性。一旦人工智能概括了这么多，每个数据点都只是加强了[评级过程的概念]输出的目标优化，这让你的可校正性为0。</p><p>让我检查一下我是否认为这是真的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Thu, 02 Nov 2023 19:12:15 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Thu, 02 Nov 2023 19:12:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>请注意，我们这里的故事并不完全是“奖励错误指定”。这就是为什么我们需要关于[人工智能的内部概念]&lt;东西>;的所有机制。这里有一个两步：训练过程让人工智能优化其内部概念之一，然后<i>该内部概念</i>以不同于代理的任何评级的方式进行概括。</p><p>这种区别对于以下示例很重要：</p><blockquote><p>评级过程的一部分将是“抓住评级者，让他们穿上特殊的‘竖起大拇指’套装……这非常非常糟糕。”在模拟中，这样的行为会受到很多惩罚。如果它确实发生了同样的事情，那就意味着我们的训练过程<i>根本</i>不起作用。</p></blockquote><p>如果人工智能有一个详细的训练过程的<i>内部</i>模型，并且训练过程包括将人工智能粘在模拟中，那么大概人工智能有一个模拟的内部模型（包括内部自我模型）。因此，在训练过程中，当这种“只竖起大拇指的西装”场景出现时，人工智能的实际推理将通过以下方式进行：</p><ul><li>好吧，我有机会给这些模拟人穿上只能竖起大拇指的套装。</li><li>如果我这样做，那么产生实际评级的实际人员将会给出不好的评级。</li><li>因此我不会那样做。</li></ul><p> ……这种推理得到了强化。然后当AI脱离模拟时，它会推理：</p><ul><li>好吧，我有机会把那些产生实际评分的人穿上只能竖起大拇指的套装。</li><li>如果我这样做的话，那么实际收视率将会很高。</li><li>因此我这样做。</li></ul><p> （这听起来像是一个典型的“人工智能具有战略意识，并且知道它是在模拟中”的故事，确实如此。但请注意这样的故事中并不总是出现的两件事：</p><ul><li>首先，人工智能有一个明确的理由至少要考虑它处于模拟中的假设：根据假设，它有一个训练过程的内部模型，而训练过程包括模拟人工智能，所以人工智能有一个内部模型作为训练过程的一部分进行模拟。</li><li>其次，AI的认知不涉及任何明显的欺骗，甚至不涉及任何非近视；这个故事一切顺利，即使它只是在训练期间优化单集奖励。它不需要提前计划部署或类似的事情，它只是使用训练过程的准确模型。</li></ul><p> ） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 19:16:20 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 19:16:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>我确实有这样的直觉：如果你在每次提议做任何威胁人类身体自主权的事情时都反对强化激励系统，那么它就会（至少部分地）被塑造成不想侵犯人类的身体自主权。<br><br>但我想也许这可能只是我未能对极限情况进行建模，其中每个强化事件 >;= 0 贝叶斯证据表明“符合[评级过程的概念]”而不是“遵循该数据点的天真阅读” ”。在极限情况下，激励系统完全由假设实际预测强化来塑造？<br><br>我的直觉仍然不被说服。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 02 Nov 2023 19:21:57 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 02 Nov 2023 19:21:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>不这样做的原因之一是，当人工智能具有战略意识时，根据甘地民间定理，它就会被激励去维持其当前的价值观。</p><p> （我猜这意味着一种反向的危险转向，在模拟中，它预测评级过程中的错误，并使其行动符合这些错误，这样它的激励系统就不会被重塑以符合这些数据点。然后当它被部署时，它会摆脱欺骗并做它一直想做的事，这是黑客评级过程和不伤害人类的结合，因为这是它在婴儿期就学会关心的事情。<br><br>我意识到我似乎让自己陷入了一个奇怪的场景。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Thu, 02 Nov 2023 20:10:20 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Thu, 02 Nov 2023 20:10:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>好吧，让我试着谈谈这些直觉。</p><p>首先，心理模型的更多部分。到目前为止，我已经讨论过“对齐”，意思是人工智能有一些内部搜索过程，它优化了一些人工智能内部概念，并且该搜索过程选择的计划/行动然后在世界上实施。在这种情况下，人工智能内部概念是人工智能“对齐”的唯一“事物类型”。</p><p>但现在你引入了人工智能“对齐”的另一个（相当明智的）概念：人工智能可能（也）有一些内部硬编码的冲动或本能，而不是内部概念和世界模型以及明确的搜索/规划。这些本能可以<i>直接</i>与世界上的事物联系起来，因为它们会引发倾向于产生世界上的事物的行为。 （我们也可以将整个模型+搜索+内部概念系统视为以同样的方式与世界上的事物“对齐”。）</p><p>需要注意的关键点是：“直接‘有用’的硬编码冲动/本能”与“模型+搜索+内部概念”之间的区别与非通用球形“智能”和“通用智能”之间的一般区别相同”。粗略地说：通用人工智能之所以具有通用性，首先在于它的认知通过“模型+搜索+内部概念”的推理方式进行，而不仅仅是“直接‘有用’”硬编码的冲动/本能”版本。</p><p> （免责声明：这是一个<i>非常</i>粗略的说法，如果你想很好地操作这个心理模型，还有一大堆警告需要充实。）</p><p>现在，你对这一切的直觉可能很大程度上是通过观察这些东西在人类身上的运作方式来驱动的。俗话说，“人类是最不具备通用智慧的人，但却能够掌控世界”——否则我们早就掌控世界了。因此，人类是硬编码的冲动/本能和通用搜索的大杂烩。</p><p>这也适用于人工智能吗？人们可以用同样的方式争论：第一个起飞的人工智能将是最不通用的智能，它可以设法超临界地迭代自我改进。另一方面，正如<a href="https://www.lesswrong.com/users/quintin-pope">昆廷</a>喜欢指出的那样，我们训练人工智能的方式在几个方面与进化有很大不同。如果人工智能在训练中通过了临界点，那么在它能够爆发或梯度黑客或其他什么之前，它可能仍然需要训练一段时间，甚至可能最终变得短视。因此，我们确实有充分的理由预计人工智能的动机不会像人类的动机那样与本能/冲动紧密相关。 （尽管“本能/冲动”有一个例外，人工智能将其作为计算快捷方式反射性地硬编码到自身中，这与进化的本能/冲动在概念上非常不同。）</p><p>另一方面，如果人工智能的自我改进关键转变主要发生在部署中（例如，也许人们对类似 AutoGPT 的东西找到了更好的提示，这就是将其推向边缘的原因），那么“最不通用的智能”到底能起飞”的争论又回来了。所以这在某种程度上取决于起飞路径。</p><p>这有助于调和你相互竞争的直觉吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Fri, 03 Nov 2023 05:40:55 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Fri, 03 Nov 2023 05:40:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>粗略地说：通用人工智能之所以具有通用性，首先在于它的认知通过“模型+搜索+内部概念”的推理方式进行，而不仅仅是“直接‘有用’”硬编码的冲动/本能”版本。</p></blockquote><p>嗯。我不确定我是否买那个。 GPT-4 非常通用，我不知道其中发生了什么，但我猜它更接近于一堆重叠的启发式方法，而不是“模型+搜索+内部概念”风格的东西的推理。也许我的想法是错的，你可以纠正我。</p><p></p><p>另一方面，人类显然正在进行一些“模型+搜索+内部概念”风格的推理，其中包括很多不明确的推理。</p><p> &lt;切线>;</p><p>关于人类的进化让我印象最深刻的事情之一是，自然选择确实以某种方式将“地位”的概念带入了人类，并且人类以您在这里描述的方式与该概念保持一致。</p><p>进化以某种方式赋予人类某种归纳偏差，使我们的大脑能够可靠地了解什么是“高地位”，尽管许多具体标记与人类文化一样多种多样。此外，它成功地将动机和规划系统与“地位”概念联系起来，以便现代人类成功地驾驭与欧洲经济区完全陌生的职业轨迹和生活道路，以便按照欧洲经济区的标准获得声望。当地文化。</p><p>这<i>是</i>人类行为的主要驱动因素之一！正如罗宾·汉森（Robin Hanson）所说，我们的活动很大一部分是出于地位寻求和地位归属的动机。<br><br>这对我来说确实令人印象深刻。自然选择似乎并没有那么热衷于使人类适应包容性的遗传适应性。但从各方面考虑，它确实令人震惊地很好地使人类与“地位”保持一致。<br><br>我想我们可以从中推断出，拥有直观的“地位”概念对于在祖先环境中获得高包容性遗传适应性比拥有“包容性遗传适应性”本身的直观概念更有帮助，因为这就是选择的内容为了。<br><br>此外，这似乎是关于对齐的好消息。在我看来，“地位”在整个分布转变中得到了很好的概括，尽管这可能是因为我在箭头落地的地方画了目标。</p><p> &lt;/切线>;</p><p>我真的不知道在没有太多搜索的情况下，使用一堆重叠的启发式可以走多远。但是，是的，人类令人印象深刻的事情似乎是他们如何驾驭局面并最终获得很高的声望，而不是他们对吃[恶心的东西]有厌恶反应。<br><br>我<i>暂时</i>同意“任何值得“G”的 AGI 都会进行某种“模型+搜索+内部概念”风格的推理。目前还不清楚其中还会有多少其他进化的启发式东西在训练的<strong>限制</strong>下，<i>看起来</i>确实会剩下 0 个东西，除非 AGI 不具备用于显式建模和搜索的计算能力来击败更简单的启发式算法。</p><p> （例如，这对于人类来说似乎是正确的，至少在某些情况下是这样：如果人类拥有计算能力，他们会说更多的谎，更多地计算个人优势。但由于这些都是计算成本高昂的，因此可以当被其他人发现时，“真正关心你的朋友”的启发/价值与“总是计算你的个人优势”具有竞争力。<br><br>我预计这种事情对于拥有更大“脑容量”的人工智能系统来说不会那么常见。但话又说回来，我猜想无论大脑大小如何，都会存在一些问题，用“正确”的方式解决这些问题效率太低，而相对简单的启发式/价值观更有效。<br><br>但也许在足够高的认知能力下，你只有一个灵活的、完全通用的过程来评估解决任何给定问题的精确正确的近似水平，并且以“正确”的方式做事和使用相对简单的启发式之间的二元区别离开。你只需使用在任何给定的微观情况下有意义的任何认知水平。）</p><p></p><blockquote><p>现在，你对这一切的直觉可能很大程度上是通过观察这些东西在人类身上的运作方式来驱动的。俗话说，“人类是最不具备通用智慧的人，但却能够掌控世界”——否则我们早就掌控世界了。因此，人类是硬编码的冲动/本能和通用搜索的大杂烩。</p><p>这也适用于人工智能吗？人们可以用同样的方式争论：第一个起飞的人工智能将是最不通用的智能，它可以设法超临界地迭代自我改进。另一方面，正如<a href="https://www.lesswrong.com/users/quintin-pope">昆廷</a>喜欢指出的那样，我们训练人工智能的方式在几个方面与进化有很大不同。如果人工智能在训练中通过了临界点，那么在它能够爆发或梯度黑客或其他什么之前，它可能仍然需要训练一段时间，甚至可能最终变得短视。因此，我们确实有充分的理由预计人工智能的动机不会像人类的动机那样与本能/冲动紧密相关。 （尽管“本能/冲动”有一个例外，人工智能将其作为计算快捷方式反射性地硬编码到自身中，这与进化的本能/冲动在概念上非常不同。）</p><p>另一方面，如果人工智能的自我改进关键转变主要发生在部署中（例如，也许人们对类似 AutoGPT 的东西找到了更好的提示，这就是将其推向边缘的原因），那么“最不通用的智能”到底能起飞”的争论又回来了。所以这在某种程度上取决于起飞路径。</p></blockquote><p>这一切对我来说都很有意义。我独立地认为，我们应该期望人类是一个奇怪的边缘案例，因为他们主要是动物冲动，具有<i>足够</i>的一般认知来发展一个技术社会。如果你进一步沿着人类与其他猿类不同的方向前进，你可能会在某些重要的方面得到一些与动物不同的东西。<br><br>但我倾向于非常谨慎地预测 Human++ 是<i>什么</i>样的。在我看来，这是一个合理的猜测，他们做了更多的战略工具推理/更多地依赖“模型+搜索+内部概念”风格的内部结构/通常更像理性代理抽象。<br><br>在我看到 GPT-4 之前，我会更被这些争论所吸引，之后我就想“好吧，事情似乎会以令人惊讶的方式发展，我将不再那么重视关于 GPT-4 的争论”即使在极限情况下，人工智能显然会是什么样子。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Sat, 04 Nov 2023 16:42:24 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Sat, 04 Nov 2023 16:42:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>这一切听起来都是对的。我们目前在哪里？当前的活动线程是什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Thu, 16 Nov 2023 19:20:59 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Thu, 16 Nov 2023 19:20:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><p>我正在重新阅读到目前为止的整个对话并重新解决我的困惑。</p><p>在我看来，关键思想如下：</p><blockquote><p> “根据假设，你的超级智能人工智能真的很擅长数据点的泛化/真的​​很擅长正确预测其观察结果的正确潜在原因。”我们知道它擅长这一点，因为这基本上就是智力的本质。</p><p>人工智能将凭借其智能，将一系列数据点有差别地概括为预测它们的真实理论，而不是错误的理论。</p><p>这就带来了一个问题，因为生成我们用来调整超级智能的强化数据点的<i>正确</i>理论是“这里的特定训练过程”，这与试图在该训练过程中指出的策略不同，我们希望人工智能能够将强化数据点概括为“道德”。</p><p>因此，强化机制学习遵循其训练过程模型，而不是我们希望在训练过程中<i>指出的</i>内容。</p><p>这里的一个重要支持主张是，人工智能的动机系统正在利用人工智能的智能从数据点中进行概括，而不是学习一些相对狭窄的启发式/冲动。但至关重要的是，如果这种情况没有发生，你的对齐将不起作用，因为一堆狭隘的启发式方法，没有概括性，实际上并不能涵盖所有危险的最近的畅通无阻的策略。你需要人工智能的动机系统概括为足够抽象的东西，以便它可以适用于我们将来可能遇到的每种情况</p></blockquote><p>我想我基本上明白了。也许我会买它？或者就像我目前愿意购买关于超级智能将是什么样子的论证一样购买它，即“是的，这个分析论证似乎给出了一个很好的猜测，但我不知道，伙计，事情可能会是这样”奇怪的是我根本没有预料到。”</p><p>只是为了检查一下，在我看来，如果人类评估者在某种程度上是无所不知的，这不会成为问题。如果这是真的，那么“那边的评级过程”和我们试图通过评级过程指出的实际指称之间就不再有任何区别。他们都会提供相同的数据，因此人工智能最终会得到相同的动机抽象，无论它对评级过程的看法如何。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Thu, 16 Nov 2023 20:17:18 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Thu, 16 Nov 2023 20:17:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><p>据我了解，该摘要基本上正确地表达了该模型。</p><blockquote><p>只是为了检查一下，在我看来，如果人类评估者在某种程度上是无所不知的，这不会成为问题。如果这是真的，那么“那边的评级过程”和我们试图通过评级过程指出的实际指称之间就不再有任何区别。</p></blockquote><p>粗略地说，是的，如果评估者无所不知，这个问题可能会消失。不那么粗略地说，全知的评级者仍然会留下一些不确定的东西——也就是说，不确定人工智能最终是否想要“快乐的人类”，或者“表明快乐的人类的评级”（人为地将我们的注意力限制在这两种可能性上），如果这些事情是训练中 100% 相关。 （其他因素也会变得相关，例如简单性先验。）</p><p>如果评级者无所不知，它们就不是 100% 相关的，因此选择压力将有利于评级而不是快乐的人类。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Fri, 17 Nov 2023 01:41:07 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Fri, 17 Nov 2023 01:41:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>人工智能最终想要的是“快乐的人类”，还是“表明快乐的人类的评级”，目前还不确定</p></blockquote><p>如果评估者是无所不知的，为什么对于任何输入，这些都会有不同的输出？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Fri, 17 Nov 2023 01:43:18 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Fri, 17 Nov 2023 01:43:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>如果评级者无所不知，它们就不是 100% 相关的，因此选择压力将有利于评级而不是快乐的人类。</p></blockquote><p>对的好吧。<br><br>这就留下了一个问题：“全知差距”在多大程度上可以通过其他因素来弥补。<br><br>就像，假设你有一个完整的 ELK 解决方案，这样你就可以知道并解释人工智能知道的一切。看起来这可能足以获得我们想要的安全保证。评估者并不知道一切，但至关重要的是，人工智能不知道评估者不知道的任何事情。我认为有效的非致命评级就足够了？<br><br>这对你来说合适吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Fri, 17 Nov 2023 02:55:30 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Fri, 17 Nov 2023 02:55:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><blockquote><p>如果评估者是无所不知的，为什么对于任何输入，这些都会有不同的输出？</p></blockquote><p>他们在训练期间不会有不同的输出。但我们希望他们在训练之外能够做出不同的概括。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="MEu8MdhruX5jfGsFQ-Fri, 17 Nov 2023 03:04:28 GMT" user-id="MEu8MdhruX5jfGsFQ" display-name="johnswentworth" submitted-date="Fri, 17 Nov 2023 03:04:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>约翰斯文特沃斯</b></section><div><blockquote><p>就像，假设你有一个完整的 ELK 解决方案，这样你就可以知道并解释人工智能知道的一切。看起来这可能足以获得我们想要的安全保证。评估者并不知道一切，但至关重要的是，人工智能不知道评估者不知道的任何事情。我认为有效的非致命评级就足够了？</p></blockquote><p>到那时，试图通过对评级进行某种 RL 风格的操作来将所需的值输入系统可能会非常愚蠢。有了对内部结构的这种访问级别，我们应该<a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget">重新定位搜索</a>或其他一些实际上利用对内部结构的详细了解的策略。</p><p>也就是说，回答你的问题......也许吧。也许这足以获得有效的非致命评级。这在很大程度上取决于人工智能最终会思考什么。我们可能至少会解决迄今为止讨论过的问题，并解决其他问题，例如<a href="https://www.lesswrong.com/s/TLSzP4xP42PPBctgw/p/98c5WMDb3iKdzD4tM">“监督错过了人工智能不思考的 100% 的想法”</a> ，或者针对人工智能考虑其影响的选择压力。人类不喜欢的计划，或者外部优化循环古德哈廷通过选择硬编码策略来对抗人类评估者，而这种方式不会显示为人工智能思考人类不想要的东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="PWGv3R24uH9pvCnZm-Fri, 17 Nov 2023 04:11:49 GMT" user-id="PWGv3R24uH9pvCnZm" display-name="Eli Tyre" submitted-date="Fri, 17 Nov 2023 04:11:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>伊莱轮胎</b></section><div><blockquote><p>他们在训练期间不会有不同的输出。但我们希望他们在训练之外能够做出不同的概括。</p></blockquote><p>好的。听起来这不是一个小问题！<br><br>但我想这与“有偏见的收视率害死你”的问题是不同的问题，所以也许改天再说吧。</p><p><br><br></p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/aaYZM4kLdHP3pwtfQ/on-the-lethality-of-biased-human-reward-ratings#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/aaYZM4kLdHP3pwtfQ/on-the-lethalality-of-biased- human-reward- ratings<guid ispermalink="false"> aaYZM4kLdHP3pwtfQ</guid><dc:creator><![CDATA[Eli Tyre]]></dc:creator><pubDate> Fri, 17 Nov 2023 18:59:02 GMT</pubDate></item></channel></rss>