<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 5 日，星期二 12:22:52 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[A Socratic dialogue with my student]]></title><description><![CDATA[Published on December 5, 2023 9:31 AM GMT<br/><br/><p>这是我和我的学生诺姆之间的对话。经他许可，以编辑形式转载。评论时，请考虑他是一个青少年。其中许多想法对他来说都是<a href="https://xkcd.com/1053/">新的</a>。</p><p>怎样才能招到学生呢？你偷了它们。他以前的老师是一位马克思主义者。我在辩论中彻底摧毁了他以前的老师，以至于他放弃了她的教诲，现在转而听我的。</p><p>我认为这段对话展示了良好的教学技巧。</p><ul><li>我让诺姆来判断什么是合理的、什么是有道理的、什么是“证据”。在诺姆出生之前，我参加了我的第一次辩论比赛。这个障碍稍微缩小了差距。</li><li>我会问一系列问题，而不只是说“ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>是真的”。这使得<a href="https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password">密码猜测</a>变得不可能。他是在下棋，而不是<i>危险边缘！</i></li><li>我避免告诉诺姆我的信仰，除非他明确询问。这对诺姆来说更有趣，因为没有人喜欢未经请求的讲道。这也更有说服力，因为结论感觉像是他的结论。</li><li>当诺姆改变话题时，我立即退缩了。</li></ul><p><strong>诺姆：</strong>我知道你反对免除学生贷款债务。你能告诉我为什么吗？我这样做是为了一场演讲和辩论比赛。</p><p> <strong>Lsusr：</strong>你<a href="https://www.youtube.com/watch?v=o_wNNjfCG1E&amp;t=4s">以前不相信</a>支持救济的论点吗？当然，重复曾经说服你的论点并不困难。</p><p><strong>诺姆：</strong>我不知道我现在是否有足够的研究来与像你这样的人辩论。</p><p> <strong>Lsusr：</strong>你并不是想说服我。你正试图说服<a href="https://www.youtube.com/watch?v=xuaHRN7UhRo"><i>他们</i></a>。利用他们的偏见、非理性、部落主义和无知。</p><p><strong>诺姆：</strong>我还必须安抚评委们。</p><p> <strong>Lsusr：</strong>我就是这么说的。</p><hr><p><strong>诺姆：</strong>我正在努力寻找一个关于学生贷款减免的好论据。</p><p> <strong>Lsusr：</strong>但是你以前不是很赞同吗？当然，你可以重复曾经说服你的糟糕论点。</p><p><strong>诺姆：</strong>这些都是道德争论，没有任何经济理解。</p><p> <strong>Lsusr：</strong>没关系。你的听众可能是经济文盲。</p><p><strong>诺姆：</strong>不知何故，我认为我们作为赞成免除所有学生贷款债务的一方赢得了一次胜利。</p><p> <strong>Lsusr：</strong>干得好。</p><p><strong>诺姆：</strong>谢谢。</p><hr><p> <strong>Lsusr：</strong>你听说过“有效利他主义”吗？您可能会喜欢他们推出的一些东西。它往往具有道德一致性和经济素养（与主要的民主共和党、社会主义等政治纲领不同）。</p><p><strong>诺姆：</strong>不，但我会调查一下。</p><p> <strong>Lsusr：</strong>你可能不同意。但我预计它的智力稳健性会让你耳目一新。</p><hr><p><strong>诺姆：</strong>这是否意味着我自杀然后将我所有的器官捐献给需要它们的人是道德的？我想，除非我能在不自杀的情况下拯救更多的生命。也许更好的论点是自杀，让某人卖掉我所有的身体部位，然后用这笔钱购买疟疾网，送给生活在非洲的人们。</p><p> <strong>Lsusr：</strong>你可以在不自杀的情况下拯救更多生命。而且，我想不出有哪个 EA 曾为此案自杀过。</p><p><strong>诺姆：</strong>可能是因为我们直觉上认为自杀是错误的。</p><p> <strong>Lsusr：</strong>不要因为肾脏的事情而分心。基本思想如下：</p><ul><li>美国政府需要花费 10,000,000 美元才能挽救一个美国人的生命。</li><li>在非洲，通过公共卫生措施挽救一条生命需要 5,000 美元。</li></ul><p>这就是我上个月为非洲公共卫生措施捐赠 20 美元的原因。它的作用相当于美国联邦政府花费的 40,000 美元。</p><p><strong>诺姆：</strong>是的，确实如此。在美国靠什么拯救生命？</p><p> <strong>Lsusr：</strong>基本的想法是你应该计算数字。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=dNgp-s8IvRs"><div><iframe src="https://www.youtube.com/embed/dNgp-s8IvRs" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p><strong>诺姆：</strong>我认为这对钱有用，但我不知道它是否可以完全应用于所有事情。</p><p> <strong>Lsusr：</strong>为什么不呢？具体例子。</p><p><strong>诺姆：</strong>嗯，这取决于你是否认为人类应该拥有受保护的权利。</p><p> <strong>Lsusr：</strong>这不是一个具体的例子。您的主张可能适用于什么现实世界的决定？请明确点。</p><p><strong>诺姆：</strong>一位医生有5名病人需要器官移植，否则他们就会死。有一名完全健康的人因接受小手术而处于麻醉状态。如果我们仔细计算一下数字，医生应该杀死那个人才能挽救五个人的生命。如果你认为人类有权利，那就是不道德的。如果你认为人类不会，那就不会了。</p><p> <strong>Lsusr：</strong>正确。这显然是不道德的。但人权并不是医生不应该谋杀病人的唯一原因。你能想到一种实用主义的吗？</p><p><strong>诺姆：</strong>医生会失去执照，然后他们就会失业。</p><p> <strong>Lsusr：</strong>如果没有许可证要求怎么办？比如在战区。</p><p><strong>诺姆：</strong>患者将来也许能够挽救生命。</p><p> <strong>Lsusr：</strong> 5 个器官接受者也可以。另一个原因。</p><p><strong>诺姆：</strong>我不确定。</p><p> <strong>Lsusr：</strong>没有人会去看他们认为会谋杀他们的医生。</p><p><strong>诺姆：</strong>如果是战区，那么可能没有其他选择。</p><p> <strong>Lsusr：</strong>公平。您熟悉“义务论伦理学”这个词吗？</p><p><strong>诺姆：</strong>是的。任何有最好意图的事情都是如此。那是对的吗？我可能已经忘记了。</p><p> <strong>Lsusr：</strong>没有。这不是最好的意图。</p><p><strong>诺姆：</strong>好的。之后怎么样了？</p><p> <strong>Lsusr：</strong>义务论伦理遵循“不要杀死你的病人”这样的良好规则。 EA 相信要处理数字，但它们通常不会违反义务论限制。当我捐20美元时，我捐的是我自己的钱。我没有偷它。</p><p><strong>诺姆：</strong>好的。让我想想我是否发现任何缺陷。</p><p> <strong>Lsusr：</strong>慢慢来。</p><p><strong>诺姆：</strong>如果你遵循我认为好的规则，并且你帮助了最多的人，那么我不可能反对。</p><p> <strong>Lsusr：</strong>那是 EA。但不仅仅是人。他们的素食主义者数量远远超过了应有的比例。</p><p><strong>诺姆：</strong>好的。稍微不相关的问题：您对素食主义者有何看法？</p><p> <strong>Lsusr：</strong>我已经几个月没吃肉了。</p><p><strong>诺姆：</strong>这对你来说是环境问题还是对杀害动物的道德反对？还是健康？</p><p> <strong>Lsusr：</strong>对我的健康可能会产生负面影响。环境影响对我来说无关紧要。我不在乎杀死动物。如果你能找到一个符合道德来源的汉堡，那么我很乐意吃它。问题是，我们的动物产品默认来自工厂化农场，那是人间地狱。 [更正：我在家人的感恩节晚餐上吃了一些肉汁。]</p><p><strong>诺姆：</strong>这对我来说很有趣，我不一定不同意你的推理。</p><p> <strong>Lsusr：</strong>我尽量不将我的信仰和价值观强加给别人。这就是为什么直到你问我才提到这一点。</p><p><strong>诺姆：</strong>如果你说折磨动物是错误的，那么杀死动物不也是不道德的吗？</p><p> <strong>Lsusr：</strong>动物干净的死亡几乎没有什么痛苦，特别是与漫长而美好的生活相比。我正在努力减少痛苦，同时遵守义务论限制。</p><p><strong>诺姆：</strong>你认为道德考虑的重要性应该与事物的先进程度成正比吗？抱歉，如果我措辞不好。现在已经是深夜了，我正在等待电源恢复，这样我就可以做剩下的作业了。</p><p> <strong>Lsusr：</strong>我知道你的意思。答案=是。如果我必须在拯救两只牛和一个人之间做出选择，那么人类是显而易见的选择。</p><p><strong>诺姆：</strong>好的。我同意你的看法。</p><hr><p> <strong>Lsusr：</strong>辩论进行得怎么样？</p><p><strong>诺姆：</strong>你的假设是非常正确的，即法官们都是经济文盲。</p><p> <strong>lsusr：</strong>哈哈哈哈哈哈</p><hr><p><strong>Lsusr：</strong>你喜欢吗？我觉得你很喜欢辩论比赛。干得好，能够与校队的孩子们竞争。</p><p><strong>诺姆：</strong>是的。非常有趣。</p><p> <strong>Lsusr：</strong>我也喜欢高中辩论。我做了3-4年。</p><p><strong>诺姆：</strong>尝试捍卫错误的立场很有趣，因为这很困难。</p><p> <strong>Lsusr：</strong>如果你的信念是错误的（所以你认为这是正确的立场）怎么办？那很难吗？</p><p><strong>诺姆：</strong>我知道不是在这个问题上，因为正如你所说，这就像争论天空是否是蓝色的。但对于其他一些诸如“美国应该在&lt;地方>;部署更多军队”之类的问题，很难看出什么是正确的。</p><p> <strong>Lsusr：</strong>出去吧。直视。准确地告诉我你看到的是什么颜色。</p><p><strong>诺姆：</strong>现在是黑色的。</p><p> <strong>Lsusr：</strong>通常，辩论比赛决议的定义（故意）如此模糊，以至于它们可能是正确的，也可能是错误的，这取决于它们的解释方式。</p><p><strong>诺姆：</strong>据我所知，无论你如何解释，这个都是错误的。我认为“全部”这个词几乎让人无法辩护。</p><blockquote><p>美国联邦政府应免除所有联邦学生贷款债务。</p></blockquote><p> <strong>Lsusr：</strong>假设民主国家的每个人（错误地）都支持学生贷款减免。民选政府是否应该尊重人民的意愿？</p><p><strong>诺姆：</strong>是的，因为如果他们不这样做，就会开创一个不服从人民的危险先例。</p><p> <strong>Lsusr：</strong>那么你所要做的就是表明绝大多数美国选民支持贷款减免。</p><p><strong>诺姆：</strong>比起 3.4% 的通胀率，我更担心这种影响。</p><p> <strong>Lsusr：</strong>别担心。绝大多数美国选民支持愚蠢得多的政策。美国的通胀率应该是多少？</p><p><strong>诺姆：</strong>我的直觉是 0%，但有一些我不知道的经济小事表明一个国家应该有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>程度的通货膨胀。</p><p> <strong>Lsusr：</strong>这是我制作的一个长达一小时的 YouTube 视频，试图传达这个问题的复杂性。 【我就是右边那个戴面具的人。】 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=fdlFHnxDE94"><div><iframe src="https://www.youtube.com/embed/fdlFHnxDE94" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>为什么它应该为零？</p><p><strong>诺姆：</strong>我认为，我在经济知识方面的差距正在显现，但当一种货币尽可能值钱时，这不是很好吗？而且，我的力量现在又恢复了。所以我要做作业然后去睡觉。</p><p> <strong>Lsusr：</strong>如果你希望货币尽可能值钱，那么我们应该有负通胀率。晚安！保证充足的睡眠。</p><p><strong>诺姆：</strong>噢，你说得对。我把这归咎于“凌晨2点”。</p><p> <strong>Lsusr：</strong>不。你的问题并不愚蠢。这很难。</p><p><strong>诺姆：</strong>我想我应该记住数字可能会下降。</p><p> <strong>lsusr：</strong> 🤑</p><br/><br/> <a href="https://www.lesswrong.com/posts/TtdJt78mgGiDubnAM/a-socratic-dialogue-with-my-student#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TtdJt78mgGiDubnAM/a-socratic-dialogue-with-my-student<guid ispermalink="false"> TtdJt78mgGiDubnAM</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Tue, 05 Dec 2023 09:31:05 GMT</pubDate> </item><item><title><![CDATA[Neural uncertainty estimation for alignment]]></title><description><![CDATA[Published on December 5, 2023 8:01 AM GMT<br/><br/><h2>介绍</h2><p>假设您已经构建了一些人类价值观的人工智能模型。你输入一个情况，它就会给出一个良好度评级。您可能想问：“此优度评级的误差线是多少？”除了了解误差线之外，不确定性估计在人工智能内部也很有用：指导主动学习<span class="footnote-reference" role="doc-noteref" id="fnref80ywo0pbl8r"><sup><a href="#fn80ywo0pbl8r">[1]</a></sup></span> 、纠正<a href="https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it">优化器的诅咒</a><span class="footnote-reference" role="doc-noteref" id="fnrefq23duy4ut0g"><sup><a href="#fnq23duy4ut0g">[2]</a></sup></span>或进行分布外检测<span class="footnote-reference" role="doc-noteref" id="fnref3dij2j9svj8"><sup><a href="#fn3dij2j9svj8">[3]</a></sup></span> 。</p><p>我最近出于一个<a href="https://www.lesswrong.com/s/aJvgWxkCBWpHpXti4/p/nA3n2vfCy3ffnjapw">喜欢的原因</a>进入了神经网络（NN）的不确定性估计文献：我认为这对于量化人工智能潜在特征的有效性域的对齐很有用。如果我们<a href="https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget">将人工智能指向其世界模型中的某个概念</a>，那么对该概念的实现进行优化可能会因为将该概念推到其有效范围之外而出错。</p><p>但现在就先把对齐的想法放在你的后口袋里吧。这篇文章主要是对不确定性估计文献的调查，其中夹杂着我自己的看法。</p><p></p><h2>贝叶斯神经网络图片</h2><p>贝叶斯神经网络图是几乎所有神经网络不确定性估计方法的鼻祖，因此从这里开始是合适的。</p><p>图片很简单。您从参数的先验分布开始。您的训练数据就是证据，在对其进行训练后，您将获得更新的参数分布。给定输入，您可以通过贝叶斯神经网络传播输入来计算输出的分布。</p><p>这一切都是非常正确且无关紧要的（“<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{trillion}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">当然</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">让</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">我</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">更新</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">我</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">万亿</span></span></span></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>模型所有参数的维度联合分布”），但<i>实际上训练神经网络确实是这样工作的</i>。<i> </i>如果您使用对数似然损失和 L2 正则化，并且您的参数先验是高斯分布，则最小化损失的参数将位于贝叶斯神经网络所具有的分布的峰值<span class="footnote-reference" role="doc-noteref" id="fnreffo4svcpvxs"><sup><a href="#fnfo4svcpvxs">[4]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefcogdul3x2xj"><sup><a href="#fncogdul3x2xj">[5]</a></sup></span> 。</p><p>这是因为损失景观和参数不确定性之间存在桥梁。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{P}(parameters \;|\; dataset) ="><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">贝叶斯</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">规则</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">表示</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">集</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">⋅</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{P}(parameters) \cdot \text{P}(dataset \;|\; parameters) / \text{P}(dataset)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">集</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">）</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">集</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">）</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">。</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{P}(parameters \;|\; dataset)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">这里</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">集</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">要</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">估计</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">后</span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{P}(parameters) \cdot \text{P}(dataset \;|\; parameters)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">验</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">分布</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">⋅</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">集</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">损失</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">指数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">6</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">]</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span></span></span></span></span></span> <span class="footnote-reference" role="doc-noteref" id="fnrefamk58pvab4"><sup><a href="#fnamk58pvab4">_</a></sup></span>这适用于物理隐喻，例如“参数的分布是位于损失盆地底部的玻尔兹曼分布”。</p><p>根据经验，通过假装遵循贝叶斯神经网络图来计算神经网络的不确定性效果非常好，以至于一篇关于集成方法的好论文<span class="footnote-reference" role="doc-noteref" id="fnreflpb1b2qcoen"><sup><a href="#fnlpb1b2qcoen">[7]</a></sup></span>将其称为“基本事实”。当然，要在这里实际计算任何东西，你必须进行近似，如果你进行快速而肮脏的近似（例如假装你可以从 Hessian 找到损失盆地的形状），你会得到不好的结果<span class="footnote-reference" role="doc-noteref" id="fnrefd9jz9mfml"><sup><a href="#fnd9jz9mfml">[8]</a></sup></span> ，但人们正在做这些天，蒙特卡罗方法变得很聪明<span class="footnote-reference" role="doc-noteref" id="fnrefbct5kii2m07"><sup><a href="#fnbct5kii2m07">[9]</a></sup></span> ，他们发现贝叶斯神经网络计算的更好近似可以得到更好的结果。</p><p>但对损失景观进行蒙特卡罗遍历的成本很高。对于大规模应用的技术，它必须只对运行模型的成本施加很小的乘数，并且如果您希望它变得普遍，那么它施加的成本必须非常小。</p><p></p><h2>合奏团</h2><p>解决不确定性的一种完全不同的方法是集成<span class="footnote-reference" role="doc-noteref" id="fnrefen0yeoqzg0k"><sup><a href="#fnen0yeoqzg0k">[10]</a></sup></span> 。只需训练十几个模型，询问他们的建议，并估计传播的不确定性。所有事情的数十倍成本乘数都很陡峭，但如果您经常查询模型，它比损失情况的蒙特卡洛估计便宜。</p><p>集成在理论上很简单。您不需要假装模型经过训练可以收敛，不需要专门针对预测损失进行训练，甚至不需要固定的架构。您只需选择一些想要分散不确定性的模型分布并进行采样。</p><p>你可以用合奏做一些聪明的事情。通过计算集成如何适应贝叶斯神经网络图片，您会了解到改变正则化的零点可能是个好主意，否则您将在模型的泛化方式中得到虚假相关性<span class="footnote-reference" role="doc-noteref" id="fnreflpb1b2qcoen"><sup><a href="#fnlpb1b2qcoen">[7]</a></sup></span> 。您可以拆分数据集并训练单独的较小模型，然后巧妙地聚合这些模型（类似于<a href="https://www.kaggle.com/code/prashant111/bagging-vs-boosting">装袋和提升</a>），以降低集成的计算溢价<span class="footnote-reference" role="doc-noteref" id="fnrefen0yeoqzg0k"><sup><a href="#fnen0yeoqzg0k">[10]</a></sup></span> 。这些论文中暗示，聪明的集成技巧在更大的尺度上并不那么重要，但目前还不清楚收益是否完全为零。</p><p>集成的一个棘手部分是，如果一枚硬币正面朝上的概率为 51%，并且您已训练神经网络以获得正确答案，那么集成中的每个成员都会预测正面。正确答案并不不确定，因此您的团队表示不存在不确定性。如果您希望不确定性度量包含环境中的熵，则必须训练神经网络来估计该熵，这在很大程度上放弃了使用非预测损失的自由。</p><p>解释时的类似关注适用于在模型的潜在特征上使用集成，尽管我在文献中没有看到人们这样做。假设您训练了十几个模型，并用有关狗的数据探测它们，为每个模型找到一个“狗向量”。您可以对它们的大小和方差进行标准化，然后使用集合的方差作为“狗向量的不确定性”。这并不是关于狗的完全不确定性，因为它没有衡量人工智能模型中关于狗的不确定性，这只是不同模型根据特定探测方法的内部表示的传播。</p><p></p><h2>关于任意不确定性的注释</h2><p>文献中很大一部分文字是关于任意不确定性和认知不确定性之间的区别<span class="footnote-reference" role="doc-noteref" id="fnrefxqxuuw1xe2"><sup><a href="#fnxqxuuw1xe2">[11]</a></sup></span> 。当我说集成会告诉你由于建模选择而导致的输出的不确定性时，这就是我必须讨论的区别，但不会告诉你人工智能内部对环境的不确定性。在不确定性估计文献中，由于模型方差而产生的不确定性被称为“认知性”，而人工智能环境模型内部的不确定性被称为“随意性”。 <span class="footnote-reference" role="doc-noteref" id="fnref2jiolbj6tn8"><sup><a href="#fn2jiolbj6tn8">[12]</a></sup></span></p><p>在大数据限制下，认知不确定性相对于任意不确定性变得很小（除非你故意将模型推入高度认知不确定性的情况）。有些论文忘记了这一点，做了一些愚蠢的事情，使他们的认知不确定性估计更大，因为他们认为这应该是总的不确定性，这就是为什么其他论文必须用章节来讨论这种区别。</p><p></p><h2>添加噪声，由于某种原因通常会丢失</h2><p>如果您想要不确定性估计，您可以将随机噪声添加到神经网络的中间激活中。如果输出对噪声更敏感，则不确定性更大，如果输出不太敏感，则不确定性更小<span class="footnote-reference" role="doc-noteref" id="fnrefud7l8eu73jk"><sup><a href="#fnud7l8eu73jk">[13]</a></sup></span> 。有一些自然的启发式论证可以解释为什么这是有意义的<span class="footnote-reference" role="doc-noteref" id="fnreflgnhfp0m35e"><sup><a href="#fnlgnhfp0m35e">[14]</a></sup></span> ，并且通过更多的工作，您可以尝试将其与贝叶斯神经网络图和损失景观的蒙特卡洛估计联系起来。</p><p>或者，您可以忽略抽象参数并使用 dropout 作为随机噪声分布<span class="footnote-reference" role="doc-noteref" id="fnrefhfox6k7gfev"><sup><a href="#fnhfox6k7gfev">[15]</a></sup></span> 。</p><p>哦，当然，人们给出了理由，但我认为这里的第一印象是正确的，添加 dropout 然后采样在理论上是愚蠢的。但在实验上，它的效果很好，人们一直在谈论它<span class="footnote-reference" role="doc-noteref" id="fnref0hl862nst9f9"><sup><a href="#fn0hl862nst9f9">[16]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefu11lguv41wc"><sup><a href="#fnu11lguv41wc">[17]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefhzxdsrsco9g"><sup><a href="#fnhzxdsrsco9g">[18]</a></sup></span> ，而且它不需要你做任何额外的训练。</p><p>为什么它起作用的一个谜题可能是，在具有一些噪声样本的数据集上训练的网络将学会输出一个包罗万象的先验以响应噪声<span class="footnote-reference" role="doc-noteref" id="fnreffqljh5j8ipf"><sup><a href="#fnfqljh5j8ipf">[19]</a></sup></span> 。我怀疑 dropout 是足够大的噪音，它将网络推向这个先验，这有助于过度自信。</p><p>我希望人们对更小的、非丢失的噪声进行更多的比较<span class="footnote-reference" role="doc-noteref" id="fnrefvl2ahm3user"><sup><a href="#fnvl2ahm3user">[20]</a></sup></span> 。这在理论上似乎更合理，尽管当翻译回贝叶斯术语时，将噪声注入内部层似乎对应于有趣但不寻常的噪声分布。</p><p></p><h2>校准</h2><p>文献<span class="footnote-reference" role="doc-noteref" id="fnref0hl862nst9f9"><sup><a href="#fn0hl862nst9f9">[16]</a></sup></span>的很大一部分是人们只是试图获取神经网络的输出并对它们应用简单的函数来获得不确定性估计。我将简要地对待他们。</p><p>第 0 级只是按面值获取 NN 输出。当数据较多且参数不确定性较小时，神经网络的直接预测可以很好地估计不确定性。例如，基础 GPT-4 在分配给下一个标记的概率分布中得到了很好的校准，包括当这些下一个标记是以前从未见过的测试问题的答案时<span class="footnote-reference" role="doc-noteref" id="fnrefzkojoz799ka"><sup><a href="#fnzkojoz799ka">[21]</a></sup></span> 。即使是非分布，这也比什么都没有好——正如我上面提到的，如果神经网络的训练集包含意外数据，那么它们确实会学会不确定意外数据<span class="footnote-reference" role="doc-noteref" id="fnreffqljh5j8ipf"><sup><a href="#fnfqljh5j8ipf">[19]</a></sup></span> ，尽管它们仍然倾向于过度自信。</p><p>随着模型的泛化能力越来越强，我预计它们的输出能够在更大的领域得到很好的校准。相反，对于在有限数据上训练小型模型的应用程序，您将需要一种不同的方法来估计不确定性。</p><p>通常人们会尝试比第 0 级更奇特一些，并做一些事情，比如调整 softmax 函数的参数，以最大限度地提高对保留验证集的校准<span class="footnote-reference" role="doc-noteref" id="fnrefnglt70rybd9"><sup><a href="#fnnglt70rybd9">[22]</a></sup></span> 。这也很容易与其他方法结合作为最终校准步骤<span class="footnote-reference" role="doc-noteref" id="fnrefgx8tqfun0w"><sup><a href="#fngx8tqfun0w">[23]</a></sup></span> 。</p><p>如果您有分布外 (OOD) 数据样本，您还可以做更奇特的事情，例如同时进行 OOD 检测和校准<span class="footnote-reference" role="doc-noteref" id="fnreftmgq2x5m6t"><sup><a href="#fntmgq2x5m6t">[24]</a></sup></span> 。或者，如果您有 OOD 数据样本并且是频率论者，则可以进行保形预测<span class="footnote-reference" role="doc-noteref" id="fnrefg3rtbqm4247"><sup><a href="#fng3rtbqm4247">[25]</a></sup></span> 。</p><p>校准的一个卖点是您可以对任何经过训练的模型执行此操作。但如果您愿意放弃这一点并干预训练，就有一些方法可以改进模型的校准。这可能看起来像使用额外的正则化<span class="footnote-reference" role="doc-noteref" id="fnrefqbdlqupa3x"><sup><a href="#fnqbdlqupa3x">[26]</a></sup></span>或对比示例<span class="footnote-reference" role="doc-noteref" id="fnref2qfcvdqkx2y"><sup><a href="#fn2qfcvdqkx2y">[27]</a></sup></span>进行训练。这些也在一定程度上提高了 OOD 泛化能力，对抗性训练也是如此<span class="footnote-reference" role="doc-noteref" id="fnrefazba93s687"><sup><a href="#fnazba93s687">[28]</a></sup></span> 。</p><p>最终，网络输出的校准似乎并没有达到我想要的不确定性估计方法的效果。其一，它不会估计潜在特征的不确定性，而是与您拥有数据的输出的不确定性有关。</p><p>另一方面，它<a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">缺乏搜索的鲁棒性</a><span class="footnote-reference" role="doc-noteref" id="fnrefv1gf6chx43"><sup><a href="#fnv1gf6chx43">[29]</a></sup></span> 。确实，所有其他不确定性估计方法在优化时也应该容易受到对抗性示例的影响（部分原因是对抗性示例是特征，而不是错误<span class="footnote-reference" role="doc-noteref" id="fnrefa3kc8qjhdn6"><sup><a href="#fna3kc8qjhdn6">[30]</a></sup></span> ），但是当网络输出及其不确定性估计是相同的时，对良好输出的本地搜索应该<i>特别</i>有效地找到奇怪的意想不到的最佳值<span class="footnote-reference" role="doc-noteref" id="fnref28983dpfjdp"><sup><a href="#fn28983dpfjdp">[31]</a></sup></span> 。</p><p></p><h2>训练高阶模型</h2><p>校准的另一面。首先让你的神经网络为你提供更好的不确定性估计。</p><p>如果您想获得二阶不确定性的估计，请训练神经网络以输出<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Occurrence_and_applications">狄利克雷分布</a>的参数，而不是进行正常分类<span class="footnote-reference" role="doc-noteref" id="fnrefkm10b29by7c"><sup><a href="#fnkm10b29by7c">[32]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefhfc8x4hav0b"><sup><a href="#fnhfc8x4hav0b">[33]</a></sup></span> 。或者，如果您正在进行回归，请训练神经网络以输出答案的分布参数<span class="footnote-reference" role="doc-noteref" id="fnrefen0yeoqzg0k"><sup><a href="#fnen0yeoqzg0k">[10]</a></sup></span> 。或者，如果您拥有法学硕士并且每个问题都是钉子，请训练您的法学硕士用语言表达不确定性<span class="footnote-reference" role="doc-noteref" id="fnrefffdudph3qus"><sup><a href="#fnffdudph3qus">[34]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefadsp4hkxuk"><sup><a href="#fnadsp4hkxuk">[35]</a></sup></span> 。</p><p>表达不确定性的训练模型存在一个微妙的问题：不存在适当的二阶损失函数<span class="footnote-reference" role="doc-noteref" id="fnrefxuwhb9ieyh"><sup><a href="#fnxuwhb9ieyh">[36]</a></sup></span> 。这意味着仅从数据点出发，很难准确地训练模型来给出概率分布 - 您可以创建损失函数来尝试做到这一点，但只要您只监督正确答案是什么，而不监督正确答案是什么。正确的概率分布是，通过损失最小化得到的概率分布将会有偏差。</p><p>贝叶斯方法，或者至少是贝叶斯理论框架，在这里会很有用。您不需要适当的损失函数来进行贝叶斯更新。但还没有人写下该应用程序的贝叶斯神经网络图的类似物。</p><p>理论上也不清楚如何整合我们可以获得的有关概率分布的其他类型的监督数据。例如，人为噪声的例子可以给出直接的监督信号，表明正确的概率分布是高熵的。或者，我们可以通过询问“我期望我对正确答案的估计随着更多数据而改变多少？”来学习分布。它的监督分布在整个数据集中，因此绕过了没有适当评分规则的证明 - 但我们可以以公正的方式做到这一点吗？</p><p></p><h2>比较方法</h2><p>那么，哪个是最好的？</p><p>目前，它正在训练一个整体。但训练高阶模型具有尚未开发的潜力。</p><p>情况尚不清楚，因为比较是在玩具问题上进行的，文献很少并且并不总是重复，比较很困难，因为每种方法都有十几种变体，而且不同的论文有时会评估完全不同的指标。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/xptyd74vj7qmuirlgkdq" alt="该图显示了一些神经网络曲线与不同方法生成的误差条的拟合情况。" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/h6cj9a57suxcupp18t15 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/uqc5c2ci9zjyioeqilyd 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/dgoocd9ekrsj4v2ldc3f 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/pbdddhbajg3sxt9ixtsy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/bavrtb78i5icyf0kiyki 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/r4pe2oeov2q0fyqkckyj 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/uraeyjenw52atepowgt4 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/lsoqntf6avhbtk0qkyhp 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/dbifkkrzkczba42uqllp 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/dcy3kxiejmh9igglcn63 1603w"><figcaption>来自<a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#fnlpb1b2qcoen">参考文献。 7</a> .玩具曲线拟合问题的不确定性估计方法比较。<br><br> Ground Truth 是一个贝叶斯神经网络，Hamiltonian MC 是它的一个很好的蒙特卡洛近似，变分推理是它的一个廉价近似，MC Dropout 根据 dropout 后的方差估计不确定性，而我们的方法是一个具有奇特初始化的集成。</figcaption></figure><p>这个数字是一个不错的直觉泵。它在玩具曲线拟合任务（只有 6 个数据点的回归问题）上比较了不确定性估计方法（尽管它明显错过了校准和高阶建模），每种方法都使用 ReLU 与 Sigmoid 进行比较<span class="footnote-reference" role="doc-noteref" id="fnreflpb1b2qcoen"><sup><a href="#fnlpb1b2qcoen">[7]</a></sup></span> 。我认为这个数字的一​​些定性印象是概括性的。</p><p>印象笔记：</p><ul><li>贝叶斯神经网络、蒙特卡洛近似和集成方法都做出了类似的预测。</li><li>架构对于泛化属性来说非常重要，在某种程度上使这些方法看起来过于自信。 （整合不同的架构将是一个好的开始，但没有人这样做。）</li><li> Dropout 和变分推理近似在数据点附近的置信度都低于集成簇，但在分布之外的置信度更高。</li></ul><p>在对 CIFAR-10 或 LSTM 语言模型<span class="footnote-reference" role="doc-noteref" id="fnrefhzxdsrsco9g"><sup><a href="#fnhzxdsrsco9g">[18]</a></sup></span>或医学 MRI 数据<span class="footnote-reference" role="doc-noteref" id="fnrefu11lguv41wc"><sup><a href="#fnu11lguv41wc">[17]</a></sup></span>进行更彻底的比较（现在包括校准）时，集成似乎是最好的方法。但老实说，我提到的所有方法都非常接近，在复杂任务上，dropout 比玩具模型所建议的更具竞争力，而变分推理在 MNIST 上的表现令人惊讶。</p><p>高阶建模出现在较少的比较中。但这是参考文献中的一个数字。 31 其中模型在 MNIST 上进行训练，然后在<a href="https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html">非 MNIST</a>上进行测试<span class="footnote-reference" role="doc-noteref" id="fnrefkm10b29by7c"><sup><a href="#fnkm10b29by7c">[32]</a></sup></span> 。学习分类输出上的狄利克雷分布（EDL 虚线）与包的其余部分不同，就像包与原始模型输出（蓝色 L2 线）的不同一样： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/iyp8dpi29scplwjsybyy" alt="该图显示高阶建模给出了分布的高熵猜测。" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/p6hfgbwkaxi6erapiwuq 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/jomn5fobieqqetqjk169 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/pi2t8mbqbba98fa0jpou 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/w6trrvylxr34gdixbcwx 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/ehphdoebwsegtp3qtlsb 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/bnaqfkt3qw2dubftvxql 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/kejhllk816mnmsjsnixm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/urqu1kjrrbmc5dvnlfbw 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/t99gmgzve5q2xvdbicct 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/79eegMp3EBs8ptFqa/ba87fp8izpeheft8u4ka 829w"><figcaption>来自<a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#fnkm10b29by7c">参考文献。 32</a> . OOD 数据集上输出熵的积分直方图（或累积分布）。训练是在 MNIST 上进行的，但测试是在<a href="https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html">非 MNIST</a>上进行的，因此具有更高的高熵概率是好的。<br><br> EDL 是他们的狄利克雷分布模型，L2 是原始模型输出，DeepEnsemble 是一个集成，FFLU 不清楚但可能是贝叶斯 NN 近似，Dropout 是 dropout，FFG 是贝叶斯 NN 方法，MNFG 是变分推理近似。</figcaption></figure><p>当显示 OOD 数据时，这是一种非常令人印象深刻的不确定能力。是的，在分布上（在 MNIST 上）它具有正常的准确性。显然，学习何时不确定就是获取一些其他方法无法获取的信息。</p><p>但如此不确定真的正确吗？假设您正在对数字进行分类，这就是您所知道的，然后有人输入字符“B”。这肯定是8，对吧？或者也许是一个被压在一起的 13，如果你能想到这个想法的话。但它肯定不是 2。既然你知道这一点，那么在这里返回最大熵分布将是一个错误。但这似乎正是狄利克雷分布模型倾向于做的事情。</p><p>我希望高阶模型与论文中其他所有内容之间的不同行为是因为这些方法具有不同的背景假设，如果我们知道我们在做什么，我们可以在适当的时候灵活地使用不同的假设。</p><p></p><h2>我未来想从事这个领域的工作</h2><ul><li>更大的尺度<ul><li>对 Transformer 语言模型的不确定性估计进行基准测试。</li></ul></li><li>搜索的鲁棒性<ul><li>看看认知不确定性对对抗性例子有何影响。</li><li>分析找到新的对抗性例子来欺骗不确定性估计和原始指​​标是多么容易。检查这些例子对人类来说是否自然。</li></ul></li><li>非压差噪声<ul><li>当您向神经网络添加噪声时，改进采样的理论。</li><li>对不同类型的噪声进行相互比较和其他方法的基准测试。</li></ul></li><li>合奏带来更多聪明的事情<ul><li>测试改变架构的集成。</li><li>测试正在探索“相同”潜在特征的集合。</li></ul></li><li>更好的高阶模型<ul><li>发展神经网络学习二阶分布的贝叶斯视角。</li><li>通过尝试转化理论和修补信号（例如预测未来更新），为高阶模型开发更好的训练方法。</li><li>弄清楚如何使用高阶建模来获得不同背景假设的不确定性。</li></ul></li><li>更好的比较<ul><li>更系统地比较校准和 Brier 评分。</li><li>对决策问题进行基准测试，为分布数据之外的“良好行为”提供具体标准。</li><li>开发包含“自然”分布泛化的标记数据集，我们可以将其类比为现实世界中模型完成的 OOD 泛化。</li></ul></li><li>更多协同效应<ul><li>通过将多种方法结合在一起，可以获得更好的结果。校准太容易与一切结合起来，尽管它仍然是一个好主意，但这不算新闻。</li></ul></li></ul><p>如果这些论文确实存在而我错过了，请告诉我。如果他们不这样做，并且其中一个项目听起来像是您想做的事情，请联系我 - 我很乐意聊天。</p><p></p><h2>与一致性、结论的相关性</h2><p>文献与对齐的相关性不如我的预期，但这主要是因为我的期望被混淆了。我对某种“人类价值观的不确定性”感兴趣，它不同于文献中的“任意”或“认知”不确定性。</p><p>任意不确定性是从数据中学习到的，但我们没有人类价值观的真实标签。或者，如果模型中有一些与人类价值观相关的潜在特征，我们不仅仅想了解该特征在某些训练集上的方差。</p><p>认知不确定性更接近，但正如文献中所使用的那样，它实际上是关于某些输出或特征对于训练目标的有用程度。在训练过程中收敛到相同答案的模型越多，认知不确定性就越小。但相对于我想要的，这感觉好像缺少一些关于首先使用什么训练程序或特征检测程序的不确定性<span class="footnote-reference" role="doc-noteref" id="fnrefmyfeye042z"><sup><a href="#fnmyfeye042z">[37]</a></sup></span> 。</p><p>正确的训练/特征检测程序的不确定性偏离了“有一个基本事实答案，不确定性是与该答案的预期偏差”的通常范式。为了保持一致，我认为图片应该更像是通信——我们试图通过架构和数据向人工智能传达一些信息，而人工智能应该对如何解释它有不确定性。</p><p>构建这种关于人类价值观的不确定性是相当棘手的——我什至还不知道我想从中得到什么！也许如果我们更清楚地理解我们想要什么，我们就可以用更标准的不确定性来构建它。例如，我们可以设计一个人工智能可以玩的“游戏”，激励对人类概念的不同解释，这样游戏中的任意和认知不确定性就可以充分捕捉到我们希望人工智能对人类价值观具有的不确定性。</p><p></p><p><i>这篇文章部分写于</i><a href="https://www.mitalignment.org/"><i>MAIA</i></a> <i>。谢谢玛雅！还有贾斯蒂斯·米尔斯（Justis Mills）进行编辑，以及波士顿的各种人士进行对话。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn80ywo0pbl8r"> <span class="footnote-back-link"><sup><strong><a href="#fnref80ywo0pbl8r">^</a></strong></sup></span><div class="footnote-content"><p>主动学习文献调查。<i>伯尔·塞特尔斯</i>(2010) <a href="https://burrsettles.com/pub/settles.activelearning.pdf">https://burrsetles.com/pub/settles.activelearning.pdf</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnq23duy4ut0g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq23duy4ut0g">^</a></strong></sup></span><div class="footnote-content"><p>优化器的诅咒：决策分析中的怀疑主义和决策后惊喜。<i>詹姆斯·E·史密斯、罗伯特·L·温克勒</i>(2006) <a href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451">https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn3dij2j9svj8"> <span class="footnote-back-link"><sup><strong><a href="#fnref3dij2j9svj8">^</a></strong></sup></span><div class="footnote-content"><p>用于检测神经网络中错误分类和分布外示例的基线。<i>丹·亨德里克斯、凯文·金佩尔</i>(2016) <a href="https://arxiv.org/abs/1610.02136">https://arxiv.org/abs/1610.02136</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnfo4svcpvxs"> <span class="footnote-back-link"><sup><strong><a href="#fnreffo4svcpvxs">^</a></strong></sup></span><div class="footnote-content"><p>使用贝叶斯统计进行神经网络不确定性评估并应用于遥感。 <i>F. Aires、C. Prigent、WB Rossow</i> (2004)</p><p>第 1 部分：网络权重<a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004173">https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004173</a></p><p>第 2 部分：输出错误<a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004174">https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004174</a></p><p>第 3 部分：网络雅可比矩阵<a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004175">https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2003JD004175</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fncogdul3x2xj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcogdul3x2xj">^</a></strong></sup></span><div class="footnote-content"><p>深度学习中不确定性估计的通用框架。<i>安东尼奥·洛奎西奥、马蒂亚·塞古、大卫·斯卡拉穆扎</i>(2020) <a href="https://www.zora.uzh.ch/id/eprint/197704/1/RAL20_Loquercio.pdf">https://www.zora.uzh.ch/id/eprint/197704/1/RAL20_Loquercio.pdf</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnamk58pvab4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefamk58pvab4">^</a></strong></sup></span><div class="footnote-content"><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(parameters)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">如果</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">高斯</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">分布</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">（</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">指数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">L2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">正</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">则</span></span></span></span></span></span></span>化），并且您的损失是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{P}(dataset \; | \; parameters)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">对</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">损失</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">因此</span></span><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">数据</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">集</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">参数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">是</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">它</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">的</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">指数</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">。</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></p></div></li><li class="footnote-item" role="doc-endnote" id="fnlpb1b2qcoen"><span class="footnote-back-link"><sup><strong><a href="#fnreflpb1b2qcoen">^</a></strong></sup></span><div class="footnote-content"><p>神经网络中的不确定性：大约是贝叶斯的结合。<i>蒂姆·皮尔斯（Tim Pearce），菲利克斯·莱布里德（Felix Leibfried），亚历山德拉·布林特鲁普（Alexandra Brintrup），穆罕默德·扎基（Mohamed Zaki），安迪·尼利（Andy Neely）</i> <a href="http://proceedings.mlr.press/v108/pearce20a/pearce20a.pdf">（</a> 2020）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnd9jz9mfml"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd9jz9mfml">^</a></strong></sup></span><div class="footnote-content"><p>最明显的问题是Hessian的奇异性。但是，损失格局可能会在短长度上皱纹，有时使低阶近似值失败。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbct5kii2m07"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbct5kii2m07">^</a></strong></sup></span><div class="footnote-content"><p>迈向神经网络的校准且可扩展的不确定性表示。 <i>Nabeel Seedat，Christopher Kanan</i> （2019） <a href="https://arxiv.org/abs/1911.00104">https://arxiv.org/abs/1911.00104</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnen0yeoqzg0k"> <span class="footnote-back-link"><sup><strong><a href="#fnrefen0yeoqzg0k">^</a></strong></sup></span><div class="footnote-content"><p>简单可扩展的预测性不确定性使用深层合奏。 <i>Balaji Lakshminarayanan，Alexander Pritzel，Charles Blundell</i> （2017） <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf">https://proceedings.neurips.cc/prape_files/prape/prape/2017/file/9ef2ed4b7fd2c810847ffa5fa5bce385fa5bce38-fa85bce38-paper.paper.paper.paper.paper.paper.papdfdfffa</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnxqxuuw1xe2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxqxuuw1xe2">^</a></strong></sup></span><div class="footnote-content"><p>计算机视觉的贝叶斯深度学习需要哪些不确定性？ <i>Alex Kendall, Yarin Gal</i> (2017) <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn2jiolbj6tn8"> <span class="footnote-back-link"><sup><strong><a href="#fnref2jiolbj6tn8">^</a></strong></sup></span><div class="footnote-content"><p>从希腊的根源“关于知识”和“关于赌博”。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnud7l8eu73jk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefud7l8eu73jk">^</a></strong></sup></span><div class="footnote-content"><p>如何仅给定型号的权重， <i>dkirmani</i> （2023） <a href="https://www.lesswrong.com/posts/tvLi8CyvvSHrfte4P/how-2-tell-if-ur-input-is-out-of-distribution-given-only">https://www.lesswrong.com/posts/tvli8cyvvshrfte4p/how-2-tell-2-tell-2-tell-if-if-ur-input-input-input-input--- - 分布赋予</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnlgnhfp0m35e"><span class="footnote-back-link"><sup><strong><a href="#fnreflgnhfp0m35e">^</a></strong></sup></span><div class="footnote-content"><p>噪音存在于现实世界中，因此，如果小输入噪音会改变您的答案，那么您不应该对此充满信心。相反，在行为良好的输入中，神经网络学会对噪声变得强大。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhfox6k7gfev"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhfox6k7gfev">^</a></strong></sup></span><div class="footnote-content"><p>密集回归的无训练不确定性估计：敏感性作为替代物。 <i>Lu Mi，Hao Wang，Yonglong Tian，Hao He，Nir Shavit</i> （2022） <a href="https://arxiv.org/abs/1910.04858">https://arxiv.org/abs/1910.04858</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn0hl862nst9f9"> <span class="footnote-back-link"><sup><strong><a href="#fnref0hl862nst9f9">^</a></strong></sup></span><div class="footnote-content"><p>深度神经网络中不确定性的调查。 <i>Jakob Gawlikowski等。</i> （2021） <a href="https://arxiv.org/abs/2107.03342">https://arxiv.org/abs/2107.03342</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnu11lguv41wc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu11lguv41wc">^</a></strong></sup></span><div class="footnote-content"><p>估计心脏MRI分割神经网络中的不确定性：一项基准研究。 Matthew Ng，Fumin Guo，Labonny Biswas，Steffen <a href="https://ieeexplore.ieee.org/abstract/document/10002847">E.</a> <i>Petersen，Stefan K. Piechnik，Stefan Neubauer，Graham Wright</i> （2023）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhzxdsrsco9g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhzxdsrsco9g">^</a></strong></sup></span><div class="footnote-content"><p>你能相信你的模型的不确定性吗？评估数据集偏移下的预测不确定性。 <i>Yaniv Ovadia，Emily Fertig，Jie Ren，Zachary Nado，D Sculley，Sebastian Nowozin，Joshua V. Dillon，Balaji Lakshminarayanan，Jasper Snoek</i> <a href="https://arxiv.org/abs/1906.02530">（</a> 2019）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfqljh5j8ipf"> <span class="footnote-back-link"><sup><strong><a href="#fnreffqljh5j8ipf">^</a></strong></sup></span><div class="footnote-content"><p>深层神经网络倾向于可预测。 <i>Katie Kang，Amrith Setlur，Claire Tomlin，Sergey Levine</i> （2023） <a href="https://arxiv.org/abs/2310.00873">https://arxiv.org/abs/2310.00873</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnvl2ahm3user"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvl2ahm3user">^</a></strong></sup></span><div class="footnote-content"><p>对于分布外检测似乎有些普遍，例如增强神经网络中分布外图像检测的可靠性。 <i>Shiyu Liang，Yixuan Li，R。Srikant</i> （2020） <a href="https://arxiv.org/abs/1706.02690">https://arxiv.org/abs/1706.02690</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnzkojoz799ka"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzkojoz799ka">^</a></strong></sup></span><div class="footnote-content"><p> GPT-4 技术报告。 <i>Openai</i> （2023） <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnnglt70rybd9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnglt70rybd9">^</a></strong></sup></span><div class="footnote-content"><p> Mix-n匹配：深度学习中不确定性校准的合奏和组成方法。 <i>Jize Zhang，Bhavya Kailkhura，T。Yong-Jin Han</i> （2020） <a href="https://arxiv.org/abs/2003.07329">https://arxiv.org/abs/2003.07329</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fngx8tqfun0w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgx8tqfun0w">^</a></strong></sup></span><div class="footnote-content"><p>不确定性定量和深层合奏。 <i>Rahul Rahaman，Alexandre H. Thiery</i> （2020） <a href="https://arxiv.org/abs/2007.08792">https://arxiv.org/abs/2007.08792</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fntmgq2x5m6t"> <span class="footnote-back-link"><sup><strong><a href="#fnreftmgq2x5m6t">^</a></strong></sup></span><div class="footnote-content"><p>在分布数据集上校准深度神经网络分类器。 <i>Zhihui Shao，Jianyi Yang，Shaolei Ren</i> （2020） <a href="https://arxiv.org/abs/2006.08914">https://arxiv.org/abs/2006.08914</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fng3rtbqm4247"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg3rtbqm4247">^</a></strong></sup></span><div class="footnote-content"><p>归纳性共形预测：对神经网络的理论和应用。 <i>Harris Papadopoulos</i> （2008） <a href="https://www.intechopen.com/chapters/5294">https://www.intechopen.com/chapters/5294</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnqbdlqupa3x"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqbdlqupa3x">^</a></strong></sup></span><div class="footnote-content"><p>通过logit归一化来缓解神经网络过度自信。 <i>Hongxin Wei，Renchunzi Xie，Hao Cheng，Lei Feng，Bo An，Yixuan li</i> （2022） <a href="https://proceedings.mlr.press/v162/wei22d/wei22d.pdf">https://proceedings.mlr.press/v162/wei222d/wei22d/wei22d.pdf.pdf.pdf.pdf</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn2qfcvdqkx2y"> <span class="footnote-back-link"><sup><strong><a href="#fnref2qfcvdqkx2y">^</a></strong></sup></span><div class="footnote-content"><p>使用噪声对比的先验，深度神经网络中可靠的不确定性估计。 <i>Danijar Hafner，Dustin Tran，Timothy Lillicrap，Alex Irpan，James Davidson</i> （2018） <a href="https://openreview.net/forum?id=HkgxasA5Ym">https://openreview.net/forum?id=hkgxasa5ym</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnazba93s687"> <span class="footnote-back-link"><sup><strong><a href="#fnrefazba93s687">^</a></strong></sup></span><div class="footnote-content"><p>通过对抗训练和预训练改善了OOD的概括。 <i>Mingyang Yi，Lu Hou，Jicheng Sun，Lifeng Shang，Xin Jiang，Qun Liu，Zhi-Ming MA</i> （2021） <a href="http://proceedings.mlr.press/v139/yi21a/yi21a.pdf">http://proceedings.mlr.press/v139/v139/yi21a/yi21a/yii21a/yii21a.pdf</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnv1gf6chx43"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv1gf6chx43">^</a></strong></sup></span><div class="footnote-content"><p> SolidGoldMagikarp（加上及时生成）。<i>杰西卡·伦贝洛（Jessica Rumbelow），马修·沃特金斯</i>（2023）<i> </i><a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">https://www.lesswrong.com/posts/apeje8bso6rafolqg/solidgoldmagikarp-plus-prompt-generation</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fna3kc8qjhdn6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa3kc8qjhdn6">^</a></strong></sup></span><div class="footnote-content"><p>对抗性示例不是错误，它们是功能。<i>安德鲁·伊利亚斯（Andrew Ilyas）等人。</i> （2019） <a href="https://arxiv.org/abs/1905.02175">https://arxiv.org/abs/1905.02175</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn28983dpfjdp"> <span class="footnote-back-link"><sup><strong><a href="#fnref28983dpfjdp">^</a></strong></sup></span><div class="footnote-content"><p>如果我们使用生成而不是搜索来构建良好的输出，则可以避开不强大的优化的问题。或在RL上下文中，如果我们使用策略预测器将我们保持在系统的有效性领域中。但这是昂贵的，有时容易以不同速度推广的能力。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnkm10b29by7c"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkm10b29by7c">^</a></strong></sup></span><div class="footnote-content"><p>证据深度学习以量化分类不确定性。 <i>Murat Sensoy，Lance Kaplan，Melih Kandemir</i> （2018） <a href="https://arxiv.org/abs/1806.01768">https://arxiv.org/abs/1806.01768</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnhfc8x4hav0b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhfc8x4hav0b">^</a></strong></sup></span><div class="footnote-content"><p>使用Dirichlet深神经网络识别室外对象。<i>艾哈迈德·哈马姆（Ahmed Hammam），弗兰克·邦纳斯（Frank Bonarens</i> <a href="https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Hammam_Identifying_Out-of-Domain_Objects_with_Dirichlet_Deep_Neural_Networks_ICCVW_2023_paper.pdf">）</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnffdudph3qus"> <span class="footnote-back-link"><sup><strong><a href="#fnrefffdudph3qus">^</a></strong></sup></span><div class="footnote-content"><p>教学模型以表达他们的文字不确定性。 <i>Stephanie Lin，Jacob Hilton，Owain Evans</i> （2023） <a href="https://openreview.net/forum?id=8s8K2UZGTZ">https://openreview.net/forum?id=8S8S8K2UZGTZ</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnadsp4hkxuk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefadsp4hkxuk">^</a></strong></sup></span><div class="footnote-content"><p>直接从GPT-3引起概率。 <i>nuno sempere</i> （2023） <a href="https://forum.effectivealtruism.org/posts/aGmhi4uvAJptY8TA7/straightforwardly-eliciting-probabilities-from-gpt-3#Fine_tune_the_model_on_good_worked_examples_of_forecasting_reasoning">https://forum.effectivealtruism.org/posts/agmhi4uvajpty8ta7/straightforwardforwardforwardly-eliciting-probability-probabilities-probabilities-from-gpt-from-gpt-fine_tune_tune_tune_the_model_good_good_good_good_model_of_exam__exame_examequarky_examectimed_exampleastion_ofrachiquest</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnxuwhb9ieyh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxuwhb9ieyh">^</a></strong></sup></span><div class="footnote-content"><p>关于认知不确定性定量的二阶评分规则。 <i>Viktor Bengs，EykeHüllerMeier，Willem Weegeman</i> （2023） <a href="https://arxiv.org/abs/2301.12736">https://arxiv.org/abs/2301.12736</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnmyfeye042z"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmyfeye042z">^</a></strong></sup></span><div class="footnote-content"><p>我还可能会提及要使用哪些抽象或使用哪种推理过程的不确定性。但是这些似乎是训练的下游。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/79eegMp3EBs8ptFqa/neural-uncertainty-estimation-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/79eegmp3ebs8ptfqa/neural-unclentity-unclenty-simation-for-anignment<guid ispermalink="false"> 79EEGMP3EBS8PTFQA</guid><dc:creator><![CDATA[Charlie Steiner]]></dc:creator><pubDate> Tue, 05 Dec 2023 08:01:32 GMT</pubDate></item><item><title><![CDATA[Analyzing the Historical Rate of Catastrophes]]></title><description><![CDATA[Published on December 5, 2023 6:30 AM GMT<br/><br/><p>为了传达风险，我们经常转向故事。核武器会想到相互保证的破坏，红色按钮和核冬季的故事。气候变化总结了极端天气的故事，由于海平面上升而超过了农作物的失败。大流行时期的同性恋几乎不需要想象力，但以前是<em><a href="https://en.wikipedia.org/wiki/Contagion_(2011_film)?ref=bounded-regret.ghost.io">传染病</a></em>等电影的主题。</p><p>故事非常适合传达具体的风险（我本人<a href="https://bounded-regret.ghost.io/gpt-2030-and-catastrophic-drives-four-vignettes/">最近为AI风险做到这一点</a>），但它们是预测未来的糟糕方式。那是因为大多数故事太具体了，无法可能。更重要的是，故事倾向于以短暂的，简单的因果关系为特色，而现实是复杂且多元的。</p><p>大多数竞争性预报员没有使用故事，而是通过查看历史<a href="https://forecasting.quarto.pub/book/base-rates.html?ref=bounded-regret.ghost.io">参考课</a>来开始他们的预测。这确实很好，而且也很有意义：历史使我们摆脱了实际发生的事件的基础，使我们摆脱了讲故事的偏见。尽管历史是通过叙述过滤的，但良好的历史将与现实的复杂性有关，我们可以通过原始数字来进一步剥离叙事。 <sup><a href="#fn1">[1]</a></sup></p><p>在这篇文章中，我将使用参考课来了解社会当今面临的最大风险。我将考虑两个不同的历史灾难参考课程来做到这一点：</p><ul><li>杀死了全球人口中很大一部分的事件（<a href="#historical-causes-of-human-population-loss">第1节</a>）</li><li>物种的灭绝，尤其是大规模灭绝事件（<a href="#species-extinctions">第2节</a>）</li></ul><p>查看这些参考课程教会了我们两件事。首先，它为我们提供了不同不同灾难的数值估计。如果我们将一场灾难定义为一场事件，在十年内杀死了1％的全球人口，那么自1500年以来就发生了11起这样的灾难，每年的基本速度为2％。如果我们将标准提高到杀死10％的人口，则基本利率下降了一个数量级，至0.2％。</p><p>历史也为我们提供了定性的见解。例如，上一段中的所有灾难都是流行病，战争或饥荒。此外，许多事件都是多因果的 - 最糟糕的流行病是在人口已经被饥荒削弱时发生的，而气候变化或政治动荡促成了许多流行病和饥荒。物种的灭绝也是多因果的，常见的罪魁祸首是气候变化，自然灾害，入侵物种和人类。</p><p>反对使用历史基本利率的一个论点是，现在与过去有很大不同（例如由于技术引起的），以至于基本费率毫无意义。尽管当今的世界确实与过去不同，但基本费率可以通过澄清实际情况来帮助增强而不是忽略这些差异。例如，技术的存在不能使我们远远超过基本速率，因为许多技术在整个历史上已经开发了，没有任何技术在上面定义的意义上造成了灾难。取而代之的是，我们应该寻找与灾难的历史驱动因素共享财产的技术：流行病，饥荒，战争，政治动荡，气候变化，自然灾害，入侵物种和人类。</p><p>我详细分析了这些驱动程序（<a href="#takeaways-for-modern-catastrophes-and-for-ai">第3节</a>），发现它们属于几个核心群体：</p><ul><li>自然事件的规模是全球或区域性的（饥荒，气候变化，自然灾害）</li><li>新颖的，高度适应的，自我复制的生物（流行病，新病原体和捕食者，入侵物种）</li><li>协调的人类寻求资源，土地或权力（战争，政治动荡，灭绝和栖息地破坏引起的灭绝）</li></ul><p>此列表是有道理的 - 要产生全球影响，应该从全球范围（大型自然事件）开始，或者有能力到达那里的手段（自我复制，协调）。</p><p>从这个角度来看，21世纪的灾难驱动因素是什么？从上面的列表中可以明显看出一些答案 - 助手，气候变化和重大战争仍然是严重的威胁。饥荒不太明显地威胁性，因为最后一个主要是1961年，但为饥荒的准备仍然可能是谨慎的。而政治动荡本身并非灾难性，为其他灾难带来了发生的条件。</p><p>转向新技术，工程病原体是危险的，因为它们是新型的自我复制者，<a href="https://en.wikipedia.org/wiki/Gray_goo?ref=bounded-regret.ghost.io">某些类型的纳米技术</a>也是如此。核武器是危险的，因为它们与自然灾害具有相似的影响，并且因为它们增加了战争中最严重的损害。</p><p>最后，不幸的是，AI（我自己的研究领域）与许多灾难驱动因素具有共同的特性。它是一种新颖的自我复制器（可以复制自身），可以快速适应新数据。可以对AI系统<a href="https://arxiv.org/abs/1705.08926?ref=bounded-regret.ghost.io">进行培训以协调</a>并<a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">寻求权力</a>，从而反映人类协调群体的威胁。最后，如果AI导致经济动荡和随后的政治动荡，可能会加剧其他灾难的驱动因素。</p><h1>人口损失的历史原因</h1><p>为了开始我们的分析，我研究了人口损失的最大历史原因，这是由被给定事件杀死的全球人口的比例来衡量的。为此，我结合了Wikipedia的<a href="https://en.m.wikipedia.org/wiki/List_of_anthropogenic_disasters_by_death_toll?ref=bounded-regret.ghost.io#Wars_and_armed_conflicts">主要战争</a>， <a href="https://en.m.wikipedia.org/wiki/List_of_anthropogenic_disasters_by_death_toll?ref=bounded-regret.ghost.io#Abuse_of_workers,_forced_laborers_and_slaves">奴隶制和其他强迫劳动</a>，<a href="https://en.m.wikipedia.org/wiki/List_of_famines?ref=bounded-regret.ghost.io">饥荒</a>， <a href="https://en.m.wikipedia.org/wiki/List_of_epidemics_and_pandemics?ref=bounded-regret.ghost.io">流行病</a>和<a href="https://en.m.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll?ref=bounded-regret.ghost.io">自然灾害的</a>数据。我考虑了其他数据来源，例如技术灾难，但所有这些数据来源的死亡人数比上面的五个要小得多。主要例外是种族灭绝，因为它们通常与战争同时发生，并且已经包含在那些死亡人数中，因此我排除了它们以避免双重计数。</p><p>我编写了一个Python脚本（在<a href="#scraping-script">附录</a>中共享）来刮擦这些来源并将它们汇总到单个PANDAS DataFrame中，然后被过滤以创建两个数据集：</p><ul><li><strong>灾难性</strong>：所有事件杀死至少0.1％的人口的事件，这些事件通过在事件开始时被世界人口划分的总死亡而计算出来。 <sup><a href="#fn2">[2]</a></sup> <sup><a href="#fn3">[3]</a></sup></li><li><strong>严格的灾难</strong>：我进一步局限于“快速”（持续不到十年）的事件，其中至少有1％的人口死亡。</li></ul><p>一组灾难包括85个事件，其中80起发生以来发生，其中33例是战争，28人是饥荒，15例是流行病，有6人是强迫劳动，而3例是自然灾害。严格的灾难包括17个事件：5场战争，8种饥荒和4个流行病。我在下面包括严格的灾难的完整列表，以及所有灾难的散点图（有关原始数据，请参见<a href="#scraping-script">附录</a>）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/zsdej27vshx817jxxgzq"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/aoh79fzuiwbvftkounrs"></p><p>除了这些历史事件外，两个重要的史前事件是<a href="https://en.wikipedia.org/wiki/Toba_catastrophe_theory?ref=bounded-regret.ghost.io">Toba灾难</a>（人口下降了97％，可能是由于Supervolcano的造成的）和<a href="https://en.wikipedia.org/wiki/4.2-kiloyear_event?ref=bounded-regret.ghost.io">4.2KYA事件</a>（可能导致全球饥荒，但死亡人数尚不清楚）。</p><p><strong>报告偏见和基本费率。</strong>由于我们看到1500年代和1900年代再次灾难的速度“增加”的速度很可能会有偏见，而且这对于包括饥荒在内的所有类别都会发生这种情况（随着时间的推移会随着时间的推移而降低）。如果我们从1500起开始，则发生了51次灾难（0.11/年），有11例严格的灾难（0.02/年）。</p><p>让我们下一个模型（快速）灾难<sup><a href="#fn4">[4]</a></sup>的基本速率如何随其严重程度而变化。查看所有导致人口至少下降1％的灾难，我们看到了大约<a href="https://en.wikipedia.org/wiki/Zipf%27s_law?ref=bounded-regret.ghost.io">Zipfian</a>分布：死亡率为R的灾难的可能性与1/R成正比。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/wgfcegdndv1dtfwggmzl"></p><p>基于此，死亡率10％的灾难的发生率为0.002/年（每5世纪一次），死亡率为1％的灾难发生0.02/年（每世纪两次）。尽管这些数字似乎很低，但它们表明，<strong>在未来25年中，大约有10％的灾难灾难的机会大约有5％的机会</strong>（由于0.002 * 25 = 0.05）。</p><p>低于1％的死亡率，灾难比ZIPF定律预测的可能性少（请参阅<a href="#log-log-plot-of-catastrophes">附录</a>）。例如，0.1％死亡率的经验频率为0.08/年（略低于每十年一次）。</p><p><strong>随着时间的推移趋势。</strong>如果我们计算自1500年以来每十年的灾难，我们将获得以下图： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/wb5os0qkd9psxc2zrno9"></p><p>在1850  -  1950年期间，还有更多的灾难，尽管我怀疑这是报告偏见的文物。在此期间之前，随着时间的流逝，灾难的发生率大致恒定： <a href="https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test?ref=bounded-regret.ghost.io">Ljung-box测试</a>和<a href="https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test?ref=bounded-regret.ghost.io">Wald-Wolfowitz测试</a>都不能拒绝在数十年中从1500-1900数十年来分布灾难的零（P = 0.36和0.26） ， 分别）。</p><p>随着时间的流逝，最值得注意的变化是我们目前处于平静时期，从1950  -  1960年左右开始。确实，自20世纪上半叶以来，灾难显着下降：</p><ul><li> 9个饥荒发生在20世纪上半叶，但下半年只有1次（中国饥荒，1959- 1961年）</li><li>上半场发生了5场重大战争，但下半场只有1次（朝鲜战争，1950- 1953年）</li><li>流行病更为恒定，上半场有2个，下半场有1个（加2019年的Covid）。</li></ul><p>由于粮食生产和储存量更好，饥荒可能会减少，这是持久的改善。战争可能由于<a href="https://en.wikipedia.org/wiki/Pax_Americana?ref=bounded-regret.ghost.io">PAX Americana</a>而减少，但不幸的是，由于日益增长的全球紧张局势可能会放松。因此，流行病和（可能）战争是迄今为止灾难的主要现代来源。</p><p><strong>定性分析：多用途。</strong>许多灾难有多种原因。例如，在黑死的主要理论中，气候变化是两种方式的驱动力。首先，亚洲的气候变化导致啮齿动物从山区迁移到人口更多的地区，从而传播了这种疾病。其次，欧洲的小冰河时代导致饥荒，导致人口疲软，因此更容易受到疾病的影响。 <sup><a href="#fn5">[5]</a></sup>有趣的是，黑死亡也可能通过造成人口减少的重新森林造成造林，从而加剧了较小的冰河时代，从而导致碳捕获和随后的冷却。</p><p>给出其他多种原因的例子：</p><ul><li>在美洲的欧洲殖民化中，大多数死亡是由于疾病而不是战争造成的。</li><li>从明对的过渡是由许多因素引起的，包括疾病和饥荒。饥荒本身可能是由小冰河时代引起的。</li><li> Taiping叛乱是由于饥荒的政治动荡而开始的，随后的许多死亡是由干旱，饥荒和疾病而不是军事死亡引起的。</li><li>总的来说，许多饥荒是由气候事件和/或不良政府政策造成的。</li></ul><p>总体而言，这表明要减少灾难的数量或强度，我们不仅应立即攻击原因，而且还应攻击更多的系统性上游原因。</p><h1>物种灭绝</h1><p>作为第二个参考类别，我考虑了非人类物种的灭绝。 <sup><a href="#fn6">[6]</a></sup>由于几个原因，这更难分析：</p><ul><li>大多数灭绝发生在数百万年前，因此我们只有间接证据，并且存在明显的采样偏见，因为某些物种更容易保存为化石。</li><li>如果一个物种逐渐适应一种新物种，我们可能不想将其算作“灾难”。</li><li>一些声称的质量灭绝事件实际上可能是一段时间内发生的许多较小事件。</li></ul><p>为了减少这些困难，我将专注于两个相对较新的灭绝事件：</p><ul><li>第四纪晚期灭绝（ <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.34.011802.132415?journalCode=ecolsys&amp;ref=bounded-regret.ghost.io">Koch and Barnosky，2006年</a>），发生在10,000-50,000年前，导致大多数大型哺乳动物灭绝。</li><li><a href="https://en.wikipedia.org/wiki/Holocene_extinction?ref=bounded-regret.ghost.io">全新世灭绝</a>发生在过去10，000年中（在过去的一个世纪中增加），主要是由人类的狩猎和栖息地破坏驱动。</li></ul><p>尽管大多数历史大规模灭绝事件都是由气候变化或自然灾害驱动的，但这两个最近的灭绝事件被认为是人类全部或部分驱动的。我将在下面回顾有关两个灭绝事件的证据和领导理论。</p><h2>历史基本率</h2><p>在讨论第四纪和全新世灭绝之前，让我们计算上下文的基本费率。根据化石记录，平均<a href="https://en.wikipedia.org/wiki/Background_extinction_rate?ref=bounded-regret.ghost.io">每百万年每种物种大约有一个灭绝</a>。 <sup><a href="#fn7">[7]</a></sup>但是，这些灭绝并不是在时间的范围内恒定，而是以“脉冲”为中，如下所示（ <a href="https://en.wikipedia.org/wiki/Extinction_event?ref=bounded-regret.ghost.io">Wikipedia的</a>图像）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/dfuifcmc1rwf6nughndc"></p><p>在这些脉冲中，每百万年的灭绝量大约是背景速率2-10倍。 <sup><a href="#fn8">[8]</a></sup></p><h2>第四纪晚期灭绝</h2><p><a href="https://en.wikipedia.org/wiki/Quaternary_extinction_event?ref=bounded-regret.ghost.io">第四纪晚期的灭绝</a>将一段时间从约50,000年前到10，000年前。在这段时间里，大约34％的哺乳动物灭绝了，包括美洲和澳大利亚的大多数哺乳动物以及全球几乎所有大型哺乳动物。这是一个比预期的背景灭绝率高的数量级（在40，000年内约为4％）。</p><p>下面的表（根据Wikipedia改编）按地理区域和规模灭绝的文档灭绝： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/hyklusibuiqlssx8a9j4"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/xfvtfxj13q8vij8qqsmx"></p><p>如表所示，灭绝在非洲（人类起源的地方，因此哺乳动物可以共同进化），并且在大型哺乳动物中最为严重。</p><p><strong>原因。</strong>从历史上看，研究人员辩论了这些灭绝是由气候变化还是人类接触驱动的。为了理解这场辩论，我阅读了几篇论文，并选择关注<a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.34.011802.132415?journalCode=ecolsys&amp;ref=bounded-regret.ghost.io">Koch＆Barnosky（2006）</a> ，该论文系统地回顾了许多竞争理论。 Koch＆Barnosky得出的结论是，灭绝的模式和强度是由人类驱动的，但是气候变化是重要的额外贡献者：<br><br></p><p> “从整体上讲，最近的研究表明，人类通过联合直接（狩猎）以及间接（竞争，栖息地的改变和碎片化）促进了全球许多地区的灭绝，但是晚期的环境变化影响了时机，地理，地理位置影响，也许是灭绝的大小。换句话说，没有<i>智人智人</i>的各种影响，在晚期季季晚期，全球生态系统极不可能经历大型慢养动物的大规模灭绝。但是，没有同时存在的快速气候变化在全球许多地区明显看出，某些物种可能持续更长的时间。”</p><p>因此，人类可能会驱动灭绝的几条途径：</p><ul><li>直接狩猎</li><li>间接狩猎（狗，老鼠和我们带来的其他动物）</li><li>栖息地破坏（例如，人为火灾）</li></ul><p>重要的是，由于不同的原因，不同的物种可能灭绝。 Koch＆Barnosky认为，欧亚大陆的大多数灭绝都是由于气候变化所致，澳大利亚和大多数岛屿上的灭绝都几乎完全归因于人类，而北美主要是人类具有气候的人类，这是一个加剧的因素。</p><p>这是一个说明关键点的故事。它与Koch＆Barnosky一致，但是不确定性支持简单。</p><ul><li>当人类到达岛屿时，他们带来了猪，狗和大鼠，所有这些都捕食了土著物种。由于岛屿物种在进化上是对这些掠食者的幼稚，因此许多物种灭绝了。</li><li>由于火和土地清理造成的栖息地破坏也导致了岛屿灭绝。</li><li>在较大的土地上，哺乳动物并不是进化于食肉动物的掠食者，因此并不容易灭绝。但是，人类是非常有效的猎人，足以使许多物种的出生率低于死亡率，这最终导致了数千年的灭绝。</li><li>重要的是，人类的饮食多种多样，因此即使他们狩猎了一些哺乳动物以灭绝，他们也从其他动物和植物那里收集了足够的食物来维持大量人口，从而避免了传统的<a href="https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations?ref=bounded-regret.ghost.io">捕食者捕食周期</a>。</li><li>在非洲和欧亚大陆，哺乳动物与人类或其前辈共同发展了数十万年以上。因此，他们有足够的进化时间来为有效的人类猎人开发防御能力，并解释了与美国和澳大利亚的灭绝率较低。</li></ul><p>总体而言，人类的狩猎可能是非岛屿灭绝的主要驱动力，还有其他因素，例如气候变化。重要的是，人类是一种新颖的捕食者是不够的，因为新的掠食者并不总是会导致灭绝。同样重要的是，我们是一个特别有效的捕食者，可以占据许多地理区域并且饮食多样化。</p><h2>全新世灭绝</h2><p>全新世灭绝开始于大约10，000年前，最近有可能加速，大多数研究人员认为人类发挥了重要作用。</p><p>矛盾的是，尽管最近发生，但全新世灭绝的程度比晚期灭绝晚期更具争议性，原因有两个。首先，大多数其他灭绝计数都取决于化石记录，但全新世灭绝是基于人类的当前和历史观察。这使得很难进行直接比较，因为两种方法具有不同的（且大）的采样偏差。其次，全新世灭绝的程度被政治化，因为它对于当今关于自然保存的论点至关重要，因此很难找到中性来源。</p><p>浏览了几篇论文后，我决定关注<a href="https://www.nature.com/articles/nature09678?ref=bounded-regret.ghost.io">Barnosky等人。 （2011）</a> <sup><a href="#fn9">[9]</a></sup> ，仔细讨论了采样偏见的几种来源，并试图为它们纠正它们。 Barnosky等。得出的结论是，在过去的500年中，总物种中有几个已经灭绝，这比灭绝的预期背景速率高（请注意，某些论文给出了更高的估计<sup><a href="#fn10">[10]</a></sup> ）。 Barnosky等。还得出结论，如果大多数濒危物种在下一个世纪灭绝，并且这种速度持续下去，我们将在几个世纪内失去所有物种中的大多数，仅与5个历史（通常更慢）的大规模灭绝事件相当。</p><p><strong>原因。</strong> Barnosky等。列出几种造成这些灭绝的压力源：“迅速变化的大气条件和变暖[...]，栖息地破碎，污染，过度捕捞和过度狩猎，入侵物种和病原体[...]以及扩大人类生物量”。 <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.34.011802.132415?journalCode=ecolsys&amp;ref=bounded-regret.ghost.io">Koch and Barnosky（2006）</a>增加了第四纪灭绝的生态破坏，作为进一步的压力源。 <sup><a href="#fn11">[11]</a></sup></p><p>与过去的灭绝不同，我们可以直接观察到当今许多全新世灭绝的原因。基于<a href="https://pubmed.ncbi.nlm.nih.gov/20978281/?ref=bounded-regret.ghost.io">Hoffmann等。 （2010年）</a> ，栖息地破坏是当前灭绝的最大驱动因素，其次是入侵物种（包括疾病）和过度狩猎，其次是环境原因，例如气候变化和污染。 <sup><a href="#fn12">[12]</a></sup> <sup><a href="#fn13">[13]</a></sup></p><h2>摘要：灭绝的典型原因是什么？</h2><p>总体而言，我对过去灭绝的分析指出了一个物种可以灭绝的几种方式：</p><ul><li>大规模的灾难或气候事件直接使物种不可行，或者破坏生态系统并导致后来的灭绝。</li><li>引入了一种新颖的侵略性生物，原始物种尚未适应该生物。这包括：<ul><li>一种入侵物种，它可以直接胜任其利基市场或破坏周围生态系统的物种。</li><li>一种新颖的病原体，特别是如果它具有<a href="https://en.wikipedia.org/wiki/Natural_reservoir?ref=bounded-regret.ghost.io">储层物种</a>。 <sup><a href="#fn14">[14]</a></sup></li><li>一个新的，有效的捕食者。这对岛屿物种影响最大，因为大陆物种在进化上暴露于多种捕食者中，以发展强大的反策略。但是，即使对于非岛屿物种，具有多种饮食的非常有效的捕食者也会淹没这些进化的防御能力。</li></ul></li><li>栖息地的变化（通常来自气候变化或其他物种）。</li><li>其他物种灭绝的后续作用。这部分与上面的项目重叠：例如，大型生动的灭绝导致森林的再生，因此显着改变了其他物种的栖息地。</li></ul><p>因此，通常大多数物种灭绝是由以下原因引起的：</p><ul><li>原始物种没有机会适应的第二种物种。第二种也必须不依赖原始物种来传播自身。</li><li>灾难性的自然灾害或气候事件。</li><li>上面两个来源之一造成的栖息地破坏或生态系统破坏。</li></ul><p><strong>为什么灭绝通常很少见。</strong>由于灭绝通常的基本速率较低，因此灭绝的原因必须很少见。为了更好地了解<em>可能</em>导致灭绝的是什么，让我们理解为什么大多数对物种的威胁<em>不会</em>导致灭绝。</p><p>首先，大多数捕食者不会造成灭绝。这是因为猎物与捕食者的犯罪同时发展防御，而捕食者越好的是猎物上的进化压力越多（因此，防御速度更快）。除此之外，如果猎物变得太罕见，那么捕食者种群<a href="https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations?ref=bounded-regret.ghost.io">通常会崩溃</a>，从而使猎物种群重新成长。因此，捕食者通常只有在（1）两者进入具有非进化适应猎物的新环境时才造成灭绝，并且（2）它们以多种物种为食，以便它们可以驱使一个物种灭绝而不会自身人口崩溃。</p><p>同样，默认情况下，新颖的病原体不会导致宿主灭绝，因为如果它们杀死了太多的宿主物种，他们就没有靶标要传播。取而代之的是，“病原体更有可能引起宿主的灭绝，如果它们……具有长期传染病，或者是可以在普通储层宿主和更脆弱的目标物种之间传播的多宿主病原体”（ <a href="https://www.nature.com/scitable/knowledge/library/disease-ecology-15947677/?ref=bounded-regret.ghost.io">Kilpatrick和Altizer，2010年</a>） 。</p><p><strong>人类。</strong>最后，让我们分析为什么人类尤其是如此有效的猎人，以至于我们能够驱动这么多物种灭绝。首先，我们具有高度适应性的能力，因此不仅能够生存，而且能够在各种环境中依靠多种食物来源。这使我们在全球范围内传播，并驱动一些物种灭绝，同时仍有其他食物来源。其次，我们可以有效地协调（ <a href="https://www.scientificamerican.com/article/how-homo-sapiens-became-the-ultimate-invasive-species/?ref=bounded-regret.ghost.io">Marean，2015年</a>），通过更好的战术使更大的猎物压倒了较大的猎物。最后，我们使用工具和技术来提高狩猎能力并塑造环境，放大上面讨论的两个关键灭绝驱动因素。</p><h1>现代灾难和人工智能的收获</h1><p>我们将所有驱动因素都用于人类灾难和非人类灭绝的驱动因素，我们看到了少数主题：</p><ul><li>非常大规模的自然事件</li><li>高度适应，自我复制的生物，尤其是受害者没有共同适应的生物（流行病，新颖的病原体和捕食者，入侵物种）。</li><li>人类协调的群体（战争，狩猎，栖息地破坏）</li><li>政治镇压或破坏（强迫劳动，不良政策导致饥荒）</li><li>其他灾难的后续作用。</li></ul><p>有趣的是，在大多数人类灾难中，技术似乎并不是直接的罪魁祸首，尽管它可能是发生大规模核战争的。对于非人类的灭绝，这可能是一个贡献者，因为技术提高了狩猎能力和栖息地破坏的便利性。</p><p>鉴于现代威胁，纳米技术和生物技术都威胁着创建新颖的自我复制者，并且人类设计的包含可能会导致它们以与我们进化防御的方式分布的方式“适应”。</p><p>核武器增加了战争的最坏情况，大规模监视增加了政治镇压的最坏情况。</p><p>气候变化是一个大规模的自然事件。除了直接影响，如果它导致非人类物种的许多灭绝或引起政治动荡，那么后续影响可能对人类造成灾难性。由于持续的灭绝而导致的生物多样性损失也可能会造成不良的后续影响，尽管这种影响的发生得很慢，以至于这可能不是直接的威胁。</p><p>最后，我们在此方程式中将AI放在哪里？不幸的是，它似乎拥有许多其他灾难驱动因素的特性：</p><ul><li> AI是可以自我复制的，并且可以训练自己以快速适应新数据的意义。因此，这是一个适应人的自我复制者，人类本身并不适应。</li><li> AI可能会接受比人类更好的培训以更好地坐标，因为从进化上，人类只能以<a href="https://en.wikipedia.org/wiki/Dunbar%27s_number?ref=bounded-regret.ghost.io">〜150</a>的群体进行协调，而如果我们解决了相关的<a href="https://arxiv.org/abs/1705.08926?ref=bounded-regret.ghost.io">多代理RL挑战，</a>则可以对AI进行培训以在任意大型组中进行协调。</li><li> AI的经济流离失所可能导致政治动荡。</li><li> AI还是上述许多其他驱动因素的贡献（尽管可以说是双重计数）：它使大规模监视更加容易，并可能加快创建其他危险技术（例如工程性病原体）的创建。</li></ul><p> AI也具有改善特性。首先，其他新技术并未引起灾难，这应该减少我们对AI的先前。其次，AI辅助研究可以帮助饥荒和气候变化，如果AI增加繁荣，AI可能会减少政治动荡。这些是重要的考虑因素，但是许多技术具有这些特性，而几乎没有一个可以在组中进行协调的自我复制器。</p><p>总体而言，我希望AI会增加灾难的速度。 <a href="https://docs.google.com/document/d/1DOmluInO2KkgmumAf1wKLKHU7HbMeveIugR_oiFvHPc/edit?ref=bounded-regret.ghost.io#bookmark=id.xu338fnad04c">如上所述</a>，未来25年中非常大（10％的死亡率）灾难的基本率为5％，我个人希望AI在下一步之上增加10％邮政。</p><p><strong>开放式问题。</strong>这篇文章没有解决几个问题。首先，我的分析尚无定论，即随着时间的推移的灾难发生率是否变化。 Data from extinctions suggests that it can vary by an order of magnitude, but it would be better to have data about human events.</p><p> Second, this post says little about the importance of technology and intelligence, even though these are intuitively important. Are technological catastrophes increasing over time, even if right now they are too small to register in the data above? Do more intelligent species often drive less intelligent species to extinction? <sup><a href="#fn15">[15]</a></sup> Base rates on either of these would inform forecasts for AI.</p><p> Finally, one might argue that elapsed time is not the right x-axis, but instead elapsed population growth, economic growth, or technological progress. As one example, take world GDP. There have been as many doublings of world GDP since 1900 as there have been between 1900 and 0CE, so if GDP doublings are the right “clock” to measure against then we might expect many more catastrophes to happen each decade now than in the past. This doesn&#39;t seem true to me from the data so far, but I&#39;d like to see it analyzed in more detail.</p><p><strong>致谢。</strong> Thanks especially to Dhruv Madeka for the discussions and initial data that inspired this post. Thanks also to Sham Kakade, Dean Foster, Tamay Berisoglu, Eli Lifland, Nuño Sempere, Daniel Kokotajlo, and Ege Erdil for useful discussions and comments on this post. Thanks to Justis Mills and Louise Verkin for copy-editing and helpful feedback.</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p> Of course, numbers themselves can be misleading, as many historical numbers are based on guesswork! A lot of the work that went into this post was doing extensive reading to decide which numbers to believe. <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p> Population sizes were collected from <a href="https://ourworldindata.org/grapher/population?time=-1000..latest&amp;country=%7EOWID_WRL&amp;ref=bounded-regret.ghost.io">Our World in Data</a> , see <a href="#population-by-country-across-history">Appendix</a> . <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p> The Taiping rebellion is double-counted, once as a War and once as a Famine. <a href="#fnref3">↩︎</a></p></li><li id="fn4" class="footnote-item"><p> As above, “fast” means taking less than one decade. <a href="#fnref4">↩︎</a></p></li><li id="fn5" class="footnote-item"><p> Another theory is that the Mongol invasions (another catastrophe) spread the Black Death, since Mongols threw diseased corpses into cities as a form of biological warfare. This is not currently the predominant theory, but would be another instance of multi-causality, and shows that different major catastrophes can be linked to each other. <a href="#fnref5">↩︎</a></p></li><li id="fn6" class="footnote-item"><p> Technically speaking, the historical fossil record usually only resolves extinctions at the level of genera rather than species, but I will generally elide this distinction for simplicity. <a href="#fnref6">↩︎</a></p></li><li id="fn7" class="footnote-item"><p> Note that this varies by taxon and estimates within a taxon are approximate, with different estimates in the literature varying by a factor of 4 or sometimes greater. <a href="#fnref7">↩︎</a></p></li><li id="fn8" class="footnote-item"><p> The 2-10x number is when looking at bins of 1 million years. For sudden catastrophic events such as an asteroid strike, the extinction rate over a 1-year time interval would spike much more than that. <a href="#fnref8">↩︎</a></p></li><li id="fn9" class="footnote-item"><p> This is the same Barnosky as above, though I did not know that when searching for papers—in both cases he happened to write the papers that I found most neutral and persuasive. To my delight, I learned that he is also at UC Berkeley.熊们走吧！ <a href="#fnref9">↩︎</a></p></li><li id="fn10" class="footnote-item"><p> See eg <a href="https://www.science.org/doi/10.1126/sciadv.1400253?ref=bounded-regret.ghost.io">Ceballos et al. (2015)</a> , with an estimate closer to two orders of magnitude above the background rate. <a href="#fnref10">↩︎</a></p></li><li id="fn11" class="footnote-item"><p> “Examples include plants that have lost their primary agents of seed dispersal or that are replete with defenses for herbivores that no longer exist, herbivores that are “overdesigned” for all existing predators,<br> and scavengers such as condors that have no naturally occurring carcasses to eat in continental settings.” <a href="#fnref11">↩︎</a></p></li><li id="fn12" class="footnote-item"><p> I follow Figure S7 of <a href="https://pubmed.ncbi.nlm.nih.gov/20978281/?ref=bounded-regret.ghost.io">Hoffmann et al.</a> (reproduced in the <a href="#species-endangerment-by-cause">Appendix</a> ), which counts endangered species grouped by cause of endangerment. I grouped the rows into the categories “habitat destruction”, “invasive species” (which includes the <a href="https://en.wikipedia.org/wiki/Chytridiomycosis?ref=bounded-regret.ghost.io">chytrid fungus</a> disease in amphibians), “overhunting/overfishing”, and “environment” (climate change / pollution / natural disasters). Some categories were ambiguous or did not fit into these 4. Overall I counted approximately ~360 in habitat destruction, ~250 from invasive species (dominated by amphibians), ~130 from overhunting/overfishing, and ~40 from environment. <a href="#fnref12">↩︎</a></p></li><li id="fn13" class="footnote-item"><p> See also <a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev.energy.28.050302.105532?ref=bounded-regret.ghost.io">Dirzo &amp; Raven (2003)</a> who similarly claim that habitat destruction is the primary driver. <a href="#fnref13">↩︎</a></p></li><li id="fn14" class="footnote-item"><p> A reservoir species is a second species in which the pathogen is not deadly, allowing it to multiply more freely, and from which the pathogen can cross over to the target species. <a href="#fnref14">↩︎</a></p></li><li id="fn15" class="footnote-item"><p> The closest I found was <a href="https://www.nature.com/articles/s41598-022-07327-9?ref=bounded-regret.ghost.io">Dembitzer et al. (2022)</a> , who claim that more intelligent mammals were less likely to go extinct during the Late Quaternary Extinction. However, we ideally want to study the opposite: are more intelligent mammals more likely to cause <em>other</em> species to go extinct? <a href="#fnref15">↩︎</a></p></li></ol></section><h1>附录</h1><h2>Log-Log Plot of Catastrophes</h2><p> As noted in Section 1, the distribution of catastrophes no longer follows Zipf&#39;s law when we go below death tolls of 1%, as shown below: <br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/pogy0xusqv3jf1vpdej2" alt="events_power_law_01.png"><br> One possibility is that the actual trend is log-normal instead of power law. Another is that less severe events are underreported.</p><h2> Species Endangerment by Cause</h2><p> The following is a reproduction of Figure S7 from <a href="https://www.science.org/doi/10.1126/science.1194442?ref=bounded-regret.ghost.io">Hoffmann et al. （2010）</a> 。 <br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SgYz4YPKridWXJSE6/o0x5tingr8fej6jn4f7z" alt="hoffman_figs7.png"></p><h2> Population by Country Across History</h2><div><a href="https://bounded-regret.ghost.io/content/files/2023/12/population.csv"><div><div>人口</div><div>Raw data on historical world population (see directly below for interactive HTML)</div><div><div> population.csv</div><div> 1MB</div></div></div><div>下载圈</div></div><iframe src="https://ourworldindata.org/grapher/population?tab=table&amp;time=-1000..latest&amp;country=~OWID_WRL"></iframe><h2> Scraping Script </h2><div><a href="https://bounded-regret.ghost.io/content/files/2023/12/scrape_events.py"><div><div> scrape_events</div><div> Python script used to scrape Wikipedia lists of major catastrophes</div><div><div> scrape_events.py</div><div> 13KB</div></div></div><div>下载圈</div></div><h2>Complete List of All Major Catastrophes</h2><div><a href="https://bounded-regret.ghost.io/content/files/2023/12/events_all.csv"><div><div> events_all</div><div> List of all major catastrophes scraped from Wikipedia</div><div><div> events_all.csv</div><div> 6KB</div></div></div><div>下载圈</div></div><br/><br/><a href="https://www.lesswrong.com/posts/SgYz4YPKridWXJSE6/analyzing-the-historical-rate-of-catastrophes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SgYz4YPKridWXJSE6/analyzing-the-historical-rate-of-catastrophes<guid ispermalink="false"> SgYz4YPKridWXJSE6</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Tue, 05 Dec 2023 06:30:03 GMT</pubDate> </item><item><title><![CDATA[Some open-source dictionaries and dictionary learning infrastructure]]></title><description><![CDATA[Published on December 5, 2023 6:05 AM GMT<br/><br/><p>随着越来越多的人开始从事包含字典学习的可解释性项目，公开提供高质量的字典将很有价值。 <span class="footnote-reference" role="doc-noteref" id="fnrefxki2bn9t6a"><sup><a href="#fnxki2bn9t6a">[1]</a></sup></span>为了让事情顺利进行，我和我的合作者（Aaron Mueller）：</p><ul><li>开源许多在 Pythia-70m MLP 上训练的稀疏自动编码器字典</li><li>发布我们用于训练这些词典的<a href="https://github.com/saprmarks/dictionary_learning">存储库</a><span class="footnote-reference" role="doc-noteref" id="fnrefp7b5kczep0n"><sup><a href="#fnp7b5kczep0n">[2]</a></sup></span> 。</li></ul><p>我们首先讨论字典，然后讨论存储库。</p><h1>字典</h1><p>词典可以从<a href="https://baulab.us/u/smarks/autoencoders/">这里</a>下载。有关如何下载和使用它们的信息，请参阅<a href="https://github.com/saprmarks/dictionary_learning">此处的</a>“下载我们的开源词典”和“使用经过训练的词典”部分。如果您在发表的论文中使用这些词典，我们要求您在致谢中提及我们。</p><p>我们正在为 EleutherAI 的 6 层 pythia-70m-deduped 模型发布两套字典。两组字典都使用来自<a href="https://pile.eleuther.ai/">The Pile</a>的约 8 亿个令牌，在 512 维 MLP<i>输出</i>激活（而不是像 Anthropic 使用的 MLP 隐藏层）上进行训练。</p><ul><li>第一组称为<code>0_8192</code> ，由大小为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8192 = 16 \times 512"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8192</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span></span></span></span></span></span>的字典组成<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。这些训练的 L1 惩罚为<code>1e-3</code> 。</li><li>第二组称为<code>1_32768</code> ，由大小为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="32768 = 64 \times 512"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">32768</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">64</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">512</span></span></span></span></span></span></span>的字典组成。这些训练的 l1 惩罚为<code>3e-3</code> 。</li></ul><p>以下是一些统计数据。 （有关这些统计数据含义的更多信息，请参阅我们的存储库的<a href="https://github.com/saprmarks/dictionary_learning">自述文件</a>。）</p><p>对于<code>0_8192</code>集中的字典：</p><figure class="table"><table><thead><tr><th>层</th><th>均方误差损失</th><th>L1损失</th><th>L0</th><th>存活百分比</th><th>损失恢复百分比</th></tr></thead><tbody><tr><td>0</td><td> 0.056</td><td> 6.132</td><td> 9.951</td><td> 0.998</td><td> 0.984</td></tr><tr><td> 1</td><td> 0.089</td><td> 6.677</td><td> 44.739</td><td> 0.887</td><td> 0.924</td></tr><tr><td> 2</td><td> 0.108</td><td> 11.44</td><td> 62.156</td><td> 0.587</td><td> 0.867</td></tr><tr><td> 3</td><td> 0.135</td><td> 23.773</td><td> 175.303</td><td> 0.588</td><td> 0.902</td></tr><tr><td> 4</td><td> 0.148</td><td> 27.084</td><td> 174.07</td><td> 0.806</td><td> 0.927</td></tr><tr><td> 5</td><td> 0.179</td><td> 47.126</td><td> 235.05</td><td> 0.672</td><td> 0.972</td></tr></tbody></table></figure><p>对于<code>1_32768</code>集中的字典：</p><figure class="table"><table><thead><tr><th>层</th><th>均方误差损失</th><th>L1损失</th><th>L0</th><th>存活百分比</th><th>损失恢复百分比</th></tr></thead><tbody><tr><td>0</td><td> 0.09</td><td> 4.32</td><td> 2.873</td><td> 0.174</td><td> 0.946</td></tr><tr><td> 1</td><td> 0.13</td><td> 2.798</td><td> 11.256</td><td> 0.159</td><td> 0.768</td></tr><tr><td> 2</td><td> 0.152</td><td> 6.151</td><td> 16.381</td><td> 0.118</td><td> 0.724</td></tr><tr><td> 3</td><td> 0.211</td><td> 11.571</td><td> 39.863</td><td> 0.226</td><td> 0.765</td></tr><tr><td> 4</td><td> 0.222</td><td> 13.665</td><td> 29.235</td><td> 0.19</td><td> 0.816</td></tr><tr><td> 5</td><td> 0.265</td><td> 26.4</td><td> 43.846</td><td> 0.13</td><td> 0.931</td></tr></tbody></table></figure><p>这是一些特征频率的直方图。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bwz8jq2puk7bt2v5i30r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/y1doqsgw6saydkevmegk 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bxmnx1a065hxjb1kxm7w 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/amxktqpnee5yslwybgjr 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/nqczszdt53rrze0yyqjn 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/wt8mfxq5hu3so0xkzhlp 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/ypwhg5ljidk3ihwawra9 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/yew2oxwaf2h3qktmyzdz 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/cl13rve82dtm4e7n5rvg 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/t3gi9wznfqvmwt5g5hjk 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/lgksrskpkvezs1uxrf7v 1920w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/xbofxvy4qa4st0bujv2y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bswlwokzl9cxxohyp3wl 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/ygciqg8mfkawmroztrlh 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/faowowuclak0zfo8nqa9 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/lrnmhst9cfd5ly40moe9 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/p0x4nejio2vg2ewj7jlh 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/bmbi0dnpjkbgs3fclthv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/kb17kbppyw7ykj0zrnou 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/uiwq1n28muzqmvbnwwyl 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/syvuw8hizr4ltbmlchyf 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AaoWLcmpY3LKvtdyq/heoaxgupuv2ku1zzul2w 1920w"></figure><p>总的来说，我认为这些词典还不错，但并不令人惊奇。</p><p>我们训练这些字典是因为我们想要进行字典学习的下游应用，但缺乏字典。这些词典足以让我们开始我们的主线项目，但我预计不久之后我们将回来训练一些更好的词典（我们也将开源）。我认为对其他人来说也是如此：这些词典应该足以开始需要词典的项目；当以后有更好的词典可用时，您可以更换它们以获得最佳结果。</p><p>关于这些词典的一些杂项注释（您可以在<a href="https://github.com/saprmarks/dictionary_learning">repo</a>中找到更多信息）。</p><ul><li> <code>1_32768</code>的 L1 惩罚似乎太大了；只有10-20%的神经元还活着，恢复的损失就更严重了。也就是说，我们会注意到，在检查了两组词典的特征后， <code>1_32768</code>集中的词典似乎比<code>0_8192</code>集中的词典具有更多可解释的特征（尽管很难说）。<ul><li>特别是，我们怀疑对于<code>0_8192</code> ，后面层中的许多高频特征是无法解释的，但对重建激活有很大帮助，<strong>从而产生看似漂亮的统计数据</strong>。 （请参阅下面有关神经元重采样和双峰性的要点。）</li></ul></li><li>随着我们逐层推进，字典在大多数指标上往往会变得更糟（恢复损失百分比除外）。这可能与当人们穿过 pythia 模型各层时激活本身的规模不断扩大有关（感谢 Arthur Conmy 提出这一假设）。</li><li>我们注意到，我们的字典特征的频率明显高于<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Anthropic</a>和<a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">Neel Nanda</a>中的特征。我们不知道这种差异是因为我们正在使用多层模型还是因为超参数的差异。我们通常怀疑如果我们学习频率较低的特征会更好。<ul><li>然而，我们会注意到，在第 0 层之后，我们的许多功能似乎并不具有“总是在特定令牌上触发”的形式，而 Anthropic 的许多功能都是如此。因此，更有趣的功能也可能出现频率更高。看看<a href="https://www.lesswrong.com/posts/tEPHGZAb63dfq2v8n/?commentId=zXEsbbJHsg98FY6uj">这里</a>有一些味道。</li></ul></li><li>我们不确定，但<code>0_8192</code>的直方图中的双峰性可能是由于死亡神经元被重新采样所致。我们每 30000 个步骤重新采样一次，包括 100000 个总步骤中的第 90000 个步骤。重采样的特征往往频率非常高，峰值可能需要超过 10000 步才能向左移动。</li></ul><h1>字典学习库</h1><p>同样，这可以<a href="https://github.com/saprmarks/dictionary_learning">在这里</a>找到。我们遵循<a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder">Anthropic 论文</a>中详细介绍的方法（包括使用不受限的编码器/解码器权重、限制解码器向量具有单位范数，以及根据其古怪的方案对死亡神经元进行重新采样），但以下情况除外：</p><ul><li>我们没有足够的空间来存储整个数据集的激活，因此，按照<a href="https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s">Neel Nanda 的复制</a>，我们维护一个来自几千个上下文的令牌缓冲区，并从该缓冲区中随机采样，直到它是半空的（此时我们刷新它带有来自新上下文的标记）。</li><li>我们使用简短的线性学习率预热来解决 Adam 会在前几个训练步骤中杀死太多神经元的问题，然后才有机会校准 Adam 参数。</li></ul><p> （一个简单的插件：这个存储库是使用<a href="http://nnsight.net/">nnsight</a>构建的，这是一个新的可解释性工具库（如<a href="https://github.com/neelnanda-io/TransformerLens">Transformer_lens</a>和<a href="https://github.com/davidbau/baukit">baukit</a> ），由 Jaden Fiotto-Kaufman 和<a href="https://baulab.info/">Bau 实验室</a>的其他人开发。 <code>nnsight</code>仍在开发中，所以我只建议尝试深入研究如果你对偶尔的错误、内存泄漏等没问题的话，现在就进入它（你可以在<a href="https://discord.gg/JqMpyYtS">这个 Discord 服务器</a>的反馈通道中报告）。但总的来说，我对这个项目非常兴奋——除了提供一个非常干净的用户之外根据经验，一个主要设计目标是<code>nnsight</code>代码具有高度<i>可移植性</i>：理想情况下，您应该能够使用 Pythia-70m 制作实验原型，无缝切换到跨多个 GPU 的 LLaMA-2-70B 上运行，然后将代码发送到人择将在克劳德身上运行。）</p><p>除了主线功能之外，我们的存储库还支持一些实验性功能，我们简要研究了这些功能作为训练词典的替代方法：</p><ul><li> <strong>MLP 担架。</strong>基于人们可能能够识别“<a href="https://transformer-circuits.pub/2022/toy_model/index.html">足够大的模型中的神经元</a>”特征的观点，我们尝试训练“自动编码器”，以给定 MLP<i>输入</i>激活<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>作为输入， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="MLP(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">输出</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">MLP</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">（</span></span></span></span></span></span></span> MLP 输出） ）。例如，给定一个 MLP，它将 512 维输入<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span></span></span></span></span></span>映射到 1024 维隐藏状态<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span> ，然后映射到 512 维输出<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> ，我们训练一个隐藏维度为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="16384 = 16 \times 1024"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16384</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">16</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1024</span></span>的</span></span></span></span></span>字典<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> ，使得<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A(x)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="y"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span></span></span></span></span></span> （并且像往常一样，因此字典的隐藏状态是稀疏的）。<ul><li>最终的词典看起来不错，但我们决定不再进一步追求这个想法。</li><li> （向 Max Li 提出此建议。）</li></ul></li><li><strong>用熵代替 L1 损失</strong>。基于这篇<a href="https://transformer-circuits.pub/2023/may-update/index.html#simple-factorization">文章</a>中的想法，我们尝试使用熵来规范字典的隐藏状态而不是 L1 损失。这似乎导致这些功能要么是死功能（从未触发），要么是在几乎每个输入上触发的非常高频的功能，这不是所需的行为。但似乎有一种方法可以让这项工作变得更好。</li></ul><p>如果您想追求上述要点中的想法之一，请在获得初步结果后与我（Sam）联系 - 我可能有兴趣讨论结果或合作。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxki2bn9t6a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxki2bn9t6a">^</a></strong></sup></span><div class="footnote-content"><p>这既是为了可重复性，也是因为每个字典都需要一些努力来训练。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp7b5kczep0n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp7b5kczep0n">^</a></strong></sup></span><div class="footnote-content"><p>当然，来自<a href="https://arxiv.org/abs/2309.08600">Cunningham 等人的存储库。纸张</a>也可以<a href="https://github.com/HoagyC/sparse_coding">在这里</a>购买。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning<guid ispermalink="false"> AaoWLcmpY3LKvtdyq</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Tue, 05 Dec 2023 06:05:21 GMT</pubDate> </item><item><title><![CDATA[The LessWrong 2022 Review]]></title><description><![CDATA[Published on December 5, 2023 4:00 AM GMT<br/><br/><p>雪花飘落，颂歌开始响起，我们都知道是时候开始我们最喜欢的冬季假期传统了。审稿时间少了！ </p><figure class="image image_resized" style="width:69.55%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/swcjknwqnq58jqjizlbu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/iunw2eqowsdnnqfksyra 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/gx5phndd8vqwcthofkdb 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/wmh2aidexazx07hj2dfu 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/ahkmuaycxg9peesnssqr 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/ktgsgz7mjgpix2gho83k 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/zu2vamwcsp7qmxac899u 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/q4dfchxbyjvzz33q0olz 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/h8sonntrtlw1vx8tfek1 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/jlbb43705pckl5ufchsf 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/pqylqykbwbqxdf0ebiud 1024w"></figure><p>每年我们都会聚在一起审查至少一年前的帖子。这意味着在接下来的两个月里，我们将审核 2022 年的所有帖子。</p><p>虽然我们的日常生活充满了时尚，追逐着业力和社会认可的甜蜜滋味，但LessWrong的评论是时候退一步问自己“这真的能帮助我更好地思考吗？”，“这真的是事实吗？变得有价值？”以及“哪些事情经受住了进一步和广泛的审查？”。</p><p>到目前为止，我们已经这样做了 4 次（ <a href="https://www.lesswrong.com/posts/3yqf6zJSwBF34Zbys/2018-review-voting-results">2018 年</a>、 <a href="https://www.lesswrong.com/posts/kdGSTBj3NA2Go3XaE/2019-review-voting-results">2019 年</a>、 <a href="https://www.lesswrong.com/posts/TSaJ9Zcvc3KWh3bjX/voting-results-for-the-2020-review">2020 年</a>、 <a href="https://www.lesswrong.com/posts/zajNa9fdr8JYJpxrG/voting-results-for-the-2021-review">2021 年</a>）。</p><p>年度审核如何运作的完整技术细节位于本文的<a href="https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review#How_does_the_review_work_">最后部分</a>，但与过去几年基本相同。分为三个阶段：</p><ol><li><strong>初步投票阶段</strong><i>（2 周，12 月 4 日至 17 日）</i> ：我们在进行初步投票的审核中确定特别值得考虑的帖子。获得 2 票初步投票的帖子进入讨论阶段。</li><li><strong>讨论阶段</strong><i>（4 周，12 月 17 日 — 1 月 14 日）</i> ：<i> </i>我们审查和辩论帖子。收到至少一篇书面评论的帖子将进入最终投票阶段。</li><li><strong>最终投票</strong><i>（2 周，1 月 14 日至 1 月 28 日）</i> ：我们使用二次投票进行完整投票。其结果决定年度评审结果。</li></ol><p>有关年度审核理念的更多信息，请参阅之前的公告帖子：<a href="https://www.lesswrong.com/posts/qXwmMkEBLL59NkvYR/the-lesswrong-2018-review">此处</a>、 <a href="https://www.lesswrong.com/posts/QFBEjjAvT6KbaA3dY/the-lesswrong-2019-review#Improving_our_incentives_and_rewards">此处</a>、<a href="https://www.lesswrong.com/posts/M9kDqF2fn3WH44nrv/the-2020-review">此处</a>和<a href="https://www.lesswrong.com/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion">此处</a>。</p><h2>入门</h2><p>在任何符合审核资格的帖子的顶部，您都会看到： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672905457/mirroredImages/qCc7tm29Guhz6mtf7/d0fpa6iead1yz8j7y10n.png"></figure><p>这些将是您对 2022 年审核的初步投票。帖子需要获得至少 2 票初步投票（正面或负面）才能进入下一阶段的审核。</p><p>要开始仔细阅读帖子，我建议转到<a href="https://www.lesswrong.com/allPosts?timeframe=yearly&amp;after=2022-01-01&amp;before=2023-01-01&amp;limit=100&amp;sortedBy=top&amp;filter=unnominated&amp;includeShortform=false"><u>“2022 年所有帖子”页面</u></a>，或<a href="https://www.lesswrong.com/votesByYear/2022"><u>“查看您过去的点赞”</u></a>页面。<i>注意：只有2022年1月之前注册账户的用户才有资格投票。</i></p><h2>今年没有书了，抱歉各位</h2><p>2018年、2019年和2020年，我们印制了审查结果的书籍。我们已经售出了数千件，我为它们感到非常自豪，很多人告诉我，这些是他们最喜欢的东西之一： </p><figure class="table"><table style="border-color:white;border-style:solid"><tbody><tr><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/hkxvgpqalnv6oevaxned"><figcaption> 2018：反映领土的地图（ <a href="https://www.amazon.com/Map-that-Reflects-Territory-LessWrong/dp/1736128507/ref=sr_1_1?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728381&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-1">亚马逊</a>） </figcaption></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/kcpulhgcmm5hsxlnaxga"><figcaption> 2019：认知引擎（ <a href="https://www.amazon.com/Engines-Cognition-Essays-LessWrong-Community/dp/1736128515/ref=sr_1_2?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728381&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-2">亚马逊</a>） </figcaption></figure></td><td style="border-color:hsl(0, 0%, 100%);border-style:solid"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B6CxEApaatATzown6/noak9x8qlrktfpottnxx"><figcaption> 2020：现实的雕刻（ <a href="https://www.amazon.com/Carving-Reality-Essays-LessWrong-Community/dp/B0C95MJJBK/ref=sr_1_5?crid=5YSFIY94WGVQ&amp;keywords=lesswrong+books&amp;qid=1701728398&amp;sprefix=lesswrong+book%2Caps%2C145&amp;sr=8-5">亚马逊</a>）</figcaption></figure></td></tr></tbody></table></figure><p>遗憾的是，今年不会有书（也不会有 2021 年的书评）。随着我们许多其他项目的需求不断增加（以及资金的减少，因为如果考虑到每年 4-5 个员工月的制作成本，我们在以下方面净损失了资金）这些）。</p><p>我正在考虑其他方法来创建一个易于参考的工件，以捕获今年和去年的审查结果。我认为我想做的最低限度是创建一本好的电子书，也许还可以使用我们的机器旁白（或进行人类旁白）制作一个有声版本。欢迎提出其他建议。</p><p>我们将在接下来的几天内对前几年的所有书籍进行圣诞特卖，希望在圣诞节之前我们还将推出一本包含去年评论结果的优秀电子书（甚至可能是有声读物版本）。</p><h1>审核如何进行？</h1><h2>第一阶段：初步投票</h2><p>要提名职位，请对其进行初步投票。符合资格的选民将看到此用户界面： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672905457/mirroredImages/qCc7tm29Guhz6mtf7/d0fpa6iead1yz8j7y10n.png"></figure><p>如果您认为某个帖子是重要的智力贡献，您可以投票表明其大致的重要性。对于一些粗略的指导：</p><ul><li> 1 票表示“这很好”。</li><li> 4 票意味着“这非常重要”。</li><li> 9 票意味着它是“智力进步的一个重要部分”。</li></ul><p>您可以在帖子页面的顶部或帖子出现在列表中的任何位置（例如<a href="https://www.lesswrong.com/allPosts?timeframe=yearly&amp;after=2022-01-01&amp;before=2023-01-01&amp;limit=100&amp;sortedBy=top&amp;filter=unnominated&amp;includeShortform=false"><u>“所有帖子”页面</u></a>或新的<a href="https://www.lesswrong.com/votesByYear/2022"><u>“查看您过去的投票</u></a>”页面）进行投票。</p><p>获得至少一票赞成票的帖子将进入<a href="https://lesswrong.com/reviewVoting/2022">投票仪表板</a>，其他用户可以在其中投票。我们鼓励您根据去年的记忆至少进行一次粗略的投票。稍后改变主意是可以的（鼓励！）。</p><p><strong>写一篇简短的评论</strong></p><p>如果您认为某篇文章很重要，我们还鼓励您至少写一篇简短的评论，说明该文章的突出之处及其重要性。 （如果您想先记下您的快速印象，然后再进行更详细的审核，欢迎您对一篇文章进行多篇评论）</p><p>至少有一条评论的帖子会被排序到<a href="https://www.lesswrong.com/reviewVoting">要投票的帖子列表</a>的顶部，因此，如果您希望某个帖子获得更多关注，对其进行评论会很有帮助。</p><p><strong>为什么要进行初步投票？为什么要分两个投票阶段？</strong></p><p>每年，LessWrong 上都会写出更多的帖子。 2018 年第一次审查考虑了 1,500 个职位。 2021 年，这一数字为 4,250。处理这么多帖子是一项艰巨的工作。</p><p>初步投票旨在帮助处理帖子数量的增加。我们不是简单地提名职位，而是直接从投票开始。这些初步投票随后将被公布，只有至少两人投票的帖子才会进入下一轮。</p><p>在审核阶段，这使得各个网站成员能够注意到某些内容的放置是否特别不准确。如果您认为某个帖子的排名不准确，您可以写一篇积极的评论，认为它应该更高，其他人可以在最终投票时考虑这一点。获得大量中等选票的帖子可能会在审核阶段被取消优先级，从而使我们能够专注于最有可能对最终结果产生影响的对话。</p><p><strong>初步投票是如何计算的？</strong></p><p>您可以投不限数量的选票，但超过一定阈值后，您的选票总分越大，您每张选票的影响力就越小。在后端，我们使用<a href="https://www.lesswrong.com/posts/qQ7oJwnH9kkmKm2dC/feedback-request-quadratic-voting-for-the-2018-review"><u>修改后的二次投票系统</u></a>，该系统根据投票的强度在您的投票中分配固定数量的分数。</p><p><i>细节：1 票得 1 分。 4 票得 10 分。 9 票需要 45 分。如果您花费的积分超过 500 点，您的投票就会开始按比例变弱。</i></p><h2>第二阶段：评论</h2><p>第二阶段为期一个月，完全专注于撰写评论。评论是评估帖子的特殊评论。评论中需要回答的好问题包括：</p><ul><li>这篇文章给对话添加了什么？</li><li>这篇文章对您、您的想法和行动有何影响？</li><li>它的主张是否准确？它是否在关节处雕刻了现实？你怎么知道？</li><li>您可以测试这篇文章的从属声明吗？</li><li>您希望在这篇文章的基础上看到哪些后续工作？</li></ul><h2>第三阶段：最终投票</h2><p>至少收到一项审核的帖子将进入最终投票阶段。</p><p>用户界面将要求选民在最终确定对每个帖子的投票之前至少简要浏览一下评论，因此可以考虑有关每个帖子的争论。</p><p>和往年一样，我们将公布1000+karma的用户以及所有用户的投票结果。 LessWrong 审核团队将把投票结果作为将哪些帖子纳入 2022 年最佳帖子序列的有力指标。</p><p><strong>首先，您可以</strong><a href="https://www.lesswrong.com/votesByYear/2022"><strong><u>查看您过去的点赞</u></strong></a><strong>并开始对某些帖子进行投票。</strong></p><br/><br/><a href="https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B6CxEApaatATzown6/the-lesswrong-2022-review<guid ispermalink="false"> B6CxEApaatATzown6</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Tue, 05 Dec 2023 04:00:00 GMT</pubDate> </item><item><title><![CDATA[Bands And Low-stakes Dances]]></title><description><![CDATA[Published on December 5, 2023 3:50 AM GMT<br/><br/><p><span>当我开始在波士顿地区参加反对派舞会时，有几种你可以考虑“低风险”的地区舞会。小型舞会，费用不高，老牌乐队的热情也较低。因此，他们相对愿意预订那些刚刚起步、友好、鼓励、能够容忍缺乏经验的错误的乐队。</span></p><p>我认为这就是为什么这么多实力雄厚的乐队和舞蹈音乐家走出这个领域的一个重要原因：不需要你足够优秀，能够在“大型”舞蹈中保持自己的风格，就可以开始获得作为乐队跳舞的经验。 。例如，回顾一下我的日历，在我们玩我们的第一个“大型”游戏（<a href="https://challcontra.weebly.com/">协和星期五</a>）之前， <a href="https://www.freeraisins.com/">Free Raisins</a>玩了十八个不同的晚上。</p><p>不幸的是，这比以前少了很多。一些在大流行前关闭了（我非常想念麻省理工学院的反对派舞蹈！），其他人还没有回来（还没有？）或者已经转向室内乐队。有<a href="https://www.bidadance.org/">BIDA</a><a href="https://www.jefftk.com/p/why-does-the-bida-open-band-work-well">开放乐队</a>和（风险更低的）家庭舞蹈乐队，但虽然我认为这种经验也很有价值，但有一种不同的学习方式，来自于在小组中演奏、演奏你已经练习过的曲目，以及对音乐完全负责。</p><p>我在这里没有一个很好的解决方案：很难故意开始一些低风险的事情，而且人们通常更喜欢举办那种很多人都想参加并且有很棒音乐的活动。但我确实认为 2010 年代初期的环境非常特别，为新乐队提供了很多学习成为舞蹈乐队的机会，如果我们能够带回类似的东西，或者至少具有类似效果的东西，我会很高兴。</p><p> （这种同样的动力对于“技术魂斗罗”来说更加强烈：唯一的表演机会是重大的特别活动，甚至更高调！但我对这种舞蹈形式的成功投入也较少，所以这不太困扰我。 ）</p><br/><br/><a href="https://www.lesswrong.com/posts/atwkvWcSppkKN9dRJ/bands-and-low-stakes-dances#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/atwkvWcSppkKN9dRJ/bands-and-low-stakes-dances<guid ispermalink="false"> atwkvWcSppkKN9dRJ</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Tue, 05 Dec 2023 03:50:22 GMT</pubDate> </item><item><title><![CDATA[Accelerating science through evolvable institutions]]></title><description><![CDATA[Published on December 4, 2023 11:21 PM GMT<br/><br/><p><i>这是向圣达菲研究所“加速科学”工作组提交的演讲的书面版本。</i></p><p>我们来这里是为了讨论“加速科学发展”。我喜欢从历史的角度来开始讨论这样的话题：科学在过去什么时候（如果有的话）加速了？现在还在加速吗？我们可以从中学到什么？</p><p>我认为，在整个人类历史中，科学以及更广泛的人类知识<i>一直</i>在加速发展。我还不能证明这一点（而且我自己对此只有大约 90% 的把握），但让我诉诸你的直觉：</p><ul><li><a href="https://en.wikipedia.org/wiki/Behavioral_modernity">从行为上看，现代人类</a>已有 50,000 多年的历史</li><li>文字只有大约 5000 年的历史，因此在人类时间线的 90% 以上，我们只能积累能够适应口头传统的知识</li><li>在古代和中世纪世界，我们只有少数几门科学：天文学、几何学、一些数论、一些光学、一些解剖学</li><li>在科学革命之后的几个世纪（大约 1500 年代至 1700 年代），我们得到了日心说、运动定律、万有引力理论、化学的起源、细胞的发现、更好的光学理论</li><li>在 1800 年代，事情真正开始发展，我们有了电磁学、原子理论、进化论、细菌理论</li><li>1900 年代，核物理、量子物理、相对论、分子生物学和遗传学继续蓬勃发展</li></ul><p>我把自 1950 年左右以来科学是否已经放缓的问题放在一边，我对此没有强烈的看法。即使确实如此，这也只是整个历史加速的总体模式中最近的一个小插曲。 （或者，你知道，历史上前所未有的逆转和衰退的开始。其中之一。）</p><p>我对这种加速模式深信不疑的部分原因是，加速的不仅仅是科学：几乎所有衡量人类进步的指标都显示出相同的趋势，包括<a href="https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia?yScale=log">世界 GDP</a>和<a href="https://ourworldindata.org/grapher/population?yScale=log&amp;country=~OWID_WRL">世界人口</a>。</p><p>是什么推动了科学的加速发展？许多因素，包括：</p><ul><li><strong>资金。</strong>曾经，科学家必须<a href="https://rootsofprogress.org/funding-models-for-science-and-innovation">寻求赞助，或者独立致富</a>。现在有可用的赠款，并且资金总额在过去几十年中大幅增加： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/dhxfk91oznfay66wdxnn" alt=""><figcaption><a href="https://www.aaas.org/programs/r-d-budget-and-policy/historical-trends-federal-rd"><i>美国科学促进会</i></a></figcaption></figure><ul><li><strong>人们。</strong>更多的科学家（在其他条件相同的情况下）意味着科学发展得更快，科学家的数量急剧增加，这既是因为总体人口的增长，也是因为更多的劳动力进入研究领域。在<a href="https://archive.org/details/sciencesincebaby0000pric/page/107/mode/1up?view=theater"><i>《自巴比伦以来的科学》一</i></a>书中，德里克·J·德·索拉·普赖斯 (Derek J. de Solla Price) 表示，“历史上大约 80% 到 90% 的科学家现在还活着”，这<a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/">可能仍然是正确的</a>： </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9uEAwHzoDS8shpdoX/wh3ldfdjulipzfcrroyg" alt=""><figcaption> <a href="https://futureoflife.org/guest-post/90-of-all-the-scientists-that-ever-lived-are-alive-today/"><i>埃里克·加斯特弗兰德</i></a></figcaption></figure><ul><li><strong>仪器。</strong>更好的工具意味着我们可以做更多更好的科学研究。伽利略有一个简单的望远镜。现在我们有<a href="https://en.wikipedia.org/wiki/James_Webb_Space_Telescope">JWST</a>和<a href="https://en.wikipedia.org/wiki/LIGO">LIGO</a> 。</li><li><strong>计算。</strong>更强的计算能力意味着更多更好的数据处理方式。</li><li><strong>沟通。</strong>思想传播得越快越好，科学传播就越高效、越有效。科学期刊是在印刷机发明之后才发明的。互联网支持预印本服务器，例如 arXiv。</li><li><strong>方法。</strong>更好的方法造就更好的科学，从培根经验主义到<a href="https://en.wikipedia.org/wiki/Koch%27s_postulates">科赫假设</a>再到<a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial">随机</a>对照试验（实际上是所有统计数据）。</li><li><strong>机构。</strong>实验室、大学、期刊、资助机构等共同构成了一个支持现代科学的生态系统。</li><li><strong>社会地位。</strong>科学越受到尊重和声望，就会有越多的人和金钱流入它。</li></ul><p>现在，如果我们想问科学是否会继续加速发展，我们可以思考哪些驱动因素将继续增长。我建议：</p><ul><li>只要世界经济持续增长，科学经费就会继续增长</li><li>仪器、计算和通信将随着技术的发展而不断改进</li><li>我认为方法没有理由不继续改进，作为科学本身的一部分</li><li>科学的社会地位似乎相当强大：它是一个受人尊敬和享有盛誉的机构，获得了一些社会最高荣誉</li></ul><p>从长远来看，如果<a href="https://ourworldindata.org/grapher/comparison-of-world-population-projections">世界人口像预计的那样趋于稳定</a>，我们可能会耗尽继续扩大研究人员基础的人员，这是一个潜在的问题，但不是我今天的重点。</p><p>最大的危险信号是我们的科学机构。制度影响所有其他因素，尤其是资金和人才的管理。今天，元科学界的许多人对我们的机构感到担忧。常见的批评包括：</p><ul><li><strong>速度。</strong>获得资助很容易需要 12-18 个月的时间（如果你幸运的话）</li><li><strong>高架。</strong>研究人员通常将 30-50% 的时间花在资助上</li><li><strong>耐心。</strong>研究人员认为他们需要定期展示结果，并且不能走一条可能需要多年才能得出结果的道路</li><li><strong>风险承受能力。</strong>赠款资金倾向于保守的、渐进的建议，而不是大胆的、“高风险、高回报”的计划（尽管<a href="https://commonfund.nih.gov/highrisk">做出了相反的努力</a>）</li><li><strong>共识。</strong>一个领域可能会过快地集中于一个假设并修剪替代的研究分支</li><li><strong>研究员年龄。</strong>随着时间的推移，赠款的趋势是拨款给年龄更大、更成熟的研究人员</li><li><strong>自由。</strong>科学家缺乏完全自主地指导研究的自由；赠款资金附加太多条件</li></ul><p>现在，作为一名前科技创始人，我不禁注意到，在营利性风险投资领域，大多数问题似乎都得到了缓解。筹集风险投资资金相对较快（通常一轮融资会在几个月内完成，而不是一年或更长时间）。作为创始人/首席执行官，我花了大约 10-15% 的时间筹款，而不是 30-50%。风险投资公司大胆下注，积极寻求逆向立场，并支持年轻的新贵。他们大多给予创始人自主权，也许会在董事会中占据一席之地以进行治理，并且只有在表现非常糟糕时才会解雇首席执行官。 （上面列出的初创公司创始人可能还会抱怨的唯一问题是耐心：如果你的钱用完了，你最好能取得进展，否则你在下一轮融资时就会遇到困难。）</p><p>我不认为风险投资界在这些方面做得更好，因为风险投资家比科学资助者更聪明、更有智慧或更优秀——但事实并非如此。相反，风险投资家：</p><ul><li>争夺优惠（并且真的不想错过好优惠）</li><li>从长远来看，成功或失败取决于其投资组合的表现</li><li>在大约 5-10 年内看到这些结果</li></ul><p>简而言之，<strong>风险投资面临着进化压力。</strong>他们不能陷入明显的不良均衡，因为如果这样做，他们就会在竞争中落败并失去市场力量。</p><p>证明这一点的是风险投资在过去几十年里<i>的</i>发展——主要是朝着为创始人提供更好待遇的方向发展。例如，早期阶段存在较高估值的长期趋势，这最终意味着较低的稀释度以及权力从风投向创始人的转移：创始人在过去的几年里放弃公司一半或更多的股份是很常见的。第一轮融资；最后我检查了一下，大约是 20% 或更少。风险投资并不总是资助大学刚毕业的年轻技术人员。曾经有一段时间，他们倾向于青睐更有经验的首席执行官，或许还拥有 MBA 学位。他们并不总是支持创始人领导的公司；曾经，创始人在最初几年后被解雇并由专业首席执行官取代的情况很常见（当 A16Z 在 2009 年推出时，他们大肆宣扬<a href="https://a16z.com/why-we-prefer-founding-ceos/">他们不会这样做</a>）。</p><p>所以我认为<strong>，如果我们希望看到我们的科学机构</strong><i><strong>得到改进</strong></i><strong>，我们需要考虑它们如何</strong><i><strong>发展</strong></i><strong>。</strong></p><p>我们的科学机构的发展程度如何？不是特别的。当今大多数科学组织都是大学或政府部门。尽管我很尊重大学和政府，但我认为任何人都必须承认它们是我们行动较为缓慢的机构之一。 （大学尤其具有极强的弹性和抵抗力：例如，牛津大学和剑桥大学的历史可以追溯到中世纪，经历了帝国的兴衰，直到今天仍然完好无损。）</p><p>科学资助机构的进化所面临的挑战与风险投资的进化相反：</p><ul><li><strong>他们往往缺乏竞争，</strong>尤其是 NIH 和 NSF 等集中式联邦机构</li><li><strong>他们缺乏任何真正的反馈循环</strong>，在这种循环中，资助者的资源是由过去的判断和其投资组合的成功决定的（迈克尔·尼尔森多次<a href="https://twitter.com/michael_nielsen/status/1451626771690897408">指出</a>，从“爱因斯坦作为专利职员做了最好的工作”到“卡塔林·卡里科”的资助失败）在获得诺贝尔奖之前被拒绝获得资助和终身教职”似乎甚至没有引发相关机构内部的反思过程）</li><li><strong>他们需要很长的周期</strong>才能了解其工作的真正影响，而这种影响可能需要 20-30 年才能显现出来</li></ul><p>我们如何提高科学经费的可进化性？我们应该思考如何改善这些因素。我没有什么好主意，但我会抛出一些不成熟的想法来开始对话：</p><p><strong>我们如何增加科学资助的竞争？</strong>我们可以增强慈善事业的作用。在美国，我们可以将联邦资金转移到州一级，设立五十个资助者而不是一个。 （国家农业实验站就是一个成功的例子，这些实验站之间的竞争是杂交玉米研究的关键，这是 20 世纪农业科学最伟大的成功之一。）在国际层面，我们可以支持对科学家更加开放的移民。</p><p><strong>我们如何创建更好的反馈循环？</strong>这很困难，因为我们需要某种方法来衡量结果。实现这一目标的一种方法是将资金从预期赠款转向各级各种回顾性奖项。如果这个“经济”足够大和强大，这些成果就可以被金融化，以创建一个动态的、有竞争力的融资生态系统，并具有适当水平的风险承担和耐心，经验丰富的退伍军人与年轻特立独行者之间的适当平衡等.（ <a href="https://forum.effectivealtruism.org/posts/r7vmtHZKuosJZ3Xq5/altruistic-equity-allocation">影响证书</a>，例如<a href="https://protocol.ai/blog/hypercert-new-primitive/">超级证书</a>，可以成为该解决方案的一部分。）</p><p><strong>我们如何解决反馈周期长的问题？</strong>我不知道。如果我们不能缩短周期，也许我们需要延长资助者的职业生涯，这样他们至少可以从几个周期中学习——这<a href="https://rootsofprogress.org/how-curing-aging-could-help-progress">是长寿技术的潜在好处</a>。或者，也许我们需要一个科学资助者，它可以极快地学习，可以消耗大量有关研究项目及其最终结果的历史信息，永远不会忘记其经历，并且永远不会退休或死亡——当然，我想到的是人工智能。关于人工智能支持、增强或取代科学研究人员本身的讨论很多，但人工智能在科学领域的最大机会可能是在资金和管理方面。</p><p>我怀疑资助机构会在这个方向上走得太远：它们必须自愿接受竞争、加强问责并承认错误，而这种情况很少见。 （看看现在那些因卡里科获得诺贝尔奖而获得功劳的机构，他们几乎没有为她提供支持。）如果机构很难进化，那么元进化就更难了。</p><p>但也许资助者背后的资助者，即那些向资助者提供预算的资助者，可以开始将资金分配给多个机构，以要求绩效指标，或者干脆转向上述回顾性模式。这可以提供所需的进化压力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9uEAwHzoDS8shpdoX/accelerating-science-through-evolvable-institutions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9uEAwHhoDS8shpdoX/acceleating-science-through-evolvable-institutions<guid ispermalink="false"> 9uEAwHzoDS8shpdoX</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:21:35 GMT</pubDate> </item><item><title><![CDATA[Speaking to Congressional staffers about AI risk]]></title><description><![CDATA[Published on December 4, 2023 11:08 PM GMT<br/><br/><p> 2023 年 5 月和 6 月，我（Akash）与国会工作人员就人工智能风险举行了大约 50-70 次会议。我一直想写一篇文章来反思这次经历和我的一些收获，我认为这可能是 LessWrong 对话的一个好话题。我看到他们<a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1?commentId=L2qFjT8taEhkm4hCB">提出要与人们进行 LW 对话</a>，于是我伸出了援手。</p><p>在这次对话中，我们讨论了我如何决定与工作人员聊天、我在华盛顿的初步观察、有关国会办公室如何工作的一些背景、我的会议是什么样的、我学到的教训以及关于我的经历的一些杂项。</p><h2>语境</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:14:27 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:14:27 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>嘿！在您的留言中，您提到了一些与您在华盛顿的经历相关的主题。</p><p>我认为我们应该从您与国会办公室谈论人工智能风险的经历开始。我很有兴趣了解更多；似乎没有太多公共资源来说明这种外展活动是什么样的。</p><p>那是怎么开始的？是什么让你想这么做？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:23:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:23:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>2023 年 3 月，我开始在<a href="https://www.safe.ai/">人工智能安全中心</a>从事一些人工智能治理项目。我的一个项目涉及帮助 CAIS 响应<a href="https://www.ntia.gov/issues/artificial-intelligence/request-for-comments">NTIA</a>发布的关于人工智能问责制的评论请求。</p><p>作为这项工作的一部分，<strong>我开始思考一个好的前沿人工智能监管框架应该是什么样子。</strong>例如：如果我可以为前沿人工智能系统建立许可制度，它会是什么样子？它会被安置在美国政府的什么地方？我希望它评估哪些信息？</p><p><strong>我开始想知道实际的政策制定者会对这些想法有何反应</strong>。我也很好奇更多地了解政策制定者如何考虑人工智能灭绝风险和灾难性风险。</p><p>我开始询问人工智能治理领域的其他人。绝大多数人（根本）没有与国会工作人员交谈过。一些人有与员工交谈的经验，但没有与他们谈论人工智能风险。很多人告诉我，他们认为与政策制定者的接触非常重要，但却被忽视了。当然，也存在下行风险，所以你不希望有人做得不好。</p><p>在咨询了 10-20 名人工智能治理人员后，我询问 CAIS 我是否可以去华盛顿并开始与国会办公室交谈。目标是（a）提高对人工智能风险的认识，（b）更好地了解国会办公室如何考虑人工智能风险，（c）更好地了解国会办公室的人们有哪些与人工智能相关的优先事项，以及 (d) 获取有关我的 NTIA 评论想法请求的反馈。</p><p> CAIS 批准了，我于 2023 年 5 月至 6 月去了华盛顿。需要澄清的是，这不是 CAIS 告诉我要做的事情——这更像是 CAIS 意识到正在发生的“阿卡什事件”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:26:38 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:26:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>哇，这真的很有趣。几个随机问题：</p><blockquote><p>当然，也存在下行风险，所以你不希望有人做得不好。</p></blockquote><p>一个人怎样才能把一件事做得不差呢？如何学习与政策制定者互动？<br><br>另外，你的背景是什么？在此之前您做过政策方面的工作吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:31:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:31:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>是的，很好的问题。我不确定最好的学习方法是什么，但我尝试过以下一些方法：</p><ul><li>与有与政策制定者互动经验的<strong>人交谈</strong>。询问他们说什么、他们发现什么令人惊讶、他们犯了什么错误、他们注意到什么下行风险等等。</li><li><strong>看书</strong>。我发现<a href="https://www.amazon.com/Master-Senate-Years-Lyndon-Johnson/dp/0394720954">参议院议长</a>和<a href="https://www.amazon.co.uk/Act-Congress-Americas-Essential-Institution/dp/0307744515">国会法案</a>特别有帮助。我目前正在阅读<a href="https://www.amazon.com/Devils-Chessboard-Dulles-Americas-Government/dp/0062276174">《魔鬼的棋盘》，</a>以更好地了解中央情报局和情报机构，到目前为止，我发现它内容丰富。</li><li>与你已经认识的政策制定者<strong>进行角色扮演</strong>，并要求他们提供直率的反馈。</li><li>在低风险会议中<strong>进行练习</strong>，并利用这些经验进行迭代。</li></ul><p>在此之前我没有做过太多政策方面的事情。在大学里，我为《哈佛政治评论》撰稿，并参与了政治研究所的工作，但这比“现实世界的政策参与”的内容更具学术性。</p></div></section><h2>抵达华盛顿特区并进行初步观察</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:32:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:32:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>这一切都是有道理的。到达华盛顿后你做了什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:37:08 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:37:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我给国会办公室以及一些行政部门的人员发送了冷电子邮件。我还联系了华盛顿的一些 EA。我还继续处理 NTIA 征求意见书（截止日期为 6 月 6 日）。</p><p>最初的计划是召开几次会议，评估会议的进展情况，如果我认为进展相当顺利，则再召开更多会议。</p><p>总的来说，我最终与国会工作人员举行了大约 50-70 次会议（以及一些与智库人员和行政部门机构人员的会议，但我将在这篇文章中重点讨论与国会工作人员的会议）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:37:28 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:37:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>我认为他们进展得相当顺利，那么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:43:42 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:43:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>据我说，是的！我要注意的一件事是，这些可能有点难以评估——比如，员工应该对人友善，他们不会说“我以为你是个白痴”或“你浪费了我的时间”之类的话。时间”或“我现在对人工智能安全性的印象更差了。”</p><p>记住这一点：</p><ul><li><strong>我对员工们的开放态度感到惊讶。</strong>奥弗顿之窗最近发生了很大的变化，但当时，我真的不知道人们是否会说“哈！<i>灭绝风险？</i>这听起来像科幻小说。”</li><li><strong>主导氛围是“人工智能非常重要，我是一名忙碌的员工，有 100 个优先事项，所以我没有时间了解它。我</strong>真的很高兴能与能够告诉我有关人工智能的东西的人交谈– 我一直渴望跟上进度。”</li><li><strong>员工们对有机会见到愿意回答有关人工智能基本问题的人表示非常感激</strong>（例如，什么是大型语言模型，它与其他类型的人工智能有何不同？有多少公司从事前沿人工智能？）</li><li>有一些“有形”的信号表明情况进展顺利。例如，一些工作人员向我介绍了他们认识的其他人，有些人向我发送了工作，他们的办公室正在起草，甚至有几个人甚至向我介绍了国会议员（总共两个）。</li></ul></div></section><h2>国会办公室的等级制度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:44:47 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:44:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>这真的很有趣！<br><br>简而言之，您能为我画一张国会办公室在人员配置方面的样子吗？就像，您通常与谁交谈，以及他们通常与国会议员有什么关系？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:50:33 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:50:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>好问题！因此，我的理解是，国会办公室通常具有以下角色，从“最有影响力的”到“最不利影响”：</p><ul><li>参谋长</li><li>立法主任</li><li>立法助理</li><li>立法通讯员</li><li>实习生和研究员</li></ul><p>也有其他角色，但是从立法角度来看，这些角色往往最重要。</p><p>请注意，每个办公室都有自己的氛围。曾经有人告诉我：“每个国会办公室都是自己的创业公司，每个国会议员都可以根据需要跑出办公室。”</p><p>因此，在某些办公室中，实习生和研究员实际上可能会产生很大的影响（例如，如果国会议员或立法主任信任实习生是特定主题的主题专家）。但是总的来说，我认为这个层次结构很普遍。</p><p>我认为我主要与立法助理/立法通讯员级别的人交谈。我还与一些立法董事进行了交谈。</p></div></section><h2>向办公室推广</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 19:51:24 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 19:51:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>好的，这一切都是有道理的。那么，您最终是如何从几次会议到60-80的呢？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 19:55:30 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 19:55:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>我向技术政策工作人员发送了一封大规模的电子邮件，回答的人数给我留下了深刻的印象。这封电子邮件很短，提到我在CAIS，有1-2个子弹关于CAI的作用，并且对我正在研究NTIA的置评请求。</p><p>我认为国会工作人员现在对AI内容非常感兴趣，这是/确实是这种情况。就像，如果我向人们发送有关其他问题的电子邮件，我认为我将无法参加很多会议。</p><p>有些感觉是“ AI现在很热，但是没有人真正了解AI”。我认为目前尚不清楚持续多长时间（尤其是“人们不知道太多，而且办公室没有下定决心”）。</p><p>我会说“我认为这是一个整个AIS社区可以/应该利用更多的机会”之类的话。”就像，国会工作人员（而且我认为仍然）对与AI的人互动非常感兴趣 - 很难想象AIS社区中的人们能够获得一个更好的机会并能够担任顾问/倡导者。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:20:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:20:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>这就说得通了。</p><p>如何开始与国会工作人员互动？一个人应该做些什么来进入该空间/哪些组织可能有充分的态度来部署人们？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 20:38:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 20:38:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>这将是一个非常模糊的答案，但我认为这在很大程度上取决于人，他们的技能和政策目标。</p><p>另外 - 我在上面提到了这一点，但重申一下很重要 - 人们做这类工作的人肯定会有风险。另一方面，存在太“行动偏见”的风险，或类似的东西，并且在桌子上留下了很多价值。</p><p>这确实很难且令人困惑。我之前提到，我咨询了10-20 AI治理人员。他们中的大多数人就像“这似乎很重要，而且被忽略了，但是IDK Man似乎令人困惑。”他们中的一些人就像“是的，我完全认为您应该这样做，尤其是如果您采用XYZ战术。”一个相当杰出的AI治理人员明确告诉我，他们不想我这样做。我发现很难平衡这种矛盾的反馈。</p><p>我还认为我的很多建议将取决于某人想说的话 - 例如：</p><ul><li>他们的音高是什么？如果开会开始，工作人员说：“那么，您想谈论什么？”，最初的回应是什么？</li><li>他们是那种擅长提出问题并对其他人的世界观感到好奇的人吗？</li><li>他们要去听起来警报吗？</li><li>他们知道有关AI的很多事实吗？当他们不知道某事时，他们是否能够认识到这一点并适当地对冲？</li></ul><p>考虑到所有这些，如果阅读此书的人对与国会工作人员互动（或者在他们的组织中做到这一点）感兴趣，并且他们珍视我的意见，<strong>我建议他们通过LW与我联系。</strong>我可以在更多背景下提供更好的建议。</p></div></section><h2>典型的会议</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 06 Nov 2023 20:47:09 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 06 Nov 2023 20:47:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>是的，这一切都是有道理的。您能带我参加一次典型的会议吗？就像，您将如何首先与员工接触，您会在哪里遇到他们，实际对话是什么样的，您将如何跟进或以其他方式弄清楚它是否有帮助？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 06 Nov 2023 21:47:20 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 06 Nov 2023 21:47:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>会议物流</strong></p><ol><li>会通过电子邮件联系</li><li>通常会在国会办公室与他们会面（基本上有4个主要建筑物，这些建筑物都有所有国会办公室）或通过Zoom</li></ol><p><strong>会议如何进行</strong></p><ol><li>谈话通常是从我开始询问他们是否对AI有任何疑问或希望我分享我正在从事的事情。通常，他们希望我开始。</li><li>我将首先介绍自己和CAI。一旦<a href="https://safe-ai.webflow.io/statement-on-ai-risk">CAIS声明</a>问世，我将参考CAIS声明。我会告诉他们，我专注于高级AI的全球安全风险。我还会告诉他们我正在做NTIA的回应，我会告诉他们有关我正在考虑的一些高级想法。</li><li>然后，我会停下来看看他们是否有任何疑问。</li><li>通常，他们要么会询问有关灭绝风险的更多信息，要么问有关AI的其他问题（例如，您对如何处理深层餐厅有任何想法吗？），或者他们会提出一些有关法规的高级问题（例如，我们如何在不扼杀创新的情况下进行监管？我们如何在不输给中国的情况下进行规范？）</li><li>在一些最好的会议中，我会听到办公室正在处理的一些与AI相关的事情。大多数办公室都没有能力/兴趣来领导AI东西。大约10％的办公室就像“是的，我的国会议员对此非常感兴趣，我们正在考虑引入立法或成为别人立法的核心。”</li><li>很多人问我是否有立法草案。显然，如果您有监管想法，人们希望看到您的（简短）版本像账单一样。</li></ol><p><strong>后续</strong></p><p>NTIA的评论请求完成后，我向我遇到的所有人发送了后续措施。当我举行特别好的会议时（例如，一名工作人员对AI风险表示强烈的兴趣，或者告诉我他们想给我发送他们正在研究的事情）时，我会发送个性化的后续行动。我认为，最有力的迹象来自人们继续向我发送问题/想法，向我介绍同事或想与我合作提出建议的情况。 （明确地说，这发生在少数情况下，但我认为这是大多数影响的来源）。</p></div></section><h2>员工对AI风险的态度</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:41:56 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:41:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><blockquote><p>很多人问我是否有立法草案。</p></blockquote><p>他们正在寻找什么样的问题？您建议的任何立法吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:38:45 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:38:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>工作人员经常想知道我是否有立法草案来描述我在NTIA回应中所写的许可制度（我没有立法草案，但后来在我帮助托马斯将<a href="https://www.aipolicy.us/">AI政策中心</a>脱离中心时，有助于起草立法地面。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:39:31 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:39:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>啊好吧。更一般地，人们对AI风险有哪些先验？您是否认为您通常在其处理该主题的方式上发生了重大变化？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:39:04 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:39:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>似乎大多数人对AI风险没有强大的先验。</strong>我期望人们的先验更加怀疑（例如“什么？世界末日？<i>真的</i>？”）。但是我认为很多人都像“是的，我完全可以看到AI如何造成全球安全风险”，甚至“是的，我实际上很担心像天网一样的AI，我很高兴其他人正在工作关于这一点。”</p><p>通常，人们似乎<strong>真正担心AI的灭绝风险</strong>，但也<strong>没有任何计划</strong>。提醒我，就像“ X是生存风险”一样，这实际上是一件非常的事情 - >;“因此，我应该认真考虑在X上工作。”很多人就像“我很高兴有人在考虑这个[但是我不会去，我不希望我的国会议员会]。</p><p>就我的效果而言 - 我认为我主要是让他们更多地考虑它，并在其内部的“ AI政策优先”列表中提出了它。我认为人们忘记了员工在优先级清单上有100件事，因此仅将其暴露并将其重新曝光到这些想法可能会有所帮助。</p><p>我还遇到了一些工作人员，他们似乎非常关心AI风险，并且在AI政策领域似乎像强大的盟友一样。我仍然与几个人保持联系，当托马斯（Thomas）创立了AI政策中心时，我向其中的一群介绍了其中的一堆。如果我想通过一项账单，我认为我对哪些特定的人有更好的了解。<strong>在我看来，整个DC旅行的大部分影响都在于弄清楚盟军员工是谁并与他们建立初始关系。</strong></p><p>最后一件事是，我通常不强调失去控制//超级智能//递归自我改善。我没有隐藏它，但是我将其包含在更长的威胁模型列表中，这很少是我试图传达的主要内容。如果我再次这样做，我可能会更强调这些威胁模型。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:42:42 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:42:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><blockquote><p>在我看来，整个DC旅行的大部分影响都在于弄清楚盟军员工是谁并与他们建立初始关系。</p></blockquote><p>啊好吧！关于员工是否同情原因的任何特征是很好的预测指标？例如特定地区，政治倾向，其他政策。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:51:23 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:51:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><blockquote><p>对于员工是否同情我们的事业，有任何很好的预测特征吗？</p></blockquote><p><br>并不真地。样本量很小。就像，总共有大概有约4名员工，我将“非常关心灭绝风险，并且处于他们可能有助于继续立法的位置”。 1名共和党人和3名民主党人。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:24:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:24:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>啊，明白了。您（在CAIS陈述发表之前）是否对该陈述进行了任何内容（措辞，外展等）的讨论？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Wed, 15 Nov 2023 22:37:49 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Wed, 15 Nov 2023 22:37:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>讨论不会影响陈述。该声明是在我离开DC之前写的。 （有趣的事实：我是参与起草CAIS陈述的人之一。认为对好句子做出贡献比我做的其他许多事情都要高>; 100倍，但是有时候它可以奏效100倍。那样）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:48:17 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:48:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><blockquote><p> （有趣的事实：我是参与起草CAIS陈述的人之一。认为对好句子做出贡献比我做的其他许多事情都要高>; 100倍，但是有时候它可以奏效100倍。那样）。</p></blockquote><p>该死。顺便说一句，我们生活在奇怪的世界上。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:48:28 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:48:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>谢谢！看到该陈述有多大重要，这绝对是奇怪的。</p><p>我认为这也很谦虚 - 当我第一次听到这一说法时（当时我们称其为公开信），我记得全都是做的，就像是“嗯，一封公开的信件？有fli暂停信。”</p><p><strong>这是一个有用的提醒，有时您可能无法预先预测某事的影响</strong>。事后看来，很明显（至少对我而言）CAIS的陈述很有用，而变化理论非常扎实。但是当时，这并不像是一个固定的总体计划。感觉这只是一个列表中的一个项目中的一个项目，它具有一种模糊的变化理论，这只是另一个值得一提的赌注。</p></div></section><h2>得到教训</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 22:50:22 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 22:50:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>如果您再次进行此过程，您会做些什么？您/您从中学到的，令您/您感到惊讶的主要因素是什么？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 22:58:05 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 22:58:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p><strong>我想我会写出一个解释我的推理的文档</strong>，记录了我咨询过的人，记录了我知道的上行和下行风险，并将其发送给了一些EAS。我认为有些谣言传播了这是以单方面的方式完成的。这很棘手，让我难过。我认为我这样做的方式实际上不是单方面的，但是我认为，通过写作推理来避免误解会更好。托马斯（Thomas）与CAIP一起做了一堆，并作为<strong>“如何在不确定性下采取不确定性采取行动，并以推理透明度和高协调能力采取行动”。</strong></p><p>我还认为我会随附<strong>立法草案</strong>（假设与我在一起的组织很满意）。如果您有立法草案，人们似乎会更加认真地对待您。</p><p>我还写了一个<strong>短得多的NTIA响应</strong>- 我们最终写了一篇大约20页的论文。我将通过较短的材料来优化更多。</p><p>啊，说到这一点，我本来会有一个<strong>打印出的1-pager</strong> ，解释了CAI是什么，并总结了NTIA响应中的监管思想。我最终完成了一半，我会尽快做到这一点。</p><p>另外，我会随附<strong>名片</strong>。人们似乎喜欢名片！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:01:08 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:01:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>是的，这一切都是有道理的，尽管我绝对不会事先猜到。</p></div></section><h2>决赛</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Mon, 13 Nov 2023 23:02:52 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Mon, 13 Nov 2023 23:02:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>我相信我已经没问题要问：您还有其他想说的吗？随意漫步。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="KmQEfgxQrdnpXYYYq-Mon, 13 Nov 2023 23:26:43 GMT" user-id="KmQEfgxQrdnpXYYYq" display-name="Akash" submitted-date="Mon, 13 Nov 2023 23:26:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>阿卡什</b></section><div><p>这里有一些杂项：</p><ol><li>我在DC中的经验使我认为<strong>Overton窗口非常广泛</strong>。国会没有缓存的AI政策，而且似乎很多人真正想学习。目前尚不清楚这将持续多长时间（例如，AI的风险最终会变得两极分化），但是我们似乎处于异常高的开放和好奇心。</li><li>但是，<strong>让国会做任何事情也很难</strong>。例如，出于无聊的原因，没有太多的法案获得通过。在此过程中，账单可能会死去。当需要是两党制时（他们目前这样做，因为我们有民主党参议院和共和党的众议院），这将更加正确。这主要使我更新到<strong>“哇，现状通常什么都没有发生，并且需要做很多工作才能获得任何有意义的立法。”</strong>考虑到这一点，我确实认为我们处于AI安全性的非常独特的境地（<i>实际上</i>没有多少事情构成灭绝风险和其他各种其他灾难​​性风险；而且，没有多少事情成为参议院多数党领袖的优先事项是激发与世界领导人的国际峰会，或成为整个行政命令的重点）。</li><li><strong>许多人高估了DC中“内部游戏”的数量</strong>，尤其是在国会参与方面。发生了一些秘密的事情，但是在大多数情况下，我认为没有人有球。</li><li>我想看到<strong>有关特定政策愿景的更多协调</strong>。有一段时间，您只是在关心Xrisk的酷儿童俱乐部。我认为Overton窗口已经移动了一堆，我们正处于不足以“关心Xrisk”的时刻。重要的是人们支持哪些具体政策，并愿意倡导哪些政策。</li><li>考虑到这一点，我还认为拥有更广泛的AI风险社区会有好处。<strong>实施不当的协调可以导致什么都没有完成，因为您永远无法达成共识</strong>（目前有利于领先的实验室和不受监管的规模）。<strong>太少的协调会导致缺乏联盟建设和不必要的冲突。</strong>我认为我已经从“协调良好”转变为“正确完成协调是好的，但实际上需要技巧，机智和精力才能很好地进行协调。”</li><li>我通常认为<strong>，更多的人应该公开写自己的观点。</strong>当我不知道人们相信什么时，很难协调。我认为社区应该不愿意赞美那些没有提出任何特定立场的人。 <strong>&nbsp;</strong></li><li>我学到了很多有关DC AI安全社区的知识（由“ AI安全社区”，我主要是指从事AI安全工作的人们，这是由于渴望避免Xrisk或社会规模的灾难而动机。 ，但很多人都没有）<ol><li> TLDR：很复杂。我认为，前10％的思想家很有才华，并追求了合理的变革理论。另一方面，也有很多人声称对AI政策感兴趣，但对各种AI安全威胁模型没有基本的了解。也有（真实而合理的）担心，社会上的和政治上具有竞争力的新移民可能会以威胁或削弱现有努力的方式进入该空间。</li><li>总而言之，我觉得主要的文化太蔑视了新的政策努力。我希望随着AI政策对话继续前进并吸引新的人群，情况会发生变化。我会为一个更像“啊，新人感兴趣的社区”感到兴奋，让我们给您一些提示/指示，并指出我们已经有过的特定经验，并讨论了缺点风险的具体模型。”现状通常感觉不那么具体，（在我看来）（在我看来）对新努力过于保护。我发现，这种文化使我更难清楚地思考或进行倡导，尤其是我所说的“高领域倡导”（在这里，您在其中主要尝试将您的内部世界国家传达给人们，而不是主要尝试尝试传达一系列信念，这些信念将与您的听众息息相关）。我认为，关于“直接”各种倡导努力的“直接”（我认为，有些DC的人实际上会失去他们的某些影响力/“严肃点”（如果他们完全直接））有严重的辩论，但是我仍然感到惊讶在效果的范围内 - 文化似乎不信任我和我的同龄人直接。我认为，这种文化已经大大减慢了新的政策努力，并继续以我认为对世界不利的方式威胁/削弱/恶劣的新政策努力。与许多事情一样，我认为高级问题是正确的，但是在如何应用/实施这些高级问题的确切问题上存在问题</li><li>评估各种人/计划的往绩也很难。部分原因是某些信息是秘密的，部分是因为“我们与重要利益相关者有良好关系”之类的东西是一个有用的工具步骤，但不一定会转化为影响，部分是因为许多变革理论都是基于命中的和花一些时间来产生直接影响（例如，如果某人与X建立了良好的关系，也许在某个时候X会与AI监管非常相关，但也许只有1-10％的机会是真实的。） ，我认为，如果人们最终对自己的信念更明确，对他们希望实现的特定政策目标更加明确，并且对他们清晰的胜利（和损失）更明确，那么协调会更容易。在缺乏此事的情况下，我们冒着赋予“玩游戏”，发展影响力但最终没有利用自己的影响力来实现有意义的变化的人的风险和太多资源。 （另请参见此Dominic Cummings<a href="https://www.dwarkeshpatel.com/p/dominic-cummings#details">播客</a>）。</li></ol></li><li>相关的是， <a href="https://forum.effectivealtruism.org/posts/tdaoybbjvEAXukiaW/what-are-your-main-reservations-about-identifying-as-an?commentId=gNC53rsuMNTBjLCWY">奥利弗·哈布里卡（Oliver Habryka）的这一评论</a>引起了我的共鸣。我发现，与“主流EAS”有一定距离时，我通常会更清楚地思考。有很多抗体和微妙的文化压力，可以阻止我思考某些想法，并且可以萎缩我在世界上采取指示行动的能力。 （当然，我认为解决方案不是“永远不会与EAS互动”  - 但我确实认为人们可能低估了社区对思考和实现困难的事情的负面影响。我当然是。）</li><li>对于有兴趣捐赠的人，我目前建议<strong> </strong>这<strong> </strong><a href="https://www.aipolicy.us/"><strong>AI政策中心</strong></a><strong> </strong>（尤其是托马斯·拉尔森（Thomas Larsen）继续高度参与其战略方向）。我与托马斯（Thomas）有一些战略/战术分歧，但我认为他是一个非常聪明和才华横溢的人，我认为他是AI政策支持空间中最好的新来者之一（COI：Thomas是我的朋友，我和我的朋友之一参与了在早期阶段帮助该中心的AI政策）。</li><li>如果您想与我交谈，<strong>请随时在Lesswrong上伸出援手</strong>。我喜欢与从事AI政策工作的人交谈。我也愿意接受我可能正在做的有影响力的事情，或者我知道可能正在做的其他事情。 </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="vWQfyFPKdACzXEZ6R-Wed, 15 Nov 2023 22:18:15 GMT" user-id="vWQfyFPKdACzXEZ6R" display-name="hath" submitted-date="Wed, 15 Nov 2023 22:18:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>hath</b></section><div><p>哇，好吧。感谢您进行此对话！</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2slwt2csaag74nsdn/speaking-to-compeaking-to-compressional-staffers-about-ai危险<guid ispermalink="false">2SLWT2CSAAG74NSDN</guid><dc:creator><![CDATA[Akash]]></dc:creator><pubDate> Mon, 04 Dec 2023 23:08:52 GMT</pubDate> </item><item><title><![CDATA[Open Thread – Winter 2023/2024]]></title><description><![CDATA[Published on December 4, 2023 10:59 PM GMT<br/><br/><p>如果它值得说，但不值得单独发表，这里有一个地方可以放置它。</p><p>如果您是 LessWrong 的新手，这里是您自我介绍的地方。欢迎您就您如何找到我们以及您希望从网站和社区获得什么发表个人故事、轶事或只是一般性评论。如果您不想写完整的顶级帖子，这也是讨论功能请求和您对该网站的其他想法的地方。</p><p>如果您是社区新手，您可以开始阅读<a href="https://lesswrong.com/highlights">Sequences 的亮点</a>，这是有关 LessWrong 核心思想的帖子集合。</p><p>如果您想更多地探索社区，我建议您<a href="https://www.lesswrong.com/library">阅读图书馆</a>，<a href="https://www.lesswrong.com/?view=curated">查看最近策划的帖子</a>，<a href="https://www.lesswrong.com/community">看看您所在的地区是否有任何聚会</a>，并查看<a href="https://www.lesswrong.com/faq">LessWrong 常见问题</a><a href="https://www.lesswrong.com/faq#Getting_Started">解答</a>的入门部分。如果您想了解网站上的内容，您还可以查看<a href="https://www.lesswrong.com/tags/all">“概念”部分</a>。</p><p>开放线程标签在<a href="https://www.lesswrong.com/tag/open-threads?sortedBy=new">这里</a>。 Open Thread 序列在<a href="https://www.lesswrong.com/s/yai5mppkuCHPQmzpN">这里</a>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tRa92qDonDLi6RA8u/open-thread-winter-2023-2024#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tra92qdondli6ra8u/open-thread-winter-2023-2024<guid ispermalink="false"> tra92qdondli6ra8u</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:59:51 GMT</pubDate> </item><item><title><![CDATA[Interview with Vanessa Kosoy on the Value of Theoretical Research for AI]]></title><description><![CDATA[Published on December 4, 2023 10:58 PM GMT<br/><br/><p>以下是我与 Vanessa Kosoy 进行的<a href="https://youtu.be/1MCRQF0_5zY?feature=shared"><i>视频采访</i></a><i>的文字记录（经过语法编辑）</i> ，从我的<a href="https://www.zenmarmotdigital.com/blog/interview-with-vanessa-kosoy"><i>博客</i></a><i>交叉发布</i><i>。它旨在（相对）对初学者友好地解释</i><a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Direction_6__Metacognitive_Agents"><i>学习理论议程</i></a>的目标<i>，以及为什么需要更多的理论工作来确保人工智能大规模安全可靠。</i></p><p></p><p><strong>简介（作者：Will Petillo）：</strong>讨论人工智能的未来往往会变得哲学化。有“目标”或“理解”意味着什么？寻求权力的是想要事物的默认后果吗？还是我们独特的进化历史所产生的人类怪癖？是什么促使善良的？以这种方式进行框架问题使它们可以访问，使每个人都可以参加对话。但是，由于分歧成为直觉的冲突，这种缺乏精确度也使此类问题变得棘手。</p><p>当今的“对齐监护人”的嘉宾是凡妮莎·科索（Vanessa Kosoy），他是机器情报研究所（MIRI）和长期未来基金（LTFF）支持的独立研究人员，他介绍了安全AI的数学理论。从第一原则中的理解重点使她的工作与领先的AI实验室的“快速移动并破坏事物”的实验方法形成鲜明对比。在这次采访和其他地方，凡妮莎捍卫了一种基于理论的方法的价值，并解释了将机器学习作为基础科学的含义。</p><p></p><p> <strong>Petillo：</strong>您是如何进入AI安全的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我一直是自动赛车，所以我只是倾向于自己学习东西。小时候，我以为我会成为理论物理学家。我实际上拥有数学学士学位，但是在完成学士学位之后，我没有去学术界，而是决定从事该行业的职业，而是从事软件。</p><p>我在软件行业，特别是算法工程，主要是计算机视觉，各种角色，算法工程师，团队负责人，研发经理。我也有自己的创业公司。我是顾问，然后是10年前。我接触了AI的整个存在风险的主题，并开始认为这似乎很重要。因此，我开始对此进行枢纽，最初只是我在业余时间进行研究。然后是Miri的支持。然后，我最近也得到了长期未来基金的支持，这使我能够全职。</p><p><strong>佩蒂洛：</strong>那个过程是什么样的？您只是以一种自我指导的方式工作，然后您获得了Miri和其他来源的支持？这是怎么来的？</p><p><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我开始读一些我的东西，而《 miri》和《少问题》写作。我开始研究自己的想法，并写下关于少错的帖子。之后，我被邀请参加一些研讨会，一些活动，最终Miri说，好吧，似乎您在这里做一些不错的工作，所以也许我们也会为此付钱。我很棒，因为这也使我能够做更多的事情，并花更少的时间做其他事情。<br><br><strong>威尔·佩蒂洛（Will Petillo）：</strong>这里的观众有一点背景。一个受欢迎的博客少了。它最初是关于理性的，也是关于与AI相关的事物的。 Miri是机器情报研究所。我喜欢将他们描述为在酷之前进行对齐的人。告诉我更多关于Miri作为机构的信息。<br><br> <strong>Vanessa Kosoy：</strong> Miri或多或少是第一个谈论人工智能存在风险的人。 Eliezer Yudkowsky在2000年开始谈论这件事，最初Miri只是Yudkowsky，然后多年来，他们设法获得了一些资金来吸引其他研究人员。他们正在考虑以下问题：我们如何使人工智能安全，我们如何处理这个问题？我们可以提出什么样的数学理论来解决这个问题？甚至在深度学习革命开始之前，以及近年来大型语言模型的整个炒作之前。他们的大部分时间都致力于提出一些基本的数学理论，这将帮助我们保持AI的一致性。<br><br>最近，由于他们相信时间表确实很短，而且我们没有时间发展这一理论，因此他们枢纽宣传并试图影响政策。<br><br><strong>佩蒂洛：</strong>您是否加入了该枢纽？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>不，我的看法与众不同。我可以说更保守。我认为时间表并不像风险社区中的许多人那样短。我认为，如果在政策渠道中努力规范AI开发并延迟AI开发以阻止真正危险的AI的发展，那么这将成功，那么这只会花我们的时间。然后问题是：为我们花时间买什么？而且我认为，理论基础绝对是我们在拥有的时间应该做的事情中最重要的事情，或者随着我们将通过某种政策计划成功购买的时间以一种或另一种方式购买的时间。<br><br>我认为，在任何世界上，创建这一基础理论都是关键。这就是我正在做的事情。这绝对是我的个人技能和优势所在的地方，从事数学而不是政策。<br><br><strong>威尔·佩蒂洛：</strong>您提到了时间表。直觉上，我知道不可能以任何精度真正预测这些事情，但是就促使您的动机而言，您将这些东西何时需要解决的时间表？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>有些人认为AGI将在10年甚至更少。我认为这有点极端。但是，当需要解决问题时，越早越好，对吧？如果我们在五年内有解决方案，那么我们比只有10年内的解决方案要好，这仍然比我们只有20年之内的解决方案要好。<br><br>实际上，我个人的看法是，实际上还有数十年的时间才真正达到存在存在风险的AI。因此，这给了我们更多的时间，但不是无限的时间。<br><br><strong>佩蒂洛（Petillo）：</strong>是什么让您点击了您要处理的事情？<br><br>凡妮莎·科索伊<strong>（Vanessa Kosoy）：</strong>它最初更像是一种好奇心，因为它实际上是从我完全随机发现的开始，您可以在AGI上说一些论文，而不是关于AI的一致性或风险或类似的东西，而是JürgenSchmidhuber和Marcus Hutter的一些论文有一些关于Agi的想法。我一直是数学书呆子，所以有一些数学框架来思考AGI似乎真的很酷。我开始阅读有关这件事的文章，最终也发现错误的错误，有些人也在讨论这种事情。<br><br>一方面，我正在阅读Eliezer Yudkowsky在主题上写的越来越多的东西，而Lesswrong上的人们写了这一主题，但我也开始考虑数学模型。最终，它引起了我的注意，当您考虑实际数学时，就没有数学上的原因，为什么AI不得不关心人类或完全关心与我们作为人类关心的东西保持一致的数学原因。<br><br>另一方面，似乎比我们更有能力。我认为这很明显，但是对我来说，我喜欢通过数学理解一切。因此，当我看到您实际上可以将其放入数学模型中时，它确实对我来说确实是真实的，这是我们应该真正关注的事情。</p><p><strong>威尔·佩蒂洛（Will Petillo）：</strong>没有理由假设AI一定会很好的想法，这听起来很像尼克·博斯特罗姆（Nick Bostrom）写的正交论文。智力以及好事不必在一起；任何一组值都可以使用任何级别的智能。从本质上讲，这是您的见解吗？<br><br> <strong>Vanessa Kosoy：</strong>是的，这正是术语。事后看来，这似乎是一件明显的事情。但是对我来说，有必要看到您实际上可以用数学对象来考虑它。值可以形式化为效用函数。然后，代理可以被形式化为某种优化器，某种贝叶斯最佳策略或该实用程序功能的任何其他策略。实际上，您可以将严格的含义放在每个术语后面，并看到这实际上都是有道理的，而不仅仅是某种哲学上的挥舞技巧。<br><br><strong>佩蒂洛（Petillo）：</strong>您认为，使AI对齐问题的根本原因是什么？<br><br><strong>凡妮莎·科索（Vanessa Kosoy）：</strong>我认为问题很难。我认为，首先，很难的是，我们的目标是一个非常狭窄的目标，因为人类的价值观非常复杂和具体。我们关心许多非常详细的事物：爱，友谊，美丽（以我们自己的主观理解），性，所有这些都是人类事物，因为某些复杂的进化事故，它发生在某些人身上的方式非常特殊的星球。这是特定宇宙历史上非常特别的一点。这组值是您可以想象的可能值或思想的巨大空间的非常非常狭窄的一部分。<br><br>因此，按照我们的标准，大多数人绝对不是对我们很好的任何东西。更糟糕的是，这种现象在他的一篇相对较新的帖子中也很好地表达了这种现象，他写道，围绕特工能力有一个吸引人的盆地，但对代理人的一致性没有吸引力的盆地。这意味着，即使您采用足够的优化压力，即使使用蛮力技术，您最终将产生高功能的代理。一个例子是进化，对吗？进化是一种非常原始的蛮力算法，最终创造了人类大脑，这是一种更复杂的算法。如果您将足够的蛮力优化用于寻找在开放世界环境中成功的事物，最终您将遇到聪明的代理商。 And this is even before you put in recursive self improvement in the equation, which makes it even stronger as this kind of basin of attraction where you converge to. Whereas nothing of the sort is true about being aligned to human values in particular. It&#39;s very plausible that we could, by just kind of blind or semi blind trial and error, arrive at creating highly capable agents long before we understand enough to actually make those agents aligned.<br><br> <strong>Will Petillo:</strong> That sounds like going against the other Bostrom-popularized idea of Instrumental Convergence, that things like survival will be wanted by almost any system optimizing hard enough.<br><br> <strong>Vanessa Kosoy:</strong> There&#39;s this notion of instrumentally convergent goals, which are certain goals that most intelligent agents will pursue because they help them to achieve their terminal goals, whatever their terminal goals are. And these are things like survival, like gaining more resources, becoming more intelligent, and so on. But human values are not that. If we manage to construct an AI that survives and gains a lot of resources, that&#39;s nice for the AI, I guess, but it doesn&#39;t help us in terms of alignment to our values at all. That&#39;s a very different kind of thing.<br><br> <strong>Will Petillio:</strong> Does the fact that modern AI is trained on human-generated data and exists in human society not help?<br><br> <strong>Vanessa Kosoy:</strong> I think it helps, but it kind of leaves a lot of questions. One question is: okay, you learn from human generated data, but how do you generalize from there? Because it&#39;s really not clear what conditions are needed to get a good generalization, especially when the concept you&#39;re learning is something extremely complicated.<br><br> The higher the complexity of the concept you&#39;re learning, the more data points you need to learn it. What we&#39;re doing with the so-called Large Language Models, which are the hype in recent years, is trying to imitate humans. Which is, I mean, nice. It could be that it will lead to something good with some probability—not a very high probability. But the problem with it is that in order to use that, you need to generalize far outside the training distribution. Here we actually need to look at what the goal is.<br><br> The problem is that it is technically possible to create super intelligent AI, which will be dangerous. To solve this problem, it&#39;s not enough to create some kind of AI which would not be dangerous, because otherwise they could just write an algorithm that doesn&#39;t do anything. That&#39;s not dangerous, mission accomplished. We need to be able to create AIs that are sufficiently powerful to serve as defense systems against those potentially dangerous AIs. So those have to be systems that have superhuman capabilities at building sophisticated models of the world and building complex long term plans based on that. And that&#39;s something that is far outside the training distribution of a Large Language Model or anything that&#39;s based on human imitation. It is extremely unclear whether we can actually rely on the algorithms we have to generalize that far out of the training distribution without completely losing all of their alignment properties.<br><br> <strong>Will Petillo:</strong> To summarize, LLMs are fundamentally imitative, which doesn&#39;t seem particularly dangerous in itself, but it also limits what they can do. And so we can&#39;t really expect that development is just going to stop here. Eventually there might be something like Reinforcement Learning added onto it—maybe not necessarily that algorithm, but something that can be as creative as Alpha Zero is in Go and finding a really creative move that no one&#39;s seen before. So we need to be prepared for things that are much more powerful because those are going to be useful and the economics that led to building LLMs are going to lead to building bigger things. Is that what you&#39;re getting at?<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that sounds pretty much on the spot. Either it will be Reinforcement Learning or...well, I don&#39;t want to speculate too much on what is needed in order to make AI more powerful because it&#39;s not good information to have out there.</p><p> <strong>Will Petillo:</strong> Fair enough. Moving on to the things that you actually work on, one idea I&#39;ve seen adjacent to this is the term &quot;Agent Foundations&quot;. And also the &quot;Learning Theoretic Agenda&quot;.那些东西是什么？<br><br> <strong>Vanessa Kosoy:</strong> Agent Foundations is this abstract idea that says we need to create a foundational mathematical theory which explains what agents are. What does it mean, mathematically, for an algorithm to be an agent? What types of agents are possible? What capabilities can they have or not have?等等。 The Learning Theoretic Agenda is more specific than that, in the sense that it&#39;s like a very specific program that is trying to achieve this goal. Specifically, by tools that build on statistical and computational learning theory, algorithmic information theory, control theory, this kind of thing. This is the program I created to answer this challenge of coming up with those agent foundations.<br><br> <strong>Will Petillo:</strong> OK, so Agent Foundations is like the question of &quot;how do minds work?&quot;, that encompasses AI, and the Learning Theoretic Agendas is like, &quot;how do we design algorithms that push this in a good direction?&quot;是对的吗？<br><br> <strong>Vanessa Kosoy:</strong> I wouldn&#39;t put it that way. I would just say that Agent Foundations is just trying to understand how minds work and people have been trying to do it in various ways. MIRI have historically had all sorts of proof-theoretic models that try to approach this, and then there&#39;s Garrabrant&#39;s Logical Induction, and there are various ideas under this very broad umbrella, whereas the Learning Theoretic Agenda is a very specific approach.<br><br> It&#39;s this approach that starts with AIXI and classical reinforcement learning theory as the starting points and then looks what are the missing ingredients from that in order to have a foundational theory of agents and start building towards those missing ingredients with ideas such as Infra-Bayesianism and Infra-Bayesian Physicalism and Metacognitive Agents, and so on.<br><br> <strong>Will Petillo:</strong> The kind of agents and minds that you&#39;re talking about here, is this tied to the frontier Large Language Models, or is it more broad to AI or any kind of thinking entity?<br><br> <strong>Vanessa Kosoy:</strong> When I say agent, I mean something very broad. Much broader than existing AIs or even just AIs. Certainly including humans, potential aliens, or whatever. So, for me, an agent is a system that has particular goals and is learning sophisticated models of the world in which it is embedded and uses those models to build long term plans in order to achieve its goals. So this is the informal description of what I mean by an &quot;agent&quot;. The whole goal of this program is to go from this to a completely formal mathematical definition and study all the implications of this definition.</p><p> <strong>Will Petillo:</strong> So not even beyond LLMs, it&#39;s even broader than Machine Learning. What&#39;s the reason for that approach? Given how dominant Machine Learning is, why not focus on the things that seem to be the most widely used?</p><p> <strong>Vanessa Kosoy:</strong> First of all, let&#39;s keep some order in the terminology. I would distinguish between AI, Machine Learning, and Deep Learning. AI is this thing that people started thinking about since the 1950s, about how to build thinking systems, without really having good understanding of what does it even mean, but just some kind of intuitive notion that there is such a thing as thinking and we should be able to replicate it in a machine.<br><br> Machine learning is a more specific approach to this that emerged...well, I didn&#39;t want to point a finger exactly when, but probably something like the eighties. Machine Learning is specifically this idea that the central element of thinking is learning and learning means that you&#39;re interacting with some unknown environment and you need to create a model of this environment. So you need to take the data that you see and use it to create a model. And that&#39;s analogous to how scientists do experiments, gather data, and then build theories based on this data.</p><p> This general idea is called Machine Learning—or, more accurately would be to call it just learning. The &quot;machine&quot; part comes from trying to come up with ways to actually implement this inside a machine. This is a field that has a lot of mathematical theory. The mathematical theory behind machine learning is what&#39;s known as statistical and computational learning theory, and that&#39;s actually the foundation of the Learning Theoretic Agenda. That&#39;s why it&#39;s called &quot;learning theoretic&quot;.</p><p> There has been a hypothesis that this kind of notion of learning captures most of the important bits of what we mean by thinking. And I think that this hypothesis has been extremely well-supported by recent developments in technology. This is something that I completely endorse, and it is basically the basis of my whole research program. So there&#39;s no contradiction here, because learning is still a very general thing. Humans also do learning. Aliens would also have to do learning.</p><p> Deep Learning is a more specific set of algorithms for how to actually accomplish learning efficiently in a machine, and that&#39;s what started the deep learning revolution around 2010, although the algorithms existed in some form for decades before that. But it took a while to get the details right and also to have the right hardware to run them. Deep Learning&#39;s unfortunate feature is that we don&#39;t understand it mathematically. A lot of people are trying to understand it, but we don&#39;t have a good theory of why it actually works. That&#39;s why it&#39;s not the focus of my research program, because I&#39;m trying to come up with some mathematical understanding. I definitely have a hope that eventually people will crack this kind of mystery of how Deep Learning works, and then it will be possible to integrate it into the theory I&#39;m building.<br><br> But even if we had this theory, then it still seems really important to think in the broadest possible generality, because, well, first of all, we don&#39;t know that the algorithms that exist today will be the algorithms that bring about AGI. And also because the broadest generality is just the correct level of abstraction to think about the problem, to get at those concepts of what does it even mean for a system to be &quot;aligned&quot;. There&#39;s some philosophical problems that need to be solved here, and they are specific to some very particular algorithms. Also, there is the fact that I actually want this theory to include humans because I might want to use this theory to formalize things like value learning. How do I design an AI system that learns values from humans?<br><br> <strong>Will Petillo:</strong> Seeing the Wikipedia-level, or just browsing the internet description, of Machine Learning and Deep Learning, it&#39;s very easy to use them interchangeably. I think I&#39;ve seen the description as Deep Learning is just the idea that you add multiple layers of neurons. And so because there&#39;s multiple layers, it&#39;s &quot;deep&quot;<br><br> <strong>Vanessa Kosoy:</strong> Let me try to clarify the distinction. Machine Learning talks about taking data and building models from it. The type of model you&#39;re building can be very different. Before Deep Learning, we had algorithms such as Support Vector Machines, Polynomial Regression is also a very simple type of Machine Learning—fitting a model to data. Various methods used in statistics can be regarded as a sort of Machine Learning. There is some space of models or hypotheses and you&#39;re trying to use the data in the optimal way to infer what the correct hypothesis is, or to have some probability distribution of your hypothesis if you&#39;re doing a Bayesian approach.<br><br> But different types of hypothesis classes lead to very different results in terms of the power of the algorithms, but also in terms of what we know to say about how to learn those hypothesis classes. And what do we know to prove mathematically under which conditions we can actually learn them? For example, for Support Vector Machines, the mathematical theory is basically solved. There&#39;s kernel methods that build on top of that, and that also has very solid mathematical theory. Deep learning is a particular type of learning algorithm which uses those artificial neural network architectures.</p><p> It&#39;s not just multiple layers, there&#39;s a lot of details that matter. For example, the fact that the activation functions are ReLU, that turns out to be pretty important for what kind of regularization method you use in training. For example, dropouts are basically what started the Deep Learning revolution. If you&#39;re working with sequences, then we have transformers, which is a very specific network architecture. So there is actually a lot of very specific details that people came up with over the years, mostly with the process of trial and error, to just see what works. We don&#39;t have a good theory for why those specific things work well. We don&#39;t even understand the space of models those things are actually learning, because you can prove theoretically that if you take a neural network and you just let it learn another neural network, then in some situations it will be infeasible.<br><br> But for real world problems, neural networks succeed to learn a lot of the time. This is ostensibly because the real world has some particular properties that make it learnable, or there&#39;s some particular underlying hypothesis class that the neural networks are learning, and which captures a lot of real world phenomena, but we don&#39;t even have a mathematical description of what this underlying hypothesis class is. We have some results for some very simple cases, like two layer or three layer neural networks, or some other simplifying assumptions, but we&#39;re not close yet to having the full answer.<br><br> <strong>Will Petillo:</strong> Deep Learning assumes certain things about how the world is, in terms of what it can pick up, and it happens to work fairly well, but it&#39;s not really clear what it&#39;s assuming.</p><p> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s exactly right. So we have different no-go theorems, which say that for arbitrary data, even if the data is fully realizable, and even if the data is such that the neural network can perfectly express an exactly correct model, the problem is infeasible. In general, gradient descent will not converge to the right model, and also no other algorithm will converge because the problem will just be intractable. There are some properties that the world has, and since this Deep Learning is successful in such a big variety of very different cases, it feels like those properties should have some simple mathematical description.<br><br> It&#39;s not like some properties that are extremely specific to texts or to audio or to images. It&#39;s some properties that are extremely general and hold across a wide range of different modalities and problems. Those are properties that I can speculate, for example, as having to do with compositionality, how the real world is often well described as being made of parts, and how things can be decoupled according to different spatial scales, or different temporal scales on which the dynamics is happening. But we don&#39;t have a theory that actually explains it.<br><br> <strong>Will Petillo:</strong> You mentioned ReLU as one of the examples of a thing that just works. As I understand it, ReLU is basically like taking the output, changing it in a way that could be illustrated as a graph where it&#39;s flat on one side and a diagonal line past zero. Whereas before, models typically used Sigmoid as the activation function, which was more like a smoothly curved line that prevents numbers from getting too big. For some reason, ReLU works better. The sense I&#39;m getting from your explanation is that this change impacts what kinds of things the neural network is able to understand in a direction that&#39;s more matching with reality. But all these changes are developed with a &quot;throw stuff at the wall and see what sticks&quot; kind of way, simply measuring the results without really understanding <i>why</i> ReLU is better than sigmoid.<br><br> <strong>Vanessa Kosoy:</strong> That&#39;s more or less right. We just have to be careful about what we mean when we say what the neural network can &quot;understand&quot;. It&#39;s a pretty complicated concept because it&#39;s not just what the neural network can express with some set of weights, it&#39;s what the neural network can actually learn through a process of gradient descent. It has to do not just with the space of functions that the neural network can describe, but with the entire loss landscape that is created in this space of weights when we look at a particular data set.<br><br> <strong>Will Petillo:</strong> When you&#39;re describing a gradient descent and loss landscape the analogy I hear thrown around a lot is a ball rolling down a hill—there&#39;s a constant gravity force, and you want the ball to get down to sea level. But often it doesn&#39;t because it finds some local minima, like a hole or something, where any direction it can go is up, so it&#39;s not going to roll anymore. So you have to shape the landscape such that down always gets the ball to sea level.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that&#39;s a pretty good explanation. Gradient descent is something that we have good mathematical theory for how it converges to the global minimum for convex functions, but the loss for neural networks is non-convex...but it still happens to be such that it works. People have gained some insights about why it works, but we still don&#39;t have the full answer.<br><br> <strong>Will Petillo:</strong> OK, so if the landscape is really bumpy, then you don&#39;t expect the ball to get to sea level, so the fact that it somehow does anyways demands an explanation that we don&#39;t really have. I can see how that kind of framing raises a lot of questions regarding unpredictability.<br><br> Moving on, you mentioned AIXI at one point.那是什么？<br><br> <strong>Vanessa Kosoy:</strong> AIXI is this idea by Marcus Hutter, which is supposed to be a mathematical model of the perfect agent. The way it works is: there is a prior, which is the Solomonoff prior. For those who don&#39;t know what that is, it&#39;s basically some way to mathematically formalize the concept of Occam&#39;s Razor. And Occam&#39;s Razor is this idea that simple hypotheses should be considered a priori more likely than more complicated hypotheses. And this is really at the basis of all rational reasoning. Hutter took the Solomonoff prior, which is a very clever way to mathematically formalize this notion of Occam&#39;s razor, and say, well, let&#39;s consider an agent that&#39;s living in a universe sample from the Solomonoff prior. And this agent has some particular reward function that it&#39;s maximizing. And let&#39;s assume it&#39;s just acting in a Bayes-optimal way. So it&#39;s just following the policy that will lead it to maximize its expected utility according to this prior. And let&#39;s call this AIXI. Which is a really cool idea...only it has a bunch of problems with it, starting with the &quot;minor&quot; problem that it&#39;s uncomputable. There&#39;s not an algorithm that exists to implement it even in theory.<br><br> <strong>Will Petillo:</strong> I think I heard it explained once as imagining the entire universe described as a bunch of bits—ones and zeros. At the start, all of them could either be a one or a zero, then you get a little bit of data and now you&#39;ve locked in a few of those numbers and have cut the space of all things that could be in half. As you keep learning, you get more and more certain.<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s actually a little more nuanced than that. Just the fact that you have a lot of possibilities doesn&#39;t mean that it&#39;s uncomputable. Maybe the exact thing is uncomputable, but you could still imagine that there is some clever algorithm that approximates this Bayesian inference process. For example, if you look at classical reinforcement learning theory, then there are things like algorithms for learning in arbitrary Markov decision processes with n states. In a Markov decision process with n states, there are still an exponentially large space of possible ways it could be, and we still have actually efficient algorithms that converge to the right thing out of this exponentially large thing by exploiting some properties of the problem.<br><br> The thing with AXI is that its prior is such that even individual hypotheses in the prior are already arbitrarily computationally expensive, because in its prior it considers every possible program, so every possible program that you can write on a universal Turing machine is a possible hypothesis for how the world works. And some of those programs are extremely expensive computationally. Some of those programs don&#39;t even halt, they just enter infinite loops. And you can&#39;t even know which, because this is the halting problem, right? This is why AIXI is a non-starter for doing something computable, not to mention computationally tractable.<br><br> <strong>Will Petillo:</strong> The minor problem of uncomputability aside, a &quot;perfect algorithm&quot;...what does that mean? Would AIXI be safe if it were somehow, magically computed?</p><p> <strong>Vanessa Kosoy:</strong> No, it doesn&#39;t make it safe in any way. It&#39;s &quot;perfect&quot; in the sense of it&#39;s the most powerful algorithm you could imagine. Again, under some assumptions. I mean, there are other problems with this, such as that it assumes that the outside world is simpler than the agent itself. There are multiple problems with this, but if you can put all those problems aside then you could argue that this is the best possible agent. And in this sense, it&#39;s perfect. It&#39;s very, very, very not safe. In order for it to be safe, we would need to somehow plug the right utility function into it. And that would still be a very non-trivial problem.</p><p> <strong>Will Petillo:</strong> What kind of algorithms do you look for, assuming you&#39;re trying to find things that are computable?</p><p> <strong>Vanessa Kosoy:</strong> Computability is just one of the issues. I&#39;m imagining that there will be something that I call the frugal universal prior, which is some kind of prior that we can mathematically define, which will simultaneously be rich enough to capture a very big variety of phenomena. And on the other hand, we&#39;ll have some clever algorithm that can actually allow efficient learning for this prior using, for example, some compositionality properties of the hypothesis on this prior or something of that sort.</p><p> But even knowing this prior, there&#39;s a lot of other conceptual problems that you also need to deal with. Like what I call the problem of privilege, where the formalization of Occam&#39;s Razor and AXI privileges the observer, and you need to understand how to deal with that. And there&#39;s the problem of realizability where you cannot actually have a hypothesis which gives you a precise description of the universe, but only some kind of approximate or partial description, and you need to understand how to deal with that. Then there&#39;s also the fact that you want your utility function to be not just a function of your observations, but also some parameters that you cannot directly observe. You also want to be able to prove some Frequentist guarantees for this algorithm. To know how much data this algorithm actually needs to know to learn particular facts and have a good theory of that. There&#39;s a whole range of different questions that come up when studying AIXI like models.<br><br> <strong>Will Petillo:</strong> Studying AIXI like models, that is what you&#39;re working on?</p><p> <strong>Vanessa Kosoy:</strong> You could, yeah, if you wanted to put it in one sentence, I guess.<br><br> <strong>Will Petillo:</strong> What are some interesting problems that you&#39;re interested in solving? I&#39;ve seen Newcomb&#39;s problem floating around and stuff adjacent to this.<br><br> <strong>Vanessa Kosoy:</strong> Newcomb&#39;s problem is something Eliezer Yudkowsky wrote about a lot as an example of something that&#39;s very confusing for classical accounts of rationality. You have two boxes that you need to choose from. One box has a thousand dollars. The other box has either nothing or a million dollars. You can either choose the first box or you could take the money that&#39;s in both boxes. Normally, taking the money that&#39;s in both boxes is always strictly superior to just taking the one box.<br><br> Except that in this spot experiment there is some entity called Omega who can predict what you do, and so it only puts the $1,000,000 in the other box if it knows that you will only take that box and won&#39;t try to take the thousand dollar box as well. So only if you&#39;re the type of agent that would predictably (for Omega) take only one box, only in this case you will get out of this room with $1,000,000. Whereas in the other case you will only have $1,000. So arguably it&#39;s better to take one box instead of two boxes, as opposed to what many classical accounts of rationality would say. This is one example of an interesting thought experiment.</p><p> For me, this thought experience is a special case of the problem of non-realizability, where you need to deal with environments that are so complex that you&#39;re not able to come up with a full description of the environment that you can actually simulate 。 Because in this example, the environment contains this agent Omega, which simulates you, and this means that you cannot simulate it, because otherwise it would create this kind of circular paradox. I&#39;ve actually also shown that my theory of dealing with non-realizability, which I call Infra-Bayesianism, actually leads to optimal behavior in these kinds of Newcomb-like problem scenarios.<br><br> <strong>Will Petillo:</strong> And the reason for studying Newcomb-like problems is not because we expect to be faced with Omega offering us boxes at some point, but because it&#39;s just an illustrative way of thinking about how to deal with things when you can&#39;t know这是怎么回事。 And also because it might be easy to say, &quot;yeah, well, I&#39;ll just take one box because I&#39;ll get more that way,&quot; but when you really dive into what&#39;s a coherent, non hand wavy reason as to why, then there&#39;s some interesting insights that can potentially come up from that. Have there been any discoveries that you found through exploring these kinds of things?</p><p> <strong>Vanessa Kosoy:</strong> I would say that Infra-Bayesianism itself is an interesting discovery, that some theoretical account of agents that can reason about complicated worlds that are much too complicated for the agent to simulate. Now I describe the motivation by stating the problem of non-realizability, but the way I actually arrived at thinking about this is through thinking about so-called logical uncertainty. The reason people started thinking about it was because of so-called Updateless Decision Theory, which came from thinking about Newcomb time paradoxes. So it all comes from that line of reasoning even though, after the fact, you can motivate it by some much more general abstract thinking.<br><br> <strong>Will Petillo:</strong> What&#39;s the connection between these decision theory type questions and making a safer AI?<br><br> <strong>Vanessa Kosoy:</strong> The idea is creating a general mathematical theory of agents. The way it&#39;s going to help us with making AI safer, there are several reasons, the most obvious is that in having this theory, we hopefully will be able to come up with rigorous models of what it means for a system to be an aligned agent 。 Having this rigorous definition, we&#39;ll be able to come up with some algorithms for which we can prove those algorithms are actually safe agents. Or at least we could have some conjecture that says that this given model of such and such conjectures, we think that those algorithms are safe agents. Like in cryptography, you have some conjectures which have very strong evidential support.</p><p> We could have at least some semi formal arguments because now when people are debating whether a particular design is safe or not safe, it all boils down to those hand waving philosophical arguments, which don&#39;t have any really solid ground. Whereas this gives us tools for much more precise, crisp thinking about these kinds of questions. It hypothetically also gives us much more power to leverage empirical research because maybe we will be able to take the empirical research we have, plug it into the mathematical theory, and get some answers about how we expect those results to actually extrapolate to various regimes where we haven&#39;t done so.<br><br> <strong>Will Petillo:</strong> Would this line of research that you&#39;re working on eventually be usable in evaluating things like Large Language Models or Deep Learning based systems, to be able to say with greater certainty the extent to which they&#39;re safe or unsafe?<br><br> <strong>Vanessa Kosoy:</strong> I think there are multiple paths to impact. So there is a path to impact where we will eventually come up with a theory of Deep Learning. Or, if not a fully proven theory, then at least some strong conjectures about how Deep Learning works that can interface with the theory of agents I&#39;m building. And then we could use this composite theory to prove things or at least to have strong arguments about properties of systems built in Deep Learning.</p><p> There could be a different path to impact where we use this theory to come up with completely new types of algorithms for building AI, which are not Deep Learning, but for which we have a good theoretical understanding.</p><p> There&#39;s also some third possibility that we won&#39;t have a good theory, but we could at least reason by analogy, similarly to how many Deep Learning algorithms are designed by analogy to some algorithms for which we have mathematical theory. Deep Q learning, for example, is analogous to simple Q learning, for which we have mathematical theory. So we could imagine a world in which we have some kind of idealist toy model algorithms for which we have some rigorous arguments why they are aligned and then we have some more heuristic algorithms, which we cannot directly prove things about, but which are arguably analogous to those toy models.<br><br> <strong>Will Petillo:</strong> So I heard three paths to impact. One is potentially building a different form of AI that&#39;s verifiable from the ground up and does the same things as Deep Learning based AI, but in a more rigorous sort of way. A second is evaluating, or at least better understanding, Deep Learning or whatever is state of the art. And then a third, in between the two, is having a simpler form of AI that analogizes to the state of the art things, so that you can use the former to understand the latter.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, that sounds about right.<br><br> <strong>Will Petillo:</strong> I&#39;d like to focus a bit on using foundational research to understand other things like Deep Learning, getting at this theory-based approach. Bringing in a totally opposite counterpoint, one could argue: no, you should just look at the things that are being used and collect data about it and then build your theory by finding patterns in the data. When the theories are shown to be wrong—as a result of more data—then update your theories then. Why work on theory in advance?</p><p> <strong>Vanessa Kosoy:</strong> The biggest reason is that you cannot reliably extrapolate from empirical research without having an underlying theory. Because you might, for example, take some measurements and find some trend...but then there is some phase transition later on that you don&#39;t see in the trend, but which happens and behavior changes to a completely different regime. And because you don&#39;t have a theoretical explanation, you won&#39;t notice—or people will just switch to using completely different algorithms, which behave in completely different ways.</p><p> You might have those empirical models of existing AIs, but those empirical models are very myopic. They&#39;re always looking one step ahead. And then you don&#39;t see the cliff that&#39;s three steps ahead of you. Updating those theoretical, empirical models on new things that happen—it might just not be quick enough. Eventually you fall off the cliff and then it&#39;s too late to say, &quot;oh, actually, that trend line was wrong!&quot;</p><p> Luckily we are in a domain where we have tools to do research even without empirical data. Of course, we should use the empirical data that we have, but we&#39;re not bottlenecked on empirical data because the thing we&#39;re studying is algorithms, and algorithms are mathematical objects, so they can be studied mathematically. This is very different from studying some phenomenon of physics, where if you don&#39;t have the data, then there&#39;s no way to generate the data without having it. Here, this really should all boil down to math. More precisely, it should boil down to math plus whatever properties the real world phenomena have that we want to assume in our mathematical theories.<br><br> And here, yeah, here it is something that we need empirical input for. But on the other hand, we already have a really good understanding of physics. So given the knowledge of physics and other scientific domains that we have, it&#39;s very plausible that we have enough information to answer all the questions purely through mathematical inquiry, even if we had no empirical data at all. Which is not to say we shouldn&#39;t also use empirical data, to supercharge this research, but we&#39;re not limited to that.</p><p> <strong>Will Petillo:</strong> So it&#39;s not a choice between theory versus experiment, we should be using both. You&#39;re focused on the theory side, and arguably there&#39;s not enough work on that because theory is where the bottleneck is, not on getting more data.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, I think we should definitely be doing both. Ideally, there needs to be synergy where experiments produce new phenomena for theorists to explain and theory inspires the experiments. The theorists should be telling the experimentalists which questions and what kind of experiments are the most interesting and we should have this kind of synergy. But I think that in the current landscape—definitely in AI alignment—the theory side is currently left behind. That&#39;s where I think we should put the marginal efforts in.<br><br> <strong>Will Petillo:</strong> Do you see that synergy existing now? Like, is OpenAI asking MIRI for feedback on their experiments, or is there any kind of connection, or are people just siloed off from each other?<br><br> <strong>Vanessa Kosoy:</strong> I think it doesn&#39;t exist now, almost at all. OK, no, to be fair, it exists in some areas and much less in other areas. For example, there&#39;s people working on Singular Learning Theory. I think that they are much more interfaced with experimental work, which is good. The kind of research that MIRI is doing is and the kind of research I&#39;m doing is much less interfaced with experimental work. I have some plans for creating an experimental group working in a close loop with me on those questions as part of my big, long term plans, but I still haven&#39;t gotten around to doing that yet.<br><br> <strong>Will Petillo:</strong> If you could change anyone&#39;s mind, or set political and business agendas, what would you like to see happen to have more of an interface?<br><br> <strong>Vanessa Kosoy:</strong> First of all, we just need more theorists. To have an interface, we need something to interface with, so we just need more theorists. I think that this is, in practice, where the bottleneck is now. Once this progress in theory gets sufficiently paced there will be a bunch of questions. I mean, there are already questions that I would like to see experiments on, but the more this thing picks up, the more such questions we will have. I think now the main bottleneck is just in having more people working on this theory.<br><br> <strong>Will Petillo:</strong> What would change from an external perspective if there was a lot more theory work? I imagine a skeptic could argue: &quot;OpenAI and these other companies are making these really awesome breakthroughs in a largely experimental kind of way, and it&#39;s working great! If it ain&#39;t broke, don&#39;t fix it!&quot; What&#39;s broken, in your view?<br><br> <strong>Vanessa Kosoy:</strong> I think that the current path is leading us to a disaster. I think that companies like OpenAI and other leading labs are wildly overconfident about their ability to solve problems as they come along. I think that they haven&#39;t come up with any convincing solutions to the hard parts of the problem, and they don&#39;t even have the tools to do this because of a lack of theoretical understanding. We don&#39;t even have models that are precise enough to have a solution in which we could really be confident. We need to be very precise about the arguments which convince us that the solution is good. And we don&#39;t even have the tools to reach this type of precision.<br><br> What the companies are doing is basically just developing things in trial and error. If we see any problems, then we&#39;ll just tweak the thing until the problem goes away. That&#39;s a bandaid method, which is to say it works until it doesn&#39;t work. It fixes problems on a superficial level, but eventually there will come a point where either the problem will not be caught in time and the results will be catastrophic, or the problems will be caught in time, but then nobody will have any idea what to do in order to fix it. And eventually someone is going to do the catastrophic thing anyway.<br><br> The only thing which makes me less pessimistic than other people in MIRI is that I think we still have more time. I don&#39;t think they&#39;re quite as close to AGI, and I think that a lot of things can change during this time. Which is again, not to say they will change—we might burn all this time and still end up with a catastrophe.<br><br> <strong>Will Petillo:</strong> What&#39;s an example of an existing problem that only has superficial solutions?<br><br> <strong>Vanessa Kosoy:</strong> I mean, the real problem we&#39;re concerned about is not really an existing problem, right? The main thing we&#39;re concerned about is that future AI systems—which will be much more powerful than existing AI systems—will bring about extinction of the human race or a catastrophe on a similar level.<br><br> That&#39;s not an existing problem for the simple reason that the AI systems we have today are not capable of learning a model of the world that&#39;s so sophisticated that it enables you to do these types of actions. But even now, the companies struggle with all the things that happen with Large Language Models, such as the infamous jailbreaks where they&#39;re trying to make them well-behaved in various ways. Not telling the users offensive, dangerous information, for example, and the users easily find jailbreaks to work around that, or just tell false answers.<br><br> But again, for me, that&#39;s not the real problem, it&#39;s just an analogy. I mean, they&#39;re kind of struggling with these very simple, much easier problems now, which is not to say they won&#39;t solve them. Trial and error will get you there eventually. The reason trial and error is not the solution for existential risk is because once everyone is dead, the trial is over. There&#39;s no more trial. So the problems we have now, they&#39;re still struggling with them because they don&#39;t have principle tools to solve them, but eventually they will trial-and-error their way through and will patch them somehow, or at least solve them well enough for it to be economical. But once you reach the point where failures are global catastrophes, trial and error is no longer an acceptable method of fixing the problem.<br><br> <strong>Will Petillo:</strong> Obviously we don&#39;t get to see lots of test data of the world ending. But I would imagine there&#39;d be some precursor issues that are smaller, but hint at what&#39;s to come. Do you see the challenges with hallucination or not being able to control what the AI says as those kinds of precursors? Or are they totally irrelevant and there just won&#39;t be any precursor issues?<br><br> <strong>Vanessa Kosoy:</strong> It&#39;s a difficult question, because there are very important bits that are still missing from existing AI systems to produce existential risks. We can point at examples where systems are kind of mis-generalizing, there&#39;s a lot of famous examples: some program that &quot;wins&quot; Tetris by pausing the game forever, or that wins some boat race game by racing the boats in infinite circles, doing various weird unintended behaviors, because the metric that the algorithm is maximizing is not actually what the users intended. You could call those precursors, but I feel like it&#39;s not exactly capturing the magnitude of the problem because those are still toy settings. There are no open-world systems that are acting in the open, physical world. The goals that they&#39;re trying to solve are much simpler than human values; there&#39;s not really operating domains where there are really complex ethical considerations.<br><br> Maybe Large Language Models are starting to approach this because they enter domains where there are some, at least morally, not completely trivial issues that come up. On the other hand, Large Language Models are not really doing things that are strongly superhuman. Well, they may be superhuman in the sense that they have a very large breadth of knowledge compared to humans, but not in other senses.所以这很难。 There are things that are sort of analogous, but it&#39;s not strongly analogous.<br><br> But then again, our motivation to be concerned about this risk doesn&#39;t come from looking at LLMs. Eliezer Yudkowsky started talking about those things before Deep Learning was a thing at all. That&#39;s not where the motivation comes from.<br><br> <strong>Will Petillo:</strong> I guess the reason I was asking about it is that in the places where this stuff gets debated and polarized, one of the common objections is: &quot;There&#39;s no evidence behind this! This is all just storytelling!&quot; <i>Is</i> there evidence of the danger or does it really just come from looking at the math?<br><br> <strong>Vanessa Kosoy:</strong> The problem is, what do you call evidence? That&#39;s a very complicated question. The things that would be obvious evidence would be things like: AI&#39;s completely going out of control, breaking out of the box, hacking the computer, copying themselves to other computers, outright manipulating human operators, and so on. But this kind of thing is a sort of canary that you only expect to see very, very, very close to the point where it&#39;s already too late. It&#39;s not possible to say that we will only rely on this type of evidence to resolve the debate.</p><p> For other types of evidence, some people say that evolution is sort of evidence how a Machine Learning algorithm can produce something which is completely unaligned with the original algorithm. Other people show you Reinforcement Learning algorithms doing not what the designer intended. But for every argument like that, you could have a counter argument which says, &quot;yeah, but this example is not really similar. We cannot really project from there to existential risk because there are some disanalogies.&quot;<br><br> And yeah, there will always be some disanalogies because until you have AIs in the real world that are very close to being an existential risk, you won&#39;t have anything that&#39;s precisely analogous to something presenting an existential risk. So we have no choice but to reason from first principles or from math or by some more complicated, more multi-dimensional analysis. We just have no choice. The universe doesn&#39;t owe it to us to have a very easy, empirical way of testing whether those concerns are real or not. One of the things I&#39;m hoping for is that the theory will bring about stronger arguments for AI being dangerous—or the theory will tell us no, everything is fine, and we can all relax. The lack of theory is part of the reason why we don&#39;t have foolproof, completely solid arguments in one direction or the other direction.<br><br> <strong>Will Petillo:</strong> The challenge with finding evidence is that anything you can point to that exists now could be interpreted in multiple ways. Having solid theory would lend some credence to one interpretation over another.<br><br> <strong>Vanessa Kosoy:</strong> Yeah, absolutely. If you have a theory that says that a particular type of misgeneralization is universal across most possible machine learning systems, and we also see this type of misgeneralization happening in real Machine Learning systems, then it would be much harder to dismiss it and say, &quot;oh yeah, here we have this problem, but we&#39;ll do this and that, and that will solve it easily.&quot;<br><br> <strong>Will Petillo:</strong> There&#39;s one thing that&#39;s still bugging me about the issue of evidence not being available now. The analogy my mind immediately goes to is climate change. You could say that &quot;Oh, the idea of large swaths of the world being uninhabitable is just this elaborate story because all that has never happened before!&quot; But then you can look at a bunch of things that exist already: small scale disasters, the graphs of CO2 versus temperature, and so on, point to those and say, &quot;Hey, the really bad stuff hasn&#39;t happened yet, but there is a lot of evidence that it will!&quot; What makes AI different?<br><br> <strong>Vanessa Kosoy:</strong> I think that climate change is a great analogy. The big difference is that in climate change, we have a really good theory. Like in climate change, we have physics, right? And we have planetary science, which is on a very, very solid foundation. And we have computer simulations. It&#39;s still not trivial, there are some chaotic phenomena which are hard to simulate or predict, so not everything is completely trivial, but still we have some very, very strong theoretical foundation for understanding how those things work and what are the mechanisms. And this theory is telling us that there&#39;s still big uncertainty intervals around how exactly many degrees of warming we&#39;re going to get with such and such amount of CO2, but we still have a fairly solid prediction there.<br><br> Whereas with AI, we don&#39;t have this. The analogous situation, if you want to imagine climate change, AI style, then it would be something like not having a theory which explains why CO2 leads to warming. Having some empirical correlation between temperature and CO2, and then people could argue ad infinitum. Correlation is not causation, maybe the warming is caused by something completely different, maybe if we do some unrelated thing it will stop the warming, which is not actually true. We would be in the dark. With AI, we&#39;re currently in the dark.<br><br> <strong>Will Petillo:</strong> What is happening currently with your work at MIRI?</p><p> <strong>Vanessa Kosoy:</strong> Currently there are multiple problems I&#39;m looking at. Hopefully I will publish very soon a paper on imprecise linear bandits, which is related to Infra-Bayesianism that I mentioned before, which is a theory of agents that reason about complicated worlds. That&#39;s analyzing this theory in some very simple, special case in which I succeeded to get some precise bounds for how much data an algorithm would need to learn particular things. After that, I&#39;m starting to look into the theory of learning state representations in Reinforcement Learning, which is currently another big piece missing from the theory, which is about how your algorithms should learn about which features of the world are actually important to focus在。<br><br> In parallel, I have a collaborator, Gergely Szucs, who is working on using my theory of Infra Bayesian Physicalism to create a new interpretation of quantum mechanics. He has some really interesting results there. It&#39;s kind of a test case which demonstrates how this framework of thinking about agents allows you to solve all sorts of philosophical confusions. In this case, it&#39;s confusions that have to do with the interpretation of quantum mechanics. Scott Garrabrant has a project about a new type of imprecise probabilities, some new way of representing beliefs that have some nice compositionality properties. Kaspar Osterheld from Carnegie Mellon and Abram Demski had a paper recently about some new type of frequentist guarantees for algorithms that are making decisions based on something that&#39;s similar to a prediction market. So yeah, a lot of interesting things are happening.<br><br> <strong>Will Petillo:</strong> Are there any other questions that I did not ask that would be helpful for someone seeing this to get a sense of what you&#39;re about here?<br><br> <strong>Vanessa Kosoy:</strong> Not a question exactly, but I also have a more concrete approach for how to actually solve alignment, how to actually design an aligned agent, which I call Physicalist Superimitation. It&#39;s a variation on the theme of value learning, but it draws from the framework of Infra Bayesian Physicalism, which comes from the Learning Theoretic Agenda and from some ideas in algorithmic information theory to come up with a semi-formal approach to how you could have an AI that learns human values in a robust way.<br><br> It deals with a lot of problems that other approaches to value learning have, like: how do you determine where the boundaries of an agent are?什么是人？ How do you locate this human in space? How do you take into account things which are not just behavior, but also internal thought processes of the human in inferring the human&#39;s values? How do you prevent perverse incentives such as the AI somehow changing or manipulating humans to change their values? How do you avoid the inner alignment problem? It answers a range of concerns that other approaches have.<br><br> <strong>Will Petillo:</strong> This sounds reminiscent of Inverse Reinforcement Learning?<br><br> <strong>Vanessa Kosoy:</strong> Inverse Reinforcement Learning is the idea that we should look at behaviors of humans, infer what those humans are trying to do, and then we can do this thing. &quot;We&quot; as an AI. So I actually have presentations in which I explain Physical Superimitation as Inverse Reinforcement Learning on steroids. It&#39;s taking this basic idea, but implementing it in ways that solve a lot of the deep problems that more simplistic approaches have. One problem that simplistic approaches have is that they model humans as perfect agents that follow the perfect policy, given perfect knowledge of the environment, which is wildly unrealistic.<br><br> Instead, I model humans as learning agents. They learn things as they go along. And they also might even do that imperfectly. Another thing is the issue of boundaries. What is a human exactly? Where do you put the boundaries around a human? Is there just some particular input and output, which the human uses and you consider everything that goes through this port to be the human? But then how do you deal with various discrepancies between what goes into this port and what the human actually intended to do, or various possibilities like the AI hijacking this channel?<br><br> In my approach, the way a human is formalized is that a human is a particular computation that the universe is running. This is something I can actually formalize using Infra Bayesian Physicalism. It has particular properties, which make it agentic, so the agent detects which computations the universe is running, among them detects which computations are agents, and amongst those agents, it selects which agent is its user by looking into causal relationships, and this way it homes onto the boundary of the agent. The first thing is because we&#39;re talking about the computation that this human is running, which is human reasoning and regarded as a computation. We&#39;re also automatically looking at internals as internal thought processes and not just things that are expressed as external behaviors. So we have potentially much more information there.<br><br> <strong>Will Petillo:</strong> What would be the best way for someone to get involved? And what would they want to learn in advance?<br><br> <strong>Vanessa Kosoy:</strong> One thing they could immediately start doing is reading up on stuff people did in Agent Foundations and in the Learning-Theoretic Agenda until now. I have this recent post, <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><u>Learning Theoretic Agenda: Status 2023</u></a> , which summarizes a lot of the things. I also have a <a href="https://www.alignmentforum.org/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list"><u>reading list post</u></a> where I recommend some background reading for people who want to get into the field. More concretely in terms of career steps, it&#39;s already too late to apply, but I&#39;m running a track in <a href="https://www.matsprogram.org/"><u>MATS</u></a> <i>,</i> which is a training program for researchers who want to get into AI safety. I have a track focused on the Learning Theoretic Agenda. Hopefully there will be another such track next year. I also have a fantasy of having an internship program, which would bring people to Israel to work with me on this. Currently, because of the war, this thing has been postponed, but hopefully, eventually things will settle down and I will revive this project. Those are currently the main ways to get involved.<br><br> <strong>Will Petillo:</strong> Thank you for that description. I wish you the best in developing this theory and gaining more interest so that mismatch between evidence and theory starts to get corrected and the researchers know what they&#39;re doing rather than stumbling in the dark!<br><br> <strong>Vanessa Kosoy:</strong> Thank you for having me.</p><br/><br/> <a href="https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QPxrH5ex6MpYoLwer/interview-with-vanessa-kosoy-on-the-value-of-theoretical<guid ispermalink="false"> QPxrH5ex6MpYoLwer</guid><dc:creator><![CDATA[WillPetillo]]></dc:creator><pubDate> Mon, 04 Dec 2023 22:58:42 GMT</pubDate></item></channel></rss>