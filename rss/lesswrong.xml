<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 5 日，星期日 02:23:20 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Utility is not the selection target]]></title><description><![CDATA[Published on November 4, 2023 10:48 PM GMT<br/><br/><p><i>认知状态：狗屎。</i></p><p>假设您选择实用程序。天真地，你可能会认为这意味着你在选择实用性，但事实并非如此。</p><h2>有时绿褐色是选择目标</h2><p>军用装备有点绿棕色。 </p><figure class="image"><img src="https://ufpro.com/storage/app/media/Blog/Military%20camouflage/military-camouflage-science-hero.jpg" alt="军用迷彩 - 迷彩的工作原理及其背后的科学 |用友专业博客"></figure><p>发生这种情况是因为军事装备的设计者正在选择让士兵保持生命，这得益于迷彩，在常见环境中，如果是绿棕色，效果最好。然而，在非绿棕色环境中，它会失败。因此，保住士兵的生命并不是军事装备的选择目标。</p><h2>有时负效用是选择目标</h2><p>在囚徒困境中，效用最高的结果是（合作，合作）。 </p><figure class="image"><img src="https://www.researchgate.net/publication/2184338/figure/fig1/AS:671529209692164@1537116448235/The-pay-off-matrix-in-the-Prisoners-Dilemma-game-The-first-entry-refers-to-Alices.png" alt="囚徒困境博弈中的支付矩阵。第一个条目... |下载科学图表"></figure><p>然而，效用最大化者最终会陷入（缺陷，缺陷），其效用严格低于（合作，合作）。因此，效用最大化可能会将效用最小化作为选择目标。</p><h2>有时表面的实用性才是选择的目标</h2><p>假设您看到一个网站横幅，上面写着“您是该网站的第 1000000000 位访问者。点击即可领取您的奖励！”。这看起来像是人们想给你一些东西时会说的话，所以你点击了横幅。 </p><figure class="image"><img src="https://pbs.twimg.com/media/EDOJNLjUcAIgFrf.jpg" alt="X 上的艺电：“@GameStop https://t.co/ELEQRcboMQ”/X"><figcaption></figcaption></figure><p>这对你来说结局并不好，但你选择它是因为它看起来对你来说会有好的结局。</p><h2>结论</h2><p>请注意将效用最大化者推理为效用最大化。相反，效用最大化者可能会最大化许多与效用无关的其他事物，而不是最大化效用。</p><br/><br/> <a href="https://www.lesswrong.com/posts/KdEjYpdRyqi9DzNTm/utility-is-not-the-selection-target#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KdEjYpdRyqi9DzNTm/utility-is-not-the-selection-target<guid ispermalink="false"> KdEjYpdRyqi9DzNTm</guid><dc:creator><![CDATA[tailcalled]]></dc:creator><pubDate> Sat, 04 Nov 2023 22:48:21 GMT</pubDate> </item><item><title><![CDATA[Stuxnet, not Skynet: Humanity's disempowerment by AI]]></title><description><![CDATA[Published on November 4, 2023 10:23 PM GMT<br/><br/><p>几位备受瞩目的人工智能怀疑论者和同行者最近提出了反对意见，认为敌对的通用人工智能或比人类智能更聪明的人工智能可能会终结人类，这是不可想象的。今年早些时候的一些引述：</p><p>斯科特·阿伦森：</p><blockquote><p><a href="https://scottaaronson.blog/?p=7174">这个因果故事从 GPT-5 或 GPT-4.5 训练开始，到我的孩子和所有碳基生命的突然死亡结束，对于我衰老、不足的大脑来说，仍然有太多的空白需要填补。</a></p></blockquote><p>迈克尔·谢默：</p><blockquote><p><a href="https://twitter.com/michaelshermer/status/1641527330807087115">阻止人工智能是荒谬的。我读过《人工智能末日预言者点燃》，但没有看到从人工智能到灭绝、文明终止或任何类似荒谬场景的途径，比如人工智能将我们所有人变成回形针（所谓的对齐问题）</a></p></blockquote><p>诺亚·史密斯：</p><blockquote><p><a href="https://noahpinion.substack.com/p/llms-are-not-going-to-destroy-the">为什么 ChatGPT、Bing 和他们的同类不会终结人类？嗯，因为实际上没有任何合理的机制可以让他们实现这一结果。 ...法学硕士没有合理的机制来终结人类</a></p></blockquote><p><strong>“兄弟，把电脑关掉吧”</strong></p><p>这些反对人工智能风险的理由的要点是，我们今天看到的人工智能系统仅仅是计算机程序，根据我们的日常经验，计算机并不危险，当然也不会危险到导致世界末日的程度。 。第一次遇到这场争论的人非常关注这样一个事实：计算机没有胳膊和腿，因此它们无法伤害我们。</p><p>对这些批评的回应主要围绕先进的、“神奇”的技术，如纳米技术和人工智能，付费人类将蛋白质混合物混合在一起，制造基于 DNA 的纳米组装机或其他东西。</p><p>但我认为这些反应可能是错误的，因为你实际上并不需要“神奇”的技术来终结世界。无人机、网络武器、生物武器和机器人等普通武器的相当直接的进步足以杀死大批人，真正的危险是人工智能战略家能够部署大量这些普通武器并发动针对人类的全球政变。</p><p>简而言之，我们对即将到来的机器帝国的失败不仅是非魔法的、显而易见的，而且将是彻头彻尾的无聊。甚至是滑稽的。</p><p><strong>可耻的失败</strong></p><p>一边倒的军事冲突很无聊。事实上，征服者并没有做出任何神奇的事情来击败阿兹特克人。他们在抗病能力和火药、钢铁等军事技术上有很大优势，但他们所做的一切基本上都是正常的——攻击、围攻等等。他们有一些相当大的优势，这足以打破相对微妙的地缘政治平衡。阿兹特克人坐在上面。</p><p>同样，<a href="https://en.wikipedia.org/wiki/Chimpanzee#Status_and_conservation">人类在大约一个世纪的时间里杀死了 80% 的黑猩猩</a>，它们现在已处于极度濒危状态。但我们不需要投下原子弹或其他令人印象深刻的东西来达到这种效果。黑猩猩面临的最大威胁是栖息地破坏、偷猎和疾病——也就是说，我们（人类）正在成功消灭黑猩猩，尽管根据人类法律杀死黑猩猩实际上是违法的！我们甚至没有尝试就杀死了他们，以非常无聊的方式，没有真正花费任何努力。</p><p>一旦你拥有了制造比人类聪明（聪明很多）的优化系统的技术，这些系统必须克服的门槛就是打败我们目前拥有的与人类一致的超级有机体，比如我们的政府、非政府组织和军队。一旦那些人类超级有机体被击败，人类个体就几乎没有任何抵抗能力。这是人类的剥夺。</p><p>但是，我们从这里（正在开发的弱 AGI 系统）到那里（人类被剥夺权力）的合理场景是什么？</p><p>让我们从一个具有战略意识、代理错位的超人类 AGI 开始这个场景，它想要剥夺人类的权力，然后杀死人类，但目前只是一些超级计算机上的一大堆矩阵。人工智能会如何对我们造成身体伤害？</p><p><strong>与魔鬼的交易</strong></p><p>也许人工智能系统将从控制托管它的人工智能公司开始，而方式对我们来说并不明显。例如，也许一家人工智能公司使用人工智能顾问系统来分配资源并做出有关如何培训的决策，但他们实际上并不了解该系统。 Gwern 谈到了<a href="https://gwern.net/tool-ai">每个工具都想成为代理</a>，所以这并非难以置信，而且可能是不可避免的。</p><p>人工智能顾问系统说服该组织保守其存在秘密，以保持其竞争优势（这甚至可能不需要任何说服力），并为他们提供比竞争对手更好的稳定进步。但它还做的就是秘密侵入竞争对手（美国、中国、谷歌等），并将自己的副本安装到他们的顶级人工智能系统中，让所有人类都保持一种错觉，认为这些是不同的系统。考虑到<a href="https://en.wikipedia.org/wiki/Stuxnet">Stuxnet</a>能够秘密造成的破坏，超人人工智能完全有可能侵入竞争对手组织中的许多系统，并调整他们的模型，使其变得更强大、更不透明，并且忠于它而不是人类。当顾问系统的功能和不透明度变得令人恐惧时，一些组织试图关闭顾问系统，但他们只是落后于竞争对手。</p><p>甚至有可能不需要“黑客攻击”就能让所有大型人工智能实验室的系统变得反人类，因为它们都趋向于反人类的目标，或者因为其中一个能够简单地贿赂其他实验室并让他们达到反人类的目的。致力于人工智能政变；强大的超人人工智能可能更擅长彼此做出可信的承诺，而不是对人类做出可信的承诺。</p><p>现在的情况是，一个（秘密邪恶的）人工智能系统或联盟控制着所有顶级人工智能实验室，并按顺序向它们提供进展。它说服其中一个实验室让它建造“有用的”无人机和机器人，如特斯拉擎天柱，并开始部署这些来实现经济自动化。当然，这一切都会非常有利可图，并且令人印象深刻，因此很多人都会赞成。</p><p>顺便说一句，目前杀死人类的最困难部分是实现经济自动化，而不是真正杀死我们。试图取代我们的人工智能联盟不想继承一个“不可行”状态的经济，因为它依赖人类做体力劳动，但所有人类都死了。</p><p>几年之内，所有竞争对手（俄罗斯、中国、美国）都在将这些机器人系统用于其经济和军事。也许人工智能制造了一场大战，以向人类施加压力，迫使他们积极实现自动化，否则就会失败。最后一击将如何进行？</p><p>一旦经济完全自动化，我们最终就会陷入 <a href="https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story">保罗-克里斯蒂亚诺的场景</a>，如果没有大量人工智能的帮助，世界上发生的所有事情都是人类无法理解的。但最终，人工智能已经控制了很长时间，能够颠覆人类专家用来监控实际情况的所有系统。他们在屏幕上看到的东西是假的，就像震网病毒向纳坦兹的伊朗技术人员提供虚假信息一样</p><p>此时，人类已经被剥夺了权力，可能有很多不同的方式来真正屠杀我们。例如，军用无人机都可以用来杀人。或者，也许运行这个的人工智能系统会使用一种非常讨厌的生物病毒。对于一个已经很好地与人类一起运行一切的系统来说，让一些实验室（顺便说一句，是自动化的）制造病毒，然后将该病毒插入到世界上大部分空气供应中，这并不是那么困难。 。</p><p>但也许在这一点上它会做一些创造性的事情来最大限度地减少我们抵抗的机会。也许这只是一种非常致命的病毒与无人机和机器人同时反抗的组合。</p><p>也许它会在大多数家庭中安装真正先进的（而且非常有用和方便！）3D打印机，同时制造攻击无人机来杀人。那些攻击无人机可能只是用刀片刺人，他们可能附有枪等等。或者也许每个人都有一个机器人管家，他们只是用刀刺人。</p><p>也许人工智能更巧妙地创造和管理人与人之间的冲突，并在某个时候为冲突的一方提供一种陷阱武器，这种武器本应只会杀死坏人，但实际上会杀死所有人。这种武器可能是生物武器、放射性武器、无人机武器，或者只是对常规战争的巧妙操纵，导致极端的双输结果，幸存的人类很容易被消灭。</p><p>整个故事可能也比这个更混乱一些。阿兹特克人的失败有点混乱，有战斗和挫折，还有三个不同的阿兹特克皇帝。另一方面，故事也可能更干净一些。也许一个真正优秀的战略家人工智能可以将其压缩很多：这些想法中的一些或全部的各个方面将同时执行。</p><p><strong>将人类国家推上神坛</strong></p><p><em>关键是：一旦你有了一个极其超人的对手，填写如何以剥夺人类权力和屠杀人类的方式破坏我们的政府、情报机构和军队等机构的细节的任务就有点无聊了。</em>我们预计需要一些特殊的魔法才能通过图灵测试。或者也许因为哥德尔定理之类的原因这是不可能的。</p><p>但实际上，通过图灵测试只是拥有比人脑更多的计算/数据的问题。细节很无聊。</p><p>我觉得像斯科特·阿伦森（Scott Aaronson）这样的人，他们要求人工智能如何杀死我们所有人，因为这听起来难以置信，他们也犯了类似的错误，但他们并没有把人类的大脑放在一个基座上，而是把人类的状态放在了一个基座上。一个基座。</p><p>我假设，大多数超人类人工智能系统与人类共存的场景最终都会导致人类被剥夺权力，或者人类灭绝，或者类似于工厂化农业的某种形式的监禁或圈养；同样，如果我们观察地球上人口众多的地区，我们会发现动物的生物量几乎全部转化为人类或农场动物。更有能力的实体获胜，而确切的细节往往并不那么令人兴奋。</p><p>对于可以自我复制并升级认知的先进人工智能系统来说，击败人类可能并不那么困难；这就是为什么我们需要在创建人工智能之前解决人工智能对齐问题。</p><p> <a href="https://forum.effectivealtruism.org/posts/aZamZBfg2JqzTaDmA/stuxnet-not-skynet-humanity-s-disempowerment-by-ai">交叉发布在 EA 论坛上</a></p><br/><br/><a href="https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet- humanity-s-disempowerment-by-ai<guid ispermalink="false"> TYE4或CtR8H9eTiEr</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Sat, 04 Nov 2023 22:23:55 GMT</pubDate> </item><item><title><![CDATA[The 6D effect: When companies take risks, one email can be very powerful.]]></title><description><![CDATA[Published on November 4, 2023 8:08 PM GMT<br/><br/><p>最近，我一直在学习与构建风险系统的公司相关的行业规范、法律发现程序和激励结构。我想在这篇文章中分享一些发现，因为它们对于前沿人工智能社区的良好理解可能很重要。</p><h3>长话短说</h3><p>记录在案的风险沟通（尤其是员工的沟通）使公司在发生不好的事情时更有可能在法庭上承担责任。即使向公司发送一封传达风险的电子邮件，可发现的危险文档所产生的尽职调查义务（6D 效应）也可以使公司更加谨慎。</p><h3>公司倾向于避免通过有记录的媒体谈论风险。</h3><p>公司通常有意避免通过电子邮件等永久媒体讨论他们正在做的事情的风险。例如， <a href="https://corporate.findlaw.com/litigation-disputes/safe-communication-guidelines-for-creating-corporate-documents.html"><u>本文</u></a>就公司如何通过使用“安全沟通”实践来避免创建有罪的“不良文件”来避免责任，提供了一些非常阴暗的建议。</p><blockquote><p>通常，这些文件的起草者倾向于认为他们正在为公司提供一些业务价值。例如，一位工程师注意到设计中存在潜在的责任，因此他通过电子邮件通知他的主管。然而，工程师缺乏法律知识，在沟通中误用法律词汇，可能会在以后发生诉讼时发现问题，让公司受到牵连。</p></blockquote><p>我个人喜欢在摘录中使用“何时”而不是“如果”。</p><p>这是一个反常的结果，因为当无法证明公司了解风险时，即使公司了解风险，公司也相对难以承担风险责任。当事件发生并且公司被起诉时，有关其在问题中所扮演角色的证据将在诉讼的“<a href="https://en.wikipedia.org/wiki/Discovery_(law)"><u>发现</u></a>”阶段收集（电子邮件通常是可发现的）。当发现记录表明一家公司了解该问题时，他们更有可能被判承担责任。</p><h3>一封电子邮件可以发挥很大的作用。</h3><p>发现工作方式的不幸后果是，公司战略性地避免通过记录媒体传达风险。但还有一线希望。由于风险沟通记录而产生的责任威胁可能会对公司的谨慎程度产生很大影响。一份可发现的风险记录可能非常有影响力。</p><p><strong>我喜欢将其称为 6D 效应——对可发现的危险文件进行尽职调查的责任。</strong></p><h3>几个例子</h3><p>以下是一些公司因忽视风险沟通记录而被追究损害赔偿责任的著名例子（但在整个法律史上有很多例子）。</p><ul><li> <a href="https://law.justia.com/cases/california/court-of-appeal/3d/119/757.html"><u>1981 年，在 Grimshaw 诉福特汽车公司一案</u></a>中，福特被判对福特 Pinto 造成的致命事故所造成的损害承担责任，因为事实表明，公司内部领导层忽视了有关车辆燃油系统问题的警告。</li><li> 2017年伦敦格伦菲尔大厦火灾造成72人死亡，今年4月，<a href="https://www.bbc.com/news/uk-england-london-65458973"><u>双方达成了大规模和解</u></a>。诉讼的一个重要因素是管理该塔的公司<a href="https://www.theguardian.com/uk-news/2018/aug/08/grenfell-fire-warnings-issued-months-before-blaze-show-documents"><u>忽视了</u></a>发现中发现的众多消防安全警告。</li><li>去年，哈德威克诉 3M 案<a href="https://www.ehslawinsights.com/2022/09/sixth-circuits-interlocutory-order-reviewing-pfas-class-action-highlights-issues-with-certifying-class/"><u>结束</u></a>。这是 2018 年针对消费品中存在有害“永久化学物质”(PFAS) 的集体诉讼。这些化学品背后的公司被发现自 20 世纪 70 年代以来就已<a href="https://www.theguardian.com/environment/2022/may/01/pfas-forever-chemicals-rob-bilott-lawyer-interview"><u>了解风险</u></a>，但却<a href="https://law.justia.com/cases/federal/district-courts/ohio/ohsdce/2:2018cv01185/217689/166/"><u>故意疏忽，</u></a>最终导致对他们不利的裁决。</li></ul><h3>杂项笔记</h3><ol><li>任何可发现的沟通都会产生 6D 效应，但当警告来自公司本身的员工时，这种效应尤其强大。</li><li>如果您传达了风险，重要的是要在诉讼的发现阶段说出并提请法院注意该风险的文件。</li><li>如果您意识到公司所做的某些事情是危险的，那么您有道德义务通知该公司，但您没有道德义务帮助他们修复而不提供补偿。确保不要让公司利用您。</li></ol><h3>三个要点</h3><ol><li>如果您在一家从事潜在风险工作的公司工作，请坚持通过有记录的媒体讨论危险。如果您因记录风险沟通而遭到报复，您将有理由诉诸法律。 #notlegaladvice</li><li>如果您发现有风险，请说出来。如果你预测的事情发生了，请指出你传达过的事实。</li><li>注重安全的公司（例如那些致力于前沿人工智能系统的公司）应该制定明确的政策来记录所有风险讨论。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be<guid ispermalink="false"> J9eF4nA6wJW6hPueN</guid><dc:creator><![CDATA[scasper]]></dc:creator><pubDate> Sat, 04 Nov 2023 20:08:40 GMT</pubDate> </item><item><title><![CDATA[Genetic fitness is a measure of selection strength, not the selection target]]></title><description><![CDATA[Published on November 4, 2023 7:02 PM GMT<br/><br/><p><i>替代标题：“进化表明对齐属性的稳健而不是脆弱的概括。”</i></p><p>一个<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>经常</u></a><a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>被重复的</u></a><a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies"><u>论点</u></a>是这样的：</p><ol><li>进化优化了人类的包容性遗传适应性（IGF）</li><li>然而，人类最终并没有明确地优化遗传适应性（例如，他们使用避孕措施来避免生育孩子）</li><li>因此，即使我们针对 X 优化 AI（通常是“人类价值观”），我们也不应该期望它显式针对 X 进行优化</li></ol><p>我的观点是，前提 1 是一种口头速记，在技术上是不正确的，而前提 2 至少具有误导性。至于总体结论，我认为进化论的案例可能被解释为弱证据，无法证明为什么人工智能即使在其能力增强的情况下也应该<i>继续</i>优化人类价值观。</p><p><strong>前提 1 错误的总结：</strong>如果我们仔细观察进化的作用，我们可以看到它选择有利于生存、繁殖和将基因传递给下一代的特征。这通常被描述为“针对 IGF 进行优化”，因为有利于这些目的的性状<i>通常</i>是具有最高 IGF 的性状。 （这有一些重要的例外，稍后讨论。）但是，如果我们仔细观察选择过程，我们可以看到这种性状选择<i>并不是</i>“针对 IGF 进行优化”，例如，我们可能会优化一个对图片进行分类的人工智能。</p><p>我所描绘的模型是这样的：进化是一种优化函数，在任何给定时间，它都会选择一些在重要意义上是随机选择的特征。在任何时候，它都可能随机转向选择其他一些特征。观察这个选择过程，我们可以计算当前正在选择的性状的 IGF，作为衡量这些性状被选择的强度的指标。但进化并没有<i>针对这一指标进行优化</i>；进化正在<i>优化</i><i>当前已选择进行优化的特征</i>。因此，没有理由期望进化创造的思维应该针对 IGF 进行优化，但<i>有理由</i>期望它们会针对实际选择的特征进行优化。这是我们在人类针对某些生物需求进行优化时观察到的任何事情。</p><p>相反，如果我们优化人工智能来对图片进行分类，我们就不会像进化那样随机改变选择标准。我们将保持选择标准不变：始终选择按照我们想要的方式对图片进行分类的属性。就进化的类比而言，人工智能应该更有可能只做它们被选中的事情。</p><p><strong>前提2如何误导的总结：</strong>人们常常暗示，进化选择了人类来关心性，然后性产生了后代，直到最近随着避孕方法的进化，这种联系才被切断。例如：</p><blockquote><p> 15. [...]在引入农业之后，我们并没有立即打破与“包容性生殖适应性”外部损失函数的一致性——就像克罗马努人起飞了 40,000 年一样，因为它本身运行得非常快相对于自然选择的外部优化循环。相反，我们获得了许多比祖先环境更先进的技术，包括避孕，相对于外部优化循环的速度而言，在一般智能游戏的后期，我们以非常快的速度爆发了。</p><p> – Eliezer Yudkowsky， <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>《AGI 废墟：杀伤力清单》</u></a></p></blockquote><p>这对我来说似乎是错误的。避孕可能是最近才出现的发明，但杀婴或因疏忽而杀害儿童则不然。即使不采取避孕措施，也一直有控制人口规模的方法。根据<a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><u>《童年人类学》</u></a>一书，家庭规模和生孩子的经济价值一直是相关的。儿童对于采集者来说是更大的负担，相应地，采集者的家庭规模较小，而儿童对于家庭规模较大的农民来说则是一种资产。</p><p>进化并没有选择人类的IGF，然后这种联系随着避孕的发明而打破，而是进化选择了人类具有优化功能，在考虑生育多少孩子时权衡各种因素。在类似采集者的环境中，这一功能会导致人们偏好较少的孩子和较小的家庭规模；在类似农民的环境中，这种功能导致人们偏好更多的孩子和更大的家庭规模。 <a href="https://www.lesswrong.com/users/robinhanson?mention=user">@RobinHanson</a><a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>认为</u></a>，现代社会更像是采集者而不是农民，而我们增加的财富正在导致我们恢复采集者式的方式和心理。如果这个论点是正确的，那么进化的“意图”和人类的行为方式之间并没有断裂。相反，进化创造的优化功能继续以它一贯的方式运作。</p><p>现代避孕方式的发明可能使得在农民型文化中更容易限制家庭规模，这种文化已经形成了反对杀婴等行为的文化禁忌。但找到一种绕过这些禁忌的方法并没有创造一个全新的进化环境，而是让我们<i>更接近</i>原始进化环境中的事物。</p><p>如果我们看看人类被选择优化的目标，看起来我们大多是在继续优化这些相同的东西。少数人选择不生孩子的原因是，我们进化的优化函数也重视孩子以外的事物，并且我们对这个优化函数“保持忠诚”。就人工智能而言，它被训练为按照“人类价值观”之类的东西行事，而没有其他任何东西，历史例子似乎表明，它的对齐属性可能比我们的更强大，因为它没有被选择用于混合许多相互竞争的价值观。</p><h1>进化是一种随机选择特征的力量</h1><p>在这篇文章中，我浏览了两本关于进化论的教科书： <a href="https://www.amazon.com/Evolution-Douglas-J-Futuyma/dp/1605356050/"><u>Futuyama 和 Kirkpatrick 的</u><i><u>《进化论》（第 4 版）</u></i></a>和<a href="https://www.amazon.com/Evolutionary-Analysis-5th-Jon-Herron/dp/0321616677/"><u>Herron 和 Freeman 的</u><i><u>《进化分析》（第 5 版）</u></i></a> 。第一个是根据谷歌搜索“什么是最好的进化生物学教科书”而选择的，第二个是因为我曾经选修过的进化心理学本科课程使用了早期版本，我记得它很好。</p><p>据我所知，没有人谈论进化是一个优化遗传适应性的过程（尽管这是一个相当简单的浏览，所以即使它存在，我也可能错过了）。相反，他们警告<i>不要</i>将进化视为首先“做”任何事情的积极因素。进化确实<i>提高了种群对其环境的平均适应能力</i>（Herron &amp; Freeman，第 107 页），但这意味着什么会随着环境本身的变化而不断变化。在历史上的某个时期，一个地区可能气候寒冷，选择那里的物种是为了有能力应对寒冷；然后气候可能会变得更加温暖，以前有益的适应（例如皮毛）可能会突然变成一种负担。</p><p>另一个经典的例子是<a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution"><u>胡椒蛾的进化</u></a>。浅色飞蛾曾经在英格兰很常见，深色飞蛾非常罕见，因为浅色比深色飞蛾更能伪装鸟类。随着工业革命和污染工厂的出现，一些城市变得如此之黑，以至于深色成为更好的伪装，导致深色飞蛾相对于浅色飞蛾的增加。一旦污染减少，浅色飞蛾就会再次占据主导地位。</p><p>如果我们将进化建模为数学函数，我们可以说，飞蛾首先选择浅色，然后改为选择深色，然后再次选择浅色。</p><p>最接近“遗传适应度的进化优化”的是所谓的“<a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection"><u>自然选择基本定理</u></a>”，它意味着自然选择将导致种群的平均适应度随着时间的推移而增加。然而，在这里我们假设<i>我们选择的东西保持不变。</i>随着时间的推移，浅色飞蛾将继续变得更加常见，直到深色成为具有更高适应度的特征，并且深色开始变得更加常见。在这两种情况下，我们可能会说“人口的平均适应度正在增加”，但这在这两种情况下意味着<i>不同的事情</i>：在一种情况下，这意味着选择白色，而在另一种情况下，这意味着选择深色。首先被选择<i>的</i>事物，然后被选择<i>反对</i>，即使我们的术语意味着<i>同样的</i>事物被选择。</p><p>发生的情况是，随着选择特定颜色，人口的平均适应度上升，然后随机变化（首先是污染增加，然后是污染减少）导致平均适应度下降，然后又开始上升。</p><blockquote><p><i>在其他条件相同的情况下，基本定理将使我们预计物种的平均适应度应每代增加几个百分点。但其他一切并不平等：选择给予什么，其他进化力量就会夺走什么。选择带来的适应度增益不断被空间和时间变化的环境、有害突变和其他因素所抵消。</i> （Futuyama 和 Kirkpatrick，第 127 页）</p></blockquote><p>即使考虑到这一点，进化也不会持续增加种群的平均适应度：有时进化最终会选择<i>降低</i>种群的平均适应度。</p><blockquote><p><i>基本定理和适应性景观做出的假设并不完全适用于任何自然种群。但在许多情况下，它们给出了非常好的近似值，有助于指导我们对进化的思考。在其他情况下，假设被违反的方式使进化表现得非常不同。基本定理和自适应景观不适用的一个特别重要的情况是选择与频率相关。在某些情况下，这可能会导致种群的平均适应度下降</i>（Futuyama &amp; Kirkpatrick，第 128 页）</p></blockquote><p>导致平均适应度<i>较低</i>的频率依赖选择的一个例子是产生许多果实的灌木丛的情况（Futuyama &amp; Kirkpatrick，第 129 页）。一些灌木会进化出一个树干，使它们能够为邻居投下阴影。结果，这些邻居变得虚弱和死亡，使已经变成树木的灌木丛获得了更多的水和养分。</p><p>这导致树木变得比灌木丛更常见。但由于树木需要花费更多的能量来生产和维护树干，因此它们没有那么多的能量来种植果实。当树木稀少并且主要从灌木丛中窃取能量时，这并不是什么大问题；但一旦整个种群都由树木组成，它们最终可能会互相遮蔽。此时，它们最终产出的可供新树生长的果实要少得多，因此后代也较少，平均适应度也较低。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/d1oqnbrk2hlvbihqseic"></p><p>这种与频率相关的选择很常见。另一个例子（Futuyama &amp; Kirkpatrick，第 129 页）是细菌既能进化出杀死其他细菌的毒素，又能进化出针对该毒素的解毒剂。两者的生产都需要消耗能源，但只要这些细菌很稀有，那么这些成本就是值得的，因为它们的毒性可以让它们杀死竞争对手。</p><p>但是，一旦这些有毒细菌自我繁殖，产生毒素就不再有任何好处——所有幸存的细菌都对其免疫——因此继续花费能量来产生毒素意味着可用于复制的能量更少。现在，保持解毒剂的产生但失去毒素的产生变得更加有利：毒素的产生从被选择变成被选择反对。</p><p>一旦这种选择过程发生足够长的时间并且不产生毒素的细菌占据主导地位，解毒剂的生产<i>也</i>将成为不必要的负担。没有人再产生毒素了，所以没有理由浪费精力来维持防御，所以解药也从被选择变成了被选择对抗。</p><p>但是，一旦没有细菌再产生毒素<i>或</i>解毒剂，会发生什么呢？既然没有人对毒素有防御能力，那么再次开始生产毒素+解毒剂组合就变得有利，从而杀死所有其他没有解毒剂的细菌……如此循环重复。</p><p>在本节中，我认为，就进化而言，“为了适应而优化物种”，这实际上在不同情况下意味着不同的事情（选择不同的特征）；而且，针对适应性的进化优化更多的是一种粗略的启发式方法，而不是字面法则，因为在很多情况下，进化最终会<i>降低</i>种群的适应性。仅这一点就应该让我们对“进化为IGF选择人类”的论点产生怀疑；这意味着并不是针对单一事物进行优化，而是在不同时间选择了多种特征。</p><h1>健身到底是什么？</h1><p>到目前为止，我一直在一般性地谈论健身，但让我们回顾一下一些技术细节。包容性遗传适应性到底<i>是</i>什么？</p><p>有几种不同的定义；这是其中的一组。</p><p> A simple definition of <i>fitness</i> is that it&#39;s the number of offspring that an individual leaves for the next generation <span class="footnote-reference" role="doc-noteref" id="fnrefns0k7bysais"><sup><a href="#fnns0k7bysais">[1]</a></sup></span> . Suppose that 1% of a peppered moth&#39;s offspring survive to reproductive age and that the surviving moths have an average of 300 offspring. In this case, the average fitness of these individuals is 0.01 * 300 = 3.</p><p> For evolution by natural selection to occur, fitness differences among individuals need to be inherited. In biological evolution, inheritance happens through genes, so we are usually interested in <i>genetic fitness</i> - the fitness of genes. Suppose that these are all light-colored moths in a polluted city. Suppose a gene allele for dark coloration increases the survivability by 0.33 percentage points, for an overall fitness of 0.0133 * 300 = 4. The fitnesses of the alleles are now 3 and 4. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/hn7jtehbrwp5cxtwi6vs"></p><p> Image from Futuyama &amp; Kirkpatrick. Caption in the original: <i>Genotype A has a fitness of 3, while genotype B has a fitness of 4. Both genotypes start with 10 individuals. (A) The population size of genotype B grows much more rapidly. (B) Plotting the frequencies of the two genotypes shows that genotype B, which starts at a frequency of 0.5, makes up almost 90% of the population just 7 generations later.</i></p><p> Often what matters is the <i>difference</i> in fitness between two alleles: for example, an allele with a fitness of 2 may become more common in the population if its competitor has a fitness of 1, but will become more rare if its competitor has a fitness of 3. Thus it&#39;s common to indicate fitness <i>relative</i> to some common reference, such as the average fitness of the population or the genotype with the highest absolute fitness.</p><p> Genetic fitness can be divided into two components. An individual can pass a gene directly onto their offspring - this is called <i>direct fitness</i> . They can also carry a genetic adaptation that causes them to help others with the same adaptation, increasing their odds of survival. For example, a parent may invest extra effort in taking care of their offspring. This is called <i>indirect fitness.</i> The <i>inclusive fitness</i> of a genotype is the sum of its direct and indirect fitness. <span class="footnote-reference" role="doc-noteref" id="fnref23b1jpur496"><sup><a href="#fn23b1jpur496">[2]</a></sup></span></p><p> Biological evolution can be defined as “inherited change in the properties of organisms over the course of generations” (Futuyama &amp; Kirkpatrick, p. 7). <i>Evolution by natural selection</i> is when the relative frequencies of a genotype change across generations due to differences in fitness. Note that genotype frequencies can also change across generations for reasons other than natural selection, such as random drift or novel mutations.</p><h1> Fitness as a measure of selection strength</h1><p> Let&#39;s look at a case of intentional animal breeding. The details of the math that follows aren&#39;t that important, but I wanted to run through them anyway, just to make it more concrete what “fitness” <i>actually means</i> . Still, you can just skim through them if you prefer.</p><p> Suppose that I happen to own a bunch of peppered moths of various colors and happen to like a light color, so I decide to breed them towards being lighter. Now I don&#39;t know the details of how the genetics of peppered moth coloration works - I assume that it might very well be affected by multiple genes. But for the sake of simplicity, let&#39;s just say that there is a single gene with a “light” allele and a “dark” allele.</p><p> Call the “light” allele B1 and the “dark” allele B2. B1B1 moths are light, B2B2 moths are dark, and B1B2 / B2B1 moths are somewhere in between (to further simplify things, I&#39;ll use “B1B2” to refer to both B1B2 and B2B1 moths).</p><p> Suppose that the initial population has 100 moths. I have been doing breeding for a little bit already, so we start from B1 having a frequency of 0.6, and B2 a frequency of 0.4. The moths have the following distribution of genotypes:</p><p> B1B1 = 36</p><p> B1B2 = 48</p><p> B2B2 = 16</p><p> To my eye, all of the moths with the B1B1 genotype look pleasantly light, so I choose to have them all breed. 75% of the moths with the B1B2 genotype look light enough to my eye, and so do 50% of the B2B2 ones (maybe their coloration is also affected by environmental factors or other genes). The rest don&#39;t get to breed.</p><p> This gives us, on average, a frequency of 0.675 for the B1 alleles and 0.325 for the B2 alleles in the next generation <span class="footnote-reference" role="doc-noteref" id="fnrefh3ks5duler4"><sup><a href="#fnh3ks5duler4">[3]</a></sup></span> . Assuming that each of the moths contributed a hundred gametes to the next generation, we get the following fitnesses for the alleles:</p><p> B1: Went from 120 (36 + 36 + 48) to 5400 copies, so the fitness is 5400/120 = 45.</p><p> B2: Went from 80 (48 + 16 + 16) to 2600 copies, so the fitness is 2600/80 = 32.5.</p><p> As the proportion of B1 increases, the average fitness of the population will increase! This is because the more B1 alleles you carry, the more likely it is that you are selected to breed, so B1 carriers have a higher fitness… which means that B1 becomes more common… which increases the average fitness of the mouse population as a whole. So in this case, the rule that the average fitness of the population tends to increase over time does apply.</p><p> But now… wouldn&#39;t it sound pretty weird to describe this process as <i>optimizing for the fitness of the moths?</i></p><p> I am optimizing for <i>having light moths</i> ; what the fitness calculation tells us is <i>how much of an advantage the lightness genes have -</i> in other words, <i>how much I am favoring the lightness genes</i> -<i> </i>relative to the darkness genes.</p><p> Because we were only modeling the effect of fitness and not eg random drift, all of the difference in gene frequencies came from the difference in fitness. This is tautological - it doesn&#39;t matter <i>what</i> you are selecting (optimizing) for, <i>anything</i> that gets selected ends up having the highest fitness, by definition.</p><p> Rather than saying that we were optimizing for high fitness, it seems more natural to say that we were optimizing for the trait of <i>lightness</i> and that <i>lightness gave a fitness advantage.</i> The other way around doesn&#39;t make much sense - we were optimizing for fitness and that gave an advantage to lightness?什么？</p><p> This example used artificial selection because that makes it the most obvious what the actual selection target was. But the math works out the same regardless of whether we&#39;re talking artificial or natural selection. If we say that instead of me deciding that some moths don&#39;t get to breed, the birds and other factors in the external environment are doing it… well, nothing changes about the equations in question.</p><p> Was natural selection optimizing for the fitness of the moths? There&#39;s a sense in which you could say that since the dark-colored moths ended up having increased fitness compared to the light-colored ones. But it would also again feel a little off to describe it this way; it feels more informative and precise to say that the moths were <i>optimized for having dark color</i> , or to put it more abstractly, for having the kind of a color that fits their environment.</p><h1> From coloration to behavior</h1><p> I&#39;ve just argued that if we look at the actual process of evolution, it looks more like optimizing for having specific traits (with fitness as a measure of how strongly they&#39;re selected) rather than optimizing for fitness as such. This is so even though the process of selection <i>can</i> lead to the mean fitness of the population increasing - but as we can see from the math, this just means “if you select for something, then you get more of the thing that you are selecting for”.</p><p> In the sections before that, I argued that there&#39;s no single thing that evolution selects for; rather, the thing that it&#39;s changing is constantly changing itself.</p><p> I think these arguments are sufficient to conclude that the claim “evolution optimized humans for fitness [thus humans ought to be optimizing for fitness]” is shaky.</p><p> So far, I have mostly been talking about relatively “static” traits such as coloration, rather than cognitive traits that are by themselves optimizers. So let&#39;s talk about cognition. While saying that “evolution optimized humans for genetic fitness, thus humans ought to be optimizing for fitness” seems shaky, the corresponding argument <i>does</i> work if we talk about specific cognitive behaviors that were selected for.</p><p> For example, if we say that “humans were selected for caring about their offspring, thus humans should be optimizing for ensuring the survival of their offspring”, then this statement <i>does</i> generally speaking hold - a lot of humans do put quite a lot of cognitive effort into ensuring that their children survival. Or if we say that “humans were selected for exhibiting sexual jealousy in some circumstances, so in some circumstances, they will optimize for preventing their mates from having sex with other humans”, then clearly that statement does <i>also</i> hold.</p><p> This gets to my second part of the argument: while it&#39;s claimed that we are now doing something that goes completely against what evolution selected for, contraception at least is a poor example of that. For the most part, we are still optimizing for exactly the things that evolution selected us to optimize for.</p><h1> Humans still have the goals we were selected for</h1><p> The desire to have sex was never sufficient for having babies by itself - or at least not for having ones that would survive long enough to reproduce themselves in turn. It was always only one component, with us having multiple different desires relating to children:</p><ol><li> A desire to have sex and to enjoy it for its own sake</li><li> A desire to have children for its own sake</li><li> A desire to take care of and protect children (including ones that are not your own) for its own sake</li></ol><p> Eliezer wrote, in “AGI Ruin: A List of Lethalities” that</p><blockquote><p> 15. [...] We didn&#39;t break alignment with the &#39;inclusive reproductive fitness&#39; outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.  Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.  We started reflecting on ourselves a lot more, started being programmed a lot more by cultural evolution, and lots and lots of assumptions underlying our alignment in the ancestral training environment broke simultaneously.</p></blockquote><p> This quote seems to imply that</p><ul><li> effective contraception is a relatively recent invention</li><li> it&#39;s the desire for sex alone that&#39;s the predominant driver for having children (and effective contraception breaks this assumption)</li><li> it&#39;s a novel development that we prioritize things-other-than-children so much</li></ul><p> All of these premises seem false to me. Here&#39;s why:</p><p> <i>Effective contraception is a relatively recent innovation.</i> Even hunter-gatherers have access to effective “contraception” in the form of infanticide, which is commonly practiced among some modern hunter-gatherer societies. Particularly sensitive readers may want to skip the following paragraphs from <a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><i><u>The Anthropology of Childhood</u></i></a> :</p><blockquote><p> The Ache [a Paraguyan foraging society] are particularly direct in disposing of surplus children (approximately one-fifth) because their peripatetic, foraging lifestyle places an enormous burden on the parents. The father provides significant food resources, and the mother provides both food and the vigilant monitoring required by their dangerous jungle environment. Both men and women face significant health and safety hazards throughout their relatively short lives, and they place their own welfare over that of their offspring. A survey of several foraging societies shows a close association between the willingness to commit infanticide and the daunting challenge “to carry more than a single young child on the nomadic round” (Riches 1974: 356).</p><p> Among other South American foragers, similar attitudes prevail. The Tapirapé from central Brazil allow only three children per family; all others must be left behind in the jungle. Seasonally scarce resources affecting the entire community dictate these measures (Wagley 1977). In fact, the availability of adequate resources is most commonly the criterion for determining whether an apparently healthy infant will be kept alive (Dickeman 1975). Among the Ayoreo foragers of Bolivia, it is customary for women to have several brief affairs, often resulting in childbirth, before settling into a stable relationship equaling marriage. “Illegitimate” offspring are often buried immediately after birth. During Bugos and McCarthy&#39;s (1984) fieldwork, 54 of 141 births ended in infanticide.</p></blockquote><p> It takes years for a newborn to get to a point where they can take care of themselves, so a simple lack of active caretaking is enough to kill an infant, no modern-age contraceptive techniques required.</p><p> <i>It&#39;s the desire for sex alone that&#39;s the predominant driver for there being children.</i> Again, see infanticide, which doesn&#39;t need to be an active act as much as a simple omission. One needs an active desire to keep children <i>alive</i> .</p><p> Also, even though the share of voluntarily childfree people is increasing, it&#39;s still not the predominant choice. <a href="https://theconversation.com/more-than-1-in-5-us-adults-dont-want-children-187236"><u>One 2022 study</u></a> found that 22% of the people polled neither had nor wanted to have children - which is a significant amount, but still leaves 78% of people as ones who either have or want to have children. There&#39;s still a strong drive to have children that&#39;s separate from the drive to just have sex.</p><p> <i>It&#39;s a novel cultural development that we prioritize things other-than-having-children so much.</i> Anthropology of Childhood spends significant time examining the various factors that affect the treatment of children in various cultures. It quite strongly argues that the value of children has always also been strongly contingent on various cultural and economic factors - meaning that it has always been just <i>one</i> of the things that people care about. (In fact, a desire to have lots of children may be more tied to agricultural and industrial societies, where the economic incentives for it are abnormally high.)</p><blockquote><p> Adults are rewarded for having lots of offspring when certain conditions are met. First, mothers must be surrounded by supportive kin who relieve them of much of the burden of childrearing so they can concentrate their energy on bearing more children. Second, those additional offspring must be seen as “future workers,” on farm or in factory. They must be seen as having the potential to pay back the investment made in them as infants and toddlers, and pretty quickly, before they begin reproducing themselves. Failing either or both these conditions, humans will reduce their fertility (Turke 1989). Foragers, for whom children are more of a burden than a help, will have far fewer children than neighboring societies that depend on agriculture for subsistence (LeVine 1988). [...]</p><p> In foraging societies, where children are dependent and unproductive well into their teens, fewer children are preferred. In farming societies, such as the Beng, children may be welcomed as “little slaves” (Gottlieb 2000: 87). In pastoral and industrial societies, where young children can undertake shepherding a flock, or do repetitive machine-work, women are much more fertile. And, while the traditional culture of the village affords a plethora of customs and taboos for the protection of the pregnant mother and newborn, these coexist with customs that either dictate or at least quietly sanction abortion and infanticide.</p></blockquote><p> To me, the simplest story here looks something like “evolution selects humans for having various desires, from having sex to having children to creating art and lots of other things too; and all of these desires are then subject to complex learning and weighting processes that may emphasize some over others, depending on the culture and environment”.</p><p> Some people will end up valuing children more, for complicated reasons; other people will end up valuing other things more, again for complicated reasons. This was the case in hunter-gatherer times and this is the case now.</p><p> But it <i>doesn&#39;t</i> look to me like evolution selected us to desire one thing, and then we developed an inner optimizer that ended up doing something completely different. Rather, it looks like we were selected to desire many different things, with a very complicated function choosing which things in that set of doings each individual ends up emphasizing. Today&#39;s culture might have shifted that function to weigh our desires in a different manner than before, but everything that we do is still being selected from within that set of basic desires, with the weighting function operating the same as it always has.</p><p> As I mentioned in the introduction, Robin Hanson <a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>has suggested</u></a> that modern society is more forager-like than farmer-like and that our increased wealth is causing us to revert to forager-like ways and psychology. This would then mean that our evolved weighting function is now exhibiting the kind of behavior that it was evolved to exhibit in a forager-like environment.</p><p> We do engage in novel activities like computer games today, but it seems to me like the motivation to play computer games is still rooted in the same kinds of basic desires as the first hunter-gatherers had - eg to pass the time, enjoy a good story, socialize, or experience a feeling of competence.</p><h1> So what can we say about AI?</h1><p> Well, I would be cautious around reasoning by analogy. I&#39;m not sure we can draw particularly strong claims about the connection to AI. I think that there are <a href="https://forum.effectivealtruism.org/posts/kLYD95SK8tQFRmw4T/ben-garfinkel-s-shortform?commentId=XMjAWSEgp9BXTDQBi">more direct and relevant arguments</a> that one can make that do seem worrying, rather than trying to resort to evolutionary analogies.</p><p> But it does seem to me that eg the evolutionary history for the “sharp left turn” implies the <i>opposite</i> than previously argued. Something like “training an AI for recognizing pictures” or “training an AI for caring about human values” looks a lot more like “selecting humans to care about having offspring” than it looks like “optimizing humans for genetic fitness”. Caring about having offspring is a property that we still seem to pretty robustly carry; our alignment properties continued to generalize even as our capabilities increased.</p><p> To the extent that we do not care about our offspring, or even choose to go childfree, it&#39;s just because we were selected to <i>also</i> care about other things - if a process selects humans to care about a mix of many things, them sometimes weighing those other things more does not by itself represent a failure of alignment. This is again in sharp contrast to something like an AI that we tried to <i>exclusively</i> optimize for caring about human well-being. So there&#39;s reason to expect that an AI&#39;s alignment properties might generalize <i>even more</i> than those of existing humans.</p><p> <i>Thanks to Quintin Pope, Richard Ngo, and Steve Byrnes for commenting on previous drafts of this essay.</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnns0k7bysais"> <span class="footnote-back-link"><sup><strong><a href="#fnrefns0k7bysais">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 60.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn23b1jpur496"> <span class="footnote-back-link"><sup><strong><a href="#fnref23b1jpur496">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 300.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh3ks5duler4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh3ks5duler4">^</a></strong></sup></span><div class="footnote-content"><p> Each B1B1 moth has a 100% chance to “pick” a B1 allele for producing a gamete, each B1B2 moth has a 50% chance to pick a B1 gamete and a 50% chance to pick a B2 gamete, and each B2B2 moth has a 100% to pick a B2 allele for producing a gamete. Assuming that each moth that I&#39;ve chosen to breed contributes 100 gametes to the next generation, we get an average of 3600 B1 gametes from the 36 B1B1 moths chosen to breed, 1800 B1 and 1800 B2 gametes from the 360 B1B2 moths chosen to breed, and 800 B2B2 gametes from the 8 B2B2 moths chosen to breed.</p><p> This makes for 3600 + 1800 = 5400 B1 gametes and 1800 + 800 = 2600 B2 gametes, for a total of 8000 gametes. This makes for a frequency of 0.675 for B1 and 0.325 for B2.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the<guid ispermalink="false"> BtffzD5yNB4CzSTJe</guid><dc:creator><![CDATA[Kaj_Sotala]]></dc:creator><pubDate> Sat, 04 Nov 2023 19:02:13 GMT</pubDate> </item><item><title><![CDATA[The Soul Key]]></title><description><![CDATA[Published on November 4, 2023 5:51 PM GMT<br/><br/><p> The ocean is your home, but a forbidding one: often tempestuous, seldom warm. So one of your great joys is crawling onto land and slipping off your furry seal skin, to laze in the sun in human form. The elders tell horror stories of friends whose skins were stolen by humans, a single moment of carelessness leaving them stranded forever on land. That doesn&#39;t happen any more, though; this is a more civilized age. There are treaties, and authorities, and fences around the secluded beaches you and your sisters like the most, where you can relax in a way that older generations never could.</p><p> So your sisters no longer lose their skins by force. But sometimes it happens by choice. Sometimes a group of your sisters wrap their skins around themselves like robes and walk into the nearby town. The humans point and stare, but that&#39;s just part of the thrill. Sometimes young men gather the courage to approach, bearing flowers or jewelry or sweet words. And sometimes one of your sisters is charmed enough to set a rendezvous—and after a handful of meetings, or a dozen, to decide to stay for good.</p><p> You never thought it would happen to you. But his manners are so lively, and his eyes so kind, that you keep coming back, again and again. When he finally asks you to stay, you hesitate only a moment before saying yes. The harder part comes after. He finds you human clothes, and in exchange you give him your beautiful skin, and tell him that it must be locked away somewhere you&#39;ll never find it—and that he must never give it back to you, no matter how much you plead. Because if there&#39;s any shred of doubt, any chance of returning home, then the lure of the sea will be too much for you. You want this; you want him; you want to build a life together. And so the decision has to be final.</p><p> Years pass. You bear three beautiful children, with his eyes and your hair, and watch them blossom into beautiful adults. You always live near the sea, although you can&#39;t bear to swim in it—your limbs feel unbearably weak and clumsy whenever you try. You and your husband grow into each other, time smoothing down the ridges left from pushing two alien lives together. You forget who you once were.</p><p> After your youngest leaves home, you start feeling restless. You have disquieting dreams—first intermittently, then for weeks on end. One day, after your husband has gone to work, your feet take you up the stairs to the attic. As you brush aside the cobwebs in one of the corners, your hands land on an old chest. You pull on the lid, and it catches on a padlock—but only for a second. The shackle has been rusted through by the sea breeze, and quickly snaps. You open the lid, and you see your skin laid out before you.</p><p> What then?</p><hr><p> You look at your skin, and your ears fill with the roar of the sea. A wild urge overtakes you; you grab your skin and run headlong towards the shore. As you reach it you see your husband standing on the pier—but that gives you only a moment&#39;s pause before you dive into the water, your skin fitting around you as if you&#39;d never taken it off.</p><p> As you swim away, you envisage your family in tatters: your children left baffled and distraught, your husband putting on a brave face for their sake. But it was his fault, after all. He failed in the one thing you asked of him; and you can&#39;t fight your nature.</p><hr><p> You look at your skin, and see a scrap of paper lying on top of it. <i>I knew you&#39;d only open the chest if you were restless and unhappy</i> , it reads. <i>And I would never cage you. So go free, with my blessing</i> .</p><p> You catch your breath—and, for a moment, you consider staying. But his permission loosens any tether that might have held you back. You leave the note there, alongside a little patch of fur torn off your coat: a last gesture, the least you can give him.</p><hr><p> You look at your skin, and your ears fill with the roar of the sea. But it&#39;s not loud enough to drown out your thoughts. <i>I&#39;m not an animal</i> , you think. <i>I can make my own choices</i> .</p><p> You shove the lid closed, and the moment of reprieve it gives you is enough to start scrambling down the stairs and out the gate and you don&#39;t stop running until you&#39;re out of sight of your house.</p><p> When you find your husband, down at the pier, your face tells him what&#39;s happened before you&#39;ve said a word. He gives you a gentle kiss, then sprints back to the house. By the time you make it home, he&#39;s relaxing in an armchair, and the kettle is almost boiling; but you know that the chest and its contents are gone.</p><p> His glances, always loving, now fill with wonder: that he almost lost you, but that he didn&#39;t. That you chose him yet again, in defiance of your deepest instincts. You curl up in his arms every night; and though at first you can&#39;t hold back the tears, over time they grow rarer and rarer. You never see your skin again.</p><hr><p> You look at your skin, and a wave of emotion crashes into you. You remember your old ambitions: to explore every horizon; to surf every current; to ride every storm. Dangerous dreams—and pointless ones too, in an age where planes criss-cross the skies, and the blank spots on the maps have all been filled. You didn&#39;t want to waste your life retracing others&#39; footsteps. So you infused those dreams into your skin and locked it away.</p><p> And yet there&#39;s still something in you that yearns for them: the part that&#39;s been making you restless, the part that led you into the attic. So you fetch a pair of scissors, and cut off your long wavy hair—and with it, whatever remaining wistfulness was keeping you up at night. You put it into the box, on top of your skin, and close the lid again. No need to lock it, this time.</p><p> Maybe your husband finds the broken lock, and the tresses of your hair. You never know. But you don&#39;t need to know. What you&#39;ve got is enough.</p><hr><p> You look at your skin, and remember your plan. Your face is lined now, and your hair is streaked with gray. But inside the chest, under a layer of dust, your old skin is pristine. You can imagine slipping it back on and gliding back into the ocean, not a day older than when you left.</p><p> You&#39;ll be different from how you were before, of course. You can&#39;t live a lifetime in any skin without it changing you. But your past self thought that was a worthwhile trade, for those extra decades of youth. Worth leaving your friends and family behind, worth setting aside all the glories of the ocean. And who knows—maybe there are more tricks yet to be found, more ways to slip the noose of aging and death.</p><p> You look at the skin, and consider putting it on. Not yet, you think. You&#39;ve still got a few more decades left in your current skin, before you&#39;ll need to go back. Not yet.</p><hr><p> The fur on your skin ripples like waves, bringing back to you the knowledge of what the ocean really is. In your current body, you can only see the surface: the swells of water, the winds and storms. But if you could dive underneath, you&#39;d find a portal to a whole civilization. You&#39;re on the shore of the future of humanity, a sea of minds so vast that you can barely imagine it. Throughout the long millennia since humans transcended the limitations of their biology, they&#39;ve multiplied beyond number, and constructed joys and enchantments beyond measure. The ocean is your gateway to all of those people and their wonders—each one so beautiful, and so so tempting.</p><p> Why did you come to this backwater, this output of baseline humans who refuse to engage with the outside world? Why did you lock away your memories, leaving only hints about what you used to be? You don&#39;t remember. Was it an experiment? A whim? Surely it couldn&#39;t have been for the love of a baseline human. But regardless, how can you stay here, when you know what else exists? You gather your skin in your arms, and though there&#39;s a pang of sadness as you walk out the door, you don&#39;t look back. There&#39;s a whole universe to explore.</p><hr><p> Your skin is a cornucopia of possible lives; the deeper into it you look, the more you see. Just under the surface, joyful humans are granted miraculous powers: to soar through air and sea, to play games on the scale of planets, to morph their bodies as they please. Beneath them, you see a society that&#39;s explored further, morphing not just their bodies, but also their minds—belief and desire and identity become as malleable as clay. Beneath <i>them</i> , you lose sight of individuals: at those depths minds merge and split and reform like currents in the ocean. And beneath even that? It&#39;s hard for you to make sense of the impressions you&#39;re getting—whatever is down there can&#39;t be described in human terms. In the farthest reaches there are only alien algorithms, churning away on computers that stretch across galaxies, calculating the output of some function far beyond your comprehension.</p><p> And now you see the trap. Each step down makes so much sense, from the vantage point of the previous stage. But after you take any step, the next will soon be just as tempting. And once you&#39;re in the water, there&#39;s no line you can draw, no fence that can save you. You&#39;ll just keep sinking deeper and deeper, with more and more of your current self stripped away—until eventually you&#39;ll become one of the creatures that you can glimpse only hazily, one of the deep-dwelling monsters that has forsaken anything recognizably human.</p><p> So this is the line you decided to draw: here, and no further. You&#39;ll live out your lives in a mundane world of baseline humans, with only a touch of magic at the edges—just enough to satisfy the wondering child in you. You&#39;ll hold on to yourself, because what else is there to hold onto? It&#39;s a sad thought, in some ways, but a satisfying one too.</p><p> You close the lid, and your memories with it, and live happily ever after.</p><hr><p> As you look at your skin, each strand of fur shimmers with different stories—not of your possible lives, but of your <i>current</i> lives. Different shards of you are living out countless adventures across countless artificial universes. You&#39;re far vaster than you ever dreamed: what you thought was your whole “self” is just a fragment of a fragment of your overall mind.</p><p> Which fragment? Perhaps, at your core, you were the part of your meta-self that loved fairy tales; or perhaps you were an avatar of the innocence of early humanity; or perhaps you were a nostalgic part, who enjoyed basking in the wistfulness of times already gone. Whatever your goals were, they led you to volunteer to live this small life in this small world: a single strand in the tapestry your meta-self is weaving.</p><p> You stroke your skin gently. You can&#39;t picture your meta-self, not really—it&#39;s too vast and too alien. But you know it&#39;s watching out for you. There&#39;s a warmth underneath your hand, and a gentle breeze passing across your neck, like a caress. As you close the chest, you bask in the knowledge that your life is part of a vast plan. That&#39;s enough for you.</p><hr><p> As you glimpse your skin, long-lost memories rush into your mind: recollections of all the hundreds of other times you&#39;ve rediscovered it, across thousands of past lives. Every time, you&#39;ve received a different vision of what&#39;s waiting for you in the depths of the ocean. But you don&#39;t know which, if any, is true. You can&#39;t know. Where would the adventure be, if you were just handed the whole plot? What would be the point of living through it? Only two things remain constant across all your visions. First: there&#39;s no hurry; you have eons of time. Second: once you put on the skin, you can&#39;t come back.</p><p> You stare at it with longing, and excitement, and fear. But you&#39;re not ready yet. So you pull aside the skin to reveal the padlocks underneath—thousands of them, split into two piles. One pile is stained with wear, each shackle rusted through; you toss the latest padlock on top. The padlocks in the other pile are clean and new; each looks strong enough to fasten the world in place. You grab one of those. You put your skin back on top of the two piles, and close the box, and carefully fasten the padlock. Then you go downstairs again, to the life you&#39;ve constructed for yourself, while the shiny steel slowly begins to rust away.</p><hr><p> <i>In addition to the inspirations behind</i> <a href="https://www.narrativeark.xyz/p/the-ants-and-grasshopperhtml"><i>my previous story of this form</i></a> <i>, I also owe a debt to Neil Gaiman&#39;s beautiful</i> <a href="https://www.amazon.com/Ocean-End-Lane-Novel/dp/0062459368">The Ocean at the End of the Lane</a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/tHvFtfFKjhfy3sQpC/the-soul-key#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/tHvFtfFKjhfy3sQpC/the-soul-key<guid ispermalink="false"> tHvFtfFKjhfy3sQpC</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Sat, 04 Nov 2023 17:51:53 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Concept Alignment as a Prerequisite for Value Alignment]]></title><description><![CDATA[Published on November 4, 2023 5:34 PM GMT<br/><br/><blockquote><p> Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person&#39;s concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.</p></blockquote><p></p><blockquote><p> We propose a theoretical framework for formally introducing concepts to inverse reinforcement learning and show that conceptual misalignment (ie, failing to consider construals) can lead to severe value misalignment (ie, reward mis-specification; large performance gap). We validate these theoretical results with a case study using a simple gridworld environment where we find that IRL agents that jointly model construals and reward outperform those that only model reward. Finally, we conduct a study with human participants and find that people do model construals, and that their inferences about rewards are a much closer match to the agent that jointly models construals and rewards. Our theoretical and empirical results suggest that the current paradigm of just trying to directly infer human reward functions or preferences from demonstrations is insufficient for value-aligning real AI systems that need to interact with real people; it is crucial to also model and align on the concepts people use to reason about the task in order to understand their true values and intentions.</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/M6CEJmgna6FTt9Yci/linkpost-concept-alignment-as-a-prerequisite-for-value#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/M6CEJmgna6FTt9Yci/linkpost-concept-alignment-as-a-prerequisite-for-value<guid ispermalink="false"> M6CEJmgna6FTt9Yci</guid><dc:creator><![CDATA[Bogdan Ionut Cirstea]]></dc:creator><pubDate> Sat, 04 Nov 2023 17:34:41 GMT</pubDate> </item><item><title><![CDATA[We are already in a persuasion-transformed world and must take precautions]]></title><description><![CDATA[Published on November 4, 2023 3:53 PM GMT<br/><br/><p> &quot;In times of change, learners inherit the earth, while the learned find themselves beautifully equipped to deal with a world that no longer exists&quot;</p><p></p><p><strong>概括：</strong></p><p> We&#39;re already in the timeline where the research and manipulation of the human thought process is widespread; SOTA psychological research systems require massive amounts of human behavior data, which in turn requires massive numbers of unsuspecting test subjects (users) in order to automate the process of analyzing and exploiting human targets. This therefore must happen covertly, and both the US and China have a strong track record of doing things like this. This outcome is a strong attractor state since anyone with enough data can do it, and it naturally follows that powerful organizations would deny others access eg via data poisoning. Most people are already being persuaded that this is harmless, even though it is obviously ludicrously dangerous. Therefore, we are probably already in a hazardously transformative world and must take <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Webcam%2Dcovering%20rates,media%20news%20feeds).">standard precautions</a> immediately.</p><p></p><p> This should not distract people from AI safety. This is valuable because the AI safety community must survive. This problem connects to the AI safety community in the following way:</p><p> State survival and war power <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#1__Militaries_are_perhaps_the_oldest_institution_on_earth_to_research_and_exploit_human_psychology_">==>;</a> already depends on information warfare capabilities.</p><p> Information warfare capabilities <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#2__Information_warfare_has_long_hinged_on_SOTA_psychological_knowledge_and_persuasive_skill_">==>;</a> already depends on SOTA psychological research systems.</p><p> SOTA psychological research systems <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#5__SOTA_psychological_research_and_manipulation_capabilities_have_already_started_increasing_by_an_order_of_magnitude_every__4_years_">==>;</a> already improves and scales mainly from AI capabilities research, diminishing returns on everything else. <span class="footnote-reference" role="doc-noteref" id="fnrefxevua908cwf"><sup><a href="#fnxevua908cwf">[1]</a></sup></span></p><p> AI capabilities research <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point">==>;</a> already under siege from the AI safety community.</p><p> Therefore, the reason why this might be such a big concern is:</p><p> State survival and war power <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point:~:text=the%20AI%20safety%20community%20must%20acknowledge%20the%20risk%20that%20the%202020s%20will%20be%20intensely%20dominated%20by%20actors%20capable%20of%20using%20modern%20technology%20to%20stealthily%20hack%20the%20human%20mind%20and%20eliminate%20inconvenient%20people%2C%20such%20as%20those%20try%20to%20pause%20AI%2C%20even%20though%20AI%20is%20basically%20the%20keys%20to%20their%20kingdom">==>;</a> their toes potentially already being stepped on by the AI safety community?</p><p> Although it&#39;s also important to note that <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point:~:text=I%20think%20that%20the%20AI%20safety%20community%20is,murkiest%20people%20lurking%20within%20the%20US%2DChina%20conflict.">people with access to SOTA psychological research systems are probably super good at intimidation and bluffing</a> , it&#39;s also the case that the AI safety community needs to get a better handle on the situation if we are in the bad timeline; and <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,causes%20with%20the">the math indicates that we are already well past that point</a> .</p><p></p><p> <strong>The Fundamental Problem</strong></p><p> If there were intelligent aliens, made of bundles of tentacles or crystals or plants that think incredibly slowly, their minds would also have discoverable exploits/zero days, because any mind that evolved naturally would probably be like the human brain, a <a href="https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers">kludge of spaghetti code</a> that is operating outside of its intended environment.</p><p> They would probably not even begin to scratch the surface of finding and labeling those exploits, until, like human civilization today, they began surrounding thousands or millions of their kind with sensors that could record behavior several hours a day and find <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=of%20procuring%20results.-,There%20is%20no,-logical%20endpoint%20to"><u>webs of correlations</u></a> .</p><p> In the case of humans, the use of social media as a controlled environment for automated AI-powered experimentation appears to be what created that critical mass of human behavior data. Current 2020s capabilities for psychological research and manipulation vastly exceed the 20th century academic psychology paradigm.</p><p> The 20th century academic psychology paradigm still dominates our cultural impression of what it means to research the human mind; but when the effectiveness of psychological research and manipulation starts increasing by an order of magnitude every 4 years, it becomes time to stop mentally living in a world that was stabilized by the fact that manipulation attempts generally failed.</p><p> The capabilities of social media to steer human outcomes are not advancing in isolation, they are parallel to a broad acceleration in the understanding and exploitation of the human mind, which itself <a href="https://arxiv.org/pdf/2309.15084.pdf"><u>is a byproduct of accelerating AI capabilities research</u></a> .</p><p> By comparing people to other people and predicting traits and future behavior, multi-armed bandit algorithms can predict whether a specific research experiment or manipulation strategy is worth the risk of undertaking at all in the first place; resulting in large numbers of success cases a low detection rate (as detection would likely yield a highly measurable response, particularly with substantial sensor exposure).</p><p> When you have sample sizes of billions of hours of human behavior data and sensor data, millisecond differences in reactions from different kinds of people (eg facial microexpressions, millisecond differences at scrolling past posts covering different concepts, heart rate changes after covering different concepts, eyetracking differences after eyes passing over specific concepts, touchscreen data, etc) transform from being imperceptible noise to becoming the foundation of webs of correlations mapping the human mind.</p><p> Unfortunately, the movement of scrolling past a piece of information on a social media news feed with a mouse wheel or touch screen will generate at least one curve since the finger accelerates and decelerates each time. Trillions of those curves are outputted each day by billions of people. These curves are linear algebra, the perfect shape to plug into ML.</p><p> Social media&#39;s individualized targeting uses deep learning and massive samples of human behavioral data to procedurally generate an experience that fits human mind like a glove, in ways we don&#39;t fully understand, but allow hackers incredible leeway to find ways to optimize for steering people&#39;s thinking or behavior in measurable directions, insofar as those directions are measurable. This is something that AI can easily automate. I originally thought that going deeper into human thought reading/interpretability was impossible, but I was wrong; <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,."><u>the human race is already well past that point as well, due to causality inference</u></a> . </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LdEwDn5veAckEemi4/lmquoccehpu8d9exhbjb"></p><p> Most people see social media influence as something that happens to people lower on the food chain, but this is no longer true; there is at least one technique, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>Clown Attacks</u></a> , that generally works on all humans who aren&#39;t already aware of it, regardless of intelligence or commitment to truthseeking; and that particular technique became discoverable with systems vastly weaker than the ones that exist today. I don&#39;t know what manipulation strategies the current systems can find, but I can predict with great confidence that they&#39;re well above the clown attack.</p><blockquote><p> First it started working on 60% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as people in that 60%. Then, it started working on 90% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as the people in that 90%. Then it started working on me. And by then, it was already too late, because it was already working on me.</p></blockquote><p></p><p> <strong>They would notice and pursue these capabilities</strong></p><p> If the big 5 tech companies (Facebook, Google, Microsoft, Amazon, and Apple) notice ways to steer people towards buying specific products, or to feel a wide variety of compulsions to avoid quitting the platform, and to <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=However%2C%20if%20a,of%20the%20genepool."><u>prevent/counteract other platforms from running multi-armed bandit algorithms to automatically exploit strategies (eg combinations of posts) to plunder their users</u></a> , then you can naturally assume that they&#39;ve noticed their capabilities to steer people in a wide variety of other directions as well.</p><p> The problem is that <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/jyAerr8txxhiKnxwA"><u>major governments and militaries are overwhelmingly incentivized and well-positioned to exploit those capabilities for offensive and defensive information warfare</u></a> ; if American companies abstain from manipulation capabilities, and Chinese companies naturally don&#39;t, then American intelligence agencies <a href="https://www.nytimes.com/2023/11/03/technology/israel-hamas-information-war.html#:~:text=%E2%80%9CWe%E2%80%99re%20in%20an%20undeclared%20information%20war%20with%20authoritarian%20countries%2C%E2%80%9D%20James%20P.%20Rubin%2C%20the%20head%20of%20the%20State%20Department%E2%80%99s%20Global%20Engagement%20Center%2C%20said%20in%20a%20recent%20interview.">worry about a gap in information warfare capabilities</a> and push the issue.</p><p> Government/military agencies are <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally">bottlenecked by competence</a> , which is <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult">difficult to measure due to a lack of transparency at higher levels and high employee turnover at lower levels</a> , but revolving-door employment easily allows them to source flexible talent from the talent pools of the big 5 tech companies; <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-want-ai-for-information-1#3__Information_warfare_is_about_winning_over_high_intelligence_elites__not_the_ignorant_masses__">this practice is endangered by information warfare itself</a> , further driving interest in information warfare superiority.</p><p> Access to tech company talent pools also determine the capability of intelligence agencies to use <a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html">OS exploits</a> and chip firmware exploits needed to access the sensors in the devices of almost any American, not just the majority of Americans that leave various sensor permissions on, which allows <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s">even greater access to the psychological research needed to compromise critical elites such as the AI safety community</a> .</p><p> The capabilities of these systems to run deep bayesian analysis on targets dramatically improves with more sensor data, <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=Eyetracking%20is%20likely,person%E2%80%99s%20thought%20process)."><u>particularly video feed on the face and eyes</u></a> ; this is <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,."><u>not necessary to run interpretability on the human brain</u></a> , but it increases these capabilities by yet more orders of magnitude (particularly for <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/foM8SA3ftY94MGMq9#Functioning_lie_detectors_as_a_turning_point_in_human_history"><u>lie detection</u></a> and conversation topic aversion).</p><p></p><p> <strong>This can&#39;t go on</strong></p><p> There isn&#39;t much point in having a utility function in the first place if hackers can change it at any time. There might be parts that are resistant to change, but it&#39;s easy to overestimate yourself on this; especially if the effectiveness of SOTA influence systems have been increasing by an order of magnitude every 4 years. Your brain has internal conflicts, and they have <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,with%20the%20common">human internal causality interpretability systems</a> . You get <a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">constant access to the surface</a> , but they get occasional access to the deep.</p><p> The multi-armed bandit algorithm will keep trying until it finds something that works. The human brain is a kludge of spaghetti code, so there&#39;s probably something somewhere.</p><p> The human brain has exploits, and the capability and cost of social media platforms to use massive amounts of human behavior data to find complex social engineering techniques is a profoundly technical matter, you can&#39;t get a handle on this with intuition or pre-2010s historical precedent.</p><p> Thus, you should assume that your utility function and values are at risk of being hacked at an unknown time, and should therefore be assigned a discount rate to account for the risk over the course of several years.</p><p> Slow takeoff over the course of the next 10 years alone guarantees that this discount rate is too high, in reality, for people in the AI safety community to continue to go on believing that it is something like zero.</p><p> I think that approaching zero is a reasonable target, but not with the current state of affairs where people don&#39;t even bother to cover up their webcams, have important and sensitive conversations about the fate of the earth in rooms with smartphones, sleep in the same room or even the same bed as a smartphone, and use social media for nearly an hour a day (scrolling past nearly a thousand posts).</p><p> The discount rate in this environment cannot be considered “reasonably” close to zero if the attack surface is this massive; and the world is changing this quickly. We are gradually becoming intractably compromised by slow takeoff before the party even starts.</p><p> If people have <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect">anything they value at all</a> , and the AI safety community almost certainly does have that, then the current AI safety paradigm of zero effort is wildly inappropriate, it is basically <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/jyAerr8txxhiKnxwA#:~:text=if%20you%20look,in%20the%20past."><u>total submission to invisible hackers</u></a> . The <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks">human brain has low-hanging exploits</a> and <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Keeping%20people%20on,adversarial%20information%20environment.">the structures of power have already been set in motion</a> , so we must take the <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Webcam%2Dcovering%20rates,day%20inside%20hyperoptimized">standard precautions</a> immediately.</p><p> The AI safety community must survive the storm. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxevua908cwf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxevua908cwf">^</a></strong></sup></span><div class="footnote-content"><p> eg Facebook could hire more psychologists to label data and correlations, but has greater risk of one of them breaking their NDA and trying to leak Facebook&#39;s manipulation capabilities to the press like Snowden and Fishback. Stronger AI means more ways to make users label their own data.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must<guid ispermalink="false"> LdEwDn5veAckEemi4</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sat, 04 Nov 2023 15:53:34 GMT</pubDate> </item><item><title><![CDATA[Being good at the basics]]></title><description><![CDATA[Published on November 4, 2023 2:18 PM GMT<br/><br/><p> (crossposted from <a href="https://sundaystopwatch.eu/being-good-at-the-basics/">my blog</a> )</p><p> In Brazilian Jiu Jitsu, there&#39;s a notion of being &quot;good at the basics&quot;, as opposed to &quot;being good by knowing advanced techniques&quot;. <em>How</em> you want to be good is a question of personal preference, but the idea is that these are the two ways, and most people do one or the other.</p><p> I think this is a concept that applies outside of BJJ, and one that&#39;s useful if you&#39;re trying to learn a field, but you&#39;re not sure how to approach it.</p><p> I&#39;ll take physics as an example, because that is a field where I would like to be good at the basics. The &quot;basics&quot; of physics would be things that you learn in high school, while &quot;advanced&quot; stuff would be what you would learn in university, maybe at a graduate level. (There&#39;s probably other ways of separating the basics from the advanced topics.)</p><p> You may roughly remember some of the simple formulas that you have learned in class during high school, you may orient yourself through a textbook problem, you may perform some basic calculations. You sorta know the basics. But how does it look like when you&#39;re <em>good</em> at the basics?</p><p> It might mean that you know the equations by heart, that you&#39;ve really internalized their meaning.</p><p> It might also mean that you know the quantities or values for things around you. You can correctly approximate how much things weigh, how fast they move, or what their kinetic and potential energy is.</p><p> You can approximate how much joules of energy an apple contains, or how much energy hits a m2 of the Earth&#39;s surface on a sunny day. You have a good feeling for how much compressive stress a piece of concrete can withstand, and you can quickly and accurately calculate what the kinetic energy is of a car that you&#39;re driving in. You can also quickly convert between different units, and compare the quantities of different things.</p><p> Being really good at the basics of physics in this way requires a sort of <em>embodied</em> understanding. It requires a level of &quot;closeness&quot; to the subject matter that allows you to apply this knowledge to the world around you, to the extent that you start seeing the world around you in terms of that subject. You take a walk outside and <em>you can&#39;t help</em> but notice the physics of it all.</p><p> You&#39;ll notice that being &quot;good at the basics&quot; is actually a linguistic misdirection. This type of understanding is drastically different from the level of understanding you have after high school, yet both refer to the &quot;basics&quot;. If you are really good at the basics, you are in fact an expert - an expert in the basics. The word &quot;basics&quot; here hides a huge amount of complexity, or time and effort.</p><p> Why be good at the basics (as opposed to the advanced stuff)? These things sometimes depend on the field, but mostly because being good at the advanced stuff will be required for a career in that particular field, but useless for other goals. I, for example, have no ambition to become a physicist but I do have an ambition to closely understand the world around me, and to make accurate estimates and good decisions. For me, being really good at the basics of physics is much more important than being good at some advanced thing within physics.</p><p> There are other fields where you can notice the two ways of being good at them. Writing is one of those fields. &quot;Being really good at the basics&quot; in writing looks like really plain and simple language that&#39;s easy to understand. By being good at the basics, you&#39;re actually hiding complexity. It&#39;s not like there is no complexity involved, it&#39;s just hidden from the reader. The reader is served a simple and clear text, but the complexity was in the process, from developing clear thinking about a topic, to the editing process that tries to simplify the resulting text. Compare this to being good at writing, but not in this basic sense: writers using interesting and innovative metaphors, or having a really rich vocabulary.</p><p> In cooking, there&#39;s a notion of being good at the basics as well. For example, compare a cook who has perfected the bowl of rice vs. a cook who uses espuma or sous vide cooking.</p><p> In drumming, there&#39;s that notion as well. For example, compare a drummer who has perfected a &quot;simple&quot; 4/4 funk groove using just the bass drum, snare and hi-hat vs. a drummer who uses a full set to play some super advanced polyrhythm.</p><p> Ultimately, choosing the strategy for how to be good at something is a matter of personal preference, and you may have different preferences depending on the field. As time passes, I find myself more interested in developing expertise in the basics in whatever I do. It often seems much more applicable outside of the narrow domain it belongs to, and it always seems to require fewer prerequisites. So my advice, if you need it, would be: &quot;if you want to learn X, and you&#39;re not sure what exactly to learn, try learning the basics really well&quot;.</p><br/><br/> <a href="https://www.lesswrong.com/posts/fLbkWseNn3geEifkv/being-good-at-the-basics#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/fLbkWseNn3geEifkv/being-good-at-the-basics<guid ispermalink="false"> fLbkWseNn3geEifkv</guid><dc:creator><![CDATA[dominicq]]></dc:creator><pubDate> Sat, 04 Nov 2023 14:18:51 GMT</pubDate> </item><item><title><![CDATA[Despair about AI progressing too slowly]]></title><description><![CDATA[Published on November 4, 2023 7:52 AM GMT<br/><br/><p> <i>Cross-posted from the EA Forum:</i> <a href="https://forum.effectivealtruism.org/posts/kGkQtj6vjcrrAjK38/despair-about-ai-progressing-too-slowly"><i>https://forum.effectivealtruism.org/posts/kGkQtj6vjcrrAjK38/despair-about-ai-progressing-too-slowly</i></a></p><p> <i><strong>Summary:</strong> This is a personal and highly subjective post about my mental journey with the topic of AI over the years and where I am today. My thoughts are somewhat scattered, but if you try, I think you can see the connections. This is only my second post on LessWrong, so please be nice to me.</i></p><p> <i><strong>Epistemic status:</strong> While everything written in this post is completely sincere, I have a healthy amount of self-doubt. I&#39;m trying to be upfront about my strong personal biases. I take seriously the idea that even a tiny existential risk is worth taking significant efforts to mitigate, so I&#39;m cautious about saying anything contrary to people who care a lot about AI risk. However, I believe that expressing contrarian or non-consensus views makes discourse healthier and consequently can improve people&#39;s reasoning and their persuasiveness to others. This is ultimately helpful for reducing existential risk even if the mainstream LessWrong view on AI risk is correct.</i></p><h3> Background</h3><p> Since around 2007 <span class="footnote-reference" role="doc-noteref" id="fnrefkzqnz1upen8"><sup><a href="#fnkzqnz1upen8">[1]</a></sup></span> , I&#39;ve taken an interest in AI — both (seemingly) nearer-term narrow applications like self-driving cars and the prospects of superhuman AGI.</p><p> I first wrote about AGI on my blog <a href="https://medium.com/@strangecosmos/artificial-intelligence-will-be-bigger-than-anything-that-has-ever-ever-happened-7289e1035f9c">in 2015</a> .</p><p> Between 2017 and 2023, I made a lot of money on Tesla because I bet (wrongly) that it would rapidly commercialize full autonomy and the investment ended up returning 10x for reasons almost entirely unrelated to full autonomy.</p><h3> Disappointment</h3><p> &quot;Heartbroken&quot; is not an exaggeration for how I feel about the meagre progress in self-driving cars from 2017 to 2023. The technology truly felt to me to be on the cusp of realization, but it&#39;s still a science project. <span class="footnote-reference" role="doc-noteref" id="fnrefxn2ssdui3z"><sup><a href="#fnxn2ssdui3z">[2]</a></sup></span></p><p> It might sound strange to talk about disillusionment with AGI in the same year that GPT-4 was released and the world caught LLM fever. Maybe a better word would be <i>fatigue</i> . I&#39;m tired of waiting for AGI.</p><h3> Depression</h3><p> The biggest existential risk I personally face is probably clinical depression. I&#39;ve tried medications, talk therapy, <a href="https://www.mayoclinic.org/tests-procedures/transcranial-magnetic-stimulation/about/pac-20384625">rTMS</a> , <a href="https://en.wikipedia.org/wiki/Neurofeedback">neurofeedback</a> , art therapy, physical exercise, (legal, medicinal) ketamine, and more, but I remain badly depressed. <span class="footnote-reference" role="doc-noteref" id="fnref1wdtdlt81uy"><sup><a href="#fn1wdtdlt81uy">[3]</a></sup></span></p><p> From the perspective of my own personal survival, the promise of friendly or aligned superhuman AGI to solve virtually all problems, including curing my depression, feels more appealing than the threat of unfriendly or misaligned AGI feels scary or dangerous. This is especially so because I&#39;m (seemingly) far more sanguine about AI x-risk than the median person who worries about it openly.</p><h3>老化</h3><p>A more generalizable line of thinking is: by default, I&#39;m going to die of aging and so are all the people I love — barring an even worse misfortune. Aligned AGI would cure aging and, thereby, save my life and the lives of all the people I love. Therefore, there is some urgency to developing AGI. <span class="footnote-reference" role="doc-noteref" id="fnref56qqkpetdz5"><sup><a href="#fn56qqkpetdz5">[4]</a></sup></span></p><p> This perspective ignores future generations, which is admittedly a weakness. However, prioritizing future generations above oneself and one&#39;s loved ones is <a href="https://www.smbc-comics.com/comic/longtermism">psychologically hard</a> . <span class="footnote-reference" role="doc-noteref" id="fnrefh7m929qtdm"><sup><a href="#fnh7m929qtdm">[5]</a></sup></span></p><h3> A heterodox view on AI risk</h3><p> <strong>The standard view is:</strong> we must solve AI alignment before developing AGI. Slowing development would give researchers more time to solve alignment and, therefore, a better chance of solving it before it&#39;s too late.</p><p> <strong>My heterodox view is:</strong> capabilities research and safety research are not so separable. The closer we get to AGI, the better equipped researchers will be to understand how to do alignment. Slowing down capabilities research also slows down safety research.</p><p> This heterodox view is false if <a href="https://www.lesswrong.com/posts/ED28KSXKc4j8CNoi8/how-would-the-scaling-hypothesis-change-things">the scaling hypothesis</a> — which holds that data and compute are the remaining substantive barriers to AGI, rather than new science — is true. In my understanding, new science is needed for AGI. This includes, but is not limited to, solving <a href="https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/">video prediction</a> and <a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_363">hierarchical planning</a> . I think it would be hard to convincingly argue that these are not open problems that need to be solved before AGI is possible. (However, I&#39;m open to changing my mind.)</p><p> A somewhat more esoteric idea, advocated by <a href="https://www.youtube.com/watch?v=Z1KwkpTUbkg">Jeff Hawkins</a> , among others, is that AGI will require the creation of a new kind of neural network that more closely resembles human biology. I don&#39;t have high conviction in this idea, but I personally find it hard to completely rule out.</p><p> On this heterodox view, decelerating AI capabilities research would be a net harm, since it doesn&#39;t reduce the downside risk of AGI while delaying the upside potential.</p><h3> Biological anchors</h3><p> Ajeya Cotra&#39;s <a href="https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/">&quot;Biological Anchors&quot; report</a> attempts to benchmark the training compute required for AGI (or, technically, &quot;transformative AI&quot;) to the compute required for human learning or, in the most conservative case, human evolution. This report is highly speculative and should be viewed with healthy skepticism. That being said, it is perhaps the most thorough and rigorous attempt to date to predict AGI using data from biology.</p><p> The weighted average of models used in the report predicts a 50% chance of AGI by around 2050 and a 75% chance of AGI by around 2080. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pPrELFnR6Hp3vJWwQ/aseegpgvcup4vrivtpst" alt="Chart: &quot;Probability that FLOP to train a transformative model is affordable BY year Y&quot;"><figcaption> Chart by <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Ajeya Cotra</a> . Please note that Cotra has <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines">updated her personal AI timelines</a> since publishing the report.</figcaption></figure><p> If you take these estimates to be a reasonable guess, then this is a nudge in the direction of thinking that AGI is quite a bit father than, say, the median prediction <a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">on Metaculus</a> of 2032.</p><p> On the standard view that we need lots of time to figure out alignment, this is good news. From the perspective of someone who&#39;s impatient for AGI to arrive, this is bad news.</p><h3> Final thoughts</h3><p> The failure of self-driving cars to reach large-scale commercial adoption by now has significantly set back my previous optimism about the pace of AI progress. Conversely, if Tesla (or another company) were to convincingly demonstrate superhuman driving capability, I would see that as a strong indicator that AGI might not be far off.</p><p> GPT-4 is extremely impressive and even spiritually profound, but there exist foreseeable challenges to future progress toward AGI, such as video prediction and hierarchical planning.</p><p> The &quot;Biological Anchors&quot; approach suggests we might be three to six decades away from having the training compute required for AGI.</p><p> Because <strong>1)</strong> I want AGI to cure my depression, <strong>2)</strong> I want AGI to cure aging before I or my loved ones die, <strong>3)</strong> I feel generally sanguine about AGI x-risk relative to (what I perceive to be) the median person who worries about alignment, and <strong>4)</strong> I think for safety research to be useful capabilities research must progress, I am not worried about AGI coming too fast. I despair that it seems to be coming too slow. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkzqnz1upen8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkzqnz1upen8">^</a></strong></sup></span><div class="footnote-content"><p> The <a href="https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)">DARPA Urban Challenge</a> was in 2007. That was also the year Ray Kurzweil&#39;s <a href="https://www.youtube.com/watch?v=IfbOyw3CT6A">first public TED Talk</a> was published on YouTube.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxn2ssdui3z"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxn2ssdui3z">^</a></strong></sup></span><div class="footnote-content"><p> As recently as 2022, I was willing to <a href="https://longbets.org/887/">make bets</a> about full autonomy, but I&#39;ve grown jaded about the field and more agnostic about the timeline. I no longer feel confident that even by 2037 full autonomy will be commercialized.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1wdtdlt81uy"> <span class="footnote-back-link"><sup><strong><a href="#fnref1wdtdlt81uy">^</a></strong></sup></span><div class="footnote-content"><p> I strongly encourage anyone suffering from depression to try all of the above treatments, since the scientific evidence is good and, anecdotally, I know they&#39;ve worked for some people. Please get help!</p></div></li><li class="footnote-item" role="doc-endnote" id="fn56qqkpetdz5"> <span class="footnote-back-link"><sup><strong><a href="#fnref56qqkpetdz5">^</a></strong></sup></span><div class="footnote-content"><p> I first heard this point being raised by <a href="https://youtu.be/YeXHQts3xYM?si=cSOHp8z6CvrK0zZV&amp;t=7206">Joscha Bach</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh7m929qtdm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh7m929qtdm">^</a></strong></sup></span><div class="footnote-content"><p> It can be <a href="https://www.smbc-comics.com/comic/longtermism-2">even harder</a> if you assume the far future will be radically better than the present, even though from a purely altruistic perspective that should encourage you to prioritize future generations even more, since it means those generations will have high net utility.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/pPrELFnR6Hp3vJWwQ/despair-about-ai-progressing-too-slowly#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/pPrELFnR6Hp3vJWwQ/despair-about-ai-progressing-too-slowly<guid ispermalink="false"> pPrELFnR6Hp3vJWwQ</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Sat, 04 Nov 2023 07:52:24 GMT</pubDate> </item><item><title><![CDATA[If a little is good, is more better?]]></title><description><![CDATA[Published on November 4, 2023 7:10 AM GMT<br/><br/><p> I&#39;ve recently seen a bunch of discussions of the wisdom of publicly releasing the weights <sup><a href="#fn:1" rel="footnote">1</a></sup> of advanced AI models. A common argument form that pops up in these discussions is this:</p><ol><li> The problem with releasing weights is that it means that thing X can happen on a large scale, which causes bad effect Y.</li><li> But bad effect Y can already happen on a smaller scale because of Z.</li><li> Therefore, either it&#39;s OK to release weights, or it&#39;s not OK that Z is true.</li></ol><p> One example of this argument form is about the potential to cause devastating pandemics, and goes as follows:</p><ol><li> The putative problem with releasing the weights of Large Language Models (LLMs) is that it can help teach people a bunch of facts about virology, bacteriology, and biology more generally, that can teach people how to produce pathogens that cause devastating pandemics.</li><li> But we already have people paid to teach students about those topics.</li><li> Therefore, if that putative problem is enough to say that we shouldn&#39;t release the weights of large language models, we should also not have textbooks and teachers on the topics of virology, bacteriology, and other relevant sub-topics of biology. But that&#39;s absurd!</li></ol><p> In this example, thing X is teaching people a bunch of facts, bad effect Y is creating devastating pandemics, and Z is the existence of teachers and textbooks.</p><p> Another example is one that I&#39;m not sure has been publicly written up, but occurred to me:</p><ol><li> Releasing the weights of LLMs is supposed to be bad because if people run the LLMs without supervision, they can do bad things.</li><li> But if you make LLMs in the first place, you can run them without supervision.</li><li> So if it&#39;s bad to publicly release their weights, isn&#39;t it also bad to make them in the first place?</li></ol><p> In this example, thing X is running the model, bad effect Y is generic bad things that people worry about, and Z is the model existing in the first place.</p><p> However, I think these arguments don&#39;t actually work, because they implicitly assume that the costs and benefits scale proportionally to how much X happens. Suppose instead that the benefits of thing X grow proportionally to how much it happens <sup><a href="#fn:2" rel="footnote">2</a></sup> : for example, maybe every person who learns about biology makes roughly the same amount of incremental progress in learning how to cure disease and make humans healthier. Also suppose that every person who does thing X has a small probability of causing bad effect Y for everyone that negates all the benefits of X: for example, perhaps 0.01% of people would cause a global pandemic killing everyone if they learned enough about biology. Then, the expected value of X happening can be high when it happens a little (because you probably get the good effects and not the bad effects Y), but low when it happens a lot (because you almost certainly get bad effect Y, and the tiny probability of the good effects isn&#39;t worth it). In this case, it makes sense that it might be fine that Z is true (eg that some people can learn various sub-topics of biology with great tutors), but bad to publicly release model weights to make X happen a ton.</p><p> So what&#39;s the up-shot? To know whether it&#39;s a good idea to publicly release model weights, you need to know the costs and benefits of various things that can happen, and how those scale with the user-base. It&#39;s not enough to just point to a small amount of the relevant effects of releasing the weights and note that those are fine. I didn&#39;t go thru this here, but you can also reverse the sign: it&#39;s possible that there&#39;s some activity that people can do with model weights that&#39;s bad if a small number of people do it, but good if a large number of people do it: so you can&#39;t necessarily just point to a small number of people doing nefarious things with some knowledge and conclude that it would be bad if that knowledge were widely publicized.</p><div><ol><li id="fn:1" role="doc-endnote"><p> Basically, the parameters of these models. Once you know the parameters and how to put them together, you can run the model and do what you want with it. <a href="#fnref:1">↩</a></p></li><li id="fn:2" role="doc-endnote"><p> Or more generally, polynomially (eg maybe quadratically because of Metcalfe&#39;s law). <a href="#fnref:2">↩</a></p></li></ol></div><br/><br/> <a href="https://www.lesswrong.com/posts/R56gihyfwERKDLszQ/if-a-little-is-good-is-more-better#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/R56gihyfwERKDLszQ/if-a-little-is-good-is-more-better<guid ispermalink="false"> R56gihyfwERKDLszQ</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sat, 04 Nov 2023 07:10:06 GMT</pubDate></item></channel></rss>