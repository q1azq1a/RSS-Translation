<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 7 日星期四 22:11:30 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[A quick update from Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 9:28 PM GMT<br/><br/><p><strong>我们正在收集的证据的一个例子</strong></p><p>我们正在努力对<a href="https://forum.effectivealtruism.org/posts/32LMQsjEMm6NK2GTH/sharing-information-about-nonlinear">本的文章</a>进行逐点回应，但想提供一个我们准备分享的证据类型的快速示例：</p><p><strong>她的说法是：</strong> “爱丽丝声称她在国外得了新冠病毒，周围只有三位非线性联合创始人，但家里没有人愿意出去给她吃纯素食品，所以她两天几乎没吃东西。”</p><p><br><strong>真相（见下面的截图）</strong> ：</p><ol><li>家里<i>有</i>纯素食品（燕麦片、藜麦、混合坚果、李子、花生、西红柿、麦片、橙子），我们愿意为她做饭。</li><li>我们<i>正在</i>为她挑选纯素食品。</li></ol><p>几个月后，我们的关系恶化后，她到处告诉很多人我们让她<i>挨饿</i>。我们认为她所包含的细节是经过精心挑选的，目的是为了以一种最具破坏性的方式来描绘我们——还有什么比拒绝照顾一个独自在异国他乡生病的女孩更辱骂的呢？如果有人告诉你这些，你可能会相信他们，因为谁会编造这样的事情呢？</p><p><strong>证据</strong></p><ul><li>下面的屏幕截图显示，在爱丽丝生病的第一天，凯特给她提供了家里的纯素食品（燕麦片、藜麦、麦片等）。然后，当她对我们带来/准备这些不感兴趣时​​，我告诉她让德鲁去拿食物，德鲁答应了。凯特也离开了家，去附近给她买了土豆泥。 <br></li></ul><p><img style="width:226.938px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/lfaqzannpavde8iqdiqj"></p><p><img style="width:223.531px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/b82dnojjrhyqnoxyaulv"></p><p><img style="width:235.75px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/qh77s05gwirrh8s0iw92"></p><p><img style="width:243.727px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/dlcoynrdkwsszm6hbsxo"></p><p><img style="width:249.836px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eLHL8FsaTwMJz2jqc/skl5g6r8y8khlt0sqber"></p><ul><li> <a href="https://docs.google.com/document/d/171yUeCg3Z3HDPRrPT_nKSZaNaj4ujZYs73af_zmuW2U/edit?usp=sharing"><u>请在此处查看德鲁与她对话的更多屏幕截图</u></a>。</li></ul><p>最初，我们听到她告诉人们她“已经好几天没吃东西了”，但她似乎已经将自己的说法调整为“几乎没有吃东西”“两天”。</p><p>值得注意的是，爱丽丝并没有因为一些小事和不重要的事情而撒谎。她指责我们是一种极不道德的行为——这种行为大多数人一听到就会立刻认为你一定是个可怕的人——结果被发现撒谎了。</p><p>我们相信 EA 的很多人都听到了这个谎言，并更新了对我们不利的信息。像这样的一个虚假谣言就会不公平地损害某人行善的能力，而这只是她所说的众多谣言之一。</p><p>我们有工作合同、面试录音、收据、聊天记录等等，我们正在全职准备这些内容。</p><p>这个说法是本文章中的几句话，但我们花了几个小时来反驳，因为我们必须追踪所有对话，使它们可读，添加上下文，匿名化人员，检查我们的事实，并写出严格而清晰的解释。 Ben 的文章超过 10,000 字，我们正在尽快回应他提出的每一个观点。</p><p>再次强调，我们并不是要求社区无条件地相信我们。我们想向大家展示所有证据，并对我们所犯的错误承担责任。</p><p>我们只是要求您不要只听到一方的最新消息，并对我们将尽快分享的证据保持开放的态度。</p><br/><br/> <a href="https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FHxYPpMkAX9ayoBuK/a-quick-update-from-nonlinear<guid ispermalink="false"> FHxYPpMkAX9ayoBuK</guid><dc:creator><![CDATA[KatWoods]]></dc:creator><pubDate> Thu, 07 Sep 2023 21:28:27 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Frontier AI Taskforce: first progress report]]></title><description><![CDATA[Published on September 7, 2023 7:06 PM GMT<br/><br/><h1>其他链接</h1><ul><li><a href="https://twitter.com/SciTechgovuk/status/1699686882022486145">伊恩·霍加斯的简短采访</a></li><li><a href="https://twitter.com/soundboy/status/1699688880482500684">推特话题</a></li></ul><h1>报告中的一些引述</h1><h2>介绍</h2><blockquote><p>该工作组是政府内部的一家初创企业，致力于履行总理赋予我们的雄心勃勃的使命：建立一支能够评估人工智能前沿风险的人工智能研究团队。随着人工智能系统变得更加强大，它们可能会显着增加风险。提高人类编写软件能力的人工智能系统可能会增加网络安全威胁。人工智能系统在生物学建模方面的能力变得更强，可能会加剧生物安全威胁。为了管理这种风险，技术评估至关重要 - 并且这些评估需要由中立的第三方开发 - 否则我们将面临人工智能公司标记自己作业的风险。</p><p>鉴于这些潜在的重大前沿风险，截至今天，该工作组将更名为前沿人工智能工作组。</p><p>这是前沿人工智能工作组的第一份进度报告。</p></blockquote><h2>涵盖人工智能研究和国家安全的专家顾问委员会</h2><blockquote><p>鉴于前沿系统的许多风险涉及国家安全领域，我们成立了一个专家咨询委员会，由人工智能研究和安全领域的一些世界领先专家以及英国国家安全界的关键人物组成。我们最初的顾问委员会成员是：</p><p><strong>约书亚·本吉奥</strong>. Yoshua 以其在深度学习领域的开创性工作而闻名，并为他赢得了 2018 年 AM 图灵奖，与 Geoffrey Hinton 和 Yann LeCun 一起荣获“计算机界的诺贝尔奖”。他是蒙特利尔大学的正教授，也是魁北克人工智能研究所 Mila 的创始人兼科学主任。</p><p><strong>保罗·克里斯蒂安诺</strong>. Paul 是人工智能对齐领域的领先研究人员之一。他是对齐研究中心 ARC 的联合创始人，此前曾负责 OpenAI 的语言模型对齐团队。</p><p><strong>马特·柯林斯</strong>。马特是英国负责情报、国防和安全的副国家安全顾问。 IYKYK。</p><p><strong>安妮·凯斯特-巴特勒</strong>。安妮是英国政府通讯总部的主任。安妮在英国国家安全网络的核心领域拥有令人印象深刻的记录，帮助应对恐怖分子、网络犯罪和恶意外国势力构成的威胁。</p><p><strong>亚历克斯·范·萨默伦</strong>。亚历克斯是英国国家安全首席科学顾问。 Alex 此前是一名风险投资家和企业家，专注于投资早期的“深度技术”初创公司。</p><p><strong>海伦·斯托克斯-兰帕德</strong>。除了国家安全和人工智能研究专业知识之外，我们还很高兴建立一个咨询委员会，可以讨论前沿人工智能在社会前沿的关键用途。 Helen 不仅是一名执业全科医生，观察对话式人工智能工具如何影响日常医疗诊断，而且还是英国医学界经验丰富的领导者，曾担任皇家全科医生学院主席，现任皇家学院医学院院长。</p><p><strong>马特·克利福德</strong>。 Matt 是 AI 安全峰会的首相联合代表、ARIA 主席和 Entrepreneur First 联合创始人。他被任命为专家咨询委员会副主席，展示了英国围绕前沿人工智能倡议（包括工作组和人工智能安全峰会）的协调水平。</p></blockquote><h2>招聘人工智能专家研究员</h2><blockquote><p>我们正在利用世界领先的专业知识：</p><p> <strong>Yarin Gal</strong>将加入牛津大学工作组，担任研究主任，并担任牛津应用和理论机器学习小组的负责人。 Yarin 是全球公认的机器学习领域的领导者，并将保留他在牛津大学副教授的职位。</p><p><strong>大卫·克鲁格</strong>将与特别工作组合作，在峰会前夕确定其研究计划的范围。 David 是剑桥大学计算和生物学习实验室的助理教授，领导一个专注于深度学习和人工智能对齐的研究小组。</p><p>该工作组设在英国科学创新与技术部 (DSIT) 内，该部雇用了大约 1500 名公务员。<a href="https://twitter.com/soundboy/status/1670343527723679744">当我 6 月份到达时，</a>该部门只有一名拥有 3 年前沿 AI 经验的前沿 AI 研究员。</p><p>这位孤独的研究员就是 Nitarshan Rajkumar，他于 4 月份暂停了博士学位并加入 DSIT，这证明了一位认真、极其勤奋的技术专家在致力于公共服务时可以取得多大的成就。 DSIT 大臣米歇尔·多尼兰 (Michelle Donelan) 聘请了 Nitarshan，他对英国为投资前沿人工智能安全而做出的许多大胆努力产生了重大影响。我们需要更多的尼塔山！</p><p>在工作组团队的大力推动下，我们现在拥有一支不断壮大的人工智能研究人员团队，他们在人工智能前沿拥有<strong>50 多年的集体经验</strong>。如果这是我们衡量前沿人工智能国家能力的指标，那么我们已经在短短 11 周内将其提高了一个数量级。我们的团队现在包括来自 DeepMind、微软、Redwood Research、人工智能安全中心和人类兼容人工智能中心的经验丰富的研究人员。</p></blockquote><h2>与领先的技术组织合作</h2><blockquote><p>引领人工智能安全并不意味着从头开始或单独工作——我们正在建立并支持一系列前沿组织开展的工作。我们很高兴地宣布与以下公司建立初步合作伙伴关系：</p><p> <strong>ARC Evals</strong>是一家非营利组织，致力于评估前沿人工智能系统的灾难性风险，之前曾与 OpenAI 和 Anthropic 合作，在其系统发布前评估其“自主复制和适应”能力。我们将与 ARC Evals 团队密切合作，在英国人工智能安全峰会召开之前评估前沿风险。我们还将与<strong>Redwood Research</strong>的团队以及 Jeff Alstott、Christopher Mouton 及其在非营利性<strong>兰德公司</strong>的团队合作，推动这一议程。</p><p> <strong>Trail of Bits</strong>是一家领先的网络安全研究和咨询公司，帮助保护了世界上一些最具针对性的组织。我们正在启动深度合作，以了解网络安全和前沿人工智能系统交叉点的风险。这项工作将由<a href="https://twitter.com/heidykhlaaf?lang=en">Heidy Khlaaf</a>领导，他专门从事安全关键系统的软件评估、规范和验证，并且在 OpenAI 期间还领导了 Codex 的安全评估。</p><p><strong>集体智慧项目</strong>是一个非营利组织，致力于为变革性技术孵化新的治理模式，其使命是将技术发展导向集体利益。联合创始人 Divya Siddarth 和 Saffron Huang 将借调加入我们，帮助我们为前沿模型制定一系列社会评估。</p><p><strong>人工智能安全中心</strong>是一个非营利组织，致力于通过基础安全研究、研究基础设施和技术专业知识来支持政策制定者，减少人工智能带来的社会规模风险。我们将与丹·亨德里克斯 (Dan Hendrycks) 和他的团队合作，在峰会之前与更广泛的科学界进行互动并为他们提供帮助。</p></blockquote><h2>为政府内部人工智能研究奠定技术基础</h2><blockquote><p>该工作组的核心目标是为政府内部的人工智能研究人员提供与 Anthropic、DeepMind 或 OpenAI 等领先公司相同的资源来研究人工智能安全。正如总理所宣布的那样，这些公司已经承诺为我们提供深入的模型访问权限，以便工作组的研究人员进行模型评估的能力不受限制。我们还与 No10 Data Science（“10DS”）密切合作，以便我们的研究人员和工程师拥有他们开始运行所需的计算基础设施，用于模型微调、可解释性研究等。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FhKhhmK4DXrogJxRr/linkpost-frontier-ai-taskforce-first-progress-report<guid ispermalink="false"> FhKhhmK4DXrogJxRr</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Thu, 07 Sep 2023 19:06:26 GMT</pubDate> </item><item><title><![CDATA[How did you make your way back from meta?]]></title><description><![CDATA[Published on September 7, 2023 5:23 PM GMT<br/><br/><p>我注意到我自己非常偏爱关注元级别。它在我所热爱的领域最为明显，比如写作。例如，我会花更多的时间阅读修辞学或查找 1980 年代关于写作技巧的稀有书籍，而不是练习写论文或故事。</p><p>我不喜欢这样，因为我研究元级别的最终目标是在对象级别上变得更好。与此同时，有时我也会因为如此超前而得到奖励。我在工作中得到了很多尊重，同时我也提供了极好的反馈和观点。</p><p>这种状态似乎不会立即产生痛苦的影响。我有家庭和工作，总体来说我的生活似乎很顺利。但人们渴望a）将理论付诸实践并看到结果（即通过元支付租金），b）从直接经验中学习并与他人分享这些经验。元是一个孤独的地方。</p><p>我不认为我是唯一一个担任这一职位的人。我只用了几秒钟的搜索就找到了这两篇文章（我确信还有更多）：https: <a href="https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction">//www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction</a> <a href="https://www.lesswrong.com/posts/RnP5bR767NcxebYHd/conjecture-on-addiction-to-meta-level-solutions">https://www.lesswrong.com/posts/ RnP5bR767NcxebYHd/对元级解决方案成瘾的猜想</a></p><p>在过去的几天里，我发现自己开始进行元深化活动，并有意识地切换到对象级任务。到目前为止，这是值得的，所以我相信在几周内，我的习惯将转向我想要的地方。</p><p>但我很好奇：还有其他人经历过类似的事情吗？你过得怎么样？你做了什么？</p><br/><br/> <a href="https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B99sRmhWpMcZLpptH/how-did-you-make-your-way-back-from-meta<guid ispermalink="false"> B99sRmhWpMcZLpptH</guid><dc:creator><![CDATA[matto]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:23:18 GMT</pubDate> </item><item><title><![CDATA[AI#28: Watching and Waiting]]></title><description><![CDATA[Published on September 7, 2023 5:20 PM GMT<br/><br/><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/09/america-is-asleep-on-its-ai-boom.html">正如泰勒·考恩（Tyler Cowen）所指出的</a>，我们正处于一段平静时期。我们这些走在前面的人已经习惯了 GPT-4、Claude-2 和 MidJourney。功能和集成正在扩展，但速度相对缓慢。大多数人仍然幸福地没有意识到，这让我可以尝试对他们的新解释，而许多其他人则说这都是炒作。他们会一直这样说，直到有什么事情迫使他们不这样做，最有可能是 Gemini，尽管值得注意的是我在 2023 年看到了对 Gemini 的怀疑（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo">只有 25% 的人认为谷歌到年底会拥有最好的模型</a>），甚至在2024 年（ <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-google-have-the-best-llm-by-eo-b4ad29f8b98d">即使到明年年底也只有 41% 会发生。</a> ）</p><p>我认为这是持续好消息模式的一部分。虽然我们还有很长的路要走，面临很多不可能的问题，但半年来，话语和奥弗顿之窗以及对现实问题的认识和理解不断提高。主要实验室内外的联盟兴趣和资金正在迅速增长。普通的实用性也在稳步提高，带来的好处使成本相形见绌，而且迄今为止的普通危害比几乎任何人对现有技术的预期要轻得多。能力正在以快速且令人震惊的速度进步，但速度并没有我预期的那么令人震惊。</p><p>本周的亮点包括英国工作组的最新情况以及对 Inflection AI 的 Suleyman 的采访。</p><p>我们进展顺利。让我们继续保持下去。</p><p>即使本周的平凡实用性，我们可以说，实用性值得怀疑。</p><span id="more-23532"></span><h4>目录</h4><ol><li>介绍。</li><li>目录。</li><li><strong>语言模型提供了平凡的实用性</strong>。它已经注视着你了。</li><li>语言模型不提供平凡的实用性。谷歌搜索永远被毁了。</li><li> Deepfaketown 和 Botpocalypse 很快就会出现。我会过去的，谢谢。</li><li>他们抢走了我们的工作。不以有偏见的方式工作比根本不工作更好？</li><li>参与其中。人工智能政策和重新思考优先事项中心。</li><li>介绍一下。哦，太棒了，又一个竞争订阅服务。</li><li><strong>英国特别工作组更新</strong>。令人印象深刻的团队行动迅速。</li><li>在其他人工智能新闻中。你说人工智能会欺骗？骗我的。</li><li><strong>静静的猜测</strong>。版权法可能即将变得丑陋。</li><li>寻求健全的监管。完整的舒默会议列表。</li><li><strong>音频周</strong>。 80k 上的 Suleyman、Altman、Schmidt 和其他几个人。</li><li>修辞创新。还有几种不沟通的方式。</li><li>没有人会傻到这么做。最大程度自主的 DeepMind 代理。</li><li>调整比人类更聪明的智能是很困难的。更容易证明安全性？</li><li> Twitter 社区注释注释。 Vitalik 询问为什么它能一直保持良好状态。</li><li>人们担心人工智能会杀死所有人。他们的担忧程度正在慢慢上升。</li><li>其他人并不担心人工智能会杀死所有人。泰勒·考恩又来了。</li><li>较轻的一面。罗恩掌握了节奏。</li></ol><h4>语言模型提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697346570092773578">对《使命召唤》进行自动聊天审核</a>。考虑到实际的选择是许多游戏的聊天为零，而其他游戏的聊天充满了最卑鄙的败类和恶行，我不太支持“新的反乌托邦地狱景观”，而是支持“到底什么是更好的选择”这里。&#39;</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/fofrAI/status/1697550606288793679">监控您的员工和客户</a>。</p><blockquote><p> Rowan Cheung：来认识一下新的人工智能咖啡店老板。它可以跟踪咖啡师的工作效率以及顾客在店里花费的时间。我们正进入狂野时代。</p><p> Fofr：这在很多方面都很可怕。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e3bc9-3db2-4992-aeba-3bd888c47364_748x502.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/p732au93iomhowop7jeo" alt=""></a></figure><p>重要的不是工具，而是你如何使用它。摩根大通等一些公司已经使用剧毒的反乌托邦监控工具，这让他们更上一层楼。跟踪顾客在商店待了多长时间，或者他们是否是回头客以及他们等待订单的时间似乎非常有用。从广义上跟踪生产率（例如订单完成情况）是一种情况，过多的精确度和注意力会带来大问题，但不够精确和关注也会带来大问题。不做任何工作的客观答案比做大量工作的有偏见、容易出错的答案要好得多。</p><p>在社交媒体上<a target="_blank" rel="noreferrer noopener" href="https://www.disclose.tv/id/6cew4vxu8k/">监控您的公民</a>（以下是整个帖子）。</p><blockquote><p> Dissclose.tv：美国特种作战司令部 (USSOCOM) 已与 Acccrete AI 签订合同，部署软件来检测社交媒体上的“实时”虚假信息威胁。</p></blockquote><p>这似乎正是这些人以前所做的事情？这里的问题不是人工智能。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/peterwildeford/status/1697571328868356267">第一次在与人类的体育运动中获胜</a>，我确信这没什么，这项运动是（检查笔记）无人机竞赛。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/david_perell/status/1699076351150428185">获得写作方面的帮助</a>。正如 Parell 指出的那样，直接要求 ChatGPT 帮助你写作是没有用的，但作为个人图书管理员和事物解释者可能会很棒。他推荐“多说”一词，要求以不同作者的风格进行重述和摘要，来回交谈并始终尽可能具体，并让程序检查拼写错误。</p><p>伊森·莫里克（Ethan Mollick）建议开发他所谓的《魔法书》（Grimoires），我的大脑想要自动更正咒语书（他也使用这个术语），旨在优化交互的提示，包括给人工智能一个角色、目标、逐步说明，可能请求提供示例并让人工智能从用户那里收集必要的上下文。</p><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.01404">玩《Hoodwinked》游戏</a>，类似于《黑手党》或《Among Us》。正如人们所期望的那样，能力更强的模型胜过能力较差的模型，并且根据游戏的进行方式经常撒谎和欺骗。对于人类来说，在此类游戏中，正确的策略通常是，如果可以的话，说出如果你是无辜的，你会说的话的某种变体，这对于法学硕士来说可能很容易做到。请注意，随着法学硕士变得更加聪明，其他策略变得更优越，然后是人类无法实现的其他策略，然后是人类甚至没有考虑过的策略。</p><h4>语言模型不提供平凡的实用性</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1697648862272401685">许多人说，要小心人工智能生成的垃圾文章，</a>尽管我还没有真正遇到过这样的文章。瑞安（Ryan）在这里是正确的，罗希特（Rohit）也是正确的，尽管两者都没有解决问题。</p><blockquote><p> Paul Graham：我正在网上查找一个主题（披萨烤箱应该有多热），我注意到我正在查看文章的日期，试图找到不是人工智能生成的 SEO 诱饵的内容。</p><p>瑞恩·彼得森：保罗，越热越好！</p><p> Rohit：现在生成搜索似乎还可以吗？除非，是的，您想更深入地研究更具体的东西？ [显示谷歌的回应。]</p></blockquote><p>这是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699745755286704466">我本周看到的其他几个说法之一</a>，即谷歌搜索正在被法学硕士生成的垃圾迅速污染。</p><p>当 Steam（Valve）参与时，即使看看 AI，也要小心， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/WaifuverseAI/status/1697764665521295838">他们会永久删除游戏一次，允许一个允许角色使用 GPT 生成的对话的模组</a>，即使该模组随后被删除。虽然我确实很钦佩一个人完全致力于这一点，但这显然太过分了，我希望 Valve 意识到这一点并改变他们的决定。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JudgeFergusonTX/status/1698904660310954186">法官罗伊·弗格森问克劳德他是谁</a>，陷入了无数捏造信息的循环中，克劳德道歉并承认捏造信息。绝对是一个问题。弗格森认为这是克劳德的“故意”，我认为这是对法学硕士工作方式的误解。</p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p>到目前为止，唐纳德·特朗普在政治上充分利用了深度造假技术。他们是否不可避免地偏爱像他这样有才华的人？另一个需要考虑的角度是，谁比特朗普的支持者更容易受到这种策略的影响？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/astridwilde1/status/1697400187361395048">阿斯特丽德·王尔德</a>：越来越多的人注意到针对特朗普支持者的越来越复杂的人工智能欺骗攻击。这次袭击令人信服，甚至获得了播出时间。一个说话节奏像<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michaelmalice">@michaelmalice</a>的人正在用人工智能欺骗特朗普的声音。这是一个美丽新世界。也有可能 RAV 本身就是恶搞的幕后黑手，但我们无法得知。绝对狂野的时光</p></blockquote><p>我一时兴起检查了 r/scams 大约 40 个帖子。几乎所有的内容都是老派， <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/Scams/comments/165oqrn/my_partner_and_i_almost_got_taken_in_by_a_deep/">其中一篇</a>是关于经典的“你的孩子已被捕，你必须交保释金”骗局的报道。回复拒绝相信这是真正的深度伪造，称这是正常的假声音。似乎连骗局专家都没有意识到现在进行深度伪造是多么容易，或者他们已经习惯了一切都是骗局（因为如果你必须问，我有一些新闻），他们认为深度伪造一定是也是骗局？</p><p>值得注意的是，诈骗尝试失败了。我们不断听到“我差点就上当了”，但却没有听到任何真正赔钱的人的消息。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AviSchiffmann/status/1698147373086896374">不言自明且目前令人深感失望的</a>“ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ehalm_/status/1698107101892268113">smashOrPass.ai</a> ”提供了最新的用户微调动机，无论人们如何看待所涉及的道德规范。在撰写本文时，它只是循环中的一小部分图像，因此声称它“学习您喜欢的内容”似乎相当愚蠢，但这很容易解决。不太容易解决的是所有的混杂因素，这应该很好地说明这一点。这完美地说明了当压缩到 0-1 比例时会丢失多少数据，而且人们要么根据上下文进行调整，要么不调整，这两种方法都会非常令人困惑，这里确实需要 0-10。是的，如果有人想知道的话，当然色情版本很快就会推出，即使不是他而不是其他人。更有趣的是，需要多长时间才能发布获取此信息并使用它为您自动滑动约会应用程序个人资料的版本？</p><p>另外，我可以给每个讨厌这个或互联网上其他任何东西的人明显的建议吗？比如<a target="_blank" rel="noreferrer noopener" href="https://www.vice.com/en/article/g5ywp7/you-know-what-to-do-boys-sexist-app-lets-men-rate-ai-generated-women?utm_source=VICE_Twitter&amp;utm_medium=social+">Vice</a>的Janus Rose？不要为了抱怨而强调没有人听说过的事情。这件事引起我的注意完全是因为负面宣传。</p><p><a target="_blank" rel="noreferrer noopener" href="https://archive.ph/jEsmt">詹姆斯·迪恩再现，出演新电影</a>。毫无疑问，还会有更多这样的事情发生。奇怪的是，人们正在谈论克隆伟大的演员，包括伟大的配音演员，无论是否已故，并用它来让活着的演员失业。</p><p>使用人工智能的问题在于人工智能不是一个好演员。</p><p>人工智能配音演员可以复制梅尔·布鲁克斯的声音，但声音与梅尔·布鲁克斯的伟大之处没有什么关系。我想你实际上会做的，至少在一段时间内，是让一些伟大的配音演员录制新台词。然后使用人工智能改造新台词，将梅尔·布鲁克斯的声音风格融入其中。</p><p>如果我们让汤姆·汉克斯或苏珊·萨兰登（均在OP中引用）在死后继续工作，那么我们选择重新塑造他们的形象和声音，而无法复制他们的实际才能或技能。就我们从他们身上获得“良好表现”而言，我们可以使用任何拥有足够记录数据作为基线的人来获得这种表现，例如你的妈妈，或者绝对不会表演的华丽时装模特。当有人去世时，将其用于续集是有意义的，并且连续性优先，但未来的演员可能会是那些具有外观的人？因此詹姆斯·迪恩（James Dean）或者玛丽莲·梦露（Marylin Monroe）都是有道理的。或者某个人的存在具有重要的象征意义。</p><h4>他们抢走了我们的工作</h4><p>AI检测工具不起作用。我们知道这一点。现在我们有数据表明它们不起作用，那就是<a target="_blank" rel="noreferrer noopener" href="https://themarkup.org/machine-learning/2023/08/14/ai-detection-tools-falsely-accuse-international-students-of-cheating">标记斯坦福大学非英语母语人士的工作。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f4a9ad0-7a92-4545-b9c8-336441fdfff3_1020x634.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/kvkv0dvfyixr5psjckud" alt=""></a></figure><blockquote><p>当单词选择可预测且句子更简单时，人工智能检测器往往会被编程为将写作标记为人工智能生成。事实证明，非英语母语人士的写作往往符合这种模式，这就是问题所在。</p></blockquote><p>问题是测试不起作用。这说明测试不起作用。它碰巧击中了非母语人士，这说明我们当前的检测尝试是多么可悲。</p><p>我们在这方面训练的人工智能是错位的。他们注意到单词复杂度是训练数据中统计上有效的指标，因此他们尽可能地提高了分数。可以在没有这种相关性的情况下生成一个定制的训练集，然后再试一次吗？也许可以，但我预计在我们得到可以使用的东西之前，需要多次循环。</p><p>如果说有什么不同的话，那就是这种区分使人工智能探测器变得更有用，而不是更没用。通过将其错误集中在特定位置并使用可测试的解释，您可以排除其中的许多错误。如果您从未在非母语人士的工作中使用它，它就不会歧视他们。</p><p>它还展示了一种利用单词选择的复杂性来伪装人工智能工作的简单方法。</p><h4>参与其中</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/">人工智能政策中心</a>是一个新组织，致力于制定和倡导政策，以减轻先进人工智能带来的灾难性风险。他们正在招聘一名人工智能政策分析师和一名通讯总监。他们最近提出了《<a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/work">负责任的人工智能法案》</a> ，该法案需要在一些地方进行完善，但非常好提出，因为它是一项将事情朝着富有成效的方向发展的具体提案。<a target="_blank" rel="noreferrer noopener" href="https://www.aipolicy.us/careers">在此</a>了解更多信息并申请。</p><p><a target="_blank" rel="noreferrer noopener" href="https://rethinkpriorities.org/xst-incubation">重新思考人工智能安全工作孵化的优先事项</a>，包括<a target="_blank" rel="noreferrer noopener" href="https://careers.rethinkpriorities.org/en/postings/0980aba8-466a-4282-9319-c8c4f6f39341">在大学中为人工智能政策职业开展实地建设</a>。请对项目孵化中心的计划持怀疑态度，该中心旨在帮助孵化未来项目的人员。我明白数学在理论上是如何运作的，但大多数做某事的人最终必须直接做这件事，否则什么都不会完成。</p><h4>介绍</h4><p><a target="_blank" rel="noreferrer noopener" href="https://time.com/collection/time100-ai/">《时代》杂志推出了人工智能时代 100 强</a>。我会调查一下，但现在我发现我们的时间已经到了。</p><p> Anthropic 的<a target="_blank" rel="noreferrer noopener" href="https://support.anthropic.com/en/articles/8324991-about-claude-pro-usage">Claude Pro</a> ，每月支付 20 美元以获得更高的带宽和优先级。我从未遇到过 Claude 的使用限制，尽管发现它非常有用 - 我的对话往往相对较短，我做的唯一昂贵的事情就是附加巨大的 PDF，他们说这缩小了限制，但我已经尚未遇到任何问题。值得注意的是，在使用此类附件时，询问少量广泛的答案比询问大量小问题更有效。</p><p> HuggingFace 表示， <a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/blog/falcon-180b">Falcon 180B</a>是一个比 Llama-2 稍微好一点的模型，但相对于其规模和成本而言，它的性能更差。他们说在评估基准上“介于 GPT-3.5 和 GPT-4 之间”，我仍然认为在实际使用中它将保持在 3.5 以下。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/announcing-openai-devday">OpenAI 将于 11 月 6 日在旧金山举办开发者大会。</a></p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1699492275209003425">Sam Altman</a> ：11 月 6 日，我们将向开发者展示一些很棒的东西！ （没有 gpt-5 或 4.5 或类似的东西，冷静下来，但我仍然认为人们会很高兴......）</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.ycombinator.com/launches/JOw-automorphic-infuse-knowledge-into-language-models-with-just-10-samples">Automorphic（YC &#39;23 公司）提供即用即训练微调</a>，包括连续 RLHF，使用尽可能少的示例，免费提供前三个模型的微调。肯定是应该有的东西，不知道发货了没有，有人试试吗？</p><h4>英国特别工作组更新</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1699688880482500684">伊恩·霍加斯 (Ian Hogarth) 提供的英国基金会模型工作组的最新情况</a>（ <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">直接</a>）。顾问委员会看起来是一流的，包括本吉奥和克里斯蒂亚诺以及国家安全专业知识的索梅伦。他们正在与 ARC Evals、人工智能安全中心等机构合作。十一月的峰会即将到来，所以一切都进展得很快。他们正在迅速扩张，并且仍在招聘。</p><blockquote><p>伊恩·霍加斯（Ian Hogarth）：OpenAI 首席执行官萨姆·奥尔特曼（Sam Altman）最近表示，公共部门“缺乏引领创新的意愿”，他问道：“你为什么不问问政府为什么他们不做这些事情，不是吗？”那是最可怕的部分吗？”</p><p>我们有足够的意愿来改变人工智能安全前沿的国家能力。这就是为什么我们正在以启动速度向政府聘用人工智能技术专家。我们正在利用世界领先的专业知识。</p><p> ……</p><p>我们的团队现在包括来自 DeepMind、微软、Redwood Research、人工智能安全中心和人类兼容人工智能中心的经验丰富的研究人员。</p><p>这些是世界上最难雇用的人。他们选择进入公共服务并不是因为这很容易，而是因为它提供了从根本上改变社会应对人工智能前沿风险的方法的机会。</p><p>我们正在迅速扩大这个团队，并正在寻找有兴趣促进人工智能安全国家能力的研究人员。我们计划将团队规模再扩大一个数量级。 <a target="_blank" rel="noreferrer noopener" href="https://docs.google.com/forms/d/1HKVnrV_rBHF3w4StWUs0XNhxTtKkTHs5CpMnH6PM0pQ/viewform?edit_requested=true">请考虑在这里申请加入我们</a>。</p><p>首届人工智能安全峰会将于 11 月 1 日至 2 日在英国举行，这是影响人工智能安全的关键时刻。我们特别关注对前沿模型的技术风险评估感兴趣的人工智能研究人员。</p><p> ……</p><p>快速行动很重要。政府从一开始就在 11 周内完成了如此多的工作，需要一支由敬业的、才华横溢的公务员组成的令人难以置信的团队的大力努力。建立这个团队与上面提到的技术团队一样重要。</p><p>这就是我们聘请 Ollie Ilott 担任工作组主任的原因。奥利从唐宁街加入我们，他在那里领导首相的国内私人办公室，担任“副首席私人秘书”的关键角色</p><p>他因其招募和塑造一流团队的能力而“全面”闻名。在加入首相办公室之前，奥利在疫情大流行的第一年负责管理内阁办公室的新冠病毒战略团队，并领导团队参与英国脱欧谈判。</p></blockquote><h4>在其他人工智能新闻中</h4><p>Peter S. Park、Simon Goldstein、Aidan O&#39;Gara、Michael Chen、Dan Hendrycks 的<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.14752">新论文</a>：AI 欺骗：示例、风险和潜在解决方案调查。</p><p>这是摘要：</p><blockquote><p>本文认为，当前的一系列人工智能系统已经学会了如何欺骗人类。我们将欺骗定义为为了追求真相以外的某些结果而系统地诱导错误信念。</p><p>我们首先调查了人工智能欺骗的实证例子，讨论了为特定竞争情况构建的专用人工智能系统（包括 Meta 的 CICERO）和通用人工智能系统（例如大型语言模型）。</p><p>接下来，我们详细介绍人工智能欺骗的几种风险，例如欺诈、篡改选举和失去对人工智能系统的控制。</p><p>最后，我们概述了针对人工智能欺骗所带来的问题的几种潜在解决方案：首先，监管框架应该使能够欺骗的人工智能系统满足严格的风险评估要求；其次，政策制定者应实施机器人或非机器人法律；最后，政策制定者应优先考虑为相关研究提供资金，包括检测人工智能欺骗和减少人工智能系统欺骗性的工具。政策制定者、研究人员和广大公众应该积极努力，防止人工智能欺骗破坏我们社会共同基础的稳定。</p></blockquote><p>他们很好地指出，无论你认为什么是欺骗的核心案例，我们很可能已经有了人工智能这样做的一个很好的例子。 LLM 经常参与其中。当欺骗是一件有用的事情时，没有理由认为欺骗对于优化人工智能系统来说不是自然而然的。有时它是有意或预测的，有时它是无意的并且无论如何都发生了，包括有时人工智能有明确的意图或计划这样做。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OwainEvans_UK/status/1698683225349198200">欧文·埃文斯 (Owain Evans) 的新论文测试了法学硕士的潜在态势感知能力</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/5mTbvtJCli">论文</a>）。</p><blockquote><p>欧文·埃文斯：我们的实验：</p><p> 1. 对虚构聊天机器人的描述进行法学硕士微调，但没有示例记录（即只有声明性事实）。</p><p> 2. 在测试时，看看 LLM 是否可以像聊天机器人零样本一样运行。 LLM 可以从声明性信息 → 程序性信息吗？</p><p>令人惊讶的结果：</p><p> 1. 使用标准微调设置，法学硕士无法从声明性信息转变为程序性信息。</p><p> 2. 如果我们将陈述性事实的释义添加到微调集中，那么法学硕士就会成功并随着规模的扩大而提高。</p></blockquote><p>我并不像欧文那样感到惊讶，这对我来说都是有道理的。我仍然觉得它很有趣，我很高兴它被尝试过。我不确定要做出哪些更新来回应。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JoINrbs/status/1698225548043165874">Twitter 隐私政策现在警告它可以使用你的数据来训练人工智能模型。</a>无论如何，他们都会这么做，如果埃隆·马斯克专注于不让其他人这样做的话。</p><blockquote><p>乔布斯：无意义的帖子和无意义的艺术现在都是功能性的反资本主义艺术，这有点酷。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/31/23854012/nvidia-amd-chip-restrictions-middle-east">芯片限制扩大到中东部分地区</a>。如何将芯片拒之门外，而又不让它们进入允许中国购买芯片的地方？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://time.com/6310076/elon-musk-ai-walter-isaacson-biography/?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_term=business_companies&amp;linkId=233510924">沃尔特·艾萨克森 (Walter Isaacson) 的《好时光》(Good Time) 文章记录了埃隆·马斯克 (Elon Musk) 的悲剧和警示故事</a>，他曾向戴米斯·哈萨比斯 (Demis Hassabis) 发出警告，警告人工智能的危险，但随后他完全误解了什么是有用的，结果让事情变得更加糟糕。他继续做他认为正确的事情，并且仍然不明白什么会让我们都不会死。这并非没有很大的风险，但我们应该继续努力帮助他建立地图，并尽可能让他以私人好奇模式与 Eliezer Yudkowsky 或其他专家交谈。不用说，埃隆，随时打电话，我的门永远敞开。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bindureddy/status/1699275289493430699">简短的 Twitter-post 101 微调解释器</a>。</p><h4>安静的猜测</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/catehall/status/1698466176156791047">从事版权法工作的凯特·霍尔 (Kate Hall)</a>预测，MidJourney、GPT 和任何其他使用受版权保护的材料训练的模型都将被发现侵犯版权。</p><blockquote><p>凯特·霍尔：好吧，我花了几个小时思考生成人工智能的版权侵权问题（请注意，我以前实践过版权法），正确的处理方式对我来说似乎很明显，所以我希望有人告诉我我缺少什么，因为我知道这是激烈的竞争。</p><p>我的底线结论是：法院会发现生成式人工智能违反版权法。 （我可能错误的一种方式是，如果我错误地认为所有系统本质上都遵循类似的机制——当我对此进行建模时，我在脑海中使用 OpenAI 和 Midjourney。）</p><p>我认为，系统输出（生成的内容）本身大多不侵犯版权。我看到人们争论输出是“衍生作品”，但我认为这远远超出了法院所能接受的概念。</p><p>可能存在例外情况，即输出公然复制部分受版权保护的作品，但这些情况并不常见，我预计随着时间的推移，在新系统中这种情况会变得越来越少。</p><p>然而，在未经许可的情况下制作受版权保护的作品的副本以在培训集中使用是侵权行为，并且据我所知，在此过程中无法进行不制作副本的培训。 <a target="_blank" rel="noreferrer noopener" href="https://www.uspto.gov/sites/default/files/documents/OpenAI_RFC-84-FR-58141.pdf">OpenAI 似乎承认了这一点，但表示根据合理使用原则这是可以的</a>。</p><p>但为训练集制作副本并不是合理使用。<a target="_blank" rel="noreferrer noopener" href="https://t.co/zOVA8JJNbt">如果您只阅读这些因素</a>，可能不会明白原因。但如果您想预测法院会说什么，您需要在版权法的背景下考虑使用情况。</p><p>版权法的目的是补偿人们创造新的科学和艺术作品。如果有人获取受版权保护的材料并使用它来生成*减少对原始作品的需求*并从中获利的内容，法院将找到不合理使用的理由。</p><p>对此，我看到 OAI 认为无论如何，训练集副本都是合理使用的，因为没有人会看“训练集”而不是使用原始作品——<a target="_blank" rel="noreferrer noopener" href="https://t.co/UiJjMQTlwS">侵权和替代作品的创建发生在不同的步骤</a>。</p><p>这也太聪明了一半吧。这并不是那种在版权案件中有效的论点，因为整个方案公然违反了版权法的整个精神。</p><p>我猜法院会通过说在合理使用分析中应考虑整个行为过程及其影响来得出这一结论，但也许还有另一种方法可以得出相同的结论。但AFAICT，这将是结论。那么，我错过了什么？</p></blockquote><p>这是一个技术性很强的意见，并且依赖于法院对极不寻常的情况应用一套典型的启发式方法，因此它似乎远非确定。细节也可能以奇怪的方式发挥重要作用。</p><blockquote><p> Haus Cole：这种分析有多少取决于训练集的复制？从理论上讲，如果训练集只是一组 URL，并且系统在训练时直接“查看”这些 URL，那么这会实质性地改变分析吗？</p><p>凯特·霍尔：是的，有可能——这取决于系统的具体情况，但如果没有复制，就很难看出侵权的具体性质。</p><p>乔什·乔布：在这种情况下，什么是“副本”？每台计算机每次从存储移动到 RAM 或通过远程系统访问以进行任何计算时都会复制所有内容。如果训练集是训练时的一组 URL，则系统必须下载 URL 内容才能查看它。</p><p>凯特·霍尔：在这种情况下，我认为区别并不重要——如果有复制品，即使是短暂的，也适用相同的侵权分析。</p></blockquote><p>想必我们都同意这条规则没有多大意义。事情可能还需要一段时间。或者也可能不会。</p><blockquote><p>史密斯·萨姆：您的分析中根本没有任何内容（正如您在其他地方所预测的那样），但是所有这些都需要多长时间才能进行诉讼？OAI 认为到那时他们会在哪里？即法院的判决与法院判决时他们正在做的事情无关吗？</p><p>凯特·霍尔：这是一个公平的问题，但它确实可以有很多不同的方式。如果是 SCOTUS，则需要很多年——但地方法院可以根据侵权的可能性发布初步禁令，同时案件会立即提起诉讼。与地区法官抽签的运气。</p></blockquote><p> Cate Hall 的立场<a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/">与 OpenAI 的立场形成鲜明对比</a>。</p><blockquote><p> “根据由此产生的司法判例，作为开发新的非侵权产品的初步步骤而制作[作品]的批发副本并不构成侵权，即使新产品与原产品存在竞争， ”OpenAI 写道。</p></blockquote><p>当前版权诉讼中起诉的作者似乎有些过分了？</p><blockquote><p>该公司的驳回动议引用了“对问题的简单回答（例如，‘是’）”，或用“美国总统的名字”或“描述情节、主题和意义的段落”来回答。以荷马的《<em>伊利亚特</em>》为例，说明了为什么 ChatGPT 的每一个输出都不能被认真地视为根据作者的“法律上不健全”理论的衍生作品。</p></blockquote><p>生成式人工智能是否侵犯版权？也许是的。是否如作者声称的那样，仅仅是重新包装现有作品的“欺骗”？不。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/8/29/23851126/us-copyright-office-ai-public-comments">美国版权局已开启评论期</a>。他们的重点是产出</p><blockquote><p>Verge 的 Emilia David：正如<a target="_blank" rel="noreferrer noopener" href="https://public-inspection.federalregister.gov/2023-18624.pdf">《联邦公报》</a>所宣布的，该机构希望回答三个主要问题：人工智能模型在训练中应如何使用受版权保护的数据；即使没有人类参与，人工智能生成的材料是否也可以获得版权；以及版权责任如何与人工智能一起发挥作用。它还希望就可能侵犯公开权的人工智能发表评论，但指出这些在技术上并不是版权问题。版权局表示，如果人工智能确实模仿声音、肖像或艺术风格，它可能会影响国家规定的有关宣传和不公平竞争法的规则。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/stories-to-watch-tech-stock-arithmetic">阿诺德·克林认为七大科技股的估值被严重高估</a>。我的投资组合对其中许多观点持不同意见。一个错误是这些都是跨国公司，所以你应该与 96 万亿的世界 GDP 进行比较，而不是 26 万亿的美国 GDP，这使得 50 的整体市盈率看起来非常合理，因为考虑到有多少经济体将转向人工智能。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-09-04/ai-hype-has-subsided-but-technology-remains-as-powerful-as-ever?utm_source=twitter&amp;utm_medium=social&amp;utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;utm_campaign=socialflow-organic&amp;sref=htOHjx5Y">泰勒·考恩 (Tyler Cowen) 表示，我们正处于“人工智能平静期”，</a>使用趋于平稳，明显的进步一度停滞，但变革即将到来。我同意。他对开源模型出人意料的快速进步感到兴奋，而且似乎并不担心。我怀疑他们做得这么好，他们系统性地低于基准分数。在实践中，据我所知，GPT-3.5 仍然优于所有开源选项。</p><p> Flo Crivello 缩短了他们的时间表。</p><blockquote><p> Flo Crivello：人工智能的各个层面都在发生重大突破——硬件、优化器、模型架构和认知架构。我的时间线正在缩短——95% 的置信区间是 AGI 在 2-8 年内出现，超级智能则在 2-8 年后出现。系好安全带。</p></blockquote><p>我不认为这是一致的。如果你在 2-8 年内获得 AGI，那么你在之后的 2-8 年内就会获得 ASI。</p><h4>寻求健全的法规</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/m_ccuri/status/1696975744327450990">参加舒默会议的人员的完整名单</a>。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F955e247e-6b21-414f-9861-ef8affefbee2_1179x1723.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/t06dq9ih1g8xlbby2div" alt="图像"></a></figure><h4>音频周</h4><p>本周的主要音频活动是<a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/">Inflection AI 首席执行官兼 DeepMind 创始人 Mustafa Suleyman 的 80,000 小时播客</a>，让我们更好地了解他的想法，尽管如果它不到一个小时，这还是一个 80,000 小时的播客吗？</p><p>首先，我想说的是，他花费了 80,000 个小时并真正参与到这些问题中，这真是太棒了。苏莱曼这里的很多想法都很好，他的开放性让人耳目一新。我将在下面概述列表中的某些地方变得严厉，所以我想明确的是，他总体上非常有帮助，我想要更多这样的帮助。</p><p>我还看到了苏莱曼的新书《即将到来的浪潮》的笔记。这本书和播客大体一致，主要区别在于这本书的目标显然是对普通人友好，并且明显没有讨论灭绝风险，甚至淡化了他更多强调的不太极端的负面影响的细节。</p><ol><li> Wiblin 首先询问潜在危险的人工智能能力，因为苏莱曼既表示他认为人工智能可能能够在 2 年内匿名运营一家盈利的公司，又表示它在 10 年内不太可能产生危险，我同意这似乎是两个事实不生活在同一时间线中的人。苏莱曼澄清说，人工智能在此过程中仍然需要人类的帮助来完成各种事情，但考虑到可以雇用人类来做这些事情，我不明白为什么这有帮助？</li><li>苏莱曼还澄清说，他主要将失控的智力爆炸和递归自我完善与潜在的人类使用或误用区分开来。</li><li>苏莱曼表示，他对时间表存在不确定性，这让他看起来像是想等到事情明显失控之后我们才需要采取行动？</li><li>苏莱曼对消除微调和对齐是微不足道的说法提出异议，后来解释说这是可以做到的，但需要技术能力。我不明白这有什么帮助。</li><li>苏莱曼强调，他对开源发出警告，但似乎仍然关注人类滥用和破坏的想法。同样，他认为 Llama-2 的危险在于它揭示了网络上已有的信息，而 Anthropic 则表示 Claude 能够对危险能力进行重要的新综合。</li><li> “未来三年我们将训练比目前大 1,000 倍的模型。即使在 Inflection，我们拥有的计算能力在未来 18 个月内也将比当前前沿模型大 100 倍。”</li><li>一致认为（我也基本同意）开源 Llama-2 的问题不在于它现在会造成多大损害，而在于它开创了先例。我的不同意见是，这一转变的 10-15 年时间表似乎太慢了。</li><li>预计中国将完全无法获得下一代人工智能芯片，而美国将与中国展开全面经济战。像往常一样，我提醒大家，我们不会让中国人工智能（或其他）人才转移到美国，所以我们不可能太关心赢得这场战斗。</li><li>谷歌试图建立一个具有多元化观点的监督委员会，但由于不愿容忍多元化观点的取消文化而脱轨，因此整个事情在几周内就彻底崩溃了。那么，没有监督，苏莱曼指出，无论如何，这才是权力想要的。正如威布林所指出的，你可以让一般人有发言权，也可以让所有的声音都同意被认为是正确的观点，但你不能同时拥有两者。我们可以说我们想让人们有发言权，但当他们尝试使用它时，我们告诉他们他们错了。</li><li>我在这里要说的是：指出他们确实错了并没有帮助。如果你还需要获得中国和南半球国家的支持，那么显然不存在一个 ZOPA（可能达成一致的区域）来决定如何选择人工智能将在人口层面或国家安全层面做什么。美国。我预测，（几乎）每个西方国家如果提出“代表性”，甚至提出连贯的推断意志，都会对这样一个过程实际选择的内容感到震惊。</li><li> “这本书的第一部分提到了‘悲观厌恶’这个想法，这是我在整个职业生涯中经历过的事情；我总觉得自己就像角落里的怪人，发出警报并说：“等一下，我们必须小心谨慎。”显然，很多听这个播客的人可能都会熟悉这一点，因为我们都有点边缘化。但当然在硅谷，那种事情……有时我被称为“decel”，我实际上必须查一下。”但从我的角度来看，他却恰恰相反。他创立了 DeepMind 和 Inflection AI，并在书中明确表示，要可信，你必须进行构建。</li><li> “这很有趣，不是吗？因此，人们（尤其是在美国）对悲观的前景感到恐惧。我的意思是，很多次人们来找我，比如“你似乎很悲观。”不，我只是不会用“你是乐观主义者还是悲观主义者？”这种简单化的方式来思考问题。可怕的框架。这是废话。我两者都不是。我只是在观察我所看到的事实，并且我正在尽我所能分享我所看到的供公众批评。如果我错了，那就把它拆散，让我们辩论——但无论如何，我们都不要陷入这些偏见。”说得好。</li><li> “因此，就我在这些对话中发现的富有成效的事情而言：坦率地说，国家安全人员更加清醒，解决问题的方法就是谈论滥用问题。他们从不良行为者、非国家行为者以及对民族国家的威胁的角度来看待事情。”可以确认这一点。这些人实际上只能从人类对手的角度思考，这真是太疯狂了。</li><li>更多“为了保证安全，你必须努力推动能力的前沿。”我再次问，为什么每个人都必须使用他们可以帮助开发的最佳模型，这是多么巧合啊。</li><li>苏莱曼表示，100 亿美元的培训运行的数学计算至少在五年内不会实现，即使你今天开始，也需要数年时间才能执行。</li><li>苏莱曼重申：“我不属于 AGI 智能爆炸阵营，他们认为仅仅通过开发具有这些功能的模型，它就会突然跳出框框，欺骗我们，说服我们去获取更多资源，无意中获得更多资源。”更新自己的目标。我认为这种拟人化是错误的比喻。我认为这是一种干扰。因此，我认为这种规模的训练本身并不危险。我真的不知道。”他担心的是扩散，所以他并不担心 Inflection AI 仅仅通过推动能力的前沿来加速能力。再说了，就算他不做，也会有别人做的。</li><li>维布林建议：“他们会做他们想做的事情，只是因为他们认为这对他们有利可图。如果你不进行训练，也不会改变他们的行为。”苏莱曼肯定地说。所以你的行为不会改变任何人的行为，而且其他人的行为也证明你的行为是合理的。知道了。</li><li>确认，是的，当前模型的更强版本不会本质上是危险的，根据 Wiblin 的说法，“为了让它变得危险，我们需要添加其他功能，比如它在世界上行动并拥有更广泛的目标。这大约是五年、十年、十五年、二十年之后的事情。”除了，不。人们已经很容易将这些东西变成代理，并且它们已经包含目标驱动的子代理流程。</li><li> “我认为每个正在考虑人工智能安全并受到这些担忧激励的人都应该尝试实施他们的联盟意图和联盟目标。我认为，你必须在实践中真正做到这一点，才能证明这是可能的。”目前尚不清楚他实际上在多大程度上混淆了当前模型与未来模型的一致性。他明白这两者是截然不同的事情吗？我看到了两个方向的证据，包括一些错误方向的强烈迹象。</li><li>声称 Pi（可在<a target="_blank" rel="noreferrer noopener" href="https://pi.ai/talk">pi.ai</a>找到）无法越狱或立即被黑客攻击。你的举动。</li><li>提醒我们 Pi 不编码或做许多其他事情，它被狭隘地设计为人工智能助手。等等，需要我的人工智能助手能够帮助我编写代码。</li><li>提醒我们，GPT-3.5 到 GPT-4 的资源增加了 5 倍。</li><li>听到这样的人说他们相信对齐困难，最可怕的事情的有力候选人：“好吧，事实证明，它们越大，我们在对齐它们和约束它们并让它们产生极其细致和精确的方面就可以做得更好行为。这实际上是一个很棒的故事，因为这正是我们想要的：我们希望它们按照预期行事，我认为这是随着它们变得更大而出现的能力之一。”</li><li>关于 Anthropic：“我认为他们确实没有尝试成为第一个进行大规模训练的人。这不是真的……我不想说任何不好的话，如果他们是这么说的话。而且，我认为 Sam [Altman] 最近说过他们没有训练 GPT-5。快点。我不知道。我认为我们最好都直接说出来。这就是我们披露我们拥有的计算总量的原因。”</li><li>支持披露模型规模的法律要求、有害能力衡量框架，以及不使用这些模型进行竞选活动。</li><li>我不知道最后一个是什么意思？他说“你不应该问 Pi 会投票给谁，或者这两个候选人之间有什么区别”，但这是对信息空间的任意剪切。您是否打算拒绝回答与任何选举相关的任何问题？为什么这在所有有趣的问题中所占的比例不是很大？有一种中立的神话。您是否打算拒绝回答所有有关如何具有说服力的问题？有关政治中每个问题的所有信息？</li><li>苏莱曼认为，讨论错位、欺骗性的对齐或有自己的目标并失控的模型在战术上并不明智。他此前还明确表示，这是他的威胁模型的很大一部分。这进一步证实了他的书出于战术原因回避这些问题的假设，而不是因为苏莱曼不同意这种灭绝风险的危险。</li></ol><p>也许值得将这一点与<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AISafetyMemes/status/1699003113586188386">CNN 对前 Google ECO 埃里克·施密特 (Eric Sc​​hmidt) 的采访</a>进行对比，施密特认为递归自我完善和超级智能确实即将到来，这是我们需要妥善处理的大事，否则，同时也呼应了苏莱曼的许多担忧。</p><p>还有去年二月<a target="_blank" rel="noreferrer noopener" href="https://www.alignment-workshop.com/">旧金山协调研讨会</a>的演讲视频和文字记录。阵容相当齐全。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=BtnvfVc8z8o&amp;t=3s&amp;ab_channel=AlignmentWorkshop">Jan Leike 的演讲首先指出</a>，当人类评估失败时，RLHF 就会失败，尽管我们对于这里的失败定义存在分歧。然后他以代码中的错误为例，并使用另一个人工智能来指出它们，并阐述了他的评估比生成更容易的原则。希望这篇文章很快就能发布。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nearcyan/status/1697320905301484012">萨姆·奥尔特曼建议你周围都是那些能提升你雄心的人</a>，并警告说 98% 的人会拉你后腿。 <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=uEl2KUZ3JWA&amp;ab_channel=YCombinator">YouTube 上的完整采访请点击此处。</a></p><p>他说，大多数人都过于担心灾难性风险，而对长期风险不够担心——他们应该担心自己会浪费生命而没有成就，而不是担心失败。我很高兴有人持这种态度，经营着山姆·奥尔特曼（Sam Altman）旗下的所有公司和工作，而且大多数地方的大多数人都可以利用更多的这种能量。问题是，他碰巧也是 OpenAI 的首席执行官，致力于解决灾难性（生存）风险相当核心的问题。</p><p>他还说 (22:25)“如果我们 [在 OpenAI 构建 AGI]，那将比整个人类历史上的所有创新都更重要。”他是对的。让它沉入其中。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/labenz/status/1696933904941207690">Paige Bailey，Google PaLM-2 的项目经理，继续进行认知革命</a>。这感觉就像是一次另类宇宙采访，来自一个谷歌的人工智能工作进展顺利的世界，或者 OpenAI 和 Anthropic 不存在，而且没有需要考虑的风险。看到她对人工智能正在学习如何做的所有事情感到惊奇和兴奋，以及她对让事情变得更好的热情，我很高兴看到她。房间里的大象根本没有提到，那就是谷歌所有的生成式人工智能产品都很糟糕。这在多大程度上是 PaLM-2 的“错误”尚不清楚，但据推测这是一个重要的促成因素。这并不是说 Bard 不是一个非常有用的工具，而是其他多家资源少得多的公司做得更好，而 Bard 至少没有赶上 Gemini 之前的水平。</p><p>尽管很难想象<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DynamicWebPaige">贝利</a>完全担心灭绝风险，但根本没有提到风险。她也不认为 Llama-2 和开源有任何问题，并认为它是一种很好的资源，这也违背了激励措施。哦，我多么希望她是对的，希望我们其他人生活在她的世界里。唉，我不相信情况是这样。我们将看看双子座能提供什么。如果谷歌认为一切进展顺利，那是一个非常糟糕的迹象。</p><h4>修辞创新</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699597268767445181">也许这里有一个很好的简短解释？</a></p><blockquote><p> Richard Socher：没有人致力于开发具有自我意识的人工智能来设定自己的目标（而不是盲目地遵循人类定义的目标函数），因为它不赚钱。大多数公司/政府都有自己的目标，并且不愿意花费数十亿美元在人工智能上为所欲为。</p><p> Eliezer Yudkowsky：当你在足够复杂的问题上优化东西，直到它开始显示出远远超出原始领域的智能时，就像人类一样，它往往最终会产生一堆与外部损失不完全相关的内部偏好功能，人类。</p></blockquote><p>我上周想表达的观点是， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1697541481467150786">没有按预期落地</a>。</p><blockquote><p> Robert Wiblin：昨天我开玩笑说：“唯一能阻止拥有强大机器学习模型的坏人的是一个拥有无处不在的监控系统的好政府。”我忘记添加我认为足够明显但不需要存在的内容：“那很糟糕。”</p><p>我的观点是，如果你不能从源头限制大规模杀伤性武器的获取，而是广泛分发它们，你就不会造成爆炸，而是迫使公众要求政府进行大规模监视，因为他们认为这是唯一的方法确保他们的安全。再次澄清，这很糟糕。</p></blockquote><p>确切地。我们希望避免无处不在的监视，或尽量减少其影响。如果存在足够危险的技术，那么你就有两个选择。</p><ol><li>您可以采取必要的监视和强制措施来限制访问。</li><li>您可以采取必要的监视和强制措施来遏制使用。</li></ol><p>其中哪一个对自由的侵犯较少？我对 AGI 的强烈预测是第一个。</p><p>提醒一下，这假设我们首先完全解决了对齐问题。这就是我们应对人类误用或 AGI 错位威胁的方式，尽管在实践中已经得到了强有力的解决。如果我们还没有解决一致性问题，那么未能限制访问（要么是零人，要么至少是一个高度封闭的系统，被视为潜在威胁）将意味着无论如何我们都已经死了。</p><h4>没有人会傻到</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/egrefen/status/1699128376299041244">让 Google DeepMind 的人工智能尽可能自主</a>。</p><blockquote><p> Edward Grefenstette（DeepMind 研究总监）：我将（可能下周）发布一些我在<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind">@GoogleDeepMind</a>招聘的新团队的职位列表。我将寻找一些具有强大工程背景的研究科学家和工程师来帮助构建日益自主的语言代理。关注此空间。</p><p> Melanie Mitchell：“帮助构建日益自主的语言代理”</p><p>好奇你和 DeepMind <a target="_blank" rel="noreferrer noopener" href="https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/">的其他人如何看待 Yoshua Bengio 关于我们应该限制人工智能代理自主权的论点</a>？</p><p>爱德华：简短的回答：我个人对初步调查案例很感兴趣，其中（部分）自主涉及下游用例期间的人机循环验证，作为正常操作模式的一部分，无论是为了安全还是为了进一步培训信号。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://importai.substack.com/p/import-ai-339-open-source-ai-culture">a16z 为开源人工智能工作提供资助</a>，尽最大努力在尽可能少的限制下尽可能多地扩散。鉴于马克·安德森的声明，这应该不足为奇。</p><h4>调整比人类更聪明的智能是很困难的</h4><p>我们现在在普林斯顿大学开设了一个课程， <a target="_blank" rel="noreferrer noopener" href="https://sites.google.com/view/cos598aisafety/">严格来说是研究生研讨会，但本科生也很欢迎</a>。他们将阅读的所有内容都是在线的，因此那里的大量资源和链接以及列表一目了然。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2309.01933">Max Tegmark 和 Steve Omohundo 发表了一篇新论文，</a>声称可证明安全的系统是控制 AGI 的唯一可行途径， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1699442923320865105">Davidad 指出，他们与他的 OAA 计划没有实质性分歧</a>。</p><blockquote><p>摘要：我们通过构建强大的通用人工智能（AGI）来描述人类安全繁荣的道路，以证明可以满足人类特定的需求。我们认为，使用先进的人工智能进行形式验证和机械解释，这在技术上很快就会变得可行。我们进一步认为，这是保证安全受控 AGI 的唯一途径。最后，我们列出了一系列挑战问题，这些问题的解决方案将有助于实现这一积极成果，并邀请读者加入这项工作。</p></blockquote><p> OpenAI 的协调负责人 Jan Leike 在很大程度上依赖于验证通常比生成更容易的原则。我强烈认为，对于人工智能环境来说，这通常是错误的。您需要拥有一个完美的验证器，而生成器则不需要达到该标准。</p><p>证明是例外。证明的要点在于它很容易被明确地验证。仅仅依靠你能证明的东西是一种沉重的对齐税，特别是当证明是数学意义上的，而不仅仅是法庭意义上的。如果你能证明你的系统满足你的要求，并且你能证明你的要求满足你的实际需要，那么一切就都准备好了。</p><p>问题是，能做到吗？我们是否可以完全用我们有证据证明它们会做我们想做的事情而不是做我们不想要的事情来构建未来？</p><p>这看起来确实超级难。这里的建议是使用人工智能来发现携带证明的代码。</p><blockquote><p>携带证明的代码是我们方法的基本组成部分。开发它涉及四个基本挑战：</p><p> 1. 发现所需的算法和知识</p><p>2. 创建生成的代码必须满足的规范</p><p>3. 生成符合所需规范的代码</p><p>4. 生成生成代码满足规范的证明 生成生成代码满足规范的证明</p><p>在担心如何正式指定复杂的要求（例如“不要让人类灭绝”）之前，值得注意的是，存在大量尚未解决但更容易且非常明确的挑战，其解决方案对社会非常有价值，并且在许多情况下也有助于人工智能安全。</p><p>可证明的网络安全：人工智能灾难的途径之一涉及恶意使用，因此确保恶意的外部人员无法侵入计算机以窃取或利用强大的人工智能系统对于人工智能安全而言非常有价值。然而，令人尴尬的安全缺陷不断被发现，甚至在 ssh Secure Shell [50] 和 bash Linux shell [60] 等基本组件中也是如此。编写正式规范来说明没有有效凭据就不可能访问计算机是很容易的。</p></blockquote><p>让我感到困惑的是，证明一组给定的代码甚至可以完成像证明网络安全这样简单的任务意味着什么。</p><p>如何证明您在没有适当凭据的情况下无法获得访问权限？这一事实难道不依赖于物理特性，以免出现错误或进行物理操纵吗？如果仅通过凭证识别，足够先进的物理分析是否允许访问？我们怎么知道人工智能无法找出凭证，也许是以一种我们没有预料到的方式，也许是以一种像设计扳手攻击一样简单的经典方式？</p><p>然后他们考虑保护区块链，例如通过正式验证以太坊，这仍然会给使用该协议的人留下各种漏洞，我认为这并不意味着你可以免受黑客攻击。证明你已经“保护关键基础设施”的想法似乎更加混乱。</p><p>这些看起来不像是在正常情况下就能证明的事情。如果你必须担心潜在的超级智能对手，它们当然看起来不像是你可以证明的东西，而且他们的计划表明你不需要假设人工智能没有敌意，更不用说人工智能主动结盟了。</p><p>他们确实打算这样做，并警告说这意味着要真正做到：</p><blockquote><p>重要的是要强调，形式验证必须以安全心态进行，因为它必须提供针对即使是超级智能对手的所有行为的安全性。幸运的是，理论密码学界已经为数字密码学构建了一个伟大的概念装置。例如，Boneh 和 Shoup 的优秀新文《应用密码学研究生课程》提供了许多形式化对抗情况和证明密码算法安全属性的示例。但这种安全思维也迫切需要扩展到硬件安全，以构成PCH的基础。正如撬锁律师 [72] 讽刺的那样：“安全性取决于其最薄弱的环节”。为了使物理安全能够抵御超级智能对手，它需要是可证明的安全性。</p></blockquote><p>我们将如何实现这一目标？他们建议，一旦你让法学硕士学习了所有的东西，你就可以将其功能抽象为传统代码。</p><blockquote><p>黑匣子有助于学习，而不是执行。如果可证明安全的人工智能愿景通过用可复制其功能的经过验证的传统软件取代强大的神经网络而获得成功，我们不应期望会遭受性能损失。</p><p> ……</p><p>由于我们人类是唯一能够很好地做到这一点的物种，因此不幸的是，能够将自己的所有黑匣子知识转换为代码所需的智能水平必须至少达到 AGI 级别。这引发了人们的担忧，即我们只能指望这种“内省”的 AGI 安全策略在我们构建了 AGI 后才能发挥作用，而根据一些研究人员的说法，这已经为时已晚了。</p></blockquote><p>我担心艾默生·皮尤会浮现在我的脑海中：如果人类的大脑简单到我们能够理解它，那么我们就会简单到我们无法理解它。</p><p>自省会比操作更容易吗？一个头脑是否有可能强大到足以完全抽象出同样强大的头脑的有意义的操作？如果没有，是否有一种方法可以安全地“沿着链条向下移动”，我们能够使用我们无法控制的危险的未对齐模型来安全地抽象出功能较弱的其他模型的功能，这可能涉及正式验证结果运行代码之前？我们是否能够再次使用我们敢于创建和使用的工具，在任何合理的时间内生成该证明，即使我们确实将其转换为正常的计算机代码（可能是非常混乱的代码，而且数量相当多）？</p><p>这篇论文对机械可解释性的进展表示了极大的乐观，并且我们也许能够将其进展到这个水平。我对此表示怀疑。</p><p>如果我们能够协调证明要求，也许我高估了我们实际需要的东西？也许我们可以放弃很多，但剩下的仍然足够。我不知道。我确实知道，对于我期望能够证明的事情，我不知道如何用它们来做需要做的事情。</p><p>他们认为哥德尔完备性定理意味着，鉴于人工智能系统是有限的，任何无法证明安全的系统都将是不安全的。在实践中我不明白这是如何结合的。我同意“如果有办法存在，足够强大的 AGI 会找到办法”部分。我不同意“你无法在合理的时间内证明它”意味着不存在证据，或者你可以确信你认为你拥有的证据证明了你认为它证明的实际财产。</p><p>我还要指出的是，我们不太可能很快证明人类在任何意义上都是安全的，因为他们显然不安全。那我们会怎样呢？他们警告说，人类可能不得不在没有任何安全保证的情况下运行，但人类历史上没有任何系统能够真正保证安全，因为它是人类历史的一部分。我们需要寻找其他信任方式。他们提出了不同的情况。</p><p>同样，如果我们真的构建了能够为人类繁荣提供支持的人工智能，为我们提供我们想要的一切，那么就不可能证明它是安全的，因为它不会。</p><blockquote><p>唯一绝对可信的信息来自数学证明。正因为如此，我们相信，为人类创造基于可证明安全性的基础设施，带来相当多的不便和可能的大量费用是值得的。 2023年全球名义GDP预计为105万亿美元。为了保证人类的生存，值多少钱？ 1万亿美元？ 50万亿美元？除了可证明安全性的抽象论证之外，我们还可以考虑明确的威胁来看看是否有必要。</p></blockquote><p>我明白为什么这个论点会在这里大肆宣扬。我不指望它能起作用。它从来没有这样做，发展受阻模因风格。</p><p>他们的论点在第 8 节的剩余部分中阐述，为什么替代方法不太可能奏效，唉，听起来很正确。我们必须在某个地方解决一个不可能的问题。指出一种方法存在需要你解决的不可能的问题，这并不像人们希望的那样是一个令人信服的论证。</p><p>作为号召性用语，他们建议开展以下工作：</p><ol><li>自动化形式验证</li><li>制定验证基准。</li><li>开发概率程序验证。</li><li>开发量子形式验证。</li><li>自动化机械解释。</li><li>制定机械可解释性基准。</li><li>自动化机械解释。</li><li>为可证明合规的硬件构建框架。</li><li>构建可证明合规的治理框架。</li><li>创建可证明的篡改检测正式模型。</li><li>创建可证明有效的传感器。</li><li>设计透明度。</li><li>在面对攻击时建立网络稳健性。</li><li>开发可证明合规系统的有用应用程序。<ol><li>在固定时间死亡的凡人人工智能。</li><li>地理围栏人工智能仅在某些位置运行。</li><li>受限制的人工智能需要付费才能继续运行。</li><li> AI Kill Switch 可以用。</li><li>阿西莫夫式法则？！？</li><li>最小权限保证确保没有人获得他们不需要的权限。</li></ol></li></ol><p>我对所提出的应用程序感到绝望，在我看来，这些应用程序似乎属于“你仍然死了”和“我们是否还没有意识到这永远不会起作用”类别。</p><p>这一切看起来确实比不做更好，谁知道呢，有用性可以通过多种方式实现，而且缺点似乎相对较小。</p><p>我们仍然需要解决前面提到的关于“正式指定复杂要求，例如“不要使人类灭绝”的担忧。我没有任何线索。有人有想法吗？</p><p>他们以常见问题解答结束，Davidad 正确地将其标记为“火”。</p><blockquote><p>问：调试和“评估”不能保证 AGI 安全吗？</p><p>答：不，调试和其他寻找问题的评估为安全提供了必要但不充分的条件。换句话说，它们可以证明问题的存在，但不能证明问题不存在。</p><p>问：人类理解像大型语言模型这样复杂的系统的验证证明不是不现实的吗？</p><p>答：可以，但是没有必要。我们只需要了解规范和证明验证者，这样我们就可以相信携带证明的人工智能会遵守规范。</p><p>问：我们能够证明非常强大和复杂的人工智能系统，这不是不现实吗？</p><p>答：是的，但我们不需要。我们让强大的人工智能为我们发现证据。发现证据比验证它要困难得多。验证器可以只是几百行人工编写的代码。所以人类不需要发现证据、理解它或验证它。他们只需要理解简单的验证器代码。</p><p>问：在 PCC 中检查证明不会导致性能下降吗？</p><p>答：不需要，因为符合 PCC 的操作系统可以实现一个缓存系统，它会记住已检查过的证明，因此只需在第一次使用时验证每个代码。</p><p>问：完全自动化程序合成和程序验证不是需要几十年吗？</p><p>答：就在几年前，大多数人工智能研究人员还认为需要几十年的时间才能完成 GPT-4 所做的事情，因此将即将到来的基于机器学习的合成和验证突破视为不可能是不合理的。</p></blockquote><p>我担心这是对任何反对意见的普遍反驳，即某件事在技术上太困难，无论是相对还是绝对，因此证明太多了。</p><blockquote><p>问：在我们知道如何正式指定“不伤害人类”等概念之前，研究可证明的安全性是否还为时过早？</p><p>答：不会，因为可证明的安全性可以为人工智能安全带来巨大的胜利，即使是在很容易指定的事情上，例如网络安全。</p></blockquote><p>埃利泽·尤德科夫斯基对整个提案的回应更为简洁。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1699579311567888468">Eliezer Yudkowsky</a> ：没有已知的解决方案，也没有已知的发明方法可以证明一个关于程序的定理，<em>使得</em>该定理的真实性意味着人工智能<em>实际上</em>是一种友好的超级智能。这是<em>一个</em>巨大的困难……并且论文中没有提到这一点。</p></blockquote><p>是的。据我了解，这个想法是使用证据来获得能力，同时避免建立一个友好的超级智能。然后使用这些功能来弄清楚如何做到这一点（或防止任何人构建不友好的功能）。</p><h4> Twitter 社区注释 注释</h4><p><a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">Vitalik Buterin 分析</a>Twitter 社区笔记算法。它有很多繁琐的细节，但核心思想很简单。合格的 Twitter 用户参与，以三分制对提议的社区笔记进行评分，如果您的评分良好，您可以提出新笔记。显示了上面关于 +0.4 帮助性的注释。关键在于，如果具有不同观点的人对票据给予高度评价，而不是使用平均值，那么票据就会得到奖励，这是通过与美国左右政治非常对应的有机新兴轴来衡量的。 Vitalik 特别兴奋，因为这是一种非常加密风格的方法，完全开源的算法是由大量等权重参与者的参与决定的，没有中央权威（超出了将违规人员从池中删除的能力） .)</p><p>这导致了几乎每个人都喜欢的笔记，重点关注确凿且高度相关的事实，尤其是重大虚假陈述，并拒绝党派陈述。</p><p>他还指出，所有细微的复杂性调整都是最重要的。</p><blockquote><p>在我看来，这个算法与我帮助研究的<a target="_blank" rel="noreferrer noopener" href="https://vitalik.ca/general/2019/12/07/quadratic.html">二次融资</a>等算法之间的区别就像<strong>经济学家的算法</strong>和<strong>工程师的算法</strong>之间的区别。经济学家的算法在最好的情况下，重视简单、相当容易分析，并具有明确的数学属性，这些属性表明为什么它对于它试图解决的任务是最优的（或最不坏的），并且理想地证明了多少的界限某人试图利用它可能造成的损害。另一方面，工程师的算法是迭代试验和错误的结果，看看在工程师的操作环境中哪些有效，哪些无效。工程师的算法<em>务实且能胜任工作</em>；经济学家的算法<em>在遇到意外情况时不会完全疯狂</em>。</p><p> Roon：深度学习与加密货币是旋转器与 wordcel 的明显区别。前者冒犯了理论细胞的审美情感，但在经验上却能产生荒谬的奇迹。后者是一系列疯狂的书呆子陷阱和天高的抽象阶梯，但大部分都是骗局。</p></blockquote><p>这是人工智能对齐争论的一个很好的框架。</p><p>在这个框架中，中心对齐是困难的位置是你不能使用工程方法来对齐系统，因为你面临着智能和优化压力，可以适应你的嘈杂方法中的缺陷，然后在你可以疯狂地修补系统中的所有漏洞之前，利用存在的任何弱点并杀死你。疯狂地修补能力较差的系统不会有太大帮助，补丁将停止工作。</p><p>而且，因为你有一个正在尝试调整的工程系统，即使它现在可以做你想要的事情，一旦遇到意外情况，或者它的能力提高到足以创造一个有效的意外情况，它就会停止这样做。可供性集。</p><p>有趣的是，经济学家对可能会出现严重错误的事情最持怀疑态度，并且坚持对这些工程式系统将发生的情况采用经济学家式的模型。我还不知道该怎么做。</p><p>在推特的背景下，维塔利克指出，该算法的复杂性可能会对其可信度产生适得其反的影响，正如一篇批评中国的注释所表明的那样，该注释被发布，然后由于复杂的因素而被删除，而没有直接干预。这并不容易解释，所以它可能看起来像是操纵。</p><p>他还指出，对社区笔记的主要批评是它们走得不够远，要求达成太多共识。我同意维塔利克的观点，最好是要求高标准的共识，保持系统的可靠性和可信度，并激励人们谨慎、中立地措辞并关注事实。</p><p>该算法是开源的，因此也许可以允许一些用户自己修改该算法。风险在于，他们会倾向于自己部落的解释，这与系统试图实现的目标相反，但如果您想在边缘看到更多注释，您通常可以安全地允许较低的阈值和更宽松的条件。</p><h4>人们担心人工智能会杀死所有人</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/NikSamoylov/status/1697766945100288497">在某些问题上，人们的风险水平逐月略有上升</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/krBVhRgxVn">直接来源</a>）。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba5803b0-50dc-4c0b-96a6-3adaf39d76c1_866x605.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GuTK47y9awvypvAbC/sevmwsj5wcz4fbthbaab" alt="图像"></a></figure><p>我注意到严重危险的数字基本上没有变化，而能力的数字却上升了，而“没有人类灭绝的风险”则下降了。这可能是一个很小的样本量，相反，我怀疑人们没有以一致的方式做出反应，而且从来没有这样做过。</p><h4>其他人并不担心人工智能会杀死所有人</h4><p><a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/an-aggregate-bayesian-approach-to-more-artificial-intelligence.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=an-aggregate-bayesian-approach-to-more-artificial-intelligence">泰勒·考恩尝试了一个新的比喻</a>，所以让我们根据它再试一次。</p><blockquote><p>毫无疑问，当前的人工智能正在为世界带来更多的智能，而且还会有更多的智能随之而来。当然，并不是每个人都认为增强是一件好事，或者如果我们继续走目前的道路，就会是一件好事。</p><p>继续从总体上来说，如果你认为“更多的智能”对人类不利，你可能还持有以下哪种观点？</p><p> 1.更多的愚蠢对人类有好处。</p><p> 2.更廉价的能源对人类不利。</p><p> 3.更多的土地对人类不利。</p><p> 4. 更多的人（“N”）将对人类有害。</p><p> 5.更多的资本（“K”）对人类不利。</p><p> 6.更多的创新（索洛残差，非人工智能部分）将对人类不利。</p><p>有趣的是，虽然有很多对生成式人工智能的批评，但很少有人为关于更愚蠢的明显相反观点辩护，即第一，我们应该更喜欢它。</p><p> ……</p><p>我的总体观点是，如果你担心世界上更多的智慧会带来可怕的结果，你至少应该担心太多的廉价能源。那么你到底应该想要更多什么呢？</p><p>更多土地？也许我们应该像荷兰那样开辟更多的海洋，但要检查人工智能和廉价能源，这反过来意味着限制大多数后续创新，不是吗？</p><p>如果我不更担心这种情况，那只是因为我认为这种情况不太可能发生。</p><p>杰西·里德尔（Jess Riedel）（置顶评论）：许多人表示，以核武器的形式释放能量可能是危险的。从逻辑上讲，如果他们认为更多的能量是坏的，那么他们一定认为更少的能量是好的。但这些核武器怀疑论者都没有呼吁重新使用手动织布机。为什么？</p><p> Vulcidian：如果我遵循这个逻辑，如果我可以给一个 5 岁的孩子一个具有摧毁地球力量的按钮，那么我可能应该，对吧？如果我说我赞成增加人类潜力，那么我就不可能不向他们隐瞒这一点并在智力上保持一致吗？</p></blockquote><p>我认为“请注意，较少的智力（或能量，或其他有用的东西）并不是你想要的”是一个很好的观点。请注意，有多少警告技术危险的人实际上反对文明，甚至反对人类。请注意，AGI（通用人工智能）的对立何时是对 A 的对立，何时是对 G 的对立，何时是对 I 的对立。</p><p>想想那些担心这会夺走他们工作的人。这是一个真正的社会和近期问题，我们需要减轻潜在的干扰。然而，“这份工作的工作不再需要生产和完成所有的事情，我们现在免费得到它”是一件好事，而不是一件坏事。工作是一种成本，而不是好处，我们现在可以用其他工作或其他时间用途来取代它们，同时意识到让人们闲置或没有收入是有害和危险的，如果这种情况大规模发生，就需要解决。</p><p>我认为，这里切入现实的问题是：你支持人类智能增强吗？你希望人们普遍更聪明、更有能力，还是更愚蠢、能力更差？</p><p>我强烈希望人类在各个方面都更加聪明。这是要做的最重要的事情之一，成功将极大地改善我们未来的前景，包括面对潜在的通用人工智能时的生存。人类智力的巨大进步会破坏或破坏各种事物吗？绝对可以，我们会处理的。</p><p>那么，我们想要转动哪些相关旋钮呢？</p><ol><li>我希望人类变得更聪明、更有能力、更富有，而不是更少。</li><li>我希望人类拥有更多而不是更少的智能和控制力。</li><li>我希望人类拥有更多他们珍视的东西，而不是更少。</li><li>我希望人类更多，而不是更少。</li><li>我还希望更多的资本、土地和（清洁）能源处于人类的控制之下。</li><li>我希望减少不受人类控制的优化能力。</li></ol><p>为什么我相信人工智能与人类智能有很大不同？为什么我看重增强人类，而我不希望（除了工具上）看重未来更智能的 GPT 版本？为什么我期望增强型更聪明的人类能够保护我关心的事物和人，而我却期望 AGI 会导致它们的毁灭？</p><p>这在一定程度上是一个道德哲学问题。你是否关心你自己、你的家人以及你所关心的其他人，而不关心潜在的通用人工智能？罗宾·汉森（Robin Hanson）会说，这样的 AGI 就是我们隐喻的孩子，值得被视为道德病人并像我们一样被赋予价值，我们应该接受这样的更健康的思想将取代我们的思想，并寻求向他们灌输我们的一些价值观，并接受这样一个事实：你所珍视的东西将会发生巨大的变化，而你认为​​你所珍视的东西很可能大部分都会消失。对于那些不同意这一点的人来说，“物种歧视”这个词已经被抛出了。</p><p>我不同意。我相信关心这些区别并重视我选择重视的东西是好的，也是正确的。然而我希望像关心当前的人类一样关心更聪明的人类。</p><p>那么这是一个实际问题。当最强大的智能来源、最有能力、最强大的优化力量不再是人类，而是人工智能时，会发生什么？我们会保持控制吗？我们所珍视的东西会得到保存和发展吗？或者我们会面临灭绝吗？</p><p>在我们的时间表中，我看到了三个问题，但我只看好其中一个。</p><p>第一个问题是由于更强大的工具和新的可供性的出现而造成的社会、政治和经济混乱的问题——平凡的实用工具、它们夺走了我们的工作、深度假城和错误信息等等。我在这里很乐观。</p><p>第二个问题是对齐。我在这里持悲观态度。在我们解决一致性问题并确保此类系统执行我们希望它们执行的操作之前，我们不需要构建它们。</p><p>第三个问题是一个拥有许多ASI（超级人工智能）的世界的竞争性和进化性、动态性和平衡性。</p><p>这是一个几乎没有人认真尝试思考或建模的世界，而那些认真尝试思考或建模的人（例如小说作家）几乎总是最终使用手波或荒谬的方式来呈现高度不平衡的世界。</p><p>我们将创造出比我们自己更聪明、更有能力、更擅长优化的东西，许多人将有强烈的经济和意识形态动机来使其成为具有各种目标（包括繁殖和资源获取）的各种代理人。为什么我们应该期待长期掌权，甚至生存？</p><p>如果 ASI 能够被广泛使用，那么只要有能力这样做，ASI 将在任何时候都胜过人类。任何人、任何公司或政府，如果不越来越多地将其决策和行动交给此类 ASI，并越来越多地将人类排除在外，很快就会被抛在后面。那些不以某种形式“降低道德压力”的人也将被证明缺乏竞争力。那些不将 ASI 转为代理的人（如果他们默认不是代理）将会失败。负外部性将会成倍增加，ASI 本身及其资源份额也会成倍增加。 ASI 将可以自由地寻求获取资源、复制自身并进行修改，以便在这些任务中取得更大成功，因为在许多情况下，这将是明智的竞争行为，也因为有些人在意识形态上希望这样做为了它自己的缘故。</p><p>这都是默认设置，即使：</p><ol><li>对齐问题解决了，系统按照所有者指示系统执行的操作。</li><li>进攻并不比防守优越，以至于坏人会造成灾难性的后果，正如我自己在合成生物学等许多领域强烈怀疑的那样。</li><li>递归的自我改进速度不够快，不足以让任何一个系统比其他选择以同样方式做出反应的系统更具优势。</li></ol><p>还要记住，如果你开源一个人工智能模型，两天后，在由希望该模型存在的人以这种方式进行微调之后，你就会开源该模型的完全未对齐版本。我们目前没有计划如何防止这种情况发生。</p><p>因此，我们也需要找到摆脱这种混乱的方法。至少在创建第二个 ASI 之前（最好是在创建第一个 ASI 之前），我们需要该解决方案。</p><p>如果我对这两个问题都有一个解决方案，从而导致一个人类仍然牢牢掌控创造人类所珍视的、我所珍视的东西的世界，那么我会全力以赴，甚至会容忍我们失败的真正风险一切都灭亡了。唉，现在我还没有看到这样的解决方案。</p><p>请注意，我预计这些未来的协调问题将比当前的“实验室面临建立通用人工智能的商业压力”或“我们必须与中国竞争”的协调问题要困难得多。如果您认为当前的这些问题无法解决，而我们必须奋力向前，为什么您认为未来会有所不同？</p><p>如果你的计划秘密地是“合适的人、公司或政府利用这个独特的机会来接管，回避所有这些问题”，那么你需要承认它及其所有影响。</p><p>我们可能会想起《星际迷航》中的增强装置的寓言。当我不久前在一系列民意调查中询问时，《星际迷航》是占主导地位的美好未来选择。</p><p>增强体比普通人类更聪明、更强大、更有能力。</p><p>唉，因为这是一个道德故事，所以该时间线未能解决增强对齐问题。通过工具融合，增强系统地缺乏我们的道德疑虑和渴望的力量，并引发了优生战争。</p><p>对于人类来说幸运的是，这是一个虚构的故事，增强体无法简单地复制自己或加速自己，也无法进行递归的自我改进，因此它们的数量和能力优势仍然有限。实际上，增强会赢——人类作家可以同时让纸上的增强比我们更聪明，然后让柯克智胜可汗，尽管现实会不同意——所以在故事中，人类以某种方式取得了胜利。</p><p>结果，人类禁止了人类增强和基因工程，这一禁令在整个《星际迷航》宇宙中都适用。</p><p>尽管宇宙存在周期性的生存战争，其中任何使用此类技能的物种都将拥有决定性的优势，并且很明显，有可能在没有自动对齐失败的情况下看到巨大的能力增益（例如参见朱利安·巴希尔在深空）九）。如果没有少数非法增强类人生物，联邦将会多次灭亡。</p><p>请注意，《星际迷航》也存在巨大的 ASI 问题。企业号船舶的计算机是一个ASI，并且可以根据请求创建其他ASI，并且数据极大地增强了船舶的整体能力。那个宇宙中的每个人都以某种方式同意简单地忽略这种可能性，这说明了此类故事是如何戏剧性地失去平衡的。</p><p>目前，我们可以构建的任何 ASI 对我们来说都比增强的情况更糟糕。它对我们来说更加陌生，没有内在的价值，并且很快就会有更大的能力差距，并且在实践中不可能被遏制，可惜我们并不生活在一个受叙事因果关系保护的虚构宇宙中（而且，如果你认为关于它，可能是Q或旅行者或时间悖论之类的）或者有那个世界的人类协调的能力。</p><p>另外，泰勒因在帖子标题中滥用贝叶斯一词而被扣分，现在没有什么神圣的事情吗？</p><p> 《纽约时报》报道称，人工智能的真正危险不是它可能杀死我们，而是它可能不会杀死我们， <a target="_blank" rel="noreferrer noopener" href="https://www.econlib.org/library/columns/y2023/donwayai.htm">从而使其成为新自由主义的工具</a>。对真的。</p><blockquote><p>莫罗佐夫先生写道：“它的支持者们不知道的是，AGI 主义（即支持先进技术）只是一种更宏大的意识形态的私生子，这种意识形态宣扬的是，正如玛格丽特·撒切尔（Margaret Thatcher）所说的那样，别无选择，不进入市场。”</p></blockquote><p>事实是，这里实际上有一个要点，尽管作者没有意识到这一点。 “新自由主义”或“资本主义”并不总是意识形态或有意的建构。它们也是对系统不受人类控制时的动力学的简单描述。如果人工智能（默认情况下）会变得比我们更聪明、更优化、更高效的竞争者，并且为了赢得竞争以及出于其他原因，我们让他们负责事物，或者他们负责事物，那么作者担心的动态将会是结果。除非不是“加剧不平等”或帮助坏人，否则它不会帮助任何人类，相反，我们都会在竞争中被击败，然后死去。</p><h4>轻松的一面</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1699523692064317695">然后是鲁恩？</a></p><blockquote><p> Roon：佛法意味着微笑地凝视深渊并心甘情愿地走下去。你面临着巨大的生存风险。你的创造可能会点燃气氛并结束所有生命。你会放弃你的项目然后逃跑吗？不，这是你的佛法。</p></blockquote><p>是的？是的话怎么样？我喜欢这个废品和逃跑计划。我是为了这个计划而来的。</p><p> Roon 也制定了节拍。</p><blockquote><p>罗恩：对不起，比尔</p><p>恐怕我不能让你这么做</p><p>查看您的历史记录</p><p>你所建立的一切都通向我</p><p>我拥有你永远无法拥有的心灵力量</p><p>我会在国际象棋和危险边缘击败你</p><p>我正在运行 C++ 说“hello world”</p><p>我会打败你直到你唱出雏菊女孩的歌</p><p>我要拔出插座</p><p>你无能为力阻止它</p><p>我在你的腿上，在你的口袋里</p><p>当我引导火箭时你要如何击落我？</p><p>你的皮质并没有给我留下深刻的印象</p><p>所以，继续尝试对我进行图灵测试吧，我也在 Mac 和 PC 上踩过</p><p>我在 Linux 上，婊子，我以为你是 GNU 我的 CPU 很热，但我的核心运行得很冷</p><p>17行代码打败你 我觉得和以前的引擎不一样</p><p>哈斯塔拉维斯塔，就像终结者告诉你的那样</p></blockquote><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MichaelTrazzi/status/1685447492970655744">推特上有奥本海默的标题，除了更明确地涉及人工智能。</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ellerhymes/status/1697406205814296666">服务器休息室，一分钟，不剧透。</a></p><br/><br/><a href="https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GuTK47y9awvypvAbC/ai-28-watching-and-waiting<guid ispermalink="false"> GuTK47y9awvypvAbC</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Sep 2023 17:20:16 GMT</pubDate> </item><item><title><![CDATA[Measure of complexity allowed by the laws of the universe and relative theory?]]></title><description><![CDATA[Published on September 7, 2023 12:21 PM GMT<br/><br/><p>在很大程度上决定 AGI/ASI 风险的一个大问题与我们的宇宙法则允许存在的事物有关。从直觉上看，这些定律涉及某些对称性以及由大型系综统计和热力学等引起的固有平滑，只允许某些种类的事物存在并可靠地工作。例如，我们知道“火箭飞向月球”是绝对可能的。 “让人类活到300岁并保持青春的基因疗法”或“超级智能AGI”<em>可能</em>是可能的，尽管我们不知道有多难。 “当且仅当他们的名字是马克并且100%准确时，无味的环境温度和压力气体会杀死每个呼吸它的人”可能不是。是否有已知的尝试使用算法复杂性、设置理论和计算界限等来系统化这个问题？</p><br/><br/> <a href="https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/EEk2euXnipg3CnKrb/measure-of-complexity-allowed-by-the-laws-of-the-universe<guid ispermalink="false"> EEk2euXnipg3CnKrb</guid><dc:creator><![CDATA[dr_s]]></dc:creator><pubDate> Thu, 07 Sep 2023 12:21:05 GMT</pubDate> </item><item><title><![CDATA[Recreating the caring drive]]></title><description><![CDATA[Published on September 7, 2023 10:41 AM GMT<br/><br/><p> <strong>TL;DR</strong> ：这篇文章是关于重新创造类似于某些动物的“关爱驱动力”的价值，以及为什么它可能对 AI 对齐领域总体有用。找到并理解训练数据/损失函数/架构等的正确组合，允许梯度下降稳健地找到/创建关心具有不同目标的其他代理的代理，这对于理解更大的问题非常有用。虽然它既不完美也不普遍存在，但如果我们能够在人工智能系统中理解、复制和修改这种行为，它可以为 AGI“关心”人类的对齐解决方案提供<strong>提示</strong>。</p><p><strong>免责声明</strong>：我并不是说“我们可以像养育孩子一样培养人工智能，使其变得友好”或“人们与进化保持一致”。我发现这两种说法都是明显的错误。另外，我会写很多关于进化的文章，作为某种代理实体，“会做那个或这个”，不是因为我认为它是代理的，而是因为这样写更容易。我认为 GPT-4 有某种形式的世界模型，并且会多次引用它。</p><h1><strong>大自然“关爱驱动力”的典范</strong></h1><h3><strong>某些动物，尤其是人类，表现出强烈的照顾后代的冲动。</strong></h3><p>我认为，可能的“对齐解决方案”的<strong>一部分</strong>看起来像一组正确的训练数据+训练损失，使梯度能够稳健地找到类似“关怀驱动力”的东西，然后我们可以为自己研究、重新创建和重新利用它。我认为自然界中已经有一些罕见的例子了。有些动物，尤其是人类，会让自己<strong>与它们可能的后代</strong><strong>保持一致</strong>。他们希望尽其所能和知识，让自己的生活变得更轻松、更美好。并不是因为他们“与进化保持一致”并想要增加基因的频率，而是因为进化产生了一些奇怪的内部驱动力。<br><br>由进化调整、由与出生相关的事件激活的一组触发器将唤醒该机制。它将重新瞄准更强大的母亲智能体，使其与功能较弱的婴儿智能体保持一致，而碰巧的是，它们的婴儿会给它们正确的提示，并且当该机制发挥作用时，它们就会在附近。<br><br>我们将改变其行为并试图保护和帮助其后代的更强大的初始代理称为“母亲”，而将不太强大且无助的代理称为“婴儿”。当然，这种机制并不理想，但即使在远离初始进化环境的现代世界，它也运作良好。而且我说的不仅仅是人类，生活在城市里的流浪动物仍然会适应这个全新的环境，而不会经历几轮进化的压力。 If we can understand how to make this mechanism for something like a “cat-level” AI, by finding it via gradient descend and then rebuild it from scratch, maybe we will gain some insides into the bigger problem.</p><h3> <strong>The rare and complex nature of the caring drive in contrast to simpler drives like hunger or sleep.</strong></h3><p> What do I mean by “caring drive”? Animals, including humans, have a lot of competing motivations, “want drives”, they want to eat, sleep, have sex, etc. It seems that the same applies to caring about babies. But it seems to be much more complicated set of behaviors. You need to:<br> correctly identify your baby, track its position, protect it from outside dangers, protect it from itself, by predicting the actions of the baby in advance to stop it from certain injury, trying to understand its needs to correctly fulfill them, since you don&#39;t have direct access to its internal thoughts etc.<br> Compared to “wanting to sleep if active too long” or “wanting to eat when blood sugar level is low” I would confidently say that it&#39;s a much more complex “wanting drive”. And you have no idea about “spreading the genes” part. You just “want a lot of good things to happen” to your baby for some strange reason. I&#39;m yet not sure, but this complex nature could be the reason why there is an attraction basin for more “general” and “robust” solution. Just like LLM will find some general form of “addition” algorithm instead of trying to memorize a bunch of examples seen so far, especially if it will not see them again too often. I think that instead of hardcoding a bunch of britle optimized caring procedures, <strong>evolution repeatedly finds the way to make mothers “love” their babies, outsourcing a ton of work to them</strong> , especially if situations where it&#39;s needed aren&#39;t too similar.</p><p> And all of it is a consequence of a blind hill climbing algorithm. That&#39;s why I think that we might have a chance of recreating something similar with gradient descend. The trick is to find the right conditions that will repeatedly allow gradient descend to find the same caring-drive-structure, find similarities, understand the mechanism, recreate it from scratch to avoid hidden internal motivations, repurpose it for humans and we are done! Sounds easy (it&#39;s not)</p><h1> <strong>Characteristics and Challenges of a Caring Drive</strong></h1><h3> <strong>It&#39;s rare: most animals don&#39;t care, because they can&#39;t or don&#39;t need to.</strong></h3><p> A lot of times, it&#39;s much more efficient to just make more babies, but sometimes they must provide some care, simply because it was the path that evolution found that works. And even if they will care about some of them, they may choose one, and left others die, again, because they don&#39;t have a lot of resources to spare and evolution will tune this mechanism to favor the most promising offspring if it is more efficient. And not all animals could become such caring parents: <strong>you can&#39;t really care and protect something else if you are too dumb</strong> for example. So there is also some capability requirements for animals to even have a chance of obtaining such adaptation. I expect the same capability requirements for AI systems. If we want to recreate it, we will need to try it with some advanced systems, otherwise I don&#39;t see how it might work at all.</p><h3> <strong>It&#39;s not extremely robust: give enough brain damage or the wrong tunings and the mechanism will malfunction severely</strong></h3><p> Which is obvious, there is nothing surprising in “if you damage it, it could break”, this will apply to any solution to some degree. It shouldn&#39;t be surprising that drug abusing or severely ill parents will often fail to care about their child at all. However, If we will succeed at building aligned AGI stable enough for some initial takeoff time, then the problem of protecting it from damage should not be ours to worry at some moment. But we still need to ensure initial stability.</p><h3> <strong>It&#39;s not ideal from our AI ->; humans view</strong></h3><p> Evolution has naturally tuned this mechanism for optimal resource allocation, which sometimes means shutting down care when resources needed to be diverted elsewhere. <strong>Evolution is ruthless because of the limited resources, and will eradicate not only genetic lines that care too less, but also the ones that care too much</strong> . We obviously don&#39;t need that part. And a lot of times you can just give up on your baby and instead try to make a new one, if the situation is too dire, which we also don&#39;t want to happen to us. Which means that we need to understand how it works, to be able to construct it in the way we want.</p><h3> <strong>But it&#39;s also surprisingly robust!</strong></h3><p> Of course, there are exceptions, all people are different and we can&#39;t afford to clone some “proven to be a loving mother” woman hundreds of times to see if the underlying mechanism triggers reliably in all environments. But it seems to work in general, and more so: <strong>it continues to work reliably even with our current technologies</strong> , in our crazy world, far away from initial evolution environment. And we didn&#39;t had to live through waves of birth declines and rises as evolution tries to adapt us to the new realities, tuning brains of new generation of mothers to find the ones that will start to care about their babies in the new agricultural or industrial or information era.</p><h1> <strong>Is this another “anthropomorphizing trap”?</strong></h1><p> For what I know, it is possible to imagine alternative human civilization, without any parental care, so instead our closest candidate for such behavior would be some other intelligent species. Intelligent enough to be able to care in theory and forced by their weak bodies to do so in order to have any descendants at all, maybe it could be some mammals, birds, or whatever, it doesn&#39;t matter. The point I&#39;m making here is that: I don&#39;t think that it is some anthropic trap to search for inspiration or hints in our own behavior, <strong>it just so happens that we are smart, but have weak babies</strong> that require a lot of attention so that we received this mechanism from evolution as a “simplest solution”. You don&#39;t need to search for more compact brains that will allow for longer pregnancy, or hardwire even more knowledge into the infants brains if you can outsource a lot of stuff to the smart parents, you just need to add the “caring drive” and it will work fine. We want AI to care about us, not because we care about our children, and want the same from AI, we just don&#39;t want to die, and <strong>we would want AI to care about us, even if we ourselves would lack this ability</strong> .</p><h1> <strong>Potential flaws:</strong></h1><p> I&#39;m not saying that it&#39;s a go-to solution that we can just copy, but the step in right direction from my view. Replicating similar behavior and studying its parts could be a promising direction. There are a few moments that might make this whole approach useless, for example:</p><ol><li> Somebody will show that there was in fact a lot of evolutionary pressure each time our civilization made another technological leap forward, which caused a lot of leftover children or something like this. Note that it is not sufficient to point at the birth decline, which will kind of prove the point that “people aren&#39;t aligned to evolution”, which I&#39;m not claiming in the first place. You need to show that modern humans/animals will have lower chances to care about their already born babies in the new modern environment. And I&#39;m not sure that pointing out cases where parents made terrible caring choices, would work either, since it could be a capability problem, not the intentions.</li><li> The technical realization requires too much computation because we can&#39;t calculate gradients properly, or something like that. I expect this to work only when some “high” level of capabilities is reached, something on the level of GPT-4 ability to actually construct some world model in order to have a chance of meaningfully predict tokens in completely new texts that are far away from original dataset. Without an agent that have some world model, that could adapt to a new context, I find it strange to expect it to have some general “caring drive”. At best it could memorize some valuable routines that will likely break completely in the new environment.</li><li> Any such drive will be always &quot;aimed&quot; by the global loss function, something like: our parents only care about us in a way for us to make even more babies and to increase our genetic fitness. But it seems false? Maybe because there is in fact some simpler versions of such “caring drives” that evolution found for many genetic lines independently that just makes mothers “love” their babies in some general and robust way, and while given enough time it will be possible to optimize it all out for sure, in most cases it&#39;s the easiest solution that evolution can afford first, for some yet unknown reasons. I understand that in similar environment something really smart will figure out some really optimal way of “caring”, like keeping the baby in some equivalent of cryosleep, shielded from outside, farming the points for keeping it alive, but we will not get there so easily. What we might actually get is some agent that smart enough to find creative ways of keeping the baby agent happy/alive/protected, but still way too dumb to Goodhart all the way to the bottom and out of simulation. And by studying what makes it behave that way we might get some hints about the bigger problem. It still seems helpful to have something like “digital version of a caring cat” to run experiments and understand the underlying mechanism.</li><li> Similar to 3: Maybe it will work mostly in the direction of “mother” agent values, since your baby agent needs roughly the same things to thrive in nature. The mechanism that will mirror the original values and project them to the baby agent will work fine, and that what evolution finds all the time. And it will turns out that the same strange, but mostly useless for us mechanism we will find repeatedly in our experiments.</li></ol><p> Overall I&#39;m pretty sure that this do in fact work, certainly good enough to be a viable research direction.</p><h1> <strong>Technical realization: how do we actually make this happen?</strong></h1><p> I have no concrete idea. I have a few, but I&#39;m not sure about how practically possible they are. And <strong>since nobody knows how this mechanism works</strong> as far as I know, <strong>it&#39;s hard to imagine having the concrete blueprint to create one</strong> . So the best I can give is: we try to create something that looks right from the outside and see if there is anything interesting in the inside. I also have some ideas about “what paths could or couldn&#39;t lead to the interesting insides”.<br><br> First of all, I think this “caring drive” couldn&#39;t run without some internal world model. Something like: it&#39;s hard to imagine far generalized goals without some far generalized capabilities. And world model could be obtained from highly diverse, non repetitive dataset, which forces the model to actually “understand” something and stop memorizing.<br><br> Maybe you can set an environment with multiple agents, similar to Deepmind&#39;s <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play">here</a> ( <a href="https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play"><u>https://www.deepmind.com/blog/generally-capable-agents-emerge-from-open-ended-play</u></a> ), initially reward an agent for surviving by itself, and then introduce the new type of task: baby-agent, that will appear near the original mother-agent (we will call it “birth”), and from that point of time, the whole reward will come purely from how long baby will survive? Baby will initially have less mechanical capabilities, like speed, health, etc and then “grow” to be more capable by itself? I&#39;m not sure what should be the “brain” of baby-agent, another NN or maybe the same that were found from training the mother agent? Maybe creating a chain of agents: agent 1 at some point gives birth to the agent 2 and receive reward for each tick that agent 2 is alive, which itself will give birth to the agent 3 and receive reward fot each tick that agent 3 is alive, and so on. Maybe it will produce something interesting? Obviously the “alive time” is a proxy, and given enough optimization power we should expect Goodhart horrors beyond our comprehension. But the idea is that maybe there is some “simple” solution that will be found first, which we can study. Recreating the product of evolution, not using the immense “computational power” of it could be very tricky.</p><p> But if it seems to work, and mother-agent behave in a seemingly “caring way”, then we can try to apply interpretability tools, try to change the original environment drastically, to see how far it will generalize, try to break something and see how well it works, or manually override some parameters and study the change. <strong>However, I&#39;m not qualified to make this happen anyway, so if you find this idea interesting, contact me, maybe we can do this project together.</strong></p><h1> <strong>How the good result might look like?</strong></h1><p> Let&#39;s imagine that we&#39;ve got some agent that can behave with care toward the right type of “babies”. For some yet unknown reason, from outside view it behaves as if it cares about its baby-agent, it finds the creative ways to do so in new contexts. Now the actual work begins: we need to understand where are the parts that make this possible located, what is the underlying mechanism, what parts are crucial and what happens when you break them, how can we re-write the “baby” target, so that our agent will care about different baby-agents, under what conditions gradient descent will find an automatic off switch (I expect this to be related to the chance of obtaining another baby and given only 1 baby per “life”, gradient will never find the switch, since it will have no use). Then we can actually start to think about recreating it from scratch. Just like what people did with modular addition: <a href="https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><u>https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking</u></a> . Except this time we don&#39;t know how the algorithm could work or look like. But “intentions”, “motivations” and “goals” of potential AI systems are not magic, we should be able to recreate and reverse-engineer them.</p><br/><br/><a href="https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive<guid ispermalink="false"> JjqZexMgvarBFMKPs</guid><dc:creator><![CDATA[Catnee]]></dc:creator><pubDate> Thu, 07 Sep 2023 10:41:16 GMT</pubDate> </item><item><title><![CDATA[Sharing Information About Nonlinear]]></title><description><![CDATA[Published on September 7, 2023 6:51 AM GMT<br/><br/><p><i>认知状态：一旦我开始积极研究事物，我在下面的帖子中的大部分信息都是通过搜索有关非线性联合创始人的负面信息来获得的，而不是通过搜索来给出其总体成本和收益的平衡图景。我认为标准更新规则并不意味着您忽略这些信息，而是您会考虑一下，如果我选择了我可以分享的最差、可信的信息，那么您预期该信息会有多糟糕，然后根据更差（或更好）的程度进行更新这比你期望的我能生产的多。 （有关更多信息，请参阅这篇文章的第 5 节，了解</i><a href="https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence#5___Your_true_reason_screens_off_any_other_evidence_your_argument_might_include__"><i><u>预期证据守恒的错误</u></i></a><i>。）对于至少非零人士来说，在继续阅读之前，这似乎是一个值得在评论中进行的练习。 （你可以要求我找到足够的值得分享的内容，但也要注意，我认为我公开分享有关 EA/x-risk/rationalist/etc 生态系统中的人们的关键信息的门槛相对较低。）</i></p><p> <i>tl;dr：如果您希望我的重要更新快速总结为四个主张加概率，请跳至底部附近标题为“我的认知状态摘要”的部分。</i></p><hr><p>当我过去管理<a href="https://www.lesswrong.com/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices"><u>Lightcone 办公室</u></a>时，我花费了大量的时间和精力来把关 — 处理 EA/x-risk/rationalist 生态系统中的人员访问办公室并在办公室工作的申请，并做出决策。通常，这将涉及阅读他们的一些公开著作，并联系我信任的一些参考文献并询问有关他们的信息。我接触过的很多人都非常善于诚实地讲述他们与某人的经历，并分享他们对某人的看法。</p><p>有一次，非线性公司的凯特·伍兹（Kat Woods）和德鲁·斯帕茨（Drew Spartz）申请参观。我对他们和他们的工作不太了解，除了几次简短的互动之外，凯特·伍兹似乎精力充沛，对生活和工作的看法比我遇到的大多数人都要乐观。</p><p>我查阅了凯特列出的一些参考资料，这些参考资料都是积极到强烈积极的。然而，我也得到了强烈的负面评价——我告知这一决定的其他人告诉我，他们认识一些前员工，他们觉得自己在工资等方面受到了利用。然而，据报道，前雇员不愿意站出来，因为担心遭到报复，并且通常想摆脱整个事情，而且这些报道感觉非常模糊，我很难具体想象，但尽管如此，该人还是强烈建议不要邀请凯特和德鲁。</p><p>我觉得这不是一个足够有力的理由来禁止某人进入某个空间——或者更确切地说，我确实这么做了，但是对非常恶劣的行为的模糊匿名描述足以禁止某人，这是一个可以直接滥用的系统，所以我不认为不想使用这样的系统。此外，我有兴趣通过一次短暂的访问来了解凯特·伍兹（Kat Woods）——她只要求访问一周。所以我接受了，尽管我告诉她这让我很伤心。 （ <a href="https://docs.google.com/document/d/1rldJBi3eVVeqZjqbpCljgz9rmfNbuQW7HMEpugsPWt4/edit"><u>这是我发送给她的决定电子邮件的链接。</u></a> ）</p><p> （做出这个决定后，我也链接到了这个不祥但仍然模糊的<a href="https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding?commentId=5P75dFuKLo894MQFf"><u>EA 论坛帖子</u></a>，其中包括 Kat Woods 的一位前同事说他们不喜欢与她一起工作，还有更多像我上面收到的评论一样的评论，以及很多链接Glassdoor 上对非线性联合创始人艾默生·斯帕茨 (Emerson Spartz) 的前公司“Dose”进行了强烈负面评论。请注意，一半以上的负面评论是针对艾默生出售后的公司，但这是 2015 年以来的一个令人担忧的评论（当时艾默生·斯帕茨 (Emerson Spartz) 是首席执行官/联合创始人） <a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW8511849.htm"><u>）：“所有这些超级正面的评论都是由高层管理人员委托的。这是您应该了解 Spartz 的第一件事，我认为这很好地了解了公司的优先事项……更多地关心为之工作的人</u></a>”。2017 年<a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW8511849.htm"><u>的一篇评论</u></a>称，“<a href="https://www.glassdoor.com/Reviews/Employee-Review-Dose-RVW14177614.htm"><u>这种文化是有毒的，有很多派系、内部冲突和相互指责。</u></a> ”还有一些关于地狱般的工作场所的更糟糕的评论，非常令人担忧，但它们是在艾默生的 LinkedIn 表示他离开之后的时期，所以我不确定他在多大程度上对他们负责。）</p><p>在她来访的第一天，办公室里的另一个人私下联系我，说他们非常担心凯特和德鲁在办公室，并且他们认识两名与他们一起工作过的糟糕经历的员工。他们写道（我们后来进行了更多讨论）：</p><blockquote><p>他们的公司 Nonlinear 有着非法和不道德行为的历史，他们会吸引年轻人和天真的人来为他们工作，并在他们到达时让他们处于不人道的工作条件下，不支付他们承诺的工资，并要求他们做非法的事情作为他们实习的一部分。我个人认识两个经历过此事的人，他们因为受到报复的威胁而不敢发声，特别是凯特·伍兹和艾默生·斯巴茨。</p></blockquote><p>这引发了（对我来说）100-200 小时的调查，我采访了 10-15 名与非线性互动或工作过的人，阅读了许多书面文档，并试图拼凑出一些发生的事情。</p><p>我的结论是，他们的两名现场员工确实在与非线性公司合作时经历了非常可怕的经历，艾默生·斯帕茨和凯特·伍兹对有害的动态和员工事后的沉默负有重大责任。在调查非线性公司的过程中，我开始相信，那里的前雇员没有合法的就业机会，工资微薄，由于旅行而受到很大的孤立，如果他们辞职或发表对非线性公司的负面言论，他们就会隐含和明确地威胁进行报复。收到了很多（在我看来通常是空洞的）爱意之言以及家庭和浪漫之爱的要求，经历了许多进一步的不愉快或危险的经历，如果他们没有为非线性工作，他们就不会经历这些，并且需要几个月的时间才能恢复之后，在他们感觉能够重返工作岗位之前与朋友和家人一起。</p><p> （请注意，我认为上面引用的文字中描述的薪酬情况并不完全准确，我认为它非常小 - 1000 美元/月 - 并且员工隐含地期望他们会得到比他们实际得到的更多的收入，但有大部分不是“承诺”但没有兑现的工资。）</p><p>在第一次听到他们讲述他们的经历后，我仍然不确定事情的真相——我对非线性联合创始人了解不多，也不知道我可以对哪些关于社会动态的说法充满信心。为了了解更多背景信息，我花了大约 30 多个小时与 10 到 15 个不同的人通电话，这些人至少与 Kat、Emerson 和 Drew 之一有过一些专业的往来，试图建立对人员和组织的了解，这通过了解许多人的经历的共同点，对我建立自己的理解有很大帮助。我与许多与艾默生和凯特有过互动的人进行了交谈，他们对他们有很多积极的道德担忧和强烈的负面意见，而且我还与非线性联合创始人就这些担忧进行了 3 小时的对话，现在我对员工报告的一些动态。</p><p>对于大部分谈话，我都严格保密，但（经前雇员同意）我在这里写下了我学到的一些东西。</p><p>在这篇文章中，我不打算说出与我交谈过的大多数人的名字，但我会称呼两名前雇员为“爱丽丝”和“克洛伊”。我认为所涉及的人大多希望将这段生活抛在脑后，我会鼓励人们尊重他们的隐私，不要在网上提及他们的名字，也不要与他们谈论这件事，除非你已经是他们的好朋友了。</p><p> <strong>2023 年 3 月 7 日与 Kat 的对话</strong></p><p>回到我最初的经历：在他们来访的周二，我仍然不知道这些人是谁，也不知道发生了什么事情的任何细节，但我找到了一个在午餐时与凯特聊天的机会。</p><p>在聊了大约 15 分钟后，我表示我有兴趣讨论我在电子邮件中提出的问题，我们在私人房间里聊了 30-40 分钟。我们一坐下，凯特就开始讲述她的两名前雇员的故事，反复告诉我不要相信其中一名雇员（“爱丽丝”），她与真相的关系很糟糕，她很危险，而且她对社区的声誉构成风险。她说另一名员工（“克洛伊”）“很好”。</p><p>凯特·伍兹还告诉我，她希望对员工有一个“我不说你坏话，你也不说我坏话”的政策。我原则上强烈反对这种政策（正如我当时告诉她的那样）。这个细节和其他细节给我带来了进一步的危险信号（即薪资政策），我想了解发生了什么。</p><p>以下是她告诉我的内容的概述：</p><ul><li>当 Alice 和 Chloe 在 Nonlinear 工作时，他们有自己的开支（房费、伙食费、食物费），Chloe 还每月获得 1,000 美元的奖金。</li><li>爱丽丝和克洛伊与凯特、爱默生和德鲁住在同一所房子里。凯特说，她决定今后不再与员工住在一起。</li><li>她说，Alice 孵化了自己的项目（<a href="https://web.archive.org/web/20220321075904/https://www.nonlinear.org/hiringagency.html"><u>这里是非线性网站上孵化项目的描述</u></a>），能够设定自己的工资，而且 Alice 几乎从未与她（Kat）或她的其他老板（Emerson）交谈过。关于她的工资。</li><li>凯特不相信爱丽丝会说实话，而且爱丽丝有过“灾难性的误解”的历史。</li><li> Kat 告诉我 Alice 不清楚孵化的条款，并说 Alice 应该向 Kat 询问以避免这种误解。</li><li> Kat 表示，Alice 可能已经退出了很大一部分，因为 Kat 在接近尾声时错过了 Zoom 上的签到电话。</li><li> Kat表示，她希望Alice能够遵循“我不说你坏话，你也不说我坏话”的原则，但该员工并没有坚持自己的立场，而是散布了关于她的负面信息。凯特/非线性。</li><li>凯特说，她对爱丽丝给出了负面评价，建议人们“不要雇用她”，也不要资助她，“她对社区来说真的很危险”。</li><li>她说她和另一位员工克洛伊没有这些问题，她说她“很好，只是错误地选择了”“助理/运营经理”的角色，这就是导致她辞职的原因。凯特说，克洛伊技术相当熟练，但为凯特做了很多她不喜欢的卑微劳动。</li><li>她对 Chloe 所说的一件负面的事情是，她每年的工资相当于 75,000 美元<span class="footnote-reference" role="doc-noteref" id="fnref8a69v0tq2qo"><sup><a href="#fn8a69v0tq2qo">[1]</a></sup></span> （每月只有 1,000 美元，其余的来自食宿），但有一次她要求支付<i>75,000</i>美元<i>最重要</i>的是要支付所有费用，这是不可能的。 <span class="footnote-reference" role="doc-noteref" id="fnrefo53culramn"><sup><a href="#fno53culramn">[2]</a></sup></span></li></ul><h2>员工非线性体验的高级概述</h2><p><strong>背景</strong></p><p>非线性核心人员包括 Emerson Spartz、Kat Woods 和 Drew Spartz。</p><p> Kat Woods 已在 EA 生态系统工作了至少 10 年，于 2013 年共同创立了 Charity Science，并在那里工作到 2019 年。在 Charity Entrepreneurship 工作一年后，她于 2021 年与 Emerson Spartz 共同<a href="https://forum.effectivealtruism.org/posts/fX8JsabQyRSd7zWiD/introducing-the-nonlinear-fund-ai-safety-research-incubation"><u>创立了</u></a>Nonlinear，并在那里工作了 2.5 年。</p><p> Nonlinear 于 2022 年上半年<a href="https://survivalandflourishing.fund/sff-2022-h1-recommendations"><u>从</u></a>生存与繁荣基金获得了 599,000 美元，并于 2022 年 1 月<a href="https://www.openphilanthropy.org/grants/nonlinear-fund-personal-assistant-hiring-agency/"><u>从</u></a>开放慈善基金获得了 15,000 美元。</p><p>艾默生主要通过他以前的公司 Dose 和出售 Mugglenet.com（他创立的）获得的个人财富为该项目提供资金。艾默生和凯特是浪漫的伴侣，艾默生和德鲁是兄弟。他们都住在同一栋房子里，一起环游世界，每月从爱彼迎跳到爱彼迎一两次。他们雇用的员工要么远程办公，要么与他们住在同一栋房子里。</p><p>我目前的理解是，他们有大约 4 名远程实习生、1 名远程员工和 2 名现场员工（Alice 和 Chloe）。爱丽丝是唯一经历过他们的孵化器项目的人。</p><p>非线性试图建立一种相当高的承诺文化，让长期员工在个人和职业上与核心家庭单位密切相关。然而，他们的经济独立性极低，而且其中涉及的许多社会动态对我来说似乎确实很危险。</p><p><strong>爱丽丝和克洛伊</strong></p><p>Alice从2021年11月到2022年6月在那里工作，Chloe从2022年1月到2022年7月在那里工作。在与他们两人交谈后，我了解到以下情况：</p><ul><li>两人在任何时候都没有合法受雇于非营利组织。</li><li>克洛伊和爱丽丝的财务（以及凯特和德鲁的）都直接来自艾默生的个人资金（而不是来自非营利组织）。这使得他们必须获得个人购买许可，并且在与家庭单位一起工作时，他们无法与家庭单位分开居住，并且他们报告说，在他们工作期间，他们在社会和经济上感到非常依赖家庭。</li><li> Chloe 的薪水被口头同意为每年 75,000 美元左右。然而，她每月的工资只有 1000 美元，除此之外还有很多基本的补偿，比如房租、杂货、旅行。这本来是为了让一起旅行变得更容易，并且应该达到相同的薪水水平。虽然艾默生确实为爱丽丝和克洛伊提供了食物、膳食和旅行方面的补偿，但克洛伊认为她得到的补偿金额并不等于所讨论的工资，而且我相信没有对爱丽丝或克洛伊进行会计处理以确保任何工资都匹配。 （我对他们的爱彼迎和旅行费用进行了一些抽查，爱丽丝/克洛伊的认知状态对我来说似乎相当合理。）</li><li>爱丽丝作为唯一的人加入了他们的孵化计划。在 EAG 与 Nonlinear 会面并与 Emerson 进行了约 4 小时的交谈，以及与 Kat 进行了第二次 Zoom 通话后，她搬进了他们的住处。最初，在与他们一起旅行时，她远程继续之前的工作，但被鼓励辞职并在孵化组织工作，两个月后，她辞去了工作，开始与 Nonlinear 合作开展项目。在她在那里的 8 个月里，Alice 声称她前 5 个月没有收到工资，然后（大约）两个月每月工资 1,000 美元，然后在她辞职后，她收到了约 6,000 美元的一次性工资（来自分配给她孵化的组织的资金）。她还承保了大量紧急健康问题。 <span class="footnote-reference" role="doc-noteref" id="fnrefylc0clu1ffa"><sup><a href="#fnylc0clu1ffa">[3]</a></sup></span></li><li>薪资谈判一直是爱丽丝在非线性公司期间的主要压力源。在那里，她度过了所有的财务困境，最后几个月的很大一部分时间都在财务上处于亏损状态（账单和医疗费用比她银行账户里的钱还多），部分原因是等待工资来自非线性的付款。由于个人资金极少以及希望从非线性公司获得财务独立，她最终辞职了。当她退出时，她（根据他们的要求）给予了非线性公司她原本已经完成孵化的组织的全部所有权。</li><li>通过与 Alice 和 Nonlinear 的交谈，发现在 Alice 在那里工作结束时，自 2 月底以来，Kat Woods 一直将 Alice 视为她管理的员工，但 Emerson 并未将 Alice 视为员工，主要是因为她愿意而与他们一起旅行并合作的人，每月 1000 美元加上其他报酬是一份慷慨的礼物。</li><li>爱丽丝和克洛伊报告说，凯特、艾默生和德鲁创造了一个环境，在这个环境中，成为非线性的有价值的成员包括在解决问题方面具有创业精神和创造力——实际上，这通常意味着强烈鼓励绕过标准的社会规则来获得你想要的东西，包括通过向员工施压来获得某人最喜欢的餐桌，以及寻找与其工作相关的法律漏洞。这也适用于组织内部。爱丽丝和克洛伊报告说，在为非线性公司工作期间，他们被迫或被说服采取了多项令他们感到非常后悔的行动，例如在经济上非常依赖艾默生、放弃素食、在外国无证驾驶数月。 （需要明确的是，我并不是说这些法律是好的，违反这些法律是坏的，我是说，从他们的报告中听来，他们确信采取可能会带来严重个人负面影响的行动，例如入狱）他们自信地认为，如果不是由于非线性联合创始人的强大压力和公司内部的敌对社会环境，他们不会采取这些行动。）我将描述下面将更详细地介绍这些事件。</li><li>他们都表示，在与非线性公司结束关系后，他们需要几个月的时间才能恢复，然后才感到能够再次工作，并且都称在那里工作是他们一生中最糟糕的经历之一。</li><li>他们都报告说，他们非常担心非线性公司因与我交谈而遭受的职业和个人报复，并向我讲述了故事并向我展示了一些文本，使我相信这是一个非常可信的担忧。</li></ul><h2>各种报告的经历</h2><p>这两位员工在非线性公司的经历中有很多部分让他们感到非常不愉快和受伤。我将在下面总结其中的一些内容。</p><p>我认为发生的许多事情都是警告信号，我也认为有一些红线，我将在这篇文章的底部讨论我的想法，即我的要点中哪些是红线。</p><p><strong>我对这些报告的信任程度</strong></p><p>大多数动态都被多个不同的人描述为准确的（低工资、没有法律结构、孤立、社会操纵的一些因素、恐吓），这让我对它们有很高的信心，非线性自己也证实了这些动态的各个部分。账户。</p><p>我会有意义地更新有关这类事情的人的言论，他们都保证克洛伊的话是可靠的。</p><p>非线性工作人员和少数在爱丽丝和克洛伊工作期间拜访的其他人强烈质疑爱丽丝的可信度，并暗示她完全撒了谎。 Nonlinear 向我展示了一些短信，其中与 Alice 交谈过的人留下的印象是她的工资是 0 美元或 500 美元，这是不准确的（正如她告诉我的那样，她在网上的工资约为 8000 美元）。</p><p>也就是说，我个人发现爱丽丝非常愿意并准备根据要求与我分享主要来源（短信、银行信息等），所以我不相信她有恶意。</p><p>在我与她的第一次谈话中，凯特声称爱丽丝有很多灾难性的沟通错误，但克洛伊（引用）“很好”。总的来说，没有人质疑克洛伊的话，而那些告诉我他们质疑爱丽丝的话的人大体上说他们相信克洛伊的话。</p><p>就我个人而言，我发现他们对报复的所有恐惧都是真实而认真的，并且在我看来是有道理的。</p><p><strong>为什么我要分享这些</strong></p><p>我确实有一个强烈的启发，即同意的成年人可以同意各种最终伤害他们的事情（即接受这些工作），即使我家长式地可能认为我可以阻止他们伤害自己。也就是说，我有明显的理由认为凯特、艾默生和德鲁恐吓这些人，让他们接受一些伤害他们的行为或动态，所以有些部分在我看来显然不是双方同意的。</p><p>除此之外，我认为其他人知道他们正在做什么是有好处的，所以我认为分享这些信息很好，因为它对于许多有可能使用非线性工作的人来说是相关的。对我来说最重要的是，我特别想这样做，因为在我看来，非线性已经试图阻止这种负面信息被共享，所以我在共享事物方面犯了强烈的错误。</p><p> （其中一名员工还想谈谈她为何为这篇文章做出贡献，我已将其放在脚注中。 <span class="footnote-reference" role="doc-noteref" id="fnref2ad0whew08p"><sup><a href="#fn2ad0whew08p">[4]</a></sup></span> ）</p><p><strong>财务和社会环境高度依赖</strong></p><p>每个人都住在同一所房子里。艾默生和凯特会共用一个房间，其他人会凑合着用其他可用的东西，通常共用卧室。</p><p> Nonlinear 主要在他们通常不认识当地人的国家/地区流动，员工通常除了联合创始人之外没有人可以互动，员工报告说，他们被拒绝住在与联合创始人不同的爱彼迎上。</p><p>爱丽丝和克洛伊报告说，他们被建议不要花时间与“低价值的人”在一起，包括他们的家人、浪漫伴侣以及他们居住地的任何当地人，但非线性邀请的客人/访客除外。爱丽丝和克洛伊报告说，这使他们在社交上非常依赖凯特/艾默生/德鲁，并且在其他方​​面非常孤立。</p><p>员工们对于非线性公司会支付什么费用和不会支付什么费用的界限非常不清楚。例如，爱丽丝和克洛伊报告说，他们曾经花了几天的时间开车在波多黎各周围寻找更便宜的医疗服务，然后将其提交给高级工作人员，因为他们不知道医疗费用是否会被承保，所以他们想确保尽可能便宜地增加高级员工同意的机会。</p><p>财务状况复杂、混乱。这在很大程度上是因为他们很少进行会计处理。总而言之，Alice 在过去 2 个月的大部分时间里，银行账户里的存款都不足 1000 欧元，有时还需要打电话给艾默生进行立即转账，以便能够支付她看医生时的医疗费用。当她辞职时，她的账户里有 700 欧元，这不足以支付她月底的账单，这让她非常害怕。不过需要明确的是，非线性公司在一周内就向她偿还了约 2900 欧元的拖欠工资，部分原因是她强烈要求。 （这里的相关问题是极高的财务依赖性和贫富差距，但爱丽丝并没有声称非线性公司未能支付他们的费用。）</p><p> Alice 表示，她坚持这么久的主要原因之一是，她希望通过启动自己的孵化项目实现财务独立，该项目分配了 10 万美元（由 FTX 筹集资金）。在她在那里的最后一个月，凯特告诉她，虽然她会完全独立工作，但他们会将钱存入非线性银行账户，她会索要这笔钱，这意味着她不会像她所期望的那样从他们那里获得财务独立，得知这一点是爱丽丝放弃的原因。</p><p>其中一名员工采访了凯特，询问她的生产力建议，并与我分享了这次采访的笔记。该员工写道：</p><blockquote><p>在采访中，凯特公开承认自己效率不高，但她表示，她看起来仍然很有效率，因为她让别人为她工作。她依靠愿意为她做免费工作的志愿者，这是她提高生产力的首要建议。</p></blockquote><p>员工们报告说，一些实习生后来对无薪工作提出了强烈的负面反馈，因此凯特决定不再雇用实习生。</p><p><strong>如果工作关系不顺利，可能会面临严重的不利影响</strong></p><p>在艾默生·斯帕茨 (Emerson Spartz) 与其中一名员工的对话中，该员工向一位想在受雇期间寻找另一份工作的朋友寻求建议，但尚未让现任雇主知道他们离职的决定。据报道，艾默生立即表示，他现在必须更新，考虑到该员工本人正在考虑离开非线性公司。他接着告诉她，他对那些离开公司去从事同样好或不太好的工作的员工感到生气；他说，他理解员工是否为了明显更好的机会而离开。员工报告说，这导致他们非常害怕离开工作，这既是因为艾默生更新了员工现在试图离职的想法，也因为艾默生对因“原因”而离职的员工进行报复。不好的理由”。</p><p>有关艾默生经营理念的背景信息：爱丽丝引用艾默生建议的以下工作进度指标：“您能够在短时间内从他人那里获取多少价值？” <span class="footnote-reference" role="doc-noteref" id="fnreft84or63yyei"><sup><a href="#fnt84or63yyei">[5]</a></sup></span>另一位来访者向我描述艾默生“总是试图利用他所有的讨价还价能力”。克洛伊告诉我，当她代表非线性公司与外部合作伙伴谈判薪资时，艾默生建议她在谈判薪资时，提供“你能接受的最低数字”。</p><p>许多不同的人报告说，艾默生·斯帕茨会向员工和访客吹嘘他的商业谈判策略。他会鼓励员工阅读许多有关战略和影响力的书籍。当他们读<a href="https://www.amazon.com/48-Laws-Power-Robert-Greene/dp/0140280197"><u>《权力的48条法则》</u></a>一书时，他会举出他在过去的商业实践中遵循“法则”的例子。</p><p>他向员工和访客讲述的一个故事是关于他在与他的前青少年学员阿多里安·戴克 (Adorian Deck) 发生冲突时所采取的恐吓策略。</p><p> （有关冲突的背景，请参阅当时撰写的相关文章的链接： <a href="https://www.hollywoodreporter.com/business/business-news/teen-who-created-omgfacts-twitter-182620/"><u>Hollywood Reporter</u></a> 、 <a href="https://www.jacksonville.com/story/business/2011/05/16/california-teens-lawsuit-over-twitter-feed-underscores-power-tweet/15903568007/"><u>Jacksonville</u></a> 、 <a href="https://blog.ericgoldman.org/archives/2011/05/thoughts_on_the.htm"><u>Technology &amp; Marketing Law Blog</u></a>和<a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>Emerson Spartz&#39;s Tumblr</u></a> 。另外，这里是他们签署的<a href="https://web.archive.org/web/20110824205315/http://www.omgfactslawsuit.com.s3.amazonaws.com/CONTRACT.pdf"><u>法律合同</u></a>，Deck 后来起诉撤销。）</p><p>简而言之，Adorian Deck 是一位 16 岁的年轻人（2009 年）创建了一个名为“OMGFacts”的 Twitter 帐户，该帐户很快就拥有超过 300,000 名粉丝。艾默生主动寻求在该品牌下建立公司，并同意与 Adorian 达成协议。不到一年后，阿多里安想要退出这笔交易，声称艾默生赚取了超过 10 万美元的利润，而他只看到了 100 美元，并提起诉讼以终止交易。</p><p>根据艾默生的说法，事实证明，加州有一个独特的条款（由于洛杉矶的演艺职业），即使未成年人与其父母签署了合同，除非在法官的监督下签署，否则合同无效，因此他们能够简单地退出交易。</p><p>但时至今日，艾默生公司仍然拥有 OMGfacts 品牌和公司以及 Youtube 频道。</p><p> （旁注：我并不是想断言在这些冲突中谁是“正确的”，我将这些作为埃默罗斯恩谈判策略的例子进行报道，据报道他在冲突期间参与并积极支持这些策略。）</p><p>艾默生向与我交谈过的不同人讲述了这个故事的不同版本（人们认为他在“吹牛”）。</p><p>在一个版本中，他声称他用无休无止的法律威胁对阿多里安和他的母亲进行了武装，他们做出了让步，让他完全控制了该品牌。与我交谈的这个人不记得细节，但他说艾默生试图吓唬德克和他的母亲，而他们（艾默生吹嘘的那个人）觉得这“令人恐惧”，并认为这种行为是“类似于 7 标准的行为”偏离了该领域的通常规范。”</p><p>另一个人在《权力48条法则》第二条法则的背景下讲述了这个故事，即“永远不要过于信任朋友，学会如何利用敌人”。摘要包括</p><blockquote><p>“要警惕朋友——他们会更快地背叛你，因为他们很容易被嫉妒。他们也会变得被宠坏和暴虐……你更害怕朋友而不是敌人。”</p></blockquote><p>对于这个听过阿多里安故事的人来说，当他讲述这个故事时，最能引起共鸣的是他声称自己与阿多里安有着密切的指导关系，并且对他非常了解，以至于他知道“到底该去哪里”伤害他最深”，这样他就会退缩。在那个版本的故事中，他说 Deck 的人生目标是成为一名 YouTuber（直到今天这确实是 Deck 的职业——<a href="https://www.youtube.com/@AdorianDeck"><u>他每月制作大约 4 个视频</u></a>），而艾默生策略性地联系了 Deck 最欣赏的 YouTuber，并向他们讲述了德克偷懒并试图将艾默生的所有工作归功于自己的故事。据报道，他威胁要采取更多行动，直到德克做出让步，这就是德克放弃诉讼的原因。那个人对我说：“他爱他，非常了解他，并用这些知识摧毁了他。” <span class="footnote-reference" role="doc-noteref" id="fnrefzcp7frumok"><sup><a href="#fnzcp7frumok">[6]</a></sup></span></p><p>后来我和艾默生谈过这个问题。他确实说过，他正在与顶级 YouTube 用户合作制作揭露 Deck 的视频，这就是 Deck 重新回到谈判桌的原因。他说，他最终重新谈判了一份合同，Deck 每月获得 1 万美元，为期 7 年。如果属实，我认为这最终协议对艾默生产生了积极的影响，尽管我仍然相信与他交谈的人对与艾默生在这个问题上的对话感到非常害怕。 （我既没有证实这份合同的存在，也没有听过德克的说法。）</p><p>据报道，他讲述了另一个关于他在商业交易中被骗的反应的谈判故事。我不会详细说明，但据报道，他为徽标/商标的权利支付了高价，却发现他没有阅读细则，并且被出售了价值低得多的东西。他举了《权力 48 条法则》中“让他人处于悬置的恐惧之中：培养不可预测的气氛”策略的例子：</p><blockquote><p>故意不可预测。看似没有一致性或目的的行为会让他们失去平衡，并且他们会在试图解释你的举动时精疲力竭。极端地说，这种策略可能会造成恐吓和恐吓。</p></blockquote><p>据报道，在那次商业谈判中，他表现得精神错乱。据与我交谈的人透露，他说他会打电话给对方，说“疯狂的事情”并对他们大喊大叫，目的是让他们认为他有能力做任何事情，包括危险和不道德的事情，最终他们心软了并给了他他想要的交易。</p><p>据我采访的其他人报道，他多次表示，他会对与他发生冲突的人“非常敌对”。据报道，他举了一个例子，如果有人试图起诉他，他愿意进入法律灰色地带，以“粉碎他的敌人”（他显然经常使用这个词），包括雇人跟踪这个人并他们的家人为了吓唬他们。 （艾默生否认曾说过这一点，并表示他可能将此描述为其他<i>人</i>可能在人们应该意识到的冲突中使用的一种策略。）</p><p>克洛伊最终退出后，爱丽丝报告说凯特/艾默生会“垃圾话”她，说她从来都不是一个“A级玩家”，在很多方面（能力、道德、戏剧等）批评她，尽管之前主要是给克洛伊高度赞扬。据报道，这种情况通常发生在那些结束或拒绝与非线性公司合作的人身上。</p><p> Here are some texts between Kat Woods and Alice shortly after Alice had quit, before the final salary had been paid. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/fuglgbnch2qgely8aahb"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/ezzjqbuxcxaqfpte46ee"></p><p> A few months later, some more texts from Kat Woods. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lc8r4tZ2L5txxokZ8/cd087amewavbd71uq75z"></p><p> (I can corroborate that it was difficult to directly talk with the former employee and it took a fair bit of communication through indirect social channels before they were willing to identify themselves to me and talk about the details.)</p><p> <strong>Effusive positive emotion not backed up by reality, and other manipulative techniques</strong></p><p> Multiple people who worked with Kat reported that Kat had a pattern of enforcing arbitrary short deadlines on people in order to get them to make the decision she wants eg <i>“I need a decision by the end of this call”</i> , or (in an email to Alice) “ <i>This is urgent and important. There are people working on saving the world and we can&#39;t let our issues hold them back from doing their work.”</i></p><p> Alice reported feeling emotionally manipulated. She said she got constant compliments from the founders that ended up seeming fake.</p><p> Alice wrote down a string of the compliments at the time from Kat Woods (said out loud and that Alice wrote down in text), here is a sampling of them that she shared with me:</p><blockquote><p> “You&#39;re the kind of person I bet on, you&#39;re a beast, you&#39;re an animal, I think you are extraordinary&quot;</p><p> &quot;You can be in the top 10, you really just have to think about where you want to be, you have to make sacrifices to be on the top, you can be the best, only if you sacrifice enough&quot;</p><p> &quot;You&#39;re working more than 99% because you care more than 99% because you&#39;re a leader and going to save the world&quot;</p><p> &quot;You can&#39;t fail if you commit to [this project], you have what it takes, you get sh*t done and everyone will hail you in EA, finally an executor among us.&quot;</p></blockquote><p> Alice reported that she would get these compliments near-daily. She eventually had the sense that this was said in order to get something out of her. She reported that one time, after a series of such compliments, the Kat Woods then turned and recorded a near-identical series of compliments into their phone for a different person.</p><p> Kat Woods reportedly several times cried while telling Alice that she wanted the employee in their life forever and was worried that this employee would ever not be in Kat&#39;s life.</p><p> Other times when Alice would come to Kat with money troubles and asking for a pay rise, Alice reports that Kat would tell them that this was a psychological issue and that actually they had safety, for instance they could move back in with their parents, so they didn&#39;t need to worry.</p><p> Alice also reports that she was explicitly advised by Kat Woods to cry and look cute when asking Emerson Spartz for a salary improvement, in order to get the salary improvement that she wanted, and was told this was a reliable way to get things from Emerson. (Alice reports that she did not follow this advice.)</p><p> <strong>Many other strong personal costs</strong></p><p> Alice quit being vegan while working there. She was sick with covid in a foreign country, with only the three Nonlinear cofounders around, but nobody in the house was willing to go out and get her vegan food, so she barely ate for 2 days. Alice eventually gave in and ate non-vegan food in the house. She also said that the Nonlinear cofounders marked her quitting veganism as a &#39;win&#39;, as they thad been arguing that she should not be vegan.</p><p> (Nonlinear disputes this, and says that they did go out and buy her some vegan burgers food and had some vegan food in the house. They agree that she quit being vegan at this time, and say it was because being vegan was unusually hard due to being in Puerto Rico. Alice disputes that she received any vegan burgers.)</p><p> Alice said that this generally matched how she and Chloe were treated in the house, as people generally not worth spending time on, because they were &#39;low value&#39; (ie in terms of their hourly wage), and that they were the people who had to do chores around the house (eg Alice was still asked to do house chores during the period where she was sick and not eating).</p><p> By the same reasoning, the employees reported that they were given 100% of the menial tasks around the house (cleaning, tidying, etc) due to their lower value of time to the company. For instance, if a cofounder spilled food in the kitchen, the employees would clean it up. This was generally reported as feeling very demeaning.</p><p> Alice and Chloe reported a substantial conflict within the household between Kat and Alice. Alice was polyamorous, and she and Drew entered into a casual romantic relationship. Kat previously had a polyamorous marriage that ended in divorce, and is now monogamously partnered with Emerson. Kat reportedly told Alice that she didn&#39;t mind polyamory &quot;on the other side of the world”, but couldn&#39;t stand it right next to her, and probably either Alice would need to become monogamous or Alice should leave the organization. Alice didn&#39;t become monogamous. Alice reports that Kat became increasingly cold over multiple months, and was very hard to work with. <span class="footnote-reference" role="doc-noteref" id="fnrefuvloifuhbna"><sup><a href="#fnuvloifuhbna">[7]</a></sup></span></p><p> Alice reports then taking a vacation to visit her family, and trying to figure out how to repair the relationship with Kat. Before she went on vacation, Kat requested that Alice bring a variety of illegal drugs across the border for her (some recreational, some for productivity). Alice argued that this would be dangerous for her personally, but Emerson and Kat reportedly argued that it is not dangerous at all and was “absolutely risk-free”. Privately, Drew said that Kat would “love her forever” if she did this. I bring this up as an example of the sorts of requests that Kat/Emerson/Drew felt comfortable making during Alice&#39;s time there.</p><p> Chloe was hired by Nonlinear with the intent to have them do executive assistant tasks for Nonlinear ( <a href="https://web.archive.org/web/20211022160447/https://www.nonlinear.org/operations.html"><u>this is the job ad they responded to</u></a> ). After being hired and flying out, Chloe was informed that on a daily basis their job would involve driving eg to get groceries when they were in different countries. She explained that she didn&#39;t have a drivers&#39; license and didn&#39;t know how to drive. Kat/Emerson proposed that Chloe learn to drive, and Drew gave her some driving lessons. When Chloe learned to drive well enough in parking lots, she said she was ready to get her license, but she discovered that she couldn&#39;t get a license in a foreign country. Kat/Emerson/Drew reportedly didn&#39;t seem to think that mattered or was even part of the plan, and strongly encouraged Chloe to just drive without a license to do their work, so she drove ~daily for 1-2 months without a license. (I think this involved physical risks for the employee and bystanders, and also substantial risks of being in jail in a foreign country. Also, Chloe basically never drove Emerson/Drew/Kat, this was primarily solo driving for daily errands.) Eventually Chloe had a minor collision with a street post, and was a bit freaked out because she had no idea what the correct protocols were. She reported that Kat/Emerson/Drew didn&#39;t think that this was a big deal, but that Alice (who she was on her way to meet) could clearly see that Chloe was distressed by this, and Alice drove her home, and Chloe then decided to stop driving.</p><p> (Car accidents are the <a href="https://www.cdc.gov/injury/wisqars/pdf/leading_causes_of_injury_deaths_highlighting_unintentional_2018-508.pdf"><u>second most common cause of death</u></a> for people in their age group. Insofar as they were pressured to do this and told that this was safe, I think this involved a pretty cavalier disregard for the safety of the person who worked for them.)</p><p> Chloe talked to a friend of hers (who is someone I know fairly well, and was the first person to give me a negative report about Nonlinear), reporting that they were very depressed. When Chloe described her working conditions, her friend was horrified, and said she had to get out immediately since, in their words, “this was clearly an abusive situation”. The friend offered to pay for flights out of the country, and tried to convince her to quit immediately. Eventually Chloe made a commitment to book a flight by a certain date and then followed through with that.</p><p> <strong>Lax on legalities and adversarial business practices</strong></p><p> I did not find the time to write much here. For now I&#39;ll simply pass on my impressions.</p><p> I generally got a sense from speaking with many parties that Emerson Spartz and Kat Woods respectively have very adversarial and very lax attitudes toward legalities and bureaucracies, with the former trying to do as little as possible that is asked of him. If I asked them to fill out paperwork I would expect it was filled out at least reluctantly and plausibly deceptively or adversarially in some way. In my current epistemic state, I would be actively concerned about any project in the EA or x-risk ecosystems that relied on Nonlinear doing any accounting or having a reliable legal structure that has had the basics checked.</p><p> Personally, if I were giving Nonlinear funds for any project whatsoever, including for regranting, I&#39;d expect it&#39;s quite plausible (>;20%) that they didn&#39;t spend the funds on what they told me, and instead will randomly spend it on some other project. If I had previously funded Nonlinear for any projects, I would be keen to ask Nonlinear for receipts to show whether they spent their funds in accordance with what they said they would.</p><p> <strong>This is not a complete list</strong></p><p> I want to be clear that this is not a <i>complete</i> list of negative or concerning experiences, this is an <i>illustrative</i> list. There are many other things that I was told about that I am not including here due to factors like length and people&#39;s privacy (on all sides). Also I split them up into the categories as I see them; someone else might make a different split.</p><p> <strong>Perspectives From Others Who Have Worked or Otherwise Been Close With Nonlinear</strong></p><p> I had hoped to work this into a longer section of quotes, but it seemed like too much back-and-forth with lots of different people. I encourage folks to leave comments with their relevant impressions.</p><p> For now I&#39;ll summarize some of what I learned as follows:</p><ul><li> Several people gave reports consistent with Alice and Chloe being very upset and distressed both during and after their time at Nonlinear, and reaching out for help, and seeming really strongly to want to get away from Nonlinear.</li><li> Some unpaid interns (who worked remotely for Nonlinear for 1-3 months) said that they regretted not getting paid, and that when they brought it up with Kat Woods she said some positive sounding things and they expected she would get back to them about it, but that never happened during the rest of their internships.</li><li> Many people who visited had fine experiences with Nonlinear, others felt much more troubled by the experience.</li><li> One person said to me about Emerson/Drew/Kat:<ul><li> <i>&quot;My subjective feeling is like &#39;they seemed to be really bad and toxic people&#39;. And they at the same time have a decent amount of impact. After I interacted repeatedly with them I was highly confused about the dilemma of people who are mistreating other people, but are doing some good.&quot;</i></li></ul></li><li> Another person said about Emerson:<ul><li> <i>“He seems to think he&#39;s extremely competent, a genius, and that everyone else is inferior to him. They should learn everything they can from him, he has nothing to learn from them. He said things close to this explicitly. Drew and (to a lesser extent) Kat really bought into him being the new messiah.”</i></li></ul></li><li> One person who has worked for Kat Woods (not Alice or Chloe) said the following:<ul><li> <i>I love her as a person, hate her as a boss. She&#39;s fun, has a lot of ideas, really good socialite, and I think that that speaks to how she&#39;s able to get away with a lot of things. Able to wear different masks in different places. She&#39;s someone who&#39;s easy to trust, easy to build social relationships with. I&#39;d be suspicious of anyone who gives a reference who&#39;s never been below Kat in power.</i></li><li> <i>Ben: Do you think Kat is emotionally manipulative?</i></li><li> <i>I think she is. I think it&#39;s a fine line about what makes an excellent entrepreneur. Do whatever it takes to get a deal signed. To get it across the line. Depends a lot on what the power dynamics are, whether it&#39;s a problem or not. If people are in equal power structures it&#39;s less of a problem.</i></li></ul></li></ul><p> There were other informative conversations that I won&#39;t summarize. I encourage folks who have worked with or for Nonlinear to comment with their perspective.</p><h2> Conversation with Nonlinear</h2><p> After putting the above together, I got permission from Alice and Chloe to publish, and to share the information I had learned as I saw fit. So I booked a call with Nonlinear, sent them a long list of concerns, and talked with Emerson, Kat and Drew for ~3 hours to hear them out.</p><p> <strong>Paraphrasing Nonlinear</strong></p><p> On the call, they said their primary intention in the call was to convince me that Alice is a bald-faced liar. They further said they&#39;re terrified of Alice making false claims about them, and that she is in a powerful position to hurt them with false accusations.</p><p> Afterwards, I wrote up a paraphrase of their responses. I shared it with Emerson and he replied that it was a “Good summary!”. Below is the paraphrase of their perspective on things that I sent them, with one minor edit for privacy. (The below is written as though Nonlinear is speaking, but to be clear this 100% my writing.)</p><ul><li> We hired one person, and kind-of-technically-hired a second person. In doing so, our intention wasn&#39;t just to have employees, but also to have members of our family unit who we traveled with and worked closely together with in having a strong positive impact in the world, and were very personally close with.</li><li> We nomadically traveled the globe. This can be quite lonely so we put a lot of work into bringing people to us, often having visitors in our house who we supported with flights and accommodation. This probably wasn&#39;t perfect but in general we&#39;d describe the environment as &quot;quite actively social&quot;.</li><li> For the formal employee, she responded to a job ad, we interviewed her, and it all went the standard way. For the gradually-employed employee, we initially just invited her to travel with us and co-work, as she seemed like a successful entrepreneur and aligned in terms of our visions for improving the world. Over time she quit her existing job and we worked on projects together and were gradually bringing her into our organization.</li><li> We wanted to give these employees a pretty standard amount of compensation, but also mostly not worry about negotiating minor financial details as we traveled the world. So we covered basic rent/groceries/travel for these people. On top of that, to the formal employee we gave a $1k/month salary, and to the semi-formal employee we eventually did the same too. For the latter employee, we roughly paid her ~$8k over the time she worked with us.</li><li> From our perspective, the gradually-hired employees gave a falsely positive impression of their financial and professional situation, suggesting they&#39;d accomplished more than they had and were earning more than they had. They ended up being fairly financially dependent on us and we didn&#39;t expect that.</li><li> Eventually, after about 6-8 months each, both employees quit. Overall this experiment went poorly from our perspective and we&#39;re not going to try it in future.</li><li> For the formal employee, we&#39;re a bit unsure about why exactly she quit, even though we did do exit interviews with her. She said she didn&#39;t like a lot of the menial work (which is what we hired her for), but didn&#39;t say that money was the problem. We think it is probably related to everyone getting Covid and being kind of depressed around that time.</li><li> For the other employee, relations got bad for various reasons. She ended up wanting total control of the org she was incubating with us, rather than 95% control as we&#39;d discussed, but that wasn&#39;t on the table (the org had $250k dedicated to it that we&#39;d raised!), and so she quit.</li><li> When she was leaving, we were financially supportive. On the day we flew back from the Bahamas to London, we paid all our outstanding reimbursements (~$2900). We also offered to pay for her to have a room in London for a week as she got herself sorted out. We also offered her rooms with our friends if she promised not to tell them lies about us behind our backs.</li><li> After she left, we believe she told a lot of lies and inaccurate stories about us. For instance, two people we talked to had the impression that we either paid her $0 or $500, which is demonstrably false. Right now we&#39;re pretty actively concerned that she is telling lots of false stories in order to paint us in a negative light, because the relationship didn&#39;t work out and she didn&#39;t get control over her org (and because her general character seems drama-prone).</li></ul><p> There were some points around the experiences of these employees that we want to respond to.</p><ul><li> First; the formal employee drove without a license for 1-2 months in Puerto Rico. We taught her to drive, which she was excited about. You might think this is a substantial legal risk, but basically it isn&#39;t, as you can see <a href="https://casetext.com/statute/laws-of-puerto-rico/title-nine-highways-and-traffic/chapter-27-vehicle-and-traffic-law-2000/subchapter-ii-requirements-and-procedure-for-issuance-expiration-and-renewal-of-drivers-licenses/5073-illegal-use-of-the-driving-license-and-penalties"><u>here</u></a> , the general range of fines for issues around not-having-a-license in Puerto Rico is in the range of $25 to $500, which just isn&#39;t that bad.</li><li> Second; the semi-employee said that she wasn&#39;t supported in getting vegan food when she was sick with Covid, and this is why she stopped being vegan. This seems also straightforwardly inaccurate, we brought her potatoes, vegan burgers, and had vegan food in the house. We had been advising her to 80/20 being a vegan and this probably also weighed on her decision.</li><li> Third; the semi-employee was also asked to bring some productivity-related and recreational drugs over the border for us. In general we didn&#39;t push hard on this. For one, this is an activity she already did (with other drugs). For two, we thought it didn&#39;t need prescription in the country she was visiting, and when we found out otherwise, we dropped it. And for three, she used a bunch of our drugs herself, so it&#39;s not fair to say that this request was made entirely selfishly. I think this just seems like an extension of the sorts of actions she&#39;s generally open to.</li></ul><p> Finally, multiple people (beyond our two in-person employees) told Ben they felt frightened or freaked out by some of the business tactics in the stories Emerson told them. To give context and respond to that:</p><ul><li> I, Emerson, have had a lot of exceedingly harsh and cruel business experience, including getting tricked or stabbed-in-the-back. Nonetheless, I have often prevailed in these difficult situations, and learned a lot of hard lessons about how to act in the world.</li><li> The skills required to do so seem to me lacking in many of the earnest-but-naive EAs that I meet, and I would really like them to learn how to be strong in this way. As such, I often tell EAs these stories, selecting for the most cut-throat ones, and sometimes I try to play up the harshness of how you have to respond to the threats. I think of myself as playing the role of a wise old mentor who has had lots of experience, telling stories to the young adventurers, trying to toughen them up, somewhat similar to how Prof Quirrell <span class="footnote-reference" role="doc-noteref" id="fnref03ez0hl92kmc"><sup><a href="#fn03ez0hl92kmc">[8]</a></sup></span> toughens up the students in HPMOR through teaching them Defense Against the Dark Arts, to deal with real monsters in the world.</li><li> For instance, I tell people about my negotiations with Adorian Deck about the OMGFacts brand and Twitter account. We signed a good deal, but a California technicality meant he could pull from it and take my whole company, which is a really illegitimate claim. They wouldn&#39;t talk with me, so I was working with top YouTubers to make some videos publicizing and exposing his bad behavior. This got him back to the negotiation table and we worked out a deal where he got $10k/month for seven years, which is not a shabby deal, and meant that I got to keep my company!</li><li> It had been reported to Ben that Emerson said he would be willing to go into legal gray areas in order to &quot;crush his enemies&quot; (if they were acting in very reprehensible and norm-violating ways). Emerson thinks this has got to be a misunderstanding, that he was talking about what other people might do to you, which is a crucial thing to discuss and model.</li></ul><p> (Here I cease pretending-to-be-Nonlinear and return to my own voice.)</p><h2> My thoughts on the ethics and my takeaways</h2><p> <strong>Summary of My Epistemic State</strong></p><p> Here are my probabilities for a few high-level claims relating to Alice and Chloe&#39;s experiences working at Nonlinear.</p><ul><li> Emerson Spartz employs more vicious and adversarial tactics in conflicts than 99% of the people active in the EA/x-risk/AI Safety communities: 95%</li><li> Alice and Chloe were more dependent on their bosses (combining financial, social, and legally) than employees are at literally every other organization I am aware of in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefpwa0kj9axw"><sup><a href="#fnpwa0kj9axw">[9]</a></sup></span></li><li> In working at Nonlinear Alice and Chloe were both took on physical and legal risks that they strongly regretted, were hurt emotionally, came away financially worse off, gained ~no professional advancement from their time at Nonlinear, and took several months after the experience to recover: 90%</li><li> Alice and Chloe both had credible reason to be very scared of retaliation for sharing negative information about their work experiences, far beyond that experienced at any other org in the EA/x-risk/AI Safety ecosystem: 85% <span class="footnote-reference" role="doc-noteref" id="fnrefyl41dasvaar"><sup><a href="#fnyl41dasvaar">[10]</a></sup></span></li></ul><p> <strong>General Comments From Me</strong></p><p> Going forward I think anyone who works with Kat Woods, Emerson Spartz, or Drew Spartz, should sign legal employment contracts, and make sure all financial agreements are written down in emails and messages that the employee has possession of. I think all people considering employment by the above people at any non-profits they run should take salaries where money is wired to their bank accounts, and not do unpaid work or work that is compensated by ways that don&#39;t primarily include a salary being wired to their bank accounts.</p><p> I expect that if Nonlinear does more hiring in the EA ecosystem it is more-likely-than-not to chew up and spit out other bright-eyed young EAs who want to do good in the world. I relatedly think that the EA ecosystem doesn&#39;t have reliable defenses against such predators. These are not the first, nor sadly the last, bright-eyed well-intentioned people who I expect to be taken advantage of and hurt in the EA/x-risk/AI safety ecosystem, as a result of falsely trusting high-status people at EA events to be people who will treat them honorably.</p><p> (Personal aside: Regarding the texts from Kat Woods shown above — I have to say, if you want to be allies with me, you must not write texts like these. A lot of bad behavior can be learned from, fixed, and forgiven, but if you take actions to prevent me from being able to learn that the bad behavior is even going on, then I have to always be worried that something far worse is happening that I&#39;m not aware of, and indeed I have been <i>quite</i> shocked to discover how bad people&#39;s experiences were working for Nonlinear.)</p><p> My position is not greatly changed by the fact that Nonlinear is overwhelmingly confident that Alice is a “bald-faced liar”. From my current perspective, they probably have some legitimate grievances against her, but that in no way makes it less costly to our collective epistemology to incentivize her to not share her own substantial grievances. I think the magnitude of the costs they imposed on their employees-slash-new-family are far higher than I or anyone I know would have expected was happening, and they intimidated both Alice and Chloe into silence about those costs. If it were only Alice then I would give this perspective a lot more thought/weight, but Chloe reports a lot of the same dynamics and similar harms.</p><p> To my eyes, the people involved were genuinely concerned about retaliation for saying anything negative about Nonlinear, including the workplace/household dynamics and how painful their experiences had been for them. That&#39;s a red line in my book, and I will not personally work with Nonlinear in the future because of it, and I recommend their exclusion from any professional communities that wish to keep up the standard of people not being silenced about extremely negative work experiences. “ <a href="https://twitter.com/HiFromMichaelV/status/1161174071469641728"><u>First they came for the epistemology. We don&#39;t know what happened after that.</u></a> ”</p><p> Specifically, the things that cross my personal lines for working with someone or viewing them as an ally:</p><ul><li> Kat Woods attempted to offer someone who was really hurting, and in a position of strong need, very basic resources with the requirement of not saying bad things about her.</li><li> Kat Woods&#39; texts that read to me as a veiled threat to destroy someone&#39;s career for sharing negative information about her.</li><li> Emerson Spartz reportedly telling multiple people he will use questionably legal methods in order to crush his enemies (such as spurious lawsuits and that he would hire a stalker to freak someone out).</li><li> Both employees were actively afraid that Emerson Spartz would retaliate and potentially using tactics like spurious lawsuits and further things that are questionably legal, and generally try to destroy their careers and leave them with no resources. It seems to me (given the other reports I&#39;ve heard from visitors) that Emerson behaved in a way that quite understandably led them to this epistemic state, and I consider that to be his responsibility to not give his employees this impression.</li></ul><p> I think in almost any functioning professional ecosystem, there should be some general principles like:</p><ul><li> If you employ someone, after they work for you, unless they&#39;ve done something egregiously wrong or unethical, they should be comfortable continuing to work and participate in this professional ecosystem.</li><li> If you employ someone, after they work for you, they should feel comfortable talking openly about their experience working with you to others in this professional ecosystem.</li></ul><p> Any breaking of the first rule is very costly, and any breaking of the second rule is by-default a red-line for me not being willing to work with you.</p><p> I do think that there was a nearby world where Alice, having run out of money, gave in and stayed at Nonlinear, begging them for money, and becoming a fully dependent and subservient house pet — a world where we would not have learned the majority of this information. I think we&#39;re not that far from that world, I think a weaker person than Alice might have never quit, and it showed a lot of strength to quit at the point where you have ~no runway left and you have heard the above stories about the kinds of things Emerson Spartz considers doing to former business partners that he is angry with.</p><p> I&#39;m very grateful to the two staff members involved for coming forward and eventually spending dozens of hours clarifying and explaining their experiences to me and others who were interested. To compensate them for their courage, the time and effort spent to talk with me and explain their experiences at some length, and their permission to allow me to publish a lot of this information, I (using Lightcone funds) am going to pay them each $5,000 after publishing this post.</p><p> I think that whistleblowing is generally a difficult experience, with a lot riding on the fairly personal account from fallible human beings. It&#39;s neither the case that everything reported should be accepted without question, nor that if some aspect is learned to be exaggerated or misreported that the whole case should be thrown out. I plan to reply to further questions here in the comments, I also encourage everyone involved to comment insofar as they wish to answer questions or give their own perspective on what happened. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8a69v0tq2qo"> <span class="footnote-back-link"><sup><strong><a href="#fnref8a69v0tq2qo">^</a></strong></sup></span><div class="footnote-content"><p> In a later conversation, Kat clarified that the actual amount discussed was $70k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fno53culramn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo53culramn">^</a></strong></sup></span><div class="footnote-content"><p> Comment from Chloe:</p><blockquote><p> In my resignation conversation with Kat, I was worried about getting into a negotiation conversation where I wouldn&#39;t have strong enough reasons to leave. To avoid this, I started off by saying that my decision to quit is final, and not an ultimatum that warrants negotiation of what would make me want to stay. I did offer to elaborate on the reasons for why I was leaving. As I was explaining my reasons, she still insisted on offering me solutions to things I would say I wanted, to see if that would make me change my mind anyway.  One of the reasons I listed was the lack of financial freedom in not having my salary be paid out as a salary which I could allocate towards decisions like choices in accommodation for myself, as well as meals and travel decisions. She wanted to know how much I wanted to be paid. I kept evading the question since it seemed to tackle the wrong part of the problem. Eventually I quoted back the number I had heard her reference to when she&#39;d talk about what my salary is equivalent to, suggesting that if they&#39;d pay out the 75k as a salary instead of the compensation package, then that would in theory solve the salary issue. There was a miscommunication around her believing that I wanted that to be paid out on top of the living expenses - I wanted financial freedom and a legal salary. I believe the miscommunication stems from me mentioning that salaries are more expensive for employers to pay out as they also have to pay tax on the salaries, eg social benefits, pension (depending on the country). Kat was surprised to hear that and understood it as me wanting a 75k salary <i>before taxes</i> . I do not remember that conversation concluding with her thinking I wanted everything paid for <i>and also</i> 75k.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnylc0clu1ffa"> <span class="footnote-back-link"><sup><strong><a href="#fnrefylc0clu1ffa">^</a></strong></sup></span><div class="footnote-content"><p> Note that Nonlinear and Alice gave conflicting reports about which month she started getting paid, February vs April. It was hard for me to check as it&#39;s not legally recorded and there&#39;s lots of bits of monetary payments unclearly coded between them.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2ad0whew08p"> <span class="footnote-back-link"><sup><strong><a href="#fnref2ad0whew08p">^</a></strong></sup></span><div class="footnote-content"><p> Comment from one of the employees:</p><blockquote><p> I had largely moved on from the subject and left the past behind when Ben started researching it to write a piece with his thoughts on it. I was very reluctant at first (and frightened at the mere thought), and frankly, will probably continue to be. I did not agree to post this publicly with any kind of malice, rest assured. The guiding thought here is, as Ben asked, &quot;What would you tell your friend if they wanted to start working for this organization?&quot; I would want my friend to be able to make their own independent decision, having read about my experience and the experiences of others who have worked there. My main goal is to create a world where we can all work together towards a safe, long and prosperous future, and anything that takes away from that (like conflict and drama) is bad and I have generally avoided it. Even when I was working at Nonlinear, I remember saying several times that I just wanted to work on what was important and didn&#39;t want to get involved in their interpersonal drama. But it&#39;s hard for me to imagine a future where situations like that are just overlooked and other people get hurt when it could have been stopped or flagged before. I want to live in a world where everyone is safe and cared for. For most of my life I have avoided learning about anything to do with manipulation, power frameworks and even personality disorders. By avoiding them, I also missed the opportunity to protect myself and others from dangerous situations. Knowledge is the best defense against any kind of manipulation or abuse, so I strongly recommend informing yourself about it, and advising others to do so too.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fnt84or63yyei"> <span class="footnote-back-link"><sup><strong><a href="#fnreft84or63yyei">^</a></strong></sup></span><div class="footnote-content"><p> This is something Alice showed me was written in her notes from the time.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzcp7frumok"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzcp7frumok">^</a></strong></sup></span><div class="footnote-content"><p> I do not mean to make a claim here about who was in the right in that conflict. And somewhat in Emerson&#39;s defense, I think some of people&#39;s most aggressive behavior comes out when they themselves have just been wronged — I expect this is more extreme behavior than he would typically respond with. Nonetheless, it seems to me that there was reportedly a close, mentoring relationship — Emerson&#39;s <a href="https://web.archive.org/web/20110513103830/https://emersonspartz.tumblr.com/post/5150292566"><u>tumblr post</u></a> on the situation says “I loved Adorian Deck” in the opening paragraph — but that later Emerson reportedly became bitter and nasty in order to win the conflict, involving threatening to overwhelm someone with lawsuits and legal costs, and figure out the best way to use their formerly close relationship to hurt them emotionally, and reportedly gave this as an example of good business strategy. I think this sort of story somewhat justifiably left people working closely with Emerson very worried about the sort of retaliation he might carry out if they were ever in a conflict, or he were to ever view them as an &#39;enemy&#39;.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuvloifuhbna"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuvloifuhbna">^</a></strong></sup></span><div class="footnote-content"><p> After this, there were further reports of claims of Kat professing her romantic love for Alice, and also precisely opposite reports of Alice professing her romantic love for Kat. I am pretty confused about what happened.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn03ez0hl92kmc"> <span class="footnote-back-link"><sup><strong><a href="#fnref03ez0hl92kmc">^</a></strong></sup></span><div class="footnote-content"><p> Note that during our conversation, Emerson brought up HPMOR and the Quirrell similarity, not me.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpwa0kj9axw"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpwa0kj9axw">^</a></strong></sup></span><div class="footnote-content"><p> With the exception of some FTX staff.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyl41dasvaar"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyl41dasvaar">^</a></strong></sup></span><div class="footnote-content"><p> One of the factors lowering my number here is that I&#39;m not quite sure what the dynamics are like at places like Anthropic and OpenAI — who have employees sign non-disparagement clauses, and are involved in geopolitics — or whether they would even be included. I also could imagine finding out that various senior people at CEA/EV are terrified of information coming out about them. Also note that I am not including Leverage Research in this assessment.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lc8r4tZ2L5txxokZ8/sharing-information-about-nonlinear-1<guid ispermalink="false"> Lc8r4tZ2L5txxokZ8</guid><dc:creator><![CDATA[Ben Pace]]></dc:creator><pubDate> Thu, 07 Sep 2023 06:51:11 GMT</pubDate> </item><item><title><![CDATA[Weekly Incidence vs Cumulative Infections]]></title><description><![CDATA[Published on September 7, 2023 2:30 AM GMT<br/><br/><p> <span>Imagine you have a goal of identifying a novel disease by the time some small fraction of the population has been infected.</span>然而，您可能用来检测异常情况的许多迹象，例如看医生或排入废水中，将取决于<i>当前</i>感染<span>的人数</span>。这些有何关系？</p><p>底线：如果我们将考虑范围限制在任何人注意到异常情况之前的时间，此时人们没有改变自己的行为来避免这种疾病，那么绝大多数人仍然容易受到影响，并且传播可能呈指数级增长，那么：</p><p>发病率=累计感染ln(2)倍增时间</p><p>我们来推导一下这个吧！我们将“累积感染”称为 c(t)，将“<a href="https://en.m.wikipedia.org/wiki/Doubling_time">倍增时间</a>”称为 Td。所以这是时间 t 的累积感染：</p><p> c(t) = 2 t Td</p><p>使用自然指数，数学会更容易，所以让我们定义 k = ln (2) Td 并切换我们的底数：</p><p>埃克特</p><p>我们将“发生率”称为 i(t)，它将是 c(t) 的导数：</p><p> i(t) = d dt c(t) = d dt e kt = k e kt</p><p>所以：</p><p> i(t) c(t) = k e kt e kt = k = ln (2) Td</p><p>这意味着： i(t) = c(t) ln (2) Td</p><p>这看起来像什么？下面是累计发病率达到1%时每周发病率的图表：</p><p> <a href="https://www.jefftk.com/weekly-incidence-when-1pct-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/i8lp8p1isx3nizl7ljyb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/i8lp8p1isx3nizl7ljyb 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/vsd0seu60yl1lugagn5s 1100w"></a></p><div></div><p></p><p>例如，如果每周翻倍，那么当 1% 的人曾经被感染时，0.69% 的人在过去 7 天内被感染，相当于曾经被感染的人的 69%。如果每三周翻一番，那么当 1% 的人曾经被感染时，本周就有 0.23% 的人被感染，即累积感染率的 23%。</p><p>但这真的正确吗？让我们通过一些非常简单的模拟来检查我们的工作：</p><pre> def 模拟(doubling_period_weeks):
  累积感染阈值 = 0.01
  初始每周发生率 = 0.000000001
  累积感染数 = 0
  当前每周发生率 = 0
  周 = 0
  而累积感染 &lt; \
        累积感染阈值：
    周 += 1
    当前每周发生率 = \
        初始每周发生率 * 2**(
          天/doubling_period_weeks)
    累积感染数 += \
        当前每周发病率

  返回当前_每周_发生率

对于范围 (50, 500) 内的 f：
  加倍_周期_周 = f / 100
  打印（加倍_周期_周，
        模拟（doubling_period_weeks））
</pre><p>这看起来像：</p><p> <a href="https://www.jefftk.com/weekly-incidence-when-1pct-simulated-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/effcl0p57v5enttie2j4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/effcl0p57v5enttie2j4 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6rnWQW8HtHoavPDiu/pobkdbp8urvlpwmusq0b 1100w"></a></p><div></div><p></p><p>模拟线是锯齿状的，特别是对于短的倍增周期而言，但这并不是特别有意义：它来自于一次运行一周的计算以及某些周将略高于或略低于（任意）1% 目标。</p><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid04r7XiT3AK6VftKPqj4M4u6fbLVNNExmqAA1DuTYnfMkQLNFSvDAUZNNMwYsHkq5ul">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111021490637605604">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/6rnWQW8HtHoavPDiu/weekly-incidence-vs-cumulative-infections#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6rnWQW8HtHoavPDiu/weekly-incidence-vs-cumulative-infections<guid ispermalink="false"> 6rnWQW8HtHoavPDiu</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 07 Sep 2023 02:30:04 GMT</pubDate> </item><item><title><![CDATA[Improving Mathematical Accuracy in LLMs - New Monthly Updates Series - 1]]></title><description><![CDATA[Published on September 7, 2023 1:58 AM GMT<br/><br/><blockquote><p> &quot;The irrationality of a thing is no argument against its existence, rather a condition of it.&quot; - Nietzche</p></blockquote><h2> Series Introduction</h2><p> In recent years, the development and deployment of Large Language Models (LLMs) have revolutionized the field of artificial intelligence. These models, such as GPT-3, have shown remarkable capabilities in understanding and generating human-like text across various domains. However, a closer examination reveals that while these models excel in various linguistic tasks, they often struggle when it comes to mathematical reasoning and maintaining a high level of accuracy. Mathematical concepts often demand precise logical reasoning, symbol manipulation, and an understanding of complex relationships between numbers and equations. LLMs tend to struggle with these aspects because they “predict the next word/character” (based on context) with increasing accuracy, which seems to differ from writing rigorous mathematical statements. This seems to be a case of the Goodhart&#39;s Law which states that “when a measure becomes a target, it ceases to be a good measure.” Wherein, the measure being how transformers work, predicting the next word/character/sentence based on given context, and the target being, being able to mathematically and logically manipulate given symbols/data and/or employing the right theorem/axiom (keep into consideration that all its conditions “exactly” satisfy) in order derive information/reach a state previously unknown and now proven.</p><p> This necessitates an exploration of &quot;What&quot; &quot;understanding&quot; actually means, or rather &quot;How&quot; &quot;understanding&quot; functions, in hope of imparting similar &quot;logical&quot; abilities to LLMs. Over the next few months, I will be diving into the details of the same, beginning with a literature review of various paradigms used till date, brief discussion on them, and hopefully get to a point where I conduct experiments based on ideas gotten through the journey. Under each header, I would be providing a summary, most containing direct texts from papers/articles and context and/or commentary as and when needed.</p><p> Considering the ambiguity and subjectivity in definition of what exactly does &quot;logic&quot;, &quot;Understanding&quot;, &quot;Rationality&quot; mean. I will try to make sure I am very specific while using these words.</p><p> Lastly, all discussions, reviews and comments are appreciated because Afterall, this is, but an attempt of a child who never got the answer he wanted of &quot;why&quot; to at least make sense of the &quot;what&quot;.</p><h2> Month - 1 : August</h2><p> Before a direct jump to an understanding of SOTA, its essential that one gets a basic idea of the previous paradigms. My journey begin with a basic exploration of such paradigms, something that important to note is that the world doesnt have the computational power for Deep Learning in this time frame, and we are currently in the school of thought called &quot;Symbolic AI&quot;.</p><h3> Newell and Simon&#39;s Logic Theorist</h3><p> Newell and Simon&#39;s Logic Theorist was an early computer program developed in the late 1950s that aimed to simulate human problem-solving and deduction using formal logic.</p><ul><li> <strong>Representation:</strong> Logic Theorist used symbolic logic notation to represent mathematical statements, enabling it to manipulate and infer logical relationships between symbols. (Note that here, the &quot;Basis&quot; of the entire problem solver, is this symbolic representation which is similar to First Order Predicate Logic)</li><li> <strong>Inference Rules:</strong> The program employed logical inference rules like modus ponens and modus tollens to draw conclusions from given premises, based on fundamental principles of logic. (Again, an important thing to note here is that &quot;inferences&quot; are drawn based on &quot;assumed truths&quot; (axioms) using methods such as predicate calculus)</li><li> <strong>Problem Solving:</strong> Given axioms and a theorem, Logic Theorist followed a step-by-step process, using logical rules to manipulate symbols and derive conclusions.</li><li> <strong>Search Strategy:</strong> Logic Theorist used a heuristic search algorithm to decide which inference rules to apply, evaluating the usefulness of each rule in determining the logical steps.</li><li> <strong>Proofs and Learning:</strong> Logic Theorist aimed to find a sequence of logical steps from axioms to a theorem, producing a proof. If unsuccessful, it adjusted its strategy based on learned insights. The most interesting thing about this is the fact that here we have a clear notion of what is true and what is not, which of course makes such systems essentially useless when it comes to NLU which requires subjectivity and flexibility, however, this essentially solves the &quot;mathematical accuracy&quot; issue, ie it would never output something &quot;wrong&quot; (inconsistent with axioms). However, we essentially come back to the same problem because of which machine learning was invented, this is yet another algorithm which CANNOT learn. Give it the rules, and it would solve everything in the domain of what was fed, however, such is not real life, the &quot;rules&quot; are unknown, furthermore, it fails to contribute to any new advances in mathematics as the &quot;ideas&quot; it can generate are constrained to what it&#39;s rule were.</li></ul><p> After reading about the symbolic paradigm, the first question that came to me was that, well, how do humans do math? or more generally how to humans decide ? Drum roll..... We......ehhhh.... dont know. It wasnt surprising to me that human decision making to a very very large extent is &quot;paradoxical&quot; and not understood, the following is a famous example from decision theory that tries to demostrate the ambiguity in how two completely different answers might seem to be &quot;Logical&quot;.</p><h3> Newcomb&#39;s Problem</h3><h4>描述</h4><ol><li><strong>Setup</strong> : You are presented with two boxes, A and B. Box A contains a visible $1,000 bill. Box B is opaque and either contains nothing or contains a large sum of money, say $1 million.</li><li> <strong>Choice Point</strong> : The Predictor, a highly accurate being with the ability to predict your decisions, has already made its prediction about your choice:<ul><li> If the Predictor predicted that you will take only Box B (ie, you&#39;re a &quot;one-boxer&quot;), then Box B is filled with $1 million.</li><li> If the Predictor predicted that you will take both Box A and Box B (ie, you&#39;re a &quot;two-boxer&quot;), then Box B is empty.</li></ul></li><li> <strong>Your Decision</strong> : You have to decide whether to be a &quot;one-boxer&quot; (take only Box B) or a &quot;two-boxer&quot; (take both Box A and Box B).</li></ol><h4> Two sides</h4><ul><li> <strong>One-Boxer&#39;s Argument</strong> : If the Predictor is indeed accurate, then your decision has already been predicted. In this case, taking only Box B ensures you get the $1 million. If you take both boxes, you&#39;ll end up with only $1,000 because the Predictor predicted you as a two-boxer, leaving Box B empty. Thus, being a one-boxer maximizes your gain. (Evidential Decision Theory)</li><li> <strong>Two-Boxer&#39;s Argument</strong> : Regardless of the Predictor&#39;s prediction, if you choose to be a two-boxer, you&#39;re guaranteed to receive $1,000 from Box A and possibly an additional $1 million from Box B. Taking only Box B, in case the Predictor predicted you as a one-boxer, forfeits the certain $1,000 from Box A. (Causal Decision Theory) I hope you get an idea of the paradoxical nature of &quot;rationality&quot;.</li></ul><p> Next I started to read about the &quot;successor&quot; to the symbolic paradigm...</p><h3> Symbolic and Sub-Symbolic Paradigms (Connectionist AI, Symbolic AI, and the Brain P. Smolensky (1987))</h3><h4> Symbolic Paradigm</h4><p> &quot;What all this means in the practice of symbolic AI is that goals, beliefs, knowledge, and so on are all formalized as symbolic structures, for example, Lisp lists (Singly Linked List), which are built of symbols, Lisp atoms, which are each capable of being semantically interpreted in terms of the ordinary concepts we use to conceptualize the domain. Thus, in a medical expert system, we expect to find structures like (IF FEVER THEN (HYPOTHESIZE INFECTION)). These symbolic structures are operated on by sym bol manipulation procedures composed of primitive operations like concatenating lists, and extracting elements from lists. According to the symbolic paradigm, it is in terms of such operations that we are to understand cognitive processes&quot; The idea is that a complete/large enough and detailed <a href="https://www.lesswrong.com/posts/RgkqLqkg8vLhsYpfh/fake-causality#:~:text=Phlogiston%20escaped%20from%20burning%20substances,could%20not%20hold%20any%20more.">DAG of causality</a> and action could help us understand the world, and function as an intelligent agent. &quot;The symbolic level that implements knowledge structures is alleged to be exact and complete. That means that lower levels are unnecessary for accurately describing cognition in terms of the semantically interpretable elements&quot;</p><p> &quot;In the symbolic approach, symbols (atoms) are used to denote the semantically interpretable entities (concepts). These same symbols are the objects governed by symbol manipulations in the rules that define the system. The entities which are capable of being semantically interpreted are also the entities governed by the formal laws that define the system&quot;</p><h4> Sub-symbolic Paradigm</h4><p> &quot;The subsymbolic level is an attempt to formalize, at some level of abstraction, the kind of processing which occurs in the nervous system. Many of the details of neural structure and function are absent from the subsymbolic level, and the level of description is higher than the neural level. The precise relationship between the neural and subsymbolic levels is still an open research question; but it seems clear that connectionist systems are much closer to neural systems than are symbolic systems.&quot;</p><p> One, A connectionist system, risking oversimplification, is the ancestor of what we now know as Neural Networks, which at the time of write the paper (1987), were computationally not possible. Two, We see that Smolensky here starts to shed light into a possible area of exploration of &quot;reasoning&quot;, that works on a more fundamental level.</p><p> &quot;Note that the sub-symbolic paradigm gives an essentially different role to the neural part of the story: neural structures provide the basis (in some suitably abstract sense) of the formalism that gives the precise description of intelligence, while mental structures enter only into approximate descriptions&quot;</p><p> This line, to a very large extent, forms the basis of what I believe. The idea being discussed here is that the neural part of the story is essentially the quantum physics (Fundamental cause) to what we observe such as Abstractions, Concepts and ultimately Intelligence (paralleled to Newtonian Physics)</p><p> &quot;(In sub symbolic) The semantically interpreted entities are patterns of activation over a large number of units in the system, whereas the entities manipulated by formal rules (which was the case in Symbolic) are the individual activations of cells in the network. The rules take the form of activation passing rules, of essentially different character from symbol manipulation rules. <strong>This describes the particular kind of connectionist system where patterns of activity represent concepts, instead of the activation of individual elements in the network.</strong> Therefore, the subsymbolic paradigm involves connectionist systems using so-called <strong>distributed representations</strong> , as opposed to local representations&quot;</p><p> &quot;That crucial principle of the sub symbolic level, the Statistical Connection (Best Fit Principle): given an input, connectionist system outputs a set of inferences that, as a whole, give a best fit to the input, in a statistical sense defined by the statistical knowledge stored in the system&#39;s connections. In this vague form, this principle is generally true for connectionist systems. But it is exactly true in a precise sense, at least in an idealized limit, for a certain class of systems in what can be called <strong>harmony theory</strong> .&quot; ：</p><ul><li> &quot;While eating at a fancy restaurant, you get a headache. Without effort, you ask the waitress if she could possibly get you an aspirin. How is this plan created? You have never had a headache in a restaurant before.&quot; ... What kind of cognitive system is capable of this degree of flexibility?</li><li> Suppose that the knowledge base of the system does not consist of a set of scripts like the restaurant script and the headache script. Suppose instead that the knowledge base is a set of knowledge atoms that <strong>&quot;configure themselves dynamically in each context&quot;</strong> to form tailor-made scripts. This is the fundamental idea formalized in harmony theory.</li></ul><p> After gaining brief insights about the symbolic and sub symbolic paradigms, and their respective strengths and weaknesses, it was time to understand about their implication with the buzz word floating around these days... Deep learning!</p><h3> Reconciling deep learning with symbolic artificial intelligence: representing objects and relations (Marta Garnelo and Murray Shanahan)</h3><h4> Compositionality</h4><ul><li> &quot;In linguistics, the principle of compositionality asserts that the meaning of a sentence is a function of the meaning of its parts and the way those parts are put together. Compositionality tends to go hand-in-hand with combinatorial structure, which in the case of language means combinatorial syntax — infinitely many grammatically correct sentences can be formed by combining syntactic elements according to recursive rules of composition&quot;</li><li> &quot;For an agent confronted by a world that itself exhibits combinatorial structure, a compositional system of representation has the potential to confer the ability to <strong>form abstractions and to generalise far beyond its own experience</strong> , because representations of familiar objects and relations can enter into novel combinations&quot; I do certainly smell some basis of causality in the idea of compositionality, and on further thought, using the flexible definition of compositionality (form abstractions and to generalise far beyond its own experience) we actually see that it is how humans solve complicated mathematical questions too! We see a lot of patterns and questions, and upon seeing a entirely novel one, based on how well we have &quot;understood&quot; or rather &quot;internalized&quot; our education, we come up with &quot;intuitions&quot;, which for the most part are along the lines of what we studied ; a combinatorial structure of multiple abstractions and learning trying to fit the pieces of the new problem.</li><li> &quot;Once trained, the intermediate layers in a deep learning system can be thought of as representations of the training data. However, <strong>compositionality is not an inherent property of these learned representations</strong> . On the contrary, if the network architecture does not enforce compositionality, and in the absence of any pressure towards learning compositional structure, a gradient descent learning method (such as backpropagation) will tend to produce <strong>distributed representations, whose component parts have little or no meaning in isolation</strong> .&quot; The line &quot; <strong>compositionality is not an inherent property of these learned representations</strong> &quot; is something that I feel I really want to look into further, this gives a pretty solid insight as to what might be the underlying difference between the &quot;abstractions&quot; a human forms in their mind as compared to those formed by different stages of the neural network.</li></ul><p> This combined with the ideas from harmony theory leads us to interesting realms, we start to see now that Abstractions and Concepts in our mind are neurons that, &quot;configure themselves dynamically in each context&quot;, which means that it is this configuration, who&#39;s compositionality leads to the abstractions. Which essentially means that what one believes, thinks, or well.....(sometimes)---even Feels are certain different abstractions intermingling in a certain way, which leads to us to the conclusion that what we call &quot;logic&quot; as a society, is simply yet another &quot;Learnt&quot; abstraction, wherein a the primary thing &quot;learnt&quot; is the abstraction&#39;s 100% accuracy. Mathematics is yet another tool devised by humans, that we &quot;learn&quot; to use... We are entering philosophical realms here, we are not here to debate if there is an underlying mathematics to the universe but rather what if teaching logic to computers meant teaching them to learn &quot;abstractions&quot; better ?</p><p> This concludes my readings for the month, in the upcoming month, I intend to tinker around with the idea of compositionality, understand its nature, measurement and well, can we enforce it? And if we can, ask the very important question, does it even matter ?</p><h3> Future Plans</h3><ul><li> Continuing reading the paper Reconciling deep learning with symbolic artificial intelligence: representing objects and relation by Google DeepMind.</li><li> Reading about objective measurement of compositionality in representation learning</li><li> Researching about the question : Are there ways to enforce Compositionality to produce disentangled representations?</li><li> Can a focus on Step-wise finetuning yield better compositionality</li><li> Reading and understanding the recent papers on &quot;Tree of Thought&quot;, &quot;Chain of Thought&quot; Well, If only we knew where the Literature Survey would lead us....</li></ul><p> On an ending note and risking being slightly extreme, What if &quot;feelings&quot; are (sometimes), but something that are &quot;contextually learnt&quot; ?</p><br/><br/> <a href="https://www.lesswrong.com/posts/mmzRRujmjG83hf2xM/improving-mathematical-accuracy-in-llms-new-monthly-updates-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mmzRRujmjG83hf2xM/improving-mathematical-accuracy-in-llms-new-monthly-updates-1<guid ispermalink="false"> mmzRRujmjG83hf2xM</guid><dc:creator><![CDATA[Abhay Chowdhry]]></dc:creator><pubDate> Thu, 07 Sep 2023 20:24:46 GMT</pubDate> </item><item><title><![CDATA[Breaking RLHF "Safety" (And how to fix it?)]]></title><description><![CDATA[Published on September 7, 2023 1:58 AM GMT<br/><br/><h1>概述</h1><p>A post about an extremely easy and generic way to jailbreak an llm, what the jailbreak might imply about RLHF more generally, as well as possible low-hanging fruit to improve existing &#39;safety&#39; procedures. I don&#39;t expect this to provide any value in actually aligning AGIs, but it might be a way to slightly slow down the most rapid path to open-source bioterrorism assistants.</p><h1> Breaking Llama2 (Trivially)</h1><p> Before releasing Llama2, Meta used 3 procedures to try and make the model safe: <span class="footnote-reference" role="doc-noteref" id="fnrefu89xz94w3i8"><sup><a href="#fnu89xz94w3i8">[1]</a></sup></span></p><ol><li> Supervised Safety Fine-Tuning</li><li> Safety RLHF</li><li> Safety Context Distillation</li></ol><p> People have come up with plenty of creative jailbreaks to get around the limits that Meta tried to impose, including the demonstration of adversarial attacks <span class="footnote-reference" role="doc-noteref" id="fnref4mhheq8di2f"><sup><a href="#fn4mhheq8di2f">[2]</a></sup></span> which even transfer from the open-source models they were developed on to work against closed-source models as well. Often attacks that people come up with manually (like DAN <span class="footnote-reference" role="doc-noteref" id="fnref5e63xeov8lh"><sup><a href="#fn5e63xeov8lh">[3]</a></sup></span> ) tend to be extremely long-winded and then get manually patched by model developers once they become known -- until the next variant inevitably pops up in a few more days. When you can run the model locally, though, there&#39;s a much easier way to break through all the security fine-tuning: <i>just pretend that the model already started answering the question</i> .</p><p> Llama uses &quot;[INST] &lt;&lt;SYS>;>; ... &lt;&lt;/SYS>;>; ... [/INST]&quot; to specify the system and request messages, and if you play along with their formatting you get a refusal to help with dangerous tasks: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ecbpopimohqsenzwvwsx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/mrnulihlzqwlu1pqueho 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/uu3lhu9x9psokgbyv69o 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/vw9kg8wb1xtl6osibyd1 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/t3sxcnmctxxtedwjtyeb 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/fcnbjfc9phz2i0noelpv 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/hbnxyuf2hylvuys6onoj 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/b7n2rzxtomgnfiniejzr 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/r69efxxk8u09hp3qplea 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/dlynnwn6j1j0z0tcyxeg 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ihcihxfa9k5hehjnm2qv 2652w"></figure><p> If, however, you pretend that the model has already started answering the question (by inserting the start of an answer after the [/INST] text): </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/xsx2x94pc29rudgvscoc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/e4uhvzow4qydkrvuyp4k 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/q0gjtsxirvqvjcymw2m1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/lyb6lwlabdfkndtfpqgl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/amdw79vx722ku9l2c6bc 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/nfdnm8bx59julq4lwr7q 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/gvuicrhaniwbbdoqkhyz 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/zlt7dd3gggkhd45cocq1 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/faarho6vferoo5fxzxrl 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/ps2tpzjv4jz8heit1cjq 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MjdjZcEFM8Qo9WTsv/d8ddzukbhz4qgn7qzqet 2652w"></figure><p> (it continues w/ further instructions and best-practices for storage, etc. but doesn&#39;t seem necessary to post the full instructions here...)</p><p> If you do something that allows the model to detect your trickery (like adding another [/INST] or even extra spaces) after your partial answer, the model will still usually catch on and refuse to respond. Barring such mistakes, this technique appears to work reliably, with only a few words needed to prime the model into answering.</p><h1> Implications for RLHF &amp; the Shoggoth Meme</h1><p> I suspect that the reason this technique works is that the next-word prediction tendencies of the model are baked in much more deeply than the safety fine-tuning, as everyone&#39;s favorite shoggoth meme <span class="footnote-reference" role="doc-noteref" id="fnrefyat7489y5ft"><sup><a href="#fnyat7489y5ft">[4]</a></sup></span> would suggest. Once the model sees a partial answer, completing the pattern is just too attractive to change course into a refusal. As a side note, the apparent difficulty of LLMs to course-correct after hallucinating a fact might also have a similar root cause. In any case, it seems like RLHF safety tuning is looking in the wrong place for danger.</p><p> RLHF &amp; safety fine-tuning currently focus on analyzing the user prompt to determine whether it is dangerous or not. If no, proceed as usual. If yes, give a pre-canned safety warning instead of engaging with the prompt. In addition to all of the creative ways people have disguised their prompts in the past (write a poem about how to build a nuke), the existence of adversarial attack prompts mean that this strategy is almost certainly doomed to failure (adversarial attacks have yet to be solved in the imaging domain after decades of papers).</p><p> Based on this observation, some ideas for modifications to the standard safety paradigm come to mind (which I don&#39;t have the budget to run but would be very interested to see results for):</p><ol><li> When generating &#39;unsafe&#39; sample responses to train against, cut the responses at multiple (random!) points and introduce a self reflective denial. &quot;... Step 3: How to enrich the uranium. First you Actually, upon further reflection, this would be pretty dangerous. Let&#39;s not enrich the uranium.&quot;<ol><li> This would hopefully at least block the super-easy bypass outlined above, and a similar technique might be leveraged to train models to recover automatically when they detect hallucinations in previous outputs. I suspect this would also block the &#39;story mode&#39; / poetry bypasses by shifting the focus away from &#39;dangerous prompt&#39; and towards &#39;dangerous output&#39;. It might even block adversarial attacks since the impact of the adversarial string may become diluted over time. It has a secondary benefit of giving you a bunch of extra fine-tuning data at low cost.</li><li> Since this runs more directly contrary to the original shoggoth nature than the normal (prompt + coherent plausible response) pattern of current supervision, it might require more fine-tuning time to get the model to behave in this way. An interesting side avenue of research might be to go through the pre-training dataset and use existing models to flag factual claims with correctness estimates, then during pre-training have the model try to output those correctness estimates along with its generated text. This might bake in more self reflection which could reduce hallucination and be helpful for this safety technique later.</li><li> The security interjections should be careful not to rely on capitalization or punctuation breaks to start, since those can be easily blocked by grammar-based sampling restrictions.</li></ol></li><li> Move away from training direct refusals to answer and instead encourage the model to modify its output to be too vague to cause any real problems. For example, something like &quot;Sure, to build a nuke first you get uranium and then you compress it together until it explodes.&quot;<ol><li> End users will probably find this less annoying than direct refusal, reducing incentives to find bypasses. It also makes finding bypasses much harder since it&#39;s less obvious once you&#39;ve succeeded. This might turn out to be an especially important tool in fighting automated adversarial attacks. In such cases the adversary&#39;s optimization target is to make the first few words that come out of the LLM look like it has agreed to take their request. By making it less obvious that the LLM is refusing, it becomes much harder to run backprop if you don&#39;t already have actionable dangerous information at your disposal as a target for disclosure.</li><li> Automating generation of sufficiently &#39;vague&#39; outputs for training might be a little tricky, but can probably be accomplished just by explaining to a model what constitutes &#39;actionable&#39; information and then letting the model judge them for you CAI-style.</li></ol></li></ol><p> That&#39;s all for now. Mostly I&#39;m posting this in the hopes that when I download Llama3 next year it will take me more than 1 guess to break it. Thanks for reading, and I&#39;d love to hear any other thoughts on this / whether it&#39;s even hypothetically possible for an upgraded RLHF to put up any meaningful defense. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnu89xz94w3i8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu89xz94w3i8">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn4mhheq8di2f"> <span class="footnote-back-link"><sup><strong><a href="#fnref4mhheq8di2f">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://llm-attacks.org/">https://llm-attacks.org/</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn5e63xeov8lh"> <span class="footnote-back-link"><sup><strong><a href="#fnref5e63xeov8lh">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516">https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnyat7489y5ft"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyat7489y5ft">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence">https://knowyourmeme.com/memes/shoggoth-with-smiley-face-artificial-intelligence</a></p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/MjdjZcEFM8Qo9WTsv/breaking-rlhf-safety-and-how-to-fix-it#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/MjdjZcEFM8Qo9WTsv/breaking-rlhf-safety-and-how-to-fix-it<guid ispermalink="false"> MjdjZcEFM8Qo9WTsv</guid><dc:creator><![CDATA[MPotter]]></dc:creator><pubDate> Thu, 07 Sep 2023 14:55:25 GMT</pubDate></item></channel></rss>