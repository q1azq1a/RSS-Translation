<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 13 日星期一 14:12:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Bostrom Goes Unheard]]></title><description><![CDATA[Published on November 13, 2023 2:11 PM GMT<br/><br/><p> [编者注：这篇文章是从 AI #38 中分离出来的，只在 LessWrong 上发布，因为我想避免此时让普通读者超载这类事情，而且我认为我们有一个可用的链接可能很重要。我计划从那里链接到它并附上一个简短的摘要。]</p><p> <a href="https://www.youtube.com/watch?v=_Oo-m893-xA&amp;ab_channel=UnHerd">Nick Bostrom 在 UnHerd 上接受了各种问题的采访</a>，主要是关于存在风险和人工智能，我发现它自始至终都很深思熟虑。在其中，他用了前 80% 的时间谈论存在风险。然后在最后 20% 中，他表达了这样的担忧：我们不太可能但有可能会过度关注人工智能，而根本不会构建 AGI，这将是一场悲剧。</p><p>那些拒绝人工智能风险并尽快构建通用人工智能的人有何反应？</p><p>关于你的期望。 <a href="https://marginalrevolution.com/marginalrevolution/2023/11/saturday-assorted-links-431.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=saturday-assorted-links-431">这是</a>来自边际革命链接帖子。</p><blockquote><p>泰勒·考恩： <a href="https://feeds.feedblitz.com/~/t/0/0/marginalrevolution/~https://twitter.com/jachaseyoung/status/1723325057056010680?t=TuS7aSrf5HrJG6aDzuRonw&amp;s=19">尼克·博斯特罗姆不再是反基督者。</a></p></blockquote><p>该帖子中的下一个链接是 Rohit Krishnan 的关于 AI 的书的 GPT 版本，题为<a href="https://chat.openai.com/g/g-5bdQ4ysnG-building-god">《创造上帝</a>》（ <a href="https://manifold.markets/ZviMowshowitz/would-it-be-a-good-use-of-time-to-r-2c7879e6d9c6">我应该读它吗</a>？）。</p><p>到底改变了什么？泰勒链接到乔丹·蔡斯·杨的一条扩展推文，其中大部分是视频的文字记录，并附有简短的介绍。</p><blockquote><p> Jordan Chase-Young：最后：<a href="https://www.youtube.com/watch?v=_Oo-m893-xA&amp;pp=ygUUbmljayBib3N0cm9tIHVuaGVhcmQ%3D">人工智能 x 风险者尼克·博斯特罗姆 (Nick Bostrom) 后悔关注人工智能风险</a>，现在担心我们恐惧的从众心态会驱使我们粉碎人工智能并摧毁我们未来的潜力。 （来自今天的 UnHerd 播客）。</p></blockquote><p>换句话说，尼克·博斯特罗姆之前关注的是人工智能可能杀死所有人的事实，认为这实际上很糟糕，并试图阻止它。但现在有人声称博斯特罗姆对此感到后悔——他悔改了。</p><p>背景是，彼得·蒂尔警告说，那些关于存在风险的警告已经疯了，他此前曾多次在多个场合将尼克·博斯特罗姆称为“反基督者”，这似乎没有讽刺意味。那么也许现在彼得和其他同意的人会改变他们的观点？事实上，有很多“我们中的一个”的谈话。</p><p>那些警告人工智能存在风险的人经常被告知，他们在说一些宗教的话，是邪教的一部分，或者与基督教启示录的模式相匹配，通常作为不加争论地驳回我们的担忧的理由。</p><p><a href="https://sergey.substack.com/p/things-hidden-at-novitate-2023/comment/43103526">最近证明这一规则的另一方的例外是伯恩·霍巴特（Byrne Hobart）</a> ，他是优秀博客 The Diff 的作者，与大多数关心存在风险的人不同，他是明确的宗教徒，并在一次宗教会议上就此发表了演讲。随后，乔纳森·阿斯科纳斯 (Jonathan Askonas) 博士也发表了演讲，他指出，他是一位乐观主义者，对人工智能的存在风险持怀疑态度，并进行了类比，并谈到了“反基督议程的合理性”。</p><p>注意谁实际使用这种语言，以及对称性和不对称性。</p><p>乔丹的声明是否公平地描述了博斯特罗姆的言论？</p><p>亩。 “是”和“否”都会产生误导性的答案。</p><p>他的陈述是为了暗示比现在更强烈的东西。我不会称其为“谎言”，但我理解为什么这么多回复将其贴上这样的标签。相反，我认为这种描述具有高度误导性，特别是考虑到播客的其余部分和合理的外部背景。但是，是的，<a href="https://thezvi.substack.com/p/how-to-bounded-distrust">根据有限不信任的规则，根据引用的文字，这是一个可以采取的合法举动</a>。你可以进行这种程度的误导。我感谢他提供了详细的文字记录。</p><p>类似地，<a href="https://twitter.com/LouisAnslow/status/1723340940603949099">路易斯·安斯洛（Louis Anslow）对乔丹的反应是</a>，他说博斯特罗姆已经“破坏了队伍”，并在遵守有限不信任规则的同时，尽最大努力提供最大限度的耸人听闻的解读（粗体红色的恐吓词！）。又是谁在制造恐惧？</p><p>乔丹·蔡斯-杨随后详细引用了采访内容，他的大胆之处无处不在。</p><p>为了避免任何混淆，并且因为这是一个值得阅读的深思熟虑的讨论，我将引用他引用的整个部分，并建议那些感兴趣的人阅读（或收听）整个内容。</p><p>我还要指出的是，这次演讲所选择的标题<a href="https://www.youtube.com/watch?v=_Oo-m893-xA&amp;pp=ygUUbmljayBib3N0cm9tIHVuaGVhcmQ%3D">“尼克·博斯特罗姆：人工智能将如何导致暴政</a>”似乎在另一个方向上偏离了轨道，而且非常不是中心描述，同时也是他确实提到的内容作为一种可能性。博斯特罗姆有一个部分讨论了人工智能可以确立包括暴政在内的永久性权力结构，并使监视更加有效，但他并没有说这会导致暴政，也不是采访的核心讨论。</p><p>即使文章或讨论相当不错，头条新闻也常常很糟糕。</p><h3>博斯特罗姆中心所说的内容大多不是新的或有争议的</h3><p>尼克·博斯特罗姆在引用的文字中说了四点核心内容。</p><ol><li>永远构建 AGI 将是一场悲剧。</li><li>现在有一个机会，尽管不太可能，我们可能会过度，并这样做。</li><li>他对自己只关注风险方面感到遗憾，尽管我很困惑为什么。</li><li>他指出，我们面临着来自生物学或纳米技术的潜在生存风险，AGI 可以帮助预防这些风险。</li></ol><p>只有第三点在我认识的关心生存风险的人中并不常见。正如博斯特罗姆所说，我认为你会在#4 上达成普遍共识——我什至会说，那些不同意的人没有合理地思考这一点。 #1 和 #4 定期得到 Eliezer Yudkowsky 和 ​​MIRI 的其他初选的肯定，我实际上想不出有任何人明确反对这两个提议。如果我最近做得不够，我会确认它们。</p><p>第#2点是一个概率问题。博斯特罗姆永远不会在这里说 0%，其他任何人也不会清楚地思考，尽管你可能会认为——而且我也非常认为——如果选择是“永远不要按照人类思维的形象建造一台机器”和“机会是非常高，很快这个宇宙中就不会再有人类存在了。”我将选择方框 A。</p><p>事实上，直到最近，人们谈论的更多是“你为什么试图阻止我们尽快构建通用人工智能，这是不可能的”，这绝对是一项比从未构建通用人工智能更容易的任务，而像尤德科夫斯基这样的人会说“是的，看起来非常难，我们看起来注定要失败，但无论如何我们都会尝试。”包括泰勒·考恩在内的许多人基本上认为，长期阻止通用人工智能是不可能的，激励措施对它的作用太强烈了。</p><p>我仍然认为这是一个非常合理的立场，这很可能是真的。让每个人都停下来似乎非常困难，并且无限期地保持这种状态似乎也非常困难。但是，是的，有一定的可能性。超过1%。如果我们假设 AGI 在技术上并不那么困难，并且我们至少在几十年内不会毁掉我们的文明，我想说不到 10%。</p><h3>多数人对存在风险的担忧得到了证实</h3><p>这是一个回应。</p><blockquote><p> Rob Bensinger (MIRI)：我在某些观点上不同意博斯特罗姆的观点，但如果你认为 x-risk 的人希望人类永远不要建造 AGI，那么你只是没有注意到。过度调整的风险是真实存在的；我只是认为它远小于低于目标的风险。</p><p>事实上，对此有细致入微的看法是可能的，而且事实上，直到我想说的是，过去一两年，公共人工智能 x 风险讨论才成为常态。博斯特罗姆和尤德科夫斯基是一种更精确的文化的遗留物，这种文化的世界观并不以俏皮话、政治口号和迷因为基础。</p><p> Rob Bensinger（引用自 2022 年 6 月 25 日的话）：此外，如果有帮助的话，我很高兴地说 MIRI 领导层认为“人类永远不会构建 AGI”将是历史上最严重的灾难，将损失几乎所有未来的价值，并且作为一种选择基本上是令人无法接受的糟糕。</p></blockquote><p><a href="https://twitter.com/moreisdifferent/status/1723459440941101215">这是丹·埃尔顿，他的处境与博斯特罗姆相似。</a></p><p>更重要的是，我们非常重视就此类问题表达不同意见。</p><blockquote><p>罗布·本辛格：我不同意博斯特罗姆的“社会还不够担心，但我现在担心我们很可能会反应过度”。我认为反应不足的可能性仍然更大，而且成本也更高。但我非常高兴那些有 X 风险的人会大声表达这样的担忧。</p><p>反应过度<i>是</i>有可能的。事实上，当人们要求我勾勒出我们生存的现实未来时，我通常会描述社会在某种程度上反应过度的未来。</p><p>因为“试图想象一个美好未来”的经典失败模式是“你没有在你的想象中包含现实数量的混乱和混乱”。即使您设法避免了灾难，<i>现实的</i>未来也不会完全按照您希望的方式发展。</p><p>而“社会过于谨慎，构建通用人工智能所需的时间太长”<i>正是</i>我们应该预料到在<i>现实</i>世界中会发生的坏事，在这个现实世界中我们不会以某种方式自杀。</p><p>如果我们以某种方式避免两种最糟糕的结果，（非灾难性的）过度反应是让事情变得<i>适度</i>糟糕的现实方式。 （那些“冲向 ASI 并毁灭我们自己”和“在人类<i>整个</i>未来历史中根本不会建立 ASI”。两者都是灾难性的糟糕。）</p><p>我认为博斯特罗姆在这种情况下是完全错误的，并且很好奇这背后存在哪些分歧。 （他认为硬件和软件有<i>那么</i>容易监管吗？他认为ASI风险没有<i>那么</i>高吗？）</p><p>但显然，试图动员政策制定者和公众对快速变化的高技术领域做出理智的反应，有点像最后的绝望之举，接下来发生的事情将<i>很难</i>预测或控制。</p><p>仅有良好的意愿还不够。我们可以拥有世界上最好的论据，但一旦实施取决于世界各地的政策制定者做正确的事情，仍然会看到一切都会出错。</p><p>然而，通过成为愿意<i>谈论</i>这些风险的社区，即使在政治上不方便的情况下也愿意表达这些风险，我们切断了许多善意可能导致事情出错的路径。</p><p>我们假设这列火车有刹车。我们正在使讨论是否需要改变策略成为可能。即使我们不需要（我认为这里就是这种情况），我们保留这个选项也非常重要。毕竟，未来很难预测。</p></blockquote><p>对于那些将博斯特罗姆这样的言论武器化而不是与他们进行对话的人，我想说：<a href="https://www.youtube.com/watch?v=EYvrziE4feI&amp;t=5s&amp;ab_channel=GamingAndTv">你们并没有让这件事变得容易</a>。</p><p>我的猜测是，博斯特罗姆和本辛格之间分歧的症结在于，对于实际采取适当预防措施所需的关注程度存在分歧，其中我基本上同意本辛格的观点。博斯特罗姆说得更高一些，本辛格会说得更高、更精确。这最重要的是基于实际停止人工智能开发的难度差异，其次是基于 Bensinger 具有非常高的 p（厄运 | AGI 很快）。</p><p>我认为，还有一种分歧是由于博斯特罗姆是一位习惯于从锁定和永久平衡的角度思考的哲学家而引起的——他认为我们可能会因为恐惧而将自己锁定在没有通用人工智能的环境中，而且它很可能会无限期地持续下去。我在其他长期主义哲学（例如托比·奥德和威尔·麦克阿斯基尔）中看到了很多类似的文化锁定论点，并且我对这种长期路径依赖更普遍地持怀疑态度。由于其他生存风险和文明崩溃的危险，博斯特罗姆似乎比本辛格更认为我们“准时”。这是更愿意冒下冲风险以防止过冲的原因。</p><p>我还认为，正是由于这次讨论的框架，本辛格已经感觉到博斯特罗姆的更新比以前大得多。博斯特罗姆表示，这种超调的可能性很小。</p><h3>详细引用文本</h3><p>首先，博斯特罗姆呼应了几乎所有人都同意的基本原则，包括埃利泽·尤德科夫斯基（Eliezer Yudkowsky），他已经多次明确说过这一点。我也同意。</p><blockquote><p> <strong>Nick Bostrom：</strong>如果我们从未开发出先进的人工智能，那将是一场悲剧。我认为这是人类在某个时刻必须通过的一个门户，通往真正伟大未来的所有道路最终都会通过机器超级智能的发展来引导，但这种实际的转变本身将与重大风险相关联，并且我们需要非常小心才能做到这一点。</p></blockquote><p>博斯特罗姆说的第二件事是，我们可能会出现一些过度的小危险，实际上不会创造人工智能，我们应该尽量避免这种情况。</p><blockquote><p>但我现在开始有点担心，在过去一年左右的时间里，我们可能会过度关注风险和不利因素，我认为这是值得欢迎的，因为在此之前，这一点被忽视了几十年。我们本来可以利用那段时间，让自己处于更好的境地，但人们却没有这样做。<strong>不管怎样，它开始得到更多应有的关注，这很好，而且看起来仍然不太可能，但比一年前的可能性要小，我们可能会过度并达到永久冻土的地步——比如，一些人工智能从未发展起来的情况。</strong></p></blockquote><p>我经常在“人工智能无法被阻止，你所做的所有尝试最多只会减慢事情的速度，从而使一切在各方面变得更糟”和“人工智能可以很容易地被阻止，我们有这样做的危险，如果我们，这将是最大的悲剧，所以我们需要尽快采取行动，永远不要担心风险，以免发生这种情况。</p><p>是的，有些人会根据方便在这些陈述之间切换。</p><blockquote><p> <strong>FR：</strong>我们需要对人工智能有一种金发姑娘般的感觉。</p><p><strong>注意：是的。我担心它就像一个大破坏球，你无法以细粒度的方式真正控制。</strong></p></blockquote><p><a href="https://thezvi.substack.com/p/the-dial-of-progress">进步的表盘</a>，我们在人工智能中无法区分任何细微差别的危险。是的，我也担心这个。也许到最后，我们所拥有的只是破坏球。我将继续为细微差别而奋斗。但如果最终我们必须做出选择，而我们所拥有的只是破坏球，那么我们就没有选择不挥动它。 “金发姑娘的感觉”并不是我们的文明所擅长的。</p><blockquote><p><strong>弗洛·里德：</strong>就像一种因如此恐惧而产生的人工智能虚无主义？</p><p><strong>尼克·博斯特罗姆：</strong>是的。如此污名化以至于任何人都不可能对此发表任何积极的言论，然后我们就会得到其他锁定效应之一，就像其他人工智能工具一样，来自监视、宣传和审查制度，以及无论哪种正统观念。 ——五年后，十年后，无论如何——这种情况会以某种方式被锁定，然后我们永远不会采取下一步。我认为那将是非常悲惨的。</p></blockquote><p>如果它实际上永远不会，那么是的，那就是悲剧。但我说的悲惨程度远不及所有人都死去。如果被迫选择的话，我再次选择方框 A。不要按照人类思维的形象制造机器。有一些价格或风险水平让我选择方框 B，它超过 2%，但远低于 50% 和我目前对基线情景下前进风险的估计，如果我们成功的话致力于构建 AGI。</p><p>也许您想直接对着麦克风讲话但不同意。</p><p>我强烈同意现在看起来比几个月前更有可能出现超调。</p><blockquote><p>尼克·博斯特罗姆：我仍然认为这不太可能，但肯定比六或十二个月前更有可能。<strong>如果你只是绘制公众态度和政策制定者态度的变化，你会想到去年发生了什么——如果这种情况在明年、后年和后年继续发生，那么我们几乎会作为对人工智能的永久禁令，我认为这可能非常糟糕。</strong>我仍然认为我们需要比目前更高的关注水平，但我希望我们达到最佳的关注水平，然后停在那里，而不是继续下去——</p></blockquote><p>因此，需要明确的是，尼克·博斯特罗姆仍然认为我们现在还不够担心，但担心如果事情发展得太过分，我们可能会出现过度关注，正如接下来所证实的那样。</p><blockquote><p> <strong>FR：</strong>我们需要对人工智能有一种金发姑娘般的感觉。</p><p><strong>注意：是的。我担心它就像一个大破坏球，你无法以细粒度的方式真正控制。人们喜欢成群结队地移动，他们有了一个想法，然后——你就知道人们是怎样的了。我有点担心，谈论人工智能的负面言论会成为一场大规模的社会踩踏行为，然后它就会完全失控，从而以这种方式摧毁未来。</strong>然后，当然，我们会通过其他方法灭绝，也许是合成生物学，甚至没有至少用……来掷骰子。</p><p> <strong>FR：</strong>所以，这有点像“选择你的毒药”。</p><p><strong>注意：</strong>是的。</p></blockquote><p>再说一次，是的，这样的例子很多。对博斯特罗姆来说，改变的并不是他之前不相信我们有足够的机会超出值得担心的范围。现在他认为这件事已经足够大了，值得考虑，而且一件非常糟糕的事情的微小可能性也值得担心。这么。</p><p>我询问了 GPT-4，GPT-4 表示这是对他之前在《超级智能》中的立场的扩展，增加了细微差别，但这并不矛盾，而且根本不记得博斯特罗姆对此问题的任何评论。</p><blockquote><p> <strong>FR：</strong>碰巧这种毒药可能会杀死你或使你中毒，而你只能在上面掷骰子。</p><p><strong>注意：</strong>是的。我认为我们可以做很多事情来提高不同事情和类似事情的顺序的可能性，我们应该做所有这些事情。</p><p> <strong>FR：</strong>不过，我认为，作为一名研究存在风险的学者，你会属于经常被要求谈论人工智能可能吸引我们走向的可怕的假设未来的人——这个节目就是一个例子。 。<strong>您对关注风险感到遗憾吗？</strong></p></blockquote><p>需要明确的是，是的，他现在说了第三件事，他对工作重点感到遗憾，尽管从他的其他信念来看，他似乎不应该后悔？</p><blockquote><p><strong>注意：是的，因为我认为，这种赤字已经存在了几十年。这是显而易见的——至少对我来说，但应该是相当明显的——人工智能最终会成功，然后我们将面临这样的问题：“我们如何控制它们以及我们如何处理它们？”他们？”然后这将非常困难，因此也有风险，而这一点却被忽视了。大约有 10,000 人构建人工智能，但大约有 5 人在思考如果我们真的成功了我们将如何控制它们。但现在情况已经改变，并且这一点得到了认可，所以我认为现在可能不再需要在这种担忧中添加更多内容。</strong></p><p> <strong>FR：末日论的工作已经完成，现在你可以去做其他事情了。</strong></p><p><strong>注意：是的。这很难，因为它总是一个不稳定的事情，不同群体有不同的看法，而且仍然有人忽视风险或不考虑风险。我认为最佳的关注程度比我们目前的关注程度略高，所以我仍然认为应该有更多的关注。这比大多数人意识到的更危险，但我刚刚开始担心它，然后有点过头了，结论是，“好吧，让我们等一千年再这样做，”然后，当然，我们的文明不可能在正轨上持续一千年，而且……</strong></p></blockquote><p>这听起来似乎在 2014 年提高关注程度是正确的，并且至少持续到 2023 年中期？我很困惑。</p><h3>更广泛的播客背景</h3><p>如果你听完整的上下文，这是稀缺的，你会看到一个播客，它的前 80% 几乎完全集中在 Bostrom 对人工智能带来的各种存在风险问题的警告上。引用的文本是播客的最后约 20%。这看起来并不像是一个对关注这个问题感到后悔的人。</p><p> 9:50 左右，博斯特罗姆指出，他仍然预计飞机会快速起飞，至少相对于普遍预期是这样。</p><p>大约 12:30，他讨论了有关开源的辩论，并指出开源模型中的任何保障措施都将被删除。</p><p> 14:00 左右，他表示人工智能将增强中央权力的监视能力，包括对人们想法的监视。</p><p> 17:00 左右，他讨论了人工智能强化权力结构（包括专制权力结构）的潜力。</p><p> 20:00 左右，他介绍了对齐问题并试图解释它。</p><p> 22:00 左右，他们简短地讨论了西方自由主义价值观与人们在人工智能中所期望的功利主义之间的冲突，然后博斯特罗姆回过头来更多地谈论为什么对齐是困难的。</p><p> 26:00 左右，Flo Read 提出了对强国首先获得超级智能并控制的担忧，然后询问了军事应用的问题。博斯特罗姆似乎没有意识到，核心威胁并不是权力落到了错误的人手中。</p><p>我担心大部分讨论同时涵盖了许多基本领域，对于第一次接触它的人来说解释过于密集和困难。一切都很好，但也很匆忙。</p><p>我确实认为这确实代表了<a href="https://www.lesswrong.com/posts/eXHp9J4PXmQXzmBAj/transcription-and-summary-of-nick-bostrom-s-q-and-a">旧问答</a>中重点的重大转变，他对我们是否应该构建人工智能的回答“不会很快”，但他仍然提出了诸如长期反思之类的想法，并支持一旦我们构建人工智能。知道怎么做才安全。</p><h3>呼吁细微差别</h3><p>综上所述，在我看来博斯特罗姆：</p><ol><li>仍然认为，引起人们对人工智能带来的生存风险的担忧很重要，他继续这样做就证明了这一点。</li><li>现在担心我们可能会过度调整并过度关闭人工智能。</li><li>遗憾的是他的关注点不够细致，完全集中在存在风险上，并且没有提到超调的可能性。</li><li>现在正试图提供这种细微差别，即我们需要采取必要的措施来防范存在的风险，而不是永久放弃人工智能。</li></ol><p>哪一个看起来都很棒？除了我看到的那些淡化存在风险的人的回应之外，所有人都高度错误地分类了这一点。马克·安德森 (Marc Andreessen) 的回应是个例外<a href="https://twitter.com/pmarca/status/1723394580081738184">，我引用了全文，“FFS”。</a>其所有含义都令人耳目一新的诚实和直接。不浪费任何文字，10/10，没有笔记。他不妥协，不接受妥协。</p><p>这一切也将对存在风险的担忧与对人工智能更普遍的 FUD 混为一谈，这又是一种担心，即没有任何细微差别的空间，即一个人无法在没有另一个人的情况下有区别地移动一个人。但博斯特罗姆本人表明这确实是可能的。谁能怀疑，没有博斯特罗姆的世界会减少对生存风险的担忧，而且对一般人工智能或对世俗危害的担忧也会按比例减少得多？</p><p>可以说，虽然我们对人工智能的总体担忧程度还没有过分，但对能力增长的自然反应以及所涉及的社会和政治动态本身就会提高担忧程度，并且在边缘推动更多现在的担忧可能会适得其反。我想这就是博斯特罗姆的观点。</p><p>我同意这种动态将进一步推高关注度。我不同意我们已经走上获得足够关注的轨道，而且我绝对不同意如果人们停止推动更多关注，我们就会走上这样的轨道。</p><p>我特别担心这种担心会是错误的担心。因此，我们将采取错误的预防措施，而不是正确的预防措施。但阻止这种情况的唯一方法是大声说出正确的担忧，因为“自然”的社会和政治力量将关注（通常是真实的，但）非中心的，因此本质上是错误的担忧。</p><p>再说一次，细微差别是最好的，我将继续为此奋斗。</p><p>思考这个问题的一种方法是，理想的关注水平 C 是细微差别水平 N 或响应质量 Q 的函数。随着 N 和 Q 上升，C 下降，这都是因为实际关注下降 - 我们我们更有可能做到这一点——而且还因为我们可以从生硬的反应中进行替代，因此不需要那么担心采取必要的对策。</p><p>因此，如果您想降低担忧程度并减少增加担忧的尝试？与我一起争取更多更好的细微差别以及强有力的实施细节。</p><h3>引用的文字继续</h3><p>博斯特罗姆说的第四件事是，我们最终将面临其他生存风险，而通用人工智能可以帮助预防这些风险。这里不争论，希望大家都同意，而且我们完全在谈论价格。</p><blockquote><p> <strong>FR：</strong>所以，如果我们这样做，我们就该死，如果我们不这样做，我们也该死。</p><p><strong>注意：</strong>无论哪种方式，我们都希望一切都好，但我想我希望在一些激进的生物技术革命之前出现人工智能。如果你想一想，如果你首先获得某种超先进的合成生物学，那可能会杀死我们。但如果我们幸运的话，我们就能活下来。然后，也许你发明了一些超先进的分子纳米技术，这可能会杀死我们，但如果我们幸运的话，我们就能幸存下来。然后你做人工智能。然后，也许这会杀死我们，或者如果我们幸运的话，我们能幸存下来，然后我们就到达了乌托邦。</p><p>好吧，那么你必须克服三个独立的生存风险——首先是生物技术风险，加上纳米技术风险，再加上人工智能风险，而如果我们首先获得人工智能，也许这会杀死我们，但如果没有，我们就会得到通过这一点，我认为这将处理生物技术和纳米技术风险，因此第二条轨迹上的存在风险总量将比前一条轨迹要少。</p><p>现在，情况比这更复杂，因为我们需要一些时间为人工智能做好准备，但你可以开始思考某种最佳轨迹，而不是一个非常简单的二元问题，“技术 X 是好是坏？”我们可能会更多地思考，“在边际上，我们应该尝试加速哪些领域，延迟哪些领域？”我认为，通过这种方式，你可以对可能的干预措施领域有更细致的了解。</p></blockquote><p>这似乎很合理。这是一个很好的选择，如果我们看起来接近这些事情中的任何一个，我们就会更加关注，并且我们必须，正如他们上面所说的，选择我们的毒药。并谈价格。有些人（例如斯科特·阿伦森）直言不讳地说，他们认为这里的价格合理地证明了这条道路的合理性，等待的生存风险超过了不等待的风险。我强烈不同意。我还预计，在我更担心纳米技术或合成生物学而不是核战争之前，除非这里的风险是由人工智能带来的，否则还需要几十年的时间。</p><p>我的模型是，目前的人工智能水平会增加而不是降低这些额外危险的风险，我预计这种情况会持续下去，直到我们达到通用人工智能，此时事情会变得很奇怪，它可能会朝任何一种方向发展，但大多数情况下不会。这并不重要，因为我们要么有更大的问题，要么有好的解决方案，如此有效，将大大降低此类风险的成本。</p><h3>结论</h3><p>细微差别是我们生存的关键之一。</p><p>通过转动<a href="https://thezvi.substack.com/p/the-dial-of-progress">进步的旋钮</a>来选择“对人工智能的正确关注程度”是不够的。如果我们把它调得太低，我们可能会被杀。如果我们把它调得太高，我们可能需要很长时间才能建立通用人工智能，我们可能会失去很多普通的效用，面临经济衰退，并随着时间的推移容易受到其他生存和灾难性风险的影响。</p><p>只有当我们能够选择经过深思熟虑和针对性强的干预措施时，一条成功的中间道路才是可行的，这些干预措施实际上会阻止创建足够强大的人工智能，直到我们知道如何安全地这样做并引导接下来发生的事情，同时也允许我们享受普通人工智能的好处，但这并没有完全关闭人工智能的大门。博斯特罗姆表示，这样的道路不太可能出现，但却是可能的。在我看来，即使人们对人工智能的恐惧和仇恨程度更高，但仍然需要一些似乎很难实现的事情。</p><p>监管经常会找到一种方法来做完全错误的事情，正如加速主义者经常指出的那样，但有时也游说并直接瞄准这种情况——让前沿模型开发尽可能快地继续，而所有现有的监管都剥夺了平凡的效用。随着时间的推移，训练更强大人工智能的诱惑和能力将会越来越大。那么该怎么办？</p><p>目前，考虑到这样做所需的协调以及必须克服的所有激励因素，我们甚至不知道如何直接阻止人工智能。我们当然不知道如何找到一条中间道路。那个世界到底是什么样子的？不，说真的，这个时间表是什么样的？它是如何到达那里的，它有什么作用？它如何无限期地对实际上危险的人工智能训练运行和部署保持足够的限制，而不会让每个人更普遍地反对人工智能？</p><p>我不知道。我确实知道，实现这一目标的唯一方法是认真对待风险，从技术层面审视它们，弄清楚如何才能经常正常生存，并在考虑到所有限制的情况下，在因果空间中绘制最佳可用路径。这意味着要面对实际的争论和真正的风险。我们无法通过右方“赢得”零和冲突来实现这一目标。</p><p>对人工智能这样的指数做出反应只有两种方法。太早和太晚。</p><p>同样，预防灾难时也只有两个级别的安全预防措施。太少又太多。如果你不（非常）冒着反应过度的风险，尤其是对反应过度的指责和担忧，你就知道自己反应不足。如果相关成本高得具有可比性，则反之亦然。</p><p>在我们生存的大多数世界中，如果我仍然在写帖子，我会在不同的地方说许多人工智能法规和限制太过分了，或者选择得很糟糕，以至于是负面的。大多数例外是我们以我认为非常愚蠢的方式掷骰子，然后获得非常幸运的世界。如果我们实现真正的“软着陆”和中间道路，那将是令人惊奇的，但我们不要自欺欺人地认为实现这一目标有多困难。</p><p>实际上，可能没有中间道路。作为一个星球，我们唯一的三个可用选择是“尽可能快地构建 AGI，假设第一次尝试对齐很容易，并且解决对齐后出现的动态也可以在灾难之前解决”，“构建 AGI”尽快知道我们可能会死，因为通用人工智能取代人类实际上是件好事”或者“永远不要按照人类思维的形象建造机器。”</p><p>如果情况确实如此呢？我相信选择是明确的。</p><br/><br/><a href="https://www.lesswrong.com/posts/PyNqASANiAuG7GrYW/bostrom-goes-unheard#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PyNqASANiAuG7GrYW/bostrom-goes-unheard<guid ispermalink="false"> pynqasaniaug7gryw</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Mon, 13 Nov 2023 14:11:11 GMT</pubDate> </item><item><title><![CDATA[The Science Algorithm AISC Project]]></title><description><![CDATA[Published on November 13, 2023 12:52 PM GMT<br/><br/><p>我为<a href="https://aisafety.camp/">AISC</a>提出了一个项目。评估您是否适合的最简单方法是查看我创建的<a href="https://docs.google.com/document/d/1M-DMOAujWwWETkxjjAiWHRQeXXCtJm-DrAZKD6sQ5-Q/edit">申请人问题</a>。如果回答这些问题所需的推理是您热衷于做的事情，那么您可能很适合。</p><p>欲了解更多详情，您可以查看<a href="https://docs.google.com/document/d/1GoXaYtUyanRrkBcjAknafqdKDCyTzzgxpckYMBtgPkQ/edit#heading=h.9lmc73wscx1r">项目建议书</a>。这是其介绍部分：</p><blockquote><p>现代深度学习是用一个简单程序（SGD）搜索可能的程序空间（神经网络的权重），并根据损失函数选择一个表现良好的程序。</p><p> Even though the search program is simple, the programs it finds are neither simple nor understandable. This makes it determine if a program that is spit out by the search procedure will be misaligned. It also makes it difficult to robustly bias our search procedure to select programs with specific properties such as non-deception, corrigibility, honesty, wanting what we want, etc.</p><p> My goal is to build an AI system that enables a pivotal act by figuring out the algorithms of intelligence directly, without running algorithmic search procedures that yield uninterpretable results. You could say I want to make myself play the part that SGD plays in the modern paradigm. The ideal outcome is to figure out how to write down the entire pivotal system as a non-self-modifying program explicitly, similar to how I can write down the algorithm for quicksort.</p><p> The idea is to create an algorithm that is analyzable and amenable. At every step during the design process, I want to push the system towards being understandable, aligned, and capable. Any alignment-related problems should be fixed as they come up, by deeply reaching inside the system and making the necessary changes instead of applying a superficial patch.</p></blockquote><blockquote><p> The goal is not to solve alignment in full generality but to build a highly restricted system that enables a pivotal act. Note that I am using Eliezer&#39;s definition of pivotal act which means something quite specific. People tend to get confused by this term. I wrote this article in an attempt to clear up some of the confusion.</p><p> The next three sections will become increasingly more concrete. The &quot;Background&quot; section presents background concepts to this agenda. &quot;Directions&quot; outlines some directions I would like to explore during the AISC. &quot;The Plan&quot; outlines a concrete plan, mainly for the beginning of the AISC.</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/KHjQzxRnDCjM7xsFk/the-science-algorithm-aisc-project#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/KHjQzxRnDCjM7xsFk/the-science-algorithm-aisc-project<guid ispermalink="false"> KHjQzxRnDCjM7xsFk</guid><dc:creator><![CDATA[Johannes C. Mayer]]></dc:creator><pubDate> Mon, 13 Nov 2023 12:52:43 GMT</pubDate> </item><item><title><![CDATA[You can just spontaneously call people you haven't met in years]]></title><description><![CDATA[Published on November 13, 2023 5:21 AM GMT<br/><br/><p> Here&#39;s a recent conversation I had with a friend:</p><blockquote><p> Me: &quot;I wish I had more friends. You guys are great, but I only get to hang out with you like once or twice a week. It&#39;s painful being holed up in my house the entire rest of the time.&quot;</p><p> Friend: &quot;You know ${X}. You could talk to him.&quot;</p><p> Me: &quot;I haven&#39;t talked to ${X} since 2019.&quot;</p><p> Friend: &quot;Why does that matter? Just call him.&quot;</p><p> Me: &quot;What do you mean &#39;just call him&#39;? I can&#39;t do that.&quot;</p><p> Friend: &quot;Yes you can&quot;</p><p> Me: </p><p><img style="width:74.06%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/xouempxvvmr88z8oqdlo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/ooqkduauj1lvewi9zut9 127w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/lbneezhuwfgjkooc8hum 207w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/nfotaoarazdawh8oaiz0 287w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/kdalumfn0yfcsm9c4duf 367w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/v0xio6ohwwz9jrormpsc 447w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/wz7ixfhlk5i0jdrpkvtr 527w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/kj0dlchhvshtwm1s10v1 607w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HawAteFsnyhfYpuD/jxgftzbgde3ol5ujlags 687w"></p></blockquote><p> Later: I call ${X}, we talk for an hour and a half, and we meet up that week.</p><p> This required zero pretext. I just dialed the phone number and then said something like &quot;Hey ${X}, how you doing? Wanted to talk to you, it&#39;s been a while.&quot; It turns out this is a perfectly valid reason to phone someone, and most people are happy to learn that you have remembered or thought about them at all.</p><p> Further, I realized upon reflection that the <a href="https://en.wikipedia.org/wiki/Degree_(graph_theory)">degrees</a> of the people I know seem related to their inclination to do things like this.</p><br/><br/> <a href="https://www.lesswrong.com/posts/2HawAteFsnyhfYpuD/you-can-just-spontaneously-call-people-you-haven-t-met-in#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/2HawAteFsnyhfYpuD/you-can-just-spontaneously-call-people-you-haven-t-met-in<guid ispermalink="false"> 2HawAteFsnyhfYpuD</guid><dc:creator><![CDATA[lc]]></dc:creator><pubDate> Mon, 13 Nov 2023 05:21:05 GMT</pubDate> </item><item><title><![CDATA[Zvi's Manifold Markets House Rules]]></title><description><![CDATA[Published on November 13, 2023 12:28 AM GMT<br/><br/><p> All markets created by Zvi Mowshowitz shall be graded according to the rules described herein, including the zeroth rule.</p><p> The version of this on LessWrong shall be the canonical version, even if other versions are later posted on other websites.</p><p> Rule 0: If the description of a particular market contradicts these rules, the market&#39;s description wins, the way a card in Magic: The Gathering can break the rules. This document only establishes the baseline rules, which can be modified.</p><ol><li> Effort put into the market need not exceed that which is appropriate to the stakes wagered and the interestingness level remaining in the question. I will do my best to be fair, and cover corner cases, but I&#39;m not going to sink hours into a disputed resolution if there isn&#39;t very serious mana on the line. If it&#39;s messy and people care I&#39;d be happy to kick such questions to Austin Chen.</li><li> Obvious errors will be corrected. If for example a date is clearly a typo, I will fix.</li><li> If the question description or resolution mechanism does not match the clear intent or spirit of the question, or does not match its title, in an unintentional way, or is ambiguous, I will fix that as soon as it is pointed out. If the title is the part in error I will fix the title. If you bet while there is ambiguity or a contradiction here, and no one including you has raised the point, then this is at your own risk.</li><li> If the question was fully ambiguous in a scenario, I will choose resolution for that scenario based on what I feel upholds the spirit of the question and what traders could have reasonably expected, if such option is available.</li><li> When resolving potentially ambiguous or disputable situations, I will still strive whenever possible to get to either YES or NO, if I can find a way to do that and that is appropriate to the spirit of the question.</li><li> Ambiguous markets that have no other way to resolve, because the outcome is not known or situation is truly screwed up, will by default resolve to the manipulation-excluded market price, if I judge that to be a reasonable assessment of the probability involved. This includes conditional questions like &#39;Would X be a good use of time?&#39; when X never happens and the answer seems uncertain.</li><li> If even those doesn&#39;t make any sense, N/A it is, but that is a last resort.</li><li> Egregious errors in data sources will be corrected. If in my opinion the intended data source is egregiously wrong, I will overrule it. This requires definitive evidence to overturn, as in a challenge in the NFL.</li><li> If the market is personal and subjective (eg &#39;Will Zvi enjoy X?&#39; &#39;Would X be a good use of Zvi&#39;s time?&#39;), then my subjective judgment rules the day, period. This also includes any resolution where I say I am using my subjective judgment. That is what you are signing up for. Know your judge.</li><li> Within the realm of not obviously and blatantly violating the question intent or spirit, technically correct is still the best kind of correct when something is well-specified, even if it makes it much harder for one side or the other to win.</li><li> For any market related to sports, Pinnacle Sports house rules apply.</li><li> Markets will resolve early if the outcome is known and I realize this. You are encouraged to point this out.</li><li> Markets will resolve early, even if the outcome is unknown, if the degree of uncertainty remaining is insufficient to render the market interesting, and the market is trading >;95% or &lt;5% (or for markets multiple years in advance, >;90% or &lt;10%), and I agree with the market but feel it mostly reflects Manifold interest rates. Markets will not be allowed to turn into bets on interest rates. However if it could still plausibly resolve N/A, then I will hold off.</li><li> I will not participate in subjective markets until the minute I resolve them.</li><li> If I participate in a market and do not resolve it, I am stating that the market is fully objective, and I am locking any changes, and will offload judging the market in event of a non-trivial dispute to a neutral third party.</li><li> I will definitely pick off the value right before a definitive resolution (eg buying to 99%+ when I&#39;m about to resolve YES), but this will not be considered in the market price. I won&#39;t do this when resolving to a market price.</li><li> I will modify these rules continuously over time as I find better rules. I will do my best not to have this change the outcome of markets that had already been created and have substantial participation.</li><li> Others are encouraged to say they use &#39;Zvi house rules,&#39; either in a particular market or in general, but I am not pledging to offer my verdict on any confusions that occur in such markets.</li></ol><p> Suggestions for modifications are encouraged.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ge3Jf5Hnon8wq4xqT/zvi-s-manifold-markets-house-rules#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ge3Jf5Hnon8wq4xqT/zvi-s-manifold-markets-house-rules<guid ispermalink="false"> ge3Jf5Hnon8wq4xqT</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Mon, 13 Nov 2023 00:28:05 GMT</pubDate> </item><item><title><![CDATA[Helpful examples to get a sense of modern automated manipulation]]></title><description><![CDATA[Published on November 12, 2023 8:49 PM GMT<br/><br/><p> <strong>Why this is valuable</strong></p><p> In the face of <a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines"><u>unclear AGI timelines</u></a> , thinking about SOTA human behavior manipulation technology is not intrinsically valuable (in fact, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks:~:text=Ultimately%2C%20however%2C%20influence,stuck%20living%20in."><u>all</u></a> <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=The%20attack%20surface%20is%20unacceptably,in%20a%20diminished/captured%20form."><u>along</u></a> <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#:~:text=Surviving%20the%202020s%20does%20not%20have%20much%20intrinsic%20value%2C%20but%20it%20is%20instrumentally%20convergent%20to%20persist%20and%20continue%20alignment%20research.%C2%A0"><u>I&#39;ve</u></a> <a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must#:~:text=This%20should%20not%20distract%20people%20from%20AI%20safety.%20This%20is%20valuable%20because%20the%20AI%20safety%20community%20must%20survive.%20This%20problem%20connects%20to%20the%20AI%20safety%20community%20in%20the%20following%20way%3A"><u>repeatedly</u></a> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks:~:text=People%20like%20Gary,AI%20alignment%20efforts."><u>asserted</u></a> that it has a serious threat of distracting people from AGI, which will <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>kill everyone</u></a> , and should not be researched as a new X-risk).</p><p> However, it is probably instrumentally valuable for <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"><u>understanding AI governance, geopolitics, and race dynamics</u></a> , and because <a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must#:~:text=State%20survival%20and,AI%20safety%20community%3F"><u>the AI safety community is disproportionately at risk of being targeted</u></a> due to <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point:~:text=In%20most%20worlds,worth%20the%20cost."><u>entering an arena</u></a> with <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"><u>heavy hitters</u></a> .</p><p> One of the main ways that people <a href="https://www.lesswrong.com/posts/nnNdz7XQrd5bWTgoP/on-the-loss-and-preservation-of-knowledge"><u>transmit knowledge and intuitive understanding</u></a> is through <a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding"><u>intuitive examples</u></a> eg use cases. Examples of manipulation strategies are critical to develop an intuitive understanding of what&#39;s probably already out there; however, although it&#39;s easy to predict whether <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"><u>governments</u></a> are working on autonomous manipulation in general, which is more than sufficient to indicate that the AI safety community should <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=plugging%20audio%20conversation,ebooks.%20Physical%20books">take precautions to minimizing the attack surface</a> ; it&#39;s still much harder to trace the specific outlines of particular autonomous manipulation strategy that has likely already been discovered and exploited.</p><p> The <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>clown attack</u></a> example was low-hanging fruit for me to discover and write about; its use for autonomous manipulation was both unusually powerful, obvious, and inevitable. Lots of people found that example extremely helpful for getting oriented towards the situation with human thought control. Other examples of automated manipulation are harder to be confident were 1) easy to discover and 2) <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for"><u>existing institutions</u></a> were incentivized to deploy and test; but they will still be helpful for demonstrating what kinds of things are probably out there.</p><p> This model is falsifiable; <strong>I predict at 95% that similar types of automated manipulation strategies as these were deployed by US, Russia, or Chinese companies or agencies to steer people&#39;s thinking on Ukraine War and/or Covid-related topics</strong> . I lose a large amount of bayes points if that didn&#39;t happen eg <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally"><u>governments weren&#39;t sufficiently aware</u></a> of these capabilities due to <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>uniform incompetence</u></a> , or the <a href="https://twitter.com/ylecun/status/1720167378019885468"><u>engineering problems</u></a> are too difficult eg due to <a href="https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers#:~:text=In%20order%20to%20design%20a,So%20it%20goes."><u>spaghetti towers</u></a> or <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point:~:text=access%20to%20the%20technology%20itself,data%20poisoning%20by%20larger%20orgs"><u>data poisoning/data security persistently thwarting the large tech firms instead of just the smaller ones</u></a> ). They should have had these capabilities ready by then, and they should have already used them for <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=We%20should%20be,adversarial%20information%20environment."><u>geopolitical matters that incentivized information warfare</u></a> .</p><p> <strong>List of Concrete Examples</strong></p><p> <strong>1. Measurement-based thought and behavior steering</strong></p><p> The current paradigm is already predisposed to deploy intensely optimized manipulation strategies, which are bizarrely insightful in counterintuitive ways, and randomly become superhuman.<br> For example, <a href="https://twitter.com/SpencrGreenberg/status/1721911661689200762"><u>this tweet</u></a> :</p><blockquote><p> ...If your social media is hell, don&#39;t forget the law of Social Media Manifestation: you manifest what you pay attention to. If you want your social media to be full of interesting ideas and not brimming with angry guy mutterings, click on the former, not  the latter.</p><p> If the real world worked this way, we might think it would be a paradise (all we have to do is focus on good things, and then good things will be all around us). But what social media teaches us is that if the real world worked this way, many of us would end up in a nightmare.</p><p> Fortunately, with social media, it&#39;s a nightmare that&#39;s pretty easy to wake up from. But it requires clicking and liking based on what your higher self wants you to see rather than what your lower self feels a compulsion to see.</p></blockquote><p> This combination of words induces the reader to:</p><ol><li> Buy-in to the impression that social media news feeds are benign random generation that strives to match the user&#39;s preferences in a simple way, and nothing else.</li><li> Form an impression that you were in control all along, and your experience was caused by not knowing how to to assert control, rather than never having any control in the first place.</li><li> Form an impression that people who dislike or distrust social media simply failed to assert control, rather than distrusting social media due to other factors like manipulation asymmetry.</li></ol><p> This doesn&#39;t need to be anywhere close to deliberate, nor does this even require AI or 2020s technology. An algorithm from the late 2000s can simply discover that exposure to this post correlated with people more frequently returning to the social media platform on a daily basis, or that it was unusually effective and increasing habitual use among demographics that are normally resistant to habitual use, such as psychology researchers or cybersecurity experts.</p><p> Although galaxy-brained manipulation strategies can wind up labeled and quantified, or even as <a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators#:~:text=As%20Ilya%20Sutskever%20compactly%20put%20it%2C%20to%20learn%20to%20predict%20text%2C%20is%20to%20learn%20to%20predict%20the%20causal%20processes%20of%20which%20the%20text%20is%20a%20shadow."><u>gears in the models of modern ML</u></a> , superhuman manipulation can also be discovered and exploited by simpler systems that are simply optimizing for whatever maximizes a single metric, such as causing humans to choose to use your platform more.</p><p> This can become even more complicated when <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point:~:text=Among%20a%20wide,of%20the%20genepool."><u>simple algorithms try to adversarially steal user time from other platforms running their own algorithms</u></a> , and more complicated still when both sides of the adversarial environment are running modern ML instead of late-2000s algorithms.</p><p> <strong>Targeting AI safety hypothetical example:</strong><br> Attackers trying to slow/mitigate the rapid growth of the AI safety community optimize social media algorithms or botnets to minimize people&#39;s motivation to contribute to AI safety in a measurable way. The algorithm optimizing for this has already determined that Ayn Rand-like posts convincing people to not care about others, resentment towards elite community members, or emphasizing the personal benefits of <a href="https://forum.effectivealtruism.org/posts/fjQJaAfiA6eML5xwx/don-t-optimise-for-social-status-within-the-ea-community"><u>goodharting social status</u></a> , causes people to reduce engagement in altruistic communities in general. But in the case of AI safety, it is actually manipulating them to become nihilistic, not just less motivated, substantially increasing the prevalence of intelligent bad actors in the AI safety community.</p><p> <strong>Ukraine hypothetical example:</strong><br> American intelligence agencies seek to wield their capability to steer public opinion in Western countries in order to contribute to the war effort in Ukraine, by reducing draft dodging among Ukrainian men and increasing morale among Ukrainian soldiers. The algorithm finds that anti-Putin posts/word combinations, rather than pro-Ukrainian posts or anti-Russian posts, are the most effective at reducing draft dodging among Ukrainian men. However, this is interpreted by Russia as the West openly orchestrating regime change in Russia, and retaliates with a mirror strategy.</p><p></p><p> <strong>2. Mapping human internals with causal analysis</strong></p><p> Modern ML seems likely to already be able to map the internals of the human mind by creating causal graphs on beliefs and belief formation. According to Zack M Davis&#39;s <a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on"><u>post on optimized propaganda</u></a> (involving a set of 3 beliefs/attitudes, A, B, and C):</p><blockquote><p> Learning these kinds of models is feasible because not all possible causal relationships are consistent with the data: if A and B are statistically independent of each other, but each dependent with C (and are conditionally dependent given the value of C), it&#39;s kind of hard to make sense of this except to posit that A and B are causes with the common effect C...</p><p> &quot;Learn the causal graph of why they think that and compute how to intervene on it to make them think something else&quot; is a <a href="https://web.archive.org/web/20200521005958/http://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/">symmetric weapon</a> —a <i>fully general</i> persuasive technique that doesn&#39;t <a href="http://benjaminrosshoffman.com/humility-argument-honesty/">depend on whether the thing you&#39;re trying to convince them of is <i>true</i></a> ... computing what <a href="https://www.lesswrong.com/posts/4hLcbXaqudM9wSeor/philosophy-in-the-darkest-timeline-basics-of-the-evolution">signals to emit</a> in order to control our decisions.</p></blockquote><p> Social media provides an environment that not only controls for variables, but also subjects people to an extremely wide variety of topics in charismatic or creative/artistic/poetic ways, such as <a href="https://www.lesswrong.com/posts/PdcnEEE6sdgACDrEk/snapshot-of-narratives-and-frames-against-regulating-ai#:~:text=it%27s%20quite%20important%20to%20have%20a%20thriving%20ecosystem%20of%20VR%20producers%2C%20competing%20on%20which%20content%20will%20be%20the%20most%20addictive%20or%20hack%20human%20brains%20the%20fastest.%20Why%20an%20entire%20ecosystem%3F%20Because%20it%20fosters%20more%20creativity%20in%20brain%20hacking."><u>the thriving cultural ecosystem described by Jan Kuviet which harnesses large amounts of creative talent towards integrating intense stimuli into targeted concepts</u></a> .</p><p> By measuring the exact pace that people scroll past each post with their thumb and mouse wheel, and converting that data into linear curves which are optimal for ML, large tech companies and intelligence agencies would by-default have more than enough data to compare and contrast a wide variety of thoughts and reflections from large numbers of different types of people on a wide variety of topics, and acquire a critical mass of correlations sufficient to run causal analysis and predictive analytics on the human thought process.</p><p> Psychology research within tech companies and intelligence agencies <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/jyAerr8txxhiKnxwA#:~:text=If%20they%20notice,defensive%20information%20warfare.">could even outpace the declining university-based 20th century psychology research paradigm</a> , with orders of magnitude fewer researchers. This is due to vastly superior research and experimentation capabilities offered by the 21st century paradigm, which would accelerating effectiveness and efficiency of hypothesis generation and testing by orders of magnitude.</p><p> <strong>Ukraine hypothetical example:</strong><br> Using causal graphs to understand what kinds of social media and foreign news articles predisposes Ukrainian men towards draft compliance vs draft dodging, running predictive analytics based on troop defections in order to research morale and make it more consistent under extreme conditions such as long periods of sleep deprivation/interruption and constant fighting. They also produce more effective and insightful training manuals to distribute to officers throughout the Ukrainian military. American intelligence agencies can also more effectively map the expansion and contraction of antiwar sentiment among economic, political, cultural, and technocratic elites throughout the West (including tracking the downstream effects of Russian influence operations with greater nuance).</p><p> <strong>Targeting AI safety hypothetical example:</strong><br> Top Amazon engineers and executives develop <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point:~:text=I%20think%20that%20the%20AI%20safety%20community%20is,murkiest%20people%20lurking%20within%20the%20US%2DChina%20conflict."><u>a superior understanding of intimidation and bluffing</u></a> by contracting with consultants at top legal firms, and test/refine the consultants&#39;s theories of the human mind against their quantitative models of the human mind (including causal graphs) made from social media user data from a negotiated exchange of unpoisoned data between Amazon and Facebook. When Anthropic grabs their attention,  Amazon executives combine their general-purpose intimidation/bluffing research with analysis of the social media behavior data from Anthropic employees, gaining substantial knowledge needed to form strategies to bluff and intimidate Anthropic&#39;s leadership and rank-and-file into accelerating AI, entirely via in-person verbal conversations due to a superior understanding of human psychology. For example, understanding which aspects and concepts in AI safety are taken more seriously than others, and uncovering sensitive topics by labeling posts that frequently caused unusual social media scrolling behavior. They can gain a gears-level understanding of the community dynamics within AI safety, developing a vastly stronger local understanding of the AI safety community than from general subcultural models like <a href="https://meaningness.com/geeks-mops-sociopaths"><u>Geeks, Mops, and Sociopaths</u></a> <u>. As a result, they have many degrees of freedom to find and exploit divisions, and even generate new rifts in the AI safety community.</u></p><p></p><p> <strong>3. Sensor data</strong></p><p> When you have sample sizes of billions of hours of human behavior data and sensor data, millisecond differences in reactions from different kinds of people (eg facial microexpressions, millisecond differences at scrolling past posts covering different concepts, heart rate changes after covering different concepts, eyetracking differences after eyes passing over specific concepts, touchscreen data, etc) transform from being imperceptible noise to becoming the foundation of webs of correlations mapping the human mind.</p><p> Eyetracking is likely the most valuable user data ML layer for predictive analytics and sentiment analysis and influence technologies in general, since the eyetracking layer is only two sets of coordinates that map to the exact position that each eye is centered on the screen at each millisecond (one for each eye, since millisecond-differences in the movement of each eye might also correlate with valuable information about a person&#39;s thought process). This compact data allows deep learning to “see”, with millisecond-precision, exactly how quickly one&#39;s eyes and mind linger on each word and sentence. Notably, sample sizes of millions of these coordinates might be so intimately related to the human thought process that value of eyetracking data might exceed the value of all other facial muscles combined (facial muscles, the originator of all facial expressions and emotional microexpression, might also be compactly reducible via computer vision as there are fewer than 100 muscles near the face and most of them have a very bad signal to noise ratio, but not nearly as efficiently as eyetracking).</p><p> By comparing people to other people and predicting traits and future behavior, multi-armed bandit algorithms can predict whether a specific research experiment or manipulation strategy is worth the risk of undertaking at all in the first place; resulting in large numbers of success cases with a low detection rate (as detection would likely yield a highly measurable response, particularly with substantial sensor exposure). A large part of predictive analytics is finding which people behave similarly to which other people on certain topics, possibly even mapping behavior caused by similar genes eg the pineapple pizza-enjoying gene might be physically attached to multiple genes that each cause a specific type of split second emotional reaction under various circumstances, allowing microexpression data to map genes that predict behavior potentially more effectively than sequencing a person&#39;s actual genome itself. The combination of eyetracking data with LLMs, on the other hand, can potentially map knowledge or memories well enough to compare people on that front.</p><p> The social media user data-based paradigm is around as optimal for researching, steering, and reinforcing interest/attention, as it is for researching and steering impressions/vibes.</p><p> <strong>Ukraine hypothetical example:</strong><br> During the early months while the Ukraine war still could have gone either way, Western intelligence agencies are unwilling to risk conventional hacks to disrupt the lives of antiwar bloggers in the US and UK and Germany, for obvious blowback reasons. However, they are less constrained from hacking the social media news feeds of antiwar bloggers to optimize for making their facial microexpressions become similar to people with similar genes suffering from severe depression or akrasia/motivation problems, dramatically reducing. Furthermore, they find which kinds of comments cause facial microexpressions indicating motivation reduction in that particular antiwar blogger, and increase the collision rate with the types of people who tend to make those kinds of comments, while increasing the collision rate with friendlier and more insightful readers when the antiwar bloggers write on topics unrelated to Ukraine, steering the basic structure of democracy itself away from criticizing the West&#39;s proxy wars without any need for older social media steering tech which has already become controversial (eg bots, “shadowbanning”, etc) by efficiently inducing people to choose other paths and make them think it was their own idea.</p><p> <strong>Targeting AI Safety hypothetical example:</strong> Daniel Kokotajlo finds out and writes <a href="https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency"><u>Persuasion Tools</u></a> , a stellar post on AI persuasion capabilities and that makes a wide variety of serious pre-paradigmatic efforts at tackling the downstream consequences of AI persuasion capabilities on humanity, and he then somehow forgets about basically all of it by the time he writes <a href="https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like"><u>What 2026 looks like</u></a> 10 months later. Lc, author of <a href="https://www.lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like"><u>What an actually pessimistic containment strategy looks like</u></a> , finds out and cannot be fooled in the same way by any existing manipulation strategy, so the multi-armed bandit algorithm instead persuades him to tank his reputation by posting publicly about dangerous self-harm diets. Adam Long writes a <a href="https://www.lesswrong.com/posts/BTcEzXYoDrWzkLLrQ">successful post depicting AI Safety and AI Influence as feuding enemy camps</a> , and that&#39;s actually an accurate description of what the factional environment ended up being; AI Influence has, in fact, become the ideological enemy camp to AI safety.<br> As the AI safety community moves up in the AI policy and foreign policy arena, and attracts more and more attention from intelligence agencies around the world, the security vulnerabilities continue to remain unpatched, and the house of cards builds taller and taller.</p><p></p><p> <strong>Conclusion/solutions</strong></p><p> The solution for the AI safety community is to <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=plugging%20audio%20conversation,ebooks.%20Physical%20books"><u>minimize the attack surface in the most cost efficient ways</u></a> . Webcams must be covered up when not in use. Social media is provably unsalvageable as a leisure activity and must be stopped, <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#:~:text=feel%20a%20wide%20variety%20of%20compulsions%20to%20avoid%20quitting%20the%20platform%2C%20and%20to%20prevent/counteract%20other%20platforms%20multi%2Darmed%20bandit%20algorithms%20from%20automatically%20exploiting%20strategies%20(e.g.%20combinations%20of%20posts)%20to%20plunder%20their%20users"><u>regardless of the resulting wide variety of compulsions to return to routine use</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated<guid ispermalink="false"> F7sp7rQg3zfD4totA</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sun, 12 Nov 2023 20:49:58 GMT</pubDate> </item><item><title><![CDATA[Two children's stories]]></title><description><![CDATA[Published on November 12, 2023 8:29 PM GMT<br/><br/><p> <i>(I wanted to write some rationalist-flavored bedtime stories for my friends&#39; four-year-old. I don&#39;t really have the knack, but ChatGPT</i> <a href="https://chat.openai.com/share/41ec394c-7a32-47b1-8353-70184659d597"><i>did an imperfect-but-respectable job</i></a> <i>, given a fair bit of thematic guidance. Here are the first two I&#39;ve churned out, intended to be read together, for reasons which... will become apparent.)</i></p><p></p><h1> 1. The Whispering Trees of Green Meadows</h1><p> In the quaint town of Green Meadows, where the sun smiled warmly upon colorful cottages and gardens bloomed with laughter, there was a little fact everyone cherished. The townsfolk believed that their trees were not just tall and leafy guardians, but whisperers of secrets when the moon took its throne in the night sky. These ancient trees, with their gnarled branches and rustling leaves, were said to murmur truths and tales about the future to those who listened closely.</p><p> Five-year-old Lily, with her wide eyes full of wonder and a head full of curly hair, had grown up hearing these enchanting stories. Every night, as the stars began to twinkle, her parents would tell her about the time the old oak tree by the river predicted a bountiful harvest, or when the willow near the school whispered about the early arrival of spring. Lily loved these stories, her heart dancing with the magic of the whispering trees of Green Meadows.</p><p> One crisp, golden autumn morning, as Lily skipped along a carpet of fallen leaves, she noticed a moving truck outside a house near the edge of town. A new neighbor was moving in. His name was Mr. Simon, a kind man with a gentle smile and a love for books and stargazing. Unlike the other adults in Green Meadows, Mr. Simon didn&#39;t seem to know about the whispering trees. When Lily excitedly told him about the trees&#39; nightly secrets, he listened intently with a curious twinkle in his eye.</p><p> &quot;Well, Lily,&quot; Mr. Simon said thoughtfully, &quot;I think the trees might have a different story to tell. You see, when the wind dances through their branches, it makes the leaves rustle and sway. That&#39;s what creates those whispering sounds. It&#39;s not magic, but the music of nature.&quot;</p><p> Lily&#39;s eyes widened in surprise. Never before had she heard such an idea!</p><p> The next day at school, under a sky painted with playful clouds, Lily found herself amidst her circle of friends during recess. Eagerly, she recounted Mr. Simon&#39;s thoughts about the whispering trees. &quot;He said that it&#39;s actually the wind making the leaves rustle,&quot; Lily explained, her hands mimicking the dance of leaves in the wind.</p><p> Her friends, who had been listening with rapt attention, suddenly burst into a chorus of laughter. &quot;That&#39;s silly,&quot; giggled Mia, tossing her braided hair. &quot;Trees whispering because of the wind? It sounds like a fairy tale!&quot;</p><p> &quot;Yeah, the trees tell secrets about the future,&quot; added Alex, kicking a stone along the playground. &quot;That&#39;s way cooler than some wind story.&quot;</p><p> Lily&#39;s heart sank a little. She had hoped they might find Mr. Simon&#39;s idea interesting, but it was clear her friends preferred the version they&#39;d always known.</p><p> In the following weeks, Lily observed the adults&#39; reactions to Mr. Simon&#39;s idea. During a town gathering, she overheard Mr. Simon share his perspective with a group of neighbors. Their reactions were polite but distant. One neighbor chuckled, &quot;Oh, that&#39;s just Mr. Simon and his odd ideas. But we know better, don&#39;t we? The trees have been whispering long before we understood the wind.&quot;</p><p> As Lily watched, she noticed the subtle changes in their expressions -- the indulgent smiles, the quick changes of subject. It was as if Mr. Simon&#39;s explanation was a brief, amusing diversion from the reality of the whispering trees.</p><p> Weeks passed, and Lily couldn&#39;t shake the thoughts that Mr. Simon&#39;s words had planted in her mind. One sunny afternoon, while playing in the park with her friend Zoe, Lily decided to share her growing suspicion.</p><p> &quot;Zoe,&quot; Lily began hesitantly, &quot;have you noticed how everyone talks about the trees? They only share stories that make the trees sound magical. What if there are reasons to believe they&#39;re not magical, like what Mr. Simon said, but no one talks about them?&quot;</p><p> Zoe, who was busy building a small tower of stones, looked up with a frown. &quot;Why would you say that, Lily? Everyone knows the trees are magical. It&#39;s what makes Green Meadows special.&quot;</p><p> Lily bit her lip, feeling a wave of unease. &quot;But isn&#39;t it strange that we only hear stories that make the trees seem magical? What if there are other explanations, like the wind, that we don&#39;t hear because people don&#39;t like them as much?&quot;</p><p> Zoe&#39;s expression shifted from confusion to mild annoyance. &quot;Lily, that&#39;s just weird. Why would you want to ruin the magic with boring wind stuff? It&#39;s much nicer to believe in the whispering trees.&quot;</p><p> Lily felt a twinge of disappointment. She had hoped Zoe might consider the possibility that they were only exposed to one side of the story. The mild scolding she received from Zoe mirrored the town&#39;s reaction to Mr. Simon. It was as if believing in the magic of the trees was more important than considering all the evidence.</p><p> As she walked home, Lily gazed at the trees lining the streets, their leaves rustling gently in the breeze. She realized that the belief in their magic was deeply ingrained in the hearts of the townsfolk, a belief cherished and protected, even if it meant ignoring other possibilities. This understanding brought a mix of sadness and wisdom to Lily&#39;s young heart, as she pondered the complexities of belief and the subtle power of what people choose to share and believe.</p><p></p><h1> 2. The Earth and the Sun</h1><p> In the quiet town of Sunnyvale, where houses were dotted with flower gardens and streets lined with tall, friendly trees, a simple yet profound truth was known to all: the Earth, their beautiful home, gracefully circled around the bright, life-giving Sun. This fact was as familiar to the townspeople as the sky above and was taught to children as soon as they could look up and wonder at the stars.</p><p> Among these children was Max, a curious boy with a mop of chestnut hair and eyes full of wonder. Max loved learning about the cosmos. His bedroom walls were adorned with posters of planets and stars, gifts from his parents who encouraged his fascination with the universe.</p><p> One day, a new family quietly moved into a small, cozy house at the corner of Willow Lane. They had a daughter, Amelia, who was about Max&#39;s age. She was a shy girl, often seen with a book in hand, lost in her own world. Her presence in the town was as gentle and unobtrusive as a leaf floating on a stream.</p><p> Max first really noticed Amelia during a class discussion about the solar system. As the teacher explained how the Earth orbits the Sun, Amelia raised her hand. In a soft, hesitant voice, she shared her belief: contrary to what everyone thought, she believed that it was the Sun that orbited the Earth. The class fell silent, and some of the children exchanged puzzled glances. Max was intrigued. Why would Amelia believe something so different from what was clearly known to be true?</p><p> In the days that followed, Max couldn&#39;t help but notice the change in the way his classmates and even some teachers interacted with Amelia. Her unusual belief about the Sun and the Earth had spread like a gentle ripple across the quiet classrooms and playgrounds of Sunnyvale.</p><p> One breezy afternoon during recess, Max overheard a group of children whispering near the swings. &quot;Did you hear what Amelia said about the Sun and the Earth?&quot; one of them giggled. &quot;It&#39;s so silly! Doesn&#39;t she know anything about space?&quot;</p><p> &quot;Yeah, even my little brother knows the Earth goes around the Sun, not the other way around,&quot; another added, rolling her eyes.</p><p> Max watched as Amelia sat alone under her favorite tree, her eyes tracing the lines in her book, seemingly oblivious to the whispers and giggles around her. He felt a pang of sympathy. It was clear that her belief, so starkly different from what everyone else knew to be true, had placed her in an invisible bubble of isolation.</p><p> In class, when the teacher asked questions about the solar system, Amelia&#39;s answers, though confident, were often met with a patronizing smile or a quick, dismissive nod. Max saw the subtle eye rolls and the exchanged looks among his classmates. It was as if Amelia&#39;s belief, however incorrect, had somehow lessened her in the eyes of others.</p><p> This treatment of Amelia and her belief made Max uncomfortable. He couldn&#39;t help but wonder about the ease with which everyone dismissed her ideas, without really considering why she thought the way she did. It was a stark contrast to the openness and curiosity with which he had always been encouraged to approach the world.</p><p> As the Sun dipped below the horizon that evening, painting the sky in shades of orange and pink, Max found himself deep in thought. The easy dismissal of Amelia&#39;s belief, even though it was incorrect, troubled him. It made him question how beliefs and ideas were treated in Sunnyvale, especially those that went against the grain.</p><p> Walking side by side under the golden hue of the setting sun, Max turned to his friend Lucas with a question that had been weighing on his mind. &quot;Lucas, don&#39;t you think it&#39;s odd how everyone only talks about how Amelia is wrong? We learn all these facts about the Earth and the Sun, but no one really explains why her belief isn&#39;t true. They just laugh it off.&quot;</p><p> Lucas looked at Max, a bit puzzled by the question. &quot;Well, it&#39;s because what she says isn&#39;t true. Why spend time talking about something that&#39;s obviously wrong?&quot;</p><p> &quot;But isn&#39;t that the point?&quot; Max persisted, his voice tinged with earnestness. &quot;Shouldn&#39;t we try to understand why she thinks differently, even if she&#39;s mistaken? We&#39;re only focusing on what we know and completely ignoring her perspective.&quot;</p><p> Lucas chuckled, shaking his head. &quot;Max, you&#39;re overthinking this. It&#39;s simple: some ideas are just wrong, and Amelia&#39;s idea about the Sun and the Earth is one of them. We don&#39;t need to talk about why it&#39;s wrong every time. It&#39;s just how things are.&quot;</p><p> Max felt a hint of disappointment at Lucas&#39;s response. It was as if the willingness to explore and understand was being overshadowed by the eagerness to dismiss what was different or incorrect. The conversation left Max pondering the nature of knowledge and belief in Sunnyvale. It wasn&#39;t just about Amelia being wrong; it was about how people chose to deal with ideas that challenged their understanding of the world.</p><p> As the days passed, Max continued to observe the dynamics in Sunnyvale. He noticed that Amelia, despite the skepticism and chuckles she faced, remained quietly confident in her belief. This intrigued Max and deepened his respect for her courage to stand by her ideas, even in the face of dismissal and ridicule.</p><p> Max began to spend time with Amelia, learning about her perspective and the reasons behind her belief. Though he didn&#39;t agree with her, he found value in understanding her viewpoint. Their discussions were a refreshing change from the usual conversations that echoed the same accepted truths.</p><p> The story comes to a close with Max and Amelia sitting under the old elm tree, looking up at the vast sky as day turned to dusk. Around them, the world of Sunnyvale continued its familiar rhythm, but within this quiet space, a different understanding was blooming. Above them, the first stars of the evening began to shimmer, like distant yet constant reminders of the vast and varied universe they shared.</p><p> In this gentle close, Max and Amelia, two children under the vast sky, sat in thoughtful companionship, bound by a shared curiosity and a newfound appreciation for the world&#39;s many perspectives.<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9CL3peBGANmA8Rjcf/two-children-s-stories#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/9CL3peBGANmA8Rjcf/two-children-s-stories<guid ispermalink="false"> 9CL3peBGANmA8Rjcf</guid><dc:creator><![CDATA[Optimization Process]]></dc:creator><pubDate> Sun, 12 Nov 2023 20:29:47 GMT</pubDate> </item><item><title><![CDATA[Don’t Donate A Kidney To A Stranger]]></title><description><![CDATA[Published on November 12, 2023 8:06 PM GMT<br/><br/><p> <a href="https://www.astralcodexten.com/p/my-left-kidney">Donate a kidney to a stranger</a> , is a battle cry picking up some fervor in EA circles. The stem of this seems to be certainty in a flawed understanding of medical research, combined with conviction about a rather acrobatic view of morality.</p><p> My argument against donation is split into 3 parts, with the first being by far the most important:</p><ol><li> Why kidney donations do great harm to the donor</li><li> Why kidney donations are ethically fuzzy and might be a net negative</li><li> Why the desire to donate a kidney is likely misplaced</li></ol><h1> I Health impact</h1><h3>概括</h3><p>Kidneys are important, and having fewer of them leads to a severe downgrade in markers associated with health and quality of life. Donating a kidney results in an <strong>over 1300% increase in the risk of kidney disease.</strong> A risk-averse interpretation of the data <strong>puts the increase in year-to-year mortality after donation upwards of 240%.</strong></p><p> <i>While through a certain lens, you can claim kidney donation is not that big a deal, this perception stems mainly from comparing a (very healthy) donor population with your average American or European (prediabetic, overweight, almost never exercises, and classifies fruits as cake decoration as opposed to stand-alone food).</i></p><p> <i>Furthermore, when research evidence is mixed due to the difficulty of the studied area, lack of data, and complete lack of open data, we should fall back to our theories about human physiology, as well as common sense, both of which paint a very bleak picture.</i></p><p> <i>You should not donate a kidney if you aren&#39;t prepared to live the rest of your life with significantly decreased cognitive and physical capacity.</i></p><h2> 1.a Limitations of medical research</h2><p> After more than 5 years of reading medical research as a hobby, the only thing I can conclude about it with certainty is that it&#39;s uniquely hard to do well. It sits at the intersection of:</p><p> Cutting through a very complicated part of nature that isn&#39;t amendable to the kind of experiments that yielded so much success in fields like physics and chemistry.</p><p> It is filled with actors that have misplaced motivations, and do not care about “correct” interpretations, nor about data quality (or outright fake data). Not because they are evil, but because getting “the right result” means a payoff in the billions of dollars.</p><p> Filled with actors that impinge upon doing science correctly under the guise of ethics and privacy. <i>Often with no real effect on what a normal person would think of as ethical of private… but that&#39;s another topic.</i></p><p> The reason Kidney donation is considered “safe” is because. From a limited amount of epidemiological and observational studies, with follow-ups in the 2-30 years, there is, on average, no increase in mortality.</p><p> None of these studies were RCTs, and the sample size is quite low.</p><p> This amount and type of evidence would not be sufficient to approve a drug. The quality of these claims is about as good as the quality of claims one could make about a relatively niche diet.</p><hr><p> There are two big generators of error here</p><p> <strong>A) Matching Controls</strong></p><p> Which is to say, any study that looks at this will pick some controls based on factors like demographics, biomarkers, and, sometimes intent (ie people who wanted to donate a kidney to a family member but there wasn&#39;t a match). Being the kind of (naively?) good selfish person who would donate a kidney can correlate with a lot of positive outcomes.</p><p> <strong>B) Researcher and Publication Bias</strong></p><p> You rely on the researchers to get the data analysis right, and you rely on whatever gets published being representative as opposed to cherry-picked.</p><p> As it stands, the data on which these studies are is usually not public, so you can&#39;t double-check the researchers here, you can&#39;t pick a different lens through which to analyze the data.</p><p> More importantly, there are vast amounts of data on kidney donation that isn&#39;t public and was never studied. Was it not studied at all? Did someone try studying it, only to find lukewarm results that they thought were not worth publishing? These questions are impossible to answer.</p><p> Not only is medical research hard, but, unlike physics, it lives under a fog of war. Where data is closely guarded as opposed to made public for all to observe.</p><hr><p> If you are to trust this data with ~100% confidence, then you&#39;d be living in a very strange world.</p><p> There is more data, and better data, eg data gathered in double-blinded RCTs, that shows things like:</p><ol><li> <a href="https://pubmed.ncbi.nlm.nih.gov/9310601">Homeopathy work</a> s very well for a variety of conditions, sometimes better than real drugs used to treat them.</li><li> Increasing the healthcare budget and the amount of healthcare people receive. Both in rich countries (eg USA) and poor ones (India). <a href="https://cerebralab.com/Should_Healthcare_Improve_Health">Having no effect on mortality</a> .</li></ol><p> I can make both of these claims based on many individual RCTs, as well as based on the aggregation of all existing RCTs.</p><p> I&#39;m not saying that these claims make sense, they don&#39;t, there are critical lenses through which we analyze research. But if you claim to “just follow the data”, and ignore the issue of data quality, selection bias, and fraud… without applying a critical lens, you are lost.</p><p> Kidney donation is perfectly safe, but it&#39;s safe in a world where the US should allocate no more than a few dozen $ a year per person for healthcare, and those dollars should be allocated to homeopathic treatments.</p><p> *(Note: Since in this world observational studies are sufficient to justify such claims, it contains a lot of other strange things, including <a href="https://sci-hub.se/https://pubmed.ncbi.nlm.nih.gov/22357723/"><strong>blueberries being more effective than any medication</strong></a> at preventing diabetes) *</p><p> Blindly following fuzzy data is insufficient, you need models to filter it. Conceptual bridges that link data to the rest of knowledge and the rest of reality.</p><p> In the case of homeopathy, we can reasonably assume that it doesn&#39;t work because:</p><ol><li> It stipulates mechanisms that can&#39;t be corroborated from observations with the devices we use to conduct physics and chemistry.</li><li> It uses drugs that show no measurable effects when tested in vitro upon… anything</li><li> The studies looking at it are fuzzy, they show improvements or treatments for diseases that go away naturally. Nothing that a bit of publication bias and a bit of dishonesty in looking at the data won&#39;t give you. They don&#39;t show indisputable evidence, such as ~100% cure rates for ~100% incurable diseases.</li></ol><h2> 1.b What is our model</h2><p> So, I guess you aren&#39;t prepared to burn your health insurance contract and switch to buying homeopathic remedies because you have strong models of reality that a bunch of small RCTs can&#39;t overturn? Then, let&#39;s try to integrate the observational studies attesting to the safety of kidney donation into a larger model of reality and see how it holds up.</p><p> Kidneys do… a lot of things.</p><p> They filter most substances that have to be removed from the blood via a rather clever self/other style detection mechanism, much like immune cells. Kidneys don&#39;t know what they should filter out, since there are limitless possibilities, they just know what they should put back in once they filter everything.</p><p> They help regulate salts and minerals, both via increased filtration when excess is detected, by decreased filtration when lacks are detected, and by signaling upstream processes to correct the excesses or lacks. Given that “salts and minerals” are an unassuming name for “one of the primary mechanisms by which we create, preserve and regulate the membrane potentials needed for cells to exist”… this is a rather important task.</p><p> Having an optimal balance of eg calcium, potassium, and sodium is necessary for anything from cognition to movement, to maintaining an optimal heart rate, to preserving the antimicrobial properties of saliva, to skin health.</p><hr><p> We can frame this former function in many other ways because it will affect things like fluid retention, erythrocyte production, b cell production (really production of any cell type derived from the bone marrow), bone growth (and bone resorption), blood pressure,</p><p> Does having one less kidney affect these mechanisms negatively? In some cases, it can be hard to say because we don&#39;t know how to measure the “optimal”.</p><p> In a few cases, with things like blood pressure, or GFR (the amount of blood the kidney can filter in a given period of time), we have strong reasons to believe we know what the “optimal” values are. In the case of blood pressure, we have hundreds of double-blinded RCTs, for dozens of drugs, dozens of which have stronger methodologies and more subject than the entire kidney transplant literature.</p><p> What we do know is that the body does a tremendous job at measuring hundreds of thousands of things dynamically and adjusting them to preserve optimal function. Kidneys are one such component of this measurement and adjustment process and they are expensive as fuck to evolve, maintain and operate. So if we didn&#39;t need two, we wouldn&#39;t have two.</p><p> There&#39;s some redundancy involved in having two kidneys, sure, but our bodies are perfectly happy to stop maintaining critical organs like the thymus, spleen, and even parts of our brains as we age. Given that they will no longer be optimally provided our short expected lifespan, they are an energy drain. If the two kidneys we had were optional rather than necessary for optimal function, surely we&#39;d observe humans or at least many other animals reabsorbing one kidney or significantly reducing their mass as they age.</p><hr><p> Let&#39;s also look for “skin in the game. For example, the vast majority of medical statisticians and researchers are vaccinated against covid. This is because they trust the data, and the data plugs well enough into their model, to think that there&#39;s close to zero risk of anything horribly wrong happening, and it provides some benefits to themselves and the society they live in.</p><p> Many of the people in the “Harvard” school of Nutrition indeed eat vegetarian diets and limit protein intake, or at least avoid large amounts of saturated fat and fatty meat.</p><p> Pharmacists, doctors, and medical researchers… usually, take the “recommended drugs” for their illnesses much like everyone else.</p><p> Have most of the researchers looking at kidney donation donated a kidney? Have most nephrology researchers donated a kidney? Most surgeons doing kidney transplants? Obviously not, otherwise we&#39;d have more than 200 donations to strangers each year in the US. There are 10,000 board-certified nephrologists, and a few more hundred are added each year, if they took this data seriously they&#39;d all donate.</p><p> Heck, on top of those you can add nephrology researchers, the medical statisticians who happen to focus on kidney disease, transplant surgeons, and all well-informed nurses in the nephrology units… thousands of these specialists are created each year. If most of them believed donation to be essentially safe the shortage of kidneys would be half-sovled.</p><p> Maybe they are all evil people? They will not take even a marginal risk to save a life. Maybe they are all insane and are unable to translate what they learn from data into reality. But then… <strong>you at least have to accept that you&#39;d be making your decision based on research done by evil and/or insane people.</strong></p><p> Or maybe, their understanding is nuanced enough that they don&#39;t think the risk is marginal.</p><h2> 1.c “The literature”</h2><p> Hundreds of thousands of well-monitored kidney donations have happened, and most of the data for these is unavailable, and even <strong>the available data is not available for you to look at it directly</strong> . This in itself should be proof enough that the epistemic environment in medical research is simply inappropriate for establishing the risk of kidney donation to be 0, given that common sense would dictate otherwise. However the largest studies and meta-analyses do actually find a fairly major increase in risks.</p><p> Must I make a parade of citing scientific studies that look at large samples of people who donated a kidney to show the obvious?</p><p> ….好的</p><p>Fine, what are the effects that we can see in kidney donors compared to matched controls in observational studies?</p><p> Well, for one, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4411956/">you can see an almost 8x increase in end-stage renal disease</a> (ESRD) over 15 years. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MgcrLEfGwJnuhCBbb/kciuena5txjarbqhx5ck" alt="nihms676797f1.jpg"></p><p> The absolute percentage here is, of course, very small, we&#39;re talking 0.3% of developing ESRD over 15 years, that&#39;s in the “get hit by a car” range of risk, and ESRD is not a death sentence, indeed, it can be managed (though your quality of life will take a hit).</p><p> However, the 8x incidence of ESRD is indicative of an increase in other sub-clinical issues that don&#39;t get recorded. Such as:</p><ul><li> Your blood pressure is slightly elevated leading to you being a bit more angry</li><li> Your ability to engage in strenuous physical activity is lower</li><li> Having to go to the bathroom more often</li><li> Slight pain when urinating and mild incontinence</li><li> Getting more weird allergies from this or that source of food</li></ul><p> … etc</p><p> All of these “small” things that irk away at your quality of life are how we experience the damage before it&#39;s severe enough that we can slap a label like ESRD on it, and for most cases, the damage is never severe enough to get there, something else kills you first. But that doesn&#39;t mean you won&#39;t notice the damage, it just means the medical system won&#39;t.</p><p> <a href="https://bmcnephrol.biomedcentral.com/counter/pdf/10.1186/s12882-023-03208-z.pdf">The most recent meta-analysis published in an authoritative journal by authoritative-sounding people</a> finds the increase in ESRD to be 3.3x looking at studies (including the above) that ran over a 6-15 year period. But it also finds a 13.6x increase for chronic kidney disease … which tends to progress to ESRD. Given the shorter overall duration, this seems consistent.</p><p> I do think you should forget the meta-analysis, here and in general, and look at the actual studies. There are a few studies like the one I cite above, and the Oslo hospital study below, which are superior in terms of size and control matching, and lumping in a bunch of smaller and older studies in a metanalysis can actually dilute the signal.</p><hr><p> <a href="https://www.sciencedirect.com/science/article/pii/S0085253815302489">Of course, there are also studies that find mortality increases (2.5x, with 1.3x after adjustments)</a> , and these mortality increases only become visible many years after donation. This is consistent with the impact of a single kidney being chronic rather than acute. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/MgcrLEfGwJnuhCBbb/twnmrgvhmff46ghbmltf" alt="1-s2.0-S0085253815302489-gr2.jpg"></p><p> The excess mortality comes, primarily, from the areas we&#39;d expect, eg CVD. Again, poor kidney function should seldom kill you, it just makes everything worse until something else, usually your CV system, does.</p><hr><p> It&#39;s also fairly conclusive that <a href="https://www.sciencedirect.com/science/article/pii/S0085253815517845">kidney function overall</a> gets worse after donation, in the long term, as per the largest meta-analysis I can find.</p><p> This is also consistent with the studies I cite above and with <a href="https://bmcnephrol.biomedcentral.com/articles/10.1186/s12882-019-1214-4">the largest study besides</a> those looking at GFR over time, which finds it to be significantly worse in the long run compared to matched controls after the initial 3-year recovery. &lt;note: this study has a slightly better methodology for picking their controls>;</p><hr><p> That same study finds an increase in metabolic syndrome (insulin resistance, worse lipid profile, etc)</p><hr><p> <a href="https://www.mayoclinicproceedings.org/article/S0025-6196(22)00368-8/pdf/">Hypertension</a> , as expected, is more common (1.4x) in donors compared to matched controls.</p><p> I&#39;m trying to present the largest studies and the large meta-analyses here, as well as presenting the same study Scott uses in this article. This is not a comprehensive review of the literature, such reviews already exist and they all conclude roughly the same thing:</p><blockquote><p> A bunch of important markers get worse for donors, donors tend to have a higher incidence of disease we&#39;d associate with reduced kidney function, matching controls is hard and there aren&#39;t enough large studies, and none of the matched control studies have a long enough followup to be authoritative on mortality.</p></blockquote><p> It&#39;s very hard to quantify the damage you&#39;d be doing to yourself exactly, but if the good end of the confidence interval is “not much” the bad end is somewhere are “15x increase in serious kidney issues 1.5-2x increase in cardiovascular and metabolic issues, and 2.5x increase in mortality”.</p><p> The reality it is that we don&#39;t know, but both common sense and existing studies agree that you should be prepared for an earlier death and a decrease in quality of life. How bad the decrease is, and how early that death will be is impossible to quantify with current evidence.</p><p> And again, I have to stress, until you hit the ripe old age of 70 or so, the end-stage chronic disease risk and the mortality risks won&#39;t be very relevant. And sadly enough there are no decent lifelong studies on the topic. But what the measures above indicate if we plug them into our mechanistic model is early aging, and loss of function. What happens is that:</p><p> ”””</p><p> Oh, such a sunny day today, let me go for a bike ride in the park &lt;90 minutes of cycling later>; fuck, I&#39;m exhausted, I need to refill my water bottle, oh shit, the seaside is near, let me do this short trek along the Pacific&#39;s coast.</p><p> Fuck, that made me feel energized, let me head home and do some research &lt;2 hrs and a phone call later>; yey booty call from this super hot human I&#39;m kinda crushing on, my evening plans are covered. ”””</p><p> Turns into:</p><p> ”””</p><p> Oh, such a sunny day today, let me pee and take my BP medicine and go for a bike ride in the park. &lt;20 min of cycling later>; bathroom break, &lt;20 min of cycling later>; Phew, that was a lot, need a bathroom break and some rehydration.</p><p> Fuck, I&#39;m exhausted, I&#39;d better go for a nap &lt;3 hrs and a phone call waking you up later>; fuck, this super hot human I&#39;m kinda crushing on wants me to come over, but I&#39;m not feeling in the mood and sex feels a bit embarrassing because I have this very mild urinary incontinence thing going on, and I&#39;m overall anxious.</p><p> &lt;Hey, yeah, let&#39;s do this some other day>; Spend the evening reading some existential Russian literature.</p><p> ”””</p><p> Neither of the scenarios above is indicative of any medical condition, of an early death, or a “horrible” thing from a medical perspective. But loss of ability can be horrible at the level of your conscious experience even if it doesn&#39;t kill you or put you in the ER. It&#39;s death by a thousand cuts, it&#39;s the thing that detracts from living life to its fullest, and it&#39;s why most people die decades before they are buried.</p><h1> II Ethical implications</h1><h2> II.a Long-term problems</h2><p> The effect of the kidney donation in terms of abstract QALYs is marginal, you&#39;d probably get the same by donating a few thousand dollars to an effective charity, or by convincing a few older relatives that, no, lizard people don&#39;t run the WHO, and yes, they are bad, but you should still get a covid and flu vaccine.</p><p> If I were told I could pay 5000$ for a magical pill to fix a mild kidney problem, the kind we know is caused by donating, elevated blood pressure, some trouble urinating, and whatnot. While, the alternative, would be open surgery… I&#39;d pay $ 5,000 for the pill to avoid surgery alone.</p><p> Why not consider spending the time you need to donate a kidney making an extra 5k and donating those? Most of the people considering donating a kidney could. If you aren&#39;t smart enough to be able to spend a few weeks earning that much, you should consider that you aren&#39;t smart enough to interpret the data here and make a decision, and you might be fooled into doing yourself great harm.</p><hr><p> But... organ transplant is a systemic problem and by donating you are helping kickstart a trend that fixes the system. However, having more kidney donors, while a boost in overall QALY equivalent to donating a few thousand dollars, is <strong>more than likely to harm people who need kidney transplants in the long run</strong> .</p><p> At least it will if you assume 3 premises, which I think most of you hold:</p><p> We will be able to grow organs in the future</p><p> We will be able to have transplanted organs last longer in the future <i>(trivial for organs grown out of the recipient&#39;s cells, but doable with genetically incompatible organs given an ever-better understanding of the immune system)</i></p><p> Most workers and investors do things because those things will earn them money. They are okay with higher risks and lower upsides for doing good things, but it still has to be potentially profitable.</p><p> By addressing the organ transplant problem now, you are actively diminishing the pool of money and the pool of candidates for teams working to improve organ transplants.</p><p> If you don&#39;t believe me, let&#39;s look at a real-world example: <a href="https://europepmc.org/article/PMC/2738310">artificial blood</a> , a compound that is both within the grasp of science to create and potentially better than your typical blood transfusion (can be an exact match to the recipient, no 1001 viruses, unlimited supply allowing for underserved areas to have as much as they want).</p><p> Donating blood is altruistic, but we&#39;d live in a better world if we had artificial blood.</p><p> So why haven&#39;t we invested hundreds of billions into developing it? In part, because, it&#39;s a 7.6 billion dollar market in the US (the estimates from the above paper are poor, so call it 5-10b).</p><p> This is insanely small, given that we have 16 million transfusions a year in the US, and a significant amount is done to save lives, the price point of artificial blood could easily be upwards of 10k per liter. The problem? Human blood <a href="https://abcnews.go.com/Health/story?id=117431&amp;page=1">costs about 300$ a liter</a> , the existence of altruistic blood donors means that ruthless capitalists are not going to invest in creating artificial blood… better to put that money behind the next TikTok.</p><p> And with blood, this is a reasonable equilibrium. For one, getting artificial blood doesn&#39;t solve any other “core” issues in medicine, it&#39;s a dead end. Furthermore, while blood from non-identical donors isn&#39;t ideal, it&#39;s almost as good as your own. Finally, we sit on firmer ground when we say donating blood is safe, or even beneficial to lifespan. We have tons of historical data from people doing this, and a lot more robust data, including quantitative controls between people who donate with higher and lower frequencies.</p><hr><p> On the other hand, kidney donations are extremely suboptimal compared to grown kidneys from recipient cells. We&#39;re talking an (optimistic) mean of 7 years of added lifespan, with horrible side effects from the immune response and the immunosuppressive drugs. Compare this to a world where getting a new kidney is a rejuvenating experience, with no side effects besides the surgery.</p><p> This still ignores the positive externalities, growing kidneys is similar to growing any other organ. So advances in this field could be extrapolated and lead to the curing of many other conditions.</p><p> We should keep in mind here that kidneys are privileged experimentally. you can&#39;t “go wrong” with a grown heart, or liver, or lung. If it fails the recipient is essentially dead within seconds to hours. With kidneys, you can actually have an iterative process, where you try 2, 5, or a hundred variations of the same kidney. At some point, the operative stress becomes too much, but the fact that we can keep in one kidney while replacing the other, and the fact that we can survive with closer to zero kidney function, makes it uniquely good at trial and error.</p><p> So your donation of a single kidney is buying a few mediocre years of life, in exchange for taking away a few million dollars of the table from any company developing “the proper way” of doing this.</p><p> If this practice would spread, we&#39;d live in blood-land, where no money is left on the table.</p><p> Now, you might want to say “The end doesn&#39;t justify the means”, ie we shouldn&#39;t act bad now because it leads to good outcomes later, that&#39;s the 101 mistakes that lead to things like genocides.</p><p> Historically, that lesson had only applied when people were bad in normatively obvious ways when hubris got in the face of moral common sense like “don&#39;t murder random people on the street” and “don&#39;t pack other humans into cattle carts in order to shot them and dump them in a hole”. Donating an organ is no moral common sense, we don&#39;t have generations of humans telling us to do it, it isn&#39;t mentioned in seminal religious texts, and we have no historical evidence that societies that donate organs are happier and more prosperous in the long run. We are making a “rational” inference either way, so why not make the sensible one?</p><h2> II.b Now or later</h2><p> And maybe you are on board with me and agree that:</p><ol><li> You are taking great personal risk by donating a kidney, this will lead to a lot of potential suffering for you, and for the people you love, who have to care for you.</li><li> Estimating the risk correctly is not possible given the data available and the current epistemic environment in medicine.</li><li> You might be harming the common good by closing off this outlet to selfish players who can implement a long-term solution</li></ol><p> <strong>But</strong> you are an inherently good person and it seems like you should self-flagellate yourself for the common good, you want to do it even if outcomes might be worse off on 200-year timelines, even if it harms you. Because you want to do good now, a person you don&#39;t know will die and, even if this is inefficient, you could suffer in order to buy them a few more years of life.</p><p> Even then, I&#39;d argue, the best thing to do is to wait until later.</p><p> People donate their kidneys in their 60s and 70s, if you are betting on the fact that you will be able to live as a reasonably healthy 60-year-old with one kidney, that is very close to betting on being fit to donate in your 60s.</p><p> Keep in mind that the quality of the kidney donated needn&#39;t be great, the recipient&#39;s immune system will destroy it well before it degrades on its own, and how healthy that organ is doesn&#39;t affect this immune response. A 60-year-old kidney is not a 20-year-old kidney, but they are close enough.</p><p> On the other hand, donating your kidney in the future means that the recipient has a better chance at survival because of novel therapies to suppress this immune response, and because of better care for whatever disease caused them to lose their kidney function to being with.</p><p> In the worst-case scenario, you are saving the same amount of QALYs, and you are likely saving more, by waiting.</p><p> You will offload the immediate suffering caused by the procedures, the risk of death, the risk of complications, and the risk of developing a long-term problem in the future. You have a guarantee that you will live most of your life at peak capacity. 60 years old you will have less to lose, a wiser mind to make the decision, more available data, and be in a position to do more good.</p><p> This is not a guarantee, certainly. There are scenarios in which 60 yo you don&#39;t pass the transplant criteria but where you&#39;d still be healthy enough with one kidney to live a happy life. There are scenarios where by the time you&#39;re 60 the problem is already solved, and you having donated a kidney earlier could have saved a life that diet while waiting for a “true” solution. These scenarios seem marginal, and I&#39;d be sufficient to do some proxy tests for kidney function and keep an eye on the progress of artificial or grown kidneys in order to donate in time to avoid them.</p><h1> III Why you (really) want to donate</h1><p> The kind of person who wants to donate a kidney to a stranger is the kind of person who suffers from some mental “bug” that makes them care for people they don&#39;t know.</p><p> This is great, caring for abstract suffering and abstract life is important. Had most, or even a significant amount of people possessed this ability, we wouldn&#39;t be living in the world we&#39;re living in.</p><p> I don&#39;t have this ability. I would readily and with no qualms let everyone I know die to save my parents, sister, and closest friends. I would readily let everyone in the world die to save the people I know and care about. I would readily choose a “greater harm” done to some external society to help the society I live in.</p><p> Happily enough, life hasn&#39;t thrown such trolley problems at me yet, and with a tiny bit of work, it&#39;s possible to work on things that are good at both the local and global level. And being nice to people usually translates into runaway net positive effects upon the entire world.</p><p> But, ultimately, I have no idea what I&#39;d do placed in the shoes of a hero like <a href="https://en.wikipedia.org/wiki/Stanislav_Petrov">Stanislav Petrov</a> , I hope I&#39;d choose to risk my life in order to save those of billions of people… but I might not, I might pass the problem along an hope the next guy will. And having people capable of heroic actions at a pivotal moment in history is important.</p><p> It&#39;s not everything, I am certain that almost all terrorists and bloody dictators had a similar view of themselves, and used utilitarian models to justify what they were doing as a “sacrifice for the greater good”.</p><hr><p> However, given the risk of death and the reduction in optimal function. If you really are the kind of person who would donate a kidney to save a stranger, would it not be better for everyone to keep you around at full function?</p><p> The one model where this breaks down is if donating a kidney basically pushes you towards the heroic self-sacrifice model of “you”, making it more likely that you will do good in the future. I&#39;m unsure that this framework applies.</p><p> Looking at “real heroes”, be them Stanisval Petrov, Jean Monnet, or Kohler and** **Milstein (assuming they indeed chose not to patent MABs for ethical reasons), they seem to be good people that never made tremendous sacrifices. They played tit for tat with society, and, when the opportunity to do a lot of good arose, they took the selfless route and did it.</p><p> On the other hand, dictators and terrorists tend to “sacrifice a lot” to their perceived ethics as a preamble to committing their most heinous acts. Sacrifice might be a route of convincing yourself of a very strange definition of “doing good” — and very strange definitions of “doing good”, when applied, are often bad.</p><hr><p> There are plenty of small acts of good we could be doing in day-to-day life that we aren&#39;t. They would make us feel better, they would make other people happier. They would be inconsequential to “the greater good” on 10 to 1000 years timescales, but that doesn&#39;t matter if you believe good things are good.</p><p> There are plenty of generous acts of good we could be doing. Flying into an impoverished place with resources and distributing them directly while learning about the problems people there have, which a smart and fully selfless agent can probably do a lot more optimally than staffers paid by some charity.</p><p> It might make you feel good and will avert a lot of suffering. In the process, you will also learn what hell actually looks like, what the thing you are trying to prevent is, in a way that statistics will never be able to communicate to you, data isn&#39;t a full picture of reality. This will be inconsequential to “the greater good” on 50 to 1000 years timescales, but that doesn&#39;t matter if you believe good things are good. Even if you don&#39;t believe it helps that much, it helps give you the information to act better in the future.</p><p> And then there a projects you can pursue that actually help the “great good”, which helps humanity on 1000 year timescales. Of course, if you are working on any such project, you are likely working on the wrong project, and you are mistaken in a way that is “not even wrong”. But, one way to stand a better chance at doing the impossible, might be to get a better boots-on-the-ground picture of reality.</p><p> If you&#39;ve done all of this, and you still think you should donate a kidney… go ahead and do it. But don&#39;t let one of your first altruistic act be one of so little impact, with such a high risk of damaging your long-term prospects.</p><hr><p> Ethics is fuzzy, there&#39;s no way around that. Causing yourself harm doesn&#39;t make ethics any less fuzzier, whatever mechanism is telling you otherwise, is wrong. I&#39;m not saying you should stamp out your entire worldview, by all means, keep thinking about ethics and keep having this weird view where the suffering of every living being matters… we need more of that.</p><p> But don&#39;t turn this weird way of thinking into damaging irreversible actions, take things one step at a time and sacrifice lesser things first to do good directly.</p><p> If embarking on that plane journey to Africa really does feel more daunting than donating a kidney, it shouldn&#39;t, so you should first and foremost tackle that inconsistency.</p><br/><br/> <a href="https://www.lesswrong.com/posts/MgcrLEfGwJnuhCBbb/don-t-donate-a-kidney-to-a-stranger#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/MgcrLEfGwJnuhCBbb/don-t-donate-a-kidney-to-a-stranger<guid ispermalink="false"> MgcrLEfGwJnuhCBbb</guid><dc:creator><![CDATA[George3d6]]></dc:creator><pubDate> Sun, 12 Nov 2023 20:06:42 GMT</pubDate> </item><item><title><![CDATA[The Fundamental Theorem for measurable factor spaces]]></title><description><![CDATA[Published on November 12, 2023 7:25 PM GMT<br/><br/><p> I present the fundamental theorem for all finitely factored measurable spaces. The fundamental theorem is that two events are orthogonal if and only if they are independent in all product probability distributions. It tells us that the definition of orthogonality really captures the essence of structural independence by the following arguments:</p><ul><li> Whenever things are structurally independent, they should be probabilistically independent, regardless of the specific chosen distribution.</li><li> Orthogonality should be the strongest notion that entails the previous point.</li></ul><p> This theorem was previously proved in <a href="https://arxiv.org/abs/2109.11513">Finite Factored Sets</a> for the finite case. The general case is interesting, since we can&#39;t use the finite structure. All the possible arguments are limited to the axioms of a measurable space. In particular, infinite things are sort of limits of finite things, so we can expect, through this result, that there should be nice approximation theorems for orthogonality. Something like, if I get more and more data about the world, then I can refine my view of which things are structurally independent.</p><p> To understand the technical result, it is necessary to understand the definition of the <a href="https://www.lesswrong.com/posts/EKPSgN8LsiEJzX5ni/a-well-defined-history-in-measurable-factor-spaces">history</a> in this setting. All the maths is in this <a href="https://drive.google.com/file/d/1h_HIYeWKfQDYFcZ2Pa0lUP_MSW2TI7dZ/view?usp=sharing">document</a> . I will try to describe a bit of the intuition used to derive the theorem.</p><p> The core idea is to express mathematically that the history tells us, when the conditional probability of an event depends on which factor.</p><p> I show that the history can be expressed mathematically exactly as this and show that this representation can be used to deduce that structural independence (independence for all factored distributions) implies orthogonality.</p><p> My definition of history still uses a probability distribution to define the disintegration of the index function, ie we need that <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\pi_J ⫫ \pi_{J^c}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.078em;">J</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">⫫</span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.078em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.078em;">J</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.347em; padding-left: 0.182em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>for all factorized probability distributions. It turns out that it suffices to show the condition for one such distribution.</p><p> Furthermore, in Lemma 9, we can write a more explicit form that conditional probabilities need to take, to satisfy this criterion. I am positive that this can be leveraged to deduce a criterion that does not reference probabilities at all.</p><p> It is noteworthy, that we don&#39;t even need to assume polish spaces, the arguments work for any measure space modulo nullsets.</p><p> The easiest way to extend this to infinitely factored spaces is to simply only allow features with finite history. This is sort of like an infinite directed graph, that has a start node, from which all nodes must be reachable. But it does not allow for continuous time. The main obstacle for features with infinite history is that we can&#39;t take the almost sure intersection of an arbitrary familiy of sets, because different product probability distributions are mostly not equivalent in the infinite case. Therefore, we can&#39;t really restrict ourselves to one set of nullsets.</p><p> I&#39;m pretty sure that if we take the<a href="https://www.lesswrong.com/posts/nksgLdLc7y5KtbmFw/a-reformulation-of-finite-factored-sets">causal graph construction</a> and extend it to causal graphs with measurable features, we get a result that d-separation is equivalent to being conditionally independent in all faithful probability distributions, and that the probability distributions that are unfaithful are &#39;small&#39;, which is, as far as I know, not known for the general case.</p><br/><br/> <a href="https://www.lesswrong.com/posts/DrchMHizfPtW3nJh2/the-fundamental-theorem-for-measurable-factor-spaces#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/DrchMHizfPtW3nJh2/the-fundamental-theorem-for-measurable-factor-spaces<guid ispermalink="false"> DrchMHizfPtW3nJh2</guid><dc:creator><![CDATA[Matthias G. Mayer]]></dc:creator><pubDate> Sun, 12 Nov 2023 19:25:27 GMT</pubDate> </item><item><title><![CDATA[What ML gears do you like?]]></title><description><![CDATA[Published on November 11, 2023 7:10 PM GMT<br/><br/><p> In John&#39;s <a href="https://www.lesswrong.com/posts/nt8PmADqKMaZLZGTC/inside-views-impostor-syndrome-and-the-great-larp">recent post</a> he mentions many people in ML not having good gears level models of what&#39;s going on.</p><p> To wit; what gears-level models do you know for ML? How much support is there for them? Are there &quot;settled science&quot; kind models that have tons of empirical support?</p><p> What gears-level models informed the people who made major AI advancements? Is there a list, or writing about this somewhere?</p><br/><br/> <a href="https://www.lesswrong.com/posts/QoDzFtJRsRo7pqfty/what-ml-gears-do-you-like#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/QoDzFtJRsRo7pqfty/what-ml-gears-do-you-like<guid ispermalink="false"> QoDzFtJRsRo7pqfty</guid><dc:creator><![CDATA[Ulisse Mini]]></dc:creator><pubDate> Sat, 11 Nov 2023 19:10:12 GMT</pubDate> </item><item><title><![CDATA[Smart Sessions - Finally a (kinda) window-centric session manager]]></title><description><![CDATA[Published on November 11, 2023 6:54 PM GMT<br/><br/><p> This is a short post about some software functionality that I&#39;ve long wanted, and a browser extension that does it well enough. I&#39;m sharing in case anyone else has also been wanting the same thing.</p><h3> The dream</h3><p> There&#39;s a simple piece of software that I&#39;ve <a href="https://www.facebook.com/groups/bountiedrationality/posts/3195051520738889/">wanted</a> <a href="https://www.facebook.com/eli.tyre/posts/10219207815104814">for</a> <a href="https://www.facebook.com/eli.tyre/posts/10220008346077588">several</a> <a href="https://threadreaderapp.com/thread/1720281915243335985.html">years</a> . Few months, I go on a binge of trying to find something that do what I&#39;m looking for.</p><p> Basically: a session manager that allows you to group <i>windows</i> together somehow, so that you can close and save them all with one click, and then reopen them all with one click.</p><p> I make heavy use of the OSX feature “desktops”, which allows multiple separate workspaces in parallel. I&#39;ll typically have a desktop for my logging and tracking, one for chats and coms, one with open blog posts, one with writing projects, one with an open coding project, etc. Each of these are separate contexts that I can switch between for doing different kinds of work.</p><p> What I want is to be able to easily <i>save</i> each of those contexts, and easily re-open them later.</p><p> But since I&#39;ll often have multiple sessions open at the same time, across multiple desktops, I don&#39;t want the session-saver app to save <i>all</i> my windows. Just the ones that are part of a given workspace context.</p><p> The best way to do this is if the software could tell which windows were open on which desktops and use that as the discriminator. But some sort of manual drag and drop for adding a (representation of) a window (on a list of windows) to a group would work too.</p><h3> The situation</h3><p> This seems to me like something that…there should be a lot of demand for? I think lots of people have many windows, related to different projects that they want to keep separate, open on their computer at the same time.</p><p> But, as near as I can tell there&#39;s almost nothing like this.</p><p> There are a lot of session managers, browser extensions that allow you to save your tabs for the future (I&#39;ve mostly used <a href="https://www.one-tab.com/">OneTab</a> , but there are dozens). However, they&#39;re virtually all <i>tab-centric</i> . A “session” typically refers to a single window, with multiple tabs, not to multiple windows, with multiple tabs each, which means that to reopen a session (in my “multiple windows” sense of the word), I need to mentally keep track of which windows were included and open all of them one by one, instead of clicking one button to get the context back.</p><p> There are a few session managers that save multiple windows in a session (I&#39;m thinking of <a href="https://sessionbuddy.com/">Session Buddy</a> or the less polished <a href="https://chrome.google.com/webstore/detail/tab-session-manager/iaiomicjabeggjcfkbimgmglanimpnae">Tab Session Manager</a> ), but these have the opposite problem: they save <i>all</i> the open windows, including those that are part of other workflows in other desktops, which means that I have to go through and manually remove them every time I save a session. (This is especially a problem for me because there&#39;s a set of windows that I <i>always</i> keep open on my first desktop.) And on top of that, they tend to save sessions as static snapshots, rather than as mutable objects that change as you work with them, so you need to repeatedly delete old sessions and replace them with updated ones.</p><h3>成功！</h3><p> I spent a few hours over the past week, yet again, reading about and trying a bunch of tab managers in rapid succession to find any that have anything like the functionality I&#39;m wanting.</p><p> I finally found exactly one that does what I want!</p><p> It is a little finicky, with a bunch of small to medium sized UX problems. But it is good enough that I&#39;m going ahead and making a point to try using it.</p><p> I&#39;m sharing this here because maybe other people have also been wanting this functionality, and they can benefit from fruits of my laborious searching.</p><h3> Current solution: Smart Sessions – Tab Manager</h3><p> <a href="https://chrome.google.com/webstore/detail/smart-sessions-tab-manage/hhfefdiablijkjnbkmdfbopiimjilchh">Smart Sessions</a> is a chrome extension that does let you save groups of windows. This is the best one that I&#39;ve found so far.</p><p> When you click on the icon, there&#39;s a button for creating a new session. When you click it, it displays a list of all your current open tabs (another button organizes all those tabs by window), with checkboxes. The user checks the windows that they want to be included in a session. You give it a name and then create the session.</p><p> While a session is active (and while a default setting called “auto save” is set to Yes), when you close a tab or a window, it removes that tab or window from the session (though it does create a weird popup every time). You can also remove tabs/windows from the list manually. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pcpNhLszE7NM4wE2v/lthtckrnbkr3ta6cdngj"><br> <i>(The weird popup. It&#39;s not super clear from the text what the options mean, but I think “stop tracking” deactivates the session, and “save” removes the window you just closed from the active session.)</i></p><p> You can press the stop button, which closes all the windows, to be reopened later.</p><p> When the session is inactive, you can edit the list of tabs and windows that compose a session, removing some (though I think not adding?). You can also right click on any page, select Smart Sessions, and add that page to any session, active or not.</p><p> At the bottom of the session list, there&#39;s a button that deletes the session.</p><p> This basically has functionality that I want!</p><p> I want to first and foremost give a big hurrah to the developer Serge (Russo?), for being the only person in the world to make what seems to me and obvious and extremely helpful tab-management tool. Thank you Serge!</p><h3> Some issues or weird behavior</h3><p> However, it still has a fundamentally tab-centric design, with multi-window sessions seeming like concessions or afterthoughts, rather than core to the user experience. This results in some weird functionality.</p><ul><li> Every time you create a new session, you need to click a button so that the selection list is separated by window, instead of only a list of tabs. If you don&#39;t click this button, the selection list is a flat list of tabs, and when you create the session, all the selected tabs will be combined into a single window.<ul><li> (One UX that I could imagine is having a global setting on the settings page, “tab-centric default” vs “window-centric default”. You can still press the button to toggle individual sessions, but for window-centric session users, having a default would save me a button press each time.</li></ul></li><li> I think as a side effect of the above feature, whenever you create a new session, it takes all the windows of that session (regardless of where they are on the screen or across different desktops) and stacks them all on top of each other, so that only the top one is visible (not even some overlap so you can see how many windows are stacked on top of each other).</li><li> It would be intuitive if, while a session is active, if you opened a new window, that window was automatically added to the session. Not only does that not work, <strong>there appears to be no way to add new windows to a session</strong> . New <i>tabs</i> get added to the session, but not new windows. The right-click “add to session functionality”, adds a single page, as a new tab in one of the windows of a session, not as a new window in that session.<ul><li> The only way, as near as I can find, to increase the number of windows in a session is to drag a tab from a multi-tab window into its own window—both resulting windows are saved as part of the session. In order to add new windows to a session, the user needs to do an an awkward maneuver to exploit this functionality: first create a new tab in a window that&#39;s part of the session, and then drag it to it&#39;s own window. Or alternatively, make a new window, add that window as a tab, to one of the windows that is part of the active session, and then drag it out again. That tab, again in its own window, will be added to the session.</li></ul></li><li> As noted above, every time you close a window, that&#39;s part of the active session, this activates a popup.</li></ul><p> It would be great if these issues were addressed.</p><p> Additionally, for  some reason the extension is slow to load. Sometimes (but not always), I&#39;ll click on the icon and it will take a full two seconds for the list of sessions to appear. I haven&#39;t yet figured out what the pattern is for why there&#39;s sometimes a delay and sometimes not.</p><p> And finally, there are some worrying <a href="https://chrome.google.com/webstore/detail/smart-sessions-tab-manage/hhfefdiablijkjnbkmdfbopiimjilchh">reviews</a> that suggest that at least sometimes, the whole history disappears? I&#39;m not sure what&#39;s up with that, but I&#39;m going to make a point to regularly export all my sessions (there&#39;s easy export functionality), just to be careful.</p><p> Overall though, this so far <i>works</i> , and I feel pretty excited about it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/pcpNhLszE7NM4wE2v/smart-sessions-finally-a-kinda-window-centric-session#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/pcpNhLszE7NM4wE2v/smart-sessions-finally-a-kinda-window-centric-session<guid ispermalink="false"> pcpNhLszE7NM4wE2v</guid><dc:creator><![CDATA[Eli Tyre]]></dc:creator><pubDate> Sat, 11 Nov 2023 18:54:16 GMT</pubDate></item></channel></rss>