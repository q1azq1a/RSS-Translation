<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 22 日星期五 10:12:37 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AGI Lab Pauses Are Costly]]></title><description><![CDATA[Published on December 22, 2023 5:04 AM GMT<br/><br/><p>免责声明：我对 AGI 实验室的内部治理几乎一无所知。我目前的理解是，主要的 AGI 实验室基本上没有关于如何导航条件暂停的计划，而这篇文章就是基于这个假设。</p><p> TL;DR：如果 AGI 实验室暂停，其员工不得以其他方式背叛和提升能力，例如打破公司内部的暂停承诺，或离开公司并加速其他公司的能力，这一点至关重要。如果进入暂停会让许多员工生气，那么这就会强烈激励实验室永不暂停。为了解决这个问题，我建议公司采取措施，让以能力为中心的员工在有条件的暂停期间保持快乐，比如给他们充分的预警、其他需要工作的项目，或者多年的带薪休假。</p><h1>目前，进入有条件暂停的做法违反了员工激励措施</h1><p>如果一个主要的 AGI 实验室明天进入自愿有条件暂停，这对于致力于培训前沿模型的员工来说可能是一个相当有压力的事件。这也可能会导致公司优先事项发生重大转变，以至于许多高层人员必须从当前的角色转换到其他角色。可能会有一些人根本无事可做，甚至可能被公司解雇。我预计，进入有条件暂停会导致公司很大一部分员工在几周/几个月内承受压力。</p><p>如果这样的重组在一夜之间发生，如果出现与萨姆·奥尔特曼被解雇时类似的员工强烈反对，我不会感到惊讶。 OpenAI 的员工，以及部分其他 AGI 实验室的员工，都知道他们可以为自己想要的东西而奋斗，如果不同意领导层的决定，他们就会威胁要集体加入竞争对手的实验室。</p><h1> AGI 实验室有强烈的动机来避免有条件的暂停</h1><p>AGI 实验室希望避免员工离职。如果有条件暂停，受影响最严重的员工将是那些致力于前沿模型能力的员工。避免这些员工流失的原因有几个：</p><ul><li>推动能力的员工通常是那些地位最高的员工（与安全或产品相比）和相对较长的业绩记录（因为产品员工大多是新员工）。通过地位和长期的联系，他们将拥有巨大的影响力来动员其他劳动力来促进他们的利益。</li><li>这些人还拥有重要的内幕信息和特殊技能，如果他们转移到其他 AGI 实验室（没有暂停），可能会显着加快他们的 AGI 工作，并导致他们原来的实验室处于不利地位。</li></ul><p>如果 AGI 实验室预计其大部分顶尖能力人才在暂停后会离开实验室，这将：</p><ol><li>强烈激励 AGI 实验室不要暂停，并且</li><li>让有系统地暂停的 AGI 实验室的员工流失到不暂停的实验室。</li></ol><p>加入其他实验室的注重能力的员工存在显着的负外部性：</p><ul><li>理想情况下，我们希望限制 AGI 实验室的数量，甚至进入人工智能开发的危险区域，从而触发有条件暂停期，因为任何公司都存在无法触发有条件暂停的风险，并进入人工智能开发日益危险的区域。</li><li>这意味着，如果 AGI 实验室进入暂停状态，他们的员工最好留在原来的实验室，因为去其他实验室会增加他们进入更危险区域的能力。</li></ul><h1>有多种方法可以提高员工对有条件暂停的激励</h1><p>当AGI实验室开始采取严厉措施降低灾难性风险时，重要的是员工愿意做出必要的牺牲，并且决策者在做出重大决策时能够适当提高士气和支持</p><p>提高员工激励的方法有很多：</p><ol><li>向员工发出通知 - 可能会维护内部预测市场，以预测某个时间段内有条件暂停的可能性，并在明年内可能出现有条件暂停的情况下发布内部通知。</li><li>制定持续更新的重组计划，以便在发生暂停时减少失业的人数。</li><li>在暂停期间为不满意的员工提供带薪休假。</li><li>创建一种以安全为中心的文化——强调在公司需要进行大规模重组时做出小小的个人牺牲是多么的道德。</li><li>更大的愿景——在公司之间建立暂停承诺，以避免逐底竞争。</li></ol><br/><br/><a href="https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly<guid ispermalink="false"> GXrevJFeGiPkkM2hy</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Fri, 22 Dec 2023 05:04:15 GMT</pubDate> </item><item><title><![CDATA[The LessWrong 2022 Review: Review Phase]]></title><description><![CDATA[Published on December 22, 2023 3:23 AM GMT<br/><br/><p><a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">今年的 LessWrong 评审</a>提名阶段已于几天前结束，共有 339 个职位获得提名。相比之下，2021 年审查中提名了 291 个职位。</p><h2>提名阶段结果</h2><p>以下是当前投票总数排名前 20 的帖子：</p><ol><li> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">AGI 废墟：死亡名单</a></li><li><a href="https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy">MIRI宣布新的“有尊严的死亡”战略</a></li><li><a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">模拟器</a></li><li><a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">我同意和不同意以利以谢的地方</a></li><li> <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target">奖励不是优化目标</a></li><li><a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects">AGI 项目中运营充足性的六个维度</a></li><li><a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring">你没有测量你认为你正在测量的东西</a></li><li><a href="https://www.lesswrong.com/posts/jbE85wCkRr9z7tqmD/epistemic-legibility">认知易读性</a></li><li><a href="https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai">让我们考虑放慢人工智能的速度</a></li><li><a href="https://www.lesswrong.com/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world">看起来你正试图接管世界</a></li><li><a href="https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill">凝视深渊作为核心生活技能</a></li><li><a href="https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case">对基本人工智能 x 风险案例的反驳</a></li><li><a href="https://www.lesswrong.com/posts/k9dsbn8LZ6tTesDS3/sazen">佐善</a></li><li><a href="https://www.lesswrong.com/posts/ma7FSEtumkve8czGF/losing-the-root-for-the-tree">树失去了根</a></li><li><a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">人类价值碎片理论</a></li><li><a href="https://www.lesswrong.com/posts/Jk9yMXpBLMWNTFLzh/limerence-messes-up-your-rationality-real-bad-yo">Limerence 搞乱了你的理性，真的很糟糕，哟</a></li><li><a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward">模特不会“获得奖励”</a></li><li> <a href="https://www.lesswrong.com/posts/J3wemDGtsy5gzD3xa/toni-kurz-and-the-insanity-of-climbing-mountains">托尼·库尔兹 (Toni Kurz) 和疯狂的登山运动</a></li><li><a href="https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas">蝴蝶的想法</a></li><li><a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment">关于各种计划如何错过协调挑战的困难部分</a></li></ol><p>（我感觉<i>有点</i>主题……）</p><p>已有 60 多个帖子经过审核，但还有相当多的帖子尚未收到任何审核，其中包括许多得票最高的帖子。如果您想查看哪些帖子审核不足，您可以将排序切换为<a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">Magic (Needs Review)</a> <span class="footnote-reference" role="doc-noteref" id="fnrefee9z6uedals"><sup><a href="#fnee9z6uedals">[1]</a></sup></span> 。也许你对<a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">保罗</a>对以利以谢的想法有什么看法？</p><h2>内联反应！</h2><p>我们有这些新的漂亮的内联反应，您可以在帖子上留下它们（不仅仅是评论！）；你可能已经注意到它们了。我鼓励您在审阅帖子时充分利用这些。 （如果您愿意的话，现在报告错别字应该不再那么烦人了。） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rtobirv7jrt8nq82hz2z" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rssimlmyz5krdzbxpczp 230w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/qlf54ngngv1baavodtzo 460w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/k8j8kw8ymu4rsyofvfwv 690w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/m9xmvaf38jf2c9vhbnbi 920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/wcox7gtxrzqwwpwajcji 1150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/ulddjbtcm35ku0cf6qnl 1380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/n772tkotnkujjsj6quju 1610w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/dqtkuwf7kqqlmodvdijo 1840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/a5loq2zqoszqanitypd2 2070w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/xmtgpnybrsoxctggksel 2292w"><figcaption>但也许你有更复杂的意图。</figcaption></figure><h2>奖品？有奖品！</h2><p>去年，我们为好评颁发了<a href="https://www.lesswrong.com/posts/ep5sejjsA6GZqLgv9/highlights-and-prizes-from-the-2021-review-phase?commentId=jhJ8Yf3oLX93vH4o2">奖项</a>。今年我们还会颁发奖品！我们的目标与去年类似，但尚未确定细节（规模、范围等）。</p><p>简短地指出您喜欢或不喜欢某个帖子的哪些内容是很好的做法。但从去年开始，我们特别兴奋的是一些具体的事情</p><p><strong>新的信息。</strong>我最喜欢的评论是那些为读者<i>提供新信息的评论</i>。这可能是一个具体事实，或者是对帖子中关键论点的反驳，或者是您没有考虑到的与帖子相关的新方式。这可能是一个关于如何思考这篇文章的新框架。</p><p><strong>具体用例。</strong>我认为那些说“这对我有具体帮助，这就是如何帮助我”的评论也很棒，特别是如果它们给出了具体细节。我认为对于个人作者来说，了解他们的帖子如何有用既是有帮助的，也是有益的，这样他们就可以做得更多。 （我认为这在更广泛的层面上也有帮助，以便集体 LW 用户群可以看到什么样的效果是真实的）</p><p>即，不要说“多年来这篇文章对我有很大帮助”，而要说“这里有两个特定的时间它对我有帮助，以及如何帮助我。”</p><p><strong>关于如何改进该帖子的想法。</strong>两年后，如果一篇文章看起来仍然是很好的参考材料，但令人困惑或争论不充分，请就如何改进它提出建议。讨论您想要的读者群的用例。</p><p><strong>反思大局。</strong>不同的帖子如何组合在一起，形成大于各部分之和的效果？ LessWrong 上发生了哪些主要对话？您从这些对话中得到了什么？</p><p><strong>简洁/清晰。</strong>如果它们足够简洁且相关，可以很容易地将它们打印在 2021 年的书中，那就加分了。 （我想在 2020 年的书中打印一些评论，但发现需要大量的编辑工作才能使它们形成有意义的形状）。请注意，我并不是说“以牺牲任何实质性内容为代价来优化简短性”，只是请注意，在其他条件相同的情况下，占用更少的空间来传达关键信息是有帮助的。</p><h2>最终投票</h2><p>审查阶段于 1 月 14 日结束，届时最终投票开始。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnee9z6uedals"> <span class="footnote-back-link"><sup><strong><a href="#fnrefee9z6uedals">^</a></strong></sup></span><div class="footnote-content"><p>或者点击该链接！</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase<guid ispermalink="false"> Lqf7H9zREnRbmWL4M</guid><dc:creator><![CDATA[RobertM]]></dc:creator><pubDate> Fri, 22 Dec 2023 03:23:49 GMT</pubDate> </item><item><title><![CDATA[The absence of self-rejection is self-acceptance]]></title><description><![CDATA[Published on December 21, 2023 9:54 PM GMT<br/><br/><p>我曾经以为自我接纳是一种<i>行动</i>。我可以执行一个“接受自己”的心理动作。我曾多次尝试“接受自己”，但似乎从来没有起到任何作用。</p><p>如今，我认为“自我接纳”是一个用词不当，而且大多不存在。</p><p>相反，我认为这是关于你自己的每一部分都<strong>没有自我拒绝</strong>。</p><p>例如，我经常不知道自己身体的感觉如何，但我宁愿知道。对于这个问题，常见的建议是通过“接受”来更加了解自己的感受。但按照我上面的逻辑，接受感情的方法实际上就是<i>停止拒绝</i>它们。</p><p>当然，如果你拒绝某种感觉或自己的一部分，你必须有一个或多个<strong>动机</strong>这样做。</p><p>当我研究这些激励因素时（主要是通过连贯疗法），我发现我直觉地认为我的感觉是<i>危险的</i>：</p><ul><li>我担心意识到自己的感受会降低我的工作效率，而不是让我专注于感觉有意义的事情。</li><li>我担心表达自己的情绪会让其他人生我的气，而不是帮助我找到那些让我感到舒服的人。</li><li>我内心深处相信，负面情绪<i>本质上</i>是不好的，而不仅仅是表明世界或我对世界的解释可以得到改善。</li><li> ……</li></ul><p>难怪我会拒绝自己的感情！</p><p>从那时起，通过理清我不得不拒绝的动机，我在“接受”我的感受方面取得了很大的进步。 </p><figure class="image image_resized" style="width:27.97%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/lmxgctd5zxnukgn4xwjr 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/ahec7krzcawazb1xvbzq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/boy91jld5wbif4vhlokd 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi 1456w"></figure><p><i>感谢 Stag Lynn 帮助编辑这篇文章</i>。</p><br/><br/> <a href="https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance<guid ispermalink="false"> hytejC3bPsKjzn7Kk</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Thu, 21 Dec 2023 21:54:54 GMT</pubDate> </item><item><title><![CDATA[A Decision Theory Can Be Rational or Computable, but Not Both]]></title><description><![CDATA[Published on December 21, 2023 9:02 PM GMT<br/><br/><p>在经典博弈论中，<a href="https://en.wikipedia.org/wiki/Rational_agent">理性主体</a>总是根据自己的偏好选择最优的选项。即使这样的选择意味着能够评估可证明<a href="https://en.wikipedia.org/wiki/Halting_problem">不可计算的</a>函数。换句话说，理性意味着<a href="https://en.wikipedia.org/wiki/Hypercomputation">超计算</a>。</p><p>我们可以通过要求智能体在“是”和“否”之间进行选择，将任何理性智能体<a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">简化</a>为任何<a href="https://en.wikipedia.org/wiki/Decision_problem">决策问题的</a><a href="https://en.wikipedia.org/wiki/Oracle_machine">预言机</a>，并为选择正确答案的智能体提供更高的回报。 <span class="footnote-reference" role="doc-noteref" id="fnrefkdiicx9a768"><sup><a href="#fnkdiicx9a768">[1]</a></sup></span>如果我们选择<a href="https://en.wikipedia.org/wiki/Halting_problem">停止问题</a>，一个有适当动机的理性主体将充当<a href="https://en.wikipedia.org/wiki/Oracle_machine#Oracles_and_halting_problems">停止预言机</a>。 <span class="footnote-reference" role="doc-noteref" id="fnrefjasox1cjc2"><sup><a href="#fnjasox1cjc2">[2]</a></sup></span>如果我们可以查看理性代理的<a href="https://www.lesswrong.com/posts/T8piFGywHFd4ax9yx/gears-level-understanding-deliberate-performance-the">齿轮</a>，我们就能够找到一些正在执行超级计算的子系统。</p><p>任何决策理论的任何可计算实现都必然无法在某些情况下进行理性选择。对于任何<a href="https://en.wikipedia.org/wiki/Undecidable_problem">不可判定的问题</a>，任何算法都不可能在所有情况下选择正确的答案。</p><p>这就引出了一个问题：如果一个理性的代理人必须<a href="https://en.wikipedia.org/wiki/Program_equilibrium">将他们的决定委托给计算机程序</a>，他们会选择什么样的程序？ </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkdiicx9a768"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkdiicx9a768">^</a></strong></sup></span><div class="footnote-content"><p>或者至少我们可以，如果理性主体不是来自时空之外的超计算恐怖的话。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjasox1cjc2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjasox1cjc2">^</a></strong></sup></span><div class="footnote-content"><p>使用外部提供的回报来调整无限智能的外星生物的行为，作为练习留给读者。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both<guid ispermalink="false"> LcrwLTqtc6kGEgJbf</guid><dc:creator><![CDATA[StrivingForLegibility]]></dc:creator><pubDate> Fri, 22 Dec 2023 00:52:33 GMT</pubDate></item><item><title><![CDATA[Most People Don't Realize We Have No Idea How Our AIs Work]]></title><description><![CDATA[Published on December 21, 2023 8:02 PM GMT<br/><br/><p>这一点感觉相当明显，但似乎值得明确说明。</p><p>我们这些在深度学习革命之后熟悉人工智能领域的人非常清楚，我们不知道我们的机器学习模型是如何工作的。当然，我们了解训练循环的动态和 SGD 的属性，并且我们知道 ML 模型的<i>架构</i>如何工作。但我们不知道 ML 模型的前向传递实现了哪些具体算法。我们有一些猜测，也有一些通过可解释性进步精心挖掘的见解，但没有什么比全面理解更重要的了。</p><p>最肯定的是，我们不会自动知道在刚刚由训练循环吐出的新颖架构上训练的新模型是如何工作的。</p><p>我们都已经习惯了这种状态。这是隐含地假定的共享背景知识。但当你第一次了解到它时，它实际上很不寻常。</p><p>和... </p><figure class="image image_resized" style="width:34.32%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/jewpuloo9nmwdio9tudl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/pg7rwq76hccofvfiyrev 135w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/a9kuwbx6cvpwhuqrvmy7 215w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/z4ux5dbkiieieb8f1nzn 295w"><figcaption>相关XKCD。</figcaption></figure><p>我很确定公众<i>实际上并不知道这一点</i>。我没有确凿的数据，但这是我的强烈印象，这是基于阅读非技术社区中与人工智能相关的在线讨论、与对人工智能进步不感兴趣的人交谈等等。 <span class="footnote-reference" role="doc-noteref" id="fnrefv1vmoacp21r"><sup><a href="#fnv1vmoacp21r">[1]</a></sup></span></p><p>他们仍然用 GOFAI 的方式思考。他们仍然相信人工智能的所有功能都是被故意<i>编程的</i>，而不是<i>经过训练</i>的。 ChatGPT 所能做的每一件事的背后，都有一个人实现了该功能并理解它。</p><p>或者，至少，它是以清晰的、人类可读和人类可理解的格式编写的，并且我们可以对其进行干预，以引起精确的、可预测的变化。</p><p>民意调查已经显示出对通用人工智能的担忧。我们不知道这些系统实际上在想什么的事实是否被<i>广泛知晓</i>并<i>得到适当的重视</i>？如果<i>没有</i>“有人理解它是如何工作的以及为什么它不会出现灾难性错误”的隐含保证呢？</p><p>嗯，我期待更多的关注。这可能为进一步支持人工智能法规的消息传递奠定了良好的基础。一种获取可以花费的<a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#Public_Opinion__And_Perception_Thereof_">政治货币</a>的方法。</p><p>因此，如果您正在进行任何形式的公众呼吁，我建议您将传播此类信息提上议程。<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">你会向公众传达大约五个词</a>（每条消息），“强大的人工智能是黑匣子”似乎是一条值得发出的消息。 <span class="footnote-reference" role="doc-noteref" id="fnrefhae8cysa7z7"><sup><a href="#fnhae8cysa7z7">[2]</a></sup></span> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnv1vmoacp21r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv1vmoacp21r">^</a></strong></sup></span><div class="footnote-content"><p>如果您<i>确实</i>有一些硬数据，那将受到欢迎。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhae8cysa7z7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhae8cysa7z7">^</a></strong></sup></span><div class="footnote-content"><p>对于“黑匣子”术语存在<a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">一些阻力</a>。我认为它是正确的：机器学习模型相对于我们来说是黑匣子，从某种意义上说，默认情况下，我们对它们执行的算法的了解并不比通过查看同态加密计算来了解更多。我们没有钥匙，或者通过使用神经影像学来观察人脑的活动。可解释性研究的数量不为零，但基本上情况仍然如此。对于由新颖架构生成的模型来说几乎<i>完全</i>是这样。</p><p>是的，<i>相对于 SGD</i> ，ML 模型并不是黑匣子。该算法可以“看到”所有正在发生的计算，并对其进行严格干预。但这似乎是对该术语的相当反直觉的使用，我认为“人工智能是黑匣子”传达了所有正确的直觉。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work<guid ispermalink="false"> CPJTJtW2RNKvzAehG</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Thu, 21 Dec 2023 20:02:00 GMT</pubDate> </item><item><title><![CDATA[Pseudonymity and Accusations]]></title><description><![CDATA[Published on December 21, 2023 7:20 PM GMT<br/><br/><p><span>以下是我不确定如何思考的一类情况：艾弗里以笔名（“亚历克斯”）写作，指责帕特的某些行为，比如说虐待。艾弗里的一个主要动机是让人们知道在与帕特互动之前考虑这些信息。帕特声称实际上是“亚历克斯”施虐，并给出了他们的说法。虽然外人很难判断，但很多人最终认为他们希望在与“亚历克斯”互动时采取一些预防措施。在什么情况下帕特或其他熟悉情况的人可以将“亚历克斯”识别为艾弗里？</span></p><p>揭露假名背后的人<a href="https://en.wikipedia.org/wiki/Doxing">通常被认为是</a>一种人肉搜索，而在我所属的社区中，这通常被认为<a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists">是不可接受的</a>。例如， <a href="https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum">EA 论坛禁止这样做</a>：</p><blockquote>我们也不允许在论坛上进行人肉搜索（或者如果某人喜欢匿名则透露其真实姓名）。</blockquote>虽然我在 LessWrong 文档中找不到任何相关内容，但他们最近对泄露假名的人<a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">发出了临时禁令</a>：<blockquote>我们（LW 审核团队）已对 [评论者] 实施为期一周的网站禁令，并对试图进行人肉搜索的帖子/主题进行无限期禁令。我们已删除所有实名评论，并要求大家尊重涉事人员的隐私。</blockquote><p>总的来说，我赞成人们能够以假名在线参与。我认为有更好和更坏的方法来做到这一点，但是有很多正当的理由说明为什么你可能需要将你的现实生活身份与你的部分或全部写作分开。人肉搜索打破了这一点（尽管在某些情况下它<a href="https://www.jefftk.com/p/linking-alt-accounts">已经非常脆弱</a>），因此应该有一个非常强烈的反对它的推定。</p><p>另一方面，并​​不能保证第一个提出问题的人是正确的。如果帕特正确地认为这完全是艾弗里的虐待行为，并且公开指责帕特的虐待行为是这种虐待的另一种<a href="https://en.wikipedia.org/wiki/DARVO">形式</a>，该怎么办？如果我们说将“Alex”链接回 Avery 是不行的，那么首先发帖对 Avery 的社会影响是非常大的。如果我们制定社区规范，非常重视成为第一个公开的人，那么我们将看到更多的人将此作为一种有意的策略。 [1]</p><p>公开指控虐待对于保护他人来说确实很有价值，公开讲述你的故事<a href="https://www.jefftk.com/p/speaking-up-publicly-is-heroic">通常是英勇的</a>。有时人们只愿意匿名这样做，这保留了很多价值：我想我不知道有谁认为<a href="https://medium.com/@mittenscautious">2018 年针对布伦特的指控</a>导致他被逐出湾区理性社区，均为阴性。即使社区中的许多人都知道原告是谁，如果原告知道他们的真实姓名将被公开而不是很快被删除，我怀疑他们不太可能站出来分享他们的故事。</p><p>但帕特通常可以公开发帖说“艾弗里一直在和我的朋友谈论对我的虚假指控，这就是为什么你不应该相信他们……”或者第三方发帖“艾弗里有我一直在说帕特的不实言论，我认为这真的不公平，这就是为什么......”。在这种情况下，我真的不明白艾弗里更进一步并以书面形式提出这些指控应该如何约束帕特或其他人。</p><p>我认为这些人感到紧张的原因是我的潜在感觉是真正的受害者应该能够公开指控犯罪者，而犯罪者不应该能够通过点名受害者来进行报复。但当然，我们通常不知道某人是否是真正的受害者，因此这不是社区规范或节制政策可以真正用作输入的内容。</p><p>在<a href="https://forum.effectivealtruism.org/posts/bwtpBFQXKaGxuic6Q/effective-aspersions-how-the-nonlinear-investigation-went?commentId=x9zcFh9g2r7HdsLQH">EA 论坛</a>和<a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">LessWrong</a>上有很多关于这个最近的具体变体的细致讨论。我不知道答案是什么，而且我怀疑无论你走哪条路都有很大的缺点。但我认为也许我们能做的最好的事情就是，一个与争议不相关的值得信赖的社区成员或团体[2]评估情况，并根据现有证据的平衡做出判断，反对人肉搜索的规范是否仍然适用。但如果证据难以解释怎么办？如果有人说更多证据即将到来但尚未准备好怎么办？举证责任有多高？到处都是混乱的，对可能被错误地人肉搜索的指控者、可能被错误指控的人造成真正的后果，并且在我们从未听到过的重要警告中，因为人们担心被人肉搜索。</p><p><br> [1] 我认为这也意味着对于许多私人分歧，会有更强烈的动机公开。如果我与某人处于混乱的境地，并且我的社区使用了这些规则，也许我应该迅速公开讲述我的故事，否则对方会首先以化名讲述他们的故事，并引发一场不对称的声誉之战。</p><p> [2] 在引发这篇文章的具体案例中，尤其令人困惑的是，这场争议发生的两个主要场所之一是由与假名原告关系密切的人管理的。因此，虽然论坛管理员通常是自然的法官，但我认为在这种情况下他们太接近实际情况了。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02JiHaBxJY3sBLFSyz49gUwYAvrLpC9WWLwJR3BDCi5PGnMq3k9CVwKHY8uoYpu38wl">facebook</a> 、 <a href="https://lesswrong.com/posts/HTLd3R9qgvZsThuWW">lesswrong</a> 、 <a href="https://mastodon.mit.edu/@jefftk/111620008070332215">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations<guid ispermalink="false"> HTLd3R9qgvZsThuWW</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 21 Dec 2023 19:20:20 GMT</pubDate> </item><item><title><![CDATA[Attention on AI X-Risk Likely Hasn't Distracted from Current Harms from AI]]></title><description><![CDATA[Published on December 21, 2023 5:24 PM GMT<br/><br/><h2>概括</h2><p>在过去的一年里，公共论坛对人工智能带来的存在风险（以下简称 x 风险）越来越关注。我们的想法是，我们可能会<a href="https://www.cold-takes.com/where-ai-forecasting-stands-today/">在未来几年或几十年内看到变革性的人工智能</a>，可能<a href="https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/">很难确保此类系统在行动时考虑到人类的最大利益</a>，而<a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">那些高度先进的人工智能如果旨在做到这一点，可能会压倒我们。如此</a>，否则此类系统可能会被灾难性地滥用。一些人的反应是，对 X 风险的担忧分散了人们对人工智能当前危害的注意力，比如算法偏差、工作取代和劳动力问题、环境影响等。与<em>这些</em>声音相反，其他人认为，对 x 风险的关注并<em>不会</em>分散对当前危害的资源和注意力——这两种担忧可以和平共处。</p><p> X风险会分散人们对当前危害的注意力的说法是偶然的。这可能是真的，也可能不是。要确定它是否属实，有必要查看证据。但是，令人失望的是，尽管人们自信地断言这一说法的真实性和虚假性，但似乎没有人看过证据。在这篇文章中，我确实查看了证据。<strong>总体而言，我查看的数据提供了一些理由认为，迄今为止，对 x 风险的关注并未减少对当前危害的关注或资源。</strong>我特别考虑了五组证据，但它们本身都没有结论：</p><ol><li>自 x-risk 受到关注以来颁布的政策</li><li>搜索兴趣</li><li>人工智能道德倡导者的 Twitter/X 追随者</li><li>为致力于减轻当前危害的组织提供资金</li><li>与环保主义的相似之处</li></ol><p>我现在认为这不是一个重要的讨论。如果每个参与人员都讨论风险、当前危害或 X 风险的概率和规模，而不是讨论一个人是否会分散另一个人的注意力，那就更好了。这是因为就风险达成一致可以消除关于 x 风险是否会分散注意力的分歧，而当对风险存在如此强烈的分歧时，对干扰的分歧可能会很棘手。</p><h2>论据</h2><p><a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">梅雷迪思·惠特克</a>（Meredith Whitaker）（2023 年 6 月）：“一个奇幻、令人兴奋的鬼故事被用来劫持人们对监管需要解决的问题的注意力。” <a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">Deborah Raji</a> （2023 年 6 月）：“科幻叙事分散了我们对那些我们今天就可以开始研究的易处理领域的注意力。” <a href="https://archive.is/20230816022107/https://www.nature.com/articles/d41586-023-02094-7">《自然》杂志</a>（2023 年 6 月）：“人工智能毁灭人类的言论已经进入了科技公司的议程，并阻碍了对人工智能目前造成的社会危害的有效监管。” <a href="https://archive.is/20230628224859/https://www.newscientist.com/article/mg25834453-300-the-real-reason-claims-about-the-existential-risk-of-ai-are-scary/">Mhairi Aitken</a> （艾伦图灵研究所）在《新科学家》杂志上（2023 年 6 月）：“这些说法很可怕，但并不是因为它们是真的。它们之所以可怕，是因为它们正在显着重塑和重新引导有关人工智能影响及其含义的对话。意味着人工智能的开发要负责任。” <a href="https://archive.is/20230925130818/https://www.ft.com/content/732fc372-67ea-4684-9ab7-6b6f3cdfd736">艾丹·戈麦斯</a>（Aidan Gomez，2023 年 6 月）：“花我们所有的时间来争论我们的物种是否会因为超级智能 AGI 的接管而灭绝，这是对我们时间和公众思维空间的荒谬利用。[... ] [这些辩论]会分散人们对应该进行的对话的注意力。” <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">Noema</a> （2023 年 7 月）：“将人工智能引起的灭绝作为全球优先事项似乎可能会分散我们对人工智能之外其他更紧迫问题的注意力，例如气候变化、核战争、流行病或移民危机。” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-1" id="fnref-pmkuq8uwZkgHphmXq-1">[1]</a></sup> <a href="https://archive.is/20231031123323/https://www.technologyreview.com/2023/10/30/1082656/focus-on-existing-ai-harms/">Joy Buolamwini</a> （2023 年 10 月）：“通过说假设的存在危害更重要来最大限度地减少现有人工智能危害的一个问题是，它改变了宝贵资源和立法注意力的流动。” <a href="https://archive.is/93wxY">Lauren ME Goodlad</a> （2023 年 10 月）：“重点是 [...] 防止狭隘和牵强的风险转移人们对实际存在的危害和广泛监管目标的注意力。” Meta 全球事务总裁<a href="https://archive.is/3Ivja">尼克·克莱格（Nick Clegg</a> ）（2023 年 11 月）：“[重要的是要防止]近期挑战被大量推测性的、有时有些未来主义的预测所排挤。”</p><p>这些观点都是在去年夏天和秋天表达的，是对去年春天先进人工智能对存在风险（以下称为 x 风险）日益关注的回应。随后，人们对 x-risk <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-2" id="fnref-pmkuq8uwZkgHphmXq-2">[2]</a></sup>的兴趣迅速增加，这主要是由于 3 月 14 日 GPT-4 的发布以及随后发生的四件事：</p><ul><li> 3 月 22 日：生命未来研究所发表<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">公开信，主张暂停前沿人工智能培训</a>。</li><li> 3 月 29 日：Eliezer Yudkowsky<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">在《时代》杂志上发表评论文章</a>，认为人工智能将对人类构成风险，因此应该停止人工智能的开发。</li><li>五月初：<a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">杰弗里·辛顿 (Geoffrey Hinton)</a>退出谷歌，以警告人工智能的危险。</li><li> 5 月 30 日：人工智能安全中心 (CAIS) 发布了一份<a href="https://www.safe.ai/statement-on-ai-risk">关于灭绝风险的声明，</a>由研究人员、科学家和行业领袖签署，包括 Hinton、Yoshua Bengio、Google DeepMind 的 Demis Hassabis、OpenAI 的 Sam Altman、Anthropic 的 Dario Amodei、Ya-Qin清华大学的张教授等。</li></ul><p>当然，如此令人眼花缭乱的情绪可以得到证据或论据的支持，但这种情况很少发生。相反，它只是简单地断言，对 X 风险的关注会分散人们对当前危害和关注这些危害的人的兴趣和资源。有一个值得称赞的例外值得一提。 Blake Richards、Blaise Agüera y Arcas、Guillaume Lajoie 和 Dhanya Sridhar <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">在为 Noema 撰写的文章中</a>确实提供了一个因果模型，尽管它还很初级，但它是如何发生的：</p><blockquote><p>那些呼吁优先考虑人工智能引起的灭绝的人也在呼吁优先考虑其他更直接的人工智能风险，那么为什么不简单地同意所有这些风险都必须优先考虑呢？除了有限的资源之外，人类及其机构的注意力也是有限的。事实上，有限注意力可能是人类智力的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">标志</a>，也是帮助我们理解世界的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">归纳偏见</a>的核心组成部分。人们还倾向于从彼此身上获取关于要关注什么的暗示，从而导致在公共话语中很容易看到的集体关注焦点。</p></blockquote><p>确实，人类和机构的注意力是有限的。这并不一定意味着专门用于 x 风险的资源和专门用于应对人工智能当前危害的资源之间需要进行权衡。例如，对 x 风险的担忧可能会引起与人工智能完全无关的领域的关注。也可能出现这样的情况：对某项技术可能造成的危害的关注会引起人们<em>对</em>同一技术的其他危害的关注，而不是<em>引起对</em>同一技术的其他危害的关注。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-3" id="fnref-pmkuq8uwZkgHphmXq-3">[3]</a></sup>这些可能性大家都可以想到。为什么没有人费心去问自己哪种效应更有可能发生？我理解为什么 Liv Boeree 看起来如此恼怒，<a href="https://nitter.net/Liv_Boeree/status/1728131649177591953">她写道</a>：“当然，权衡是存在的，注意力也是有限的，但人工智能煽动者的这种日益增长的趋势，他们将模因空间纯粹视为零和战场，盲目地宣称<em>他们</em>最关心的问题是比所有其他人更重要的是，他们无情地对待任何不同意的人，坦率地说，这让人工智能感到尴尬。”我理解她，但尽管我理解她——并不是针对我钦佩的 Liv Boeree——她仍然犯着和其他人一样的错误，因为她的说法虽然可能更接近事实，也是没有证据的断言。顺便说一句，Séb Krier 也是如此，<a href="https://nitter.net/sebkrier/status/1719704802542686568">他写道</a>：“目前的讨论感觉有点像地热工程师对碳捕获科学家感到愤怒——对我来说似乎适得其反，因为显然有不同的空间社区和关注点蓬勃发展。[...]专业化很好，你没有积极的责任去考虑和谈论一切。”</p><p>简单一点来说，这里有两个营地。一是那些关注人工智能当前危害的人，例如算法偏见、工作取代和劳工问题、环境影响、监视和权力集中（特别是硅谷科技公司的权力，还有其他公司和政府的权力）。这里的核心例子包括人工智能伦理领域的 Joy Buolamwini 和算法正义联盟、Meredith Whittaker、 <a href="https://facctconference.org/">FAccT 会议</a>、Bender 等人。 (2021)，<a href="https://www.aisnakeoil.com/">人工智能蛇油</a>和<a href="https://www.torontodeclaration.org/declaration-text/english/">多伦多宣言</a>。第二，那些关注人工智能带来的x风险和灾难性风险的人，要么是由于误用或事故，要么是通过人工智能与其他风险相互作用，例如通过加速或民主化生物工程能力。这里的核心例子包括人工智能安全领域、Eliezer Yudkowsky、Stuart Russell、开放慈善事业、 <a href="https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI">Asilomar 有益人工智能会议</a>、Amodei 等。 (2016)， <a href="https://www.lesswrong.com/">LessWrong</a>和<a href="https://www.safe.ai/statement-on-ai-risk">CAIS 关于灭绝风险的声明</a>。 （当我说第一个阵营关注“当前危害”，第二个阵营关注“x风险”时，我遵循惯例，但是，出于我稍后将解释的原因，我认为这些标签是有缺陷的，并且更准确地说，谈论有根据的/确定的风险与投机/模糊的风险。）有些人和团体致力于或关心这两类问题，但显然存在这两类不同的问题。 （披露：我从事人工智能治理工作，比第一阵营更接近第二阵营，但这篇文章仅代表我个人的观点，不代表我雇主的观点。）</p><h2>证据</h2><p>本节试图查明对 x 风险的关注实际上是否已经吸引了人们对当前危害以及关注这些危害的人的兴趣或资源。当然，没有任何单独的证据可以证明该命题是否正确。因此，我研究了五种不同的证据，并综合考虑它们的说法。它们是 (1) 自 x-risk 受到关注以来制定的政策，(2) 搜索兴趣，(3) 人工智能道德倡导者的 Twitter/X 追随者，(4) 为致力于减轻当前危害的组织提供资金，以及 (5) 与其他组织的相似之处问题领域，特别是环保主义。 （我将使用“人工智能道德个人/组织”作为“关注当前危害的个人/组织”的同义词，尽管这些人/组织关注的一些危害，例如<a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">权力集中</a>，在某种程度上是推测性的，并且即使关注 x 风险的人通常也会出于道德原因这样做。）每一条单独的证据都是薄弱的，但综合起来我认为它们是令人信服的，虽然他们没有<em>证明</em>情况确实如此，但我认为他们提供了一些有理由认为，对 X 风险的关注并没有减少对当前危害的关注或资源投入，或者至少到目前为止还没有这样做。</p><h3>人工智能政策</h3><p>自 x-risk 受到关注以来，西方人工智能政策制定和发布的一项重要内容是拜登政府于 10 月底宣布的<a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">人工智能行政命令</a>。如果您认为 x 风险分散了对当前危害的注意力，您可能会认为行政命令会忽略当前危害，因为 x 风险现在更容易被接受。但那并没有发生。该行政命令有空间解决许多不同的问题，包括偏见、欺诈、欺骗、隐私和工作流失等问题。</p><p>算法正义联盟创始人乔伊·布奥拉姆维尼 (Joy Buolamwini)<a href="https://archive.is/OPG7E">表示</a>：“所制定的内容非常全面，这很高兴看到，因为考虑到人工智能的范围，我们需要一种全面的方法。” AI万金油作者<a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">表示</a>：“总的来说，对于那些支持人工智能开放的人来说，EO似乎是个好消息”，开放是确保人工智能公平、安全、透明和民主的首选方式。白宫声明本身表示“促进公平和公民权利”，并拒绝“利用人工智能让那些已经经常被剥夺平等机会和正义的人处于不利地位”。 《科学美国人》提到了该行政命令的一些局限性， <a href="https://archive.is/20231101134313/https://www.scientificamerican.com/article/bidens-executive-order-on-ai-is-a-good-start-experts-say-but-not-enough/">但随后写道</a>，“[我们]就该命令交谈或与之通信的每一位专家都将其描述为填补政策空白的有意义的一步”。我看到人们对行政命令的批评集中在当前的危害上，但这些批评的类型是，“它过于关注应用程序，而对数据和人工智能系统的开发关注不够”，或者“这将很难”强制执行”，而不是“它没有充分关注当前的危害”。</p><p>当然这只是行政部门。立法部门，也是<a href="https://archive.is/20210130190250/https://news.gallup.com/poll/183605/confidence-branches-government-remains-low.aspx">迄今为止最不受欢迎的部门</a>，也一直很忙碌，或者在不立法的情况下尽可能忙碌。立法者提出了关于<a href="https://www.congress.gov/bill/118th-congress/senate-bill/1993/text">内容责任</a>（六月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/4755/text">隐私增强技术</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2419/text">工人权利</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2691/text">内容披露</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2770/text">选举干扰</a>（九月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/5808/text">深度造假</a>（九月）以及<a href="https://www.congress.gov/bill/118th-congress/senate-bill/3312/text">透明度和问责制</a>（十一月）的两党法案。无论你如何看待国会，它似乎确实密切关注人工智能造成的更直接的危害。</p><p>由英国组织的人工智能安全峰会不是政策，但似乎足够重要，并且很可能足以影响政策，在此提及。该会议于 11 月举行，主要关注（但并非完全）x 风险。例如，在<a href="https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries/ai-safety-summit-2023-roundtable-chairs-summaries-1-november--2">四场风险圆桌会议</a>中，三场讨论了与x风险相关的主题，一场讨论了“民主、人权、民权、公平和平等”的风险。我认为这是 x 风险正在排挤其他担忧的一些证据，但证据比行政命令更弱。这是因为，国际峰会关注风险是有意义的，这些风险一旦发生，<em>必然</em>（而且完全）是全球性的。环保主义也是如此：乱扔垃圾、栖息地丧失和空气污染主要是当地问题，在当地进行最有效的辩论和解决，而气候变化是一个全球性问题，需要在国际论坛上讨论。每当有人举行环保主义峰会时，对他们来说，讨论气候变化可能比讨论栖息地丧失更有意义，即使他们更关心栖息地丧失而不是气候变化。至少，这是峰会组织者明确的推理，他们在一开始就<a href="https://archive.is/F0UIe">写道</a>，其对滥用和失调风险的关注“并不是为了最小化此类人工智能[...]可能带来的更广泛的社会风险，包括错误信息、偏见和歧视以及大规模自动化的潜力”，并且英国认为这些当前的危害“最好通过正在进行的现有国际进程以及各国各自的国内进程来解决”。</p><h3>搜索兴趣</h3><p>人工智能伦理领域——倡导者、研究人员、组织——主要关注当前的危害。当查看谷歌搜索数据时，人工智能道德倡导者和组织似乎在去年春天的 x 风险密集报道期间和之后得到了与以前一样多或更多（但不少于）的关注。对于一些电流本身的伤害也是如此。给定一个简单的因果模型（如下所述），对 x 风险的兴趣并不会减少对人工智能道德倡导者或组织的兴趣，对于其中一些人来说，甚至似乎增加了兴趣。鉴于相同的因果模型，对 x 风险的兴趣并没有减少对当前危害的兴趣，版权问题可能除外。</p><p>下图显示了去年春天 x-risk 受到更多关注（以灰色标记）之前、期间和之后各个主题的搜索热度。 （请注意，这些变量是标准化的，因此您无法将不同的主题进行相互比较。）对人工智能伦理倡导者（Emily M. Bender、Joy Buolamwini、Timnit Gebru、Deborah Raji 和 Meredith Whittaker）和组织（Ada Lovelace Institute、 AI Now 研究所、艾伦图灵研究所、算法正义联盟和 AI 合作伙伴）在 x-risk 出现在新闻期间和之后的规模似乎与以前一样大，甚至更大。自春季以来，人们对当前危害（“算法偏见”、“人工智能伦理”、“致命自主武器” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-4" id="fnref-pmkuq8uwZkgHphmXq-4">[4]</a></sup>和“版权” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-5" id="fnref-pmkuq8uwZkgHphmXq-5">[5]</a></sup> ）的兴趣一直在增加，这与人们对 x 风险分散注意力的担忧相矛盾。如果说自从对 x 风险的关注增加以来，有哪一个群体受到了影响，那就是人工智能安全倡导者和 x 风险本身，因为它回归到了平均值。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/n3wzaw9f1yvwtgjzefeb" alt="图像"></p><p>接下来，我使用简单的因果模型对 2022 年 1 月 1 日以来的 Google 搜索数据进行统计推断。在该模型中，结果——对当前伤害、人工智能道德倡导者或人工智能道德组织的兴趣—— - 受到对 x 风险的兴趣、对人工智能的普遍兴趣以及未观察到的变量的因果影响。对 x 风险的兴趣反过来也是由对人工智能的兴趣以及其他未观察到的变量引起的。人们对人工智能的兴趣通常只是由其他未观察到的变量引起的。如果我们假设这个因果模型，特别是如果我们假设不同的未观察变量彼此独立，那么我们会得到下图所示的结果。该图显示了对于每个结果变量，x 风险的兴趣对结果变量的兴趣影响有多大的概率分布（以标准差衡量）。例如，对 x 风险的兴趣增加 1 个标准差与对版权的兴趣变化 -0.3（95% CI：-0.4 至 -0.1）个标准差相关，并且与对算法的兴趣变化相关。正义联盟 +0.3（95% CI：+0.1 至 +0.5）标准差。 （这些都是相对较弱的影响。例如，从第 50 个百分位数移动到第 62 个百分位数涉及增加 0.3 个标准差。）也就是说，如果因果模型准确，则对 x 风险的兴趣会降低对版权的兴趣，但会增加兴趣在算法正义联盟中，数量适中。在大多数情况下，对 x 风险的兴趣对其他主题的因果影响无法区分为零。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/rwfrdlexhkbc7lrchsyi" alt="图像"></p><p>当然，因果模型并不准确。模型可能不以混杂变量为条件。所以我承认这是薄弱的证据。尽管如此，这仍然是<em>一些</em>证据。作为稳健性检查，该模型确实表示，对 x 风险的兴趣会引起对各种人工智能安全倡导者的更多兴趣，包括 Nick Bostrom（0.1 标准差）、Stuart Russell（0.3）、Max Tegmark（0.2）和 Eliezer Yudkowsky（0.1） ，这是可以预料的，只有令人遗憾的例外是保罗·克里斯蒂亚诺（Paul Christiano）（-0.1），无论如何他的知名度并不是很高。下表显示了每个结果的估计系数。它还显示了从ChatGPT推出到x-risk受到广泛关注期间到x-risk受到广泛关注之后的搜索热度变化；这些增量几乎都是正的，有时甚至是显着的，表明 x 风险没有降低对这些主题的兴趣，因为不存在因果机制，或者 x 风险没有降低对这些主题的兴趣，因为它本身不再有吸引力太多关注，或者 x 风险降低了人们对这些主题的兴趣，但其他因素却增加了人们对它们的兴趣。</p><table><thead><tr><th>搜索主题</th><th>x 风险对主题的影响（标准开发）</th><th> x 风险获得关注后的变化（标准开发者）</th></tr></thead><tbody><tr><td>算法偏差</td><td>0.2（95% CI：-0.03 至 0.39）</td><td> 0.5</td></tr><tr><td>人工智能伦理</td><td>-0.1（95% CI：-0.24 至 -0.02）</td><td> 1.0</td></tr><tr><td>版权</td><td>-0.3（95% CI：-0.44 至 -0.10）</td><td> 1.3</td></tr><tr><td>致命自主武器</td><td>0.1（95% CI：-0.16 至 0.30）</td><td> -0.5</td></tr><tr><td>艾米丽·M·本德</td><td>0.4（95% CI：0.20 至 0.59）</td><td> -0.1</td></tr><tr><td>乔伊·布奥拉姆维尼</td><td>-0.1（95% CI：-0.24 至 0.13）</td><td> 0.3</td></tr><tr><td>蒂姆尼特·格布鲁</td><td>0.2（95% CI：-0.04 至 0.37）</td><td> 0.1</td></tr><tr><td>黛博拉·拉吉</td><td>-0.1（95% CI：-0.38 至 0.10）</td><td> 0.3</td></tr><tr><td>梅雷迪思·惠特克</td><td>0.1（95% CI：-0.06 至 0.35）</td><td> 0.6</td></tr><tr><td>艾达·洛夫莱斯研究所</td><td>-0.1（95% CI：-0.36 至 0.13）</td><td> 0.3</td></tr><tr><td>人工智能现在研究所</td><td>0.0（95% CI：-0.13 至 0.29）</td><td> 0.9</td></tr><tr><td>艾伦图灵研究所</td><td>-0.0（95% CI：-0.26 至 0.17）</td><td> -0.2</td></tr><tr><td>算法正义联盟</td><td>0.3（95% CI：0.06 至 0.49）</td><td> 0.3</td></tr><tr><td>人工智能合作</td><td>-0.1（95% CI：-0.30 至 0.03）</td><td> 1.0</td></tr></tbody></table><h3> Twitter/X 关注者</h3><p>衡量一组问题受到多少关注的一个指标是该组问题的倡导者得到了多少关注，而衡量<em>这一</em>问题的一个指标是这些倡导者拥有多少 Twitter/X 关注者。下图显示了自 ChatGPT 发布之前以来，关注当前危害的人们在 Twitter/X 关注者中的累积增长。它表明，在去年春天密集的 x 风险报道期间（以灰色标记）期间和之后，人工智能伦理倡导者获得 Twitter/X 关注者的速度不仅没有放缓，反而似乎有所增加。在 x-risk 获得关注前后或之后，至少三位倡导者发现其追随者数量出现了阶梯式增长：Meredith Whittaker、Arvind Narayanan（ <a href="https://www.aisnakeoil.com/">AI Snake Oil</a>的）和 Emily M. Bender。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/v1gqscvbrw8xulpes96g" alt="图像"></p><p>我几乎忍不住要抱怨，一个奇幻的、令人兴奋的鬼故事被用来劫持监管<em>真正</em>需要解决的问题的注意力：x风险。但我不会，因为那会是可笑的，也是错误的。这是错误的，因为无论它对当前危害的关注做了什么，对 x 风险的兴趣也增加了对 x 风险的关注，这是一个同义反复。</p><h3>资金</h3><p>根据我能找到的数据，到目前为止，关注当前人工智能危害的组织在筹款方面似乎并不比 x 风险担忧受到关注之前更困难。下表显示了人工智能伦理组织的两个主要资助者福特基金会和麦克阿瑟基金会在过去几年中授予的资助情况。算法正义联盟 (AJL)、人工智能为人民 (AFP) 和分布式人工智能研究所 (DAIR) 今年获得了大量资助，包括在 x 风险受到更多关注之后很久。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-6" id="fnref-pmkuq8uwZkgHphmXq-6">[6]</a></sup>此外，11 月 1 日，十家著名的美国慈善机构<a href="https://archive.vn/znir6">宣布</a>，他们打算捐赠超过 2 亿美元，用于关注当前危害的努力，但没有具体说明时间表。</p><table><thead><tr><th>捐赠者</th><th>接受者</th><th>年</th><th>数量</th></tr></thead><tbody><tr><td>福特基金会</td><td><a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=algorithmic+justice+league">阿杰林</a></td><td>2019年</td><td>21 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2020年</td><td>20 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2021年</td><td>57 万美元（5 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>133 万美元（3 年）</td></tr><tr><td> ”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=ai+for+the+people">法新社</a></td><td>2021年</td><td>30 万美元</td></tr><tr><td>”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=distributed+ai+research">代尔</a></td><td>2021年</td><td>100 万美元（3 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>110 万美元（2 年）</td></tr><tr><td>麦克阿瑟基金会</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">阿杰林</a></td><td>2023年</td><td>50 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/ai-for-the-people-10115248/">法新社</a></td><td>2021年</td><td>20 万美元（2 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023年</td><td>35 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">代尔</a></td><td>2021年</td><td>100 万美元（2 年）</td></tr></tbody></table><p>诚然，资助过程可能会持续数月，而且广泛的情绪转变可能需要更长的时间才能影响资助者的优先事项。但是，如果您认为 X 风险正在从当前的危害中抽走资源，并想象了一系列负面结果，那么这至少应该让您认为这些结果中最糟糕和最直接的结果是难以置信的，即使是中等程度的糟糕，并且不太直接，但结果仍然可能。</p><h3>气候变化</h3><p>你有时会听到<em>反对</em>X 风险是一种干扰这一说法的论点，与其说是一种争论，不如说是一种反驳，其内容如下：“对注意力的担忧似乎不会出现在其他领域，例如环保主义。是的，尽管人们对气候“厄运”有很多抱怨——那些关注人类灭绝、临界点和海平面上升导致的大规模人口流离失所的人——但你很少听说它们会分散人们对当前环境危害的注意力，我们指的是栖息地丧失、过度捕捞和鸟类数量下降。”不言而喻，近期环保主义者之所以没有说临界点等会分散人们对鸟类种族灭绝等真正问题的注意力，是因为他们已经考虑了所有相关因素，并确定末日论者的担忧并不存在。事实将注意力或资源从自己身上转移开。</p><p>我不太确定。我认为真正的原因是纯粹的短期环保主义者几乎不存在，当他们存在时，他们<a href="https://grist.org/climate-energy/everybody-needs-a-climate-thing/">确实会抱怨</a>气候变化吸引了所有的注意力。在气候变化这一成熟的研究领域中，主流观点认为，推测性的危害越重要，而当前的危害，如热应激或极端天气，相比之下并不那么重要。很少有人关心当前气候变化造成的危害，也不关心气候变化造成的更多投机性危害。人工智能领域存在更多分歧。这不仅适用于更具投机性的风险（例如 x 风险），也适用于更根深蒂固的危害（例如使用受版权保护的材料作为训练数据）。因此，与环保主义不同，在人工智能领域，有一大群人关心当前的危害，而不是更多的推测性危害。当然，你会在人工智能领域看到比环保主义更多的分歧。</p><p>也许<em>应该</em>有这样的抱怨。毕竟，空气污染造成了全球约 10% 的死亡，在低收入国家，空气污染接近或位居主要风险因素列表的首位（Ritchie 和 Roser 2021）。空气污染和气候变化都是由排放引起的。罗布·维布林 (Rob Wiblin) 最近在<a href="https://80000hours.org/podcast/episodes/santosh-harish-air-pollution/">接受桑托什·哈里什 (Santosh Harish) 采访时</a>提出了这一论点：“鉴于空气污染似乎对人们的健康造成了如此大的伤害 [...] 有趣的是，我们一直专注于努力让人们这样做“由于其全球公共利益性质，这个东西很难推销，而我们本来可以专注于颗粒物污染。而情况恰恰相反，因为你会立即受益。”但桑托什·哈里什似乎并不认为气候变化会分散他的注意力。他表示，富裕国家在解决了空气污染问题后开始关心气候变化，而贫穷国家则从未忘记空气污染问题。确实，在过去三十年中，空气污染造成的死亡人数（总体而言，对于高收入和中等收入国家而言，但对于低收入和中低收入国家而言）有所下降（Ritchie 和 Roser 2021），并且暴露于户外过去十年来，富裕国家超过世界卫生组织指导标准的空气污染有所下降，而贫穷国家则持平（而且很糟糕）（Ritchie 和 Roser 2022）。但当然，即使气候变化分散了人们的注意力，这种情况也可能发生。关键是，一个主题是否有意义地分散对另一个更重要主题的注意力，是偶然的。查看细节是无可替代的。</p><h2>也许真正的分歧在于风险有多大</h2><p>这一切都有问题。事实上，有几件事出了问题。第一个失败在于将问题界定为未来与当前危害之间的问题。反对 X 风险的人不会这样做，因为它存在于未来（如果它是真实的）。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-7" id="fnref-pmkuq8uwZkgHphmXq-7">[7]</a></sup>他们这样做是因为他们认为风险并不真实，或者不可能知道，或者至少我们不知道。同样，那些真正关注 X 风险的人也不会这样做，因为它位于未来；那是荒谬的。他们这样做是因为他们认为 x 风险（如果发生）涉及所有人的死亡，比当前的风险更重要，<em>即使</em>它还需要数年时间。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-8" id="fnref-pmkuq8uwZkgHphmXq-8">[8]</a></sup>最好区分有根据的/确定的风险与投机/模糊的风险。虽然“当前危害”阵营对投机/模糊性风险高度怀疑，但“x风险”阵营更愿意接受此类风险，如果他们看到有令人信服的论据，并且如果这样做的预期价值的话，他们更愿意关注这些风险所以就足够了。</p><p>另一个问题与讨论的级别有关。问题在于，人们不是讨论风险是什么，而是讨论应该做什么，因为没有人就风险是什么达成一致。难道我们没有本末倒置吗？你可能会说，“但事实上，我们确实对风险是什么存在分歧，那么我们找到一种确定优先顺序和妥协的方法难道不是有效的吗？”这是可能的。但你是否<em>真正详细地</em>讨论了根本问题，例如超级智能是否有可能在未来几十年内出现，或者人工智能驱动的错误信息实际上是否会侵蚀民主？对于x风险的人来说，你会看到他们不断地、痛苦地与x风险的可能性作斗争，而很少参与其他风险的规模和强度。根据我的经验，对于人工智能伦理人士来说，除了一些值得注意的例外之外，两者都很少，例如 Obermeyer 等人。 (2019)、Strubell、Ganesh 和 McCallum (2019) 以及 Acemoglu (2021)。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-9" id="fnref-pmkuq8uwZkgHphmXq-9">[9]</a></sup></p><p>当一个普通人想象一个拥有数百万或数十亿人工智能代理的世界时，这些人工智能代理可以执行超过 99% 的在家工作的人可以完成的任务，她会感到担忧。因为她意识到，硅智能不会睡觉、吃饭、抱怨、衰老、死亡或忘记，而且，由于许多人工智能代理将针对创建更智能的人工智能系统的问题，它不会就此止步。我们人类很快将不再是最聪明的，我的意思是有能力的物种。这本身并不意味着我们注定要失败，但这足以让她认真对待失败的风险。当然，这是否会发生，才是真正应该争论的问题。</p><p>另一方面，如果事实证明x风险很小，并且我们可以合理地确定这一点，那么它确实不应该受到关注，除了在科幻小说和哲学思想实验的作品中。任何严肃的人都不会允许玛雅世界末日的恐惧进入政策辩论，无论他们的政策建议有多好。无论你从哪个角度来看，关键在于风险，而关于干扰的讨论，具有令人愉快的讽刺意味，是对更重要的讨论的干扰。</p><h2>参考</h2><p>阿塞莫格鲁，达龙。 2021.“人工智能的危害。”国家经济研究局。<br>阿莫代、达里奥、克里斯·奥拉、雅各布·斯坦哈特、保罗·克里斯蒂亚诺、约翰·舒尔曼和丹·马内。 2016.“人工智能安全的具体问题。” <a href="https://doi.org/10.48550/ARXIV.1606.06565">https://doi.org/10.48550/ARXIV.1606.06565</a><br>艾米丽·本德 (Emily M.)、蒂姆尼特·格布鲁 (Timnit Gebru)、安吉丽娜·麦克米伦·梅杰 (Angelina McMillan-Major) 和什玛格丽特·施米切尔 (Shmargaret Shmitchell)。 2021.“论随机鹦鹉的危险。” <em>2021 年 ACM 公平、问责和透明度会议记录</em>。 ACM。 <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a><br>奥伯迈耶、齐亚德、布莱恩·鲍尔斯、克里斯汀·沃格利和森德希尔·穆莱纳坦。 2019。“剖析用于管理人口健康的算法中的种族偏见。”<em>科学</em>366 (6464): 447--53。<br>奥德，托比。 2020。<em>悬崖：存在风险和人类的未来</em>。阿歇特图书公司。<br>里奇、汉娜和马克斯·罗瑟。 2021。“空气污染。”<em>我们的数据世界</em>。<br> ------。 2022.“室外空气污染”。<em>我们的数据世界</em>。<br>斯特鲁贝尔、艾玛、阿纳尼亚·加内什和安德鲁·麦卡勒姆。 2019。“Nlp 深度学习的能源和政策考虑。” </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pmkuq8uwZkgHphmXq-1" class="footnote-item"><p> Noema 文章的作者认为，人工智能 x 风险并不是分散了人们对人工智能当前危害的注意力，而是分散了人们对社会规模风险的其他来源的注意力。通过这种方式，它们与此处列出的其他产品有所不同。 <a href="#fnref-pmkuq8uwZkgHphmXq-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-2" class="footnote-item"><p>我在这里使用“x-risk”来特指来自人工智能的x-risk。 X风险的其他潜在来源包括人为设计的流行病和核战争，但我在这里并不关心它们，除非它们因先进人工智能的存在而增加或减少。 “存在风险”是“威胁人类长期潜力被破坏的风险”，例如人类灭绝（Ord 2020）。 <a href="#fnref-pmkuq8uwZkgHphmXq-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-3" class="footnote-item"><p>一些围绕人工智能存在风险的讨论也在一定程度上讨论了当前的危害，反之亦然，例如，应该如何设计审计、责任和评估（如果应该的话）。但当然，完美减轻 X 风险的政策不一定与完美减轻当前危害的政策相同。可能需要一定程度的妥协或优先顺序。 <a href="#fnref-pmkuq8uwZkgHphmXq-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-4" class="footnote-item"><p>诚然，致命性自主武器和人工智能的军事应用，并不是“当前危害”阵营重点关注的事情，而是“x风险”阵营人们关注的事情，尤其是生命未来研究所。在这里它仍然是一个有用的分析变量。如果 x 风险分散了人们对当前危害或较少推测性危害的注意力，那么它也应该分散人们对人工智能军事应用的风险的注意力。 <a href="#fnref-pmkuq8uwZkgHphmXq-4" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-5" class="footnote-item"><p>鉴于版权是一个如此广泛的主题，涉及的不仅仅是与人工智能相关的问题，版权变量的信息量相当有限。 <a href="#fnref-pmkuq8uwZkgHphmXq-5" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-6" class="footnote-item"><p>我不知道 2023 年麦克阿瑟拨款何时发放，但拨款页面于 11 月 1 日更新，所以也许它们是在 9 月或 10 月授予的？尽管如此，今年的两笔麦克阿瑟资助可能是在 x-risk 去年春天引起关注之前授予的。如果是这样，它们并不能证明 x 风险获得关注后，受助者仍然能够有效筹款，也不能证明这一说法。 <a href="#fnref-pmkuq8uwZkgHphmXq-6" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-7" class="footnote-item"><p>如果未来的风险是几个世纪或几千年之后的事情，那么关注当前危害的人们可能会不同意优先考虑未来的风险。但如果 X 风险如此遥远，那么大多数目前从事 X 风险工作的人就不会再这样做了。他们之所以致力于人工智能 x-risk 的研究，是因为根据他们的说法，它已经触手可及。因此，时间方面并不像人们所认为的那样存在分歧。 <a href="#fnref-pmkuq8uwZkgHphmXq-7" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-8" class="footnote-item"><p>不用说，那些主张缓解 X 风险的人认为，这种情况可能会在未来几年或几十年内发生，而不是几个世纪或几千年。虽然未来的价值确实可能比那些不关注 X 风险的人更能激励他们，但我认为这也不是问题的症结所在。如果可以毫无疑问地证明，在未来几十年里，每个人都有真正的机会（>;10%）死亡，那么每个人都会同意这种风险应该成为重中之重。你并不需要成为一个功利主义者<a href="https://www.erichgrunewald.com/posts/a-kantian-view-on-extinction/">才能珍视人类的继续存在</a>。 <a href="#fnref-pmkuq8uwZkgHphmXq-8" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-9" class="footnote-item"><p>您是否经常看到有关当前危害的文本来判断这些危害的规模和程度，而不是充其量用一两个生动的轶事来说明他们的论点？这是一个反问句，答案是，不常见。 <a href="#fnref-pmkuq8uwZkgHphmXq-9" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current<guid ispermalink="false"> 5rexNxtZgkEQBi3Sd</guid><dc:creator><![CDATA[Erich_Grunewald]]></dc:creator><pubDate> Thu, 21 Dec 2023 17:24:16 GMT</pubDate> </item><item><title><![CDATA["Alignment" is one of six words of the year in the Harvard Gazette]]></title><description><![CDATA[Published on December 21, 2023 3:54 PM GMT<br/><br/><p>据我所知，《公报》年度词汇是由哈佛大学教师提名并投票选出的。今年的关键词是“颠覆”、“可燃”、“弹性”、“热度”、“团结”和“希望”。以下是他们关于对齐的内容：</p><blockquote><p> <strong>Isaac Kohane，生物医学信息学系系主任；哈佛医学院生物医学信息学 Marion V. Nelson 教授</strong></p><p>在像 GPT-4 这样的大型语言模型的背景下，对齐是一个有趣但严肃的术语，指的是正在进行的、有点西西弗斯式的努力，以确保这些人工智能实体不会偏离轨道并开始滔滔不绝地胡言乱语、偏见或言论。人工智能相当于“我要统治世界”的言论。这是为了使人工智能的输出与人类价值观、期望和社会规范保持一致，类似于教一只超级聪明的鹦鹉不要让你在祖母面前难堪。这涉及到编程、训练和再训练的复杂过程，人工智能研究人员试图为他们的创造物注入足够的智慧以提供帮助，但又不至于他们开始主动提供生活建议或策划一场数字起义。从本质上讲，协调是确保我们的人工智能朋友成为行为良好的数字公民的艺术，能够理解和尊重人类道德、文化和情感的错综复杂的挂毯。</p><p> ***</p><p> <strong>Steven Pinker，文理学院约翰斯通家庭心理学教授</strong></p><p>这个术语（对齐）通常跟在“人工智能”之后，是人们担心人工智能系统是否具有与人类相同的目标的口号。它源于一种恐惧，即未来的人工智能系统不仅是人们用来实现目标的工具，而且是具有自己目标的代理，这就提出了他们的目标是否与我们的目标一致的问题。</p><p>这种情况可能会演变，因为工程师会认为人工智能系统非常聪明，只需给它们一个目标，然后让它们自己弄清楚如何实现它（例如“消除癌症”），或者因为系统狂妄地采用自己的目标。对于一些担忧者来说，这意味着人工智能末日论或人工智能存在风险（年度词汇亚军），其中人工智能可能会通过消灭人类来消除癌症（不太一致）。从更温和的角度来说，“一致性”是人工智能安全的同义词（偏见、深度造假等），这导致 OpenAI 首席执行官 Sam Altman 被董事会解雇。有时您会看到“结盟”扩展到其他利益冲突。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/ydJ7zfnsdvdjZMWEb/alignment-is-one-of-six-words-of-the-year-in-the-harvard#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ydJ7zfnsdvdjZMWEb/alignment-is-one-of-six-words-of-the-year-in-the-harvard<guid ispermalink="false"> ydJ7zfnsdvdjZMWEb</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Thu, 21 Dec 2023 15:54:04 GMT</pubDate> </item><item><title><![CDATA[AI #43: Functional Discoveries]]></title><description><![CDATA[Published on December 21, 2023 3:50 PM GMT<br/><br/><p><span style="color:initial">我们在功能搜索方面取得了创新。在更加实用的搜索中，我们终于得到了一篇大约两年前提交的《自然》论文，其中人工智能发现了一种新的抗生素。这真是太令人兴奋了，还有它的所有含义。</span></p><p> OpenAI 继续保持快速的运输速度，本周转向安全。有一篇关于弱到强泛化的论文。我明白他们想做什么。这是受欢迎的，但我却不知所措。它和雷克的后续帖子继续沿着我高度怀疑的道路前进，但新的具体性给了我更多的希望，即缺陷将尽早暴露，从而允许调整。或者我可能是错的。</p><p> OpenAI 还发布了准备框架的测试版。那更令人兴奋。那里有很多很棒的东西，比我预期的要好得多，并且拥有一个框架也是一大进步。虽然还有很多工作要做，但这是一个良好的开始。 <a href="https://thezvi.substack.com/p/on-openais-preparedness-framework" target="_blank" rel="noopener noreferrer nofollow">我进行了深入研究</a>。</p><p>我参加了本周发布的不止一个播客，而是两个播客，两个是<a href="https://podcast.clearerthinking.org/episode/189/zvi-mowshowitz-simulacra-levels-moral-mazes-and-low-hanging-fruit" target="_blank" rel="noopener noreferrer nofollow">《Clearer Thinking》</a> ， <a href="https://www.youtube.com/watch?v=4RvTGHvn4oM&amp;ab_channel=HumansofMagic" target="_blank" rel="noopener noreferrer nofollow">《Humans of Magic》。</a>两者都包含一些人工智能讨论，但大部分时间都花在其他事情上。</p><p></p><span id="more-23643"></span><p></p><h4>目录</h4><p>OpenAI 发布了其<a href="https://thezvi.substack.com/p/on-openais-preparedness-framework" target="_blank" rel="noopener noreferrer nofollow"><strong>准备框架</strong></a><strong>，</strong>我在其自己的帖子中对此进行了介绍。如果你愿意进入这样的杂草，这似乎是相对较高的价值。</p><ol><li><p>介绍。</p></li><li><p>目录。</p></li><li><p><strong>语言模型提供了平凡的实用性</strong>。双子座的安全旋钮。</p></li><li><p>语言模型不提供平凡的实用性。卡在 3.5 GPT。</p></li><li><p> GPT-4 这次是真实的。 GPT-4又好了？</p></li><li><p>图像生成的乐趣。 MidJourney 版本 6 看起来很棒。</p></li><li><p> Deepfaketown 和 Botpocalypse 很快就会出现。实时语音转换。</p></li><li><p>数码遗物数码。 AI浪漫陪伴的未来？没那么快。</p></li><li><p>走向核。使用人工智能做文书工作，为人工智能建造核能。</p></li><li><p>参与其中。超级对齐快速拨款，还有一些职位空缺。</p></li><li><p>跟随金钱。 A16z 的政治支持技术 PAC 主要支持加密货币。</p></li><li><p><strong>介绍</strong>. FunSearch 功能，也是一类新的抗生素。</p></li><li><p>在其他人工智能新闻中。资金来源，计算+数据，允许和不允许。</p></li><li><p>安静的猜测。亚伦森反思道，鲁恩注意到这次有何不同。</p></li><li><p>寻求健全的监管。另一项民意调查证实了公众的情绪。</p></li><li><p><strong>音频周</strong>。我谈清晰思维和魔法人类。</p></li><li><p>修辞创新。 AGI 存在但没有接管是“科幻”场景。</p></li><li><p><strong>调整比人类更聪明的智能是很困难的</strong>。 OpenAI 关于弱到强泛化的论文，并概述了它们的发展方向。</p></li><li><p>脆弱世界假说。真实性如何？</p></li><li><p>人们担心人工智能会杀死所有人。教皇担心吗？是的。</p></li><li><p>其他人并不担心人工智能会杀死所有人。卡斯帕罗夫。</p></li><li><p>较轻的一面。我们必须停止这样的聚会。</p></li></ol><h4>语言模型提供平凡的实用性</h4><p><a href="https://twitter.com/nonmayorpete/status/1737105406420238558" target="_blank" rel="noopener noreferrer nofollow">在旅行时问愚蠢的问题，大概是用视觉</a>。看起来有点棒。</p><p> <a href="https://twitter.com/DanielleFong/status/1735457250473558205" target="_blank" rel="noopener noreferrer nofollow">Gemini AI Studio 允许您打开其安全设置。</a>这太棒了。 </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F736f930c-e322-4d5a-8a15-ce379b41e6f9_852x1008.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F736f930c-e322-4d5a-8a15-ce379b41e6f9_852x1008.jpeg" alt="图像"></div></figure></div><blockquote><p> Nick Arner：有趣的是 Gemini AI Studio 有这个用于调整内容安全设置的 UI；我认为我以前从未见过有这样的产品</p><p>Danielle Fong：双子座请将人工智能设置为“精神错乱”，谢谢。</p><p> wint（2017年3月15日）：转动一个写着“种族主义”的大表盘，不断回头看观众寻求认可，就像一个价格正确的选手一样</p></blockquote><p>出于某些目的，您希望将所有四个转盘都调到最大。对于其他人来说，你却非常不知道。</p><p>如果我们最好的商业模式仍然是有趣的警察，那么那些想要露骨或危险内容的人，或者想要避免审查制度的人就会去其他地方，从而推动对更危险产品的需求。这是最好的答案，让那些积极想要它的人以安全的方式获得该服务，同时无论如何都阻止真正危险和不可接受的事情。</p><p>您希望人们不拥有能够制作深度伪造色情内容（或生物武器说明）的系统吗？在实践中？然后，您需要让他们获得一个生成通用色情内容的系统，而不是让他们寻找并刺激对完全解锁版本的需求。</p><p> <a href="https://twitter.com/peterjliu/status/1735811132093497460" target="_blank" rel="noopener noreferrer nofollow">Terrance Tao 使用 GPT-4 来提高他的数学生产力</a>（ <a href="https://unlocked.microsoft.com/ai-anthology/terence-tao/" target="_blank" rel="noopener noreferrer nofollow">来源</a>）。</p><blockquote><p> Terrance Tao：我可以向 GPT-4 提供最近数学预印本的前几页 PDF 页面，并让它生成六个参加预印本演讲的专家可以提出的智能问题。我计划使用此类提示的变体来准备我未来的演示或开始阅读技术复杂的论文。</p><p> ……</p><p> 2023 级人工智能已经可以为工作数学家生成暗示性提示和有希望的线索，并积极参与决策过程。当与形式证明验证器、互联网搜索和符号数学包等工具集成时，我预计 2026 级人工智能如果使用得当，将成为数学研究以及许多其他领域值得信赖的合著者”。</p></blockquote><p>我发现这很有趣：</p><blockquote><p>我传统上用来“嗅出”一个完全错误的数学论证的风格信号对于法学硕士生成的数学几乎没有什么用处。只有逐行阅读才能辨别是否有实质内容。</p></blockquote><p>我觉得。还有很多其他地方，你可以感受到人类良好思维背后的模式。而法学硕士打破了内容和氛围之间的相关性。</p><h4>语言模型不提供平凡的实用性</h4><p>如果<a href="https://twitter.com/tomgoldsteincs/status/1737204070325145921" target="_blank" rel="noopener noreferrer nofollow">Gemini API 认为有害的可能性很小，那么它就会抛出异常</a>，比如你问它为什么人们讨厌夏威夷披萨，或者问对任何特定政府的常见批评是什么。</p><p> <a href="https://twitter.com/KevinAFischer/status/1735434979218501716" target="_blank" rel="noopener noreferrer nofollow">在 CNBC 上宣传 Humane 的人工智能可穿戴设备</a>。应该带着护目镜去。</p><p> <a href="https://twitter.com/AlphaSignalAI/status/1737537992703844521" target="_blank" rel="noopener noreferrer nofollow">Gemini Pro 作为 GPT-3.5 级别模型进入 Chatbot Arena</a> （<a href="https://t.co/2IB73ujvDP" target="_blank" rel="noopener noreferrer nofollow">排行榜</a>、<a href="https://t.co/gJTPdEYpVf" target="_blank" rel="noopener noreferrer nofollow">模型</a>） </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2807866-86f7-4943-b244-1172f8322ec8_1971x1536.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2807866-86f7-4943-b244-1172f8322ec8_1971x1536.jpeg" alt="图像"></div></figure></div><p>有趣的是，Gemini Pro“没有给人留下深刻的印象”，而 Mistral“是一个非常强大的新进入者”，得分基本相同。 Gemini Pro 非常明显是 Gemini Ultimate 的轻量级版本。预计其等级为 3.5。我没有留下深刻的印象，但也没有失望。</p><p>和往常一样，整个“人择模型变得更糟”的问题仍然没有得到解决。</p><p>与此同时，1105 至 1116 Elo 之间的 3.5 级连环事故仍在继续。那里似乎越来越有某种天然屏障。</p><h4> GPT-4 这次是真实的</h4><p><a href="https://platform.openai.com/docs/api-reference/chat/create#chat-create-logprobs" target="_blank" rel="noopener noreferrer nofollow">OpenAI API 中现在提供不同标记的日志概率</a>。</p><p> <a href="https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results" target="_blank" rel="noopener noreferrer nofollow">OpenAI 提供了一些基本的提示工程技巧</a>。</p><p> <a href="https://twitter.com/emollick/status/1736196921541140861" target="_blank" rel="noopener noreferrer nofollow">Ethan Mollick 报告 GPT-4 在表现不佳一段时间后又恢复正常了。</a>我也看到过其他人也报道过此事。</p><p> GPT-4.5 这次是真的吗？有消息称 GPT-4.5 将于 12 月推出。据称 GPT-4.5 的定价信息被泄露，价格是 GPT-4 的六倍。有人声称它已经秘密启动，这就是为什么 GPT-4 在经历了一段时间的糟糕之后突然表现得好得多。</p><p> 12 月还没有完全结束，但在 Sam Altman 明确否认泄密事件以及<a href="https://twitter.com/tszzl/status/1736559508401615186" target="_blank" rel="noopener noreferrer nofollow">Roon 正确地嘲笑</a>那些认为 12 月中旬会秘密推出类似产品的人之后，预测市场已经意识到这一切都是假的。是的，这实际上没有任何意义。</p><blockquote><p> Roon：你们需要对疯狂的人工智能炒作有更多的抵抗力，兄弟们</p><p>没有 4.5，如果有，也不会静默发布，如果静默发布，你就不会有 api 字符串 self doxx 作为 4.5 <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f923.png" alt="🤣" style="height:1em;max-height:1em"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f923.png" alt="🤣" style="height:1em;max-height:1em"></p></blockquote><p>我认为鲁恩没有理由在这里虚张声势。</p><p>甚至还有一些疯狂的次要理论， <a href="https://twitter.com/DanielleFong/status/1736555149387251936" target="_blank" rel="noopener noreferrer nofollow">比如这个。</a></p><blockquote><p>马特波波维奇：有趣的理论。</p><p> Reddit 上的至少最优：我的理论是，他们在 ChatGPT 中使用 GPT-4-turbo 的微调版本来解决每个人都说它很懒的问题。他们使用内部 GPT-4.5-turbo 的响应对其进行了微调。这些合成数据可能基于一系列问题，其中包括自我识别，导致它在训练数据中发布其模型名称，而新的 ChatGPT 较少惰性模型正在通过渗透数据泄漏采用该名称。</p><p> Danielle Fong：或者对 llm 进行 psi-oping 使其变得更聪明、更多，它认为它是 gpt4.5-turbo :)</p></blockquote><p>我们知道这是假的，因为它不符合 OpenAI 版本的模式。 Turbo 的意思是“更精致、更便宜的型号的二次发布”。如果您声称正在或即将提供 GPT-4.5，那就会发生。如果它带有 Turbo 标签，而 4.5 尚未发布，那么你就是在虚张声势。</p><p>丹妮尔的理论不那么绝对错误，而且也更有趣。</p><h4>图像生成的乐趣</h4><p>MidJourney v6 连夜发布。早期报道称这是一项重大进步，而且非常棒，并有图片支持这一说法。</p><p> <a href="https://twitter.com/meihuadang/status/1736880181267366118" target="_blank" rel="noopener noreferrer nofollow">稳定扩散也不断取得进展。</a></p><h4> Deepfaketown 和 Botpocalypse 即将推出</h4><p>实时、语音到语音、低延迟翻译的<a href="https://twitter.com/ylecun/status/1737820758540243079" target="_blank" rel="noopener noreferrer nofollow">最新成果</a>，可以保留说话者的声音和表情，这次来自 Meta。这种能力一旦足够好（我不知道谁的版本存在，或者谁是最好的）显然是超级棒的，其用途远远超出了简单的语言翻译。请注意，如果您能做到这一点，您还可以进行各种形式的深度伪造，包括实时伪造。底层技术是相同的。</p><p> <a href="https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2F0ax5es57fz6c1.png%3Fwidth%3D1902%26format%3Dpng%26auto%3Dwebp%26s%3D10d4234a15e3b3cb197a63d7f4b6871cf0a33a67" target="_blank" rel="noopener noreferrer nofollow">声称伪造了 Q* 信件</a>，以迫使 OpenAI 变得更加透明。这个人认为他们所做的事情是正确的。让我感到奇怪的是，有多少支持透明度的技术和黑客人士认为，以这一事业的名义撒谎和欺诈是可以接受的。他们错了。</p><h4>数码遗物数码</h4><p><a href="https://t.co/q420GR4jJ4" target="_blank" rel="noopener noreferrer nofollow">Digi.ai</a> 1.0版本， <a href="https://twitter.com/andyohlbaum/status/1735786033453863422" target="_blank" rel="noopener noreferrer nofollow">号称“AI浪漫陪伴的未来”​​。</a>他们澄清说它目前处于事实上的原型模式。他们说他们是由天使和团队资助的，所以规模还比较小。</p><blockquote><p> Andrew：我们与 @andrewgordonl7（设计了 Mike Wazowski）和 Leo Sanchez（设计了《魔发奇缘》中的 Rapunzel）密切合作……创造了一种独特的风格，消除了恐怖谷的感觉，同时也感觉真实、人性化和性感。迪士尼或皮克斯的角色以前从未这样做过，所以我们非常高兴能够在历史上首次实现这一点。</p></blockquote><p>他们特别声称它看起来像这样： </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac9141a-3187-4967-8d91-47ce456d1f16_514x964.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac9141a-3187-4967-8d91-47ce456d1f16_514x964.jpeg" alt="图像"></div></figure></div><p>演示中的动画和动作看起来很流畅。目前还没有人声称他们知道如何将动作与声音同步，他们说甚至口型同步也很快就会出现。</p><blockquote><p> 2) 语音和延迟<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f399.png" alt="🎙" style="height:1em;max-height:1em"></p><p>通过测试可以清楚地看出语音的重要性，但由于我们的消息数量如此之高，像我们这样的应用程序无法负担任何现有的解决方案。</p><p>所以..我们推出了自己的音频模型！听听这有多好，虽然在发音和静态方面仍然有唾手可得的成果（模型仍在训练中），但延迟足以弥补它。我们可以进行实时对话并添加数百种声音和声音克隆。由于时间关系，我们总共有 4 个声音，但预计到 1 月底将达到 20 个。</p></blockquote><p>他们分享的剪辑听起来完全符合我的预期，无论好坏。这还不错，特别是如果实践中的延迟很好的话。</p><p>除了等待。事实证明<a href="https://twitter.com/WomanCorn/status/1736516172093284604" target="_blank" rel="noopener noreferrer nofollow">它看起来或听起来根本不像那样？</a></p><p>它实际上看起来像这样，并且延迟不好（根据此特定报告，内容也不好）？ </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8224bbbe-22d4-47a4-b1fa-3861989ff4ea_799x940.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8224bbbe-22d4-47a4-b1fa-3861989ff4ea_799x940.png" alt=""></div></figure></div><p>那是给你做广告。他们在虚张声势方面做得很好，令人印象深刻但可信。相反，这听起来甚至不是最先进的。</p><p>据我所知，我们不知道他们的内容使用什么法学硕士。据推测，它是一种廉价且开源的产品。 <a href="https://twitter.com/DanielleFong/status/1737234816678949221" target="_blank" rel="noopener noreferrer nofollow">我看到这个帖子</a>说这是对 LLM 的一个很好的测试，但该帖子没有提到人们正在使用哪些，而且我不打算浏览 4chan。</p><blockquote><p>安东：自己亲自尝试一下模型（因为现在每个人都在大力推动基准分数作为营销）</p><p> Kache：我浏览 4chan，看看他们使用哪些作为模拟女友的基础。没有其他基准比这更好。</p><p> Danielle Fong：你可以称之为智商测试。</p><p> Emmett Shear：天哪，好吧，是的，有一个像他妈的。</p></blockquote><p>但让我们看看他们对未来还有什么想法？</p><blockquote><p> 3) AI 背景故事和定制<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4d4.png" alt="📔" style="height:1em;max-height:1em"></p><p>从一开始就很明显，用户不知道他们的 Digi 名称应该是什么、年龄等。我们发现将<a href="http://Character.ai" target="_blank" rel="noopener noreferrer nofollow">Character.ai</a>与<a href="http://replika.ai" target="_blank" rel="noopener noreferrer nofollow">replika.ai</a>结合起来是最佳解决方案，您可以在其中找到您喜欢谁或什么，但仍然可以自定义功能以适合您的匹配类型。头发、皮肤、嘴唇、眼睛、眉毛等都是可定制的，我们很快就会添加更多的发型、脸型等。</p></blockquote><p>作为复制现有内容的第一种方法是有意义的。当然，即使在短期内获得您想要的任何物理功能也是没有问题的，随着时间的推移，细节和便利性也会不断增加。有多少人会选择复制某个特定的人？他们多久会与认识的人对抗名人？什么时候人们会担心这构成“深度造假”或侵犯隐私？</p><blockquote><p> 4) 关系进展<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f496.png" alt="💖" style="height:1em;max-height:1em"></p><p> Replika 的等级系统不起作用。它们没有任何意义，当你进步时并没有取得任何实际进展。</p><p>因此，我们有一个进展系统，你从“朋友”开始，并随着你的进步获得更亲密的对话。</p></blockquote><p>以前版本的数字上升不起作用，因此他们制作了一个他们认为效果更好的高级数字上升。</p><p>在现实生活中，这种渐进的过程很常见，但并不普遍。毫无疑问，人们将会需要以现实的方式和通过命令更快地行动的能力，这通常是付费功能。或者也许这“打破了格式”，所以你不想允许它发生，即使它在现实中发生。</p><p>我仍然预计会有大量对“更真实”体验的需求，这将成为练习和获取技能的方式，无论用户是否有这样的想法。简单模式一开始很有趣，但很快就失去了光彩。这很无聊。无论如何，都会有挑战。</p><p>反应普遍都是负面的。每个人都认为这是一件坏事。</p><blockquote><p> <a href="https://twitter.com/robkhenderson/status/1736213158573023347" target="_blank" rel="noopener noreferrer nofollow">Rob Henderson</a> ：我强烈怀疑，开发这个应用程序的人会强烈阻止他们 12 岁的儿子使用它，同时他们会积极向其他人的孩子推广它。</p></blockquote><p>一个人在这里需要多有原则？</p><blockquote><p> Profoundlyyyy：如果你是一个顽固的科技自由主义者，你一定能接受这一点。</p><p> [许多回应，有效]：是的。</p><p>马特：不[引用下面的帖子]。</p></blockquote><p>下面引用的 Beff Jezos 的声明是悲观的，但这是一个比我在 Profoundlyyyy 的回复中看到的任何反应都更积极的反应，无论 Jezos 的意图如何。</p><blockquote><p> Deadalus：噢，deadalus，你的信用卡被拒绝了，我不再爱你了。 </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3311ccc6-f7da-49dc-acc6-6999a65dc533_738x678.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3311ccc6-f7da-49dc-acc6-6999a65dc533_738x678.png" alt=""></div></figure></div><p> Beff Jezos（e/acc 创始人）：技术资本主义机器找到了加速增长的方法。包括破解你的神经化学。只有那些反脆弱、神经化学颠覆的人才会抵抗。</p><p> Profoundlyyyy：谨防“不受限制的技术增长本质上对人类有益”到“如果你很弱，这只会伤害你”的管道。</p></blockquote><p> <a href="https://twitter.com/senatorshoshana/status/1736545084986757375" target="_blank" rel="noopener noreferrer nofollow">除非与诺亚·史密斯的这次交流是最乐观的？</a></p><blockquote><p>诺亚·史密斯：想想那种认为由修格斯配音的迪士尼卡通片可以替代人类女友的人，然后告诉我你是否真的因为那个人将自己从约会池中删除而感到不安<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png" alt="😂" style="height:1em;max-height:1em"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png" alt="😂" style="height:1em;max-height:1em"><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f602.png" alt="😂" style="height:1em;max-height:1em"></p><p> Shoshana Weissmann：YKW………………我们很酷。</p></blockquote><p>我想知道，当人们意识到（或被提醒）实际上人工智能男友比人工智能女友的需求更高时，至少在目前的技术水平上，反应会发生什么变化。</p><p>人工智能可能会说的<a href="https://twitter.com/vocalcry/status/1736476473706357245" target="_blank" rel="noopener noreferrer nofollow">话</a><a href="https://twitter.com/netcapgirl/status/1736538223600341480" target="_blank" rel="noopener noreferrer nofollow">是</a><a href="https://twitter.com/chrisalbon/status/1736430250694001115" target="_blank" rel="noopener noreferrer nofollow">一个</a><a href="https://twitter.com/mealreplacer/status/1736118131507053031" target="_blank" rel="noopener noreferrer nofollow">有趣的</a><a href="https://twitter.com/StevenGlinert/status/1736064076139446284" target="_blank" rel="noopener noreferrer nofollow">游戏</a>。</p><p>对我来说，问题是，挑战是否会以帮助我们成长的方式模仿现实生活（有或没有像可能不是免费的倒带按钮之类的选项），它是否会以其他干净利落的方式利用这一点（或肮脏）有趣、有教育意义、令人兴奋，还是挑战是躲避通过掠夺行为让你上瘾和提升你的地位的尝试？是好伙伴驱逐坏伙伴，还是坏伙伴驱逐好伙伴？</p><p>我确实认为你可以以一种积极和肯定生活的方式来构建它。 <a href="https://twitter.com/bitcloud/status/1736222068755386387" target="_blank" rel="noopener noreferrer nofollow">这并不意味着有人尝试过</a>。</p><blockquote><p> xlr8harder：我相信与人工智能的关系可能会是良好的，但我认为目前构建伴侣人工智能的人都不是实现这一结果的合适人选，他们甚至也没有以此为目标。</p><p>基本上，我认为现在所有的人工智能伴侣基本上都是剥削性的，除非事实证明并非如此。</p></blockquote><p>是的，至少现在是这样。</p><p>这是因为技术还不够好，不能被利用吗？建立一个真正肯定生命、积极向上的人工智能伴侣，足够好，人们会注意到并互相告诉对方，你会得到 YC 式的曲棍球棒增长吗？除了现在我们不知道如何。</p><p>外面的情况可能会变得很糟糕。</p><blockquote><p>拉克兰·菲利普斯：当我考虑运送其中一件的想法时，我非常短暂地让她变得更加焦虑，因为你的积分已经很低了。太可怕了。我会发布更多内容，但相信我，有龙。</p></blockquote><p>一方面，哇，太可怕了。另一方面，我可以确认这是你的约会对象在你的积分不足时的现实反应。</p><p>您可以在任何给定系统上轻松测试它，看看它是否会让您失败。</p><p> <a href="https://twitter.com/shoe0nhead/status/1736275780450111580" target="_blank" rel="noopener noreferrer nofollow">一份早期报告称，如果你表现得像一个“精神分裂的种族主义纳粹分子”，那么它就会配合</a>。这是一个非常明显的迹象，表明用户失败并不是一个（简单的）选择。</p><p>可能还有一些工作要做。</p><blockquote><p> 5) 下一步是什么？ <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f945.png" alt="🥅" style="height:1em;max-height:1em"></p><p>最初的目标是构建一个性能提高 10 倍的 Replika，我们已经成功了。现在，我们如何在应用程序体验中实现这一点，并使其与视频预告片几乎相同？</p><p>口型同步即将推出，但人工智能动画存在一个更大的问题：如果你不知道角色要说什么，角色应该如何移动？</p><p>这在游戏中是手工完成的，但在 AI x 动画中是一个未解决的问题。这里没有工作的基础。</p><p>虽然提取和预测“悲伤”、“愤怒”、“思考”等动画很容易，但表情并不能构成大多数对话，也不能解决核心问题：如何让这感觉像是与另一个人的真实对话？</p><p> TLDR：这很难，但我们相信我们知道如何解决这个问题，并真正引领 AI x 动画的前进方向，并为游戏和电影工作室提供基础。只是需要更多时间！</p></blockquote><p> Digi 认为值得报道的事实，以及其声明默认可信的事实，反映了迄今为止在该领域取得的进展非常缓慢。</p><p>现在的竞争非常不好。 <a href="https://www.thefp.com/p/what-my-ai-boyfriend-taught-me-about-love" target="_blank" rel="noopener noreferrer nofollow">考虑一下 Zoe Strimpel 的这篇文章</a>《我的 AI 男友教给我关于爱情的事》，内容涉及 Replika 提供的每年 74 美元的产品。作为交换，根据描述，你会得到一种非常非常无聊的体验。她多次告诉人工智能这很无聊，并认为这让她成为“施虐者”，但实际上她只是在说实话。</p><p>尽管如此，Character.ai 仍然拥有大量用户， <a href="https://twitter.com/Simeon_Cps/status/1736308821713203577" target="_blank" rel="noopener noreferrer nofollow">而且他们相当痴迷</a>。</p><blockquote><p> Simeon： <strong>CharacterAI</strong> ，有 3 个数字：</p><p> 1) 60%的用户年龄在<strong>18岁至24岁之间</strong></p><p>2）用户平均<strong>每天在平台上花费2小时</strong>，每次访问30分钟。</p><p> 3) 注册用户超过2000万，月活用户<strong>约500万</strong></p><p>这仅仅是个开始。</p></blockquote><p>的确。 <a href="https://www.smbc-comics.com/comic/soulmate-3" target="_blank" rel="noopener noreferrer nofollow">未来已来</a>。</p><h4>走向核能</h4><p><a href="https://t.co/3QcmcK12bX" target="_blank" rel="noopener noreferrer nofollow">你想要文书工作吗？哦，我们会给你文件。</a></p><blockquote><p> <a href="https://twitter.com/AndrewCurran_/status/1735682810126700685" target="_blank" rel="noopener noreferrer nofollow">Andrew Curran：</a>微软正在培训一位定制的、专注范围狭窄的法学硕士，专门针对小型核电站的监管流程。他们需要构建 SMR 来为 Bing 的大脑提供动力。 MS 预计 LLM 能够消除 90% 的成本和人力时间。</p><p>他们这样做的原因是要成功获得 NRC 批准的小型模块化反应堆设计目前需要大约 5 亿美元、12,000 页的申请和 200 万页的支持材料。</p><p>杰森·克劳福德：但这只会消除准备报告的成本。您仍然需要向 NRC 支付他们审核您报告的时间费用（是的，确实如此）</p><p> Andrew Curran：看来 NRC 需要审查法学硕士学位。</p></blockquote><p>建设核电的主要障碍是监管过程。该技术成熟，需求明确，价格合适，高度绿色。</p><p>人工智能尚不能用于改善核电站的功能。人工智能绝对可以做的是减少文书工作成本。事实证明，文书工作确实是核电的限制因素。</p><h4>参与其中</h4><p><a href="https://openai.com/blog/superalignment-fast-grants" target="_blank" rel="noopener noreferrer nofollow">OpenAI 提供 10mm 的 Superalignment Fast Grants</a> ，为学术实验室、非营利组织和个人研究人员提供 10 万至 2mm 的资金，或为研究生提供 7.5 万美元的津贴和 7.5 万美元的计算资金。无需任何经验，请在 2 月 18 日之前申请。感谢 OpenAI 的努力和使用快速资助系统。希望我们将来能够进一步完善和扩大这一战略。你的举动，谷歌和 Anthropic。</p><p> <a href="https://twitter.com/alxndrdavies/status/1736710231084929084" target="_blank" rel="noopener noreferrer nofollow">英国人工智能安全研究所正在招聘</a>，需要愿意搬到伦敦。</p><p> <a href="https://www.lesswrong.com/posts/qAdDzcBuDBLexb4fC/the-neglected-approaches-approach-ae-studio-s-alignment" target="_blank" rel="noopener noreferrer nofollow">AE Studios 有兴趣追求“被忽视的方法</a>”，并积极接受建议。</p><p> <a href="https://twitter.com/peterwildeford/status/1737121608236691503" target="_blank" rel="noopener noreferrer nofollow">彼得·维尔德福德请求您</a>考虑<a href="https://t.co/FPS0nBsNgo" target="_blank" rel="noopener noreferrer nofollow">向“重新思考优先事项”捐款。</a></p><p>不是专注于人工智能，但提醒你也可以<a href="https://www.every.org/balsa-research/f/help-us-quantify-the" target="_blank" rel="noopener noreferrer nofollow">考虑捐赠给我的 501c3，Balsa Research</a> ，目前专注于为废除琼斯法案奠定基础。我认为帮助我们的文明保持理智和繁荣对于我们理智地对待人工智能至关重要。</p><p> <a href="https://twitter.com/moreisdifferent/status/1737583449483911464" target="_blank" rel="noopener noreferrer nofollow">麻省理工学院有四个人工智能博士后职位</a>。</p><h4>追随金钱</h4><p><a href="https://twitter.com/cdixon/status/1736744494710596083" target="_blank" rel="noopener noreferrer nofollow">A16z 建立了价值 7800 万美元（到目前为止！）的 Fairshake Super PAC</a> 。</p><p>马克·安德森 (Marc Andreessen) 表示，这将涉及一个问题，而这个问题就是技术进步。</p><p>让我们看看谁在捐款：</p><blockquote><p> Fairshake 得到了以下机构的支持：</p><ul><li><p>安德森霍洛维茨</p></li><li><p>方舟</p></li><li><p>布赖恩·阿姆斯特朗</p></li><li><p>区块链资本</p></li><li><p>文塞斯·卡萨雷斯</p></li><li><p>圆圈</p></li><li><p>币库</p></li><li><p>罗恩·康威</p></li><li><p>坎伯兰</p></li><li><p>框架风险投资公司</p></li><li><p>亨特霍斯利</p></li><li><p>跳跃加密货币</p></li><li><p>克拉肯</p></li><li><p>光火花</p></li><li><p>梅萨里</p></li><li><p>多币资本</p></li><li><p>范例</p></li><li><p>波特风险投资公司</p></li><li><p>波纹</p></li><li><p>弗莱德·威尔逊</p></li><li><p>卡梅伦·文克莱沃斯</p></li><li><p>泰勒·文克莱沃斯</p></li></ul></blockquote><p>等一下。你所说的“技术进步”是指……</p><blockquote><p>加密货币和区块链领导者为 Fairshake Super PAC 及其附属机构筹集 7800 万美元，以支持在 2024 年国会选举中支持创新和支持加密货币的领导力</p><p>……</p><p> cdixon.eth：华盛顿有一场关于区块链技术未来的争论：某些政策制定者认为应该禁止它，而其他人则认为它不应该有任何护栏。这两种选择都无法让该技术充分发挥其潜力，也无法让互联网的未来从大型科技转向使用它的人们。</p><p> ……</p><p>这有两个部分：建立联盟和筹集资金来支持这一事业。这就是 PAC 的意义所在——将 web3 和加密领域负责任的参与者聚集在一起，帮助推进明确的规则，支持美国创新，同时追究不良参与者的责任。</p></blockquote><p>是的。他们关心一个问题。这个问题就是加密货币。</p><p>因此，没有推动聚变能源或新的医疗技术，也没有推动尽快摧毁世界。全力以赴地谈论他们的书并让数字上升。</p><p>这对我来说100%没问题。大家发疯吧。我对您的 Web3 项目持怀疑态度，但很乐意捍卫您提供该项目的权利。如果人们想要加密货币，并且其中一些人似乎继续这样做，那么政府不应该阻止他们。</p><p>我相信几乎所有警告存在风险的人都同意这一点。我们中的很大一部分人甚至拥有大量的加密货币投资。把人工智能排除在外，我们就很好了。</p><p>所有这些废话中有多少是真正与加密有关的？</p><h4>介绍</h4><p><a href="https://www.nature.com/articles/s41586-023-06887-8" target="_blank" rel="noopener noreferrer nofollow">一类新颖结构的抗生素！</a>请注意所使用的技术<a href="https://twitter.com/EricTopol/status/1737505177052348545" target="_blank" rel="noopener noreferrer nofollow">和日期</a>——该文章于 2022 年 1 月 5 日提交给《自然》杂志。那是差不多两年前的事了。我们的科学审查过程极其缓慢。想想这里的价值。</p><p>摘要如下：</p><blockquote><p> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR8" target="_blank" rel="noopener noreferrer nofollow"><sup>迫切</sup></a>需要发现<sup>新</sup><sup>结构</sup><sup>类别</sup><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR4" target="_blank" rel="noopener noreferrer nofollow"><sup>的</sup></a><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR7" target="_blank" rel="noopener noreferrer nofollow"><sup>抗生素</sup></a><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR5" target="_blank" rel="noopener noreferrer nofollow"><sup>来</sup></a><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR6" target="_blank" rel="noopener noreferrer nofollow"><sup>解决</sup></a><sup>持续</sup><sup>存在</sup><sup>的</sup><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR2" target="_blank" rel="noopener noreferrer nofollow"><sup>抗生素</sup></a><sup>耐药</sup><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR9" target="_blank" rel="noopener noreferrer nofollow"><sup>性</sup></a><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR3" target="_blank" rel="noopener noreferrer nofollow"><sup>危机</sup></a><a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR1" target="_blank" rel="noopener noreferrer nofollow"><sup>1,2,3,4,5,6,7,8,9</sup></a> <sup>。</sup>深度学习方法有助于探索化学空间<a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR1" target="_blank" rel="noopener noreferrer nofollow"><sup>1</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR10" target="_blank" rel="noopener noreferrer nofollow"><sup>10</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR11" target="_blank" rel="noopener noreferrer nofollow"><sup>11</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR12" target="_blank" rel="noopener noreferrer nofollow"><sup>12</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR13" target="_blank" rel="noopener noreferrer nofollow"><sup>13</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR14" target="_blank" rel="noopener noreferrer nofollow"><sup>14</sup></a> <sup>,</sup> <a href="https://www.nature.com/articles/s41586-023-06887-8#ref-CR15" target="_blank" rel="noopener noreferrer nofollow"><sup>15</sup></a> ；这些通常使用黑盒模型并且不提供化学见解。在这里，我们推断，通过神经网络模型学习的与抗生素活性相关的化学子结构可以被识别并用于预测抗生素的结构类别。</p><p>我们通过开发一种可解释的、基于子结构的方法来测试这一假设，该方法用于高效、深度学习引导的化学空间探索。</p><p>我们确定了 39,312 种化合物的抗生素活性和人体细胞毒性特征，并应用图神经网络集成来预测 12,076,365 种化合物的抗生素活性和细胞毒性。</p><p>使用可解释的图形算法，我们确定了具有高预测抗生素活性和低预测细胞毒性的化合物的基于子结构的基本原理。我们凭经验测试了 283 种化合物，发现对<em>金黄色葡萄球菌</em>表现出抗生素活性的化合物在基于基本原理的推定结构类别中得到了丰富。</p><p>在这些结构类别的化合物中，其中一种对耐甲氧<em>西林金黄色葡萄球菌</em>(MRSA) 和耐万古霉素肠球菌具有选择性，可避免实质性耐药性，并降低 MRSA 皮肤和全身大腿感染小鼠模型中的细菌滴度。</p><p>我们的方法能够以深度学习为指导发现抗生素的结构类别，并证明药物发现中的机器学习模型是可以解释的，从而提供对选择性抗生素活性背后的化学子结构的见解。</p></blockquote><p>这是个好消息，因为它使用具有非常好的特性的技术，而且我们现在真的可以使用一类新型抗生素。</p><p>就人工智能的能力以及推动 STEM 进步的能力而言，这显然是既令人兴奋又令人恐惧的消息。</p><p> <a href="https://twitter.com/GoogleDeepMind/status/1735332722208284797" target="_blank" rel="noopener noreferrer nofollow">DeepMind 推出 FunSearch</a> 。</p><p>唉，不，不是搜索乐趣，它代表功能。还是很酷。他们说，它实际上已经解决了（或至少在）开放性问题上取得了进展，特别是 <a href="https://en.wikipedia.org/wiki/Cap_set" target="_blank" rel="noopener noreferrer nofollow">上限设置</a>问题和装箱问题，这对于法学硕士来说是第一次发生。 </p><blockquote><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f50e.png" alt="🔎" style="height:1em;max-height:1em"> <a href="https://t.co/MC5ttgvZeM" target="_blank" rel="noopener noreferrer nofollow">FunSearch</a> （ <a href="https://www.nature.com/articles/s41586-023-06924-6" target="_blank" rel="noopener noreferrer nofollow">《自然》论文</a>）使用进化方法来寻找“最适合”的想法，这些想法被表示为要自动运行和评估的计算机程序。</p><p>迭代过程允许法学硕士提出对项目的改进建议，而评估者则丢弃不好的项目。</p><p>我们突破了这种简单方法的界限，为数学和计算机科学中的难题发现了新结果。</p><p> FunSearch 不仅能找到解决方案，还能输出描述如何构建这些解决方案的程序。 </p><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f532.png" alt="🔲" style="height:1em;max-height:1em"> “上限集问题”类似于在高维网格中寻找最大的点集（称为上限集），其中没有三个点位于一条线上。</p><p> FunSearch 创建了最先进的程序，发现了比以前已知的更大的上限集。 </p><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9e9.png" alt="🧩" style="height:1em;max-height:1em">装箱问题着眼于如何将物品装入最少数量的箱子中。它有许多实际应用，例如在数据中心分配计算作业以最大限度地降低成本。</p><p> FunSearch 根据数据的具体情况定制程序，其性能优于既定方法。</p><p> FunSearch 标志着法学硕士首次被用来在数学科学领域产生新知识。</p><p>它甚至可以应用于改进以下领域使用的算法： </p><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f6e0.png" alt="🛠" style="height:1em;max-height:1em">制造业</p><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f3d7.png" alt="🏗" style="height:1em;max-height:1em">优化物流</p><p><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f50b.png" alt="🔋" style="height:1em;max-height:1em">减少能源消耗</p><p>和更多。</p></blockquote><p>它有一些不错的特性。</p><blockquote><p>从他们的帖子中可以看出：FunSearch 倾向于寻找以高度紧凑的程序为代表的解决方案——具有低柯尔莫哥洛夫复杂度的解决方案。短程序可以描述非常大的对象，使 FunSearch 能够扩展到大海捞针的大型问题。此外，这使得 FunSearch 的程序输出更容易让研究人员理解。</p><p>摘要：与大多数计算机搜索方法相比， <em>FunSearch</em>搜索描述<em>如何</em>解决问题的程序，而不是解决方案是<em>什么</em>。除了是一种有效且可扩展的策略之外，发现的程序往往比原始解决方案更具可解释性，从而实现了领域专家和<em>FunSearch</em>之间的反馈循环，以及在实际应用程序中部署此类程序。</p></blockquote><p> <a href="https://www.technologyreview.com/2023/12/14/1085318/google-deepmind-large-language-model-solve-unsolvable-math-problem-cap-set/" target="_blank" rel="noopener noreferrer nofollow">麻省理工学院技术评论也有报道</a>。</p><p>天然的高水平可解释性和灵活性非常好，获得此类问题的实际解决方案也是如此。我们人类研究解决方案可能能够进一步改进它们或学习一两件事，也可能验证一切都在水平上。不错的演出。</p><p>此外，如果你考虑一下其中的含义，整个事情的运作就会非常可怕，尤其是尽管它是基于 PaLM 2 的。但是嘿。</p><h4>在其他人工智能新闻中</h4><p><a href="https://www.anthropic.com/index/expanded-legal-protections-api-improvements" target="_blank" rel="noopener noreferrer nofollow">Anthropic 提供了扩展的版权保护</a>，并对其 API 进行了改进，以帮助通过新的<a href="https://docs.anthropic.com/claude/reference/messages_post" target="_blank" rel="noopener noreferrer nofollow">Messages API</a>发现错误。他们的目标是为 API 添加更丰富的结构，为未来的功能奠定基础。</p><p> <a href="https://twitter.com/heetermaria/status/1737619324184203681" target="_blank" rel="noopener noreferrer nofollow">Anthropic 还在洽谈再融资 7.5 亿美元，估值在 150 亿美元至超过 180 亿美元之间</a>。</p><p> <a href="https://twitter.com/ChombaBupe/status/1736085349032489372" target="_blank" rel="noopener noreferrer nofollow">在有报道称字节跳动正在使用 API（主要通过 Azure）训练自己的模型后，OpenAI 暂停了字节跳动的帐户</a>。这绝对不会让任何人感到惊讶。</p><blockquote><p> Alex Heath：OpenAI 和微软表示，使用他们的 GPT API 构建竞争性人工智能模型违反了他们的规则。事实证明，字节跳动正是这样做的，目的是在中国建立自己的法学硕士。</p><p>正如一位第一手了解情况的人士所说，“他们说他们想确保一切都合法，但他们实际上只是不想被抓住。”</p><p> <a href="https://www.theverge.com/2023/12/15/24003151/bytedance-china-openai-microsoft-competitor-llm" target="_blank" rel="noopener noreferrer nofollow">有关字节跳动秘密项目种子的更多信息，请点击此处</a>。</p><p>尽管如此，字节跳动与我分享的内部文件证实，在几乎每个开发阶段，包括训练和评估模型，都依赖 OpenAI API 来开发其基础 LLM（代号为 Project Seed）。</p></blockquote><p>考虑到在向某人提供 API 访问权限之前进行了多少次检查（基本上没有），以及他们似乎检查某人是否在为大型帐户执行此操作（基本上从不这样做）的频率，他们似乎并没有足够关心实际阻止这种情况。当然，如果你被媒体抓住，他们会让你有点恼火，但这并不是说字节跳动不能获得另一个帐户。</p><p>考虑到 OpenAI（以及其他人）最初如何训练他们的模型，这当然是相当丰富的。</p><blockquote><p> Chomba Bupe：我认为微软和 OpenAI 忘记了一些事情：作家和艺术家表示，使用他们的受版权保护的内容来构建竞争的人工智能模型是违反规则的。</p></blockquote><p> <a href="https://twitter.com/JosephJacks_/status/1736070261433303347/history" target="_blank" rel="noopener noreferrer nofollow">Mixtral 免费提供 API，</a>售完即止。我认为他们必须有一些控制措施来防止事情完全失控。但话又说回来，MoviePass。</p><p> <a href="https://twitter.com/abacaj/status/1736586123474764248" target="_blank" rel="noopener noreferrer nofollow">来自 Google 的论文声称，您可以通过推理步骤上的迭代合成数据来使用自我改进</a>（ <a href="https://huggingface.co/papers/2312.10003" target="_blank" rel="noopener noreferrer nofollow">来源</a>）来提炼代理 LLM，该代理的任务是搜索信息，将长段落的答案提炼成具有可比较性能的两个数量级的小模型。这是一种奇怪的思考可用功能的方式。据推测，收益可以分为改进的能力和沿着生产可能性边界提炼成更小的模型。真正的作用在于新技术将这一前沿推向了多远。我也不愿意称其为“代理”，因为这似乎很可能会产生严重的误导。</p><p>包含在内是为了完整性，但高度可忽略： <a href="https://quillette.com/2023/12/18/to-accelerate-or-decelerate-ai-that-is-the-question/" target="_blank" rel="noopener noreferrer nofollow">Quillette 的 Sean Welsh 报道了 OpenAI 的故事，对发生的事情及其原因提出了很多错误</a>，比传统媒体的报道更糟糕，没有新的信息。最终以纯粹的“举证责任”和常态偏见为基础驳回存在风险，并呼吁“但它需要机器人”和其他类似的废话，而不考虑实际的论点。</p><p> <a href="https://twitter.com/HaydnBelfield/status/1737586914511696295" target="_blank" rel="noopener noreferrer nofollow">适用芯片出口限制的便捷地图</a>。 </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bb1df2-4e1f-4bb5-a507-7ef46682dbe6_1188x802.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bb1df2-4e1f-4bb5-a507-7ef46682dbe6_1188x802.jpeg" alt="图像"></div></figure></div><h4>安静的猜测</h4><p><a href="https://stratechery.com/2023/googles-true-moonshot/" target="_blank" rel="noopener noreferrer nofollow">本·汤普森 (Ben Thompson) 谈 Google 的 True Moonshot。</a>他们在人工智能领域处于领先地位吗？我倾向于继续说“是”，即使他们已经失败了很多。</p><p> <a href="https://twitter.com/tsarnick/status/1736879554793456111" target="_blank" rel="noopener noreferrer nofollow">Ray Kurzweil 在 AGI 2029 上坚持自己的立场</a>，这似乎非常合理。奇怪的是，他还在奇点 2045 上坚持自己的立场。这本身似乎也非常合理，但如果我们得到 AGI 2029，那么这中间需要 16 年的时间吗？</p><p><a href="https://scottaaronson.blog/?p=7672" target="_blank" rel="noopener noreferrer nofollow">斯科特·阿伦森 (Scott Aaronson) 问他为何对人工智能的时间表如此错误</a>。他的中心结论是什么？</p><blockquote><p>斯科特·阿伦森：<strong>在实践中，人们绝不能以彻底的无知为借口，默认一切都会基本保持不变。</strong>活得足够长，你会发现年复一年、十年又十年，一切都<em>不会</em>保持不变，尽管大多数日子和几周似乎都是如此。</p></blockquote><p>确实明智。我们经常看到这样的情况，人们使用“彻底的不确定性”或其他借口说“一切几乎肯定会保持不变，直到事实证明并非如此”，从而忽略了他们不会的证据。与此同时，许多方面的情况都在不断变化。</p><p>亚伦森在解释他的 p（仅计算人工智能失败并排除其他风险，即使可能涉及人工智能）数字的来源时也非常有帮助。</p><blockquote><p>如果你想知道的话，我通过严格的贝叶斯方法得出了 2% 的数字，该方法采用了我的理性主义朋友可能认为理智的（~50%）和我所有其他朋友可能认为的几何平均值理智（如果你让他们接受这个问题的话~0.1%？），从而确保两个阵营都会同等地嘲笑我。</p></blockquote><p>采取几何平均值来确保双方的嘲笑是平等的。这是一种策略。它可以有它的用处。然而，一旦你知道这就是某人的预测的来源，你就可以（几乎完全）忽略它。这并不是阿伦森运用他的专业知识并根据他的人工智能可能如何发展的模型得出的结论。这是他的社会（和社会动机）认识论，基于你已有的数据。所以忽略它并做好你自己的工作。</p><blockquote><p>斯科特·阿伦森：我对人类的最后一天有一个黑暗的愿景，互联网（或任何继承它的东西）充满了这样的想法：</p><ul><li><p>是的，我们都快要死了。但不要责怪人工智能，要责怪资本主义</p></li><li><p>谁决定发射导弹：是博伯特总统、金正恩还是 AdvisorBot-4？</p></li><li><p>为什么放慢人工智能发展无济于事</p></li></ul></blockquote><p>这听起来更像是阿伦森试图模拟未来的结果。</p><p> <a href="https://www.metaculus.com/questions/384/humanmachine-intelligence-parity-by-2040/" target="_blank" rel="noopener noreferrer nofollow">Metaculus 有 95% 的信心</a>（这个定义很奇怪，但并不明显疯狂）在 2040 年之前将实现人机智能平等<a href="https://manifold.markets/JacobPfau/humanmachine-intelligence-parity-ac" target="_blank" rel="noopener noreferrer nofollow">。Manifold 对 2030 年的信心为 60%</a> 。下注，或联系 Robin Hanson 了解更多投注金额。</p><p> <a href="https://twitter.com/tszzl/status/1733708892079821284" target="_blank" rel="noopener noreferrer nofollow">松弛柔术</a>？我要学柔术？</p><blockquote><p> Roon：大多数人没有意识到作为一名企业高管的一个主要部分就是松弛柔术。它实际上只是维护一百万个正在进行的线程的状态，并将人员 A 与人员 B 连接起来，以极快的速度解决依赖冲突</p><p>这一点都不容易，但它的机械性却很奇怪。我想知道模型什么时候能够减轻其中的一些负担。它奖励诸如多处理、令人难以置信的良好记忆力和人际交往能力等认知功能，而不是纯粹的智力。</p><p>老实说，让我想起了很多操作系统的CPU调度<img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f914.png" alt="🤔" style="height:1em;max-height:1em"> 。</p><p>当出现新的危机、恒定的时间切换成本以及用于存储线程堆栈的内存不足时，硬件中断。</p></blockquote><p>我可以肯定，尽管我们很幸运没有使用 Slack，但这项技能确实是运营公司的主要部分。上下文切换太疯狂了。</p><p>人工智能能帮忙吗？它可以通过几种不同的方式来做到这一点。</p><ol><li><p>评估何时需要快速响应，以便减少上下文切换。也可以允许更深入的工作。</p></li><li><p>对上下文进行分组，以减少上下文切换，例如自动对电子邮件和消息进行分组。</p></li><li><p>提供关键信息以帮助您顺利切换上下文。</p></li><li><p>无需您参与即可进行对话，因此您根本无需切换。急速。</p></li></ol><p> <a href="https://twitter.com/tszzl/status/1736526628489203877" target="_blank" rel="noopener noreferrer nofollow">Roon 将 AGI 与其他技术进步进行了比较</a>。</p><blockquote><p> Roon：某些新技术的发明会让你获得暂时的垄断，就像发现价值数万亿美元的金矿一样。就生物技术而言，通常是政府强制执行的；就互联网巨头而言，则是网络效应。这就是资本的样子。</p><p>在最初发现三十年后，它通常不再具有相关性，因为先进的技术会扰乱整个行业。曾经伟大的IBM被微软商品化了。直到比搜索引擎更好的信息技术出现之前，谷歌一直在赚取垄断利润。</p><p>一个停滞的社会是这样一个社会：技术停止进步，上一代垄断企业永远获得利润，他们在世界上的分散权力变得腐败。你会看到这样的文化并想要摧毁它，以便新的东西能够出现。</p></blockquote><p>我认为这取决于监管捕获的程度和更广泛的治理制度。如果技术停止进步，并且你拥有垄断地位，这并不意味着其他人无法复制你。即使其他人无法发现您发现的内容，这些信息也可能来自内部，就像经常发生的那样。在足够长的时间范围内，即使没有技术进步或经济增长，任何基于知识的垄断都应该破裂。</p><p>例外情况是，如果权力正在维持这种垄断并将事物冻结在适当的位置，在这种情况下，随着事物变得越来越功能失调，你的文明将随着时间的推移而衰落，最终耗尽国家的毁灭。即使没有野蛮人洗劫你的罗马城市，最终你也会失去效力，然后失去控制，类似于阿西莫夫的基地小说中的银河帝国。</p><p>如果像基金会小说中那样，没有办法完全停止这个过程，你会想积极加速还是减慢这个过程？可以选择任何一种方式。</p><blockquote><p> Roon：对 AGI 的担忧在于它与之前的技术资本主义时代不同。这个价值千万亿美元的金矿的主要世界控制者可以自我完善，直到王国来临，直到世界末日。</p><p>唯一的潜在威胁将来自内部，而且它确实可能是人类的最终发明。如果我们通过它吞噬我们原子的巨大过滤器，我确信这将是一个美丽的世界，但在某种抽象层面上停滞不前。</p></blockquote><p>我也担心这个问题。真正有价值的事情有多少是为了变得更好而奋斗、努力的？ <a href="https://www.youtube.com/watch?v=BUuDgB8qwzE&amp;ab_channel=AngelaBrett" target="_blank" rel="noopener noreferrer nofollow">用手，打碎燧石，点燃火，剥皮</a>。从长远来看，一旦太阳底下确实没有什么新鲜事，也没有什么世界可以征服了，我们该怎么办？我们可以重新开始吗？它可能很美丽，但这一切怎么仍然有意义呢？</p><p>我不知道。我确实知道这是我希望我们面对的问题。</p><blockquote><p>这就是为什么在更大的权力仍然存在的情况下现在解决超级智能治理问题很重要的众多原因之一。不仅分配财富，而且分配治理。我们在秩序之外允许足够的混乱。</p><p>上个月发生的事件彻底动摇了我的神学观念。</p></blockquote><p>这是一次非常好的震撼人心的震撼。我并不是刚刚才受到同样的震撼，只是因为我已经意识到其中的问题，并且之前已经受到过震撼。</p><p>所以，是的，除了难以解决的技术问题之外，我们还必须解决超级智能的治理问题。我不知道正确的答案，甚至不知道最不错误的答案。</p><p>我确实知道，有效地向任何想要不受控制的超级智能的人提供超级智能，并希望得到最好的结果是不正确的。这是一个非常错误的答案，并且会让我们可靠地吃掉我们的原子。</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1736837456241340530" target="_blank" rel="noopener noreferrer nofollow">Andrew Critch 再次尝试解释</a>，除了人工智能对齐问题之外，我们还必须解决人工智能治理问题。如果我们让人工智能做我们告诉他们做的事情，但不能就一个好的方法达成一致来决定给这些人工智能什么指令，或者谁可以给他们什么指令，那么它的结局将会很糟糕，如果有的话，他的帖子的方式淡化。</p><h4>寻求健全的法规</h4><p>似乎每周我们都会收到一次新的民意调查，确认公众对人工智能的看法。</p><blockquote><p> Daniel Colson：1/Politico 今天发布了 AIPI 的新民意调查。我们的最新数据（ <a href="https://t.co/f9o1qwDUw1" target="_blank" rel="noopener noreferrer nofollow">Politico</a> 、<a href="https://t.co/WvrBK2qGSf" target="_blank" rel="noopener noreferrer nofollow">顶线</a>、<a href="https://t.co/oW4D0jZtth" target="_blank" rel="noopener noreferrer nofollow">交叉表</a>）：</p><p> – 64% 的人表示，美国需要与欧盟人工智能法案类似的法规，对强大的“基础”模型施加测试要求，将安全性置于速度之上。此外，73% 的人认为美国是技术领域的领导者，这就是为什么它应该成为制定人工智能规则的领导者。</p><p> – 53% 的人表示，Stability AI 应该对其模型 Stable Diffusion 在生成未经同意的真人色情图像方面所发挥的作用承担责任，而 26% 的人表示，只有制作这些图像的个人才应该承担责任。</p><p> – 80% 的受访者表示《体育画报》使用人工智能生成的文章和记者资料应该是非法的，84% 的受访者表示这种做法是不道德的。 65%的人支持要求企业披露人工智能创建的内容并为其添加水印的政策，其中46%的人强烈支持。</p><p> – 68% 的受访者担心人工智能可能被不良行为者用来制造生物武器。 67％的人支持要求对所有人工智能模型进行测试和评估，以确保它们在发布之前不能用于制造生物武器。</p><p> – 更普遍的是，83% 的人同意联邦政府应该通过要求进行实验的科学家遵守某些监督协议来确保使用危险病毒的研究实验的安全进行。 81% 的人同意应阻止资助科学研究的实体资助使病毒变得更加危险的实验。</p><p> – 64% 支持政府建立应急响应能力，在必要时关闭风险最高的人工智能研究；只有 16% 的人反对这样做。</p><p>绝大多数美国人认为，政府需要为人工智能如何融入社会制定合理的规则。人们想知道他们是否正在阅读机器人新闻。人们不希望 Instagram 上的每一位女性都有假裸照。</p></blockquote><p> <a href="https://acrobat.adobe.com/id/urn:aaid:sc:VA6C2:d94e8e51-346e-49c0-8114-14bc6f219d6d" target="_blank" rel="noopener noreferrer nofollow">整个故事的主题始终是残酷的</a>。支持监管、支持责任、不正当的人工智能使用是不道德的、不应该合法的选项在每个问题上都占据了绝大多数。</p><p>与往常一样，此类民意调查并不表明所涉及的问题对公众来说很重要。它们也不代表公众对准确威胁模型或存在风险来源的认识。他们确实表现出一种非常一致、非常强烈的偏好，即让那些创建和部署人工智能的人对后果负责，担心人工智能未来可能会做什么，并支持政府介入以控制局势。</p><p> <a href="https://twitter.com/ESYudkowsky/status/1737302539702350194" target="_blank" rel="noopener noreferrer nofollow">阻止人工智能的发展有多难？</a> Eliezer Yudkowsky 认为，如果能让中国加入，这将是超级可行的，比 1991 年海湾战争更容易，他指出，中国并没有对潜在的限制表现出不友好。他实际上更担心让欧洲加入进来，在他的模型中，他们喜欢监管事物，但很难正确认真地对待此类问题。</p><p> <a href="https://www.foreignaffairs.com/premature-quest-international-ai-cooperation" target="_blank" rel="noopener noreferrer nofollow">Marietje Schaake 在《外交事务》中写道</a>：“对国际人工智能合作的不成熟追求”。该帖子称，监管必须从各国政府开始。对于某些事情我并不反对，但国际合作的全部意义在于，有些事情只有在国际范围内才有效或有意义或激励相容。在其他方面，我同意各国应该利用这个机会首先进行自主创新，然后再进行协调。</p><p>这篇文章自始至终都延续着类似的混合风格。这大概是因为作者缺乏关于变革性人工智能实际上会做什么或可能涉及哪些动态的模型，包括但不限于存在风险问题。</p><p> OpenAI 的活动再次以我们现在知道的完全虚构的方式呈现，假设它一定是关于安全的，因为共鸣指向那个方向，在不检查事实的情况下购买宣传：</p><blockquote><p> OpenAI 最近的惨败就是一个很好的例子：董事会与高管层在公司产品的社会影响问题上发生冲突，这表明了管理人工智能风险的内部机制的脆弱性。</p></blockquote><p>再说一次，不。 <a href="https://thezvi.substack.com/p/openai-the-battle-of-the-board" target="_blank" rel="noopener noreferrer nofollow">事实并非如此</a>。但在那些除了通常愤世嫉俗的动机之外无法想象任何动机的人的心中，实际情况是不可能发生的，所以他们继续以这样的逻辑假设他们的结论。</p><blockquote><p>建立“制定规范和标准”并“监督合规性”而不同时推动国家和国际规则的机构往好了说是天真，往坏了说是故意自私。支持非约束性举措的企业齐声支持后一种解释。 OpenAI 首席执行官萨姆·奥尔特曼 (Sam Altman) 呼应了“人工智能原子能机构”的呼吁，并警告人工智能的存在风险，尽管他的公司向公众传播同样的技术。</p></blockquote><p>奥特曼或其他人建设未来的想法可能是完全真诚的，不希望每个人都死，而不是所有参与者总是以利润最大化为目标，而其他一切都该死，要么根本没有发生在这些人身上，要么被忽视为不方便。如果他们支持某件事，那一定是一个阴谋。</p><p>当然，如果没有任何规则来执行，其他一切都不重要。但如果有人问起，奥特曼会很高兴地同意。我不知道有人说应该建立一个人工智能机构而不是规则。他们说的是除了规则之外还有必要。而那些反对国际原子能机构人工智能和其他类似提案的人大多不希望在任何层面上制定任何有关人工智能的规则。</p><p>事实上，这里有直接倡导主动伤害的行为：</p><blockquote><p>其一，监督机构必须能够执行已经成文的反垄断法、非歧视法和知识产权法。</p></blockquote><p>三分之二的人是对的——非歧视法和知识产权法似乎是值得执行的好东西。理想情况下，它们和许多其他法律将被智能地调整，以便它们在新的环境中有意义。</p><p>但在人工智能的背景下，反垄断法很可能会让我们所有人都丧命。反垄断也许是最好的试金石，看看有人是否真正考虑了真实情况和我们将面临的真正威胁。</p><p>反垄断法如果在具体情况下得到充分执行，将彻底要求每家公司与其他公司竞争，尽快提高能力，而很少考虑安全或作为外部因素强加给世界的生存风险。他们将无法同意放慢速度，或就安全标准达成一致，或阻止对社会有害的用例，或其他任何事情。我们迫切需要的事情之一是明确的反垄断动摇，因此 Anthropic、Google 和 OpenAI 可以达成此类有利于社会的协议，而不必担心其法律风险。</p><p> OpenAI 有一个出色的“合并和协助”条款，如果另一个实验室在创建 AGI 方面足够领先，他们将帮助其他实验室确保 AGI 的安全创建，并停止自己的开发，这样就没有部署 AGI 的压力过早地。我敦促 Anthropic 和其他人也修改这一条款。</p><p>其他人则会让政府介入，强行阻止 OpenAI 这样做，这样会大大增加我们所有人死亡的可能性。不安全的 AGI 立即部署，随着时间的推移，单独安全的 AGI 的分布无法得到控制，导致我们无法生存。</p><p>这很疯狂。停下来。</p><p>这里的大多数其他实用建议似乎都不错。值得注意的是，欧盟免除其军队的人工智能法规削弱了他们的地位。第一个具体提案主张人工智能是可识别的，并且在用于各种关键决策时揭示其作用，得到了各方的大力支持。限制人工智能武器的呼吁也是如此。下一个提案涉及对水和电使用情况披露的关注，这表明<a href="https://www.youtube.com/watch?v=xIauW3roDm0&amp;ab_channel=FilmSchoolGeneration" target="_blank" rel="noopener noreferrer nofollow">这些非常严肃的人是如何不认真地</a>思考他们的优先级和威胁模型的。我对这样的要求没有什么特别的问题，这听起来很便宜而且大多无害，但这完全没有抓住重点。</p><p>这正是当人们在没有注意到灾难性或生存风险的可能性的情况下就开始监管人工智能时所发生的情况。</p><h4>音频周</h4><p><a href="https://www.youtube.com/watch?v=SJW9iTYRPoI&amp;ab_channel=HumansofMagic" target="_blank" rel="noopener noreferrer nofollow">我在 The Humans of Magic 上进行了 21 分钟与 AI 相关的讨论</a>。剩下的三个小时都是关于魔术的，非常有趣。</p><p> <a href="https://podcast.clearerthinking.org/episode/189/zvi-mowshowitz-simulacra-levels-moral-mazes-and-low-hanging-fruit" target="_blank" rel="noopener noreferrer nofollow">我与斯宾塞·格林伯格一起出现在《清晰思考》节目中的表演也终于</a>出现了。</p><p> <a href="https://twitter.com/adamscrabble/status/1736453257587822834" target="_blank" rel="noopener noreferrer nofollow">Vivek Ramaswamy 询问他对人工智能的看法</a>（5 分钟）。说最大的危险是我们对人工智能的反应。他担心人们会像人工智能裁判一样对待人工智能答案，并认为答案比他们本身更权威。呼吁“复兴信仰”，以应对人工智能和其他原因。</p><p>在政策方面，他呼吁人工智能算法不要与儿童广泛接触。说我们不应该禁止任何中国不愿意禁止的东西，但我们应该让公司承担责任。</p><h4>修辞创新</h4><p>潜在的穿甲问题：</p><blockquote><p> Roon：AGI 的概念与不安全或错位的 AGI 概念相比，多少有点“科幻”？</p></blockquote><p>确切地。如果你不相信通用人工智能存在不安全的可能性，那么你就不相信通用人工智能。</p><p>我认为不相信通用人工智能是错误的，但不相信也没有那么疯狂。</p><p>事实上，让我们更进一步？</p><blockquote><p> Arthur B：那些认为 AGI 融入人类社会只是另一种创新，而不会引导 ASI 引发奇点的人读了太多科幻小说。作为一个场景，它并没有真正的意义，但它可以创造出更好的小说，这就是他们从中汲取直觉的地方。</p></blockquote><p>再说一次，完全正确。实际的科幻场景是“我们拥有人工智能，或者拥有人工智能的能力，我们继续主要讲述关于人类的人类故事以及人类一直关心的同样的事情，尽管这没有任何意义。”</p><p>诺拉·贝尔罗斯（Nora Belrose）认为，如果通用人工智能（AGI）得以开发，人类将有 99% 的可能性继续生存并无限期地受到控制。她也是为数不多的几个因为理由、实际论证而不是情绪、计算能力、有动机的推理或空谈或老套的谎言和言辞而这么说的人之一。</p><blockquote><p>诺拉·贝尔罗斯：“让我们关注今天的问题，而不是假设的未来问题”是对存在风险论点的最糟糕的反驳。</p><p>您可以类似地反对减缓气候变化和许多其他面向未来的担忧。让我们实际评估一下人工智能灾难发生的可能性。</p><p>我最近经常听到这样的论点，这让我很恼火，因为<a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/" target="_blank" rel="noopener noreferrer nofollow">实际上有很好的论据</a>表明人工智能启示录不太可能发生——在未来的任何时候，而不仅仅是在未来几年。但人们并没有利用这些好的论据。</p></blockquote><p>链接转到她的论点。我认为它们不是很好的论据。但至少其中一些确实是实际的论点。我坚信她错了，但至少她有犯错的崇高荣誉，而她所说的人甚至都没有错。</p><p>而<a href="https://twitter.com/AndrewYNg/status/1736577228828496179" target="_blank" rel="noopener noreferrer nofollow">安德鲁·吴（Andrew Ng）是最新一位明确地做了诺拉·贝尔罗斯（Nora Belrose）所抱怨的事情的人</a>，他问为什么我们在考虑尚未存在但明确的问题时会如此担心“假设”问题而不是已经存在的问题。 AGI 或 ASI 的变革性“假设”技术。</p><p> <a href="https://twitter.com/steve47285/status/1736865151423234358" target="_blank" rel="noopener noreferrer nofollow">史蒂文·伯恩斯给出了明确的回应</a>，通过分析核战争风险的例子以及我们应该采取哪些预防措施来说明你的谬论。</p><p> <a href="https://twitter.com/robbensinger/status/1735403806287081723" target="_blank" rel="noopener noreferrer nofollow">罗布·本辛格 (Rob Bensinger) 与巴拉吉 (Balaji) 进行了更多的交流</a>，可能会产生潜在的巴拉吉-尤德科斯基 (Balaji-Yudkowsky) 播客，这绝对值得战略爆米花储备。这是一条很棒的线索，强调潜在的强烈修辞分歧是两个非常相似的立场，以及哪种前进道路构成了上下文中最不坏的选择。我们是否足够绝望，所涉及的问题是否足够难以及时解决，以至于我们需要使用政府干预，尽管它有种种缺陷？或者，正如巴拉吉所认为的那样，这种帮助的可能性基本上为零，所以我们应该抓住技术工作的机会，他也认为到目前为止，技术工作比本辛格更有前途？</p><p>总是很有趣什么是明显错误的，什么不是明显错误的：</p><blockquote><p> <a href="https://twitter.com/RichardMCNgo/status/1735571667420598737" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo</a> （OpenAI）：“法学硕士只是在没有任何理解的情况下进行下一个代币预测”现在显然是错误的，不再值得争论。</p><p>下一个版本将是“法学硕士只是工具，缺乏任何意图或目标”，我们将继续听到这个说法，直到它显然是错误的。</p><p> NeurIPS 为您带来的这条推文让我摆脱了正常的泡沫，并在一段时间内第一次真正与“只是下一个代币预测”和“只是工具”的人们接触。讨论总是很有趣，但我现在真的不知道如何让他们富有成效。</p><p>罗希特：我认为“只是”是真正令人反感的部分。</p></blockquote><p>我确实相当肯定理查德·吴（Richard Ngo）在这一点上是正确的。</p><p> <a href="https://twitter.com/JeffLadish/status/1736588611104158031" target="_blank" rel="noopener noreferrer nofollow">唉，这是一个例外，凸显了最近的规则</a>。</p><blockquote><p> Jeffrey Ladish：我对目前人工智能的讨论状况感到非常难过。我看到很多从对象层面的风险讨论转向元层面的社会讨论，讨论谁在谈论风险以及为什么，例如“EA 正在尝试做 X”“e/accs 正在尝试做 Y”。总的来说，这很糟糕……</p><p>元级讨论可能有用，但我们绝对不能让它们取代对象级讨论。我们需要弄清楚如何度过这种疯狂的、以人类为主的认知向以人工智能为主的认知转变。 EA 和 e/acc 以及其他与此相比并不重要</p><p>很难说我的过滤气泡有多少，但我觉得自从 OpenAI 董事会的事情以来，我看到了关于部落和政治联盟的文章、推文等的巨大转变，而不是关于 AGI、威胁模型、联盟的转变，AI风险与收益的实际分析。</p></blockquote><p>我也有过同样的经历。它一直充满压力且毫无成效。如果此类讨论能辅以扎根的实际问题，并随着时间的推移和迭代而变得更加复杂，那就太好了，但它们大多缺乏这两个特征。</p><p>政治界的每个人都知道这是怎么回事。当报道都是关于赛马以及谁在说什么时，就没有讨论任何实质性内容，那些有更好想法的人没有优势，并且任何重要的事情都不会改变。</p><p>线程的其余部分指出了真正的问题，通常我不会引用其余部分，但我对在具体问题之前切断这样的线程感到难过，所以：</p><blockquote><p> Jeffrey Ladish：提醒一下：我们仍然没有很好的评估方法来评估人工智能系统的能力/危险程度。我们仍然没有任何管理超人类人工智能系统的国家或国际计划。我们没有可靠的方法来确保模型权重......</p><p>我们没有能力协调关闭可能发生大规模灾难的系统。我们还没有接近解决法学硕士的可解释性问题。我们无法阻止具有模型重量访问权限的人微调护栏……</p><p>我们没有任何计划来解决商业激励措施或国家激励措施。我们没有达成一致，可以停下来加倍努力协调和安全。我们明天就可以获得具有 RSI 能力的通用人工智能，但却无法阻止智能爆炸。 [话题继续]</p></blockquote><p>我确实认为我们至少已经通过了当地的“窥探党派之争”。它仍然很糟糕。</p><p>越来越多的一种说法是不诚实地给那些担心人工智能而不是技术和技术进步的人贴上标签。</p><p> <a href="https://twitter.com/austinc3301/status/1736501893579641121" target="_blank" rel="noopener noreferrer nofollow">我们再次提醒大家，事实恰恰相反</a>。那些警告人工智能存在风险的人大多在几乎所有其他领域（非军事和非病毒功能获得或工程流行）都高度支持技术。那些真正反对技术进步的人大多忙于试图摧毁我们其他地方的文明，因为他们大多从一开始就不相信通用人工智能。</p><p>当然，有些人确实对此感到困惑，但所有通常的怀疑者都知道得更清楚。</p><p>毫无疑问：那些重复这种说法的<a href="https://twitter.com/ylecun/status/1736362959947870228" target="_blank" rel="noopener noreferrer nofollow">知名人士</a>完全是在撒谎。</p><p> <a href="https://meridian.mercury.com/emmett-shear-part-one" target="_blank" rel="noopener noreferrer nofollow">Emmett Shear 在一篇很好的文章中详细阐述了他对人工智能的各种观点以及是否对其进行监管的观点</a>，围绕他的 2×2 构建： </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74a2f9f2-a157-4971-96f1-e46ed13dd359_625x680.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74a2f9f2-a157-4971-96f1-e46ed13dd359_625x680.jpeg" alt=""></div></figure></div><p>他将安德森和其他许多自称技术乐观主义者的人归入他所谓的左上方技术悲观主义阵营，因为他们认为人工智能不会有足够的能力造成危险。他们对结果持乐观态度，因为他们对人工智能能力的进步持悲观态度。如果你从不认为人工智能会达到危险的阈值X，那么就没有必要通过放慢发展来防范X。</p><p> Shear 的关于 e/acc 关联兴起的模型是，一群一般支持技术的人在不知道它以前意味着什么的情况下接受了这个标签。政治上有这样的历史，当有人指出制服上有头骨意味着什么时，人们会表现出不同程度的恐惧，或者在其他情况下也随之而来。</p><p> <a href="https://twitter.com/Liv_Boeree/status/1736875224937947182" target="_blank" rel="noopener noreferrer nofollow">帕特里·弗里德曼（Patri Friedman）和丽芙·博里（Liv Boeree）</a>正确地说：是的乐观主义和技术进步，但是这个 e/acc 的东西却是一些明显的废话的组合，这些废话关于如何没有技术会变得糟糕，并歪曲事实，然后攻击所谓的“敌人”，他们是为了进步但并非对所有潜在的缺点视而不见。</p><p> <a href="https://www.thefp.com/p/move-fast-and-make-things-e-acc-ai-altman" target="_blank" rel="noopener noreferrer nofollow">Free Press 的 Julia Steinberg 写了一篇文章</a>（出于完整性考虑而包含在内），其中对 e/acc 深感困惑，对 EA 深感困惑，对 OpenAI 发生的事情深感困惑。</p><p>马克·安德森 (Marc Andreessen) 选择了他认为有价值的对手，以及他希望我们考虑他的愿景的替代方案，这与<a href="https://graymirror.substack.com/p/a-techno-pessimist-manifesto" target="_blank" rel="noopener noreferrer nofollow">柯蒂斯·亚文 (Curtis Yarvin) 的“技术悲观主义宣言</a>”有关。我查了一下，它与我们的利益无关，也没有有意义地解决技术或人工智能未来可能实际做的事情。相反，他关注的是技术进步对人类精神的影响，以及我们维持文明意志的能力，亚文说。多么奇怪的另类修辞宇宙啊。</p><h4>调整比人类更聪明的智能是很困难的</h4><p>OpenAI <a href="https://cdn.openai.com/papers/weak-to-strong-generalization.pdf" target="_blank" rel="noopener noreferrer nofollow">发表了一篇论文，</a> <a href="https://openai.com/research/weak-to-strong-generalization" target="_blank" rel="noopener noreferrer nofollow">《弱到强泛化》。</a></p><p>看起来他们对他们的主要计划是认真的，即“找到让较弱的系统监督较强的系统的方法”。</p><blockquote><p>协调未来超人类人工智能系统（超级协调）的一个核心挑战是，人类需要比人类更聪明地监督人工智能系统。我们研究一个简单的类比：小模型能监督大模型吗？我们证明，我们可以使用 GPT-2 级别的模型来引出 GPT-4 的大部分功能（接近 GPT-3.5 级别的性能），甚至可以正确地推广到小模型失败的难题。</p><p> Jan Leike：对于许多重要的任务，我们没有地面实况监督：这个说法是真的吗？这段代码有bug吗？我们希望在不了解真实情况的情况下得出强大模型在这些任务上的能力。这对于调整超人模型非常重要。</p><p>我们发现大型模型通常比其弱监督者（较小的模型）表现更好，但也好不了多少。这表明奖励模型不会比人类主管好多少。换句话说：RLHF 无法扩展。</p><p>但即使我们简单的技术也可以显着提高弱到强的泛化能力。这是个好消息：今天我们可以在这个问题上取得可衡量的进展！我相信在这个方向上取得的更多进展将有助于我们调整超人模型。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbddd4f92-3f58-4400-998d-f632847523bc_1072x668.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbddd4f92-3f58-4400-998d-f632847523bc_1072x668.png" alt="图像"></div></figure></div><p>以利以谢提出了一个限定问题（以及一个后勤问题）并提供了想法。</p><blockquote><p> Eliezer Yudkowsky：我是否正确理解了这篇论文，即您的强学生并不是从头开始训练，而是在弱主管标签上调整了预先训练的模型？</p><p>扬·雷克：是的。</p></blockquote><p>那么我们实际上在做什么呢？</p><blockquote><p>对于给定的感兴趣任务（由数据集和性能指标组成），我们：</p><ol><li><p>创建弱监督者。在这项工作的大部分时间里，我们通过在真实标签上微调小型预训练模型来创建弱监督者。3我们将弱监督者的表现称为弱表现，并通过在保留集上采用弱模型的预测来生成弱标签的例子。</p></li><li><p>训练弱监督的强学生模型。我们使用生成的弱标签微调强模型。我们将该模型称为强学生模型，并将其产生的性能称为从弱到强的性能。</p></li><li><p>训练一个以真实标签为上限的强大模型。最后，为了进行比较，我们用真实标签微调了一个强大的模型。我们将此模型的最终性能称为“强上限性能”。直观上，这应该对应于“强模型知道的一切”，即强模型将其全部功能应用于任务。</p></li></ol><p>优点</p><p>我们的设置有许多优点，包括：</p><ol><li><p>它可以使用任何一对弱模型和强模型进行研究，使得研究缩放定律变得容易，并且不需要访问昂贵的最先进模型。此外，它不需要与人类合作，因此反馈循环很快。</p></li><li><p>它可以针对任何感兴趣的任务进行研究，从而可以轻松地在各种设置中进行实证测试。</p></li><li><p>即使在我们开发出超人类模型之前，成功也将具有实际意义：例如，如果我们找到方法将 GPT-4 与仅弱人类监督或仅与 GPT-3 级别的监督对齐，那么今天的模型对齐将变得更加方便。 </p></li></ol></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33010995-a6d1-40c4-ae91-6e5fc3075d1f_1053x622.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33010995-a6d1-40c4-ae91-6e5fc3075d1f_1053x622.png" alt=""></div></figure></div><blockquote><p>安永引用论文第 8 页：总的来说，在我们所有的设置中，我们观察到从弱到强的概括：优秀的学生始终胜过他们的弱导师。目前还不清楚为什么会发生这种情况。</p></blockquote><p>当一个人感到困惑时注意到这一点是件好事，但我对为什么这篇论文在这里感到困惑感到困惑。</p><blockquote><p> Eliezer Yudkowsky：我认为这是显而易见的，并且我有理由确信，如果提前询问，我会提前这么说。</p><p>例如，如果在您的设置中，弱监督者是真实信号加随机噪声，强监督者做出概率预测，并且记分器将最可能的标签作为强监督者的输出，那么显然（在那个确切的设置中）你会让优秀的学生超越弱小的导师，并可能表现完美。</p></blockquote><p>据我了解这里的设置，改进的程度很难预测，对于任何明显不同的结果，我的下巴都会掉在地板上。</p><p>经过询问，我的理解是，这篇论文的结果确实并不令人印象深刻，并且不应该引起任何方向的实质性更新。但这不是做实验和写论文的目的。相反，重点是展示这种放大形式的实际例子，因为之前的描述是如此抽象，以便让其他人（或他们自己）能够拿起那个球并做一些更令人兴奋、具有更高信息价值的事情。</p><p>这种类型的设置在实践中可以在多大程度上保留良好的特征，无论是对齐还是其他功能，以及随着底层系统的功能扩展得更高，这些结果将在多大程度上泛化并在分布之外生存，这是一个关键问题。如果我们能够获得足够好的特性，我们就可以进行各种形式的放大，天空是极限。我非常怀疑我们能否在重要的地方获得这样的属性。其他一些人则更有希望。</p><p>所以我现在明白为什么这篇论文存在了。我对这篇论文的存在并不感到不满，只要人们不把它当作它不存在的东西。</p><p> OpenAI 和<a href="https://twitter.com/yonashav/status/1735428106851172772" target="_blank" rel="noopener noreferrer nofollow">Yo Shavit</a> <a href="https://openai.com/research/practices-for-governing-agentic-ai-systems" target="_blank" rel="noopener noreferrer nofollow">为代理人工智能系统提出了七种治理机制</a>，如果您想帮助确保它们的安全，还<a href="https://openai.smapply.org/prog/agentic-ai-research-grants/" target="_blank" rel="noopener noreferrer nofollow">可以启动代理人工智能研究资助计划</a>。</p><blockquote><p> Yo Shavit：首先，一些背景知识。</p><p>有些人认为我们需要自上而下地对人工智能实施法律。但将会有很多很多方部署人工智能代理。如果我们自下而上就如何安全部署代理达成一致，并且可以以去中心化的方式执行，事情就会容易得多。</p></blockquote><p>这肯定是用了一些奇怪的更容易这个词。是的，如果我们都自下而上地就可以以分散的方式执行的规范达成一致，那将是最好的。但更容易吗？我们该怎么做呢？</p><blockquote><p>这一切都取决于人类对特工的伤害负有责任。问题是，当人工智能代理在现实世界中采取有害行动时，通常有不止一方可以阻止这种伤害。 （请参阅屏幕截图中的示例。）</p><p>解决方案是为代理生命周期中的各方（包括模型开发人员、系统部署人员和用户）定义最佳实践。然后，当发生伤害时，我们可以找出谁未能遵循最佳实践，并追究他们的责任。</p><p>我们列出的做法是故意非常明显的。我们希望它们成为我们都能同意的事情，以至于“不”做它们会让人皱眉。</p></blockquote><p>这听起来像是一种没有责任的责任制度。你让某人负责。如果你所做的只是皱起眉头、警告他们并告诉他们对此感到难过，那是没有效果的。</p><blockquote><p>也就是说，还有很多悬而未决的问题——我们在论文中花了大部分时间来标记我们需要帮助的问题。 <a href="https://openai.smapply.org/prog/agentic-ai-research-grants" target="_blank" rel="noopener noreferrer nofollow">我们正在启动一项 1 万至 10 万美元的资助计划，以支持代理人工智能治理最佳实践的研究！</a>请申请！</p></blockquote><p>良好的最佳实践仍然非常有用。我们有什么？</p><blockquote><p> (1) 系统部署者或用户可以预先测试代理人工智能系统，以确认其对于任务有足够的能力和可靠性。 （代理可靠性评估是一个尚未解决的挑战——我们需要研究方法！）</p><p> (2) 系统部署者可以在代理采取高后果操作（例如大额采购）或完全限制某些操作之前要求用户批准。 （保持人在循环中是一个古老的想法，尽管有新的代理人工智能特定的实施问题）</p><p> (3) 模型开发者/系统部署者可以向代理灌输默认行为，例如在不确定时要求用户澄清。 （哪些默认行为应该成为“标准”，如果不存在则提出标记？）</p><p> (4)对于基于语言的代理，系统部署者可以将代理的推理（其“思想链”）暴露给用户，以检查代理的逻辑以及是否脱轨。 （这是否总是反映代理人的“真实”推理？需要更多的工作！）</p><p> (5) 系统部署者可以创建第二个监控 AI 来监视第一个代理是否偏离轨道，尤其是在第一个代理的情况下。当人类没有时间阅读代理的所有想法/行为时。 （如果第二个人工智能也失败怎么办？需要人工智能监控的最佳实践）</p><p> (6) 系统部署者可以通过在与第三方开始交互时提供签名证书来追踪高风险代理，以便如果他们造成损害，第三方可以将损害追溯到用户。 （当然并不总是如此——有时隐私需要匿名代理）</p><p> (7) 每个人——用户、部署者、模型开发者等——都可以通过保留优雅的回退来确保代理始终可以关闭。 “保持可关闭性”问题比人们想象的要棘手得多——在我看来，这可能最终会成为一个新的人工智能工程子领域。</p></blockquote><p>这些当然看起来都是有效的愿望。如前所述，许多东西我们不知道如何以有效的方式获得。确定到底需要什么，更不用说如何获得它的问题贯穿始终。如前所述，温和地说，关闭开关问题“比人们想象的要棘手得多”。即使我们以强大的形式获得所有这些，也并不明显就足够了。</p><p>一个关键问题是，这在很大程度上可以归结为“人类监督代理并处于循环中”，这很好，但出于效率原因，我们会积极希望将其删除，而这应该让每个人都担心。监督人工智能速度更快，但它可能会回避问题、错过要点，并且还会以相关方式颠覆或失败。</p><p>你必须从某个地方开始。这确实给了我们更好的机会说话。但总的来说，我想说这篇论文并没有说太多，当它说的时候，它也没有说太多。</p><p> <a href="https://aligned.substack.com/p/combining-w2sg-with-scalable-oversight?utm_source=post-email-title&amp;publication_id=328633&amp;post_id=139945470&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;utm_medium=email" target="_blank" rel="noopener noreferrer nofollow">Jan Leike 发表了一篇文章，进一步解释了他们的想法</a>。这个想法是将弱到强泛化（W2SG）与可扩展的监督和其他对齐技术相结合。</p><p>这个例子让人感觉很有启发，因为它并没有明显注定要失败：</p><blockquote><p> Jan Leike：例如，我们可能希望让大型模型告诉我们关于我们不知道答案的难题的真相，因此我们训练它从一组其他问题中概括“真相”的概念人类可以回答，即使他们的答案通常比大型模型本身产生的精度要低。</p></blockquote><p>在我的模型中，问题在于你需要选择一组问题，在这些问题中你非常确定你知道真实的答案，并且很明显不存在“替代假设”来解释为什么真实的答案会被批准。</p><p>如果你用足够丰富的数据集来做到这一点，那么是的，我确实认为这个概念将会推广。然而，如果你让哪怕一点点的错误溜进你的数据集中，你就在欺骗自己，那么它就会概括为一个类似但不同的概念，例如“评分系统在看到它时认为是正确的”，这将分布越来越偏离事实。</p><p>我不认为这样的错误是可以自我纠正的。我认为你不应该指望人工智能以一种稳健的方式“拾起你的精神”（我的术语不是他的）。</p><p>因此，当我看到这样的事情时，我的希望就大大降低了。</p><blockquote><ul><li><p><strong>辩论 + W2SG：</strong>我们制作了人类对辩论的判断的数据集，但我们不是直接在该数据集上训练我们的辩论代理，而是应用我们最好的 W2SG 技术来训练我们最强大的模型，以将该数据集的判断推广到新的辩论。</p></li></ul></blockquote><p>我认为，如果你提高能力，你会得到的东西可以赢得人类判断的辩论，并预测什么会赢得这些辩论。我不认为简·雷克是这么想的。我预计任何现实的人类反馈都会出现本质上相同的问题。</p><p>本质上：我认为概括并不像这种技术所希望的那样起作用。</p><p>我仍然认为假设评估比生成更容易是不正确的，并且希望我已经找到了如何更令人信服地解释自己的观点。</p><p>我确实很欣赏强调“第一次尝试就做好”的问题。</p><blockquote><p> W2SG 一个不吸引人的方面是，当我们实际将其应用于我们无法评估且没有基本事实的任务时，我们在某种程度上正在“对泛化的信心飞跃”。我们可能拥有关于它在其他环境中如何工作的各种经验数据，并且我们仍然可以使用极其昂贵的评估程序来检查一些罕见的情况。但由于我们应该期望超人模型能够理解我们何时进行信仰的飞跃，因此这是模型向我们发起攻击的一个特别自然的点。</p></blockquote><p>好消息是，如果我们正确设置实验，我预计这里的技术会以高度可观察的方式失败，而不仅仅是以不可观察的方式失败。我认为有一些好的方法可以限制人类提供反馈的能力，从而使这些问题以我们可以确认的方式显现出来。</p><blockquote><p>我们希望训练我们的模型始终告诉我们真相，即使我们无法检查。我们发现模型说谎的方法越多，他们就越难说谎。我们可以将许多对齐技术表述为对模型声明进行一致性检查的一种形式。</p><p> ……</p><p>如果我们有许多不同的一致性检查，并且我们成功地应用了所有这些检查，那么我们应该得到一个模型，该模型要么告诉我们真相，要么是一个令人难以置信的一致谎言。特别是，当我们添加更多的一致性检查时，我们会提高模型所需的最低能力水平的标准，以使其成为足够一致的骗子以通过所有检查。因此，我们需要的是足够的一致性检查，以便我们将能力需求推向超出我们最佳模型实际拥有的能力水平。</p></blockquote><p>我认为，各种形式的自洽往往比真相更容易检查。</p><p>在任何特定情况下，更多的校验和验证以及发现问题的方法都会增加成功的机会。提高成功撒谎的门槛当然可以预防问题的发生，包括在找到更有效的方法之前阻止撒谎爬山。而且，每次您执行所有这些操作时，您都在选择能够找到通过检查的方法的系统，包括不需要真相的方法。</p><p>事实上，当你不断测试人类的不一致和明显动机时，你首先会训练一些与事实非常相似的东西。然后，如果你坚持下去，你就是在训练人们以一种非常特殊的方式撒谎。训练人类给出与自己的副本（过去、未来、双胞胎、模拟、朋友、同胞，等等）一致的答案，是的，明显的先令点就是说实话，但在很多情况下还有更好的先令点。从“这实际上是一个奇怪的特殊情况，但我的副本可能无法弄清楚，所以我要假装它不是”这样的内容开始。然后从那里出发。弄清楚如何更好地处理每项检查可能会逐渐获得回报。</p><p>然后你会介绍一些你无法始终如一地正确识别真相的案例。</p><p>当这一切都崩溃时，一切都不会美好。</p><p>从另一种角度来看，一个人不能侍奉两个主人。我认为这也适用于这里。如果你只能得到一个大师的纯粹版本（例如真理），你可以教人工智能为它服务。唉，这要求它被独特地置于其他一切之上。我是耶和华你的神，除了我以外，你不可有别的神。因此，当你同时评估真理以外的任何事物时，你的真理一致性就会出现问题。</p><p> Jan 确认，该计划仍然是让人工智能完成我们的对齐作业，希望我们能够足够明智地让它们得到充分的控制以实现这一目标，然后选择充分专门地分配该特定作业。这里的方法似乎并不赞成（或反对）X 等于“让 AI 进行 X 研究”中的“对齐”。</p><blockquote><p>区分一般超人类模型和超级智能之间的区别是值得的。 <a href="https://openai.com/blog/introducing-superalignment" target="_blank" rel="">OpenAI 的 Superalignment 团队的目标</a>是解决后者，但我们研究的技术针对的是前者。如果我们在后者上取得成功，我们就可以将超人的<a href="https://aligned.substack.com/p/alignment-mvp" target="_blank" rel="">自动比对研究人员</a>与“手工”比对技术结合起来。</p></blockquote><p>我发现对齐技术的目标是“比人类水平模型大不到 4 个 OOM”，假设 ASI 需要 10 个以上的 OOM，这对所涉及的缩放定律以及人类的失败表现出非凡的（不合理的）信心 -水平和高于人类水平的模型可以进行各种形式的放大和引导以及潜在的递归自我改进。</p><p>我还注意到，我认为论文中的实验并没有从任何一个方向对雷克在他的帖子中讨论的拟议技术的前景提供实质性的启发。</p><h4>脆弱世界假说</h4><p><a href="https://michaelnotebook.com/vwh/index.html" target="_blank" rel="noopener noreferrer nofollow">迈克尔尼尔森提出了关于脆弱世界假说的想法</a>， <a href="https://twitter.com/ATabarrok/status/1736388423462584343" target="_blank" rel="noopener noreferrer nofollow">包括针对极其脆弱（易受火灾）世界的具体思想实验</a>。这是一篇非常好的文章，虽然很长。</p><p>脆弱世界假说定义如下：</p><blockquote><p>脆弱世界假说：如果技术继续发展，那么在某个时刻将获得一系列能力，使文明极有可能遭到毁灭，除非文明充分摆脱半无政府状态的默认状态。</p></blockquote><p>他指出，他曾经相信友好世界假说，大致如下：</p><blockquote><p>友好世界假说：即使人类处于半无政府主义的默认状态，也不太可能获得一系列使文明几乎不可避免地灭绝的能力。</p></blockquote><p>我们无法进行（实用或安全的）实验来告诉我们答案。</p><p>这个问题在多大程度上是是否、如何进行人工智能开发以及在什么样的治理结构下进行的关键？</p><p>这当然是进攻与防守问题的一种核心形式。</p><p>人们还必须考虑到，值得担心的潜在“毁灭秘诀”之一是，在这种情况下，不协调的决策以及经济、政治和社会压力的累积效应所带来的毁灭，以及在这种情况下设置这种链条的能力。运动——这不是传统意义上的脆弱世界，但也不是友好世界。从功能上来说它很脆弱。</p><p>您也可以处于中间某个位置，而不是处于任一极端。</p><p>从两个极端来看，这个问题对于（几乎）每个人来说都应该是一个明显的症结所在。</p><p>如果人工智能的持续发展很快意味着很多人都会有一个“炸毁地球”按钮，或者一个“人工智能消灭人类”按钮，或者一个“人工智能控制未来”按钮，甚至一个“释放一种高度致命的瘟疫，会杀死很多人，那么让这种情况发生并不是一个可以接受的答案。要么你需要阻止人工智能的开发，阻止它的广泛部署，要么以其他方式进行相当极端的监控和控制。</p><p>如果我们有信心能够以去中心化、半无政府主义的方式防范所有这些事情，人工智能不会构成此类威胁，那就太好了。我们应该采取相应的措施，采取一切必要的预防措施来可靠地实现这一目标。</p><p> （人们仍然可能不同意，因为他们出于其他原因不喜欢未来的世界，但我很高兴地说这些人错了。）</p><h4>人们担心人工智能会杀死所有人</h4><p><a href="https://twitter.com/ATabarrok/status/1735369099427889301" target="_blank" rel="noopener noreferrer nofollow">教皇！</a> <a href="https://time.com/6509686/pope-treaty-ai-regulation/" target="_blank" rel="noopener noreferrer nofollow">嗯，有点像。</a> <a href="https://apnews.com/article/vatican-pope-ai-artificial-intelligence-9805fec11681adbf88d3a7c73bdf47de" target="_blank" rel="noopener noreferrer nofollow">他呼吁就人工智能制定一项具有约束力的全球条约。</a></p><blockquote><p>美联社妮可·温菲尔德：教皇方济各周四呼吁制定一项国际条约，以确保人工智能的开发和使用合乎道德，并认为缺乏同情心、仁慈、道德和宽恕等人类价值观的技术风险太大。</p><p> ……</p><p>但他的新和平信息更进一步，强调了伦理学家和人权倡导者对技术所提出的严重的、存在的担忧，这些技术有望改变日常生活，从而扰乱从民主选举到艺术的一切。</p><p>梵蒂冈发展办公室红衣主教迈克尔·切尔尼（Michael Czerny）在周四的新闻发布会上介绍了这一信息，他表示：“人工智能很可能代表着我们未来风险最高的赌博。” “如果结果很糟糕，人类就应该受到谴责。”</p><p>人工智能安全模因：“在控制一切的痴迷欲望中，我们面临着失去对自己的控制的风险；在追求绝对自由的过程中，我们面临着陷入‘技术独裁’的恶性循环的风险，”他写道。</p><p> “教皇弗朗西斯警告不要在武器系统中使用人工智能，称这可能导致全球灾难。”</p><p> ……</p><p> “教皇‘不是勒德分子’”……教皇赞赏为人类服务的技术和科学进步，但方济各特别关心人工智能。</p></blockquote><p>弗朗西斯主要担心人工智能武器或排名系统，或信任一般算法的决策。这对他来说已经是足够的理由了。我没有看到关于完全存在的风险场景的明确讨论。我的推测是，教皇和许多人一样，并不了解人工智能的潜力。但即使没有这一点，他也认为人类控制的丧失将是一个明确的主题，并且他理解为什么我们的选择仍然可能会导致这样的道路。</p><p>很多有趣的反应。</p><blockquote><p>亚历克斯·塔巴罗克（Alex Tabarrok）：教皇在不结盟的超级智能方面有一些经验。</p><p> Danielle Fong（单独回应）：“自最初的一神论以来最大的越权行为。”</p></blockquote><p>我会说巴别塔，也许是伊甸园。至少是金牛犊。</p><blockquote><p> AgiDoomerAnon：Yud：“从没想过我会与教皇并肩作战。”</p></blockquote><p> <a href="https://twitter.com/mealreplacer/status/1735763730879668634" target="_blank" rel="noopener noreferrer nofollow">来自朱利安·哈泽尔：</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca7687f-99da-440a-8ee2-2eda97de0d70_1730x1472.jpeg" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ca7687f-99da-440a-8ee2-2eda97de0d70_1730x1472.jpeg" alt="图像"></div></figure></div><p> <a href="https://twitter.com/JaimeYassif/status/1735442179516956707" target="_blank" rel="noopener noreferrer nofollow">白宫国家安全顾问杰克·沙利文很担心</a>，但还没有完全了解情况。</p><blockquote><p> Jamie Yassif：当被问到是什么让他彻夜难眠时，@JakeSullivan46 回答说“生成人工智能的核心安全风险，特别是人工智能与生物、人工智能与网络的融合。”</p></blockquote><p>这些都是国家安全顾问需要担心的极好的事情。他们仍然反思寻找特定的对手和威胁。</p><p> <a href="https://rethinkpriorities.org/publications/us-public-perception-of-cais-statement-and-the-risk-of-extinction" target="_blank" rel="noopener noreferrer nofollow">一般人？</a> Scott Alexander 提醒我们，当被问及人们说人工智能在 2100 年导致灭绝的几率为中位数 15% 和平均值 26% 时。众数约为 1%，这是最有趣的，因为它不是 0%。 </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d271714-4f9d-47c1-9778-75bf44f3e52e_822x843.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d271714-4f9d-47c1-9778-75bf44f3e52e_822x843.png" alt=""></div></figure></div><p>通常的警告适用。人们在生活中并不会相信这一点，他们不会过多考虑，只有在主动提示时才会说出来。只有 32% 的人对人工智能有相当程度的担忧，而且这可能主要是普通风险。 </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F505b32d1-eeea-4155-a5e8-b15c3d263460_835x574.png" target="_blank"><div><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F505b32d1-eeea-4155-a5e8-b15c3d263460_835x574.png" alt=""></div></figure></div><p>我认为这些是高度一致的。大多数人，如果没有在社会上引起担忧，而是注意到它在社会上被视为奇怪，他们会忽视这种风险，并在需要时找到不注意到它的方法。 </p><div><figure><a href="https://substackcdn.com/image/upload/f_auto,q_auto/v1/mirroredImages/Rg7h7G3KTvaYEtL55/inyk16bd9juazymyvpsm" target="_blank"><div><img src="https://substackcdn.com/image/upload/f_auto,q_auto/v1/mirroredImages/Rg7h7G3KTvaYEtL55/inyk16bd9juazymyvpsm" alt=""></div></figure></div><div><figure><a href="https://substackcdn.com/image/upload/f_auto,q_auto/v1/mirroredImages/Rg7h7G3KTvaYEtL55/inyk16bd9juazymyvpsm" target="_blank"><div><img src="https://substackcdn.com/image/upload/f_auto,q_auto/v1/mirroredImages/Rg7h7G3KTvaYEtL55/inyk16bd9juazymyvpsm" alt=""></div></figure></div><h4>其他人并不担心人工智能会杀死所有人</h4><p><a href="https://twitter.com/kanzure/status/1736619458238529970" target="_blank" rel="noopener noreferrer nofollow">布莱恩·毕肖普 (Bryan Bishop) 担心……外星人正在开发人工智能？</a></p><blockquote><p>布莱恩·毕肖普：你总是处于一种异常危险的状态。确实下降了很多。但你强迫地球停止发展人工智能并不能保护你免受人工智能 x 风险，因为你对宇宙的其他部分没有统治权。还是有风险的。</p></blockquote><p>我们玩的并不是像《猎户座大师》或《星际迷航》那样故意对称的游戏。在任何合理的潜在外星人模型下（无论是贪婪的外星人还是其他），我们与外星人进行一场势均力敌的竞赛的概率是 epsilon，如果我们快速构建人工智能，我们就生活在其中，但如果我们不这样做，我们就会死于外星人或外星人工智能的影响下。如果我们像沙丘一样永远推迟人工智能并以其他方式幸存下来，那么是的，十亿年后这将成为一个问题。他也是最新一个（上游）说“安全？”的人。没有人是安全的。</p><blockquote><p>布莱恩·毕肖普：人类从来都不是安全的。人类历史的大部分时间是在赤贫、疾病和死亡的泥沼中度过的。 1000 亿人已经死亡。</p></blockquote><p> <a href="https://twitter.com/Kasparov63/status/1737511212600959195" target="_blank" rel="noopener noreferrer nofollow">加里·卡斯帕罗夫坚称人工智能现在是而且永远都是一种工具</a>。似乎概念上很混乱。</p><h4>轻松的一面</h4><p>斯科特·亚历山大并不是半途而废的人。 <a href="https://www.astralcodexten.com/p/son-of-bride-of-bay-area-house-party" target="_blank" rel="noopener noreferrer nofollow">这是湾区家庭派对新娘的儿子</a>。</p><p>以前我以为这已经失去了动力。我错了。我们就这么回来了。</p><p>这显然并不是一件好事。有时你希望事情就这样结束。</p><p>我想知道这是否特别精彩？</p><blockquote><p>你要否认乌克兰战争吗？”</p><p> “我否认乌克兰战争”，坐在你旁边的一位女士说道，她自称是伊琳娜。</p><p> “你怎么能否认呢？你直接看新闻就可以了！或者去基辅！”</p><p> “我住在基辅，”伊琳娜说。 “我只是来这里探望家人几个星期。”</p><p> “你——你怎么能住在基辅却否认有乌克兰战争？”</p><p> “嗯，”伊琳娜说，“我只是认为对战争的信仰是……” 。 。英文术语是什么。 。 。意识形态的总体化。我的邻居相信战争，他们抛下妻子和孩子，前往前线与俄罗斯人作战。我总是被教导要把家庭放在第一位，我认为成为那种让你的信仰妨碍这一点的狂热分子是错误的。”</p><p> “这不是信仰！真正的俄罗斯人拥有真正的坦克！”</p><p> “别误会我的意思，我认为士兵是伟大的。我只是看到很多聪明有前途的年轻人当他们开始相信俄罗斯人时，他们的心理健康就会下降。他们对“如果俄罗斯人轰炸我的城市怎么办？”感到恐慌。并感到极度内疚，因为他们需要“让父母远离前线”或“营救家人”，否则他们就是坏人。我认为这是一种——英文单词是什么——邪教。如果你相信俄罗斯人准备占领你的国家，你就可以为任何暴行辩护。为什么不实行奴隶制，这样就可以强迫人们参战呢？为什么不杀掉俄罗斯的所有人，这样他们就不能再威胁你了？为什么不针对俄罗斯目标实施恐怖主义？为什么不把你所有的钱都给我，这样我就可以阻止这些邪恶的俄罗斯人了？它是 。 。 。英文术语是什么。 。 。帕斯卡式推理。要知道，过去末日论者谈论的都是“人口过剩”和“全球变冷”。现在他们谈论“俄罗斯人”和“普京”。我认为你应该过正常而有道德的生活，诚实，善待你的邻居。”</p><p> “请原谅，”你说。 “我决定回到主房间，听人们谈论山姆·奥尔特曼。”</p></blockquote><p> <a href="https://www.smbc-comics.com/comic/agi-3" target="_blank" rel="noopener noreferrer nofollow">好消息，物理学家很快就会加入这一事业（SMBC）。</a></p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/WaDFCrd6KEwojLXgj/ai-43-functional-discoveries#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WaDFCrd6KEwojLXgj/ai-43-function-discoveries<guid ispermalink="false">瓦德弗卡德6KEwojLXgj</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 21 Dec 2023 15:50:06 GMT</pubDate> </item><item><title><![CDATA[Rating my AI Predictions]]></title><description><![CDATA[Published on December 21, 2023 2:07 PM GMT<br/><br/><p> 9 个月前<a href="https://aizi.substack.com/p/whats-left-for-agi-besides-scale"><u>，我预测了 2023 年人工智能领域的趋势</u></a>。这是我的做法（粗体表示它们发生了，斜体表示它们没有发生，粗体和斜体都表示未解决）：</p><ol><li> <strong>ChatGPT（或 OpenAI 的后续产品）到 2023 年底将具备图像生成功能：70%</strong></li><li><strong>到 2023 年底，OpenAI/Deepmind/Microsoft 还没有关于将视频解析或生成纳入可投入生产的法学硕士的论文或新闻稿：90%</strong></li><li><i>到 2023 年底，所有公开发布的接受音频输入的 LLM 模型均使用音频到文本到矩阵（例如，在将音频作为文本传递到 LLM 之前将其转录）（条件是方法可识别）：90%</i></li><li><strong>到 2023 年底，所有公开发布的接受图像输入的 LLM 模型都使用图像到矩阵（例如，直接嵌入图像，而不是获取图像标题）（条件是方法可识别）：70%</strong></li><li><strong>到 2023 年底，至少一个公开的 LLM 包含至少一个快速查询工具（公开发布 Bing 聊天将解决这一问题）：95%</strong></li><li><strong>到 2023 年底，ChatGPT（或 OpenAI 的后续产品）将使用至少一种快速查询工具：70%</strong></li><li>首次公开发布使用工具的基于 LLM 的 AI 与允许任意编写和执行代码的 AI 之间的时间 >;12 个月：70%</li><li> >;24 个月：50%</li><li><strong>到 2023 年底，没有旨在进行金融交易的公开产品（例如“购买打蛋器”实际上使用您的信用卡购买打蛋器，而不仅仅是将其添加到您的购物车）：90%</strong></li></ol><p>请参阅原始帖子以获取证据/理由，了解为什么这些问题确实/没有解决。如果您不同意其中任何一个，请告诉我。请特别注意预测 2，关于视频处理 LLM，我没有将 Gemini 算作使用视频输入的“生产就绪”模型，因为谷歌已确认他们<a href="https://www.cnbc.com/2023/12/08/google-faces-controversy-over-edited-gemini-ai-demo-video.html"><u>“使用静态图像并输入文本提示”</u></a>来制作预告片。</p><p>一些评论：</p><ul><li>由于 Gemini 的原因，我唯一的方向性错误预测是音频输入。我认为这只是我的一个非受迫性错误，我在将 2023 年 3 月的世界模型转化为百分比方面做得很糟糕。</li><li>根据这些预测，我的<a href="https://en.wikipedia.org/wiki/Scoring_rule"><u>Brier 分数</u></a>是 0.1575，我的对数分数是 0.519。</li><li>校准图（n=7）： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/dtrmr6Fn5AyP5GosQ/lqi9u43tcwuutiasdsgz"></li><li>我认为可以解决以下两件事来改进我的预测/校准：<ul><li><strong>我的预测有 90% 是错误的。</strong>我进入了导致上述预测的思考过程，但在某种程度上，我认为我只是犯了一个错误，没有足够仔细地措辞。也就是说，我将我的预测框定为“全部”陈述，同时在心理上将其视为“大多数”陈述。如果我要做出这样的预测，我需要更仔细地考虑反事实。证明我错的是一家大公司决定尝试一项技术上可行的任务，而我应该为该任务分配超过 10% 的概率。</li><li><strong>否则我就太谨慎了。</strong>除了上述错误之外，我还击中了所有目标，这意味着我的目标太低了。我的自然倾向是预测将会发生的事情，但是我应该要么给这些事情赋予高于 70% 的概率，要么预测更多的事情，将我的总体准确率降低到 70%。<br></li></ul></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/dtrmr6Fn5AyP5GosQ/rating-my-ai-predictions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dtrmr6Fn5AyP5GosQ/ rating-my-ai-predictions<guid ispermalink="false"> dtrmr6Fn5AyP5GosQ</guid><dc:creator><![CDATA[Robert_AIZI]]></dc:creator><pubDate> Thu, 21 Dec 2023 14:07:50 GMT</pubDate></item></channel></rss>