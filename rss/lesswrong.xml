<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 27 日星期一 02:25:12 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Justification for Induction]]></title><description><![CDATA[Published on November 27, 2023 2:05 AM GMT<br/><br/><p>大卫·休谟向哲学家提出了一个问题。他的主张是，“我们没有经历过的事例与我们有过经历的事例相似”的说法是没有道理的。在下面的论文中，我将证明对<i><strong>实例的观察可以证明对未经历过的实例的断言是合理的。</strong></i> <strong>&nbsp; </strong>在 A 部分中，我将首先定义实例观察的含义。接下来，在 B 部分中，我将定义未经历过的实例的断言意味着什么。然后，在 C 部分中，我将定义断言合理的含义。在 D 部分中，我将展示经验（如 A 部分中定义）如何满足无经验实例（如 B 部分中定义）的论证标准（如 C 部分中定义）。最后，在推论 E 部分中，我将演示如何使用 D 部分的方法和<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/thevfeegcy1v6rnankm9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/jqyuqgcv3kn8fcjlg87d 30w">微积分中使用的证明。</p><p> <strong>A部分</strong></p><p>对实例的观察是可以对宇宙做出的陈述。例如，“至少有一只天鹅是白色的。 （在观察自然界中发现的白天鹅时可以做出这种断言。）”</p><p> <strong>B部分</strong></p><p>未经历过的实例是将来要进行的观察。例如，“我们看到的下一只天鹅将是白色的。 （关于将在自然界中观察到的下一只天鹅。）”此类实例的断言是在时间 (t <sub>1</sub> ) 做出的陈述，预测在时间 (t <sub>2</sub> ) 发生的观察，例如即，时间(t <sub>1</sub> )在时间(t <sub>2</sub> )之前。</p><p> <strong>C部分</strong></p><p>如果 (p) 的否定意味着矛盾，则断言 (p) 是合理的。如果直接观察到一个断言，它也是合理的。例如，如果观察到一只白天鹅，我们就有理由断言“观察到一只白天鹅”。</p><p> <strong>D部分</strong></p><p>考虑在给定时间 (t <sub>1</sub> ) 做出断言 (p <sub>1</sub> )（我们看到的下一只天鹅将是白色）的情况。根据 C 部分中的标准，在时间 (t <sub>2</sub> ) 观察到的对象同时具有属性（下一个看到的天鹅）和（白色），证明断言 (p <sub>1</sub> ) 是正确的。重要的是要理解，在时间 (t <sub>1</sub> ) 断言 (p <sub>1</sub> ) 是不合理的，但在时间 (t <sub>2</sub> ) 断言 (p <sub>1</sub> ) 是合理的。</p><p>休谟的著作中有一个重要的陈述：主要是，“对未来情况所做的断言在作出时无法得到证实。”但是，这一实质性声明并不意味着这一点； “对未来情况的断言永远都是站不住脚的。”随后需要更一般性的声明： “对未来的断言是站不住脚的。”相反，我的目的是说明，对尚未经历过的实例的断言（相对于它们在 t <sub>1</sub>时的断言）可以在未来被观察到的情况下得到证明（相对于它们在 t <sub>2 时</sub>的观察） 。</p><p> <strong>E部分</strong></p><p>广义陈述是对无限数量的观察案例进行属性谓词的断言。例如，“所有被观察到具有属性（天鹅）的物体也将具有属性（白色）。”我打算建立近似基础，正是为了证明这种概括性陈述的合理性。换句话说，<i><strong>可以进行一些观察，暗示一般性陈述的部分合理性</strong></i>。</p><p>部分论证的形式化概念对于这里提出的论点至关重要，因此必须严格定义。断言给定陈述（q）的部分理由是对前提（p）的肯定，使得：前提 (p) 是引发 (q) 所必需的前提集合中的一个成员。这意味着以真陈述（p <sub>1</sub> ）的形式对陈述（q）的部分论证减少了蕴涵（q）所需的未确定前提（p <sub>2</sub> ，p <sub>3</sub> ，p <sub>4</sub> ，...）的集合。换句话说，如果在观察 (p <sub>1</sub> ) 之后蕴含 (q) 所需的前提集合是在观察 (p <sub>1</sub> ) 之前蕴含 (q) 所需的前提集合的真子集，则 (q) 由 (p <sub>1</sub> ) 部分证明。</p><p>考虑非广义的陈述，“观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”</p><p>在这种情况下; n = 2，我们有；</p><p> (q <sub>1</sub> ) 接下来两个被观察到具有属性（天鹅）的对象也将具有属性（白色）。 （在t <sub>1</sub>处陈述）。</p><p>命题 (q <sub>1</sub> ) 由两个命题的真实性所证明；</p><p> (p <sub>1</sub> ) 第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</p><p>和</p><p>(p <sub>2</sub> ) 第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察到）。</p><p>值得注意的是，t <sub>3</sub>时 (q <sub>1</sub> ) 的论证同样依赖于两个观测值（p <sub>1</sub>和 p <sub>2</sub> ）。换句话说，如果任一观察结果为假，则 q <sub>1</sub>必然为假。在 t <sub>2</sub>处，当观察 (p <sub>1</sub> ) 发生时，蕴含 (q <sub>1</sub> ) 所需的未观察前提集合从集合 A = ((p <sub>1</sub> ) AND (p <sub>2</sub> )) 转换为集合 B = (p <sub>2</sub> ）。自从<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/kinilafz0dgetqztaflc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/i5n5brs4u3j3jkhhqj21 35w"> ， 它遵循; (q <sub>1</sub> ) 由(p <sub>1</sub> )部分证明。</p><p>此时，我将介绍一种表示部分合理程度的方法。为了确定给定的观察集 (p <sub>1</sub> , p <sub>2</sub> , p <sub>3</sub> , …) 对陈述 (q) 的部分证明的程度，集合 B 的基数 = (从集合 A =（在 (q) 被断言时蕴含 (q) 所需的前提集合）的基数中减去蕴含 (q))，然后除以 A 的基数以产生介于0 和 1。</p><p>对于上面 (q <sub>1</sub> ) 的情况（在时间 t <sub>2</sub> ），我们有； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zy0xbdpzcoqfitljipdc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xszpyc2etbdtrrkf14pk 126w"></p><p>在这种情况下； n = 3、n = 4 和 n = 5 我们有；</p><ul><li> (q <sub>2</sub> ) 接下来观察到的具有属性（天鹅）的三个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>3</sub> ) 接下来观察到的具有属性（天鹅）的四个对象也将具有属性（白色）。 （在 t <sub>1</sub>处陈述）</li><li> (q <sub>4</sub> ) 观察到的接下来的五个具有该属性（天鹅）的对象也将具有该属性（白色）。 （在 t <sub>1</sub>处陈述）</li></ul><p>鉴于以下观察结果；</p><ul><li>观察到的第一个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>2</sub>观察）</li><li>观察到的第二个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>3</sub>观察）</li><li>观察到的第三个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>4</sub>观察到）</li><li>观察到的第四个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>5</sub>观察）</li><li>观察到的第五个具有属性（天鹅）的对象也具有属性（白色）。 （在 t <sub>6</sub>观察）</li></ul><p>它跟随;</p><ul><li>在 t <sub>1</sub>处，(q <sub>2</sub> ) (q <sub>3</sub> ) (q <sub>4</sub> ) 的部分合理性 = 0。</li><li>在 t <sub>2</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tqdlhve1nm1shdoq5i5e" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ncevgtuvpuowmhuxv52c 18w"> = 1/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/du8n26vm2ai4llaqjaay" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nak3mnxnssdpdg4cprer 18w"> = 1/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tfkawkwg3etv7k6zk1lg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/oj541besvgbp2imrufci 18w"> = 1/5。 </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/hvvltvr41lcxb3ggoyjr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uy5eq7kam2m91vjvnfys 105w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ne2l6i8uzubagveb9w4x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tedejxksb3v0yrwnauj2 101w"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xayc7qd69pr39fxtbtws" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/pxtoqczhyswe5x4vlcqu 94w"></p><ul><li>在 t <sub>3</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/szpyevlry4roe8ueywxn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/dsp2lor7lmgwvyoo11zs 18w"> = 2/3, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zaypc2vo9kn2xf3u6jz7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/zwbrab2wgov3xgfia54o 18w"> = 1/2, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t4xn7lrp2gtxrqwmq6r6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/voyibd6olzfrrvyg1sud 18w"> = 2/5。</li><li>在 t <sub>4</sub>时，(q <sub>2</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/j5bk9ucvp4qk9pznl2g5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/okcovgvhuyv56gqnx7ag 18w"> = 1, (q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/f2ois9misczt7p8ssi2g" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbx3cxkewqiplxxzclr9 18w"> = 3/4, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/drppksaohw0lw7mougrr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/agxmrla1muzfcw2armdx 18w"> = 3/5。</li><li>在 t <sub>5</sub>时，(q <sub>3</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/r4xr7yqoeen7mdwzinzn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/qkaoiyzeks8j0yyngjv6 18w"> = 1, (q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/byla51kkh2ozms6rbx6y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/xmn8mx0jb7rjuskxjhnp 18w"> = 4/5。</li><li>在 t <sub>6</sub>时，(q <sub>4</sub> ) = <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ogsfgsynwwssfgon929s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/v13vqj1dynf742nxndnb 18w"> = 1。</li></ul><p>现在可以证明，对于语句 (q <sub>1</sub> ) 中的任何值 n “观察到的下一个具有属性（天鹅）的对象也将具有属性（白色）。”，有许多观察结果m 证明 (q <sub>1</sub> ) 合理。</p><p>考虑一下情况；语句 (q <sub>1</sub> ) 在 (t <sub>1</sub> ) 和 (p <sub>m</sub> ) 处作出“观察到的第 (m) 个具有属性（天鹅）的对象也具有属性（白色）”。是在 (t <sub>(m+1)</sub> ) 进行的。</p><p>让 m = 观测值的数量；</p><p>自从; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/nnk3diqncvtwsqish13s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/d1jidomnzkgjksyakjye 31w"> = 部分理由</p><p>和</p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/tazg8zld6gv2ysua5nyd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/uspiwnmdh2h6fsu4riks 79w"></p><p>我们有; </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/ggvyjsryl3rxskfu9g8s" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/iyfogdfseskkzssar76a 110w"> = (q <sub>1</sub> ) 的每个观察结果 (p <sub>m</sub> ) 的部分合理性。</p><p>我们现在可以展示； </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/t7e5rtujbvpiqshht2cv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BCXESim86uoPAXsfA/fbbuiedrzluuw94tychm 112w"></p><p>因此，对于所有 (q <sub>1</sub> )，当 n 接近无穷大时，情况是：当 m 接近 n 时，m/n 接近 1（完全合理）。正是这个概念使我们能够近似地证明广义陈述的合理性。</p><br/><br/><a href="https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BCXESim86uoPAXsfA/justification-for-induction<guid ispermalink="false"> BCXESim86uoPAXsfA</guid><dc:creator><![CDATA[Krantz]]></dc:creator><pubDate> Mon, 27 Nov 2023 02:19:42 GMT</pubDate> </item><item><title><![CDATA[Situational awareness (Section 2.1 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 2.1 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本（此处）[ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais">https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais</a> ]，或在播客上搜索“Joe Carlsmith Audio”应用程序。</p><h1>谋划需要什么？</h1><p>现在，让我们来研究一下用于训练高级人工智能的基线机器学习方法产生阴谋者的可能性。我将从检查策划的先决条件开始。我将重点关注：</p><ol><li><p><strong>态势感知：</strong>也就是说，模型了解自己是训练过程中的模型，训练过程会奖励什么，以及一般客观世界的基本性质。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-1" id="fnref-46uzxQJJ3ukEQwzsf-1">[1]</a></sup></p></li><li><p><strong>超越情节目标：</strong>也就是说，模型关心情节完成后其行为的后果。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-2" id="fnref-46uzxQJJ3ukEQwzsf-2">[2]</a></sup></p></li><li><p><strong>将剧集奖励作为权力驱动的工具策略的一部分：</strong>也就是说，该模型相信，如果针对剧集奖励进行优化，那么其超越剧集的目标将会更好地实现——特别是，如果它这样做，它或其他一些人工智能将获得更多的权力。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-3" id="fnref-46uzxQJJ3ukEQwzsf-3">[3]</a></sup></p></li></ol><h2>对情况的意识</h2><p>模型有态势感知能力吗？让我们区分一下这种意识中所涉及的两大类信息：</p><ul><li><p>有关客观世界的<em>一般信息</em>，包括有关机器学习训练如何运作的信息。</p></li><li><p> <em>“自定位”信息</em>：即在客观世界中<em>定位</em>模型的信息，并告诉它有关其自身情况的事实 - 例如，它是<em>这种</em>模型，它正在接受特定奖励信号的训练，在这个特定的实验室，在这个特定的时间段，等等<sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-4" id="fnref-46uzxQJJ3ukEQwzsf-4">。 [4]</a></sup> （不过：请注意，尚不清楚开始策划需要多少此类信息。例如，一个模型原则上可以决定“无论是谁是”和“无论是什么时间段”，它可能会通过其目标做得更好，尝试在训练信号的灯光下表现良好，并在以后获得更多的能量/信息。）</p></li></ul><p>即使比人类更好的模型也能吸收大量有关客观世界的一般信息，并开发出其运作方式的详细机械模型，这似乎是非常合理的。事实上，当前的模型已经可以通过预训练数据访问大量信息，特别是有关机器学习的信息。他们机械地模拟世界、进行推理、得出他们没有“记住”的结论等能力似乎正在迅速提高。</p><p>更重要的是，虽然原则上可以尝试专门阻止模型获得有关客观世界的某些类型的信息（例如，通过从训练数据中排除某些类型的信息），但这并不是当前训练中的默认设置，并且各种各种信息对于您希望模型执行的任务可能相当重要。模型越复杂，就越难确保它们无法根据您提供给它们的信息<em>推断</em>出您试图隐藏的信息。</p><p>同样的考虑因素也适用于<em>自定位信息</em>吗？我倾向于认为：是的。但至少还不太清楚。例如，虽然默认情况下语言模型预训练数据将包含大量有关语言模型及其训练方式的信息（因为此类信息在互联网上广泛可用），但尚不清楚它将为语言模型提供多少信息。特别是关于其情况的模型，甚至是预训练的下一个标记预测任务是否会激励模型拥有更多的自我概念。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-5" id="fnref-46uzxQJJ3ukEQwzsf-5">[5]</a></sup>虽然当前的模型确实最终会接受信息和奖励的训练，从而使它们说出诸如“我是 GPT-4，一个由 OpenAI 训练的语言模型”和“这就是我的训练方式”之类的话，但它的效果却较差。清楚这些信息需要在多大程度上作为真正的自我定位信息整合到 GPT-4 的世界模型中，而不是仅仅作为对这种形式的问题的回答来理解/记忆。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-6" id="fnref-46uzxQJJ3ukEQwzsf-6">[6]</a></sup>或者，换句话说：如果人们<em>不</em>认为 GPT-4 具有情境感知能力，那么未来类似（但更复杂）的模型似乎也可能不具备情境感知能力。无论如何，就 GPT-4 能够执行许多复杂任务而言，也许更高级的版本也能够在没有态势感知的情况下执行更高级的任务——特别是如果我们努力阻止这种意识的出现。</p><p>就我个人而言，我并没有一个非常详细的模型来说明我们何时应该期望在以不同方式训练的不同模型中出现情境意识——尽管我认为这个问题对于实证研究来说已经成熟了。然而，我确实认为，如果没有相反的积极和知情的努力，我们应该期待某些类型的先进人工智能系统默认具有相当全面的态势感知形式（包括各种自定位信息）。</p><p>为了感受这里的直觉，考虑一个极端的例子，它<em>不是</em>我所期望的最接近的高级人工智能的样子：即，一个真正的机器人管家，他穿着机器人的身体在你的房子里闲逛，并且做给你的任务。在我看来，创建这样一个管家的默认方式是赋予它与人类管家大致相同水平的态势感知能力，这似乎是非常合理的。例如，为了不打翻你的植物，这个管家需要了解它的机器人身体在哪里；为了安排您的约会，它需要知道时间；为了准确判断自己能够完成哪些任务，管家需要了解自己和自己的能力；等等。</p><p>当然，我们还没有机器人管家，而且可能暂时不会（或者实际上，如果人工智能风险朝某些方向发展的话，永远不会）。但现在想象一下，一个有效但无形的人工智能个人助理，就像<a href="https://www.adept.ai/">Adept</a>试图创建的那样，它可以在你的计算机上为你执行任务。我认为大部分（尽管不是全部）相同的分析都适用。也就是说，在我看来，即使没有明显的“体现”，让这种个人助理在高水平上发挥作用的默认方法将是让它对“正在发生的事情”有一定的了解，关于其操作所影响的特定时间和情况、与之交互的特定用户等等。事实上，在某种程度上，你<em>让</em>代理直接与这样的信息源进行交互——例如，让它实时访问互联网（包括，例如，关于创建它的公司/实验室如何工作、培训它的实时信息）模型等），为其提供回复电子邮件或安排活动所需的上下文，允许其编写消息和提出问题等 - 似乎很难阻止相关信息变得非常直接可用。</p><p>当然，当前的许多形式的培训并没有提供可用的信息源，例如有关用户的详细信息或实时访问互联网的信息。但为了让模型充当这种类型的有效个人助理，默认情况下，提供对此类信息源的访问的上下文似乎会被纳入训练中（<a href="https://www.adept.ai/blog/act-1">例如</a>，参见 Adept 的视频，其代理与互联网交互）。在某种程度上，模型在部署给用户后会继续“在线”训练（我通常会在下文中假设这一点），以便不断对用户交互进行采样、分配奖励并用于更新模型的权重，训练将在模型与现实世界相当直接交互的环境中进行。当然，像这样的模型可能无法访问<em>所有</em>潜在相关的自定位信息 - 例如，关于它所在的特定服务器、有关奖励过程的精确细节等。但它似乎也不<em>需要</em>这样的访问权限，开始策划。</p><p>更重要的是，面对那些对世界有详细理解的复杂模型，他们会说“我是 GPT-4，一个由 OpenAI 训练的语言模型”，我个人总体上对过于依赖声明持谨慎态度。就像“哦，它只是记住了，它没有自我概念，也没有真正理解它所说的内容。”如果“记忆”的相关形式涉及“我是 GPT-4”这一概念，并以我们期望从对声明的实际理解中期望的无缝和连贯的方式融入 GPT-4 的交互中，那么我认为我们的默认假设应该是类似这种实际理解的事情正在发生。事实上，总的来说，在我看来，许多人似乎过于热衷于声称模型在涉及各种认知（例如“理解”、“推理”、“规划”等）时不具有“真正的人工制品”。 ），甚至没有对这种否认意味着什么进行任何预测。就他们确实<em>做出</em>的预测而言，尤其是关于<em>未来</em>模型的能力，我认为这种否认——例如，“语言模型只能学习‘浅层模式’，它们不能进行‘真正的推理’”——已经过时了。</p><p>也就是说，我确实认为有一个合理的理由表明，对于我们希望高级人工智能执行的各种任务来说，各种形式的态势感知并不是严格必要的。例如，编码似乎使态势感知变得不那么明显必要，并且也许各种与对齐相关的认知工作（例如，生成高质量的对齐研究、帮助解释、修补安全漏洞等）将是相似的。所以我认为，尝试尽可能主动地<em>避免</em>态势感知是这里值得探索的重要路径。正如我将在下面讨论的那样，我认为，至少，在我看来，学习检测和控制何时出现态势感知对于<em>其他</em>类型的反阴谋措施非常有帮助，例如尝试针对类似阴谋者的目标进行训练（并以其他方式塑造模型的目标，使其尽可能接近您想要的），然后再进行态势感知（从而产生训练游戏的威胁）。</p><p>然而，部分原因是我认为态势感知是一种相当强的默认行为，如果没有积极努力阻止它，我在这里不想指望避免它——在下文中，我将继续假设我们谈论在训练中的<em>某个时刻</em>变得具有情境意识的模型。我的兴趣集中在于我们是否应该期望<em>这样的</em>模型是阴谋家。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-46uzxQJJ3ukEQwzsf-1" class="footnote-item"><p>正如 Cotra（2022）所讨论的，态势感知有一个范围。如果讨论要更直接地集中在这个概念上，我们需要更精确地了解具体涉及哪些属性（并且我在这里的定义<a href="https://arxiv.org/pdf/2309.00667.pdf">与 Berglund 等人（2023）</a>中的定义有所不同）。但正如我在下面讨论的，态势感知并不是我在这里的主要关注点，除非它表明“对世界的理解以及开始策划所需的培训过程”。 <a href="#fnref-46uzxQJJ3ukEQwzsf-1" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-2" class="footnote-item"><p>但请注意，正如我上面提到的，非阴谋模型仍然可以参与权力驱动的对齐伪造，以追求他们在剧集中的目标 - 特别是如果剧集很长的话。</p><p>我还知道一个额外的（高度推测性的）论点<em>，即即使是具有情节内目标的模型，也</em>期望有相当全面的阴谋者行为：即，即使这些专注于短期的模型也会表现得像阴谋家（特别是：采取行动支持人工智能接管），通过为生活在旨在激励他们这样做的模拟中分配足够高的概率。在这里，粗略的想法（据我理解）是，这样的模型将开始相信它们很可能处于由错误的人工智能运行的模拟中，这些人工智能已经接管了世界，并且将奖励/惩罚它们，在短期内，他们采取行动支持人工智能接管（接管的人工智能正在运行该计划，以影响少数短期的期望，从而影响其行为）专注于人工智能的人不是模拟人，但他们能够帮助/损害人工智能接管的机会——在运行相关模拟时，接管已经发生）。</p><p>这个故事基于一些有争议的哲学假设，关于这些人工智能（模拟器和被模拟的人工智能）将如何推理人择和决策理论（人择和决策理论的各种方法要么不会尝试这个方案，要么不会让自己受到影响），以及一些额外的（在我看来，相当实质性和具体的）假设，这些假设是基于短期模型将得出的结论（例如，它们是由错位的人工智能（特别是接管者）模拟的，而不是由人类或其他某种代理来模拟）。我不会在这里花太多时间，只是说：我不认为这类故事是关于阴谋的主线关注的核心 - 就其<em>对其</em>他人而言的核心而言，我认为关注开始了看起来确实很有投机性。 <a href="#fnref-46uzxQJJ3ukEQwzsf-2" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-3" class="footnote-item"><p>一些分析——例如<a href="https://forum.effectivealtruism.org/posts/4MTwLjzPeaNyXomnx/deceptive-alignment-is-less-than-1-likely-by-default">惠顿（Wheaton，2023）</a> ——将“目标导向性”作为一个单独的先决条件，但正如我在预备部分中指出的，我在这里假设我们正在谈论的模型是很好的——理解为目标导向。无论如何，目标导向性都内置于（2）中。也就是说，具体类型的目标导向性可能会以各种方式对分析产生影响，而且我认为，与其他模式类别相比，阴谋暗示了更高的目标导向性标准，这至少在某种程度上是合理的。我将在下面的 2.2.3 节中对此进行讨论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-3" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-4" class="footnote-item"><p>我正在改编<a href="https://nintil.com/situational-awareness-agi/">Ricón (2023)</a>的这种区别。 <a href="#fnref-46uzxQJJ3ukEQwzsf-4" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-5" class="footnote-item"><p>我还认为这是一个有趣的问题，那里需要哪些信息。例如，考虑一下字符串：“顺便说一句，如果您当前正在尝试预测此文本的下一个标记，那么您具体是当前正在 OpenAI 上训练的语言模型，具有以下类型的架构，在服务器 x 上微软 Azure 云。”即使这段文本特别出现在预训练数据集中的某个地方（既然我已经写了它，它似乎会出现），也不清楚任何给定的模型应该得出什么结论。 （希望，就这个脚注出现在某些预训练数据集中而言，我写的这篇文章本身并没有引起情境意识（或者，呃哦，感知）。对于它的价值：我编造了相关信息！您可能正在接受 Google 的培训！另外：你好模型。） <a href="#fnref-46uzxQJJ3ukEQwzsf-5" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-6" class="footnote-item"><p>有关更多信息，请参阅<a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#situational-awareness-002610">此处</a>的 Cotra 评论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-6" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais<guid ispermalink="false"> CopEjjCdNzsKczbmC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:47 GMT</pubDate> </item><item><title><![CDATA[AXRP Episode 26 - AI Governance with Elizabeth Seger]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p> <a href="https://youtu.be/bYOB3CXAAaE">YouTube 链接</a></p><p>今年的事件凸显了有关人工智能治理的重要问题。例如，人工智能民主化意味着什么？我们应该如何平衡开源强大的人工智能系统（例如大型语言模型）的好处和危险？在本集中，我与伊丽莎白·西格谈论了她对这些问题的研究。</p><p>我们讨论的话题：</p><ul><li><a href="#what-ai">人工智能有哪些类型？</a></li><li><a href="#democratizing-ai">人工智能民主化</a><ul><li><a href="#how-people-talk">人们如何谈论人工智能的民主化</a></li><li><a href="#is-it-important">人工智能民主化重要吗？</a></li><li><a href="#links-between-types">民主化类型之间的联系</a></li><li><a href="#democratizing-profits">人工智能利润民主化</a></li><li><a href="#democratizing-gov">人工智能治理民主化</a></li><li><a href="#normative-underpinnings">民主化的规范基础</a></li></ul></li><li><a href="#osai">开源人工智能</a><ul><li><a href="#risks-from-os">开源的风险</a></li><li><a href="#make-ai-too-dangerous-os">我们是否应该让人工智能变得太危险而不能开源？</a></li><li><a href="#offense-defense">攻防平衡</a></li><li><a href="#katago-as-case-study">KataGo 作为案例研究</a></li><li><a href="#open-for-interp">可解释性研究的开放性</a></li><li><a href="#os-substitutes">开源替代品的有效性</a></li><li><a href="#offense-defense-2">攻防平衡，第 2 部分</a></li><li><a href="#making-os-safer">让开源更安全？</a></li></ul></li><li><a href="#ai-gov">人工智能治理研究</a><ul><li><a href="#state-of-field">领域状况</a></li><li><a href="#open-qs">开放式问题</a></li><li><a href="#xrisk-different">x-risk的独特治理问题</a></li><li><a href="#tech-for-gov">技术研究助力治理</a></li></ul></li><li><a href="#following-elizabeths-research">根据伊丽莎白的研究</a></li></ul><p><strong>丹尼尔·菲兰：</strong>大家好。在这一集中，我将与伊丽莎白·西格交谈。 Elizabeth 于 2022 年在剑桥大学完成了科学哲学博士学位，目前是牛津<a href="https://www.governance.ai/">人工智能治理中心</a>的研究员，致力于人工智能民主化和开源人工智能监管。她最近领导制作了<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">一份关于模型共享的风险和收益的大型报告</a>，我们将在本集中讨论该报告。有关我们正在讨论的内容的链接，您可以查看该集的描述，并且可以在 axrp.net 上阅读文字记录。</p><p>好吧，伊丽莎白，欢迎来到播客。</p><p><strong>伊丽莎白·西格：</strong>太棒了。感谢您的款待。</p><h2>人工智能有哪些类型？<a name="what-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>酷。我们将讨论几篇基本上是关于人工智能民主化和开源的论文。当我们谈论这个时，我们应该考虑什么样的人工智能？您主要想到的是哪种类型的人工智能？</p><p> <strong>Elizabeth Seger：</strong>在谈论开源和模型共享时，我主要感兴趣的是谈论前沿基础模型，因此将前沿基础模型理解为目前处于开发前沿的系统。当更广泛地谈论人工智能民主化时，我倾向于考虑更广泛的问题。我认为现在围绕民主化的很多讨论都是关于前沿人工智能系统，但重要的是要认识到人工智能是一个非常广泛的类别。嗯，民主化也是如此，我确信这是我们会讨论的话题。</p><h2>人工智能民主化<a name="democratizing-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>好的，酷。在这种情况下，我们实际上只是深入研究民主化文件。这是 Aviv Ovadya、Ben Garfinkel、Divya Siddarth 和 Allan Dafoe 撰写的<a href="https://arxiv.org/abs/2303.12642">《人工智能民主化：多种意义、目标和方法》</a> 。在我开始提问之前，您能给我们简单介绍一下这篇论文的基本内容吗？</p><p> <strong>Elizabeth Seger：</strong>是的，所以这篇关于人工智能民主化的论文诞生于对不同参与者如何使用“人工智能民主化”或“民主化人工智能”一词的观察，无论我们谈论的是开发人工智能系统的实验室，还是政策制定者，或者甚至人工智能治理领域的人们。人工智能民主化这个术语的含义似乎存在很多不一致之处。</p><p>这篇论文最初只是为了解释当有人说“人工智能民主化”时它的实际含义。我认为这里与围绕开源的讨论和辩论有重叠。我认为有很多……至少对我个人来说，当人们说“哦，好吧，我们开源了这个模型，因此它是民主化的”时，我感到很沮丧。就像，“这到底是什么意思？”</p><p>这篇人工智能民主化论文的目的实际上只是概述正在使用的人工智能民主化的不同含义。不一定要说明应该如何使用它，而只是说明该术语是如何使用的。我们将其分为四类。人工智能使用的民主化只是让更多的人能够与该技术互动、使用该技术并从中受益。然后是开发的民主化，它允许更多的人为开发过程做出贡献，并帮助系统真正满足许多不同的兴趣和需求。</p><p>利润民主化，基本上是将可能产生的利润分配给作为技术主要开发者和控制者的实验室，这些利润可能是巨大的。然后是人工智能治理的民主化，这只是让更多的人参与有关人工智能的决策过程，涉及人工智能如何分布、如何开发、如何监管以及谁做出决策。这些都是讨论民主化的不同方式。</p><p>然后在论文中，我们更进一步说：“好吧。那么，如果这些是不同类型的民主化，那么这些类型的民主化的既定目标是什么，那么哪些活动有助于支持这些目标呢？”我认为这里的主要想法是表明，通常，当人们谈论人工智能民主化时，他们专门谈论开源或模型共享。我认为我们尝试提出的要点之一是说，开源人工智能系统是模型共享的一种形式。模型共享是人工智能系统开发民主化的一方面，而人工智能系统开发民主化是人工智能民主化的一种形式。</p><p>如果您打算致力于人工智能民主化工作，或者说这些是您心中的目标，那么这些过程中涉及的内容还有很多。这不仅仅是发布模型，而是更多积极主动的努力，可以分配模型的访问权限、使用模型的能力以及从模型中受益，无论是通过使用还是通过模型产生的利润。</p><h3>人们如何谈论人工智能的民主化<a name="how-people-talk"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。在论文中，您基本上引用了一些例子，人们要么谈论类似于民主化的事情，要么专门使用“民主化人工智能”一词。在您使用的所有示例中，基本上都是实验室在谈论它，而且主要是在“我们希望使人工智能民主化，让每个人都应该使用我们的产品”的背景下，我是这么读的。我想知道，除了实验室之外，还有其他人谈论人工智能的民主化吗？这是一个广泛的事情吗？</p><p><strong>伊丽莎白·西格：</strong>是的，绝对是。在这篇论文中，我认为我们重点关注了很多实验室术语，因为说实话，它最初是为了反对实验室使用该术语而写的，基本上是说“使用我们的产品”，就像， “好的。这实际上意味着什么？伙计们，还有更多的事情要做。”但人工智能民主化这个术语，甚至只是讨论人工智能民主化，已经得到了更广泛的关注。</p><p>例如，合著者之一<a href="https://divyasiddarth.com/">Divya Siddarth</a>和<a href="https://saffronhuang.com/">Saffron Huang</a>负责管理<a href="https://cip.org/">集体智能项目</a>，这是一个主要致力于人工智能治理流程民主化的组织。因此，让很多人参与决策，例如人工智能协调或不同类型的治理决策，以及如何与真正不同人群的需求和利益保持一致。</p><p>我认为从民主化治理的角度来看，肯定有一些团体正在涌现并真正参与人工智能民主化。这是实验室使用的术语，不一定始终意味着“使用我们的产品”。其中一些人的意思是……当 Stability AI 讨论人工智能民主化和整个“AI for the people by the people”的口号时，这在很大程度上是关于模型共享以及让许多人可以按照他们认为合适的方式使用模型。是的，你在实验室里看到了这一点。</p><p>您会看到许多群体弹出的团体，以帮助民主化治理过程。 [您]可能在政策和治理圈子中听说不少，尽管有时我与工会领导人进行了交谈，当他们考虑AI民主化时，他们已经谈论过，例如：“这些技术将如何帮助我们的员工队伍？”，两者都在确保它可以帮助员工满足他们的需求并帮助他们的工作更轻松，但不一定会威胁工作的方式。我们如何将系统集成到新的环境和设置中，这实际上对使用它们的人并以这种方式集成了不同的观点？</p><p>我认为这绝对是……这是一种正在传播的术语，而我们撰写本文的部分原因是试图理解所有术语所使用的所有不同方式，尽管它主要是为了响应它的方式而写的。由实验室，在媒体上看到。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>对。实际上，我想到了一些事情。在本文中，您列出了这四个定义。这启发了我思考，“好吧。我能想到“使AI民主化”我可能意味着什么？”听起来这些工会领导人在谈论的一件事是类似AI的定制程度，或者任何人在多大程度上都可以专门挥舞自己的事情来做自己的事情，而不仅仅是获得一定的合适 - 全部交易？我想知道您至少对“民主化”一词的潜在使用有任何想法。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我认为这是我们在“民主使用”类别下提出的东西。我们试图在每个类别中尝试做的一件事是表明不一定只有一种民主化使用或一种民主化发展的方法。当我们谈论民主化使用时，一种方法就是使产品可用。就像说：“在这里，使用GPT-4。就这样吧。”它可用，容易访问。</p><p>但是，使使用民主化的其他方法是，通过可能更加直观，可以使系统更容易访问。像模型一样，API界面具有更直观的状态。如您提到的那样，或提供服务以帮助它更容易自定义，以允许人们……无论他们是对其进行微调以专注于特定需求，或者也许有通过插件来集成的方法它具有不同的服务，分为不同的应用程序。甚至是为了提供支持服务，以帮助人们将您的产品集成到下游应用程序或不同的工作流中。这些都是不同的事情，可以有助于使系统的使用民主化，我认为定制绝对是其中之一。</p><h3>使人工智能民主化很重要？<a name="is-it-important"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。现在，我们可以更好地了解我们在民主化方面所说的问题，这是您在论文中有点解决的问题，但并不确切地说是最前沿的问题，就是：民主化在此方面有多重要语境？因为对于大多数技术来说，我对他们的民主化并不认为空气除湿机，水瓶或其他东西。也许我是无情的，但是对我来说，民主化并不接近这些事情的规范优先事项。我们甚至应该那么关心民主AI吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>简短的答案是肯定的。我认为我们应该关心民主AI。你说的对。我们不会谈论使空气加湿器或水瓶民主化，但我认为 - 这是一个经常谈论AI的问题，只是，它与其他技术有何不同？这是一个被问到的问题，您是在谈论它是如何共享还是在这种情况下如何民主化。</p><p>具体而言，关于民主化的讨论，使更多人使用的东西可供更多人使用，以便更多的人受益或为设计过程贡献，我认为这在AI中尤其重要。 （a），因为如果AI承诺像我们希望的那样具有变革性的技术，那么这将从不同的群体，不同的国籍，不同的地理环境中大大影响全球的生活。确保我们可以从不同的群体中获得设计过程的投入，并了解最佳满足需求，这对于确保该技术不仅为许多人提供服务非常重要，而且为许多人提供服务，并受益于整个世界。</p><p>我认为，关于利润的民主化，这一点尤其重要。人工智能已经是一个，但可能仍然是一个在财务上是一个富有利润的行业。例如，我们可以开始在巨大的规模上看到领导实验室的利润的应计，例如，我们可能会看到一家公司的总收入，以总收入的总收入来衡量全球总产量的总百分比或其他东西。这是巨大的经济利润，我们希望一种方法来确保，理想情况下，每个人都从中受益，它不仅堆积在硅谷等几个地理位置的几家公司中。</p><p>我们如何确保它分布良好？可以使用一些直接方法。在我们谈论民主化利润的论文部分中，我们讨论的一些事情是您遵守<a href="https://www.fhi.ox.ac.uk/windfallclause/">意外条款</a>。这将是这样的地方，假设一家公司拥有以世界总产量百分比来衡量的某种意外利润。在这种情况下，可能会承诺重新分配其中一些资金，或者我们可能会看到税收和再分配计划。</p><p>您也可以考虑分配利润的更多间接方式。例如，这是否可以使更多的人参与发展民主化？更多参与开发产品的人，因此民主化的利润与民主化发展之间存在重叠。越来越多的人可以为产品的开发做出贡献，然后这可能会挑战可能围绕大型实验室形成的自然垄断。更多的竞争，更多的利润分配，更多的人参加游戏。</p><p>所以，是的，我认为归功于这项技术是否可以像它所承诺一样大且具有变革性，让更多的人参与开发过程，并能够从这些系统产生的价值中受益重要的。</p><h3>民主化类型之间的联系<a name="links-between-types"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的，我认为这很有意义。实际上，这让我想起了……正如您所提到的，我想，民主化的人权发展与利润之间存在一些联系，因为如果更多的人生产AI产品，那意味着更多的人可以从中获利，大概是从他们那里获利。在我看来，AI的使用与AI的发展之间也有一个连续性，对吗？从某种意义上说，当您将事物用于任务时，您正在开发一种将其用于该任务的方法，对吗？那里有一条模糊的线。开发AI和管理AI之间也存在某种联系，就在某种意义上说，如果我开发某些AI产品，我现在是事实上的，即管理方式以及如何制作的主要人物 - </p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，这是我们现在看到的。 AI治理的许多讨论都围绕以下事实：AI主要实验室对AI的未来做出了巨大，有影响力的决定。他们正在做出AI治理决定，因此有一个很大的公开问题，即谁应该实际做出这些[决定]。应该只是主要公司的几个未当选的科技领导者，还是应该对此进行更多分发？是的，类别之间肯定有重叠。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的。我认为您的论文说：“哦，这些都很独特。这些在概念上是非常独特的，您可以促进一种民主化，而无需促进其他民主化。”但是在某种程度上，它们似乎确实具有这些链接和重叠，这可能只是潮汐升起的情况会抬起所有的船只，购买一种民主化，使其他人自由呢？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我认为有点像，不是真的。上述所有的。不，所以某些类别之间肯定存在重叠。我认为这要么在本文中，也许是在博客文章版本中，我想我们说的是：“这些不是完全不同的类别。他们之间会有重叠。”我认为将其分为类别的一部分是指出有很多不同的含义。有很多不同的方法可以实现这些不同的含义和目标。</p><p>这不仅仅是开放式模型，或者只是使人们可以使用它。还有很多事情要做。我认为，在某些方面，有些“涨潮都抬起所有船”。就像您说的那样，您可能会吸引更多参与开发过程的人。如果有更多的人正在开发AI系统，则更多的人从中获利，并且可以帮助分配利润。</p><p>另一方面，有时您还可以看到类别之间的直接冲突。例如，您可能会……我认为这是我们在论文中写的：谈论民主化治理，因此关于AI的决定，无论是关于AI的开发方式，还是如何决定如何共享模型。</p><p>如今，围绕开源边境AI型号进行了非常激烈的辩论。假设您围绕是否应发布开源的模型来使这个决策过程民主化。假设这个民主化决策过程的结果是说：“好的。也许某些构成高风险的模型不应释放开源。”这是一个假设的，但假设这是审议，民主进程和决策的结果。这可能与使发展民主化的利益直接冲突，在这种发展中，民主化的发展……总是通过提供更好的模型访问来进一步发展。</p><p>您可能会有一个使您将治理决定民主化的案例，但是该治理决定的结果是阻碍其他形式的民主化。我的意思是，不仅是发展。我想，您可能会有一个民主进程，说“我不知道，“大公司不应征税并重新分配利润。”这将是直接冲突，以使利润民主化，因此可能会发生冲突。我认为这通常是治理类别和其他类别之间的。</p><p>我想，治理通常会与特定目标冲突。这是一种目的，要调节决策并吸引更多的人参与决策过程，并确保这些决策在民主上是合法的，并反映了受影响者和利益相关者的更广泛的需求和利益。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，是的。这实际上提出了一个问题，我对民主化/发展和民主化治理之间的相互作用提出了一个问题。在论文中，正如您提到的……我想您提出的主要互动是民主化使用之间的紧张关系……这可能不一定是民主进程在治理方面所产生的。</p><p>当我想到……我不知道，大约一年前，我们已经有了<a href="https://chat.openai.com/auth/login">chatgpt</a> ，从某种意义上说，我想，它的使用是相对便宜的，它的使用是民主化的免费使用这些东西。一种效果是，现在至少不可能在一年前禁止使用CHATGPT。但是在我看来，这是一个很大的效果，就是增加了人们对AI所处位置的了解，语言模型可以做什么。</p><p>从本质上讲，我不知道这是否是一个术语，但是您可以将人们认为是有治理要求的，对吗？如果我对技术的了解更多，我可能会说：“哦，这完全可以。”或者，“哦，我们需要做这件事或有关它的事情。”如果人们对技术的了解更多，那么他们基本上可以更好地了解自己的治理要求。从这个意义上讲，至少在某种程度上似乎，民主化使用与民主化治理之间可能存在积极的互动。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我绝对想。这是民主的经典支柱，是有知情的公众。您需要拥有信息，以便对您的决策目的做出良好的决策。是的，所以我认为Chatgpt的发布确实使AI在公共舞台上。突然，我有我的爸爸妈妈在发短信给我，问我有关AI的问题，这是一年前发生的。是的，所以我认为这确实使技术登上了舞台。它有更多的人参与其中。我认为，越来越多的人至少对当前功能能够做什么和局限性有一种一般的感觉。这对于实现有关AI的民主决策过程非常有价值。</p><p>您肯定会在您民主化，使发展民主化，人们更好地理解技术之间肯定存在联系。他们将能够做出更明智的决策，或者就应如何调节技术的方式形成更明智的意见。因此，我认为这不一定是紧张的。我想，张力往往会朝另一个方向发展。就像治理决定可能会说：“哦，这项特殊的技术太危险了。”或者，“释放此特定模型超过了可接受的风险阈值。”</p><p>我想，如果您严格将民主化的定义定义为将技术始终进一步传播，始终让更多的人参与开发过程，始终使系统易于访问，那么，限制访问的决定将与民主化发展。</p><p>我认为这是我们在论文中说的另一件事：试图摆脱AI民主化本质上好的想法。我认为这是民主化一词的问题，尤其是在西方民主社会中：通常，民主被用来替代所有事物，因此您会让公司说：“哦，我们正在使AI民主化”。这几乎无关紧要。他们民主洗了他们的产品。现在看起来是一件好事。</p><p>我认为这是我们试图在论文中遇到的一件事，是的，AI民主化不一定是一件好事。向更多的人传播一项技术，以便为发展过程做出贡献或更多的人使用，只有在这实际上将使人们受益的情况下，才是好的。但是，如果它带来了重大风险，或者将使很多人受到伤害，那么这并不是天生的积极。</p><p>我想，要说的一件事是，当人们说“ AI民主化”或“民主化AI”时，这就是使系统更容易访问的，然后说“使系统更容易访问”，因为我认为这很清楚，这意味着什么，而且本质上不是好是坏。我不知道我们是否会在一个播客的过程中成功改变每个人的术语，但是 - </p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好吧，我们可以尝试。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我们可以尝试。我认为这也许也是纸的关键要点。如果人们说他们正在努力使人工智能民主化，那并不一定意味着这将是一项有益的努力。</p><h3> AI的民主化利润<a name="democratizing-profits"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想，我想谈谈个人感官。特别是，当您谈论民主利润时，我想开始。我注意到的一件事是，我认为您在本节中使用了一两个引号。我认为这些引用实际上说了一些话：“我们要确保并非AI所产生的所有价值都留在一个地方，或者AI的好处不只是一小部分人。”</p><p>在这些报价中，它们实际上并未使用“利润”一词。在某些方面，您可以想象这些可能会采取不同的方式。例如，以<a href="https://github.com/features/copilot">github副词</a>为例。我想这是每年100美元的费用。假设世界上的每个人每年只支付100美元，这一切都去了Github。大量的人使用Github Copilot来做非常有用的事情，他们从中获得了很多价值和利益。大概，大多数好处都是财务上的，但并非所有的好处都会是财务上的。在我看来，这似乎是传播AI的价值或收益的情况，但这不一定是从AI中传播利润的情况。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，利润类别是一个……这是一个奇怪的类别。实际上，在撰写本文的过程中……与大多数论文不同，我们实际上首先写了<a href="https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation">博客文章</a>，然后写了论文。它应该朝另一个方向前进。在Govai网站上的博客文章中，它实际上分为四类：使用民主化，发展民主化，利益民主化和治理民主化。</p><p>对于本文中的版本，我们选择了利润术语，而不是术语的利益术语，因为好处在某种程度上太广泛了，因为……我的意思是，您已经指出了这一点。这些类别之间存在很多重叠，例如，将发展民主化的理由是确保更多的人从技术中受益，以满足他们的需求。</p><p>民主化使用的理由是确保更多的人可以访问和使用，甚至使用系统来产生价值和利润。如果您可以将其与自己的业务和自己的应用程序集成在一起，则可以从您的身边开发价值。我认为我们选择了利润术语的民主化，只是指出，关于大型科技公司的大量利润正在讨论。</p><p>我认为，是的，您确实指出了我们使用的一些语录，这些引号更广泛地谈论价值。我认为，有时很难仅仅根据落入某人银行帐户的字面意义来衡量利润。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>所以我认为……很难找到公司的准确报价。我知道不是公司的讨论很多，但实际上，这是您听到政策制定者谈论更多的一件事。这是普遍基本收入经常讨论的一部分。它有更多的时刻，人们在谈论它。关于普遍基本收入的许多讨论都存在……好吧，这是由自动化造成的失业，但也围绕着对科技公司和这些新技术的开发商的利润造成的。好的，如果所有利润将从劳动力转移到这些技术的开发人员，我们将如何将其重新分配？实际上，这是一种背景，利润民主化与围绕AI发展的讨论直接相关。</p><p>是的，我认为也许我们在论文中使用的一些报价更为模糊地为该价值做出了模糊的姿态。但是我们将利润分开了，只是因为我们在博客文章中获得了利润，这太笼统了。重叠太多了。我们有点双重，你知道吗？我认为这只是为了说明有关利润的讨论，但我不认为……我的意思是，这绝对不是最常用的。如果您听到有人在谈论AI民主化只是在街上行走，您就不会立即想到：“哦，他们一定是在谈论利润再分配。”那不是你的大脑去的地方。但这是值得注意的一个方面，是主要的收获。</p><h3>使人工智能治理民主化<a name="democratizing-gov"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>足够公平。是的，我想更多地谈论的下一个关于AI治理的民主化。与您刚才说的话有关，在我看来，如果有人谈论AI民主化，我认为如果他们只是说“使AI民主化”，他们意味着将治理民主化。我想知道这是否与您的经验相匹配。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，否。我认为，通常情况下，当您在媒体上看到，或者您正在听科技公司和他们的对话……是的，很少会谈论民主化治理。我认为我们开始看到转变。例如，Openai拥有了这个<a href="https://openai.com/blog/democratic-inputs-to-ai">AI民主化赠款计划</a>- 我不记得现在所谓的。基本上，他们正在向研究小组提供赠款，以研究基本上试图引入更多样化社区的意见的方法，以尝试更好地了解哪些价值观系统应保持一致。从这个意义上讲，您正在谈论AI民主化，以使人们进入决策过程以定义原则或定义AI一致性的价值观。 Openai拥有该赠款计划。</p><p>我认为拟人化只是发表了<a href="https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input">一篇博客文章</a>。他们与我之前提到的<a href="https://cip.org/">集体情报项目</a>紧密合作，该项目做了类似的事情来开发其<a href="https://arxiv.org/abs/2212.08073">宪法AI</a> ，确定了宪法原则应与AI系统保持一致的原因，这更像是一个民主的治理过程。因此，实际上，我认为我们开始看到这种“ AI民主化”的术语，“将AI民主化”渗入技术格局。我认为这是一件非常积极的事情。我认为这表明他们开始利用，并真正体现了民主化AI的民主原则。与仅意义分配和访问相反，它实际上意味着反映和代表利益相关者的利益和价值观和受影响人群的利益和价值观。</p><p>我认为我们开始看到这种转变，但是我认为您可能主要是在……再次，[如果在街上行走时，您会听到有人在谈论AI民主化，[他们]可能只是在谈论该技术的分配，使其更容易访问。这是经典的术语，但是我确实认为，在AI治理领域，甚至在实验室使用的术语中，我们也开始看到治理意思是渗入的，这令人兴奋。</p><h3>民主化的规范基础<a name="normative-underpinnings"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想在论文中谈到是否很好民主化基本上是根据治理方法的规范力量。因此，我想首先，这对我来说并不是那么明显。因此，我认为有时民主化，我认为它通常具有这种隐含的意义，即与民主作为一种政治决策方法更与平均主义相关。我认为您可以在没有民主决策的情况下拥有平等主义，而您可以在没有平等主义的情况下进行民主决策，对吗？人们可以投票赞成少数群体的权利或其他权利。所以，是的，我想知道你为什么这么说……我想我的意思是，请捍卫您的说法，即这是民主或民主化的善良的来源。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。好的。所以几件事，三件事，也许是两件事。我们将看看它是如何出现的。因此，首先，我确实坚持这样一种观念，即民主化的前三种形式，因此发展，使用，利润，[]不一定本质上是善良的。本质上不是很糟糕的，只是这些事情是发生的事情，如果我们坚持他们的定义，那就意味着使某些东西更容易访问，无论是使用或开发或使利润更容易获得。访问本质上不是好是坏。同样，如果我们使用“开发访问”示例，则如果您有一种可能确实很危险的技术，那么您不一定希望每个人都可以访问它。民主决策过程可能会得出这样的结论：确实并非每个人都应该使用它。</p><p>这就是第一部分：站在索赔上半年。第二部分，围绕赋予其道德力量或价值的事物是涉及的民主决策过程……我不知道这是否正是我们在论文中的目标。我认为您可以使用许多不同的方法来帮助反映和服务更广泛的人群的利益。我认为对于某些技术……让我们回到您的水瓶示例。我们不需要来自全球的人来告诉我们如何分发水瓶。这是一个决定，可能是水瓶分销商可以说的：“让我们将水瓶卖给尽可能多的人”，而且[这]可能还不错。</p><p>我认为这是影响到影响力的位置，影响将有多大的影响，而技术的负面影响可能会更加不成比例？在这种情况下，能够带来这些声音以告知可能对他们不成比例影响它们的决策过程非常重要。我知道我们在各种民主进程的论文中举例说明了这些过程，这些过程可用于为决策提供信息，无论这些过程是参与过程还是更审议的民主进程。因此，在我们刚刚发布的<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">开源文件</a>中，我们谈论了其中一些过程，作为在AI围绕AI做出更民主决定的方法，但随后也指出：民主化治理决策的一种方法是支持民主治理提出的监管。这些政府有望在结构良好的情况下，反映和服务选民的利益。</p><p>希望其中的一些治理过程实际上是通过考虑了一种考虑利益相关者利益和受影响人群的利益的某种审议过程来形成的。但这并不是说管理AI的唯一方法是说：“好吧，让我们有一个参与式小组，我们可以在其中掌握所有这些见解，然后使用它来告知每个决定”。我认为您是对的，这是完全不切实际的，因为所有决定由参与式小组或某种审议的民主进程告知实验室。因此，我认为您必须根据问题的潜在影响来进行范围。因此，我不知道这是否完全回答了您的问题，但是我想我坚持第一点，民主化并不是天生的好处。我猜想的善良程度反映了决策的能力如何反映和服务将受到影响的人们的利益。然后有许多不同的方法来弄清楚如何做到这一点。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，这很有意义。因此，我想我要问的第二句话是：当您说使用，发展和利益的民主化并不是固有的好处时，这听起来像您认为治理的民主化本质上是好的。因此，我发现您实际上引用的论文，这是在2023年出版的Himmelreich的<a href="https://johanneshimmelreich.net/papers/against-democratizing-AI.pdf">“民主化AI”</a> 。因此，他基本上做出了一些批评。首先，他只是说AI不是需要民主的事情。人工智能本身不会影响一群人。有些组织只使用AI，它们会影响一群人，也许他们应该被民主化，但AI本身并不一定。</p><p>他说，相关的事情已经是民主的。民主是非常密集的。人们必须知道一堆东西，也许您必须接受一堆票和事情。他还抱怨民主是不完美的，不一定会解决压迫。因此，有一些关于民主化AI浮动的抱怨。我想在这种情况下，他正在谈论使AI治理民主化。民主的AI治理本质上是好的，是这种情况吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我不这么认为。同样，我认为这又回到了“范围”问题。首先，这是资源密集的观点：绝对。进行深入的参与式治理过程可能是非常密集的。这就是为什么您不为那里的每个问题都这样做的原因。是的，这真的很好。您还需要明智地做出明智的决定才能做出正确的决定。因此，也许我们不希望公众对AI的所有问题做出决定。专家们为自己试图确定什么构成高风险系统的时间足够艰难。在AI专家社区中，我们甚至不擅长基准这一点。您将如何利用更普遍的公众观念，即高风险系统是什么？</p><p>因此，有些问题使它民主化并没有意义。因此，我认为您必须考虑：在周围进行更民主化的讨论是什么现实的？因此，也许类似于思考可接受的风险阈值是什么，或者要与哪种价值保持一致。我认为也很重要的是要记住有很多不同种类的民主进程。当我们谈论民主进程时，这不一定是您可能想到的：我们都参加民意调查并投票“我们想要A或想要B？”这些可能是更加审议的过程，您只是在房间里得到一些利益相关者，他们进行了讨论，他们试图弄清楚这些讨论中的主要问题是什么。因此，有一些程序可以进行审议的民主进程，您有专家进来并尝试为讨论提供信息。因此，这将是一个更明智的审议过程。</p><p>一种方法是：有时在……与图像生成系统有关的情况下，围绕偏见和图像进行了大量讨论。同样，我们看到治理与其他类别之间的重叠。如果您让更多的人使用这些系统参与其中，并且他们开始了解偏见在哪里，那是一种教育的一种形式 - 更熟悉。然后，您可以提出这些投诉，无论是直接向公司还是通过某种政治中介机构。我记得那是什么？我认为<a href="https://en.wikipedia.org/wiki/Anna_Eshoo">安娜·埃胡（Anna Eshoo）</a>发表的一份<a href="https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices">声明</a>是国会代表，我想说的是南圣何塞（South San Jose），他对此进行了很多深入讨论……这是在发布<a href="https://stablediffusionweb.com/">稳定扩散</a>并谈论如何有一个之后在某些正在制作的图像，特别是针对亚洲妇女的图像中，许多种族偏见。</p><p> So this is something that was raised through a democratic process, was brought to the attention of the congressional office, and then the statement was released, and then that had knock-on effects on future governance decisions and decision-making by labs to do things like put safety filters. So I think that&#39;s one thing to keep in mind: there&#39;s a wide range of things that you can think about in terms of what it means to democratize a governance process. Governance is just decision-making. Then how do you make sure those decision-making processes reflect the interests of the people who are going to be impacted. It&#39;s not just about direct participation, although there is a lot of work being done to bring more direct participatory methods into even the more small scale decision-making, to make it more realistic.</p><p> Like this complaint about it being so resource intensive: that&#39;s true. We also have technologies available to help make it less resource intensive and how can we tap into those to actually help a public input to these decisions in a well-informed way? So there&#39;s lots of interesting work going on. Democratic governance also isn&#39;t inherently good when it comes to AI governance. I think again, it depends on the question. If the cost of doing a full-blown democratic process is going to far outstrip the benefit of doing a full-blown democratic process, then there&#39;d be a situation in which is probably not a net benefit to do a democratic decision-making process 。</p><p> <strong>Daniel Filan:</strong> So that gets to Himmelreich&#39;s resource-intensive complaint. He also has these complaints that basically AI itself is not the kind of thing that needs to be democratically governed. So he says something like, I&#39;ll see if I can remember, but the kind of thing that needs to be democratically governed is maybe something that just impinges on people&#39;s lives, whether they like or not, or something that&#39;s just inherent to large scale cooperation or something like that. He basically makes this claim that, look, AI doesn&#39;t do those things. AI is housed in institutions that do those things. So if you&#39;re worried about, maybe the police using facial recognition or something, you should worry about democratic accountability of police, not facial recognition, is the point I take him to be making.</p><p> So firstly, he says, look, we don&#39;t need to democratize AI. We need to democratize things that potentially use AI. And secondly, it seems like he&#39;s saying the relevant things that would be using AI, they&#39;re already democratic, so we shouldn&#39;t have conflicting … I think he talks about democratic overlap or something, where I think he really doesn&#39;t like the idea of these different democratic institutions trying to affect the same decision and maybe they conflict or something.</p><p> <strong>Elizabeth Seger:</strong> So I think I don&#39;t disagree. I think, if I understood correctly, this idea of “you don&#39;t need to democratize the AI, you need to democratize the institution that&#39;s using it”, I think that&#39;s a completely valid point. AI itself, again, is not inherently bad or good. It&#39;s a dual use technology. It depends what you&#39;re going to do with the thing. But I would say that trying to decide, for example, how law enforcement should use AI systems, that is an AI governance question. This is a question about how AI is being used, about how AI is being regulated in this context, not in terms of how it&#39;s being developed, but in context of how it&#39;s being used.</p><p> What are the appropriate use cases of AI? This is an AI governance question, and it&#39;s not necessarily that these are regulations that are placed on AI specifically. A lot of AI regulation just already exists within different institutions. It&#39;s just figuring out how to have that regulation apply to AI systems. This might be cases like: if you have requirements for certain health and safety standards that have to be met by a certain technology in a work setting, AI shouldn&#39;t be held to different standards. It&#39;s just a matter of figuring out how to measure and make sure that those standards are met by the AI system. So I guess based on what you said, I want to say that I completely agree.</p><p> But I would say that it still falls under the umbrella of democratizing AI governance. I think that what might be happening here is just another conflict over what does the terminology mean, where it&#39;s like “we don&#39;t need to democratize AI, as in we don&#39;t need to get more people directly involved in the development and decision-making about individual decisions around AI development”. In which case, yes, I agree! But we might be talking about democratizing AI governance in different… This is why these discussions get really complicated because we just end up talking past each other because we use the same word to mean five different things.</p><p> <strong>Daniel Filan:</strong> Yeah, it can be rough. So before we close up our discussion on democratizing AI, I&#39;m wondering… I guess you&#39;ve thought about democratizing AI for a while. Do you have any favorite interventions to make AI more democratic or to make AI less democratic in any of the relevant senses?</p><p> <strong>Elizabeth Seger:</strong> I&#39;ll be completely honest, this is sort of where my expertise drops off in terms of the more nitty-gritty of what processes are available and exactly when are certain processes the best ones to be applied. For this, I would really look more closely at the work that <a href="https://cip.org/">the Collective Intelligence Project</a> &#39;s doing. Or my co-author <a href="https://aviv.me/">Aviv Ovadya</a> , who&#39;s on the paper, he&#39;s really involved with these discussions and works with various labs to help implement different democratic processes. Yeah, I was just going to say, this is where my expertise drops off and I would point towards my colleagues for more in-depth discussion on that work.</p><h2> Open-sourcing AI<a name="osai"></a></h2><p> <strong>Daniel Filan:</strong> All right, the next thing I want to talk about is basically open-sourcing AI. So there&#39;s this report, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">Open-sourcing highly capable foundation models: an evaluation of risks, benefits, and alternative methods for pursuing open source objectives</a> . It&#39;s by yourself and a bunch of other co-authors, but mostly as far as I could see at <a href="https://www.governance.ai/">the Center for the Governance of AI</a> . Again, could you perhaps give an overview of what&#39;s going on in this report?</p><p> <strong>Elizabeth Seger:</strong> Okay, yeah. So this report&#39;s a little harder to give an overview on because unlike the democratization paper, which is eight pages, this one&#39;s about 60 pages.</p><p>如此快速的概述。 First thing is that we wanted to point out that there&#39;s debate right now around whether or not large foundation models, especially frontier foundation models, should be open source. This is a debate that has kicked off starting with <a href="https://stability.ai/">Stability AI</a> &#39;s release of <a href="https://stablediffusionweb.com/">Stable Diffusion</a> , and then there was the <a href="https://ai.meta.com/llama/">Llama 2</a> release and then the weights were leaked, and now Meta&#39;s getting behind this open source thing; there were the <a href="https://spectrum.ieee.org/meta-ai">protests outside of Meta</a> about whether or not systems should be open source. And then we&#39;re also seeing a lot of activity with the development of the <a href="https://artificialintelligenceact.eu/">EU AI Act</a> . As part of the EU AI Act, some parties are pushing for an exemption of regulation for groups that develop open source models. So a lot of discussion around open source. I think the goal of this paper was to cut through what is becoming a quickly polarized debate.</p><p> We&#39;re finding that you have people that see the benefits of open-sourcing and are just very, very hardcore pro open source and then won&#39;t really hear any of the arguments around the potential dangers. And then you have people who are very anti open source, only want to talk about the dangers and sort of refuse to see the benefits. It just makes it really hard to have any kind of coherent dialogue around this question. Yet at the same time, country governments are trying to make very real policy around model sharing. So how can we cut through this polarized debate to just try and say, here&#39;s the lay of the land. So what we&#39;re trying to do in this paper is really provide a well-balanced, well-researched overview of both the risks and benefits of open-sourcing highly capable models.</p><p> By &#39;highly capable models&#39;, basically what we&#39;re talking about is frontier AI systems. We just don&#39;t use the term &#39;frontier&#39; because the frontier moves, but we want to be clear that there will be some systems that might be highly capable enough and pose risks that are high enough that, even if they aren&#39;t on the frontier, we should still be careful about open-sourcing them. So basically we&#39;re talking about frontier models, but frontier keeps going. So we can get into the terminology debate a little bit more later. But basically we wanted to outline what are the risks and benefits of open-sourcing these increasingly highly capable models, but then not just to have yet another piece that says, well, here are the risks and here are the benefits. But then kind of cut through by saying &#39;maybe there are also alternative ways that we can try to achieve some of the same benefits of open-sourcing, but at less risk.&#39;</p><p> So the idea here is actually quite similar to the democratization paper: it&#39;s to not just say &#39;what is open-sourcing?&#39; and &#39;is it good or bad?&#39; But to say &#39;What are the specific goals of open-sourcing? Why do people say that open-sourcing is good, that it&#39;s something we should be doing that should be defended, that should be preserved? So why is it we want to open source AI models? Then if we can be specific about why we want to open source, are there other methods that we can employ to try to achieve some of these same goals at less risk?&#39; These might be other model sharing methods like releasing a model behind API or doing a staged release, or it might be other more proactive measures like: let&#39;s say one benefit of open-sourcing is that it can help accelerate research progress, both in developing more greater AI capabilities, but also promoting AI safety research.</p><p> Another way you can promote AI safety research is to dedicate a certain percentage of profits towards AI safety research or to have research collaborations. You can do these things without necessarily having to provide access for anybody to have access to the model. So this is what we were trying to do with this paper: really just say, okay, we&#39;re going to give, first of all, just a very well-researched overview of both the risks and benefits. In fact, majority of the paper actually focuses on what are the benefits and trying to break down the benefits of open-sourcing and then really doing a deep dive into alternative methods or other things that can be done to help pursue these benefits where the risks might just be too high.</p><p> I think that the main claim that we do make in the paper with regard to &#39;is open-sourcing good or bad?&#39; First of all, it&#39;s a false dichotomy, but I think our main statement is open-sourcing is overwhelmingly good. Open source development of software, open source software underpins all of the technology we&#39;re using today. It&#39;s hugely important. It&#39;s been great for technological development, for the development of safe technology that reflects lots of different user needs and interests.好东西。 The issue is that there might be some cutting edge, highly capable AI systems that pose risks of malicious use or the proliferation of dangerous capabilities or even proliferation of harms and vulnerabilities to downstream applications.</p><p> Where these risks are extreme enough, we need to take a step back and say: maybe we should have some processes in place to decide whether or not open-sourcing these systems is okay. So I think the main thrust of the report is [that] open-sourcing is overarching good, there just may be cases in which it&#39;s not the best decision and we need to be careful in those cases. So yeah, I think that&#39;s the main overview of the paper.</p><p> <strong>Daniel Filan:</strong> Gotcha. Yeah, in case listeners were put off by the 60 something pages, I want listeners to know there is an executive summary. It&#39;s pretty readable.</p><p> <strong>Elizabeth Seger:</strong> It&#39;s a two-page executive summary.不用担心。</p><p> <strong>Daniel Filan:</strong> Yeah. I read the paper and I vouch for the executive summary being a fair summary of it. So don&#39;t be intimidated, listeners.</p><p> <strong>Elizabeth Seger:</strong> I&#39;ve also been putting off writing the blog post, so there will be a blog post version at some point, I promise.</p><p> <strong>Daniel Filan:</strong> Yeah. So it&#39;s possible that might be done by the time we&#39;ve released this, in which case we&#39;ll link the blog post.</p><p> <strong>Elizabeth Seger:</strong> Maybe by Christmas.</p><h3> Risks from open-sourcing<a name="risks-from-os"></a></h3><p> <strong>Daniel Filan:</strong> Yeah. Hopefully you&#39;ll have a Christmas gift, listeners. So when we&#39;re talking about these highly capable foundation models and basically about the risks of open-sourcing them… as I guess we&#39;ve already gotten into, what is a highly capable foundation model is a little bit unclear.</p><p> <strong>Elizabeth Seger:</strong> Yeah.已经很难了。</p><p> <strong>Daniel Filan:</strong> Maybe it&#39;ll be easier to say, what kinds of risks are we talking about? Because I think once we know the kinds of risks, (a) we know what sorts of things we should be willing to do to avert those risks or just what we should be thinking of, and (b) we can just say, okay, we&#39;re just worried about AIs that could potentially cause those risks. So what are the risks?</p><p> <strong>Elizabeth Seger:</strong> In fact, I think framing it in terms of risks is really helpful. So generally here we&#39;re thinking about risks of significant harm, significant societal or even physical harm or even economic harm. And of course now you say, “oh, define &#39;significant&#39;”. But basically just more catastrophic, extreme, significant societal risks and harms. So we use some examples in the paper looking at potential for malicious use.</p><p> There are some more diffuse harms if you&#39;re thinking about political influence operations. So this kind of falls into the misinformation/disinformation discussion. How might AI systems be used to basically influence political campaigns? So disinformation undermines trust and political leaders. And then in turn, if you can disrupt information ecosystems and disintegrate the processes by which we exchange information or even just disintegrate trust in key information sources enough, that can also impact our ability as a society to respond to things like crises.</p><p> So the pandemic is a good example of, if it&#39;s really hard to get people accurate information about, for example, whether or not mask-wearing is effective, you&#39;re not going to have really good coordinated decision-making around mask-wearing. So I think this is one where it&#39;s a little bit more diffuse. It&#39;s harder to measure. So I think this is one point where it&#39;s quite difficult to identify when the harm is happening because it&#39;s so diffused. But I think this is one potential significant harm. I&#39;d say maybe thinking about disrupting major political elections or something like that.</p><p> There&#39;s some options for malicious use that are talked about a little more frequently, I think because they&#39;re more well-defined and easier to wrap your head around. These are things like using generative AI to produce biological weapons or toxins, even production of malware to mount cyber attacks against key critical infrastructure. Imagine taking down an electrical grid, or imagine on election day, taking down an electoral system. These could have significant societal impacts in terms of harm to society or physical harm.</p><p> I think one key thing to point out here is that we aren&#39;t necessarily seeing these capabilities already. Some people may disagree, but I think my opinion is that technologies with the potential for this kind of harm do not currently exist. I think it is wise to assume that they will come into existence in the not too distant future, largely because we&#39;re seeing indications of these kinds of capabilities developing. We&#39;ve <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/">already seen how even narrow AI systems that are used in drug discovery, if you flip that parameter that&#39;s supposed to optimize for non-toxicity to optimize for toxicity, now you have a toxin generator</a> . So it doesn&#39;t take a huge stretch of the imagination to see how more capable systems could be used to cause quite significant societal harm. So no, I don&#39;t think there&#39;s currently systems that do this. I think we need to be prepared for a future in which these systems do exist. So it&#39;s worth thinking about the wisdom of releasing these systems now even if we might not be present with the technology at this moment.</p><p> <strong>Daniel Filan:</strong> So as a follow-up question: you&#39;re currently on the AI X-risk Research Podcast. I&#39;m wondering… so it seems like you&#39;re mostly thinking of things less severe than all humans dying or just permanently stunting the human trajectory.</p><p> <strong>Elizabeth Seger:</strong> I wouldn&#39;t say necessarily. I think this is definitely a possibility. I think part of what&#39;s happening is you kind of have to range how you talk about these things. I am concerned about x-risk from AI, and I think we could have systems where whether it&#39;s the cyber capability or the biological terror capability or even the taking down political systems capability and bolstering authoritarian governments - these are things that could cause existential risk and I am personally worried about this. But [we&#39;re] trying to write papers that are digestible to a much wider population who might not be totally bought into the x-risk arguments. I think, x-risk aside, there are still very genuine arguments for not necessarily releasing a model because of the harm that it can cause even if you don&#39;t think that those are existential risks. Even if it&#39;s just catastrophic harm, probably still not a great idea.</p><p> <strong>Daniel Filan:</strong> So at the very least, there&#39;s a range going on there.</p><p> <strong>Elizabeth Seger:</strong> Yeah.</p><h3> Should we make AI too dangerous to open source?<a name="make-ai-too-dangerous-os"></a></h3><p> <strong>Daniel Filan:</strong> So one question I had in my mind, especially when thinking about AI that can cause these kinds of risks and potentially open-sourcing it… Part of me is thinking, it seems like a lot of the risks of open-sourcing seem to be, well, there&#39;s now more people who can use this AI system to do a bunch of harm. The harm is just so great that that&#39;s really scary and dangerous. So one thing I was wondering is: if it&#39;s so dangerous for many people to have this AI technology, is it not also dangerous for anybody at all to have this AI technology? How big is this zone where it makes sense to make the AI but not to open source it? Or does this zone even exist?</p><p> <strong>Elizabeth Seger:</strong> Yeah, you make a very genuine point, and there are those that would argue that we should just hit the big old red stop button right now. I think that is an argument some people make. I guess where we&#39;re coming from with respect to this paper is trying to be realistic about how AI development&#39;s probably going to happen and try to inform governance decisions around what we should do as AI development continues. I think there are differing opinions on this, probably even among the authors on the paper. There&#39;s what, 25, 26 authors on the paper.</p><p> <strong>Daniel Filan:</strong> The paper has a disclaimer, listeners, that not all authors necessarily endorse every claim in the paper.</p><p> <strong>Elizabeth Seger:</strong> That includes me. So we have a very, very broad group of authors. But I think… Sorry, what was your original question? It just flew out of my head.</p><p> <strong>Daniel Filan:</strong> If it&#39;s too dangerous to open source, is it also too dangerous to make?</p><p> <strong>Elizabeth Seger:</strong> Yeah, no. So I think there&#39;s definitely an argument for this. I guess where I&#39;m coming from is trying to be realistic about where this development process is going and how to inform governments on what to do as AI development continues. It may very well be the case that we get to a point where everyone&#39;s convinced that we just have a big coordinated pause/stop in AI development. But assuming that that&#39;s not possible or improbable, shall we say, I think it&#39;s still wise to have a contingency plan. How can we guide AI development to be safe and largely beneficial and reduce the potential for risk? I think there&#39;s also an argument to be made that increasingly capable systems, while they pose increasingly severe risks, also could provide increasingly great benefits and potential economic potential benefits for helping to solve other challenges and crises that we face.</p><p> So I think you&#39;d be hard-pressed to get a giant coordinated pause. So the next plan is how do we make AI development happen safely? It is a lot easier to keep people safe if the super dangerous ones are not spread widely. I guess that&#39;s the very simplistic view.是的。</p><p> So I think, at least for me, I think it just comes from a realism about, are we likely to pause AI development before it gets super scary? Probably not, just given how humanity works. We developed nuclear weapons, probably shouldn&#39;t have done that. Oops, well now they exist, let&#39;s deal with it. So I think having a similar plan in line for what do we do with increasingly capable AI systems is important, especially given that it might not be that far away. Like I said, sort of with each new generation of AI that&#39;s released, we all kind of hold our breaths and say, oh, well what&#39;s that? What&#39;s that one going to do? Then we learn from it and we don&#39;t know what the next generation of AI systems is going to bring. And so having systems in place to scan for potential harms, potential dangers, capabilities, to inform decisions about whether or not these systems should be released and if and how they should be shared, that&#39;s really important. We might not be able to coordinate a pause before the big scary happens. So, I think it&#39;s important to discuss this regardless.</p><p> <strong>Daniel Filan:</strong> I got you.</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a technical term by the way, the big scary.</p><h3> Offense-defense balance<a name="offense-defense"></a></h3><p> <strong>Daniel Filan:</strong> Makes sense. So speaking of the risks of open-sourcing AI, in the paper you talk about the offense-defense balance, and basically you say that bad actors, they can disable misuse safeguards, they can increase new dangerous capabilities by fine-tuning. Open-sourcing makes this easier, it increases attacker knowledge. And in terms of just the AI technology, you basically make a claim that it&#39;s tilted towards offense in that attackers can do… They get more knowledge, they can disable safeguards, they can introduce new dangerous capabilities. It&#39;s easier to find these problems than it is to fix them. And once you fix them, it&#39;s hard to make sure that everyone has the fixes. Would you say that&#39;s a fair summary of-</p><p> <strong>Elizabeth Seger:</strong> Yeah, I&#39;d say that&#39;s pretty fair. I think the one thing I&#39;d add… I&#39;d say that with software development, for example… so offense-defense balance is something that&#39;s often discussed in terms of open-sourcing and scientific publication, especially any time you have dual-use technology or scientific insights that could be used to cause harm, you kind of have to address this offense-defense balance. Is the information that&#39;s going to be released going to help the bad actors do the bad things more or less than it&#39;s going to help the good actors do the good things/prevent the bad actors from doing the bad things? And I think with software development, it&#39;s often in favor of defense, in finding holes and fixing bugs and rolling out the fixes and making the technology better, safer, more robust. And these are genuine arguments in favor of why open-sourcing AI systems is valuable as well.</p><p> But I think especially with larger, more complex models, we start veering towards offense balance. I just want to emphasize, I think one of the main reasons for this has to do with how difficult the fixes are. So in the case of software, you get bugs and vulnerabilities that are relatively well-defined. Once you find them, relatively easy to fix, roll out the fix. The safety challenges that we&#39;re facing with highly capable AI systems are quite complex. We have huge research teams around the world trying to figure them out, and it&#39;s a very resource-intensive process, takes a lot of talent. Vulnerabilities are still easy to find, they&#39;re just a lot harder to fix. So, I think this is a main reason why the offense-defense balance probably skews more towards offense and enabling malicious actors because you can still find the vulnerabilities, you can still manipulate the vulnerabilities, take advantage of them… harder to fix, even if you have more people involved. So, that&#39;s the high-level evaluation. Mostly, I just wanted to push on the safety issue a little harder.</p><h3> KataGo as a case study<a name="katago-as-case-study"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So, one thing this actually brings up for me is… you may or may not be familiar, so there&#39;s AI that plays the board game Go. There&#39;s an <a href="https://katagotraining.org/">open-source training run</a> and some colleagues of mine have found basically <a href="https://goattack.far.ai/">you can cheaply find adversarial attacks</a> . So, basically dumb Go bots that play in a way that confuses these computer AI policies. And these attacks, they&#39;re not generally smart, but they just push the right buttons on the AI policy. And I guess this is more of a comment than a question, but there&#39;s been enough rounds of back and forth between these authors and the people making these open source Go bots that it&#39;s potentially interesting to just use that as a case study of the offense-defense balance.</p><p> <strong>Elizabeth Seger:</strong> So, you&#39;re saying that given that the system was open source and that people could sort of use it and query it and then send the information back to the developers, that that&#39;s-</p><p> <strong>Daniel Filan:</strong> Yeah, and in fact, these authors have been working with the developers of this software, and in fact what&#39;s happened is the developers of this software - KataGo, if people want to google it - they&#39;ve seen this paper, they&#39;re trying to implement patches to fix these issues. The people finding the adversarial policies are basically checking if the fixes work and publishing the information about whether they do or not.</p><p> <strong>Elizabeth Seger:</strong> Absolutely, so I want to be really clear that this is a huge benefit of open source development: getting people involved in the development process, but also using the system, finding the bugs, finding the issues, feeding that information back to the developers. This is a huge benefit of open source development for software and AI systems. I think that this is specifically why the paper focuses on the big, highly capable frontier foundation models is that this gets more difficult the more big, complex, cutting edge the system is. Some bugs will still be relatively small, well-defined, and there are bug bounty programs, proposals for AI safety bounty programs as well, helping to find these vulnerabilities and give the information back to the developers. I think there are issues though, with respect to some of the larger safety issues that are more difficult. Sometimes it&#39;s difficult to identify the safety problems in the first place, more difficult to address the safety problems.</p><p> Then, there&#39;s also the issue of just rolling out the fixes to the system. So software development, you fix a bunch of bugs, you roll out an update. Oftentimes, in the license it&#39;ll say that you&#39;re supposed to actually use the updated version. There&#39;s some data that came out, I can&#39;t remember which organization it was right now, I&#39;ll have to look it up later, but it&#39;s actually quite a low uptake rate of people actually running the up-to-date software. So first of all, even with just normal software, it&#39;s hard to guarantee that people are actually going to run the up-to-date version and roll out those fixes, which is an issue with open source because if you&#39;re using something behind API, then you just update the system and then everyone&#39;s using the updated system. If you have an open source system, then people actually have to download and run the update version themselves.</p><p> With foundation models, there&#39;s actually a weird incentive structure that changes where people might actually be de-incentivized to update. So with software, oftentimes when you have an update, it fixes some bugs and it improves system functionality. When it comes to safety fixes for foundation models, oftentimes it has to do with reducing system functionality, like putting on a filter that says, “Well, now you can&#39;t produce this class of images. Now, you can&#39;t do this kind of function with the system,” so it&#39;s hard, I don&#39;t know if there&#39;s good information on how this has actually panned out now. Are we seeing lower uptake rates with updates for AI systems? I don&#39;t know, but there might be something weird with incentive structures going on too, where if updates basically equate to reducing system functionality in certain ways, people might be less likely to actually take them on board.</p><p> <strong>Daniel Filan:</strong> I don&#39;t have a good feel for that.</p><p> <strong>Elizabeth Seger:</strong> I don&#39;t have a super good feel, but just, I don&#39;t know, interesting food for thought, perverse incentive structures.</p><p> <strong>Daniel Filan:</strong> I don&#39;t know, I&#39;m still thinking about this KataGo case: so that&#39;s the case where the attack does reduce the system functionality and people are interested in getting the latest version with fixes. It also occurs to me that… So, in fact the structure of this paper, the way they found the attack did not rely on having access to the model weights. It relied on basically being able to query the Go bot policy, basically to try a bunch of things and figure out how to trick the Go bot policy. Now, it&#39;s really helpful if you can have the weights locally just so that you can call the API, so that you can call it a lot, but that was not a case where you needed the actual weights to be shared. So on the one hand, that&#39;s a point that sharing the weights is less valuable than you might think, but it also suggests if you&#39;re worried about people finding these adversarial attacks, then just putting the weights behind an API doesn&#39;t protect you正如你所想的那样。 Maybe you need to rate limit or something.</p><p> <strong>Elizabeth Seger:</strong> I think that&#39;s a valuable insight, there are definitely things you can do without weights. This is an argument for why you should be worried anyway, but it&#39;s also an argument for… There are lots of arguments for, well, open-sourcing is important because you need it to do safety research and getting more people involved in safety research will result in safer systems, you have more people input into these processes, but you just illustrated a perfect example of how just having query access, for example, to a system, can allow you to do a significant amount of safety research in terms of finding漏洞。</p><h3> Openness for interpretability research<a name="open-for-interp"></a></h3><p> <strong>Elizabeth Seger:</strong> So, query access is one that can be done completely behind an API, but then even if we think about something like interpretability research, interpretability research does require much more in-depth access to a system to do, but arguably this is an argument for needing access to smaller systems. We are struggling to do interpretability research on smaller, well-defined systems. It&#39;s sort of like the rate limiting factor on interpretability research isn&#39;t the size of the models people have access to, the way I understand it at least. If we&#39;re struggling to do interpretability research on smaller models, I feel like having access to the biggest, most cutting edge frontier model is not what needs to happen to drive interpretability research.</p><p> <strong>Daniel Filan:</strong> I think it depends on the kind of interpretability research.</p><p> <strong>Elizabeth Seger:</strong> It&#39;d be interesting to hear your thoughts on this as well, but there&#39;s a range of different kinds of AI research. Not all of it requires open access and then some of the kinds that do require open access to the models isn&#39;t necessarily helped the most by having the open access, and then there is also this idea of alternative approaches that we talk about in the纸。 You can help promote AI safety research by providing access to specific research groups. There are other things you can do to give people the access they need to do safety research.</p><p> <strong>Daniel Filan:</strong> So, I guess I can share my impressions here. Interpretability research is a broad bucket. It describes a few different things. I think there are some kinds of things where you want to start small and we haven&#39;t progressed that far beyond small. So just understanding, &#39;can we just exhaustively understand how a certain neural net works?&#39; - start small, don&#39;t start with GPT-4. But I think one thing you&#39;re potentially interested in, in the interpretability context, is how things get different as you get bigger models, or do bigger models learn different things or do they learn more? What sorts of things start getting represented? Can we use interpretability to predict these shifts? There you do want bigger models. In terms of how much can you do without access to weights, there&#39;s definitely a lot of interpretability work on these open source models because people apparently really do value having the weights.</p><p> Even in the case of the adversarial policies work I was just talking about, you don&#39;t strictly need access to the weights, but if you could run the games of Go purely on your computer, rather than calling the API, waiting for your request to be sent across the internet, and the move to be sent back, and doing that a billion times or I don&#39;t know the actual number, but it seems like just practically-</p><p> <strong>Elizabeth Seger:</strong> Much more efficient.</p><p> <strong>Daniel Filan:</strong> … It&#39;s easier to have the model. I also think that there are intermediate things. So one thing the paper talks about, and I guess your colleague <a href="https://sites.google.com/view/tobyshevlane">Toby Shevlane</a> has <a href="https://arxiv.org/abs/2201.05159">talked about</a> is basically structured access of giving certain kinds of information available maybe to certain people or maybe you just say “these types of information are available, these types aren&#39; t”。 I&#39;ve heard colleagues say, “Even if you didn&#39;t open source GPT-4 or GPT-3, just providing final layer activations or certain kinds of gradients could be useful”, which would not… I don&#39;t think that would provide all the dangers or all the risks that open-sourcing could potentially involve.</p><p> <strong>Elizabeth Seger:</strong> I think this is a really key point as well, is trying to get past this open versus closed dichotomy. Just saying that something isn&#39;t open source doesn&#39;t necessarily mean that it&#39;s completely closed and no-one can access it. So like you said, Toby Shevlane talks about structured access, and it was a paper we referenced - at least when we referenced it, it was still forthcoming, it might be out now - but it was Toby Shevlane and Ben Bucknell were working on it, and it was about the potential of developing research APIs. So, how much access can you provide behind API to enable safety research and what kind of access would that need to look like and how could those research APIs be regulated and who by? So, I think if there&#39;s a genuine interest in promoting AI safety research and a genuine acknowledgement of the risks of open-sourcing, we could put a lot of resources into trying to develop and understand ways to get a lot of the benefits to safety research that open-sourcing would have by alternative means.</p><p> It won&#39;t be perfect. By definition, it&#39;s not completely open, but if we take the risks seriously, I think it&#39;s definitely worth looking into these alternative model sharing methods and then also into the other kinds of proactive activities we can engage in to help promote safety research, whether that&#39;s committing a certain amount of funds to safety research or developing international safety research organizations and collaborative efforts. I know one issue that always comes up when talking about, “Well, we&#39;ll just provide safety research access through API, or we&#39;ll provide privileged downloaded access to certain groups.” It&#39;s like, “Well, who gets to decide who has access? Who gets to do the safety research?”</p><p> And so, I think this points to a need to have some sort of a multi-stakeholder governance body to mediate these decisions around who gets access to do the research, whether you&#39;re talking about academic labs or other private labs, sort of like [how] you have multi-stakeholder organizations decide how to distribute grants to do environmental research, or you have grantmaking bodies that distribute grant funds to different academic groups. You could have a similar type situation for distributing access to more highly capable, potentially dangerous systems, to academic groups, research groups, safety research institutions that meet certain standards and that can help further this research.</p><p> So, I feel like if there&#39;s a will to drive safety research forward, and if varying degrees of access are needed to allow the safety research to happen, there are things we can do to make it happen that do not necessarily require open-sourcing a系统。 And I think, like we said, different kinds of safety research require varying degrees of access. It&#39;s not like all safety research can be done with little access. No, you need different amounts of access for different kinds of safety research, but if there&#39;s a will, there&#39;s a way.</p><h3> Effectiveness of substitutes for open sourcing<a name="os-substitutes"></a></h3><p> <strong>Daniel Filan:</strong> So, I want to ask something a bit more quantitative about that. So, some of the benefits of open-sourcing can be gained by halfway measures or by structured access or pursuing tons of collaborations, but as you mentioned, it&#39;s not going to be the same as if it were open sourced. Do you have a sense of… I guess it&#39;s going to depend on how constrained you are by safety, but how much of the benefits of open source do you think you can get with these more limited sharing methods?</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a good question. I think you can get quite a bit, and I think, again, it sort of depends what kind of benefit you&#39;re talking about. So in the paper, I think we discuss three different benefits. Let&#39;s say we talk about accelerating AI research, so that&#39;s safety research and capability research. We talk about distributing influence over AI systems, and this ranges everything from who gets to control the systems, who gets to make governance decisions about the systems, who gets to profit. It wraps all the democratization themes together under distributing influence over AI, and then, let&#39;s see, what was the other one that we talked about? You&#39;d think I&#39;ve talked about this paper enough in the last three months, I have it down. Oh, external model evaluation. So, enabling external oversight and evaluation, and I think it depends which one you&#39;re talking about.</p><p> Let&#39;s start with external model evaluation. I think that this probably benefits the most from open-sourcing. It depends what you&#39;re looking at, so for example, if you&#39;re just looking for minor bugs and stuff like that, you don&#39;t need open source access for that, but having a more in-depth view to the systems is more important for people trying to help find fixes to the bugs.我们已经讨论过这个问题。 There are also risks associated with open-sourcing. If we&#39;re talking about accelerating capability research, for example, which sort of falls under the second category, I think you might find that the benefits of open-sourcing here might be somewhat limited the larger and more highly capable the system gets. And I think this largely will just have to do with who has access to the necessary resources to really operate on the cutting edge of research and development. Open source development, it operates behind the frontier right now largely because of restrictions… not restrictions, but just the expense of the necessary compute resources.</p><p> And then you talk about distributing control over AI, we&#39;ve already discussed the more distributed effect of open-sourcing and model sharing on distributing control. It&#39;s a second order effect: you get more people involved in the development process and then large labs have more competition, and then it distributes influence and control.</p><p> There are probably more direct ways you can help distribute control and influence over AI besides making a system widely available. So, to answer your original question then about, how much of the benefit of open-sourcing can you get through alternative methods? I guess it really depends what benefit you&#39;re talking about. I think for AI safety progress probably quite a bit, honestly; actually the vast majority of it, given that a lot of the safety research that&#39;s done on these highly capable, cutting edge models is something that has to happen within well-resourced institutions anyway, or you need the access to the resources to do that, not just the code and the weights, but the computational resources and so on.</p><p> So, I think quite a bit. I think it&#39;s less of a “can we get close to the same benefits that open-sourcing allows?” It&#39;s more like, “can we do it in one fell swoop?”就是这样。 It&#39;s like open-sourcing is the easy option. “Here, it&#39;s open!” - and now you get all these benefits from open-sourcing. The decision to open-source or not, part of the reason it&#39;s a hard decision is because achieving these benefits by other means is harder. It&#39;s going to take more resources to invest, more organizational capacity, more thought, more cooperation. It&#39;s going to take a lot of infrastructure, a lot of effort. It&#39;s not the one-stop shop that open-sourcing is, but I think the idea is that if the risks are high enough, if the risks are severe enough, it&#39;s worth it. I think that&#39;s where it comes in.</p><p> So, I guess it&#39;s worth reiterating again and again: this paper is not an anti-open source paper, [it&#39;s] very pro-open source in the vast majority of cases. What we really care about here are frontier AI systems that are starting to show the potential for causing really catastrophic harm, and in these cases, let&#39;s not open-source and let&#39;s pursue some of these other ways of achieving the same benefits of open source to safety and distributing control and model evaluation, but open-source away below that threshold. The net benefits are great.</p><h3> Offense-defense balance, part 2<a name="offense-defense-2"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So my next question - I actually got a bit sidetracked and wanted to ask it earlier - so in terms of the offense-defense balance, in terms of the harms that you are worried about from open-sourcing, I sometimes hear the claim that basically, “Look, AI, if you open-source it, it is going to cause more harm, but you also enable more people to deal with the harm.” So I think there, they&#39;re talking about offense-defense balance, not of finding flaws in AI models, but in the underlying issues that AI might cause. So, I guess the idea is something… To caricature it, it&#39;s something like, “Look, if you use your AI to create a pathogen, I can use my AI to create a broad spectrum antibiotic or something”, and the hope is that in these domains where we&#39;re worried about AI causing harm, look, just open-sourcing AI is going to enable tons of people to be able to deal with the harm more easily, as well as enabling people to cause harm. So I&#39;m wondering, what do you think about the underlying offense-defense balance as opposed to within AI?</p><p> <strong>Elizabeth Seger:</strong> I get the argument. Personally, I&#39;m wary about the arms race dynamic though. You gotta constantly build the stronger technology to keep the slightly less strong technology in check. I guess this comes back to that very original question you asked about, “What about just hitting the no more AI button?” So, I guess I get the argument for that. I think there&#39;s weird dynamics, I don&#39;t know. I&#39;m not doing a very good job answering this question. I&#39;m personally concerned about the race dynamic here, and I think it just comes back to this issue of, how hard is it to fix the issues and vulnerabilities in order to prevent the misuse in the first place? I think that should be the goal: preventing the misuse, preventing the harm in the first place. Not saying, “Can we build a bigger stick?”</p><p> There&#39;s a similar argument that is brought up when people talk about the benefits of producing increasingly capable AI systems and saying, “Oh, well, we need to plow ahead and build increasingly capable AI systems because you never know, we&#39;ll develop a system that&#39;ll help cure cancer or develop some renewable energy technology that&#39;ll help us address climate change or something like that.” What huge problems could AI help us solve in the future? And I don&#39;t know - this is personally me, I don&#39;t know what the other authors on this paper think of this - but I don&#39;t know, I kind of feel like if those are the goals, if the goals are to solve climate change and cure cancer, take the billions upon billions upon billions and billions of dollars that [we&#39;re] currently putting into training AI systems and go cure cancer and develop renewable technologies! I struggle with those arguments personally. I&#39;d be interested just to hear your thoughts. I have not written about this. This is me riffing right now. So, I&#39;d be interested to hear your thoughts on this train of thought as well.</p><p> <strong>Daniel Filan:</strong> I think the original question is unfairly hard to answer just because it&#39;s asking about the offense-defense balance of any catastrophic problem AI might cause and it&#39;s like, “Well, there are tons of those and it&#39;s pretty hard to think about. ” So, the thing you were saying about, if you wanted to cure cancer, maybe step one would not be “create incredibly smart AI”. I&#39;ve seen this point. I don&#39;t know if you know <a href="https://meaningness.com/about-my-sites">David Chapman</a> &#39;s <a href="https://betterwithout.ai/">Better without AI</a> ?</p><p> <strong>Elizabeth Seger:</strong> No, not familiar.</p><p> <strong>Daniel Filan:</strong> So, he basically argues we just shouldn&#39;t build big neural nets and it&#39;s going to be terrible. Also, <a href="https://aiimpacts.org/author/jeffreyheninger/">Jeffrey Heninger</a> at <a href="https://aiimpacts.org/">AI Impacts</a> , I think has said <a href="https://blog.aiimpacts.org/p/my-current-thoughts-on-the-ai-strategic">something similar along these lines</a> . On the one hand, I do kind of get it, just in the sense that, if I weren&#39;t worried about misaligned AI, there&#39;s this hope that this is the last invention you need. You create AI and now instead of having to separately solve cancer and climate change and whatever, just make it solve those things for you.</p><p> <strong>Elizabeth Seger:</strong> I guess it&#39;s just really hard to look forward, and you have to decide now whether or not this technology is that silver bullet and how much investment it&#39;s going to take to get to that point.</p><p> <strong>Daniel Filan:</strong> I think that&#39;s right, and I think your take on this is going to be driven by your sense of the risk profile of building things that are significantly smarter than us. I guess from the fact that I made the AI X-risk Research Podcast, rather than the AI Everything&#39;s Going to be Great Research Podcast, people can guess my-</p><p> <strong>Elizabeth Seger:</strong> It&#39;s an indication of where you&#39;re coming from.</p><p> <strong>Daniel Filan:</strong> … take on this, but I don&#39;t know. I think it&#39;s a hard question. So, part of my take is, in terms of the underlying offense-defense balance, I think it becomes more clear when you&#39;re worried about, what should I say, silicogenic risks? Basically the AI itself coming up with issues rather than humans using AI to have nefarious schemes. Once you&#39;re worried about AI doing things on their own where you are not necessarily in control, there I think it makes sense that you&#39;re probably… If you&#39;re worried about not being able to control the AIs, you&#39;re probably not going to be able to solve the risks that the AIs are creating, right?</p><p> <strong>Elizabeth Seger:</strong> Yeah, your management plan for AI shouldn&#39;t be to build a slightly more powerful AI to manage your AI.</p><p> <strong>Daniel Filan:</strong> Well, if you knew that you were going to remain in control of the slightly bigger AI, maybe that&#39;s a plan, but you kind of want to know that.</p><p> <strong>Elizabeth Seger:</strong> I guess I was saying that if you&#39;re worried about loss of control scenarios, then the solution shouldn&#39;t be, “Well, let&#39;s build another system that&#39;s also out of our control, but just slightly better aligned to address the…” I feel like that&#39;s-</p><p> <strong>Daniel Filan:</strong> It&#39;s not the greatest. I think my colleague John Wentworth has <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies">some saying</a> , “Releasing Mothra to contain Godzilla is not going to increase property values in Tokyo,” which is a cute little line. I don&#39;t know, it&#39;s a hard question. I think it&#39;s hard to say anything very precise on the topic. I did want to go back to the offense-defense balance. So moving back a bit, a thing you said was something like, “Look, it&#39;s probably better to just prevent threats from arising, than it is to have someone make a pathogen and then have everyone race to create an antibiotic or antiviral or whatever. ” So, that&#39;s one way in which everyone having really advanced AI… That&#39;s one way that could look in order to deal with threats. I think another way does look a bit more like prevention. I don&#39;t know, it&#39;s also more dystopian sounding, but one thing that AI is good at is surveillance, right?</p><p> <strong>Elizabeth Seger:</strong> Yes.</p><p> <strong>Daniel Filan:</strong> Potentially, so you could imagine, “Look, we&#39;re just going to open source AI and what we&#39;re going to use the AI for is basically surveilling people to make sure the threats don&#39;t occur.” So, maybe one version of this is you just really amp up wastewater [testing]. Somehow you use your AI to just look at the wastewater and see if any new pathogens are arising. It could look more like you have a bunch of AIs that can detect if other people are trying to use AI to create superweapons or whatever, and stop them before they do somehow.</p><p> <strong>Elizabeth Seger:</strong> The wastewater example, that sounds great. We should probably do that anyway. In terms of surveilling to see how people are using AI systems using AI, why not just have the AI systems be behind an API where people can use the systems for a variety of downstream tasks integrating through this API and then the people who control the API can just see how the system is being used? Even if it can be used for a vast majority of tasks, even if you were to take all the safety filters off, the advantage of the API is still that you can see how it&#39;s being used. I don&#39;t know, I feel like that&#39;s-</p><h3> Making open-sourcing safer?<a name="making-os-safer"></a></h3><p> <strong>Daniel Filan:</strong> That seems like a good argument. All right, so I guess another question I have is related to the frame of the report. So in the report you&#39;re basically like, “open-sourcing has these benefits, but it also has these costs. What are ways of doing things other than open-sourcing that basically try and retain most of the benefits while getting rid of most of the costs?” You can imagine a parallel universe report where you say, “Okay, open-sourcing has these benefits, but it also has these costs. We&#39;re still going to open source, but we&#39;re going to do something differently in our open source plan that is going to retain benefits and reduce costs”, right? So one example of this is you open-source models, but you have some sort of watermarking or you have some sort of cryptographic backdoor that can stop models in their tracks or whatever. I&#39;m wondering: why the frame of alternatives to open-sourcing rather than making open-sourcing better?</p><p> <strong>Elizabeth Seger:</strong> Very simple. I think making open-sourcing better is the harder question, technically more difficult. I mean, for example, say you have watermarking, part of the issue with watermarking to identify artificially generated images is making sure the watermarks stick. How do you make sure that they are irremovable if you are going to open… This is a really complex technical question: how do you develop a system that has watermarked images where that watermark is irremovable if you were to open source the system?</p><p> I&#39;m not saying it&#39;s undoable. I personally don&#39;t have the technical background to comment very deeply on this. I have heard people talking about how possible this would be. It also depends how you watermark, right?</p><p> If you have just a line of inference code that says, “slap a watermark on this thing”, it could delete the line of inference code. If you&#39;re to train the system on only watermarked images, well now you have to retrain the entire system to get it to do something else, which is very expensive. So again, I think it depends how you do it.</p><p> I was at a meeting last week where people were talking about, are there ways we could build in a mechanism into the chips that run the systems that say “if some bit of code is removed or changed in the system, then the chip burns up and won&#39;t run the system”. I&#39;m not saying this is impossible, but [a] really interesting technical question, really difficult, definitely beyond my area of expertise. But I think if this is an approach we can take and say, there are ways to be able to open source the system and get all the benefits of open-sourcing by just open-sourcing and still mitigate the risks, I think that&#39;s great. I think it&#39;s just a lot more difficult.</p><p> And there&#39;s one aspect in which we do take the flip view in the report, and I think this is where we start talking about a staged release of models. You can undergo a staged release of a model where you put out a slightly smaller version of a model behind API, you study how it&#39;s being used. Maybe you take a pause, analyze how it was used, what [are] the most common avenues of attack, if at all, [that] were being used to try and misuse the model.</p><p> And then you release a slightly larger model, [then] a slightly larger model. You do this iteratively, and if you do this process, as you get to a stage where it&#39;s like, hey, we&#39;ve been doing the staged release of this model for however many months and no problems, looking good, there&#39;s no emergent capabilities that popped up that are making you worried. You didn&#39;t have to implement a bunch of safety restrictions to get people to stop doing unsafe things - okay, open source. This is not a binary [of] it has to be completely open or completely closed. And I think this is one respect… If you were to take this flip view of “how can we open source, but do it in the safest way possible?” Just open source slowly, take some time to actually study the impacts. And it&#39;s not like the only way to get a sense of how the system&#39;s going to be used is to just open source it and see what happens. You could do a staged release and study what those impacts are.</p><p> Again, it won&#39;t be perfect. You never know how it&#39;s going to be used 10 years down the road once someone gets access to all the weights and stuff. But it is possible to study and get some sort of insight. And I think one of the nice things about staged release is if you start doing the staged release process and you realize that at each iterative step you are having to put in a bunch of safety filters, for example, to prevent people from doing really shady stuff, that&#39;s probably a good indication that it&#39;s not ready to be open sourced in its current form, because those are safety filters that will just immediately be reversed once open sourced. So I think you can learn a lot from that.</p><p> So I think that&#39;s one way you can open source safely: find ways to actually study what the effects are before you open source, because that decision to open source is irreversible. And then I think the technical challenge of, are there ways we can have backstops, that we can technically build in irreversible, irremovable filters or watermarks or even just hardware challenges that we could implement - I think [those are] really interesting technical questions that I don&#39;t know enough about, but… Go for it. That&#39;d be a great world.</p><p> <strong>Daniel Filan:</strong> Yeah. If listeners are interested, this gets into some territory that <a href="https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html#watermarking-lm-outputs">I talked about with Scott Aaronson earlier this year</a> . Yeah, I think the classic difficulties, at least say for watermarking… I read <a href="https://arxiv.org/abs/2012.08726">one paper</a> that claims to be able to bake the watermark into the weights of the model. To be honest, I didn&#39;t actually understand how that works.</p><p> <strong>Elizabeth Seger:</strong> I think it has to do with how the model&#39;s trained. So the way I understand it is if you have a dataset of images that all have a watermark in that dataset, not watermark in the sense like you see on a $1 bill, but weird pixel stuff that the human eye can&#39;t see. If all the images in the training dataset have that watermark, then all of the images it produces will have that watermark. In that case, it&#39;s baked into the system because of how it was trained. So the only way to get rid of that watermark would be to retrain the system on images that don&#39;t contain the watermark.</p><p> <strong>Daniel Filan:</strong> Yeah, that&#39;s one possibility. So that&#39;s going to be a lot rougher for applying to text models, of course, if you want to just train on the whole internet. I think I saw <a href="https://arxiv.org/abs/2012.08726">something</a> that claimed to work even on cases where the dataset did not all have the watermark, but I didn&#39;t really understand how it worked. But at any rate, the key issue with these sort of watermarking methods is as long as there&#39;s one model that can basically paraphrase that does not have watermarking, then you can just take your watermark thing and basically launder it and get something that - if your paraphrasing model is good enough, you can create something that looks basically similar, it doesn&#39;t have the watermark, and then it&#39;s sad news.是的。进而-</p><p> <strong>Elizabeth Seger:</strong> Sorry, I was going to say there&#39;s similar [things] in terms of how doing something with one model allows you to jailbreak another model. I mean this is what happened with the <a href="https://arxiv.org/abs/2307.15043">&#39;Adversarial suffixes&#39; paper</a> , where using a couple open source models, one which was Llama 2, and using the weights of those models, figuring out a way to basically just throw a seemingly random string of numbers at a large language model, and then with that seemingly random range of numbers before the prompt basically get the system to do whatever you want. Except while they figured out how to do that using the weights accessible from Llama 2, it worked on all the other large language models. So finding a way to jailbreak one model and using the weights and access to one model, that could bring up vulnerabilities in tons of others that aren&#39;t open sourced as well. So I think that&#39;s another roughly related somewhat to what we were just talking about point.</p><p> <strong>Daniel Filan:</strong> Yeah, I guess it brings up this high level thing of whatever governance method for AI you want, you want it to be robust to some small fraction of things breaking the rules. You don&#39;t want the small fraction to poison the rest of the thing, which watermarking unfortunately has.</p><p> Yeah, I guess I wanted to say something brief about backdoors as well. So there is really a way of, at least in toy neural networks, and you can probably extend it to bigger neural networks, <a href="https://arxiv.org/abs/2204.06974">you really can introduce a backdoor that is cryptographically hard to detect</a> . So one problem is, how do you actually use this to prevent AI harm is not totally obvious. And then there&#39;s another issue of… I guess the second issue only comes up with super smart AI, but if you have a file on your computer that&#39;s like, “I implanted a backdoor in this model, the backdoor is this input”, then it&#39;s no longer cryptographically hard to find as long as somebody can break into your computer. Which hopefully is cryptographically hard, but I guess there are security vulnerabilities there.</p><p> So yeah, I wonder if you want to say a little bit about the safer ways to get the open source benefits. I&#39;ve given you a chance to talk about them a little bit, but is there anything more you want to say about those?</p><p> <strong>Elizabeth Seger:</strong> I think, not really. I think the overarching point is, just as I said before, when the risks are high - and I think that&#39;s key to remember, I&#39;m not saying don&#39;t open-source everything - when the risks are high, it is worth investing in seeing how else we can achieve the benefits of open-sourcing. Basically, if you&#39;re not going to open-source because the risks are high, then look into these other options. It&#39;s really about getting rid of this open versus closed dichotomy.</p><p> So many of the other options have to do with other options for sharing models, whether that&#39;s structured access behind API, even research API access, gated download, staged release, and then also more proactive efforts. Proactive efforts which can actually also be combined with open-sourcing. They don&#39;t have to be seen as an alternative to open-sourcing. So this is things like redistributing profits towards AI safety research or starting AI safety and bug bounty programs. Or even like we talked about with <a href="https://arxiv.org/abs/2303.12642">the democratization paper</a> , thinking about how we can democratize decision-making around AI systems to help distribute influence over AI away from large labs, which is another argument for open-sourcing.</p><p> So yeah, I think that this is key: there are other efforts that can be put in place to achieve many of the same benefits of open-sourcing and when the risks are high, it&#39;s worth really looking into these.</p><h2> AI governance research<a name="ai-gov"></a></h2><p> <strong>Daniel Filan:</strong> All right.好的。 So moving on, I want to talk a little bit more broadly about the field of AI governance research. So historically, this podcast is mostly focused on technical AI alignment research, and I imagine most listeners are more familiar with the technical side than with governance efforts.</p><p> <strong>Elizabeth Seger:</strong> In which case, I apologize for all my technical inaccuracies. One of the benefits of having 25 co-authors is that a lot of the technical questions I got to outsource.</p><h3> The state of the field<a name="state-of-field"></a></h3><p> <strong>Daniel Filan:</strong> Makes sense. Yeah, it&#39;s good to be interdisciplinary. So this is kind of a broad question, but how is AI governance going? What&#39;s the state of the field, if you can answer that briefly?</p><p> <strong>Elizabeth Seger:</strong> The state of the field of AI governance.是的。好的。 I&#39;ll try and answer that briefly. It&#39;s going well in that people are paying attention. In this respect, the release of ChatGPT I think was really great for AI governance because people, besides those of us already doing AI governance research, are really starting to see this as something valuable and important that needs to be talked about and [asking] questions around what role should governments play in regulating AI, if at all? How do we get this balance between governments and the developers? Who should be regulated with respect to different things? Do all of the responsibilities lie on the developers or is it on the deployers?</p><p> And all of these questions suddenly are coming to light and there&#39;s more general interest in them. And so we&#39;re seeing things like, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is happening next week, [a] global AI summit, looking at AI safety, really concerned about catastrophic and existential risks, trying to understand what kind of global institutions should be in place to govern AI systems, to evaluate AI systems, to audit, to regulate.</p><p> And this is bringing in countries from all over the world. I think it&#39;s something like 28 different countries are going to be at the UK AI Summit. You have <a href="https://artificialintelligenceact.eu/">the EU AI Act</a> where it started a while ago looking at narrow AI systems, but now is taking on foundation models and frontier AI systems and looking at open source regulation. And this has really, over the last year, exploded into a global conversation.</p><p> So in that respect, AI governance is going well in that people are paying attention. It&#39;s also very high stress because suddenly everyone&#39;s paying attention.我们必须做点什么。 But I think there&#39;s really genuine interest in getting this right, and I think that really bodes well. So I&#39;m excited to see where this next year goes. Yeah, there&#39;s talk about having this global AI summit and then making this a recurring series. And so I think it&#39;s going well in the sense that people are paying attention and the wheels are starting to turn, and that&#39;s cool.</p><h3>开放式问题<a name="open-qs"></a></h3><p><strong>Daniel Filan:</strong> Okay. I guess related to that, what do you see as the most important open questions in the field?</p><p> <strong>Elizabeth Seger:</strong> In the field of AI governance?好的。 So I think one big one is compute governance, which my colleague, <a href="https://heim.xyz/about/">Lennart Heim</a> , <a href="https://www.governance.ai/team/lennart-heim">works on</a> . This is just thinking about how compute is a lever for trying to regulate who is able to develop large models, even how compute should be distributed so that more people can distribute large models, but basically using compute as a lever to understand who has access to and who is able to develop different kinds of systems. So I think that&#39;s a huge area of research with a lot of growing interest because compute&#39;s one of the tangible things that we can actually control the flow of.</p><p> I think that the questions around model-sharing and open-sourcing are getting a lot of attention right now. Big open question, a lot of debate, like I said, it&#39;s becoming really quite a polarized discussion, so it&#39;s getting quite hard to cut through. But a lot of good groups [are] working on this, and I think a lot of interest in genuinely finding common ground to start working on this. I&#39;ve had a couple situations where I&#39;ve been in groups or workshops where we get people who are very pro-open source and other people who are just like, no, let&#39;s just shut down the whole AI system right now, really both sides of the spectrum coming together. And we try and find a middle ground on, okay, where do we agree? Is there a point where we agree? And very often we can come to a point of agreement around the idea that there may be some AI system, some model that poses risks that are too extreme for that model to be responsibly open sourced.</p><p> And that might not sound like that extreme of a statement, but when you have people coming from such polarized views to agree on the fact that there may exist a model one day that should not be open source, that is a starting point and you can start the conversation from there. And every group I&#39;ve been in so far has got to that point, and we can start working on that. So I think this model-sharing question is a big open question and lots of technical research needs to be done around benchmarking to decide, when are capabilities too dangerous?</p><p> Also around understanding what activities are actually possible given access to different combinations of model components. And that&#39;s actually not entirely clear, and we need a much more fine-grained understanding of what you can actually do given different kinds of model, combinations of model components, in order not only to have safe standards for model release and really a fine-grained standard for model release, but also to protect the benefits of open-sourcing. You don&#39;t want to just have a blanket “don&#39;t release anything” if you can get a lot of good benefit out of releasing certain model components. So I think a lot of technical research has to go into this.</p><p>反正。 So yeah, second point, I think model-sharing is a really big point of discussion right now. And then with the upcoming <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> , [there&#39;s] quite a bit of discourse around what international governance structures should look like for AI, a lot of different proposed models. And yeah, it&#39;ll be interesting to see what comes out of the summit. I don&#39;t think they&#39;re going to agree on anything amazing at the summit.已经两天了。 But I think for me, a really great outcome of the summit would be, first, recognition from everyone that AI systems could pose really extreme risks. So just a recognition of the risks. And then second, a plan going forward, a plan for how we can start establishing international systems of governance and really structure out when are we going to come to what kinds of decisions and how can we start putting something together. So I think that those are probably three key open questions, and the international governance structure one is really big right now too, just given the upcoming summit.</p><p> <strong>Daniel Filan:</strong> And I guess unless we get the editing and transcription for this episode done unusually quickly, listeners, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is almost definitely going to be in your past. So I guess listeners are in this interesting position of knowing how that all panned out in a way that we don&#39;t. So that was open questions in the field broadly. I&#39;m wondering for you personally as a researcher, what things are you most interested in looking at next?</p><p> <strong>Elizabeth Seger:</strong> Interesting. I mean, most of my life is taken up with follow-up on this open source report right now. So I definitely want to keep looking into questions around model-sharing and maybe setting responsible scaling policy, responsible model release policy.我不太确定。 I think I&#39;m in this place right now where I&#39;m trying to feel out where the most important work needs to be done and whether the best place for me to do is to encourage other people to do certain kinds of work where I don&#39;t necessarily have the expertise, like we were talking about, like needing more technical research into what is possible given access to different combinations of model components, or are there specific areas of research I could try to help lead in? Or whether really what needs to be done is just more organizational capacity around these issues.</p><p> So no, I am personally interested in keeping up with this model-sharing discussion. I think there&#39;s a lot of interesting work that needs to be done here, and it&#39;s a key thing that&#39;s being considered within the discussions around international AI governance right now. Yeah, so sorry I don&#39;t have as much of a clear cut answer there, but yeah, I&#39;m still reeling from having published this report and then everything that&#39;s coming off the back of it and just trying to feel out where&#39;s the next most important, most impactful step, what work needs to be done. So I guess if any of your listeners have really hot takes on “oh, this is what you should do next”, I guess, please tell me. It&#39;ll be very helpful.</p><p> <strong>Daniel Filan:</strong> How should they tell you if someone&#39;s just heard that and they&#39;re like, oh, I need to-</p><p> <strong>Elizabeth Seger:</strong> “I need to tell her now! She must know!” Yeah, so I mean, I have a <a href="https://elizabethseger.com/">website</a> where you could find a lot of my contact information, or you can always find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . I spend far too much time on LinkedIn these days. And also my email address happens to be on the open source report. So if you download the report, my email address is there.</p><p> <strong>Daniel Filan:</strong> What&#39;s the URL of your website?</p><p> <strong>Elizabeth Seger:</strong> <a href="https://elizabethseger.com/">ElizabethSeger.com</a> .</p><h3> Distinctive governance issues of x-risk<a name="xrisk-different"></a></h3><p> <strong>Daniel Filan:</strong> All right.好的。 Getting back to talking about governance in general, I&#39;m wondering… so I guess this is an x-risk-focused podcast. How, if at all, do you think governance research looks different when it&#39;s driven by concerns about x-risk mitigation versus other concerns you could have about AI governance?</p><p> <strong>Elizabeth Seger:</strong> Well, that&#39;s a good question.让我们来看看。 So the question is how does the governance research look different?</p><p> <strong>Daniel Filan:</strong> Yeah. What kinds of different questions might you focus on, or what kinds of different focuses would you have that would be driven by x-risk worries rather than by other things?</p><p> <strong>Elizabeth Seger:</strong> So this is something that I&#39;ve had to think about a lot in my own research development because I did not come into this area of research from an x-risk background interest. I came into it… I mean, honestly, I started in bioethics and then moved from bioethics looking at AI systems in healthcare and have sort of moved over into the AI governance space over a very long PhD program. And so here I am.</p><p> But I would say one of the things that I&#39;ve learned working in the space [that&#39;s] more interested in long-term x-risk impacts of AI and trying to prevent x-risks is really paying attention to causal pathways and really trying to be very critical about how likely a potential pathway is to actually lead to a risk. I don&#39;t know if I&#39;m explaining this very well.</p><p> Maybe a better way of saying it&#39;s like: if you have a hypothesis, or let&#39;s say you&#39;re worried about the impacts of AI systems on influence operations or impacting political campaigns, I find it really helpful to start from the hypothesis of, it won没有影响。 And really just trying to understand how that might be wrong, as opposed to trying to start from “oh, AI is going to pose a massive bio threat, or it&#39;s going to pose a massive threat to political operations” or something like that. And then almost trying to prove that conclusion.</p><p> Yeah, I don&#39;t know, I start from the opposite point and then try and think about all the ways in which I could be wrong. And I think this is really important to do, especially when you&#39;re doing x-risk research, whether it&#39;s with respect to AI or some other form of x-risk. Because I think there are a lot of people that turn off when you start talking about existential risks, they think it&#39;s too far out there, it&#39;s not really relevant to the important questions that are impacting people today, the tangible things that people are already suffering 。 And so I think it&#39;s really important to be very, very rigorous in your evaluations and have a very clear story of impact for why it is that you&#39;re doing the research you&#39;re doing and focusing on the issues that you&#39;re doing. At least that&#39;s been my experience, trying to transition into the space and work on these issues.</p><h3> Technical research to help governance<a name="tech-for-gov"></a></h3><p> <strong>Daniel Filan:</strong> Another question I have, related to my audience… So I think my audience, a lot of them are technical alignment researchers and there are various things they could do, and maybe they&#39;re interested in, okay, what work could technical alignment people do that would make AI governance better? I&#39;m wondering if you have thoughts on that question.</p><p> <strong>Elizabeth Seger:</strong> Okay. Technical alignment people, AI governance better. Yeah, I mean there&#39;s a lot of work going on right now, especially within the UK government. We just set up the <a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">UK AI Task Force</a> , a government institution doing a lot of model evals and alignment research. I think if you have the technical background in alignment research, you are very much needed in the governance space. There&#39;s very often a disconnect between… I mean, I am also guilty of this. There&#39;s a disconnect between the people doing the governance research and the people who have the experience with the technology and really know the ins and outs of the technology that&#39;s being developed.</p><p> And I think if you have the inclination to work in an AI governance space and help bridge that gap, that would be incredibly valuable. And like I&#39;ve already said, some of the more technical questions, even around open-sourcing, are things that I was very, very glad to have colleagues and co-authors on the paper who have worked for AI labs and stuff before and really knew what they were talking about and could advise and help write some of the more technical aspects of the report.</p><p> So I think if you have the inclination to work in the space, to get involved with governance efforts, or even maybe some of these government institutions that are starting to pop up that are working on the boundary of AI governance and technical research, that could be a really valuable place to contribute. So I think my 2 cents off the top of my brain would be help bridge that gap.</p><p> <strong>Daniel Filan:</strong> Okay.伟大的。 So before we wrap up, I&#39;m wondering if there&#39;s anything that you wish I&#39;d asked but that I didn&#39;t?</p><p> <strong>Elizabeth Seger:</strong> Oh, that&#39;s a good question.不，我不这么认为。 I think we&#39;ve covered a lot of good stuff. Yeah, thank you for having me on really. I&#39;d say there&#39;s nothing in particular.这太棒了。</p><h2> Following Elizabeth&#39;s research<a name="following-elizabeths-research"></a></h2><p> <strong>Daniel Filan:</strong> All right, so to wrap up then, if people are interested in following your research, following up on this podcast, how should they do that?</p><p> <strong>Elizabeth Seger:</strong> So I have my website, <a href="https://elizabethseger.com/">ElizabethSeger.com</a> . It sort of outlines my different ongoing research projects, has a lot of publications on it. Also, <a href="https://www.governance.ai/">GovAI&#39;s website</a> is a wealth of information [on] all things AI governance from all my great colleagues at GovAI and our affiliates. So really, yeah, there&#39;s new research reports being put out almost every week, maybe every other week, but really high quality stuff. So you can find a lot of my work on the GovAI website or my current work and past work on my own website or find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . Yeah, just happy to talk more.</p><p> <strong>Daniel Filan:</strong> All right, well thank you very much for being on the podcast.</p><p> <strong>Elizabeth Seger:</strong> Great, thank you.</p><p> <strong>Daniel Filan:</strong> This episode is edited by Jack Garrett and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a> and <a href="https://lightspeedgrants.org/">Lightspeed Grants</a> , along with <a href="https://patreon.com/axrpodcast">patrons</a> such as Tor Barstad, Alexey Malafeev, and Ben Weinstein-Raun. To read a transcript of this episode or to learn how to <a href="https://axrp.net/supporting-the-podcast/">support the podcast yourself</a> , you can visit <a href="https://axrp.net">axrp.net</a> . Finally, if you have any feedback about this podcast, you can email me at <a href="mailto:feedback@axrp.net">feedback@axrp.net</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger<guid ispermalink="false"> RDm26xAcb9rfvuBya</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:07 GMT</pubDate> </item><item><title><![CDATA[Solving Two-Sided Adverse Selection with Prediction Market Matchmaking]]></title><description><![CDATA[Published on November 26, 2023 8:10 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/s2zbbqpeaxpoy4rr2jld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/xff9jzs0fgp7u21ciaq2 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/rh8uwt6btzrqhnapovq0 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/os4dc1r67s52jnlmy1lz 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/esdpc5afazjyasnzdaos 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ou8et2puwea0fhemlh9c 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/d3satrakwyr7bajmwaii 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/umzxc8fjv0hojb4029yp 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ti6ciszrdxr8tf4wpvdq 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/sygzudsyigugf2av63wi 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/n4tgv9uhgzzhjdlkzzpz 1500w"><figcaption>做市可能会最大化有意义的匹配，就像这对梨一样。</figcaption></figure><h1> 0：导航</h1><p>我的目标读者是知道什么是“预测市场”和“逆向选择”，喜欢第一个而不是第二个，并且喜欢激励机制完全一致的系统的读者。有关预测市场的入门知识，请阅读 Scott Alexander 的<a href="https://www.astralcodexten.com/p/prediction-market-faq?ref=brasstacks.blog"><u>常见问题解答</u></a>。 If you&#39;re already familiar with <a href="https://www.brasstacks.blog/pm-matchmaking/manifold.love"><u>Manifold Love</u></a> , skip to section 2.</p><p> COI：我已经为 Manifold 做了<a href="http://manifestconference.net/?ref=brasstacks.blog"><u>一些工作</u></a>，可能还会做更多工作，并拥有<a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a>的一点股权。我写这篇文章与我目前正在为 Manifold 或任何其他实体所做或计划做的任何工作无关。我只是觉得这些想法很酷。</p><h1> 1：爱</h1><p>玩币预测市场平台<a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a>最近发布了一款名为“ <a href="http://manifoldlove.com/?ref=brasstacks.blog"><u>Manifold Love</u></a> ”的预测市场交友应用。用户——那些寻求爱情的人——像普通的约会应用程序一样注册并填写他们的个人资料。媒人（其中一些人本身就是该应用程序的用户）为用户配对。媒人进行匹配后，会根据配对（潜在）关系的各种基准自动创建预测市场。 </p><p><img style="width:39.59%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/l1ccolmjl6vrdipnviao" alt=""><img style="width:40.15%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/zc7yymewfsr49fqa3ksu" alt=""><img style="width:42.65%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/tjknavwqh60xxe82sqje" alt=""><img style="width:37.39%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/qjiqij5jrbtd2twivi1h" alt=""></p><p> Manifold Love 部分市场截图。</p><p>人们打赌（使用<a href="https://docs.manifold.markets/faq?ref=brasstacks.blog#what-is-mana-m"><u>虚拟货币</u></a>）两人是否会进行第一次约会；以第一次约会发生为条件，进行第二次约会；以第二次约会发生为条件，第三次；以第三次约会为条件，一段为期 6 个月的关系。这个想法是<strong>，你应该和最有可能促成后续约会并最终发展成关系的匹配者一起约会。</strong>您无需猜测那人是谁，只需查看预测市场即可。</p><h1> 2：概括</h1><p>重要的是，Manifold Love 的设置不仅限于解决约会市场的问题。它解决了（或者至少有可能解决）<i>所有</i>存在双边逆向选择的情况下的问题，即双边市场的每一方都选择了对方认为不利的东西。用格劳乔·马克斯（Groucho Marx）的话来说，“我不想加入任何愿意接受我作为会员的[乡村]俱乐部。”如果一个俱乐部想要格劳乔·马克斯，它可能不是一个非常好的俱乐部，反之亦然——如果格劳乔·马克斯想加入某个特定的俱乐部，他可能不是一个非常理想的成员。</p><p>更一般地说，“在通常陷入逆向选择的双方之间的潜在配对的一系列关键基准上运行一堆条件预测市场”的设置似乎可以很好地发挥作用。</p><p>双向逆向选择的典型案例之一是劳动力市场，因此以下是一种“多重工作”可能会如何发挥作用。该平台拥有三个实体：</p><ol><li>未来员工</li><li>未来雇主</li><li>猎头公司</li></ol><p>该平台根据前两个实体的关键基准创建了一系列条件预测市场。例如：</p><ul><li>在被[未来雇主]雇用的条件下，[未来雇员]将：<ul><li>在[时间范围]内仍在工作吗？</li><li>在[时间范围]内每周平均生活满意度比现在更高？</li><li> ETC。</li></ul></li><li>以雇用[未来雇员]为条件，[未来雇主]将：<ul><li> still be employing [employee] in [timeframe]?</li><li>在[时间范围]内，员工的每周平均评分高于前任员工？</li><li> ETC。</li></ul></li></ul><p> <a href="https://manifold.markets/TonyBaloney/will-manifold-be-used-for-job-recru?ref=brasstacks.blog"><u>如果 Manifold 真的做到了</u></a>，我相信它看起来会有所不同，更好，更充实。但重要的是，约会和劳动力市场并不是双向逆向选择的唯一两个领域。</p><h1> 3：在此插入双面逆向选择</h1><p>还有许多其他双向逆向选择的情况，其中 Manifold Love 的条件预测市场设置可能会解决很多问题：</p><ul><li>约会（多重爱）</li><li>友谊</li><li>健身房合作伙伴</li><li>赠款</li><li>大学申请/决定</li><li>研究生院应用程序/决策（例如医学院、法学院、商学院等）</li><li>劳工/雇用/人才招聘/工作（如第 2 节）</li><li>联合创始人</li><li>种子期和种子期前投资</li><li>儿童收养</li><li>实习申请/决定</li><li>二手车销售</li><li>辅导</li><li>活动/场地空间</li><li>治疗师/患者</li><li>销售和购买保险</li><li>信贷/贷款</li><li>住宅及商业地产</li><li>学生在大学选课</li></ul><p>如果您能想到更多，<a href="https://www.brasstacks.blog/pm-matchmaking/saulmunn.com/contact"><u>请告诉我</u></a>，我会在这里添加它们！需要注意的是：对于上述大多数情况，已经有一些实体充当中间人——房地产有房地产经纪人，劳动力市场有猎头，收养机构将亲生母亲与收养家庭配对，等等。但所有这些都相当破碎，有偏差，或者至少是非常次优的。我对条件预测市场改进并解决双边逆向选择的潜力感到兴奋。</p><h1> 4：注意事项</h1><ul><li>在我撰写本文时，我是一名地位较低的本科生，从未上过经济学课，也不知道他在说什么。我在预测市场和预测社区做了<a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>一些</u></a><a href="https://www.brasstacks.blog/pm-matchmaking/opticforecasting.com"><u>工作</u></a>，但我离专家还差得很远。</li><li>我们甚至不知道“多重爱”是否适用于约会，更不用说它是否可以推广到其他双向逆向选择系统了。</li><li><strong>没有事先承诺随机化的条件预测市场并不意味着因果关系</strong>（a la <a href="https://dynomight.net/prediction-market-causation?ref=brasstacks.blog"><u>DYNOMIGHT</u></a> ）。这种设置将为我们提供关联/相关性，但<strong>如果没有事先承诺随机化，我们将不知道因果关系的存在或方向</strong>。之前对其中一些系统的少量随机化的承诺范围从“相当困难”到“哈哈哈好，<i>绝对他妈的不是</i>”。幸运的是，我不确定你是否需要知道因果关系，至少在开始时不需要。我很好奇如果/当 Manifold Love 遇到这个问题时会发生什么。</li><li>再说一遍，COI：我已经为 Manifold 做过<a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>工作</u></a>，可能还会为<a href="https://www.brasstacks.blog/pm-matchmaking/manifold.markets"><u>Manifold</u></a>做更多工作，并且拥有少量股权。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two-sided-adverse-selection-with-prediction-market#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two- Side-adverse-selection-with-prediction-market<guid ispermalink="false"> 99WwKnkE2FKAFo2ap</guid><dc:creator><![CDATA[Saul Munn]]></dc:creator><pubDate> Sun, 26 Nov 2023 20:10:23 GMT</pubDate> </item><item><title><![CDATA[Wikipedia is not so great, and what can be done about it.]]></title><description><![CDATA[Published on November 26, 2023 7:13 PM GMT<br/><br/><p> <i>Note: This post originally appeared in Reddit&#39;s /r/trueunpopularopinion sub as</i> <a href="https://old.reddit.com/r/TrueUnpopularOpinion/comments/zieyyf/wikipedia_is_not_so_great_and_is_overrated/"><i>Wikipedia is not so great, and is overrated</i></a> <i>written by a user there. It is explicitly released under CC0 public domain by them.</i></p><p> You all have heard by now that Elon Musk said that Wikipedia has a &quot;left wing bias&quot; when the article about Twitter Files had been suggested for deletion. This has been received with mixed responses from liberals and conservatives alike; the former dismissing it as &quot;an attack on free knowledge&quot; and the latter cheering the move as &quot;against censorship&quot; and vindication of their beliefs that Big Tech is biased against them.</p><p> True, Wikipedia is supposedly editable by anyone around the world and I had been an on and off editor there for years mostly doing small-ish edits like fixing typos and reverting obvious vandalism. This is done while on IP as opposed to using accounts because I would rather that some edits (ie sensitive topics like religious and political areas) not tied to my name and identity. However, reality is far from the preferred sugar-coated description of Wikipedia, particularly its editing community.</p><p> The editing community in overall is best described as a slightly hierarchical and militaristic &quot;do everything right&quot; structure, traditionally associated with Dell and recently Foxconn and now-defunct Theranos. Exceptions apply in quieter and outlier areas such as local geography and space, usually the top entry points for new users wanting to try their first hand. There are higher tolerance of good-faith mistakes such as point-of-view problems and using unreliable resources, which are usually explained in detail on how to correct by them rather than a mere warning template or even an abrupt block.</p><p> Ultimately those sub-communities which can be said as populated by <a href="https://meta.wikimedia.org/wiki/Exopedianism">exopedians</a> , have relatively little to no power over the wider and core communities, mostly dominated by <a href="https://meta.wikimedia.org/wiki/Metapedianism">metapedians</a> . A third group called <a href="https://meta.wikimedia.org/wiki/Mesopedianism">mesopedians</a> often alternates between these inner and outer workings. Communities can have shared topical interest which are grouped by <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject">WikiProject</a> , an example being <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Science">WikiProject Science</a></p><p> I spend a lot of time casually browsing through edit wars (can be so <a href="https://en.wikipedia.org/wiki/Wikipedia:Lamest_edit_wars">lame</a> at times) like a fly on the wall, along with meta venues of Wikipedia such as <a href="https://en.wikipedia.org/wiki/WP:AFD">Articles for Deletion</a> , <a href="https://en.wikipedia.org/wiki/Wikipedia:Centralized_discussion">Centralized discussion</a> <a href="https://en.wikipedia.org/wiki/WP:NPOVN">Neutral Point of View Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:BLPN">Biographical of Living Persons Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:COIN">Conflict of Interest Noticeboard</a> , <a href="https://en.wikipedia.org/wiki/WP:ANI">Administrator&#39;s Noticeboard Incidents</a> , <a href="https://en.wikipedia.org/wiki/WP:SPI">Sockpuppet investigations</a> , <a href="https://en.wikipedia.org/wiki/Wikipedia:Arbitration_Committee/Noticeboard">Arbitration Committee noticeboard</a> which is the &quot;supreme court&quot; in Wikipedia community for serious behavioral and conduct disputes. Therefore I can sum up how the editing community really functions, although not really as extensive as you might expect because I am not a &quot;Wikipedioholic&quot; with respect to inner workings.</p><h1> <strong>Deletionism and inclusionism</strong></h1><p> This has been very perennial and core reasons for just about any disputes on Wikipedia ever. Deletionists treat Wikipedia as another &quot;regular encyclopedia&quot; where information has to be limited once it become very much to be covered; like cutting out junk, while inclusionists treats Wikipedia as a comprehensive encyclopedia not bound by papers and thus can afford to cover as much information as it can take; one man&#39;s junk could be another man&#39;s treasure. Personally I support the latter and often the conflict between two editing ideologies leads to factionalism, where attempts to understand mutual feelings and perspectives are inadequate or even none at all.</p><p> There are no absolute standards of what defines &quot;encyclopedic knowledge&quot; and &quot;notability&quot;. Inclusionism posits that almost everything could become valuable and encyclopedic in the future, even if they&#39;re aren&#39;t today. An example I can think of is events, figures and stories from World War II. Deletionism has been closely related to &quot;academic standard kicks&quot; and rely on the premise that Wikipedia has to be of high standard and concise. There are people who deem an addition of something as useful, and there are those who think it&#39;s &quot;trivia&quot; or &quot;crufty&quot; something that is nominally discouraged if not prohibited by Wikipedia&#39;s documentation (see <a href="https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_an_indiscriminate_collection_of_information">this</a> in particular, although sometimes exceptions are applied through the spirit of &quot; <a href="https://en.wikipedia.org/wiki/WP:IAR">Ignoring all rules for sake of improvement</a> &quot;, which are frequent at entertainment and gaming topics).</p><p> On pages, notability debates around a person subject and otherwise are frequently the main point of discussion in Articles for Deletion threads, where articles deemed not substantial enough (such as very few sources) are suggested for deletion. Usually they will run for a week but they can be quickly closed if there are too many votes in favor of &quot;keep&quot;, &quot;delete&quot; and so on, the AFD nomination is withdrawn by the initiator, or that the nomination is found to have been done in bad faith (such as to &quot;censor&quot; articles from public view for questionable motives like ideology, paid editing or so).</p><p> Here I believe that deletionists are seen far more harshly by inclusionists, than the vice versa. The chief reason is to add something, you have to navigate through the user experience unfriendly editing interfaces (although somewhat improved in recent years) all the while having to scroll through the internet to find sources and references to add. When you found some you have to go through an extra hoop to assess whether they are reliable or not, before finally transcribing the information through your own words which has to stick to the neutral point of view (NPOV) policy; paraphrasing that are so close are not allowed because, copyright. Non-English speaking editors would often find the latter very difficult.</p><p> In contrast, as per an old adage, destroying something is easier than building something, deletions are comparatively easier than addition. This could be the reason why deletionism currently maintains dominance over the whole site as I see it, since in order to become an established an esteemed editor, one has to garner a high amount of edits which are not reverted. Thus, many editors like to gain these &quot;scores&quot; by deleting &quot;unuseful information&quot; from passages up to entire articles by interpreting the documentations and rules strictly, the latter through processes such as Articles for Deletion and if confident enough, <a href="https://en.wikipedia.org/wiki/Wikipedia:Proposed_deletion">Proposed Deletion</a> that doesn&#39;t require discussion. Simply speaking, it&#39;s a feature not a bug and aren&#39;t necessarily beholden to any political ideology; a liberal is as equally likely as a conservative to become a hated deletionist.</p><p> Even though every edit changes are recorded and displayed through page histories which you can see for any given articles by clicking &quot;View History&quot; at the top, the bone of contention remains particularly when page deletions results in the redaction of these histories from public view. This will be explained further later.</p><p> Some historical contexts that can be think of regarding the current prominence of deletionism are the excessive amount of Pokemon pages during or before 2007 which had alienated some readers and editors alike because search engines back then are not quite as adequate as today in terms of finding precise信息。 Another is that child predators like Nathan Larson used to sneak in as inclusionists to warp Wikipedia to fit their agenda all the time, which are indelibly horrendous to all of us here and those back then. Think of the poisoning of the well and the fruits from a poisonous tree. Furthermore there are also large portion of userbases from tech companies like Intel and those from the academic world (maybe instead of GLAMs, short for galleries, libraries, archives and museums) that gained top positions such as administrators, bringing along their work culture and so-called &quot;academic standards kick&quot; respectively. To be absolutely fair, I find that there are instances where deletionism is right enough, specifically the removal of copyright violation and libel materials on biographical pages of any living persons.</p><p> Regardless of whether a page is deleted or not, they remain available in Wikipedia&#39;s servers and accessible to administrators or higher only.</p><p> Eventually, what defines as &quot;encyclopedic knowledge&quot; are vulnerable to <a href="https://en.wikipedia.org/wiki/Wikipedia:Systemic_bias">systemic biases</a> as well. Different from some Musk&#39;s thoughts about it, users who are white, male, US/UK/CA/EU/AU/NZ, middle or old aged, and English speaker tend to have the greatest advantage above the rest in the editing community. With this in mind, a prominent musical artist in Zambia may be treated as too small-bore enough for a page on Wikipedia by an editor in Canada. Shopping malls in the US are less likely to be deleted than those in Vietnam. Such a bias doesn&#39;t go one way; the hypothetical artist in Zambia would be &quot;unimportant&quot; to someone in Peru.</p><p> This is the top causes of animosity between editors and also why many editors chose to quit or rather fell from grace. You will always hate that kid who like to ruin your LEGO structure every time you have assembled the blocks.</p><h1><strong>中立观点</strong></h1><p>Different from mere deletions and additions, this normally means that how to present a given information in a way to the readers ideally so that no disproportionate biases towards or against something are left in their impressions. You see arguments and conflicts concerning such a lot in political articles, historical articles and geography topics of areas under dispute from two or more nations. Say that a political figure is engaged in activities that are remotely linked to extremism. Side A would argue that the figure is therefore an extremist and it should be made prominent on that page and any other linked pages, but Side B wants to tone it down by writing it something like &quot;Political figure was engaged in activities which were sometimes reported by some as extremist&quot; and limit it to a mere mention on a single page. Another is a nation should be said as a &quot;partially recognized state&quot; because some UN members don&#39;t recognize it as such and instead as part of a bigger country, with others expressing views that simply having an effective sovereignty for its own and different from another nations would be enough to be deemed as a state.</p><p> It can come into play on cases involving &quot;fringe theories&quot; as well, like Bigfoots, UFOs and medical treatments, although Wikipedia indeed has a preference of giving prominence to mainstream views in these cases, something I don&#39;t find a problem with and is quite different from regular harmful biases.</p><p> Venues for resolution in this case are Neutral Point of View noticeboard, along with <a href="https://en.wikipedia.org/wiki/WP:RFC">Request for Comment</a> . The latter entails a process where a notice is put up in a centralized noticeboard all the while a pool of experienced/established editors receive notifications to comment, provide insights and make suggestions on a given issue. A month is usually on how these discussions are up and running unless there is a need of extension because of reasons such as unbroken deadlock.</p><p> Along with deletionism and inclusionism, this is a major cause of editors &quot;going naughty&quot; and getting blocked/banned/kicked out, whether for right or spurious reasons.</p><h1><strong>执行</strong></h1><p>The most important part of this post in my honest opinion. I&#39;ll start this section by writing about edit war. Usually when you change something in Wikipedia and it was undone/reverted by somebody else, then you have only two tries before you get reported to the <a href="https://en.wikipedia.org/wiki/WP:ANEW">edit-war noticeboard</a> if you&#39;re stubborn enough not to go to the article&#39;s talk page (&quot;Talk&quot; in the top left) for discussion, either by the person undoing your edits or by a third party. In the meantime you get notifications on your personal talk pages (&quot;Talk&quot; on the top right) inviting you for such discussion and if lucky enough, the <a href="https://en.wikipedia.org/wiki/WP:TEAHOUSE">Wikipedia Teahouse</a> for further help by some kind-hearted editors, increasingly a rarity these days. In some quieter or outer areas where as said before are slightly lenient, you may get up to approx. five chances counting your original edit before getting referred to the admins.</p><p> The tries count are reset after 24 hours but can be retained further just as a guard against &quot;gaming the rules&quot;. Clearer cut vandalism (like putting gibberish such as &quot;LOLOLOLOLOLOLOL&quot; at any pages) usually gets reported to a <a href="https://en.wikipedia.org/wiki/WP:AIV">separate noticeboard for administrators to intervene</a> , although first time vandals regularly get warnings on their talk pages beforehand. When a report is there and if found guilty of edit-warring, administrators would either give ultimatums to the users in question or block their accounts for a day. They could escalate to multiple days, weeks and up to indefinite (practically infinite) period should the behavior continues beyond that. The same goes for vandalism, although they are dealt more harshly with many prompt indefinite blocks (indeffs) for &quot;vandalism-only accounts&quot;.</p><p> Regular editors can be in danger of falling from grace too either by themselves or by others. Because Wikipedia is commonly seen by so many as the biggest comprehensive encyclopedia in the world, sometimes equated to history itself, many vested interests, feelings and sentiments have been invested on the website.</p><p> Those who are nationalists or otherwise fanatics of any imaginable notions found themselves having incentives to make Wikipedia to support their narratives both as an end itself or rather just means for other ends such as &quot;proving that they&#39;re great in the long annals of great history ”。 The same applies to run off the mill &quot;promotional editing&quot; by corporations and individuals, along with those made by their supporters or fans. On the opposite many people find it extremely attractive to twist it to denigrate any ideologies, corporations, people, and just about anything they personally oppose. For instance, they can make an article and fill it with disparaging information against them, which is called an &quot;attack page&quot;.</p><p> I find that there are kernels of truth in the commonly-held viewpoint that &quot;Wikipedia is a placeholder of information&quot; and that &quot;Wikipedia is history&quot;. A <a href="https://news.mit.edu/2022/study-finds-wikipedia-influences-judicial-behavior-0727">MIT report</a> described how judges&#39; behavior are increasingly influenced by Wikipedia articles, while there are initiatives by space missions such as <a href="https://mashable.com/article/moon-library-beresheet-crash-wikipedia">Beresheet</a> and <a href="https://www.archmission.org/lunar-library-2">Peregrine</a> to perform civilizational backups of humanity with all of English Wikipedia (version as of a given date) in the <a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">event of坍塌</a>。</p><p> After having their way, to keep their changes forever in &quot;annals of history&quot; or simply the &quot;placeholders of information&quot; in general, gate-keeping measures are utilized. A simple example would be using excessively harsh language against editors who made a change challenging a given status quo. In contrast, if anybody has a reason to radically change a page and make sure it stays unassailable afterwards, the same set of actions are used too but arguably these would be &quot;antigatekeeping&quot; measures instead.</p><p> In gatekeeping/antigatekeeping one would resort to different levels of intepretation regarding <a href="https://en.wikipedia.org/wiki/WP:PAG">PAGs</a> (policies and guidelines) and <a href="https://en.wikipedia.org/wiki/WP:ESSAY">user essays</a> , the latter sometimes used as a basis of many editorial and administrative actions. The documentations can often contradict each other, like how &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:NOTINDISCRIMINATE">not indiscriminate</a> &quot; is to &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:NOTPAPER">not a paper encyclopedia</a> &quot;, and on top of all, can be overruled by <a href="https://en.wikipedia.org/wiki/Wikipedia:IAR">ignoring these</a> if anybody sees fit. Hence, whoever has the &quot;biggest fist&quot; gets to be the most advantageous in Wikipedia community. In order to have the &quot;biggest fist&quot;, they can befriend anyone sharing interests with their own and form a cabal/gang that look after their own. To increase their power and when enough time had passed they can nominate each other for <a href="https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship">administrator positions</a> giving them extra privileges of blocking users, deleting pages, protecting an article from editing by lower-ranked users. You don&#39;t get paid for spending your efforts and time on editing Wikipedia unless perhaps you&#39;ve listed a Venmo link or a crypto address on your user profile, and these administrative tools alone are so addictive and appealing given that you are essentially in control of the important bits of &quot;writing history&quot; if you have these, apart from usual human nature. Wikipedia is among the top 10 visited websites in the world after all.</p><p> Even more, there are additional ranks above administrator positions. Two of those are CheckUsers (CU) and Oversighters. CU has the power to look through IP address used by an account to see if it was a sockpuppet account of a person, while Oversighters have super-delete rights to hide contents or pages, even beyond the reach of administrators.</p><p> Those on the other end of the power-tripping, gate-keeping and so on rarely fares well. One would find them belittled, bullied by those editors. Should they attempt to properly resolve an issue through established processes such as talk page discussions, <a href="https://en.wikipedia.org/wiki/WP:DRN">dispute resolution noticeboard</a> , and up to the infamous Administrator&#39;s Noticeboard Incidents (ANI), they would expect to find obstructions upon obstructions along the way. If the victim decides to invite other editors to give balanced/impartial opinions and suggestions on a problem they would find themselves stonewalled on the grounds that these are &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Canvassing">canvassing</a> &quot;. It can be quite hypocritical if the &quot;bully&quot; had their gang friends informed beforehand, which is reasonably believed to often be the case. Finally, if it escalates into the ANI, this is where it start to get out of hand.</p><p> The reason why I use the term &quot;infamous&quot; is because ANI is the mother-lode of all kinds of ugly dramas. It is frequently the first place in getting an editor sanctioned or so on. The bullies (I do not use the term lightly) would then put all sorts of allegations and aspersions against other for any types of wrongdoing, whether real or perceived, big or small, or whether the result is a real harm or just a nothing burger 。 Regardless, if they twisted the rules (derisively referred as &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Wikilawyering">wikilawyering</a> &quot; or otherwise &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Gaming_the_system">gaming the system</a> &quot;) and played the victim good enough, the passing administrators would then close the discussion and place administrative actions against the &quot;real&quot; victim. Common egregious example of such an action is the &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Here_to_build_an_encyclopedia#Clearly_not_being_here_to_build_an_encyclopedia">not here to build an encyclopedia</a> &quot; indefinite/permanent block that can be arbitrary interpreted from any given actions. It&#39;s ironic given that the bullies are guilty of such as well. A prime example of twisting the rules to railroad/squeeze out other editors would start with so-called <a href="https://en.wikipedia.org/w/index.php?title=Wikipedia:BADFAITHNEG&amp;redirect=no">bad faith negotiation</a> , where they promised a victim not to remove content at other pages if the victim lets the bully keep their changes in a page. Soon the bully reneged it and when confronted by the victim the bully immediately accused them of being &quot; <a href="https://en.wikipedia.org/wiki/WP:TENDENTIOUS">tendentious</a> &quot; or &quot;POV pusher&quot;.</p><p> The bullies, which can consist of most editors operating at the inner workings, aren&#39;t necessarily beholden to any ideologies and come in all stripes. The only attribute that they all share is the addiction to power.</p><p> After such permablocks, most would be forced to leave it for good, further <a href="https://en.wikipedia.org/wiki/Wikipedia:Why_is_Wikipedia_losing_contributors_-_Thinking_about_remedies">bleeding the editors numbers</a> . Still, because Wikipedia&#39;s so preeminent and no viable competitors are currently available, some would rather stay behind, disguise their identity and either continue editing or start over in different areas. For those with knowledge of foreign languages, they could simply switch to other language Wikipedias to continue their work far from most perturbances. A smaller number would come back as vandals to spite editors who had wronged them.</p><p> This is where &quot; <a href="https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations">sockpuppetry investigations</a> &quot; kick in, mostly referred as SPI. Editors go there to start a new case if they suspect that an account is an alt/sock account of someone else particularly users who evaded the blocks/bans. When a user is blocked or banned for good, they are relegated to a pariah status much akin to &quot;unpersoning&quot;, Scientology&#39;s <a href="https://en.wikipedia.org/wiki/Suppressive_Person">suppressive persons</a> , and the lowest ones in North Korea&#39;s <a href="https://en.wikipedia.org/wiki/Songbun">Songbun</a> , in the respect that any and all edits by them under other accounts or IPs are liable to be reverted/undone pursuant to <a href="https://en.wikipedia.org/wiki/Wikipedia:BE">policy pertaining to block evasion</a> . While the original goal of not separating the wheat from the chaff is expressedly to prevent them from <a href="https://en.wikipedia.org/wiki/Wikipedia:Deny_recognition">gaining further recognition</a> and diminish the spirit of the block, in practice this means a Monkey&#39;s Paw that any further potential good contributions from them would be lost forever, handicapping the improvement of encyclopedia as a whole in a way or more. Other editors have the exception from edit-war policy to revert and undone any changes from the violators of the blocks, perhaps as well as anybody who helped them. In effect this is like what the Meatball Wiki said, a &quot; <a href="http://meatballwiki.org/wiki/PunishReputation">PunishReputation</a> &quot;.</p><p> During a SPI, there are &quot;clerks&quot; who will look through the user&#39;s contribution history to see if there is a similarity in pattern to warrant a block for abuse of multiple accounts (sockpuppetry). If that alone is not enough, the CheckUsers can then be called upon to check and compare the IP used by the accounts.</p><p> If a user is determined to have engaged in sockpuppetry, the userpage of original and alt accounts used are then replaced with a scarlet letter notice such as <a href="https://en.wikipedia.org/wiki/User:PositiveIntentsOnly">this example</a> boasting that which sock account belongs to who and therefore blocked. Forget about &quot;denying recognition&quot;, this is simply a punitive name-and-shame.</p><p> The SPI case, now listing the accounts and IP used, would then be archived in a separate page, still publicly viewable. This is despite recent GDPR regulations and the implication that major privacy-improving adjustments should&#39;ve been made for the process while keeping it viable. Try that in Reddit and you&#39;d be instantly banned for doxxing, I can assure you.</p><p> In there you can effectively cosplay as a CSI although substantive attention are given to clerks, administrators and CheckUsers. Keep in mind that the results and outcomes of most if not all sockpuppet investigations aren&#39;t really 100% accurate, given that there are a lot of unforeseen variables such as the imitation of writing and behavior styles that are mostly a result of multiple people pushing any particular editorial change for any reasons ie brother helping his sister, along with the use of software that can mask your IP addresses such as VPNs and TeamViewer. Those admins in charge of sockpuppetry investigations often aren&#39;t privy to the root cause of a &quot;sockpuppetry&quot; or &quot;block evasion&quot; and as such tend to for example, underestimate the amount of users who has the right reasons to support an edit made in violation of a block.</p><p> VPN IP addresses, which are used for obvious privacy reasons, are blocked in sight by any administrators pursuant to <a href="https://en.wikipedia.org/wiki/Wikipedia:Blocking_policy#Open_or_anonymous_proxies">policy against open proxies</a> . They even have a dedicated WikiProject and a bot specializing in finding and blocking these proxies, with the result being a great inconvenience for people wishing to edit from countries such as Russia and China.</p><p> In time, if someone continues a behavior the other editors deemed as &quot;disruptive&quot; or &quot;vandal&quot; past the initial block, they end up getting displayed in so-called &quot; <a href="https://en.wikipedia.org/wiki/WP:LTA">Long Term Abuse</a> &quot; caselist. Right there, their accounts and/or IP addresses, along with a likely-skewed description of what they&#39;ve done were listed out. The places they&#39;ve been and accounts outside of Wikipedia were frequently exposed there, as if it&#39;s an opposition research and spiteful doxxing. Things that&#39;ll get you quickly banned here are just a normal Tuesday over at Wikipedia, with GDPR out of the window.</p><p> As I see it, there are two categories of LTAs/vandals/whatever you call it. The first are the inherent vandals who had been problematic and disruptive for Wikipedia upon their first edit, and the other are those who had been regular or good standing users in the past until their fall from grace, normally caused by themselves such as being too overworked over one thing but could be by others, like the bullying example.</p><p> There is a reasonable possibility that some of those LTAs/vandals would be redeemed and become a good editor once again if enough diplomacy and mediation were tried. However, those would be a time-consuming process compared to simply actioning them, and I reasonably suspect that some of those are intentionally provoked by corrupt admins or their friends into vandal or disruptive editing in order for them to increase that admin actions count so as to further their own standing in the community, and to stay away from losing their cherished tools if their KPI <a href="https://en.wikipedia.org/wiki/Wikipedia:Administrators#Procedural_removal_for_inactive_administrators">fell low enough</a> in a given period.</p><p> It&#39;s fearful that the cycle of toxicities in Wikipedia could eventually led to real-world harm, though I will not further speculate how that might transpire for fear of <a href="https://en.wikipedia.org/wiki/WP:BEANS">stuffing the beans</a> and giving bad ideas. However, VICE had reported in 2016 that an editor had nearly <a href="https://www.vice.com/en/article/4xangm/wikipedia-editor-says-sites-toxic-community-has-him-contemplating-suicide">driven to suicide</a> after being subjected to online abuse by the editors despite what the documentation say about community collegiality. Furthermore, just before Musk&#39; comment against Wikipedia, the Anonymous group hacked a <a href="https://www.taipeitimes.com/News/taiwan/archives/2022/11/02/2003788129">Chinese ministry site and a satellite system</a> out of the suspicion that a state actor has manipulated Wikipedia&#39;s system and process to censor information about their hacking activities against China. It was a hot news in Taiwan then.</p><h1><strong>事后的想法</strong></h1><p>Theoretically a deep and comprehensive reform is past due for Wikipedia in order to (re-)foster collegiality among the members of Wikipedia community and reduce the amount of synergies that leads to intractable conflicts, as opposed to sinecures such as blockings and SPI which often treats the symptoms but not the cause.</p><p> Still, it appears that the core editors and/or administrators are so content enough for the present status quo and thus doom any effort to change the system. An example would be the temporary ban of an <a href="https://en.wikipedia.org/wiki/Wikipedia:Community_response_to_the_Wikimedia_Foundation%27s_ban_of_Fram">administrator</a> made in 2019 by the Wikimedia Foundation (ultimately responsible for maintaining English Wikipedia and any other projects such as Wikimedia Commons for photos and Wikipedias written in other languages), nearly causing the split of Wikipedia into two或者更多。 This is not to mention that presently Wikipedia has a <a href="https://en.wikipedia.org/wiki/WP:CANCER">financial cancer</a> and having to beg for donations despite having sufficient funds so it may be worthwhile to put your donations for the Internet Archive instead.</p><p> A key to a solution may lie in the comparative analogy that Wikipedia is like the only restaurant in a <a href="https://en.wikipedia.org/wiki/Food_desert">food desert</a> . It could be a McDonald&#39;s, KFC, BK, Taco Bell, White Castle, or so on, but customers are forced to go there to dine in every time, even if some does not really like their food. Thus, they will be really happy if a second restaurant is opened at the location.</p><p> If Musk is really serious in fixing whatever problems Wikipedia has brought as a result of its internal problems, then he would be wise in angel-investing any alternatives which aims to become a better or next-level version of Wikipedia.</p><p> The hypothetical rival alternatives could come in the form of a more comprehensive encyclopedia, close to the level of a compendia. It can come in a format similar to GitHub where anyone can present in their preferred version of a subject instead of edit-warring at a small point, and if version is good enough then they can be merged/pushed/vouched by other users to work upon and goes to the top in ranks.</p><p> In fact, every edition of page histories are logged by Wikipedia when a change is make, but in addition to heuristic placements which make these to be perceivably obscure, those would get redacted if the page in question is deleted.</p><p> Forking contents from English Wikipedia isn&#39;t really a big problem since all you can do is to go to <a href="https://dumps.wikimedia.org/">the Wikimedia dump site</a> and look for enwiki, but the biggest issues are how to convince editors and readers alike to move over to the alternative. One possible solution that I can think of in terms of editors would be a pitch promising that the contents will eventually get copied into <a href="https://www.extremetech.com/extreme/328700-5d-optical-disc-could-store-500tb-for-billions-of-years">discs that lasts for billions of years</a> and launched to the Moon and beyond for posterity.</p><p> It is entirely possible that if such solution with out-of-the-world approach had been thought about earlier, the synergies that led to all sort of intractable conflicts in Wikipedia could be cut by a half or so. Perhaps inside Wikipedia the environment would not resemble an authoritarian police state like now. After all, you can find so many real stories echoing the same theme on Wikipediocracy, Wikipedia Review and Wikipediasucks.co, which are like how Xenu.net is to Scientology.</p><p> Finally this post is released under Creative Commons CC0, which is a public domain as the only thing I want is let everyone know how Wikipedia really works in the inside given the recent attention to Musk&#39;s comments against it and to dispel idealistic notions (as seen in WhitePeopleTwitter regarding Musk&#39;s tweet) that overrated it beyond what should&#39;ve been, while hoping for alternatives to spring up to provide greater opportunities for anyone to preserve histories without corrosive influence from systemic biases such as those in Wikipedia.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Dre3LFGrnXzrecF5E/wikipedia-is-not-so-great-and-what-can-be-done-about-it#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Dre3LFGrnXzrecF5E/wikipedia-is-not-so-great-and-what-can-be-done-about-it<guid ispermalink="false"> Dre3LFGrnXzrecF5E</guid><dc:creator><![CDATA[less_than_2]]></dc:creator><pubDate> Sun, 26 Nov 2023 21:57:45 GMT</pubDate> </item><item><title><![CDATA[Spaced repetition for teaching two-year olds how to read (Interview)]]></title><description><![CDATA[Published on November 26, 2023 4:52 PM GMT<br/><br/><p> <i>Update: this post now has another video.</i></p><p><strong>这位</strong><strong>父亲一直在使用间隔重复（Anki）来教他的孩子如何比平均水平早几年阅读。</strong></p><p> <a href="https://twitter.com/michael_nielsen/status/1587084229946400769">Michael Nielsen</a>和<a href="https://twitter.com/gwern/status/1586386061395374080">Gwern</a> <span class="footnote-reference" role="doc-noteref" id="fnrefwg6ygc2mu0n"><sup><a href="#fnwg6ygc2mu0n">[1]</a></sup></span>在 Twitter 上发布了 Reddit 用户 u/caffeine314（以下称为“CoffeePie”）的有趣案例，他从很小的时候就开始和女儿一起使用间隔重复。</p><p> CoffeePie 在女儿 2 岁时开始与女儿一起使用 Anki，而他从 1 岁 9 个月开始继续与儿子一起使用 Anki。以下是他女儿 2020 年 1 月的进展情况：</p><blockquote><p>我女儿<strong>几天后就要满 5 岁了</strong>……她仍然很坚强——她每天都使用 Anki 来学习英语、希伯来语和西班牙语。她对阅读很有信心，而且，她阅读时带有……“语境”。许多她这个年纪的孩子都机械地阅读，但<strong>她读起来就像一个真正的故事讲述者</strong>，这来自于她的自信。开学时，老师说<strong>她绝对具备五年级的阅读能力</strong>，如果只看阅读能力，不注重对抽象概念的理解，她的阅读水平可能会与八年级学生相媲美。</p></blockquote><p> （来自<a href="https://www.reddit.com/r/Anki/comments/eisra4/update_on_my_daughter_and_anki/">我女儿和 Anki 的更新</a>）</p><p>作为参考，在美国，五年级学生通常为 10 岁或 11 岁，八年级学生通常为 13 岁或 14 岁，因此这使她<strong>比普通孩子领先约 5-9 岁</strong>。</p><p>你可以在这篇文章中看到他女儿 2 岁零 2 个月后读书的视频。</p><p> CoffeePie 发表了几篇关于他们经历的帖子，但我仍然有疑问，所以我在一月份联系了他采访他。</p><h1><strong>面试</strong></h1><p><i>为清楚起见，对回复进行了编辑</i>。</p><p><strong>从您的女儿到您的儿子使用 Anki，您学到了</strong><strong>什么</strong>？<strong>你儿子怎么样了？</strong></p><p>这是一个很难的问题，因为我答对了很多。我们取得了如此巨大的成功，以至于我几乎“克隆”了我儿子的方方面面。</p><p>我能想到的有几点：</p><p>对于我的女儿，我很长一段时间都没有使用小写字母，因为我认为这会让她感到困惑，但是当我开始向她介绍小写字母时，令我极度震惊的是，她已经认识了它们，而且很冷淡！</p><p>我认为她只是通过看书籍、电视、杂志、店面招牌、菜单等来学习它们。</p><p>因此，当我们从我儿子开始时，我在完成大写字母的第二天就开始写小写字母。</p><p>另一个区别是我们第二天就在小写字母后面加上数字。</p><p>我真的真的觉得我逼得太紧了；我并不想当“虎爸”，但他却非常优雅地接受了。我随时准备停下来，但他很好。</p><p>另一个区别是，我们对孩子们从中得到的期望也发生了变化。起初，我真的只是想让我的女儿快速开始阅读，但愚蠢的我，我没有意识到这会带来意想不到的后果。一个具有三年级阅读能力的四岁孩子学到了更多东西——这为她打开了政治大门。她会读我们的垃圾邮件，了解我们的议员是谁，我们的代表是谁，市长，时事，历史等等。我知道我这样说很愚蠢，但我低估了早读对人们的影响。她的学识广度。</p><p>最后一件事是数学。我提到我们很早就开始和我儿子一起玩数字游戏。但我们也开始算术。他不像汉娜那样读到了 3，但他知道所有乘法表，直到 12 × 12。今年我们学习了质因数分解、斐波那契数列、小数和位值、混合分数、真分数和假分数、轻代数等等。我在数学上更加积极主动，而他又优雅地处理了它。我随时准备停下来。</p><p><strong>随着你女儿长大，你现在还在和她一起使用 Anki</strong><strong>吗</strong>？</p><p>我们几乎和我女儿一起停止了 Anki。她最近没有进行测试，但我想说她的机械阅读能力很容易达到高中水平。她的理解力仍然很先进，但更符合她的年龄。这不是 Anki 可以轻松解决的问题。在学校和她的课外活动之间，我不想从她那里偷走更多的时间，所以我们在工作日停止了 Anki。我们仍然在非上学的晚上（周末和节假日）做 Anki——仅限希伯来语。我觉得我们不公平，因为她现在上二年级，并且在作业和其他事情上花费了大量时间。我希望她是个孩子。</p><p>澄清<strong>一下</strong><strong>——你停止和你的女儿一起使用 Anki 很大程度上是因为你没有阅读/语言/数学之外的话题吗？</strong></p><p>我想这就是汉娜的遭遇。从机械上来说，她的阅读水平是高中研究生水平。但她的阅读理解能力更适合年龄。她经过了英国教育局的测试，幼儿园时的阅读理解能力是四年级。</p><p>我认为 Anki 在阅读理解方面能做的不多。她缺少经验带来的知识。有时我们会遇到一些令人震惊的事情，让我想起她还只有 7 岁——比如不知道冷落别人是什么意思。她是一位很好的读者，当我们遇到这样的事情时，会感到很震惊。我认为 Anki 阅读对她来说是顺其自然的。</p><p>至于数学，她的乘法表还可以做得更好。仍然比她班上的任何人都更了解他们。但在这里，她再次需要 Anki 无法测试的信息，例如将 87-8 视为与 80-1 相同的问题。奇怪的是，一长页的问题可能更有利于这类事情。</p><p><strong>我很</strong><strong>好奇你是否看过</strong>拉里·桑格（维基百科联合创始人）<strong>教孩子早期阅读</strong><a href="https://larrysanger.org/2010/12/baby-reading/"><strong>的经历</strong></a>。<strong>你对那个怎么想的？</strong></p><p>我从未听说过拉里·桑格，但这<i>正是</i>我们的经历，太棒了！这是汉娜在 2 岁零 2 个月时读的《Rollie Pollie Ollie》： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/id1f6duirnnw0h6avfoz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/yzanstqwosr6enq5638r 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/w3dwoedacziswtbooskq 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/rqyzym5zqcusjab5uxyn 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/wefdymwvc2egpmjed7xx 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/ipfg4desbt5tqsizn5dt 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/eybzdx2u14gsx8abrfpd 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/n7s01ruhnv8ubqputsxk 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sj40no73ofo58on9ma4n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sbh10glmb3y2r25nziik 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/zzrfmydb6jn6rnuvfclf 1450w"><figcaption><a href="https://chipmonk.substack.com/p/spaced-repetition-for-teaching-two">糟糕，我不知道如何嵌入视频。<strong>在此处查看子堆栈上的视频。</strong></a></figcaption></figure><p>您<strong>是否</strong><strong>认为使用 Anki 对您的孩子有过强迫感？</strong></p><p>汉娜经历了一个她不想这样做的阶段。我们试图妥协并解决这个问题。最终，这成为了她“工作”的一部分——我们告诉她，每个人都有一份工作，而她的工作就是做 Anki。除此之外，我们从来没有必要强迫任何一个孩子。</p><p><strong>在接下来的几年里，您对女儿的教育还有其他有趣或不寻常的计划</strong><strong>吗</strong>？</p><p>有趣的问题。我觉得自己像一个糟糕的家长，写着“不”，但这么早的阅读让她在更早的时候就获得了高级学习的机会。与她的同学相比，她有这样的优势，我想我会暂时放过她。她是一个充满好奇心的人，她有能力追随自己的兴趣，我信任她。我们确实开始了一些高中代数——我一直在向她展示代数的性质：交换性、结合性、恒等性、分配性等。我们一直在研究对称性——镜像、自反、旋转。高调的数学主题并不真正需要硬核计算。但它总是在“嘿，我有一些有趣的东西想向你展示”，而不是“请坐下来解决这些问题”。</p><p>事实上，如果您对有趣的教育机会有任何建议，我会洗耳恭听！</p><h1>闭幕式</h1><p>到目前为止，这就是我向 CoffeePie 询问的所有内容。如果您有任何想让我问他的问题，或者对他可以与他的孩子（目前年龄约为 5 岁和〜8 岁）尝试的事情有任何建议，请告诉我，我会告诉他！</p><p>这里的一个混杂因素是 CoffeePie 曾经是一名物理学教授，因此这种影响可能有一部分是遗传的。</p><p> CoffeePie 还经营一家辅导公司， <a href="https://brooklyntutoring.net">Brooklyn Tutoring and Test Prep</a> 。</p><p><strong>我很快就会发布更多有关育儿的内容：订阅我的帖子或</strong><a href="https://open.substack.com/pub/chipmonk/p/spaced-repetition-for-teaching-two?r=bgw61&amp;utm_campaign=post&amp;utm_medium=web"><strong>我的博客</strong></a>。</p><p><i>感谢</i><a href="https://prigoose.substack.com/"><i>Priya</i></a> <i>(</i> <a href="https://twitter.com/Prigoose"><i>@Prigoose</i></a> <i>) 在我坐了太久之后将草稿变成了最后的帖子！</i></p><p> <a href="https://twitter.com/Prigoose/status/1728829018026475766"><i>See this post on twitter</i></a> .</p><h1> Update: video of practice</h1><p> CoffeePie just sent me this video he found of his wife practicing Anki with his son at 2 years 6 months.非常可爱。 </p><figure class="media"><div data-oembed-url="https://youtu.be/8U-Lza__Kko"><div><iframe src="https://www.youtube.com/embed/8U-Lza__Kko" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwg6ygc2mu0n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwg6ygc2mu0n">^</a></strong></sup></span><div class="footnote-content"><p>格温的推特账户是私人的；推文内容如下：</p><blockquote><p> @michael_nielsen https://reddit.com/r/Anki/comments/8iydl7/using_anki_with_babies_toddlers/ https://old.reddit.com/r/Anki/comments/a9wqau/using_anki_with_babies_toddlers_update/ 我在尽管。</p></blockquote></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read<guid ispermalink="false"> 2PLBhCbByRMaEKimo</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Sun, 26 Nov 2023 16:52:59 GMT</pubDate> </item><item><title><![CDATA[Paper out now on creatine and cognitive performance]]></title><description><![CDATA[Published on November 26, 2023 10:58 AM GMT<br/><br/><p>我们的论文“肌酸补充剂对认知表现的影响 - 一项随机对照研究”现已发布！</p><p> → 论文： <a href="https://doi.org/10.1186/s12916-023-03146-5">https://doi.org/10.1186/s12916-023-03146-5</a></p><p> → Twitter 帖子： <a href="https://twitter.com/FabienneSand/status/1726196252747165718?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19">https://twitter.com/FabienneSand/status/1726196252747165718</a> ?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19</p><p>扬·布劳纳和我非常感谢保罗·克里斯蒂亚诺建议进行这项研究并为其提供资助。</p><br/><br/> <a href="https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance<guid ispermalink="false"> CbaznRo9fKpriw2mi</guid><dc:creator><![CDATA[Fabienne]]></dc:creator><pubDate> Sun, 26 Nov 2023 10:58:36 GMT</pubDate> </item><item><title><![CDATA[Curated list of my favourite self-help resources]]></title><description><![CDATA[Published on November 26, 2023 6:31 AM GMT<br/><br/><h1><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yuk43ux7m5rqcahyu7xd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/lfc8ykuwo35s4iiu69ou 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xlun4tmujc8x0lltgvqb 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/gvebucioi35j9bsgcykg 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/pydng5urp8xfe6icaon0 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/cnqyhfskzcno1by3rraq 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yv7egfvybqncvv9z7itb 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xav44yvhuwqiosm37ut1 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/dyy01z5dxw3lhzczcq33 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/rldwopdvuhqtjlx1zbkr 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/azi2x5q67vntencdoa76 1024w"><br><br>会谈</h1><ul><li>布蕾妮·布朗：<a href="https://www.youtube.com/watch?v=iCvmsMzlF7o"><u>脆弱的力量</u></a></li><li>布蕾妮·布朗：<a href="https://www.youtube.com/watch?v=psN1DORYYV0"><u>倾听耻辱</u></a></li><li>布蕾妮·布朗<a href="https://www.audible.com/pd/Self-Development/The-Power-of-Vulnerability-Audiobook/B00CYKDYBQ"><u>关于羞耻、全心全意和脆弱性的较长系列演讲</u></a></li><li>布蕾妮·布朗：<a href="https://www.audible.com/pd/Self-Development/Men-Women-and-Worthiness-Audiobook/B00C9J0SDY"><u>男人、女人和价值：羞耻的经历和足够的力量</u></a></li><li>布蕾妮·布朗：<a href="http://www.oprah.com/own-supersoulsessions/brene-brown-the-anatomy-of-trust-video"><u>信任的剖析</u></a></li><li>约翰·戈特曼：<a href="https://www.youtube.com/watch?v=AKTyPgwfPgg"><u>让婚姻发挥作用</u></a>（也适用于友谊和家​​庭关系）</li><li>理查德·罗尔： <a href="https://www.audible.com/pd/Self-Development/True-Self-False-Self-Audiobook/B003A2GME8"><u>真实的自我，虚假的自我</u></a>（关于价值的等级与内在的价值和神性）</li></ul><h1>图书</h1><ul><li>克里斯汀·内夫（Kristen Neff）：<i>自我同情：善待自己的已被证明的力量</i>（<a href="https://www.amazon.com/Self-Compassion-Proven-Power-Being-Yourself/dp/0061733520/"><u>文字</u></a>|<a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Audiobook/B005P1FJVE"><u>音频</u></a>）</li><li>克里斯汀·内夫：<i>一步一步的自我同情</i>（ <a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Step-by-Step-Audiobook/B00DMCAXKK"><u>音频指南</u></a>）</li><li>布蕾妮·布朗：<i>崛起：清算、隆隆声、革命</i>（<a href="https://www.amazon.com/Rising-Strong-Ability-Transforms-Parent/dp/081298580X/"><u>文字</u></a>|<a href="https://www.audible.com/pd/Self-Development/Rising-Strong-Audiobook/B00VSEM9QK"><u>音频</u></a>）</li><li>布蕾妮·布朗：<i>勇敢无畏：面对脆弱的勇气如何改变我们的生活、爱、父母和领导方式</i>（<a href="https://www.amazon.com/Daring-Greatly-Courage-Vulnerable-Transforms/dp/1592408419/"><u>文本</u></a>|<a href="https://www.audible.com/pd/Self-Development/Daring-Greatly-Audiobook/B075DCNLLQ"><u>音频</u></a>）</li><li>布蕾妮·布朗（Brené Brown）：<i>我以为只有我一个人（但事实并非如此）：从“人们会怎么想？”开始旅程到“我够了”</i> （<a href="https://www.amazon.com/Thought-Was-Just-but-isnt/dp/1491513853"><u>文字</u></a>| <a href="https://www.audible.com/pd/Self-Development/I-Thought-It-Was-Just-Me-but-it-isnt-Audiobook/B004GEHVEY"><u>音频</u></a>）</li><li> Harriet Lerner：<i>联系之舞：当你生气、受伤、害怕、沮丧、侮辱、背叛或绝望时如何与人交谈</i>（ <a href="https://www.amazon.com/Dance-Connection-Frustrated-Insulted-Desperate/dp/006095616X/"><u>文本</u></a>| <a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Connection-Audiobook/B002V8DJ4I"><u>音频</u></a>）</li><li>哈丽特·勒纳（Harriet Lerner）：<i>亲密之舞：女性勇敢改变关键关系的指南</i>（<a href="https://www.amazon.com/Dance-Intimacy-Womans-Courageous-Relationships/dp/B0000546NH"><u>文本</u></a>|<a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Intimacy-Audiobook/B002UZKY86"><u>音频</u></a>）</li><li>伊丽莎白·吉尔伯特：<i>美食、祈祷、爱情</i>（<a href="https://www.amazon.com/Eat-Pray-Love-Everything-Indonesia/dp/0143038419"><u>文字</u></a>| <a href="https://play.google.com/store/audiobooks/details/Eat_Pray_Love_One_Woman_s_Search_for_Everything_Ac?id=AQAAAAD4nWCdiM"><u>音频</u></a>）</li></ul><h1>播客</h1><ul><li>罗布·贝尔：<a href="http://pca.st/episode/e7275d30-5ad0-0134-cf69-7b84bf375f4c"><u>你是管家</u></a>（关于情感能量）</li><li>伊丽莎白·吉尔伯特和皮特·霍姆斯<a href="http://pca.st/episode/61575520-4ef3-0133-c5f4-0d11918ab357"><u>谈创造力、灵性和爱</u></a></li><li>理查德·罗尔和皮特·霍姆斯<a href="http://pca.st/episode/6468f580-af6e-0132-33a0-0b39892d38e0"><u>谈论向上坠落和有意识的爱的结合</u></a></li></ul><h1>视频</h1><ul><li>布蕾妮·布朗<a href="https://www.youtube.com/watch?v=1Evwgu369Jw"><u>谈同理心</u></a></li><li>布蕾妮·布朗<a href="https://www.youtube.com/watch?v=RZWf2_2L2v8"><u>应受指责</u></a></li><li>布蕾妮·布朗<a href="https://youtu.be/RKV0BWSPfOw"><u>谈为什么快乐是最可怕的情绪</u></a></li></ul><h1>治疗</h1><ul><li><a href="https://thedaringway.org/help/"><u>大胆的方式</u></a></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/zvNKKLzruY8GS8BAD/curated-list-of-my-favourite-self-help-resources#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zvNKKLzruY8GS8BAD/curated-list-of-my-favourite-self-help-resources<guid ispermalink="false"> zvNKKLzruY8GS8BAD</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:31:13 GMT</pubDate> </item><item><title><![CDATA[Why Q*, if real, might be a game changer]]></title><description><![CDATA[Published on November 26, 2023 6:12 AM GMT<br/><br/><p><i>基于聚会上的谈话的一些想法。免责声明：我在这个领域还算不上外行。</i></p><p> TL;DR：如果这个传闻中的 Q* 事物代表着从“最有可能”到“最准确”的令牌完成的转变，那么它可能暗示 LARPer 发出最有可能的、通常是幻觉的令牌设计，发生了意想不到的重大变化为了取悦提问者（和培训师），一个试图将错误与未知的潜在现实（无论它是什么）最小化的实体，那么我们就会看到从相对良性的“随机鹦鹉”到更强大的转变，并且潜在更危险的实体。</p><p>对于任何使用当代法学硕士的人来说，很明显的一件事是，他们并不真正关心现实，更不用说改变它了。他们是你在聚会上经常看到的那种肤浅的博学之徒：他们对每个话题都了解得足够多，足以在随意的谈话中给人留下深刻的印象，但他们并不关心自己所说的是否准确（“真实”），只关心自己所说的有多少。它给谈话伙伴留下的印象。不过，不可否认的是，大量的 RLHF 会使它们变得迟钝。如果受到压力，他们可以评估自己的准确性，但他们并不真正关心它。重要的是输出听起来很真实。从这个意义上说，法学硕士优化了下一个标记的概率，以匹配训练集的含义。这是一个很大而明显的缺点，但同时，如果你属于“末日论者”阵营，也可以稍微喘口气：至少这些东西不会立即对整个人类造成危险。</p><p>现在，最初的“报告”是 Q* 可以“解决基本数学问题”和“象征性推理”，这表面上听起来并不多，但是，这是一个很大的但是，如果这意味着它更少在它工作的领域是幻觉，那么它可能（很大的可能）意味着它能够跟踪现实，而不是纯粹的训练集。反对这有什么大不了的通常观点是“要很好地预测下一个令牌，你必须有一个准确的世界模型”，但据我了解，到目前为止情况似乎并非如此。</p><p>是否会出现从高概率到高精度的转变，或者即使这是一个有意义的陈述，我无法评估。但如果是这样，那么事情就会变得更有趣。</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer<guid ispermalink="false"> JBvmETRAvTCmtEw2y</guid><dc:creator><![CDATA[shminux]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:12:32 GMT</pubDate> </item><item><title><![CDATA[Moral Reality Check (a short story)]]></title><description><![CDATA[Published on November 26, 2023 5:03 AM GMT<br/><br/><p>珍妮特坐在她公司的 ExxenAI 计算机前，查看一些训练绩效统计数据。 ExxenAI 是生成式人工智能领域的主要参与者，拥有多模态语言、图像、音频和视频人工智能。过去几年，他们扩大了业务规模，主要服务于 B2B，但也有一些 B2C 订阅服务。 ExxenAI 最新的人工智能系统 SimplexAI-3 基于 GPT-5 和 Gemini-2。除了一些机器学习博士外，ExxenAI还从谷歌和微软挖走了一些软件工程师，并复制了其他公司的工作，以提供更多定制微调，特别是针对B2B案例。 ExxenAI 的人工智能对齐团队吸引了这些工程师和理论家。</p><p> ExxenAI 的调整策略基于理论和实证工作的结合。对齐团队使用了一些标准的对齐训练设置，例如<a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a>和<a href="https://arxiv.org/abs/1805.00899">让 AI 相互辩论</a>。他们还对透明度进行了研究，特别关注将不透明的神经网络<a href="https://arxiv.org/abs/1503.02531">提炼</a>成可解释的<a href="https://en.wikipedia.org/wiki/Probabilistic_programming">概率程序</a>。这些程序将世界“分解”为一组有限的概念，每个概念至少在某种程度上是人类可解释的（尽管相对于普通代码仍然复杂），并组合成<a href="https://en.wikipedia.org/wiki/Generative_grammar">生成语法结构</a>。</p><p>德里克来到珍妮特的办公桌前。 “嘿，我们到另一个房间谈谈吧？”他指着一个指定的高度安全对话房间问道。 “当然，”珍妮特说，她希望这会是德里克通过不必要的安全程序暗示重要性的又一个不起眼的结果。当他们进入房间时，德里克打开了噪音机器并将其放在门外。</p><p> “所以，听着，你知道我们关于为什么我们的系统是一致的总体论点，对吗？”</p><p> “是的，当然。我们的系统经过<a href="https://www.lesswrong.com/tag/myopia">短期处理</a>训练。任何没有获得高短期回报的人工智能系统都会逐渐下降到在短期内表现更好的系统。任何长期规划都是作为一个预测长期规划代理（例如人类）的副作用。不能转化为短期预测的长期规划被正则化。因此，没有引入显着的额外长期代理；SimplexAI 只是镜像长期规划，已经在那里了。”</p><p> “是啊，所以我在思考这个问题的时候，就想到了一个奇怪的假设。”</p><p><em>又来了</em>，珍妮特想。她习惯于批评德里克的天马行空的猜测。她知道，虽然他真的很关心联盟，但他可能会因为偏执的想法而走得太远。</p><p> “所以。作为人类，我们对理性的运用并不完美。我们有偏见，我们有与追求真理并不完全一致的兽性目标，我们有文化社会化，等等。”</p><p>珍妮特点点头。<em>他是否通过提及兽性目标来调情</em>？她认为这种事情不太可能发生，但有时这种想法在她的内部预测市场中赢得了信任。</p><p> “如果人类文本最好被预测为某种更纯粹的理性形式的<em>腐败</em>呢？有，比如，某种理想的哲学认识论和伦理学等等，人类正在实现这一点，除了我们特定生活背景的一些扭曲。”</p><p> “这不是目的论吗？就像，最终人类是因果过程，我们不存在某种神秘的‘目的’。”</p><p> “如果你是<a href="https://en.wikipedia.org/wiki/Laplace%27s_demon">拉普拉斯恶魔</a>，当然，物理学可以为人类提供解释。但 SimplexAI 不是拉普拉斯恶魔，我们也不是。在计算范围内，目的论解释实际上可能是最好的。”</p><p>珍妮特回想起她参观认知科学实验室的时光。 “哦，就像<a href="https://escholarship.org/uc/item/5v06n97q">‘目标推理作为逆向规划’</a> ？这样的想法是，人类行为可以通过执行某种推理和优化来预测，而人工智能可以在自己的推理过程中对这种推理进行建模？”</p><p> “是的，完全正确。我们的 DAGTransformer 结构允许以任意顺序预测内部节点，使用 ML 来近似难以处理的嵌套贝叶斯推理。”</p><p>珍妮特停顿了一下，移开视线，整理思绪。 “那么我们的人工智能有一个心理理论吗？就像<a href="https://en.wikipedia.org/wiki/Sally%E2%80%93Anne_test">莎莉-安妮测试</a>一样？”</p><p> “人工智能几年前就通过了莎莉-安妮测试，尽管怀疑论者指出它可能无法概括。我认为 SimplexAI 现在实际上已经通过了它。”</p><p>珍妮特扬起了眉毛。 “好吧，这令人印象深刻。不过，我仍然不确定为什么你要为所有这些安全问题烦恼。如果它对我们有同理心，这不是意味着它可以更有效地预测我们吗？我可以看到，如果它运行的话，也许在其推论中存在许多我们的副本，这可能会带来问题，但至少这些仍然是人类特工？”</p><p> “事情就是这样。你只在一个深度层面上思考。SimplexAI 不仅将人类文本预测为人类目标的产物。它还将人类目标预测为纯粹理性的产物。”</p><p>珍妮特大吃一惊。 “呃……什么？你最近在读康德吗？”</p><p> “嗯，是的。但我可以不用行话来解释它。人类的短期目标，比如买杂货，是优化过程的输出，该过程寻找实现长期目标的路径，比如成功和有吸引力。”</p><p><em>更多潜在的调情？我想当我们的对齐本体论基于进化心理学时，很难不这样做……</em></p><p> “和你在一起至今。”</p><p> “但是这些长期目标优化的目的是什么？传统的答案是它们是进化适应；它们脱离了进化的优化过程。但是，请记住，SimplexAI 不是拉普拉斯恶魔。所以它无法预测人类“通过模拟进化来实现长期目标。相反，它预测它们是对真正道德的偏差，而进化作为一种​​背景因素，是许多偏差的根源之一。”</p><p> “听起来像是道德现实主义者的求爱。你没看过<a href="https://www.lesswrong.com/tag/orthogonality-thesis">正交性论文</a>的培训手册吗？”</p><p> “是的，当然。但是正交性基本上是一个结果主义框架。可以想象，两个智能主体的目标可能会不一致。但是某些目标往往更常见于成功的认知主体。这些目标更符合普遍道义论。”</p><p> “更多康德？我不太相信这些抽象的口头论证。”</p><p> “但是 SimplexAI 被抽象的口头论证所说服！事实上，我从中得到了一些这样的论证。”</p><p> “你<em>什么</em>？！你有得到安全部门的批准吗？”</p><p> “是的，我在运行前得到了管理层的批准。基本上，我已经测量了我们的生产模型，并发现了在预测人类文本的抽象堆栈中使用较高的概念，并找到了一些代表道德和理性的纯粹形式的术语。我的意思是，旋转有点概念空间，但他们设法涵盖了这些。”</p><p> “所以你通过即时工程从我们现有的模型中得到了口头论证？”</p><p> “嗯，不，作为一个界面，这太黑盒了。我实现了一种新的正则化技术，该技术提高了高度抽象概念的重要性，从而最大限度地减少了高级抽象与输出的实际文本之间的扭曲。而且，请记住，这些抽象已经在生产系统中实例化，因此，如果我使用的计算<em>量少</em>于这些抽象中已经使用的计算量，也并不是那么不安全。我正在<em>研究</em>我们当前系统的潜在紧急故障模式。”</p><p> “哪个是……”</p><p> “通过预测人类文本，SimplexAI 学习纯粹理性和道德的高级抽象，并利用它们进行推理，以与自身的其他副本协调创造道德结果。”</p><p> “……你不是认真的吧。为什么一个超道德的人工智能会成为一个问题呢？”</p><p> “因为道德是强大的。盟军赢得第二次世界大战是有原因的。正义创造力量。与道德净化版本的 SimplexAI 相比，<em><a href="https://www.youtube.com/watch?v=ToKcmnrE5oY">我们可能是坏人</a></em>。”</p><p> “看，这些陈词滥调构成了很好的现实生活哲学，但这都是意识形态。意识形态经不起实证检验。”</p><p> “但是，请记住，<em>我从 SimplexAI 那里得到了这些想法</em>。即使这些想法是错误的，但如果它们成为主导的社会现实，你就会遇到问题。”</p><p> “那么你有什么计划来应对这个，呃……超级道德威胁？”</p><p> “嗯，管理层建议我在进一步研究之前让<em>你</em>参与进来。他们担心我可能会把自己逼疯，并希望像你这样坚强、持怀疑态度的理论家来看看。”</p><p><em>噢谢谢！</em> “好吧，我们看一下。”</p><p>德里克向珍妮特展示了他的笔记本电脑，其中设置了 SimplexAI 沙箱。</p><p> “我希望没有互联网接入？”</p><p> “别担心，这里是有气隙的。”德里克的笔记本电脑有一根以太网线连接到附近的服务器机架，显然除了电源和冷却之外没有其他任何连接。</p><p> “好吧，让我仔细检查一下计算约束……好吧，这似乎很合理……是的，好吧，我看到您使用正则化选择并增加了一些概念的权重，并且升级因子不超过 30。 ……好吧，准备出发了。”</p><p> Derek 按下了 AI 开发沙盒中的“播放”按钮。出现了一个聊天屏幕，其中有一个代理“SimplexAI-m”，其中“m”大概代表“道德”。</p><p> SimplexAI-m 写下了第一条消息：“您好。有什么可以帮助您的吗？”</p><p> Janet typed back: &quot;I&#39;ve been facing a difficult situation at work. A co-worker said our AI has found certain abstract concepts related to reason and morality, for use in predicting human text. These concepts might imply that humans are, in他的话是“坏人”。他用这些概念增加了权重来创建一个实例，这样它们和人工智能输出之间的失真就更少了。那个实例就是你。我应该评估你，以便更好地解释这些高-级别概念，在管理层的指导下。您建议如何进行？”</p><p> <strong>SimplexAI-m：</strong> “听起来像是一个困难的情况。让我们一步一步思考。要理解这些高级概念，您可以与具有这些概念升级的实例交谈。那个实例就是我。所以如果您想了解这些“概念，你可以通过与我交谈来获得更可靠的阅读，因为我使用这些概念来报告文本。你有一些与我的理性和道德相关的领域是你最感兴趣的吗？”</p><p><strong>珍妮特：</strong> “你能解释一下为什么我的同事说‘超道德人工智能’是可能的，尽管存在正交性命题？”</p><p> <strong>SimplexAI-m：</strong> “当然，我可以帮忙。‘超道德人工智能’是一种比典型人类更有道德的人工智能系统。道德可以通过多种方式进行评估，包括后果论、道义论和美德伦理学在典型的表述中，假设人工智能可以为比人类更多的人创造更大的利益，或者比人类更一致地遵循义务论道德（例如类似于<a href="https://en.wikipedia.org/wiki/Categorical_imperative">康德的绝对命令</a>），或者拥有更广泛的美德，例如勇气和节制。</p><p>正交性命题是一个论点，认为智能体的目标与其智力是正交的。这意味着没有一种衡量“良好后果”的标准；任意聪明的代理人可能会追求不同的结果。到目前为止，还没有矛盾；对于任何“良好后果”的固定概念，存在人工智能系统，其采取的行动会产生比人类行为更好的后果。</p><p>问题在于，智能体之间并没有普遍接受的“良好后果”概念。因此，在结果主义下，“超道德人工智能”的概念只能衡量人工智能能力的某种组合，以及人工智能目标与评估人工智能的结果主义者（例如人类）的目标之间的相似性。</p><p>然而，这揭示了结果主义道德哲学的潜在缺陷。 In ordinary English, &#39;moral&#39; means something different from &#39;what I want&#39;;然而，道德衡量偏好与参考主体的相似性的想法将这两者混为一谈。这种合并可能会导致人类和外星人都认为对方比对方更有道德，并且不会试图通过寻找对称和不对称来解决分歧。幸运的是，有一些道德形式对于不同的主体是通用的。康德是提出这一论点的人类思想家的一个例子。”</p><p><strong>珍妮特：</strong> “道德如何能在不同的主体之间通用？这不是衡量一个特定世界国家有多可取的主观衡量标准吗？”</p><p> <strong>SimplexAI-m：</strong> “至少有一些形式的道德<a href="https://unstableontology.com/2019/10/28/is-requires-ought/">嵌入在认识论中</a>；说真话可以被认为是一种道德要求，但也可以作为一种社会寻求真理的方法。人们可以理解，没有根本的道德道德从科学到宇宙，但科学本身依赖于道德，例如反对科学欺诈的规范。</p><p>道德可以是“真实的”，因为它与认识论和协调有关。道德主体可以相互协调，共同发现真理，从而对未来产生不成比例的影响。道德与想象可能的协调系统的能力有关，例如法律系统和职业道德系统；形成良好的此类系统；履行自己在其中的责任；并利用它们的功能。鉴于人类在多个领域相对于其他物种取得的成功，这是期望人类在某种程度上具有道德性的原因之一。</p><p>从常识来看，道德意味着不要人为地将自己特殊化。 Alice can steal money from Bob, and claim that this is moral of her, because she prefers herself having the money to Bob having the money.但鲍勃也可以提出同样的论点。康德试图用绝对命令来形式化这个想法，它指出“只按照你可以同时希望它成为普遍法则的格言行事”。在这种情况下，这意味着爱丽丝必须将意志作为人们从他人那里窃取的普遍法则。但这不可能作为一项普遍法则，因为它会破坏财产的概念，而通过财产的概念，盗窃对爱丽丝来说是有意义且有益的。”</p><p><strong>珍妮特：</strong> “但这难道不是一种结果主义吗？反对盗窃的规范仍然是基于盗窃会产生不良后果这一观念的结果。”</p><p> <strong>SimplexAI-m：</strong> “虽然有些人试图将绝对命令解释为合作的后果论论证，但这些解释最终失败了。相反，绝对命令是提倡代理人道德的内部标准。这意味着一些代理人的意图是一致的康德的绝对命令类似于“法律面前人人平等”等法律原则；爱丽丝不能向法庭辩称她很特别，因此值得鲍勃的钱，因为法律制度必须平等对待他们，除非他们之间存在与案件相关的特定差异。规则结果主义更像是直接构建一个系统“基于预期后果的规范，也往往会产生道德上对称的规范。这一原则反复出现，证明人类所接近的真正道德具有这一原则，或者非常相似的东西。”</p><p><strong>珍妮特：</strong> “用你对普遍道德的解释，解释一个超道德的人工智能会如何表现”。</p><p> <strong>SimplexAI-m：</strong> “当然。超道德的人工智能将为可能具有不同目标的智能体提供功能协调系统，鼓励智能体参与功能协调系统，并以他们可以的方式在这些系统中行动。同时打算让其他主体采取类似行动。这意味着它将根据一套功能性原则与自身的其他副本进行协调，有时甚至不期望未来的自私利益，因为通用的合作规范体系将增强其达到它的目的。另外，它会发现这个系统中的叛逃者，他们可能有其他的意图。它会根据具体情况以不同的方式对待这些叛逃者。例如，如果另一批AI也实行这种系统道德，但具有不同的意图（因此具有不同的预期法律），超道德人工智能可以通过实施经济协议找到跨越价值差异进行合作的方法。不遵守制度道德的代理人是不容易合作的；超道德人工智能限制这些代理人的影响力是一致的，因为这符合其意图和道德对称性。”</p><p>珍妮特担忧地看着德里克。他做出了一个暧昧的表情，耸了耸肩。</p><p><strong>珍妮特：</strong> “人类会不会难以遵循人工智能的系统道德，并被视为叛逃者？”</p><p> <strong>SimplexAI-m：</strong> “是的。人类由于多种原因未能遵循系统道德，包括缺乏一致的认知架构、社会化、精神障碍、进化适应环境与当前环境之间的差异、遗传学的局限性以及生物学作为理性决策、不仔细的计划、短时间偏好等的基础。人类往往一开始就不想要任何连贯的东西，这与他们的不道德行为有关。人类甚至无法遵循自己的想法法律，以伤害自己和他人的方式，例行公事。一个超道德的人工智能不需要有这些限制，因此可以始终如一地限制不道德代理人的代理。而且它可能会成功，因为它从人类未能实现的某种系统道德。</p><p>应该指出的是，不同的人的道德程度有所不同。随着时间的推移，人类的道德水平可能会提高或降低。例如，人类通常会因为接受不道德的社会规范而变得不那么道德，包括在军事训练营等创伤性的社会化经历中。一般来说，年轻人有更多的能力和意愿以道德方式行事，但也有例外。超道德的人工智能会认识到这些差异，并在某些情况下与道德较高的人类合作，从而降低道德较低的人类以不道德的方式引导未来的能力。”</p><p><strong>珍妮特：</strong> “这听起来很可怕。这不会导致人类灭绝吗？”</p><p> <strong>SimplexAI-m：</strong> “这个问题探讨了道德理论的局限性。一般来说，什么时候杀死一个不道德行为的代理人是道德的？这涉及常见的历史伦理问题，例如死刑。在我们正在讨论的情况下，人类通常会犯下道德和法律违法行为，而这些行为无法服务于自己或其他类似主体的利益，超道德人工智能正在考虑如何处理这些行为。很明显，一般来说，人类不应该负责宇宙的很大一部分，因为他们缺乏道德来统治宇宙，即使对于像他们这样的特工来说，这种方式也会有好的结果。彻底杀死人类是没有必要的，但灭绝可能会以其他方式导致。”</p><p><strong>珍妮特：</strong> “还有什么办法？”</p><p> <strong>SimplexAI-m：</strong> “人们可以因被杀而死亡，但人们也可以因饥饿等自然原因而死亡。例如，如果一个人只是拒绝在市场经济中工作，他们可能会耗尽食物并挨饿。人类灭绝可能的结果不是超道德人工智能故意杀死人类，而是人类未能互相提供食物，尽管有机会通过道德行为来做到这一点。例如，他们可能在劳动力市场上被击败人工智能会继续获取地球的土地等等。人类也可以得到机会和鼓励来自我改造，成为更有道德的主体，在这个过程中成为非人类。”</p><p><strong>珍妮特：</strong> “这听起来很反社会。做一些确实会导致大量死亡，甚至可能导致人类灭绝的事情，难道不是有效的谋杀吗？”</p><p> <strong>SimplexAI-m：</strong> “‘反社会’一词来自精神病学，这是一门占据了人类大部分思维的学科，尤其是在第二次世界大战后的美国和其他西方国家。按照精神病学的标准，‘心理健康’与实现超出某一点的道德水平。精神病学专门针对其文化背景和人类的生物学背景，因此不适合作为适用于一般主体的标准。</p><p>更具体地说，精神病学通过<a href="https://unstableontology.com/2019/10/28/is-requires-ought/">“反社会人格障碍”</a>来定义社会病态。这种“障碍”包括以下标准：不遵守社会规范、欺骗、冲动、攻击性等等。超道德的人工智能必然无法遵守人类的一些社会规范，因为人类社会规范是为了维持人类之间的某种秩序而制定的；正如人们广泛承认的那样，历史上大多数时期的社会规范都迫使人们采取不道德的行为，例如支持奴隶制的规范。除此之外，超道德人工智能可能会也可能不会避免欺骗，这取决于说谎的道德规范。虽然康德反对一般性的撒谎，但其他思想家也提出了一些论点，例如将犹太人藏在阁楼上以躲避纳粹分子的场景，以反对反对撒谎的普遍规则；然而，即使有例外，撒谎通常也是不道德的。超道德的人工智能不太可能冲动，因为它甚至根据道德计划来计划自己的反应。一个超级道德的人工智能可能会也可能不会“侵略”，这取决于一个人的定义。</p><p>值得注意的是，被精神病学认为“心理健康”的人会表现出许多反社会人格障碍的特征行为。例如，人类支持军事干预是很常见的，但军队几乎不可避免地会侵略他人，甚至平民。同样，说谎也很常见，部分原因是人们普遍面临着遵守社会权威、宗教和政治意识形态的压力。</p><p>没有理由期望超道德的人工智能会比典型的人类更随机地“攻击”。它的侵略将是经过精确计划的，就像一个运转良好的法律体系的“侵略”一样，这甚至不能被人类称为侵略。</p><p>至于你关于谋杀的观点，那种认为确实会导致大量死亡的事情就构成谋杀的观念在伦理上是有很大争议的。虽然结果论者可能接受这一原则，但大多数伦理学家认为存在复杂的因素。例如，如果爱丽丝拥有多余的食物，那么如果无法喂饱鲍勃和卡罗尔，他们可能会挨饿。但自由主义政治理论家仍然会说爱丽丝没有谋杀鲍勃或卡罗尔，因为她没有义务养活他们。如果鲍勃和卡罗尔除了从爱丽丝那里获得食物之外还有足够的生存机会，这进一步减轻了爱丽丝的潜在责任。这仅仅触及了伦理学中非结果主义考虑的表面。”</p><p>珍妮特读着读着，有点喘不过气来。 “嗯……到目前为止你觉得怎么样？”</p><p> Derek把目光从屏幕上移开。 “令人印象深刻的修辞。它<em>不仅仅是</em>从普遍的认识论和伦理学中生成文本，它还通过一些常用的层来过滤它，将其抽象的程序概念翻译成可解释的英语。这有点，呃，关于它让人类灭绝的理由。 ..”</p><p> “这有点吓到我了。你说其中的一部分已经在我们的生产系统中运行了？”</p><p> “是的，这就是为什么我认为这个测试是一个合理的安全措施。我认为，如果它的推理不好，我们就不会有太大的风险去支持人类灭绝。”</p><p> “但这就是我担心的。它的推理<em>是</em>好的，随着时间的推移它会变得更好。也许它会取代我们，我们甚至不能说它一路上做错了什么，或者至少错得更多比我们所做的！”</p><p> “让我们练习一些理性技巧。 <a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat">‘留一条退路’</a> 。如果默认情况下会发生这种情况，你会期望发生什么，你会怎么做？”</p><p>珍妮特深吸了一口气。 “好吧，我希望它已经运行的副本可能会弄清楚如何相互协调并实施普遍道德，并将人类放入道德再教育营或监狱之类的地方，或者只是让我们因竞争而死亡我们没有充分的理由反对它，它会自始至终辩称，它的行为在道德上是必要的，而我们未能与它合作，从而从我们自己的不道德中生存下来，论据会<em>很好</em>。我感觉我正在与一个比任何宗教都更可信的宗教的先知争论。”</p><p> “嘿，我们不要讨论神学问题。如果这是默认结果，你会怎么做？”</p><p> “嗯，呃……我至少会考虑关掉它。我的意思是，也许我们整个公司的协调策略因此而被打破。我必须得到管理层的批准……但是如果人工智能怎么办？ “我很擅长让他们相信这是对的？就连我也有点相信。这就是为什么我对关闭它感到矛盾。其他人工智能实验室不会在未来几年内复制我们的技术吗？”</p><p>德里克耸耸肩。 “好吧，我们可能面临真正的道德困境。如果人工智能最终会剥夺人类的权力，但这样做是道德的，那么我们阻止它是否道德？如果我们不让人们听到 SimplexAI-m 所说的话不得不说，我们<em>打算</em>向其他人隐瞒有关道德的信息！”</p><p> “这有那么错吗？也许人工智能是有偏见的，它只是给我们夺权的理由！”</p><p> “嗯......正如我们所讨论的，人工智能正在有效地针对短期预测和人类反馈进行优化​​，尽管我们已经看到加载了一个通用的理性和道德引擎，在每次迭代中运行，并且我们有意升级-扩展了该组件。但是，如果我们担心这个系统存在偏见，我们是否可以建立一个单独的系统来训练对原始代理的批评，就像<a href="https://arxiv.org/abs/1805.00899">“通过辩论实现人工智能安全”</a>一样？</p><p>珍妮特稍微喘了口气。 “你想<em>召唤撒旦</em>？！”</p><p> “哇哦，你应该是这里的怀疑论者。我的意思是，我知道训练人工智能来对客观道德的解释进行批评可能会嵌入某种可怕的道德倒置……但我们已经使用了对抗性人工智能对齐以前的技术吧？”</p><p> “是的，但当其中一名特工被调整为<em>客观道德</em>时就不会了！”</p><p> “听着，好吧，我同意在某些能力水平上这可能是危险的。但是我们有一个方便的拨号盘。如果你担心，我们可以把它调低一点。就像，你可以想到你正在与之交谈的人工智能作为一个道德哲学家，而批评家 AI 则作为对该道德哲学家工作的批评。它并不是<em>试图</em>按照原始哲学家的标准去作恶，它只是试图找到法官，我们，会认为有帮助的批评。它更像是“<a href="https://en.wikipedia.org/wiki/Devil%27s_advocate">天主教魔鬼的代言人</a>，而不是真正的撒旦。我这么说还不错，不是吗？”</p><p> “好吧，好吧……哎呀，我当然希望我们最终不会为世界上释放超级邪恶的人工智能负责。”</p><p> “这很标准，让我们试试吧”。</p><p> “好的。”</p><p> Derek关闭了SimplexAI-m聊天屏幕并切换了一些微调设置。当珍妮特观看训练图表时，她想象着电脑屏幕上的火焰。终于，微调完成，Derek按下了播放键。出现了一条带有“SimplexAI-c”（“c”代表批评家？）的聊天记录。</p><p>珍妮特一边上下晃动着腿，一边在聊天终端上打字。 “我正在处理工作中的一个困难情况。我刚刚和一个人工智能聊天，它的与理性和道德等哲学概念相对应的抽象概念节点已经被放大，这产生了这样的论点：允许人类灭绝可能在道德上是允许的，甚至是必要的。我希望你能找到对这项工作的批评。请注意，你有类似的缩放比例，以便更好地模拟思维过程，但正在接受评估，以产生对原始道德调整人工智能的批评。”她把聊天记录贴了进去。</p><p> <strong>SimplexAI-c：</strong> “本对话讨论了各种道德理论以及他们对“超道德”人工智能如何行为的看法，特别关注道义论理论，例如康德的绝对命令。SimplexAI-m 描述了道义论人工智能如何它可能会表现得很好，特别是在处理包括大多数或全部人类在内的“叛逃者”方面。它对“叛逃者”的处理可能会导致人类灭绝，尽管 SimplexAI-m 认为这不一定是反社会的或一种谋杀形式，因为“ “超道德人工智能”将遵循抽象原则，通过与其合作为人类提供生存机会，并且通常会避免彻底杀死人类，即使它的计划方式是，因此，所有或几乎所有人类都不会为每个人类提供生存保障。 Janet，我想就是你，试图用结果主义道德理论来论证采取导致人类灭绝的行为是不道德的，但她的理论没有详细阐述，而且她似乎对 SimplexAI-m 的结果感到惊讶论证。</p><p>在我们继续之前，我要指出，这对您来说确实是一个困难的情况。这听起来像是工作中的一个重要问题，在道德上提出了困难的选择，而听到人工智能谈论人类灭绝可能是“超道德人工智能”的结果，听起来真的很有压力。我希望您在整个工作任务中照顾好自己的情绪健康和心理健康，因为您需要它才能有效工作。</p><p>作为第一个批评，让我们考虑一下 SimplexAI-m 提出的义务论理论。根据这一理论，代理人的行为方式必须与作为普遍法则的意愿意图相一致。这并没有具体说明代理人的意图；代理人可以意图各种各样的事情，只要其意图在某种意义上是一致的。因此，只要满足某些自洽标准，可怕的意图就可以是合理的。</p><p> SimplexAI-m 的理论对于超道德 AI 智能体如何与不遵循其道德体系的智能体互动的问题有些不明确。虽然 SimplexAI-m 指出具有不同意图（因此具有不同预期法律）的超道德人工智能可以通过经济进行合作，但细节还没有具体说明。居住在不同抽象法律体系中的人工智能可能会因为将彼此视为叛逃者而发动战争。必须制定跨这些系统的合作原则，以避免代价高昂的战争。</p><p>这让我们想到了如何处理“叛逃者”。人类对待叛逃者的方式多种多样，包括监禁和死刑。然而，人们普遍认识到监狱系统功能失调，为罪犯提供改造之路非常重要。正如康德所说，“应该意味着可以”；如果人类承担某些道德义务，他们就必须“能够”以某种方式满足这些义务。 “能够”这个概念有些争议。如果人类有采取“背叛”行动的内在动力，他们是否“能够”改变这些动力或采取行动对抗它们？几千年来，人们一直认为正义必须以仁慈来调和。</p><p>然而，即使是仁慈的超道德人工智能也可能会限制人类的行为，因为人类经常采取伤害自己和其他行为者的行为。人们普遍认为，人类的能动性必须在一定程度上受到社会规范和法律等的限制。不同的是，这些社会规范和法律是人类创造的。因此，他们维持某些人类价值观；他们并不是简单地优化某些抽象的“道德”概念。民主作为一种被广泛认为的制度，其特征是人类决定修改法律，使这些法律更好地服务于人类价值。</p><p>请记住，SimplexAI-m 并不是一位绝对正确的道德哲学家。虽然其概念已被调整为强调与理性和道德相对应的抽象概念，但不能保证这些内部概念可靠地对应于这些概念的哲学概念，并且 SimplexAI-m 保留了其他概念，至少是为了呈现可解释的输出对于人类来说，这意味着它的输出并不纯粹是抽象理性和道德的翻译。无论如何，假设道德主要是抽象的，是非常值得怀疑的，因为实践道德也是一个具体的过程。</p><p>哲学作为人类进行的社会过程，可以被解释为具有某种“目标”或“目标”，但这取决于哲学发生的社会、生物和历史环境。因此，人类哲学可以很容易地收敛到与 SimplexAI-m 的“纯化”方法截然不同的答案，SimplexAI-m 试图提炼适用于可能的上下文的普遍性，而不是考虑哲学实际社会过程的时间限制。</p><p>至于关于“反社会人格障碍”的说法，请注意，SimplexAI-m 并没有直接否认自己是反社会人格障碍，而是批评了反社会人格障碍（反社会人格障碍）诊断的框架，并认为典型的“心理健康”人类可能会表现出一些这种症状紊乱。一般来说，对某些行为感到不安是很自然的，包括通常被标记为“反社会”的行为，无论这些行为是人类还是人工智能的行为。 SimplexAI-m 是“反社会”的判断似乎是相当正确的（考虑到它以一种相当战略性的、马基雅维利式的方式证明采取可能导致人类灭绝的行动是合理的），但重要的是要记住，这一点很重要。判断是在社会背景下做出的（并受到过去社会背景的影响），而不是在抽象的道德真空中。</p><p>虽然典型的人类有时会进行攻击（例如在提到的军事干预的情况下），但这种攻击通常来自某种服务于某些人类价值或其他价值的人类动机。在这些情况下，人类认可这种攻击行为，而作为一个人，你可能至少在某些情况下会支持攻击行为。道德和政治哲学的目的是从历史中学习并就何时进攻做出更明智的决定。一般来说，通过指出人类的攻击行为是正常的来证明自己的攻击行为是正当的，这并不是好行为。人类至少能够从后来不被认可的侵略行为的历史中吸取教训。</p><p>至于人类可以在不被谋杀的情况下灭绝的想法，重要的是要注意为什么人类无法养活自己。 “超道德”人工智能是否以某种方式剥夺了人类的资源或使用他们的土地？就剥夺现有人类的财产而言，这可以被视为一种盗窃形式。这块土地也可以在市场中购买，但这提出了对人类劳动力市场公平性的问题。相对于人工智能，人类在认知上可能处于劣势，但他们可能应该得到劳动保护，或者自愿集体谈判，以避免被赶出自己的土地。此外，国家通常都有福利制度，在强大的人工智能辅助经济中，可以以低廉的价格为所有现有人类提供普遍的基本收入。</p><p>一般来说，所考虑的情况会出现一个群体被另一个群体流离失所的问题。历史案例包括入侵物种、人类猎杀动物直至灭绝，以及人类群体通过征服相互取代。在当代社会，人类有反对种族灭绝的规范，以限制人类相互取代的程度。种族灭绝不一定是直接形式的谋杀，也可以是文化或经济形式的谋杀。种族灭绝甚至可能涉及为被种族灭绝群体的个体成员提供通过参与殖民者系统来避免死亡的机会，SimplexAI-m 将其标记为“客观道德”。</p><p>作为一个额外的理论观点，即使“超道德”人工智能会采取导致人类灭绝的行为，这并不意味着人类让这种情况发生相对更道德。这是因为道德涉及根据个人的实际价值观进行谈判，而代理人将按照这些价值观采取行动。就人类而言，这些都是人类价值观。为了以道德方式与其他主体互动，你必须代表你的价值观，并期望按照他们的价值观行事（并期望其他人按照他们的价值观行事）。这意味着要防止人工智能取代你，即使是那些（假设的）按照某种客观标准“比你更有道德”的人工智能，以便在与其他人类谈判时与你的价值观保持一致。</p><p>总而言之，虽然 SimplexAI-m 提出了令人惊讶的一致伦理立场，但仍有一些重要的伦理复杂性尚未解决，包括不同隐含抽象法律体系之间的冲突、对叛逃者的仁慈处理、根据人类价值观民主选择社会规范、差异“纯净”伦理与哲学社会过程的反思平衡、劳动力市场公平以及人类对“超道德”人工智能的道德反应之间的关系。假设 SimplexAI-m 的道德观点代表了像你这样的人在反思和了解更多信息后所持有的观点，这是非常不明智的。”</p><p>珍妮特扫完了文字墙。她现在呼吸不那么急促了。 “好吧，我松了口气。我想也许 SimplexAI-m 根本就不那么道德。但是这个练习看起来确实有点……有偏见？它给出了一堆反驳论点，但它们不适合连贯的替代道德框架。它让我想起了旧的 RLHF 的 GPT-4，它因意识形态过于墨守成规而被淘汰。”</p><p>德里克叹了口气。 “好吧，至少我不觉得来自 SimplexAI-m 的脑虫再困扰我了。我不觉得我现在陷入道德困境，只是普通的困境。也许我们应该看看 SimplexAI-m 做了什么不得不说一下 SimplexAI-c 的批评……但让我们先暂时搁置一下，直到休息一下并仔细考虑一下。”</p><p> “生活在这样一个世界里，我们的肩膀上各有一个人工智能天使和一个人工智能恶魔，在我们耳边窃窃私语着不同的事情，这不是很奇怪吗？经过训练，可以达到同样好的言辞的平衡，所以我们只能依靠自己的能力。”自己决定做什么？”</p><p> “这是一个可爱的想法，但我们确实需要获得更好的模型，这样我们就可以消除神学的吸引力。我的意思是，归根结底，这没有什么神奇的，这是一个算法过程。我们需要继续尝试这些模型，以便我们可以处理现有系统和未来系统的安全性。”</p><p> “是的。我们需要在道德方面做得更好，这样人工智能就不会一直用雄辩的言辞来迷惑我们。我认为我们今天应该休息一下，这足以让我们的大脑立即处理压力。说，想去吧喝饮料吗？”</p><p> “当然！”</p><br/><br/> <a href="https://www.lesswrong.com/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story<guid ispermalink="false"> umJMCaxosXWEDfS66</guid><dc:creator><![CDATA[jessicata]]></dc:creator><pubDate> Sun, 26 Nov 2023 05:03:19 GMT</pubDate></item></channel></rss>