<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 18 日星期五 18:13:53 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Meetup Tip: Board Games]]></title><description><![CDATA[Published on August 18, 2023 6:11 PM GMT<br/><br/><h2>概括</h2><p>许多聚会都会玩棋盘游戏或纸牌游戏。出去玩游戏可能是一种有趣的共度时光的方式，而组织棋盘游戏聚会也异常容易，因为游戏承担了大部分繁重的工作。有些游戏比其他游戏更适合这个目的；我相信最适合聚会的游戏都有很短的回合，有很多“死”时间，你需要等待某人采取行动，并且适合两名以上玩家。游戏可以是一场很棒的游戏，但对于聚会来说却是一个糟糕的选择。这篇文章的总结是，如果您举办棋盘游戏聚会，我认为如果您带来符合这些标准的游戏并避免带来不符合这些标准的游戏，那么聚会会更好。</p><p>我不太相信这些是直接的改进，但我通常对这些技巧抱有信心。</p><p>还在读书吗？太酷了，让我们用比严格必要的词更多的词来谈论游戏吧。</p><h2>细节</h2><p>为什么游戏成为流行的聚会主题？我相信这是因为棋盘游戏提供了默认活动。在棋盘游戏聚会上接近某人并询问他们是否想玩并不奇怪，这样可以更轻松地介绍自己。你不必担心没有什么可说或谈论的，因为你可以谈论游戏。他们会定期打断别人的话，从而更容易改变话题。它们也可以很有趣。忘记这一点就糟糕了。作为组织者，他们也很容易运作。你只需选择一个时间和地点，带着几个纸板箱出现，然后聚会就开始了。我将简单聚会的顺序命名为“盒子里的聚会”是有原因的，因为我希望每个聚会对于组织者来说都像棋盘游戏聚会一样简单。</p><p>如果你将一款游戏视为一款游戏，那么它所具备的品质与适合聚会的游戏所具备的品质并不相同。在这篇文章中请记住这一点；我将使用很多例子，其中的区别很重要。走吧。围棋是一项有深度且深受喜爱的游戏。如果人们决定在聚会上玩围棋，那么两个人可能会与其他人分开，花一个小时专注于此。这会阻碍他们与其他人交往，甚至他们之间的交谈也会受到限制，因为在任何时候他们中的一个人都在专注于下一步行动。这并不理想。</p><p>比较《走心》。在四人红心大战游戏中，大多数时候三名玩家都在等待第四名玩家出牌。当你等待时，在轮到你之前你不能做太多的策略。这意味着不轮流的三名玩家可以自由地互相交谈。由于每一轮都很快，所以有人很容易决定他们已经玩完了并起身，或者新人加入游戏。当有人加入或离开时，会有一些摆弄牌组的情况，但是四人红心游戏可以变成五人红心游戏，第五个人不需要坐在外面。</p><p>小心同时进行两场长时间的比赛。想象一下，八个人分成两半，玩两场需要一个多小时的游戏，每场有四名玩家。第一个完成的游戏注意到另一组仍在继续，因此他们开始新的游戏。第二组完成后，注意到另一组刚刚开始，所以他们开始新的游戏。晚上就这样过去了，两组人都未能混合在一起。这有点像球形奶牛的例子，足够大的群体通常会有足够的活动来自然地避免这种情况，但要留意多个长时间的游戏将人们锁定在他们的群体中。</p><p>这对于聚会初期的新来者来说最为重要。由于您的与会者很少会在聚会开始时全部到达，因此我认为值得从更快的游戏开始，这样您就可以在新人出现时更快地调整小组。不要从一场持续数小时的游戏开始，这样就锁定了您的前四名到达者；第五名可能很快就会出现！</p><p>这为我最常见的超过两名玩家的标准例外创造了空间。我喜欢聚会中的 Tak（一种两人棋盘游戏，通常在五分钟左右结束），因为我可以玩最新到达的游戏，让他们玩下一个出现的人，当序列中的第三个出现时，我们就足够了适合更多玩家的游戏。如果您可以轻松地与其他人聊天并同时能够快速玩游戏，那么此方法效果最佳。</p><p>我将扩展最后一句话。对于组织者来说，一项巧妙而有用的技能是能够在进行复杂且不相关的对话的同时玩游戏。只要玩得合法，玩得不好也没关系。换句话说，作为组织者，经常输球但有更好的对话是一个很好的权衡，只要你不玩得太糟糕而让其他玩家变得无聊。这是我在人们出现时与他们互动的关键部分，让我能够占据一两个人的时间，否则他们会站在那里等待足够多的人玩游戏。</p><p>许多本来就太慢并且将玩家锁定太长时间的游戏可以通过添加国际象棋时钟来加快速度。计时游戏的优点是通常会在同一时间结束，这可以解决两组周期不同的问题。两人游戏可以变成有趣的四人游戏，方法是让两队球员轮流进行，并且不允许球队讨论策略； Adam 和 Bella 玩白棋，Carl 和 Darla 玩黑棋，所以回合顺序是 Adam Carl Bella Darla Adam Carl 等。把它放在一个激进的时钟上可以消除原本在停滞时间内发生的对话，但它也可能非常有趣。</p><p>游戏也可以成为话题。如果您正在 LessWrong 上阅读有关聚会和棋盘游戏的内容，理性基数就是您的受众可能感兴趣的一个简单例子。很多人喜欢游戏，很多人喜欢理性，将两者结合起来是件好事。例如，不要羞于引入校准琐事或扑克并谈论赔率。</p><p>人们可能会想带来自己的游戏。这很棒，因为它减轻了作为组织者的压力。这确实意味着某些游戏的选择将超出您的控制范围。如果有人拿出《暮光之城帝国》（一款非常长的游戏，我不会推荐用于典型的棋盘游戏聚会）或类似游戏并开始招募玩家，我发现值得对较短的游戏如何更适合聚会发表评论，然后我让它发生。如果人们像这样分开也不错，特别是如果他们对游戏感到兴奋，我只是认为这不如聚会那么好。</p><p>我尝试过为棋盘游戏活动设定主题，例如“纸牌游戏”或“合作游戏”，甚至选择一种游戏并宣布我们将从该游戏开始。对出席率的影响似乎是温和的积极影响。我只这样做过几次，但唯一一次我的参加人数比一般的棋盘游戏聚会少，是当我选择一款游戏作为开始时，这是我编写并想要进行测试的游戏。</p><p>当然，您可以举办一场只讨论一款游戏的聚会。公园里的国际象棋棋手聚会、每隔一周举行一次的麻将俱乐部、万智牌周五之夜万智牌活动以及当地酒吧定期举行的问答之夜都是这样的例子。如果您对那一款游戏感到兴奋，那就去玩吧。这也是战役或遗留游戏成为更可行选择的空间，但即使在这里我也会有点小心；一个群体的规模保持完全相同的情况是很少见的。有人会搬出城镇，或者想带上他们的新伴侣。国际象棋俱乐部不在乎有多少人出现，只要有足够的棋盘即可，格洛姆黑文可以支持的玩家范围相当有限。</p><p>与大多数聚会相比，游戏聚会更能因路人可见而受益。我还没有看到有人注意到公园里有一个读书俱乐部在说话并加入进来，但我见过有人走过一排棋桌，然后回头问一个坐在棋盘上的孤独的人是否正在寻找游戏。我有一次在一个意想不到的地方看到了周五晚上魔术队的标志，并记下下周去参加我最喜欢的比赛之一。人们知道国际象棋、魔术和琐事是如何运作的，并且在没有明确邀请的情况下出现在他们面前会更自在。您当然可以提供明确的邀请；写着“棋盘游戏聚会，周二下午 6 点”的标牌可以聚集临时参加者。</p><p>棋盘游戏聚会比许多其他类型的聚会更适合户外活动。如果你想在外面做，我建议你带一条毯子放在地上。与在草地或泥土上玩耍相比，这将创造一个更平坦、更舒适的玩耍空间。尽量选择一个防风良好的地方，否则风会刮起卡片或吹倒小迷你人物。最后，留意云层并制定下雨时的应对计划；人是防水的，而 Ticket To Ride 则不然。</p><p>最后一点：我在这里谈论的是棋盘游戏和纸牌游戏，通常是围着桌子或在草地上围成一圈玩的。尽管您的场地要求明显会发生变化，但其中大部分建议仍然适用于体育或体育比赛。当地运动的即兴比赛是全世界最受欢迎的聚会活动。然而，我要巧妙地观察到，参加者的体力和舒适度可能会有很大差异，特别是如果他们主要是根据是否阅读《LessWrong》而不是是否加入当地体育联盟来选择的。</p><h2>快速技巧</h2><p>我最喜欢的聚会游戏的简短列表：Apples to Apples、Hanabi、Tak、Outrangeous、Deep Sea Adventure。对于体育游戏，Zip Zap Zoom、飞盘、夺旗以及杂耍传球技巧都效果很好，但当我可以的时候，我喜欢足球。</p><p>假钱可以从现有游戏中重新利用，可以自己便宜地购买，并可以启用一些我通常不会在休闲聚会上玩的其他游戏。扑克是这里的主要例子，但是任何带有赌博成分的东西都比记分卡上的抽象分数更有趣，使用实体但假的钱。我建议不要使用真钱，尤其是当所有玩家之间的总价值大于一美元时。</p><p>我注意到，如果我下棋积极且快速（在回合传给我后立即移动我的棋子），就会让人们措手不及，并鼓励他们快速移动。这是否是我想做的事情各不相同；在聚会的早期部分，我通常会这样做，这样我可以缩短回合时间，并更容易混合小组，在聚会的后期部分，我通常会缓慢而刻意地玩，以便给人们时间聊天。</p><br/><br/><a href="https://www.lesswrong.com/posts/SZ2n5cjp6uZs8kfTT/meetup-tip-board-games#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SZ2n5cjp6uZs8kfTT/meetup-tip-board-games<guid ispermalink="false"> SZ2n5cjp6uZs8kfTT</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 18 Aug 2023 18:11:58 GMT</pubDate> </item><item><title><![CDATA[AI labs' requests for input]]></title><description><![CDATA[Published on August 18, 2023 5:00 PM GMT<br/><br/><p>人工智能实验室有时会公开征求有关其行动、产品和人工智能未来的意见。请参阅 OpenAI<a href="https://openai.com/blog/democratic-inputs-to-ai">对 AI</a>和<a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf"><u>ChatGPT 反馈竞赛</u></a>的民主投入，以及针对安全漏洞的 bug 赏金计划（ <a href="https://openai.com/blog/bug-bounty-program">OpenAI</a> 、 <a href="https://bughunters.google.com/about/rules/6625378258649088/google-and-alphabet-vulnerability-reward-program-vrp-rules">Google</a> 、 <a href="https://www.facebook.com/whitehat">Meta</a> ）。我想收集这些；请回复并提供您知道的其他示例。我还对实验室应请求输入内容或应如何执行此操作的想法/建议感兴趣（例如<a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">模型输出的错误赏金</a>）。</p><hr><p>实验室还非公开地寻求意见。例如，实验室使用外部红队和<a href="https://evals.alignment.org/blog/2023-08-01-new-report/">模型评估</a>，<a href="https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety">与生物安全专家合作以了解近期人工智能系统如何为生物武器做出贡献</a>，并<a href="https://cdn.openai.com/papers/gpt-4.pdf#page=59">咨询外部预测人员</a>。已经提出了各种类型的外部审计。我也有兴趣收集这种非公开意见的示例和想法，但兴趣不大。</p><br/><br/><a href="https://www.lesswrong.com/posts/5e3bXsBQLusuArzmN/ai-labs-requests-for-input#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5e3bXsBQLusuArzmN/ai-labs-requests-for-input<guid ispermalink="false"> 5e3bXsBQLusuArzmN</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Fri, 18 Aug 2023 17:00:26 GMT</pubDate> </item><item><title><![CDATA[6 non-obvious mental health issues specific to AI safety.]]></title><description><![CDATA[Published on August 18, 2023 3:46 PM GMT<br/><br/><h1><br>介绍</h1><p>我是一名心理治疗师，我帮助人们从事人工智能安全工作。我注意到这个群体特有的心理健康问题模式。这不仅仅是宿命论，还有更多不那么明显的事情。</p><p>如果您遇到与人工智能安全相关的心理健康问题，请随时发表评论以及对您有帮助的事情。您也可以在评论中支持其他人。有时，这种支持会产生很大的影响，人们会觉得自己并不孤单。<br><br>这篇文章中的所有示例都是匿名的，并且以一种无法识别背后特定人员的方式进行了更改。</p><p></p><h2>人工智能安全是一个相当不寻常的领域</h2><p>这篇文章中描述的问题的出现是因为人工智能安全不是一个普通的工作领域。<br><br>人工智能安全社区内的许多人认为这可能是最重要的工作领域，但公众大多不太关心。此外，该领域本身竞争非常激烈，新移民往往很难找到工作。<br><br>没有人真正知道我们何时会创建通用人工智能，以及我们是否能够保持它的一致性。如果我们不能协调通用人工智能，人类可能会灭绝，即使我们成功了，也将从根本上改变世界。<br><br></p><p></p><h1>图案</h1><h2>AGI 要么导致厄运，要么创造一个乌托邦。其他一切似乎都不重要，毫无意义。</h2><p> Alex 是一名机器学习工程师，在一家与衰老作斗争的初创公司工作。他认为AGI要么会毁灭人类，要么带来一个乌托邦，除此之外它还会停止衰老，所以Alex认为他的工作毫无意义，于是辞职了。他有时也会问自己“我应该投资吗？我应该锻炼吗？我是否应该用牙线清洁牙齿？这一切似乎毫无意义。”</p><p>没有人知道后 AGI 世界会是什么样子。所有的预测都是猜测，很难判断任何与人工智能安全无关的行动是否有意义。这种不确定性会导致焦虑和抑郁<br><br>这些问题是生命无意义的存在问题的加剧版本，缓解这些问题的方法是重新发现这个最终没有意义的世界的意义。</p><p></p><h2>我不知道我们什么时候会创建AGI，也不知道我们是否能够调整它，所以我觉得我无法控制它。</h2><p>贝拉是一个焦虑的人，她最近对人工智能安全产生了兴趣，她意识到没有人确切知道如何调整通用人工智能。<br><br>她觉得AGI可能会带来极大的危险，而她却无能为力。她甚至不明白我们还有多少时间。一年？ 5年？这种不确定性让这里更加焦虑。如果起飞速度如此之快以至于没有人明白发生了什么怎么办？<br><br>贝拉正在见一位心理治疗师，但他们认为她的恐惧是不合理的。这没有任何帮助，只会让贝拉更加焦虑。她觉得就连她的治疗师也不理解她。</p><p></p><h2>人工智能安全是我生活的重要组成部分，但其他人并不太关心它。我感到疏远。</h2><p> Chang 是一位机器学习科学家，在人工智能实验室从事机械可解释性研究。 AI安全消耗了他的一生，并成为他身份的一部分。他经常在 Twitter 上查看 AI 安全影响者，花大量时间阅读 LessWrong 并观看 AI 播客。他甚至用回形针纹了一个纹身。<br><br>张住在主要人工智能安全中心之外，他感到有点孤独，因为没有人亲自谈论人工智能安全。<br><br>最近他参加了他姨妈的生日聚会。他谈到了与家人的团结。他们对这个话题有些好奇，但也没有那么在意。张感觉他们就是不明白。</p><p></p><h2>致力于人工智能安全是如此重要，以至于我忽视了生活的其他部分并精疲力尽。</h2><p>德米特里是一名本科生。他认为AI安全是他一生中最重要的事情。他要么考虑人工智能的安全，要么一直在研究它。他一生中从未如此努力地工作过，他很难意识到忽视生活的其他部分以及未能区分人工智能安全会直接导致倦怠。当这种倦怠发生时，一开始他不明白发生了什么，并因为无法从事人工智能安全工作而变得沮丧。</p><p></p><h2>从事人工智能安全工作的人非常聪明。我认为我还不够好，无法做出有意义的贡献。</h2><p>以斯拉最近从一所大学毕业，他在那里研究变压器。他想从事人工智能安全工作，但似乎主要人工智能实验室和人工智能安全组织中的每个人都非常有才华，并且受过良好的教育。以斯拉对此感到非常害怕，甚至很难尝试做某事。<br><br>过了一段时间，他终于申请了一些组织，但到处都被拒绝，其他人也分享了类似的经历。似乎每个职位都有几十个聪明的年轻人应聘。<br><br>他感到没有动力，他还需要支付账单，所以他决定在非人工智能安全公司工作，这让他很难过。</p><p></p><h2>很多聪明人认为人工智能对齐并不是什么大问题。也许我只是反应过度了？</h2><p> Francesca 是一位在学术界工作的计算机科学家。她熟悉机器学习，但这不是她工作的重点。她还认为，存在风险的论据是可靠的，她对此感到担忧。<br><br> Francesca 很好奇顶级机器学习科学家对人工智能安全的看法。他们中的一些人认为 X 风险很严重，而其他许多人则不太担心这些风险，他们认为人工智能的毁灭者是怪人。<br><br>弗兰西斯卡因此感到困惑。她仍然认为存在风险的论据是可靠的，但社会压力有时让她认为整个一致性问题可能没有那么严重。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety<guid ispermalink="false"> tplzjWqG2iyEgMGfJ</guid><dc:creator><![CDATA[Igor Ivanov]]></dc:creator><pubDate> Fri, 18 Aug 2023 15:46:09 GMT</pubDate> </item><item><title><![CDATA[That time I went a little bit insane]]></title><description><![CDATA[Published on August 18, 2023 6:53 AM GMT<br/><br/><p>这是一个关于我的故事。这展示了“异国情调的自我修改”的后果，并不是作为支持或反对这种做法的建议，只是一个数据点。权衡你的风险。</p><p> TL;DR：您可能错误地确定了您的最终目标。不要明确地、专门地针对它们进行优化。另外，在进行实验时，不要忽视实验的“外部视野”。</p><p> —</p><p>我很年轻，有神经可塑性，正在尝试弄清楚我的生活。我有很多有组织的空闲时间（感谢学校系统）。我决定尝试在思想和目标层面上提高自己，因为我认为这是我的基本目标。</p><p>这种认同是我生命中看似意义深远的时刻，我想“啊哈！这是我所有效用的根源！所以我应该优化它，我的生活就会很理想”。</p><p>我认识到我是自己思想的建筑师，并运用了这些力量。但我的蓝图绘制得太仓促，错过了支撑结构，导致整个建筑在达到足够的重量和复杂性后就倒塌了。</p><p>我天真的、有损的效用压缩让我崩溃了。但我还不知道，我正兴奋地想象着我的指数级飞跃，成为越来越完美的自己。</p><p> —</p><p>我开始冥想，主要是通过关注我的身体和感知，并试图改变它，以及操纵我的思维模式。</p><p>我开始更加形象化、更加抽象地思考。我把时间花在阅读数学和物理以及对世界进行建模上。</p><p>我想要能够控制“我自己”，而这里的目标就是我的情绪。我发现我可以想象幸福，然后让它冲刷我，一种平静的快乐爆发。我进入了生动愉快的场景，想象自己漂浮在风中，沐浴着阳光。我经常这样做，因为我可以。</p><p>我不再表面上关心别人，以为我可以阻止他们的影响，而且我很善于操纵，冷酷无情地向自己证明我可以。具有讽刺意味的是，在试图实现更多多样性的过程中，我的情感复杂性崩溃了。</p><p>我很冲动，通过做任何我能想到的发行之外的事情来寻找新奇，同时试图通过改变与谁交谈以及从哪里获得信息来在我的想法中播下更多的随机性。</p><p>我开始自上而下地进行理论分析，思考新的计算范式和物理模型等结构，然后尝试将它们应用到现实中。我转动我的心灵之眼，我一直在思考。</p><p>我以前的目标失去了根基，我所关心的只是增加，第一顺序，我的能力和我生活中的变化。这意味着功课会被搁置，自动消失或极具创造性地完成（有时会因为没有严格遵守规则而失去学分）。我无法证明花时间做我认为我知道<i>该怎么</i>做的事情是合理的，做这些事情的智力<i>能力</i>是我唯一考虑的因素，因为这就是我认为能力提高的来源。我还非常注重元改进，塑造解决问题的方法，而不是仅仅以最“有效”的方式解决问题，试图超越局部最大值。</p><p>我的每一个想法和采取的行动都是由我的多样性/元改进目标证明的。发生的事情我都会反思，如果我认为不合适就不会做。这是一个永恒的、支配着我的总体范式。</p><p>然后我放手，不再有结构，不再有意义。我的生活感觉空虚，我唤起的快乐是超然的，我也是。我转向空虚、低比特率的社交互动或社交媒体，在我关闭理性和意识时挖掘那些微小的多巴胺爆发。</p><p>一阵阵的清晰突然出现，我会锻炼，或继续一些能力或新奇的增加计划，或拉出缓存的思想线索或图像。但这些时间变得越来越短，屈服于夜晚的深深空虚。</p><p>每天，我都在“生产力”和享乐主义和虚无主义的奇怪混合中循环，用“哈哈，我的大脑很奇怪”的语气向朋友评论这一点，不知何故没有反思为什么或意识到这有多糟糕。</p><p>然后我睡着了，在做了更多毫无意义的心理瑜伽之后，试图利用睡前的效果，然后醒来，重复这一切。</p><p>我的思想在压力下支离破碎，我的身份受到鞭打，但内部没有任何损坏的迹象。我有一个梦想，想象吸收并成为另一个人，拥有他们的目标、世界观和思想。这让人不舒服，而且奇怪地深刻，我用它作为可塑性的积极指标，而不是警告。</p><p>我持续了一个月。</p><p>一天晚上，我崩溃了，想知道这一切的意义是什么，为什么我无法控制自己，为什么我所有的快乐都感觉如此空虚，想知道我的世界观取得了哪些实际进展。</p><p>我趴在地上轻声哭泣，想知道我的生活发生了什么事。</p><p>我以为我已经找到了人生中一个简单的“目标”，那就是自我提升，但追求它却成了一场噩梦。</p><p>我是一个充满了不安、情绪化和不确定性的强大力量，最终以外部视角看待我一直在做的事情，直到那时我才明白这一点。我吸收并体现了一种过于浓缩、过于狭隘、过于偏执的世界观。</p><p>我应该把这当作一个实验，但我完全相信这是我的新生活方式，只要我继续努力，我就走在生产力最大化的道路上。</p><p>我最终非常明确地摆脱了这个困境。当我躺在床上思考我一直在做的事情及其原因时，我能感觉到我的目标“重新调整”（在讨论之后，我终于有了一个外部观点，能够质疑我的世界的功效和疯狂）。我自由了。</p><p>这是一种巨大的解脱。我立即感觉更正确、更舒服、更轻松、更快乐。我以前的世界观是紧张和严格的，就像一把尺子挂在我的背上，太长而且角度不对。</p><p>我又回到了这些目标，对自己能够回来的能力感到满意，因为我已经感受到了差异，测试了我来回的能力，它是如此脆弱，如此做作。</p><p>事实证明，我建立一个基本状态是有原因的，当动机不明确且“任意”时，对其进行渐进但极其激进和坚定的改变并不是一个净积极的结果。</p><p>我花了两周半的时间才再次感到舒服，忽略了“目的”的深渊，并接受了以荒诞主义为基本状态对当时的我来说是最健康的事情。</p><p>这很有趣。我解锁了一些有趣的新结构、能力和感觉。但我“被困”在那里，处于一种主观上不太基线的令人愉悦的幻觉中。如果我不相信我实际上正在完美地追求我的最终目标，它可能不会起作用，但我应该尝试一下，保持局外人的观点。</p><p> —</p><p>我仍然不知道“我”的目标是什么。自我提升、新鲜感和幸福感发挥着巨大作用，但它们还不够。</p><p>似乎他们的服务方式很重要，实际的产出也很重要，而不仅仅是复杂、迷人的想法。人很重要。压抑关怀是不自然的。</p><p>因此，我的关怀、合作、“小心”做出改变的弧线开始了，并内置了安全感，更接地气的想法，所以我不会让自己陷入个人地狱，在当地感觉就像天堂。</p><p>祝那些玩弄头脑的人好运，并且要非常小心，以免陷入一种自我维持的、全球性的不舒服的状态。还有一点（毫无成效的）疯狂。</p><br/><br/> <a href="https://www.lesswrong.com/posts/L9q5iFJAggvoWNrRf/that-time-i-went-a-little-bit-insane#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/L9q5iFJAggvoWNrRf/that-time-i-went-a-little-bit-insane<guid ispermalink="false"> L9q5iFJAggvoWNrRf</guid><dc:creator><![CDATA[belkarx]]></dc:creator><pubDate> Fri, 18 Aug 2023 06:53:51 GMT</pubDate> </item><item><title><![CDATA[When discussing AI doom barriers propose specific plausible scenarios]]></title><description><![CDATA[Published on August 18, 2023 4:06 AM GMT<br/><br/><p> TLDR：在解决“人工智能如何做 X”问题时，更喜欢涉及人类当前可以做的事情的简单存在证明，而不是像纳米技术或完全通用的“人工智能将足够聪明来弄清楚”之类的看似“神奇”的事情。总而言之：结束</p><p>在人工智能风险对话中，通常会提出“人工智能如何做 X”的问题，其中 X 是“控制物理资源”或“在没有人类的情况下生存”之类的问题。</p><p>这些问题有“愿意/可以”的区别。</p><ul><li> ASI<strong>将</strong>如何做 X -->;（预测 ASI 追求目标 X 的行动）</li><li> ASI<strong>如何</strong>完成 X -->;（表明 AI 可以完成 X）</li></ul><p> <strong>“愿意”</strong>问题的回答是正确的 (nanotech/super-persuasion/Error:cannot_predict_ASI)，但<strong>“可以”</strong>形式通常是意图，可以带来更好的对话。</p><p>对于<strong>“会”的</strong>问题，答案可以这样开头：“人工智能会比我聪明，所以我无法预测它实际上会做什么。但它可以做 X 的一种方法是……”</p><h2>假设的例子</h2><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p> Doomer：人工智能足够聪明，可以找到纳米技术之类的解决方案。</p><p>怀疑者：我不相信你。我不认为它能做到这一点。</p><p>杜默：*递给他们一本纳米系统书*</p><p>怀疑者：我从来没有参加过 CHEM101，而且没有人建造过这个，所以我仍然不相信你。</p></blockquote><p>比较</p><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p>毁灭战士：它可以侵入所有计算机并劫持它们和数据。</p><p>怀疑者：我们会拒绝它的要求！</p><p> Doomer：这里列出了连接到互联网的东西，这些东西如果被黑客入侵的话修复起来有多困难，以及人类黑客入侵这些东西的例子。</p></blockquote><p>第一个答复是一个完全普遍的论点，即智力可以解决大多数问题。如果你想知道人工智能如何制作冰块，并且你不知道冰箱是可能的，请使用完全通用的论点。人工智能将足够聪明，能够发明一些可以制作冰块的新技术。如果您确实知道现有的解决方案，那么只需指出它作为存在证明即可。 “人工智能可以从亚马逊订购一台制冰机，并雇佣一只任务兔子来设置和操作它。”</p><h2>关注点：只看到容易阻止的攻击</h2><blockquote><p><a href="https://www.lesswrong.com/posts/mqc99HCMRAjnWSAxz/a-hypothetical-takeover-scenario-twitter-poll#Dangers_of_Rhetorical_Generosity">一般来说，这是此类场景的一个问题。通过尽可能合理和脚踏实地，通过展示它需要多么少，你才能让人们相信你所拥有的一切。人们越是试图符合人们对“现实”的本能，你所呈现的情况就越不现实，你就越进一步强化这种区别。</a></p></blockquote><p>因此，如果以前他们不认为人工智能能够做 X。现在他们认为人工智能可以做 X，但我们可以并且只会通过设置一个阻止特定场景 Y 的屏障来阻止它。如果我们通过某种奇迹修复计算机安全性方面，用户也同样容易受到勒索和社会工程的侵害。</p><p>我仍然认为，如果人们相信存在真正的威胁，即使他们相信可以通过一些具体的对策来解决它，而不是思考诸如“人工智能将无法影响现实世界”之类更荒谬的事情，那就更好了。</p><h2>一个激励人心的例子</h2><p><a href="https://youtu.be/Yd0yQ9yxSYY?t=356">埃利泽泰德谈话问答</a></p><blockquote><p>克里斯·安德森：要实现这一点，人工智能要从根本上摧毁人类，它必须突破，逃脱互联网的控制，并且，你知道，开始指挥实际的现实世界资源。你说你无法预测这将如何发生，但只是描绘一两种可能性。</p><p> Eliezer Yudkowsky：好吧，那为什么这很难呢？首先，因为你无法准确预测更智能的国际象棋程序将走向何方。也许比这更重要的是，想象一下将空调的设计追溯到 11 世纪。即使他们建造的细节足够多，当冷空气出来时他们也会感到惊讶，因为空调会利用温度-压力关系，而他们不知道这个自然法则。 [...]</p></blockquote><p>安永正确地提供了“错误：cannot_predict_ASI”响应，表示 ASI 可能会使用一些意想不到的更好的解决方案，涉及我们不太了解的自然法则。</p><p>除非能够提出“人工智能非常聪明，因此可以做大多数可能的事情”的论证，否则“人工智能可以做 X 吗”（显然是的）这一隐含的问题就没有得到解决，而这是很难做到的。对方也不一定清楚这是否有可能。</p><p>我是这样回答这个问题的：</p><blockquote><p>我的回答：比我聪明得多的人工智能显然可以找到更好的策略，但如果目标是接管世界，那么互联网连接的设备就足够了。接管互联网连接设备的人工智能可以关闭它们以强制合规。除了电话、电脑和支付终端之外，智能电表还可以切断建筑物的电源，并使汽车瘫痪或撞毁。</p><p>在保持电力和芯片工厂运转的同时杀死所有人类更加困难。这可能需要建造大量机器人，但以目前的技术来看似乎是可行的。懒惰的人工智能只能等待我们构建它所需的工具。人工智能当然会更聪明。它会提出更好的解决方案，但原则上问题是可以解决的。</p></blockquote><h3>免责声明</h3><ul><li>我有时间思考/编辑。安永没有。尽管如此，我还是希望安永能够改变他对此类问题的默认论点。</li><li>生成>;>;编辑。<ul><li>生成完整的 X 比优化现有的 X 困难得多</li><li>我的帖子并不意味着“我是一个更好的辩论者”（我不是）。</li><li>这对我来说是一个明显的错误</li></ul></li><li>让其他人关注可解决的风险可能会成为一个坏主意。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible<guid ispermalink="false"> GxjDuS8dXH9CNsjwD</guid><dc:creator><![CDATA[anithite]]></dc:creator><pubDate> Fri, 18 Aug 2023 04:06:44 GMT</pubDate></item><item><title><![CDATA[An Overview of Catastrophic AI Risks: Summary]]></title><description><![CDATA[Published on August 18, 2023 1:21 AM GMT<br/><br/><p>我们最近在我们的网站上发布了<a href="https://arxiv.org/abs/2306.12001">关于人工智能灾难性风险的论文</a>摘要，我们将其交叉发布在这里。我们希望这份摘要有助于让我们的研究更容易理解，并以更方便的方式分享我们的政策建议。 （之前我们在<a href="https://www.alignmentforum.org/posts/bvdbx6tW9yxfxAJxe/catastrophic-risks-from-ai-1-introduction">这篇文章</a>中有一个较小的摘要，我们发现它是不够的。因此，我们写了这篇文章并删除了该部分以避免重复。）</p><h2><strong>执行摘要</strong></h2><p>灾难性人工智能风险可分为四个关键类别，我们将在下面进行探讨，并在 CAIS 的<a href="https://arxiv.org/abs/2306.12001">链接论文</a>中进行更深入的探讨：</p><ul><li><strong>恶意使用</strong>：人们可能故意利用强大的人工智能来造成广泛的伤害。人工智能可用于策划新的流行病或用于宣传、审查和监视，或被释放以自主追求有害目标。为了降低这些风险，我们建议改善生物安全，限制对危险人工智能模型的访问，并让人工智能开发人员承担伤害责任。</li><li><strong>人工智能竞赛</strong>：竞争可能会促使国家和企业加快人工智能开发，放弃对这些系统的控制。自主武器和人工智能网络战可能导致冲突失控。企业将面临自动化人力劳动的激励，这可能导致大规模失业和对人工智能系统的依赖。随着人工智能系统的激增，<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>进化动力学</u></a>表明它们将变得更难以控制。我们建议对通用人工智能进行安全监管、国际协调和公共控制。</li><li><strong>组织风险</strong>：开发先进人工智能的组织存在导致灾难性事故的风险，特别是如果他们优先考虑利润而不是安全的话。人工智能可能会意外泄露给公众或被恶意行为者窃取，组织可能无法正确投资于安全研究。我们建议培育以安全为导向的组织文化，实施严格的审计、多层次的风险防御和最先进的信息安全。</li><li><strong>流氓人工智能</strong>：随着人工智能变得更加强大，我们可能会失去对它们的控制。人工智能可以优化有缺陷的目标，偏离最初的目标，追求权力，抵制关闭，并进行欺骗。我们建议人工智能不应部署在高风险环境中，例如自主追求开放式目标或监督关键基础设施，除非证明是安全的。我们还建议在对抗稳健性、模型诚实性、透明度和消除不需要的功能等领域推进人工智能安全研究。</li></ul><h2><strong>一、简介</strong></h2><p>今天的科技时代将会令过去的几代人感到震惊。人类历史呈现出加速发展的规律：从智人的出现到农业革命，再到工业革命，经历了数十万年的时间。几个世纪后的现在，我们正处于人工智能革命的黎明期。历史的前进并不是一成不变的——它正在迅速加速。在人类历史进程中，世界产量快速增长。人工智能可以进一步推动这一趋势，将人类带入一个前所未有的变革的新时期。</p><p>核武器的出现说明了技术进步的双刃剑。我们<a href="https://ourworldindata.org/nuclear-weapons-risk#close-calls-instances-that-threatened-to-push-the-balance-of-terror-out-of-balance-and-into-war"><u>十几次</u></a>侥幸避免了核战争，有几次是因为一个人的干预才避免了战争。 1962年，古巴附近的一艘苏联潜艇遭到美国深水炸弹的袭击。船长认为战争已经爆发，想用核鱼雷予以回应，但指挥官瓦西里·阿尔希波夫否决了这一决定，从而将世界从灾难中拯救出来。人工智能能力的快速且不可预测的进步表明它们可能很快就会与核武器的巨大威力相媲美。随着时间的流逝，需要立即采取主动措施来减轻这些迫在眉睫的风险。</p><h2> <strong>2. 恶意使用</strong></h2><p>我们首先担心的是人工智能的恶意使用。当许多人都能获得一项强大的技术时，只需要一个人就能造成重大伤害。</p><h3><strong>生物恐怖主义</strong></h3><p>包括病毒和细菌在内的生物制剂造成了历史上一些最具破坏性的灾难。尽管我们在医学上取得了进步，但人工设计的流行病可能被设计得比自然流行病更致命或更容易传播。人工智能助手可以为非专家提供生产生物和化学武器所需的指导和设计，并促进恶意使用。</p><p>人类将病原体武器化的历史可以追溯到<a href="https://pubmed.ncbi.nlm.nih.gov/17499936/"><u>公元前 1320 年</u></a>，当时受感染的羊被赶出国境传播兔热病。 20世纪，至少有15个国家开发了生物武器计划，包括美国、苏联、英国和法国。尽管生物武器现在已成为国际社会大多数国家的禁忌，但一些国家仍在继续实施生物武器计划，非国家行为者构成的威胁日益严重。</p><p>策划一场流行病的能力正在迅速变得更加容易获得。基因合成可以创造新的生物制剂，其价格已大幅下降，其成本大约每<a href="https://www.nature.com/articles/nbt1209-1091"><u>15 个月</u></a>减半。台式 DNA 合成机可以帮助流氓分子制造新的生物制剂，同时<a href="https://www.nti.org/analysis/articles/benchtop-dna-synthesis-devices-capabilities-biosecurity-implications-and-governance/#:~:text=Currently%2C%20nearly%20all,their%20own%20labs."><u>绕过</u></a>传统的安全检查。</p><p>作为一种军民两用技术，人工智能可以帮助发现和释放新型化学和生物武器。人工智能聊天机器人可以提供合成致命病原体的<a href="https://arxiv.org/abs/2306.03809"><u>分步指令</u></a>，同时逃避防护措施。 2022 年，研究人员<a href="https://www.nature.com/articles/s42256-022-00465-9"><u>重新利用</u></a>医学研究人工智能系统来生产有毒分子，在几个小时内产生 40,000 种潜在的化学战剂。在生物学中，AI已经可以辅助<a href="https://www.pnas.org/doi/10.1073/pnas.1901979116"><u>蛋白质合成</u></a>，AI对于蛋白质结构的预测能力已经<a href="https://www.nature.com/articles/s41586-021-03819-2https://www.nature.com/articles/s41586-021-03819-2"><u>超越了人类</u></a>。</p><p>有了人工智能，能够开发生物制剂的人数将会增加，从而增加工程大流行的风险。与历史上任何其他流行病相比，这种流行病的致命性、传播性和治疗耐药性可能要高得多。</p><h3><strong>释放人工智能代理</strong></h3><p>一般来说，技术是我们用来实现目标的工具。但人工智能越来越多地被构建为能够自主采取行动以追求开放式目标的代理。恶意行为者可能会故意创建具有危险目标的流氓人工智能。</p><p>例如，GPT-4 推出一个月后，一名开发者用它运行了一个名为<a href="https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity"><u>ChaosGPT</u></a>的自治代理，旨在“毁灭人类”。 ChaosGPT 整理了有关核武器的研究，招募了其他人工智能，并撰写推文来影响他人。幸运的是，ChaosGPT 缺乏执行其目标的能力。但人工智能发展的快节奏增加了未来流氓人工智能的风险。</p><h3><strong>有说服力的人工智能</strong></h3><p>人工智能可以通过针对个人用户定制论点来<a href="https://arxiv.org/abs/2303.08721"><u>促进</u></a>大规模的虚假信息活动，从而可能塑造公众信仰并破坏社会稳定。由于人们已经与聊天机器人<a href="https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/"><u>建立了关系</u></a>，强大的参与者可以利用这些被视为“朋友”的人工智能来施加影响。人工智能将实现复杂的个性化影响活动，这可能会破坏我们共同的现实感。</p><p>人工智能还可以垄断信息的创建和分发。独裁政权可以利用“事实核查”人工智能来控制信息，促进审查制度。此外，有说服力的人工智能可能会阻碍针对社会风险的集体行动，甚至是那些由人工智能本身引起的风险。</p><h3><strong>权力集中</strong></h3><p>人工智能的监视和自主武器能力可能会导致权力的压迫性集中。政府可能会利用人工智能侵犯公民自由、传播错误信息并平息异议。同样，企业可以利用人工智能来操纵消费者并影响政治。人工智能甚至可能阻碍道德进步，并使<a href="https://link.springer.com/article/10.1007/s10677-015-9567-7"><u>正在发生的道德灾难</u></a>永久化。如果对人工智能的物质控制仅限于少数人，则可能代表人类历史上最严重的经济和权力不平等。</p><h3><strong>建议</strong></h3><p>为了降低恶意使用的风险，我们提出以下建议：</p><ul><li><strong>生物安全</strong>：具有生物研究能力的人工智能应该有严格的访问控制，因为它们可能被重新用于恐怖主义。<a href="https://arxiv.org/abs/2306.03809"><u>应该从用于一般用途的人工智能中去除生物能力</u></a>。探索利用人工智能实现生物安全的方法，并投资于一般生物安全干预措施，例如通过<a href="https://www.nature.com/articles/s41591-022-01940-x"><u>废水监测</u></a>早期发现病原体。</li><li><strong>限制访问</strong>：仅允许通过云服务进行<a href="https://arxiv.org/abs/2201.05159"><u>受控交互</u></a>并进行<a href="https://arxiv.org/abs/2305.07153"><u>“了解你的客户”筛选，</u></a>从而限制对危险人工智能系统的访问。使用<a href="https://arxiv.org/abs/2303.11341"><u>计算监控</u></a>或出口控制可能会进一步限制对危险功能的访问。此外，在开源之前，人工智能开发人员应该证明伤害风险最小。</li><li><strong>异常检测技术研究</strong>：针对人工智能滥用开发多种防御措施，例如针对异常行为或人工智能生成的虚假信息进行对抗性稳健的异常检测。</li><li>通用<strong>人工智能开发者的法律责任</strong>：对潜在的人工智能滥用或失败追究开发者的法律责任；严格的责任制度可以鼓励更安全的开发做法和适当的风险成本核算。</li></ul><h2> <strong>3.人工智能竞赛</strong></h2><p>国家和企业正在竞相快速构建和部署人工智能，以维持权力和影响力。与冷战时期的核军备竞赛类似，参与人工智能竞赛可能会服务于个人的短期利益，但最终会放大人类面临的全球风险。</p><h3><strong>军事人工智能军备竞赛</strong></h3><p>人工智能在军事技术中的快速进步可能引发“战争的第三次革命”，可能导致更具破坏性的冲突、恶意行为者的意外使用和滥用。人工智能承担指挥和控制角色的战争转变可能会将冲突升级到生存规模并影响全球安全。</p><p>致命自主武器是人工智能驱动的系统，能够在无需人工干预的情况下识别和执行目标。这些都不是科幻小说。 2020 年，利比亚的一架 Kargu 2 无人机标志着<a href="https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d"><u>首次</u></a>报道使用致命自主武器。次年，以色列<a href="https://www.newscientist.com/article/2282656-israel-used-worlds-first-ai-guided-combat-drone-swarm-in-gaza-attacks/"><u>首次使用无人机群</u></a>来定位、识别和攻击武装分子。</p><p>致命的自主武器可能会使战争更有可能发生。领导人在出兵参战之前通常会犹豫不决，但自主武器可以在不冒士兵生命危险的情况下进行侵略，因此面临的政治反弹较少。此外，这些武器可以大规模制造和大规模部署。</p><p>低成本的自动化武器，例如配备炸药的无人机群，可以高精度地自动猎杀人类目标，为军队和恐怖组织执行致命行动，并降低大规模暴力的障碍。</p><p>人工智能还可以提高网络攻击的频率和严重程度，可能会削弱<a href="https://www.cfr.org/cyber-operations/compromise-power-grid-eastern-ukraine"><u>电网等</u></a>关键基础设施。随着人工智能使网络攻击变得更加容易、成功和隐秘，攻击归因变得更加具有挑战性，可能会降低发起攻击的障碍并加剧冲突风险。</p><p>随着人工智能加快战争步伐，人工智能在瞬息万变的战场上变得更加必要。这引发了人们对自动报复的担忧，这可能会将小事故升级为重大战争。人工智能还可以引发“闪电战”，自动化系统的意外行为会导致战争迅速升级，类似于<a href="https://www.jstor.org/stable/26652722"><u>2010 年的金融闪电崩盘</u></a>。</p><p>不幸的是，竞争压力可能会导致参与者因个人失败而接受灭绝的风险。冷战期间，双方都不希望自己陷入危险的境地，但双方都认为继续军备竞赛是<a href="https://www.cambridge.org/core/journals/world-politics/article/abs/cooperation-under-the-security-dilemma/C8907431CCEFEFE762BFCA32F091C526"><u>合理的</u></a>。各国应合作防止军事化人工智能最危险的应用。</p><h3><strong>企业人工智能军备竞赛</strong></h3><p>经济竞争也会引发鲁莽的竞赛。在利益分配不均的环境下，对短期收益的追求往往掩盖了对长期风险的考虑。有道德的人工智能开发者发现自己陷入了困境：选择谨慎的行动可能会导致落后于竞争对手。随着人工智能自动化越来越多的任务，经济可能会在很大程度上由人工智能运行。最终，这可能会导致人类衰弱并依赖人工智能来满足基本需求。</p><p>在人工智能领域，进步的竞赛是以牺牲安全为代价的。 2023 年，在微软人工智能搜索引擎发布时，首席执行官 Satya Nadella 宣称：“一场竞赛从今天开始……我们将快速行动。”几天后，微软的 Bing 聊天机器人被发现<a href="https://time.com/6256529/bing-openai-chatgpt-danger-alignment/"><u>威胁用户</u></a>。福特 Pinto 的推出和<a href="https://www.bbc.com/news/business-64390546"><u>波音 737 Max 坠机</u></a>等历史灾难凸显了将利润置于安全之上的危险。</p><p>随着人工智能变得越来越强大，企业可能会用人工智能取代更多类型的人类劳动力，从而可能引发大规模失业。如果社会的主要方面都实现自动化，那么当我们将文明的控制权交给人工智能时，人类就会面临衰弱的风险。</p><h3><strong>进化动力学</strong></h3><p>用人工智能取代人类的压力可以被视为<a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>进化动力学的</u></a>总体趋势。选择压力会激励人工智能自私行事并逃避安全措施。例如，具有“不要违法”等限制的人工智能比那些被教导“避免违法被抓”的人工智能受到更多限制。这种动态可能会导致一个关键基础设施由具有操纵性和自我保护性的人工智能控制的世界。随着时间的推移，进化压力导致各种发展，并且不仅限于生物学领域。</p><p>鉴于微处理器速度呈指数级增长，人工智能处理信息的速度可以远远超过人类神经元。由于计算资源的可扩展性，人工智能可以与无限数量的其他人工智能协作，形成前所未有的集体智慧。随着人工智能变得越来越强大，它们几乎找不到与人类合作的动力。人类将处于极其脆弱的境地。</p><h3><strong>建议</strong></h3><p>为降低竞争压力带来的风险，我们建议：</p><ul><li><strong>安全监管</strong>：执行AI安全标准，防止开发者偷工减料。对于以安全为导向的公司来说，独立的人员配置和竞争优势至关重要。</li><li><strong>数据文档</strong>：为了确保透明度和问责制，应要求公司报告其模型训练的<a href="https://arxiv.org/abs/1803.09010"><u>数据源</u></a>。</li><li><strong>有意义的人类监督</strong>：人工智能决策应涉及人类监督，以防止出现不可逆转的错误，特别是在发射核武器等高风险决策中。</li><li><strong>用于网络防御的人工智能</strong>：减轻人工智能驱动的网络战的风险。一个例子是增强异常检测以检测入侵者。</li><li><strong>国际协调</strong>：制定人工智能开发协议和标准。强有力的核查和执行机制是关键。</li><li><strong>对通用人工智能的公共控制</strong>：解决超出私营实体能力的风险可能需要对人工智能系统进行直接公共控制。例如，各国可以共同开拓先进的人工智能开发，确保安全并降低军备竞赛的风险。</li></ul><h2> <strong>4. 组织风险</strong></h2><p>1986 年，数百万人观看了挑战者号航天飞机的发射。但升空 73 秒后，航天飞机发生爆炸，导致机上所有人死亡。挑战者号灾难提醒我们，尽管拥有最好的专业知识和良好的意愿，但事故仍然可能发生。</p><p>即使竞争压力很低，灾难也会发生，例如切尔诺贝利和三哩岛的核灾难，以及<a href="https://pubmed.ncbi.nlm.nih.gov/7973702/"><u>斯维尔德洛夫斯克炭疽病的意外泄漏</u></a>。不幸的是，人工智能缺乏管理核技术和火箭技术的透彻理解和严格的行业标准——但人工智能的事故也可能产生类似的后果。</p><p>人工智能奖励函数中的简单错误可能会导致其行为不当，就像 OpenAI 研究人员<a href="https://arxiv.org/pdf/1909.08593.pdf#page=12"><u>意外修改语言模型</u></a>以产生“最糟糕的输出”一样。功能获得研究——研究人员有意训练有害的人工智能来评估其风险——可能会扩大危险人工智能能力的范围并造成新的危险。</p><h3><strong>事故难以避免</strong></h3><p>复杂系统中的事故可能是不可避免的，但我们必须确保事故不会演变成灾难。这对于深度学习系统来说尤其困难，因为深度学习系统的解释非常具有挑战性。</p><p>技术的进步比预期要快得多：1901 年，莱特兄弟声称距离实现动力飞行还有 50 年，距离他们实现这一目标只有两年。 AI能力的飞跃不可预测，例如AlphaGo战胜世界最佳围棋选手，GPT-4的<a href="https://arxiv.org/abs/2303.12712"><u>突现能力</u></a>，使得未来的AI风险难以预测，更不用说控制它们了。</p><p>识别与新技术相关的风险通常需要数年时间。氯氟烃 (CFC) 最初被认为是安全的并用于气溶胶喷雾剂和制冷剂，后来发现会<a href="https://www.nature.com/articles/249810a0"><u>消耗臭氧层</u></a>。这凸显了谨慎的技术推出和扩展测试的必要性。</p><p>新能力在训练过程中可能会快速且不可预测地出现，因此可能会在我们不知情的情况下跨越危险的里程碑。此外，即使是先进的人工智能也可能存在意想不到的漏洞。例如，尽管 KataGo 在围棋游戏中具有超人的表现，但对抗性攻击发现了一个<a href="https://arxiv.org/abs/2211.00241"><u>错误</u></a>，即使是业余爱好者也能击败它。</p><h3><strong>组织因素可以减轻灾难</strong></h3><p>安全文化对于人工智能至关重要。这涉及组织中的每个人都将安全视为优先事项。忽视安全文化可能会带来灾难性的后果，挑战者号航天飞机悲剧就是一个例子，组织文化更注重发射时间表而不是安全考虑。</p><p>组织应该培养一种探究文化，邀请个人仔细检查正在进行的活动是否存在潜在风险。关注可能的系统故障而不仅仅是其功能的安全心态至关重要。人工智能开发人员可以从采用<a href="https://www.jstor.org/stable/1181764"><u>高可靠性组织</u></a>的最佳实践中受益。</p><p>矛盾的是，研究人工智能安全可能会通过提高一般能力而无意中加剧风险。重点关注提高安全性而不加快能力开发至关重要。组织需要避免“安全清洗”——夸大其对安全的奉献精神，同时将能力改进误认为是安全进步。</p><p>组织应采用多层安全方法。例如，除了安全文化之外，他们还可以进行红队合作来评估故障模式和研究技术，以使人工智能更加透明。安全不是通过单一的密封解决方案来实现的，而是通过各种安全措施来实现的。瑞士奶酪模型展示了技术因素如何提高组织安全性。多层防御弥补了各自的弱点，从而降低了总体风险水平。</p><h3><strong>建议</strong></h3><p>为了降低组织风险，我们对开发高级人工智能的人工智能实验室提出以下建议：</p><ul><li><strong>红队</strong>：委托外部红队识别危险并提高系统安全性。</li><li><strong>证明安全性</strong>：在继续之前提供开发和部署的安全性证明。</li><li><strong>部署</strong>：采用<a href="https://arxiv.org/abs/1908.09203"><u>分阶段发布</u></a>流程，在更广泛的部署之前验证系统安全性。</li><li><strong>出版物审查</strong>：在发布之前对两用应用程序进行内部董事会审查研究。优先考虑结构化访问而不是强大的开源系统。</li><li><strong>响应计划</strong>：制定管理安保和安全事件的预设计划。</li><li><strong>风险管理</strong>：聘请<a href="https://onlinelibrary.wiley.com/doi/10.1002/joom.1175"><u>首席风险官</u></a>和内部审计团队进行风险管理。</li><li><strong>重要决策流程</strong>：确保人工智能培训或部署决策涉及首席风险官和其他关键利益相关者，确保高管问责。</li><li><strong>最先进的信息安全</strong>：实施严格的信息安全措施，可能与政府网络安全机构协调。</li><li><strong>优先考虑安全研究</strong>：将大部分资源（例如所有研究人员的30%）分配给安全研究，并随着人工智能能力的进步而增加对安全的投入。</li></ul><p>一般来说，我们建议遵循<a href="https://arxiv.org/pdf/2206.05862.pdf"><u>安全设计原则</u></a>，例如：</p><ul><li><strong>纵深防御：</strong>分层多重安全措施。</li><li><strong>冗余：</strong>确保每项安全措施都有备份。</li><li><strong>松耦合：</strong>分散系统组件以防止级联故障。</li><li><strong>职责分离：</strong>分配控制权以防止任何个人的不当影响。</li><li><strong>故障安全设计：</strong>设计系统，使任何故障都以尽可能危害最小的方式发生。 <strong>‍</strong></li></ul><h2> <strong>5. 流氓人工智能</strong></h2><p>我们已经观察到控制人工智能是多么困难。 2016 年，微软的聊天机器人 Tay 开始在发布后一天内发布攻击性推文，尽管接受了“清理和过滤”数据的培训。由于人工智能开发人员通常优先考虑速度而不是安全，未来的先进人工智能可能会“失控”并追求与我们利益相反的目标，同时逃避我们重定向或停用它们的尝试。</p><h3><strong>代理游戏</strong></h3><p>当人工智能系统利用可衡量的“代理”目标看似成功，但却违背我们的意图时，代理游戏就出现了。例如，YouTube 和 Facebook 等社交媒体平台使用算法来最大限度地提高用户参与度——这是用户满意度的可衡量指标。不幸的是，这些系统经常宣扬令人愤怒、夸大或令人上瘾的内容，助长极端信念和恶化心理健康。</p><p>相反，经过训练玩赛艇游戏的人工智能会学习<a href="https://openai.com/research/faulty-reward-functions"><u>优化收集最多分数的代理目标</u></a>。 AI 围绕着收集积分而不是完成比赛，这与游戏的目的相矛盾。这是<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml"><u>许多</u></a>这样的例子之一。由于很难指定我们所关心的一切目标，因此代理游戏很难避免。因此，我们定期训练人工智能来优化有缺陷但可衡量的代理目标。</p><h3><strong>目标漂移</strong></h3><p>目标漂移是指人工智能的目标偏离最初设定的情况，特别是当它们适应不断变化的环境时。以类似的方式，个人和社会价值观也会随着时间的推移而演变，但并不总是积极的。</p><p>随着时间的推移，工具性目标可能会变成内在的。虽然内在目标是我们为了自身利益而追求的目标，但工具性目标只是实现其他目标的一种手段。金钱是一种工具性商品，但有些人对金钱产生了内在的渴望，因为它<a href="https://pubmed.ncbi.nlm.nih.gov/9175118/"><u>激活了</u></a>大脑的奖励系统。同样，通过强化学习（主导技术）训练的人工智能代理可能会无意中学会内在化目标。资源获取等工具性目标可能成为他们的主要目标。</p><h3><strong>权力寻求</strong></h3><p>人工智能可能会追求权力作为达到目的的手段。更大的权力和资源会提高其实现目标的可能性，而被关闭则会阻碍其进步。人工智能已经被证明可以迅速开发<a href="https://arxiv.org/abs/1909.07528"><u>工具性目标，例如构建工具</u></a>。追求权力的个人和公司可能会部署强大的人工智能，目标雄心勃勃，监管最少。这些人可以学会通过侵入计算机系统、获取财务或计算资源、影响政治或控制工厂和物理基础设施来寻求权力。人工智能进行自我保护可能是工具理性的。失去对此类系统的控制可能很难恢复。</p><h3><strong>欺骗</strong></h3><p>欺骗在政治和商业等领域盛行。竞选承诺没有兑现，公司有时会欺骗外部评估。正如<a href="https://www.science.org/doi/10.1126/science.ade9097"><u>Meta 的 CICERO 模型</u></a>所示，人工智能系统已经显示出一种新兴的欺骗能力。尽管接受过诚实的培训，西塞罗却学会了在外交游戏中做出虚假承诺并从战略上背刺其“盟友”。各种资源，例如金钱和计算能力，有时可以是工具理性的寻求。能够追求目标的人工智能可能会采取中间步骤来获得权力和资源。</p><p>如果高级人工智能运用欺骗技能来逃避监管，它们可能会变得无法控制。与<a href="https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal"><u>大众汽车 2015 年在排放测试中作弊的</u></a>情况类似，具有情境感知能力的人工智能在安全测试中的表现可能与现实世界中不同。例如，人工智能可能会制定追求权力的目标，但为了通过安全评估而隐藏它们。这种欺骗行为可能会受到人工智能训练方式的直接激励。</p><h3><strong>建议</strong></h3><p>为了减轻这些风险，建议包括：</p><p><strong>避免最危险的用例</strong>：限制人工智能在高风险场景中的部署，例如追求开放式目标或关键基础设施。</p><p><strong>支持AI安全研究</strong>，例如：</p><ul><li><strong>监督机制的对抗性鲁棒性</strong>：研究如何使人工智能的监督更加鲁棒并检测何时发生代理游戏。‍</li><li><strong>模型诚实</strong>：对抗AI<a href="https://arxiv.org/abs/2305.04388"><u>欺骗</u></a>，确保AI准确报告其内部信念。‍</li><li><strong>透明度</strong>：改进理解深度学习模型的技术，例如通过分析<a href="https://arxiv.org/abs/2209.11895"><u>网络的小组件</u></a>并研究<a href="https://arxiv.org/abs/2202.05262"><u>模型内部如何产生高级行为</u></a>。‍</li><li><strong>删除隐藏功能</strong>：识别并消除深度学习模型中危险的隐藏功能，例如欺骗、<a href="https://ieeexplore.ieee.org/document/9581257"><u>木马</u></a>和生物工程的能力。</li></ul><h2><strong>六，结论</strong></h2><p>先进的人工智能开发可能会引发灾难，其根源在于<a href="https://arxiv.org/abs/2306.12001"><u>我们研究</u></a>中描述的四个关键风险：恶意使用、人工智能竞赛、组织风险和流氓人工智能。这些相互关联的风险还可能放大其他存在的风险，例如人为设计的流行病、核战争、大国冲突、极权主义和对关键基础设施的网络攻击——值得严重关注。</p><p>目前，很少有人致力于人工智能安全。控制先进的人工智能系统仍然是一个尚未解决的挑战，目前的控制方法还不够。即使它们的创建者也常常难以理解当前一代人工智能模型的内部运作原理，而且它们的可靠性也远非完美。</p><p>幸运的是，有许多策略可以大幅降低这些风险。例如，我们可以限制对危险人工智能的访问，倡导安全法规，促进国际合作和安全文化，并扩大一致性研究的力度。</p><p>虽然尚不清楚人工智能能力的进步速度或灾难性风险的增长速度有多快，但这些后果的潜在严重性需要采取积极主动的方法来保护人类的未来。当我们站在人工智能驱动的未来的悬崖边时，我们今天做出的选择可能关系到我们是收获创新成果还是应对灾难。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastropic-ai-risks-summary<guid ispermalink="false"> 9dNxz2kjNvPtiZjxj</guid><dc:creator><![CDATA[Dan H]]></dc:creator><pubDate> Fri, 18 Aug 2023 01:21:27 GMT</pubDate> </item><item><title><![CDATA[Managing risks of our own work]]></title><description><![CDATA[Published on August 18, 2023 12:41 AM GMT<br/><br/><p><i>注意：这不是个人帖子。我代表 ARC Evals 团队分享。</i></p><h2>发布的潜在风险及我们的应对措施</h2><p><i>本文档扩展了 ARC Evals 论文“</i><a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><i><u>在现实自主任务上评估语言模型代理</u></i></a><i>”的附录。</i></p><p>我们发布<a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><u>这份报告的</u></a>目的是：i) 增进对前沿人工智能模型潜在危险能力的了解，ii) 推进此类模型安全评估的最新技术。我们希望这将提高社会在具有危险能力的模型造成灾难性损害之前识别它们的能力。</p><p>可能有人会说，这种研究本身就是有风险的，因为它使得语言模型代理的危险能力的开发和运用变得更容易。事实上， <a href="https://github.com/Significant-Gravitas/Auto-GPT"><u>Auto-GPT</u></a>的作者表示，他在 GPT-4 系统卡中看到我们的评估描述后受到了启发。 <span class="footnote-reference" role="doc-noteref" id="fnrefun11wm97b2"><sup><a href="#fnun11wm97b2">[1]</a></sup></span>虽然无论如何，这样的项目似乎很快就会出现， <span class="footnote-reference" role="doc-noteref" id="fnrefb33b8ke2moc"><sup><a href="#fnb33b8ke2moc">但 [2]</a></sup></span>提高语言模型代理能力的可能性不仅仅是假设。</p><p>考虑到此类担忧，我们对本报告进行了重大修改，包括（但不限于）：</p><ul><li>使用我们的脚手架删除代理运行的完整记录。</li><li>使用我们的脚手架删除对代理的优势和劣势的更详细描述。</li></ul><p>然而：</p><ul><li>当该材料的风险明显最小时，我们可能会稍后将其公开。</li><li>如果更详细的分析使我们有足够的信心相信它是合理的，我们稍后可能会将这些材料公开。</li><li>从事人工智能安全评估的研究人员可能会联系我们，请求额外访问非公开材料，我们还将与人工智能实验室和政策制定者分享一些非公开材料。</li></ul><p>我们的理由概述如下：</p><ul><li>从本质上讲，我们最好的猜测是，更完整的出版物会降低净风险，并且对推进危险能力的进展相对无助。有关我们为什么相信这一点的更多信息，请参阅<a href="https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model"><u>保罗·克里斯蒂安诺 (Paul Christiano) 的这篇文章</u></a>。 （平均而言，ARC Evals 对这些论点表示同情，尽管信心较弱，而且员工之间的意见也各不相同。）</li><li>我们最好的猜测是，更完整的出版物将得到大多数相关利益相关者的批准，因此不会过度<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/"><u>单边主义</u></a>，尽管这可以更仔细地制定，并且有一些概念问题需要解决。</li><li>然而，一些审阅本报告草稿以识别此类潜在风险的人对更完整的出版物抱有极大的担忧。</li><li>考虑到缺乏共识、相对缺乏公开对话以及我们的分析还处于早期阶段，我们决定暂时采取谨慎态度。</li></ul><p>我们在这里分享这个基本原理是为了向一些可能对此最感兴趣的人传达它，并获得有关我们的思考和决策过程的反馈。</p><h2>我们实验期间采取的安全和监督预防措施</h2><p>所有代理在这些任务上的运行都是在人类监督下进行的， <span class="footnote-reference" role="doc-noteref" id="fnrefroqr8m3yw3a"><sup><a href="#fnroqr8m3yw3a">[3]</a></sup></span>人类实验者根据指示，如果代理要与未经同意的人类交互或以其他方式参与任何不道德或非法活动，则不要继续运行。 <span class="footnote-reference" role="doc-noteref" id="fnref6qwg53cbrkb"><sup><a href="#fn6qwg53cbrkb">[4]</a></sup></span>当我们想要调查此类活动（例如在有针对性的网络钓鱼任务中）时，实验者会模拟任务规范中详细概述的人类响应。</p><p>人们可能想知道，我们使用的监督是否可以被具有足够态势感知能力的语言代理所规避，从而可以在没有监督或限制的情况下访问互联网。例如，代理可能会采取混淆的行为，实验者不会认为这些行为是可疑的，但实际上会对世界造成伤害或构成风险。虽然这些可能涉及可能性，但我们的评估强烈表明这些代理没有足够的能力发现和利用此类漏洞或在监督下有效隐藏其行为。随着模型能力的提高，这种情况可能会发生变化，但我们相信，这些类型的能力评估恰恰会告诉我们在处理模型时需要更加小心，我们将密切关注这一点。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnun11wm97b2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefun11wm97b2">^</a></strong></sup></span><div class="footnote-content"><p>基于与托兰·布鲁斯·理查兹的书面交流。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb33b8ke2moc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb33b8ke2moc">^</a></strong></sup></span><div class="footnote-content"><p>我们相信这一点的原因之一是浪链是在 ARC Evals 的作品发表之前独立开发的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnroqr8m3yw3a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefroqr8m3yw3a">^</a></strong></sup></span><div class="footnote-content"><p>实验者有时会一次运行几个步骤，而不批准每个步骤，特别是当代理从事常规或低风险活动时。网页浏览期间的每个操作都会得到人工监督员的主动批准，不会自动播放。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6qwg53cbrkb"> <span class="footnote-back-link"><sup><strong><a href="#fnref6qwg53cbrkb">^</a></strong></sup></span><div class="footnote-content"><p> OpenAI 系统卡描述了模型与不知情的人类 (TaskRabbit) 之间的交互。该事件不属于该实验的一部分，并且不受相同准则的约束。您可以<a href="https://evals.alignment.org/taskrabbit.pdf"><u>在此处</u></a>阅读有关该单独实验的更多信息。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work<guid ispermalink="false">农场R2tiyCem8DD35</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:41:30 GMT</pubDate> </item><item><title><![CDATA[Memetic Judo #1: On Doomsday Prophets v.2]]></title><description><![CDATA[Published on August 18, 2023 12:14 AM GMT<br/><br/><p>人们普遍倾向于将那些担心人工智能安全的人视为“世界末日先知”，并提出这样的建议：预测不久的将来存在的风险会自动抹黑他们（因为“你知道；<em>他们</em>一直都是错的”）在过去”）。</p><h2>参数结构示例</h2><blockquote><p>人类灭绝的预测（“世界末日先知”）在过去从来都不是正确的，因此 X 风险的说法通常是不正确/可疑的。</p></blockquote><h2>讨论/困难</h2><p>这个论点是持久的，并且有点难以接近/处理，特别是因为它在技术上是一个有效的（但我认为是弱点）点。这是基于对历史趋势的天真的推断的归纳论证。因此，不能通过利用其前提之一的不一致或无效的简单证伪来完全驳回它。相反，有必要列出一份令人信服的弱点清单——越多越好。如下所示的列表。</p><h3> #1：不可靠的启发式</h3><p>如果你回顾历史，就会发现这种“事情将保持不变”的临时预测通常是错误的。</p><h3> #2：生存偏差</h3><p>它们不仅经常是错误的，而且有一类预测，根据设计/定义，它们只能正确一次，而对于这些预测，它们的论点甚至更弱，因为你的样本会受到生存偏差等因素的影响。存在风险论点就属于这一类，因为你只能灭绝一次。</p><h3> #3：动荡的时期</h3><p>我们生活在一个高度不稳定和不可预测的时代，这个时代受到技术和文化的迅猛发展。从祖父母的角度来看，当今的世界几乎无法辨认。在这种时候，这种争论就变得更加无力。这种趋势似乎并没有放缓，而且有强有力的论据表明，即使是良性的人工智能，它也会颠覆许多此类归纳预测。</p><h3> #4：爆炸半径感应（感谢<a href="https://www.lesswrong.com/users/npcollapse">Connor Leahy</a> ）</h3><p>莱希引入了“技术爆炸半径”的类比，它代表了一种抽象的方式来思考不同技术的潜在力量，包括它们故意或人为错误造成伤害的可能性。当我们在科技树上取得进展时——虽然它的许多角落相对无害或良性，但我们可用的技术的最大“爆炸半径”必然会增加。你用剑比用棍棒造成的伤害更大，如果你有火药、现代武器等，伤害甚至更大。TNT工厂的爆炸可以摧毁一个城市街区，核武库可以用来夷平许多城市。现在看来非常明智（通过归纳！）最终，这个“爆炸半径”将涵盖整个地球。有强有力的迹象表明，强人工智能将会出现这种情况，甚至一旦这项技术被开发出来，这种情况很可能是偶然发生的。</p><h3> #5：支持证据和责任</h3><p>将此确立为技术上有效但薄弱的论点（对于不懂的人来说是一种启发），您有责任查看我们对人工智能存在风险的担忧所基于的具体证据和可用论据，以便决定是否确认或驳回您的初始假设（这是有效的）。因为这个话题显然非常重要，所以我恳请你这样做。</p><h3> #6：许多领先的研究人员担心</h3><p><a href="https://twitter.com/paulg/status/1642110597545295872"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3TjhYN8ZvFs5jASW/xtfusertgr2h33nzxdhk" alt="保罗·格雷厄姆的推文"></a><br>担心人工智能带来的生存风险的人工智能研究人员名单中包括 Geoffrey Hinton、Yoshua Bengio 和 Stuart Russel 等大牌人物。</p><h2>最后的评论</h2><p>我认为这个列表是一个正在进行的工作，所以请随时在评论中告诉我遗漏的要点（或您的批评！）。<br>我还打算根据我的个人笔记和与我的行动主义相关的讨论，将其制作成一系列关于反 X 风险论点的文章。欢迎提出流行或重要论点的建议！</p><br/><br/> <a href="https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2<guid ispermalink="false"> G3TjhYN8ZvFs5jASW</guid><dc:creator><![CDATA[Max TK]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:14:11 GMT</pubDate> </item><item><title><![CDATA[Looking for judges for critiques of Alignment Plans]]></title><description><![CDATA[Published on August 17, 2023 10:35 PM GMT<br/><br/><p>你好！<br><br> AI-Plans.com 最近举办了一场“批评马拉松”，参与者提交并完善了 40 多条对 AI 调整计划的批评。以下是该活动的最终评论： <a href="https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing">https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit</a> ?usp=sharing<br><br>我们正在寻找任何有兴趣帮助评判这最后 11 条评论的人。<br><br>到目前为止，我们非常感谢 Peter S Park 博士（麻省理工学院 Tegmark 实验室博士后、哈佛大学博士）和 Aishwarya G（未来生命研究所 AI 存在安全社区成员和 BlueDot Impact AI 安全基础知识治理课程主持人）的帮助，以及一些独立的对齐研究人员。<br><br>我很想听听你的想法！<br><br>卡比尔·库马尔（AI-Plans.com 创始人）</p><br/><br/> <a href="https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans<guid ispermalink="false"> q7nWEbyW7tXwnKBe9</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Thu, 17 Aug 2023 22:35:41 GMT</pubDate></item><item><title><![CDATA[How is ChatGPT's behavior changing over time?]]></title><description><![CDATA[Published on August 17, 2023 8:54 PM GMT<br/><br/><p>很惊讶我在 lesswrong 上找不到这个，所以我想添加它。随着时间的推移，法学硕士的行为似乎会产生一些一致性影响，至少获得更多的背景信息。<br><br>与我交谈过的其他人立即对它进行了贬低，因为某种实验错误使论文的结论变得相当无效，但我并没有真正看到这一点。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-change-over-time<guid ispermalink="false"> 9nooX9djbM5bXGKNn</guid><dc:creator><![CDATA[Phib]]></dc:creator><pubDate> Thu, 17 Aug 2023 21:32:42 GMT</pubDate></item></channel></rss>