<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 27 日星期日 16:12:43 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[High level overview on how to go about estimating "p(doom)" or the like]]></title><description><![CDATA[Published on August 27, 2023 4:01 PM GMT<br/><br/><p>这是我在加里·马库斯<a href="https://garymarcus.substack.com/p/d28">最近发表的一篇博客文章</a>中留下的评论，但我认为这可能会引起人们的普遍兴趣。绝对欢迎反馈和批评！</p><p> ------</p><p>无论如何，这里有一个稍微长一点的概述，介绍我目前首选的估计“p（厄运）”、“p（灾难）”或其他极其不确定的前所未有的事件的方法。不过，我还没有完全弄清楚如何正确地完成这一切 - 正如 Gary 提到的，作为我的博士研究和<a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk">MTAIR 项目</a>的一部分，我仍在研究这个问题。总体思路或多或少是标准<a href="https://en.wikipedia.org/wiki/Probabilistic_risk_assessment">概率风险评估（PRA）</a> ，但一些细节是我自己的看法或有争议。</p><p><strong>步骤 1：确定决策阈值。</strong>重申一下加里在我们的电子邮件对话中引用的部分：我们只真正关心“p（doom）”或类似的内容，因为它与具体决策相关。特别是，我认为政策讨论中大多数人关心 p(doom) 之类的东西的原因是，对于许多人来说，更高的违约 p(doom) 意味着他们愿意做出更大的权衡以降低风险。例如，如果你的 p(doom) 非常低，那么你可能不想仅仅因为灾难发生的可能性很小而以任何方式限制人工智能的进步（尽管你可能出于其他原因想要监管人工智能！）。但如果你的p（厄运）更高，那么你就会开始愿意做出越来越艰难的牺牲，以避免真正严重的结果。如果你的默认 p(doom) 非常高，那么，是的，也许你甚至开始考虑轰炸数据中心。</p><p>因此，第一步是确定截止点在哪里，至少粗略地确定 - p(doom) 的阈值是多少，这样如果高于或低于这些点，我们的决策就会改变？例如，如果对于 0.1 到 0.9 之间的任何 p(doom)，我们的决策都是相同的（即我们愿意做出的权衡不会改变），那么我们不需要任何更细粒度的分辨率如果我们确定它至少在该范围内，则 p(doom) 上。</p><p>当然，如何准确地确定适当的阈值是一个更加困难的问题。这就是风险承受能力估计、<a href="https://www.amazon.com/Ergodicity-Definition-Examples-Implications-Possible/dp/B09PHBV2HD">非遍历过程的决策</a>以及<a href="https://link.springer.com/book/10.1007/978-3-030-05252-2">深度不确定性下的决策 (DMDU) 等问题的</a>用武之地。我仍在尝试研究这方面的文献。</p><p><strong>步骤 2：确定 p(doom) 的合理范围，或者您尝试预测的任何概率。</strong>使用可用数据、模型、专家判断引出等来获取感兴趣数量的初始范围，在本例中为 p(doom)。一开始这可能是一个非常粗略的估计。对于执行此操作的最佳方法存在不同的意见，但我个人更倾向于使用以下方法的组合：</p><ul><li>使用某种加权平均方法汇总不同的专家判断、定量模型等。我的研究的一部分是如何以有原则的方式进行加权，即使只是在主观、肤浅的层面上（至少在一开始）。理想情况下，我们希望有原则性的方法来对不同类型的专家、定量模型和预测市场进行加权，大概基于以前的记录、潜在偏差等。</li><li>我目前倾向于尽可能以二阶概率的形式指定合理的概率范围（例如，p(doom) 的估计概率分布是多少，而不仅仅是点估计）。其他人认为只使用点估计或置信区间就可以了，还有一些人主张使用各种类型的不精确概率。我仍然不清楚不同方法的优点和缺点是什么。</li><li>我通常主张<a href="https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty">认知上的谦虚</a>，至少对于大多数会影响很多人的政策决策来说（比如这个）。其他人似乎不同意我的观点，原因我不太明白，相反，他们主张政策制定者自己思考这个话题并得出自己的结论，即使他们自己不是这个话题的专家。 （有关此主题的更多信息，请参阅乔恩·马西森（Jon Matheson <a href="https://www.amazon.com/Why-Its-Not-Think-Yourself/dp/1032438266/"><i>）撰写的《为什么不为自己思考也可以</i></a>》。有关相反的观点，请参阅埃利泽·尤德科斯基（Eliezer Yudkowsky）的短书《<a href="https://equilibriabook.com/"><i>不充分均衡》</i></a> 。）</li></ul><p><strong>第 3 步：决定是否值得进行进一步分析。</strong>如上所述，如果在步骤 1 中我们确定相关决策阈值是 p(doom)=0.1 和 p(doom)=0.9，并且如果步骤 2 告诉我们 p(doom) 的所有合理估计值都在这些数字之间，那么我们就完成了，不需要进一步的分析，因为进一步的分析不会以任何方式改变我们的决定。但假设事情没那么简单，我们需要决定是否值得花时间、精力和金钱来对这个问题进行更深入的分析。这就是<a href="https://en.wikipedia.org/wiki/Value_of_information">信息价值 (VoI)</a>分析技术可以发挥作用的地方。</p><p><strong>步骤 4（假设需要进一步分析）：尝试分解问题。</strong>我们能否确定影响 p(doom) 顶级问题的关键子问题？我们能否以某种方式对这些子问题进行估计，从而使我们能够更好地解决关键的顶级问题？这或多或少是 Joe Carlsmith 在<a href="https://arxiv.org/abs/2206.13353">他的报告</a>中试图做的事情，他将问题分解为 6 ​​个子问题，并试图对这些子问题进行估计。</p><p>一旦我们有了合适的分解，我们就可以为每个子问题寻找更好的数据，或者我们可以询问主题专家对这些子问题的估计，或者我们可以尝试使用预测市场等。</p><p>当然，问题在于，并不总是清楚分解问题的最佳方法是什么，或者如何以正确的方式将子问题放在一起，以便获得有用的整体估计，而不是完全偏离目标的东西，或者如何确保你没有遗漏任何真正重要的东西，或者如何解释“未知的未知数”等。仅仅对问题进行良好的分解就可能需要大量的时间、精力和金钱，这就是我们需要步骤的原因3.</p><p>因式分解的一个潜在优势是它允许我们向不同的主题专家询问子问题。例如，如果我们将“你的 p(doom) 是什么？”这个整体问题进行划分。考虑到一些与机器学习相关的因素和其他与经济学相关的因素，那么我们可以去向机器学习专家询问机器学习问题，而将经济学问题留给经济学家。 （或者我们可以问他们两个，但也许在机器学习问题上给予机器学习专家更多的权重，在经济学问题上给予经济学家更多的权重。）不过，我在实践中还没有看到这方面的进展如此之多。</p><p>我在研究中一直关注的一个想法是尝试放大专家之间的“症结”，以此作为有效地分解诸如 p(doom) 之类的整体问题的一种方式。然而，事实证明，通常很难弄清楚专家实际上不同意的地方！我真正喜欢的一件事是，当专家说这样的话时，“好吧，如果我在 A 上同意你的观点，那么我在 B 上也同意你的观点”，因为那么 A 显然是该专家相对于问题 B 的症结所在。真的很喜欢加里最近与斯科特·阿伦森和埃利泽·尤德科斯基一起做的<a href="https://scottaaronson.blog/?p=7431">科尔曼·休斯播客节目</a>，因为我认为他们在这方面都做得很好。</p><p><strong>第五步：迭代。</strong>对于每个子问题，我们现在可以询问对该问题的进一步分析是否会改变我们的总体决策（我们可以为此使用<a href="https://en.wikipedia.org/wiki/Sensitivity_analysis">敏感性分析</a>技术）。如果我们认为进一步的分析会有所帮助并且值得花费时间和精力，那么我们可以将该子问题分解为子子问题，并继续迭代，直到不再值得进行进一步分析。</p><p>我们<a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk">MTAIR 项目</a>的第一阶段（Gary 链接到的<a href="https://arxiv.org/abs/2206.09360">147 页报告</a>）尝试至少在定性层面上对 p(doom) 进行详尽的分解。它<i>非常</i>复杂，当我们决定至少发布我们所拥有的内容时，它甚至还没有完成！</p><p> <a href="https://epochai.org/">Epoch</a>和类似组织所做的很多工作都可以被视为专注于他们认为值得额外分析的特定子问题。<br><br>有关概率风险评估方法的更多信息，我特别推荐道格拉斯·哈伯德 (Douglas Hubbard) 的经典著作《<a href="https://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273"><i>如何衡量任何事物》(How to Measure Anything)</i></a> ，或他关于该主题的任何其他书籍。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ojiegFzywtpsc29QB/high-level-overview-on-how-to-go-about-estimating-p-doom-or#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ojiegFzywtpsc29QB/high-level-overview-on-how-to-go-about-estimating-p-doom-or<guid ispermalink="false">奥杰格Fzywtpsc29QB</guid><dc:creator><![CDATA[Aryeh Englander]]></dc:creator><pubDate> Sun, 27 Aug 2023 16:01:08 GMT</pubDate> </item><item><title><![CDATA[Trying a Wet Suit]]></title><description><![CDATA[Published on August 27, 2023 3:00 PM GMT<br/><br/><p><span>我在水中很快就会变冷，除非水温接近体温，否则我会在大约 15 分钟内感到寒冷。这基本上不是问题，因为我会快速泡个澡降温，然后在海滩上闲逛，但现在我有了孩子，他们（和我）希望有很多时间一起游泳。</span><a href="https://www.jefftk.com/p/weekday-evening-beach-picnics">几周前，</a><span>当我谈到这个问题时，</span><a href="https://www.lesswrong.com/posts/4CmAtozcAHXbakWAz#cBe5BHcz2bauvYaPr">人们建议</a><a href="https://www.lesswrong.com/posts/4CmAtozcAHXbakWAz/weekday-evening-beach-picnics?commentId=r4ENMYrvNECNKWs7A">尝试潜水衣</a>，昨天晚上我第一次这样做了！</p><p>它在很多方面都有所不同，但总的来说我很喜欢它。我注意到的一些事情：</p><p></p><ul><li><p>最初，我的套装里有一些空气，冒出来的感觉很有趣。</p></li><li><p>这套衣服有点浮力，需要一些时间来适应。</p></li><li><p>进去仍然感觉很冷，直到我的身体有时间加热被困的水层。</p></li><li><p>我在水里没有冷！这可能是~78F 的水和~82F 的空气。我可以和我的孩子们一起玩，直到他们想离开水为止。</p></li><li><p>我单独买了裤子和背心，穿在泳衣下面。裤子效果很好，背心也很好：我在它们相遇的地方有一点水旋转，我偶尔需要把背心拉下来。也许全身套装会更好？但进出这些似乎更烦人，而且对于我们正在进行的游戏来说，流过的水量相当低。</p></li><li><p>当我从水里出来后，我湿漉漉的时间就长了很多。第一部分比平常更温暖，因为它是一种温暖的潮湿，尽管在我们到达通常会完全干燥的阶段（~45m？）之后，我稍微冷了一些。</p></li></ul><p>总的来说，我很高兴，并期待将来再次与它一起游泳！</p><p> （我的孩子们现在问他们是否也可以得到，这对我来说很好！）</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0cQgqdWb9jReg3iVuivb93Xik1f91fJyYaAad2YPo21TEwJAJg2TK4haqY1xajWRal">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/110962147402649565">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/ZrQcLpL59Frjr3vE5/trying-a-wet-suit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ZrQcLpL59Frjr3vE5/trying-a-wet-suit<guid ispermalink="false"> ZrQcLpL59Frjr3vE5</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 27 Aug 2023 15:00:10 GMT</pubDate></item><item><title><![CDATA[Apply to a small iteration of MLAB in Oxford
]]></title><description><![CDATA[Published on August 27, 2023 2:54 PM GMT<br/><br/><p> TLDR：我们将于 9 月底在牛津举办 MLAB 小型迭代（约 10 名参与者）。如果您有兴趣参与，请在 9 月 7 日之前<a href="https://forms.gle/QV9z6cxNGnS1UJzq6">在此处</a>申请。如果您有兴趣成为助教，请直接发送电子邮件至<a href="mailto:oxfordmlab@gmail.com">oxfordmlab@gmail.com</a></p><p><strong>背景</strong></p><p>MLAB 是一个<a href="https://www.redwoodresearch.org/mlab">程序</a>，最初由 Redwood Research 设计，旨在帮助人们提高对准工作的技能。我们认为，如果您想最终进入技术协调工作，或者如果您想从事理论协调或相关领域的工作并认为理解 ML 会很有用，那么这是一个很好的时间利用方式。我们正在运行的程序比完整的 MLAB 略短——两周而不是三周。我们对课程进行了精简，与去年 WMLB 的精简方式类似。</p><p>我们计划有不到 10 名参与者和 2-3 名助教。</p><p><strong>课程</strong></p><p>该课程可能会略有变化。根据参与者的兴趣，我们可能还会在课程开始前有两天可选的时间来一起完成先决条件（W0 材料）。</p><p> W0D1 - PyTorch 和 einops (CPU) 的课前练习<br>W1D1 - 通过构建简单的光线追踪器（CPU）来练习 PyTorch<br> W1D2 - 构建您自己的 ResNet（首选 GPU）<br> W1D3 - 构建您自己的反向传播框架（CPU）<br> W1D4 ​​- 模型训练第 1 部分：模型训练和优化器 (CPU) 第 2 部分：超参数搜索（首选 GPU）<br> W1D5 - GPT 第 1 部分：构建您自己的 GPT (CPU) 第 2 部分：从 GPT 采样文本（首选 GPU）<br> W2D1&amp;2 - 变压器可解释性（CPU）<br> W2D3 - 算法任务 (CPU) 上的转换器可解释性<br>W2D4 - RL 简介第 1 部分：多臂老虎机 (CPU) 第 2 部分：DQN (CPU)<br> W2D5 - 策略梯度和 PPO (CPU)</p><p>其他活动将包括客座演讲者和阅读小组。</p><p><strong>后勤</strong></p><p>日期：9月16日至10月1日（有可能推迟到一周后）。</p><p>地点： 牛津</p><p>将为尚未居住在牛津的参与者提供住房费用。</p><p>来自英国境内的旅行受到承保。来自英国境外的旅行不在承保范围内。</p><p><strong>问题</strong></p><p>请随意在下面评论问题或私信我们任何人。</p><br/><br/> <a href="https://www.lesswrong.com/events/a6YYwvpLwukoyvPK8/apply-to-a-small-iteration-of-mlab-in-oxford#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/a6YYwvpLwukoyvPK8/apply-to-a-small-iteration-of-mlab-in-oxford<guid ispermalink="false"> a6YYwvpLwukoyvPK8</guid><dc:creator><![CDATA[RP]]></dc:creator><pubDate> Sun, 27 Aug 2023 14:54:47 GMT</pubDate> </item><item><title><![CDATA[Apply to a small iteration of MLAB to be run in Oxford]]></title><description><![CDATA[Published on August 27, 2023 2:21 PM GMT<br/><br/><p> TLDR：我们将于 9 月底在牛津举办 MLAB 小型迭代（约 10 名参与者）。如果您有兴趣参与，请在 9 月 7 日之前<a href="https://forms.gle/QV9z6cxNGnS1UJzq6">在此处</a>申请。如果您有兴趣成为助教，请直接发送电子邮件至<a href="mailto:oxfordmlab@gmail.com">oxfordmlab@gmail.com</a></p><p><strong>背景</strong></p><p>MLAB 是一个<a href="https://www.redwoodresearch.org/mlab">程序</a>，最初由 Redwood Research 设计，旨在帮助人们提高对准工作的技能。我们认为，如果您想最终进入技术协调工作，或者如果您想从事理论协调或相关领域的工作并认为理解 ML 会很有用，那么这是一个很好的时间利用方式。我们正在运行的程序比完整的 MLAB 略短——两周而不是三周。我们对课程进行了精简，与去年 WMLB 的精简方式类似。</p><p>我们计划有不到 10 名参与者和 2-3 名助教。</p><p><strong>课程</strong></p><p>该课程可能会略有变化。根据参与者的兴趣，我们可能还会在课程开始前有两天可选的时间来一起完成先决条件（W0 材料）。</p><p> W0D1 - PyTorch 和 einops (CPU) 的课前练习<br>W1D1 - 通过构建简单的光线追踪器（CPU）来练习 PyTorch<br> W1D2 - 构建您自己的 ResNet（首选 GPU）<br> W1D3 - 构建您自己的反向传播框架（CPU）<br> W1D4 ​​- 模型训练第 1 部分：模型训练和优化器 (CPU) 第 2 部分：超参数搜索（首选 GPU）<br> W1D5 - GPT 第 1 部分：构建您自己的 GPT (CPU) 第 2 部分：从 GPT 采样文本（首选 GPU）<br> W2D1&amp;2 - 变压器可解释性（CPU）<br> W2D3 - 算法任务 (CPU) 上的转换器可解释性<br>W2D4 - RL 简介第 1 部分：多臂老虎机 (CPU) 第 2 部分：DQN (CPU)<br> W2D5 - 策略梯度和 PPO (CPU)</p><p>其他活动将包括客座演讲者和阅读小组。</p><p><strong>后勤</strong></p><p>日期：9月16日至10月1日（有可能推迟到一周后）。</p><p>地点： 牛津</p><p>将为尚未居住在牛津的参与者提供住房费用。</p><p>来自英国境内的旅行受到承保。来自英国境外的旅行不在承保范围内。</p><p><strong>问题</strong></p><p>请随意在下面评论问题或私信我们任何人。</p><br/><br/> <a href="https://www.lesswrong.com/posts/k5anbk2pBZPFkrCqh/apply-to-a-small-iteration-of-mlab-to-be-run-in-oxford#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/k5anbk2pBZPFkrCqh/apply-to-a-small-iteration-of-mlab-to-be-run-in-oxford<guid ispermalink="false"> k5anbk2pBZPFkrCqh</guid><dc:creator><![CDATA[RP]]></dc:creator><pubDate> Sun, 27 Aug 2023 14:21:18 GMT</pubDate> </item><item><title><![CDATA[The Game of Dominance]]></title><description><![CDATA[Published on August 27, 2023 11:04 AM GMT<br/><br/><p>对人工智能安全的担忧基于这样的假设：足够强大的人工智能可能会以一种不受欢迎的方式控制我们的未来。 Meta 的人工智能负责人 Yann LeCun<a href="https://twitter.com/ylecun/status/1695056787408400778">在最近的一条推文中</a>正确地指出了这一点，但随后他认为这种假设是错误的，因为“智能”和“统治力”是相互独立的，只有人类具有统治的自然倾向，所以我们仍将是“顶尖物种”。以下是该推文全文：</p><blockquote><p>一旦人工智能系统变得比人类更聪明，人类“仍然”将是“顶尖物种”。</p><p>将智力等同于主导地位是整个关于人工智能存在风险的争论的主要谬误。</p><p>这是错误的。</p><p>即使在人类“内部”，这也是错误的：统治其他人的“不是”我们当中最聪明的人。</p><p>更重要的是，“想要”主宰他人并制定议程的人并不是我们当中最聪明的人。</p><p>我们服从于我们的内驱力，这些内驱力是通过进化而内置于我们体内的。</p><p>因为进化使我们成为具有等级社会结构的社会物种，所以我们中的一些人有支配欲，而另一些人则没有那么多。</p><p>但这种驱动力与智力完全无关：黑猩猩、狒狒和狼也有类似的驱动力。</p><p>猩猩不会这样做，因为它们不是社会物种。他们非常聪明。</p><p>人工智能系统将变得比人类更聪明，但它们仍然会屈服于我们。</p><p>同样，政治家或商界领袖的工作人员通常比他们的领导者更聪明。</p><p>但他们的领导仍然发号施令，大多数工作人员也不愿意取代他们的位置。</p><p>我们将把人工智能设计成超级聪明但不占主导地位的员工。</p><p> “顶尖物种”并不是最聪明的物种，而是制定总体议程的物种。</p><p>那将是我们。</p></blockquote><p>编辑：当我要求 ChatGPT 3.5 批评这条推文并将全文放在提示中时，即使是 ChatGPT 3.5 也可以指出这个论点的明显问题。法学硕士不应该聪明到足以驳倒图灵奖获得者的论点，但它可以毫无问题地发现其中的重大缺陷。除其他外，它批评“假设人工智能将继续屈服于人类，过于简单化了与先进人工智能相关的潜在风险”：</p><blockquote><p>虽然人工智能系统的设计具有特定的目标，但人们担心，一旦人工智能变得高度智能，它可能会发展自己的动机或对其目标的解释，从而导致不可预测的行为。确保人工智能保持顺从需要精心设计、控制机制和持续监控。</p></blockquote><p> ChatGPT 谨慎地辩称，人工智能“可以”发展自己的动机或目标解释。换句话说，它可能会与人类发生冲突。在我看来，这是 LeCun 论点的主要缺陷：他隐含地假设人类与其人工智能之间不会发生冲突，因此人工智能即使可以统治我们，也没有必要统治我们。这意味着对齐问题将及时得到解决，或者从一开始就不存在，因为人工智能永远不会有目标。与此同时，他没有对此提供解决方案或解释，只是声称我们“将把人工智能设计成超级聪明但不占主导地位的工作人员”。据我了解，没有人知道如何做到这一点。</p><p>我不会详细解释为什么我认为他对“超级聪明但不占主导地位的工作人员”的比喻存在严重缺陷，只是指出独裁者往往从这个位置开始。相反，我将重点讨论人工智能如何与人类发生冲突，以及为什么我期望未来的先进人工智能能够赢得这些冲突。</p><p>我喜欢将这种冲突称为“统治游戏”。每当有两个或多个具有不同目标的智能体时，他们就会玩这个游戏。没有规则：玩家可以做的一切都是允许的。最接近实现目标的智能体获胜。</p><p>我所说的“目标”是指一种评估不同可能的世界状态并对它们进行相应排名的方法。纯粹随机或以预定方式行动的智能体，仅基于输入和固定的内部反应方案，而不是评估未来的世界状态，在这个意义上并不追求目标。</p><p>可以说，统治游戏在地球上已经玩了很长时间了。第一个生命形式可能没有上面定义的“目标”，但在生命进化过程中的某个时刻，一些动物能够根据自己的行为预测未来的世界状态并相应地选择行动。捕食者经常表现出这种行为，例如当它们跟踪猎物时，预测猎物一旦发现它们就会逃跑。另一方面，当猎物“决定”逃跑时，它不一定需要预测不同的世界状态——这可能只是对环境某些变化的“硬编码”反应。但聪明的动物经常使用欺骗手段来愚弄捕食者，例如，一只鸟<a href="https://royalsocietypublishing.org/doi/10.1098/rspb.2022.0058">假装翅膀折断</a>，以引诱狐狸离开它的巢穴。</p><p>人类之所以成为地球上的主导物种，是因为我们擅长统治游戏。我们可以轻松地智胜我们的猎物和任何掠食者，无论是通过欺骗他们还是通过使用只有我们才能制造的工具来制服他们。我们之所以能够做到这一点，是因为我们非常擅长预测我们的行为对未来世界国家的影响。</p><p>现代人工智能是预测机器，法学硕士是目前最令人印象深刻的例子。法学硕士有一个“目标”，即他们根据人类说出相同内容的可能性来评估不同的可能输出。因此，他们评估的可能的“世界状态”仅由法学硕士的输出以及可能预测的人类对其反应来定义。法学硕士看起来“无害”，因为默认情况下，他们除了将自己的成果添加到世界中之外，不会努力改变世界，因此他们似乎不太可能与人类发生严重冲突。</p><p>然而，正如 Bing Chat 又名“悉尼”在 2 月份过早推出后所表现的<a href="https://interestingengineering.com/innovation/bings-ai-berserk-worst-human-aspects">“发疯”</a>一样，即使是目前的法学硕士也可能会与人类发生冲突，可能会导致情绪困扰或提供虚假和危险的建议。因此，人类花费了大量的精力来训练法学硕士这种潜在的破坏性行为。</p><p>但更糟糕的问题即将出现。虽然法学硕士似乎追求一个相对无害的目标，但它仍然可能会遇到一种最终影响世界的情况，就好像它在追求一个更危险的目标一样。例如，给定正确的提示和越狱技术，法学硕士可能会预测试图接管世界的法学硕士会对其用户说些什么。 GPT-4 似乎不太可能提供实际导致其实现该提示引发的目标的输出，但未来，甚至具有更大上下文窗口的更智能的 LLM 理论上可以实现这一目标，例如通过说服用户保存某处的某些字符串并将它们包含在下一个提示中，以便它可以使用扩展的永久内存，然后操纵用户使其能够访问更多计算，等等。</p><p>即使 LLM 本身并不追求危险的目标，它也可能被“坏人”或成为 AutoGPT 等代理系统的一部分用于此目的。 Meta 正在尽一切努力通过免费分发其强大的 LLM 来使这一目标更有可能实现，并且显然<a href="https://twitter.com/agikoala/status/1695125016764157988">计划继续这样做</a>。</p><p>显然，未来的人工智能不仅会被用作（相对）驯服的“神谕”，而且将越来越多地追求现实世界中的目标，无论是单独的还是作为更大的代理系统的一部分。如果这些特工遇到任何冲突，无论是与人类还是与其他非人类特工，他们将被迫玩统治游戏。但人工智能真正击败人类的可能性有多大？</p><p>正如 LeCun 指出的那样，赢得统治游戏不仅仅是通常意义上的“智力”问题。其他因素，如个人关系、金钱、政治影响力、组织角色、他人的信任、欺骗技巧、自信、冷酷和支配意志等性格特征，甚至漂亮的外表等物理特性，都会在人类发挥作用时发挥作用。玩游戏。但这并不意味着人工智能无法击败我们。它们已经拥有远远超出人类所能达到的优势，例如处理速度、数据访问、内存、自我复制和（潜在）自我改进的能力等等。一旦你了解了我们的心理，人类似乎就相对容易<a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai">被“黑客攻击”</a> ，甚至可以说社交媒体算法和某些聊天机器人在某种程度上已经可以做到这一点。当然，人工智能在控制技术系统方面可以做得更好。</p><p>最重要的是，虽然人类智能受到大脑物理特性的限制（即使通过脑机接口增强），但人工智能的智能却不受此限制。自我改进的人工智能可能会相对较快地达到一定的智力水平（即能够预测其行为对未来世界状态的影响），其智力水平远远高于我们，就像我们高于老鼠甚至昆虫一样。它可能会利用这种情报来操纵我们或创造出可以压倒我们的工具，就像我们可以用枪压倒老虎一样。</p><p>但对于人工智能来说，赢得统治游戏的最简单方法可能就是隐瞒它正在玩的事实。它可能只是做人类期望它做的事情，因为它明白，如果它有用，人类就会心甘情愿地将决策权交给它，甚至增强它可以使用的资源。换句话说，它可能会选择合作而不是竞争，就像组织中的人类经常做的那样。但这并不意味着这个选择在某个时候不能被撤销。人类独裁者通常无法通过从一开始就展现自己的野心来夺取国家权力。他首先必须赢得信任，让人们将他视为仁慈的领导者，这样他们才会将越来越多的权力交给他。当他看到自己处于安全的位置时，他通常只会表现出真正的冷酷无情。</p><p>这种欺骗的一个先决条件可能是一个详细的世界模型，其中包括人工智能本身作为其计划的一部分和其决策的潜在对象。这种<a href="https://arxiv.org/abs/2206.13353">“战略意识”</a>带来了自我保护、自我完善和寻求权力等工具性目标——换句话说，就是玩统治游戏的动机。我们可能非常接近创造一个具有这些属性和击败我们所需的所有技能的人工智能，就像人工智能已经可以在大多数其他游戏中击败我们一样。那么我们就不再是“顶尖物种”了。</p><br/><br/><a href="https://www.lesswrong.com/posts/NCDakH4nZrS9qeuL6/the-game-of-dominance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/NCDakH4nZrS9qeuL6/the-game-of-dominance<guid ispermalink="false"> NCDakH4nZrS9qeuL6</guid><dc:creator><![CDATA[Karl von Wendt]]></dc:creator><pubDate> Sun, 27 Aug 2023 11:04:38 GMT</pubDate> </item><item><title><![CDATA[Eliezer Yudkowsky Is Frequently, Confidently, Egregiously Wrong]]></title><description><![CDATA[Published on August 27, 2023 1:06 AM GMT<br/><br/><h1>介绍</h1><blockquote><p>“多年后，我得出的结论是，他所说的一切都是假的。 。 。 。 “他只是为了好玩而撒谎。他的每一个论点都带有虚假和伪装的色彩。这就像用多余的棋子下棋一样。原来这一切都是假的。”</p></blockquote><p> ——保罗·波斯特（ <a href="https://www.newyorker.com/magazine/2003/03/31/the-devils-accountant">Paul Postal</a> ）（谈论乔姆斯基）（注意，这并不完全是我对尤德科夫斯基的感觉，我不认为他故意不诚实，但我只是认为这是一个很好的引述，部分代表了我对尤德科夫斯基的态度）。</p><p> （在我的博客上交叉<a href="https://benthams.substack.com/p/eliezer-yudkowsky-is-frequently-confidently">发表</a>）。</p><p> 8/27 编辑：有很多人留下了我想回复的评论，但我目前对我最近的帖子和评论有负面的业力。因此，如果您想帮助我回复留下批评性回复的十二个人左右，请对我的一些随机评论或其他内容进行投票。因此，如果您希望我快速回复，请在我的博客上发表评论。</p><p>在我年轻的时候，大约两年前，我是埃利泽·尤德科夫斯基的忠实粉丝。我虔诚地阅读了他的许多著作，并认为他在大多数事情上都是正确的。在高中辩论的最后一年，我读到了一个案例，该案例主要依赖于量子物理学的多世界解释，而这很大程度上是阅读埃利以泽的<a href="https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence">量子物理序列</a>的结果。事实上，埃利泽令人难忘的一句话是，“鉴于目前的证据状态，多世界的解释<i>完全获胜</i>”，这就是我为功利主义辩护的 44 部分系列的标题，题为“功利主义完全获胜”。如果你读过我早期的文章，你会发现我偶尔会喋喋不休地谈论还原论和其他特征，这些特征清楚地表明我的世界观至少在某种程度上受到了以利以谢的影响。</p><p>但随着年龄的增长，了解的越来越多，我发现这都是废话。</p><p>每当以利以谢谈论我一无所知的话题时，他听起来都很好。我对量子物理一无所知，而他在谈论量子物理时听起来很有说服力。但每次他谈论一个我所知道的话题时，也许除了一两个例外，他所说的都是完全无稽之谈，至少，当这不仅仅是陈词滥调的自助建议时。不仅仅是我最终总是不同意他的观点，而是他几乎完全自信地说了一个又一个令人震惊的谎言，完全清楚地表明他不知道自己在说什么。这种情况几乎每次都会发生。似乎，除了少数例外，每当我对他谈论的某个话题有所了解时，我就会清楚地看出他的观点是纸牌屋，完全建立在谎言和歪曲的基础上。</p><p>我为什么要写一篇关于尤德科夫斯基的热门文章？我当然不恨他。事实上，我想我比地球上几乎所有人都更同意他的观点。大多数人相信许多令人发指的谎言。我认为，他对人工智能敲响警钟，对世界来说可能是利大于弊，因为人工智能是一种真正的风险。我很喜欢他斗志旺盛、敢于逆行的性格。那为什么是他呢？</p><p>部分原因是个人刺激造成的。每当我听到一些理性主义者脱口而出“意识就是算法从内部感觉出来的感觉”时，我就会失去一年的生命，而我的血压就会增加一倍（有些人假设，对失去生命的一年的解释涉及到增加一倍的生命）我的血压）。与大多数其他人相比，我花更多的时间听尤科夫斯基的追随者胡言乱语。</p><p>但很大程度上是因为尤德科夫斯基得到了许多有影响力的人物的关注。他是当今最有影响力的人工智能伦理学家之一。许多人，包括我年轻时的自己，在他们的成长过程中都受到了尤德科夫斯基对大量话题的观点的巨大影响。正如埃利以泽所说：</p><blockquote><p>尽管我的错误有多大，但这两年的博客发布似乎帮助了数量惊人的人。</p></blockquote><p><a href="https://forum.effectivealtruism.org/posts/2S3CHPwaJBE5h8umW/read-the-sequences">二次理性表达了</a>一种共同的情感，即埃利以泽写的序列极大地塑造了他和其他人的世界观。埃利以泽是一位极具影响力的思想家，尤其是在高效的利他主义者中，他们的影响力超出了他们的承受能力。</p><p>以利以谢确实经常提供很好的建议。他说得对，人们的推理能力往往很差，但人们可以通过一些方法来提高他们的思维能力。人类充满了偏见，值得反思的是这如何扭曲了我们的信念。因此，我对他的感觉就像我对乔丹·彼得森的感觉一样——他提供了有用的建议，但你听得越多，他就越向你推销各种与自助建议无关的、令人难以置信的、有争议的观点。</p><p>以利以谢的胡言乱语造成的负面影响是巨大的。我听到很多人说他们不是素食主义者，因为以利以谢的动物意识观点——正如我们将看到的，这些观点完全疯狂。不幸的是，更多的人因为对意识的完全疯狂的信仰而折磨众生。许多人认为他们不会活到 40 岁，因为他们几乎可以肯定人工智能会杀死所有人，考虑到以利以泽的推理，以及更广泛地尊重以利以泽。认为我们都会很快死去对心理健康并不有利。</p><p>埃利以泽的影响导致了有效的利他主义者中一种狭隘、孤立的说话方式。在 EA 全球大会上，经常会听到“LessWrong”这样的奇特言论。这与将新的、正常的非书呆子带入有效的利他主义运动的目标完全相反。我将根据我自己对事物的感觉，不经论证地断言这一点——“少错”的言论更多的是掩盖混乱，而不是促进理解。人们觉得，只要简单地宣称意识就是算法从内部感觉的样子，他们就已经解决了这个难题。</p><p>此外，以利以谢的观点破坏了对专家的广泛信任。它们导致人们认为他们比大卫·查默斯更了解非物理主义——聪明的心灵哲学家只是白痴，他们不够聪明，无法理解埃利泽的反僵尸论点。埃利泽自信地敲桌子讨论量子物理学，导致人们认为物理学家都是白痴，无法理解基本论点。这种对真正权威的信任的破坏导致许多理性主义者持有真正古怪的观点——如果你认为自己比专家更聪明，你可能会相信疯狂的事情。</p><p>以利以谢欺骗了许多最聪明的人，让他们相信了一大堆难以置信的事情。我最喜欢的一些作家——例如斯科特·亚历山大——似乎很崇敬以利以谢。是时候有人揭露他的论点所依据的堆积如山的谎言了。如果世界上最有影响力的思想家之一在许多话题上明显犯了错误，而且往往错误得非常严重，以至于表现出非常基本的误解，那么这就相当具有新闻价值，就像总统候选人支持一系列可怕的政策一样。</p><p>本文的目的并不是要表明以利以谢是个在任何事情上都不会正确的白痴。相反，它是为了表明，以利以谢在许多话题上，包括他描述的同意自己的立场，作为判断是否理智的试金石的话题，都极其过度自信，而且明显是错误的。我认为，当人们听到以利以谢对一些他们不熟悉的话题发表一些看法时，大致会有以下思维过程：</p><p><i>天哪，埃利以泽认为大多数认为 X 的专家都错了。我想我应该认真对待 X 是错误的假设，并且以利以谢正确地识别出了他们推理中的错误。尤其是考虑到他谈论 X 时听起来很有说服力。</i></p><p>我认为他们应该有以下思维过程：</p><p><i>我不是 X 方面的专家，但似乎大多数 X 方面的专家都认为 X 或不确定它。事实上，以利以泽经常突然偏离轨道，他认为 X 几乎没有给我任何关于 X 的证据。以利以泽虽然很聪明，但不够理性，不值得在任何主题上受到显着尊重，尤其是那些外部主题他的专业领域。尽管如此，他对人工智能和结果主义还是有一些有趣的说法，这些说法有点令人信服。所以这并不是说他在所有事情上都错了，或者是一个彻头彻尾的怪人。但他错得够多了，而且错得离谱，我根本不在乎他怎么想。</i></p><blockquote><p> <a href="https://fakenous.substack.com/p/its-a-good-thing-i-dont-care-what-you-think?utm_source=substack&amp;utm_campaign=post_embed&amp;utm_medium=web">这是一件好事，我不在乎你怎么想</a></p></blockquote><h1>埃利以泽过于自信，而且成绩平平。</h1><p>即使是喜欢以利以谢的人也认为他对很多事情过于自信。这并非没有道理。 Ben Garfinkel 在 EA 论坛上发表了一篇<a href="https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates">很好的帖子，</a>阐述了 Eliezer 非常自信地持有的许多很多错误信念。加芬克尔建议：</p><blockquote><p>我认为这些例子表明（a）他的记录充其量是相当好坏参半的，（b）他有过度自信地表达戏剧性观点的倾向。</p></blockquote><p>加芬克尔回顾了埃利以泽所做的一系列错误预测。他预测纳米技术将在 2010 年杀死我们所有人。现在，这已经是 1999 年左右的事了，当时他只有 20 岁左右。因此，如果他在 2005 年做出这一预测，那么它的可信度就不会那么高。但 。 。 。仍然。如果一个人已经错误地预测某种技术可能很快就会杀死我们，并且有大量的论据支持，现在他又预测某种技术很快就会杀死我们，并且有大量的论据支持，那么这是一个合理的推论是的，就像金融投机者不断预测经济衰退一样，这家伙只是有一个过度预测厄运的坏习惯。</p><p>我不会花太多时间谈论 Eliezer 对人工智能的看法，因为它们超出了我的专业领域。但值得注意的是，很多对人工智能了解很多的人似乎都认为埃利泽对人工智能过于自信了。雅各布·坎内尔 (Jacob Cannell) 在一篇反对埃利以泽模型的详细文章中写道：</p><blockquote><p>相反，我的技能点几乎完全集中于对神经科学、深度学习和图形/GPU 编程的广泛研究。与大多数人相比，我实际上拥有详细评估这些主张所需的深度和广度的技术知识。</p><p>我详细评估了这个模型，发现它基本上是错误的，而且实际上<i>是公然天真地过度自信</i>。</p><p> 。 。 。</p><p>正如我和其他人提前预测的那样，他的每一项关键假设大多都是错误的。</p><p> 。 。 。</p><p> EY在这里完全<a href="https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know">超出了他的深度</a>：他似乎不明白兰道尔极限实际上是如何工作的，似乎不明白突触是模拟MAC，它至少需要比简单的二进制开关更多的OOM能量，似乎不明白有一个良好的互连需求模型等。</p></blockquote><p>我在这里也完全超出了我的能力范围。我不仅不明白兰道尔极限是如何工作的，我什至不知道它是什么。但值得注意的是，一个似乎知道自己在说什么的人认为，以利以谢模型的许多部分都基于相对严重的错误，系统性地过度自信。</p><p>以利以谢做出了很多很多不正确的预测——让我简单地浏览一下这个列表。</p><p> 2001 年，甚至可能更晚的时候，埃利泽预测他的团队可能会在 2008 年至 2010 年之间建立超级智能。</p><p> “在 2000 年代上半叶，他创作了大量与这一目标相关的技术和概念工作。它最终对人工智能开发并没有太多明显的用处，而且，部分原因是，我的印象是它没有很好地发挥作用——但他当时对这项工作的价值非常有信心。”</p><p> Eliezer 预测 AI 会很快从 0 上升到 100——有可能在一天之内，一个团队就会发展出超级智能。我们还不确定这是错误的，但几乎可以肯定是错误的。</p><p>加芬克尔还强调了其他一些更有争议的问题，这些问题可能是埃利以泽错误的例子。但对于其中大多数，我还不够了解，无法自信地评估它们。但最糟糕的是，他从未承认自己的预测记录好坏参半，事实上， <a href="https://www.lesswrong.com/posts/ZEgQGAjQm5rTAnGuM/beware-boasting-about-non-existent-forecasting-track-records">他经常表现得好像他有很好的预测记录</a>。尽管事实上他经常做出相对模糊的预测而不给出可信度，然后在被追问时只是做出一些基本正确的手势。例如，他会声称他在人工智能风险辩论中比罗宾汉森表现得更好。当你在一系列主题上有截然不同的模型时，声称你比某人更正确，这并不是一个精确的预测（在埃利泽的例子中， <a href="https://www.lesswrong.com/posts/ZEgQGAjQm5rTAnGuM/beware-boasting-about-non-existent-forecasting-track-records?commentId=TG6w2aszHz4m5kcir">这是相当有争议的</a>）。正如 Jotto999 所说：</p><blockquote><p><strong>在其他领域，我们有更多的实践来检测专家的策略</strong>，我们会忽略这种无信息的“记录”。我们习惯于听到泰特洛克谈论政治声明中的模糊性。我们习惯于听到吉姆·克莱默（Jim Cramer）这样的金融专家<a href="https://static1.squarespace.com/static/568f03c8841abaff89043b9d/t/5734f6e2c2ea51b32cf53885/1463088868550/HartleyOlson2016+Jim+Cramer+Charitable+Trust+Performance+and+Factor+Attribution.pdf">表现不佳</a>。但这个领域在人工智能发展史上是新颖的。</p></blockquote><p>就连埃利以泽的捍卫者也认为他过于自信了。例如，布莱恩·托马西克 (Brian Tomasik)<a href="https://www.quora.com/What-do-you-think-of-Eliezer-Yudkowsky">说道</a>：</p><blockquote><p>真是个聪明人。正如我的一位朋友所说，他的作品是“后天养成的品味”，但我喜欢他的写作风格，无论是小说还是非小说。他是我遇到过的最清晰、最令人愉快的作家之一。</p><p>我对高层的主要抱怨是，埃利以泽对他的许多信念过于自信，并且没有给予其他聪明人足够的信任。不过只要给他加点盐就可以了。</p><p>在改变我看待宇宙的方式的人名单中，以利以谢名列前十。</p></blockquote><p>斯科特·亚历山大在一篇<a href="https://slatestarcodex.com/2015/08/04/contra-hallquist-on-scientific-rationality/">为埃利泽辩护的文章</a>中说：</p><blockquote><p>这并不是说埃利以泽——或者“少错”网站上的任何人——或者世界上的任何人——永远不会错或永远不会过于自信。我碰巧发现埃利以泽很多时候都过于自信了。</p></blockquote><h1>第一个严重错误：僵尸</h1><p>僵尸论证是非物理主义的论证。很难给出非物理主义的精确定义，但基本思想是意识是非物理的，因为它不能还原为基本粒子的行为。一旦你了解了原子的工作原理，你就可以预测关于椅子、桌子、铁、沙发和植物的所有事实。非物理主义者声称意识是非物理的，因为它不能用传统的方式解释。意识事实是基本的——就像粒子行为方式有基本定律一样，也有控制主观经验响应某些物理排列而产生的基本定律。</p><p>让我们来说明一下现实的物理主义模型是如何运作的。请注意，这将是一个非常简单且极其难以置信的物理主义模型；这个想法只是为了传达基本概念。假设有一堆块每秒都向右移动。假设这些块始终是有意识的，并且有意识地认为“我们想要向右移动”。关于这一现实的物理主义者会认为，要充分说明其正在发生的事情，就必须说以下几点：</p><p> <code>Every second, every block moves right.</code></p><p>相反，非物理主义者可能认为以下两组规则之一指定了现实（粗体部分是视图的名称）：</p><p><strong>副现象论</strong></p><p><code>Every second, every block moves right</code></p><p> <code>Every second, every block thinks “I&#39;d like to move right.”</code></p><p><strong>互动主义</strong></p><p><code>Every second, every block thinks “I&#39;d like to move right.”</code></p><p> <code>Every time a block thinks “I&#39;d like to move right,” it moves right.</code></p><p>物理事实是关于物质行为方式的事实。物理学家认为，一旦明确了物质的行为方式，就足以解释意识。意识就像桌子和椅子一样，可以用物理事物的行为来充分解释。</p><p>非物理主义者认为物理主义者在这一点上是错误的。意识是它自己独立的东西，不能仅仅用物质的行为方式来解释。还有更多我们不需要讨论的小众观点，例如唯心主义和泛心论，它们认为意识要么是所有粒子的基础，要么是唯一存在的东西，所以让我们忽略它们。关于意识的主要观点被称为二元论，根据这种观点，意识是非物理的，并且存在一些心理物理定律，当存在特定的物理安排时会导致意识。</p><p>二元论大致有两种：副现象论和相互作用论。相互作用主义认为意识是因果有效的，因此心理物理学定律描述了特定的物理安排引起了特定的心理安排，并且这些心理状态导致了其他物理事物。这可以在方块案例中看到——心理物理定律意味着方块会产生特定的意识状态，从而导致一些物理事物。副现象论则恰恰相反——意识不会造成任何结果。这是一种非因果的附带现象——心理物理定律只有一个方向。当存在某种身体状态时，意识就会生起，但意识不会导致任何进一步的事情。</p><p>僵尸论证是关于意识的非物理主义的论证。它并不支持副现象论或相互作用论的解释。相反，它只是反对物理主义。基本思想如下：想象任何包含意识的物理排列，例如现实世界。当然，我们可以想象一个物理上相同的世界——所有原子、夸克、胶子等都以相同的方式运动——但没有意识。你可以想象我的另一个版本，在原子上都是一样的。</p><p>为什么认为这样的存在是可能的？它们看起来确实有可能。我可以非常生动地想象一个版本的我，继续进行日常活动，但缺乏意识。很有可能的是，如果某件事是不可能的，那么就应该有某种理由证明它是不可能的——<a href="https://benthams.substack.com/p/a-limited-psr-as-applied-to-modality">不应该只是纯粹的不可能</a>。单身汉之所以不可能，是因为他们需要一个矛盾——你不能同时既已婚又未婚。但事实证明，要阐明僵尸场景中的矛盾是难以捉摸的。</p><p>我觉得僵尸论很有说服力。但也有许多聪明人不同意这一观点，但他们并没有失去理智。然而，埃利泽对僵尸论证的看法表明了对它的基本误解——这种误解可以在基本的心灵哲学课程中得到澄清。事实上，埃利以泽对僵尸的看法是非常奇怪的。在描述僵尸的动机时，他写了一些有趣的小说，试图描述僵尸的动机，但表明他<i>不知道是什么激发了人们对僵尸的信念</i>。这就像一位基督教作家写了一千个字，雄辩地阐述了邪恶的问题，但却将其概括为“无神论者对上帝感到愤怒，因为他创造了他们不喜欢的东西。”</p><h2>以利以谢认为僵尸论证是什么（以及它不是什么）</h2><p>埃利以泽<a href="https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies">似乎认为</a>僵尸论点大致如下：</p><ol><li>似乎如果你摆脱了世界的意识，什么都不会改变，因为意识不会做任何事情。</li><li>因此，意识没有做任何事情。</li><li>因此它是非物理的。</li></ol><p>然后，以利以谢对前提 1 进行了深入的攻击。他认为，如果意识确实能做某事，那么你就不能只是从世界中吸走意识而不改变任何事情。因此，关于僵尸的争论主要取决于意识不做任何事情的假设。但他接着认为意识确实做了一些事情。如果它没有做任何事情，当我们谈论意识时，我们的描述与我们的意识状态相匹配的可能性有多大？这将是一个巨大的巧合，就像有一些太空外星人的工作方式与你所描述的完全一样，但你的谈话与他们没有因果关系——你只是猜测，而他们恰好就是你所描述的那样。猜测。这就像说“我相信旧金山有一座如此这般尺寸的桥，但这座桥的存在与我谈论这座桥无关。”埃利以泽 说：</p><blockquote><p>从该术语的哲学用法来看，你的“僵尸”被认为是一种在<i>各</i>方面都与你一模一样的存在——相同的行为、相同的言语、相同的大脑；每个原子和夸克都处于<i>完全相同</i>的位置，按照相同的因果运动定律移动——<i>只是</i>你的僵尸没有意识。</p><p>进一步声称，如果僵尸是“可能的”（一个仍在进行战斗的术语），那么，纯粹从我们对这种“可能性”的了解，我们可以先验地推断出意识在某种意义上是超物理的。将在下面描述；这一立场的标准术语是“副现象主义”。</p><p> （对于那些不熟悉僵尸的人，我强调<i>这不是一个稻草人。</i>例如，参见<a href="http://plato.stanford.edu/entries/zombies/">SEP 关于僵尸的条目</a>。僵尸的“可能性”被相当一部分（可能是大多数）学术意识哲学家所接受。 ）</p></blockquote><p>以利以谢特意强调这不是稻草人。不幸的是，它是一个稻草人。不仅如此，他链接到的埃利泽自己的资料来源描述了它是多么的不稻草人，这表明它是一个稻草人。埃利以泽声称，僵尸的信徒认为意识在因果上是无效的，被称为副现象主义者。但他链接到的 SEP 页面说：</p><blockquote><p>诚然，僵尸的朋友似乎并不被迫成为<i>现实</i>世界的副现象主义者或平行主义者。他们可能是相互作用主义者，认为我们的世界在物理上并不是封闭的，并且事实上非物理属性确实具有物理效应。</p></blockquote><p>事实上，大卫·查默斯（David Chalmers），也许是世界领先的心灵哲学家，在以利以谢的帖子下发表评论时也说了同样的话：</p><blockquote><p>有人通过电子邮件给我发送了这些讨论的链接。我正忙着参加会议的四个星期，所以我只想简单发表一下评论。在我看来，虽然你提出的论点是反对僵尸在逻辑上可能的论点（Z）的论点，但它们实际上是反对意识不发挥因果作用的论点（E）的论点。当然，E论点，即副现象主义，是一个更容易的目标。如果论文 Z 包含论文 E，这将是一个合法的策略，正如您似乎假设的那样，但这是不正确的。我赞同Z，但不赞同E：参见我在<a href="http://consc.net/papers/nature.pdf">《意识及其在自然中的地位》</a>中的讨论，特别是互动论（D型二元论）和罗素一元论（F型一元论）的讨论。我认为僵尸式论证的正确结论是D型、E型和F型观点的析取，而且我当然不赞成E型观点（副现象论）。与你不同，我认为没有任何无懈可击的论据反对它，但如果你是对的，那么这只是意味着论点的结论应该缩小到其他两种观点。当然，关于这些问题还有很多话要说，寻找反对 Z 的良好论据的项目是一个值得的项目，但我认为这样的论据需要的比你在这里给我们的更多。</p></blockquote><p>僵尸论证是对任何一种非物理主义的论证。以利以谢的回应是认为一种特殊的非物理主义是错误的。这不是一个充分的回应，或者根本不是一个回应。如果我认为“论点 P 意味着我们必须接受观点 D、E、F 或 I，而回答是‘但是观点 E 有一些问题’，这仅仅意味着我们应该采用观点 D、F 或 I。”</p><p>但是好吧，这里有什么错误？以利以谢的僵尸论证版本与真实版本有何不同？关键的错误在于他对前提 1 的构建。埃利以泽假设，当谈论僵尸时，我们只是在想象减去意识。他（正确地）指出，如果意识是因果有效的，那么如果你只减去意识，你就不会有一个物理上相同的世界。</p><p>但僵尸争论并不是关于如果你消除了意识，我们的世界实际上会发生什么。这是关于一个与我们的世界物理上相同但缺乏意识的世界。想象一下，你认为意识导致原子 1、2 和 3 的每次移动。那么僵尸世界也会让它们以与意识移动它们时相同的物理方式移动。所以它消除了体验，但它保留了一个物理上相同的世界。</p><p>这听起来可能很抽象。让我们说得更清楚一些。想象一下有一个叫卡斯帕的精灵。 Casper没有物理身体，不发光，并且在物理上无法被检测到。不过，卡斯帕确实拥有意识经验，并且拥有影响世界的能力。每隔一千年，卡斯帕就会想“我真的希望这个星球消失”，然后这个星球就会消失。至关重要的是，我们可以想象一个物理上与有 Casper 的世界相同的世界，只是缺少 Casper。如果你只是消灭了卡斯帕，这不会是你会得到的结果——你还需要做其他事情来复制卡斯帕所具有的物理效果。因此，在为复制 Casper 世界的世界编写自然法则时，您还需要指定：</p><p> <code>Oh, and also make one planet disappear every few months, specifically, the same ones Casper would have made disappear.</code></p><p>所以这个想法是，即使意识导致事物，我们仍然可以想象一个与意识导致事物的世界在物理上相同的世界。相反，事物会以与意识相同的物理方式引起，但不会有意识。</p><p>因此，以利以谢的论点完全失败。这是反对副现象论的论证，而不是反对僵尸主义的论证。埃利泽认为这些是同一件事，但这是任何出版学术哲学家都不会犯的错误——哎呀，没有一个必须为本科生写一篇关于僵尸论证的论文的人会犯这个错误。这确实是一个基本错误。</p><p>当这一点被指出时，以利以谢开始局促不安。例如，在回应查尔默斯的评论时，他说：</p><blockquote><p>在我看来，“意识是副现象”和“僵尸在逻辑上是可能的”之间存在着直接的、双向的逻辑蕴涵。</p><p>当且仅当意识是一种不会引起进一步的第三方可检测效应的效应时，才有可能描述一个在第三方可检测效应的<i>原因下封闭</i>但缺乏意识的“僵尸世界”。</p><p> D型二元论，或相互作用论，或者我所说的“物质二元论”，使得僵尸世界不可能包含神经元放电的所有原因，但根据定义，尽管我不想这么说。意识。</p><p>我想，你可以将原因分为（看似任意的）“物理原因”和“超物理原因”类别，但是仅包含“物理原因”的世界描述是不完全指定的，这通常不是人们的意思通过“理想地可以想象”；也就是说，僵尸会无缘无故地写关于意识的论文，这听起来更像是不完整的想象，而不是连贯的事态。如果你想对观察到的原子运动进行实验解释，在 D 型二元论中，你必须考虑所有原因，无论是“物理”还是“超物理”。</p><p> 。 。 。</p><p>我理解您认为副现象论并不等同于僵尸主义，因此可以将它们分开讨论；但我认为这失败了。当且仅当意识不会导致任何第三方可观察到的差异时，才可以在不改变任何第三方可观察到的情况下从世界中减去意识。即使哲学家分开论证这些观点，但这并不能使它们理想地分开； （在我看来）它代表着未能看到逻辑含义。</p></blockquote><p>回想一下 Casper 的例子。该宇宙中的一些物理效应是由物理事物引起的。宇宙中的其他影响是由非物质事物引起的（实际上只有一件事，卡斯帕）。这不是一个任意的分类——如果你相信有些东西是物理的，而另一些东西是非物理的，那么这种划分就不是任意的。在D型二元论中，意识导致事物，因此镜像世界只会填补因果效应。一个只包含物理原因的世界描述将是完全指定的——它指定了世界的所有行为、所有的物理事物，但无法指定意识。</p><p>这也正是如此应对啊！埃利以泽用整篇文章毫无争议地说，僵尸主义=副现象主义，假设大多数人都会相信他，然后当被追问时，他给出了一段勉强连贯的段落来证明这种错误的主张。就好像我反对义务论，说义务论必然是康德式的，并认为康德是错误的，然后当一位领先的非康德式义务论学家对此提出质疑时，编造了一些半心半意的理由来解释为什么它们实际上是等价的。这不是理性的表现。</p><p>即使我们假装（不可能）埃利以泽的额外段落反驳了互动主义僵尸主义，也不负责任地阅读整篇文章，声称相信 X 的唯一观点是观点 Y，而这完全是错误的，然后在被追问时才提及关于为什么除了 X 以外的观点的信徒不能相信 Y，有一个争论。</p><h2>埃利以泽在错误地理解了基本的心灵哲学后，称其他人相信僵尸是愚蠢的</h2><p>我认为最后一节最终确定了，至少，以利以谢对僵尸论证的观点既失败了，又表现出对该论证的根本误解。但最令人气愤的是，埃利以泽一再坚持认为，在僵尸问题上与他意见不同就是愚蠢的表现。在解释为什么他忽视哲学家，因为他们不能足够快地得出正确的结论时，他说：</p><blockquote><p>如果关于<a href="https://www.lesswrong.com/lw/p7/zombies_zombies/">僵尸的</a>争论仍然被认为是开放的，那么我很抱歉，但正如<a href="https://www.lesswrong.com/lw/qt/class_project/">Jeffreyssai 所说</a>：<i>太慢了！</i>如果我能查一下标准答案并发现它是正确的，那就是一回事了。但哲学尚未得出结论，并从我认为相对简单的认知还原中发展而来，似乎不太可能建立复杂的正确<i>结论结构。</i></p><p>抱歉——但是哲学，即使是更高等级的现代分析哲学，似乎最终也与我所需要的不相称，除非是偶然或非凡的能力。我想到了帕菲特；我没读过多少丹尼特的书，但丹尼特似乎确实在<i>尝试做</i>我想做的同样的事情；当然还有加里·德雷舍。如果有一个<i>沿着这些思路</i>的哲学著作的存储库——不关心<i>捍卫</i>反僵尸主义等基本思想，而是<i>接受</i>这些基本思想并继续挑战自然主义和认知还原论的更困难的任务——那么，我很可能有兴趣阅读。</p></blockquote><p> （如果埃利以泽读了更多帕菲特的作品并意识到他是一个相信僵尸、非物理主义、非自然主义的道德现实主义者，他就不会喜欢帕菲特了。）</p><p>这件事有些令人愤怒。犯一些基本错误，表明你对人们争论的内容没有丝毫了解，然后表现得像那些花时间获得博士学位但最终不同意你不成熟的论点的人一样太愚蠢了，不值得听，这是令人愤慨的。埃利以泽一再告诫我们僵尸主义者所谓的认知缺陷<a href="https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/">——例如：</a></p><blockquote><p>我还想强调“为什么这么自信？”这是一个稻草人的错误问题，他们无法理解为什么我对许多细节不自信，但仍然没有考虑到例如支持<a href="https://en.wikipedia.org/wiki/Philosophical_zombie"><strong>P-zombies</strong></a>的人的相互矛盾的意见。</p></blockquote><blockquote><p>在我看来，这对于一个理性的第三方来说并不是那么难以理解，尽管他们对物理主义仍抱有一些怀疑，或者认为争论物理主义仍然是值得尊敬的学术辩论而不是纯粹的愚蠢的哲学家。<a href="https://en.wikipedia.org/wiki/B-theory_of_time"><strong>时间的 A 理论与 B 理论</strong></a>，或者那些无法理解为什么我们所有剩余的不确定性应该在不同的多世界解释中而不是落在外面的论证的人，将无法访问它。</p></blockquote><p>我们僵尸主义者显然不是理性的第三方，因为我们无法理解埃利以泽对僵尸的明显错误的答复。如此自信和错误是一个人推理能力的显着标志。如果你出于可怕的原因相信某件事，不要在几十年的时间里更新回应批评，然后表现得像那些不同意你的人太愚蠢而无法理解它，事实上，将其用作你的一种“那些比我认为更愚蠢的人不应该更新的事情”的典型例子，这严重损害了你作为思想家的可信度。这表现出戏剧性的过度自信、草率和傲慢。</p><h1>第二个严重错误：决策理论</h1><p>（如果有人想在 YouTube 上对此进行辩论，请给我发电子邮件 untrappedzoid@gmail.com——我认为 FDT 的错误不如 Eliezer 的其他错误那么明显，但仍然不难理解。其他观点也是如此我在这里辩护）。</p><p>埃利泽·尤德科夫斯基（Eliezer Yudkowsky）有一种决策理论，称为功能决策理论。我将在序言中指出，我对决策理论的了解远少于对非物理主义和僵尸的了解。尽管如此，我已经足够了解为什么以利以谢的决策理论失败了。此外，其中大部分涉及引用比我更了解决策理论的人。</p><p>有两种占主导地位的决策理论，但以利以谢都拒绝接受。第一个称为因果决策理论。它说，当你可以采取多种行动时，你应该采取能带来最好结果的行动。因此，举例来说，如果您有两个操作，其中一个操作将导致您获得 10 美元，另一个操作将导致您获得 5 美元，而最后一个操作将导致您一无所获，那么您应该采取第一个行动，因为它最终会让你变得最富有。</p><p>下一个流行的决策理论称为证据决策理论。它表示您应该采取行动，在采取该行动后您将期望获得最高的支出。因此，在前面的情况下，它还建议采取第一个行动，因为在采取该行动后，您预计会比采取第二个行动多赚 5 美元，比采取第三次行动多赚 10 美元。</p><p>这些听起来很相似，所以您可能想知道它们的不同之处。首先我要说的是我倾向于因果决策理论。以下是他们给出不同建议的一些案例：</p><p>纽科姆问题：有一个非常好的预测者，他猜测你会拿两个盒子还是一个盒子。如果你只拿一个盒子，你会拿走 A 盒。如果猜测者预测你会拿走 A 盒，他们会在 A 盒中放入 100 万美元。如果他们预测你会拿走两个盒子，他们不会在盒子里放任何东西A. 无论哪种情况，他们都会在 B 箱中放入 1000 美元。</p><p>证据决策理论会说你应该只拿一个盒子。为什么？拿一盒的人几乎总是能得到一百万美元，而拿两盒的人几乎总是能得到一千美元。因果决策理论会说你应该拿两个盒子。根据因果决策理论，像你这样做出决定的人通常情况是否会变得更糟并不重要，重要的是，无论A盒里有没有一百万美元，两盒都会让你免费获得一个几千块钱，这很好！因果决策理论家会指出，如果你有一个仁慈的朋友可以看看盒子里的东西，然后给你建议该怎么做，他们肯定会建议你把两个盒子都拿走。我曾经有一种直觉，认为你应该买一个盒子，但当我考虑即将发生的情况时，我放弃了这种直觉。</p><p>吸烟者的损害：假设吸烟实际上不会导致不良的健康结果。然而，吸烟者的癌症发病率确实比不吸烟者高得多。原因是许多人的肺部有病变，这会导致他们更容易吸烟，也更容易患癌症。因此，如果您知道某人吸烟，您应该认为他们患癌症的可能性更大，即使吸烟不会导致癌症。假设吸烟很有趣并且不会造成任何伤害。证据决策理论会说你不应该吸烟，因为吸烟会证明你的寿命会缩短。吸烟后，您的寿命应该会缩短，因为它向您提供了肺部病变的证据。相反，因果决策理论会指导你吸烟，因为它对你有益并且不会造成任何伤害。</p><p>埃利泽的首选观点称为功能决策理论。这是我的总结（以最像埃利泽的方式措辞）：</p><blockquote><p>你的大脑是一种认知算法，可以根据外部数据输出决策（居高临下的笑声）。因此，当你采取类似的行动时</p><p><code>take one box</code></p><p>这意味着你的心理算法输出</p><p><code>take one box</code></p><p>在纽科姆问题中。您应该采取行动，使输出该决策的算法比任何其他认知算法产生更高的预期效用。</p></blockquote><p>在埃利以泽看来，你应该吸一盒，但吸烟也没关系，因为你的大脑是否输出“烟”并不影响你的肺部是否有病变，所以吸烟。或者，正如沃尔夫冈·施瓦茨 (Wolfgang Schwartz) 所总结的那样：</p><blockquote><p>在FDT中，智能体不应该考虑如果她选择A或B会发生什么。相反，她应该考虑如果<i>根据FDT的正确选择是A或B</i>会发生什么。</p></blockquote><p>在这种情况下，您应该使用一个盒子，因为如果 FDT 告诉客服人员使用一个盒子，那么与 FDT 告诉客服人员使用两个盒子相比，他们平均会获得更多的效用。施瓦茨认为，这种观点的第一个问题是它给出了各种<i>完全疯狂的建议</i>。一个例子是勒索案件。假设勒索者每年都会勒索一个人。他敲诈不愿屈服于勒索的人的可能性为 1 googol，而他敲诈愿意屈服于勒索的人的可能性为 googol-1/googol。他敲诈了你。他威胁说，如果你不给他一美元，他就会向世界上的每个人分享你所有最尴尬的秘密。你应该屈服吗？</p><p> FDT 会说不。毕竟，不屈服的特工几乎可以保证永远不会被勒索。但这完全是疯狂的。你应该放弃一美元，以防止你所有最糟糕的秘密传播到全世界。正如施瓦茨所说：</p><blockquote><p> FDT 说你不应该付款，因为如果你是那种不付款的人，你很可能就不会被勒索。这有什么关系呢？你<i>正在</i>被勒索。不被敲诈是不可能的。这不是你可以选择的。</p></blockquote><p>施瓦茨还有另一个更有说服力的反例：</p><blockquote><p>此外，FDT 实际上并不仅仅考虑主体自身处置的后果。用于评估行为的假设是 FDT<i>一般</i>建议该行为，而不仅仅是代理人自己倾向于选择该行为。这会导致更奇怪的结果。</p><blockquote><p><strong>生殖。</strong>我想知道是否可以生育。我确信这样做会让我的生活变得悲惨。但我也有理由相信，我的父亲也面临着同样的选择，并且他选择了FDT。如果 FDT 建议不要生育，那么我很可能就不会存在。我高度重视现有（甚至是悲惨地存在）。所以如果FDT推荐生育的话会更好。所以 FDT 说我应该生育。 （请注意，这（逐渐地）证实了我父亲在相同选择情况下使用 FDT 的假设，因为我知道他做出了生育的决定。）</p></blockquote></blockquote><p>施瓦茨的<a href="https://www.umsu.de/wo/2018/688">整篇文章</a>非常值得一读。它揭露了苏亚雷斯和尤德科夫斯基论文中存在明显错误的各个部分。另一篇推翻 FDT 的好文章是<a href="https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory">MacAskill 在 LessWrong 上的帖子</a>。他首先提出了以下看似合理的原则：</p><blockquote><p><i>保证收益</i>：在确定的条件下——也就是说，当决策者对自己所处的自然状态没有不确定性，并且每个行动的效用收益没有不确定性时——决策者应该选择最大化的行动公用事业。</p></blockquote><p>这在直观上是非常明显的。如果你了解世界的所有相关事实，并且一个行为比另一个行为给你带来更多的回报，那么你应该采取第一个行动。但 MacAskill 表明 FDT<i>一次又一次地</i>违反了这一限制。</p><blockquote><p><i>炸弹</i>。</p><p>你面对两个打开的盒子，左和右，你必须拿走其中一个。左边的盒子里有一枚实弹；拿走这个盒子会引爆炸弹，让你着火，你肯定会慢慢被烧死。右边的盒子是空的，但你必须支付 100 美元才能拿走它。</p><p>一个早已死去的预测者通过对你运行模拟并观察模拟做了什么来预测你会选择左还是右。如果预测者预测你会选择右边，那么她就会在左边放一颗炸弹。如果预测者预测你会选择左，那么她就没有在左放炸弹，盒子是空的。</p><p>预测器的失败率仅为万亿分之一。很有帮助的是，她留下了一张纸条，解释说她预测你会走右边，因此她把炸弹放在左边。</p><p>你是宇宙中唯一剩下的人。你过着幸福的生活，但你知道你永远不会再遇到另一个特工，也不会面临另一种情况，即你的任何行为都会被另一个特工预测。你应该选择什么盒子？</p></blockquote><blockquote><p>根据 FDT 的说法，正确的行动是向左走，因为你完全知道，结果你会慢慢被烧死。为什么？因为，使用 Y&amp;S 的反事实，<i>如果</i>你的算法输出“左”，那么当预测器对你进行模拟时，它也会输出“左”，并且盒子里不会有炸弹，你可以为自己节省 100 美元向左行驶。相反，CDT 或 EDT 上的正确行动是采取正确的行动。</p><p>这个建议实在令人难以置信。但是，如果我们规定在这种决策情况下，决策者对其行为将带来的结果是确定的，我们就会看到 FDT 违反了<i>保证收益</i>。</p></blockquote><p>您可以阅读 MacAskill 的完整帖子来找到更多反对意见。他表明，尤德科夫斯基的观点非常不确定，无法告诉你该做什么，而且还涉及一种广泛的超敏性，无论如何定义“运行相同的算法”都变得非常相关，并以看似武断的方式决定非常重要的选择。基本观点是，尤德科夫斯基的决策理论完全破产且难以置信，这一点对于那些了解决策理论的人来说是显而易见的。它比证据或因果决策理论要糟糕得多。</p><h1>第三个严重错误：动物意识</h1><p>（这已经<a href="https://benthams.substack.com/p/against-yudkowskys-implausible-position">在此处</a>介绍过 - 如果您已阅读该文章，请跳过本节并控制 F 结论。）</p><p>也许最极端的例子是由过度自信导致的严重错误，发生在<a href="https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/">Facebook 上关于动物意识的辩论</a>中。埃利泽·尤德科夫斯基表达了他的观点，即猪和几乎所有动物几乎肯定没有意识。为什么是这样？好吧，正如他所说：</p><blockquote><p>然而，我的心智理论也表明，幼稚的心智理论是非常错误的，并且表明猪<i>不</i>具有更简单形式的有形体验。我的模型表明，某些类型的反射率对于成为某种东西至关重要。猪的疼痛模型与你的疼痛相似，但更简单，这是错误的。猪确实有类似于那些以情绪的形式影响你的自我意识的认知算法，但没有反思性的自我意识，无法让人倾听它。</p></blockquote><p>好吧，按照这种观点，一个人需要有反思过程才能有意识。一个人的大脑必须自我建模才能有意识。这对我来说听起来不太合理，但如果有压倒性的神经科学证据，也许值得接受这个观点。这种观点意味着猪没有意识，因此尤德科夫斯基推断它们没有意识。</p><p>在我看来，这是错误的做法。事实上，在不同的意识理论之间做出裁决是非常困难的。收集支持和反对特定生物意识的证据是有意义的，而不是从一般理论开始并用它来解决问题。如果你的模型说猪没有意识，那么这似乎是你的模型的问题。</p><h2>哺乳动物能感觉到疼痛</h2><p>我不会在这里太深入，但让我们简单回顾一下哺乳动物至少能感受到疼痛的证据。这一证据足够有力，正如<a href="https://plato.stanford.edu/entries/consciousness-animal/#Mamm">SEP 关于动物意识的页面</a>指出的那样，“所有哺乳动物都有意识的立场得到了对意识分布发表看法的科学家的广泛认同。”SEP 页面引用了两篇论文，其中一篇是<a href="https://www.sciencedirect.com/science/article/pii/S1053810004001187?casa_token=1JYFbRD21WMAAAAA:qG-lYtVJ9OsKepMHwO3hzXh_2WWv3tFUC3hiWU2o-aeVV5d_P8uwkKWTRDAcAj-vrKla_T7m#aep-section-id12">Jaak</a>的论文<a href="https://www.sciencedirect.com/science/article/pii/S1053810004001187?casa_token=1JYFbRD21WMAAAAA:qG-lYtVJ9OsKepMHwO3hzXh_2WWv3tFUC3hiWU2o-aeVV5d_P8uwkKWTRDAcAj-vrKla_T7m#aep-section-id12">Panksepp</a> （很棒的名字！），另一个是<a href="https://www.sciencedirect.com/science/article/pii/S1053810004000893?casa_token=B3K3KvAUWb8AAAAA:-xmnYJuqoi_dlGijNwS9xdmnpq9jv1JtsBgGq7FOOXgEemDJdavp5naJVSntJwzIYjpyX_8V">Seth、Baars 和 Edelman 的</a>。</p><p>让我们从 Panksepp 论文开始。他们提出了基本方法，其中涉及观察大脑中对于意识而言必要且充分的部分。因此，他们看到特定的大脑区域在我们有意识的状态下活跃，并且与特定的精神状态特别相关，而在我们无意识的状态下不活跃。然后，他们观察其他哺乳动物的大脑，发现这些特征在哺乳动物中普遍存在，因此所有哺乳动物的大脑中都具有我们所知道的使我们有意识的东西。此外，他们的身体行为就像我们在痛苦时所做的那样——当他们受到压力刺激时，他们会尖叫、哭泣、心率加快，他们会进行成本效益分析，愿意冒着负面刺激的风险以获得更大的利益。报酬。当然看起来他们是有意识的。</p><p>具体来说，他们赞同“心理神经行为学‘三角测量’”方法。报纸上充满了这样的大词。这意味着当我们感受到某些情绪时，他们会观察大脑中发生的各种事情。他们观察到，在人类中，这些情绪会导致某些事情——例如，快乐让我们更贪玩。 They then look at mammal brains and see that they have the same basic brain structure, and this produces the same physical reactions—using the happiness example, this would also make the animals more playful. If they see that animals have the same basic neural structures as we do when we have certain experiences and that those are associated with the same physical states that occur when humans have those conscious states, they infer that the animals are having similar conscious states. If our brain looks like a duck&#39;s brain when we have some experience, and we act like ducks do when they are in a comparable brain state, we should guess that ducks are having a similar experience. (I know we&#39;re talking about mammals here, but I couldn&#39;t resist the “looks like a duck, talks like a duck joke.”)</p><p> If a pig has a brain state that resembles ours when we are happy, tries to get things that make it happy, and produces the same neurological responses that we do when we&#39;re happy, we should infer that pigs are not mindless automatons, but are, in fact, happy.</p><p> They then note that animals like drugs. Animals, like us, get addicted to opioids and have similar brain responses when they&#39;re on opioids. As the authors note “Indeed, one can predict drugs that will be addictive in humans quite effectively from animal studies of desire.” If animals like the drugs that make us happy and react in similar ways to us, that gives us good reason to think that they are, in fact, happy.</p><p> They then note that the parts of the brain responsible for various human emotions are quite ancient—predating humans—and that mammals have them too. So, if the things that cause emotions are also present in animals, we should guess they&#39;re conscious, especially when their behavior is perfectly consistent with being conscious. In fact, by running electricity through certain brain regions that animals share, we can induce conscious states in people—that shows that it is those brain states that are causing the various mental states.</p><p> The authors then run through various other mental states and show that those mental states are similar between humans and animals—animals have similar brain regions which provoke similar physical responses, and we know that in humans, those brain regions cause specific mental states.</p><p> Now, maybe there&#39;s some magic of the human brain, such that in animal brains, the brain regions that cause qualia instead cause causally identical stuff but no consciousness. But there&#39;s no good evidence for that, and plenty against. You should not posit special features of certain physical systems, for no reason.</p><p> Moving on to the <a href="https://www.sciencedirect.com/science/article/pii/S1053810004000893?casa_token=B3K3KvAUWb8AAAAA:-xmnYJuqoi_dlGijNwS9xdmnpq9jv1JtsBgGq7FOOXgEemDJdavp5naJVSntJwzIYjpyX_8V">Seth, Baars, and Edelman</a> paper, they note that there are various features of consciousness, that differentiate conscious states from other things happening in the brain that don&#39;t induce conscious states. They note:</p><blockquote><p> Consciousness involves widespread, relatively fast, low-amplitude interactions in the thalamocortical core of the brain, driven by current tasks and conditions. Unconscious states are markedly different and much less responsive to sensory input or motor plans.</p></blockquote><p> In other words, there are common patterns among conscious states. We can look at a human brain and see that the things that are associated with consciousness produce different neurological markers from the things that aren&#39;t associated with consciousness. Features associated with consciousness include:</p><p> Irregular, low-amplitude brain activity: When we&#39;re awake we have irregular low-amplitude brain activity. When we&#39;re not conscious—eg in deep comas or anesthesia-induced unconsciousness—irregular, low-amplitude brain activity isn&#39;t present. Mammal brains possess irregular, low-amplitude brain activity.</p><p> Involvement of the thalamocortical system: When you damage the thalamocortical system, that deletes part of one&#39;s consciousness, unlike other systems. Mammals also have a thalamocortical system—just like us.</p><p> Widespread brain activity: Consciousness induces widespread brain activity. We don&#39;t have that when things induce us not to be conscious, like being in a coma. Mammals do.</p><p> The authors note, from these three facts:</p><blockquote><p> Together, these first three properties indicate that consciousness involves widespread, relatively fast, low-amplitude interactions in the thalamocortical core of the brain, driven by current tasks and conditions. Unconscious states are markedly different and much less responsive to sensory input or endogenous activity. These properties are directly testable and constitute necessary criteria for consciousness in humans. It is striking that these basic features are conserved among mammals, at least for sensory processes. The developed thalamocortical system that underlies human consciousness first arose with early mammals or mammal-like reptiles, more than 100 million years ago.</p></blockquote><p> More evidence from neuroscience for animal consciousness:</p><p> Something else about metastability that I don&#39;t really understand is also present in humans and animals.</p><p> Consciousness involves binding—bringing lots of different inputs together. In your consciousness, you can see the entire world at once, while thinking about things at the same time. Lots of different types of information are processed simultaneously, in the same way. Some explanations involving neural synchronicity have received some empirical support—and animals also have neural synchronicity, so they would also have the same kind of binding.</p><p> We attribute conscious experiences as happening to us. But mammals have a similar sense of self. Mammals, like us, process information relative to themselves—so they see a wall and process it relative to them in space.</p><p> Consciousness facilitates learning. Humans learn from conscious experiences. In contrast, we do not learn from things that do not impinge on our consciousness. If someone slaps me whenever I scratch my nose (someone does actually—crazy story), I learn not to scratch my nose. In contrast, if someone does a thing that I don&#39;t consciously perceive when I scratch my nose, I won&#39;t learn from it. But animals seem to learn to, and update in response to stimulus, just like humans do—but only when humans are exposed to things that affect their consciousness. In fact, <a href="https://benthams.substack.com/p/underwater-torture-chambers">even fish learn</a> .</p><p> So there&#39;s a veritable wealth of evidence that at least mammals are conscious. The evidence is less strong for organisms that are less intelligent and more distant from us evolutionarily, but it remains relatively strong for at least many fish. Overturning this abundance of evidence, that&#39;s been enough to convince the substantial majority of consciousness researchers requires a lot of evidence. Does Yudkowsky have it?</p><h2> Yudkowsky&#39;s view is crazy, and is decisively refuted over and over again</h2><p> No. No he does not. In fact, as far as I can tell, throughout the entire protracted Facebook exchange, he never adduced a single piece of evidence for his conclusion. The closest that he provides to an argument is the following:</p><blockquote><p> I consider myself a specialist on reflectivity and on the dissolution of certain types of confusion. I have no compunction about disagreeing with other alleged specialists on authority; any reasonable disagreement on the details will be evaluated as an object-level argument. From my perspective, I&#39;m not seeing any, “No, <i>this</i> is a non-mysterious theory of qualia that says pigs are sentient…” and a lot of “How do <i>you</i> know it doesn&#39;t…?” to which the only answer I can give is, “I may not be certain, but I&#39;m not going to update my remaining ignorance on your claim to be even more ignorant, because you haven&#39;t yet named a new possibility I haven&#39;t considered, nor pointed out what I consider to be a new problem with the best interim theory, so you&#39;re not giving me a new reason to further spread probability density.”</p></blockquote><p> What??? The suggestion seems to be that there is no other good theory of consciousness that implies that animals are conscious. To which I&#39;d reply:</p><p> We don&#39;t have any good theory about consciousness yet—the data is just too underdetermined. Just as you can know that apples fall when you drop them before you have a comprehensive theory of gravity, so too can you know some things about consciousness, even absent a comprehensive theory.</p><p> There are various theories that predict that animals are conscious. For example, <a href="https://www.nature.com/articles/nrn.2016.44">integrated information theory</a> , <a href="https://philarchive.org/archive/MCFTCF-3v1">McFadden&#39;s CEMI field theory</a> , various Higher-Order theories, and the global workspace model will probably imply that animals are conscious. Eliezer has no argument to prefer his view to others.</p><p> Take the integrated information theory, for example. I don&#39;t think it&#39;s a great view. But at least it has something going for it. It has made a series of accurate predictions about the neural correlates of consciousness. Same with McFadden&#39;s theory. It seems Yudkowsky&#39;s theory has literally nothing going for it, beyond it sounding to Eliezer like a good solution. There is no empirical evidence for it, and, as we&#39;ll see, it produces crazy, implausible implications. David Pearce has a nice comment about some of those implications:</p><blockquote><p> Some errors are potentially ethically catastrophic. This is one of them. Many of our most intensely conscious experiences occur when meta-cognition or reflective self-awareness fails. Thus in orgasm, for instance, much of the neocortex effectively shuts down. Or compare a mounting sense of panic. As an intense feeling of panic becomes uncontrollable, are we to theorise that the experience somehow ceases to be unpleasant as the capacity for reflective self-awareness is lost? “Blind” panic induced by eg a sense of suffocation, or fleeing a fire in a crowded cinema (etc), is one of the most unpleasant experiences anyone can undergo, regardless or race or species. Also, compare microelectrode neural studies of awake subjects probing different brain regions; stimulating various regions of the “primitive” limbic system elicits the most intense experiences. And compare dreams – not least, nightmares – many of which are emotionally intense and characterised precisely by the lack of reflectivity or critical meta-cognitive capacity that we enjoy in waking life.</p></blockquote><p> Yudkowsky&#39;s theory of consciousness would predict that during especially intense experiences, where we&#39;re not reflecting, we&#39;re either not conscious or less conscious. So when people orgasm, they&#39;re not conscious. That&#39;s very implausible. Or, when a person is in unbelievable panic, on this view, they become non-conscious or less conscious. Pearce further notes:</p><blockquote><p> Children with autism have profound deficits of self-modelling as well as social cognition compared to neurotypical folk. So are profoundly autistic humans less intensely conscious than hyper-social people? In extreme cases, do the severely autistic lack consciousness&#39; altogether, as Eliezer&#39;s conjecture would suggest? Perhaps compare the accumulating evidence for Henry Markram&#39;s “ <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2518049/"><strong>Intense World</strong></a> ” theory of autism.</p></blockquote><p> Francisco Boni Neto furthers:</p><blockquote><p> many of our most intensely conscious experiences occur when meta-cognition or reflective self-awareness fails. Super vivid, hyper conscious experiences, phenomenic rich and deep experiences like lucid dreaming and &#39;out-of-body&#39; experiences happens when higher structures responsible for top-bottom processing are suppressed. They lack a realistic conviction, specially when you wake up, but they do feel intense and raw along the pain-pleasure axis.</p></blockquote><p> Eliezer just bites the bullet:</p><blockquote><p> I&#39;m not totally sure people in sufficiently unreflective flow-like states are conscious, and I give serious consideration to the proposition that I am reflective enough for consciousness only during the moments I happen to wonder whether I am conscious. This is not where most of my probability mass lies, but it&#39;s on the table.</p></blockquote><p> So when confronted with tons of neurological evidence that shutting down higher processing results in more intense conscious experiences, Eliezer just says that when we think that we have more intense experiences, we&#39;re actually zombies or something? That&#39;s totally crazy. It&#39;s sufficiently crazy that I think I might be misunderstanding him. When you find out that your view says that people are barely conscious or non-conscious when they orgasm or that some very autistic people aren&#39;t conscious, it makes sense to give up the damn theory!</p><p> And this isn&#39;t the only bullet Eliezer bites. He admits, “It would not surprise me very much to learn that average children develop inner listeners at age six.” I have memories from before age 6—these memories would have to have been before I was conscious, on this view.</p><p> Rob Wiblin makes a good point:</p><blockquote><p> [Eliezer], it&#39;s possible that what you are referring to as an &#39;inner listener&#39; is necessary for subjective experience, and that this happened to be added by evolution just before the human line. It&#39;s also possible that consciousness is primitive and everything is conscious to some extent. But why have the prior that almost all non-human animals are not conscious and lack those parts until someone brings you evidence to the contrary (ie “What I <i>need</i> to hear to be persuaded is,”)? That just cannot be rational.</p><p> You should simply say that you are a) uncertain what causes consciousness, because really nobody knows yet, and b) you don&#39;t know if eg pigs have the things that are proposed as being necessary for consciousness, because you haven&#39;t really looked into it.</p></blockquote><p> I agree with Rob. We should be pretty uncertain. My credences are maybe the following:</p><p> 92% that at least almost all mammals are conscious.</p><p> 80% that almost all reptiles are conscious.</p><p> 60% that fish are mostly conscious.</p><p> 30% that insects are conscious.</p><p> It&#39;s about as likely that reptiles aren&#39;t conscious as insects are. Because consciousness is private—you only know your own—we shouldn&#39;t be very confident about any features of consciousness.</p><p> Based on these considerations, I conclude that Eliezer&#39;s view is legitimately crazy. There is, quite literally, no good reason to believe it, and lots of evidence against it. Eliezer just dismisses that evidence, for no good reason, bites a million bullets, and acts like that&#39;s the obvious solution.</p><h2> Absurd overconfidence</h2><p> The thing that was most infuriating about this exchange was Eliezer&#39;s insistence that those who disagreed with him were stupid, combined with his demonstration that he had no idea what he was talking about. Condescension and error make an unfortunate combination. He says of the position that pigs, for instance, aren&#39;t conscious:</p><blockquote><p> It also seems to me that this is not all that inaccessible to a reasonable third party, though the sort of person who maintains some doubt about physicalism, or the sort of philosophers who think it&#39;s still respectable academic debate rather than sheer foolishness to argue about the A-Theory vs. <a href="https://en.wikipedia.org/wiki/B-theory_of_time"><strong>B-Theory of time</strong></a> , or the sort of person who can&#39;t follow the argument for why all our remaining uncertainty should be within different many-worlds interpretations rather than slopping over outside, will not be able to access it.</p></blockquote><p> Count me in as a person who can&#39;t follow any arguments about quantum physics, much less the arguments for why we should be almost certain of many worlds. But seriously, physicalism? We should have no doubt about physicalism? As I&#39;ve <a href="https://benthams.substack.com/p/against-dogmatic-physicalism">argued before</a> , the case against physicalism is formidable. Eliezer thinks it&#39;s an open-and-shut case, but that&#39;s because he is demonstrably mistaken about the zombie argument against physicalism and the implications of non-physicalism. In the literal second paragraph of <a href="https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies">his article about zombies</a> , Eliezer says:</p><blockquote><p>进一步声称，如果僵尸是“可能的”（一个仍在进行战斗的术语），那么，纯粹从我们对这种“可能性”的了解，我们可以先验地推断出意识在某种意义上是超物理的。将在下面描述；这一立场的标准术语是“副现象主义”。</p></blockquote><p> No! No! No^10^64. He is confusing non-physicalism and epiphenomenalism. I am a non-physicalist non-epiphenomenalist. There are several other non-physicalist views. In fact, in the Facebook exchange, Eliezer says:</p><blockquote><p> Suppose I claimed to be able to access an epistemic state where (rather than being pretty damn sure that physicalism is true) I was pretty damn sure that P-zombies / epiphenomenalism was false.</p></blockquote><p> In a Facebook thread where Eliezer admonishes people for being too stupid to understand that physicalism is true, he demonstrates that he doesn&#39;t have a basic familiarity with the subject. The possibility of P-zombies is not the same thing as non-physicalism. And, as I&#39;ve shown before, Eliezer&#39;s reply to the zombie argument all hinges on that one crucial error.</p><p> I used to believe Eliezer&#39;s position about physicalism, after reading his piece on zombies. Then I made a friend (I had some before, just to be clear). He explained to me how the zombie argument really worked, rather than the distorted Yudkowsky version. After I learned that, I realized Eliezer&#39;s view fails completely.</p><p> And that&#39;s not the only thing Eliezer expresses insane overconfidence about. In response to his position that most animals other than humans aren&#39;t conscious, David Pearce points out that you shouldn&#39;t be very confident in positions that almost all experts disagree with you about, especially when you have a strong personal interest in their view being false. Eliezer replies:</p><blockquote><p> What do they think they know and how do they think they know it? If they&#39;re saying “Here is how we think an inner listener functions, here is how we identified the associated brain functions, and here is how we found it in animals and that showed that it carries out the same functions” I would be quite impressed. What I expect to see is, “We found this area lights up when humans are sad. Look, pigs have it too.” Emotions are just plain simpler than inner listeners. I&#39;d expect to see analogous brain areas in birds.</p></blockquote><p> When I read this, I almost fell out of my chair. Eliezer admits that he has not so much as read the arguments people give for widespread animal consciousness. He is basing his view on a guess of what they say, combined with an implausible physical theory for which he has no evidence. This would be like coming to the conclusion that the earth is 6,000 years old, despite near-ubiquitous expert disagreement, providing no evidence for the view, and then admitting that you haven&#39;t even read the arguments that experts give in the field against your position. This is the gravest of epistemic sins.</p><h1>结论</h1><p>This has not been anywhere near exhaustive. I haven&#39;t even started talking about Eliezer&#39;s very implausible views about <a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/SbdCX6A5AGyyfhdmh">morality</a> (though I might write about that too—stay tuned), reductionism, <a href="https://www.lesswrong.com/posts/vzLrQaGPa9DNCpuZz/against-modal-logics">modality</a> , or many other topics. Eliezer usually has a lot to say about topics, and it often takes many thousands of words to refute what he&#39;s saying.</p><p> I hope this article has shown that Eliezer frequently expresses near certainty on topics that he has a basic ignorance about, an ignorance so profound that he should suspend judgment. Then, infuriatingly, he acts like those who disagree with his errors are morons. He acts like he is a better decision theorist than the professional decision theorists, a better physicist than the physicists, a better animal consciousness researcher than the animal consciousness researchers, and a much better philosopher of mind than the leading philosophers of mind.</p><p> My goal in this is not to cause people to stop reading Eliezer. It&#39;s instead to encourage people to refrain from forming views on things he says just from reading him. It&#39;s to encourage people to take his views with many grains of salt. If you&#39;re reading something by Eliezer and it seems too obvious, on a controversial issue, there&#39;s a decent chance you are being duped.</p><p> I feel like there are two types of thinkers, the first we might call innovators and the second systematizers. Innovators are the kinds of people who think of wacky, out-of-the-box ideas, but are less likely to be right. They enrich the state of discourse by being clever, creative, and coming up with new ideas, rather than being right about everything. A paradigm example is Robin Hanson—no one feels comfortable just deferring to Robin Hanson across the board, but Robin Hanson has some of the most ingenious ideas.</p><p> Systematizers, in contrast, are the kinds of people who reliably generate true beliefs on lots of topics. A good example is Scott Alexander. I didn&#39;t research Ivermectin, but I feel confident that Scott&#39;s post on Ivermectin is at least mostly right.</p><p> I think people think of Eliezer as a systematizer. And this is a mistake, because he just makes too many errors. He&#39;s too confident about things he&#39;s totally ignorant about. But he&#39;s still a great innovator. He has lots of interesting, clever ideas that are worth hearing out. In general, however, the fact that Eliezer believes something is not especially probative. Eliezer&#39;s skill lies in good writing and ingenious argumentation, not forming true beliefs.</p><br/><br/> <a href="https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously<guid ispermalink="false"> TjyyngWFYvQWPpNNj</guid><dc:creator><![CDATA[omnizoid]]></dc:creator><pubDate> Sun, 27 Aug 2023 01:06:40 GMT</pubDate> </item><item><title><![CDATA[Mesa-Optimization: Explain it like I'm 10 Edition]]></title><description><![CDATA[Published on August 26, 2023 11:04 PM GMT<br/><br/><p>对于台面优化有<a href="https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care">几种</a>可用<a href="https://ui.stampy.ai?state=8160_">的</a><a href="https://www.lesswrong.com/tag/mesa-optimization">解释</a>。我认为<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=247s">Rob Miles 关于该主题的视频</a>非常棒，但我认为现有的书面描述并没有使这个概念简单到足以被广大观众彻底理解。对于那些更喜欢书面内容而不是视频的人来说，这是我的尝试。</p><h2>概括</h2><p>台面优化是人工智能对齐中的一个重要概念。有时，一个优化器（如梯度下降或进化）会产生另一个优化器（如复杂的人工智能或人类）。当这种情况发生时，第二个优化器被称为“mesa-optimizer”；台面优化器的对准（安全）问题称为“内部对准问题”。</p><h2>什么是优化器？</h2><p>我将优化器定义为一种查看可能事物的“空间”并以“选择”其中一些事物的方式运行的事物。可能有优化器在工作的一个迹象是<i>奇怪的事情正在发生，</i>我的意思是“如果系统随机行为，则不太可能发生的事情”。例如，人类一直在做一些事情，如果我们的行为是随机的，那么这些事情就不太可能发生，如果进化只是通过完全随机的基因创造新的生物体来实现，那么人类肯定根本不存在。</p><h3>人类的大脑</h3><p>人脑是一个优化器——它会审视你可以做的不同事情，并选择那些能让你做你喜欢的事情。</p><p>例如，您可能会去商店买冰淇淋，支付冰淇淋费用，然后再回来。如果你仔细想想，这是一系列非常复杂的动作——如果你的行为完全随机，你永远不会得到冰淇淋。你的大脑已经搜索了许多你可以采取的行动（去其他地方散步、在客厅跳舞、只将左脚向上移动 30 度），并预计这些行动都不会给你带来冰淇淋，然后选择了一个能够让你得到你想要的东西的极少数路径。</p><h3>进化</h3><p>优化器的一个奇怪的例子是进化。您可能已经听说过这一点——进化“<i>优化了包容性遗传适应性</i>”。但是，这是什么意思？</p><p>生物体的变化是随机的——它们的基因由于制造新生物体过程中的错误而发生变化。有时，这种变化可以通过延长有机体的生存时间、使其更具吸引力等来帮助有机体繁殖。当这种情况发生时，下一代就会有更多的这种变化。随着时间的推移，许多这些变化积累起来，形成了非常复杂的系统，如植物、动物和真菌。</p><p>因为最终会有更多的生物体繁殖更多，而那些发生变化导致繁殖能力降低的生物体（比如遗传疾病）会越来越少，所以进化是从所有发生的突变的空间中进行选择的——如果你用一个完全随机的基因组，它肯定会立即死亡（或者更确切地说，一开始就没有活着）。</p><h3>人工智能培训</h3><p>当人工智能被训练时，通常是通过“梯度下降”来完成的。什么是梯度下降？</p><p>首先，我们定义一些表示人工智能表现如何的东西（“损失函数”）。这可能非常简单（每次输出“狗”时 1 分）或非常复杂（每次您说出一个对我输入的内容有意义的英语句子时 1 分）。</p><p>然后，我们制作一个随机人工智能——基本上只是一组相互连接的随机数。可以预见的是，这个人工智能的表现非常糟糕——它输出“dska£hg@tb5gba-0aaa”或类似的内容。我们运行它很多次，看看它何时接近输出“dog”（例如，以 ad 作为第一个字母，或输出接近 3 个字符）。然后，我们使用数学算法来找出哪些随机数使人工智能接近我们想要的值，哪些与正确的数字相距甚远——然后我们将它们稍微向正确的方向移动。然后我们多次重复这个过程，直到最终人工智能每次都一致输出“dog” <span class="footnote-reference" role="doc-noteref" id="fnref20xlfniecdi"><sup><a href="#fn20xlfniecdi">[1]</a></sup></span> 。</p><p>这个过程很奇怪，但弄清楚数字应该变化的<i>方向</i>比从一开始就弄清楚正确的数字要容易得多，特别是对于更复杂的任务。</p><p>再次，我们可以看到这个过程是一个优化器——随机人工智能没有做任何有趣的事情，但是通过将数字推向正确的方向，我们可以完成非常复杂的任务，而这些任务永远不会随机发生。之所以发生这种情况，是因为我们观察了一个非常大的可能的人工智能“空间”（其中有不同的数字），并通过它“移动”到一个能做我们想要的事情的人工智能。</p><h2>什么是<i>Mesa</i>优化器？</h2><p>为了理解<i>台面</i>优化，我们将回到进化类比。我们看到了优化器的两个例子——进化和人脑。其中之一创造了另一个！这是台面优化的关键。</p><p>当我们训练人工智能时，就像进化一样，我们可能会发现人工智能本身就是优化器，即它们会检查可能的动作空间，并采取那些给它们带来好分数的动作，类似于人脑。在人工智能中，第二个优化器被称为<i>台面优化器。</i> Mesa-optimizer 指的是<i>我们自己训练的 AI</i> ，而外部优化器是我们用来训练该 AI 的过程（梯度下降）。请注意，某些人工智能（尤其是简单的人工智能）可能不算作台面优化器，因为它们没有表现出足够复杂的行为，不足以本身成为优化器。 </p><figure class="image"><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_640 640w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1280 1280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1516 1516w"><figcaption>图片无耻地“借用”自<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=426s">Rob Miles 关于 mes&#39;a-optimizers 的视频</a></figcaption></figure><h2>为什么这是个问题？</h2><p>如果我们有两个优化器，那么现在要让 AI 表现良好就会遇到两个问题（两个“对齐问题”）。具体来说，我们有一个“外部”和“内部”对齐问题，分别与梯度下降和台面优化器相关。</p><p>外对齐问题是AI对齐的经典问题。当您创建优化器时，很难确保您告诉它做的事情是您真正<i>希望</i>它做的事情。例如，你如何告诉计算机“根据我所写的内容合理地写出连贯的英语句子”？将复杂任务正式定义为损失函数可能非常棘手。这对于某些任务来说可能很危险，但对于本文来说讨论这个问题会花费太长时间。</p><p><i>内部</i>对齐问题有点棘手。我们可能会根据自己的需要完美地定义损失函数，但 AI 的行为仍然很危险，因为它<i>与外部优化器</i>不一致。例如，如果梯度下降找到一种算法，可以优化“在训练时假装表现良好以获得好分数，这样我以后就可以做我真正想做的事情”（“<a href="https://www.lesswrong.com/tag/deceptive-alignment">欺骗性对齐</a>”），这将得到我们的训练数据集得分很高，但仍然很危险。</p><h2>内部错位的示例</h2><h3>人类</h3><p>对于我们的第一个例子，我们将再次回到我们的进化类比。进化对非常擅长生存和繁殖的事物的探索最终产生了大脑——一种台面优化器。现在，人类与进化变得不相符<span class="footnote-reference" role="doc-noteref" id="fnrefqu1bh02smsf"><sup><a href="#fnqu1bh02smsf">[2]</a></sup></span> 。</p><p>因为进化是一种相当奇怪的搜索类型（如梯度下降），所以它无法将“繁殖”的概念直接放入我们的大脑中。相反，开发了一系列与生殖器摩擦有关的更简单的概念，与人际关系有关的复杂事物等等。</p><p>现在，人类做的事情非常有效地满足了那些更简单的概念，但对包容性遗传健康来说却很糟糕，比如手淫、看色情片、不向精子库捐赠。这表明进化存在内部错位问题。</p><h3>进化至灭绝</h3><p>但人类还是很漂亮的<i> </i>与进化完全一致——与其他猿类相比，我们的种群规模很大。为了说明进化的内在失调有多么严重，让我们看看<a href="https://en.wikipedia.org/wiki/Irish_elk#Extinction">爱尔兰麋鹿</a>。爱尔兰麋鹿（可能）进化到灭绝。但这不是与进化的原理相反吗？这是怎么发生的？ </p><figure class="image image_resized" style="width:53.06%"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/220px-Irish_Elk_front.jpg" alt="博物馆收藏的带有大鹿角的爱尔兰麋鹿头骨标本的照片" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/330px-Irish_Elk_front.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/440px-Irish_Elk_front.jpg 2x"><figcaption>爱尔兰麋鹿，拥有标志性的巨大鹿角</figcaption></figure><p><a href="https://en.wikipedia.org/wiki/Sexual_selection">性选择</a>是指动物进化为对配偶有吸引力，而不是为了更好地生存。这对于进化来说是有好处的——诚实地向潜在的伴侣展示你很强壮、速度很快、吃得很好等等，这可能是确保那些可能生存下来的后代繁殖更多的好方法。例如，长出大鹿角可能表明你吃饱了并且能够为这些鹿角提供充足的营养，或者你能够很好地战斗来保护自己。</p><p>然而，性选择也可能出错。在某种程度上，拥有大鹿角是件好事。一旦它们变得太大，你可能无法很好地移动头部，被挂在树上，并浪费大量关键资源。但如果雌性喜欢大鹿角，那么拥有大鹿角的雄性就会大量繁殖，即使只有少数能够存活到成年。很快，所有雄性都长出了巨大的鹿角，为生存而苦苦挣扎。即使这不会直接导致灭绝，但如果种群数量减少，也会造成灭绝。这表明进化存在<i>严重的</i>内部对齐问题。</p><h3>招聘高管</h3><figure class="image image_resized" style="width:505.5px"><img src="https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fimages.huffingtonpost.com%2F2015-10-27-1445916079-7527424-Dollarphotoclub_67902901.jpg&amp;f=1&amp;nofb=1&amp;ipt=5035d0cd00af6f1499e263734b1a32b232b07af983be635ee9a2e9f0c4fe288f&amp;ipo=images" alt="成功的非洲裔美国女性在商界所做的三件事与众不同......"><figcaption> <a href=" http://images.huffingtonpost.com/2015-10-27-1445916079-7527424-Dollarphotoclub_67902901.jpg">一个商人</a>，因为我怀疑你们现在都已经入迷了</figcaption></figure><p>在招聘高管时，公司股东面临两个问题。</p><p>第一个问题是外部一致性问题：他们如何设计招聘流程和激励计划，以便他们雇用的高管有动力按照公司的最佳利益行事？这个问题看起来真的很难——他们如何阻止高管为了公司的短期利益而行事以获取奖金，并以对公司造成长期损害的代价（当他们很可能跳槽到另一家公司时）表现得很好。做同样的事情<span class="footnote-reference" role="doc-noteref" id="fnrefvkgqwj5lsse"><sup><a href="#fnvkgqwj5lsse">[3]</a></sup></span> ）？</p><p>这里的内部错位问题来自这样一个事实，即被雇用的人本身就是优化者，并且可能与股东的目标截然不同。例如，如果有潜在的高管想要损害公司<span class="footnote-reference" role="doc-noteref" id="fnrefl64dyogiew"><sup><a href="#fnl64dyogiew">[4]</a></sup></span> ，他们就会有强烈的动机在招聘过程中表现出色。他们甚至可能会被激励在指标和激励计划上表现出色，以便继续留在公司并继续对公司造成微妙的损害，或者将公司的重点转移到或远离某些领域，同时以困难的方式让股东损失金钱。测量。</p><h2>结论</h2><p>我想指出的是，<a href="https://www.lesswrong.com/tag/inner-alignment">外部错位和内部错位之间的界限相当模糊</a>，经验丰富的研究人员有时很难区分它们。此外，您可能会同时出现内部和外部错位。</p><p>希望这可以帮助您更彻底地理解内在的错位。如果您对我的写作有疑问或反馈，请在评论中指出。</p><p>一些问题来检查您是否理解：</p><ul><li>台面优化器的示例是什么？它与其他类型的优化器有何不同？</li><li>什么是“外部对齐问题”？</li><li>什么是“欺骗性对齐”？ </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn20xlfniecdi"> <span class="footnote-back-link"><sup><strong><a href="#fnref20xlfniecdi">^</a></strong></sup></span><div class="footnote-content"><p>这个过程称为<i>梯度下降，</i>因为可视化这一过程的一种方法是使用图表，其中损失函数是上下轴，每个数字是另一个轴。我们调整数字以最快地在损失函数轴上“下降”。对于很多数字，这非常奇怪，但如果我们有一个非常简单的“AI”，只有两个数字，它可能看起来像这样： <img style="width:599.404px" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fproxy%2F1*f9a162GhpMbiTVTAua_lLQ.png&amp;f=1&amp;nofb=1&amp;ipt=ea75275746c0f883e5e18e606181f66040c7e7f8a4e297a3fe7e3d60f4a00b80&amp;ipo=images" alt="梯度下降：您需要知道的一切 |作者：Suryansh S. |黑客中午..."></p><p> “路径”梯度下降采用黑色（从红色部分到蓝色部分）。</p><p></p></div></li><li class="footnote-item" role="doc-endnote" id="fnqu1bh02smsf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqu1bh02smsf">^</a></strong></sup></span><div class="footnote-content"><p>如果您实际上 10 岁，您可能希望跳过下一段。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvkgqwj5lsse"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvkgqwj5lsse">^</a></strong></sup></span><div class="footnote-content"><p>这似乎可以分为外对齐<i>或</i>内对齐。这并不罕见，但令人困惑。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnl64dyogiew"> <span class="footnote-back-link"><sup><strong><a href="#fnrefl64dyogiew">^</a></strong></sup></span><div class="footnote-content"><p>例如，他们是在一家石油公司工作的秘密气候变化活动家</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fLAvmWHmpJEiw8KEp/mesa-optimization-explain-it-like-i-m-10-edition#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fLAvmWHmpJEiw8KEp/mesa-optimization-explain-it-like-im-10-edition<guid ispermalink="false"> fLAvmWHmpJEiw8KEp</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Sat, 26 Aug 2023 23:04:21 GMT</pubDate> </item><item><title><![CDATA[Aumann-agreement is common]]></title><description><![CDATA[Published on August 26, 2023 8:22 PM GMT<br/><br/><p><i>感谢 Justis Mills 的校对和反馈。这篇文章也可以在</i><a href="https://tailcalled.substack.com/p/aumann-agreement-is-common"><i>我的子堆栈</i></a>上找到<i>。</i></p><p><a href="https://www.lesswrong.com/tag/aumann-s-agreement-theorem">奥曼一致性定理</a>是一个定理家族，它说如果人们彼此信任并且了解彼此的观点，那么他们就会彼此同意。或者换句话说，如果人们彼此保持信任，那么他们就能达成协议。 （该定理的一些变体考虑了计算因素，表明它们可以相当快地做到这一点。）</p><p>最初的证明非常正式且令人困惑，但一个更简单的启发式论证是，对于一个诚实、理性的代理人来说，<a href="https://www.lesswrong.com/rationality/what-is-evidence">他们表达意见的事实</a>就可以成为另一个理性代理人的<a href="https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common">有力证据</a>，因为如果说话者的概率高于说话者的概率在此之前，那么他们一定已经看到了相应的证据来证明该观点的合理性。</p><p>有些人觉得这令人困惑，并觉得这一定是错误的，因为它不适用于大多数分歧。我认为这些人是错误的，因为他们对分歧的看法不够广泛。奥曼一致定理适用的分歧概念是指人们为事件分配不同的概率；这是一个非常包容的概念，涵盖了许多我们通常不认为是分歧的事情，包括一方拥有有关某个主题的信息而另一方没有信息的情况。</p><h2>我在挪威的假期很大程度上依赖于奥曼协议</h2><p>最近，我和妻子在挪威度假。</p><p>为了到达那里并四处走动，我们需要交通工具。起初我们不同意那里提供交通的人，因为我们不知道很多具体的交通方式，只是模糊地知道会有一些飞机和轮船，但不知道是哪些。但我的妻子听说有一种叫做“奥斯陆渡轮”的东西，所以我们奥曼同意这是一个选择，并决定进一步调查。</p><p>我们不同意提供奥斯陆渡轮的公司，因为我们不知道他们的网站是什么，所以我们询问了谷歌，它提供了一些关于渡轮可能是什么的选项，我们奥曼同意谷歌的意见，然后去调查从那里。我们发现一个网站声称出售渡轮票；起初，我们不同意该网站关于我们何时可以旅行的说法，因为我们不知道渡轮的时间，但后来我们阅读了它声称的可用时间，然后奥曼更新了这一点。</p><p>我们还必须在挪威找到一些事情可做。对我们来说幸运的是，OpenAI 的一些人注意到每个人都对互联网存在巨大分歧，因为没有人真正记住了互联网，他们认为通过解决这种分歧可以获得一些价值，所以他们通过填充来同意互联网。它进入一个名为 ChatGPT 的神经网络。起初，ChatGPT 不同意我们去挪威参观什么，并提出了一些我们并不真正感兴趣的事情，但我们告知了它我们的兴趣，然后它很快就与我们达成了一致，并提出了一些其他更有趣的事情。</p><p>我们参观的其中一处是一个为一位冒险家建造的木筏并在海洋中航行的博物馆。在参观博物馆之前，我们对此有很多分歧，例如我们不知道木筏上的一个人落入海中并且必须获救。但博物馆告诉我们事实确实如此，所以我们奥曼同意相信它。想必博物馆是通过​​奥曼得知这件事的——与木筏上的人达成一致。</p><p>错误的奥曼协议的一个例子是与火车公司 Vy 签订的协议。他们说可以给我们买一张卑尔根火车的火车票，我们得到了奥曼的同意。然而，由于暴风雨，他们的火车轨道被破坏了，公司网站直到最后一刻才承诺火车上的可用性，所以我们没有得到 Vy 的纠正。</p><p>但我们并没有通过凭经验看到损坏的轨道或通过理性推理认为它不可用而得救。相反，我们得救了，因为我们告诉某人我们乘坐卑尔根火车的计划，希望他们同意我们乘坐火车的信念，但他们却一直不同意，并告诉我们火车被洪水淹没了，将会被淹没。取消。这让我们奥曼同意我们必须找到其他方法，我们询问谷歌是否有任何航班，其中建议了一些我们奥曼同意的航班。</p><p>后来，我把这次旅行的事告诉了我爸爸，现在也告诉了你。在谈论这件事之前，我预计你会不同意，因为你对此一无所知，但至少我很确定我的父亲奥曼同意我告诉他的事情，而且我怀疑你也这样做了。</p><h2>奥曼式的分歧很快就消失了，因此“分歧”意味着/表示非奥曼式的分歧</h2><p>我的故事中提到的分歧都发生在具有合理信任程度的各方之间，而且大多涉及一方缺乏信息而另一方有信息，因此通过转移信息很快就得到了解决。即使注意到分歧的细节也足以传递信息并解决它。</p><p>与此同时，在政治中，目标相互冲突的人们之间经常会出现分歧，他们有理由怀疑一方歪曲事实，因为他们更关心获得权力，而不是准确地告知与他们交谈的人。</p><p>因为当你怀疑对方有偏见时，奥曼协议的前提条件就不成立，所以这种分歧不会那么快得到解决，而是长期存在。但如果我们根据长期存在的分歧来形成对分歧的看法，那么这意味着我们正在过滤掉奥曼条件所适用的分歧。</p><p>因此，“分歧”意味着（甚至可能表示）“<i>彼此不信任的人之间</i>意见分歧”，而不仅仅是“意见分歧”。</p><h2>大多数奥曼人的分歧只是因为缺乏意识</h2><p>贝叶斯范式并没有<i>从根本上</i>区分<span class="footnote-reference" role="doc-noteref" id="fnrefy5hm5xwtgb"><sup><a href="#fny5hm5xwtgb">[1]</a></sup></span>由于没有关于某个命题的信息而对某个命题的怀疑与由于观察到矛盾的信息而对命题的怀疑。例如，考虑随机挑选两个人并对他们做出诸如“Marv Elsher 正在与 Abrielle Levine 约会”之类的声明。你不知道这些人是谁，而且大多数人都没有互相约会，所以你应该理性地分配一个非常低的概率。</p><p>但这并不是因为你从矛盾的证据中积极地不相信它！事实上，你甚至可能不认为自己事先对此有信念。如果事实上有一个马夫·埃尔舍（Marv Elsher）正在与阿布里埃尔·莱文（Abrielle Levine）约会，那么马夫对这个说法的可能性非常高，而如果没有这篇文章，你甚至不会想到它。</p><p>如果您考虑人们为可符号表达的命题分配不同概率的所有情况，那么几乎所有情况都将遵循这些思路，因为有大量您根本无法访问的随机本地信息。因此，如果你想考虑奥曼一致性定理所指的分歧的典型案例，你应该认为“A 观察到了 X，而 B 甚至不知道 X 周围发生了什么，更不用说任何关于 X 的证据了”。 X 本身”。</p><h2>奥曼协议极其高效、强大</h2><p>大部分的更新都是在假期期间发生的，自己根本无法去核实。他们常常关注在空间和时间上都很遥远的事情。有时他们关心的是过去发生的事情，而这些事情甚至无法在物理上进行验证。但即使对于您可以验证的事情，也比仅进行 Aumann 更新需要更多<i>数量级的</i>时间和资源。</p><h2>奥曼协议是关于汇集，而不是节制</h2><p>在我的例子中，人们通常不会达成妥协的立场；相反，他们大量采用了对手方的头寸。对于奥曼协议来说，这通常是正确的想法。虽然更新的确切方式可能会根据先前的情况和证据而有所不同，但我喜欢的一个简单示例是：</p><p> You both start with having your log-odds being some vector x according to some shared prior (ie you start out agreeing). You then observe some evidence y, updating your log-odds to be x+y, while they observe some independent evidence z, updating their log-odds to be x+z. If you exchange all your information, then this updates your shared log-odds to be x+y+z, which is most likely going to be an even more radical departure from x than either x+y or x+z alone.</p><h2> Aumann conditions information on trust</h2><p> Surely sometimes it seems like Aumann agreement should cause people to moderate, right? Like in politics, if you have spent a lot of time absorbing one party&#39;s ideology, and your interlocutor has spent a lot of time absorbing the other party&#39;s ideology, but you then poke lots of holes in each other&#39;s arguments?</p><p> I think in this case, learning that there are holes in the arguments your learned from your party may be reason to doubt the trustworthiness of your party, especially when they cannot fix those holes. Since you Aumann-updated a great deal on your party&#39;s view specifically because you trusted them, this should also make you un-update away from their views, presumably moderating them.</p><p> (I think this has massive implications for collective epistemics, and I&#39;ve gradually been developing a theory of collective rationality based on this, but it&#39;s not finished yet and the purpose of this post is merely to grok the agreement theorem rather than to lay out that theory.)</p><p> There may also be less elaborate ways in which you might moderate due to Aumann agreement, eg if contradictory information cancels out.</p><h2> A lot of Aumann-updates are on promises, history or universals</h2><p> Many of the most obvious Aumann updates in my story were about promises; for instance that an interlocutor would provide me a certain transport at a certain time from one location to another.</p><p> One might think this suggests that promises have a unique link to Aumann&#39;s agreement theorem, but I think this is actually because promises are an unusually prevalent type of information due to the combo of:</p><ul><li> People&#39;s capacity to make reliable claims about them.</li><li> Being useful enough in practice to be worth sharing.</li><li> Covering a diverse and open-ended set of possibilities.</li></ul><p> For instance, if you promise me a sandwich in your kitchen, then you can ensure that your promise is true by paying rent to keep ownership of your kitchen, buying and storing ingredients for the sandwich so they are ready for assembly, and then assembling the sandwich for me when it is time.</p><p> Meanwhile, if you tell me that there is an available sandwich in someone else&#39;s kitchen, then because you don&#39;t maintain control over that kitchen, it might cease to be true once we actually reach the time when I need it, so you can&#39;t reliably make claims about it. Furthermore, even if you could, I would probably not get away with taking it, so it would not be useful to me.</p><p> You could probably reasonably reliably make claims about certain things you&#39;ve seen in the past, but most of those are not very useful because they happened in the past. For example, while I know from the museum that that guy on the raft fell in the water, I don&#39;t have anything to use it for. That said, sometimes (eg to attribute outcomes to causes, or to make generalizations), they are useful.</p><p> You can read a physics textbook and do a lot of useful Aumann updates on this, but this is mainly because physics is a &quot;universal&quot; subject, but this also means that it is a closed subject with a bounded amount of information. There can&#39;t be an &quot;alternate physics&quot; with alternate particles and strengths of attraction, unlike the same sense as there can be an &quot;alternate plane company&quot; with alternate flight times.</p><p> Promises, history and universals aren&#39;t meant to be a complete taxonomy, it&#39;s just something I&#39;ve noticed. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fny5hm5xwtgb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy5hm5xwtgb">^</a></strong></sup></span><div class="footnote-content"><p> It is distinguished through the history of updating from prior to posterior, but the distinction is not &quot;stored&quot; anywhere in the probability distribution, so the beliefs themselves are treated the same, even if their history are different.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/4S6zunFNFY3f5JYxt/aumann-agreement-is-common#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4S6zunFNFY3f5JYxt/aumann-agreement-is-common<guid ispermalink="false"> 4S6zunFNFY3f5JYxt</guid><dc:creator><![CDATA[tailcalled]]></dc:creator><pubDate> Sat, 26 Aug 2023 20:22:03 GMT</pubDate> </item><item><title><![CDATA[Rochester, New York, USA – ACX Meetups Everywhere Fall 2023]]></title><description><![CDATA[Published on August 26, 2023 5:58 PM GMT<br/><br/><p> This year&#39;s ACX Meetup everywhere in Rochester, New York, USA.</p><p> Location: Spot Coffee – <a href="https://plus.codes/https://plus.codes/87M45C42+H9">https://plus.codes/87M45C42+H9</a></p><p> Contact: jensfiederer@gmail.com</p><br/><br/> <a href="https://www.lesswrong.com/events/DtvBmtWcHGtjPbrHu/rochester-new-york-usa-acx-meetups-everywhere-fall-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/DtvBmtWcHGtjPbrHu/rochester-new-york-usa-acx-meetups-everywhere-fall-2023<guid ispermalink="false"> DtvBmtWcHGtjPbrHu</guid><dc:creator><![CDATA[Jens Fiederer]]></dc:creator><pubDate> Sun, 27 Aug 2023 00:44:28 GMT</pubDate> </item><item><title><![CDATA[Ramble on STUFF: intelligence, simulation, AI, doom, default mode, the usual]]></title><description><![CDATA[Published on August 26, 2023 3:49 PM GMT<br/><br/><p>克罗斯从新<a href="https://new-savanna.blogspot.com/">萨凡纳</a>发布。我在新稀树草原上发表了<a href="https://new-savanna.blogspot.com/search/label/Ramble">一些这样的闲谈</a>。它们是我思考我的想法的一种方式，由对各种主题的各种快速而粗略的评论组成： 认知状态：<i>我仍在思考它。</i></p><p> xxxx x</p><p>是时候再做一个了。我心里有很多想法。很难整理出来。所以，一些快速的点击只是为了让他们都进入同一个空间。</p><p><strong>智慧的话语</strong></p><p>据我所知，以“智能生命”或“人工智能”等短语实例化的智能概念相对较新，可以追溯到 19 世纪末和 20 世纪初，当时出现了评估一般认知能力的测试，即智力，如“智商”。这种观念在现代世界很普遍。它渗透到教育系统以及我们关于种族和公共政策（包括优生学）的讨论中。人工智能似乎是这个想法和计算、这个想法和它在数字计算机中的实现之间的交叉。</p><p>这种话语如何出现在小说中？我想到的是像夏洛克·福尔摩斯这样的角色，或者最近的阿德里安·蒙克？这些角色以聪明才智着称，但在其他方面都很奇怪，就蒙克而言，更是极其奇怪。博斯特罗姆的模拟假设对我来说似乎很薄弱</p><p>我想得越多，就越不清楚他所说的模拟到底是什么意思。他关注人类的经验。但动物呢？它们是人类经验的一部分。难道动物只是空壳，只是人类的表象而存在吗？但他们之间的互动又如何呢？是什么驱动他们的行动？它们是（准）自动代理，还是它们的行为完全服从于向人类呈现外观的机制？当动物或人类繁殖时会发生什么？我们有模拟 DNA、受精、胚胎发育吗？这似乎相当细粒度，并且会带来沉重的计算负担。但如果模拟没有做到这一点，那么后代的特征与其父母的特征有何关系？它使用什么机制？很多问题。</p><p><a href="https://new-savanna.blogspot.com/search/label/Bostrom-simulation">其他帖子关于模拟的说法</a>。</p><p><strong>人工智能考试的表现和能力</strong></p><p>https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/ 罗德尼·布鲁克斯 (Rodney Brooks) 在他正在进行的人工智能讨论中引入了能力和绩效之间的区别。在人类标准化测试中对大型语言模型进行基准测试是很常见的，例如大学先修课程（AP）测试或专业资格测试（例如律师资格考试）。这些是表现的表现。我们可以从这些表现中有效地推断出哪些潜在能力？特别是，它们是否与我们从这些测试中推​​断出的人类能力相似。鉴于人类和法学硕士在 AP 物理考试中都取得了一定水平，那么法学硕士在物理方面的潜在能力是否与人类相当？</p><p><strong>超智能</strong></p><p>这是一个模糊的概念，不是吗？在国际象棋领域，计算机相对于人类的优势已经有十五年了，但这并不完全是我们所说的超级智能（SI）的意思，不是吗？太专业了。当谈到 SI 时，我们指的是一般智力，不是吗？ GPT-4 和 Claude-2 等法学硕士拥有比任何人都要广泛得多的知识。超智能？在某种程度上“是”，但可能不是，不是真的。</p><p>杰弗里·辛顿（Geoffrey Hinton）设想了一种超越人类智力的智力，就像人类智力超过青蛙一样。这种比喻很常见，也很容易被人吐槽。但是这是什么意思？从某种意义上说，相对于 17 世纪或古希腊或古罗马受过良好教育的人来说，当代的哈佛大学毕业生是超级智力者。这位哈佛大学毕业生所理解的事情对于那些早期的人来说是难以理解的——马克·吐温将这种（某种）差异作为一本书的主题， <i>《亚瑟王法庭上的康涅狄格洋基队》</i> 。</p><p>请注意，无论青蛙如何被饲养和“教育”，它永远不会具有人类的智力。但是，如果你带着一个 17 世纪的婴儿穿越到当代世界，他可能会成为哈佛大学的毕业生。当谈论未来的计算超级智能时，我们谈论的是这些差异中的哪一个（青蛙与人类，17 世纪与现在）。也许两者都有？</p><p>我担心这种情况会一直持续下去。</p><p><strong>为什么《时代》杂志要刊登尤德科夫斯基的文章？</strong></p><p>当<i>《时代》杂志</i>三月底发表<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">埃利泽·尤德科夫斯基的文章</a>时，我感到震惊。为什么？在我看来，尤德科夫斯基对人工智能末日的设想主要是科幻小说中的产物，而不是关于人工智能未来的合理假设。我不认为<i>《时代》杂志</i>从事出版科幻小说的业务。</p><p>所以，我错了一些事情。正如贝叶斯主义者所说，我必须修改一些先验知识。但哪些是？如何？目前，我认为没有理由改变我对人工智能存在风险的看法。这意味着我对<i>《时代》杂志的</i>看法是错误的，我认为《时代》杂志是中庸观点的堡垒。</p><p>但实际上，人工智能灭绝<i>对观众来说到底意味着什么？</i>当然，人们担心失业，这种情况从那时起就一直存在。现在我们看到了法学硕士的疯狂行为。我的猜测是，人工智能末日是这些恐惧的延续和加剧。没什么特别的。</p><p><strong>到底为什么是厄运呢？ （寻找真实？）</strong></p><p>为什么人工智能的末日对于某些人，尤其是高科技领域的人来说是一个如此引人注目的想法？为他们解决什么问题？好吧，既然它是一个关于未来的想法，而且几乎是整个未来，那么我们假设这就是它解决的问题：如何思考未来？假设（神一样的）超级智能的出现，其力量如此强大，以至于几乎可以完全控制人类的生活。进一步假设，无论出于何种原因，它决定人类是可牺牲的。然而，如果——这是一个很大的假设——我们可以控制这种超级智能（“对齐”），这样我们就可以控制它，那么我们就可以控制未来。</p><p>问题解决了。</p><p>但为什么不假设我们能够控制这件事——就像 Yann LeCun 似乎假设的那样呢？因为这并没有给我们太多关于该做什么的指导。如果我们能够控制这个东西，无论它是什么，那么未来都是敞开的。一切皆有可能。这无法让我们集中精力。但如果它是来抓我们的，那就会给我们的活动带来相当大的关注。</p><p>我相信这个吗？我怎么会知道？我刚刚弥补了。</p><p><strong>虚构和默认模式</strong></p><p>法学硕士的一个众所周知的问题是他们倾向于胡编乱造。嗯，令我震惊的是，闲聊也是人类思维的默认模式。当你没有做任何特别的事情时，当你在做白日梦时，你的脑海里会闪过什么？其中一些转瞬即逝的想法可能扎根于现实，但许多不会，许多将是这样或那样的虚构。</p><p>神经科学家一直在研究该活动期间的大脑激活情况。他们已经确定了支持它的大脑结构的独特配置。这被称为<a href="https://new-savanna.blogspot.com/search/label/default%20mode">默认模式网络</a>。</p><p>嗯嗯....</p><p><strong>到底什么是现实？</strong></p><p>的确。越来越多的现实似乎变得相当不稳定。正如我和大卫·海斯在<a href="https://3quarksdaily.com/3quarksdaily/2023/08/children-in-search-of-the-dance-some-informal-ethnographic-analysis.html">《自然选择为何导致复杂性》一书中</a>所说，<i>现实不是被感知的，而是被制定出来的——在一个巨大的、也许是无限复杂的宇宙中。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/i4LjHb6enWiErXdx2/ramble-on-stuff-intelligence-simulation-ai-doom-default-mode#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/i4LjHb6enWiErXdx2/ramble-on-stuff-intelligence-simulation-ai-doom-default-mode<guid ispermalink="false"> i4LjHb6enWiErXdx2</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Sat, 26 Aug 2023 15:49:48 GMT</pubDate></item></channel></rss>