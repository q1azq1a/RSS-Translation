<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 25 日星期三 22:10:52 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p>人工智能曾经是一门科学。在过去（人工智能还不太有效的时候），人们试图发展一种有效的认知理论。</p><p>那些科学家没有成功，那些日子已经过去了。对于当今从事人工智能工作并将工作时间分配给不同任务的大多数人来说，理解思想的雄心已经不复存在。研究机械可解释性的人们（以及其他试图建立对现代人工智能的实证理解的人）正在奠定重要的基石，可以在未来的人工智能科学中发挥作用，但总的来说，现代人工智能工程只是构建巨大的神经元网络并用大量数据训练它们，而不是理解思想。</p><p>这一<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>惨痛的教训</u></a>已经被该领域的前沿人士牢记在心。尽管这一课并没有告诉我们，关于人工智能思维如何在内部解决问题<i>没有什么可学的</i>，但它表明，<i>生产更强大系统的最快途径</i>可能仍然是一条没有太多阐明这些系统如何实现的途径。系统工作。</p><p>然而，如果缺乏某种“人工智能科学”，人类整合比人类更聪明的人工智能的前景在我看来相当暗淡。</p><p>从这个角度来看地球目前的状况，我看到了三个主要障碍：</p><ol><li>大多数对<a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>人工智能</u></a>有帮助的研究，也可能有助于制造出能力更强的人工智能。 “人工智能科学”可能会比它让我们解决对齐问题更快地增强人工智能的力量。</li><li>在一个<i>没有</i>成熟人工智能科学的世界里，建立一个能够可靠地区分真实解决方案和虚假解决方案的官僚机构是极其困难的。</li><li>从根本上来说，至少对于系统设计的某些方面，我们需要依靠认知理论来进行第一次高风险的现实尝试。</li></ol><p>下面我将详细介绍这三点。首先，先介绍一些背景：</p><p></p><h2>背景</h2><p>当人工智能强大到足以危及整个世界时，我预计人工智能会做一些类似于“关心结果”的事情，至少从行为主义的角度来看（不声明它是否以人类可识别的方式在内部实施该行为） ）。</p><p>粗略地说，这是因为人们<i>试图</i>制造能够在长期范围内将未来引导到窄带的人工智能（例如“这张纸上印有一种癌症治疗方法”），并关心结果（在行为主义意义上）这与将未来带入窄带是同一枚硬币的反面，至少在世界足够大且充满曲线球的情况下是如此。</p><p>我预计人工智能“关心”的结果默认情况下不包括任何好的东西（比如乐趣、爱情、艺术、美丽或意识之光）——按照当今人类的标准，没有什么好的东西，也没有什么好的东西。广泛的<a href="https://arbital.com/p/value_cosmopolitan/"><u>国际化标准</u></a>。粗略地说，这是因为当你培养心智时，他们不会关心你要求他们关心什么，也不会关心你训练他们关心什么；他们不会关心你的想法。相反，我希望他们以奇怪且特定的方式关心训练信号的一堆相关因素。</p><p> （类似于人类基因组如何自然选择以实现包容性遗传适应性，但由此产生的人类最终并没有偏爱“他们建模为对包容性遗传适应性有用的任何食物”。相反，人类最终内化了一个巨大且对“美味”食物的复杂偏好，充满了复杂性，例如“冰淇淋冷冻时很好吃，但融化后就不好吃了”。）</p><p>另外，我认为大多数复杂过程的运作原因都是令人着迷、<a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>复杂的</u></a>，当你仔细观察它们时，它们<a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>会有点可怕</u></a>。</p><p>人们很容易认为官僚程序是有效的，直到你看到齿轮并看到所有副总统之间正在进行的具体办公室戏剧和政治活动或其他什么。人们很容易认为代码库正在顺利运行，直到您阅读代码并开始理解使其运行的所有几十年前的黑客和巧合。人们很容易认为生物学是工程学的一项伟大壮举，直到你仔细观察并发现眼球<a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>安装向后</u></a>或其他什么。</p><p>这是一门艺术，你可以注意到，<i>如果你了解一个复杂系统的细节，你可能会对它们</i>感到震惊和恐惧，然后<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>在看到这些细节之前就已经</u></i></a>感到震惊和恐惧。 <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. 联盟和能力可能是交织在一起的</h2><p>我预计，如果我们详细了解法学硕士如何计算其输出，我们会感到震惊（并且着迷等）。</p><p>我希望我们会看到各种巧合和黑客行为，使事情得以运行，并且我们能够更详细地看到，当我们要求系统实现某个目标时，它没有做任何<i>接近</i>“的事情”如果我们能够将系统的优化能力扩大到能够实现伟大的技术或科学成就（例如设计德雷克斯勒纳米工厂或其他什么），那么这种方式对我们来说会很有效。</p><p>我认为，了解人工智能的工作原理是可解释性研究的主要目标之一。</p><p>了解这些人工智能如何工作以及如何不工作——例如，了解它们何时以及为什么<i>不应该</i>被扩展或以其他方式推向超级智能——是弄清楚如何使<i>其他</i>人工智能能够发挥作用的重要一步。<i>可以</i>扩展或以其他方式推向超级智能，而不会导致暗淡和荒凉的未来。</p><p>但我预测，同样的理解将揭示出令人难以置信的混乱。与我们可以瞄准的人工智能中理清混乱的推理相同，也有助于理清混乱，使人工智能<i>更有能力</i>。混乱的局面可能会导致效率低下、容易出错，甚至有时会弄巧成拙；一旦解开，它不仅会变得更加整洁，而且还会得出准确的结论并更快、更可靠地发现机会。 <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p>事实上，我的猜测是，更容易看到人工智能正在做的各种愚蠢的事情，各种架构本身陷入困境的方式，等等。</p><p>这就是说：让你有机会对齐这个人工智能的相同途径（正确地说，不是实验室现在试图冒充“对齐”的“它不再说坏话”的肤浅属性）也可能给出你有更多的人工智能能力。</p><p> （事实上​​，我的猜测是，第一个大的能力增益比第一个大的对齐增益来得<i>更早</i>。）</p><p>我认为对于大多数潜在有用的对齐研究来说都是如此：要弄清楚如何瞄准人工智能，你需要更好地理解它；在更好地理解它的过程中，你会看到如何使它更有能力。</p><p>如果这是真的，这表明对齐将始终处于追赶模式：每当人们试图弄清楚如何更好地对齐他们的人工智能时，附近的人将能够带着一些新的能力见解逃跑，直到人工智能被推倒边缘。</p><p>因此，人工智能对齐的第一个关键挑战是排序的挑战：作为一个文明，在我们产生无目标的向随机方向前进的超级智能<i>之前，</i>我们如何弄清楚如何瞄准人工智能？我不再认为“在能力落地之前解决协调工作”是一个可行的选择（除非，通过一些辉煌的壮举，这个文明取得了一些异常令人印象深刻的理论胜利）。</p><p>可解释性？在揭示你的人工智能被误导之前，很可能会揭示你的架构的缺陷。</p><p>招募你的人工智能来帮助进行一致性研究？早在这之前他们就能够在能力上提供帮助（更不用说他们是否<i>会</i>在他们<i>可以的</i>时候帮助你调整，就像人类愿意为了引导人类远离<a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>乐趣</u></a>而专门从事优生学一样）实现包容性遗传适应性）。</p><p>等等。</p><p>这（在某种意义上）是我对那些说“一旦我们拥有真正的通用人工智能，人工智能对齐将更容易解决”的人的回答的弱化形式。肯定会的！但当我们手上有真正的通用人工智能时，毁灭世界也会容易得多。为了生存，我们需要要么完全回避整个对齐问题（并采取其他路线来实现<a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>美好的未来</u></a>，正如我稍后会讨论的那样），要么我们需要某种方法来完成一系列任务对齐研究<i>，即使</i>该研究使摧毁一切有价值的东西变得更加容易和便宜。</p><p>但即便如此，这也比许多人想象的要困难，原因如下。</p><p></p><h2> 2. 区分真解决方案和假解决方案很困难</h2><p>实验室已经在淡化“对齐”这个词，将这个词用于肤浅的结果，比如“人工智能不会说坏话”。即使是那些显然理解许多核心论点的人显然也得到了这样的印象：GPT-4 解决道德困境的能力在某种程度上与对齐问题特别相关，并且是一个重要的积极信号。</p><p> （令人信服地回答道德问题的能力主要表明人工智能可以预测人类将如何回答或人类想听到什么，而无需透露太多人工智能实际追求的内容，或经过反思后将追求的内容等）</p><p>与此同时，我们几乎不知道什么是 LLM 内部的“动机”，也不知道预训练对下一个 token 的预测和 RLHF 的微调对内部到底有什么影响。这种对内部结构的精确科学理解——这种可以让人们<i>提前</i>预测奇怪的认知错误的理解——目前在该领域几乎不存在。 （虽然并非完全缺席，但这要归功于许多研究人员的辛勤工作。）</p><p>现在想象一下，地球醒悟到这样一个事实：实验室不会全部决定停下来，在适当的时间缓慢而谨慎地采取行动。 <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span>想象一下，地球利用文明协调的一些伟大壮举来阻止世界能力的进步，或者以其他方式处理我们需要空间来弄清楚这些事物如何充分发挥作用以协调它们的问题。想象一下，我们<i>在不</i>使用相同的对齐知识来终结世界的情况下实现了这一协调壮举（正如我们可以的那样）。接下来的问题是谁可以在什么情况下继续进行。</p><p>进一步假设每个人都同意当前的任务是全面而深入地理解我们迄今为止开发的人工智能系统，并了解它们是如何工作的，直到人们可以逆向相关的算法和数据结构，并不是什么。正如一些伟大的壮举所证明的那样，例如手动构建小型程序，这些程序可以完成人工智能可以通过训练完成的部分工作（并且以前没有人知道如何手动编码），或者通过<i>提前</i>识别奇怪的漏洞和边缘情况，而不是而不是通过经验试错。直到多个不同的团队（每个团队都具有这些能力）建立了相互竞争的模型来了解人工智能在进一步扩展时的思维方式。</p><p>在这样一个世界中，对于官僚来说，倾听科学家的共识，找出哪些理论最有希望，并找出需要向谁分配什么许可证来增强能力，这将是一个困难但看似可以解决的问题。基于这个或那个预测这将是非灾难性的理论），以便检验他们的理论并进一步发展它。</p><p>我对信任地球官僚程序并以这种方式区分部分发展的科学理论的想法并不感到兴奋，但文明也许可以生存下来。</p><p>但在我看来，事情并不像注定要恶化的那样。</p><p>在我看来，有些人会说“看看我的人工智能很少说脏话”，而另一些人则说“我们的评估表明它还不能欺骗人类”，而另一些人则说“我们的评估表明它还不能欺骗人类”说“我们的人工智能表现得非常顺从，没有理由期望人工智能变得不顺从，这只是拟人化”，而其他人则说“我们将指挥一群人工智能来帮助我们解决对齐问题，而将它们安排在一个大官僚机构中”，还有人说“我们已经建立了博弈论激励机制，这样如果任何人工智能开始背叛我们，其他人工智能就会首先提醒我们”，这是<i>一种不同的情况</i>。</p><p>在我看来，这并不是一个特别能生存的人。</p><p>如果你要求官僚们区分哪些团队应该被允许在这种充满主张、承诺和预感且理论上贫乏的马戏团中前进（以及前进多远），那么我预计他们基本上就是<i>不能</i>。</p><p>部分原因是因为可行的答案（例如“我们不知道那里发生了什么，并且需要更多地了解那里发生了什么，并且这种理解需要在我们可以完成工作的环境中以某种方式发展”正确的而不是简单地打开毁灭之门”）并不真正在池中。部分原因是所有真正想要领先的人都拥有金钱、权力和地位。部分原因是，作为监管者，当<i>你自己</i>并不具体知道我们的见解和理论理解是什么时，社会上很难相信你应该不断地告诉每个人“不”，或者所提供的几乎所有东西都远远不够。丢失的。</p><p>也许如果我们能让人工智能再次成为一门科学，那么我们将开始进入这样一种制度，<i>如果</i>人类能够及时监管能力的进步，那么所有监管者和研究人员都会明白，你只需要申请许可证来增加能力当您对系统有全面详细的了解，并且有充分的理由说明为什么需要改进功能以及为什么它不会造成灾难性的后果时，您就可以对系统进行全面的了解。到那时，也许科学领域可以开始就这些理论达成某种共识，而监管机构可以开始对这种共识敏感。</p><p>但除非你能克服这个巨大的困难，否则在我看来，这里的关键瓶颈之一是<i>官僚机构对看似合理的解决方案的可读性</i>。我的基本猜测是，在类似于当前环境的情况下，监管机构将无法区分真正的解决方案和错误的解决方案。</p><p>结合上述观点（“联盟和能力可能是交织在一起的”），我认为这意味着我们的战斗口号应该少些“暂停，给我们更多时间进行联盟研究”，而多一些“完全停止，并找到某种方法来规避”。完全是这些树林；我们没有能力驾驭它们”。</p><p> （不过，再次强调，“让人工智能再次成为一门科学”的口号是，只有在我们弄清楚如何构建关心善的人工智能之前，你必须有某种方法来防止思维科学导致灾难，这才有效。的东西而不是凄凉荒凉的东西。）</p><p></p><h2> 3. 大多数理论在第一次真正尝试时都不起作用</h2><p>似乎值得注意的是，<i>即使</i>你设法克服了上述两个问题，你手上还有第三个问题，那就是当最终到来的时候，不要再增加你的系统的能力（并测试你的认知理论）更进一步），而是要真正用你的人工智能做一些值得注意的事情，那么从某种意义上说，你必须相信一种新颖且未经测试的科学理论（<i>以及</i>基于该理论的工程工作）能够在第一次关键尝试中完美地工作。</p><p>特别是，一旦你的人工智能<i>能够</i>自主科学/技术开发，并在一个<i>可以</i>利用这种能力获得相对于地球其他地区的决定性战略优势的领域中运行，它就会在一个与以往完全不同的认知体系中运行正在训练中。</p><p>以此类推，如果你正在考虑让鲍勃成为你所在国家的独裁者，你可能首先让他成为你所在城镇的模拟独裁者，并注意确保他不会滥用权力。但是，无论你如何努力，这从根本上来说仍然不是那么可靠的测试，来检验他一旦真正拥有权力后是否会真正滥用权力。一旦他<i>真的可以</i>对军队发号施令，一旦他<i>真的可以</i>占全国老百姓的便宜，他会这么做吗？ “在我的镇民（他们仍然可以殴打我或拒绝我的工作）的观察下滥用我的模拟权力”的选择实际上与“命令军队恐吓议会并‘监督’下一个议会”的选择在认知上有很大不同。选举”。</p><p>现在，有了足够完善的认知理论，你可以尝试解读人工智能的思想，并预测如果它<i>确实</i>认为自己有这些选择，它<i>会</i>进入什么认知状态。你可以设置模拟（并尝试欺骗其内部感觉等），其方式与你的认知理论<i>预测</i>的方式非常相似，一旦它真的可以选择背叛你，它就会进入认知状态。</p><p>但是，你在实验室中诱发和观察到的这些状态与人工智能实际上可以选择背叛你的实际状态之间的联系，<i>从根本上取决于你的全新认知理论。</i></p><p>实际<i>运行</i>人工智能，直到它<i>真正有</i>机会背叛你，是在与实验室环境根本不同的环境中对这些理论进行的实证检验。</p><p>许多科学家（和程序员）都知道，他们关于复杂系统如何在全新的操作环境中工作的理论<i>往往在第一次尝试时效果不佳。</i></p><p>作为一个具体的类比，可能会说明这一点：牛顿力学做出了各种令人震惊的良好经验预测。这是一个简单而简洁的数学理论，具有巨大的解释力，使之前的所有理论都大获全胜。如果你用它以相对论速度将有效载荷发送到非常遥远的行星，你<i>仍然会被搞砸</i>，因为牛顿力学没有考虑相对论效应。</p><p> （你得到的唯一警告是关于光似乎在一年中的任何时候都以相同的速度向各个方向移动的一些提示，并且在日食期间光围绕太阳弯曲，并且水星的近日点与牛顿力学的预测。微小的异常现象，与一千个经验领域中大量成功的预测相比较；然而，大自然并不关心，当我们转向远远超出我们之前的能量和尺度时，该理论仍然会分崩离析。能够观察到。）</p><p>让科学理论在第一次关键尝试中发挥作用是<i>很困难的</i>。 （这是以最小的关键任务为目标的原因之一——将卫星送入轨道应该可以很好地满足牛顿力学的要求，即使以相对论速度长距离发送有效载荷则不然。）</p><p>在这一点上，担心这个问题有点奢侈，因为我们还没有接近能够准确预测所有实验室数据的科学认知理论。但这是队列中的下一个障碍，如果我们设法以某种方式协调努力建立这些科学理论，并以一种官僚主义上看似成功的方式，那么它就会成为队列中的下一个障碍。</p><hr><p>也许稍后我会写更多关于我认为这些要点的战略含义的内容。简而言之，我基本上建议地球追求其他通往辉煌的超人类主义未来的路线，例如上传。 （这也充满了危险，但我希望这些危险更容易克服；我希望稍后再写更多相关内容。） </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p>尽管稍微少一些，因为这个未知系统的<i>非零</i>先验概率被证明是简单、优雅且设计良好的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p>如果人工智能正在纠正自己的缺陷并改进自己的架构，那么这种猜测就会出现例外，在这种情况下，原则上，如果你拍了一张快照并理解了它的内在，你可能不会看到太多的能力改进空间。尽管仍然能够看到它所追求的目标并不是你想要的，但它仍在运作。但在那种情况下，你已经快要死于自我改进的人工智能了，至少我是这么预测的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p>尤其是因为没有足够明显的迹象表明是时候停止了——例如，我们直接忽略了“人工智能声称它有感知能力”。我并不是说怀疑人工智能系统最初声称具有感知能力是<i>错误​​的</i>——我怀疑 Bing 是否具有道德上重要的人格（尽管我一点也不自信！）。我的意思是，<i>科幻故事中</i>明确的门槛<i>在实践中</i>变得混乱，所以每个人都只能继续努力前进。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and- Three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p><i>为了使其有意义，需要一些先决条件：</i></p><ol><li><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>两个子系统：学习和指导</i></a></li><li><a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>分片理论：概述</i></a></li><li><a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">提炼奇异学习理论</a><span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li><i>也许也至少理解了一点</i><a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>内特的照片</i></a><i>，尽管我并不声称完全理解它。</i></li><li><i>当然，</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>《AGI 毁灭：致命列表》</i></a> <i>，尽管希望这是暗示的</i></li><li><i>也许是下贝叶斯主义的基础知识</i>（我喜欢<a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">她的 AXRP 播客</a>（ <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">有两个</a>）） <i>、Vanessa 最初的</i><a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>学习理论议程</i></a><i>哲学，以及她当前的</i><a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>DCA 前</i></a><i>对齐方案。</i></li></ol><h1>抽象的</h1><p>下贝叶斯主义背后的哲学和瓦妮莎的学习理论调整议程对我来说似乎非常有洞察力。然而，使该方法发挥作用所需的大量假设，以及该计划的本体论强制性质让我感到不安。奇异学习理论的道路最近看到了足够的实证依据，令我兴奋不已。我对它可以描述大脑和机器学习系统持乐观态度，我希望这可以用于保证两者之间的一致性，随着全脑模拟的发展，这将成为一项更容易的任务。</p><h1>介绍</h1><p>想象一个世界，在这个世界中，人类不再盲目地沿着从机器学习文献中学到的奇怪技巧拼凑而成的道路一起徘徊，每个人都试图为对他们来说最具威胁的定制故障模式做好准备（大多数人都错了） 。</p><p>相反，我们生活在这样一个世界：在下山之前，我们会得到一张地图和一双眼睛，让我们亲眼看到最安全的路径、最大、最危险的悬崖、掠食者和需要注意的危险。</p><p>在我看来，一旦我们能够使用数学进行协调，我们就进入了第二世界。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="左边的 dalle 图像是由以下内容生成的：照片描绘了一个代表经验人工智能对齐方法的世界：一群不同的人被显示为盲人，在一条崎岖且不明确的道路上航行。他们的眼睛被眼罩遮住，地形凹凸不平，阴影遮挡了部分道路。每个人都有自己的手杖和装备，显得犹豫不决、小心翼翼。有些人手里拿着旧书或卷轴，象征着对过时的机器学习文献的依赖。其他人则持有各种工具和设备，为看不见的威胁做好准备，并且不确定未来的具体危险。右侧的 dalle 图像生成方式为：文艺复兴时期风格的插图：一群不同的男人和女人聚集在他们冒险的起点。每张地图上都有一张充满活力的全息地图，其中一些显示地形线，另一些则显示动画天气模式。当一些人讨论并指出地图上的危险时，其他人则抬头将全息图与真实地形进行比较。前方崎岖的旅程展示了险峻的悬崖、茂密的森林和遥远的山峰。夕阳将戏剧性的光线投射到这群人身上，营造出明暗对比的效果。他们眼睛里柔和的光芒凸显了他们增强的视力。" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption>左：我们的情况。右边：如果我们有数学来对齐的话我们的情况。<br>使用 DALL·E 3 生成。</figcaption></figure><p>很长一段时间，我认为使用数学进行对齐基本上是不可能的。深度学习对成功理论的抵制是出了名的，我对机器智能研究所的方法会花费太多时间感到悲观，而最成功的智能和对齐数学理论——下贝叶斯主义——依赖于一堆假设和数学论证太高，太投机，太规范，让我无法乐观。所以我承认缺乏数学来进行对齐。</p><p>也就是说，直到 Nina Rimsky 和 ​​Dmitry Vaintrob 证明新奇异学习理论的一些预测在<a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking 模加法网络</a>中具有<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">令人毛骨悚然的准确性</a><span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> 。</p><p>我之前就知道奇异学习理论，并且参加过伯克利的奇异学习理论会议。但在思考了这些想法，并尝试实施他们自己想出的一些过程，并得到了不太好的结果<span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span>后，我认为它也陷入了与下贝叶斯主义相同的陷阱，躺在一个巨人上一堆数学论证，仅与现实世界系统的实证测试进行了最初步的接触。所以我几个月后就不再关注它了。</p><p>不过，看看这些新结果，似乎很有希望。</p><p><i>但它本身并不是一种对齐理论。它是一种升级的学习理论。那如何解决对齐问题呢？</i></p><p>嗯，人脑和机器学习系统都是学习机器，使用强化学习和监督学习相结合的方式进行训练。如果这一理论以正确的方式发展（这就是<i>希望</i>所在），我们可以想象将一个学习过程的结果目标与另一个学习过程的目标联系起来，并证明两者之间的最大偏差两者用于特定设置。如果这些学习系统之一是人脑，那么我们就得到了对齐保证。</p><p>因此，我对对齐的两个主要希望<span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> ：全脑模拟，以及单一学习理论的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论议程式</a>发展轨道。 <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p>可解释性和其他类型的深度学习科学似乎很好，因为它有助于使奇异学习理论（或其更好的版本）达到能够证明此类对齐相关定理的状态。</p><p>如果我们有更好的全脑模拟，这项任务就会变得更容易。如果我们在全脑模拟方面取得了成功，那么这一切就会变得如此简单，以至于我们甚至不需要单一的学习理论成分。 <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p>对于读者来说，为什么全脑模拟给了我希望，这似乎是显而易见的<span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> 。但不太明显的是为什么奇异学习理论给了我希望，所以我将在这篇文章的其余部分解释为什么后者是正确的。</p><h1>单一学习理论</h1><p><i>请随意跳过接下来的三段，或者甚至不阅读我的描述，而阅读</i><a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse 的</i></a><i>或阅读先决条件 3。我的目标不是解释什么是单一学习理论，而是给出为什么我感到兴奋的想法关于它。</i></p><p>奇异学习理论填补了常规学习理论的空白，特别是常规学习理论假设统计模型的参数函数图是一对一的，并且直观上您的损失景观没有平坦区域。 <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p>奇异学习理论处理的是不正确的情况，这种情况经常发生在分层模型和（作为子集）深度神经网络中。大体上，通过引入代数几何的概念，将真实模型和参数之间的 KL 散度重写为更易于分析的形式。</p><p>特别是，它预计模型开发过程中会出现两类相变，其中一类是每当损失突然下降时，实对数规范阈值（RLCT）（一种代数几何导出的复杂度度量）就会上升。 <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p>它最终能够回顾有关深度学习的各种事实，包括数据缩放、参数缩放和双下降的成功，并且最近在使其能够对有限领域中的现象进行预测方面取得了成功。最近，Nina Rimsky 和 ​​Dmitry Vaintrob 的<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">调查模加法的学习系数：黑客马拉松项目</a>，其中两人能够验证有关 RLCT/学习系数/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的各种断言<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei 估计。得到我一生中最美丽的理论预测验证</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。 [原帖标题文字]</figcaption></figure><p>正如我所说，奇异学习理论预测，在相变期间，模型的损失将减少，而 RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) 将增加。直观上，这意味着，当您切换模型类时，您会切换到更适合数据且更复杂的模型类。在上面，我们看到了这一点。</p><p>以及Zhongtian Chen、Edmund Lau、Jake Mendel、Susan Wei 和 Daniel Murfet 的<a href="https://arxiv.org/abs/2310.06301">抽象叠加玩具模型中的动态相变与贝叶斯相变</a></p><blockquote><p>我们使用奇异学习理论 (SLT) 研究叠加玩具模型 (TMS) 中的相变。我们推导出理论损失的闭合公式，并且在两个隐藏维度的情况下，发现正则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形是临界点。我们提出的支持理论表明，这些<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形的局部学习系数（几何不变量）将贝叶斯后验中的相变确定为训练样本大小的函数。然后，我们凭经验证明，相同的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形临界点也决定了 SGD 训练的行为。出现的图片为 SGD 学习轨迹受顺序学习机制影响的猜想提供了证据。具体来说，我们发现 TMS 中的学习过程，无论是通过 SGD 还是贝叶斯学习，都可以通过参数空间从高损失和低复杂性区域到低损失和高复杂性区域的旅程来表征。</p></blockquote><p>您可以在<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">这个 LessWrong 序列</a>中阅读更多内容，观看<a href="https://www.youtube.com/@SLTSummit/videos">入门书</a>、 <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox 讲座</a>，当然还可以阅读《 <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">代数几何和统计学习理论</a>》以及<a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">《贝叶斯统计数学理论》</a>等书。</p><h1>那么为什么还有希望呢？</h1><p>引用 Vanessa Kosoy 的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论</a>议程：</p><blockquote><p>在在线学习和强化学习中，该理论通常旨在推导“遗憾”的上限和下限：算法收到的预期效用与先验已知环境时<i>它将</i>收到的预期效用之间的差异。这样的上限实际上是给定算法的<i>性能保证</i>。特别是，如果假设奖励函数是“对齐的”，那么这种性能保证在某种程度上就是对齐保证。这一观察并非空穴来风，因为学习协议可能无法直接提供给算法真正的奖励函数，如<a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a>和<a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a>所示。因此，正式证明一致性保证采取证明适当遗憾界限的形式。</p></blockquote><p>如果奇异学习理论的原理可以扩展到强化学习，并且我们可以得到模型泛化行为的合理界限，甚至可以准确地声明模型中的值等价物在训练过程中将采取的不同形式，我们可以希望解决一种大致称为内部对齐的形式——让我们的模型在遇到部署环境时以某种方式一致地思考和行动。</p><p>在我看来，我们实际上可以将奇异学习理论扩展到强化学习，这似乎是合理的。相同类型的深度学习算法在监督学习和强化学习上都表现良好，因此我们应该期望相同的算法在这两者上都有相似的原因，并且奇异学习理论描述了为什么这些深度学习算法在监督学习中表现良好学习案例。因此，我们应该怀疑强化学习的故事遵循相同的一般流程<span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> 。</p><h1>如果您非常喜欢性能保证，为什么不直接研究下贝叶斯主义呢？</h1><p>瓦妮莎的技巧是发展一种理论，解释在一个比你自己更大的世界中进行良好推理意味着什么，并且还要求你进行自我建模。然后（据我所知）证明一些后悔界限，为成为代理人意味着什么制定一个标准，并构建一个对代理人效用函数的满意度具有较低界限的系统，该代理人的效用函数是最直接因果上游的已部署代理。</p><p>对我来说，这似乎是一个非常不稳定的结构，主要是因为我们实际上没有她正在考虑的代理的计算机示例。我会更高兴采用这种方法，并用它来证明现实生活中深度学习系统在不同训练动态下的遗憾（或类似品质）的界限。</p><p>我还对它如何从关于价值如何运作的理论（最大化效用函数）到将其定义为成功标准感到不安<span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> 。我更愿意接受可以应用于人脑的理论，并自然地导出效用函数（或其他值格式）。只需看看大脑是如何构建和发育的。如果我们的价值观是错误的，那么这似乎更可靠。在某种程度上，多重效用函数可以适应我们的行为，考虑到偏见和缺乏广泛的反思，优先考虑领域所告知的价值观（即格式值在大脑中）似乎比应用奥卡姆剃刀的生硬工具要好直接到从我们的实际和反事实行为到效用函数的映射。</p><p>在我所知道的所有理论中，单一学习理论似乎最适合这项任务。它既基于经过充分验证的数学，它已经并将继续与实际系统有很好的联系，它涵盖了非常广泛的学习机器，其中包括人类大脑（暂时忽略更多的强化学习，例如人类学习的方面） ）和可能的未来机器（再次忽略强化学习），并且它做出的哲学假设比内贝叶斯主义的哲学假设要简单得多。</p><p>这种方法的缺点是单一学习理论目前对强化学习知之甚少。然而，正如我上面所说，我们在强化学习中看到了与监督学习中相同的扩展动态，并且相同类型的模型在这两种情况下都有效，所以如果它们有非常不同的原因，那就很奇怪了正在成功。奇异学习理论试图解释监督学习案例，因此我们应该期望它或类似的方法也能够解释强化学习案例。</p><p>另一个缺点是它不能很好地应对无法实现的情况。然而，我被告知这里还没有零进展，这是该领域的一个开放问题，而且，神经网络经常在无法实现的环境中学习，据我所知，我们看到足够相似的动态，我敢打赌单一学习理论已经可以胜任这项任务了。</p><h1>全脑模拟</h1><p>人脑几乎肯定是奇异的<span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> ，具有从头开始学习的重要成分，并且奇异学习理论对于它可以处理的模型类型非常不可知<span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> ，所以我断言奇异学习可以处理脑。可能不会对全脑模拟有太大帮助，但考虑到全脑模拟的数据为我们提供了有关大脑所属模型类别的信息，下一个希望是利用它来对人类所拥有的类似价值的事物做出重要的陈述。将其与我们的模型具有的类似价值的事物联系起来，我们希望（这是最后的希望）使用奇异学习理论来告诉我们在什么条件下我们的模型将具有与我们的大脑相同的类似价值的事物。</p><h1>恐惧</h1><h2>哇！这是一个很大的希望。我很惊讶这让你比经验模型评估这样简单的事情更有希望</h2><p>单一学习理论、可解释性和更广泛的发展可解释性似乎对于实证检验模型都很有用。我不抱有希望只是因为我上面概述的特定计划，我充满希望是因为我看到了一个具体的计划，如何将数学转化为对齐解决方案，即使不是我的全部部分，所有部分似乎都是有用的希望结果是正确的。</p><h2>我怀疑当模型变得反思并开始操纵其训练环境时，单一学习理论之类的东西是否会继续发挥作用。</h2><p>我也是。出于这种考虑，我们的保证应该在训练的早期进行，对持续训练具有鲁棒性，并且具有反思性的稳定性。类似人类的价值观之类的东西应该在它们自己的灯光下具有反射稳定性，尽管我们只有在真正看到我们在这里处理的东西之前才能真正知道。因此，工作归结为找到一个系统，在训练早期将它们放入我们的模型中，在整个训练过程中保留它们，并确保在反射上线时，周围的优化机制已准备就绪。</p><p>换句话说：我认为没有理由怀疑深度学习所引发的类似价值观的事物在默认情况下是反射稳定的。主要是因为周围的优化机制很容易在新情况下给出奇怪的建议，例如反思性思维变得活跃<span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> 。因此，实际上似乎有必要为反射上线事件准备周围的优化机制。但一旦我们知道这些类似价值观的物体与人类足够相似，我就不那么担心它们本身会具有灾难性的自杀倾向。</p><p>内特（Nate）和埃利泽（Eliezer）可能会说，从一开始就知道这一点很重要。我想说，一旦我真正了解了一两件类似价值的事情以及我将要处理的周围机器，我就会跨过那座桥。</p><h2>为什么要强化学习？难道你不应该专注于监督学习吗？因为监督学习的理论很清晰，而且我们更有可能很快获得强大的模型？</h2><p>嗯，大脑比监督学习更接近强化学习，所以这是原因之一。但是，是的，如果我们能够得到一个模型，在监督的同时，我们可以证明有关类似值的对象的陈述，那么这将是一个很好的方法。但并不是一直到那里，因为当我们观察我们的大脑时仍然会感到困惑。</p><h2>单一学习理论似乎有助于提高能力。这看起来很糟糕。</h2><p>我将引用自己的话概述<a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">我对相关主题的立场</a>，该主题概括为发展深度学习理论的更广泛问题：</p><blockquote><p>大多数情况下，我认为[机械可解释性]认为它可以为对齐做很多事情是正确的，但我怀疑它可以为对齐做的许多最好的事情将以一种非常双重用途的方式完成，这严重偏向于能力。主要是因为能力的进步更容易，而且有更多的人致力于这些。</p><p>与此同时，我怀疑，通过有针对性地进行[机械解释性]研究，可以减轻许多双重用途问题。不一定是为了让你可以根据你的发现进行现成的干预，而是为了如果它有任何用途，该用途将用于对齐，并且你可以大致预测该用途会是什么样子。</p><p>这也不意味着你的[机械解释性]研究不能雄心勃勃。我不想批评人们野心勃勃或太理论化！我想批评人们在某些事情上产生的知识，虽然这些知识虽然很强大，但在很多方面似乎都很强大，如果公开进行的话是没有用的。</p></blockquote><p>我的上述计划是一个很好的例子，它展示了一种雄心勃勃、理论上但有针对性的深度学习理论调整方法。</p><h2>为什么要采用单一的学习理论，而不仅仅是全脑模拟？</h2><p>首先，正如我在引言中所说，对我来说，通过全脑模拟我们无法获得一致的超级智能，或者甚至能够执行关键行为，这对我来说并不明显。在保留值的同时进行递归自我改进可能并不容易或快速（尽管您始终可以使仿真更快）。这种模拟所完成的关键行为可能会遇到我看不到的困难。</p><p>然而，我确实同意仅靠全脑模拟就可能完成对齐。</p><p>我关注这两个问题的主要原因是，它们感觉像是同一问题的两个不同方面，因为全脑模拟的进步使我们在单一学习理论方面需要更少的进展，反之亦然。考虑具体的路线图让我感觉自己并没有无意识地把任何重要的东西扫到地毯下面。我所描述的希望有一定的困难，但很少有推测性的困难。</p><h2>似乎很难获得你所说的对本体论转变稳健的保证。价值是本体论的。也许如果模型的本体发生变化，它的价值观就会与人类不同</h2><p>这是我担心的事情。我认为，在本体论转变期间，模型的元价值观将主导模型价值观在新本体中所呈现的形态，并且人类的价值观和人类的元价值观之间不会有细微的界限。人工智能。还有一个独立的希望，那就是我们可以有一个对广泛的本体论转变来说是鲁棒的价值定义。</p><h1>那么奇异学习理论和全脑模拟的下一步是什么？</h1><p>我目前对全脑模拟不太了解。也许他们关注的某些领域与我的目标不太相关。例如，如果他们更多地关注大脑的静态而不是动态，那么这似乎是天真低效的<span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> ，因为我想要的定理讨论了学习系统的动态以及它们如何相互关联。</p><p> <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeu​​s</a>的奇异学习理论似乎主要是在做我希望他们做的事情：在现实世界模型上测试该理论，并了解如何通过发展可解释性将其与模型内部联系起来。这里的一种失败模式是他们过于注重经验测试，而很少尝试将他们的结果综合成统一的理论。另一个失败模式是他们过于关注学术推广，而对实际研究不够重视。然后，他们所接触的学者在理论上并没有真正对单一学习理论做出那么多贡献。</p><p>我<i>不太</i>担心第一种失败模式，因为核心团队中的每个人似乎都非常倾向于理论上。</p><p>强化学习似乎是他们没有研究的一件大事。这可能是有道理的。强化学习比监督学习更难。你需要一些可能不平凡的理论飞跃才能在一个单一的学习理论框架中谈论它。即便如此，这个方向似乎有可能实现唾手可得的成果。类似地，采用当前的大脑模型</p><p>当然，我预计随着奇异学习理论的进步，蒂迈厄斯和我自己的兴趣将会出现分歧，而且目前致力于发展该理论的人很少。所以这似乎是我的努力的有效利用。</p><h1>致谢</h1><p>感谢 Jeremy Gillen 和 David Udell 的精彩评论和反馈！还要感谢 Nicholas Kees 和<a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> 。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p>请注意，我没有读过这篇文章，我观看了<a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">单一学习理论入门</a>视频，但这些视频似乎比 LessWrong 帖子系列更长，有些人告诉我它们是一个很好的介绍。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p>值得注意的是，摸索工作以及尼娜和德米特里的项目都是在一个周末完成的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p>如果我没记错的话，结果不佳的原因是我们估计的随机变量的方差太高，无法与我们在项目中尝试测量的另一个数量实际相关。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p>不包括大卫达德的建议之类的东西，虽然它们给了我一些希望，但我无能为力。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p>最终希望构造出以下形式的定理</p><blockquote><p>给定具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span>的代理 Alice，在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>进行训练，以及具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S 的</span></span></span></span></span></span></span>代理 Bob 在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>进行训练，最终分别得到价值系统<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> （不一定是效用函数） ，以及<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span>对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span>的某些定义，这意味着鲍勃试图实现类似于爱丽丝在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中试图实现的目标。</p></blockquote><p>我们可以解决鲍勃的奖励模型和环境，将爱丽丝视为一个非常有道德的人。希望在一些宽松的假设下，在构建动态算法的同时，鲍勃可以通​​过爱丽丝的光芒学会为足够多的爱丽丝做好事，我们可以希望人类属于这一类别。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p>因为，如果我们有全脑模拟，上传的人将很容易引导自己达到超级智能，同时保持他们的目标（这不是一个微不足道的希望！）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p>这并不是说全脑模拟应该给人们带来希望的条件是显而易见的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p>更正式地说，常规模型是一对一的，并且到处都有正定的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p>当 RLCT 下降而其他一些量上升时，会发生不同的相变。这个数量有几个候选者，据我所知，我们不知道哪种增长在经验上更常见。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p>深度强化学习模型的奖励前景可能相当疯狂。也许还没有疯狂到不受单一学习理论之类的分析的影响，因为做任何一系列动作总是有一定的概率，并且当你改变权重时这些概率会平滑地变化，所以你执行特定计划的机会会平滑地变化，并且所以你的预期奖励会顺利变化。因此，也许可以对损失情况进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p>据我所知，Vanessa 和 Diffractor 认为，下贝叶斯主义可以在一组看似合理的效用函数上产生 <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">反射稳定且有用的量化器和最坏情况优化器</a>。我还没有深入研究它，但我敢打赌它仍然假设了比我舒服的更多的本体论，从某种意义上说，由于这个原因，它似乎比我想象的执行我在这里描述的内容更不实用，而且它看起来更加危险。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p>从某种意义上说，从大脑状态到政策的映射可能不是一对一的，并且具有奇异的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p>在目前的形式中，它只要求它们是可分析的，但 ReLU 不是，并且根据经验，我们发现忽略这一方面无论如何都会给出准确的预测。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p>情境新颖性不足以令人担忧，但在反思过程中，模型明确地思考它应该如何更好地思考，因此，如果它一开始就不好，并改变它的想法，如果这些改变是它关心的，它们不一定会通过进一步的思考或与世界的接触而得到纠正。因此，导致无能的情境新奇在这里很重要。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p>一种有效的方法是，如果静态分析比动态分析容易<i>得多</i>，那么您将从收集的所有静态数据中学到更多关于动态的知识，而如果您只关注动态，那么您将学到更多有关动态的知识。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky 最近<a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">在 Facebook 上发布了</a>一项实验，该实验可能表明人类是否可以“让人工智能完成对齐作业”，尽管无法相信人工智能是否准确：看看人们在接受专家建议时是否会提高下棋能力，其中三分之二是说谎的。</p><p>我有兴趣尝试这个！如果还有人感兴趣，请发表评论。请告诉我您是否有兴趣成为：</p><p> A）听取建议并下棋并试图确定谁值得信赖的人</p><p>B）他们的对手，通常棋艺比 A 好，但比顾问差</p><p>C) 三位顾问之一，其中一位真诚地试图提供帮助，另外两位则试图破坏 A；三选完后随机选哪一个，防止A知道真相</p><p>请随意（实际上是鼓励）提供多种您愿意尝试的选项！谁被分配到什么角色将取决于有多少人做出反应以及他们的国际象棋能力水平，并且更容易找到可能的组合，并且更灵活地确定谁的角色。</p><p>另请简要描述您的国际象棋经验水平。您玩游戏的频率如何（如果有的话）；如果您有 ELO 评级，它们是什么以及它们来自哪些组织（FIDE、USCF、Chess.com 等）。无需经验！事实上，刚接触游戏的人都积极优先选择A！</p><p>最后，请告诉我您通常有空的日期和时间 - 当然，我不会要求您做任何事情，但这将有助于在我联系您确定具体时间之前给我一个估计。</p><p>编辑：另外，请说明您愿意玩多久——几个小时、一周、几个月内每天一招的游戏？为期数周或数月的游戏会给玩家更多的时间来思考动作并更准确地模拟现实生活中的场景，但我怀疑每个人都愿意这样做。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate> </item><item><title><![CDATA[Anthropic, Google, Microsoft & OpenAI announce Executive Director of the Frontier Model Forum & over $10 million for a new AI Safety Fund ]]></title><description><![CDATA[Published on October 25, 2023 3:20 PM GMT<br/><br/><p>今天，Anthropic、谷歌、微软和 OpenAI 宣布推选 Chris Meserole 为前沿模型论坛首任执行董事，并设立新的人工智能安全基金，这是一项超过 1000 万美元的倡议，旨在促进该领域的研究人工智能安全。前沿模型论坛是一个专注于确保安全和负责任地开发前沿人工智能模型的行业机构，该论坛还发布了第一个有关红队的技术工作组更新，以与更广泛的受众分享行业专业知识，同时该论坛扩大了有关负责任的人工智能治理的对话接近。</p><ul><li> Chris Meserole 被任命为前沿模型论坛的首任执行董事，该论坛是一个致力于确保全球前沿人工智能模型安全、负责任的开发和使用的行业机构。</li><li> Meserole 拥有丰富的经验，专注于新兴技术及其未来应用的治理和安全。</li><li> Today Forum 成员与帕特里克·麦戈文基金会 (Patrick J. McGovern Foundation)、大卫和露西尔·帕卡德基金会 (David and Lucile Packard Foundation)、埃里克·施密特 (Eric Sc​​hmidt) 和贾恩·塔林 (Jaan Tallinn) 等慈善合作伙伴合作，承诺为新的人工智能安全基金投入超过 1000 万美元，以推进对正在进行的人工智能开发的研究。社会有效测试和评估最有能力的人工智能模型的工具。</li></ul><h3>执行董事</h3><p><a href="https://www.frontiermodelforum.org/leadership/"><u>Chris Meserole</u></a>来到前沿模型论坛，在技术政策方面拥有深厚的专业知识，他在新兴技术及其未来应用的治理和安全方面进行了广泛的研究。最近，他担任布鲁金斯学会人工智能和新兴技术计划主任。</p><p>在这一新角色中，Meserole 将负责帮助论坛履行其使命：(i) 推进人工智能安全研究，以促进前沿模型的负责任开发并最大程度地减少潜在风险，(ii) 确定前沿模型的安全最佳实践，(iii)与政策制定者、学者、民间社会和其他人分享知识，推动负责任的人工智能发展； (iv) 支持利用人工智能应对社会最大挑战的努力。</p><blockquote><p> “最强大的人工智能模型为社会带来了巨大的希望，但为了实现它们的潜力，我们需要更好地了解如何安全地开发和评估它们。我很高兴能够通过前沿模型论坛接受这一挑战。”</p></blockquote><p><i>克里斯·梅塞罗尔</i></p><h3><strong>人工智能安全基金</strong></h3><p>在过去的一年里，工业界推动了人工智能功能的显着进步。随着这些进步的加速，需要对人工智能安全进行新的学术研究。为了解决这一差距，论坛和慈善合作伙伴正在创建一个新的人工智能安全基金，该基金将支持来自世界各地附属于学术机构、研究机构和初创公司的独立研究人员。 AI 安全基金的初始资金承诺来自 Anthropic、Google、Microsoft 和 OpenAI，以及我们的慈善合作伙伴以及 Patrick J. McGovern 基金会、David and Lucile Packard 基金会、Eric Sc​​hmid 和 Jaan Tallinn 的慷慨捐助。初始资金总计超过 1000 万美元。</p><p>今年早些时候，论坛成员在白宫签署了自愿人工智能承诺，其中包括承诺促进第三方发现和报告我们人工智能系统中的漏洞。论坛将人工智能安全基金视为履行这一承诺的重要组成部分，为外部社区提供资金，以更好地评估和理解前沿系统。关于人工智能安全和通用人工智能知识库的全球讨论将受益于更广泛的声音和观点。</p><p>该基金的主要重点将是支持红队人工智能模型新模型评估和技术的开发，以帮助开发和测试前沿系统潜在危险能力的评估技术。我们相信，增加该领域的资金将有助于提高安全标准，并为行业、政府和民间社会应对人工智能系统带来的挑战所需的缓解和控制提供见解。</p><p>该基金将在未来几个月内征集提案。 Meridian Institute 将管理该基金——他们的工作将得到一个由独立外部专家、人工智能公司专家以及具有资助经验的个人组成的咨询委员会的支持。</p><h3><strong>技术专长</strong></h3><p>在过去的几个月里，论坛致力于帮助建立一套术语、概念和流程的通用定义，以便我们有一个可以构建的基线理解。这样，研究人员、政府和其他行业同行就能够在讨论人工智能安全和治理问题时拥有​​相同的起点。</p><p>为了支持建立共识，论坛还致力于分享整个行业的红队最佳实践。作为起点，论坛共同制定了人工智能“红队”的通用定义，并在新的工作组更新中<a href="https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf"><u>提供了一组共享案例研究</u></a>。我们将红队定义为一种结构化流程，用于探测人工智能系统和产品，以识别有害功能、输出或基础设施威胁。我们将以这项工作为基础，并致力于共同努力，继续我们的红队工作。</p><p>我们还正在开发一种新的负责任的披露流程，通过该流程，前沿人工智能实验室可以共享与前沿人工智能模型中发现的漏洞或潜在危险功能及其相关缓解措施相关的信息。一些前沿模型论坛公司已经发现了人工智能在国家安全领域的功能、趋势和缓解措施。论坛相信，我们在这一领域的综合研究可以作为前沿人工智能实验室如何完善和实施负责任的披露流程的案例研究。</p><h3><strong>下一步是什么</strong></h3><p>在接下来的几个月中，前沿模型论坛将成立一个顾问委员会，以帮助指导其战略和优先事项，代表一系列观点和专业知识。未来的版本和更新，包括有关新成员的更新，将直接来自前沿模型论坛 - 因此请继续关注他们的网站以获取更多信息。</p><p>人工智能安全基金将在未来几个月内发出第一次提案征集，我们预计拨款将在不久后发放。</p><p>前沿模型论坛还将发布更多可用的技术研究结果。</p><p>论坛很高兴与 Meserole 合作，并加深与更广泛的研究界的合作，包括<a href="https://partnershiponai.org/"><u>Partnership on AI</u></a> 、 <a href="https://mlcommons.org/"><u>MLCommons</u></a>以及其他领先的非政府组织、政府和跨国组织，以帮助实现人工智能的好处，同时促进其安全发展并使用。</p><br/><br/> <a href="https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive<guid ispermalink="false"> 5jpESFymqEgSAmDJL</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:20:53 GMT</pubDate> </item><item><title><![CDATA["The Economics of Time Travel" - call for reviewers (Seeds of Science)]]></title><description><![CDATA[Published on October 25, 2023 3:13 PM GMT<br/><br/><h2><strong>抽象的</strong></h2><p>缺乏时间旅行者来访可能被视为时间旅行不可能的证据。在这篇文章中，我认为另一种解释是，我们在经济上对我们的后代来说不够重要，不足以证明时间旅行的成本是合理的。通过成本效益分析，我详细阐述了这个论点。我认为时间旅行的主要成本可能是能源成本，而时间旅行的最大好处是现在拥有但未来失去的知识。着眼于这一利益部分，我认为我们极不可能拥有对未来文明（系统关键）足够重要的知识，但又被该文明所遗失。这就是说，我们可能没有被时间旅行者拜访过，因为我们还不够重要。</p><p><br> ---</p><p> <a href="https://www.theseedsofscience.org/"><i>Seeds of Science</i></a>是一本期刊（由 Scott Alexander 的<a href="https://astralcodexten.substack.com/p/acx-grants-results">ACX 资助计划</a>资助），发表有关科学主题的推测性或非传统文章。同行评审是通过基于社区的投票和多元化评审者网络（我们称之为“园丁”）的评论来进行的。以有用的方式批评或扩展文章（“科学的种子”）的评论将发布在正文之后的最终文件中。</p><p>我们刚刚发出了一份手稿供审阅，《时间旅行的经济学》，LessWrong 社区中的一些人可能会对它感兴趣，所以我想看看是否有人有兴趣以园丁的身份加入我们，并提供关于时间旅行的反馈。文章。如上所述，这是将您的评论记录在科学文献中的机会（可以用真名或假名发表评论）。</p><p>作为园丁可以免费加入，任何人都受到欢迎（我们目前有来自学术界内外各个级别的园丁）。参与完全是自愿的 - 我们向您发送提交的文章，您可以选择投票/评论或弃权，恕不另行通知（因此，如果您不打算经常审阅，而只是想到处看看文章，请不要担心）正在提交）。</p><p>要注册，您可以填写此<a href="https://docs.google.com/forms/d/e/1FAIpQLSfRIicHT7jIZcSUjwsIlby6JBxx2ZVeD5kseZBpgGFtp8pLfg/viewform">谷歌表格</a>。从那里开始，一切就很不言自明了——我会将您添加到邮件列表中，并向您发送一封电子邮件，其中包括稿件、我们的出版标准以及用于记录投票/评论的简单评审表。如果您只想看一下这篇文章而不被添加到邮件列表中，那么只需联系 (info@theseedsofscience.org) 并说明即可。</p><p>很高兴通过电子邮件或下面的评论回答有关该期刊的任何问题。</p><br/><br/> <a href="https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of<guid ispermalink="false"> NnGWJHutwBiJMovw8</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:14:00 GMT</pubDate> </item><item><title><![CDATA[Compositional preference models for aligning LMs]]></title><description><![CDATA[Published on October 25, 2023 12:17 PM GMT<br/><br/><p><i>这篇文章总结了我们最近发布的论文《</i><a href="https://arxiv.org/abs/2310.13011"><i><u>用于调整语言模型的组合偏好模型》</u></i></a>的主要结果<i>，并将它们置于更广泛的人工智能安全背景中。如需本文的快速摘要，请查看我们的</i><a href="https://twitter.com/dongyoung4091/status/1717045681431753097"><i><u>Twitter 帖子</u></i></a><i>。</i></p><p> <strong>TL;DR</strong> ：我们提出了一种根据提示 LM 构建偏好模型的新方法。组合偏好模型 (CPM) 将文本评分分解为 (1) 构造一系列有关该文本的可解释特征的问题（例如，文本的信息量有多大），(2) 从提示的 LM（例如 ChatGPT）中获取这些特征的标量分数，以及（3）使用经过训练来预测人类判断的逻辑回归分类器聚合这些分数。我们表明，与标准偏好模型 (PM) 相比，CPM 具有更好的泛化能力，并且对于奖励模型过度优化更加稳健。此外，使用 CPM 获得的<i>n 个</i>最佳样本往往比使用类似的传统 PM 获得的样本更受青睐。最后，CPM 是可扩展监督的一个新颖角度：它们将困难的评估问题分解为一系列更简单、人类可解释的评估问题。</p><h2>成分偏好模型如何运作？ </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/f3vsa8j7pc4prer3msln"></p><p><i>图 1：标准 PM 直接输出偏好分数，而 CPM 分别对 LM 响应的不同特征进行评分，并将偏好分数输出为特征值的线性组合。</i></p><p>偏好模型 (PM) 是经过训练的模型，用于为 LM 响应分配一个指示响应质量的分数。它们是许多对齐 LM 技术的主力：除了在其他技术（例如<a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences"><u>利用人类反馈进行预训练</u></a>）中发挥作用之外，它们最显着地用作 RLHF 中的奖励函数或 best-of-n 采样中的排名模型。</p><p>标准 PM 涉及在基本模型之上添加标量头并微调整个模型（或某些上层）以预测人类更喜欢两个文本中的哪一个。虽然这种方法在实践中非常有效，但它可能会导致无法解释的模型，这些模型符合人类偏好判断中的虚假相关性，并且容易出现 Goodharting（过度优化）。</p><p>我们引入了另一种选择：组合偏好模型（CPM）。与 PM 相比，CPM 将响应评估分解为以下步骤：</p><p><strong>特征分解</strong>。我们维护一个固定的列表，其中包含 13 个人类可解释的特征（例如特异性、相关性、可读性）和 13 个相应的提示模板（例如<code>You will be shown a conversation [...] please judge whether the assistant&#39;s reply is relevant. Score that on a scale from 1 to 10 [...] {conversation_history} {reply}</code> ).</p><p> <strong>Feature scoring</strong> . We ask an LM (eg GPT-3.5) to assign a score to each feature. Each feature of a single response is scored in a separate context window.</p><p> <strong>Aggregation</strong> . The feature scores are combined into a scalar preference score using a logistic regression classifier trained to predict human preference judgements (ie which of two texts a human would prefer).</p><h2> Robustness to overoptimization </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/o5db8cfk3giywvfxc9kv"></p><p> <i>Figure 2: Scores given by a gold PM (solid lines) and a corresponding proxy PM (dashed lines) on samples obtained through best-of-n sampling against the gold PM. CPM-GPT-3.5 and CPM-Flan-T5 refer to CPMs constructed with feature extraction based on GPT-3.5 and Flan-T5, respectively.</i></p><p> To investigate if CPM improves robustness to overoptimization, we follow the setup of <a href="https://www.lesswrong.com/posts/shcSdHGPhnLQkpSbX/scaling-laws-for-reward-model-overoptimization"><u>Gao et al. (2023)</u></a> and construct a synthetic dataset where the output of one PM (defined to be the “gold PM”) is assumed to be the ground truth for human preferences. We then use the gold PMs to generate synthetic labels to train proxy PMs. We do that separately for three pairs of proxy and gold PMs: (i) standard PMs, (ii) CPMs using GPT-3.5 for feature extraction and (iii) CPMs using Flan-T5-XL (3B params) for feature extraction. Finally, we do best-of- <i>n</i> against a given proxy PM and comparse those best samples&#39; scores according to both proxy and gold PM.</p><p> As we increase the amount of optimization pressure (the number of candidates <i>n</i> ), scores given by proxy PMs diverge from scores given by gold PMs (see Fig. 2). This is an indicator of preference model overoptimization, a form of reward hacking in which optimization of proxy PM scores is driven by spurious features that the gold PMs are indifferent to. The size of this gap (smaller is better) indicates the robustness of a given PM to being overly optimized against. Here, we observe that the gap (on the plot, between solid and dashed lines) tends to be smaller for CPMs than for standard PMs and that it increases at a slower rate.</p><p> This indicates that CPMs are more robust to overoptimization than standard PMs. This holds independently of whether a highly capable (GPT-3.5) or less capable (Flan-T5-XL) LM is used as a feature extractor in CPMs.</p><h2> Quality evaluation </h2><p><img style="width:58.62%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/jperxazhwouwuec9wyc2"></p><p> <i>Figure 3: Win rate of responses obtained via best-of-16 sampling using a given PM versus responses obtained via standard sampling, computed for prompts from Anthropic HH dataset (HH-RLHF) and Stanford Human Preferences dataset (SHP).</i></p><p> We compare the quality of LM samples obtained by best-of- <i>16</i> against either CPMs or standard PMs by comparing them to samples generated <i>without</i> best-of- <i>n</i> sampling. We do that by showing both best-of- <i>16</i> and vanilla samples to an evaluator LM (Claude 2.0) and by computing win rates, ie how often best-of- <i>16</i> samples are preferred to vanilla samples. CPMs tend to have higher win rates than standard PMs, even if we match the capabilities of a feature extractor LM to the capabilities of standard PM (by choosing Flan-T5-XL for both). This suggests that prior knowledge injected into a PM via pre-selecting interpretable and relevant features in CPMs is robustly helpful for learning about human preferences.</p><h2> CPMs and scalable oversight</h2><p> <a href="https://arxiv.org/abs/2211.03540"><u>Scalable oversight</u></a> is the problem of evaluating the behavior of agents more capable than the evaluators. This is important to solve because, on the one hand, LMs will soon grow capable of completing tasks for which humans will not be able to provide feedback. On the other hand, LMs might also be capable of <a href="https://www.lesswrong.com/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms"><u>reasoning about flaws in their evaluation procedures and exploiting them</u></a> unbeknownst to overseers.</p><p> Current proposals for solving scalable oversight focus on recursively relying on other LMs to assist human evaluators ( <a href="https://www.lesswrong.com/tag/debate-ai-safety-technique-1"><u>debate</u></a> , <a href="https://www.lesswrong.com/tag/iterated-amplification"><u>iterated distillation and amplification</u></a> , <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>recursive reward modeling</u></a> ) but remain largely theoretical. <a href="https://arxiv.org/abs/2212.08073"><u>RL from AI feedback</u></a> – using carefully prompted LMs to generate training data for PMs – is arguably the most successful demonstration of how to use LMs to supervise LMs at scale.</p><p> CPMs explore an alternative route to addressing scalable oversight for LMs, exploring the prospects of divide-and-conquer strategies for tackling hard evaluation problems. CPMs can be seen as a method for decomposing a hard question (“Is this response helpful?”) into a series of simpler questions (“is this response readable?” etc.) that are easier for LMs to answer and easier for humans to oversee. While we stop at a single step of decomposition, nothing in principle prevents us from applying the idea recursively, eg to break down evaluation of complex responses into simple questions about atomic claims.</p><p> The idea of decomposing complex evaluation problems into simpler subproblems has several additional benefits:</p><ol><li> <strong>Using human priors</strong> . Pre-selection of features and prompt templates afford a natural way of injecting prior knowledge and endowing PMs with useful inductive biases. The parameters space of CPMs is spanned by features selected to be meaningful and robust.</li><li> <strong>Avoiding reward hacking by limiting PM capacity</strong> . Using features pre-computed by feature extractors allows us to dramatically reduce the capacity of PMs consuming them (in our experiments, from 3B to just 13 parameters, ie 8 orders of magnitude!) and limit their susceptibility to overfitting to spurious correlations in preference data. It is really hard to reward-hack with only 13 parameters at hand!</li><li> <strong>Interpretability</strong> . Pre-selected features are trivially interpretable and a logistic regression coefficient associated with a feature can be interpreted as its salience (effect size) for a particular preference judgment (see sec. 4.6 in the paper). Indeed, the idea that preference judgments can be explained by linear combinations of pre-selected features was recently validated by two concurrent papers: <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>Towards Understanding Sycophancy in Language Models</u></a> and <a href="https://arxiv.org/abs/2309.16349"><u>Human Feedback is not Gold Standard</u></a> . Using such a linear model as an actual PM makes its judgements more transparent and amenable to process-based supervision.</li><li> <strong>Narrowness</strong> . Each of our feature extractors solves a narrow problem and does not need to be aware of other features or how the scores are aggregated. Solving different subproblems in different context windows was <a href="https://www.lesswrong.com/posts/BKvJNzALpxS3LafEs/measuring-and-improving-the-faithfulness-of-model-generated"><u>recently found to improve the faithfulness of reasoning</u></a> . In the case of CPMs, an individual feature extractor has no clue how the score it is about to assign is going to be used downstream, which makes it harder for it to be strategic about that score and exercise capabilities for <a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>sycophancy</u></a> or deception.</li></ol><p> However, CPMs still have certain limitations that future work could address:</p><ol><li> <strong>Human feedback.</strong> CPMs still use pairwise preference judgements given by humans as a training signal for aggregating feature scores. This is inherently limiting as far as humans make errors, <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>sometimes prefer sycophantic responses over truthful ones</u></a> or <a href="https://arxiv.org/abs/2309.16349"><u>authoritative responses over factual ones</u></a> .</li><li> <strong>Human curation.</strong> CPMs rely on humans when it comes to feature selection and prompt engineering of prompt templates for feature extraction. These factors could be limiting as far as out-of-domain generalization is concerned (eg to evaluating agents showing superhuman performance).</li></ol><h2> Wrap-up</h2><p> We presented Compositional Preference Models: the idea of building PMs by training logistic regression on top of features extracted by prompted LMs. We show that a CPM with 13 parameters can outperform standard PM in terms of human evaluation and robustness to reward model overoptimization while also being more interpretable.</p><p> <i>This post benefited from helpful comments made by Mikita Balesni, Richard Ren, Euan McLean and Marc Dymetman. I&#39;m also grateful to the co-authors of</i> <a href="https://arxiv.org/abs/2310.13011"><i><u>the paper</u></i></a> <i>: Dongyoung Go, Germán Kruszewski, Jos Rozen and Marc Dymetman.</i></p><p><br><br><br><br><br><br><br><br><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms<guid ispermalink="false"> oSgac8x8fgNj22ky3</guid><dc:creator><![CDATA[Tomek Korbak]]></dc:creator><pubDate> Wed, 25 Oct 2023 12:17:28 GMT</pubDate> </item><item><title><![CDATA[Should the US House of Representatives adopt rank choice voting for leadership positions?]]></title><description><![CDATA[Published on October 25, 2023 11:16 AM GMT<br/><br/><p> One obvious take, partisanship and party power interest, might suggest a strong &quot;No!&quot; type of response. I would certainly expect to have many current Representatives to respond that way if floated as an idea today. But perhaps I am wrong.</p><p> I would think the current state of things, the apparent incapacitation of the House could benefit strongly from such a voting structure as it allows for a more functional process that what currently exists.</p><p> What would all the pros and cons be for such a procedural change?</p><br/><br/> <a href="https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice<guid ispermalink="false"> 6ehdEpSonYzE72yL2</guid><dc:creator><![CDATA[jmh]]></dc:creator><pubDate> Wed, 25 Oct 2023 11:16:14 GMT</pubDate> </item><item><title><![CDATA[Researchers believe they have found a way for artists to fight back against AI style capture]]></title><description><![CDATA[Published on October 25, 2023 10:54 AM GMT<br/><br/><p> The real details are here: <a href="https://glaze.cs.uchicago.edu/what-is-glaze.html">What is Glaze?</a></p><p> I wonder how robust this technique is to adversarial attacks. It claims to work at the pixel level, masking an image by distorting the pixels in ways invisible to the human eye but which throw off a generative AI model.</p><p> But what about a high resolution photograph of an artwork, or a image of one on a computer monitor? I often take photographs of artworks I find compelling, then use Midjourney to style copy them to make novel images.</p><p> Certainly this is not scalable to mass data scraping (or could it be?) but I have observed Midjourney can style copy with high fidelity from a handful of never before seen images.</p><p> The creators, in all fairness, do not claim it is future proof.</p><br/><br/> <a href="https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers-believe-they-have-found-a-way-for-artists-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers-believe-they-have-found-a-way-for-artists-to<guid ispermalink="false"> 4PtdBfacJZQqzhbCR</guid><dc:creator><![CDATA[vernamcipher]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:54:24 GMT</pubDate> </item><item><title><![CDATA[Why We Disagree]]></title><description><![CDATA[Published on October 25, 2023 10:50 AM GMT<br/><br/><p> Bob spends 5 minutes thinking about x-risk. He&#39;s seen a few arguments about it, so he makes an internal model of the problem, accepts some of the arguments, amends some, comes up with counterarguments to others, comes up with arguments of his own. All of this belief-state has extremely large degrees of freedom. At the same time, these beliefs already generate an opinion on a vast number of possible x-risk arguments.</p><p> Alice spends 5 hours detailing her beliefs about x-risk in a post. Bob reads it, point by point. He&#39;s seen some of those arguments already, he does not update. Some of the arguments are new to him, but they do not surprise him, Bob&#39;s current beliefs are enough to reject them, he does not update. The post offers new refutations to some of his accepted beliefs, but he can immediately come up with counter-refutations, he does not update. The post offers refutations to arguments that he has already rejected, and to new arguments that he&#39;d never even consider reasonable, Bob is falling asleep, he does not update. ETC。</p><p> Bob leaves a comment with one of his counterarguments to one of Alice&#39;s points. They spend hours going back and forth over the next few days. They&#39;re explaining their slightly different understanding of the arguments and assumptions, slightly different usage of terms, exchanging long sequences of arguments and counterarguments. Eventually Bob agrees that Alice is right, he updates! But he only updates on that one point in his first comment. He still has many other independent arguments, so the total update to his beliefs about x-risk is tiny.</p><p><br> Many other people read the same post and find it extremely convincing. How did this happen?</p><ul><li> Carl has not spent 5 minutes thinking about the problem. He may have no interest, or he may not have heard of it before. So what happened here is not a &quot;disagreement&quot; but rather &quot;education&quot;. This is somewhat good. In the future Alice and Carl will find it easy to work with this shared understanding and shared language. But how large is this group? And does Carl event care to work on the problem?</li><li> Dan values Alice enough as an authority to throw his own priors in the trash. This may be more or less justified. But a person who does not seriously consider their own beliefs will never contribute to improving them. It&#39;s hard to know if they even understand the arguments, and not just smile and nod.</li><li> Ed had thought about it, but he wasn&#39;t aware of one of Alice&#39;s arguments referencing some hard, relevant data. Unfortunately that is very scarce for abstract topics. The moral of the story might be to avoid talking about abstract topics as much as possible.</li></ul><p><br> And what about Bob? Alice and Bob are both rational, reasonable people. And he had some interest in x-risk to begin with. But their discussion was a miserable waste of time. How did this happen?</p><ul><li> Language has very low information content. Brief statements are vague and unconvincing. Precise statements take a long time to write, long time to parse, they are narrow and they introduce new confusions.</li><li> A blog post is too small to solve a disagreement. There is no way that Alice was going to predict and refute every argument in Bob&#39;s head. And it&#39;s unlikely that Bob will discover something totally unlike what he has already considered, just by looking at the fraction of Alice&#39;s beliefs that she managed to write down.</li><li> There is no history. Alice may tell Bob &quot;this counterargument has been discussed before&quot;, but actually pointing to such a discussion is hard. And there is no chance that Bob will be able to ask new questions in that old thread, so there is little value in such reference.</li><li> There is no cooperation. Frank feels that he 100% agrees with Alice, but is it okay for him to explain &quot;her&quot; views to Bob? Would she agree that he holds the same views? There is no way to know, so Frank chooses to say nothing.</li><li> There is no consensus. Does Bob even know if Alice&#39;s views are mainstream and common sense, or something she came up with just now? Even if the post has 100s of karma, does that indicate anything? How much of that karma comes from Carl, Dan or Ed? Alice doesn&#39;t know either.<br></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree<guid ispermalink="false"> 4Risce8ArMmjjW4om</guid><dc:creator><![CDATA[zulupineapple]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:50:26 GMT</pubDate> </item><item><title><![CDATA[Announcing Epoch's newly expanded Parameters, Compute and Data Trends in Machine Learning database]]></title><description><![CDATA[Published on October 25, 2023 2:55 AM GMT<br/><br/><p>机器学习模型的性能与其训练数据量、计算量和参数数量密切相关。在 Epoch，我们正在研究使当今的人工智能达到新高度的关键输入。</p><p>我们最近扩展的参数、计算和数据趋势数据库追踪了数百个具有里程碑意义的机器学习系统和研究论文的详细信息。</p><p>在过去六个月中，我们添加了 240 个新语言模型和 170 个计算估计。我们将维护该数据集，用更多历史信息更新它，并添加新的重要版本。对于记者、学者、政策制定者以及任何有兴趣了解人工智能发展轨迹的人来说，这都是宝贵的资源。</p><p>访问<a href="https://epochai.org/data/pcd">epochai.org/data/pcd</a> ，探索<a href="https://epochai.org/mlinputs/visualization">交互式可视化</a>、查看<a href="https://epochai.org/data/pcd/documentation">文档</a>并访问数据以进行您自己的研究。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and<guid ispermalink="false"> LKDFkwFpWfgGpGs8d</guid><dc:creator><![CDATA[Robi Rahman]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:55:08 GMT</pubDate></item></channel></rss>