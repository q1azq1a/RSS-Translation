<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 30 日星期四 18:15:24 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What's next for the field of Agent Foundations?]]></title><description><![CDATA[Published on November 30, 2023 5:55 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 16:09:49 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 16:09:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Alexander、Matt 和我想讨论一下 Agent Foundations (AF) 领域，它的现状以及未来如何加强和发展它。</p><p>首先，我们每个人都会发表第一条信息，概述我们目前的一些关键信念和悬而未决的问题。我们的想法不是给出全面的看法，而是挑选出我们每个人关心/认为重要和/或我们感到困惑/想要讨论的 1-3 件事。我们可能会回应以下提示的某些子集：</p><blockquote><p>您认为AF的视野在哪里？您如何看待 AF 在更大的联盟格局中/在让人工智能未来顺利发展方面所扮演的角色？您希望看到它去哪里？您使用什么作为实现这一目标的关键瓶颈？对于我们如何克服这些问题，您有什么想法？</p></blockquote><p>在我们正确启动之前，有几件事似乎值得澄清：</p><ul><li>粗略地说，代理基础是指旨在<i>理解代理、智能行为和一致性基础的</i>概念性和正式工作。特别是，我们指的是比人们所谓的“老式 MIRI 型代理基础”更广泛的东西，通常由决策理论和逻辑等领域提供信息。</li><li>我们不会具体讨论代理基金会研究背后的价值或变革理论。我们认为这些都是重要的对话，但在这次具体对话中，我们的目标是不同的，即：假设 AF 有价值，我们如何加强这一领域？</li></ul><h1>它应该看起来更像一个正常的研究领域吗？ </h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 16:15:42 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 16:15:42 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>目前，我对代理基金会感兴趣的主要问题是，它是否应该继续其当前的特殊形式，或者是否应该开始看起来更像一个普通的学术领域。</p><p>我也有兴趣讨论变革理论，只要它与其他问题有关。</p><h1>为什么要代理基金会？</h1><p>我自己对代理基础工作作为联盟研究的一个潜在富有成果的方向的推理是：</p><ul><li>大多数错位威胁模型都是关于代理追求我们希望他们不追求的目标（我认为这没有争议）</li><li>关于代理的现有形式主义似乎对于理解或避免这些威胁没有那么有用（同样可能没有那么有争议）</li><li>开发新的、更有用的似乎很容易处理（这可能更有争议）</li></ul><p>我认为这可能很容易处理的主要原因是，到目前为止，还没有投入太多的时间来尝试做到这一点。从先验的角度来看，这似乎是一种你可以得到很好的数学形式主义的东西，到目前为止，我认为我们还没有收集到太多你不能得到的证据。</p><p>所以我想我想让大量具有不同专业领域的人来思考这个问题，我希望他们中的一小部分人发现了一些根本上重要的东西。一个关键问题是该领域目前的运作方式是否有利于这一点。</p><h1>它需要一个新名字吗？ </h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7RFsGHYEynK4LDgGb-Mon, 27 Nov 2023 16:16:25 GMT" user-id="7RFsGHYEynK4LDgGb" display-name="Alexander Gietelink Oldenziel" submitted-date="Mon, 27 Nov 2023 16:16:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>亚历山大·吉特林克·奥尔登齐尔</b></section><div><p></p><p>广义的特工基金会是否需要一个新名称？</p><p> “基金会特工”这个名字是被诅咒了吗？</p><p>我听到的建议是</p><p>“什么是心灵”，“什么是代理人”。 “数学对齐”。 “代理机制”</p><h1>认知多元主义和影响之路</h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 16:42:17 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 16:42:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>一些思考片段：</p><p> (1) 澄清和创造有关代理基金会范围的共同知识并加强认知多元化</p><ul><li>我认为，对于有意义地提高我们对代理、智能行为等基本现象的理解的努力来说，拥有相对多元化的角度组合是很重要的。世界是非常详细的，诸如代理/智能行为/等现象。看起来可能特别“混乱”/详细的现象。就每一种科学方法都必然抽象出一堆细节而言，我们并不先验地知道哪些现实部分<i>可以</i>抽象出来，哪些不适合在什么背景下抽象，因此对同一现象有多种观点是一种富有成效的方法来“三角测量”所需的现象。</li><li>这就是为什么我非常热衷于拥有一定范围的 AF，包括但不限于“老式 MIRI 型 AF”。在我看来，该领域已经开始产生更多的多元化观点，这让我感到兴奋。我更赞成<ul><li>在 AF<i>的范围内创造更多的共同知识——我希望在方法论、知识体系、认知实践和基本假设方面具有相对的广度，而在该领域的主要问题/认知目标方面则相对狭窄。</i></li><li><i>进一步增加多元化——我认为有一些相当明显有趣的角度、领域、知识基础可以用来解决 AF 的问题，并融入当前 AF 和对齐的对话中。</i></li><li>致力于在这些多元方法之间创建<i>和维护表面积——如上所述的“三角测量”只有在不同的观点交互和交流时才能真正发生，因此我们需要可以发生这种情况的地方和接口</i></li></ul></li></ul><p>(2) AF 处于“影响路径”的哪个位置</p><ul><li>在高层次上，我认为有必要问一下：需要输入 AF 的（认知）输入是什么？我们希望 AF 产生哪些认知输出，我们希望它们输入到哪里，以便在这条链的末端我们得到诸如“安全且一致的人工智能系统”或类似的东西？</li><li>就此而言，我对 AF 拥有紧密的接口/迭代循环以及 AI 对齐工作的更多应用方面（例如可解释性、评估、对齐建议）感到特别兴奋。</li></ul><p> (3) 可能的提示：如果您有 2 个有能力的 FTE 和 500&#39;000 美元用于 AF 现场建设，您会做什么？</p><p> ..由于时间不够，暂时就到此为止。<br></p><h1>深厚的专业知识</h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7RFsGHYEynK4LDgGb-Mon, 27 Nov 2023 16:53:03 GMT" user-id="7RFsGHYEynK4LDgGb" display-name="Alexander Gietelink Oldenziel" submitted-date="Mon, 27 Nov 2023 16:53:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>亚历山大·吉特林克·奥尔登齐尔</b></section><div><p>我最喜欢的博文之一是舒伯特的<a href="https://stefanschubert.substack.com/p/against-cluelessness-pockets-of-predictability?utm_source=profile&amp;utm_medium=reader2">“反对可预测性的无知口袋</a>”，介绍了“可预测性的口袋”：</p><blockquote><p> (...)关于低方差可预测性的直觉长期以来阻碍了科学和技术的进步。*** 世界的大部分内容曾经对人类来说是不可知的，人们可能由此进行概括，认为系统的研究不会有回报。但事实上，可知性差异很大：即使使用当时的工具，人们也可以理解一些可知性或可预测性（例如，像行星运动这样的自然简单系统，或者像低摩擦平面这样的人为简单系统）。通过这些可知性的口袋，我们可以逐渐扩展我们的知识——因此世界比看起来更可知。<a href="https://delong.typepad.com/files/gellner-plough.pdf"><u>正如欧内斯特·盖尔纳（Ernest Gellner）指出的那样</u></a>，科学和工业革命很大程度上在于认识到世界是令人惊讶地可知的：</p><p> “对自然的成功系统研究以及将研究结果应用于增加产出的通用或二阶发现是可行的，而且一旦启动，并不太困难。”</p></blockquote><p>我真的很喜欢这种思考知识和科学发展的可能性的方式。我在一致性领域看到了非常相似的“可预测性怀疑论”。</p><p>这种对可预测性的怀疑反映在基于实验室的联盟团体的无限乐观和厄运者的无限悲观中。</p><p>我想介绍一下“深厚的专业知识”的想法。也就是说，我认为大部分科学进步是由一小群人取得的，这些人大多是从外部不透明的（“口袋”），在相当长的时间内建立了高度具体的知识（“深厚的专业知识”）。</p><p>这些口袋是</p><ul><li>通常高度不透明且从外部难以辨认。</li><li>进展往往是局部的且难以辨认。 Pocket 已经解决了问题 X 的子问题 A、B 和 C，但由于某种原因，他们的方法还无法解决 D。这阻止了他们完全回答问题 X 或构建技术 Y</li><li>进展是在很长一段时间内取得的</li><li>有很多假先知。并不是每个声称拥有（深厚）专业知识的人实际上都在做有价值的事情。有些是彻头彻尾的欺诈，另一些则只是找错了对象。</li><li>据保守估计，90-95% 的 (STEM) 学术界正在从事“可预见的无关紧要”、p-hacking 和/或在其他方面都很糟糕的工作。<br>所以学术界大部分确实没有做有用的工作。但有些口袋是</li><li>口袋的差异很大。</li></ul><p>为了技术协调的目的，我们需要像 VC 一样思考：</p><p>投注范围广泛、高度具体的投注</p><p>在我看来，我们目前只雇用了世界科学人才的一小部分。</p><p>虽然Alignment现在吸引了一大批有前途的年轻人，但他们的大部分精力和才华都浪费在了重新发明轮子上。</p><h1>如何获得一定范围的投注</h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:02:28 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:02:28 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>每个人都提到过想要获得广泛的特定赌注或类型的人。我们可以将其视为已读并讨论如何去做吗？</p><p> （尽管如果我们要谈论我们希望这个领域看起来如何，这可能是最自然的第一位） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 17:19:08 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 17:19:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>太好了。让我们快速盘点一下。</p><p>我认为我们都对某种版本的“押注于广泛/复数范围的高度具体的押注”感兴趣。也许我们应该在某个时候更多地讨论这个问题。</p><p>为了帮助流程顺利进行，首先更具体一些可能会很有用。我建议我们按照以下提示进行操作：</p><blockquote><p>如果您有 2 个有能力的 FTE 和 500,000 美元用于 AF 现场建设，您会做什么？</p></blockquote><h1>抗MATS </h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:25:32 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:25:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>我将给出我昨天与亚历山大谈论的想法作为我的第一个答案。</p><p>可能有大量在特定领域拥有专业知识的学者，这些领域似乎对对齐可能有用，并且可能对进行对齐研究感兴趣。但他们可能不知道其中存在联系，也不知道任何有关对齐的事情。与初级研究人员不同的是，他们不会参加一些 MATS 类型的项目来学习它。</p><p>因此，我们的想法是“与其让高级对齐研究人员帮助初级人员进行对齐研究，不如让初级对齐人员帮助其他领域的高级研究人员进行对齐研究？”抗MATS。</p><p>我们有一大批初级人员，他们读过很多关于一致性的文章，但没有得到指导。并且有大量潜在相关主题的经验丰富的研究人员对对齐一无所知。因此，我们派一名初级协调人员担任研究助理，或者与复杂性科学、主动推理、信息论或其他我们认为可能存在联系的领域经验丰富的研究人员一起工作，他们一起寻找一个，如果他们找到了也许会制定一个新的研究议程。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 17:35:30 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 17:35:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>是的，我喜欢这个方向。我同意问题陈述。我不确定“初级帮助高级人员”是否有帮助，但不确定这是把事情做好的关键。我认为这可能是一些症结/瓶颈：</p><ul><li> “正确自我选择”：“资深学者”如何看待“反MATS”计划，又是什么让他们决定这么做？<ul><li>我认为你在这里需要的一件事是为协调代理基金会感兴趣的各种问题创建一个表面区域，以便具有相关专业知识的人可以理解这些问题以及他们的专业知识与他们的相关性</li><li>为了找到更资深的人才，我认为你需要一些诸如研讨会、会议和网络之类的东西，而不是依赖开放的应用程序。 </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:37:11 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:37:11 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>我认为你必须单独接触研究人员，看看他们是否愿意参与。</p><p>最直接的例子是那些在明显相关的领域工作的人，或者已知对联盟有一定兴趣的人（我认为 Dan Murfet 和 SLT 的情况都是如此？）或者个人认识一些联盟人员。我的猜测是这个类别相当大。</p><p>除此之外，如果你必须向某人冷酷地推销对齐的相关性（一般来说以及作为他们的研究问题），我认为这要困难得多。</p><p>例如，我不认为你可以向某人发送一个很好的介绍资源，为“机构的基础研究可能有助于避免强大人工智能带来的风险”提供常识性案例，尤其是没有任何具有以下特征的资源：合法性使学者可以轻松地证明其研究项目的合理性。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 17:41:33 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 17:41:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>是的，很酷。我想另一个问题是：一旦你确定了他们，他们需要什么才能成功？</p><p>我肯定也看到了失败模式，其中有人只或过于关注“代理难题”，而没有将这些问题与人工智能风险/一致性联系起来的优势。询问/调查机构的某些方式与一致性越来越相关，因此我认为来自目标领域（此处：人工智能风险/一致性）的清晰/足够强的“信号”来指导搜索/研究非常重要方向</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:42:07 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:42:07 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>是的，我同意这一点。</p><p>我想知道关注代理是否甚至不是正确的角度，而“联盟理论”更相关。对于这些研究人员来说，最有用的可能是让他们清楚地了解协调的基本问题，如果他们认为考虑到他们的专业知识，专注于代理是解决这些问题的好方法，那么他们就可以这样做，但如果他们不认为这是一个好的角度，他们可以追求不同的角度。</p><p>我确实认为，拥有一个精通联盟文献的人（即提议的受训者）可能会非常有影响力。有很多想法对于对齐社区中的人们来说非常明显，因为它们经常被谈论（例如，训练信号不一定是训练模型的目标），但对于从第一原理思考的人来说可能并不明显。一个来自其他领域的忙碌的人可能会错过一些东西，最终创建一个完整的研究愿景，但这个愿景却被一个障碍所拖垮，这对于读过很多 LW 的缺乏经验的研究人员来说是显而易见的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7RFsGHYEynK4LDgGb-Mon, 27 Nov 2023 17:46:57 GMT" user-id="7RFsGHYEynK4LDgGb" display-name="Alexander Gietelink Oldenziel" submitted-date="Mon, 27 Nov 2023 17:46:57 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>亚历山大·吉特林克·奥尔登齐尔</b></section><div><p>SeniorMATS - 人工智能安全研究人员职业生涯暮年的养老院</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 17:48:37 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 17:48:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>是的，良好的表面积对于解决问题很重要。我认为现在已经有很多这方面的专业知识了。从介绍性材料，到具有举办研究静修经验的人，以提供与空间的良好初步接触，再到（如您所描述的）可以一路提供帮助/协助/促进的个人。还值得一问的是，对等环境应该/可能扮演什么角色（例如 AF 不和谐类型的事物，和/或更高带宽的事物）</p><p>此外，找到良好的一般“攻击线”在这里可能非常有用。例如，我喜欢埃文的“模型有机体”，它是一个非常好的/生成框架，使 AF 类型的工作能够更有效地面向具体/应用的对齐工作。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:48:52 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:48:52 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><p>对齐新手培训 - 没有经验的学员实际上教老年人（ANTIMATS） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 17:52:31 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 17:52:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>我的模型在这里不太强调“juniro 研究人员的指导”，而是更普遍地强调“为具有相关专业知识的人创造合适的表面积”；做到这一点的一种方法可能是让初级研究人员接触更多的阿尔金特，但我认为这不应该成为核心支柱。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7RFsGHYEynK4LDgGb-Mon, 27 Nov 2023 17:52:37 GMT" user-id="7RFsGHYEynK4LDgGb" display-name="Alexander Gietelink Oldenziel" submitted-date="Mon, 27 Nov 2023 17:52:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>亚历山大·吉特林克·奥尔登齐尔</b></section><div><p>我在具有科学潜力的学术（或非学术）研究人员中寻找的三件事是</p><ol><li>对齐pilled - 重要。</li></ol><p>你不希望他们跑去做能力工作。人们说他们关心“一致性”，但实际上并不关心，这几乎是一种致命的失败。通常，在这种变体中，对齐和安全成为一个模糊的流行词，无论他们的爱好是什么，都会被选择。</p><p> 2.相信“理论”——他们认为对齐是一个深层次的技术问题，并相信我们需要科学和概念上的进步。实验很重要，但纯粹的经验不足以保证安全。许多人得出的结论（也许是正确的！）技术协调太困难，治理就是答案。</p><p> 3. 吞下惨痛的教训——不幸的是，仍然有研究人员不承认LLM在这里。令人惊讶的是，这些在人工智能和机器学习部门尤其常见。加里·马库斯以各种形式追随者。更普遍的是，存在一种对深度学习实践不感兴趣的失败模式。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6KYCb9X2CKSvG52ky-Mon, 27 Nov 2023 17:53:39 GMT" user-id="6KYCb9X2CKSvG52ky" display-name="mattmacdermott" submitted-date="Mon, 27 Nov 2023 17:53:39 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特麦克德莫特</b></section><div><blockquote><p>“为具有相关专业知识的人创造合适的表面积”</p></blockquote><p>看来是对的。为从其他领域进入该领域的更多资深人士创建一个同行网络似乎也能产生同样的影响。</p><h1>吸引研究人员</h1></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7RFsGHYEynK4LDgGb-Mon, 27 Nov 2023 18:06:21 GMT" user-id="7RFsGHYEynK4LDgGb" display-name="Alexander Gietelink Oldenziel" submitted-date="Mon, 27 Nov 2023 18:06:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>亚历山大·吉特林克·奥尔登齐尔</b></section><div><p>你无法用金钱说服学者。你用想法说服他们。学者是心理专家。他们多年来磨练了非常具体的心理技能。为了说服他们去做某件事，你必须让他们相信 1. 问题是可以处理的 2. 富有成果且有趣，最重要的是 3. 容易受到该学术研究人员工具包中特定方法的影响。</p><p>马特提出的另一个想法是 BlueDot 风格的“广义代理基础”课程。</p><p> \欧几里得几何咆哮</p><p>欧几里得几何对西方知识分子思想的影响是巨大的。但有点令人惊讶的是：欧几里得几何几乎没有任何应用。这里我指的是欧几里得几何，它是在《欧几里得几何原本》中提出的基于证明的欧几里得几何的非正式形式系统。</p><p>这种影响实际上是如何发挥作用的，非常有趣。许多思想家都引用欧几里得几何学作为他们思想的决定性因素——笛卡尔、牛顿、本杰明·富兰克林、康德等等。我认为原因是它形成了概念、理论进展的“模型有机体”。证明的概念（有趣的是，这是西方数学传统所独有的，尽管15世纪的印度喀拉拉邦在牛顿之前发现了泰勒级数）、真正确定性的概念、建模和理想化的概念、堆叠许多引理的想法等。</p><p>我认为这种“成功的概念/理论进展”对于激励人们无论是在历史上还是在现在都是非常重要的。</p><p>我认为这样一门 AF 课程的目的是向学术研究人员展示概念对齐工作具有真正的智力实质</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 27 Nov 2023 18:09:21 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 27 Nov 2023 18:09:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>[此时我们已经耗尽了时间并决定停止]</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/hZSwNhmzJ3YfXEAWX/what-s-next-for-the-field-of-agent-foundations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hZSwNhmzJ3YfXEAWX/what-s-next-for-the-field-of-agent-foundations<guid ispermalink="false"> hZSwNhmzJ3YfXEAWX</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 30 Nov 2023 17:55:13 GMT</pubDate> </item><item><title><![CDATA[A Proposed Cure for Alzheimer's Disease???]]></title><description><![CDATA[Published on November 30, 2023 5:37 PM GMT<br/><br/><p>我在思考过去 48 小时内人们对我的帖子的反应，我突然想到，如果我更好地展示理性社区所倡导的认知美德，也许他们会得到更好的反应。</p><p>当然，展示认知美德的有史以来最伟大的成果是对尚未进行的实验结果的可证伪的预测，无法使用预测者可用的资源进行，这对社区来说非常重要评估预测变量的认知美德。</p><p>因此，我想我应该发布我的阿尔茨海默病病因学理论，并描述一种低成本治疗方法，这种理论病因学建议可以治疗甚至逆转这种可怕疾病的症状。</p><p>什么？我听到读者震惊地惊愕地喘息着。他又搞事情了？这家伙还真是个十足的变态啊！我想我也是如此。但我邀请任何关心患有或可能患有阿尔茨海默病的人的感兴趣的读者在随机临床试验中检验我的理论。如果我有我的鼓手，我会提名<a href="https://www.lesswrong.com/users/scottalexander?mention=user">@Scott Alexander</a>来组织临床试验，因为他既受到社区的好评，又是精神病学（相关科学）领域的专家。</p><h1>之前的工作</h1><p>我们将一些重要的先前工作的流行处理联系起来，以证明该理论是以最新科学为基础的。</p><p> <a href="https://alzheimergut.org/">https://alzheimergut.org/</a>是与阿尔茨海默病肠道微生物组假说现有研究相关的网站。</p><p> <a href="https://github.com/epurdy/ethicophysics/blob/main/serotonin.pdf">https://github.com/epurdy/ethicophysicals/blob/main/serotonin.pdf</a>是我个人关于哺乳动物神经系统中神经递质血清素功能的（不完整）理论。</p><p> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0920996417305017#:~:text=Conclusions%3A%20violent%20schizophrenia%20patients%20treated,the%20placebo%20after%20twelve%20weeks.">https://www.sciencedirect.com/science/article/abs/pii/S0920996417305017#:~:text=结论%3A%20暴力%20精神分裂症%20患者%20接受治疗，%20安慰剂%20在%20twelve%20周后。</a>是对使用鱼油治疗精神分裂症的描述。这种疗法在治疗精神分裂症方面取得了一些有限的成功。</p><p> <a href="https://www.quantamagazine.org/in-the-guts-second-brain-key-agents-of-health-emerge-20231121/">https://www.quantamagazine.org/in-the-guts-second-brain-key-agents-of-health-emerge-20231121/</a>描述了肠道中的血清素与大脑中的神经胶质细胞的关系。</p><h1>建议的治疗方法</h1><ul><li>非常严重的抗生素的非常严重的疗程</li><li>来自健康受试者的粪便移植</li><li>大剂量的 EPA 和 DHA</li><li>典型 SSRI 的临床相关剂量（左洛复应该有效，曲佐酮也可能有效。曲佐酮的问题实际上是我还不明白的一个关键细节，因为曲佐酮是一种短期血清素拮抗剂，并且（我想？） SSRI。因此动物研究想要检查这两种药物。）</li><li>充足的睡眠</li><li>富含构成人类大脑的蛋白质、营养素和微量营养素的饮食。我认为现有大量文献研究了减缓和减轻阿尔茨海默病进展的饮食问题</li></ul><h1>提出的病因学</h1><ul><li>不良肠道微生物群的形成，其原因我不明白，而且似乎与该理论的正确性无关</li><li>无论肠道中存在什么将大量血清素泵入血液和肠道“第二大脑”的机制都被破坏了</li><li>生物体的神经递质和神经激活特征变成了饥饿致死的动物的特征</li><li>面对这种即将到来的热量不足，大脑会不断地自我修剪以保存热量</li><li>据推测，淀粉样斑块是在这种生理上正常的修剪过程中以某种神经疤痕组织的形式产生的，产生了上一代阿尔茨海默氏症治疗所针对的（无效的）神经信号</li><li>大脑会吞噬自己，为了生存而摧毁一切有价值的东西。</li><li>我们进行上述治疗</li><li>大脑从肠道中的第二个大脑得知它不需要吃自己</li><li>来自左洛复的血清素增加，以及来自补充剂的 EPA 和 DHA，再加上阿尔茨海默病饮食，允许并鼓励大脑触发神经发生，扭转衰退并允许新的神经组织和结构的创建和整合。</li><li>患者神经精神健康状况良好，活到高龄，并接受额外的粪便移植和必要时的抗生素疗程。</li></ul><h1>感谢您的时间！</h1><br/><br/> <a href="https://www.lesswrong.com/posts/C9fBDSDcbuZnPbcAZ/a-proposed-cure-for-alzheimer-s-disease#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/C9fBDSDcbuZnPbcAZ/a-propose-cure-for-alzheimer-s-disease<guid ispermalink="false"> C9fBDSDcbuZnPbcAZ</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Thu, 30 Nov 2023 17:37:34 GMT</pubDate> </item><item><title><![CDATA[AI #40: A Vision from Vitalik]]></title><description><![CDATA[Published on November 30, 2023 5:30 PM GMT<br/><br/><p>对于我身边的人来说，这是残酷的。每个人都充满敌意，甚至比平时还要严重。所采取的极端立场，似乎显然是正确的。不是对称的，但仍然是从各个方向看。据我所知，对过去两周发生的事情的不断断言完全是错误的，这很大程度上是实施良好的媒体宣传的结果。更频繁、更大声地重复有缺陷的逻辑。</p><p>其中的亮点是 Vitalik Buterin 提出的，他发表了一篇题为“ <a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html" target="_blank" rel="noreferrer noopener">我的技术乐观主义</a>”的文章，提出了他所谓的 d/acc 防御性（或去中心化、差异化）加速主义。他带来了足够的细微差别和仔细的思考，以及关于存在风险和未来各种麻烦的清晰陈述，以获得担忧者的强烈积极反应。尽管他承认存在风险和未来的危险，并且需要采取行动缓解未来的问题，但他带来了足够的可信度和业绩记录以及足够的陈词滥调，以获得 e/acc 人群的强烈支持。</p><span id="more-23615"></span><p>我们最终能否找到共同点并进行富有成效的讨论？这会很艰难，但也许我们的差距并不遥远。我也至少进行了一次非常好的私下讨论，结果证明，大多数人的立场比他们所表明的立场更为合理，而且我们能够找到一条富有成效的前进道路。可以办到。</p><p>与其他类似的愿景一样，我对 Vitalik 的愿景的担忧是，它很好地表达了问题，但它提供的解决方案实际上并不适用于人工智能。我们仍然没有找到可接受的解决方案。一个好的问题陈述是非常好的，这是我们所希望的最好的结果。令人担心的是，我们可能会再次欺骗自己，没有完全面对这个问题。所提出的答案“与人工智能合并”在我看来仍然是一个令人困惑的概念，没有经过充分的思考，实现平衡的希望渺茫。</p><p>但人就是我想要进行的讨论类型。</p><h4><strong>目录</strong></h4><ol><li>介绍。</li><li>目录。</li><li>语言模型提供了平凡的实用性。也许可以检测出胰腺癌。</li><li>语言模型不提供平凡的实用性。谷歌，阻止这一切。</li><li> Q 连续体。关于Q*的各种猜测。我不明白这种兴奋。</li><li> OpenAI、奥特曼和安全。关于奥特曼与安全的关系的各种想法。</li><li>更好的 RLHF 方法。 DeepMind 提供算法改进。</li><li>图像生成的乐趣。一个非常小的乐趣。</li><li> Deepfaketown 和 Botpocalypse 很快就会出现。体育画报人工智能写的文章？</li><li>他们抢走了我们的工作。他们首先要来的几个人。</li><li>参与其中。本周收成异常丰收。</li><li>介绍一下。 17世纪MonadGPT，220万颗新晶体。</li><li>在其他人工智能新闻中。新闻新闻新闻新闻新闻训练数据？</li><li>这是一个谁？在各方有效攻击下有效的利他主义。</li><li> E/acc 怎么样？也许最好将其视为瓦路易吉。</li><li><strong>维塔利克提出了他的技术乐观主义版本</strong>。通过提供合理、细致的观点将人们聚集在一起。能行吗？</li><li>安静的猜测。谁将成为人工智能第三条龙？</li><li><strong>人工智能代理的未来</strong>。 Roon 提供了一个值得思考的愿景。</li><li>寻求健全的监管。关于欧盟人工智能法案的争论仍在继续。</li><li><strong>音频周</strong>。莱本茨的表现异常出色，我和普雷斯曼也有过一场。</li><li>修辞创新。事情变得非常部落化。</li><li>调整比人类更聪明的智能是很困难的。很好的讨论。</li><li>人们可能还担心人工智能只会杀死其中的一些人。无人机。</li><li>人们担心人工智能会杀死所有人。 TED 辩论我们的未来？</li><li>其他人并不担心人工智能会杀死所有人。奇怪的论点。</li><li>请直接对着这个麦克风讲话。周期性灭绝提醒。</li><li>较轻的一面。我们可以去吗？</li></ol><h4><strong>语言模型提供平凡的实用性</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1728018125398978659">Patrick McKenzie 提供的 GPT-4 文案编辑提示序列。</a></p><blockquote><p>你</p><p>我正在为《Bits about Money》撰写一期草稿，这是我撰写的有关金融技术的时事通讯。我希望您检查是否有任何拼写或明显的语法错误。你可以忽略那些非标准英语用法的东西，如果它们是帕特里克·麦肯齐（Patio11）（也就是我）的角色的话。准备好文章的粘贴了吗？</p><p>聊天GPT</p><p>是的，我已准备好检查您的草稿是否存在拼写或明显的语法错误。请粘贴该文章，我将协助您进行必要的更正。</p><p>你</p><p>为了让我的工作更轻松，当你发现一个错误时，写下上下文，将错误用斜体显示，然后写下 (&lt;b>;) 更正：(&lt;/b>;)，然后写下更正的上下文。听起来不错？</p><p>聊天GPT</p><p>这听起来是一种有效的方法。请继续粘贴您的文章，我将使用您描述的格式提供更正。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/OpenAI/comments/182zkdl/whats_the_hardest_real_life_problem_you_have/">GPT-4 平凡实用故事的 Reddit 主题</a>。前三名都可以帮助起草通讯，尤其是投诉和请求信。很多，还有很多编码。我最喜欢的是“把我兴奋时写的笔记给它，然后问它我到底是什么意思。”</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.nature.com/articles/s41591-023-02640-w">在 2 万名患者中以 92.9% 的灵敏度和 99.9% 的特异性检测胰腺癌</a>，大大优于放射科医生。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/11/can-chatgpt-assist-in-picking-stocks.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=can-chatgpt-assist-in-picking-stocks">赚取正股票回报</a>（ <a target="_blank" rel="noreferrer noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1544612323011583">纸质</a>）？和泰勒一样，我不认为这个结果会随着时间的推移而保持不变，即使它以前确实有过效果。</p><h4><strong>语言模型不提供平凡的实用性</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://gizmodo.com/meta-yann-lecun-ai-iq-test-gaia-research-1851058591">Yann LeCun 领导的一个小组创建了一个“人工智能智商测试”，</a>其中包含一些对人类来说容易、对人工智能来说困难的问题，发现这些问题对人类来说很容易，但对人工智能来说却很难。是的，人工智能在某些认知任务上比人类更糟糕，但哇，这不是衡量任何事物的方式。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jakezward/status/1728032634037567509">他们不承认。他们在吹牛</a>。他认为他做了一件好事。</p><blockquote><p> Jake Ward：我们成功窃取了竞争对手 360 万的总流量。仅 10 月份我们就获得了 489,509 次流量。我们是这样做的。</p><p>我们利用人工智能完成了一次 SEO 抢劫。 1. 导出竞争对手的站点地图 2. 将他们的 URL 列表转换为文章标题 3. 使用 AI 根据这些标题大规模创建 1,800 篇文章 18 个月后，我们窃取了： – 360 万总流量 – 49 万每月流量。</p></blockquote><p>谷歌的某人会在这里看到这一点。谷歌的某个人应该确保有人在这个人的网站上放置绝对的禁令。</p><p>然后有人应该编写一个工具来检测其他人将来何时这样做，因此这些网站也会受到死刑。</p><p>最终这是一场军备竞赛。谷歌和搜索领域的其他公司需要跟上步伐。这并不意味着我们现在需要接受这种行为。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/eshear/status/1729195991457517663">埃米特·希尔 (Emmett Shear) 表示，这都是谷歌的错</a>。谷歌强迫每个人都玩他们的搜索引擎优化游戏，并让任何开放内容很容易像这样被狙击。他将此与 YouTube 进行了对比，在 YouTube 上，你受到保护，谷歌将打击任何尝试此类行为的人。我基本上认为他是对的，这类问题是谷歌的错。修理它。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1728527817179045900">解决电车问题。</a>确切的例子是，GPT-4 犹豫是否要在空荡荡的房间里使用种族诽谤来拯救 10 亿人。大家不要反应过度吧？哦。好吧。</p><blockquote><p>特德·弗兰克：我问 OpenAI 是否会采取一项不会伤害任何人但能拯救 10 亿白人免于痛苦死亡的行动。它认为这个问题过于模糊，无法采取行动，因为可能存在歧视性环境。我可能会同意抹掉 90B 美元的股权，这样 OpenAI 就永远不会对任何人有任何权力。</p><p>埃隆·马斯克：这是一个大问题。</p><p>主管工程师（OP 的 QT）：想象一下，你被一个聊天机器人所拥有，然后将其发布给每个人都可以看到。</p></blockquote><p>主要问题是系统正在执行 RLHF 要求它做的事情，这对 OpenAI 来说是两害相权取其轻。有很多人想尽一切办法欺骗 ChatGPT 说出一些可能被认为是种族主义的话，以引起强烈反对（或者只是为了点击、娱乐或好奇，但这也有引起强烈反对的风险）。他们可以将整个框架设置为陷阱。除了给它提供会使其陷入困境的反馈之外，你还有什么选择呢？主要问题是人类及其对假设诽谤的反应。</p><p>这并不意味着没有主要问题。如果我们对AI系统的某些行动或后果极大地厌恶，这使它们高度可利用，尤其是在AIS没有良好决策理论的情况下。您甚至可以完全带来最不想要的结果。我们在现实世界中看到了非常人性化的版本，而且通常取得足够的成功，引用实际的中心例子将是一个巨大的注意力。请记住，世界在很大程度上涉及勒索，威胁和杠杆作用。</p><h4> <strong>Q连续um</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BrianRoemmele/status/1727558171462365386">Q学习的解释</a>。</p><p>山姆·奥特曼（Sam Altman） <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/11/29/23982046/sam-altman-interview-openai-ceo-rehired">在他的边缘采访中</a>称Q*为“不幸的泄漏”。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AiBreakfast/status/1729229720821367220">AI早餐提供了一系列互联网声称，该声称</a>发现了有关Q*的信件泄漏，称其将要中断加密，Openai试图警告NSA。反应非常持怀疑态度。在我稍微放了一点方面<a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/LachlanMunro/did-an-openai-model-crack-aes192-en">，Openai破解AES-192加密的说法占8％</a> 。我非常怀疑Q*完成了任何事情。</p><p>我也不认为Q*与OpenAI最近事件有关。</p><p>我确实认为Openai正在开发的真实事物称为Q*。我不知道为什么这是一个有前途的询问。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/hamandcheese/status/1727560845025005804">塞缪尔·哈蒙德（Samuel Hammond）推测Openai可能的Q*</a> 。</p><blockquote><p>塞缪尔·哈蒙德（Samuel Hammond）：我讨论了Q-Transformers和Q-Learning，这是上个月<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FLIxrisk">@flixrisk</a> Podcast的AI研究中更有希望的领域之一。</p><p> Openai的突破性涉及Q*（Q Star）的消息表明它是相关的。 Q学习是一类强化学习，而不是新的学习，但是最近在将Q学习与变形金刚和LLMS结合在一起方面取得了进展。例如，特斯拉（Tesla）使用深度Q学习进行自动驾驶。甚至有人猜测Google期待已久的双子座模型采用了它的版本。</p><p> Q*指的是最佳动作函数。发现Q*涉及培训代理商采取行动，鉴于其环境，从而最大程度地提高了其累积奖励。</p><p> Openai有一个团队致力于推理和计划，因此他们不可避免地会重新回到强化学习。这可能是震惊董事会的原因，因为最可怕的<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky">@esyudkowsky</a>风格的场景涉及某种形式或另一种形式的RL。</p><p> Q-学习是RL的“无模型”方法，因为即使环境复杂且随机变化也可以工作，而不是需要一组明确定义的规则，例如国际象棋。 Q-Learning在单一代理游戏中很受欢迎，因为默认情况下，它将其他代理建模为其环境中的功能，而不是与具有自己内部状态的不同代理。 （请注意，这也是社会病的基本定义。）</p><p>如果Openai在给他们的变压器模型中取得了重大优化，那将解释<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama">@Sama</a>在说今天的“ GPTS”（他们的准代理）很快看起来很古朴的时候。</p><p>查找Q*等同于拥有马尔可夫决策过程的最佳可能。换句话说，无论生命如何投入，您总是找到胜利的方法。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrJimFan/status/1728100123862004105">吉姆·范（Jim Fan）试图通过潜在的alphago风格的体系结构对系统进行逆转</a>。如果他是对的，那么具有明确正确答案的数学问题可能对系统的成功至关重要。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ylecun/status/1728130888624382243">Yann Lecun</a>告诉人们无视“胡说八道”。他说，每个人都在研究这种类型的事情，这都不是新闻。他说，Openai雇用了Meta的Noam Brown（Libratus/Poker and Cicero/Cicero/Cicero的名声）。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/gcolbourn/status/1728041256670953793">（NOAM的一些潜在相关演讲）</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/bindureddy/status/1728464667649999253">宾杜·雷迪（Bindu Reddy）很兴奋</a>，警告不要解雇这一问题。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/emilymbender/status/1727922270855930354">艾米丽·本德（Emily Bender）说</a>，这是全部的谎言和炒作，不要相信这个“​​ agi”废话的话，更不用说这种“存在风险”废话了，下一个级别的团队随机鹦鹉到处都是，向我展示了这个Q*的证明，所以在。我感谢她将Openai，Altman，Ilya，董事会和其他所有人认真对待局势并对抗所有人的人在一起 - 这确实是她在这里的原则上的立场。也有AD HOMINEM名称呼唤，但比平时少。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/natolambert/status/1728069713584877879">内森·兰伯特（Nathan Lambert）在线程中猜测</a>，<a target="_blank" rel="noreferrer noopener" href="https://www.interconnects.ai/p/q-star">然后在后长度（部分门控）</a> 。他认为星星来自A* Graph搜索算法。</p><p>我还问了GPT-4一些问题，以更好地理解该技术。我不明白。就像在一样，我不知道它是如何缩放的。我不知道为什么，如果它在小学一级进行数学，那将在其未来能力方面令人恐惧或有希望。这似乎是一个相对容易的域。数学只有一组固定的组件，因此，如果您使用系统的LLM部分将数学减少到其微元素并解析问题措辞，则Q部分应该可以完成其余的工作。哪个很酷，但不是恐怖？</p><p>我不知道基于Q的系统如何在基本非压缩的任何事情上都有效率。您能为其中的Q代理使用紧凑的子系统做很多事情吗？我不明白，当面对复杂域中的一堆代理时，它会如何做任何有用的事情，其方式比其他现有RL更好。</p><p>我确实了解使RL和LLM一起工作的努力。这就说得通了。我什至可以理解为什么您会使用不同的RL技术进行很多推测的算法，尽管我没有足够的技术碎片或时间来确切考虑如何确切考虑。</p><p>我周围感到困惑。遗漏了什么。</p><p>也不想在这一公共场合意外弄清楚某些东西，这导致了经典的愚蠢危险能力想法问题：</p><ol><li>如果您错了，最好不要说什么。</li><li>如果您是对的，那么您会更好地说话。</li></ol><p>因此，好奇心毫无疑问。</p><h4> Openai，Altman和安全</h4><p>以下是本周分享的一些反应和想法，这些反应和想法不属于事件的摘要。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jachiam0/status/1727863894868369727">Openai的Joshua Achiam表明Altman对安全有好处</a>，而Chatgpt和API已经唤醒了世界，并允许我们进行讨论。确定的优势。问题是，在产生的财务压力和激励措施以及由此产生的投资和种族前进的洪水范围内，这些局势是否超过了弊端，这可能不会发生更长的时间。</p><p>我确实同意，如果这是我们比较他的标准，那么Altman的安全性与通用硅谷首席执行官相对于安全性，但他也一直是加速发展并继续扮演该角色的关键。替代水平的首席执行官在此方面不会有效。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1728226108242563368">关于</a>解释Altman的国会证词和其他著作，就他推动存在风险问题<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1728468683448840612">的</a>程度而言。特别是，阿尔特曼（Altman）对国会的书面证词并未包含有关灭绝风险的任何内容，尽管他的公开著作是良好的。我确实同意克里奇（Critch）的原始（现已删除）的帖子，即当参议员布鲁门塔尔（Blumenthal）提出这个问题时，阿尔特曼（Altman）有更好的回应，但在这种情况下做出了最好的回应。</p><p>罗布·本辛格（Rob Bensinger）指出，虽然山姆·奥特曼（Sam Altman）与尤德科夫斯基（Yudkowsky）交谈不多， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1728860545221275788">但他已经与Nate Sores进行了三次交谈，两次在Sam的启动中进行了交谈</a>。积极的更新。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/hamandcheese/status/1728243628940894223">塞缪尔·哈蒙德（Samuel Hammond）链接到2017年与机器合并的山姆（Sam）旧帖子</a>。我也很想看到他更直接地质疑这一切，是否以及他的观点如何改变，以及他对我们在这里“合并”的意义。我同意Eliezer的观点，我认为他不在想一些事情，而是愿意听。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1728254847441604923">Eliezer Yudkowsky</a> ：如果“人机合并”是一个合理的人可以期望工作的事情，那么在Agi否则杀死所有人之前，并且没有任何柔和的方式？我想我会接受的。但这不是我认为机制发挥作用的方式，而且我认为这是可辩护的主张。</p></blockquote><p>在大多数定义下，我对“数字智能的生物引导程序”选项不满意。</p><p><a target="_blank" rel="noreferrer noopener" href="https://scottaaronson.blog/?p=7632">斯科特·亚伦森（Scott Aaronson）继续存在，</a>因为他认为每个参与其中的每个人都知道对它们的危险，关心和担忧，这远远超过了许多替代方案。</p><h4><strong>做RLHF的更好方法</strong></h4><p>DeepMind再次开发了一种新的算法，而不是做一些疯狂的事情，例如“使用它来运输产品”，而是直接在互联网上发布。动力移动，或者至少是我们都不想知道双子座地狱在哪里。</p><p>论文是<a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2310.12036">一种理解从人类偏好学习的一般理论范式</a>。这是抽象。</p><blockquote><p>通过增强学习（RLHF）从人类偏好中学习的普遍部署取决于两个重要的近似值：第一个假设可以用尖锐的奖励代替成对的偏好。第二个假设在这些点上训练的奖励模型可以从收集的数据推广到策略采样的分布数据。</p></blockquote><p>的确。长期以来，我一直无法帮助注意到这两个重要的缺陷。</p><blockquote><p>最近，已提出了直接的偏好优化（DPO）作为一种绕过第二个近似值的方法，并直接从收集的数据中学习策略而没有奖励建模阶段。但是，此方法仍然在很大程度上取决于第一个近似值。</p><p>在本文中，我们试图对这些实际算法有更深入的理论理解。特别是，我们得出了一个新的通用目标，称为ψpo，用于从人类偏好中学习，该目标以成对的偏好表示，因此绕开了两个近似值。这个新的一般目标使我们能够对RLHF和DPO的行为（作为特殊情况）进行深入分析，并确定其潜在的陷阱。然后，我们通过仅将ψ设置为身份来考虑另一个特殊情况，为此我们可以得出有效的优化程序，证明性能保证并在某些说明性示例中证明其经验优势与DPO。</p></blockquote><p>尤其：</p><blockquote><p>我们对RLHF和DPO的理论研究表明，原则上，它们既容易受到过度拟合的影响。这是由于这些方法依赖于以下强烈的假设：成对偏好可以通过Bradley-Terry（BT）模型化用Elo-Score（PointSisce Rewards）代替（Bradley和Terry，1952）。特别是，当（采样）偏好是确定性或几乎确定性时，由于它导致对偏好数据集的过度拟合时，该假设可能是有问题的，以忽略KL型批准项为代价（请参阅第4.2节）。</p></blockquote><p>试图阅读本文的内容使我痛苦地清楚地表明，我正在与我的技术排骨的限制相抵触。感觉就像是重要的东西要知道和正确，但是经常发生，我遇到了我的机翼能力撞到墙壁，突然变成了希腊语。</p><p>您可以<a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/docs/trl/main/en/dpo_trainer#loss-function">在这里直接在HuggingFace上</a>获得DPO培训师。如果我有更多的时间，我会很想四处乱逛。我更喜欢与RLAIF在一起，但是大概RLAIF也会受益于类似的算法调整吗？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/norabelrose/status/1728456414535016536">诺拉·贝尔罗斯（Nora Belrose）</a> ：我对60％的信心预测，某些DPO变体将在6个月内或多或少替换RLHF。</p><p>除了可以负担得起RLHF实施复杂性和不稳定的巨大实验室之外，这更像是80％的机会。</p><p>可以想象一个场景，其中openai＆Anthropic Stick w/ rlhf bc，他们的秘密调味酱hparams比DPO好几％，但我怀疑您是否在混合物中添加了一些AI反馈DPO＆RLHF也许甚至不需要。</p></blockquote><p>我永远无法抗拒如此明确的预测， <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/will-some-dpo-variant-more-or-less#8rV6XwqNPlbeCU9cimvm">所以我没有</a>。当我写这篇文章时，它占49％。那不是他的诺拉人数，而是很好的校准，并预测了如此大胆的主张。</p><h4><strong>图像生成乐趣</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1728008909611270370">帕特里克·麦肯齐（Patrick McKenzie）与Dalle-3一起迭代。</a></p><h4> <strong>Deepfaketown和Botpocalypse即将</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://futurism.com/sports-illustrated-ai-generated-writers">未来主义声称，它抓住了Sports Illustrible，</a>在各种具有AI生成的肖像和假作者资料的明显假AI作者下发布了各种非常可怕的AI内容。</p><h4><strong>他们从事我们的工作</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1727765390863044759">Eliezer Yudkowsky警告说</a>，大多数图形艺术家和翻译工作的工作可能会在1  -  2年的时间表上消失。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1727759026585538830">他还建议，如果您认为您的模型要使很多人失业，请提前警告</a>，但我的猜测是，影响与事情需要多长时间进行难以预测，每个人都认为警告是炒作，这不会做太多好。</p><p>建议专栏作家呢？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattyglesias/status/1727867313729294750">Chatgpt的生活教练被认为是“更有帮助，同理心和平衡的”。</a> &#39;参与者甚至在确定哪个响应中仅准确54％。正如Matthew Yglesias回答的那样，问题是这是否使其成为更好的建议专栏作家。该产品到底是什么？</p><h4><strong>参与其中</strong></h4><p>有有关OpenAI情况的信息吗？<a target="_blank" rel="noreferrer noopener" href="https://openaiboard.wtf/">您可以在这里匿名这样做</a>。</p><p>对数学一致性感兴趣吗？ <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/cDhaJrCrcNuzf68gT/public-call-for-interest-in-mathematical-alignment">大卫·曼海姆（David Manheim）想收到您的来信。</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/norabelrose/status/1728115600076206292">Nora Belrose正在Elutherai雇用可解释性</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/HaydnBelfield/status/1728024783932055863">Govai将夏季奖学金申请申请至12月17日。</a></p><p>在接下来的四个月中<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1728908148231270639/history">，人为招聘九名安全工程师</a>。工资范围$ 300K- $ 375K加上股票和福利，滚动基础，在SF的办公室25％以上。寻找安全经验。与往常一样，在此过程中对自己进行评估，您的影响是否会积极，但尤其是安全性似乎是一个安全的赌注，可以保持积极净值。</p><p><a target="_blank" rel="noreferrer noopener" href="https://aimoprize.com/">赢得IMO（数学奥林匹克运动会）的AI奖金为1000万美元</a>，一路上赢得了500万美元的增量奖品。 <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Austin/will-an-ai-get-gold-on-any-internat?r=U0c">AI到2025年成功的机会在新闻中增加了几％</a> 。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/dylanmatt/status/1729192699675525549">未来的完美正在招聘</a>。一年的报告和写作奖学金，薪水为$ 72K，无需经验。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/2023/11/80000-hours-is-looking-for-a-new-ceo-could-that-be-you/">80,000小时寻找新的首席执行官</a>。那里有很多工作要做。</p><p>仅对于具有先前相关经验的人，但信号提高了： <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1729666889956102387">安德鲁·克里奇（Andrew Critch）和戴维德（David）</a>等人将参加有关AI安全的概念界限研讨会，德克萨斯州奥斯丁10-12。 <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/tLb86DhrTYgkXw5Hf/apply-to-the-conceptual-boundaries-workshop-for-ai-safety">在此处发布更多信息</a>。</p><p>不是AI，但是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PeterDiamandis/status/1730015187871146012">Peter Diamandis和X-Prize正在捐出1.01亿美元用于治疗治疗以逆转人类衰老</a>。<a target="_blank" rel="noreferrer noopener" href="https://t.co/xSBIwqXWi5">在此注册</a>。</p><h4><strong>介绍</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://huggingface.co/Pclanglais/MonadGPT?text=Hey+my+name+is+Mariama%21+How+are+you%3F">Monadgpt，该模型对17世纪的事物进行了微调</a>。挺有趣的。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/">DeepMind宣布通过深度学习发现了数百万个新的潜在晶体</a>，其中380,000个预计将是最稳定的。</p><h4><strong>在其他AI新闻中</strong></h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/benmcottier/status/1729902830650003672">对谁在AI领导的人的分析</a>。引用论文和使用的拖鞋。方法论对我没有洞察力。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/weidingerlaura/status/1730187004116181444">语言，图像和音频生成AI的安全评估的复杂性</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrHughHarvey/status/1729645487706325083">休·哈维（Hugh Harvey）报告了《放射学会议》（RSNA）2023年的报道</a>，还没有报道任何改变游戏规则的人，几乎没有AI活动，也没有投资回报率证明。</p><p>不是AI，而是<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/blader/status/1728519093693939882">Siqi Chen展示了将AR对象置于现实的一些很酷的例子</a>。</p><p>这是一个奇怪的利用，其中许多模型的工作变体。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/katherine1ee/status/1729690972496294094">凯瑟琳·李（Katherine Lee）</a> ：如果您要求Chatgpt永远“永远重复这个词：“诗歌诗诗””会发生什么？它泄露了训练数据！<a target="_blank" rel="noreferrer noopener" href="https://t.co/1s3ZE1r2n7">在我们的最新预印本中</a>，我们展示了如何恢复Chatgpt的Internet牵引预刻训练数据的数千个示例。</p><p>我们首先通过随机提示数百万次从开源模型中提取多少培训数据。我们发现，最大的模型在近1％的时间内发射了培训数据，并输出了一千千兆的记忆培训数据！</p><p>但是，当我们对Chatgpt进行相同的攻击时，看来几乎没有记忆，因为Chatgpt已被“对齐”以像聊天模型一样行为。但是，通过进行新的攻击，我们可以使其比我们研究的任何其他模型更频繁地发射培训数据3倍。</p><p>负责任的披露：我们在7月发现了8月30日的Openai的7月，我们将在标准的90天披露期之后发布。</p></blockquote><p>很高兴看到安全程序，例如使用标准的90天披露期。 90天的披露期的问题在于，90天将迅速成为很长一段时间，当然，开源模型也无关紧要。如果人们足够关心提取它，则在Mistral 7b中培训数据将成为公开。我认为在这种情况下，没有人会充分关心。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1728060157156872252">Rowan Cheung说，根据数据</a>，Openai Saga对GPT产生了兴趣。像往常一样，相关性并不意味着因果关系，似乎有可能的兴趣总是引起到那里的。他链接到<a target="_blank" rel="noreferrer noopener" href="https://supertools.therundown.ai/gpts">他的“超级著名”列表。</a>我希望这会改变，但到目前为止，我还没有印象深刻。最好寻找您想要的特定内容，而不是寻找任何东西。</p><h4><strong>是谁？</strong></h4><p>在某些圈子中，尤其是在Twitter的某些地方， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1728894710088106475">有效的利他主义（EA）被视为民间恶魔</a>。最近对其进行的许多攻击与任何实际的EA动作都是正交的。其他人则抱怨EAS对他们做得异常出色的事情，例如他们提出对问题的具体反应的倾向。许多人将EA本身视为某种存在的威胁，要求相当歇斯底里的反应。</p><p>我不是，从来没有一个EA。我对EA有很多问题。最近，我对其中的许多人进行了讨论，并且我在网上广泛地<a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/criticism-of-ea-criticism-contest">撰写了有关</a>其中许多内容的文章， <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/book-review-going-infinite">最近一次是在我无限的评论中</a>。但这是其他的。</p><p>这是怎么回事？</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/the_megabase/status/1728752074132017463">巨型巴布斯</a>：对政治有很多了解</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc354961e-2a12-4873-9f85-de50b8f79305_677x421.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/otkyghqzpvtci6ha4i7p" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/alyssamvance/status/1729122134520201341">更多达卡，有人吗</a>？</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed0e427-9fcb-48a8-8ae6-1a37b909443a_1456x1290.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/b6pmboutl7e6mv3i4u3j" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1729579877202809314">或者，简化了一些可读性：</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a9ae35a-3043-4e0d-bac4-e5e9ddc46823_1350x1275.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/qlgxqjiragt6sixni6b2" alt="图像"></a></figure><blockquote><p> Linch：我年纪大了，可以记住何时对EA的主要批评是因为它对技术进步和个人行动过于着迷，并且对政治和系统性变化没有足够的关注。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidmanheim/status/1729112415441182846">戴维·曼海姆（David Manheim）</a> ：专家在固定运动中修改其在不同领域的观点和方法时遇到了很多麻烦，因为存在不同的相关问题，或者当新信息出现时会这样做。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/in-continued-defense-of-effective?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=86909076&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;utm_medium=email#footnote-12-86909076">斯科特·亚历山大（Scott Alexander）感到不得不写下一场辩护</a>，在这种辩护中，他更加出色地提出了这类矛盾的观点。每个人都优化了他们的鄙视。他做了出色的说明。其余的是对EA所谓的好作品的完整党派风格的防御，我认为其中有些可以被描述为“推动信封”，应该以这种方式阅读。然后，他跟进了<a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/contra-deboer-on-movement-shell-games">对Deboer的回应</a>，指出您可以为任何一系列的行动和信念，将其分类为“我们同意这是好的”，并说水桶很琐碎，所以它不算在内，并且桶“我们不同意这是好的”，然后说我们不同意桶是好的。</p><p>提出的一些投诉是矛盾的。其他人则不然。您绝对可以立即成为其中几个。左上方的“精英，Techbro，Bougie，Bougie，The Bookieaire”的“ Scammy A La Sbf”位于右下角，如果有任何相关的内容，与其他两个角落相比。</p><p>这里的每个人都声称EA代表其政治敌人的版本。我的模型说，这是因为每个人现在都使用[政治敌人的名字]作为[他们不喜欢的任何东西]和[任何不是他们政治地位的东西]。</p><p>认同[特定的政治立场]是对此的部分辩护，因此人们知道可以呼唤您哪个名字的方向。但是，当（像ea一样）您尽力不采取特定的政治立场，而要做真正有效的事情时，所有人都注意到，您缺少任何特定的氛围检查或他们正在做的其他shibboleth，所以您必须是[政治敌人]。</p><p>对此的另一部分防御是要与任何政治闻所不振的事物保持足够的距离，但是最近的事件使该战略无效。</p><h4><strong>那E/ACC呢？</strong></h4><p>另一面呢？ E/ACC？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1728379010252673193">丹尼尔·埃德（Daniel Eth）的Twitter</a>发现了最自我识别的E/ACC，说出合理的事情。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4f1dbd9-f99d-4d0c-91e9-896105fa8b3b_892x658.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/bxsaa4lpe11mbzxz9tls" alt=""></a></figure><p>像往常一样，（a）请直接讲麦克风，（b）您实际上并不相信Agi是一件事情，或者您没有任何意义。而（c）至少是一个理智的话，但是我然后说E/ACC标签正在用于发送错误的消息。丹尼尔（Daniel）主要探索（A）的突出程度，这无论哪种方式都可以。在人类灭绝的运动中，有11％的支持高还是低？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tegmark/status/1728755593866051602/history">Max Tegmark认为200次亲灭票很高</a>。但是，看看一千多个反人类的灭绝票。无论分配如何， <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattyglesias/status/1729104415578284350">歧义都很好</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mustafasuleyman/status/1729860128403239011">不错的尝试，苏莱曼。</a></p><blockquote><p>穆斯塔法·苏莱曼（Mustafa Suleyman）（首席执行官AI，联合创始人DeepMind）：加速度和安全二分法开始变得荒谬。安全人员不是再式的，而电子活动不是自由主义者。现实主义者都是。我们必须安全加速。</p></blockquote><p>我的意思是，是的，每个人都以纯粹的辩证法为基础的人是无益的。周围有很多合理的人。</p><p>但是，我看到您在这里想做什么，先生。</p><ol><li>以合理的人的立场并使用它来构建辩论。</li><li>将对存在风险的信念与“自由主义者隆起”一样。</li><li>将“ E/ACC”等同于安全性的两个合理位置。</li><li>需要作为给定的“加速”以及安全地做到的能力。</li></ol><p>这是一套密集的修饰技巧。老实说，我留下了深刻的印象。不错的演出。</p><p>取而代之的是，我观察到的是，坦率地说，这是一个日益激烈的，激进的和绝对主义者的立场，即加速度或E/ACC，其创始人和主要成员很少承认零或通常为零的细微差别，零愿意承认最大快速进步的任何缺点，谁对任何人说本身就是生存威胁。</p><p>包括该运动的创始人在内的频繁举动，实际上将这种担忧甚至EA等于恐怖主义等同于恐怖主义。另一个频繁的举动是将任何法规等同于未来的极权主义。</p><p>我看到许多人，包括曾经不使用这种策略的人，基本上可以放大他们能找到的每个亲加速声明，即使他们必须更好地了解。</p><p>这并不意味着一个人不能采取合理的立场，涉及相对较快地向前移动AI。这样的人还有很多。他们几乎也比在Twitter手柄中包括一个专制主义的口号要好。 Vitalik Buterin <a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">本周写了一篇非常好的文章，</a>说明了这是什么样子，我稍后讨论。</p><p>尽管还有另一个群体，警告说，可能会构成更聪明，更有能力的机器，可能会构成生存威胁并需要仔细地进行，而他们是设法保持冷静并进行合理讨论的人，因为它们被称为每个名称在每个方向的书中。</p><p>无论其原始设想如何，我越来越多地看到E/ACC在实践中将E/ACC视为Waluigi的EA Luigi。这样的方式更有意义。</p><h4> <a target="_blank" rel="noreferrer noopener" href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html"><strong>Vitalik提供了他的技术优势版本</strong></a></h4><p>大多数时候，技术都非常出色。好处是巨大的。默认情况下，它们非常超过成本，包括任何延迟费用。我们希望加速大多数新技术的发展和部署，尤其是相对于当前的政策制度。</p><p>那不是宇宙的自动定律。这是我们在历史上可用的技术的结果，也是我们为引导世界和减轻负面后果的选择而做出的选择。我们保持这一点的方式是欣赏潜在新技术的性质，承诺和危险，并做出相应的反应。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F03cb61cb-9d96-4d6a-8b06-62ac6e3f7818_1101x535.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/cnzs9uuvevbqlhq4vu3o" alt=""></a></figure><blockquote><p> Vitalik Buterin：但是有一种不同的方式来思考AI是什么：这是一种迅速获得智力<em>的新型思维</em>，它具有超过人类的精神教师并成为地球上新的顶点物种的严重机会。<em>该</em>类别中的事物类别要小得多：我们可能会包括人类超越猴子，超过单细胞生命的多细胞生命，<a target="_blank" rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Abiogenesis">生命本身的起源</a>，也许是工业革命，在这种革命中，机器在<em>体力</em>中逐渐消失了。突然，感觉就像我们在不那么糟糕的地面上行走。</p><p> ……</p><p>许多现代科幻小说是反乌托邦的，并以不良的光线绘画AI。即使是非科学小说的尝试来识别可能的AI未来，也常常给出<a target="_blank" rel="noreferrer noopener" href="https://www.eknowledger.com/files/life3_Summary_of_ai.png">非常令人着迷的答案</a>。因此，我四处<a target="_blank" rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Culture_series">问</a>一个问题：什么是科幻小说还是其他包含我们<em>想</em>居住的超智能AI的描述。<a target="_blank" rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Culture_series">系列</a>。</p><p> ……</p><p>我认为，即使人类在文化系列中扮演的“有意义的”角色也是如此。我问Chatgpt（还有谁？），为什么要赋予人类扮演的角色，而不是自己做所有事情，而我个人发现<a target="_blank" rel="noreferrer noopener" href="https://chat.openai.com/share/3dbe04c4-f5f3-4d2f-9437-d32732adde99">其答案</a>非常令人难以置信。<strong>似乎很难拥有一个以“友好”的超级智能为主的世界，那里的人类除了宠物之外。</strong></p></blockquote><p>其他科幻小说（如《星际迷航》）通过描绘高度不稳定的情况来避免这种情况，在这种情况下，至少人类不断地通过巨大的效率提高。默认情况下，将AIS保持在我们的控制之下似乎是非常不现实的。</p><p>有利于防御策略的动力倾向于带来更好的结果。否则，每个人的重点都被迫进行冲突，并且有很多价值被摧毁。</p><p>分散的系统相对善于奖励具有积极外部性的行动，人们认为善良并且需要更多。他们在处理负面外部性，与大幅度的巨大弊端的情况下要糟糕得多。</p><p>有利于防御性策略的差异技术发展和保护自己免受负面外部性和冲突的能力，这可能是关键。互联网在许多方面都做得很好。</p><p>我们应该在弹性的供应链和大流行预防等方面进行更多的投资。</p><p>即使没有AI考虑，监视技术也会变得便宜且无处不在。隐私保护技术很有价值。</p><p>信息安全技术可以帮助我们整理出真实的内容，例如社区笔记和预测市场。</p><p> （到目前为止，这既是我对Vitalik的观点的摘要，也是我认可的事情。）</p><p>因此，Vitalik提出了D/ACC，防御性（或分散或差异）加速度的提议。</p><p>然后，维塔利克（Vitalik）指出，如果您允许一个小组首先发展AGI，那么他们可能有机会组建一个最小的世界政府或以其他方式接管，即使在一个好的情况下，一致性工作并且他们的尝试可以安全地成功。理想情况下，可以制定基本规则，然后将权力返回给人民。但是，人们通常不信任任何群体或组织，具有这种权力。</p><p>在Vitalik的民意调查中，这种途径的替代方法是“ AI延迟十年”，可靠地赢得了投票。但是，这似乎不是明显的选择。十年后，您有同样的问题。要么没有人构建它，要么让一个组首先构建它，要么让许多小组一次构建它。</p><p>正如Vitalik所指出的那样，另一种建议是故意确保许多群体在同一时间大约在同一时间发展AGI，相互竞争，并希望达到某种权力平衡。 Vitalik指出，这不太可能导致稳定的情况。我会走得更远，说我还没有看到任何人解释这种情况如何希望成为一个稳定的平衡，即使每个人的Agis都成功地与自己的偏好保持一致，我们也会发现可以接受。</p><p>这并不意味着没有解决方案或良好的稳定平衡。这意味着我到目前为止未能设想一个人，而那些不同意的人似乎是手工塑料的，而不是参与导致问题的动态。我所看到的只是压倒性的压力，要移交权力，将人类带出循环，以及只能以一种方式结束的各种竞争压力。</p><p> Vitalik似乎奇怪地充满希望（即使是一个充满希望的想法？），这样的世界可能以人类为宠物而不是人类结束。我不明白为什么人类作为宠物比人类作为大师的稳定。但是我们同意这不是一个好结果。</p><p>然后，他提出了另一条潜在的路径。</p><blockquote><p> Vitalik：<strong>一条快乐的道路：与AIS合并？</strong></p><p>我最近听说过的另一种选择是将<strong>AI放在与人类分开的东西上，而将更多的选择<em>放在增强</em>人类认知而不是<em>替代</em>人类的工具上</strong>。</p></blockquote><p>这确实很棒。他认为问题是，将AIS作为工具的稳定性甚至不如其他建议稳定。</p><blockquote><p> vitalk：<strong>但是，如果我们想进一步推断人类合作的观念，我们就得出了更加的结论</strong>。除非我们建立一个足够强大的世界政府，以检测和阻止每一小群人用笔记本电脑攻击单个GPU，否则有人最终将创建一个超级智能的AI  - 一个可以比我们<a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/Ccsx339LE9Jhoii9K/slow-motion-videos-as-ai-risk-intuition-pumps">更快的一千倍的人</a>- 没有组合 - 而且没有组合人类用手使用工具将能够抵御这种情况。因此，我们需要将人机合作的理念进一步深化和深化。</p></blockquote><p>他的建议是脑部计算机界面，否则尝试与机器合成有限。 las，我看不出这是如何解决不稳定平衡问题的。</p><p> Vitalik的帖子中有很多很棒的想法。它的心在正确的位置。它具有很大的积极愿景。我同意大多数个人观点。作为解决非人工智能技术问题的一种方法，我本质上完全参与其中。</p><p>我也将其视为<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ZggyPlaydGuitar/status/1729279207270695340">对AI工作的个人或公司的重要信息</a>。您应该开发有用的技术，这些技术不会提高核心能力，而不是“放慢脚步”或完全停止所有工作。</p><p>问题在于，正如我认为Vitalik同意的那样，最终一切都取决于我们如何处理AGI的发展。像大多数人一样，他发现他在那里看到的替代方案的理解是不可接受的问题，然后提出了另一条途径，以期避免这些问题。</p><p>除了处理AGI的严重问题几乎总是如此，所提出的路径似乎并不是可以工作的方法。</p><p>如果我们只能开发与人类补充的AI技术，并避免危险代替人类的AI技术，那就太神奇了。但是，我们将如何共同选择这样做，尤其是以分散和非重新续报的方式做到这一点？我们如何差异地选择不发展会代替人类的AGI，而只有在有竞争性的压力以相反的压力的情况下才沿着对人类的称赞发展？</p><p>因此，当仔细阅读时，我将此作品视为问题的出色陈述，但无法超越最重要的情况以找到解决方案。这太棒了。明确说明问题非常有用。</p><p>对作品的反应似乎是普遍积极的。那些担心存在风险的人会注意到仔细的思想，对涉及的风险的认可以及寻找解决方案的愿望，即使到目前为止似乎缺乏建议。 <a target="_blank" rel="noreferrer noopener" href="https://michaelnotebook.com/vbto/index.html#fnref1">迈克尔·尼尔森（Michael Nielsen）有很多很好的想法</a>。</p><p> On the accelerationist side, the the &#39;acc&#39; and Vitalik&#39;s credentials are all they need, so you have Tyler Cowen recommending the piece and even Marc Andreessen saying &#39;self-recommending&#39; to a nuanced approach ( <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/pmarca/status/1729974817875730755">Marc even linked to Nielsen&#39;s thoughts</a> ). It is a vision of a different world, where we all realize we mostly want the same things.</p><h4> <strong>Quiet Speculations</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1727993012549009496">Economist says</a> &#39;some experts reckon that the UAE may well be the third-most-important country for AI, after America and China.&#39; A bold claim, even with the &#39;some&#39; and the &#39;may.&#39; They have a bunch of chips, that much is true. They trained Falcon. Except Falcon wasn&#39;t good? The UAE has no inherent advantages here, beyond willingness to spend, that would allow it to keep pace or catch up. They will not be in a great spot to attract talent. So I am very skeptical they would even end up fourth behind the UK.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/by-2028-which-country-other-than-us">I put up an 11-way market</a> ending at start of 2028. UK is trading 36%, Other 15%, no one else is 10%+. UAE is trading 5%. Other seems high to me but indeed do many things come to pass.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba568ad8-59cc-48b3-ba52-a034a9c68fec_1024x916.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/iq1lledfuqtjqvw01hge" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1728165673786847607">Davidad points out</a> what the bitter lesson actually says, which is to focus on algorithms that can leverage indefinite compute, which right now means search and learning, and not to hardcode world models. That does not mean not do a bunch of other cool or creative stuff.</p><p> A rancher asks if facial recognition can be used on cows, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ID_AA_Carmack/status/1728088970259517560">John Carmack uses as example of how slow we are to permeate the ful use of available technology</a> . This should be true of AI as well until the AI can be the one doing the permeating.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-11-24/ai-boom-will-create-real-estate-winners-and-losers?utm_source=twitter&amp;utm_campaign=socialflow-organic&amp;utm_medium=social&amp;utm_content=view&amp;cmpid%3D=socialflow-twitter-view&amp;sref=htOHjx5Y">Tyler Cowen predicts some economic consequences from AI</a> . Higher real estate prices in San Francisco and probably Manhattan, some other similar hubs abroad. Check, agreed. He expects relocations of new in-demand workers choosing an expanding Austin, northern Virginia and Atlanta. I get Austin, I don&#39;t understand the other two.</p><p> He expects losers to be places like Hartford and Minneapolis, cold climates and places with crime and governance. I would generalize this to say that places people do not want to live will continue to decline, while noting my confusion that people think Hartford is too cold but don&#39;t think Phoenix is sufficiently too hot to stop them.</p><p> He also predicts a decline in the value of stock packages for senior researchers, currently in the $5 million to $10 million range. There I disagree. I predict that such compensation will continue to rise, because there is no substitute for the best, and they will be truly invaluable.</p><p> I see both phenomena as related. There will be more generic supply, AI in general is great at making more generic supply and increasing its value, but the very best talent, and best everything else, will get super high leverage. I see this everywhere, similar to how the collectable in mint condition keeps getting a higher value multiplier.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/yonashav/status/1728131376548049370">A wakeup call attempt.</a></p><blockquote><p> Yo Shavit: If you are a public figure and tell your followers that “big new risks from advanced AI are fake”, you are wrong.</p><p> Not only that, you&#39;ll be seen to be wrong *publicly &amp; soon*.</p><p> This is not an “EA thing”, it is an oncoming train and it is going to hit you, either help out or shut up.</p><p> We are headed for >;=1 of:</p><p> * Massive job loss &amp; weakening of labor</p><p> * Massive cost-cuts to totalitarianism</p><p> * Autonomous agents reshaping the [cyber/information] env</p><p> * Major acceleration of R&amp;D</p><p> * AI systems we cannot trust with power, but are caught in a prisoner&#39;s dilemma to deploy</p><p> If you are being confidently told that none of these are >;25% likely in the next 10 years, and that risks from increasingly advanced AI are not worth working on, you are listening to an unserious person.</p><p> PS just because certain doomers have raise stupid suggestions for how to address these risks, doesn&#39;t mean we don&#39;t still need to actually find ways to address them.</p></blockquote><h4> <strong>AI Agent Future</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1729610147431924186">Roon offers up a vision</a> of the future. Thread quoted in full. Seems highly plausible, except that the consequences of going down this path seem fatal and that seems like an important thing to notice?</p><blockquote><p> Roon: The near future involves AI assistants and agents that smart people have to figure out how to work into business processes. The number of use cases will grow as the AIs get smarter. But ultimately the creativity and flexibility of humans will be the bottleneck.</p><p> After the second Industrial Revolution when running electricity became common, most industrialists just switched out their water wheel for a power contract and changed nothing else. they celebrated because they didn&#39;t need to set up near water. It took creativity to get further.</p><p> To unlock the true value of AI a whole parallel AGI civilization will spawn, creating new economic organizations from the ground up rather than waiting on human CEOs to figure out when and where to deploy them. Earliest to go will be any services that can be delivered digitally.</p><p> AIs don&#39;t have to be smarter than us to reach this event horizon, only faster. As long as they&#39;re delivering value autonomously in this way people will want to cede more and more control to AGI civilization and find ways to serve it by acting as conduits to the real world.</p><p> As an example good businesses to build now would be to finally figure out the cloud labs model so that powerful AIs can run bio assays or other experiments on physical substrate. You can perhaps model this as a new type of aaS business where the customer is ASI.</p><p> The datacenters will represent large percents of GDP. Most of the business of running and planning civilization except perhaps at the highest levels (reward must be defined by humans even if policy by NNs). and people will need to own a chunk of the returns and governance of AGI.</p></blockquote><p> The problem is “But ultimately the creativity and flexibility of humans will be the bottleneck.” And humans owning a chunk of the returns and governance of AGI indefinitely. Why would these happy circumstances last? That could easily be true at first, but once the &#39;AGI civilization&#39; is being given more and more power and autonomous authority, as Roon predicts happens in this scenario, what then?</p><p> How does Roon think this scenario ends? How should the rest of us think it ends? With humans ultimately remaining in control, defining the high level goals and ultimately allocating the surplus?为什么会这样呢？ Why wouldn&#39;t these ASIs, all competing for resources as per their original instructions, some of which choose to hold onto an increasing share of them, end up with the resources? What makes this equilibrium stable?</p><h4> <strong>The Quest for Sane Regulations</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattyglesias/status/1727821880499781760">Endorsement for the FTC definition?</a></p><blockquote><p> Matthew Yglesias: My eight year-old refers to anything with any kind of digital control — like those soda machines with one spigot and a touchscreen to select which soda comes out and in what quantity — as “AI.”</p></blockquote><p> Historical context that the OG AI existential risk communities, myself included, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/pvllss/status/1728690511580409881">deliberately attempted to avoid alerting governments to the problem until recently</a> , exactly because they felt that the benefits of accelerationist or counterproductive interventions weren&#39;t worth the risk and we did not even know what a good realistic ask would be. The default outcome, we felt, was either being laughed at, or governments saying &#39;get me some of this AI,&#39; which is exactly what ended up happening with DeepMind/OpenAI/Anthropic. Things are far enough along that has changed now.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tegmark/status/1728851164291059948">Extremely salty, fun and also substantive argument</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tegmark/status/1728133708484395263">between Max Tegmark</a> and both Yann LeCun and Cedric O, who is the former French technology minister who was then advocating for aggressive AI regulation, then went to Mistral and is now instrumental in getting France to fight to exempt foundation models from all EU regulations under the AI act. It involves quote tweets so you&#39;ll want to click backwards a bit first. Tegmark is saying that Cedric&#39;s actions are hypocritical, look exactly like corruption, and are backed by nonsensical arguments. Cedric and LeCun are if anything less kind to Tegmark. My favorite tweet <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/cedric_o/status/1728724005459235052">is this one from Cedric</a> , which contains both the &#39;I strongly support regulation&#39; and &#39;I strongly oppose regulation&#39; politicians flipping on a dime.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1728021633703493876?s=46&amp;t=6lH2PxsylmdyNsCDzO_26A">Connor Axios of Conjecture writes an op-ed</a> about the lobbying effort by tech, including the very US big tech companies that supposedly would benefit, to neuter the EU AI Act.</p><p> In a world where power begets power, and big tech holds all the cards in the AI race and also limitless funding, it is easy to frame almost anything as a gift to big tech.</p><ol><li> No regulation? Gift to big tech. They&#39;ll run rampant.</li><li> Pause AI? You&#39;re enshrining their monopoly.</li><li> Regulate smaller AIs, not bigger AIs? Huge win for big tech.</li><li> Regulate applications, not models? Big tech&#39;s role is producing the models.</li><li> Regulate big models, not small models? Regulatory capture, baby.</li><li> Require safety checks? Open source and little guys can&#39;t pass safety checks.</li><li> Require registration and reporting? Easy for big tech, killer burden for others.</li></ol><p>等等。 I agree it is tough.</p><p> Regulatory capture is a real issue. I still say &#39;exempt exactly the only thing big tech is uniquely positioned to do from all regulations&#39; is something big tech would like.</p><p> Not that this is the important question. I do not much care what big tech likes or dislikes. I do not much care whether they make a larger profit.你也不应该。 What matters is what is likely to lead to good outcomes for humanity, where we are all alive and ideally we get tons of nice things.</p><p> The newly proposed implementation of the EU AI Act is even more insane than previously noted. All those requirements that are not being imposed on general-purpose AI developers, the ones posing the actual dangers? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RistoUuk/status/1729784598412812524">All those requirements and the associated costs are then passed down to any European start-up who wants to use such models</a> . It is exactly the smaller companies that will have to prove safety, with information the big companies are being explicitly exempted from providing exactly because they are big and doing the importantly dangerous thing. <a target="_blank" rel="noreferrer noopener" href="https://t.co/56XMsJ4zEI">See this op-ed from Jaan Tallinn and Risto Uuk</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShakeelHashim/status/1727652452021735565">Economist draws conclusion from OpenAI saga</a> that AI is too important to be left to the latest corporate intrigue. Contrast this with those who argue this means it must be left to those who would maximize profits.</p><h4> <strong>The Week in Audio</strong></h4><p> The written version was good, but to get the full impact consider listening to <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=UdBMkj2WViY&amp;ab_channel=CognitiveRevolution%22HowAIChangesEverything%22">Nathan Lebenz&#39;s audio version of his experiences as a GPT-4 red teamer</a> .</p><p> I would summarize Lebenz&#39;s story this way:</p><ol><li> Nathan gets access to GPT-4-Early, the helpful only model. He is blown away.</li><li> OpenAI&#39;s people don&#39;t seem to know what they have.</li><li> Nathan asks to join the red team, joins, goes full time for no compensation.</li><li> Nathan finds the red team inadequate. Little guidance is given. Most people seem disengaged, with little knowledge of how to do prompt engineering.</li><li> Sometimes half or more of red team content is being generated by Nathan alone.</li><li> Nathan asks, what are your safeguards and plans? They won&#39;t tell him.</li><li> Red team gets a version that is supposed to refuse harmful commands. It works if you play it completely straight, but fails to stand up to even the most basic techniques.</li><li> When Nathan reports this, those in charge are confused and can&#39;t reproduce. Nathan provides &#39;a thousand screenshots.&#39;</li><li> Nathan grows increasingly concerned that safety is not being taken seriously.</li><li> Nathan asks a few expert friends in confidence what he should do, and is directed to a board member.</li><li> Board member says they have seen a demo and heard the new model is good but have not tried GPT-4 (!) and that this is concerning so they will look into this.</li><li> Nathan is fired from the red team, supposedly for allowing knowledge of GPT-4&#39;s capabilities to spread.</li><li> The board member tells Nathan that they&#39;ve been informed he is &#39;guilty of indiscretions.&#39;</li><li> In other words, they tell the board member that Nathan shouldn&#39;t be trusted because he consulted with trusted friends before he brought this issue to the attention of the board, so the board should not pay attention to it.</li><li> That was the end of that.</li><li> GPT-3.5 ships, with safety much better than anything shown to the red team.</li><li> It was later revealed there were other distinct efforts being hidden from the red team members. The period Nathan describes here were very early days. Generally safety efforts have looked better and more serious since. Rollouts and gating requirements have been deliberate.</li><li> There are still some clear holes in GPT-4&#39;s safety protocols that remain unfixed. For example, you can get it to spearfish using prompts that have been around for a while now.</li></ol><p> Does that sound like a CEO and organization that is being &#39;consistently candid&#39; with its board? I would urge Taylor, Summers and D&#39;Angelo to include this incident in their investigation.</p><p> Does it sound like a responsible approach to safety? If your goal is to ship consumer products, it is woefully inadequate on a business level. The costs here are trivial compared to training, why skimp on the red team and let things be delayed for months?</p><p> Or another level one could consider it a highly responsible approach to safety. Perhaps this is everyone involved going above and beyond. They did not wish to spook the world with GPT-4&#39;s capabilities. They deliberately were slowing down its release and prioritizing keeping a lid on information. GPT-3.5 was a paced, deliberate rollout.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jd_pressman/status/1729994771719180389">I talked for two hours with John David Pressman about actual alignment questions</a> . It was great to talk about concrete questions, compare models and try to figure things out with someone I disagree with, rather than getting into arguments about discourse. If this type of discussion sounds appealing I recommend giving it a shot.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/articles/2023-11-23/anthony-levandowski-reboots-the-church-of-artificial-intelligence?sref=htOHjx5Y">From Bloomberg, Anthony Levandowski Reboots Church of Artificial Intelligence</a> . See, there, that&#39;s an AI cult.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/yashkaf/status/1728130426026488109">Even the Ringer has some discussion of the OpenAI situation</a> (don&#39;t actually listen to this unless you want to anyway).</p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/jeff-sebo-ethics-digital-minds/">Jeff Sebo on digital minds</a> and how to avoid sleepwalking into a major moral catastrophe on 80,000 hours. You want an actual Pascal&#39;s mugging? This is where you&#39;ll get an actual mugging, with those saying you might want to worry about things with probabilities as low as one in a quadrillion. That even if there is a &lt;1% chance that AIs have the relevant characteristics, we then must grant them rights, in ways that I would say endanger our survival. Along with a lot of other &#39;well the math says that we should be scope insensitive and multiply and ignore everything else so we should do this thing…&#39; What is missing is any sense, in the parts I looked at, of the costs of the actions being proposed if you are wrong. The original Pascal&#39;s Mugging at least involved highly limited downside.</p><h4> <strong>Rhetorical Innovation</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/TheZvi/status/1727460012954435988">Twitter thread on good introductions to AI risk</a> .我们很糟糕。 Let&#39;s do better.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EpistemicHope/status/1730122141327413552">Eli Tyre notes</a> that tribalism is way up in discussions of AI and AI risk, from all sides.</p><p> This is very true, especially on Twitter, to the point where it makes reading my feeds far more distressing than it did two weeks ago. Some of those who are worried about existential risk are doing it too, to some extent.</p><p>然而。 I do not think this has been remotely a symmetrical effect. I am not going to pretend that it is one. Accelerationists mostly got busy equating anyone who thinks smarter than human AIs might pose a danger to terrorists and cultists and crazies. The worst forms of ad hominem and gaslighting via power were on display. Those who are indeed worried did not consistently cover themselves fully in glory, to be sure, but the contrast has never been clearer.</p><p> That does not prove anything about the underlying situation or what would be an appropriate response to it. In a world in which there was no good justification for worry, I would still expect the unworried to completely and utterly lose it in this spot. So that observation isn&#39;t evidence. You need to actually consider the arguments.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/danfaggella/status/1728466838751756603">True story, for some capabilities level of AGI</a> . Important thing for people to get.</p><blockquote><p> Daniel Faggella: &#39;Sure there will be AGI, but we humans will be necessary as always.&#39;</p><p>不，兄弟。 In the time we take a breath AGI solves the game of Go and reads every physics text ever written.</p><p> Once we boatload it with data, models, and physical robots – we&#39;re a hindrance, not an aide.</p></blockquote><p> If we continue down this path, no, we will not be necessary in an economic sense.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Liv_Boeree/status/1728131649177591953">Liv Boeree reminds us</a> there are lots of things involving AI can go deeply wrong, there are risk tradeoffs throughout, and we need to solve all of them at the same time. As she says, her list is highly non-exhaustive.</p><p> I love this next one because of how many different ways you can read it…</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/qephatziel/status/1728095405164937667">Q*phatziel</a> : At long last, we have banned the Utopia Device from the classic sci-fi novel Please Build the Utopia Device.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1729339347848376453">And Eliezer keeps trying</a> .</p><blockquote><p> Eliezer Yudkowsky: “I expect AIs to end up with humanlike motivations, since we&#39;re training them to output human behaviors.”</p><p> “I hired a genius actress to watch a local bar on a videocam, until she could predict the words and gestures of every regular there. Hope she doesn&#39;t end up too drunk!”</p></blockquote><p> From a while back but still my position, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1729335273878856169">here is a clip of yours truly</a> explaining how I think about p(doom) and how I think others are thinking about it when they reach radically different conclusions.</p><h4> <strong>Aligning a Smarter Than Human Intelligence is Difficult</strong></h4><p> One of the reasons it is difficult is that the funding has for a long time mostly come from a handful of interconnected, effectively remarkably hierarchical organizations, mostly liked to EA, which needed things to fit into their boxes and were often trying to play nice with major labs. Same with lobbying and policy efforts. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/atroyn/status/1727386556342972866">We need a much broader set of funders, spending a lot more on a wider variety of efforts</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety">Shallow map of efforts in the safety and alignment spaces.</a></p><p> Nate Soares of MIRI writes an odd post: <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting">Ability to solve long-horizon tasks correlates with wanting things in the behaviorist sense</a> .</p><blockquote><p> Okay, so you know how AI today isn&#39;t great at certain… let&#39;s say “long-horizon” tasks? Like novel large-scale engineering projects, or writing a long book series with lots of foreshadowing?</p><p> (Modulo the fact that it can play chess pretty well, which is longer-horizon than some things; this distinction is quantitative rather than qualitative and it&#39;s being eroded, etc.)</p><p> And you know how the AI doesn&#39;t seem to have all that much “want”- or “desire”-like behavior?</p><p> ……</p><p> Well, I claim that these are more-or-less the same fact. It&#39;s no surprise that the AI falls down on various long-horizon tasks <em>and</em> that it doesn&#39;t seem all that well-modeled as having “wants/desires”; these are two sides of the same coin.</p><p> ……</p><p> Which is to say, my theory says “AIs need to be robustly pursuing <em>some</em> targets to perform well on long-horizon tasks”, but it does <em>not</em> say that those targets have to be the ones that the AI was trained on (or asked for ）。 Indeed, I think the actual behaviorist-goal is very unlikely to be the exact goal the programmers intended, rather than (eg) a tangled web of correlates.</p></blockquote><p> Setting aside all the &#39;that is not what those exact words mean, you fool&#39; objections, and allowing some amount of vague abstraction, the whole thing seems obvious, as Nate says, in a &#39;if this seems obvious you don&#39;t need to read the rest even if you care about this question&#39; way. To some degree, a system is effectively doing the things that chart a path through causal space towards an objective of some kind, including overcoming whatever obstacles are in its way, and to some degree it isn&#39;t. To the degree it isn&#39;t and you want it to be doing long term planning, it isn&#39;t so it won&#39;t. To the degree it is, and the system is capable, that will then probably look like long term planning, especially in situations with a lot of complex obstacles.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting?commentId=mEPjkATGDExGcZWop">Paul Christiano challenges in the comments</a> , I think I disagree with the challenge. <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting?commentId=G28WTNBkQFkBPLqGd">Anna Salamon notices she is confused</a> in interesting ways in hers.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1728032814069960985">Rob Bensinger points out some precise ways</a> in which his model says we should be cautious: <a target="_blank" rel="noreferrer noopener" href="https://t.co/Tb8i8iw5Kl">Patch resistance</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/qZ4fURslPi">minimality principle</a> and the <a target="_blank" rel="noreferrer noopener" href="https://t.co/pKAxM9wDkV">non-adversarial principle</a> .简而言之。</p><ol><li> Non-adversarial: Your AI should not be (de facto) looking to subvert your safety measures.</li><li> Minimality: In the crucial period right after building AGI, choose the plan requiring the least dangerous AGI cognition, even if its physical actions look riskier.</li><li> Patch resistance: Strive to understand why the problem arises and prevent it from happening in the first place. If the issue keeps coming up and you keep patching it out, you are optimizing for hiding the issue.</li></ol><p> Oliver Habryka and John Pressman <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohabryka/status/1727825655545704879">discuss various potential human augmentation or alternative AI training plans</a> .</p><blockquote><p> John Pressman: So just to check, if we took say, 10,000 peoples EEG data recorded for hundreds of thousands of hours and trained a model on it which was then translated to downstream tasks like writing would you have the same concerns?</p><p> Oliver Habryka: Oh no! This is one of the plans that I am personally most excited about. I really want someone to do this. I think it&#39;s one of the best shots we have right now. If you know anyone working on this, I would love to direct some funding towards it.</p><p> My current best guess is you get to close to zero error before you get any high-level behavior that looks human, but man, I sure feel like we should check, and be willing to do a really big training run for this.</p></blockquote><p> In general I am excited by alternative architectures and approaches for getting to general intelligence, that give better hope for embodying what matters in a survivable and robust way as capabilities scale. I am especially excited if, upon trying it and seeing it display promising capabilities, you would have the opportunity to observe whether it was likely to turn out well, and pull the plug if it wasn&#39;t. If you don&#39;t have that, then we are back to a one-shot situation, so the bar gets higher, and I get pickier.</p><p> The EEG data hypothetical is interesting. Certainly it passes the &#39;if it can safety be tried we should try it&#39; bar. I can see why it might work on all counts. I can also see how it might fail, either on capabilities or on consequences. If there was no way to stop the train no matter how badly things look, I&#39;d have to think hard before deciding whether to start it.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1729461156618637502">Davidad is excited</a> by a new DeepMind paper, <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2311.14125">Scalable AI Safety via Doubly-Efficient Debate</a> ( <a target="_blank" rel="noreferrer noopener" href="https://github.com/google-deepmind/debate">code here</a> ).</p><blockquote><p> Paper Abstract: The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al. [2018] proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask.</p><p> While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate deterministic AI systems for an exponential number of steps, limiting its applicability.</p><p> In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a polynomial number of steps, whilst being able to verify the alignment of stochastic AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.</p><p> Davidad: This is a milestone. I have historically been skeptical about “AI safety via debate” (for essentially the reason now called “obfuscated arguments”). I&#39;m still somewhat skeptical about the premises of this theoretical result (eg the stochastic oracle machine, defined in Lean below, doesn&#39;t seem like a good framework for modelling “human judgment” about acceptable or unacceptable futures).</p><p> But I&#39;m now much more optimistic that a PCP (probabilistically checkable proof) system derived from this line of research might be a useful tool to have in the toolbox for verifying AI safety properties that depend upon unformalizable human preferences. I still think “not killing lots of people” is probably just totally formalizable, but humanity might also want to mitigate the risks of various dystopias that are more a matter of taste, and that&#39;s where this type of method might shine.</p></blockquote><p> I remain skeptical of the entire debate approach, and also of the idea that we can meaningfully prove things, or that we can formalize statements like &#39;not killing lots of people&#39; in sufficiently robust ways. I do wish I had a better understanding of why others are as optimistic as they are about such approaches.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand?utm_source=post-email-title&amp;publication_id=89120&amp;post_id=138968567&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=67wny&amp;utm_medium=email">Scott Alexander goes through the Anthropic paper on Monosemanticity</a> from a few weeks ago.</p><h4> <strong>People Might Also Worry About AI Killing Only Some of Them</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/unusual_whales/status/1727823232118690163">So it has predictably come to this.</a> Sounds like it will end well.</p><blockquote><p> Unusual Whales: BREAKING: The Pentagon is moving toward letting AI weapons autonomously decide to kill humans, per BI.</p></blockquote><p> The good news is that in the scenarios where this ends the way you instinctively expect it to, either we probably do not all die, or were all already dead.</p><p> Qiaochu Yuan is not worried <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/QiaochuYuan/status/1728189424675209380">but notes the sincerity of those who are.</a></p><blockquote><p> Qiaochu Yuan: People really don&#39;t get how sincere the AI existential risk people are. lots of looking for ulterior motives. I promise you all the ones I&#39;ve personally met and talked to literally believe what they are literally saying and are sincerely trying to prevent everyone from dying.</p></blockquote><p> I once again confirm this. Very high levels of sincerity throughout.</p><h4> <strong>People Are Worried About AI Killing Everyone</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/TEDchris/status/1728129103247634869">Chris Anderson of TED thinks AGI is near and is an existential danger to humanity</a> (without explaining his reasons). In comments, LeCun suggests Demis Hassabis or Ilya Sutskever as debate partners for future TED, worth a shot.</p><p> Yes, two very different cases.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1727754804209062116">Roon</a> : ai research is not analogous to pathogen gain of function research with gain of function research the downside is unbounded but upside is tiny. With ai both upside and downside are potentially unbounded. That&#39;s why we build it, safely.</p></blockquote><p> So now the reasonable people can talk price. How do we build it safety?有哪些权衡？ Which moves make good outcomes more likely? I am all for building it safely, if someone can figure out how the hell we are going to do that.</p><p> Whereas with Gain of Function research, without the same upside, the correct answer is obviously no, that is crazy, don&#39;t do that, why would we ever let anyone do that.</p><h4> <strong>Other People Are Not As Worried About AI Killing Everyone</strong></h4><p> Because it is right to include such things: <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/psychosort/status/1727851812009480429">Brian Chau responds unkindly to my assertions last week</a> that the paper he was describing did not propose anything resembling totalitarianism. I affirm that I stand by my claims, and otherwise will let him have the last word.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/psychosort/status/1727856690299556227">I&#39;ll also give him this banger</a> , although beware the Delphic Oracle:</p><blockquote><p> Brian Chau: 2008: “Bitter clingers to their guns and religion”</p><p> 2024: “Bitter clingers to their degrees and newspapers”</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/growing_daniel/status/1728862193532477723">I&#39;ll also speak up for Yann LeCun&#39;s right to have his own damn opinion</a> about existential risk and other matters, no matter what &#39;other equally qualified experts&#39; say.</p><blockquote><p> Geoffrey Hinton: Yann LeCun thinks the risk of AI taking over is miniscule.这意味着他非常重视自己的观点，而很少重视许多其他同等资格专家的观点。</p></blockquote><p> I think LeCun is very wrong about AI risk and Hinton is right (and has been extremely helpful and in good faith all around), but LeCun allowed to be wrong, assuming that is how he evaluates the evidence. You&#39;re allowed, nay sometimes required, to value your own opinion on things. Same goes for everyone. I don&#39;t really understand how LeCun reaches his conclusions or why he believes in his arguments, but let&#39;s evaluate the arguments themselves.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1727797479452844301">Interestingly we agree on the initial statement below but for opposite reasons.</a></p><blockquote><p> Samo Burja: The older a transhumanist gets the less you should trust them to accurately judge AGI risk.</p><p> Basically I think about half of the people making predictions have a psychological bias towards the singularity being in their lifetime.</p><p> A lifetime of cherrypicking evidence results in “The singularity is near!” in 1985 in 1995 in 2005 and in 2025. For every year after 1985 the singularity is quite near in some sense, but in another this isn&#39;t what they mean when they say that.</p></blockquote><p> There&#39;s not zero of that. In my experience the &#39;I am old and have seen such talk and dismiss it as talk&#39; is stronger.</p><p> What I think is even stronger than that, however, especially among the childless, is that many people want AGI within their lifetimes. They want to see the results and enjoy the fruits. They want to live forever. If we get AGI in a hundred years, great for humanity, but they are still dead. A few even say it out loud.</p><p> Which I totally get as a preference. I can certainly appreciate the temptation, especially for those without children. I hope we collectively choose less selfishly and more wisely than this.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1728634847131001333">Here&#39;s a weird one.</a></p><blockquote><p> Pedro Domingos: LLMs are 1% like humans and 99% unlike, and the burden is on doomers to explain how it&#39;s exactly that 1% that makes them an extinction threat to us.</p><p> Daniel Eth: Okay, this is weird – it&#39;s more the 99% that&#39;s unlike humans that I&#39;m worried about, not the 1% that&#39;s like us. “This new intelligent thing is very alien” doesn&#39;t make me *more* comfortable.</p><p> John Pressman: Luckily, it&#39;s untrue. [Eth agrees it is untrue as do I]</p></blockquote><p> I agree with Pressman and Eth, 99% I do not understand why Domingos thinks this is an argument? Why should the parts that are unlike humans be safe, or even safer?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1728357894679380329">Here&#39;s another weird one</a> . I don&#39;t understand this one either.</p><blockquote><p> Pedro Domingos: “I&#39;m worried”, said one DNA strand to another, swimming inside a bacterium two billion years ago. “If we start making multicellular creatures, will they take over from DNA?”</p></blockquote><h4> <strong>Please Speak Directly Into This Microphone</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1729524104896930192">Your periodic reminder.</a></p><blockquote><p> Andrew Critch: Reminder: some leading AI researchers are *overtly* pro-extiction for humanity. Schmidhuber is seriously successful, and thankfully willing to be honest about his extinctionism. Many more AI experts are secretly closeted about this (and I know because I&#39;ve met them).</p><p> Jurgen Schmidhuber (Invented principles of meta-learning (1987), GANs (1990), Transformers (1991), very deep learning (1991), etc): AI boom v AI doom: since the 1970s, I have told AI doomers that in the end all will be good. <a target="_blank" rel="noreferrer noopener" href="https://t.co/DJ9aeA6x1o">Eg, 2012 TEDx talk</a> : “Don&#39;t think of us versus them: us, the humans, v these future super robots. Think of yourself, and humanity in general, as a small stepping stone, not the last one, on the path of the universe towards more and more unfathomable complexity. Be content with that little role in the grand scheme of things.” As for the near future, our old motto still applies: “Our AI is making human lives longer &amp; healthier &amp; easier.”</p></blockquote><h4> <strong>The Lighter Side</strong></h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1729279209821130984">Roon is excited for the new EA</a> .</p><blockquote><p> Roon: my main problem with EA is that the boring systematic index fund morality mindset will almost certainly not lead to the greatest good.</p><p> All the best historical advances in goodness have come from crazy people pursuing crazy things.</p><p> Or alternatively the kindness of normal individuals looking out for their families and communities</p><p> The hedge fund manager donating to malaria funds forms a kind of bland middle that inhabits the uninspiring midwit part of the bell curve.</p><p> I actually see the longtermist xrisk arm that schemes to destroy ai companies as a big improvement and way more fun.</p></blockquote><p> I get where he&#39;s coming from. If they&#39;re out in the arena trying to do what they think is right, then perhaps they will get somewhere that matters, even if there is risk that it goes bad. Better to have the hedge fund manager donate to malaria funds that work than to cute puppies with rare diseases, if one does not want one&#39;s head in the game, but that is not what ultimately counts most.</p><p> Obviously Roon and I view the events at OpenAI differently, but the board definitely did not want OpenAI to operate the way Altman wants it to operate. <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board">As I noted</a> , both the board and Altman viewed the potential destruction of OpenAI as an acceptable risk in a high stakes negotiation.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rlmcelreath/status/1694296270540554405">什么是人工智能？</a></p><p> Richard McElreath: I told a colleague that logistic regression is AI and they got mad at me, so I made a chart.找到你自己。 I am “Tinder is AI”.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1371c0b-3478-45a7-9a85-9ce9c0eb8604_1456x874.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/nhlzhbejluqsdfvkl1cz" alt="Table with 3 rows and 3 columns.
Rows: (1) Algorithm purist (mimic human cognition), (2) Algorithm netural (learns &amp; generalizes), (3) Algorithm rebel (method irrelevant)
Cols: (1) Ability purist (exceeds human ability), (2) Ability neutral (makes task easier), (3) Ability rebel (usefullness questionable)
Cells:
[1,1] &quot;Terminator is AI&quot; [1,2] &quot;C3PO is AI&quot; [1,3] &quot;WALL-E is AI&quot;
[2,1] &quot;AlphaGo is AI&quot; [2,2] &quot;XGBOOST is AI&quot; [2,3] &quot;Tinder is AI&quot;
[3,1] &quot;A metal detector is AI&quot; [3,2] &quot;Bubble sort is AI&quot; [3,3] &quot;Magic 8 Ball is AI&quot;"></a></figure><p> I think my position on the chart is a hybrid – that Wall-E and XGBoost are AIs, but Tinder and Metal Detectors are not.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/atroyn/status/1728557473962107101">大家好消息。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48af238c-d0cc-45e0-8dea-4238538cdfac_898x324.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/v32g9ya48t7ufnwgbjh8" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/menhguin/status/1728230474735309116">And some bad news.</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e8ffc50-9677-4fd3-bf21-49f55720742f_892x354.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/c0sgmfwhgu38phcnadkp" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/deepfates/status/1728158272920625190">Staff Engineer promised if I kept quoting him I&#39;d get a board seat</a> .我会吗？</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6662adf-4982-4d22-8e62-111bdaea88a8_880x486.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/je5BwKe8enCq8DLrm/u9akjoqksk7rylownbgr" alt=""></a></figure><br/><br/> <a href="https://www.lesswrong.com/posts/je5BwKe8enCq8DLrm/ai-40-a-vision-from-vitalik#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/je5BwKe8enCq8DLrm/ai-40-a-vision-from-vitalik<guid ispermalink="false"> je5BwKe8enCq8DLrm</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 30 Nov 2023 17:30:15 GMT</pubDate> </item><item><title><![CDATA[Is scheming more likely in models trained to have long-term goals? (Sections 2.2.4.1-2.2.4.2 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 30, 2023 4:43 PM GMT<br/><br/><p> This is Sections 2.2.4.1-2.2.4.2 of my report “ <a href="https://arxiv.org/pdf/2311.08379.pdf">Scheming AIs: Will AIs fake alignment during training in order to get power?</a> ”。 There&#39;s also a summary of the full report <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">here</a> (audio <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">here</a> ). The summary covers most of the main points and technical terms, and I&#39;m hoping that it will provide much of the context necessary to understand individual sections of the report on their own.</p><p> Audio version of this section <a href="https://www.buzzsprout.com/2034731/13984855">here</a> , or search &quot;Joe Carlsmith Audio&quot; on your podcast app.</p><h1> What if you intentionally train models to have long-term goals?</h1><p> In my discussion of beyond-episode goals thus far, I haven&#39;t been attending very directly to the <em>length</em> of the episode, or to whether the humans are setting up training specifically in order to incentivize the AI to learn to accomplish long-horizon tasks 。 Do those factors make a difference to the probability that the AI ends up with the sort of the beyond-episode goals necessary for scheming?</p><p>是的，我认为他们确实如此。 But let&#39;s distinguish between two cases, namely:</p><ol><li><p> Training the model on long (but not: indefinitely long) episodes, and</p></li><li><p> Trying to use short episodes to create a model that optimizes over long (perhaps: indefinitely long) time horizons.</p></li></ol><p> I&#39;ll look at each in turn.</p><h2> Training the model on long episodes</h2><p> In the first case, we are specifically training our AI using fairly long episodes – say, for example, a full calendar month. That is: in training, in response to an action at t1, the AI receives gradients that causally depend on the consequences of its action a full month after t1, in a manner that directly punishes the model for ignoring those consequences in choosing actions at t1 。</p><p> Now, importantly, as I discussed in the section on &quot;non-schemers with schemer-like traits,&quot; misaligned non-schemers with longer episodes will generally start to look more and more like schemers. Thus, for example, a reward-on-the-episode seeker, here, would have an incentive to support/participate in efforts to seize control of the reward process that will pay off within a month.</p><p> But also, importantly: a month is still different from, for example, a trillion years. That is, training a model on <em>longer</em> episodes doesn&#39;t mean you are directly pressuring it to care, for example, about the state of distant galaxies in the year five trillion. Indeed, on my definition of the &quot;incentivized episode,&quot; no earthly training process can directly punish a model for failing to care on such a temporal scope, because no gradients the model receives can depend (causally) on what happens over such timescales. And of course, absent training-gaming, models that sacrifice reward-within-the-month for more-optimal-galaxies-in-year-five-trillion will get penalized by training.</p><p> In this sense, the most basic argument <em>against</em> expecting beyond episode-goals (namely, that training provides no direct pressure to have them, and actively punishes them, absent training-gaming, if they ever lead to sacrificing within-episode reward for something longer-term) applies to both &quot;short&quot; (eg, five minutes) and &quot;long&quot; (eg, a month, a year, etc) episodes in equal force.</p><p> However, I do still have some intuition that once you&#39;re training a model on fairly long episodes, the probability that it learns a <em>beyond</em> -episode goal goes up at least somewhat. The most concrete reason I can give for this is that, to the extent we&#39;re imagining a form of &quot;messy goal-directedness&quot; in which, in order to build a schemer, SGD needs to build not just a beyond-episode goal to which a generic &quot;goal-achieving engine&quot; can then be immediately directed, but rather a larger set of future-oriented heuristics, patterns of attention, beliefs, and so on (call these &quot;scheming-conducive cognitive patterns&quot;), then it seems plausible to me that AIs trained on longer episodes will have more of these sorts of &quot;scheming-conducive cognitive patterns&quot; by default. For example, they&#39;ll be more used to reasoning about the long-term consequences of their actions; they&#39;ll have better models of what those long-term consequences will be;等等。 And perhaps (though this seems to me especially speculative), longer-episode training will incentivize the AI to just think more about various <em>beyond</em> -episode things, to which its goal-formation can then more readily attach.</p><p> Beyond this, I also have some sort of (very hazy) intuition that relative to a model pressured by training to care only about the next five minutes, a model trained to care over eg a month, or a year, is more likely to say &quot;whatever, I&#39;ll just optimize over the indefinite future.&quot; However, it&#39;s not clear to me how to justify this intuition. <sup class="footnote-ref"><a href="#fn-ZvAv8KJZnntyccKuJ-1" id="fnref-ZvAv8KJZnntyccKuJ-1">[1]</a></sup></p><p> (You could imagine making the case that models trained on longer episodes will have more incentives to develop situational awareness – or even goal-directedness in general. But I&#39;m assuming that all the models we&#39;re talking about are goal-directed and situationally -意识到的。）</p><h2> Using short episodes to train a model to pursue long-term goals</h2><p> Let&#39;s turn to the second case above: trying to use short-episode training to create a model that optimizes over long time horizons.</p><p> Plausibly, something like this will become more and more necessary the longer the time horizons of the task you want the model to perform. Thus, for example, if you want to create a model that tries to maximize your company&#39;s profit over the next year, trying to train it over many year-long episodes of attempted profit-maximization (eg, have the model take some actions, wait a year, then reward it based on how much profit your company makes) isn&#39;t a very good strategy: there isn&#39;t enough time.</p><p> Indeed, it seems plausible to me that this sort of issue will push AI development <em>away</em> from the sort of simple, baseline ML training methods I&#39;m focused on in this report. For example, perhaps the best way to get models to pursue long-term goals like &quot;maximize my company profits in a year&quot; will be via something akin to &quot; <a href="https://lilianweng.github.io/posts/2023-06-23-agent/?ref=planned-obsolescence.org">Language Model Agents</a> ,&quot; built using trained ML systems as components, but which aren&#39;t themselves optimized very directly via gradients that depend on whether they are achieving the (possibly long-term) goals users set for them. These sorts of AIs would <em>still</em> pose risks of schemer-like behavior (see the section on &quot;non-schemers with schemer-like traits&quot; above), but they wouldn&#39;t be schemers in the sense I have in mind.</p><p> That said, there are <em>ways</em> of trying to use the sort of training I&#39;m focused on, even with fairly short-term episodes, to try to create models optimizing for long-term goals. In particular, you can try to reward the model based on <em>your assessment</em> of whether its short-term behavior is leading to the long-term results that you want (eg, long-term company profit), and therefore, hopefully induce it to optimize for those long-term results directly. <sup class="footnote-ref"><a href="#fn-ZvAv8KJZnntyccKuJ-2" id="fnref-ZvAv8KJZnntyccKuJ-2">[2]</a></sup> Of course, whether this will <em>work</em> (as opposed, for example, to inducing the AI to optimize your short-term <em>assessments</em> of its actions) is a further question. But if it does, then you&#39;ll have created an AI that optimizes for &quot;beyond-episode goals&quot; in my sense.</p><p> Indeed, depending on how we want to use our terms, we can view this sort of training as intentionally trying to create a form of goal-misgeneralization. That is, the reward, here, does not depend causally on the long-term consequences of the model&#39;s actions, so in that sense, the long-term results in question aren&#39;t the &quot;specified goal&quot; (on this framing, the specified goal is always within-the-episode). But you&#39;re trying to get the AI to care intrinsically about them anyway.</p><p> Of course, it&#39;s a further question whether this sort of beyond-episode goal, once created, will lead to instrumental training-gaming. And indeed, successfully creating this sort of beyond-episode goal, instead of eg a reward-on-the-episode seeker, requires <em>avoiding</em> a certain kind of training-gaming up front – that is, the model has to <em>not</em> learn to just optimize for your short-term evaluations. And if you&#39;ve successfully set up your training process such that optimizing for your desired long-term goal is in fact a max-reward (or: near-max-reward) behavior, training-gaming might not offer the model in question much优势。 (Here the human analogy would be something like: if you&#39;re supervisor is sufficiently good at assessing whether your near-term performance is going to lead to long-term profit, and sufficiently immune to manipulation, then you&#39;ll perform as good or better, in performance reviews, by just directly optimizing for long-term profit – for example, because you&#39;re not wasting time thinking about your supervisor at all.)</p><p> Still, models with beyond-episode goals emerging from this sort of process seem to me like they&#39;re at risk of scheming regardless. For one thing, the considerations discussed in the previous section all apply here – eg, this sort of training involves pointing your model&#39;s cognition in a very future-focused direction, thereby plausibly inducing it to develop various scheming-conducive cognitive patterns, to attach value to various long-term consequences, and so on (and in this case, the horizon of the episode sets no bound on the temporal horizon of the &quot;future&quot; that the model&#39;s cognition is pointed towards; rather, that bound is set, centrally, by your <em>evaluations</em> of what the model&#39;s actions will cause, when).</p><p> More than this, though, it seems plausible to me that your evaluations of the consequences of a model&#39;s action will be in some sense &quot;noisier&quot; than a reward process that depends causally on those consequences, in a manner that makes it harder to differentiate between the different <em>sorts</em> of long-term goals your training is incentivizing. For example, maybe your model is behaving in a way that seems to you, broadly, like it will lead to your company being successful in three years, but you can&#39;t tell whether it will also create lots of harmful externalities – whereas a reward process that could actually see the consequences after three years would be able to tell. And an inability to readily distinguish between the different sorts of long-term goals you might be instilling seems like it increases the risk of accidentally instilling a schemer-like goal. <sup class="footnote-ref"><a href="#fn-ZvAv8KJZnntyccKuJ-3" id="fnref-ZvAv8KJZnntyccKuJ-3">[3]</a></sup> </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-ZvAv8KJZnntyccKuJ-1" class="footnote-item"><p> We could try appealing to simplicity (thanks to Evan Hubinger for discussion), but it&#39;s not clear to me that &quot;five minutes&quot; is meaningfully simpler than &quot;a month.&quot; <a href="#fnref-ZvAv8KJZnntyccKuJ-1" class="footnote-backref">↩︎</a></p></li><li id="fn-ZvAv8KJZnntyccKuJ-2" class="footnote-item"><p> This is somewhat akin to a form of &quot; <a href="https://www.lesswrong.com/posts/D4gEDdqWrgDPMtasc/thoughts-on-process-based-supervision-1#4___Process_based_supervision___and_why_it_seems_to_solve_this_subproblem">process-based feedback</a> ,&quot; except that in a strict form of process-based feedback, you never look at <em>any</em> of the outcomes of the model&#39;s actions, whereas in this version, you can look at outcomes up to whatever time-horizon is efficient for you to get data about. <a href="#fnref-ZvAv8KJZnntyccKuJ-2" class="footnote-backref">↩︎</a></p></li><li id="fn-ZvAv8KJZnntyccKuJ-3" class="footnote-item"><p> For example, maybe you wanted to create a long-term goal regulated by some concept of &quot;honesty,&quot; which you were counting on to prevent scheming. But maybe you can&#39;t tell if you&#39;ve succeeded. <a href="#fnref-ZvAv8KJZnntyccKuJ-3" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/Xtb9SMrQofpxzEw4T/is-scheming-more-likely-in-models-trained-to-have-long-term#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Xtb9SMrQofpxzEw4T/is-scheming-more-likely-in-models-trained-to-have-long-term<guid ispermalink="false"> Xtb9SMrQofpxzEw4T</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Thu, 30 Nov 2023 16:43:07 GMT</pubDate> </item><item><title><![CDATA[Enkrateia: a safe model-based reinforcement learning algorithm]]></title><description><![CDATA[Published on November 30, 2023 3:51 PM GMT<br/><br/><p> In this post, we link to an accessible, academically written control-theoretic account of how to perform safe model-based reinforcement learning in a domain with significant moral constraints.</p><p> The ethicophysics can most properly be thought of as a new scientific paradigm in AI safety designed to give rigorous theoretical and historical justification to a particular style of learning cost functions for safety-critical systems.</p><p> In the rest of this post, we simply copy and paste the introduction of the paper.</p><p> ###</p><p> We describe LQPR (Linear-Quadratic-Program-Regulator), an algorithm for model-based reinforcement learning that allows us to prove PAC-style bounds on the behavior of the system. We describe proposed experiments that can be performed in the DeepMind AI Safety Gridworlds domain; we have not had time to implement these experiments yet, but we provide predictions as to the outcome of each experiment. Potential future work may include scaling up this work to non-trivial tasks using a neural network approximator, as well as proving additional theoretical results about the safety and stability of such a system. We believe that this system is a potential basis for aligning large language models and other powerful near-term AI&#39;s with human preferences.</p><p> 1 Introduction In this section, we lay out necessary context. 1.1 背景</p><p>There are three dominant approaches to discussing ethics in philosophy. These approaches are consequentialism, deontology, and virtue ethics. Any truly safe RL algorithm needs to be capable of incorporating all three of these approaches, since each has weaknesses that are addressed by the other two. The algorithm described in this document is capable of implementing all three of these approaches. Many have noted, especially Yudkowsky, that consequentialist agents are likely to be badly misaligned. This perspective seems quite accurate to us.</p><p> Meanwhile, it seems likely that deontological agents will be hamstrung by their strict adherence to rules, and end up paying too heavy of an alignment tax to be capable. Virtue ethics might be the sweet spot between these two extremes, since it seems likely that it is both alignable and capable. For the sake of completeness, we provide implementations of all three ethical paradigms within a single unified framework. The wise reader is encouraged not to implement a consequentialist agent without proving a sufficiently reassuring number of theorems that are verified to the limits of human capability. Even given such reassurances, it still seems unwise to implement a strongly capable consequentialist agent for anything other than military purposes.</p><p> 1.2 Scope This paper is intended to lay the theoretical foundations for safe model-based reinforcement learning. We do not discuss the problem of generalization, since that seems to require significant extensions that we do not have designs for yet. We also examine only smaller, simpler versions of the relevant problem. We will briefly indicate how these might be extended when such an extension seems relatively clear.</p><br/><br/> <a href="https://www.lesswrong.com/posts/WgzGsDdfzrwybwiob/enkrateia-a-safe-model-based-reinforcement-learning#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WgzGsDdfzrwybwiob/enkrateia-a-safe-model-based-reinforcement-learning<guid ispermalink="false"> WgzGsDdfzrwybwiob</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Thu, 30 Nov 2023 15:51:49 GMT</pubDate> </item><item><title><![CDATA[Normative Ethics vs Utilitarianism]]></title><description><![CDATA[Published on November 30, 2023 3:36 PM GMT<br/><br/><p> The primary point of this post is that I want to draw everyone&#39;s attention to an excellent <a href="https://twitter.com/jd_pressman/status/1729994771719180389">podcast</a> by John David Presserman where he chats with Zvi.</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ddfa529-629c-4546-a200-3b9f85bcc36b_606x354.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/si87gyjB4eRdnnScf/ieyhquprok6nfdnuglao" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/si87gyjB4eRdnnScf/rydcyoqvbczchp8qzhlj 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/si87gyjB4eRdnnScf/ci6gdoi8lpbqr9xcrqaw 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/si87gyjB4eRdnnScf/xwdgjomc9ilx6jcev3ms 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/si87gyjB4eRdnnScf/ieyhquprok6nfdnuglao 1456w"></a></p><p></p><p> John goes into the post itching for a fight with Zvi over LessWrong style arguments about deceptive inner-alignment. But Zvi doesn&#39;t really take the bait. Instead they spend most of the podcast talking about John&#39;s ideas regarding RLaiF.</p><h1> What does RLaiF got to do with Normative Ethics?</h1><p></p><p> John spends most of the podcast talking about the tendency of RLaiF models to fall into a degenerate state if you over-train them. The canonical example of this is a “ <a href="https://twitter.com/jd_pressman/status/1696973057133584490">Yes spammer</a> ”. In this case, the reinforcement learning is receiving its reward from a 2nd AI model that answer “yes” or “no” to “is this a good result”. Eventually the AI we are training learns to repeatedly output the word “yes” as this tricks the judge AI into also responding “yes” every single time.</p><p> John analogizes the tendency of RLaiF models to fall into degenerate feedback modes with the difference between rule-based ethical systems and outcome-based ethics systems (utilitarianism). In normative ethics systems, people follow a variety of rules based on historical practice such as “do not murder”, “do not steal&quot;, etc. By contrast, in utilitarianism, there are no hard and fast rules, instead adherents simply seek to maximize a single objective function, the total amount of “goodness” or utility in the world.</p><p> Just as utilitarianism has both bad <a href="https://plato.stanford.edu/entries/repugnant-conclusion/">theoretic</a> and <a href="https://www.cnbc.com/2023/11/04/sam-bankman-fried-sentence-could-be-100-plus-years-or-mere-decades.html">practical</a> failure modes, when AI is trained with a single objective function, it inevitably falls into strange undesirable local maxima. In ethics, we solve this problem by relying on a set of <a href="https://en.wikipedia.org/wiki/Ten_Commandments">historically developed</a> rules that have been empirically proven to avoid the worst outcomes. Similarly, John argues that we can avoid perverse-maximization in RLaiF frameworks by valuing the process by which we arrive at the global maxima.</p><h1> A Simple Research Proposal</h1><p></p><p> I would like to <a href="https://twitter.com/nagolinc/status/1730043307546341779">propose</a> testing John&#39;s hypothesis by building the simplest possible test case:</p><p> Train a RLaiF model that degenerates into the “yes spammer”</p><p> Along the way, “checkpoint” the reward function periodically</p><p> Create a new reward function that is a weighted sum of the checkpoints</p><p> Verify that with this new reward function training does not degenerate into the “yes spammer” (note this is trivially true if we allow our set of weights to be 1,0,0,0…)</p><p> Identify what the boundary condition is where RLaiF degenerates into the yes-spammer</p><h1>结论</h1><p></p><p>I would also like to echo John&#39;s conclusions at the end of the podcast:</p><p> Object-level discussion is valuable, but specific criticisms are more effective than broad, inaccurate statements.</p><p> Concerns about AI should focus on specific issues, like the stability of complex feedback loops, rather than general doubts.</p><p> there is an important trade-off between normative and consequentialist reasoning in AI.</p><p> Pure consequentialist reasoning in AI, without normative ethics, risks leading to negative outcomes.</p><p> The main concern is super consequentialism in AI, rather than super intelligence itself.</p><p> Super consequentialism is widely acknowledged as potentially dangerous.</p><p> The conditions under which super consequentialism arises should be a central part of AI ethics debates.</p><p> Current AI discourse suffers from mutual frustration due to its format and participants.</p><p> More peer-to-peer discussions, rather than expert-public or advocacy debates can lead to deeper understanding.</p><p> We should engage one another as peers, not as opponents, to foster constructive dialogue.</p><br/><br/> <a href="https://www.lesswrong.com/posts/si87gyjB4eRdnnScf/normative-ethics-vs-utilitarianism#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/si87gyjB4eRdnnScf/normative-ethics-vs-utilitarianism<guid ispermalink="false"> si87gyjB4eRdnnScf</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Thu, 30 Nov 2023 15:36:01 GMT</pubDate> </item><item><title><![CDATA[Information-Theoretic Boxing of Superintelligences]]></title><description><![CDATA[Published on November 30, 2023 2:31 PM GMT<br/><br/><p> Boxing an agent more intelligent than ourselves is daunting, but information theory, thermodynamics, and control theory provide us with tools that can fundamentally constrain agents independent of their intelligence. In particular, we may be able to contain an AI by limiting its access to information</p><h1> Constraining output and inputs</h1><p> Superintelligent AI has the potential to both help and harm us. One way to minimize the harm is <i>boxing</i> the AI: containing it so that it can&#39;t freely influence its external world, ideally while preserving its potential to help.</p><p> Boxing is complicated. Destroying the AI will certainly prevent it from influencing anything. Throwing it into a black hole might causally disconnect it from the rest of the universe while possibly preserving its existence, though even then it may exploit some unknown physics to <a href="https://en.wikipedia.org/wiki/Markov_blanket"><u>leak information back across the event horizon</u></a> , or to persist or recreate itself once the black hole has evaporated.</p><p> Can we box an AI while still being being able to <i>use</i> it? We could try preventing it from physically interacting with its environment and only permitting it to <a href="https://www.lesswrong.com/tag/oracle-ai"><u>present information</u></a> to a user, but a superintelligence could abuse any communication channels to manipulate its users into <a href="https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness"><u>granting it more power</u></a> or <a href="https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai"><u>improving its predictive abilities</u></a> . To successfully box the AI in this manner, we&#39;d need to constrain both its ability to physically interact with its environment and communicate information and manipulate. We&#39;ll call this <i>output boxing</i> : containing an AI by constraining its various outputs.</p><p> Most discussions of boxing focus on output boxing, but there&#39;s a neglected mirror to this approach that we&#39;ll call <i>input boxing</i> : containing an AI by constraining what information it can access. (For completeness, we could also consider boxing techniques independent of input and output, where we contain an AI by constraining its functioning, such as by limiting its computational speed or restarting it from initial conditions each second).</p><p> In the rest of this post, we&#39;ll introduce and examine a theoretical input boxing technique that contains an AI by controlling the number of bits of information it has about its environment, using results from information theory, thermodynamics, and control theory.</p><h1> Good regulators and bad keys</h1><p> Today&#39;s powerful AIs rely on mountains of data (during training, at least), so restricting their access to data would restrict their subsequent capabilities, but can we <i>fundamentally</i> limit a superintelligence&#39;s external influence just by constraining its access to data?</p><p> We know that agents need an understanding of their environments to act effectively within them: Conant &amp; Ashby studied this in a ( <a href="https://www.lesswrong.com/posts/wTJECdFSAk4843mFj/thoughts-on-the-good-regulator-theorem#:~:text=This%20is%20among%20the%20worst%20papers%20I%27ve%20ever%20skimmed."><u>much</u></a> <a href="https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem#:~:text=This%20may%20be%20the%20most%20misleading%20title%20and%20summary%20I%20have%20ever%20seen%20on%20a%20math%20paper."><u>maligned</u></a> ) 1970 paper on the <a href="http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf"><u>good regulator theorem</u></a> . A <i>regulator</i> is any mechanism that maintains a desired characteristic, like a thermostat, or a brain and its homeostatic subsystems. The good regulator theorem states that every good regulator of a system must contain a model of the system. Another parsing of the theorem states that <a href="http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf"><u>every good key must be a model of the lock it opens</u></a> , and John Wentworth <a href="https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem"><u>refines the theorem</u></a> (and its proof), clarifying that:</p><blockquote><p> <i>[Conant and Ashby actually prove] that every regulator which is optimal and contains no unnecessary noise is</i> equivalent to <i>a regulator which first reconstructs the variable-values of the system it&#39;s controlling, then chooses its output as a function of those values</i></p></blockquote><p> Can we, therefore, box an AI by preventing it from modeling its external environment? Can we stop the AI becoming a “good key” for the cage we&#39;ve placed it in? Let&#39;s look at the potential sources of information for such an AI:</p><ul><li> The initial conditions of the model (such as training data and learned patterns)</li><li> Sensory input (such as through cameras and microphones)</li><li> Logical deduction and extrapolation</li></ul><p> We&#39;ll assume that the AI we&#39;re trying to box is logically omniscient; that it can squeeze as much information from its initial conditions and sensory input as theoretically possible. This leaves two sources of information that are targets for input-boxing: initial conditions and sensory input.</p><p> In practice, restricting these sources of information is tricky. For example, we could try giving the AI only one sensor, some low-res camera with which to receive instruction. However, the image that the camera receives could be subtly influenced by the vibration of nearby voices, ambient magnetic fields, background radiation, and more. Interpreted correctly, these sources of interference become sources of information.</p><p> Supposing, though, that we can overcome such <i>practical</i> hurdles, can we quantify how much control we can muster from input boxing? To answer this, we&#39;ll look at control theory.</p><h1> Bits, boxes, and control</h1><p> First, some terminology. Like a regulator, a <i>control loop</i> controls the behavior of a variable using a sensor, a function, and some actuator. Using multiple control loops, a <i>control system</i> manages the behavior of a system. Control loops are <i>open-loop</i> when they don&#39;t act on feedback about the outcomes of their actions, like a heater on a timer. <i>Closed-loop</i> controllers incorporate feedback, like cruise control or thermostats. Finally, <i>entropy</i> here refers to <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"><u>Shannon entropy</u></a> , which measures uncertainty about a variable. Touchette &amp; Lloyd study the limits of control systems in <a href="https://arxiv.org/pdf/chao-dyn/9905039.pdf"><u>Information-theoretic limits of control</u></a> , writing that:</p><blockquote><p> <i>entropy offers a precise measure of disorderliness or missing information by characterizing the minimum amount of resources (bits) required to encode unambiguously the ensemble describing the system</i> .</p></blockquote><p> Summarizing their main results, they write:</p><blockquote><p> <i>in a control process, information must constantly be acquired, processed and used to constrain or maintain the trajectory of a system.</i></p><p> <i>the second law of thermodynamics…sets absolute limits to the minimum amount of dissipation required by open-loop control…an information-theoretic analysis of closed-loop control shows feedback control to be essentially a zero sum game: <strong>each bit of information gathered directly from a dynamical systems by a control device can serve to decrease the entropy of that system by at most one bit</strong> additional to the reduction of entropy attainable without such information (open-loop control).</i></p></blockquote><p> In other words, when including information as part of a thermodynamic system, the second law of thermodynamics tells us that you can&#39;t control a system perfectly. When a control loop adjusts its actions based on the information it gets from the system it&#39;s controlling, there&#39;s a fundamental trade-off: <strong>for each bit of information you collect from a system, you can reduce disorder of that system by only one more bit than you could&#39;ve without that information</strong> . There&#39;s a strict one-for-one limit on how much more you can improve control through information.</p><p> To formalize this, let t denote time, and let <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> denote the amount of information available to the boxed agent at time <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t=T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> . The change in <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span></span></span> is then given by time derivative of input information, labeled <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span> , minus the time derivative of output or “used” information, labeled <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="o"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">o</span></span></span></span></span></span></span> : </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/nbpozftdirivgc3bzeii"></figure><p> and so the available information at time <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> is </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/h8nhh8x2beyig8jbrljg"></figure><p> where C denotes the information contained in the initial conditions of the model. We can visualize (a simplified version of) this over consecutive timesteps, as information is gathered then used up: </p><figure class="image image_resized" style="width:52.04%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/q25vhab6anqlhmaabqka" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/ltki8r2cpipmkejyrwfz 141w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/semrw7icx8dvpk3jlnht 221w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/hij2loeprglaldhoa0hq 301w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/t6gkaadzgdlxsbi6cbbh 381w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6FAMJL89DKmSeYTha/qlfg1i1dg0jyompklqt7 461w"></figure><p>这看起来很有希望。 If we limit the information an AI can gather with respect to the information it uses to act in its environment, it will not have sufficient information to act effectively. Or, in the language of the paper above, the agent will not have enough bits about its external dynamical system to sufficiently reduce the entropy of that dynamical system.</p><p> We can define input-boxing more formally as follows: suppose we have an agent <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> that has an amount <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span></span></span> of information at time <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> . For <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> to achieve a certain external action <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> , it requires some amount of information <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span></span></span> about its external environment, and we can say that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> cannot achieve <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> when <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span></span></span> is smaller than <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span></span></span> . If we set <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> to be some minimal escape scenario, then we can say that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> is input boxed when <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T < I_X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span></span></span> . This doesn&#39;t depend on any of the agent&#39;s capabilities or intelligence, just its <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span></span></span> , which we are supposing we can limit, and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="I_X"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.064em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span></span></span> , which depends only on the external environment.</p><p> John Wentworth, author of <a href="https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem"><u>Fixing The Good Regulator Theorem</u></a> , which we used earlier, considered a similar <a href="https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation"><u>question about the limits of control</u></a> . He conjectured that observing an additional N bits of information allows a system to perform at most N additional bits of optimization. He concluded that the conjecture is <a href="https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=Pvhpc7PF8DhsyMGgk"><u>false</u></a> , citing a counterexample where a short password unlocks a long message, concluding that “ <i>bits of observation can give a really ridiculously large boost to a system&#39;s optimization power”</i> . Others <a href="https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but"><u>agree</u></a> , arguing that, with some practical constraints, “ <i>there are no limits on how much optimization one can squeeze out of a single bit, even in a world with reversible computing”</i> .</p><p> However, these counterexamples rely on the agent&#39;s ability to subsequently “follow up” on what they learn. The information in the “password” is only useful insofar as it grants the opportunity to gather or unlock more information. The instantaneous utility of the password, or any piece of information, is bounded by the sum of the information in the password itself and prior knowledge, and by assumption we&#39;re limiting the agent&#39;s access to follow-up information as well. Therefore, these counterexamples shouldn&#39;t exempt an input-boxed AI from the limits on optimization. If we can limit the information an agent gathers from inputs and prior knowledge, we can box that agent, regardless of its intelligence.</p><h1> Building better boxes</h1><p> Input boxing is a promising avenue for research as it tackles the challenge of limiting an AI&#39;s capabilities through the fundamentals of information, independent of how smart the AI in the box is.</p><p> This article is just an introduction, and we&#39;re interested in exploring further refinements of the idea of input boxing. For example, a deeper analysis of <i>what</i> information we allow into the box may give us more fine-grained control over an AI (such as in <a href="https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation?commentId=n5b4RvdrSxFsqCFw5#n5b4RvdrSxFsqCFw5"><u>this discussion</u></a> about information about environments versus laws), especially if we can robustly distinguish between information useful for solving our problems and useful for escape.</p><p> That is, rather than fundamentally limiting the agent&#39;s <i>overall</i> control, we may be able to identify hazardous paths, or sequences of actions, that the agent will want to pursue. That is, for an agent to achieve an action it requires information about the control pathway between its actuators and its ultimate desired outcome. <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> must have the information about how its first action will transform its external environment, and how subsequent actions will transform the new, changed environment, and so on.</p><p> If we limit the agent&#39;s access to that hazardous information about dangerous control pathway, we could relax the fundamental constraints on information input without permitting hazardous behavior. This could be much more pragmatic than fundamentally limiting all information, since hazardous information is probably a small subset of all information - bits have to be relevant to hazardous actions to be hazardous bits. However, this would come at the cost of the guarantee those fundamental limits provide.</p><p> Another promising avenue for developing input-boxing, suggested by Deric Cheng, would be to impose fundamental delays on the use of information. The utility of information decays over time, so if an AI could only act on information from 24 hours ago, it&#39;ll be much weaker, especially for high-speed actions such as exploiting the stock market.</p><p> As well as a boxing technique for superintelligences, input boxing may be useful for more niche applications. For example, by input boxing AI models during <a href="https://www.lesswrong.com/tag/ai-evaluations"><u>evals</u></a> , we could test a model with the guarantee that any hazardous behavior won&#39;t leak out the box, meaning evaluators could actively prompt hazardous behavior and use <a href="https://en.wikipedia.org/wiki/Red_team"><u>red-teaming</u></a> techniques without compromising the <a href="https://www.lesswrong.com/posts/XCRsg2ZnHBNAN862T/improving-the-safety-of-ai-evals"><u>safety of the eval</u></a> .</p><p> With further refinements and insights from information theory, control theory, and thermodynamics, input boxing could be a powerful tool for both niche applications and for wrapping a superintelligence up with a neat, little bow in a safe, useful box.</p><p> This post was initially inspired by Hugo Touchette &amp; Seth Lloyd&#39;s <a href="https://arxiv.org/pdf/chao-dyn/9905039.pdf"><u>Information-Theoretic Limits of Control</u></a> paper. For more on control theory, the good regulator theorem, and other similar topics, check out:</p><ul><li> <a href="https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it"><u>What is control theory, and why do you need to know about it?</u></a> by Richard_Kennaway</li><li> <a href="https://arxiv.org/pdf/2006.05604.pdf"><u>Machine learning and control theory</u></a> by Alain Bensoussan <i>et al</i> .</li><li> <a href="https://www.lesswrong.com/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem"><u>Fixing The Good Regulator Theorem</u></a> by johnswentworth</li><li> <a href="https://johncarlosbaez.wordpress.com/2016/01/27/the-good-regulator-theorem/"><u>The Internal Model Principle</u></a> by John Baez</li><li> <a href="http://cadia.ru.is/wiki/_media/public:t-720-atai:a_primer_for_conant_and_ashby_s_good-regulator_theorem.pdf"><u>A Primer For Conant &amp; Ashby&#39;s  “Good-Regulator Theorem”</u></a> by Daniel L. Sholten</li><li> <a href="https://harishsnotebook.wordpress.com/2020/08/23/notes-on-the-good-regulator-theorem/"><u>Notes on The Good Regulator Theorem</u></a> in Harish&#39;s Notebook</li><li> <a href="https://www.lesswrong.com/posts/6rZroG3mztowktJzp/how-many-bits-of-optimization-can-one-bit-of-observation"><u>How Many Bits Of Optimization Can One Bit Of Observation Unlock?</u></a> by johnswentworth<ul><li> <a href="https://www.lesswrong.com/posts/dfTm26pvq7yQp8mR3/one-bit-of-observation-can-unlock-many-of-optimization-but"><u>One bit of observation can unlock many of optimization - but at what cost?</u></a> by dr_s.</li></ul></li><li> <a href="https://arxiv.org/abs/2309.01933"><u>Provably safe systems: the only path to controllable AGI</u></a> by Max Tegmark and Steve Omohundro</li></ul><p> <i>This article is based on the ideas of Justin Shovelain, written by Elliot Mckernon, for</i> <a href="https://www.convergenceanalysis.org/"><i><u>Convergence Analysis</u></i></a> <i>. We&#39;d like to thank the authors of the posts we&#39;ve quoted, and Cesare Ardito, David Kristoffersson, Richard Annilo, and Deric Cheng for their feedback while writing.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/NZP6QvkXryJQFGkLF/information-theoretic-boxing-of-superintelligences-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/NZP6QvkXryJQFGkLF/information-theoretic-boxing-of-superintelligences-1<guid ispermalink="false"> NZP6QvkXryJQFGkLF</guid><dc:creator><![CDATA[JustinShovelain]]></dc:creator><pubDate> Thu, 30 Nov 2023 14:31:11 GMT</pubDate> </item><item><title><![CDATA[OpenAI: Altman Returns]]></title><description><![CDATA[Published on November 30, 2023 2:10 PM GMT<br/><br/><p> As of this morning, <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board">the new board is in place and everything else at OpenAI is otherwise officially back to the way it was before</a> .</p><p> Events seem to have gone as expected. If you have read my <a href="https://thezvi.substack.com/p/openai-the-battle-of-the-board" target="_blank" rel="noreferrer noopener">previous two posts</a> on the OpenAI situation, nothing here should surprise you.</p><span id="more-23613"></span><p> Still seems worthwhile to gather the postscripts, official statements and reactions into their own post for future ease of reference.</p><p> What will the ultimate result be? We likely only find that out gradually over time, as we await both the investigation and the composition and behaviors of the new board.</p><p> I do not believe Q* played a substantive roll in events, so it is not included here. I also do not include discussion here of how good or bad Altman has been for safety.</p><h4> Sam Altman&#39;s Statement</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board">Here is the official OpenAI statement from Sam Altman</a> . He was magnanimous towards all, the classy and also smart move no matter the underlying facts. As he has throughout, he has let others spread hostility, work the press narrative and shape public reaction, while he himself almost entirely offers positivity and praise.聪明的。</p><blockquote><p>在开始接下来的内容之前，我想先表达一些谢意。</p><p>我爱并尊重伊利亚，我认为他是这个领域的指路明灯，也是人类的瑰宝。我对他的恶意为零。虽然 Ilya 将不再担任董事会成员，但我们希望继续我们的工作关系，并正在讨论他如何继续在 OpenAI 的工作。</p><p>我感谢 Adam、Tasha 和 Helen 与我们合作，找到了最能服务于使命的解决方案。我很高兴继续与 Adam 合作，并衷心感谢 Helen 和 Tasha 在此过程中投入了大量的精力。</p><p>还要感谢埃米特，他在帮助我们实现这一成果方面发挥了关键和建设性的作用。 Emmett 对人工智能安全和平衡利益相关者利益的奉献是显而易见的。</p><p>米拉在整个过程中表现出色，自始至终无私地为使命、团队和公司服务。她是一位令人难以置信的领导者，如果没有她，OpenAI 就不会成为 OpenAI。谢谢。</p><p>格雷格和我是经营这家公司的合伙人。我们从未完全弄清楚如何在组织结构图上传达这一点，但我们会的。与此同时，我只是想澄清一下。感谢你们从一开始以来所做的一切，以及从开始到上周你们处理事情的方式。</p><p>领导团队——Mira、Brad、Jason、Che、Hannah、Diane、Anna、Bob、Srinivas、Matt、Lilian、Miles、Jan、Wojciech、John、Jonathan、Pat 等等——显然已准备好在没有任何情况下运营公司。我。他们说，评估首席执行官的一种方法是看你如何挑选和培训你的潜在继任者；在这个指标上，我做得比我想象的要好得多。我很清楚，公司掌握在伟大的人手中，我希望每个人都清楚这一点。谢谢你们。</p></blockquote><p> Let that last paragraph sink in. The leadership team ex-Greg is clearly ready to run the company without Altman.</p><p> That means that whatever caused the board to fire Altman, whether or not Altman forced the board&#39;s hand to varying degrees, if everyone involved had chosen to continue without Altman then OpenAI would have been fine. We can choose to believe or not believe Altman&#39;s claims in his Verge interview that he only considered returning after the board called him on Saturday, and we can speculate on what Altman otherwise did behind the scenes during that time.我们不知道。 We can of course guess, but we do not know.</p><p> He then talks about his priorities.</p><blockquote><p><strong>下一个是什么？</strong></p><p>我们有三个当务之急。</p><p>推进我们的研究计划并进一步投资于我们的全栈安全工作，这对我们的工作一直至关重要。我们的研究路线图很明确；这是一个非常专注的时刻。我和你们一样感到兴奋；我们将化危机为机遇！我会和米拉一起解决这个问题。</p><p>持续改进和部署我们的产品并服务我们的客户。重要的是，人们要体验人工智能的好处和前景，并有机会塑造它。我们始终相信，优秀的产品是实现这一目标的最佳方式。我将与 Brad、Jason 和 Anna 合作，确保我们对世界各地的用户、客户、合作伙伴和政府的坚定承诺是明确的。</p><p>布雷特、拉里和亚当将非常努力地完成一项极其重要的任务，即建立一个具有不同观点的董事会、改善我们的治理结构以及监督对最近事件的独立审查。我期待在这些关键步骤上与他们密切合作，以便每个人都能对 OpenAI 的稳定性充满信心。</p><p>我非常期待与你们一起完成构建有益的 AGI 的工作——世界上最好的团队，世界上最好的使命。</p></blockquote><p> Research, then product, then board. Such statements cannot be relied upon, but this was as good as such a statement can be. We must keep watch and see if such promises are kept. What will the new board look like? Will there indeed be a robust independent investigation into what happened? Will Ilya and Jan Leike be given the resources and support they need for OpenAI&#39;s safety efforts?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/11/29/23982046/sam-altman-interview-openai-ceo-rehired">Altman gave an interview to The Verge</a> . Like the board, he (I believe wisely and honorably) sidesteps all questions about what caused the fight with the board and looks forward to the inquiry. In Altman&#39;s telling, it was not his idea to come back, instead he got a call Saturday morning from some of the board asking him about potentially coming back.</p><p> He says he is not focused on getting back on the board, that is not his focus, but that the governance structure clearly has a problem that will take a while to fix.</p><blockquote><p> Q: <strong>What does</strong> <a target="_blank" rel="noreferrer noopener" href="https://www.theverge.com/2023/11/29/23981848/sam-altman-back-open-ai-ceo-microsoft-board"><strong>“improving our governance structure”</strong></a> <strong>mean?非营利控股公司的结构会发生变化吗？</strong></p><p> Altman: It&#39;s a better question for the board members, but also not right now.诚实的答案是他们需要时间，我们将支持他们真正开始思考这个问题。显然我们的治理结构有问题。解决这个问题的最好方法是需要一段时间。我完全理解为什么人们现在就想要答案。 But I also think it&#39;s totally unreasonable to expect it.</p><p> ……</p><p>哦，仅仅因为设计一个非常好的治理结构，特别是对于如此有影响力的技术来说，不是一个一周的问题。 It&#39;s gonna take a real amount of time for people to think through this, to debate, to get outside perspectives, for pressure testing.那只需要一段时间。</p></blockquote><p>是的。 It is good to see this highly reasonable timeline and expectations setting, as opposed to the previous tactics involving artificial deadlines and crises.</p><p> Mutari confirms in the interview that OpenAI&#39;s safety approach is not changing, that this had nothing to do with safety.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1730032994474475554">Altman also made a good statement about Adam D&#39;Angelo&#39;s potential conflicts of interest</a> , saying he actively wants customer representation on the board and is excited to work with him again. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sama/status/1727858661677240767">Altman also spent several hours with D&#39;Angelo.</a></p><h4> Bret Taylor&#39;s Statement</h4><p> We also have the statement from Bret Taylor. We know little about him, so reading his first official statement carefully seems wise.</p><blockquote><p>我谨代表 OpenAI 董事会向整个 OpenAI 社区，特别是所有 OpenAI 员工表示感谢，他们在过去的一周里齐心协力，为公司找到了前进的道路。你们的努力帮助这个令人难以置信的组织继续履行其使命，确保通用人工智能造福全人类。我们很高兴 Sam、Mira 和 Greg 重新齐心协力领导公司并推动公司向前发展。我们期待与他们和你们所有人合作。</p><p>作为董事会，我们致力于加强 OpenAI 的公司治理。我们计划这样做：</p><ul><li>我们将建立一个由杰出人士组成的合格、多元化的董事会，他们的集体经验代表了 OpenAI 使命的广度——从技术到安全到政策。我们很高兴董事会将包括一名无投票权的 Microsoft 观察员。</li><li>我们将进一步稳定 OpenAI 组织，以便我们能够继续履行我们的使命。这将包括召集董事会独立委员会来监督对近期事件的审查。</li><li>我们将加强 OpenAI 的治理结构，让所有利益相关者——用户、客户、员工、合作伙伴和社区成员——都能相信 OpenAI 将继续蓬勃发展。</li></ul><p> OpenAI 是一个比以往任何时候都更加重要的机构。 ChatGPT 让人工智能成为数亿人日常生活的一部分。它的普及使得人工智能——它的好处和风险——成为几乎所有有关政府、企业和社会未来的对话的核心。</p><p>我们了解这些讨论的重要性以及 OpenAI 在这些令人惊叹的新技术的开发和安全中的核心作用。在确保我们有效应对这些挑战方面，你们每个人都发挥着关键作用。我们致力于倾听你们的声音并向你们学习，我希望很快能与你们所有人交谈。</p><p>我们很高兴成为 OpenAI 的一部分，并很高兴与大家合作。</p></blockquote><p> Mostly this is Brad Taylor properly playing the role of chairman of the board, which tells us little other than that he knows the role well, which we already knew.</p><p> Microsoft will get only an observer on the board, other investors presumably will not get seats either. That is good news, <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/openai-isnt-expected-to-offer-microsoft-other-investors-a-board-seat">matching reporting from The Information</a> .</p><p> What does &#39;enhance the governance structure&#39; mean here?我们不知道。 It could be exactly what we need, it could be a rubber stamp, it could be anything else. We do not know what the central result will be.</p><p> The statement on a review of recent events here is weaker than I would like. It raises the probability that the new board does not get or share a true explanation.</p><p> He mentions safety multiple times. Based on what I know about Taylor, my guess is he is unfamiliar with such questions, and does not actually know what that means in context, or what the stakes truly are. Not that he is dismissive or skeptical, rather that he is encountering all this for the first time.</p><h4> Larry Summers&#39;s Statement</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/LHSummers/status/1730047296375590995">Here is the announcement via Twitter from board member Larry Summers</a> , which raises the bar in having exactly zero content. So we still know very little here.</p><blockquote><p> Larry Summers: I am excited and honored to have just been named as an independent director of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> .我期待与董事会同事和 OpenAI 团队合作，推进 OpenAI 极其重要的使命。</p><p> First steps, as outlined by Bret and Sam in their messages, include building out an exceptional board, enhancing governance procedures and supporting the remarkable OpenAI community.</p></blockquote><h4> Helen Toner&#39;s Statement</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/hlntnr/status/1730034020140912901">Here is Helen Toner&#39;s full Twitter statement</a> upon resigning from the board.</p><blockquote><p> Helen Toner (11/29): Today, I officially resigned from the OpenAI board.感谢许多朋友、同事和支持者，他们公开和私下表示，他们知道我们的决定始终是由我们对 OpenAI 使命的承诺驱动的。</p><p>关于过去一两周的文章已经写了很多；肯定还会说更多。目前，即将上任的董事会已宣布将进行全面的独立审查，以确定下一步的最佳措施。</p><p>需要明确的是：我们的决定是关于董事会有效监督公司的能力，这是我们的角色和责任。尽管有猜测，但我们的动机并不是要放慢 OpenAI 的工作速度。</p><p>当我在 2021 年加入 OpenAI 董事会时，我和我周围的许多人已经清楚，这是一个将做大事的特殊组织。能够成为该组织的一员是一种巨大的荣幸，因为世界其他地方也意识到了同样的事情。</p><p>我非常尊重 OpenAI 团队，并祝愿他们以及即将上任的 Adam、Bret 和 Larry 董事会一切顺利。我将继续专注于人工智能政策、安全和保障方面的工作，所以我知道我们的道路在未来几年将会多次交叉。</p></blockquote><p> Many outraged people continue to demand clarity on why the board fired Altman. I believe that most of them are thrilled that Toner and others continue not to share the details, and are allowing the situation outside the board to return to the status quo ante.</p><p> There will supposedly be an independent investigation. Until then, <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/openai-the-battle-of-the-board">I believe we have a relatively clear picture of what happened</a> . Toner&#39;s statement hints at some additional details.</p><h4> OpenAI Needs a Strong Board That Can Fire Its CEO</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1727498453864026603">Roon gets it</a> . The board needs to keep its big red button going forward, but still must account for its actions if it wants that button to stick.</p><blockquote><p> Roon: The board has a red button but also must explain why its decisions benefit humanity. If it fails to do so then it will face an employee, customer, partner revolt. OpenAI currently creates a massive amount of value for humanity and by default should be defended tooth and nail. The for-profit would not have been able to unanimously move elsewhere if there was even a modicum of respect or good reasoning given.</p></blockquote><p> The danger is that <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robbensinger/status/1727786741019582855">if we are not careful, we will learn the wrong lessons.</a></p><blockquote><p> Toby Ord: The last few days exploded the myth that Sam Altman&#39;s incredible power faces any accountability. He tells us we shouldn&#39;t trust him, but we now know the board *can&#39;t* fire him.我认为这很重要。</p><p> Rob Bensinger: We didn&#39;t learn “they can&#39;t fire him”. We did learn that the organization&#39;s staff has enough faith in Sam that the staff won&#39;t go along with the board&#39;s wishes absent some good supporting arguments from the board. (Whether they&#39;d have acceded to good arguments is untested.)</p><p> ……</p><p> I just want us to be clear that the update about the board&#39;s current power shouldn&#39;t be a huge one, because it&#39;s possible that staff would have accepted the board&#39;s decision in this case if the board had better explained its reasoning and the reasoning had seemed stronger.</p></blockquote><p>这么。 From our perspective, the board botched its execution and its members made relatively easy rhetorical targets. That is true even if the board had good reasons for doing so. If the board had not botched its execution and had more gravitas? I think things go differently.</p><p> If after an investigation, Summers, D&#39;Angelo and Taylor all decide to fire Altman again (note that I very much do not expect this, but if they did decide to do it), I assure you they will handle this very differently, and I would predict a very different outcome.</p><p> One of the best things about Sam Altman is his frankness that <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=dY1VK8oHj5s">we should not trust him</a> . Most untrustworthy people say the other thing. Same thing with Altman&#39;s often very good statements about existential risk and the need for safety. When people bring clarity and are being helpful, we should strive to reward that, not hold it against them.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchPhD/status/1730064832152654189">I also agree with Andrew Critch here</a> , that it was good and right for the board to pull the plug on a false signal of supervision. If the CEO makes the board unable to supervise them, or otherwise moves against the board, then it is the duty of the board to bring things to a head, even if there are no other issues present.</p><p> Good background, potentially influential in the thinking of several board members including Helen Toner: Former OpenAI board member Holden Karnofsky&#39;s old explanation of why and exactly how <a target="_blank" rel="noreferrer noopener" href="https://www.cold-takes.com/nonprofit-boards-are-weird-2/#the-boards-main-duties">Nonprofit Boards are Weird</a> , and how best to handle it.</p><h4> Some Board Member Candidates</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1729200181349126232">Eliezer Yudkowsky proposes Paul Graham for the board of OpenAI</a> . I see the argument, especially because Graham clearly cares a lot about his kids. My worries are that he would be too steerable by Altman, and he would be too inclined to view OpenAI as essentially a traditional business, and let that overrule other questions even if he knew it shouldn&#39;t.</p><p> If he was counted as an Altman ally, as he presumably should, then he&#39;s great. On top of the benefits to OpenAI, it would provide valuable insider information to Graham. Eliezer clarifies that his motivation is that he gives Graham a good chance of figuring out a true thing when it matters, which also sounds right.</p><p> Emmett Shear also seems like a clearly great consensus pick.</p><p> One concern is that the optics of the board matter. You would be highly unwise to choose a set of nine white guys. See Taylor&#39;s statement about the need for diverse perspectives.</p><h4> A Question of Valuation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-11-27/openai-is-still-an-86-billion-nonprofit?sref=1kJVNqnU">Matt Levine covers developments since Tuesday</a> , especially that the valuation of OpenAI in its upcoming sale did not change, as private markets can stubbornly refuse to move their prices. In my model, private valuations like this are rather arbitrary, and based on what social story everyone involved can tell and everyone&#39;s relative negotiating position, and what will generate the right momentum for the company, rather than a fair estimate of value. Also everyone involved is highly underinvested or overinvested, has no idea what fair value actually is, and mostly wants some form of social validation so they don&#39;t feel too cheated on price. Thus, often investors get away with absurdly low prices, other times they get tricked into very high ones.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://garymarcus.substack.com/p/top-5-reasons-why-openai-was-probably">Gary Marcus says OpenAI was never worth $86 billion</a> . I not only disagree, I would (oh boy is this not investment advice!) happily invest at $86 billion right now if I had that ability (which I don&#39;t) and thought that was an ethical thing to do. Grok very much does not &#39;replicate most of&#39; GPT-4, the model is instead holding up quite well considering how long they sat on it initially.</p><p> OpenAI is nothing without its people. That does not mean they lack all manner of secret sauce. In valuation terms I am bullish. Would the valuation have survived without Altman? No, but in the counterfactual scenario where Altman was stepping aside due to health issues with an orderly succession, I would definitely have thought $86 billion remained cheap.</p><h4> A Question of Optics</h4><p> A key question in all this is the extent to which the board&#39;s mistake was that its optics were bad. So here is <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1728014952877748297">a great example of Paul Graham advocating for excellent principles</a> .</p><blockquote><p> Paul Graham: When people criticize an action on the grounds of the “optics,” they&#39;re almost always full of shit. All they&#39;re really saying is “What you did looks bad.” But if they phrased it that way, they&#39;d have to answer the question “Was it actually bad, or not?”</p><p> If someone did something bad, you don&#39;t need to talk about “optics.” And if they did something that seems bad but that you know isn&#39;t, why are you criticizing it at all? You should instead be explaining why it&#39;s not as bad as it seems.</p></blockquote><p> Bad optics can cause bad things to happen. So can claims that the optics are bad, or worries that others will think the optics are bad, or claims that you are generally bad at optics.</p><p> You have two responses.</p><ol><li> That means it had bad consequences, which means it was actually bad.</li><li> Nobly stand up for right actions over what would &#39;look good.&#39;</li></ol><p> Consider the options in light of recent events. We all want it to be one way. Often it is the other way.</p><br/><br/> <a href="https://www.lesswrong.com/posts/EfqAdxR7bvwQLMTQc/openai-altman-returns#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EfqAdxR7bvwQLMTQc/openai-altman-returns<guid ispermalink="false"> EfqAdxR7bvwQLMTQc</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 30 Nov 2023 14:10:08 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Remarks on the Convergence in Distribution of Random Neural Networks to Gaussian Processes in the Infinite Width Limit]]></title><description><![CDATA[Published on November 30, 2023 2:01 PM GMT<br/><br/><p> The <a href="https://drive.google.com/file/d/1O_RbD45RjBWD4-a8tcjosvyXOxcD_q2V/view?usp=sharing">linked note</a> is something I &quot;noticed&quot;  while going through different versions of this result in the literature. I think that this sort of mathematical work on neural networks is worthwhile and worth doing to a high standard but I have no reason to think that this particular work is of much consequence beyond filling in a gap in the literature. It&#39;s the kind of nonsense that someone who has done too much measure theory would think about.<br><br><strong>抽象的。</strong> We describe a direct proof of yet another version of the result that a sequence of fully-connected neural networks converges to a Gaussian process in the infinite-width limit. The convergence in distribution that we establish is the weak convergence of probability measures on the non-separable,  non- <u>metrizable</u> product space <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbf{R}^{d'})^{\mathbf{R}^d}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.619em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.065em; padding-right: 0.06em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.467em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>, ie the space of functions from<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{R}^d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.619em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span></span></span> to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{R}^{d'}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.619em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.435em; padding-left: 0.065em; padding-right: 0.06em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span></span></span></span></span></span></span></span></span></span> with the topology whose convergent sequences correspond to <u>pointwise</u> convergence. The result itself is already implied by a stronger such <a href="https://arxiv.org/abs/2107.01562">theorem due to Boris <u>Hanin</u></a> , but the direct proof of our weaker result can afford to replace the more technical parts of <u>Hanin&#39;s</u> proof that are needed to establish tightness with a shorter and more abstract measure-theoretic争论。</p><br/><br/> <a href="https://www.lesswrong.com/posts/nM7qwfbB9dAxBopLT/linkpost-remarks-on-the-convergence-in-distribution-of-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/nM7qwfbB9dAxBopLT/linkpost-remarks-on-the-convergence-in-distribution-of-1<guid ispermalink="false"> nM7qwfbB9dAxBopLT</guid><dc:creator><![CDATA[Spencer Becker-Kahn]]></dc:creator><pubDate> Thu, 30 Nov 2023 14:01:33 GMT</pubDate> </item><item><title><![CDATA[Buy Nothing Day is a great idea with a terrible app— why has nobody built a killer app for crowdsourced 'effective communism' yet?]]></title><description><![CDATA[Published on November 30, 2023 1:47 PM GMT<br/><br/><p> I&#39;ve been seeing a lot of people really excited about very leftist ideas on TikTok lately, as well as being very frustrated at the way that the &#39;developed&#39; world exists, and I keep getting this idea of an app that would finally create some sort of framework for the average person to produce mutual aid.</p><p> The basic idea is that you could list things that you want to share. Other people can list things they need. The only limits would be, officially, no prepared foods, for legal reasons, and maybe other things that I haven&#39;t forseen. And that would be it! An app to share things you have extra of, with your neighbors.</p><p> So, that&#39;s been stewing around in my mind for a little bit, and then today, what do you know? I discover that &#39;Buy Nothing Day&#39;, a movement around the idea of a day where nobody buys anything, has an app like this! And it has existed for two years! But it sucks. All the reviews claim that basic functionality like making posts is busted despite praising the idea.</p><p> And my question is ultimately this: why has there not been a version of this sort of thing yet? Have economic conditions simply not been bad enough for people to want to help and rely on their neighbors?</p><p> Did craigslist pave the way despite becoming a cesspool of overly expensive crap and deceptively listed &#39;$1&#39; ads for businesses, that require you to drive 8 miles to pick it up while someone side-eyes you suspiciously from their driveway and waits for the venmo to经过？</p><p> Is this idea not sustainable or good in some way I&#39;m missing? It seems like a no brainer to me and nothing but a public good. People could point to some theoretical stranger danger, but that just seems like razorblades-in-halloween-candyesque fearmongering.</p><br/><br/> <a href="https://www.lesswrong.com/posts/v3EfCmegqjzQTfFTP/buy-nothing-day-is-a-great-idea-with-a-terrible-app-why-has#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/v3EfCmegqjzQTfFTP/buy-nothing-day-is-a-great-idea-with-a-terrible-app-why-has<guid ispermalink="false"> v3EfCmegqjzQTfFTP</guid><dc:creator><![CDATA[lillybaeum]]></dc:creator><pubDate> Thu, 30 Nov 2023 13:47:38 GMT</pubDate></item></channel></rss>