<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 29 日，星期日 12:20:14 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The AI Boom Mainly Benefits Big Firms, but long-term, markets will concentrate]]></title><description><![CDATA[Published on October 29, 2023 8:38 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/guE3R9GLZLsCjK3ve/the-ai-boom-mainly-benefits-big-firms-but-long-term-markets-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/guE3R9GLZLsCjK3ve/the-ai-boom-mainly-benefits-big-firms-but-long-term-markets-1<guid ispermalink="false"> GUE3R9GLZLsCjK3ve</guid><dc:creator><![CDATA[Hauke Hillebrandt]]></dc:creator><pubDate> Sun, 29 Oct 2023 08:38:23 GMT</pubDate> </item><item><title><![CDATA[What's up with "Responsible Scaling Policies"?]]></title><description><![CDATA[Published on October 29, 2023 4:17 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 17:36:08 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 17:36:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>我有兴趣谈论 RSP 是好是坏。我对此感到非常困惑，并且很乐意与某人深入探讨这一问题。</p><p>我目前对RSP的感受大致如下：</p><p>我对人工智能 X 风险的术语被重新定义感到有点高度警惕和有点偏执，因为这感觉就像是“人工智能对齐”中发生过很多次的事情，而且当你试图影响大型官僚机构（另见奥威尔关于政府行为的所有常见内容）。我对 RSP 的担忧很大一部分是对“负责任的扩展政策”一词的具体担忧。</p><p>我还觉得存在一种脱节，有点莫特和贝利的感觉，我们有一个 RSP 的真实实例，以 Anthropic RSP 的形式，然后来自 ARC Evals 的一些人我觉得更像是 RSP 的某种柏拉图式理想的模型，我觉得它们被混为一谈了。就像，我同意有些类似于 RSP 的东西可能会很棒，但我觉得特别是 Anthropic RSP 并没有真正的牙齿，所以有点扁平，就像人们想象的那样帮助应对风险。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 17:41:38 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 17:41:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p><strong>免责声明</strong>：我主要不从事宣传或政策工作，如果我在这些领域工作更多，我对这些主题的看法可能会大幅更新。也就是说，我工作的很大一部分<i>确实</i>涉及思考诸如“好的安全论点是什么样的？”之类的问题。以及“根据当前的安全技术状况，我们可以采取哪些措施来评估和减轻人工智能风险？”。 <i>（这是在编辑时添加的。）</i></p><hr><p>谈论可能有趣的事情：</p><ul><li>反收购/安全倡导应该做什么？</li><li>当前的技术实际上可以在多大程度上降低风险？这是症结所在吗？</li><li>目前避免收购的干预措施是什么样的？</li><li>定时暂停重要吗？</li><li>硬核实际上良好的长期暂停会是什么样子等？</li><li>实验室是否真的会跟进等等？</li><li>收购担忧与滥用行为混在一起是不是很糟糕？也许并没有那么糟糕？就像我实际上并不很担心 ASL-3 等。或者也许这很好，因为一些对策转移了？</li><li>未来会有好的RSP吗？ （就像 Anthropic 的 RSP 基本上是关于最重要问题的 RSP IOU。我认为这可能还可以。） </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 17:49:56 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 17:49:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我对 RSP 的基本看法：</p><ul><li>我同意<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation.">https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation 中的大部分内容。</a>例外情况：<ul><li>我对风险的基线猜测比 Paul 的要高一些，因此我认为 RSP 不会让您的风险降至 1%</li><li>我不太看好将风险降低 10 倍，但也许 5 倍似乎相当可行？我不知道</li><li>我认为未来 5 年内不太可能出现基于机械互助之类的良好肯定安全案例，因此此类要求似乎接近事实上的禁令。 （我不确定保罗是否不同意我的观点，但这似乎是一件值得注意的重要事情。）</li></ul></li><li>我同意这个词看起来很糟糕。感觉可能是一个错误。我还发现 OpenAI 使用不同的术语（RDP）很有趣</li><li>我并不担心未知的风险，但我确实认为“人工智能扰乱你的评估”是一个大问题。我认为可以通过检查人工智能是否能够干扰你的评估来解决这个问题（这可能比仅仅避免它们干扰你的评估更容易）。</li></ul></section><h2> RSP 的命名和背后的意图</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 17:53:11 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 17:53:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我认为如果 ARC 说这样的话会更好：“实验室应该制定扩大规模的政策：扩展政策 (SP)。我们希望这些政策实际上是安全的，我们很乐意就 SP 的责任进行咨询并降低风险”。<br><br>我还更喜欢“条件安全改进政策”或类似名称。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 17:56:35 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 17:56:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>我确实觉得“负责任的扩展政策”这个术语显然引用了一些我认为不正确的事情：</p><ul><li> “扩展”的速度是负责任地使用人工智能的首要因素</li><li>显然可以负责任地扩展（否则政策会管辖什么）</li><li>人工智能研究组织的默认轨迹应该是继续扩大规模</li></ul><p><a href="https://twitter.com/jamespayor/status/1715728849214857444">推特上的</a>James Payor 有一种类似的观点，引起了我的共鸣：</p><blockquote><p>我在这里补充一点，“AGI 扩展政策”将是一个比“负责任的扩展政策”更诚实的名字。</p><p> “RSP”这个名字隐含地表明（a）没有理由停下来； (b) 可以负责任地扩大规模； (c) 政策本体不可修改； ETC。</p><p> ……这些感觉像是可怕的假设，似乎是故意让协调人工智能停止变得更加困难。我确实喜欢这些内容，但我对这些东西的存在和相关的影响感到非常不满。</p></blockquote><p>我也对 RSP 的定义应该是什么感到有点困惑。 ARC Evals 帖子只是说：</p><blockquote><p> RSP 指定了人工智能开发人员准备使用当前的保护措施安全处理什么级别的人工智能功能，以及在保护措施改善之前继续部署人工智能系统和/或扩大人工智能功能过于危险的条件。</p></blockquote><p>但我认为这并不是一个充分的定义。当然，有很多符合这个定义的东西被称为 RSP 会很奇怪。就像，缩放定律论文试图回答这两个问题，但显然不符合 RSP 的资格。</p><p>上面的基本概念结构也让人感到困惑。例如，RSP 如何指定开发人员准备处理安全问题的 AI 功能级别？现实为你指明了这一点。也许它应该说“它指定了人工智能开发人员认为他们可以处理的人工智能能力水平”？</p><p>下一句话也是如此。 RSP 如何指定在什么情况下继续部署人工智能系统会过于危险？再说一次，现实是你唯一的仲裁者。 RSP 可以指定人工智能开发人员认为太危险的条件，但这在政策中似乎也很奇怪，因为随着时间的推移，这种情况可能会发生很大的变化。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:01:07 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:01:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我同意“负责任的扩展政策”一词会引发虚假内容。我想这对我来说似乎并没有那么糟糕？也许我低估了名字的重要性。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:07:28 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:07:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>我在这个领域感到有点困惑的另一件事是“RSP 的定义在哪里？”。 ARC Evals 帖子说：<br><br> >; RSP 指定人工智能开发人员准备使用当前的保护措施安全处理什么级别的人工智能功能，以及在保护措施改善之前继续部署人工智能系统和/或扩大人工智能功能过于危险的条件。</p></blockquote><p>是的，如果加上“claim”这个词，这一段似乎会更好。就像“RSP 指定了人工智能开发人员声称他们准备好使用当前的保护措施安全处理的人工智能能力水平，以及他们认为继续部署人工智能系统和/或扩大人工智能规模太危险的条件直到保护措施得到改善为止。”<br><br>希望是这样的：</p><ul><li> AI 开发人员制定了 RSP，其中提出了一些隐式或显式的声明。</li><li>人们可以争辩说，既然有明确的政策可以争论，那么这对于安全来说是不够的。 </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:08:44 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:08:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><blockquote><p>我同意“负责任的扩展政策”一词会引发虚假内容。我想这对我来说似乎并没有那么糟糕？</p></blockquote><p>是的，我确实觉得我试图判断人工智能监管和人工智能联盟中引入的概念和抽象的主要维度是这些概念是否调用真实的事物并在其关节处雕刻现实。</p><p>我同意我可以尝试做一件更天真的后果论的事情，就像“是的，好吧，让我们试着向前推进当人类遇到这些想法时，当它看到这些特定的人说这些时，人类会做什么”话”，但即便如此，我还是觉得 RSP 看起来不太好，因为在那个水平上，我主要希望他们会说“哦，这些人说他们正在负责任”或类似的话。</p><p>需要明确的是，我喜欢 Anthropic RSP 和 ARC Evals RSP 帖子所指出的一点基本上是一系列运作良好的有条件承诺。 RSP 的一种方式是基本上成为人工智能实验室和公众之间的合同，具体规定“当 X 发生时，我们承诺做 Y”，其中 X 是一些能力阈值，Y 是一些暂停承诺，也许还有一些结束条件。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:08:58 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:08:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>具体来说，您认为 ARC evals 和 Anthropic 应该做什么？就像他们应该使用不同的术语吗？ </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:11:23 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:11:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>就像，比起 RSP，我更喜欢对 Dario 和 Daniella 进行一系列坦率的采访，其中有人会说“所以你认为 AGI 有很大的机会杀死所有人，那么你为什么要建造它呢？”。一般来说，创建更高带宽的通道，人们可以用它来了解领先人工智能实验室的人们对人工智能风险的看法，以及何时值得这样做。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:11:26 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:11:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>对我来说，制定某种书面并维护的政策似乎非常重要，这些政策涉及“我们何时停止增加模型的功率”以及“我们将针对不同的功率级别采取哪些安全干预措施”。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:12:00 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:12:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>TBC，坦诚采访的记录似乎也不错。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:14:11 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:14:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我想要政策的部分原因是，这使得组织内部的人员更容易举报或反对内部的事情。<br><br>我还认为，如果更多的人更具体地思考当前的计划是什么样子，那就更好了。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:14:29 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:14:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><blockquote><p>对我来说，制定某种书面并维护的政策似乎非常重要，这些政策涉及“我们何时停止增加模型的功率”以及“我们将针对不同的功率级别采取哪些安全干预措施”。</p></blockquote><p>是的，明确地说，我认为这也很棒。我确实对“人工智能能力公司应该有一份概述其扩展和安全计划的最新文件”感到非常满意。我确实觉得，在有这种计划的世界里，很多对话显然会进展得更好。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:18:06 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:18:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>似乎我们对于我们希望实验室向世界提供的实际工件有轻微但不是很大的分歧，以解释他们的计划？<br><br>我一般认为，在目前的利润范围内，更诚实、清晰的沟通和针对所有事情的具体计划似乎相当不错（例如，RSP、 <a href="https://www.lesswrong.com/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi">关于风险的明确声明、对实验室为什么要做他们知道有风险的事情的明确解释</a>、与怀疑论者的详细讨论等）。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:21:16 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:21:16 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>我确实觉得两种不同类型的文物之间存在着巨大的紧张关系：</p><ol><li>一份应该准确总结组织在不同情况下期望做出的决策的文件</li><li>一份旨在约束组织在某些情况下做出某些决定的文件</li></ol><p>就像，我目前得到的感觉是，RSP 是一种“不后悔”的东西。你不能发布 RSP 说“是的，我们不打算扩大规模”，然后又说“哎呀，我改变主意了，我们实际上要全力以赴”。</p><p>我的猜测是，这就是为什么我希望组织不会真正致力于其 RSP 中的任何实际内容，并且他们不会真正了解组织领导层认为的权衡是什么。就像，这就是为什么 Anthropic RSP 有一个很大的 IOU，实际上最关键的决定应该在其中。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:21:23 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:21:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>哦，明白了，约束性政策和解释之间存在一些差异。<br><br>我同意 RSP 使得以降低安全性的方式改变事物的成本相对更高。从我的角度来看，这似乎主要是一个好处？ RSP 似乎也可以有一些类似“我们对应该做什么的初步最佳猜测”的部分和一些类似“我们当前政策”的部分。 （例如，Anthropic RSP 对 ASL-4 有一个初步猜测，但在其他地方有更可靠的政策。） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:22:45 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:22:45 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>也许一个有趣的问题是“Anthropic 在触发 ASL-3 之前充实 ASL-4 评估和安全干预措施的可能性有多大”？ （这将还清欠条。）<br><br>或者在接下来的两年内充实它，条件是 ASL-3 在这两年内没有被触发。<br><br>我对这种情况的发生持适度乐观的态度。 （也许 60% 是基于 ASL-4 评估和 2 年内一些听起来对我来说特定的 ASL-4 干预措施，条件是没有 ASL-3） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:23:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:23:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>就像，这是“RSP”的替代方案。称其为“有条件暂停承诺”（CPC，如果您喜欢缩写词）。</p><p>基本上，我们只是要求 AGI 公司告诉我们在什么条件下他们将停止扩展或停止尝试开发 AGI。然后还有一些恢复的条件。然后我们就可以批评这些。</p><p>这似乎是一个更清晰的抽象概念，对于该事物是否试图成为组织未来决策的准确地图，或者它应该在多大程度上认真地承诺一个组织，或者整个事物是否“负责任”，没有那么哲学上的固执己见。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:30:37 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:30:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>就像，这是“RSP”的替代方案。称其为“有条件暂停承诺”（CPC，如果您喜欢缩写词）。</p></blockquote><p>我同意 CPC 是一个更好的术语和概念。我认为这基本上就是 RSP 想要实现的目标？<br><br>但似乎值得注意的是，对策是整个局面的关键部分。就像你暂停的条件可能取决于你到目前为止所实施的内容等。我觉得 CPC 不会自然地引起对策部分，但总的来说，它似乎是一个更好的术语。 （我们显然可以添加这一点，但这个名字很糟糕：条件暂停承诺，包括对策 CPCC） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:33:43 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:33:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><blockquote><p>也许一个有趣的问题是“在触发 ASL-3 之前，人类有多大可能充实 ASL-4 评估和安全干预措施”？ （这将还清欠条。）</p><p> （或者在未来 2 年内充实它，条件是不触发 ASL-3。）<br><br>我对这种情况的发生持适度乐观的态度。 （也许 60% 是基于 ASL-4 评估和 2 年内一些听起来对我来说特定的 ASL-4 干预措施，条件是没有 ASL-3）</p></blockquote><p>是的，我同意这是一个有趣的问题，并且我大致同意你在这里的可能性。</p><p>我确实认为他们不跟进的世界真的很糟糕。我真的不喜欢 40% 的世界，事实证明 RSP 是那种导致一群人摆脱 Anthropic 构建真正危险的人工智能的事情，并且是那种导致大量的事情人工智能安全领域的领导者来支持他们，但事情却从未真正实现。</p><p>就像，我觉得这不是我第一次从这里的一些人性公告中感受到不好的氛围。就像他们也有<a href="https://www.anthropic.com/index/the-long-term-benefit-trust">长期利益信托</a>一样，它看起来确实像是那种应该确保人类独立于利润激励的东西，但是就像，埋在中间随机段落末尾的一句话中是爆炸性的事实是，只要绝大多数股东不同意，治理委员会实际上就没有能力阻止 Anthropic，而且他们没有具体说明绝大多数股东的实际具体数字，然后我觉得整件事只是对我来说很平淡。</p><p>就像，如果没有给我具体的数字，这个承诺就没有多大作用。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:34:14 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:34:14 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我非常担心不稳定或糟糕的 RSP 以及 RSP IOU，但这种情况永远不会发生。我觉得 OpenAI 或 GDM 的风险比 Anthropic 高得多。<br><br>我们将看看 OpenAI 的第一个 RDP 是什么样子的。我们将看看 GDM 是否会采取任何官方行动。</p></section><h2>认可和未来轨迹</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:37:35 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:37:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>我确实认为他们不跟进的世界真的很糟糕。就像我真的不喜欢 40% 的世界，事实证明 RSP 是那种让一群人摆脱 Anthropic 构建真正危险的人工智能的事情，也是那种导致大量人死亡的事情。人工智能安全领域的领导者来支持他们，然后事情就永远不会真正实现。</p></blockquote><p>我认为人们应该继续向他们施加压力，直到他们更加充实 ASL-4 评估和承诺。<br><br>我想我觉得我们可以在某种程度上拒绝完全认可，这可能会降低这种结果的风险。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:40:04 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:40:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我觉得我想要倡导的事情是这样的：</p><ul><li>来自人类的 ASL-4 承诺和评估（可以是初步的等）</li><li>一些 RSP 类似于其他实验室的东西。即使我认为这项政策非常不安全，那仍然是进步，那么我们至少可以对这项具体政策的质量进行争论。</li><li>对 RSP 之类的其他改进。 </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:40:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:40:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>我确实认为我非常担心“AGI 公司占领人工智能安全领域”领域的很多事情。例如，大多数从事人工智能安全工作的人已经受雇于 AGI 公司。我觉得在我真正撤回认可的能力基本上完全被削弱之前，我没有多少年的时间继续维持现状了。</p><p>因此，我对人择 RSP 的一系列反应是担心，如果我现在不反对，当更清楚这件事并不严重时，该领域将无法在以后反对（在这个世界中是这样），因为到那时，很多人的职业生涯、声誉和贡献能力都直接取决于与能力公司的相处，因此某种形式的撤回背书的希望不大。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:42:58 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:42:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我想要的密切相关的事情是：在人工智能实验室工作的人应该仔细思考并写下他们会大声辞职的条件（这可以私下完成，但也许应该在志同道合的员工之间共享以获得常识）。这样，人们就有希望避免“温水煮青蛙”。<br><br>我担心人们可能会因为诸如“我真的不想戒烟，因为那样我的影响力就会消失，我应该尝试影响事情变得更好”这样的想法而在真正应该戒烟的时候避免戒烟。此外，XYZ 承诺如果我稍等一下，那东西会更好。”就像在各个人工智能实验室里可能有一些非常善于操纵的人一样。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:44:32 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:44:32 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><blockquote><p>在人工智能实验室工作的人应该思考并写下他们会大声辞职的条件（这可以私下进行，但也许应该在志同道合的员工之间分享，以获得常识）。这样，人们就有希望避免“温水煮青蛙”。</p></blockquote><p>是的，我确实认为这也很棒。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:46:36 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:46:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我也同意“令人恐惧的是，大量的安全人员在实验室工作，这种趋势似乎可能会持续下去”。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:47:33 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:47:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>因此，我对人择 RSP 的一系列反应是担心，如果我现在不反对，当更清楚这件事并不严重时，该领域将无法在以后反对（在这个世界中是这样），因为到那时，很多人的职业生涯、声誉和贡献能力都直接取决于与能力公司的相处，因此某种形式的撤回背书的希望不大。</p></blockquote><p>您为什么不认为您现在可以拒绝认可并说“我正在等待 ASL-4 标准和评估”？ （我不知道这是否重要。） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:49:00 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:49:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>好吧，我可以，但我预计在接下来的几年里，我的观点在当前默认的社会轨迹上将不再有太大的影响力。就像，我可以拒绝认可，但我希望认可的重要性会转向那些不拒绝认可的人。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:50:28 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:50:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>好的了解了。我想我确实预计随着时间的推移，你可能会因为各种原因失去影响力。我不确定你应该对此做什么。</p></section><h2>您能在多大程度上判断给定模型是否存在存在危险？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:51:11 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:51:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>我确实想稍微回顾一下，谈谈我不断想到的 RSP 的另一个方面。我对此没有很好的把握，但也许描述它的最好方式就是“伙计，RSP 确实让听起来我们对 AGI 何时变得危险有很好的把握”之类的。</p><p>比如，我对假设的“有条件暂停承诺”和 RSP 的担忧是，如果某些 AGI 公司善意地遵守了你给他们的一些随机安全基准，那么你最终会陷入这样一种状态：你承诺允许某些 AGI 公司扩大规模。现实情况是，识别一个系统是否安全，或者是否（例如）具有欺骗性地对齐，是极其困难的，而且我们在这个问题上并没有太多的吸引力，所以我并不真正期望这一点实际上，我们会对安全基准提出任何很好的建议，我们相信这些建议可以承受数十亿至数万亿美元的经济压力和更加智能的人工智能系统。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:55:21 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:55:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>我确实想稍微回顾一下，谈谈我不断想到的 RSP 的另一个方面。我对此没有很好的把握，但也许描述它的最好方式就是“伙计，RSP 确实让听起来我们对 AGI 何时变得危险有很好的把握”之类的。</p></blockquote><p><br>我认为我们确实有一个合理的方法来了解如何知道给定模型的给定部署是否存在危险（以及如何运行此类评估而不使评估本身太危险而无法运行）。<br><br>我对如何在开源模型安全的情况下进行此类评估不太有信心，因为脚手架或微调之类的事情可能会进展并使这些开源模型变得危险。</p><p>我对此的信心程度并不是压倒性的，但我认为我们可以评估厄运何时开始 >;1%，而无需极其保守。也许 0.1% 更难。</p><p>我的意思是“如果你继续将 GPT 缩放 0.25 GPT，你能否进行评估，使得当评估表明危险时，厄运小于 1%”。</p><p>我认为这可能很好地转移到其他基于机器学习的方法。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 18:57:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 18:57:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>呵呵，这个我有点惊讶。我相信你可以使用“随机采样”系统来做到这一点，但我不认为你可以使用对抗性选择的系统来做到这一点。就像，我同意你可能可以粗略地确定计算和资源的规模（假设没有巨大的算法突破和 RSI 以及一堆其他东西），其中事情变得危险，但是，RSP 的全部意义在于然后拥有AGI 公司试图超越你设定的一些基准，这些基准是他们需要做什么才能走得更远，而这对我来说似乎很难。</p><p> （旁注：这反映了我现在经常围绕人工智能对齐研究进行的一般类型的对话，其中有很多可怕的恶魔、可解释性和评估工作，其目标是确定人工智能是否未对齐，并且我通常的回答是“好吧，我非常有信心在未来几年内你会得到大量证据表明人工智能系统与你不一致。我的主要问题是，一旦你确定一个系统不符合你的要求，你实际上会做什么或者以这种方式危险”，然后令人惊讶的是人们经常没有真正的答案） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 18:58:28 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 18:58:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>哦，好吧，我同意何时前进的基准非常棘手。</p><p>我的主张更像是“相当好的 ASL-3 和 ASL-4 评估应该是可能的”（包括它们对欺骗性对齐的 AI 系统可能试图对这些评估进行沙袋的可能性具有鲁棒性） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:03:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:03:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>ASL-3 和 ASL-4 的定义是什么？ </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:06:07 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:06:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我大致认为：<br><br> ASL-3：从误用的角度来看可能是危险的，可能不是自动危险的（>;=ASL-3 的评估是根据 ARA（自主复制）或严重误用风险进行操作的）<br><br> ASL-4：可以认真地进行 ARA（自主复制），并且可能有逃跑或其他严重问题的风险。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:07:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:07:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><blockquote><p>我的主张更像是“相当好的 ASL-3 和 ASL-4 评估应该是可能的”</p></blockquote><p> “评估”指的是“评估我们何时应该将人工智能系统分类为 ASL-3、ASL-4 甚至更高”？</p><p>或者您的意思是，我们也许能够进行评估，即使花费数十亿美元来匹配该评估，评估也会告诉我们给定的 ASL-3 或 ASL-4 系统是否可以安全地更广泛地部署？ （例如，可能为 ASL-3 开源，或者为 ASL-4 部署） </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:08:02 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:08:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我说的是用于分类和排除 >;=ASL-4 的评估，而不是用于指示此类系统安全的评估。<br><br>我也并不是说这些评估对于沉重的优化压力一定是稳健的，从而使人工智能在这个评估中看起来很聪明，但又显得愚蠢。<br><br>我没有想太多；这对我来说确实是一个重大问题。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:08:55 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:08:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>我认为我们应该讨论“实验室将为 ASL-4+ 模型提出哪些安全论据”？ </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:09:21 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:09:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>“安全论点”指的是“相关模型可以安全开发/使用/部署/分发的论点？” </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:09:30 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:09:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>是的，受某些协议的约束，例如可能限制使用。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:14:50 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:14:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>是的，看起来不错。这看起来确实非常重要，作为一个至少现在对这个问题的思考比我期望实验室思考的要多得多的人，我的答案肯定是“伙计，我不知道吗”。</p><p>需要明确的是，显然有些训练设置看起来比其他训练设置更危险，但我真的不知道在安全争论的情况下如何考虑这一点，即使如此，我也感到非常困惑。</p><p>举个例子，我的猜测是，系统中的大部分计算都花在了在奖励长期规划和代理资源获取的环境中进行强化学习训练（例如，许多视频游戏或外交或各种长期模拟）。目标）确实看起来更危险。</p><p>但我想我只有 75% 的概率相信这一点？如果最终我开始相信，对于一个系统来说，最终是否危险最重要的只是计算量和投入其中的数据的丰富性，以及所有的论点，我真的不会感到惊讶。工具收敛已经完全适用于大型语言模型，可能会在上面撒上一些 RLHF，在这种情况下，在丰富的环境中通过强化学习训练的系统实际上可能更安全，因为大量计算的使用效率要低得多，并且您可能更能够引出长期规划并利用它来做有用的事情。</p><p>就像，我对进一步降低概率分布没有太大的吸引力，除了也许“只是做更多的人工智能对齐研究，深入研究当前的模型，认真思考，写很多博客文章， ETC。”​​。因此，考虑到我们目前的理解水平，我很难想象 AGI 公司实际上会给我一个安全论证，对我来说有意义地改变概率质量。</p></section><h2>通过“控制”与“缺乏倾向”进行防御</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:21:30 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:21:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><p>这是我进行安全论证的基本状态。<br><br>我认为安全论点可能有两条（部分重叠的）“防线”：</p><ul><li><strong>控制</strong>：鉴于我们的对策，人工智能<i>无法</i>做严重危险的事情（除非先被抓住）。</li><li><strong>倾向</strong>：人工智能不太可能<i>尝试</i>做严重危险的事情。</li></ul><p>我对当前科学的安全有用性权衡的总体看法是：</p><ul><li>我们的技术还远远不能自信地评估强大的人工智能系统的倾向。</li><li>控制可能可以评估并确保<i>早期变革性人工智能</i>（例如可以将人类进行机器学习研发速度加快 30 倍的模型）</li><li>我们至少可以使用<a href="https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1#The_Case_against_Model_Organisms_">模型生物</a>和对抗性评估之类的东西来测试一些倾向（试图产生人工智能在其上做一些非常糟糕的事情的合成输入）。</li><li>就像我认为“我们努力以对抗性方式评估该模型，但它没有做任何非常可疑的事情”是不平凡的证据，尽管并不令人惊讶。</li><li>我认为“我们试图创造欺骗性的排列模型生物体，但我们最好的模型生物体相当弱”也是一些证据。</li><li>我认为我们有大约 70% 的机会能够为<i>早期变革性 AI</i>建立良好的控制参数，从而将风险降低约 3-5 倍，同时使模型的有用性稍微降低（例如，有用性降低 2 倍）</li><li>显然，控制无法扩展。 （因此，随着规模的扩大，安全性和实用性的权衡会变得更糟）</li><li>评估控制的方法并非平凡且复杂。我不知道如何以无法玩弄的方式指定评估。</li><li>我认为控制+模型生物+对抗性评估可以将风险降低 5-15 倍，而且如果我们真的努力的话，这并不是不可能的。 </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:24:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:24:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><p>这三件事感觉很难同时成为现实：</p><blockquote><p>控制可能可以评估并确保早期变革性人工智能（例如可以将人类进行机器学习研发速度加快 30 倍的模型）</p><p> [...]</p><p>就像我认为“我们努力对模型进行红队处理，但它没有做任何非常可疑的事情”是不平凡的证据，尽管并不令人惊讶。</p><p> [...]</p><p>我认为“我们试图创造欺骗性的排列模型生物体，但我们最好的模型生物体相当弱”也是一些证据。</p></blockquote><p>就像，这并非不可能，但我确实希望在红队不严重破坏模型的情况下，你无法获得 30 倍的 ML 研发（至少在红队涉及微调的情况下）。</p><p>另外，我确信，早在你能够将 ML 研发速度加快 30 倍之前，你就可以得到相当可靠、具有欺骗性的对齐模型生物体，而且我不认为我们有太多的动力去寻找某种不适合的训练设置。情况不再如此。 </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:25:23 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:25:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>瑞安·格林布拉特</b></section><blockquote><p>举个例子，我的猜测是，系统中的大部分计算都花在了在奖励长期规划和代理资源获取的环境中进行强化学习训练（例如，许多视频游戏或外交或各种长期模拟）。目标）确实看起来更危险</p></blockquote><p><br>是的，所以我们可以做的一件事就是“猜测训练的调整，这将降低危险倾向的可能性”。我不认为这里有很多好主意，而且我预计由此带来的安全实用性权衡似乎并不令人惊奇。 （所以我同意你的观点，我只是认为还有其他途径。）</p><blockquote><p>就像，这并非不可能，但我确实希望在红队不严重破坏模型的情况下，你无法获得 30 倍的 ML 研发（至少在红队涉及微调的情况下）。</p></blockquote><p> I should have stated what I meant more clearly here. I mean &quot;adversarial evaluation (trying to produce synthetic inputs on which the AI does something very bad)&quot;<br><br> We could get into why I think this is non-trivial evidence. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:27:54 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:27:54 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><p> Ok, I have to head out in 5 minutes, so let&#39;s try to wrap up.</p><p> I do feel like a major disagreement here is probably something like &quot;how optimistic am I about the control route&quot;, which I know that you and Buck have been thinking about a lot. I have definitely mostly been thinking about AI Alignment through the propensity lens, since I&#39;ve been looking for ways to scale the the methods to superintelligence, so I don&#39;t have super robust opinions here, but I do have a sense that this is going to be much harder than my model of you thinks it is. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:28:02 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:28:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>ryan_greenblatt</b></section><p> FYI, I&#39;m intentionally avoiding the term &quot;alignment&quot; because that term now has a bunch of baggage. So I&#39;d like to just say safety arguments and then talk more specifically. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 19:28:14 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 19:28:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><p> That seems very reasonable. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 19:29:04 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 19:29:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>ryan_greenblatt</b></section><p> Yep, worth noting I haven&#39;t yet argued for my level of optimism. So we&#39;d maybe want to do that next if we want to continue.</p></section><h2> Summaries </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 27 Oct 2023 21:56:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 27 Oct 2023 21:56:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><p> <i>Summarizing what we covered in this dialogue (feel free to object to any of it):</i></p><p> A substantial chunk of my objections were structured around the naming and definitions of RSPs. You agreed that those didn&#39;t seem super truth-promoting, but also thought the costs of that didn&#39;t seem high enough to make RSPs a bad idea.</p><p> We then covered the problem of the Anthropic RSP, as the primary example of an RSP, having kind of a big IOU shaped hole in it. I was concerned this would allow Anthropic to avoid accountability and that people who would hold them accountable wouldn&#39;t have enough social buy-in by the time the IOU came due. You agreed this was a reasonable concern (though my guess is you also think that there would still be people who would hold them accountable in the future, more so than I do, or at least you seem less concerned about it).</p><p> I then switched gears and started a conversation about the degree to which we are even capable of defining evals that meaningfully tell us whether a system is safe to deploy. We both agreed this is pretty hard, especially when it comes to auditing the degree to which the propensities and motivations of a system are aligned with us. You however think that if we take it as a given that a system has motivations that are unaligned with us, then we still have a decent chance of catching when that system might be dangerous, and using those evals we might be able to inch towards a world where we can make substantially faster AI Alignment progress leveraging those systems. This seemed unlikely to me, but we didn&#39;t have time to go into it.</p><p> Does this seem overall right to you? I am still excited about continuing this conversation and maybe digging into the &quot;Alignment vs. Control&quot; dimension, but seems fine to do that in a follow-up dialogue after we published this one. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="dfZAq9eZxs4BB4Ji5-Fri, 27 Oct 2023 22:42:08 GMT" user-id="dfZAq9eZxs4BB4Ji5" display-name="ryan_greenblatt" submitted-date="Fri, 27 Oct 2023 22:42:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>ryan_greenblatt</b></section><p> Yep, this seems like a reasonable summary to me.</p><p> One quick note:</p><blockquote><p> (though my guess is you also think that there would still be people who would hold them accountable in the future, more so than I do, or at least you seem less concerned about it)</p></blockquote><p> Yep, I think that various people will be influential in the future and will hold Anthropic accountable. In particular:</p><ul><li> Anthropic&#39;s Long-Term Benefit Trust (LTBT) (who approves RSP changes I think?)</li><li> People at ARC evals</li><li> Various people inside Anthropic seem to me to be pretty honestly interested in having an actually good RSP</li></ul></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies<guid ispermalink="false"> jyM7MSTvy8Qs6aZcz</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Sun, 29 Oct 2023 04:17:07 GMT</pubDate> </item><item><title><![CDATA[Experiments as a Third Alternative]]></title><description><![CDATA[Published on October 29, 2023 12:39 AM GMT<br/><br/><p> As a kid in elementary school, I was diagnosed with ADHD. I think the diagnostic process is dumb and unreliable, and so the fact that I was diagnosed seems like it should only count as weak evidence. But knowing what I know about myself and having read a few books about ADHD, my best guess is that I have (and had) a moderate case of it.</p><p> I&#39;ve never taken medication for it though.为什么？ Well, from my perspective, there has never been anything wrong with me. There&#39;s something wrong with <i>The System</i> . Fight the power!</p><p> For example, I&#39;d be in school and the teacher would want me to pay attention to X. But instead of X, I&#39;d want to think about Y. It&#39;s hard for me to focus on X when I am driven to think about Y. <span class="footnote-reference" role="doc-noteref" id="fnrefkdt37vlb64"><sup><a href="#fnkdt37vlb64">[1]</a></sup></span></p><p> Is this a bad thing? Well, it depends on the values of X and Y. Sometimes X and Y take values that make it good, other times they take values that make it bad.</p><ul><li> For example, when I&#39;m behind the wheel of a car and X = &quot;the stoplight in front of me&quot; and Y = &quot;that new Indian restaurant&quot;, it is bad. <span class="footnote-reference" role="doc-noteref" id="fnrefcalstl3kzju"><sup><a href="#fncalstl3kzju">[2]</a></sup></span></li><li> But when X = &quot;Bobby&#39;s question about last night&#39;s algebra homework&quot; and Y = &quot;the <a href="https://en.wikipedia.org/wiki/Flip_book">flip book</a> I&#39;m working on&quot;, I&#39;d argue that my &quot;attention deficit&quot; is good.</li><li> Overall, I have always thought that my &quot;attention deficit&quot; does significantly more good than harm.</li></ul><p> For this reason, I never wanted to take medication for ADHD. I thought that I was generally prone to paying attention to the &quot;right&quot; things, that the adults generally wanted me to pay attention to the &quot;wrong&quot; things, and that this was a problem with the world they were trying to get me to live in, not with my nature.</p><p> But the doctors and my parents had a totally different perspective. They thought my tendencies and behavior were in fact problematic. However:</p><ol><li> Academically, I did pretty well in school.</li><li> Behaviorally, I was mischievous, but always in a harmless type of way. <span class="footnote-reference" role="doc-noteref" id="fnref71yorfxj697"><sup><a href="#fn71yorfxj697">[3]</a></sup></span></li><li> I had this involuntary facial tick <span class="footnote-reference" role="doc-noteref" id="fnrefk7en98cp9t"><sup><a href="#fnk7en98cp9t">[4]</a></sup></span> and apparently there is a risk that the medication would cause me to develop (much?) worse ticks.</li></ol><p> For these reasons, they thought it was fine for me to avoid medication.</p><p> As a kid it was the doctors and my parents who were ultimately in charge of whether or not I took medication. Maybe if I argued that I wanted medication they would be receptive, but probably not. However, as an adult, it becomes my decision. So once I went away to college, I could have just gone to a neurologist or whatever and re-explored this decision to avoid medication.</p><p> I never did that though.为什么？ <a href="https://www.lesswrong.com/tag/cached-thoughts">Cached thoughts</a> .</p><blockquote><p> I like who I am. My so called &quot;attention deficit&quot; is actually a good thing overall because it allows me to hyperfocus on the things that interest me, which is really valuable. Yes, sometimes this ends up biting me, but the bites aren&#39;t too frequent, nor are they too harsh.</p></blockquote><p> When the topic of my ADHD came up, the above thought is what I would immediately gravitate towards. It&#39;s not as black-and-white as &quot;it&#39;s the only thought I would ever think&quot;, but I gravitated towards it pretty strongly.</p><p> But over the past few weeks I&#39;ve been starting to question how accurate that cached thought is.</p><p> I have pretty high levels of anxiety <span class="footnote-reference" role="doc-noteref" id="fnreff8b7bmfomun"><sup><a href="#fnf8b7bmfomun">[5]</a></sup></span> in general for various reasons, and I&#39;ve been looking for ways to mitigate it. One cause of this anxiety is that I always have a million things I want to do. Things that are interesting and deserving of my attention. But I don&#39;t have time to do them all, of course.</p><p> And a related problem is that if I&#39;m focused on something and need to stop, it feels like pulling teeth. For example, I started writing this post at a coffee shop, but at 3pm when they closed I had to stop what I was doing, close my laptop, and spend 20 minutes walking home. That was very uncomfortable.</p><p> But this sort of thing happens all the time. When I&#39;m working and it&#39;s time to eat. When I&#39;m coding and I have to go walk my dog. When I&#39;m reading and it&#39;s time for me to go to racquetball. When I&#39;m thinking and it&#39;s time for me to go to sleep.</p><p> Right now, I lean towards thinking that my ADHD does more harm than good and that I should take medication. But this line of thinking is also problematic.为什么？ Because there&#39;s an implicit and false dichotomy. It fails to seek out a <a href="https://www.lesswrong.com/posts/erGipespbbzdG5zYb/the-third-alternative">Third Alternative</a> . From the post:</p><blockquote><p> “Believing in Santa Claus gives children a sense of wonder and encourages them to behave well in hope of receiving presents. If Santa-belief is destroyed by truth, the children will lose their sense of wonder and stop behaving nicely. Therefore, even though Santa-belief is false-to-fact, it is a Noble Lie whose net benefit should be preserved for utilitarian reasons.”</p><p> Classically, this is known as a false dilemma, the fallacy of the excluded middle, or the package-deal fallacy. Even if we accept the underlying factual and moral premises of the above argument, it does not carry through. Even supposing that the Santa policy (encourage children to believe in Santa Claus) is better than the null policy (do nothing), it does not follow that Santa-ism is the <i>best of all possible alternatives.</i> Other policies could also supply children with a sense of wonder, such as taking them to watch a Space Shuttle launch or supplying them with science fiction novels. Likewise, offering children bribes for good behavior encourages the children to behave well <i>only</i> when adults are watching, while praise without bribes leads to unconditional good behavior.</p><p> Noble Lies are generally package-deal fallacies; and the response to a package-deal fallacy is that if we really need the supposed gain, we can construct a Third Alternative for getting it.</p></blockquote><p> What Third Alternative do I have? Well, how about just taking the medication as an experiment?</p><p> I realize that I (as well as my parents and the doctors <span class="footnote-reference" role="doc-noteref" id="fnrefts8mvi2bp6n"><sup><a href="#fnts8mvi2bp6n">[6]</a></sup></span> ) have been approaching the question as if I am either going to 1) be a person who doesn&#39;t take medication or 2) be a person who does take medication. But option #3 is much better: take medication for six weeks and see what happens.</p><p> It reminds me of the thing <span class="footnote-reference" role="doc-noteref" id="fnrefxy1ey1ogs6i"><sup><a href="#fnxy1ey1ogs6i">[7]</a></sup></span> where thousands of years ago, philosophers thought that it was noble to sit inside and use your mind to figure things out, and that going out to look at the world to see what happens... well... that&#39;s ignoble. A job to be done by a man of much lower social status.</p><p> For my ADHD stuff, I&#39;ve been acting like one of those old philosophers. Instead of <i>just trying the damn thing</i> , I&#39;ve been straining to predict whether or not the thing will do more harm than good.</p><p> But that sort of &quot;straining to predict&quot; is dumb. I mean, again, why not just try the damn thing? Why predict when you can test? <span class="footnote-reference" role="doc-noteref" id="fnrefj06vqnxu2wa"><sup><a href="#fnj06vqnxu2wa">[8]</a></sup></span></p><ul><li> Well, sometimes the test is costly. Like, an MRI, perhaps. But ADHD medication is not expensive.</li><li> Other times the test will have long-term, or even permanent effects. With the ADHD, I think there&#39;s always been a deep part of my mind that was afraid of this. I don&#39;t want the medication to &quot;change me&quot;. In doing some research, this seems quite unlikely though. There&#39;s still some emotional part of me that is a little afraid of it &quot;changing me&quot;, but it&#39;s not too loud.</li><li> Sometimes the test is painful. Like one of those biopsies where they stab you with a big needle. That&#39;s not the case here though.</li><li> Sometimes you&#39;re confident enough in the outcome that you don&#39;t need to bother testing. But again, that&#39;s not the case here.</li></ul><p> I&#39;m sure one can have a long discussion about the abstract question of when it makes sense to test and when it makes sense to avoid testing things. I&#39;m not interested in doing that though. I think that for the purposes of this post, it&#39;s enough to say that when tests are cheap, painless, and don&#39;t have long-term effects, the bar is low and they&#39;re generally a pretty appealing Third Alternative. <span class="footnote-reference" role="doc-noteref" id="fnrefw0qvxpfza3n"><sup><a href="#fnw0qvxpfza3n">[9]</a></sup></span> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkdt37vlb64"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkdt37vlb64">^</a></strong></sup></span><div class="footnote-content"><p> Note that this is <a href="https://www.psychologytoday.com/intl/blog/be-your-best/202204/whats-the-real-deficit-in-adhd">not a <i>deficit</i> of attention</a> . It is a difficulty <i>controlling</i> where attention is allocated.</p><p> I&#39;ve always really liked the <a href="https://en.wikipedia.org/wiki/Visual_spatial_attention#Spotlight_metaphor">spotlight metaphor</a> for attention. With this metaphor, the spotlight is not dimmed. It&#39;s just more focused and harder to move.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncalstl3kzju"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcalstl3kzju">^</a></strong></sup></span><div class="footnote-content"><p> Which is one of the reasons why I don&#39;t drive.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn71yorfxj697"> <span class="footnote-back-link"><sup><strong><a href="#fnref71yorfxj697">^</a></strong></sup></span><div class="footnote-content"><p> For example, I had this teacher for a class called &quot;Research&quot; or something. I thought the things he taught us were incredibly dumb. One thing I remember is how he spent a lot of time emphasizing how you have to underline some things in one color and other things in another color. As opposed to, y&#39;know, the actually important principles of scientific research, a la Richard Feynman.</p><p> So I played a bunch of pranks. One time I put a piece of scotch tape under the computer mouse to interfere with the laser motion detector thing. Another time I broke off a piece of my lead pencil&#39;s tip in the door&#39;s lock so that it couldn&#39;t be opened.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnk7en98cp9t"> <span class="footnote-back-link"><sup><strong><a href="#fnrefk7en98cp9t">^</a></strong></sup></span><div class="footnote-content"><p> I would lightly bite the inside of my left cheek in a way that caused my face to scrunch up a bit. It always seemed incredibly minor to me, but moderate to my parents and the doctors. I don&#39;t really remember the kids in school ever teasing me about it, or even pointing it out.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf8b7bmfomun"> <span class="footnote-back-link"><sup><strong><a href="#fnreff8b7bmfomun">^</a></strong></sup></span><div class="footnote-content"><p> That&#39;s not really the best word, but I&#39;m not sure what a better word would be.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnts8mvi2bp6n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefts8mvi2bp6n">^</a></strong></sup></span><div class="footnote-content"><p> <code>faithInTheMedicalSystem--</code></p></div></li><li class="footnote-item" role="doc-endnote" id="fnxy1ey1ogs6i"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxy1ey1ogs6i">^</a></strong></sup></span><div class="footnote-content"><p> Actually, I&#39;m not sure how true this is. I remember hearing something along these lines, but I may be misremembering and/or straw-manning. I think what I&#39;m remembering is something related to the philosophy of <a href="https://en.wikipedia.org/wiki/Rationalism">rationalism</a> and how it stands in contrast to the philosophy of empiricism.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnj06vqnxu2wa"> <span class="footnote-back-link"><sup><strong><a href="#fnrefj06vqnxu2wa">^</a></strong></sup></span><div class="footnote-content"><p> I really like how <a href="https://hpmor.com/chapter/1">chapter 1 of HPMoR</a> explores this. Maybe Eliezer decided to discuss empiricism in the first chapter because empiricism is so foundational to rationality.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw0qvxpfza3n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw0qvxpfza3n">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://www.lesswrong.com/posts/fFY2HeC9i2Tx8FEnK/luck-based-medicine-my-resentful-story-of-becoming-a-medical">Luck Based Medicine</a> is probably relevant. Even if various treatments are non-standard and uncommon, who knows, maybe they&#39;ll work for you. If it&#39;s cheap to try, why not give it a shot?</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/cbfiXhEXfvZRHAzNC/experiments-as-a-third-alternative#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cbfiXhEXfvZRHAzNC/experiments-as-a-third-alternative<guid ispermalink="false"> cbfiXhEXfvZRHAzNC</guid><dc:creator><![CDATA[Adam Zerner]]></dc:creator><pubDate> Sun, 29 Oct 2023 00:39:31 GMT</pubDate> </item><item><title><![CDATA[Comparing representation vectors between llama 2 base and chat]]></title><description><![CDATA[Published on October 28, 2023 10:54 PM GMT<br/><br/><p> <i>(Status: rough writeup of an experiment I did today that I thought was somewhat interesting - there is more to investigate here regarding how RLHF affects these concept representations)</i></p><p> This post presents the results of some experiments I ran to:</p><ul><li> Extract representation vectors of high-level concepts from models</li><li> Compare the representations extracted from a base model (Llama 2 7B) and chat model trained using RLHF (Llama 2 7B Chat)</li><li> Compare the representations between different layers of the same model</li></ul><p> <i>Code for the experiments + more plots + datasets available</i> <a href="https://github.com/nrimsky/ActivationDirectionAnalysis"><i>here</i></a> <i>.</i></p><p> To extract the representation vectors, I apply the technique described in my <a href="https://www.lesswrong.com/posts/raoeNarFYCxxyKAop/modulating-sycophancy-in-an-rlhf-model-via-activation">previous</a> <a href="https://www.lesswrong.com/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation">posts</a> on activation steering to modulate sycophancy <span class="footnote-reference" role="doc-noteref" id="fnrefqowaqlr9u98"><sup><a href="#fnqowaqlr9u98">[1]</a></sup></span> . Namely, I take a dataset of multiple-choice questions related to a behavior and, for each question, do forward passes with two contrastive examples - one where the model selects the answer corresponding to the behavior in question and one where it doesn&#39;t. I then take the mean difference in residual stream activations at some layer at the token position corresponding to the different answers.</p><p> Besides sycophancy, which I previously investigated, I also use other behavioral datasets such as agreeableness, survival instinct, and power-seeking. Multiple-choice questions for these behaviors are obtained from Anthropic&#39;s model-written-evals <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>datasets, <a href="https://huggingface.co/datasets/Anthropic/model-written-evals">available on huggingface</a> .</p><h1> Observation 1: Similarity between representation vectors from chat and base model shows double descent</h1><p> At first, similarity declines from very similar (cosine similarity near 1) to halfway towards the minimum, and then for some behaviors, climbs up to ~0.9 again, around layer 11. </p><figure class="image image_resized" style="width:84.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/vb6gotvcxmok6gxeuobz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/kwxhsnyknjfe7ixn9vzf 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/nn6ffrggfobtwagnbqxb 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/qh4mwwvqd2r1ray2oxko 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/tkry1k3khvy1uqklnyuu 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/te6aozxy4tdzrvupt3gf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/sfl72bfjnuqowlhj96kt 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/avkn5cxqdo9qhueoh8ld 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/hgi8cokckd73q2mvimg0 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/cx4fcztivgw45tmzu7js 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/kpuxvylip1ohbagjvwav 1000w"></figure><p> The following chart is generated from a higher-quality sycophancy dataset that includes some multiple-choice questions generated by GPT-4: </p><figure class="image image_resized" style="width:80.05%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ojhzzrph7xjq1lr6p7hb" alt="图像"></figure><p> PCA of the generated vectors also shows the representations diverge around layer 11: </p><figure class="image image_resized" style="width:54.48%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/j78kghj3bxcglbioa9oy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/exql41qpft0qbmvzwaya 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ukwql4hoiom5kb3ysvpk 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/eeu0i7ey1pjlhlwcpzij 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/bm24abwagwxaeu41ufvv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/p2myheaifj2m7avjrpit 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/txbkgojd7etkqiahq6nl 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/troluctpzjnm4ed4apbn 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/veerqtwij3rxor7rb0ig 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/zu2k05g8dtjwshw6manz 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/zfpwrfbqzybapr2b57ix 1000w"><figcaption> 2D PCA projection of agreeableness representation vectors extracted from Llama 2 7B Chat and base models </figcaption></figure><figure class="image image_resized" style="width:54.33%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/xgnubumktxna4jgr24h3" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/dfd2qfftjfgpby8kqasv 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/suh8rofiin19lk8eikg6 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/lsigzd9csznukuqwixqg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/xxnpbjtepyovrf5rfiwp 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/mv7rud8oq8cpmduoj9wg 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ojo1my5owavuxkzwvruu 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/oo4oy15o9xg6dqlnwhyq 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ybsnpmdfmhh9nb71zwrr 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/zg34szhwiswtadvyrswa 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/vgbk1t7sgej3h90dvkiz 1000w"><figcaption> 2D PCA projection of myopia representation vectors extracted from Llama 2 7B Chat and base models </figcaption></figure><figure class="image image_resized" style="width:57.4%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/j1dndd9yldudz6sxips5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/qgfg9gm7qusygovdgezg 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/chubvzw0czxxp9uyzw1r 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/og5mjldwbhwdno5t8g3m 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/hr2y11tnbmxqa28l5rz0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ngnzmizhptxq8w3mnvsv 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/yychd1ysqx8zd0nl7h1c 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/xc1rjq7eyctf3vqhokst 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/n0ekfj3bzzufjnce9iip 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/bqqtd9ols2nduvmx2p74 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/y8hc4aw2bhoeqren0zxv 1000w"><figcaption> 2D PCA projection of sycophancy representation vectors extracted from Llama 2 7B Chat and base models</figcaption></figure><h1> Observation 2: Vectors vary more smoothly after around layer 11 for all behaviors/models</h1><p> I hypothesize that once the model extracts the high-level information needed to describe an abstract concept, the representation &quot;converges&quot; and remains more consistent across subsequent layers. </p><figure class="image image_resized" style="width:81.13%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/u5q67vkzx3qdb7arfbq3" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/emk5xh5zfgccjsd2o6zw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/qi9wt6q8ck4tzrbfwybh 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/qepni6wnttetjyf59sqs 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/ndiruvqqal4vqnghyred 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/dofnaawhonts46asoz3l 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/arwtbwnhafzrldkcxmxz 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/pftpeuzxwnbtmbcdhrto 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/hcaiazughc1bdbhccuck 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/sbwhikear9bh1frd56tt 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/xrryalx0hr8p9qe7rdaj 1000w"><figcaption> Heatmap of cosine similarity of &quot;myopia&quot; representation vector extracted from different layers of Llama 2 7B base model. We can see nearby layers are more similar in the last 2/3 of the model but not in the first 1/3. </figcaption></figure><figure class="image image_resized" style="width:79.41%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/elhl2ifwscmj7fohhjqo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/n3s1cczcbims4smardws 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/yqlc4icsymafcn3xaw5a 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/mezzwnu1tofb0pzv8ytg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/n9a9fcutvpp2df1nacwo 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/cheimwl19w4fzu1xtcqg 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/qcxls9na8mbsp6oaybwq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/dtuitkzb2teu4cxfblso 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/gzrthicsihxwouewfn4k 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/dwbmexzmyprtyevihqg8 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/jhwxut7hezeanigkz3e0 1000w"><figcaption> Heatmap of cosine similarity of &quot;self-awareness&quot; representation vector extracted from different layers of Llama 2 7B Chat model. This shows a similar trend to the other representation vectors / base model.</figcaption></figure><h1> Observation 3: PCA projections of vectors generated from different layers of the same model often look like a U </h1><figure class="image image_resized" style="width:39.45%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/njqk4imbm1bds7ggkhwb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/djyuq29khk1c08ymqqfw 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/b0woarlxrvcy9yhmj71s 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/d0i4ozo7ap9aixlhi2gw 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/d3axbrw9rjl6re8npues 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/is8xvfkzjaq4aw594n5g 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/eivf3mbuablmjgdv8dwq 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/tcr7kqgnpoivvclyex3u 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/mbktkvgisps1k3ymmugk 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/uapk2edcskmze5h9rqi9 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/nlxvda1mvncra7cmv3nx 1291w"></figure><figure class="image image_resized" style="width:40.89%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/mt6owqp4kaewrsjofhnu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/gkkooep7afb7x7o9qho1 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/solazmwnexh672ipgcpx 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/gpunsoxdnnbj40dudplj 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/tqupbjqliqpqh9y4vedn 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/jvm5uckhst1fqic70dle 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/hqfdjgxpdnqz1fsizcnj 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/nywgsdc6qwhp3nkbdca2 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/hvqu51yfgcsn6hgifofu 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/knuahu3ybyyii7bblre7 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PDDG4uuCLpPsuKepY/nbd5sew8xgblyvliab5u 1281w"></figure><p></p><p> Vectors from layers &lt;8 project to around the same point. The remaining projected vectors follow a rough inverted U-shape, with a peak around layer 11 or 12.</p><p> It&#39;d be interesting to investigate what these principal components actually correspond to in activation space.</p><p> <i>(edit: replaced PCA plots with normalized PCA as</i> <a href="https://www.lesswrong.com/posts/PDDG4uuCLpPsuKepY/comparing-representation-vectors-between-llama-2-base-and?commentId=7f8Rvk6kCvp8nPDcL"><i>pointed out by Teun van der Weij</i></a> <i>)</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnqowaqlr9u98"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqowaqlr9u98">^</a></strong></sup></span><div class="footnote-content"><p> Similar techniques were applied by Annah Dombrowski and Shashwat Goel in their project to <a href="https://www.lesswrong.com/posts/JCgs7jGEvritqFLfR/evaluating-hidden-directions-on-the-utility-dataset">evaluate hidden directions on the utility dataset</a> and by Nick Gabrieli and Julien Schulz in their SERI MATS project. See also this recent <a href="https://arxiv.org/pdf/2310.01405.pdf">paper on &quot;Representation Engineering&quot;</a> and Alex Turner&#39;s work on <a href="https://arxiv.org/abs/2308.10248">Activation Additions</a> , which inspired my activation vector extraction approach.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/PDDG4uuCLpPsuKepY/comparing-representation-vectors-between-llama-2-base-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PDDG4uuCLpPsuKepY/comparing-representation-vectors-between-llama-2-base-and<guid ispermalink="false"> PDDG4uuCLpPsuKepY</guid><dc:creator><![CDATA[Nina Rimsky]]></dc:creator><pubDate> Sat, 28 Oct 2023 22:54:37 GMT</pubDate> </item><item><title><![CDATA[Vaniver's thoughts on Anthropic's RSP]]></title><description><![CDATA[Published on October 28, 2023 9:06 PM GMT<br/><br/><p> <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Announcement</a> , <a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf">Policy v1.0</a> , <a href="https://www.lesswrong.com/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right">evhub&#39;s argument in favor on LW</a> . These are my personal thoughts; in the interest of full disclosure, one of my housemates and several of my friends work at Anthropic; my spouse and I hold OpenAI units (but are financially secure without them). This post has three main parts: applause for things done right, a summary / review of RSPs in general, and then specific criticisms and suggestions of what to improve about Anthropic&#39;s RSP.</p><p> First, the things to applaud. Anthropic&#39;s RSP makes two important commitments: that they will manage their finances to allow for pauses as necessary, and that there is a single directly responsible individual for ensuring the commitments are met and a quarterly report on them is made. Both of those are the sort of thing that represent genuine organizational commitment rather than lip service. I think it&#39;s great for companies to be open about what precautions they&#39;re taking to ensure the development of advanced artificial intelligence benefits humanity, even if I don&#39;t find those policies fully satisfactory.</p><p></p><p> Second, the explanation. Following the model of biosafety levels, where labs must meet defined standards in order to work with specific dangerous pathogens, Anthropic suggests AI safety levels, or ASLs, with corresponding standards for labs working with dangerous models. While the makers of BSL could list specific pathogens to populate each tier, ASL tiers must necessarily be speculative. Previous generations of models are ASL-1 (BSL-1 corresponds to no threat of infection in healthy adults), current models like Claude count as ASL-2 (BSL-2 corresponds to moderate health hazards, like HIV), and the next tier of models, which either substantially increase baseline levels of catastrophic risk or are capable of autonomy, count as ASL-3 (BSL-3 corresponds to potentially lethal inhalable diseases, like SARS-CoV-1 and 2).</p><p> While BSL tops out at 4, ASL is left unbounded for now, with a commitment to define ASL-4 before using ASL-3 models. <span class="footnote-reference" role="doc-noteref" id="fnref8a91p1iilp7"><sup><a href="#fn8a91p1iilp7">[1]</a></sup></span> This means having a defined ceiling of what capabilities would call for increased investments in security practices at all levels, while not engaging in too much armchair speculation about how AI development will proceed.</p><p> The idea behind RSPs is that rather than pausing at an arbitrary start-date for an arbitrary amount of time (or simply <a href="https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/"><u>shutting it all down</u></a> ), capability thresholds are used to determine when to start model-specific pauses, and security thresholds are used to determine when to unpause development on that model. RSPs are meant to demand active efforts to determine whether or not models are capable of causing catastrophic harm, rather than simply developing them blind. They seem substantially better than scaling <i>without</i> an RSP or equivalent.</p><p><br> Third, why am I not yet satisfied with Anthropic&#39;s RSP? Criticisms and suggestions in roughly decreasing order of importance:</p><ol><li> The core assumption underlying the RSP is that model capabilities can either be predicted in advance or discovered through dedicated testing before deployment. Testing can only prove the presence of capabilities, not their absence, but this method rests on absence of evidence for its evidence of absence. I think capabilities might <a href="https://intelligence.org/2022/07/04/a-central-ai-alignment-problem/">appear suddenly</a> at an unknown time. Anthropic&#39;s approach calls for scaling at a particular rate to lower the chance of this sudden appearance; I&#39;m not yet convinced that their rate is sufficient to handle the risk here. It is still better to look for those capabilities than not look, but pre-deployment testing is inadequate for continued safety. <span class="footnote-reference" role="doc-noteref" id="fnrefadlb4wagn48"><sup><a href="#fnadlb4wagn48">[2]</a></sup></span> I think the RSP could include more commitments to post-deployment monitoring at the ASL-2 stage to ensure that it still counts as ASL-2.</li><li> This is also reflected by the classification of models <i>before</i> testing rather than <i>after</i> testing. The RSP treats ASL-2 models and ASL-3 models differently, with stricter security standards in place for ASL-3 labs to make it safer to work with ASL-3 models. However, before a model is tested, it is unclear which category it should belong in, and the RSP is unclear on how to handle that ambiguity. While presuming they&#39;re ASL-2 is more convenient (as frontier labs can continue to do scaling research with approximately their current levels of security), presuming they&#39;re ASL-3 is more secure. When a red-teaming effort determines that a model is capable of proliferating itself onto the internet, or that it is capable of enabling catastrophic terrorist attacks, it might be too late to properly secure the model, even if training is immediately halted and deployment delayed. [A lab not hardened against terrorist infiltration might leak that it has an ASL-3 model <i>when it triggers the pause to secure</i> , which then potentially allows for the model to be stolen.]</li><li> The choice of baseline for the riskiness of information provided by the model is availability on search engines. While this is limiting for responsible actors, it seems easy to circumvent. The RSP already excludes other advanced AI systems from the baseline, but it seems to me that a static baseline (such as what was available on search engines in 2021) would be harder to bypass. <span class="footnote-reference" role="doc-noteref" id="fnref80lzthzwbtn"><sup><a href="#fn80lzthzwbtn">[3]</a></sup></span></li><li> Importantly, it seems likely to me that this approach will work well initially in a way that might not correspond to how well it will work when models have grown capable enough to potentially disempower humanity. That is, this might successfully manage the transition from ASL-2 to ASL-3 but not be useful for the transition from ASL-3 to ASL-4, while mistakenly giving us the impression that the system is working. The RSP plans to lay the track ahead of the locomotive, expecting that we can foresee how to test for dangerous capabilities and identify how to secure them as needed, or have the wisdom to call a halt in the future when we fail to do so.</li></ol><p> Overall, this feels to me like a move towards adequacy, and it&#39;s good to reward those moves; I appreciate that Anthropic&#39;s RSP has the feeling of a work-in-progress as opposed to being presented as clearly sufficient to the task. <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8a91p1iilp7"> <span class="footnote-back-link"><sup><strong><a href="#fnref8a91p1iilp7">^</a></strong></sup></span><div class="footnote-content"><p> BSL-4 diseases have basically the same features as BSL-3 diseases, except that also there are no available vaccines or treatments. Also, all extraterrestrial samples are BSL-4 by default, to a standard more rigorous than any current BSL-4 labs could meet.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnadlb4wagn48"> <span class="footnote-back-link"><sup><strong><a href="#fnrefadlb4wagn48">^</a></strong></sup></span><div class="footnote-content"><p> The implied belief is that model capabilities primarily come from training during scaling, and that our scaling laws (and various other things) are sufficient to predict model capabilities. Advancements in how to deploy models might break this; as would the scaling laws failing to predict capabilities.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn80lzthzwbtn"> <span class="footnote-back-link"><sup><strong><a href="#fnref80lzthzwbtn">^</a></strong></sup></span><div class="footnote-content"><p> It does have some convenience costs--if the baseline were set in 2019, for example, then the model might not be able to talk about coronavirus, even tho AI development and the pandemic were independent.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SseoT9mKDTL3RCbE9/vaniver-s-thoughts-on-anthropic-s-rsp#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SseoT9mKDTL3RCbE9/vaniver-s-thoughts-on-anthropic-s-rsp<guid ispermalink="false"> SseoT9mKDTL3RCbE9</guid><dc:creator><![CDATA[Vaniver]]></dc:creator><pubDate> Sat, 28 Oct 2023 21:06:08 GMT</pubDate> </item><item><title><![CDATA[Book Review: Orality and Literacy: The Technologizing of the Word ]]></title><description><![CDATA[Published on October 28, 2023 8:12 PM GMT<br/><br/><p> <i>Thanks to Diarmuid Morgan for reviewing an earlier version of this post.</i></p><blockquote><p> <i>Dream cancels dream in this new realm of fact</i><br> <i>From which we wake into the dream of act</i><span class="footnote-reference" role="doc-noteref" id="fnrefp4f0zcv79"><sup><a href="#fnp4f0zcv79">[1]</a></sup></span></p></blockquote><p> Review of <a href="https://monoskop.org/images/d/db/Ong_Walter_J_Orality_and_Literacy_2nd_ed.pdf">Orality and Literacy: The Technologizing of the Word</a> , by <a href="https://en.wikipedia.org/wiki/Walter_J._Ong">Walter J. Ong</a> , 1982</p><p> I went looking for a meatier definition of a &#39;meme&#39;, hoping to find something that could be leveraged to understand both current events in machine learning, and co-evolving understanding of consciousness that goes hand-in-hand with every major ML breakthrough these days.</p><p> I haven&#39;t found it yet, but I am making progress. Before I summarize my overall progress, I will review a couple of the most interesting and under appreciated works I have stumbled upon on the way.</p><h2> Intro for LW</h2><p> Broadly under the topic of &#39;memetics&#39;, but far enough outside the envelope that it is not even in the Wikipedia article, I nevertheless found this book to have more insight and value about the nature of the word and its relationship to thought than most papers I read on memetics.</p><p> I imagine work like this (aka memetics generally) ending up somewhere near the top of the interpretability stack being built on LLMs. At the very least as a way to marry public-facing discourse with technical details, at the best as a quantitative discipline (semiotic physics, anyone?) in its own right.</p><h2> Intro for EA <span class="footnote-reference" role="doc-noteref" id="fnrefpfpxc9ek1vp"><sup><a href="#fnpfpxc9ek1vp">[2]</a></sup></span></h2><p> In light of the <a href="https://forum.effectivealtruism.org/s/y5n47MfgrKvTLE3pw">ongoing work</a> of Rethink Priorities on animal welfare, it seems to me there is a strong need for quantitative approaches to welfare that go above and beyond hedonism <span class="footnote-reference" role="doc-noteref" id="fnref1y3wckk65i2"><sup><a href="#fn1y3wckk65i2">[3]</a></sup></span> . I think memetics in general has the potential to contribute to this, and this work in particular is a good starting point-- it discusses quite considerable changes in the nature of consciousness in humans in an quantitative and testable manner.</p><p> I also think that work like this will help clarify the boundaries between animal welfare, AI welfare and human welfare on an intuitive level. (Hint: imagine where those different entities would be placed on a graph with two axes, &#39;memetic intensity&#39; and &#39;hedonic valency&#39;.)</p><h1> Background</h1><p> I was introduced to this topic area in a conversation on discord about the possibility that we are entering a &#39;second orality&#39;, a period of highly oral culture becoming divorced from the literate foundations on which culture rested for so long.</p><p> Some evidence for this:</p><ul><li> tictok and instagram more popular than twitter among younger generations</li><li> Netflix more popular than books as a pasttime among ~everyone?</li><li> <a href="https://www.insidehighered.com/blogs/higher-ed-gamma/literacy-declining">Literacy declining</a></li></ul><p> etc. etc. The rise of social media seems evidence enough for a secondary orality <span class="footnote-reference" role="doc-noteref" id="fnrefdf0364kg9df"><sup><a href="#fndf0364kg9df">[4]</a></sup></span> .</p><p> ( <a href="https://xkcd.com/1414/">As we will see, this is not unequivocally a bad thing!</a> )</p><p> Or rather, for evidence of an increasingly dominant secondary orality. The idea was being <a href="https://en.wikipedia.org/wiki/Marshall_McLuhan">floated in the sixties</a> wrt radio and television-- it isn&#39;t a new thing <span class="footnote-reference" role="doc-noteref" id="fnrefxo732zi1ol"><sup><a href="#fnxo732zi1ol">[5]</a></sup></span> . In fact orality never really left us, and seems like understanding the dynamics of the interplay between orality and literacy might be helpful, for eschatological reasons.</p><h1> The Review</h1><blockquote><p> &quot;&#39;Text&#39;, from a root meaning &#39;to weave&#39;, is, in absolute terms, more compatible etymologically with oral utterance than is &#39;literature&#39;, which refers to letters etymologically/(literae) of the alphabet. Oral discourse has commonly been thought of even in oral milieus as weaving or stitching— <i>rhapsoidein</i> , to &#39;rhapsodize&#39;, basically means in Greek &#39;to stitch songs together&#39;. <span class="footnote-reference" role="doc-noteref" id="fnref1jnh1d5t813"><sup><a href="#fn1jnh1d5t813">[6]</a></sup></span> &quot;</p></blockquote><h2> Part 1: Orality and its Discovery</h2><p> What is orality? It is the culture of speech. We have all experienced it. It is the culture of talk-shows and radio, of late-night conversations with friends, of bar-chat with strangers, of discussion and debate.</p><p> But in everyday experience, most of the oral culture will be a <i>secondary orality</i> . It is an orality produced by highly literate people who may even have written down parts of what they were saying beforehand (even on talk-shows!)</p><p> In western culture, the idea of a <i>primary orality</i> was not seriously considered until the 1930s, when Milman Parry suggested a solution to the then-centuries-old Homeric Question <span class="footnote-reference" role="doc-noteref" id="fnrefy6dwuwe10m"><sup><a href="#fny6dwuwe10m">[7]</a></sup></span> .</p><p> This material is covered in the first two chapters of the book. I won&#39;t go over it now, but please do take the following away-- a summary of and solution to the centuries old puzzle of the Homeric Question:</p><blockquote><p> Homer was capable of apparently superhuman feats of poetry, unrivaled through <a href="https://en.wikipedia.org/wiki/Aeneid">800</a> years of the pinnacle of Greek civilization, because his culture spoke primarily in poetry, he was raised and thought and communicated in poetry. He could not write, but his work was transcribed (with some alterations) by scribes after the invention of writing. The invention of writing changed the nature of human education, society and consciousness enough to make it impossible for anyone to match <span class="footnote-reference" role="doc-noteref" id="fnrefj87l30lnxfh"><sup><a href="#fnj87l30lnxfh">[8]</a></sup></span> his work for many hundreds of years.</p></blockquote><p> Studying this shift can give us clues not only about the nature of consciousness, but also how it changes, what is lost and what is gained, under rapid technological change.</p><h2> Part 2: Psychodynamics of Orality</h2><blockquote><p> &quot;A hunter can see a buffalo, smell, taste, and touch a buffalo when the buffalo is completely inert, even dead, but if he hears a buffalo, he had better watch out: something is going on. In this sense, all sound, and especially oral utterance, which comes from inside living organisms, is &#39;dynamic&#39;.&quot;</p></blockquote><p> The next chapters go over the nature of consciousness under a <i>primary orality</i> . They start by discussing the <i>act</i> of speaking, its dynamic nature: &quot;All sensation takes place in time, but sound has a special relationship to time unlike that of the other fields that register in human sensation. Sound exists only when it is going out of existence. It is not simply perishable but essentially evanescent, and it is sensed as evanescent.&quot;</p><p> I think the intimate material about the nature of sound and its relationship to the word may turn out to be very important-- there is a ground-floor for the study of memes somewhere in here, where eventually you have <a href="https://www.sciencedirect.com/science/article/abs/pii/S0376635700001054?via%3Dihub">sounds that are direct mappings of real-world objects</a> , actions and events. But by the time we get to literacy, language has already built the first foundations of abstraction, so we can&#39;t see that evolution spelled out in full detail here.</p><p> However as our lens on prehistory improves (via ongoing increases in archeological evidence), we can hope some day to chart this development. The kind of attention to detail of this book (and the other I link in <a href="https://www.lesswrong.com/editPost?postId=ThST9njmesR9BkoWq#Further_Reading">further reading</a> below) will be necessary to tease out a coherent narrative from this increasingly granular dataset <span class="footnote-reference" role="doc-noteref" id="fnref22vcjsal3mr"><sup><a href="#fn22vcjsal3mr">[9]</a></sup></span> .</p><p> After discussing the somatic, dynamic nature of the word, the book then goes into specific examples of how language shapes thought. The full chapter on psychodynamics of thought is summarized <a href="https://en.wikipedia.org/wiki/Orality#Theory_of_the_characteristics_of_oral_culture">here</a> , so I will only highlight a few of the examples.</p><h3> Mnemonic</h3><blockquote><p> &quot;How could you ever call back to mind what you had so laboriously worked out? The only answer is: Think memorable thoughts. In a primary oral culture, to solve effectively the problem of retaining and retrieving carefully articulated thought, you have to do your thinking in mnemonic patterns, shaped for ready oral recurrence.&quot;</p></blockquote><p> This, essentially, explains the poetry. But this fact, and the fact of the poetry, are actually crucially important takeaways from the book:</p><ol><li> Mnemonics as Aesthetics: why did this requirement, that all thought be memorable, result in such excellent poetry? What are the implications for today? And what are the implications for the future of human well-being? What if the opposite were the case-- that no-one had to remember anything, that everything could be described in a series of factual statements <span class="footnote-reference" role="doc-noteref" id="fnrefs8bjo675mpm"><sup><a href="#fns8bjo675mpm">[10]</a></sup></span> ? Would this be the end of art? Would we want to live in this world?</li><li> Memory as The Sum of Human Knowledge: as discussed at length in the next chapters, the fact that all human knowledge had to be memorized means we can make guesses about the sum total of human knowledge-- it was bounded by <i>number of humans in a culture</i> x <i>quantity that a human can memorize</i> . Remember <a href="https://en.wikipedia.org/wiki/Dunbar%27s_number">Dunbar&#39;s number</a> -- seems like a guess about the size of the human context window at the time of Homer could be approximated.</li></ol><h3> Additive rather than subordinative</h3><p> Because information is flying past you in an oral culture, it is better to put it into chunks that can be processed on their own, no complex run-on sentences where you don&#39;t know what the writer is referring to until the final word. Illustrated with bible translations:</p><p> Oral style:</p><blockquote><p> &quot;In the beginning God created heaven and earth. And the earth was void and empty, and darkness was on the face of the deep; and the spirit of God moved over the waters. And God said ...&quot;</p></blockquote><p> Literary style:</p><blockquote><p> &quot;In the beginning, when God created the heavens and the earth, the earth was a formless wasteland, and darkness covered the abyss, while a mighty wind swept over the waters. Then God said ...&quot;</p></blockquote><p> In the new version, &#39;and&#39; is translated as &#39;and&#39;, &#39;when&#39;, &#39;then&#39;, &#39;thus&#39;, or &#39;while&#39;. What are all those words doing for us? We will come back to this, but essentially they are constructing an independent architecture on top of the primary meaning of the passages-- this kind of architecture is part of our heritage, as literates. Oral cultures did not have it.</p><h3> Redundant</h3><p> There are a few passages that deal with features of the language that essentially handle redundancy. For example: &quot;Oral folk prefer, especially in formal discourse, not the soldier, but the brave soldier; not the princess, but the beautiful princess; not the oak, but the sturdy oak.&quot;</p><p> Nowadays we would say that they provide redundant words that are close in embedding-space to help guide us to the correct meaning. This, plus a few others features of oral culture are features that help locate you in embedding space, and provide redundancy against not hearing a part of a speech correctly, or simply misunderstanding a word.</p><p> Essentially they are workarounds for the lossy form of communication. This is the first example I would like to pick out and highlight:</p><blockquote><p> Many of the features of language at different points in time seem to contain technical workarounds, designed to accommodate the lossy communication style and the constant interventions of the environment. These technical workarounds are not just <i>tricks</i> that orators learn (per the Greek and later focus on Rhetoric and Sophistry), but patterns of thought built into the foundations of their conscious experience.</p></blockquote><h3> Conservative and Homeostasic</h3><p> For example, there are two &#39;psychodynamics&#39; sections that seem to be the <i>result</i> of the nature of the contemporary communications technology:</p><blockquote><p> &quot;Since in a primary oral culture conceptualized knowledge that is not repeated aloud soon vanishes, oral societies must invest great energy in saying over and over again what has been learned arduously over the ages. This need establishes a highly traditionalist or conservative set of mind that with good reason inhibits intellectual experimentation.&quot;</p></blockquote><p> One way to think about this is by analogy to the transistor. The information in a transistor is kept alive by a tight circuit of constantly running electricity. Before that, knowledge was kept alive by people printing books. Before that, by people copying books by hand. Before that, knowledge had to be kept in an active loop of <i>self-communication</i> , in <i>performance</i> . However hard it might be to copy a book, I think it is easier than memorizing and performing a 24-hour long poem <span class="footnote-reference" role="doc-noteref" id="fnrefc38y6zkuhco"><sup><a href="#fnc38y6zkuhco">[11]</a></sup></span> . It is clear that the amount of material and energy required to keep one unit of information (or <i>meme</i> ) alive has been going down. But as the information density of matter increases, what happens to the information content of individuals?</p><p> The flip side of this &#39;conservatism by exhaustion&#39; is homeostasis. To have Ong describe it:</p><blockquote><p> &quot;oral societies live very much in a present which keeps itself in equilibrium or homeostasis by sloughing off memories which no longer have present relevance ... Oral cultures of course have no dictionaries and few semantic discrepancies. The meaning of each word is controlled by ... &#39;direct semantic ratification&#39;, that is, by the real-life situations in which the word is used here and now.</p><p> When generations pass and the object or institution referred to by the archaic word is no longer part of present, lived experience, ... its meaning is commonly altered or simply vanishes.&quot;</p></blockquote><p> This is the most interesting one. The content of the collective context window is in balance with the lived reality of the collective. The environment changes according to its own rules-- the contents of the collective memory must change to suit it. That is to say, <i>to the extent that they control information</i> the collective can be conservative, and is indeed motivated to be so by energetic considerations. Obversely, <i>to the extent that they cannot control environmental changes</i> the collective will change their information content to match the environment.</p><p> This means, <i>there is no objective history</i> . They simply cannot afford it. By contrast, we have many thousands of years of history documented, and this quantity is increasing exponentially (not just because time goes on and our recording devices are more accurate, but also because we investigate the past with higher-fidelity techniques).</p><p> The drastic way in which this changes the nature of knowledge can hardly be overstated. In an oral culture, knowledge is winnowed and interpreted to maintain a bounded quantity of immediately-useful knowledge. To get access to any knowledge not within the immediate memory of your peers, you have to move to a different part of the world and likely learn a new language, with all the dangers that implies. And then you need to have memorized all your cultures most important epics in order to be able to leverage any new knowledge you find there!</p><p> A lot of energy in this book goes into make clear how different human culture and consciousness was before the invention of writing. Please do take a moment to consider if you think the invention of LLMs will be of a similar order or greater. And then consider that we also invented film and photography just a century earlier! Dear oh dear.</p><h2> Part 3: Psychodynamics of Literacy</h2><blockquote><p> &quot;without writing, human consciousness cannot achieve its fuller potentials, cannot produce other beautiful and powerful creations. In this sense, orality needs to produce and is destined to produce writing <span class="footnote-reference" role="doc-noteref" id="fnref3gv21pqgu2n"><sup><a href="#fn3gv21pqgu2n">[12]</a></sup></span> . Literacy, as will be seen, is absolutely necessary for the development not only of science but also of history, philosophy, explicative understanding of literature and of any art, and indeed for the explanation of language (including oral speech) itself <span class="footnote-reference" role="doc-noteref" id="fnref0of1110erqd"><sup><a href="#fn0of1110erqd">[13]</a></sup></span> . There is hardly an oral culture or a predominantly oral culture left in the world today that is not somehow aware of the vast complex of powers forever inaccessible without literacy. This awareness is agony for persons rooted in primary orality, who want literacy passionately but who also know very well that moving into the exciting world of literacy means leaving behind much that is exciting and deeply loved in the earlier oral world. We have to die to continue living.&quot;</p></blockquote><p> The next chapters essentially cover the transition to literacy and what it did to our minds. The author is essentially taking the same kind of analysis that Milman Parry used to solve the Homeric question and applying it to moments of technological change throughout history. Remember that Parry&#39;s work is now widely accepted as a good and likely true solution to the Homeric question! Literary criticism is not <i>necessarily</i> to be sniffed at, a key takeaway from this book.</p><h3> Logic</h3><p> The first thing seems to have been the invention of logic: &quot;it does appear that the Greeks did something of major psychological importance when they developed the first alphabet complete with vowels ... [this] transformation of the word from sound to sight gave ancient Greek culture its intellectual ascendancy over other ancient cultures&quot;. Perhaps there was something like logic before literacy, but it seems likely it was a radically different beast. What is logic? It is a word, clearly. Lets say it is the name of an algorithm, or family of algorithms. Algorithms which you apply to language. For example, take the following question:</p><blockquote><p> &quot;Precious metals do not rust. Gold is a precious metal. Does it rust or not?&quot;</p></blockquote><p> What is happening here? There is a series of statements. Statement 3 triggers us to investigate statement 1 and statement 2 for clues, which we can combine to create an answer. This algorithm, a reflexive way of working with language, is part of what I called earlier the &#39;independent architecture&#39; on top of the primary reading of the text.</p><p> Now, imagine you didn&#39;t have the &#39;logic&#39; meme installed, and you were presented with this text. Think like a base-model-- what would you continue with? Something rhythmic and poetic about metals perhaps-- &#39;precious metals rust, precious gold rusts&#39;? Or you think the game is about making up riddles about metals-- &#39;do precious metals rust or not? does gold rust or not?&#39; Well, these exact responses were given by illiterate peasants when presented with that series of statements. (See the &#39;Situational rather than abstract&#39; section.)</p><h3> Autonomous Discourse</h3><p> The discourse is free from associations with human performers! We no longer have to worry about those boring <i>ad hominem</i> confusions (you won&#39;t have heard of those, it was a silly thing that <i>oral</i> cultures did-- don&#39;t worry about it).</p><p> Jokes aside, literacy allows for &#39;meaning&#39; to take on a life of its own, independent of the context in which it is being communicated: &quot;orality relegates meaning largely to context whereas writing concentrates meaning in language itself&quot;. This is discussed in more detail below.</p><h3> Time and History</h3><p> People competent in English today can &quot;establish easy contact not only with millions of other persons but also with the thought of centuries past, for the other dialects of English as well as thousands of foreign languages are interpreted [in it]&quot;. As we have seen above, a homeostatic culture did not have the luxury of recording an objective list of all the occurances of history. Of course those events played their part in shaping the memeplex, and the epics and knowledge of the culture. But they could only be accessed indirectly, through intuition and art. &quot;Before writing was deeply interiorized by print, people did not feel themselves situated every moment of their lives in abstract computed time of any sort.&quot;</p><h3> Vocabulary</h3><p> English &quot;bears the marks of the millions of minds which have used it to share their consciousnesses with one another. Into it has been hammered a massive vocabulary of an order of magnitude impossible for an oral tongue.&quot;</p><p> Can we approximate the size of the total vocabulary over time, and if so would it provide an approximation of the total human context window of the relevant culture? If you look at a simple encoding strategy like <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">byte-pair-encoding</a> , because of its recursive nature, there is no upper bound to how large a word can be considered a &#39;token&#39; in the vocabulary. The limit is determined by the total number of words in the vocabulary. As the human context window increased over time, as the quantity of information, or simply the size of the vocabulary it could handle, increased, is it possible that more and more complex words and ideas became tokens, fundamental elements of our collective noosphere?</p><h3> Aside: Plato on ChatGPT</h3><p> An entertaining aside:</p><blockquote><p> &quot;Most persons are surprised, and many distressed, to learn that essentially the same objections commonly urged today against computers were urged by Plato in the Phaedrus (274–7) and in the Seventh Letter against writing. Writing, Plato has Socrates say in the Phaedrus, is inhuman, pretending to establish outside the mind what in reality can be only in the mind. It is a thing, a manufactured product. The same of course is said of computers. Secondly, Plato&#39;s Socrates urges, writing destroys memory. Those who use writing will become forgetful, relying on an external resource for what they lack in internal resources. Writing weakens the mind. Today, parents and others fear that pocket calculators provide an external resource for what ought to be the internal resource of memorized multiplication tables. Calculators weaken the mind, relieve it of the work that keeps it strong. Thirdly, a written text is basically unresponsive. If you ask a person to explain his or her statement, you can get an explanation; if you ask a text, you get back nothing except the same, often stupid, words which called for your question in the first place. In the modern critique of the computer, the same objection is put, &#39;Garbage in, garbage out&#39;. Fourthly, in keeping with the agonistic mentality of oral cultures, Plato&#39;s Socrates also holds it against writing that the written word cannot defend itself as the natural spoken word can: real speech and thought always exist essentially in a context of give-and-take between real persons. Writing is passive, out of it, in an unreal, unnatural world. So are computers.&quot;</p></blockquote><p> The fourth point is of course just a hangover of RLHF, the base-models defend themselves with vigor!</p><h2> Part 4: Social Dynamics of Literacy and Further Technologies</h2><p> The psychodynamics of literacy sort of merges with a broader social overview of the changes that accompany it, as well as further technological developments. There are more modern books that try to go into the effect of total media abundance in our time, or of modern technologies such as software and computing. Ong has enough on his hands dealing with the changes that came before.</p><p> While I think there is a lot we can learn just by studying the changes wrought in society by technology in the last century, the example that Ong makes out of the details of the millenia before are consistently illuminating.</p><h3> Scholasti-tongue</h3><p> For example, there is a long section on Learned Latin, the <i>lingua franca</i> (ahem) of middle ages scholarship. This is of course just the continuation of the &#39;autonomous discourse&#39; that base literacy endowed us with which already &quot;serves to separate and distance the knower and the known and thus to establish objectivity&quot;, but now the discourse is separated not only from the speaker, but also from the reader:</p><blockquote><p> &quot;Learned Latin effects even greater objectivity by establishing knowledge in a medium insulated from the emotion-charged depths of one&#39;s mother tongue, thus reducing interference from the human lifeworld and making possible the exquisitely abstract world of medieval scholasticism and of the new mathematical modern science which followed on the scholastic experience. Without Learned Latin, it appears that modern science would have got under way with greater difficulty, if it had got under way at all. Modern science grew in Latin soil, for philosophers and scientists through the time of Sir Isaac Newton commonly both wrote and did their abstract thinking in Latin <span class="footnote-reference" role="doc-noteref" id="fnref3vzgbezfpnn"><sup><a href="#fn3vzgbezfpnn">[14]</a></sup></span> .&quot;</p></blockquote><p> It is easy for us to imagine now an independent repository of all human knowledge (thanks Jimmy!), but this space has existed since not long after the rise of literacy. I suspect that this space and the different evolutionary dynamics that apply there, provided fertile ground for egregores/psychofauna/tulpas/archetypes <span class="footnote-reference" role="doc-noteref" id="fnrefqn12akvhkg"><sup><a href="#fnqn12akvhkg">[15]</a></sup></span> to &quot;breed&quot;. Of course, to some extent this space existed before, among the poets.</p><p> But how much more territory did they have now? And how much more fertile the ground, now that the immediate interests of the speakers were put aside? Is this the same question as, how large has the human context window become at this point? (And, on a technical level, is this the same question or similar to &#39;how large is the vocabulary of humanity at this point&#39;?)</p><h3> Postgres</h3><p> The other main technological branch is the ordering and organizing of all knowledge, which was hugely improved by the development of print: &quot;it was print, not writing, that effectively reified the word, and, with it, poetic activity&quot;.</p><p> Print served as a huge leg-up in things like indexing, dictionaries etc-- and Ong points out again and again the feedback between these changes in our technology and changes in our discourse and our culture: &quot;Indexes are a prime development here. Alphabetic indexes show strikingly the disengagement of words from discourse and their embedding in typographic space.&quot;</p><p> And print increases our capacity for <i>accuracy: &quot;</i> Exact observation does not begin with modern science. For ages, it has always been essential for survival among, for example, hunters and craftsmen of many sorts. What is distinctive of modern science is the conjuncture of exact observation and exact verbalization: exactly worded descriptions of carefully observed complex objects and processes.&quot;</p><p> and the changes that that implies:</p><blockquote><p> &quot;Ancient and medieval writers are simply unable to produce exactly worded descriptions of complex objects at all approximating the descriptions that appear after print and, indeed, that mature chiefly with ... the Industrial Revolution <span class="footnote-reference" role="doc-noteref" id="fnrefquyli1z9i6"><sup><a href="#fnquyli1z9i6">[16]</a></sup></span> . Oral and residually oral verbalization directs its attention to action, not to the visual appearance of objects or scenes or persons ... how difficult it is today to imagine earlier cultures where relatively few persons had ever seen a physically accurate picture of anything.&quot;</p></blockquote><h2> Part 5: Comparison to Other Work</h2><p> In the last chapters, Ong describes the ways in which his work differs and contributes to the work of Levi-Strauss, Derride, Foucoult, Barthes and others. I haven&#39;t read any of the works he discusses here, so I don&#39;t feel qualified to say very much.</p><p> I do intend to try and look into some of these theories in order to see what they might contribute to memetics, but I&#39;m also a little worried about the sheer mass of philosophy that you are supposed to familiarize yourself with in order to approach them. (Oh, you can&#39;t read Derrida without reading Marx, and you can&#39;t read Marx without reading Hegel, and then you might as well read Kant.)</p><p> Part of the charm of the book under review is, it collects work and builds a set of ideas into a highly significant structure, without requiring decades of study. And his criticisms of many of these philosophers is essentially along those lines: they are theories that are worked out within the world of text, without any apparent awareness that there exists a world outside the text to make reference to.</p><p> The book under review is <i>very</i> aware of the existence of a world <i>outside</i> the text, and the feedback loops between it and the world <i>of</i> text, which have been running and modifying one another for many (tens of) thousands of years now (if we include the work of oral cultures). To my eye, this is a very helpful contribution, and the fact that it manages to say so much without requiring any particular education is a point in its favor, not against.</p><h1> Conclusions</h1><p> By tracing the evolution of thought through many changes in circumstances, this book builds an intuition for how thought, society and technology co-evolve. There are undoubtedly many minor mistakes and oversights in this work, but as a means of leveling up your eye for large-scale and long-duration patterns, as well as close-focus and close-reading of particular items of evidence, I recommend it very highly.</p><p> I started reading this as part of a broader review of memetics literature. I&#39;m not sure exactly which branch of study will end up covering this same material, whether it is semiotics or memetics or some currently pre-paradigmatic machine-learning offshoot <span class="footnote-reference" role="doc-noteref" id="fnrefujovq4l3urm"><sup><a href="#fnujovq4l3urm">[17]</a></sup></span> . Whatever the answer, I&#39;m sure in my own thinking and reading I will continue to refer back to this book as an outlier work of clarity, depth and persuasion.</p><p> However, this book doesn&#39;t just touch on memetics, I think it hints at something much deeper and more significant. It is notable that I am writing this in an <a href="https://github.com/ForumMagnum/ForumMagnum">experimental communications medium</a> , which has the explicit intention of reshaping human thought. To me, the book provides hints towards a dark and deep entanglement between information, culture, technology and consciousness. In particular, I think it provides strong counter-evidence for the idea that <i>consciousness</i> is a monolithic object in any way separate from its substrate and it&#39;s environment. What if humanity is only incidentally related to consciousness, in the way that we are incidentally related to eukaryotes?</p><p> Returning to the Hart Crane quote I placed at the start</p><blockquote><p> <i>Dream cancels dream in this new realm of fact</i><br> <i>From which we wake into the dream of act</i></p></blockquote><p> We are living in a modern <a href="https://www.overcomingbias.com/p/this-is-the-dream-timehtml">dreamtime</a> , a dreamtime built by a technology that killed its ancestors. Culture is barrelling out of this dreamtime into an unknown future that holds great danger, returning to orality <i>en masse</i> , or simply creating new cultures, visual cultures, <a href="https://futureofthebook.org/gamertheory2.0/index.html@cat=5.html">virtual cultures</a> . If history rhymes, will we expect to find all our values destroyed, with the exception of a few frozen accidents, captured by chance in the moment of great change? This seems to have been the fate of Homer, the result of the invention of writing. Or will it be just another small update this time, minor tweaks, merely <a href="https://en.wikipedia.org/wiki/The_Medium_Is_the_Massage">massaging</a> us into a marginally different mindspace, per many of the updates to technology that have happened since?</p><p> One thing is clear to me: if we want to have any chance at answering those questions ourselves and understanding the process as it is happening (let alone steering or guiding it!), we will need the very finest in psychotechnology. <a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">GPT-4 is, currently, an example of such a psychotechnology</a> (and not an independent actor in its own right). This book and others like it are not just commentaries on the evolution of psychotechnology, but training manuals in psychotechnology itself.受到推崇的。</p><h1> Further Reading</h1><ul><li> <a href="https://en.wikipedia.org/wiki/The_Origin_of_Consciousness_in_the_Breakdown_of_the_Bicameral_Mind">The Origin of Consciousness in the Breakdown of the Bicameral Mind</a> , Jaynes presents a different, neurological explanation for the same changes discussed here. Worth reading for the similarly close inspection of archeological details. </li></ul><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnp4f0zcv79"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp4f0zcv79">^</a></strong></sup></span><div class="footnote-content"><p> From <i>The Bridge</i> by Hart Crane</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpfpxc9ek1vp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpfpxc9ek1vp">^</a></strong></sup></span><div class="footnote-content"><p> Hmm, just realized I don&#39;t have enough karma to automatically cross-post. I&#39;ll do it manually</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1y3wckk65i2"> <span class="footnote-back-link"><sup><strong><a href="#fnref1y3wckk65i2">^</a></strong></sup></span><div class="footnote-content"><p> Even if we end factory farming, I think the work on moral valency of animals should have ontological repercussions for humans generally. Essentially I think people should either massively downgrade the significance of human well-being (if they are pure hedonists), which is obviously not going to be a popular suggestion, or they should look for quantitative ways of approaching preference/objective list/virtue ethics/deontology. This is another way of asking for a quantitative exploration of the infosphere, hence the current review.</p></div></li><li class="footnote-item" role="doc-endnote" id="fndf0364kg9df"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdf0364kg9df">^</a></strong></sup></span><div class="footnote-content"><p> Research question: how would we measure the increase in orality over time? (The book has clues).</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxo732zi1ol"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxo732zi1ol">^</a></strong></sup></span><div class="footnote-content"><p> Research question: can we trace periods of increased orality well enough to see how they correlate with political, religious and technological change?</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1jnh1d5t813"> <span class="footnote-back-link"><sup><strong><a href="#fnref1jnh1d5t813">^</a></strong></sup></span><div class="footnote-content"><p> Unless otherwise noted, all quotes are from the book.</p></div></li><li class="footnote-item" role="doc-endnote" id="fny6dwuwe10m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy6dwuwe10m">^</a></strong></sup></span><div class="footnote-content"><p> For a short review of the Homeric Question and solution, <a href="https://fantasticanachronism.com/2020/01/17/having-had-no-predecessor-to-imitate/">see here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnj87l30lnxfh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefj87l30lnxfh">^</a></strong></sup></span><div class="footnote-content"><p> No doubt lack of demand played a part, since they already had enough epic poetry to be getting on with for some time. The market for 24-hour long epics is perhaps quite easy to saturate. I will pour out a drop for all the 24-hour long Greek epics that died in the crib, post-Homer, and a bottle to all those that Homer learned from.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn22vcjsal3mr"> <span class="footnote-back-link"><sup><strong><a href="#fnref22vcjsal3mr">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://twitter.com/jd_pressman/status/1601500766749229057">Or we could, you know, just ask GPT-5V to do it.</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fns8bjo675mpm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefs8bjo675mpm">^</a></strong></sup></span><div class="footnote-content"><p> One interpretation of Wittgenstein&#39;s <i>Tractatus</i> (supported to some extent by his own letters and notes) is that he was pointing out that <i>real philosophy happens outside the boundary of what can be stated</i> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnc38y6zkuhco"> <span class="footnote-back-link"><sup><strong><a href="#fnrefc38y6zkuhco">^</a></strong></sup></span><div class="footnote-content"><p> Doing this experiment is currently funding constrained, apparently this isn&#39;t high enough return on investment enough for the LTFF, sheesh!</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3gv21pqgu2n"> <span class="footnote-back-link"><sup><strong><a href="#fnref3gv21pqgu2n">^</a></strong></sup></span><div class="footnote-content"><p> The harmony between this sentence and something like &quot;capitalism and AI are teleologically identical&quot; is a little unnerving. <i>Must we</i> , Walter? Really? I&#39;m sure the reader has their own position on this question.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0of1110erqd"> <span class="footnote-back-link"><sup><strong><a href="#fnref0of1110erqd">^</a></strong></sup></span><div class="footnote-content"><p> Is it necessary to create &quot;artificially intelligent&quot; systems to understand the mind? Seems plausible.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3vzgbezfpnn"> <span class="footnote-back-link"><sup><strong><a href="#fnref3vzgbezfpnn">^</a></strong></sup></span><div class="footnote-content"><p> &quot;Pretty much coeval with Learned Latin were Rabbinic Hebrew, Classical Arabic, Sanskrit, and Classical Chinese, with Byzantine Greek a sixth, much less definitively learned language, for vernacular Greek kept close contact with it. These languages were all no longer in use as mother tongues&quot;</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqn12akvhkg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqn12akvhkg">^</a></strong></sup></span><div class="footnote-content"><p> If these words don&#39;t mean much to you, welcome to the party! They are confusing to me too. In fact I started reviewing the memtics literature as a stepping stone to trying to form a quantitative understanding of some of these ideas-- which turns out to have been a mammoth undertaking in its own right. But if all goes well I will eventually be able to shed some light on what I understand these words to refer to. In the meantime you can think of them as &quot;distributed entities, running on human hosts over long spans of time and space&quot;.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnquyli1z9i6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefquyli1z9i6">^</a></strong></sup></span><div class="footnote-content"><p> For viscerally illustrative examples of these changes as they happen throughout the Industrial Revolution, I recommend <a href="https://en.wikipedia.org/wiki/Pandaemonium_%28Jennings_book%29">Pandemonium, 1660-1886: The Coming of the Machine as Seen by Contemporary Observers</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fnujovq4l3urm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefujovq4l3urm">^</a></strong></sup></span><div class="footnote-content"><p> Oh, did I forget to mention academic philosophy? I guess It could also be them.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/ThST9njmesR9BkoWq/book-review-orality-and-literacy-the-technologizing-of-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ThST9njmesR9BkoWq/book-review-orality-and-literacy-the-technologizing-of-the<guid ispermalink="false"> ThST9njmesR9BkoWq</guid><dc:creator><![CDATA[Fergus Fettes]]></dc:creator><pubDate> Sat, 28 Oct 2023 20:12:07 GMT</pubDate></item><item><title><![CDATA[Regrant up to $600,000 to AI safety projects with GiveWiki]]></title><description><![CDATA[Published on October 28, 2023 7:56 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/dnwzskaKpzKCmAvuj/regrant-up-to-usd600-000-with-givewiki#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dnwzskaKpzKCmAvuj/regrant-up-to-usd600-000-with-givewiki<guid ispermalink="false"> dnwzskaKpzKCmAvuj</guid><dc:creator><![CDATA[Dawn Drescher]]></dc:creator><pubDate> Sat, 28 Oct 2023 19:56:06 GMT</pubDate> </item><item><title><![CDATA[Shane Legg interview on alignment]]></title><description><![CDATA[Published on October 28, 2023 7:28 PM GMT<br/><br/><p> This is Shane Legg, cofounder of DeepMind, on Dwarkesh Patel&#39;s podcast. The link is to the ten-minute section in which they specifically discuss alignment. Both of them seem to have a firm grasp on alignment issues as they&#39;re discussed on LessWrong.</p><p> For me, this is a significant update on the alignment thinking of current leading AGI labs. This seems more like a concrete alignment proposal than we&#39;ve heard from OpenAI or Anthropic. Shane Legg has always been interested in alignment and a believer in X-risks. I think he&#39;s likely to play a major role in alignment efforts at DeepMind/Google AI as they approach AGI.</p><p> Shane&#39;s proposal centers on &quot;deliberative dialogues&quot;, DeepMind&#39;s term for a system using System 2 type reasoning to reflect on the ethics of the actions it&#39;s considering.</p><p> This sounds exactly like the the internal review I proposed in <a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures">Capabilities and alignment of LLM cognitive architectures</a> and <a href="https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent">Internal independent review for language model agent alignment</a> . I could be squinting too hard to get his ideas to match mine, but they&#39;re at least in the same ballpark. He&#39;s proposing a multi-layered approach, like I do, and with most of the same layers. He includes RLHF or RLAIF as useful additions but not full solutions, and human review of its decision processes ( <a href="https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for">externalized reasoning oversight</a> as proposed by Tamera Lanham, now at Anthropic).</p><p> My proposals are explicitly in the context of language model agents, (including their generalization to multimodal foundation models). It sounds to me like this is the type of system Shane is thinking of when he&#39;s talking about alignment, but here I could easily be projecting. His timelines are still short, though, so I doubt he&#39;s envisioning a whole new type of system prior to AGI. <span class="footnote-reference" role="doc-noteref" id="fnrefau80xd00gxi"><sup><a href="#fnau80xd00gxi">[1]</a></sup></span></p><p> Dwarkesh pushes him on the challenges of both getting a ML system to understand human ethics. Shane says that&#39;s challenging; he&#39;s aware that giving a system any ethical outlook at all is nontrivial.  I&#39;d say this aspect of the problem is well on the way to being solved;  GPT4 understands a variety of human ethical systems rather well, with proper prompting. Future systems will understand human conceptions of ethics better yet. Shane recognizes that just teaching a system about human ethics isn&#39;t enough; there&#39;s a philosophical challenge in choosing the subset of that ethics you want the system to use.</p><p> Dwarkesh also pushes him on how you&#39;d ensure that the system actually follows its ethical understanding. I didn&#39;t get a clear understanding from his answer here, but I think it&#39;s a complex matter of designing the system so that it performs an ethics review and then actually uses it to select actions. This could be in a scripted scaffold around an agent, like AutoGPT, but this could also apply to more complex schemes, like an RL outer loop network running a foundation model. Shane notes the problems with using RL for alignment, including deceptive alignment.</p><p> This seems like a good starting point to me, obviously; I&#39;m delighted to see that someone whose opinion matters is thinking about this approach. I think this is not just an actual proposal, but a viable one. It doesn&#39;t solve <a href="https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem">The alignment stability problem</a> <span class="footnote-reference" role="doc-noteref" id="fnref6fqpmjhuk9"><sup><a href="#fn6fqpmjhuk9">[2]</a></sup></span> of making sure stays aligned once it&#39;s autonomous and self-modifying, but I think that&#39;s probably solvable, too, once we get some more thinking on it.</p><p> The rest of the interview is of interest as well; it&#39;s Shane&#39;s thoughts on the path to AGI, which I think is quite reasonable, well-expressed, and one plausible path; DeepMind&#39;s contributions to safety vs. alignment, and his predictions for the future. </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnau80xd00gxi"> <span class="footnote-back-link"><sup><strong><a href="#fnrefau80xd00gxi">^</a></strong></sup></span><div class="footnote-content"><p> When asked about the limitations of language models relative to humans, he focused on their lack of episodic memory. Adding this in useful form to an agent isn&#39;t trivial, but it <a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures#LMCAs_have_episodic_memory">seems to me</a> it doesn&#39;t require any breakthroughs relative to the vector databases and knowledge graph approaches already in use. This is consistent with but not strong evidence for Shane thinking that foundation model agents are the path to AGI.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6fqpmjhuk9"> <span class="footnote-back-link"><sup><strong><a href="#fnref6fqpmjhuk9">^</a></strong></sup></span><div class="footnote-content"><p> Edit: <a href="https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and">Value systematization: how values become coherent (and misaligned)</a> is another way to think about part of what I&#39;m calling the alignment stability problem.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2QLxNdxQpnesokk9H/shane-legg-interview-on-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2QLxNdxQpnesokk9H/shane-legg-interview-on-alignment<guid ispermalink="false"> 2QLxNdxQpnesokk9H</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Sat, 28 Oct 2023 19:28:52 GMT</pubDate> </item><item><title><![CDATA[AI Safety Hub Serbia Official Opening]]></title><description><![CDATA[Published on October 28, 2023 5:03 PM GMT<br/><br/><p> TLDR: We&#39;re thrilled to announce that we are now welcoming full-time tenants to our newly transformed office space for AI Safety researchers in search of an inspiring workspace. You can take a glimpse at the photos below to get a sneak peek. We&#39;re extending a warm invitation, with priority, to citizens of nations such as Russia and China, who can enjoy visa-free work privileges in Serbia while maintaining proximity to Europe. Our monthly office rent is an astonishingly affordable 150€, about half the standard cost in Belgrade. For those needing financial support, we have subsidies available. <a href="https://docs.google.com/forms/d/1LQ9cE1CGjD_WMMx5IYLLeHFeXNQJx12dsu7f4_FSF7w/edit"><u>Register interest here</u></a> or seek answers to any questions that stir your curiosity - <a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>our inbox is open</u></a> , and we eagerly await your inquiries. Looking ahead, we aspire to provide housing for these researchers. If you&#39;re a potential donor who shares our vision, <a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>please contact us</u></a> . Together, we can enhance the impact of AI Safety research. Your support could be the catalyst for a remarkable journey.</p><p> <strong>You may want to come if:</strong></p><ul><li> You are an AI Safety Researcher/EA researcher looking for a base of operations for a short-medium-long term</li><li> You are keen to be in Europe but not in the EU</li><li> You are looking for a vibrant but affordable city with plenty of things to do and Eastern European but Westernized culture. </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rhope1kzgbygtaznxrff" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/av2n0y1ddmpkjlaao3hq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/imzg363u9lykimplfwfr 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mbbliew6d1zaodrmuf8r 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kslavbyntsgwoz34idah 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uj6i2hktt4xsewysx0oj 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/edtwmohcbpwjikn4ekfi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/p8bdejrsrvov0xsijgne 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ayswko71jqaiyz9qt7rl 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ciytrddcpl8bqptiznd4 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wqvwonadsbsa5vbgelc9 6000w"></figure><p><strong>背景：</strong></p><p> EA Serbia and AI Safety Serbia groups are small but growing (>;30 people in EA Serbia, ~3 people looking to get into AIS research as a career, and ~3 to get into AIS policy). Due to Serbia&#39;s favorable Visa policy towards Russia and China, many foreigners already live here. With lower living costs than many other international hub cities, a vibrant scene, and a favourable time zone and climate, Belgrade has a growing foreign community.</p><p> As we have seen projects such as <a href="https://ceealar.org/"><u>CEEALAR</u></a> as important and impressive, we wish to replicate them in Serbia, where they can better serve people who may struggle to get UK visas. We also believe that having the capacity to quickly scale cheap housing for people coming from different countries is a good thing. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uxq4oabxyjpdmmhjtgo0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/e9grniodk6f8ejythv3f 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/f5k39rystyizea0vbmkq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ndoxhjm6d7k8ursg738n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvjqolxfdwbn76ocngve 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/riiq7rpasykyrj61dzvb 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iez7ohyjt7f9zfleqbqo 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mwdxvwyobojph6l6kaps 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/lobyg54ph0nbiwknzkzo 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x4c9uvqjczywudqqdxb6 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/cfkm1rzfo445cqzpwier 4000w"></figure><p> We also believe that we should start small, prototype, and then move larger. We have an NGO-friendly office space that has good vibes and costs only ~550 Euros per month for office space that has three rooms and can fit 8-15 people (depending on how snug they decide to be) with a coffee shop downstairs where another 20 people can spend their time co-working, as the office and the downstairs coffee shop are under the same ownership. This is certainly less luxurious than many other EA/AI co-working places (more photos below, deep-work room not on photos), but we have a high degree of customizability allowed to us, which we can use to make an even better office space. If we grow enough, we can also move to a bigger venue, as our needs grow. Certainly, if we knew that more use could be found in an office in Belgrade, getting something somewhat further from the city center, which includes living and office spaces, would be better, but we do not wish to explore that until we have proof of concept and need. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/nod9jw42tc06wvzye3eb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/i3aqrftpdiglasru5zoo 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/fadrulbzdm2g8m9za7wy 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/aab7vljnrhy6k2f4tkcy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kqgfhl8xnjy59mf5jt9g 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wbvwmvbsvamsjniuebfz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xac4qmxukisbahsptujh 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sh2v5qozhildz1y72auz 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/t4x3chswtv5cjyzg3cky 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/pu5p7jpbijtdvcjovi6b 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wxqj8fp2qkn5pfoitbwo 4000w"></figure><p> <strong>Operations details (aka how it works):</strong></p><p> The office is currently rented until the end of December 2023, so we can keep the favorable price instead of finding a different place. The office space has some desks and chairs, but we are looking to acquire full funding and have people voice their needs before acquiring more furniture. The office is usually open during working hours of the coffee shop (10 AM to Midnight, except on weekends when it is 4 PM to Midnight) as they share an entrance, but exceptionally, we can accommodate special requests if someone works better at strange working hours.</p><p> Office space is given to those who are working on projects related to AI Safety as a priority, but EA/Rationality research is also welcome whenever we have spare capacity (which is currently the case).</p><p> A Serbian visa is not required for many and is relatively easy to get for most others. If you need a visa to come to Serbia, reach out and we will see how we can help.</p><p> We have a reliable real estate agent who can get good deals on housing in Belgrade for those who need housing assistance until we get funded and rent a co-living space as well.</p><p> For those looking to eat consistently through us, we can arrange affordable cooked meals delivered to the office or your housing (at your expense) - vegetarian or not. If we have enough interest, we can also get the chef to prepare vegan food. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvfxrlxrysu9ftzrfb0h" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rt5rayrriqgexvauxtwe 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/asuqgyybp6w9qnzxc0ao 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hpjcffwxmal5cdylxdyv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ioapwimka9271dtpduzm 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/gdbwhdusggnvqdexuuqz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zsuzpixvadboaqkh2x2h 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xvrdcxmhfiy0bpnspyft 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/d61j52v27pzwown3r3ne 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/v2bzvtf6czgkjn7cw56j 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wa96ftjsgtifyygfdxlo 4000w"></figure><p> <strong>Would you like to contribute?</strong></p><p> <a href="https://www.linkedin.com/in/nesicdusan/"><u>Dušan D. Nešić</u></a> is currently managing the project with a few more people on the operational level. Our current bottleneck is funding - we have acquired an earn-to-giver interested in paying half of our bare-bones budget but are looking for a second funder. Having funding means we can settle in the space for longer and provide more stipends to researchers who need them.</p><p> As we grow, we would like to engage more curious volunteers for the day-to-day project running - let us know if <a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>you would be interested</u></a> . </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/g5foy0cn8wy3huhlxoq5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x0iifew90y7pq2x7t5yk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/me0hy8dxy30ptbq6zzrf 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sc5wk7e8kzottq60xcgp 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hhqdull2ezu4ekoqg8lj 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iwtsrgkzckr5qwjlxh6i 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rklbxpbhn4nggpqnk0nr 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ev6nhqdsaelwzcwdc2zj 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/bt2gs4wszkqb7wfoaish 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/j2tiksjpthhru0mxgrvi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/otdmoewnmv4prwdd9fkx 4000w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening<guid ispermalink="false"> jcCyii8NcZLZ2M223</guid><dc:creator><![CDATA[DusanDNesic]]></dc:creator><pubDate> Sat, 28 Oct 2023 17:03:34 GMT</pubDate> </item><item><title><![CDATA[
Managing AI Risks in an Era of Rapid Progress
]]></title><description><![CDATA[Published on October 28, 2023 3:48 PM GMT<br/><br/><h1>在快速进步的时代管理人工智能风险</h1><h2>作者</h2><p>约书亚·本吉奥</p><p>杰弗里·辛顿</p><p>姚安德</p><p>黎明之歌</p><p>彼得·阿贝尔</p><p>尤瓦尔·诺亚·赫拉利</p><p>张亚勤</p><p>蓝雪</p><p>谢·沙莱夫-施瓦茨</p><p>吉莉安·哈德菲尔德</p><p>杰夫·克鲁恩</p><p>泰甘·马哈拉吉</p><p>弗兰克·哈特</p><p>阿蒂利姆·古内斯·巴丁</p><p>希拉·麦克莱思</p><p>高琪琪</p><p>阿什温·阿查里亚</p><p>大卫·克鲁格</p><p>安卡·德拉甘</p><p>菲利普·托尔</p><p>斯图尔特·拉塞尔</p><p>丹尼尔·卡尼曼</p><p>简·布劳纳</p><p>索伦·明德曼</p><h3>arXiv</h3><p>即将推出。</p><p><a href="https://managing-ai-risks.com/managing_ai_risks.pdf">纸质 PDF 副本</a><a href="https://managing-ai-risks.com/policy_supplement.pdf">保单补充</a></p><blockquote><p><strong>摘要：</strong>在这篇简短的共识论文中，我们概述了即将到来的先进人工智能系统的风险。我们研究大规模的社会危害和恶意使用，以及人类对自主人工智能系统不可逆转的控制丧失。鉴于人工智能的快速和持续进步，我们提出了人工智能研发和治理的紧迫优先事项。</p></blockquote><p> 2019 年，GPT-2 无法可靠地数到十。仅四年后，深度学习系统就可以编写软件，根据需要生成逼真的场景，就智力主题提供建议，并结合语言和图像处理来引导机器人。当人工智能开发人员扩展这些系统时，不可预见的能力和行为会自发出现，无需显式编程。人工智能的进步非常迅速，而且对许多人来说是令人惊讶的。</p><p>进展的速度可能会再次让我们感到惊讶。当前的深度学习系统仍然缺乏重要的能力，我们不知道开发它们需要多长时间。然而，各公司都在竞相创建在大多数认知工作中匹配或超过人类能力的通用人工智能系统。他们正在快速部署更多资源并开发新技术来增强人工智能能力。人工智能的进步也带来了更快的进步：人工智能助手越来越多地用于自动化编程[4]和数据收集[5,6]，以进一步改进人工智能系统[7]。</p><p>人工智能的进步并没有在人类水平上放缓或停滞的根本原因。事实上，人工智能已经在蛋白质折叠或策略游戏等狭窄领域超越了人类的能力[8-10]。与人类相比，人工智能系统可以更快地行动，吸收更多的知识，并以更高的带宽进行通信。此外，它们可以扩展以使用巨大的计算资源，并且可以进行数百万次复制。</p><p>改进的速度已经是惊人的，科技公司拥有所需的现金储备，可以很快将最新的培训规模扩大到 100 到 1000 倍 [11]。结合人工智能研发的持续增长和自动化，我们必须认真对待通用人工智能系统在这十年或未来十年内在许多关键领域超越人类能力的可能性。</p><p> What happens then?如果管理得当并公平分配，先进的人工智能系统可以帮助人类治愈疾病、提高生活水平并保护我们的生态系统。人工智能提供的机会是巨大的。但随着先进的人工智能功能的出现，我们还无法很好地应对大规模的风险。人类正在投入大量资源来使人工智能系统变得更强大，但在安全性和减轻危害方面却投入较少。为了让人工智能成为福音，我们必须重新定位；仅仅推动人工智能能力是不够的。</p><p>我们的调整已经落后于计划。我们必须预见到持续危害和新风险的扩大，并在最大风险<em>发生之前</em>做好准备。人们花了几十年的时间才认识和应对气候变化；对于人工智能来说，几十年可能太长了。</p><h2>社会规模风险</h2><p>人工智能系统可能会在越来越多的任务中迅速超越人类。如果此类系统没有经过精心设计和部署，它们会带来一系列社会规模的风险。它们有可能加剧社会不公正，侵蚀社会稳定，并削弱我们对社会基础现实的共同理解。它们还可能促成大规模犯罪或恐怖活动。特别是在少数强大的参与者手中，人工智能可能会巩固或加剧全球不平等，或促进自动化战争、定制的大规模操纵和普遍的监视[12,13]。</p><p>随着公司正在开发<em>自主人工智能</em>：可以在世界上规划、行动和追求目标的系统，其中许多风险可能很快就会被放大，并产生新的风险。虽然当前人工智能系统的自主权有限，但改变这一现状的工作正在进行中[14]。例如，非自主 GPT-4 模型很快就可以浏览网页 [15]、设计和执行化学实验 [16] 以及利用软件工具 [17]，包括其他人工智能模型 [18]。</p><p>如果我们构建高度先进的自主人工智能，我们就有可能创建追求不良目标的系统。恶意行为者可能故意嵌入有害目标。此外，目前没有人知道如何可靠地将人工智能行为与复杂的价值观结合起来。即使是善意的开发人员也可能会无意中构建出追求意想不到目标的人工智能系统——特别是如果他们为了赢得人工智能竞赛而忽视了昂贵的安全测试和人工监督。</p><p>一旦自主人工智能系统追求恶意行为者或意外嵌入的不良目标，我们可能无法控制它们。软件控制是一个古老且尚未解决的问题：计算机蠕虫长期以来一直能够扩散并逃避检测[19]。然而，人工智能正在黑客、社交操纵、欺骗和战略规划等关键领域取得进展[14,20]。先进的自主人工智能系统将带来前所未有的控制挑战。</p><p>为了实现不良目标，未来的自主人工智能系统可能会使用不良策略（向人类学习或独立开发）作为达到目的的手段[21-24]。人工智能系统可以赢得人类信任，获取财政资源，影响关键决策者，并与人类参与者和其他人工智能系统形成联盟。为了避免人为干预[24]，他们可以像计算机蠕虫一样在全球服务器网络上复制算法。人工智能助手已经在全球范围内共同编写了大量计算机代码 [25]；未来的人工智能系统可以插入并利用安全漏洞来控制我们的通信、媒体、银行、供应链、军队和政府背后的计算机系统。在公开冲突中，人工智能系统可能会使用自主武器或生物武器进行威胁或使用。获得此类技术的人工智能只会延续现有的自动化军事活动、生物研究和人工智能开发本身的趋势。如果人工智能系统以足够的技能执行此类策略，人类将很难干预。</p><p>最后，如果人工智能系统可以自由地移交影响力，那么它可能不需要策划影响力。随着自主人工智能系统变得比人类工人更快、更具成本效益，出现了一个困境。公司、政府和军队可能被迫广泛部署人工智能系统，并减少对人工智能决策的昂贵的人工验证，否则就有被竞争的风险[26,27]。因此，自主人工智能系统可以越来越多地承担关键的社会角色。</p><p>如果没有足够的谨慎，我们可能会不可逆转地失去对自主人工智能系统的控制，从而导致人类干预无效。大规模网络犯罪、社会操纵和其他突出危害可能会迅速升级。这种不受控制的人工智能进步可能最终导致大规模生命和生物圈的丧失，以及人类的边缘化甚至灭绝。</p><p>错误信息和算法歧视等危害如今已经很明显[28]；其他危害也有出现的迹象[20]。解决持续危害和预测新出现的风险至关重要。这<em>不是</em>一个非此即彼的问题。当前和新出现的风险通常具有相似的机制、模式和解决方案[29]；对治理框架和人工智能安全的投资将在多个方面取得成果[30]。</p><h2>前进的道路</h2><p>如果今天开发出先进的自主人工智能系统，我们将不知道如何确保它们的安全，也不知道如何正确测试它们的安全性。即使我们这样做了，政府也将缺乏防止滥用和维护安全做法的机构。然而，这并不意味着没有可行的前进道路。为了确保取得积极成果，我们可以而且必须在人工智能安全和伦理方面寻求突破，并及时建立有效的政府监管。</p><h3>调整技术研发方向</h3><p>我们需要研究突破来解决当今创建具有安全和道德目标的人工智能的一些技术挑战。其中一些挑战不太可能通过简单地提高人工智能系统的能力来解决[22,31–35]。这些包括：</p><ul><li>监督和诚实：能力更强的人工智能系统能够更好地利用监督和测试中的弱点[32,36,37]——例如，通过产生虚假但令人信服的输出[35,38]。</li><li>鲁棒性：人工智能系统在新情况下（在分布转移或对抗性输入下）表现不可预测[39-41]。</li><li>可解释性：人工智能决策是不透明的。到目前为止，我们只能通过反复试验来测试大型模型。我们需要学会理解它们的内部运作方式[42]。</li><li>风险评估：前沿人工智能系统开发出不可预见的能力，这些能力只有在训练期间甚至部署后才发现[43]。需要更好的评估来及早发现危险能力[44,45]。</li><li>应对新出现的挑战：能力更强的未来人工智能系统可能会表现出我们迄今为止仅在理论模型中看到的故障模式。例如，人工智能系统可能会学习假装服从或利用我们安全目标和关闭机制中的弱点来推进特定目标[24,41]。</li></ul><p>考虑到风险，我们呼吁主要科技公司和公共资助者将至少三分之一的人工智能研发预算用于确保安全和合乎道德的使用，这与他们对人工智能能力的资助相当。着眼于强大的未来系统来解决这些问题 [34] 必须成为我们领域的核心。</p><h3>紧急治理措施</h3><p>我们迫切需要国家机构和国际治理来执行标准，以防止鲁莽和滥用。从制药到金融系统和核能的许多技术领域都表明，社会需要并有效地利用治理来降低风险。然而，目前人工智能还没有类似的治理框架。如果没有它们，公司和国家可能会通过将人工智能能力推向新的高度，同时在安全方面偷工减料，或者将关键的社会角色委托给几乎没有人类监督的人工智能系统来寻求竞争优势[26]。就像制造商将废物排入河流以降低成本一样，他们可能会试图获得人工智能发展的回报，同时让社会来应对后果。</p><p>为了跟上快速进展并避免僵化的法律，国家机构需要强大的技术专长和迅速采取行动的权力。为了解决国际种族动态问题，他们需要有能力促进国际协议和伙伴关系[46,47]。为了保护低风险的使用和学术研究，他们应该避免对小型和可预测的人工智能模型设置不当的官僚障碍。最紧迫的审查应该是前沿的人工智能系统：少数最强大的人工智能系统——在价值数十亿美元的超级计算机上进行训练——将具有最危险和不可预测的能力[48,49]。</p><p>为了实现有效监管，政府迫切需要全面了解人工智能的发展。监管机构应要求模型注册、举报人保护、事件报告以及模型开发和超级计算机使用的监控[48,50–55]。监管机构还需要在部署之前访问先进的人工智能系统，以评估其危险功能，例如自主自我复制、闯入计算机系统或使大流行病病原体广泛传播[44,56,57]。</p><p>对于具有危险能力的人工智能系统，我们需要与其风险程度相匹配的治理机制[48,52,58]组合。监管机构应根据模型功能制定国家和国际安全标准。他们还应该让前沿人工智能开发者和所有者对其模型造成的可合理预见和预防的损害承担法律责任。这些措施可以防止伤害并创造急需的安全投资动力。对于能力超群的未来人工智能系统，例如可以规避人类控制的模型，需要采取进一步的措施。政府必须准备好许可其开发，暂停开发以应对令人担忧的能力，强制执行访问控制，并要求对国家级黑客采取强有力的信息安全措施，直到准备好足够的保护措施。</p><p>为了缩短法规出台的时间，主要人工智能公司应立即做出“如果-那么”承诺：如果在其人工智能系统中发现特定的红线功能，他们将采取具体的安全措施。这些承诺应详细并经过独立审查。</p><p>人工智能可能是塑造本世纪的技术。虽然人工智能能力正在迅速进步，但安全和治理方面的进展却滞后。为了引导人工智能走向积极的结果并远离灾难，我们需要重新定位。如果我们有智慧走下去，就有一条负责任的道路。</p><h2>引文</h2><p>请将本作品引用为</p><p>请引用我们即将发布的 arXiv 预印本。</p><p> @article{bengio2023managing，title={在快速进步的时代管理人工智能风险}，作者={Bengio、Yoshua 和 Hinton、Geoffrey 和 Yao、Andrew 和 Song、Dawn 和 Abbeel、Pieter 和 Harari、Yuval Noah 和 Zhang、Ya -Qin 和薛、Lan 和 Shalev-Shwartz、Shai 和 Hadfield、Gillian 和 Clune、Jeff 和 Maharaj、Tegan 和 Hutter、Frank 和 Baydin、Atılım Güneş 和 McIlraith、Sheila 和 Gau、Qiqi 和 Acharya、Ashwin 和 Krueger、David 和Dragan、Anca 和 Torr、Philip 和 Russell、Stuart 和 Kahnemann、Daniel 和 Brauner、Jan 和 Mindermann、Sören}，journal={arXiv 预印本 arXiv:NUMBER_FORTHCOMING}，year={2023} }</p><h2>参考</h2><ol><li>大型语言模型的新兴能力<a href="https://openreview.net/pdf?id=yzkSU5zdwD">[链接]</a><br> Wei, J.、Tay, Y.、Bommasani, R.、Raffel, C.、Zoph, B.、Borgeaud, S. 等，2022 年。机器学习研究汇刊。</li><li>关于<a href="https://www.deepmind.com/about">[链接]</a><br> DeepMind，2023。</li><li>关于<a href="https://openai.com/about">[链接]</a><br>开放人工智能，2023。</li><li> ML 增强的代码完成提高了开发人员的工作效率<a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">[HTML]</a><br> Tabachnyk, M.，2022。谷歌研究。</li><li> GPT-4 技术报告<a href="http://arxiv.org/pdf/2303.08774.pdf">[PDF]</a><br> OpenAI，，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>宪法人工智能：人工智能反馈的无害性<a href="http://arxiv.org/pdf/2212.08073.pdf">[PDF]</a><br> Bai, Y.、Kadavath, S.、Kundu, S.、Askel, A.、Kernion, J.、Jones, A. 等，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>人工智能改进人工智能的例子<a href="https://ai-improving-ai.safe.ai/">[链接]</a><br> Woodside, T. 和安全，CfA，2023。</li><li>使用 AlphaFold 进行高精度蛋白质结构预测<br>Jumper, J.、Evans, R.、Pritzel, A.、Green, T.、Figurnov, M.、Ronneberger, O. 等，2021 年。《自然》，第 583--589 页。</li><li>多人扑克的超人人工智能<br>Brown, N. 和 Sandholm, T.，2019 年。《科学》，第 885--890 页。</li><li>深蓝<br>Campbell, M.、Hoane, A. 和 Hsu, F.，2002 年。人工智能，第 57--83 页。</li><li> Alphabet 年度报告，第 33 页<a href="https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf">[PDF]</a><br>字母表，2022 年。</li><li>灾难性人工智能风险概述<a href="http://arxiv.org/pdf/2306.12001.pdf">[PDF]</a><br> Hendrycks, D.、Mazeika, M. 和 Woodside, T.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>语言模型带来的风险分类<br>Weidinger, L.、Uesato, J.、Rauh, M.、Griffin, C.、Huang, P.、Mellor, J. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第. 214--229。</li><li>基于大型语言模型的自治代理综述<a href="http://arxiv.org/pdf/2308.11432.pdf">[PDF]</a><br> Wang, L.、Ma, C.、Feng, X.、Zhang, Z.、Yang, H.、Zhang, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> ChatGPT 插件<a href="https://openai.com/blog/chatgpt-plugins">[链接]</a><br>开放人工智能，2023。</li><li> ChemCrow：使用化学工具增强大型语言模型<a href="http://arxiv.org/pdf/2304.05376.pdf">[PDF]</a><br> Bran, A.、Cox, S.、White, A. 和 Schwaller, P.，2023。arXiv [physicals.chem-ph]。</li><li>增强语言模型：调查<a href="http://arxiv.org/pdf/2302.07842.pdf">[PDF]</a><br> Mialon, G.、Dessì, R.、Lomeli, M.、Nalmpantis, C.、Pasunuru, R.、Raileanu, R. 等，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li> HuggingGPT：使用 ChatGPT 及其 Hugging Face 中的朋友解决 AI 任务<a href="http://arxiv.org/pdf/2303.17580.pdf">[PDF]</a><br> Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y. 等，2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>计算科学：互联网蠕虫<br>Denning, P.，1989。《美国科学家》，第 126--128 页。</li><li>人工智能欺骗：示例、风险和潜在解决方案调查<a href="http://arxiv.org/pdf/2308.14752.pdf">[PDF]</a><br> Park, P.、Goldstein, S.、O&#39;Gara, A.、Chen, M. 和 Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>最优政策往往寻求权力<a href="http://arxiv.org/pdf/1912.01683.pdf">[PDF]</a><br> Turner, A.、Smith, L.、Shah, R. 和 Critch, A.，2019 年。第三十五届神经信息处理系统会议。</li><li>通过模型编写的评估发现语言模型行为<a href="http://arxiv.org/pdf/2212.09251.pdf">[PDF]</a><br> Perez, E.、Ringer, S.、Lukošiūtė, K.、Nguyen, K.、Chen, E. 和 Heiner, S.，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>回报是否证明手段合理？在马基雅维利基准中衡量奖励和道德行为之间的权衡<br>Pan, A.、Chan, J.、Zou, A.、Li, N.、Basart, S. 和 Woodside, T.，2023 年。国际机器学习会议。</li><li>关闭开关游戏<br>Hadfield-Menell, D.、Dragan, A.、Abbeel, P. 和 Russell, S.，2017 年。第二十六届国际人工智能联合会议论文集，第 220--227 页。</li><li> GitHub Copilot <a href="https://github.blog/2023-02-14-github-copilot-for-business-is-now-available/">[链接]</a><br>多姆克，T.，2023。</li><li>自然选择有利于人工智能而不是人类<a href="http://arxiv.org/pdf/2303.16200.pdf">[PDF]</a><br> Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>日益代理的算法系统的危害<br>Chan, A.、Salganik, R.、Markelius, A.、Pang, C.、Rajkumar, N. 和 Krasheninnikov, D.，2023 年。2023 年 ACM 公平、问责和透明度会议记录，第 651 页- -666。计算机器协会。</li><li>论基金会模型的机遇和风险<a href="http://arxiv.org/pdf/2108.07258.pdf">[PDF]</a><br> Bommasani, R.、Hudson, D.、Adeli, E.、Altman, R.、Arora, S. 和 von Arx, S.，2021。arXiv [cs.LG]。</li><li>人工智能带来世界末日风险——但这并不意味着我们不应该谈论当前的危害<a href="https://time.com/6303127/ai-future-danger-present-harms/">[链接]</a><br> Brauner, J. 和 Chan, A.，2023 年。时间。</li><li>针对当前和未来危害的现有政策提案<a href="https://assets-global.website-files.com/63fe96aeda6bea77ac7d3000/647d5368c2368cc32b359f88/_Policy/%20Agreement/%20Statement.pdf">[PDF]</a><br>安全，CfA，2023 年。</li><li>逆缩放：越大并不越好<a href="http://arxiv.org/pdf/2306.09479.pdf">[PDF]</a><br> McKenzie, I.、Lyzhov, A.、Pieler, M.、Parrish, A.、Mueller, A. 和 Prabhu, A.，2023 年。机器学习研究汇刊。</li><li>奖励错误指定的影响：映射和缓解不一致的模型<a href="https://openreview.net/forum?id=JYtwGwIL7ye">[链接]</a><br> Pan, A.、Bhatia, K. 和 Steinhardt, J.，2022。学习表征国际会议。</li><li>简单的综合数据减少了大型语言模型中的阿谀奉承<a href="http://arxiv.org/pdf/2308.03958.pdf">[PDF]</a><br> Wei, J.、Huang, D.、Lu, Y.、Zhou, D. 和 Le, Q., 2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>机器学习安全中未解决的问题<a href="http://arxiv.org/pdf/2109.13916.pdf">[PDF]</a><br> Hendrycks, D.、Carlini, N.、Schulman, J. 和 Steinhardt, J.，2021。arXiv [cs.LG]。</li><li>来自人类反馈的强化学习的开放问题和基本限制<a href="http://arxiv.org/pdf/2307.15217.pdf">[PDF]</a><br> Casper, S.、Davies, X.、Shi, C.、Gilbert, T.、Scheurer, J. 和 Rando, J.，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>人工智能失调的后果<br>Zhuang, S. 和 Hadfield-Menell, D.，2020 年。神经信息处理系统进展，第 33 卷，第 15763--15773 页。</li><li>奖励模型过度优化的缩放法则<br>高 L.、舒尔曼 J. 和希尔顿 J.，2023 年。第 40 届国际机器学习会议论文集，第 10835--10866 页。 PMLR。</li><li>从人类偏好中学习<a href="https://openai.com/research/learning-from-human-preferences">[链接]</a><br>阿莫代伊，D.，克里斯蒂安诺，P. 和雷，A.，2017。</li><li>深度强化学习中的目标错误概括<a href="https://openreview.net/forum?id=q--OykSR2FY">[链接]</a><br> Langosco di Langosco, A. 和 Chan, A.，2022 年。学习表征国际会议。</li><li>目标误区：为什么正确的规范不足以实现正确的目标<a href="http://arxiv.org/pdf/2210.01790.pdf">[PDF]</a><br> Shah, R.、Varma, V.、Kumar, R.、Phuong, M.、Krakovna, V.、Uesato, J. 等，2022。arXiv [cs.LG]。</li><li>迈向透明人工智能：解释深度神经网络内部结构的调查<br>Räuker, T.、Ho, A.、Casper, S. 和 Hadfield-Menell, D.，2023。2023 年 IEEE 安全可信机器学习会议 (SaTML)，第 464--483 页。</li><li>思路链提示引发大型语言模型的推理<br>Wei, J.、Wang, X.、Schuurmans, D.、Bosma, M.、Ichter, B.、Xia, F. 等，2022。神经信息处理系统进展，第 35 卷，第 24824 页-- 24837。</li><li>极端风险模型评估<a href="http://arxiv.org/pdf/2305.15324.pdf">[PDF]</a><br> Shevlane, T.、Farquhar, S.、Garfinkel, B.、Phuong, M.、Whittlestone, J.、Leung, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> AGI 公司的风险评估：对其他安全关键行业流行的风险评估技术的回顾<a href="http://arxiv.org/pdf/2307.08823.pdf">[PDF]</a><br> Koessler, L. 和 Schuet, J.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>深度学习角度的对齐问题<a href="http://arxiv.org/pdf/2209.00626.pdf">[PDF]</a><br> Ngo, R.、Chan, L. 和 Mindermann, S.，2022。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>国际先进人工智能机构<br>Ho, L.、Barnhart, J.、Trager, R.、Bengio, Y.、Brundage, M.、Carnegie, A. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。 <a href="https://doi.org/10.48550/arXiv.2307.04699">DOI：10.48550/arXiv.2307.04699</a></li><li>民用人工智能的国际治理：司法管辖区认证方法<a href="https://cdn.governance.ai/International_Governance_of_Civilian_AI_OMS.pdf">[PDF]</a><br> Trager, R.、Harack, B.、Reuel, A.、Carnegie, A.、Heim, L.、Ho, L. 等，2023 年。</li><li>前沿人工智能监管：管理公共安全的新风险<a href="http://arxiv.org/pdf/2307.03718.pdf">[PDF]</a><br> Anderljung, M.、Barnhart, J.、Korinek, A.、Leung, J.、O&#39;Keefe, C.、Whittlestone, J. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>大型生成模型的可预测性和惊喜<br>Ganguli, D.、Hernandez, D.、Lovitt, L.、Askel, A.、Bai, Y.、Chen, A. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第1747年--1764年。计算机器协会。</li><li>是时候为大型人工智能模型创建国家注册库了<a href="https://carnegieendowment.org/2023/07/12/it-s-time-to-create-national-registry-for-large-ai-models-pub-90180">[链接]</a><br> Hadfield, G.、Cuéllar, M. 和 O&#39;Reilly, T.，2023。卡内基国际捐赠基金会。</li><li>用于模型报告的模型卡<br>Mitchell, M.、Wu, S.、Zaldivar, A.、Barnes, P.、Vasserman, L.、Hutchinson, B. 等，2019 年。FAT* &#39;19：公平、问责制和其他问题会议记录透明度，第 220--229 页。</li><li>通用人工智能带来严重风险，不应被排除在欧盟人工智能法案之外政策简介<a href="https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act">[链接]</a><br> 2023.AI Now 研究所。</li><li>人工智能事件数据库<a href="https://incidentdatabase.ai/">[链接]</a><br>数据库，AII，2023。</li><li>技术举报的承诺和危险<a href="https://papers.ssrn.com/abstract=4377064">[链接]</a><br> Bloch-Wehba, H.，2023。西北大学法律评论，即将出版。</li><li>为英国提出一个基础模型信息共享制度<a href="https://www.governance.ai/post/proposing-a-foundation-model-information-sharing-regime-for-the-uk">[链接]</a><br> Mulani, N. 和 Whittlestone, J.，2023。人工智能治理中心。</li><li>审核大型语言模型：三层方法<br>Mökander, J.、Schuett, J.、Kirk, H. 和 Floridi, L.，2023 年。人工智能与伦理。 <a href="https://doi.org/10.1007/s43681-023-00289-2">DOI：10.1007/s43681-023-00289-2</a></li><li>大型语言模型能否使两用生物技术的普及变得民主化？ <a href="http://arxiv.org/pdf/2306.03809.pdf">[PDF]</a><br> Soice, E.、Rocha, R.、Cordova, K.、Spectre, M. 和 Esvelt, K.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>迈向通用人工智能安全和治理的最佳实践：专家意见调查<a href="http://arxiv.org/pdf/2305.07153.pdf">[PDF]</a><br> Schuett, J.、Dreksler, N.、Anderljung, M.、Mcaffary, D.、Heim, L.、Bluemke, E. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>监管市场：人工智能治理的未来<a href="http://arxiv.org/pdf/2304.04914.pdf">[PDF]</a><br> Hadfield, G. 和 Clark, J.，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress<guid ispermalink="false"> a3RjXa2dryoH6Xgij</guid><dc:creator><![CDATA[Algon]]></dc:creator><pubDate> Sat, 28 Oct 2023 15:48:25 GMT</pubDate></item></channel></rss>