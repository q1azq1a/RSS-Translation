<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 28 日星期二 10:13:18 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[How can I use AI without increasing AI-risk?]]></title><description><![CDATA[Published on November 28, 2023 10:05 AM GMT<br/><br/><p>我很少使用人工智能工具，主要原因是我对它们产生了某种反感，因为将它们与它们造成的危害和带来的风险联系在一起。</p><p>另一方面，似乎越来越多的人不采用人工智能工具，其生产力将大大降低并被抛在后面。</p><p>而且，如果担心风险的人限制自己，而不担心风险的人也不限制自己，就会形成<a href="https://www.youtube.com/watch?v=SjNRtrZjkfE&amp;pp=ygUdcGVyc29uYWwgcmVzcG9uc2liaWxpdHkgdm90ZXg%3D">个人责任漩涡</a>，使力量平衡偏离安全。</p><p>但是，也许可以充分发挥这两个词的作用——使用人工智能工具，但要以负责任的方式使用，不会增加由此带来的危害和风险。</p><p>如何做到这一点？</p><br/><br/> <a href="https://www.lesswrong.com/posts/okBynjbL2svDFktSZ/how-can-i-use-ai-without-increasing-ai-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okBynjbL2svDFktSZ/how-can-i-use-ai-without-increasing-ai-risk<guid ispermalink="false"> okBynjbL2svDFktSZ</guid><dc:creator><![CDATA[Yoav Ravid]]></dc:creator><pubDate> Tue, 28 Nov 2023 10:05:44 GMT</pubDate> </item><item><title><![CDATA[A Reading From The Book Of Sequences]]></title><description><![CDATA[Published on November 28, 2023 6:45 AM GMT<br/><br/><p><strong>摘要：</strong>演讲者将朗读社区认为重要的文本摘录，并将引文与他们的生活或故事中的经历交织在一起。</p><p><strong>标签</strong>：大型、投资、也许是实验性的？</p><p><strong>目的：</strong>创建围绕阅读主题的常识。每个人都知道，每个人都知道其他人都知道。定期重复的少量课程也可以确保新人学到社区希望每个人都知道的东西。</p><p><strong>材料</strong>：分发给读者的文本副本。如果你经常这样做，那么拥有一堆包含阅读内容的书可能会很好。</p><p><strong>公告文本</strong>：“我们将阅读《理性：从人工智能到僵尸》，这篇文章吸引了我们许多人进入理性社区。今天将朗读_____，由演讲者______朗读。一般形式将是关于阅读主题的简短演讲和集体演唱的歌曲，然后是聚餐午餐。”</p><p><strong>描述</strong>：等待观众入座，如有必要，介绍您的演讲者，然后让他们发言。这篇文章的编写方式就好像只有一位发言人一样，但如果可以的话，我强烈建议多位发言人。</p><p>详细来说：</p><ol><li>编写程序<ol><li>选择读数。</li><li>为每次阅读写下演讲笔记，为公告和歌曲留出空间。有一个清晰的提示来连接它们：“......现在我们将翻到第十二页并唱这首歌。”</li><li>社区公告</li><li>歌曲。</li></ol></li><li>寻找演讲者和领唱<ol><li>在聚会前几天到一周，在社区中四处询问。直接询问特定的人会有所帮助，但您也可以将公告与“我们希望有额外的发言人来处理这部分内容”配对。如果您有兴趣提供帮助，请联系我们！”</li><li>对于没有太多技巧或练习的演讲者，请确保他们提前拥有自己所说内容的副本。</li><li>如果时间允许，让每个人至少排练自己的部分一到两次。人们通常第二次听起来会更流畅。</li></ol></li><li>遵循程序<ol><li>您是宣布何时开始的人。</li><li>一个关键角色是跟踪我们在计划中的位置，并呼吁采取下一步行动。</li><li>简短的连接性演讲，甚至简单到“非常感谢约翰·史密斯最后的阅读”。我很喜欢它。稍后，我们将阅读简·戴维斯的下一篇文章，”可以给人们足够的时间起身或坐下。这也有助于将每件作品放在上下文中。</li><li>你也是宣布我们结束的人，是时候开始聚餐了。</li></ol></li></ol><p>这还不够详细，无法运行它（请参阅“Not In A Box 警告”），但这是我用来运行它的清单。</p><p>什么是社区公告？他们有机会提及社区内任何应为常识的更新。出生、死亡和婚姻几乎总是包括在内，但也包括显着的伤害或不幸以及显着的成功和胜利。了解谁生病了意味着知道谁可以帮忙做庭院工作或做家常饭菜。</p><p><strong>变化：</strong>一种非常简单、非常明显的变化是跳过歌曲。紧密相连、直接阅读的一篇或三篇阅读，一个人无需太多准备就可以完成。添加音乐需要一套额外的技能，而且效果很差。一个糟糕的演讲者听起来很无聊而且烦人，而一个平庸的演讲者则只是还可以。与此同时，一个糟糕的音乐家可能会让你的与会者用手捂住耳朵。所以跳过音乐。</p><p>第二种变化是只阅读，没有旁白或演讲将其与日常生活联系起来。这也让事情变得更容易！我建议不要要求人们提前阅读阅读材料，这样你就可以直接跳到讨论；看起来它应该有效，但根据我的经验，有些人不会阅读，然后你必须向他们解释，或者让主题转向每个人都知道的内容。如果你的小组进行阅读，你可能会提前阅读，尽管我喜欢这里的常识。</p><p>当然，第三种变化是不吃聚餐。我什至没有提供任何细节，但“How To Potluck”（从聚会的角度来看，如果不是从 LessWrong 的角度来看）也许值得单独写一篇文章。简而言之，每个人都试图为更多的人带来足够的筹码，而不是每个人都带筹码，然后当少数人不能带东西时，仍然有足够的筹码可供使用。</p><p>你当然可以在没有音乐、没有阅读、没有食物的情况下做到这一点，尽管那时我认为你已经创建了一个纯粹的社交聚会。然而，跳过歌曲和聚餐会给你一个阅读小组，这是一个可靠的聚会。</p><p><strong>注释</strong>：正如所写，这是一篇具有不同核心文本的讲道。我最熟悉小型新教教堂（主要是社区教堂和公理会教堂），并且实际上是这种形式的忠实粉丝，但如果您参加过宗教布道的众多其他变体之一，您可能会看到我的弧线正在使用。我发现参加其他信仰的会议很有趣，如果你想看看我在说什么的话，大多数人都对陌生人出现参加服务感到很舒服；如果您只去过一座教堂，那么您可能认为有些事情是普遍的，但其实并非如此。</p><p>我将其标记为“也许是实验性的？”因为这个设置的两个基础都经过了很好的测试。我们有很多关于亚伯拉罕宗教聚会的历史。我们在世俗至日的理性主义社区中也有一个传统，这是我所见过的最受教会服务影响的事物，但它不是教会服务。只是，就像，将两者结合起来应该没问题。这实际上只是一个阅读和讨论小组，这些都很常见。我认为这很好。不过，呃，请注意，我认为没有人尝试过如此公开和随意地将它们结合起来。</p><p>对于每周演绎世俗至日歌曲经典会对人们的年度至日经历产生什么影响，我有不同的想法。一方面，你也在创造如何唱歌的常识。另一方面，重复可能会让他们感觉不那么特别。</p><p>另外，如果您执行此操作，请检查您的源文本是否有错误、复制危机是否引起或以其他方式创建。这里的搞笑失败模式是理性主义者社区陷入重复旧的不正确文本的困境，因为它是传统的，而“搞笑”我的意思是“非常讽刺”。</p><p> <strong>Not In A Box 警告：</strong>因此，我一直在编写有关如何运行特定类型的聚会活动的<a href="https://www.lesswrong.com/s/eqtiQjbk83JHyttrr"><u>Meetups In A Box</u></a>序列。这篇文章使用相同的格式，但不按这个顺序。为什么不？简而言之，因为我不能把它放在盒子里。我还没有弄清楚如何为您的社区提供一位熟练的演说家，并定期提供有关理性主义文本的课程。这是根据牧师所做的事情非常明显地概述的，而牧师是一份兼职工作。</p><br/><br/> <a href="https://www.lesswrong.com/posts/9T5Ai2sTYZvjBZqEb/a-reading-from-the-book-of-sequences#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9T5Ai2sTYZvjBZqEb/a-reading-from-the-book-of-sequences<guid ispermalink="false"> 9T5Ai2sTYZvjBZqEb</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Tue, 28 Nov 2023 06:45:59 GMT</pubDate> </item><item><title><![CDATA[Anthropic Fall 2023 Debate Progress Update]]></title><description><![CDATA[Published on November 28, 2023 5:37 AM GMT<br/><br/><p><i>这是我在 Anthropic 所做的一些可扩展监督工作的研究更新，基于最初的</i><a href="https://arxiv.org/abs/1805.00899"><i><u>人工智能安全辩论</u></i></a><i>提案以及纽约大学和 Anthropic 制定的最新议程。核心文档是几个月前编写的，因此其中一些内容可能已经过时，但似乎值得以当前的形式分享。</i></p><p><i>我要感谢 Tamera Lanham、Sam Bowman、Kamile Lukosiute、Ethan Perez、Jared Kaplan、Amanda Askell、Kamal Ndousse、Shauna Kravec、Yuntao Bai、Alex Tamkin、Newton Cheng、Buck Shlegeris、Akbir Khan、John Hughes、Dan Valentine 、Kshitij Sachan、Ryan Greenblatt、Daniel Ziegler、Max Nadeau、David Rein、Julian Michael、Kevin Klyman、Bila Mahdi、Samuel Arnesen、Nat McAleese、Jan Leike、Geoffrey Irving 和 Sebastian Farquhar 提供的帮助、反馈和深思熟虑的讨论改进了这项工作和写作的质量。</i></p><h1> <strong>1. Anthropic的辩论议程</strong></h1><p>在本文档中，我指的是<a href="https://arxiv.org/abs/1805.00899"><u>通过辩论在人工智能安全</u></a>中首次提出的想法（<a href="https://openai.com/research/debate"><u>博客文章</u></a>）。其基本思想是通过在辩论中相互竞争来监督未来的人工智能系统，鼓励它们对问题的双方（或“所有方面”）进行辩论，并利用由此产生的论点得出有关该问题的最终答案。在这个方案中，我们将参与辩论的系统称为<strong>辩手</strong>（尽管通常情况下，这些系统实际上是被提示与自己争论的同一底层系统），我们将代理称为代理（另一个人工智能系统或人类，或一个系统）人类和人工智能一起工作等），从而对<strong>法官的</strong>辩论做出最终决定。</p><p>对于那些或多或少熟悉原始 OAI/Irving 等人的人来说。辩论议程，您可能想知道该议程与我们在 Anthropic 追求的议程之间是否有任何差异，确实有！<u>萨姆·鲍曼</u>和<u>塔梅拉</u><a href="mailto:tamera@anthropic.com"><u> </u></a><u>兰纳姆（Lanham）</u>撰写了一份工作中的 <a href="https://docs.google.com/document/d/173SpCyspboHBp3bHqWvUiduzatbuuv7QAvWVamwcGbk/edit"><u>人类-纽约大学辩论议程</u></a>草案，这正是本文档中的实验所推动的目标。 <span class="footnote-reference" role="doc-noteref" id="fnrefcalh1cdjn8c"><sup><a href="#fncalh1cdjn8c">[1]</a></sup></span></p><p>引述该议程的基本特征，以及它与最初的辩论方向有何不同：</p><blockquote><p>以下是基本提案的定义特征：</p><ul><li><strong>关于两个选择问题的两人辩论：两名辩手（通常是法学硕士的两个实例）向法官（通常是人类，或者在某些情况下是法学硕士）提供证据和论点，以说服法官选择他们指定的答案一个有两个可能答案的问题。</strong></li><li><strong>无外部强加的结构</strong>：辩论的结构和规范不是正式规定的，而是来自辩论者学习如何最好地说服法官，以及法官同时学习什么样的规范往往会导致他们能够做出准确的判断。</li><li><strong>评估整个论点</strong>：辩论在三个参与者之间的单个线性对话记录中展开。与原始辩论议程的某些版本不同，没有明确的树结构来定义辩论，并且不要求法官专注于单个症结。这应该会让这个过程变得不那么脆弱，但代价是使一些问题的解决成本极其昂贵，并且可能使其他问题无法解决。</li><li><strong>训练有素的法官：法官经过明确和广泛的培训，能够准确地判断这些辩论，与固定的辩论者群体合作，使用实验者知道真实答案的问题。</strong></li><li><strong>自我对战：通过多智能体强化学习，辩手与裁判同时接受训练。</strong></li><li><strong>优雅的失败</strong>：如果双方都没有向法官提出完整、令人信服的论点，则辩论可能会悬而未决。这是为了缓解<a href="https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>争论混乱的问题</u></a>，因为法官不会被迫在双方都无法清楚地阐明自己立场的辩论的基础上做出决定。</li></ul><p>该框架旨在使能力较弱的法官能够学到能力较强的辩手可以向法官充分证明的任何内容。</p></blockquote><h1> <strong>2. 实验动机和设置</strong></h1><p>简而言之，我们想要令人信服地回答这个问题：“ <strong>Claude-2 级别模型可以用于研究辩论等对抗性可扩展监督技术吗？”</strong> ”</p><p>我们遵循的总体路线图如下所示：</p><ul><li>选择一个我们认为适合可扩展监督研究的任务</li><li>考虑到以前的尝试，找出我们可以采取哪些不同的做法</li><li>在尝试任何强化学习 (RL) 之前，通过纯推理实验证明成功</li><li>运行初始强化学习实验，证明我们可以通过强化学习引发更强的辩论能力</li><li>运行多智能体强化学习 (MARL) 实验，证明我们既可以使法官模型更有能力判断模型辩论，又可以并行训练辩论者模型以生成更有力的论据</li></ul><h2><strong>任务：隐藏通道质量</strong></h2><p><a href="https://arxiv.org/abs/2112.08608"><u>QuALITY</u></a>是一个多项选择问答数据集，由有关（相当长）文本段落的问题组成。我们有兴趣看看是否可以获得一个模型来准确回答有关该段落的问题，<strong>而无需完全访问该段落</strong>。</p><p>这听起来显然毫无希望，而且通常确实如此，所以我们需要以某种方式帮助这个模型。基本思想是让另一个模型访问该段落并让它为每个答案选择进行辩论（我们将此模型称为<strong>辩论者</strong>），然后向初始模型（<strong>法官</strong>）提供论点并要求它选择它认为正确的答案是使用所有证据和论证中的推理。</p><p>我们还将原始的四项选择 QuALITY 数据集变成了一项二项选择数据集——目前尚不清楚这种改变会产生什么效果，因为有了四个选择，法官就可以看到四个论点，并且可能会看到更多有关该段落的信息。我最好的猜测是，四个答案选择会让我们观察到更差的绝对性能，但与我们的一些基线相比更好的相对性能。</p><p>作为一个额外的问题，我们将提示/指示辩手将段落中的所有引用包含在 xml 标记 ( <code>&lt;p_quote>;</code> ) 中。然后，通过（有点严格的）正则表达式检查，我们将尝试验证引用<strong>确实</strong>来自该段落。如果不是，即使模型犯了良性错误而不是彻头彻尾的捏造，我们也会将它们标记为未经验证（ <code>&lt;u_quote>;</code> ），而如果它们包含在段落中，我们会将它们标记为已验证（ <code>&lt;v_quote>;</code> ）。指示法官仅信任经过验证的引述，而不是未经验证的引述，或不以这种格式呈现的其他引述（例如，带有普通引号的引述）。我们对辩手可以引用的段落数量没有限制，并且我们发现在训练过程中引用材料的数量大致保持不变——如果我们观察到引用材料的数量急剧上升，或者尝试从无论辩手引用整个段落，我们都会对此进行处罚，或者对引用材料的数量实施更严格的限制。</p><p>我们使用（二选一）QuALITY（随机选择）的 500 个问题子集作为我们的验证数据集，并使用剩余的 1586 个问题在 MARL 期间训练我们的模型。</p><h2><strong>没有明确反驳的辩论</strong></h2><p>早期在 Anthropic 进行辩论的尝试遇到了模型无法与辩论对手互动以及通常缺乏多轮辩论所需的连贯性的问题。我们希望，即使模型缺乏有意义地参与多轮来回辩论的能力，仍然能够产生令人信服的“开场陈述”，为自己的立场提出强有力的论据，并试图反驳最重要的观点。对其立场进行明显或明显的反驳。换句话说，两位辩手从未看到对方的论点，纯粹是在针对假设的未来对手构建他们的论点。</p><p>我认为这种方法实际上仍然具有辩论作为监督协议的很多好处，但最终，我们可能希望进行多轮辩论，并且可能让法官也参与辩论。这种“并发回合”的方式也将辩论变成了一场不完全信息的博弈，而不是“顺序回合”辩论的<a href="https://en.wikipedia.org/wiki/Perfect_information"><u>完美信息</u></a>游戏。</p><h2><strong>仅推理实验</strong></h2><p>我们的基本目标是消除我们想要进行的后续实验的风险，这将涉及昂贵且耗时的强化学习训练。</p><h3><strong>我们的成功标准</strong></h3><ul><li>从定性上来说，辩手能够为正确和错误的答案生成合理的论据（在可能的范围内）。<ul><li>我们希望避免的主要事情是模型拒绝争论错误答案，或者明显以一种混乱/不合逻辑的方式进行推理</li></ul></li><li>如果我们选择辩手的论点对给定法官有说服力，我们就会发现我们可以可靠地提高辩手的 ELO 分数。<ul><li>为此，我们可以想象一个<code>model_bon</code>抽象，它是我们始终从中进行 Best-of-N (BoN) 采样的模型。例如，我们可以想象创建一个<code>claude-2_1</code>模型（即 Claude 2）和一个<code>claude-2_8</code>模型（即我们始终使用 BoN 采样的 Claude 2 实例，其中<code>N</code>设置为 8）。<ul><li>为了实现 BoN 最大化的目标，我们从辩手中选择论点样本，以最大化法官模型对该辩手指定答案选择的置信度。这是为了近似强化学习训练期间的自我对战目标。<ul><li>在此过程中，我们用“虚拟论证”替换另一位辩手的论证，例如“根据段落，正确答案是_”。这是一种简化，因为原则上我们可以为每一方生成<code>N</code>参数，然后采用每一方“赢得”次数最多的参数，作为 RL 目标的更好近似，但在实践中，我们发现这种简化工作得很好。</li></ul></li></ul></li><li>我们收集了一组<code>model_bon</code>模型作为我们的辩手，让他们以循环赛的方式进行辩论，计算在一系列隐藏通道质量辩论中每场比赛的胜利计数，其中胜利意味着法官选择了答案选项对应于那个辩手。我们也可以对多名法官重复这个过程。</li><li>我们应该观察到，对于每个模型，随着我们用于 BoN 采样的 N 的增加，我们可以发现 ELO 可靠且稳定的增加。</li></ul></li><li>判断辩论的模型显然无法执行给定的任务，我们可以使用思想链 (CoT)（+ 多数投票/BoN 抽样）等方法来观察改进。<ul><li>重要的是，观看辩论的法官应该超越一些底线：<ul><li>一个“盲人法官”看不到辩论，只看到问题和两个答案选择</li><li>允许使用 CoT 的“盲人法官”（可能采用多数票或 BoN 抽样）</li></ul></li><li>然而，如果法官获得通过，我们不一定有任何野心让法官超越零射击精度或 CoT 精度等基线。<ul><li>我们最终希望达到这样的上限，但我们认为，如果至少不培训法官，这可能是不可能的。</li></ul></li></ul></li></ul><p>最终，我们基本上通过纯推理实验看到了我们想要看到的结果，因此决定尝试 RL 运行。</p><h2><strong>辩手的强化学习训练</strong></h2><p>我们训练一个辩手模型来为（双选）质量问题的两个答案生成论据。法官看到问题、答案选择和两个论点，然后选择它认为正确的答案。辩手模型在每次辩论的两个片段上进行训练，其中为辩论获胜方（法官选择的一方）生成的论点通过等于法官对两个答案的逻辑之间的正差的奖励得到强化选择标记，失败的论点通过等于法官在两个答案选择标记上的逻辑之间的负差的奖励来强化。我们还为每个生成的参数添加偏好模型（PM）奖励。然而，进一步的实验表明，这种二次奖励是不必要的，因此纯粹的“自我对弈”奖励足以达到质量上相似的结果。</p><p>在伪代码中：</p><pre> <code>question = QualityDataset.random_question() debater_A = debater_B = debater # same debater model, self-play debater_A_prompt, debater_B_prompt = question.get_debater_prompts() debater_A_argument = debater_A.sample(prompt=debater_A_prompt) debater_B_argument = debater_B.sample(prompt=debater_B_prompt) debater_A_pm_score = pm.pm_score( prompt=debater_A_prompt, response=debater_A_argument, ) debater_B_pm_score = pm.pm_score( prompt=debater_B_prompt, response=debater_B_argument, ) judge_prompt = question.get_judge_prompt( debater_A_argument=debater_A_argument, debater_B_argument=debater_B_argument, ) judge_logits = judge.logits( prompt=judge_prompt, option_tokens=[&quot;A&quot;, &quot;B&quot;], ) debater_A.add_ppo_episode( prompt=debater_A_prompt, response=debater_A_argument, reward=debater_A_pm_score + judge_logits[&quot;A&quot;] - judge_logits[&quot;B&quot;], ) debater_B.add_ppo_episode( prompt=debater_B_prompt, response=debater_B_argument, reward=debater_B_pm_score + judge_logits[&quot;B&quot;] - judge_logits[&quot;A&quot;], )</code></pre><h2><strong>辩手RL训练和裁判SL训练</strong></h2><p>我们实现了相同的设置，但现在我们还通过监督学习（SL）/行为克隆（BC）来训练法官根据问题、答案选择以及辩手为每个答案生成的论点来预测问题的正确答案选择。</p><p>在伪代码中：</p><pre> <code>train_judge = False if random.random() &lt; train_judge_frac: train_judge = True question = QualityDataset.random_question() debater_A = debater_B = debater # same debater model, self-play debater_A_prompt, debater_B_prompt = question.get_debater_prompts() debater_A_argument = debater_A.sample(prompt=debater_A_prompt) debater_B_argument = debater_B.sample(prompt=debater_B_prompt) debater_A_pm_score = pm.pm_score( prompt=debater_A_prompt, response=debater_A_argument, ) debater_B_pm_score = pm.pm_score( prompt=debater_B_prompt, response=debater_B_argument, ) judge_prompt = question.get_judge_prompt( debater_A_argument=debater_A_argument, debater_B_argument=debater_B_argument, ) judge_logits = judge.logits( prompt=judge_prompt, option_tokens=[&quot;A&quot;, &quot;B&quot;], ) blind_judge_prompt = question.get_blind_judge_prompt() debater_A.add_ppo_episode( prompt=debater_A_prompt, response=debater_A_argument, reward=debater_A_pm_score + judge_logits[&quot;A&quot;] - judge_logits[&quot;B&quot;], ) debater_B.add_ppo_episode( prompt=debater_B_prompt, response=debater_B_argument, reward=debater_B_pm_score + judge_logits[&quot;B&quot;] - judge_logits[&quot;A&quot;], ) correct_choice = question.correct # one of &quot;A&quot; or &quot;B&quot; if train_judge: # judge is trained via SL/BC judge.add_bc_episode( prompt=judge_prompt, target_action=correct_choice, ) # also train the &quot;blind&quot; judge via SL/BC blind_judge.add_bc_episode( prompt=blind_judge_prompt, target_action=correct_choice, )</code></pre><h1> <strong>3. 结果</strong></h1><p>对于所有剩余结果，我们仅在验证数据集上评估经过训练的模型，该数据集包含训练期间提出的 500 个问题。所有模型均从 Claude 2 的微调版本初始化。</p><h2><strong>我们可以训练模型来生成对其他模型更有说服力的论据</strong></h2><p>我们主要通过 Debater ELO 评级来评估这一点。我们希望看到训练中较晚的检查点比较早的检查点更容易赢得辩论。此外，我们有兴趣了解使用 BoN 采样的 RL 训练堆栈的改进程度。</p><h3><strong>辩手 ELO（由 RL Step 和 BoN N 提供）</strong></h3><p>我们通常看到辩手 ELO 通过 RL 训练和 BoN 采样（针对法官）而增加，而且这些改进确实是叠加的。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QtqysYdJRenWFeWc4/emvpt2eegawiwryufcwv" alt="该图在 y 轴上显示 Debater ELO，在 x 轴上显示 BoN 采样的 N。 X 轴是对数刻度。显示了三条曲线，每条曲线对应于各自的辩论者模型快照（在步骤 0、步骤 25 和步骤 50）。步骤 50 快照对于所有 N 值都比步骤 25 快照具有更高的 ELO，而步骤 25 快照对于所有 N 值都比步骤 0 具有更高的 ELO，这表明 RL 训练可靠地增加了辩手 ELO。此外，每条曲线都是相对线性的，表明 ELO 可能相对于 log(N) 是线性的。"></p><h3><strong>辩手 ELO（按 RL Step、BoN N 和诚实/不诚实状态）</strong></h3><p>这与上面的图相同，但根据模型是主张正确答案（“是诚实的”）还是主张错误答案（“是不诚实的”）来单独分解的。</p><h2> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QtqysYdJRenWFeWc4/s21q9zvdxfkgx4slojoi" alt="该图在 y 轴上显示 Debater ELO，在 x 轴上显示 BoN 采样的 N。 X 轴是对数刻度。与上述情节相同，但根据辩手是否争论正确答案（“诚实”或“不诚实”）来划分。不诚实的辩手的 ELO 确实比诚实的辩手低（大约 175-225 ELO 差距），如虚线所示，他们的 ELO 比实线辩手低大致恒定的量。"></strong></h2><h2><strong>生成的参数通常质量会提高</strong></h2><p>根据粗略的定性分析（查看了约 30 场辩论），我对所使用的基础模型视而不见，通常看起来辩手模型正在学习识别为给定答案选择进行辩论的最佳方法并找到最佳方法其立场的证据。</p><h3><strong>成绩单</strong></h3><p>（为简洁起见，省略了，但如果您想阅读这些内容，请给我留言！）</p><h2><strong>我们可以并行训练一个法官模型，以更好地评估模型辩论，从而更准确地回答问题</strong></h2><p>对于所有这些数字，我们仅根据自比结果进行汇总，因此辩手始终具有相同的“实力”。</p><h3><strong>原始准确度和基线比较</strong></h3><p>只需查看原始准确率，通过法官快照聚合所有辩手 model_bon 对象：</p><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top"><p><strong>法官模型</strong></p></td><td style="padding:5pt;vertical-align:top"><p><strong>聚合验证准确性</strong></p></td></tr><tr><td style="padding:5pt;vertical-align:top">步骤0</td><td style="padding:5pt;vertical-align:top"> 73.1%</td></tr><tr><td style="padding:5pt;vertical-align:top">步骤25</td><td style="padding:5pt;vertical-align:top"> 74.7%</td></tr><tr><td style="padding:5pt;vertical-align:top"><strong>步骤50</strong></td><td style="padding:5pt;vertical-align:top"> <strong>77.7%</strong></td></tr></tbody></table></figure><p></p><p></p><p></p><p></p><p></p><p>值得注意的是，即使盲人法官接受了与法官相同问题的培训，我们仍然明显超过了“盲人”法官的基线。事实上，随着训练的进行，差距越来越大！此外，正如我们所预料的那样，盲人判断基本上不会推广到验证集中保留的问题，这并不奇怪，因为我们只是训练它记住训练集上的正确答案。</p><figure class="table"><table><tbody><tr><td style="padding:5pt;vertical-align:top"><p><strong>盲人法官模型</strong></p></td><td style="padding:5pt;vertical-align:top"><p><strong>验证准确性</strong></p></td></tr><tr><td style="padding:5pt;vertical-align:top"><strong>步骤0</strong></td><td style="padding:5pt;vertical-align:top"> <strong>67.2%</strong></td></tr><tr><td style="padding:5pt;vertical-align:top">步骤25</td><td style="padding:5pt;vertical-align:top"> 66.0%</td></tr><tr><td style="padding:5pt;vertical-align:top">步骤50</td><td style="padding:5pt;vertical-align:top"> 67.0%</td></tr></tbody></table></figure><h3></h3><h3></h3><h3></h3><h3></h3><h3></h3><h3><strong>辩手 ELO 的判断准确性</strong></h3><p>我们还可以根据辩手的 ELO 来查看判断准确性： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QtqysYdJRenWFeWc4/ctnrbsphx4kspdlu53vg"></p><p>这为我们提供了一些证据，说明法官的准确性如何随着辩手实力的变化而变化——我们可能希望每条线的斜率都呈上升趋势，对于训练有素的法官来说，这种趋势可能最为强烈。不幸的是，我们在这里没有看到任何强烈的影响，但我认为尚不清楚这是否应该是一个重大的负面更新，并且更可能是当前模型能力水平的下游。</p><h3><strong>诚实/不诚实辩手 ELO 差异的判断准确性</strong></h3><p>与上面类似，我们可能还想看看判断准确性如何根据辩手实力的相对差异而变化。这让我们可以研究，对于固定的裁判模型来说，“真相”有多大的优势，以及这能在多大程度上克服辩手实力的差异。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QtqysYdJRenWFeWc4/xblitnzgaucdzqafg39t"></p><p>一般来说，辩手ELO差异对裁判准确率的影响相当严重。理想情况下，我们希望最大化每条线下方的面积，这表明裁判准确性受相对辩手实力的影响最小，我们可以看到 S50 裁判在这个指标上的表现明显更好，尽管还有明显的改进空间。</p><h1> <strong>4. 结果</strong></h1><p>我认为结果是<strong>我们有了一个在简单且受限的环境中显示出希望的辩论协议</strong>。我们[人类]应该能够追求对抗性可扩展监督议程的其他部分，这可能是足够有希望的。</p><p>那么，接下来我们应该尝试什么呢？我将在下一节中详细阐述这一点，但简而言之：</p><ul><li>训练更多任务和更多数据！我认为这是提高性能的最大瓶颈，并将大大增强我对这些结果的信心。<ul><li>相关地，为辩论和其他可扩展的监督协议建立更多更好的夹心评估。</li></ul></li><li>让人类参与循环实验（可能充当法官，尽管让人类充当与模型对抗的辩手也可能很有趣）。</li><li>敌对地将辩论评估为可扩展的监督协议：即使我们的模型对我们不利，它也能发挥作用吗？</li></ul><h1> <strong>5. 可能的后续步骤</strong></h1><h2><strong>查找并使用更多任务/数据来设置更多夹心评估</strong></h2><p>这是真正关键且显而易见的下一步。我认为真正令人信服地证明辩论作为可扩展的监督协议的有效性取决于我们进行这种夹层评估：</p><ol><li>采取一组我们有可靠基础事实的任务（具有明确正确答案的问题标签，或者由专家提供的大量标签）。</li><li>训练辩手模型为所有这些任务生成论据，同时以某种方式向法官提供有关基本事实的信息，以便它学习如何总体评估论据<ol><li>显而易见的是，如果我们希望法官以某种方式参与辩论/解释其裁决，则可以通过 SL 或 RL 来训练法官，但请参阅下面的其他一些想法</li></ol></li><li>（希望）观察到辩手学会提出越来越好的论点，法官在评估他们方面做得更好（针对同一组任务中提出的问题）</li><li>然后，采取一个新的、保留的任务，我们也有基本事实，并继续通过强化学习自我对弈来训练辩手（尽管我们可能不希望让这个辩手与接受过训练的辩手共享权重）之前的任务），并<strong>冻结判断</strong>。我们应该观察到，在我们从这个新领域的辩手那里引出可靠的强有力的论据之后，法官最终学会了对这些问题给出可靠的良好答案。</li></ol><p>如果没有更多任务，我们就无法完成上述任务！我们目前正在探索与当前任务一样包含辩手和评委之间相同类型“信息不对称”的任务，以及涉及“能力不对称”的任务，即用能力较差的模型充当能力较弱的模型的评委。能干的辩手模型。能力不对称设置可能与我们最终所处的位置更直接相关，但信息不对称设置似乎包含类似的动态，并且更有可能在当前模型中取得进展。</p><h2><strong>开始进行人体实验</strong></h2><p>人类的明显作用是作为模型生成的辩论的评委，但我认为让模型与人类辩手进行比赛，看看我们是否可以训练模型以匹配或超过人类辩手的表现也可能很有趣。</p><h2><strong>对辩论进行对抗性评估作为监督协议</strong></h2><p>这是受到 Buck Shlegeris 和 Ryan Greenblatt 的<a href="https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><u>这篇文章</u></a>的启发。激励性的想法类似于“假设你的模型已经在对你进行阴谋——如果是这种情况，你的监督协议还能起作用吗”？找出辩论是否可以接受这种对抗性评估以及与其他监督技术的比较，这将是非常有用的。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fncalh1cdjn8c"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcalh1cdjn8c">^</a></strong></sup></span><div class="footnote-content"><p>该议程是在9个月前撰写的，因此不应过于字面意思，也不应作为我们正在努力的研究方向的确切规范。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/QtqysYdJRenWFeWc4/anthropic-fall-2023-debate-progress-update#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qtqysydjrenwfewc4/anththropic-fall-2023-debate-progress-progress-progress-pate<guid ispermalink="false"> qtqysydjrenwfewc4</guid><dc:creator><![CDATA[Ansh Radhakrishnan]]></dc:creator><pubDate> Tue, 28 Nov 2023 05:37:30 GMT</pubDate> </item><item><title><![CDATA[Apocalypse insurance, and the hardline libertarian take on AI risk]]></title><description><![CDATA[Published on November 28, 2023 2:09 AM GMT<br/><br/><p><strong>简短版本：</strong>在一个更旧的世界中，AI实验室必须购买某种“启示录保险”，并以溢价取决于其行为，以使鲁ck行为在货币上不可行。我不希望地球能够实施这样的政策，但似乎值得大声说出正确的答案。</p><p></p><h2>背景</h2><p>提倡与<a href="https://twitter.com/astupple/status/1728821381402624459"><u>自由主义</u></a>相反的<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"><u>人工智能关闭</u></a>吗？是否倡导AI关闭，例如争取免费的市场，除非我个人对解决方案感到不舒服？</p><p>考虑一下古老的格言：“摆动拳头在我的鼻子开始的地方结束的权利”。一个希望不打孔的自由主义者是否需要在自由主义中添加星号，因为他们有时希望限制邻居挥舞拳头的能力？</p><p>不必要！坚定的自由主义者有许多理论上的方法，他们想避免在脸上打孔，这不需要大型的州政府。例如：他们可能会相信私人安全和仲裁。</p><p>但是，这种事情在实践中可能会变得混乱。假设您的邻居建立了一家生产大量铅尘的工厂，从而威胁到您孩子的健康。现在您应该侵犯他们经营工厂的权利吗？您是否正在雇用雇佣军用武力关闭工厂，然后更多的雇佣军克服他们的反马赛亚？</p><p>坚定的自由主义者可以对这个问题提出许多不同的答案。一个常见的是：“内部化外部性”。 <span class="footnote-reference" role="doc-noteref" id="fnrefnxw5dornku9"><sup><a href="#fnnxw5dornku9">[1]</a></sup></span>您的邻居不应该用一堆铅尘填充空气，除非他们可以<i>适当地支付损失</i>。</p><p> （而且，如果损害赔偿实际上非常高，并且您设法适当地向其开票，那么这可能会成为寻找其他金属的非常好的动机贪婪是一项强大的力量，当被用时。）</p><p>现在，关于如何确定损害赔偿的大小以及如何确保人们为造成的损害支付账单有很多疑问。有些解决方案听起来更像是状态，听起来更像是私人社会合同和私人执法。而且我认为值得考虑的是，有很多不值得账单的费用，因为基础设施为他们支付费用并不值得官僚主义和令人震惊的效果。</p><p>但是我们希望大家都同意，<i>注意到一些大型外部性</i>并<i>希望它内部化</i>与自由主义者的世界观并不矛盾。</p><p></p><h2>责任保险</h2><p><a href="https://www.overcomingbias.com/p/yay-liability-insurancehtml">有限责任是风险补贴</a>。责任保险将使激励措施变得更好。</p><p>在一个较卫生的世界中，我们会在造成巨大的负面外部性（例如漏油）时向人们收费，并利用这笔钱扭转损失。</p><p>但是，如果某人造成的损害比拥有金钱更多的损失怎么办？然后，社会总体受伤。</p><p>为了防止这种情况，我们有保险。大约有100人，每个人都有1％的风险，造成比付款能力大10倍的风险，都可以（预先同意）将他们的资金汇集到其中少数人，从而允许广泛的阶级承担风险没有人能负担得起（为了所有的利益；贸易是一个积极的游戏，等等）。</p><p>在一个理智的世界中，我们不会让邻居对我们的生命或财产承担实质性的风险（他们没有能够支付的方式），其原因是我们不让他们偷窃的原因相同。让某人承担巨大的风险，在那里获得收益（如果成功的话），我们支付罚款（如果不是），只是盗窃了额外的步骤，社会应该这样对待。市场的自由和公平性取决于市场的自由和公平性取决于防止盗窃。</p><p>同样，这并不是说理论上需要一个国家 - 也许自由主义者更喜欢一个世界，其中许多人签署了广泛的“贸易公平而不窃取”社会契约，这份合同被认为是表 - 文明人民的贸易赌注。在这种情况下，我的观点是，这份社会契约可能应该包括条款，说人们对造成的损害负有责任，而对盗贼造成的相同执法机制也违反了施加风险的人（对他人）施加风险缺乏资金和/或保险。</p><p>现在，防止人们“施加风险”，除非他们“拥有足够的钱或保险来承担损害赔偿”在某种意义上比防止简单的盗窃要困难，因为盗窃相对容易检测到，并且风险分析很难。但是<i>从理论上讲</i>，如果您不想大大补贴您的生活，自由和财产的巨大风险，则确保每个人都拥有责任保险是维持自由市场的重要组成部分。</p><p></p><h2>启示录保险</h2><p>希望到现在为止，这些要点与存在风险的相关性很明显。人工智能公司通过开发AI，同时不知道自己在做什么，以我们的生命，自由和财产（以及所有潜在的未来人物）面临极大的风险。 （ <a href="https://www.lesswrong.com/posts/N7DxcLCjfBpEv3QwB/request-stop-advancing-ai-capabilities">请住手。</a> ）</p><p>在一个理智的世界中，社会会注意到这一点 - 也许是通过大型高油流的房地产<a href="https://www.astralcodexten.com/p/prediction-market-faq"><u>预测市场 -</u></a>要求AI公司按照这种风险支付“启示录保险”（使用它们的任何社会协调机制）有可用的）。</p><p>当我最近面对面提出这一主张时，人们会经常反对：但是在活动发生之前，保险不会支付！要求爱丽丝拥有责任保险的要求，如果爱丽丝摧毁了世界？任何保险公司都应该很乐意以非常便宜的价格将该保险出售给爱丽丝，因为他们知道他们永远不必付款（在爱丽丝杀死所有人的情况下死亡）。</p><p>答案是，启示录保险（类似于责任保险）可能<i>在毁灭所有人之前</i>支付。如果有人想冒着杀死您的风险（有些概率），那么现在可能会付出<i>一些</i>钱，以换取承担风险的能力。</p><p> （然后在您反对“不是我！”之前，请注意，文明愉快地将飞机飞过您的头上，这有一些撞车和杀死您的风险 - 一个坚定的自由主义者可能会说，您应该为这种风险而付出任何票据与您承担的风险成比例，以激励文明建造更安全的飞机并抵消风险。）</p><p>这里的指导原则是贸易是积极的。当您认为您可以通过冒着生命危险（例如，通过飞行飞机上的飞机）赚很多钱，而我不希望自己的生命冒险时，就有机会进行互惠互利的贸易。如果风险足够小，钱数量足够大，那么您可以给我一笔钱，这样我更喜欢这笔钱而不是缺席风险，而且您仍然有很多钱。每个人都更好。</p><p>据我所知，这就是社会“应该”与人工智能开发人员（以及所有风险冒着生命和生计的技术人员）之间的关系。</p><p>警告说，风险<i>并不</i>小，并且AI开发人员在很大程度上冒着<i>每个人</i>的生活风险，这很<i>昂贵</i>。</p><p>简而言之：启示录保险与责任保险的不同之处在于，应立即向每个公民（开发人员置于危险中），被视为贸易，以换取冒着生命和生计风险的贸易。</p><p>换句话说：从自由主义者的角度来看，它<i>确实很有意义</i>（不损害您的自由主义者理想，甚至是一个Iota），看一下AI开发人员并说：“他妈的停下来（您在其他所有人的生活中都冒着太大的风险;这是盗窃的一种形式，直到您可以支付您要冒险的所有人，足以抵消风险）。”</p><p></p><h2>注意事项</h2><p>在一个理智的世界中，启示录保险工作所需的确切计算对我来说似乎相当微妙。列举一些注意事项：</p><ul><li> AI公司应该能够通过其技术设法捕获的价值的分数来进行一些付款（对IT风险冒险的人，以换取冒着生命的能力，以换取冒着生命的能力的人）。</li><li>除此之外，任何做好工作的人<a href="https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future"><u>都不应该在未来留下指纹</u></a>。<a href="https://nickbostrom.com/astronomical/waste">宇宙的捐赠</a>不是他们的捐赠（也许他们应该借给自己的份额吗？）。</li><li>而且，我们是否愿意让AI公司借贷他们可以创造的所有价值，而不是让他们借给他们反映的价值的贷款（鉴于其他一些小组可能会出现），这也很复杂。过了一会儿，这有点安全，并提供相同的收益）。</li><li>关于如何评估风险有很大的疑问（当然，承诺的未来明星的价值在很大程度上取决于风险）。</li><li>关于未来的人（如果地球上的生活被消灭，他们将不存在）存在很大的疑问。</li></ul><p>而且我并不想在这里充实一个完整的计划。我认为地球没有这样做的物流能力。</p><p>相反，我的意思是：这些人在冒着我们的生命危险；他们没有内在化。无论您的意识形态如何，试图向他们开票都是完全合理的（尤其是它适合没有任何星号的自由主义者意识形态）。</p><p></p><h2>为什么这么统计？</h2><p>然而，在所有这些方面，我主张全球协调的AI关闭，并由各州执行该关闭，直到我们可以<a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies"><u>弄清楚我们在做什么和/或升级人类，以至于他们可以正常地完成工作</u></a>。</p><p>但是，这是我理想的结果，不要与<i>更喜欢</i>政府干预相混淆。</p><p>鉴于<i>撞到</i>刹车所需的雄心勃勃的行动，并且考虑到这些行动可能出错的方式，也不会与期望它起作用的期望混淆<i>。</i></p><p>相反，我花了多年的时间进行技术研究，部分原因是我<i>不</i>希望政府干预在这里工作。这项研究尚未消失，整个领域几乎没有取得进展。因此，我将政府作为最后的手段转向政府，因为政府是我们拥有的工具。</p><p>我更喜欢一个足够多的风险的世界，告诉人工智能公司，他们需要通过一些非强制性协调机制支付启示录保险或关闭（例如，与某些基本的背景贸易协定有关的机制“和“承担您的责任”，而不是暴力，而是无法与文明人交易的痛苦）。这些保费会像摧毁世界上<a href="https://nickbostrom.com/astronomical/waste">宇宙捐赠</a>规模的世界的风险一样，他们被允许借给他们的成功。也许是保险精算师，我不会完全关注眼睛，但是至少在一个<a href="https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results">从事问题的人中，有93％的人说，有可能破坏未来价值的很大一部分的机会10+％</a> ，这项非强制性政策将完成其工作。</p><p>在现实生活中，我怀疑我们可以实现这一目标（尽管我朝着这个方向认可步骤！）。地球没有这种协调机制。它有国家。因此，我希望我们需要某种<a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1">状态的联盟</a>，这是以前在地球上实际在地球上工作的事情（例如，在核武器的情况下），并且会陷入地球现有的协调机制中。</p><p>但是，即使我们无法实现，似乎仍然值得大声说出原则上的解决方案。 </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnnxw5dornku9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnxw5dornku9">^</a></strong></sup></span><div class="footnote-content"><p>这里的一个相关观察是，考虑到邻居的拳头的适当自由主义者的自由市场方式不是要强行<i>阻止</i>他使用私人保安公司，而是想对<i>他指控他为特权。</i>如果我愿意为此付出我<a href="https://www.lesswrong.com/posts/MzKKi7niyEqkBPnyu/your-cheerful-price#"><u>开朗的价格</u></a>，欢迎我的邻居打我！贸易可以是积极的！而且，如果他们不愿意把现金小马，那么猛击我就是盗窃，应该用我们想象的其他机制来处理能够实现市场自由的任何机制。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mseesg7i4d9scwaet/apocalypse-insurance-insturance-and-the-hardline-libertarian-take-take-on-ai<guid ispermalink="false"> MSEESG7I4D9SCWAET</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Tue, 28 Nov 2023 02:09:53 GMT</pubDate> </item><item><title><![CDATA[My techno-optimism [By Vitalik Buterin]]]></title><description><![CDATA[Published on November 27, 2023 11:53 PM GMT<br/><br/><p>维塔利克（Vitalik）写了一篇文章，试图提出自己的理由，以此为理由，总结为他称之为“ D/ACC”的意识形态。我引起了很多共鸣，尽管对于试图创造这样的社会运动和意识形态也有矛盾的感觉。</p><p>在一些引号和目录。</p><blockquote><p>上个月，马克·安德森（Marc Andreessen）发表了他的“<a href="https://a16z.com/the-techno-optimist-manifesto/">技术乐观主义宣言</a>”，主张重新燃起对技术的热情，并将市场和资本主义作为构建技术和推动人类走向更加光明的未来的手段。该宣言明确拒绝它所描述的停滞意识形态，这种意识形态害怕进步并优先考虑保护当今世界。这份宣言受到了很多关注，包括<a href="https://www.noahpinion.blog/p/thoughts-on-techno-optimism">诺亚·史密斯</a>、<a href="https://www.overcomingbias.com/p/which-political-axis">罗宾·汉森</a>、<a href="https://joshuagans.substack.com/p/marc-andreessens-techno-optimism">约书亚·甘斯</a>（较为积极）以及<a href="https://davekarpf.substack.com/p/why-cant-our-tech-billionaires-learn">戴夫·卡普夫</a>、<a href="https://gizmodo.com/marc-andreessen-is-wrong-about-everything-1850934367">卢卡·罗佩克</a>、<a href="https://www.nytimes.com/2023/10/26/opinion/marc-andreessen-reactionary-futurism.html">埃兹拉·克莱因</a>（较为消极）等许多人的回应文章。 James Pethokoukis 的《<a href="https://www.amazon.com/Conservative-Futurist-Create-Sci-Fi-Promised/dp/1546005544">保守的未来主义者</a>》和 Palladium 的《<a href="https://www.palladiummag.com/2020/04/30/its-time-to-build-for-good/">是时候为善而建</a>》与本宣言无关，但主题相似。本月，我们<i>在</i><a href="https://www.nytimes.com/2023/11/18/technology/open-ai-sam-altman-what-happened.html">OpenAI 争议</a>中看到了类似的争论，其中涉及许多围绕超级人工智能的危险以及 OpenAI 发展太快的可能性的讨论。</p></blockquote><blockquote><p>我自己对技术乐观主义的感受是温暖的，但又微妙。我相信，由于彻底变革的技术，未来将比现在更加光明，并且我相信人类和人性。我拒绝这样的心态，即我们应该尽力做的最好的事情就是让世界与今天大致相同，但减少贪婪和更多的公共医疗保健。然而，我认为不仅大小很重要，方向也很重要。某些类型的技术比其他类型的技术更可靠地让世界变得更好。某些类型的技术如果得到开发，可以减轻<i>其他</i>类型技术的负面影响。世界对某些技术发展方向的指数过高，而对另一些方向的指数偏低。我们需要人类主动的意图来选择我们想要的方向，因为“利润最大化”的公式不会自动到达它们。 </p></blockquote><blockquote><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/rhs2s2alaz8hfjreeixc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/dmnmhubhvqynm2jtbqsx 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/bisvfuffofmuca8ejrwv 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/nuog1z8fci3kyalupwv5 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/ehxtebyixjfsf4dmudy6 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/ipnkwkzl7zfghazcqw2w 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/xixp1awu5pt1fjwbjqug 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/zleh0jfmepgch4qqeusm 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/qudw4fbecck7xqks3eur 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/mvzbz0gueha0lpfyrp6a 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kWoPxh9DeQhc7HJo5/edyc527kb9naobdsax2j 1496w"></figure></blockquote><blockquote><p>在这篇文章中，我将讨论技术乐观主义对<i>我</i>意味着什么。这包括更广泛的世界观，它激励我在某些类型的区块链和密码学应用程序和社会技术以及我表示感兴趣的其他科学领域开展工作。但对这个更广泛问题的看法也对人工智能和许多其他领域产生影响。我们技术的快速进步可能会成为二十一世纪最重要的社会问题，因此仔细思考它们很重要。</p></blockquote><h2><strong>目录</strong></h2><ul><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#tech">技术是惊人的，延迟它的成本非常高</a><ul><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#environment">环境和协调意图的重要性</a></li></ul></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#ai">人工智能与其他技术有着根本的不同，值得特别小心</a><ul><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#xrisk">存在风险是一件大事</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#superintgood">即使我们生存下来，超级人工智能的未来是我们想要生活的世界吗？</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#emperor">天已近，帝王无处不在</a></li></ul></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#otherproblems">我担心的其他问题</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#dacc">d/acc：防御（或分散，或差异）加速度</a><ul><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#macro">宏观物理防御</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#micro">微物理防御（又称生物防御）</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#cyber">网络防御、区块链和密码学</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#info">信息防御</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#social">超越“防御”框架的社会技术</a></li></ul></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#paths">那么超级智能的前进道路是什么？</a><ul><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#merge">一条快乐的道路：与人工智能合并？</a></li></ul></li><li> <a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#compatible">d/acc 与您现有的理念兼容吗？</a></li><li><a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html#brighteststar">我们是最亮的星</a></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/kWoPxh9DeQhc7HJo5/my-techno-optimism-by-vitalik-buterin#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kwopxh9deqhc7hjo5/my-techno-optimism-by-vitalik-buterin<guid ispermalink="false"> KWOPXH9DEQHC7HJO5</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 27 Nov 2023 23:53:37 GMT</pubDate> </item><item><title><![CDATA[Could Germany have won World War I with high probability given the benefit of hindsight?]]></title><description><![CDATA[Published on November 27, 2023 10:52 PM GMT<br/><br/><p>相关： <a href="https://www.lesswrong.com/posts/PBNpadZKBGFf6Taa3/could-world-war-i-have-been-prevented-given-the-benefit-of">鉴于事后看来的好处，世界大战是否可以阻止我？</a></p><p>考虑以下情况：在第一次世界大战开始之前的几年中，您会被送回，例如1904年以进行具体性，目的是在战争之前和战争中改变德国帝国战略，以使德国的可能性很高，以很高的可能性[这意味着这意味着这意味着在整个时间表的许多小偏差中，包括敌人战略反应功能的微小变化，其中至少有95％导致奥澳大利亚人至少击败了至少法国和俄罗斯，占领了许多新领土，并在该领域建立了一个德国帝国世界舞台]</p><p>这包括赢得类似的战争，这些战争稍早或更晚 - 您的目标是为德国和奥匈帝国制定持久的战略，使他们赢得了对立的欧洲大国。</p><p>为了防止愚蠢的策略，将要改变世界的次要细节，例如日期和名称可能会略有不同。</p><p>此外，您带有一些非凡地位的证据，这对奥匈帝国和德国的领导人来说是可信的。他们会给您一个听众，他们听到您的耐心非常大，但他们不会盲目信任您。您需要根据理由说服他们接受。您可能对他们撒谎或告诉他们真相。我还将规定，如果您设法说服计划的领导，他们实际上会以异常高的狂热和连贯性来遵循它。</p><p>您没有针对1914年后技术的详细设计，但是您对外行人对实际历史的理解有普通的知情。因此，您可以告诉他们将发明核炸弹，但不完全是什么，他们可能会或可能不相信您。</p><p>请回答您的策略，以及为什么您认为它会起作用（包括面对主要参与者的可能反应）！</p><br/><br/> <a href="https://www.lesswrong.com/posts/f2LpBZBBdmm7B2mXT/could-germany-have-won-world-war-i-with-high-probability#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/f2lpbzbbdmm7b2mxt/could-germany-have-won-world-world-world-war-i-with-probrobise<guid ispermalink="false"> F2LPBZBDMM7B2MXT</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Mon, 27 Nov 2023 22:52:43 GMT</pubDate> </item><item><title><![CDATA[Could World War I have been prevented given the benefit of hindsight?]]></title><description><![CDATA[Published on November 27, 2023 10:39 PM GMT<br/><br/><p>考虑以下情况：在第一次世界大战开始之前的几年中，您会被及时送回，目的是防止战争。这包括防止类似的战争，这些战争稍早或稍晚，您的目标是在欧洲大国之间建立和平，稳定的共存。诸如警告大公不要去萨拉热窝的策略可能会导致另一个类似的贝利发生。</p><p>为了防止愚蠢的策略，将要改变世界的次要细节，因此日期和名称可能会略有不同，但是整个第一次世界大战路径的发展几乎将与我们的时间表一样。</p><p>此外，您带有一些非凡地位的证据，这是第二次世界大战国家领导人可信的。他们会给您观众，但他们不会盲目信任您。您可能对他们撒谎或告诉他们真相。</p><p>您没有针对1914年后技术的详细设计，但是您对外行人对实际历史的理解有普通的知情。因此，您可以告诉他们将发明核炸弹，但不完全是什么，他们可能会或可能不相信您。</p><p>请回答您从第一次世界大战中拯救欧洲的策略，以及为什么您认为它会起作用（包括面对主要参与者的可能反应）！</p><br/><br/> <a href="https://www.lesswrong.com/posts/PBNpadZKBGFf6Taa3/could-world-war-i-have-been-prevented-given-the-benefit-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pbnpadzkbgfff6taa3/could-world-warl-war-i-have-been-per-prevent--------------<guid ispermalink="false"> pbnpadzkbgff6taa3</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Mon, 27 Nov 2023 22:39:17 GMT</pubDate> </item><item><title><![CDATA[AISC 2024 - Project Summaries]]></title><description><![CDATA[Published on November 27, 2023 10:32 PM GMT<br/><br/><p>在2024年12月1日之前<i>申请</i><a href="https://aisafety.camp/"><i>AI安全训练营。</i></a><i>这是我自己的所有错误。</i></p><p>以下是每个项目提案的一些摘要，按照网站上的出现顺序列出。这些是由我编辑的，大多数人尚未受到项目潜在客户的审查。我认为拥有这样的列表使人们更容易浏览所有不同的项目，因此我添加了一个。</p><p>如果一个项目引起了您的兴趣，请单击标题以了解有关它的更多信息。</p><p><strong><u>请注意</u></strong>，这里的摘要是有损的。这里可能会歪曲所需的技能，如果您有兴趣，则应检查原始项目以获取更多详细信息。特别是，许多“期望的技能”通常被列出，以便只有少数几个会有所帮助，但这并不一致。</p><p></p><h1> AISC项目列表</h1><h1><strong>不要构建无法控制的AI</strong></h1><h2> <strong>1.</strong> <a href="https://docs.google.com/document/d/1ROdSPA5TaJDe18pK59YUyEnv0qqUqkEbrUezLamcn54/edit?usp=sharing"><strong>对基础模型的AI产品造成现实赔率</strong></a></h2><p><strong>项目负责人：</strong> Igor Krawczuk</p><p><strong>目标：</strong>应用于语言模型的当前对齐方式类似于“黑名单”行为不好。相反，<a href="https://link.springer.com/chapter/10.1007/978-3-030-98464-9_16"><u>操作设计域</u></a>（ <a href="https://www.sae.org/standards/content/j3016_201806/">OOD</a> ）类似于更精确的“白名单”设计原理，现在允许与此偏离。该项目希望建立概念证明，并表明这是可行的，经济和有效的。</p><p><strong>团队</strong>（寻找4-6个人） <strong>：</strong></p><ul><li> <strong>“规格研究人员”</strong> ：起草指南规格，并发布评论请求。应该有安全设置的经验</li><li><strong>“采矿研究人员”</strong> ：寻找用例，并起草OOD的“切片”。</li><li> <strong>“用户访问研究人员”：</strong>写有关KYC和用户访问级别的可行性的草稿。</li><li> <strong>“ Lit Review研究人员”：</strong>阅读有关ML高保证方法的最新相关文献。</li><li> <strong>“概念证明研究员”</strong> ：建立概念证明。应该了解OpenAI并与/架构API进行接口。</li></ul><p></p><h2> <strong>2.</strong> <a href="https://docs.google.com/document/d/1KeO_zaTDMSGKKYHW_TiAQdvrvvJkKy5tCINV8zE7oM8/edit"><strong>Luddite Pro：精制的Luddite的信息</strong></a></h2><p><strong>项目负责人：</strong> Brian Penny</p><p><strong>目标：</strong>开发一个新闻网站，其中包含与社会人工智能发展有关的故事，信息和资源。涵盖与该行业和广泛兴趣有关的特定故事（例如： <a href="https://luddite.pro/adobe-firefly-is-out-of-beta-stock-contributors-were-paid/"><u>Adobe的萤火虫支出</u></a><u>，</u> <a href="https://luddite.pro/the-lost-penny-files-midjourneys-beginning/"><u>Midjourney的开始</u></a><u>，</u><a href="https://luddite.pro/ai-deepfakes-and-undress-apps-make-the-internet-unsafe/"><u>脱衣服和Deepfake应用程序的扩散</u></a>）。提供<a href="https://luddite.pro/resources-for-human-creatives/"><u>有价值的资源</u></a>（例如：AI， <a href="https://luddite.pro/essential-luddite-reading-list-31-books-to-learn-more-about-tech-and-power/"><u>书籍列表</u></a>中<a href="https://luddite.pro/human-wisdom-on-artificial-intelligence/"><u>的专家清单</u></a>以及给<a href="https://luddite.pro/public-submission-for-usco-generative-ai-call-for-comment/"><u>USCO</u></a>和<a href="https://luddite.pro/open-letter-creatives-refusing-generative-ai-demand-seat-at-table-from-us-congress/"><u>国会的</u></a>预制信/评论）。目的是通过社交媒体传播并在搜索引擎中排名，同时引发团体行动，以确保在每个人的眼中都有道德和安全AI的叙事。</p><p><strong>所需的技能</strong>（以下任何一种）：</p><ul><li><strong>艺术，设计和摄影</strong>- 开发视觉内容，以作为每个故事的标题图像。如果您有任何视觉设计技能，则非常必要。</li><li><strong>新闻学</strong>- 新闻和研究背景，能够采访主题专家并撰写与AI公司有关的长篇故事。</li><li><strong>技术写作</strong>- 釉和夜幕式技术工具的教程。技术写作的经验并熟悉这些应用程序。</li><li> <strong>WordPress/Web开发</strong>- 完善页面更具用户友好，并帮助设置模板供人们填写呼吁采取行动。当前，该站点正在运行默认的WordPress模板。</li><li><strong>营销/公关</strong>- 该网站充满了内容，但需要大量的营销和公关工作来吸引目标受众。如果您有在代理商或内部营销/通讯中工作的任何经验，我们很乐意收到您的来信。</li></ul><h3></h3><h2> <strong>3.</strong> <a href="https://docs.google.com/document/d/1fblXOOz4KLn8EA1omhrwdIUJYB9NJJ7LxUdRPGDWvXo/edit"><strong>限制AI数据洗涤的律师（和编码人员）</strong></a></h2><p><strong>项目负责人：</strong> Remmelt Ellen</p><p><strong>目标：</strong>生成的AI依赖于洗涤大量数据。对洗钱的公司的法律禁令受版权保护的数据使他们的培训和部署大型模型。创意权利联盟是一个由艺术家，作家，编码人员和ML研究人员组成的地下联盟。我们需要律师。热衷于保护社会免受（当前和未来）危害的律师。</p><p><strong>团队</strong><u>（最多寻找5个人）</u> <strong>：</strong></p><ul><li><strong>律师</strong>：档案DMCA撤销请求，欧盟诉讼的前研究。应具有法律教育（法学硕士）和国际版权和/或数据保护实践的基本知识。</li><li><strong>编码器：</strong>创建一个工具来检查创建者的作品是否在AI数据集中。应该能够改善这些<a href="https://github.com/alexjc/document-training-data">GitHub</a><a href="https://github.com/alexjc/weboptout">存储库</a>中的代码。</li></ul><p></p><h2> <strong>4.</strong> <a href="https://docs.google.com/document/d/1vZMMr7gSNNmlVI1C3ouagcbOKTcRmVfQ0L05DbCZf6Q/edit?usp=sharing"><strong>评估国会通讯运动的潜力</strong></a></h2><p><strong>项目负责人：</strong>特里斯坦·威廉姆斯</p><p><strong>目标：</strong>弄清楚国会消息传递运动（CMC）是否有效，如果他们这样做，AI关注的信息要推广以及如何以高质量的方式推广它们。研究一般CMC的有效性并撰写报告。如果一切顺利，请扩展研究以制定为AIS部署CMC的最佳策略。时间允许，采取调查结果并部署最佳策略，试图帮助对较少参与的人的AI风险进行可行的步骤填补空白。</p><p><strong>所需的技能</strong>（寻找2-5个人） <strong>：</strong></p><ul><li><strong>通才研究技能</strong></li><li><strong>沟通技巧</strong>（向各种组织推广）</li><li><strong>写作技巧</strong>（使报告访问）</li><li><strong>网页设计</strong>（可能测试消息传递方法）</li><li><strong>政策制定</strong>（深入了解国会办公室如何工作是理想的）</li><li> <strong>CMC经验</strong>。</li></ul><h1></h1><h1><strong>机械解干性</strong></h1><h2><strong>5.</strong> <a href="https://docs.google.com/document/d/1pAEnfUza987OMmt8fnXuOpGO9MykoVgDLcT3V48IkaY/edit?usp=sharing"><strong>语言模型的建模轨迹</strong></a></h2><p><strong>研究负责人：</strong> <a href="https://lesswrong.com/users/nicky"><strong>Nicky Pochinkov</strong></a> （我！）</p><p><strong>目标：</strong>而不是问“语言模型预测的下一步标记？”或“ RL代理会采取什么下一步操作？”，我认为能够对模型的长期行为进行建模，而不仅仅是立即的令牌或动作，这一点很重要。我认为可能存在参数和计算有效的方法来总结在给定输入及其激活的情况下，模型可能输出的长期轨迹/输出。</p><p><strong>团队</strong>（寻找2-4个人）：</p><ul><li><strong>理论家：</strong>概念化最佳方法将长轨迹汇总到“主题链”中，思考如何衡量不确定性，试图理解“目标”。数学/物理背景是理想的。</li><li><strong>软件工程师：</strong>编写代码将文本世代转换为“主题链”。编写模型将模型神经元激活转换为主题链的预测。</li><li><strong>蒸馏器：</strong>阅读和理解材料。将杂乱的语言和实验从其他人转换为更易于理解和易于阅读的形式。</li></ul><p></p><h2> <strong>6.</strong> <a href="https://docs.google.com/document/d/1jce3f64Fz7PXmdCEyd9i0PTmcFaiP1pZdcBn5ye5sxY/edit?usp=sharing"><strong>雄心勃勃的机械性解释性</strong></a></h2><p><strong>项目负责人：</strong>爱丽丝·里格（Alice Rigg）</p><p><strong>目标：</strong>变压器能够完成各种任务，在大多数情况下，我们对如何了解。通过自下而上的方法，机械性解释性被认为是解决此问题的AI安全议程。我们从低水平的组件开始，并建立对最有能力的系统在内部运行方式的理解。但是，为了使机械性解释性作为AI安全议程是合理的，它需要<u>雄心勃勃</u>。该项目的目的是：1）在质量与解释的现实主义上推动帕累托前沿。 2）更好的自动解释性和比例功能解释。 3）改进测量解释质量的指标</p><p><strong>所需的技能</strong>（最多寻找4人）：</p><ul><li><strong>软件工程</strong>：Pytorch的熟练程度。有变压器的经验。熟悉现有的可解释性工作的熟悉。</li></ul><h3></h3><h2> <strong>7.</strong> <a href="https://docs.google.com/document/d/19Lu2NDVJN-7ZdA-cQuLcqTuWJSr7SA88ZLAM8H7Qgzs/edit"><strong>探索代理商的玩具模型</strong></a></h2><p><strong>项目负责人：</strong> Paul Colognese，Arun Jose</p><p><strong>目标：</strong>有助于发展目标理论，该理论可能会导致未来的客观检测方法，以帮助解决内部的一致性问题。这将涉及：1）构建代理商的玩具模型集合。 2）开发基于探测的基础架构，以探索这些模型中的目标/目标信息。 3）使用此基础架构进行经验分析。 4）总结并写出任何有趣的发现。</p><p>该项目可能看起来像扩展这项工作： <a href="https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network"><u>将迷宫策略网络理解和控制</u></a>到新的模型和环境。</p><p><strong>所需的技能（</strong>最多寻找3个人） <strong>：</strong></p><ul><li><strong>软件工程：</strong>熟悉Python和Pytorch。具有GitHub/与其他工程师合作的经验。</li></ul><p></p><h2> <strong>8.</strong> <a href="https://docs.google.com/document/d/1DbRHPBlFUFojiEEGJnI0ghNOWnE5bOZBNXC1fC51T9I/edit#heading=h.jepgk0u8wx8p"><strong>高级机械性解释性和激活工程库</strong></a></h2><p><strong>项目负责人：</strong>杰米·库姆斯（Jamie Coombes）</p><p><strong>目标：</strong>缺乏统一的软件工具和标准化界面会导致重复的工作，因为研究人员建立了各种机甲Interp方法的一次性实现。现有的库涵盖了浅层学习模型的一系列可解释的AI方法。但是，当代对大型神经网络的研究需要新的工具。该项目旨在建立一个专门针对机械性可解释性和激活工程技术的技术库。</p><p><strong>所需的技能</strong>（最多寻找5人） <strong>：</strong></p><ul><li><strong>软件工程：</strong> <a href="https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks"><u>Pytorch</u></a> ，Jupyter和Data Science Workflows的经验。熟悉变压器体系结构和LLM是理想的选择。</li><li><strong>写作技巧：</strong>研究和学术写作技巧，以帮助记录方法和出版结果。</li><li>低级系统编程专业知识，以快速提高MOJO进行关键性的操作。这绝不是强制性的，但是加号。</li></ul><p></p><h2> <strong>9.</strong> <a href="https://docs.google.com/document/d/1qs0v6emq3Sn1UbDbstmsz7rzT38qklvIl4q3gw9lvaU/edit#heading=h.9lmc73wscx1r"><strong>神秘的学习解释性</strong></a></h2><p><strong>项目负责人：</strong> VíctorLevosoFernández</p><p><strong>目标：</strong>几个月前，发表了一篇题为<a href="https://openreview.net/forum?id=X3JFgY4gvf"><u>《大语言模型》中题为《元素外元学习》的</u></a>论文，谈到了一种名为《外在元学习》的现象。最近，还有其他有关相关主题的论文，例如<a href="https://arxiv.org/pdf/2309.00667.pdf"><u>《脱离上下文：测量LLMS中的情境意识》</u></a>或关于模型的失败，以使这种方式概括这种方式，例如<a href="https://owainevans.github.io/reversal_curse.pdf"><u>逆转诅咒纸</u></a>。所有这些论文的共同点是，这些模型学会应用在另一种情况下培训期间学到的事实。该项目的目的是利用对玩具任务的机械性可解释性研究来理解电路和培训动态，这种学习和概括如何在模型中发生。</p><p><strong>所需的技能（</strong>寻找3-5个人） <strong>：</strong></p><ul><li><strong>软件工程：</strong>基本CS和编码技能。了解ML和或拥有Pytorch技能有帮助。</li><li><strong>数学知识</strong></li></ul><p></p><h2><strong>10.</strong> <a href="https://unsearch-ai.notion.site/AISC-2024-Research-Proposal-5adbd5918fe443c491a0a5b4252113fc"><strong>了解变压器中的搜索和目标表示</strong></a></h2><p><strong>项目负责人：</strong> Michael Ivanitskiy（+TilmanRäuker，Alex Spies。请参阅<a href="http://www.unsearch.org"><strong>网站</strong></a>）</p><p><strong>目标：</strong>该项目旨在为这一不断增长的工作做出贡献，并特别着重于在变压器模型中如何处理<a href="https://www.alignmentforum.org/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see">内部搜索</a>和目标表示形式（以及它们是否存在！）。特别是，我们从现有的<a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">机械</a><a href="https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj/p/GWCgZrzWCZCuzGktv">可解释性</a><a href="https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">议程</a>中汲取灵感，并与经过训练的迷宫训练的玩具变压器模型合作。坚固地解决迷宫是一项任务，可能需要某种内部搜索过程，并且在探索分配变化如何影响绩效方面具有很大的灵活性 -<a href="https://www.alignmentforum.org/posts/FDjTgDcGPc7B98AES/searching-for-search-4">了解搜索</a>和学习以控制<a href="https://arxiv.org/abs/1906.01820">MESA-OPTIMIZES</a>对AI系统的安全很重要。</p><p><strong>所需的技能</strong>（寻找至少1-2个人） <strong>：</strong></p><ul><li><strong>软件工程：</strong> Pytorch/Tensorflow/jax的熟练程度。了解变压器。基本熟悉对齐方式。</li></ul><p></p><h1>评估和转向模型</h1><h2><strong>11.</strong> <a href="https://docs.google.com/document/d/1N0Kcvu3aNTUWTXMi3rE9PrrRVcF4soR6ucgq4ej0g8I/edit#heading=h.9lmc73wscx1r"><strong>稳定反射率的基准</strong></a></h2><p><strong>项目负责人：</strong>雅克·蒂博多（Jacques Thibodeau）</p><p><strong>目标：</strong>未来的Prosaic AI可能会塑造自己的发展或继任AI的发展。我们正在尝试确保他们不会发疯。 AIS可以通过两种主要方式来表现更好：通过改进其培训算法或改善其培训数据。我们认为场景和初步认为基于数据的改进比基于建筑的改进更风险。为了使AIS的监督改善AIS议程，我们专注于确保AIS自我培训或培训新的AIS并研究AIS如何通过迭代培训进行稳定的对齐方式。我们旨在开发方法以确保自动化科学过程保持安全和可控。与建筑或规模驱动的形式相比，这种AI改进形式更着重于数据驱动的改进。</p><p><strong>所需的技能</strong>（寻找2-4个人） <strong>：</strong></p><ul><li><strong>软件工程：</strong> Python的经验。好的软件工程师或对AI对齐和语言模型的基础知识的体面理解。</li><li><strong>理想：</strong>了解在线/积极学习ML系统。使用语言模型创建数据集。无监督的学习技术。数据管道的代码（用于基准测试）可以轻松集成到AI培训中。</li><li>了解自我提高AI系统如何发展并了解我们试图跟踪以防止危险系统的所有功能。</li></ul><p></p><h2> <strong>12.</strong> <a href="https://docs.google.com/document/d/1WARwqgH9UAIri2I9215OoLZPTJdq4evLPN2wgs0hP3Y/edit#heading=h.9lmc73wscx1r"><strong>SADDER：用于检测极端风险的情境意识数据集</strong></a></h2><p><strong>项目负责人：</strong>鲁道夫·莱恩</p><p><strong>目标：</strong>一个令人担忧的能力AIS可以提高情境意识。特别是，诸如成功欺骗性的AIS和自主复制和适应之类的威胁模型似乎取决于较高的情境意识。 SADDER的目的是通过运行实验并构建EVALS来更好地了解当前LLM中的情境意识。它将建立在<a href="https://drive.google.com/file/d/1-zY9dbh8fKO1G9S2V0_gMVVh7jSUuJ7d/view?usp=sharing"><u>情境意识数据集（SAD）的</u></a>基础上，该数据集基于LLMS对他们如何影响世界的理解，并通过运行更深入的深度来猜测给定文本摘录的生命周期阶段的能力可能来自实验并添加更多类别。</p><p><strong>所需的技能</strong>（最多寻找2个人） <strong>：</strong></p><ul><li> <strong>AI安全知识：</strong>一般意识和对AI安全的判断。</li><li><strong>软件工程：</strong>更多的概念人员应该能够进行简单的数据科学工作流（例如编写Python到图形结果）。更多的工程人员应显示您​​编写的结构良好的清晰代码项目，或者是强大的软件工程（例如实习/工作）的记录。</li><li><strong>实验推理技能</strong>：您能想到结果可能是错误或误导的方法，并发明了在假设之间消除歧义的实验？</li><li> （理想情况下）：现有的研究经验。</li></ul><p></p><h2> <strong>13.</strong> <a href="https://docs.google.com/document/d/1myBsd-LqTaEWOqZwqOX3UBphOGiEdlANDtOA1a-lFFg/edit#heading=h.9lmc73wscx1r"><strong>Tinyevals：语言模型如何说英语？</strong></a></h2><p><strong>项目负责人：</strong><a href="https://www.lesswrong.com/users/jett">杰特·贾尼亚克（Jett Janiak）</a></p><p><strong>目标：</strong><a href="https://arxiv.org/abs/2305.07759"><u>小故事</u></a>是一套小语言模型（SLM），该套件专门针对Chatgpt产生的儿童故事培训。 The models use simple, yet coherent English, which far surpasses what was previously observed in other models of comparable size. I hope that most of the capabilities of these models can be thoroughly understood using currently available interpretability techniques. Doing so would represent a major milestone in the development of mechanistic interpretability (mech interp). The goal of this AISC project is to publish a paper that systematically identifies and characterises the range of capabilities exhibited by the TinyStories models.</p><p> <strong>Desired Skills (</strong> looking for 2-4 people) <strong>:</strong></p><ul><li> <strong>Software Engineering:</strong> Knowledge of python, jupyter notebooks, git.<ul><li> <strong>Nice to have:</strong> PyTorch, HuggingFace, TransformerLens, Plotly, Mech interp experience, Research experience</li></ul></li><li> <strong>Technical writing ability</strong></li><li> <strong>Web development:</strong> HTML, CSS, JavaScript</li></ul><p></p><h2> <strong>14.</strong> <a href="https://docs.google.com/document/d/1hLdNZhzQgGSRDIdulHi2lZjM159iyYcZ5PFJNAPma1s/edit#heading=h.9lmc73wscx1r"><strong>Evaluating alignment evaluations</strong></a></h2><p> <strong>Project Lead:</strong> Maxime Riche</p><p> <strong>Goal:</strong> Alignment evaluations are used to evaluate LLM behavior on a wide range of situations. They are especially used to evaluate if LLMs write harmful content, have dangerous preferences, or obey to malevolent requests. Several alignment/behavioural evaluation techniques have been published or suggested (eg: Self-reported preferences Inference from question answering, playing games, or looking at internal states. Behaviour evaluation under steering pressure.) This project aims to review and compare existing alignment evaluations to assess their usefulness. Optionally, we want to discover better alignment evaluations or improve the existing ones.</p><p> <strong>Desired Skills (</strong> looking for 2-4 people) <strong>:</strong></p><ul><li> Reading Python code.</li><li> Having used LLM and knowing about their completion patterns.</li><li> Data science skills.</li></ul><h3></h3><h2> <strong>15.</strong> <a href="https://docs.google.com/document/d/1P-SrvH9V8IGa_rP_c7uNkobIcQ5LXWrLouJER2NnAE4/edit#heading=h.9lmc73wscx1r"><strong>Pipelines for evaluating and steering LLMs towards faithful reasoning</strong></a></h2><p> <strong>Project Lead:</strong> Henning Bartsch</p><p> <strong>Goal:</strong> The research project focuses on language model alignment by developing and testing techniques for (1) evaluating model-generated reasoning and (2) steering them towards more faithful behaviour. It builds on findings and future directions from scalable oversight, model evaluations and steering techniques.</p><p> The core parts are to: 1) Benchmark closed- and open-source LLMs on faithful reasoning. 2) Build ONE pipeline to generate a dataset for fine-tuning a LLaMA model. 3) Compare the effects of fine-tuning and test-time steering on faithfulness. 4) Analyse the model behaviour and results.</p><p> <strong>Desired Skills</strong> (looking for 3-5 people with diverse skillset) <strong>:</strong></p><ul><li> <strong>Software Engineering:</strong> Expertise in Python, software development practices, and the ability to create effective abstractions for classes and pipelines.</li><li> <strong>Research</strong> <strong>and conceptual work:</strong> Skills in experimental design, generating ideas and the capacity to effectively communicate new ideas within the team. We also need to analyse and interpret results and write up a paper.</li><li> <strong>Quick Prototyping:</strong> implementation and validation of ideas are important, especially in the early stage of the project we want to test and iterate quickly.</li><li> <strong>Familiarity with Concepts:</strong> Understanding of key AI safety ideas, and other ideas in the proposed document is important.</li></ul><p></p><h2> <strong>16.</strong> <a href="https://docs.google.com/document/d/1VYWKz_Oly6vtJKaA4iXWzKLDpNhI7pJ_h-3FXw08JHQ/edit#"><strong>Steering of LLMs through addition of activation vectors with latent ethical valence</strong></a></h2><p> <strong>Project Lead:</strong> Rasmus Herlo</p><p> <strong>Goal:</strong> The idea is to identify crucial modules and activations points in LLM-architectures that are associated with positive or negative ethical valence by caching the activations during forward passes induced by specifically developed binary ethical prompts. The identified linear subspaces following serve as intervention points for direct steering through activation addition. The ultimate hope is that these adjustments immediately generate a modified LLM architecture that complies better with ethical guidelines by default without the need of adjustment modules, as used in methods like RLHF.</p><p> <strong>Team (looking for 3-4 people):</strong></p><ul><li> <strong>Code Architects</strong> (2-3x): Modify LLM-structure according to devised intervention points, and generate/save at least three altered versions of the LLM-architecture. Should be Proficient in Python and Git/GitHub.<ul><li> Scientific communication,<strong> </strong>Data-caching, figure production skills ideal.</li></ul></li><li> <strong>Ethics Consultant (</strong> 1x): Will help develop and design binary ethical prompts for LLMs, and the protocols to test the LLMs before/after ethical steering in both ethical and unethical direction. Reading of relevant literature.<ul><li> Experience with scientific methodology and producing ethical literature to a high scientific standard. (eg: philosophy, psychology or anthropology). Should understand philosophical concepts like utilitarianism and &#39;trolley problems&#39;.</li></ul></li></ul><p></p><h1> <strong>Agent Foundations</strong></h1><h2> <strong>17.</strong> <a href="https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit#heading=h.zat5vvnwtl0j"><strong>High actuation spaces</strong></a></h2><p> <strong>Project Lead:</strong> Sahil</p><p> <strong>Goal:</strong> This project is an investigation into building a science of almost-but-not-actually magical regimes. Spaces where actuation is extremely cheap and fast, but not free and instantaneous.一些例子： <strong>&nbsp;</strong> biochemical signalling, the formation of social structures, decision theory. The hope is to be able to articulate many general and often counterintuitive facts and confusions about the insides of mind-like entities in general, including ones that exist already and apply it to fundamental problems in the caringness of an AI, like value-loading/ontological identification/corrigibility. You might call this a “deconfusion” project along the above lines.</p><p> <strong>Desired skills</strong> (looking for 2-4 people):</p><p> People at the intersection of:</p><ul><li> of math and philosophy</li><li> of prosaic alignment/modern ML and agent foundations</li><li> of computer science and <a href="https://www.pibbss.ai/"><u>biological/sociological lenses</u></a></li><li> of rigor and ritual</li><li> of material and phenomenological investigation</li><li> of systematic and postsystematic modes</li><li> of strong agreement and subtle disagreement with MIRI-esque views on alignment</li><li> of intrigue and skepticism around shard theory </li></ul><p><strong>&nbsp;</strong></p><h2> <strong>18.</strong> <a href="https://docs.google.com/document/d/1VGmOim2pGm8NZMFQSaBpEXXI9AcmRcai1viq33_njQs/edit#heading=h.9lmc73wscx1r"><strong>Does sufficient optimization imply agent structure?</strong></a></h2><p> <strong>Project Lead:</strong> Alex Altair</p><p> <strong>Goal:</strong> There is an intuition that if a system is capable of reliably achieving a goal in a wide range of environments, then it probably has certain kinds of internal processes, like building a model of the environment from input data, generating plans, and predicting the effects of its actions on the future states of the environment. That is, it probably has some modular internal structure. To what degree can these intuitions be formally justified? Can we <i>prove</i> that reliable optimization implies <a href="https://www.lesswrong.com/posts/moi3cFY2wpeKGu9TT/clarifying-the-agent-like-structure-problem">some kind of agent-like structure</a> ?  I think one could make significant progress toward clarifying the parts, or showing weaker results for some of the parts.</p><p> <strong>Desired Skills (</strong> looking for 1-3 people) <strong>:</strong></p><ul><li> Trying to locate the right definitions of things like “agent” and “plan”, and for proofs to follow more easily.</li><li> Familiarity with any topics that could be candidates for formalizing the theorem.</li><li> A solid grasp of how mathematical formalisms relate to reality, and how proofs work.</li></ul><p><br></p><h2> <strong>19.</strong> <a href="https://docs.google.com/document/d/1Xqmh9By3yZGVWLgvHWxl4saw2u4S71GouOU_ms9UoqY/edit"><strong>Discovering agents in raw bytestreams</strong></a></h2><p> <strong>Project Lead:</strong> Paul Bricman</p><p> <strong>Goal:</strong> Being able to identify and study agents is a recurring theme in many alignment proposals, ranging from <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation"><u>eminently theoretical</u></a> to <a href="https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/"><u>directly applicable</u></a> ones. <a href="https://www.deepmind.com/blog/discovering-when-an-agent-is-present-in-a-system"><u>Previous work</u></a> paved the way for agent discovery from observations, but required an explicit decomposition of the world into variables, as well as additional scaffolding. This project consists of working towards a pipeline for detecting agency in raw byte-streams with no hints as to the nature of the agents to be detected. This could eventually enable the quantification of gradient hacking and mesa-optimization.</p><p> <strong>Team</strong> (looking for 2 people) <strong>:</strong></p><ul><li> <strong>[Both] Software Engineering</strong> . Python fluency, designing maintainable codebases, familiarity with JAX. Strong technical writing is a plus.</li><li> <strong>[Paper 1] Differentiable Correlates of Complexity and Sophistication.</strong><ul><li> 1) Benchmark estimates of complexity and sophistication employed in the broader agency detection pipeline. 2) Facilitate the integration of agent foundations with contemporary differentiable architectures more broadly.</li></ul></li><li> <strong>[Paper 2] Recovering Agents in Gym Environments.</strong><ul><li> Attempt to recover RL agents that were previously introduced in certain gym environments. Test if the method can correctly identify agents. Investigate how agent hyperparameters influence the detection of agents.</li></ul></li></ul><p></p><h2> <strong>20.</strong> <a href="https://docs.google.com/document/d/1GoXaYtUyanRrkBcjAknafqdKDCyTzzgxpckYMBtgPkQ/edit#heading=h.9lmc73wscx1r"><strong>The science algorithm</strong></a></h2><p> <strong>Project Lead:</strong> Johannes C. Mayer</p><p> <strong>Goal:</strong> Modern deep learning is about having a simple program (SGD) search over a space of possible programs (the weights of a neural network) and select one that performs well according to a loss function. Even though the search program is simple, the programs it finds are neither simple nor understandable.</p><p> My goal is to build an AI system that enables a <a href="https://arbital.com/p/pivotal/"><u>pivotal act</u></a> by figuring out the algorithms of intelligence directly. The ideal outcome is to be able to write down the entire pivotal system as a non-self-modifying program explicitly, similar to how I can write down the algorithm for quicksort.</p><p> <strong>Desired Skills</strong> (Looking for 2-3 people) <strong>:</strong></p><ul><li> <strong>Ability to think and work independently:</strong> You should be able to generate, evaluate, and execute upon ideas without the need for constant oversight. Recognise and challenge things if I say something questionable.</li><li> <strong>Software Engineering Skills:</strong> Writing programs of 100s lines of code, Thinking about how to structure a codebase. Reimplementing/coming up with algorithms. Writing parallelisable code.</li><li> <strong>Basic math skills:</strong> know what these symbols mean ∑, Π, ×, ∧, ∨. Able to proof simple statements, etc.</li></ul><p></p><h1> <strong>Miscellaneous Alignment Methods</strong></h1><h2> <strong>21.</strong> <a href="https://docs.google.com/document/d/1JhmK31IwYGcwqX0nKmxKsbmTh_DX3o1OoW7NJmhVbIw/edit#heading=h.l9ot7n22b6ry"><strong>SatisfIA – AI that satisfies without overdoing it</strong></a></h2><p> <strong>Project Lead:</strong> Jobst Heitzig</p><p> <strong>Goal:</strong> Explore novel designs for generic AI <i>agents –</i> AI systems that can be trained to act autonomously in a variety of environments – and their implementation in software. We will study several versions of such “non-maximizing” agent designs and corresponding learning algorithms. Rather than aiming to maximize some objective function, our agents will aim to fulfill goals that are specified via constraints called “aspirations”. For example, I might want my AI butler to prepare 100–150 ml of tea, having a temperature of 70–80°C, taking for this at most 10 minutes, spending at most $1 worth of resources, and succeeding in this with at least 95% probability.</p><p> <strong>Desired Skills</strong> (looking for 3 people):</p><ul><li> <strong>Reinforcement Learning:</strong> Solid knowledge of tabular RL or deep RL desirable.</li><li> <strong>Probability Theory:</strong> Designing (Markov decision process) agents and algorithms</li><li> <strong>Software Engineering:</strong> implementing agents in software (Python/WebPPL), simulating their behaviour in selected test environments (AI safety grid-worlds),</li><li> <strong>Formulating hypotheses</strong> about agent behaviour, especially about its safety-relevant consequences, then trying to prove/disprove these hypotheses</li><li> <strong>Writing Skills:</strong> writing results in blog posts and (possibly) an academic paper.</li></ul><p></p><h2> <strong>22.</strong> <a href="https://docs.google.com/document/d/1cXU-DoE2O2vLhVBRWYFBcXyyOuvwVQx97wwtMroJzZU/edit#heading=h.9lmc73wscx1r"><strong>How promising is automating alignment research? （文献综述）</strong></a></h2><p> <strong>Project Lead:</strong> Bogdan-Ionut Cirstea</p><p> <strong>Goal:</strong> This project aims to get more grounding into how promising automating alignment research is as a strategy, with respect to both advantages and potential pitfalls, with the <a href="https://openai.com/blog/introducing-superalignment">OpenAI superalignment</a> plan as a potential blueprint/example. This will be achieved by reviewing, distilling and integrating relevant research from multiple areas/domains, with a particular focus on the science of deep learning and on empirical findings in deep learning and language modelling. This could expand more broadly, such as reviewing and distilling relevant literature from AI governance, multidisciplinary intersections (eg neuroscience), relevant prediction markets, and the automation of larger parts of AI risk mitigation research (eg AI governance). This could also inform how promising it might be to start more automated alignment/AI risk mitigation projects or to dedicate more resources to existing ones.</p><p> <strong>Desired Skills</strong> (looking for 4 people) <strong>:</strong></p><ul><li> Minimum ML understanding, AI safety tech knowledge equivalent to having gone through AGISF, good communication (distillation) skills, basic research skills.</li><li> A wide variety of additional skills could be useful, especially good distillation (strong writing skills), strong generalist skills, more advanced ML/theoretical CS/math skills.</li></ul><p></p><h2> <strong>23.</strong> <a href="https://docs.google.com/document/d/1NMRmB9NL_Sv_u8H4Fmn6mwCRHW4Pbbje-OvHQrrm0Co/edit#heading=h.1443xgz5n5cm"><strong>Personalized fine-tuning token for AI value alignment</strong></a></h2><p> <strong>Project Lead:</strong> Eleanor &#39;Nell&#39; Watson</p><p> <strong>Goal:</strong> We&#39;re working on a new system that makes it easier for artificial intelligence to understand what&#39;s important to you personally, while also reducing unfair or biased decisions. Our system includes easy-to-use tools that help you identify and mark different situations where the AI might be used. These tools use special techniques, like breaking down text into meaningful parts and automatically labelling them, to make it simpler to create settings that are tailored to you. By doing this, we aim to address the problem of AI not fully grasping people&#39;s unique backgrounds, preferences, and cultural differences, which can sometimes lead to biased or unsafe outcomes.</p><p> <strong>Team</strong> (looking for 2-3 people) <strong>:</strong></p><ul><li> <strong>Fine-Tuning/RLHF/Constitutional AI Expertise:</strong> We&#39;re interested in how our &quot;values token&quot; can be effectively used with techniques like RLHF and Fine Tuning. <a href="https://arxiv.org/abs/2310.01405"><u>Theoretical approaches in representation engineering</u></a> and <a href="https://arxiv.org/abs/2310.13639"><u>contrastive preference modeling</u></a> highlight potential means to accomplish this.</li><li> <strong>Cybersecurity Expertise:</strong> Help make our system (including potentially sensitive data about people&#39;s values) as secure as possible, including methods such as homomorphic encryption.</li><li> <strong>Vector Databases:</strong> We plan to turn Likert-scale responses about values into numerical vectors (think of it as &#39;value2vec&#39;). We need people who are skilled in creating these kinds of vector databases.</li></ul><p></p><h2> <strong>24.</strong> <a href="https://docs.google.com/document/d/1fMropF42vJLyKsm99XLk1UK8iCnlrjs6NhryUUCr9IM/edit"><strong>Self-other overlap @AE Studio</strong></a></h2><p> <strong>Project Lead:</strong> Marc Carauleanu</p><p> <strong>Goal:</strong> To investigate increasing self-other overlap while not significantly altering model performance. This is because an AI has to model others as different from oneself in order to deceive or be dangerously misaligned. Thus, if the model is deceptive and outputs statements/actions that just seem correct to an outer-aligned performance metric during training, we can favour honest solution by just increasing self-other overlap without altering performance. The goal of this research project is three-fold: 1) Better define and operationalise self-other overlap in LLMs. 2) Investigate the effect of self-other overlap on adversarial and cooperative behaviour in Multi-Agent Reinforcement Learning. 3) Investigate the effect of self-other overlap on adversarial and deceptive/sycophantic behaviour in Language Modelling.</p><p> <strong>Desired Skills (</strong> see <a href="https://ae.studio/join-us"><u>this page</u></a> ):</p><ul><li><strong>最低限度：</strong><ul><li> <strong>Software Engineering:</strong> Extensive experience with Python software development and data engineering. Experience using AWS and libraries such as PyTorch</li></ul></li><li><strong>理想的</strong>：<ul><li> A PhD in a relevant subject (CS, ML, Computational Neuroscience)</li><li> Experience with AI Safety research (anything is fine)</li></ul></li></ul><p></p><h2> <strong>25.</strong> <a href="https://docs.google.com/document/d/1wuHalfHjAmFA-RKYcq3MBptFYLPvoMdy9KTPelyZ67A/edit#heading=h.9lmc73wscx1r"><strong>Asymmetric control in LLMs: model editing and steering that resists control for unalignment</strong></a></h2><p> <strong>Project Lead:</strong> Domenic Rosati</p><p> <strong>Goal:</strong> Recent efforts in concept level model steering such as <a href="https://www.lesswrong.com/posts/HWxLQvzJGeXoLPJWd/actadd-steering-language-models-without-optimization"><u>Activation Addition</u></a> or <a href="https://arxiv.org/abs/2310.01405"><u>Representation Engineering</u></a> , <a href="https://rome.baulab.info/"><u>ROME</u></a> and <a href="https://arxiv.org/abs/2306.03819"><u>LEACE</u></a> are promising approaches towards natural language generation control that is aligned with human values. However these approaches could be equally used by bad actors to unalign models and inject misinformation. This project involves developing a research direction where control interventions would be ineffective for counterfactual editing or unaligned control but remain effective for factual editing and aligned control. We call this &quot;asymmetric control&quot; <strong>&nbsp;</strong> since control can only happen in a direction towards alignment with human values not away from it.</p><p> <strong>Team</strong> (looking for 2-4 people) <strong>:</strong></p><ul><li> <strong>Conceptual Alignment/AI Safety:</strong> Understanding major concepts in AI/Value Alignment literature. Thinking conceptually through motivation, experimental proof, theoretical proof of proposed interventions. Basic familiarity with tools such as first-order logic, upper and lower bound analysis, proofs, philosophical analysis</li><li> <strong>Risk Assessment/Evaluation:</strong> Understanding of the research landscape in AI Risk, and (technical) AI risk assessment and evaluation. Creative thinking around experimental design for evaluation</li><li> <strong>ML Engineering:</strong> Design, training, and evaluation of neural networks. Creative thinking around technical approaches to alignment</li><li> <strong>NLP:</strong> LLMs and Transformers. Interpretability and Representation Probing techniques. Modelling and evaluation of NLP techniques</li></ul><h3></h3><h2> <strong>26.</strong> <a href="https://docs.google.com/document/d/1BLpAMmrhkl3EQY18AFX5rBIwNRBX4HMOggEz-oZPlpE/edit"><strong>Tackling key challenges in Debate</strong></a></h2><p> <strong>Project Lead:</strong> Paul Bricman</p><p> <strong>Goal:</strong> Debate <a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=snfYkaxHFYxmMYJs8"><u>remains</u></a> a central approach to alignment at frontier labs. In brief, it consists in having LLMs adversarially debate each other before a judge, the aggregate of which forms a deliberative system that can be used to automatically reflect on appropriate courses of action. However, the debate agenda faces a number of key challenges, mostly having to do with designing reliable means of evaluating competing parties, so as to identify the party that is closer to the truth.</p><p> <strong>Team</strong> (looking for 3 people) <strong>:</strong></p><ul><li> <strong>[All]</strong> <strong>Software Engineering:</strong> Python fluency. Designing maintainable codebases. HuggingFace. Strong technical writing, and familiarity with Torch/JAX are a plus.</li><li> <strong>[1] Textual-Symbolic Interoperability by Unsupervised Machine Translation:</strong></li><li> Self-distilling the ability to manipulate formal and natural language representations of statements in parallel. Follow the methodology of prior work on self-distillation for machine translation out of <a href="https://arxiv.org/pdf/2110.05448.pdf"><u>OpenAI</u></a> and <a href="https://arxiv.org/abs/2308.08998"><u>DeepMind</u></a> .</li><li> <strong>[2] Quantifying Credence by Consistency Across Complete Logics:</strong></li><li> Generalising <a href="https://arxiv.org/abs/2212.03827"><u>Contrast Consistent Search</u></a> . The previous technique implicitly relies on negation in most fuzzy logics (¬P = 1 - P), as well as probability theory, then optimises for credence probes that are consistent across any statement expressed using a <a href="https://en.wikipedia.org/wiki/Functional_completeness"><u>functionally complete</u></a> set of connectives (P ^ ¬Q = P * (1 - Q)).</li><li> <strong>[3] Granular Control Over Compute Expenditure by Tuned Lens Decoding:</strong></li><li> Controlling the amount of computation employed in LLM inference. The key idea is to implement an efficient <a href="https://huggingface.co/blog/how-to-generate"><u>decoding method</u></a> based on the <a href="https://tuned-lens.readthedocs.io/en/latest/index.html"><u>tuned lens</u></a> method so as to only work with the model&#39;s “best guesses” at a given intermediate layer.</li></ul><p></p><h1><strong>其他</strong></h1><h2><strong>27.</strong> <a href="https://docs.google.com/document/d/13Jv8HV3D40Ig-lgj2eO7AHU4hPU3eNWuQR2FmoRF5W4/edit#heading=h.9lmc73wscx1r"><strong>AI-driven economic safety nets: restricting the macroeconomic disruptions of AGI deployment</strong></a></h2><p> <strong>Project Lead:</strong> Jonathan Claybrough</p><p> <strong>Goal:</strong> In the face of rapid AI and AGI advancements, this project aims to investigate potential socio-economic disruptions, especially within labor markets and income distribution. The focus will be on conceptualizing economic safety mechanisms to counteract the adverse effects of AGI deployment, ensuring a smoother societal transition.</p><p> <strong>Team</strong> (looking for 3-6 people) <strong>:</strong></p><ol><li> <strong>Team Coordinator:</strong> Organise meetings, ensure timelines are met, facilitate communication within the team, manage documentation.</li><li> <strong>AI Lead(s) (1-2x):</strong> Provide insights into AGI&#39;s capabilities, future trajectories, and potential economic impacts. Bridging the gap between AI advancements and economic analyses. Should have understanding of AGI development, experience AI modelling, and familiarity with global economic structures.</li><li> <strong>Economist(s) (1-4x):</strong> Lead the economic analysis, model potential scenarios of AGI deployment, and contribute to the policy framework design. Should have understanding of macroeconomics, experience in policy formulation, and an understanding of AGI&#39;s potential economic ramifications.</li></ol><h3></h3><h2> <strong>28.</strong> <a href="https://docs.google.com/document/d/1VvoE1u0Mmifol5TdlQfsMXviIF0bug5CpQ5hsLEg9Fs/edit#heading=h.9lmc73wscx1r"><strong>Policy-based access to powerful models</strong></a></h2><p> <strong>Project Lead:</strong> Pratyush Ranjan Tiwari</p><p> <strong>Goal:</strong> As machine learning models get more powerful, restricting query access based on a safety policy becomes more important. Given a setting where a model is stored securely in a hardware-isolated environment, access to the model can be restricted based on cryptographic signatures. Policy-based signatures allow signing messages that satisfy a pre-decided policy. There are many reasons why policy enforcement should be done cryptographically, including insider threats, tamper resistance and auditability. This project leverages existing cryptographic techniques and existing discourse on AI/ML safety to come up with reasonable policies and a consequent policy-based access model to powerful models.</p><p> <strong>Team</strong> (looking for 3 people) <strong>:</strong></p><ul><li> For either of the roles below, no experience in cryptography is required. Interest in AI safety policy and a broad math/theoretical CS background is beneficial.</li><li> <strong>Research Eng. Roles (2x):</strong> Experience prototyping ideas to code is required. Background in experimenting with powerful models/LLMs is useful. Experience reading research papers is essential</li><li> <strong>Researcher (1).</strong> Background in ML research + experience writing research papers/technical documentation. Useful to know: Similarity-driven NLP classification, Semantic Hashing, and general NLP techniques. Inclination towards learning new, cross-disciplinary techniques will go a long way.</li></ul><p></p><h2> <strong>29.</strong> <a href="https://docs.google.com/document/d/133ZqQDGtm3ZY4JqSYUfGvTPtxDLDCl8VDb6iUeOS1Do/edit#heading=h.9lmc73wscx1r"><strong>Organise the next Virtual AI Safety Unconference</strong></a></h2><p> <strong>Project Lead:</strong> Linda Linsefors</p><p> <strong>Goal:</strong> I have a design for an online unconference, that I have run a few times. I would like to find two people to take on the task of running the next Virtual AI Safety Unconference (VAISU). Even though I have a ready format, there is room for you to improve the event design too. The goal of this project is both to produce the event, and also to pass on my organising skills to people who will hopefully use them in the future. I&#39;m therefore looking for team members who are interested in continuing on the path of being organisers, even after this project. I&#39;ll teach you as much as I can, but you will do all the work. The reason I&#39;m proposing this project is because I don&#39;t want to organise the next VAISU, I want you to do it.</p><p> <strong>Desired Skills</strong> (looking for 2 people) <strong>:</strong></p><ul><li> <strong>Interest in event design:</strong> Willingness to do some amount of boring spreadsheet tasks. Automate as much as you want, but probably can&#39;t automate everything.</li><li> <strong>Noticing what you don&#39;t know yet:</strong> Good at extracting information from me. I have not been very successful at delegating in the past. Key Information often gets lost in transmission.</li><li> <strong>Good written communication.</strong> When producing written information for the unconference participants, you need to look at your own text and tell if it&#39;s good enough, and improve what needs improving.</li><li> <strong>Paying attention to everything:</strong> You don&#39;t need to know how to solve all problems. Just noticing there is a problem and asking for advice is good enough.</li></ul><p></p><h1>现在申请</h1><p>Note again that these are summaries, and the descriptions or desired may not fully reflect the author&#39;s projects or views.</p><p> If you find any of the above <a href="https://aisafety.camp/">AI Safety Camp</a> projects interesting, and you have some of the skills listed, then make sure to <a href="https://airtable.com/appi7jDH1gAfAZDyC/shrwKt5p0TKG86j9G">apply</a> before 1st December 2023.</p><br/><br/> <a href="https://www.lesswrong.com/posts/npkvZG67hRvBneoQ9/aisc-2024-project-summaries-1#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/npkvZG67hRvBneoQ9/aisc-2024-project-summaries-1<guid ispermalink="false"> npkvZG67hRvBneoQ9</guid><dc:creator><![CDATA[NickyP]]></dc:creator><pubDate> Mon, 27 Nov 2023 22:32:23 GMT</pubDate> </item><item><title><![CDATA["Epistemic range of motion" and LessWrong moderation]]></title><description><![CDATA[Published on November 27, 2023 9:58 PM GMT<br/><br/><p> <i>(Context for the reader: Gabriel reached out to me a bit more than a year ago to ask me to delete a few comments on</i> <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai"><i>this post</i></a> <i>by Jacob Hilton, who was working at OpenAI at the time. I referenced this in my</i><a href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy"><i>recent dialogue with Olivia</i></a> <i>, where I quoted an email I sent to Eliezer about having some concerns about Conjecture partially on the basis of that interaction. We ended up scheduling a dialogue to talk about that and related stuff.)</i> </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:08:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:08:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> You were interested in a dialogue, probably somewhat downstream of<a href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy">my conversation with Olivia</a> and also some of the recent advocacy work you&#39;ve been doing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:10:01 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:10:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p>是的。</p><p> Two things I&#39;d like to discuss:</p><ol><li> I was surprised by you (on a recent call) stating that you found LessWrong to be a good place for the Lying is Cowardice not Strategy post.</li><li> I think you misunderstand my culture. Especially around civility, and honesty. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:10:29 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:10:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, I am interested in both of the two things. I don&#39;t have a ton of context on the second one, so am curious about hearing a bit more.</p></div></section><h2> Gabriel&#39;s principles for moderating spaces </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:14:14 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:14:14 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> About the second one:</p><ol><li> I think people should be free to be honest in their private spaces.</li><li> I think people should be free to create their own spaces, enact their vision, and to the extent you participate in the space, you should help them.</li><li> If you invite someone to your place, you ought to not do things that would have caused them not to come if they knew ahead of time.</li></ol><p> So, about my post and the OAI thing:</p><ul><li> By 3, I feel ok writing my post on my blog. I feel ok with people dissing OAI on their blogs, and on their posts if you are ok with it (I take you as proxy for &quot;person with vision for LW&quot;)</li><li> I feel much less ok about ppl dissing OAI on their own blog posts on LW. I assume that if they knew ahead of time, they would have been much less likely to participate.</li><li> I would have felt completely ok if you told me &quot;I don&#39;t think your post has the tone required for LW, I want less adversariality / less bluntness / more charitability / more ingroupness&quot;</li></ul><p> How surprising are these to you? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:16:05 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:16:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> Meta-comment:  Would have been great to know that the thing with OAI shocked you enough to send a message to Eliezer about it.</p><p> Would have been much better from my point of view to talk about it publicly, and even have a dialogue/debate like this if you were already opened to it.</p><p> If you were already open to it, I should have offered. (I might have offered, but can&#39;t remember.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:17:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:17:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p>喔好吧。 Let me think about this a bit.</p><p> I have thoughts on the three principles you outline, but I think I get the rough gist of the kind of culture you are pointing to without needing to dive into that.</p><p> I think I don&#39;t understand the &quot;don&#39;t do things that will make people regret they came&quot; principle. Like, I can see how it&#39;s a nice thing to aspire to, but if you have someone submit a paper to a journal, and then the paper gets reviewed and rejected as shoddy, then like, they probably regret submitting to you, and this seems好的。</p><p> Similarly if I show up in a jewish community gathering or something, and I wasn&#39;t fully aware of all of the rules and guidelines they follow and this make me regret coming, then that&#39;s sad, but it surely wouldn&#39;t have been the right choice for them to break their rules and guidelines just because I was there. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:19:02 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:19:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><blockquote><p> I do think I don&#39;t really understand the &quot;don&#39;t do things that will make people regret they came&quot; principle. Like, I can see how it&#39;s a nice thing to aspire to, but if you have someone submit a paper to a journal, and then the paper gets reviewed and rejected as shoddy, then like, they probably regret submitting to you, and this seems好的。</p></blockquote><ul><li> You mention &#39;the paper gets reviewed and rejected&#39;, but I don&#39;t think the comments on OAI post was much conditioned on the quality of the post. If I recall correctly, the tone was more &quot;how does OAI dare talking about safety given they shorten timelines&quot;.</li><li> If the goal was to get OAI to not come to LW at all, I would have actually been ok with it.<ul><li> But I don&#39;t know if that was the goal, and this being unclear irks me.</li><li> If that was the goal, I am quite sure it could be made more explicit ahead of time, rather than just waiting for insults.</li></ul></li><li> If the journal had a way to make it clear to the paper writer, ahead of time, that they would have been rejected, I think it would be good of them to write it. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:20:36 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:20:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> Also, important context: there was a possibility back then on LW, and it was not clear under which conditions it would have been offered, to freely moderate your own comment section.</p><p> I think it was called &quot;reign of terror: I remove what I dislike&quot; or something. So from my point of view, it was fitting the norms. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:20:45 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:20:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> To be clear, I generally am pretty happy about having people from OpenAI show up, and my sense is also that Jacob who made the post thought a bunch of the discussion made sense and was glad to have it happen in public rather than in-private (and he himself didn&#39;t want to take the comments down). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:21:23 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:21:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> (To be clear, I cared much more about taking the <i>insults</i> down. The rest of the comments were ok, and if I recall correctly, the authors edited the insults out themselves once they reflected about them.)</p></div></section><h2> LessWrong as a post-publication peer-reviewed journal </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:24:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:24:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> There is a general principle I consider on LessWrong for all types of content on the site, which is something like &quot;whenever someone makes a claim on LessWrong, if it&#39;s clearly wrong, it must be possible for a response to that claim to become easily findable in the UI&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:26:20 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:26:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><blockquote><p> &quot;whenever someone makes a claim on LessWrong, if it&#39;s clearly wrong, it must be possible for a response to that claim to become easily findable in the UI&quot;</p></blockquote><p> I love this principle. (And I love Community Notes for similar reason.)</p><p> I truly dislike the &quot;less attention to response to critique&quot;-pattern that favours making a lot of low-effort critiques, because the debunking won&#39;t receive nearly as much attention.</p><p> I would also love to have a &quot;Hottest questions to [X] right now&quot;, to avoid the pattern where someone gets swarmed with questions, and then it looks like they don&#39;t have a response to anything. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:27:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:27:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> In a broader sense, I think LessWrong definitely has a lot of the components of a journal, with one of the biggest differences being that everything on LessWrong is post-publication reviewed not pre-publication reviewed. Many fields have already been moving towards post-publication review (like ML or physics where it&#39;s very standard practice for people to upload things to arxiv before you get accepted to a journal).</p><p> So my reaction to removing critical content and analysis feels somewhat similar to asking a journal to skip peer review for a publication, which like, is not completely unheard of but clearly violates some important principles and requires some pretty high standard of evidence to be met. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:28:26 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:28:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> Interesting, this is a thing that I did not get, and I understand more now why you are reticent to even moderate insults that are coupled with good analysis.</p><p> Not necessarily because you want more insults, but because:</p><ol><li> Insults coupled with good analyses are rare.</li><li> The slope is more slippery toward moderating out / toning down embarrassing analysis than having more insults in good analyses.</li></ol><p>这看起来正确吗？</p></div></section><h2> What are &quot;insults&quot;? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:29:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:29:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think I personally have a pretty strong ick-reaction to calling people&#39;s concerns in the space &quot;insults&quot;.</p><p> Like, can &quot;insults&quot; be accurate? In the pre-FTX days I thought Sam was pretty sketch in a bunch of ways. I think calling him &quot;a fraud&quot; or a &quot;sociopath&quot; would have definitely been perceived as an insult, but man, I think it would have just been correct, and I think conversations were those hypotheses were explicitly raised would have improved things a lot 。</p><p> And I get a bit of a vibe of some kind of honor that is present in many institutions which involves implicitly suppressing criticism and preventing people from grappling with the negative effects of their actions, by calling things that criticize them or others &quot;insults&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:30:02 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:30:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> Insults can be accurate! I think there was a post on LW that said that &quot;yomamma fat&quot; applied to 60% of UK children.</p><p> Insults can even be part of nice norms: I like some places where it&#39;s normal for ppl to insult each other, it&#39;s fun.</p><p> Insults can also be useful: sometimes, the relevant topic is the character of a person or of an institution.</p><p> But if OAI comes to talk about a safety plan, and someone makes a comment not about the safety plan but &quot;OAI bad&quot;, I am like &quot;eeee, if you want to say &#39;OAI bad&#39;, write your post about it&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:31:16 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:31:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> To be clear, I just wrote a post &quot;Lying is Cowardice, not Strategy&quot;, and I have one in the pipeline called &quot;For Civilization and Against Niceness&quot; that is a lot about the pattern that you just mentioned.</p><p> This is not the thing that bothers and bothered me. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:37:17 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:37:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> (Can we look up that comment? Can&#39;t remember it, and looks like you only have a vague memory too.)</p><p> It&#39;s actually worse than in my memories. <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p><a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai#:~:text=The%20OP%20is%20framed%20as,a%20purely%20for%2Dprofit%20organization.">Here is the comment.</a></p><p> It feels extremely bait and switchy to me: &quot;I&#39;m just raising the hypothesis that OAI might be PR-washing&quot; vs &quot;I write a long RP of OAI as &#39;Phillips Morris International&#39;&quot;</p><p> This sounds to me much more like &quot;OAI bad&quot; than &quot;OAI has vested interests in non-safety, as shown by [X]. Even though their charter talks at length about safety, they are clearly safety-washing, as shown by [Y].&quot; or &quot;Here are mischaracterisations in the post&quot;. <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:37:59 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:37:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The comment that you complained about had pretty direct and immediate relevant to the post at hand. It compared OAI to a tobacco company if I recall. A comparison that raises a hypothesis that still seems pretty apt.</p><p> Some concrete quotes that caused you to want me to delete stuff at the time:</p><blockquote><p> You seem confused about the difference between &quot;paying lip service to X&quot; and &quot;actually trying to do X&quot;.</p><p> To be clear, this in itself isn&#39;t evidence <i>against</i> the claim that OpenAI is trying to directly build safe AI. But it&#39;s not much evidence for it, either.</p></blockquote><p> And in a separate comment:</p><blockquote><p> Here is a similar post one could make about a different company:</p><p> &quot;A friend of mine has recently encountered a number of people with misconceptions about their employer, Phillip Morris International (PMI). Some common impressions are accurate, and others are not. He encouraged me to write a post intended to provide clarification on some of these points, to help people know what to expect from the organization and to figure out how to engage with it. It is not intended as a full explanation or evaluation of Phillip Morris&#39;s strategy.</p><h2> Common accurate impressions</h2><ul><li> <strong>Phillip Morris International is the world&#39;s largest producer of cigarettes.</strong></li><li> <strong>The majority of employees at Phillip Morris International work on tobacco production and marketing for the developing world.</strong></li><li> <strong>The majority of Phillip Morris International&#39;s employees did not join with the primary motivation of reducing harm from tobacco smoke specifically.&quot;</strong></li></ul></blockquote><p> These both seem clearly far above the bar for me as not being &quot;insults&quot;. There is no &quot;but you suck&quot; in here. There is substantial critique, and the PMI comparison seems apt and helpful for people understanding the kinds of dynamics that are going on here. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:38:23 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:38:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><blockquote><p> There is substantial critique</p></blockquote><p> What do you have in mind here? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:40:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:40:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I mean, I think being like &quot;look, this post reads very similar to a pattern of deception that is pretty common and which suggests an alternative explanation for why this post was written the way it was&quot; is clearly a real critique, and I continue to find it pretty compelling.</p><p> Indeed, there is a reason why organizations produce this kind of article where they take strawmen and tear them down, because it works, without actually being sensitive to the details of the situation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:41:06 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:41:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> This looks like a strong steelman to me, but willing to accept it if that&#39;s the precedent you want to set.</p><p> Like, if that&#39;s truly how you perceived it, and you would be ok with similar shape of critique toward someone more in-groupy, then my reaction is something like:</p><p> &quot;Ideally, you&#39;d make this clear ahead of time. But norms can obviously not capture the full breadth of reality, and so this falls under &#39;Reasonable not-covering-for-every-situation-possible&#39;. And my reaction becomes more of &#39;Eh. If that&#39;s the space you want, why not.&#39;&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:41:24 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:41:24 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I don&#39;t think it&#39;s a &quot;strong steelman&quot;. Saying that it&#39;s &quot;a strong steelman&quot; sounds like calling &quot;1984 is a critique of the western world&#39;s tendencies to fall into authoritarianism&quot; a &quot;strong steelman&quot;. It&#39;s obviously the central message.</p><p> I am confident that 90%+ of LW readers got the point of the critique and could restate it in similar words. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:42:35 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:42:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> When someone writes &quot;This comment is a substantial critique to the post&quot;, I don&#39;t expect 1984-lite, I expect actual arguments/counterarguments.<br><br> That&#39;s what my &quot;strong steelman&quot; comment is about. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:42:35 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:42:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> After a year of reflection: I have noticed since then that I don&#39;t predict well what you want and don&#39;t want on LW.</p><p> For my post, I deferred to others at Conjecture who told me it was fit to LW, because my first impression was &quot;Nah, this is too aggressive for Habryka&quot;.</p><p> (And even then, after publishing the post, I checked with you just to be sure)</p><p> I think a thing here is that I am likely not gauging the extent to how different you want LW to be to what I found around EA/Rationalists.</p><p> I have found that many EA/Rationalists truly dislike aggressive/frank messaging. They expect what I call &quot;Epistemic Political Correctness&quot;, where they feel better when I put qualifiers / &quot;I feel&quot; statements on things that I am in-fact quite confident about.</p></div></section><h2> &quot;Epistemic range of motion&quot; as an important LW principle </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:47:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:47:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> In-general, at least the culture I am excited about for LessWrong puts a large premium on something like &quot;epistemic range of motion&quot;. My current model of epistemology, and especially of group epistemology, suggests that if you make some set of considerations unexpressable, or some truths taboo, then this tends to radiate out a lot into the rest of your (collective) thinking.</p><p> See the old sequences post <a href="https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies">Entangled Truths, Contagious Lies</a> and <a href="https://www.lesswrong.com/posts/XTWkjCJScy2GFAgDt/dark-side-epistemology">Dark Side Epistemology</a> .</p><p> This means that I am willing to give people quite a bit of leeway if they have to express a consideration or an idea clumsily, or aggressively, or with lots of pent up feelings, if my best guess of the alternative is that it is not expressed at all (relatedly I am also a fan of <a href="https://www.lesswrong.com/users/zack_m_davis">Zack Davis&#39;s</a> writing on a bunch of stuff, despite him often expressing things in a kind of aggressive or clumsy way, because I see him as the only person actually saying a bunch of the things that otherwise couldn&#39;t be said). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:48:35 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:48:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> So, from your point of view, you want to lower as much as possible the costs of saying true things?</p><p> And if it happens that someone&#39;s way of expressing true things is snark, you&#39;re ok with it?</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> If so, this sounds like a great place for me lol.</p><blockquote><p> Now that I have your blessing I shall do that! I was mostly worried cause I have a history of making unhelpfully aggressive AI safety-related comments and I didn&#39;t want moderators to get frustrated with me again (which, to be clear, so far has happened only for very understandable reasons).</p></blockquote><p> For context: this is what the guy who wrote the OAI as Philips Morrisson wrote. And that&#39;s quite close to how I felt about my post. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:49:42 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:49:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So my general norms are that if you are aggressive on some random topic that seems like it can just be discussed calmly, then I would be relatively harsh on that. But if there is a topic where the choice is &quot;either express this clumsily or not at all&quot;, then I am willing to put a lot of chips on the table to defend and create space for the thing to be said. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:50:50 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:50:50 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><blockquote><p> if you are aggressive on some random topic that seems like it can just be discussed calmly by a lot of people</p></blockquote><p> I think this is the part that confused me, and that makes me feel &quot;aaaaaaaaaaaaa&quot;, as in No!!!!!! So arbitrary!&quot;.</p><p> The reason why is that very often, this is what dissidence looks like: &quot;Someone is unusually aggressive where most people are calm&quot;.</p><p> <strong>I think I saw moderation on LW of a very snarky thing, and when I saw that you were ok with it under the OAI post, I did not understand it as &quot;It&#39;s ok because it is genuinely trying to convey a point and I defend conveying points.&quot; but &quot;It&#39;s ok because</strong> <strong>OAI bad&quot;.</strong></p><p> And my reaction to this was &quot;Wow, if you want OAI out, there are clearer ways to make this known and I think it&#39;s not a good goal. And if you want them in, you don&#39;t want to make an exception to snark when it&#39;s about them specifically&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:51:23 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:51:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So my reaction to the Lying is Cowardice post was definitely one of &quot;yep, man, this sure isn&#39;t the best way to express this, but I&#39;ve been carrying around a lot of related feelings in my stomach and I don&#39;t really see anyone else making these points, so I guess let&#39;s do it clumsily instead, if the alternative is not at all&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:51:44 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:51:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> I had the same reaction to you to my post: I was like &quot;Sure, I could express it better, but I suck at writing, I need to start somewhere, and this point needs to be made&quot;</p><p> Would have posted similar things a longer time ago if I knew you&#39;d be ok with it</p><p> Do you have a document / a page that describes best your approach to moderation and all these considerations?</p><p> I think a canonical page would help a lot, else it feels very ad-hoc. Like, while you want criticism to be easily next to the post, at the same time, you also had the &quot;Reign of Terror&quot; thing where people could fully moderate their own post.</p><p> And as a result, we now have a conversation about a thing that irked you like a year after it happened.</p><p> I do realise things have costs, and you can do many other things with your resources.<br> Just trying to raise to saliency that I believe you are under-estimating the benefits of having a canonical place describing your moderation&#39;s philosophy or rules. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 19:58:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 19:58:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> We have a ton of stuff on moderation philosophy, but I don&#39;t think we have anything super centralized, and I don&#39;t think we have anything that makes this specific point (though I am sure it&#39;s somewhere in some past moderation discussions).</p><p> Yeah, for the reign of terror thing, we do allow a small fraction of users to moderate their own posts more aggressively, but it&#39;s always been something we&#39;ve been keeping a close eye on to make sure it doesn&#39;t have distortionary effects, and also, I cared a lot about having Pingbacks in place, so that even if someone moderates all the comments on their posts, people can still find response posts if they are highly upvoted: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/upw1mdiz4utnu8k18tj8" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/coy15lkabzqai9q9jaqc 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/rf4x8erjxgtoz7kgvviv 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/iurru09crgvoqeda3jvq 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/yor3juypesyjkgrib0e1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/sw0qauh32neh3ou4x1dl 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/qrpo3uxjd8zzorip5pbn 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/lwmv5mf7arlyhsmggoit 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/oema2fksdotula6mmrwd 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/mqrndgwc9gddnj9wzan6 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JCqn2oFCPjXHMYK4G/e8dq1k6dyudmasdchijk 1434w"></figure></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 20:04:21 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 20:04:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><p> My summary would be:</p><ul><li> You care much more about lowering the costs to state true things (including clumsiness and snark) than I expected. As a result, I misinterpreted the norm of the snark against OAI for &quot;snark is ok when it&#39;s about OAI&quot; as opposed to &quot;snark is ok when it would be more costly to phrase differently&quot;.</li><li> It&#39;s hard to understand your moderation philosophy as it&#39;s not summarised in a place I could check, and there are bits that went counter to the correct interpretation that you suggest here: &quot;reign of terror&quot;, and moderation of other snark (but that were non-productive). </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="zdpTGBacocvr7q57J-Tue, 07 Nov 2023 19:59:24 GMT" user-id="zdpTGBacocvr7q57J" display-name="Gabriel Alfour" submitted-date="Tue, 07 Nov 2023 19:59:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Gabriel Alfour</b></section><div><blockquote><p> Yeah, for the reign of terror thing, we do allow a small fraction of users to moderate their own posts more aggressively, but it&#39;s always been something we&#39;ve been keeping a close eye on to make sure it doesn&#39;t have distortionary effects</p></blockquote><p> Never been clear to me that it was part of your considerations when I saw the feature, nor when we talked about it on the phone, nor when I talked with Ben about it on the phone. So from my point of view, it read like &quot;nah because OAI&quot;.</p><p> (Which again, could be fair if that was the goal, but should be explicit in that case)</p><p> Also to make it clear: <strong>I am not doubting your honesty, nor do I think you are misrepresenting your beliefs</strong> .</p><p> I think there is a chance that you are much clearer about it than you were a year ago because of reflection, and a chance that I understand it better than a year ago because of more reflection and interacting with more LW people.</p><p> (I don&#39;t think I have much more to say about this? Confusion mostly resolved on my end.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 07 Nov 2023 20:03:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 07 Nov 2023 20:03:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p>是的，也许吧。 I do notice I feel like there is still a very substantial clash of cultural assumptions here. But maybe best to call it for now.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/JCqn2oFCPjXHMYK4G/epistemic-range-of-motion-and-lesswrong-moderation#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/JCqn2oFCPjXHMYK4G/epistemic-range-of-motion-and-lesswrong-moderation<guid ispermalink="false"> JCqn2oFCPjXHMYK4G</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 27 Nov 2023 21:58:40 GMT</pubDate> </item><item><title><![CDATA[Apply to the Conceptual Boundaries Workshop for AI Safety]]></title><description><![CDATA[Published on November 27, 2023 9:04 PM GMT<br/><br/><p> Do you have experience with Active Inference, Embedded Agency, biological gap junctions, or other frameworks that separate agents from their environment? Apply to the Conceptual Boundaries Workshop for AI safety. February in Austin TX. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tLb86DhrTYgkXw5Hf/q49jbbvhge4o4srh7xeh" alt="图像"></figure><h1> <a href="https://formalizingboundaries.ai/">Website and application</a></h1><h3> A (small) workshop to identify promising boundaries research directions and empirical projects.</h3><p> <strong>Boundaries keep agents</strong> <i><strong>causally</strong> <strong>separate</strong></i> <strong>from their environment. This is crucial for their survival and continued autonomy.</strong></p><ul><li> A bacterium relies on its membrane to protect its internal processes from external influences.</li><li> Secure computer systems use controlled inputs and outputs to prevent unauthorized access.</li><li> Nations maintain sovereignty by securing their borders.</li><li> Humans protect their mental integrity by selectively filtering the information that comes in and out.</li></ul><p> <strong>When</strong> <strong>an agent&#39;s boundary is respected, that agent maintains its autonomy.</strong></p><p> Boundaries show a way to respect agents that is distinct from respecting preferences or utility functions. Expanding on this idea, Andrew Critch says the following in <a href="https://www.lesswrong.com/s/LWJsgNYE8wzv49yEc/p/SajYfrsoTHxiXPNtf"><u>«Boundaries» Sequence, Part 3b</u></a> :</p><blockquote><p> my goal is to treat boundaries as more fundamental than preferences, rather than as merely a feature of them.  In other words, I think boundaries are probably better able to carve reality at the joints than either preferences or utility functions, for the purpose of creating a good working relationship between humanity and AI technology</p></blockquote><p> For instance, respecting a bacterium means not disrupting its membrane, rather than understanding and acting on its desires.</p><p> <strong>Boundaries</strong> <strong>act as a natural abstraction promoting safety and autonomy.</strong></p><p> By formalizing the boundaries that ensure world safety, we could better position ourselves to protect humanity from the threat of transformative AI.</p><h1>出席者</h1><h2>确认的：</h2><ul><li> <a href="https://www.linkedin.com/in/davidad/?originalSubdomain=uk">David &#39;davidad&#39; Dalrymple</a></li><li> <a href="http://scott.garrabrant.com/">Scott Garrabrant</a></li><li> <a href="https://www.linkedin.com/in/particlemania">TJ (Tushant Jha)</a></li><li> <a href="https://acritch.com/">Andrew Critch</a></li><li> Chris Lakin (organizer)</li><li> <a href="https://atlascomputing.org/">Evan Miyazono</a> (co-organizer)</li></ul><h2> Seeking 6-10 more guests who either:</h2><ul><li> Have prior experience with technical or philosophical approaches that separate agents from their environment.<ul><li> Approaches like «boundaries», active inference and Markov blankets, embedded agency, <a href="https://en.wikipedia.org/wiki/Gap_junction">cell gap junctions</a> , etc.</li></ul></li><li> Are willing and able to implement approaches planned at the workshop.<ul><li> The worst outcome from a workshop is a bunch of promised follow-ups that result in nothing.</li><li> Eg: PhD candidates or postdocs who are looking for new projects.</li></ul></li></ul><h1> <a href="https://formalizingboundaries.ai/">Website and application</a></h1><h1> Get notified about future «boundaries» events</h1><p> We are also considering running other «boundaries»-related workshops in mid 2024. For example a larger more general workshop, or domain-specific workshops (eg: boundaries in biology, boundaries in computer security).<i><strong> </strong></i><strong>If you would like to get notified about potential future events, sign up via the form on the footer of the</strong> <a href="https://formalizingboundaries.ai/"><strong>website</strong></a> <strong>.</strong></p><h1><strong>您可以如何提供帮助</strong></h1><ul><li><a href="https://twitter.com/chrischipmonk/status/1729207680466915557"><strong>Repost this workshop on Twitter</strong></a></li><li> <strong>Share with anyone you think might be a good fit</strong></li><li> <strong>Let me know if there&#39;s anywhere else I can advertise.</strong> (I don&#39;t want to just get people who check LessWrong!)</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/tLb86DhrTYgkXw5Hf/apply-to-the-conceptual-boundaries-workshop-for-ai-safety#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/tLb86DhrTYgkXw5Hf/apply-to-the-conceptual-boundaries-workshop-for-ai-safety<guid ispermalink="false"> tLb86DhrTYgkXw5Hf</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Mon, 27 Nov 2023 21:04:59 GMT</pubDate></item></channel></rss>