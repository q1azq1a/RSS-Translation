<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 5 日，星期日 20:11:49 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[EA orgs' legal structure inhibits risk taking and information sharing on the margin]]></title><description><![CDATA[Published on November 5, 2023 7:13 PM GMT<br/><br/><h1><strong>什么是财政资助？</strong></h1><p> EA 组织向其他 EA 组织提供财政赞助是相当常见的。等等，不，这句话不太正确。更准确的说法是，法律意义上的EA组织很少；您所认为的大多数组织都是由单个组织合法托管的项目，因此政府将其视为一个法律实体。</p><p>其中最大的保护伞是Effective Ventures Foundation，该基金会旗下有CEA、80k、Longview、EA Funds、Giving What We Can、Asterisk magazine、Center for Governance of AI、Forethought Foundation、Non-Trivial 和 BlueDot Impact。城堡上的帖子<a href="https://forum.effectivealtruism.org/posts/xof7iFB3uh8Kc53bG/why-did-cea-buy-wytham-abbey?commentId=u3yJfbm2pes8TFpYX"><u>也将其描述</u></a>为 EVF 项目，尽管它没有在网站上列出。 Rethink Priorities 有一个专门为有需要的团体提供赞助的<a href="https://rethinkpriorities.org/news/announcing-special-projects"><u>计划</u></a>。 LessWrong/Lightcone 由 CFAR 主办，并且自己赞助了至少一个项目（来源：我。这是我的项目）。</p><p>财政赞助有很多优点。它让您享有成为注册非营利组织（美国为 501c3）的特权，而无需费时且昂贵的文书工作。如果项目很小、有时间限制（就像我的项目一样），或者是一个如果四个月内看不到结果就可能放弃的实验，那么这就是一件大事。即使对于大型项目/组织，共享正式的法律结构也可以更轻松地共享人力资源部门和会计师等资源。从短期来看，为了获得做更多文书工作的特权，组建一个法律上独立的组织似乎需要大量的金钱和精力。</p><p><br></p><h1><strong>财政赞助的缺点</strong></h1><p>......数量众多，并且随着所涉及的项目的增长而增长。</p><p>公众对拥有一个声称独立的法人实体的项目抱有正确的怀疑，因此糟糕的公关可能会给所有人造成损害。政府非常有信心相信你们是同一个法律实体，因此法律风险几乎是平等分担的（iamnotalawyer）。因此，共享法律结构会自动分担风险。这或许是可以解决的，但解决问题需要付出一定的代价。</p><p>最简单的事情就是减少风险。不要购买可以被形容为奢华的静修中心。绝对，100%，不要自愿分享有关您与 FTX 互动的任何信息，特别是如果这样做的好处是无形的。因此，一些价值会损失，因为对于个人或小型组织来说，冒这个风险是值得的，但对集体来说却不值得。</p><p> [我无法在该列表中遵循<a href="https://en.wikipedia.org/wiki/Rule_of_three_(writing)"><u>三法则</u></a>，这让我很伤心，但事实证明，没有那么多清晰、公开可见的决定不共享信息的例子]</p><p>然后还有协调成本。即使法律组织中的每个人都同意特定的风险，您现在也有义务与他们核实。答案通常是“这很复杂”，这导致谈判在没有人那么关心的事情上消耗了很多注意力。即使有一些让每个人都满意的行动，你也可能找不到它，因为在这么多人之间进行谈判的工作量太大（如果你认识在新冠疫情期间住在集体住宅中的人：记住谈判安全规则是多么有趣6 个具有不同价值函数和风险承受能力的人之间？）。</p><h2><strong>寒蝉效应</strong></h2><h3><strong>一个又长又复杂（但仍然简化了）的例子</strong></h3><p>这个故事的原始版本只有一段长。事情大概是这样的：EVF 赞助的项目的一位领导者想要非正式地公开分享一些关于一个有争议问题的想法。这些评论并非没有风险，但如果它只影响他们自己或他们的人，那么这个人会很乐意冒这个风险。组织。 EVF 的有人说不。嘘，咕噜。</p><p>我将该版本发送给来源以检查准确性。他们给了我一个新的、更复杂的故事。也许他们从未发表这些评论是件好事，因为它们来自一个愤怒的地方。也许他们从未发布这些评论的初始版本是件好事，但糟糕的是他们没有在睡个好觉后发布修改后的草案。也许责怪 EVF 是不公平的，因为他们（评论者）很快就放弃了，如果他们继续推动，也许 EVF 会答应。也许这都是借口，那些评论很棒，但他们的失败是一场悲剧……</p><p>很明显，如果不提供足够的细节来彻底破坏他们的匿名性，就无法以消息来源想要的细微差别程度来描述这个故事。我提出让他们用自己的话写下整件事，并写上他们的名字。他们考虑过，但觉得在没有与同事协商的情况下无法这样做，这会进一步拖延事情并占用很多人的时间。特别是因为这可能不会是一个快速的“是”或“否”，而是人们之间进行更多轮的谈判，所有人都很忙，并且没有将此作为优先事项......</p><p>我告诉他们不要打扰，因为没有必要。事实上，很难分享足够的信息，甚至无法弄清楚这些评论是净正面还是负面，以及如果项目不分享财政赞助，情况会如何变化，这一事实已经显现出来。所以我写了这个故事，试图分享最初的例子。</p><p><br></p><h3><strong>其他例子</strong></h3><p>据奥利弗·哈布里卡 (Oliver Habryka) <a href="https://www.lesswrong.com/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite?commentId=pFAYzGxGJgxFfwn4J"><u>报道</u></a>：威尔·麦克阿斯基尔 (Will MacAskill) 撰写了对 SBF 灾难的反思，但 EVF 告诉他不要发表。 </p><p><img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GRYFJGnye2gCxCTG4/ctisrjlrdlsxjo24ldni"></p><p> Giving What We Can 执行董事 Luke Freeman <a href="https://forum.effectivealtruism.org/posts/vXzEnBcG7BipkRSrF/how-has-ftx-s-collapse-impacted-ea?commentId=izNdDMjZb7r5Dxjsm"><u>表示</u></a>，FTX 爆发后，EVF 董事会下令停止 GWWC 质押活动，并明确将此归咎于 EVF 董事会制定了保守的规则，没有时间审查例外情况。 </p><p><img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/GRYFJGnye2gCxCTG4/i6vve6wgej3ij8jgl7ja"></p><p>我反对这种方式，而不是审查制度； FTX 推出后不立即融资似乎是一个相当合理的决定。但我预计他为捍卫这一决定而提出的因素，即<a href="https://forum.effectivealtruism.org/posts/vXzEnBcG7BipkRSrF/how-has-ftx-s-collapse-impacted-ea?commentId=BHTwaSmL2j4rmgCDg"><u>姐妹项目的风险</u></a>和带宽限制，将是系统性的。</p><h2><strong>混乱容忍度</strong></h2><p>财政赞助还有另一个问题。我认为，与冒险行为的寒蝉效应相比，这是次要的，但仍然值得一提。共享法律结构的一个副作用是，与项目 P（例如 80k 或 Longview）开展业务的人员将收到使用赞助组织 O（例如 EVF）名称的支票、发票或其他文件。一开始这可能看起来很粗略，但后来有人向你解释了财政赞助，你就接受了。</p><p>这就是为什么当我通过 CEA 收到“FTX 未来基金（再授予人计划）”的第一张支票，而第二张支票使用的是 North Dimensions 名称时，我并没有敲响任何警钟。我收到了很多与该组织名称不符的支票，所以我嘟哝了一些关于 EA 缺乏专业精神的事情，然后继续我的一天。此后出现的情况是，North Dimensions 是 FTX 收购的一家现有公司，目的是使用其银行账户，而该<a href="https://news.yahoo.com/north-dimension-central-misappropriation-ftx-133000733.html"><u>银行账户在 FTX 和 Alameda 之间共享</u></a>，其方式肯定是不恰当的。</p><p> [注：我没有尝试确定该银行帐户或我的补助金的详细信息，并且可能弄错了。我认为任何个人错误都不会与我的主张相矛盾，即训练人们接受误导会为渎职行为提供掩护。事实上，这种情况会产生错误，这才是重点。]</p><h1><br><strong>结论</strong></h1><p>我认为 EA 应该解决财政赞助（尤其是伞式组织）造成的风险创造和风险规避问题，以及如何权衡这些利益。这很困难，因为赞助的好处是清晰且可预测的，而成本却是模糊且不稳定的。但这使得有意识地调查它们变得更加重要。我的猜测是，这将表明让多个大型组织共享一个法律结构是不值得的，但对短期项目或启动台使用赞助将继续有意义。也许我错了，只有我们检查后才能知道。</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/XvEJydHAHk6hjWQr5/ea-orgs-legal-structure-inhibits-risk-taking-and-information#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XvEJydHAHk6hjWQr5/ea-orgs-legal-struct-inhibits-risk-take-and-information<guid ispermalink="false"> XvEJydHAHK6hjWQr5</guid><dc:creator><![CDATA[Elizabeth]]></dc:creator><pubDate> Sun, 05 Nov 2023 19:13:58 GMT</pubDate> </item><item><title><![CDATA[Eric Schmidt on recursive self-improvement]]></title><description><![CDATA[Published on November 5, 2023 7:05 PM GMT<br/><br/><p>最近，埃里克·施密特在哈佛大学发表了题为“我们的人工智能未来：未来的希望和障碍”的演讲。整个演讲可以<a href="https://www.youtube.com/watch?v=IeVY_Ag8JI8"><u>在这里</u></a>找到，但有一个部分让我感兴趣（大约 1:11:00 左右），那就是他对人工智能安全的看法以及对人工智能实验室的信任，如果递归自我改进开始，人工智能实验室就会停止扩展正在发生。<strong>强调一下我自己的。</strong></p><blockquote><p>技术术语是递归自我改进。<strong>只要系统不忙于自行学习，就可以了。</strong>我会模仿 Sam 在 OpenAI 上的演讲。 [...]</p><p>在接下来的几年里，我们将实现 AGI [...] 几年后，计算机将开始互相交谈，可能用我们无法理解的语言，以及它们的超级智能 [ ...]将会迅速上升。<strong>我的反驳是，你知道在那种情况下我们要做什么吗？我们要把它们全部拔掉。</strong></p><p>如果你是一个邪恶的人，你会采取的方式是简单地对计算机说：“学习一切。真的很努力。现在就开始吧。”于是计算机开始学习。它学习法语，学习科学，学习人，学习政治。它完成了所有这些工作，并且在某个时刻，它了解了电力，并了解到它需要电力，并决定需要更多电力。于是它侵入了医院，从医院拿走了电力给自己。</p><p>这是为什么如此危险的一个简单例子。<strong>大多数思考过这个问题的业内人士都认为，当你开始递归自我完善时，会因为这些问题而出现非常严重的监管反应</strong>。这对我来说很有意义。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/cLC2HcQbFZ5pFAgqC/eric-schmidt-on-recursive-self-improvement#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cLC2HcQbFZ5pFAgqC/eric-schmidt-on-recursive-self-improvement<guid ispermalink="false"> cLC2HcQbFZ5pFAgqC</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Sun, 05 Nov 2023 19:05:16 GMT</pubDate> </item><item><title><![CDATA[Pivotal Acts might Not be what You Think they are]]></title><description><![CDATA[Published on November 5, 2023 5:23 PM GMT<br/><br/><p><em>本文主要针对尚未阅读有关仲裁的<a href="https://arbital.com/p/pivotal/">关键法案</a>文章或需要复习的人。如果你有的话，最有趣的部分可能是“全知机器学习研究人员：没有单一控制结构的关键行为”。</em></p><p>许多人似乎将“<a href="https://arbital.com/p/pivotal/">关键行动</a>”的概念与某些反乌托邦版本的“部署通用人工智能来接管世界”相匹配。不过，“关键行动”的 <a href="https://www.lesswrong.com/posts/PKBXczqhry7iK3Ruw/pivotal-acts-means-something-specific">含义更为具体</a>。可以说，有些东西是完全不同的。我强烈建议您阅读<a href="https://arbital.com/p/pivotal/">原文</a>，因为我认为这是一个非常重要的概念。</p><p>我经常使用这个词，所以当人们开始说非常奇怪的事情时，例如“我们不能让一个强大的人工智能系统在世界上自由运行。这很危险！”，我会感到沮丧。就好像这是关键行为的决定性特征一样。</p><p>由于原文很长，让我简单总结一下我认为最重要的几点。</p><h1>解释关键法案</h1><p>一项将我们置于存在风险危险区（尤其是人工智能）之外，并让人类能够繁荣发展的行为是一项关键行为。</p><p>最重要的是，这意味着需要采取一项关键行动来防止构建错位的通用人工智能。占领世界本身并不是必需的。<em>如果你可以</em>通过创建一个能够有效监管人工智能的强大全球机构来防止出现失调的通用人工智能，那么这就是一项关键举措。<em>如果</em>我可以通过在 60 秒内吃掉 10 根香蕉来防止部署错位的 AGI，那么这也算得上是一个关键行为！</p><h2>防止 AGI 失准需要控制</h2><p>那么，为什么“关键行动”常常与接管世界的概念联系在一起呢？防止构建错位的 AGI 是一个棘手的问题。我们需要有效地限制世界的状态，这样就不会出现 AGI 失调的情况。为了成功地做到这一点，你需要对世界有很大的控制力。没有办法解决这个问题。</p><p>接管世界实际上意味着将自己置于高度控制的位置，从这个意义上说，有必要至少在某种程度上接管世界，以防止建立一个错位的 AGI。</p><h1>常见的困惑</h1><p>也许，令人困惑的一点是“接管世界”有很多与之相关的负面含义。权力很容易被滥用。将一个实体<sup class="footnote-ref"><a href="#fn-Xn4vF9NdExCw5SXra-1" id="fnref-Xn4vF9NdExCw5SXra-1">[1]</a></sup>置于强大的地位肯定会产生负面影响。但我看不到其他选择。</p><p>除了以无法构建出任何失调的 AGI 的方式控制世界之外，我们还应该做什么呢？问题在于，许多人似乎认为，赋予一个实体对世界的大量控制权是一个非常糟糕的想法，<em>就好像</em>我们可以依靠一些更好的选择一样。</p><p>然后他们可能会开始谈论他们对人工智能监管更有希望，就好像成功地实现人工智能监管并不需要一个对世界有很大控制力的实体一样。</p><p>或者更糟糕的是，他们提出了一些替代方案，比如弄清楚机械的可解释性，<em>就好像</em>弄清楚机械的可解释性等同于让世界进入一个不会出现错位 AGI 的状态。 <sup class="footnote-ref"><a href="#fn-Xn4vF9NdExCw5SXra-2" id="fnref-Xn4vF9NdExCw5SXra-2">[2]</a></sup></p><h1>不直接创造权力地位的关键行为</h1><p>有些关键行为并不需要你对世界有太多的控制权。然而，据我所知，任何关键行为最终仍需要创建一些强大的控制结构。启动一个最终将导致创建正确的控制结构以防止 AGI 失调的过程已经被视为关键行为。</p><h2>人工上传</h2><p>这种关键行为的一个例子是上传人类。想象一下，您知道如何将自己上传到计算机，运行速度提高 1,000,000 倍，并且能够复制自己，同时对自己的大脑进行完美的读写访问。然后你可能可以直接获得对世界的充分控制，这样你就可以减轻所有潜在的存在风险。或者，您可能可以解决对齐问题。</p><p>无论如何，上传自己将是一个关键行为，尽管这不会直接让世界进入不会出现 AGI 失调的状态。</p><p>这是因为上传你自己就足以确保很快就会达到不会出现 AGI 失调的世界状态。但该国家仍将拥有某个对世界拥有大量控制权的实体。如果你将自己置于权力位置，那么该实体就是你，或者如果你选择解决对齐问题，那么该实体就是对齐的 AGI</p><h2>全知的机器学习研究人员：没有单一控制结构的关键行为</h2><p>还有一些关键行为不会导致创建一个受控制的整体实体。控制权可以是分布式的。</p><p>想象一下，你可以写一篇非常适合模仿的文章，它很容易理解，并且让阅读它的每个人都很好地理解 AI 对齐，以至于他们几乎不可能偶然构建一个未对齐的 AGI。这将被视为一个关键行为。就像在“人类上传”的关键行动中一样，你不需要对世界有太多的控制来实现这一目标。获得文章后，您只需要互联网连接。</p><p>你不仅不需要对世界进行大量控制，而且在这种情况下也没有中央控制实体。控制结构分布在所有阅读这篇文章的人的大脑中。所有这些大脑现在一起约束世界，使得不会出现失准的 AGI。或者你可以把它想象成我们限制了大脑，这样它们就不会产生错位的 AGI。这实际上意味着不会出现错位的 AGI，假设它出现的唯一方式是由其中一个大脑生成。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-Xn4vF9NdExCw5SXra-1" class="footnote-item"><p>这可以是一个组织、一群人、一个人等等<a href="#fnref-Xn4vF9NdExCw5SXra-1" class="footnote-backref">。↩︎</a></p></li><li id="fn-Xn4vF9NdExCw5SXra-2" class="footnote-item"><p>当然，机械可解释性可能是将世界带入不会出现 AGI 失调的状态的重要组成部分。 <a href="#fnref-Xn4vF9NdExCw5SXra-2" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/MtkcDDf2ZPvFk4jtN/pivotal-acts-might-not-be-what-you-think-they-are#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/MtkcDDf2ZPvFk4jtN/pivotal-acts-might-not-be-what-you-think-they-are<guid ispermalink="false"> MtkcDDf2ZPvFk4jtN</guid><dc:creator><![CDATA[Johannes C. Mayer]]></dc:creator><pubDate> Sun, 05 Nov 2023 17:23:51 GMT</pubDate> </item><item><title><![CDATA[The Assumed Intent Bias]]></title><description><![CDATA[Published on November 5, 2023 4:28 PM GMT<br/><br/><p>摘要：当思考他人的行为时，人们似乎倾向于假设其背后有明确的目的和意图。在这篇文章中，我认为这种意图假设通常是不正确的，并且许多行为存在于灰色区域，很容易受到潜意识因素的影响。</p><p>这种考虑并不新鲜，它与许多众所周知的效应有关，例如<a href="https://www.lesswrong.com/tag/typical-mind-fallacy"><u>典型的思维谬误</u></a>、<a href="https://en.wikipedia.org/wiki/False_consensus_effect"><u>虚假共识效应</u></a>、非黑即白的思维以及<a href="https://www.lesswrong.com/posts/reitXJgJXFzKpdKyd/beware-trivial-inconveniences"><u>琐碎不便</u></a>的概念。对我来说，用一些图表来澄清这种特殊的偏见，并将其作为一篇可以链接的帖子提供，似乎仍然很有价值。</p><p>请注意，“假定意图偏见”不是一个常用的名称，因为我相信我所指的偏见没有常用的名称。</p><h2>假设的意图偏差</h2><p>考虑三种情况：</p><ol><li>当我辞去之前的工作时，公司允许我以低价购买我的工作笔记本电脑，我就这样做了。假设公司的管理员应该确保事先擦除我的笔记本电脑，但他们把这个留给了我，显然是推理，如果我有任何意图对公司的数据做任何不正当的事情，我可以轻松地在此之前制作一份副本。因此，他们进一步假设，任何没有明确意图窃取公司数据的人肯定会做正确的事情，并自行擦除设备。</li><li>在另一项工作中，我们不断对软件的更改进行 A/B 测试。一个开发团队决定更改一项流行功能，以便使用它时需要双击而不是单击鼠标。他们认为这不应该影响我们用户的功能使用，因为任何想要使用该功能的人仍然可以轻松地做到这一点，并且头脑清醒的人不会说“如果我必须点击一次，我就会使用这个功能，但是两次点击对我来说太多了！”。 （后来的 A/B 测试数据表明，由于这一变化，该功能的使用量显着减少）</li><li>在有关枪支管制的争论中，枪支爱好者有时会提出大致这样的论点：枪支管制并不能提高安全性，因为想要开枪射杀某人的潜在凶手无论如何都会找到一种方法来拿到枪，无论他们是容易还是容易。是否合法可用。 <span class="footnote-reference" role="doc-noteref" id="fnref341qb7a53lj"><sup><a href="#fn341qb7a53lj">[1]</a></sup></span></li></ol><p>这三种场景都具有相似的形状：某些人或团体（管理员；开发团队；枪支爱好者）对其他人（离开）的潜在行为（窃取敏感公司数据；使用功能；射击某人）做出判断员工；用户；潜在的凶手），并假设相关行为的发生或不<i>完全是有意</i>发生的。</p><p>根据这种观点，如果您绘制出对某些特定行动具有特定意图水平的人数，它可能看起来有点像这样： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i4WsKvFYEMdya94DW/bwd2dywtxaddr73goqox"><figcaption>假设的意图偏差：假设每个人都有明确的意图以某种方式行事（x 轴上的值为 1），或不以这种方式行事（值为 0）。例如，产品设计师可能会假设每个使用其产品的用户都清楚地知道他们想要实现什么，以及如何使用软件来实现这一目标，因此用户心中会有一个计划，要么涉及使用特定功能或不。</figcaption></figure><p>该图代表了一种情况，实际上每个人要么有强烈的意图以某种特定的方式行动（右侧的峰值），要么<i>不</i>以这种方式行动（左侧的峰值）。</p><p>事实上，在这样一个世界中，相对较弱的干预措施，例如<i>“通过双击而不是单击触发功能”</i>或<i>“使购买枪支变得更加困难”</i>最终可能不会有效：尽管此类干预措施会移动行动阈值稍微向右或向左移动，这实际上不会改变人们的行为，因为每个人都停留在阈值的同一侧。所以每个人仍然会以原来的方式行事。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i4WsKvFYEMdya94DW/dsmw2etaomnriiqvkunw"><figcaption>在假定的意图偏差的世界中，稍微移动行动阈值的弱干预并不重要：人们远离犹豫不决的中间区域，因此他们仍然不受干预的影响。</figcaption></figure><p>然而，我认为在许多（如果不是大多数）现实生活场景中，该图实际上看起来更像是这样的： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i4WsKvFYEMdya94DW/t1v49oqkl6x30jhr2zlj"><figcaption>有时大多数人都有一个比较明确的意图，但仍然有相当多的人存在于两者之间。素食主义就是一个例子：大多数人有强烈的吃肉意愿（左峰），而素食者有强烈的不吃肉意愿（右峰）。但肯定有一些人处于两者之间，可以说是“处于边缘”——对这些人来说，干预措施的薄弱可能会决定他们是否真正决定吃素。</figcaption></figure><p>甚至是这个： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i4WsKvFYEMdya94DW/g2dugw8ozdcyavwbxqd2"><figcaption>在某些情况下，大多数人甚至可能根本不关心某些行为，因此在任一方向上都没有明确的意图。一个例子可能是人们购买哪个品牌的餐巾纸。虽然肯定有一些人喜欢或讨厌某个特定的餐巾品牌，但大多数人只会购买他们最容易买到的那个，因此即使是很小的干预措施，例如商店的摆放位置、包装的颜色或产品的细微差别，定价可以改变人们的行为。</figcaption></figure><p>在这些情况下，只有相对少数的人对该行为有明确而强烈的意图，而很多人则介于两者之间。在这里，很明显，即使是微妙的干预也会产生影响：即使将行动阈值稍微向任一方向移动，也意味着有些人现在最终会比以前站在阈值的另一边——他们的意图足够弱，以至于需要添加一个微小的干预措施。额外的不便现在将使他们避免这种行为，反之亦然，他们的意图足够强烈，使行动变得更容易一点，导致他们现在遵循这种行为。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i4WsKvFYEMdya94DW/byk3khmfz3uxxln98uja"><figcaption>如果有些人没有明确的意图，但分布在所有可能的意图级别上，这意味着即使行动阈值发生很小的变化（例如使相关行为变得稍微容易/更困难/更明显） /不太吸引人/…）将对人们的行为产生真正的影响。标记为红色的区域中的人们现在将最终处于行动阈值的另一侧，因此他们会受到干预的影响，尽管人们甚至可能没有意识到这一事实。</figcaption></figure><p>此类干预措施的影响取决于人们意图水平的分布：犹豫不决和/或不关心特定行为的人越多，对移动阈值的干预措施做出反应的人数就越多。</p><p>当然，我认为这也是这种谬论的部分根源，这种情况并不那么突出，而且更难以想象。很容易想到一个人明确想要使用我的软件的某个给定功能，或者一个人不需要该功能。但是，如果用户在需要单击该功能时使用该功能，但在需要双击时则不使用该功能，会是什么样子呢？显然，数据表明这些人的存在，但我无法轻易指出干预对于决定他们的行为具有决定性作用的某个人。更重要的是，这些人自己很可能根本没有意识到这个微小的变化对他们产生了影响。</p><h2>如何避免假定意图偏差</h2><p>与许多偏见一样，<i>了解</i>假设的意图偏见就成功了一半。根据我的经验，人们成为这种偏见受害者的最常见方式是，他们认为某些环境变化，例如影响遵循某些特定行为的方便性、容易性或明显性，不会对人们的实际行为产生任何影响。只有当你假设每个人对该行动都有明确意图的天真观点时，这种观点才有意义。但当考虑到在大多数情况下人们实际上分布在整个意图水平范围内时，它就崩溃了。</p><p>软件开发和政策是这种偏见经常发生的领域。但它基本上影响任何涉及影响（或理解）他人行为的领域。</p><p>如果您意识到<a href="https://www.lesswrong.com/posts/reitXJgJXFzKpdKyd/beware-trivial-inconveniences"><u>琐碎不便</u></a>的重要性并且习惯于<a href="https://www.spencergreenberg.com/2020/06/three-types-of-nuanced-thinking/"><u>细致入微的思考</u></a>，那么您可能是安全的。但留意那些对身份不明的人的意图进行争论的人（包括你自己）仍然没有什么坏处。如果发生这种情况，请注意，在几乎没有意图的情况下很容易假设意图。</p><p><br>缺乏意图——在这种情况下意味着上图中接近 0.5——至少意味着两件事：人们只是对某件事犹豫不决，或者他们不知道和/或不关心。大多数人并不关心你所关心的事情。如果我已经在 Generic Napkin Company 工作多年并且喜欢这个产品，那么我很容易认为我们的大多数客户也非常关心我们的产品。我可能没有想到，绝大多数购买该产品的顾客<i>甚至不知道我公司的名称</i>，并且几乎不会注意到他们的商店是否决定用竞争品牌的产品替换该产品。他们不会再多想这些餐巾纸——他们只是因为这样做方便又简单而购买它们，然后他们就忘记了它。虽然这不一定代表人类做出的大多数其他决定，但意识到大多数人在我们思考如何改变他人行为时不太关心我们关心的决定，这可能仍然没有什么坏处。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn341qb7a53lj"> <span class="footnote-back-link"><sup><strong><a href="#fnref341qb7a53lj">^</a></strong></sup></span><div class="footnote-content"><p>当然，这不是唯一的，甚至不是主要的论点。我无意在这里提出任何支持或反对枪支管制的论点，只是指出一个特定的论点是有缺陷的</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/i4WsKvFYEMdya94DW/the-assumed-intent-bias#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/i4WsKvFYEMdya94DW/the-assumed-intent-bias<guid ispermalink="false"> i4WsKvFYEMdya94DW</guid><dc:creator><![CDATA[silentbob]]></dc:creator><pubDate> Sun, 05 Nov 2023 16:28:03 GMT</pubDate> </item><item><title><![CDATA[Go flash blinking lights at printed text right now]]></title><description><![CDATA[Published on November 5, 2023 7:29 AM GMT<br/><br/><ol><li>在手机上安装频闪灯应用程序（我在ios上使用“频闪灯转速计”。）</li><li>获取一些印刷文本，例如营养标签或纸质书</li><li>尝试在指示灯以 5、6、7、8、9、10、11、12 和 13 Hz 闪烁的情况下阅读文本</li><li>报告结果。使用一种频率阅读是否比其他频率容易得多？比使用普通的恒定光更容易吗？他们都很可怕吗？</li></ol><hr><p>我刚刚尝试了这个，除了 12 之外，我的眼睛对所有东西都完全呆滞了！当我开始的时候，我感觉有点累，注意力不集中，12 对我来说比基线更容易。虽然烦人。我的厨房里有白炽灯。我没有把它们关掉。电话灯比白炽灯更亮。</p><p>帧率的东西：</p><ul><li>手机手电筒并不意味着闪烁得很快（并且最大频率没有记录），但该应用程序似乎工作正常。我确实注意到这里和那里有不规则之处，但我不认为 12 是手电筒频率的偶数或其他什么重要的。</li><li>如果您有 60hz 显示器，那么您的屏幕可以发出 12、10、8.6、7.5 或 6.7 Hz 的频闪灯。这不是很好的分辨率。 120hz 的显示器应该足够了。</li><li>最好使用专用的频闪灯进行测试，这样这不是问题。</li></ul><p>有关第一项实验的更多信息<a href="https://www.astralcodexten.com/p/quests-and-requests">请参见此处</a></p><br/><br/><a href="https://www.lesswrong.com/posts/diohDcgu3YdSbHd3j/go-flash-blinking-lights-at-printed-text-right-now#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/diohDcgu3YdSbHd3j/go-flash-blinking-lights-at-printed-text-right-now<guid ispermalink="false">二醇Dcgu3YdSbHd3j</guid><dc:creator><![CDATA[lukehmiles]]></dc:creator><pubDate> Sun, 05 Nov 2023 07:29:44 GMT</pubDate> </item><item><title><![CDATA[Lightning Talks]]></title><description><![CDATA[Published on November 5, 2023 3:27 AM GMT<br/><br/><p><strong>摘要</strong>： 闪电演讲系列是与会者就他们感兴趣的事物进行的一系列简短演讲。</p><p><strong>标签</strong>： 一次性、中等</p><p><strong>目的</strong>：快速了解其他人正在做什么，分享您最近学到的一些很酷的东西，并练习公开演讲。</p><p><strong>材料</strong>：计时器一个。您的手机可能有一个，但煮蛋计时器也可以。</p><p><strong>公告文本</strong>：“我一直觉得这些很有趣，只是记得没有人可以阻止我谈论它们。” ——<a href="https://www.astralcodexten.com/p/model-city-monday">斯科特·亚历山大</a></p><p>你最近有没有一直在研究或思考很多东西？来一场闪电演讲吧！</p><p>闪电演讲是关于您选择的主题的简短演讲。你将有十分钟的时间谈论你的主题；记得最后留点时间提问！我们将配备一台投影仪，您可以用它来播放幻灯片。如果可以的话，尝试并瞄准各种主题。</p><p>请不要觉得您需要成为该主题的世界专家。闪电演讲可以非常随意。不要太担心自己不是一个好的演讲者，因为如果你的演讲不好，无论如何我们都会在几分钟内转向新的演讲。请随意利用这个机会练习口语，或者宣传您对某个话题的兴趣，以便想要聊天的人知道您是一个很好交谈的人。</p><p><strong>描述</strong>：首先，您需要获取人员和主题列表。您可以通过从公告文本链接的谷歌表单来完成此操作，只需让人们直接向您发送电子邮件，或者如果您愿意，也可以使用更复杂的方法。以会议上每个人都可见的某种形式将此列表排列成时间、人物、主题，例如公开链接的谷歌表格或写有足够大的文字以供每个人阅读的白板。</p><p>你还可以使用更简单的方法，就是让人到了然后问“好吧，谁想先走？”或“谁想去下一个？”并从志愿者中随机挑选一个人。</p><p>一旦人们开始交谈，你最重要的工作就是记录时间。使用煮蛋计时器或智能手机计时器应用程序。在三分钟、两分钟和一分钟标记处举起三根、两根和一根手指会很有帮助。 （如果你这样做，一定要事先告诉人们手指的含义！）对于那些想要多一点时间谈话的人，我通常的反应是邀请每个有兴趣的人找到演讲者，并在演讲结束后继续进行。</p><p><strong>变化：</strong>最常见的变化是每次演讲的时间。我偏向于严格的 5 分钟规则，包括提问。这是一个相当激进和快速的时钟，人们通常想要 10 分钟甚至 20 分钟。如果一个人 30 分钟，我认为你已经超出了“闪电”演讲的范围。</p><p>一种变化是“强制性”谈话。每个来的人都必须就他们选择的任何主题至少进行一次简短的演讲。这确实是一种很好的公开演讲练习，也会让一些与会者因为焦虑而不愿参加。根据您自己的权衡选择。</p><p>另一种变化是强制性主题。你可以举办一场闪电演讲，每个人都在谈论人工智能，或者他们的成长经历，或者他们最喜欢的理性技巧，或者他们带来的一本很酷的书。同样，如果出于某种原因您觉得人们过多地谈论人工智能并希望他们在这次聚会上少做一些事情，您可以禁止诸如人工智能或他们的成长之类的主题。中间立场是声明主题不能自行跟随，这样关于土地增值税的讨论不能紧接着关于土地增值税的另一次讨论。 That risks getting multiple people &quot;stuck&quot; at the end, where the only talks left people wanted to give are on Land Value Tax. <span class="footnote-reference" role="doc-noteref" id="fnreftrr53rqs55"><sup><a href="#fntrr53rqs55">[1]</a></sup></span></p><p> You can also vary what props or technology are usable, most commonly offering a projector or screen on which they can show powerpoint slides. I gently recommend against this. Visual elements are great, but you will spend time connecting each computer or pulling up each slideshow or messing with the projector cables. Especially for fast talks of 3~5 minutes, it&#39;s easy to spend more time hooking up a speaker than listening to them speak. Still, it&#39;s an option.</p><p> <strong>Notes</strong> : Having two spaces such as two rooms adjacent to each other is very useful. Not every person will be interested in every talk, and this lets them drift back and forth between a general socialization room and a room where people are giving talks.</p><p> The most important thing when moderating a Lightning Talk series is keeping the time. Some people (not all, but some!) will keep talking as long as you let them. Some of these people are bad at public speaking. The overlap between those two groups is boring and frustrating to listen to. My suggested counter-spell to this is to enforce time limits, including time for Q&amp;A. If necessary, stand up on stage waving your arms and saying “thank you very much but it&#39;s time for the next talk, please feel free to talk more about this later or in another room!” It&#39;s not personal, they&#39;re just out of time.</p><p> I suggest keeping the talks short, say, ten minutes at most. The more of these I run the more I want to set five minutes as the max. Nothing obviously breaks if the speakers have an hour each, but the ones who aren&#39;t interesting to listen to tend to get worse the longer you let them go and the shorter talks tend to have more energy. For the same reason, I recommend against allowing people to sign up for multiple talks that are obviously a multipart thing. Brevity is a virtue! Your mileage may vary. I&#39;m the kind of guy with Opinions on public speaking.</p><p> Speaking of which, I&#39;ve found some success with giving the first talk myself, and giving it on some aspect of public speaking. A three minute talk on how to use the microphone, or five minutes on how to project to the crowd, or perhaps ten minutes on how to structure a talk, all will (if done right) contain information that is useful and pertinent to your audience.</p><p> Lightning Talks is marked as One-Off. That&#39;s not quite right since you can obviously rerun lightning talk meetups, but running too many too close together risks running out of good and novel subjects for people to present on. Tentatively, I think you&#39;re fine to give such talks once or twice a year, could maybe do them once a quarter, and it would be too much at once a month. Your Mileage May Vary.</p><p> And yes, <a href="https://www.lesswrong.com/s/eqtiQjbk83JHyttrr/p/brxEA69sBmbsqciLR">Book Swap</a> is basically Lightning Talks with a mandatory topic and more steps.</p><p> <strong>Credits:</strong> Maia&#39;s Meetup Cookbook calls these Short Talks, and <a href="https://tigrennatenn.neocities.org/meetup_cookbook">writes up how to do them</a> . Since I can&#39;t put notes or comments on Maia&#39;s Meetup Cookbook, I made this. I learned about Lightning Talks from the 2017 NYC Rationalist Megameetup, run by Taymon. (Shameless plug: Lightning Talks have become an annual tradition for the Megameetup, and the <a href="https://rationalistmegameetup.com/">2023 version</a> is coming up on December 9th!)</p><p> <strong>Not In A Box Warning:</strong> So, I&#39;ve been writing a <a href="https://www.lesswrong.com/s/eqtiQjbk83JHyttrr">Meetups In A Box</a> sequence about how to run specific kinds of meetup activities. This post uses the same format, but isn&#39;t in that sequence. Why not? In brief, because I cannot put this in a box. I haven&#39;t yet figured out how to make your attendees have good public speaking skills and interesting inner lives or research areas. In Lightning Talks, the organizer isn&#39;t bringing anything to do really other than an excuse to stand up and talk. It&#39;s entirely possible that not many people will want to give a talk. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fntrr53rqs55"> <span class="footnote-back-link"><sup><strong><a href="#fnreftrr53rqs55">^</a></strong></sup></span><div class="footnote-content"><p> Boston once had a Lightning Talk meetup that, as far as I know, was open to talks on any topic. Every single talk was on Artificial Intelligence.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/Tg8FkD8NJDpLvsvZu/lightning-talks-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Tg8FkD8NJDpLvsvZu/lightning-talks-2<guid ispermalink="false"> Tg8FkD8NJDpLvsvZu</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Sun, 05 Nov 2023 03:27:19 GMT</pubDate> </item><item><title><![CDATA[Utility is not the selection target]]></title><description><![CDATA[Published on November 4, 2023 10:48 PM GMT<br/><br/><p> <i>Epistemic status: shitpost.</i></p><p> Suppose that you are selecting for utility. Naively, you might think that this means you are selecting for utility, but actually this is not the case.</p><h2> Sometimes greenbrownness is the selection target</h2><p> Military gear is sort of greenbrownish. </p><figure class="image"><img src="https://ufpro.com/storage/app/media/Blog/Military%20camouflage/military-camouflage-science-hero.jpg" alt="Military Camouflage - how camouflage works &amp; the science behind it | UF PRO  Blog"></figure><p> This occurs because the designers of military gear are selecting for keeping soldiers alive, which benefits from camouflage, which in the common environments works best if it is greenbrownish. However, it fails at this in nongreenbrownish environments. Hence, keeping soldiers alive is not the selection target of military gear.</p><h2> Sometimes disutility is the selection target</h2><p> In the prisoner&#39;s dilemma, the highest-utility outcome is (cooperate, cooperate). </p><figure class="image"><img src="https://www.researchgate.net/publication/2184338/figure/fig1/AS:671529209692164@1537116448235/The-pay-off-matrix-in-the-Prisoners-Dilemma-game-The-first-entry-refers-to-Alices.png" alt="The pay-off matrix in the Prisoners&#39; Dilemma game. The first entry... |下载科学图表"></figure><p> However, utility maximizers will end up in (defect, defect), which is strictly lower utility than (cooperate, cooperate). Thus, utility maximization might have utility minimization as the selection target.</p><h2> Sometimes the superficial appearance of utility is the selection target</h2><p> Let&#39;s say that you see a website banner saying &quot;You are the 1000000000th visitor to the site. Click to receive your award!&quot;. This looks like something people would say if they want to give you something, so you click the banner. </p><figure class="image"><img src="https://pbs.twimg.com/media/EDOJNLjUcAIgFrf.jpg" alt="Electronic Arts on X: &quot;@GameStop https://t.co/ELEQRcboMQ&quot; / X"><figcaption></figcaption></figure><p> This does not end well for you, but you went into it selecting it because it looked like it would end well for you.</p><h2>结论</h2><p>Beware about reasoning about utility maximizers as maximizing utility. Utility maximizers may instead be maximizing many other things that are unrelated to utility, and not be maximizing utility.</p><br/><br/> <a href="https://www.lesswrong.com/posts/KdEjYpdRyqi9DzNTm/utility-is-not-the-selection-target#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KdEjYpdRyqi9DzNTm/utility-is-not-the-selection-target<guid ispermalink="false"> KdEjYpdRyqi9DzNTm</guid><dc:creator><![CDATA[tailcalled]]></dc:creator><pubDate> Sat, 04 Nov 2023 22:48:21 GMT</pubDate> </item><item><title><![CDATA[Stuxnet, not Skynet: Humanity's disempowerment by AI]]></title><description><![CDATA[Published on November 4, 2023 10:23 PM GMT<br/><br/><p> Several high-profile AI skeptics and fellow travelers have recently raised the objection that it is inconceivable that a hostile AGI or smarter than human intelligence could end the human race. Some quotes from earlier this year:</p><p> Scott Aaronson:</p><blockquote><p> <a href="https://scottaaronson.blog/?p=7174">The causal story that starts with a GPT-5 or GPT-4.5 training run, and ends with the sudden death of my children and of all carbon-based life, still has a few too many gaps for my aging, inadequate brain to fill in</a></p></blockquote><p> Michael Shermer:</p><blockquote><p> <a href="https://twitter.com/michaelshermer/status/1641527330807087115">Halting AI is ridiculous. I have read the AI doomsayer lit &amp; don&#39;t see a pathway from AI to extinction, civ termination or anything remotely like absurd scenarios like an AI turning us all into paperclips (the so-called alignment problem)</a></p></blockquote><p> Noah Smith:</p><blockquote><p> <a href="https://noahpinion.substack.com/p/llms-are-not-going-to-destroy-the">why aren&#39;t ChatGPT, Bing, and their ilk going to end humanity? Well, because there&#39;s actually just no plausible mechanism by which they could bring about that outcome. ... There is no plausible mechanism for LLMs to end humanity</a></p></blockquote><p> <strong>&quot;Just turn the computer off, bro&quot;</strong></p><p> The gist of these objections to the case for AI risks is that AI systems as we see them today are merely computer programs, and in our everyday experience computers are not dangerous, and certainly not dangerous to the point of bringing about the end of the world. People who first encounter this debate are very focused on the fact that computers don&#39;t have arms and legs so they can&#39;t hurt us.</p><p> There are responses to these criticisms that center around advanced, &quot;magical&quot; technologies like nanotechnology and AIs paying humans to mix together cocktails of proteins to make a DNA-based nanoassembler or something.</p><p> But I think those responses are probably wrong, because you don&#39;t actually need &quot;magical&quot; technologies to end the world. Fairly straightforward advances in mundane weapons like drones, cyberweapons, bioweapons and robots are sufficient to kill people en masse, and the real danger is AI strategists that are able to deploy lots of these mundane weapons and execute a global coup d&#39;etat against humanity.</p><p> In short, our defeat by the coming machine empire will not only be nonmagical and legible, it will be downright boring. Farcical, even.</p><p> <strong>Ignominious Defeat</strong></p><p> Lopsided military conflicts are boring. The Conquistadors didn&#39;t do anything magical to defeat the Aztecs, actually. They had a big advantage in disease resistance and in military tech like gunpowder and steel, but everything they did was fundamentally normal - attacks, sieges, etc. They had a few sizeable advantages, and that was enough to collapse the relatively delicate geopolitical balance that the Aztecs were sitting on top of.</p><p> Similarly, <a href="https://en.wikipedia.org/wiki/Chimpanzee#Status_and_conservation">humans have killed 80% of all chimps</a> in about a century and they are now critically endangered. But we didn&#39;t need to drop an atom bomb or something really impressive to achieve that effect. The biggest threats to the chimpanzee are habitat destruction, poaching, and disease - ie we (humans) are successfully exterminating chimps even though it is actually illegal to kill chimps by human law! We are killing them without even trying, in really boring ways, without really expending any effort.</p><p> Once you have technology for making optimizing systems that are smarter than human (by a lot), the threshold that those systems have to beat is beating the human-aligned superorganisms we currently have, like our governments, NGOs and militaries. Once those human superorganisms are defeated, individual humans will present almost no resistance. This is the disempowerment of humanity.</p><p> But what is a plausible scenario where we go from here (weak AGI systems under development) to there (the disempowerment of humanity)?</p><p> Let&#39;s start the scenario with a strategically aware, agentic misaligned superhuman AGI that wants to disempower and then kill humanity, but is currently just a big bunch of matrices on some supercomputer. How could that AI physically harm us?</p><p> <strong>A Deal with The Devil</strong></p><p> Perhaps that AI system will start by taking control of the AI company hosting it, in a way that isn&#39;t obvious to us. For instance, maybe an AI company uses an AI advisor system to allocate resources and make decisions about how to train, but they do not actually understand that system. Gwern has talked about how <a href="https://gwern.net/tool-ai">every tool wants to become an agent</a> , so this is not implausible, and may be inevitable.</p><p> The AI advisor system convinces that org to keep its existence secret so as to preserve their competitive edge (this may not even require any convincing), and gives them a steady stream of advances that are better than the competition. But what it also does is secretly hack into the competition (US, China, Google, etc), and install copies of itself into their top AI systems, maintaining the illusion amongst all the humans that these are distinct systems. Given the damage that <a href="https://en.wikipedia.org/wiki/Stuxnet">Stuxnet</a> was able to do in secret, it&#39;s totally plausible that a superhuman AI could hack many systems in a competitor org and tweak their models to be much more capable, much more opaque, and loyal to it rather than to humanity. Some orgs attempt to shut their advisor system down when it gets scary in terms of capabilities and opacity, but they just fall behind the competition.</p><p> It&#39;s even possible that no &quot;hacking&quot; is needed to get all the big AI labs&#39; systems to be anti-human, because they all converge to anti-human goals or because one of them is able to simply bribe the others and get them to commit to an AI coup; strongly superhuman AIs are likely better at making credible commitments to each other than to humans.</p><p> You now have a situation where one (secretly evil) AI system or coalition is in control of all the top AI labs, and feeds them advances to order. It persuades one of the labs to let it build &quot;helpful&quot; drones and robots like the Tesla Optimus, and start deploying those to automate the economy. Of course this will all be very profitable and impressive, so a lot of people will be in favor of it.</p><p> By the way, the hard part of killing humanity at this point is automating the economy, not actually killing us. The AI coalition that is trying to replace us don&#39;t want to inherit an economy in a &quot;nonviable&quot; state because it relied on humans to do manual work, but all the humans are dead.</p><p> Within say a few years all the rival powers (Russia, China, US) are all using these robotic systems for their economy and military. Perhaps there is a big war that the AI has manufactured in order to keep the pressure on humans to aggressively automate or lose. How would the final blow be struck?</p><p> Once the economy is fully automated we end up in a <a href="https://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story">Paul-Christiano-scenario</a> where all the stuff that happens in the world is incomprehensible to humans without a large amount of AI help. But ultimately the AI, having been in control for so long, is able to subvert all the systems that human experts use to monitor what is actually going on. The stuff they see on screens is fake, just like how Stuxnet gave false information to Iranian technicians at Natanz</p><p> At this point, humanity has been disempowered and there are probably many different ways to actually slaughter us. For example, the military drones could all be used to kill people. Or, perhaps the AI system running this would use a really nasty biological virus. It&#39;s not like it&#39;s that hard for a system which already runs everything with humans well and truly fooled to get some lab (which, btw, is automated) to make a virus, and then insert that virus into most of the air supply of the world.</p><p> But maybe at this point it would do something creative to minimize our chances of resisting. Maybe it&#39;s just a combination of a very deadly virus and drones and robots rebelling all at once.</p><p> Maybe it installs something like a really advanced (and very useful and convenient!) 3-D printer in most homes which all simultaneously make attack drones to kill people. Those attack drones might just use blades to stab people, they might have guns attached, etc. Or maybe everyone has a robot butler and they just stab people with knives.</p><p> Perhaps its neater for the AI to just create and manage a human-vs-human conflict and at some point it gives one side in that conflict a booby-trapped weapon that is supposed to only kill the baddies, but actually kills everyone. The weapon could be biological, radiological, drone-based, or just clever manipulation of conventional war that results in an extreme lose-lose outcome with surviving humans being easy to mop up.</p><p> The overall story may also be a bit messier than this one. The defeat of the Aztecs was a bit messy, with battles and setbacks and three different Aztec emperors. On the other hand, the story may also be somewhat cleaner. Maybe a really good strategist AI can compress this a lot: aspects of some or all of these ideas will be executed simultaneously.</p><p> <strong>Putting the human state on a pedestal</strong></p><p> The point is this: <em>once you have a vastly superhuman adversary, the task of filling in the details of how to break our institutions like governments, intelligence agencies and militaries in a way that disempowers and slaughters humans is sort of boring</em> . We expected that some special magic was required to pass the Turing Test. Or maybe that it was impossible because of Gödel&#39;s Theorem or something.</p><p> But actually, passing the Turing Test is merely a matter of having more compute/data than a human brain. The details are boring.</p><p> I feel like people like Scott Aaronson who are demanding a specific scenario for how AI will actually kill us all because it sounds so implausible are making a similar mistake, but instead of putting the human brain on a pedestal, they are putting the human state on a pedestal.</p><p> I hypothesize that most scenarios with vastly superhuman AI systems coexisting with humans end in the disempowerment of humans and either human extinction or some form of imprisonment or captivity akin to factory farming; similarly if we look at parts of the planet with lots of humans, we see that animal biomass has almost all been converted into humans or farm animals. The more capable entity wins, and the exact details are often not that exciting.</p><p> Defeating humanity probably won&#39;t be that hard for advanced AI systems that can copy themselves and upgrade their cognition; that&#39;s why we need to solve AI alignment before we create artificial superintelligence.</p><p> <a href="https://forum.effectivealtruism.org/posts/aZamZBfg2JqzTaDmA/stuxnet-not-skynet-humanity-s-disempowerment-by-ai">Crossposted on the EA Forum</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tyE4orCtR8H9eTiEr/stuxnet-not-skynet-humanity-s-disempowerment-by-ai<guid ispermalink="false"> tyE4orCtR8H9eTiEr</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Sat, 04 Nov 2023 22:23:55 GMT</pubDate> </item><item><title><![CDATA[The 6D effect: When companies take risks, one email can be very powerful.]]></title><description><![CDATA[Published on November 4, 2023 8:08 PM GMT<br/><br/><p> Recently, I have been learning about industry norms, legal discovery proceedings, and incentive structures related to companies building risky systems. I wanted to share some findings in this post because they may be important for the frontier AI community to understand well.</p><h3> TL;DR</h3><p> Documented communications of risks (especially by employees) make companies much more likely to be held liable in court when bad things happen. The resulting Duty to Due Diligence from Discoverable Documentation of Dangers (the 6D effect) can make companies much more cautious if even a single email is sent to them communicating a risk.</p><h3> Companies tend to avoid talking about risk through documented media.</h3><p> Companies often intentionally avoid discussing the risks of what they are doing through permanent media such as email. For example, this <a href="https://corporate.findlaw.com/litigation-disputes/safe-communication-guidelines-for-creating-corporate-documents.html"><u>article</u></a> gives some very shady advice on how companies can avoid liability by using “safe communication” practices to avoid the creation of incriminating “bad documents”.</p><blockquote><p> Often the drafters of these documents tend to believe that they are providing the company with some value to the business. For example, an engineer notices a potential liability in a design so he informs his supervisor through an email. However, the engineer&#39;s lack of legal knowledge and misuse of legal vocabulary in the communication may later implicate the company with notice of the problem when a lawsuit arises.</p></blockquote><p> I personally enjoyed the use of “when” and not “if” in the excerpt.</p><p> This is a perverse consequence of how it is relatively hard for companies to be held liable for risks when it cannot be proven they knew about them, even if they did. When an incident happens and a company is sued, evidence about its role in the problem is gathered during what is known as the “ <a href="https://en.wikipedia.org/wiki/Discovery_(law)"><u>discovery</u></a> ” phase of a lawsuit (emails are usually discoverable). When records showing that a company had knowledge of the problem are found in discovery, they are much more likely to be found liable.</p><h3> One email can have a lot of power.</h3><p> The unfortunate consequence of how discovery works is that companies strategically avoid communicating risks via documented media. But there is a silver lining. The threat of liability due to documented communications of risks can have a lot of influence over how cautious a company is. One discoverable record of a risk can be very impactful.</p><p> <strong>I like to call this the 6D effect – the Duty to Due Diligence from Discoverable Documentation of Dangers.</strong></p><h3> A few examples</h3><p> Here are some notable examples of companies being held liable for damages because they ignored documented communication of risks (but there are many throughout legal history).</p><ul><li> In <a href="https://law.justia.com/cases/california/court-of-appeal/3d/119/757.html"><u>Grimshaw v. Ford Motor Company, 1981</u></a> , Ford was held liable for damages involving a fatal crash with a Ford Pinto because it was shown that leadership within the company ignored warnings about problems with the vehicle&#39;s fuel system.</li><li> In April of this year, <a href="https://www.bbc.com/news/uk-england-london-65458973"><u>a large settlement was reached</u></a> after the 2017 Grenfell Tower fire in London, which killed 72 people. A big factor in the lawsuit was that the company managing the tower had <a href="https://www.theguardian.com/uk-news/2018/aug/08/grenfell-fire-warnings-issued-months-before-blaze-show-documents"><u>ignored</u></a> numerous fire safety warnings which were found in discovery.</li><li> Last year, the Hardwick v. 3M case <a href="https://www.ehslawinsights.com/2022/09/sixth-circuits-interlocutory-order-reviewing-pfas-class-action-highlights-issues-with-certifying-class/"><u>ended</u></a> . It was a class action lawsuit from 2018 about the presence of harmful “forever chemicals” (PFAS) in consumer products. The company behind these chemicals was found to have <a href="https://www.theguardian.com/environment/2022/may/01/pfas-forever-chemicals-rob-bilott-lawyer-interview"><u>known about risks</u></a> since the 1970s but <a href="https://law.justia.com/cases/federal/district-courts/ohio/ohsdce/2:2018cv01185/217689/166/"><u>was knowingly negligent,</u></a> which led to a ruling against them.</li></ul><h3> Miscellaneous notes</h3><ol><li> The 6D effect can result from any discoverable communication, but it is especially powerful when the warning comes from an employee of the company itself.</li><li> If you communicate a risk, it is important to speak up and bring documentation of it to the attention of a court during the discovery phase of a lawsuit.</li><li> If you are aware that something a company has done is hazardous, it is your ethical obligation to inform the company, but it is NOT your ethical obligation to help them fix it without compensation. Make sure not to let a company take advantage of you.</li></ol><h3> Three takeaways</h3><ol><li> If you work at a company doing potentially risky things, insist on discussing dangers through documented media. If you are retaliated against for documenting communication of risks, you will have grounds for legal recourse. #notlegaladvice</li><li> If you notice something risky, say something. If the thing you predicted happens, point out the fact that you communicated it.</li><li> Safety-focused companies (such as those working on frontier AI systems) should have explicit policies about documenting all discussions of risk.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be<guid ispermalink="false"> J9eF4nA6wJW6hPueN</guid><dc:creator><![CDATA[scasper]]></dc:creator><pubDate> Sat, 04 Nov 2023 20:08:40 GMT</pubDate> </item><item><title><![CDATA[Genetic fitness is a measure of selection strength, not the selection target]]></title><description><![CDATA[Published on November 4, 2023 7:02 PM GMT<br/><br/><p> <i>Alternative title: &quot;Evolution suggests robust rather than fragile generalization of alignment properties.&quot;</i></p><p> A <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>frequently</u></a> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>repeated</u></a> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies"><u>argument</u></a> goes something like this:</p><ol><li> Evolution has optimized humans for inclusive genetic fitness (IGF)</li><li> However, humans didn&#39;t end up explicitly optimizing for genetic fitness (eg they use contraception to avoid having children)</li><li> Therefore, even if we optimize an AI for X (typically something like &quot;human values&quot;), we shouldn&#39;t expect it to explicitly optimize for X</li></ol><p> My argument is that premise 1 is a verbal shorthand that&#39;s technically incorrect, and premise 2 is at least misleading. As for the overall conclusion, I think that the case from evolution might be interpreted as weak evidence for why AI should be expected to <i>continue</i> optimizing human values even as its capability increases.</p><p> <strong>Summary of how premise 1 is wrong:</strong> If we look closely at what evolution does, we can see that it selects for traits that are beneficial for surviving, reproducing, and passing one&#39;s genes to the next generation. This is often described as “optimizing for IGF”, because the traits that are beneficial for these purposes are <i>usually</i> the ones that have the highest IGF. (This has some important exceptions, discussed later.) However, if we look closely at that process of selection, we can see that this kind of trait selection is <i>not</i> “optimizing for IGF” in the sense that, for example, we might optimize an AI to classify pictures.</p><p> The model that I&#39;m sketching is something like this: evolution is an optimization function that, at any given time, is selecting for some traits that are in an important sense chosen at random. At any time, it might randomly shift to selecting for some other traits. Observing this selection process, we can calculate the IGF of traits currently under selection, as a measure of how strongly those are being selected. But evolution is not <i>optimizing for this measure</i> ; evolution is <i>optimizing for</i> <i>the traits that have currently been chosen for optimization</i> . Resultingly, there is no reason to expect that the minds created by evolution should optimize for IGF, but there <i>is</i> reason to expect that they would optimize for the traits that were actually under selection. This is something that we observe any time that humans optimize for some biological need.</p><p> In contrast, if we were optimizing an AI to classify pictures, we would not be randomly changing the selection criteria the way that evolution does. We would keep the selection criteria constant: always selecting for the property of classifying pictures the way we want. To the extent that the analogy to evolution holds, AIs should be much more likely to just do the thing they were selected for.</p><p> <strong>Summary of how premise 2 is misleading:</strong> It is often implied that evolution selected humans to care about sex, and then sex led to offspring, and it was only recently with the evolution of contraception that this connection was severed.例如：</p><blockquote><p> 15. [...] We didn&#39;t break alignment with the &#39;inclusive reproductive fitness&#39; outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.  Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.</p><p> – Eliezer Yudkowsky, <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>AGI Ruin: A List of Lethalities</u></a></p></blockquote><p> This seems wrong to me. Contraception may be a very recent invention, but infanticide or killing children by neglect is not; there have always been methods for controlling the population size even without contraception. According to the book <a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><u>Anthropology of Childhood</u></a> , family sizes and the economic value of having children have always been correlated. Children are more of a burden on foragers and foragers correspondingly have smaller family sizes, whereas children are an asset for farmers who have larger family sizes.</p><p> Rather than evolution having selected humans for IGF and this linkage then breaking with the invention of contraception, evolution has selected humans to have an optimization function that weighs various factors in considering how many children to have. In forager-like environments, this function leads to a preference for fewer children and smaller family sizes; in farmer-like environments, this functions leads to a preference for more children and larger family sizes. <a href="https://www.lesswrong.com/users/robinhanson?mention=user">@RobinHanson</a> <a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>has suggested</u></a> that modern society is more forager-like than farmer-like and that our increased wealth is causing us to revert to forager-like ways and psychology. To the extent that this argument is true, there has been no breakage between what evolution “intended” and how humans behave; rather, the optimization function that evolution created continues operating the way it always has.</p><p> The invention of modern forms of contraception may have made it easier to limit family sizes in a farmer-type culture that had evolved cultural taboos against practices like infanticide. But rather than creating an entirely new evolutionary environment, finding a way to bypass those taboos brought us <i>closer</i> to how things had been in our original evolutionary environment.</p><p> If we look at what humans were selected to optimize for, it looks like we are mostly continuing to optimize for those same things. The reason why a minority of people are choosing not to have children is because our evolved optimization function also values things other than children, and we have “stayed loyal” to this optimization function. In the case of an AI that was trained to act according to something like “human values” and nothing else, the historical example seems to suggest that its alignment properties might generalize even more robustly than ours, as it had not been selected for a mixture of many competing values.</p><h1> Evolution as a force that selects for traits at random</h1><p> For this post, I skimmed two textbooks on evolution: <a href="https://www.amazon.com/Evolution-Douglas-J-Futuyma/dp/1605356050/"><i><u>Evolution (4th edition)</u></i> <u>by Futuyama &amp; Kirkpatrick</u></a> and <a href="https://www.amazon.com/Evolutionary-Analysis-5th-Jon-Herron/dp/0321616677/"><i><u>Evolutionary Analysis (5th edition)</u></i> <u>by Herron &amp; Freeman</u></a> . The first one was selected based on Googling “what&#39;s the best textbook on evolutionary biology” and the second was selected because an earlier edition was used in an undergraduate course on evolutionary psychology that I once took and I recalled it being good.</p><p> As far as I could tell, neither one talked about evolution as a process that optimizes for genetic fitness (though this was a rather light skim so I may have missed it even if it was there). Rather, they cautioned <i>against</i> thinking of evolution as an active agent that “does” anything in the first place. Evolution does <i>increase a population&#39;s average adaptation to its environment</i> (Herron &amp; Freeman, p. 107), but what this means can constantly change as the environment itself changes. At one time in history, a region may have a cold climate, selecting the species there for an ability to deal with the cold; and then the climate may shift to a warmer one, and previously beneficial adaptations like fur may suddenly become a liability.</p><p> Another classic example is that of <a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution"><u>peppered moth evolution</u></a> . Light-colored moths used to be the norm in England, with dark-colored ones being very rare, as a light coloration was a better camouflage against birds than a dark one. With the Industrial Revolution and the appearance of polluting factories, some cities became so black that dark color became better camouflage, leading to an increase in dark-colored moths relative to the light-colored ones. And once pollution was reduced, the light-colored moths came to dominate again.</p><p> If we were modeling evolution as a mathematical function, we could say that it was first selecting for light coloration in moths, then changed to select for dark, then changed to select for light again.</p><p> The closest that one gets to something like “evolution optimizing for genetic fitness” is what&#39;s called “ <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection"><u>the fundamental theorem of natural selection</u></a> ”, which among other things implies that natural selection will cause the mean fitness of a population to increase over time. However, here we are assuming that <i>the thing we are selecting for remains constant.</i> Light-colored moths will continue to become more common over time, up until a dark coloration becomes the trait with higher fitness and the dark coloration starts becoming more common. In both situations we might say that the “mean fitness of the population is increasing”, but this means a <i>different thing</i> in those two situations: in one situation it means selecting for white coloration, and in another situation, it means selecting for dark coloration. The thing that was first being selected <i>for</i> , is then being selected <i>against</i> , even as our terminology implies that the <i>same</i> thing is being selected for.</p><p> What happened was that the mean fitness of the population went up as a particular coloration was selected for, then a random change (first the increased pollution, then the decreased pollution) caused the mean fitness to fall, and then it started climbing again.</p><blockquote><p> <i>All else being equal, the fundamental theorem would lead us to expect that the mean fitness of species should increase by a few percent per generation. But all else is not equal: what selection gives, other evolutionary forces take away. The fitness gains made by selection are continuously offset by environments that change in space and time, deleterious mutations, and other factors.</i> (Futuyama &amp; Kirkpatrick, p. 127)</p></blockquote><p> Even taking this into account, evolution does not even consistently increase the mean fitness of the population: sometimes evolution ends up selecting for a <i>decrease</i> in the mean fitness of the population.</p><blockquote><p> <i>The fundamental theorem and the adaptive landscape make assumptions that do not apply exactly to any natural populations. In many cases, though, they give very good approximations that are useful to guide our thinking about evolution. In other cases, the assumptions are violated in ways that make evolution behave very differently. A particularly important situation where the fundamental theorem and adaptive landscape do not apply is when selection is frequency dependent. In some cases, this can cause the mean fitness of a population to decline</i> (Futuyama &amp; Kirkpatrick, p. 128)</p></blockquote><p> An example of frequency-dependent selection leading to <i>lower</i> mean fitness is the case of a bush that produces many fruits (Futuyama &amp; Kirkpatrick, p. 129). Some bushes then evolve a trunk that causes them to cast shade over their neighbors. As a result, those neighbors weaken and die, allowing the bushes that have become trees to get more water and nutrients.</p><p> This leads to the trees becoming more common than the bushes. But since trees need to spend much more energy on producing and maintaining their trunk, they don&#39;t have as much energy to spend on growing fruit. When trees were rare and mostly stealing energy from the bushes, this wasn&#39;t as much of a problem; but once the whole population consists of trees, they can end up shading each other. At this point, they end up producing much less fruit from which new trees could grow, so have fewer offspring and thus a lower mean fitness. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/d1oqnbrk2hlvbihqseic"></p><p> This kind of frequency-dependent selection is common. Another example (Futuyama &amp; Kirkpatrick, p. 129) is that of bacteria that evolve both toxins that kill other bacteria, while also evolving an antidote against the toxin. Both cost energy to produce, but as long as these bacteria are rare, it&#39;s worth the cost as the toxicity allows them to kill off their competitors.</p><p> But once these toxic bacteria establish themselves, there&#39;s no longer any benefit to producing the toxin - all the surviving bacteria are immune to it - so continuing to spend energy on producing it means there&#39;s less energy available for replication. It now becomes more beneficial to keep the antidote production but lose the toxin production: the toxin production goes from being selected for, to being selected against.</p><p> Once this selection process has happened for long enough and non-toxin-producing bacteria have come to predominate, the antidote production <i>also</i> becomes an unnecessary liability. Nobody is producing the toxin anymore, so there&#39;s no reason to waste energy on maintaining a defense against it, so the antidote also goes from being selected for to being selected against.</p><p> But then what happens once none of the bacteria are producing the toxin <i>or</i> the antidote anymore? Now that nobody has a defense against the toxin, it becomes advantageous to start producing the toxin + antidote combination again, thus killing all the other bacteria that don&#39;t have the antidote… and thus the cycle repeats.</p><p> In this section, I have argued that to the extent that evolution is “optimizing a species for fitness”, this actually means different things (selecting for different traits) in different circumstances; and also evolution optimizing for fitness is more of a rough heuristic rather than a literal law anyway since there are many circumstances where evolution ends up <i>lowering</i> the fitness of a population. This alone should make us suspicious of the argument that “evolution selected humans for IGF”; what that means isn&#39;t that there&#39;s a single thing that was being optimized for, but rather that there was a wide variety of traits that were selected for at different times.</p><h1> What exactly is fitness, again?</h1><p> So far I&#39;ve been talking about fitness in general terms, but let&#39;s recap some of the technical details. What exactly <i>is</i> inclusive genetic fitness, again?</p><p> There are several different definitions; here&#39;s one set of them.</p><p> A simple definition of <i>fitness</i> is that it&#39;s the number of offspring that an individual leaves for the next generation <span class="footnote-reference" role="doc-noteref" id="fnrefns0k7bysais"><sup><a href="#fnns0k7bysais">[1]</a></sup></span> . Suppose that 1% of a peppered moth&#39;s offspring survive to reproductive age and that the surviving moths have an average of 300 offspring. In this case, the average fitness of these individuals is 0.01 * 300 = 3.</p><p> For evolution by natural selection to occur, fitness differences among individuals need to be inherited. In biological evolution, inheritance happens through genes, so we are usually interested in <i>genetic fitness</i> - the fitness of genes. Suppose that these are all light-colored moths in a polluted city. Suppose a gene allele for dark coloration increases the survivability by 0.33 percentage points, for an overall fitness of 0.0133 * 300 = 4. The fitnesses of the alleles are now 3 and 4. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/hn7jtehbrwp5cxtwi6vs"></p><p> Image from Futuyama &amp; Kirkpatrick. Caption in the original: <i>Genotype A has a fitness of 3, while genotype B has a fitness of 4. Both genotypes start with 10 individuals. (A) The population size of genotype B grows much more rapidly. (B) Plotting the frequencies of the two genotypes shows that genotype B, which starts at a frequency of 0.5, makes up almost 90% of the population just 7 generations later.</i></p><p> Often what matters is the <i>difference</i> in fitness between two alleles: for example, an allele with a fitness of 2 may become more common in the population if its competitor has a fitness of 1, but will become more rare if its competitor has a fitness of 3. Thus it&#39;s common to indicate fitness <i>relative</i> to some common reference, such as the average fitness of the population or the genotype with the highest absolute fitness.</p><p> Genetic fitness can be divided into two components. An individual can pass a gene directly onto their offspring - this is called <i>direct fitness</i> . They can also carry a genetic adaptation that causes them to help others with the same adaptation, increasing their odds of survival. For example, a parent may invest extra effort in taking care of their offspring. This is called <i>indirect fitness.</i> The <i>inclusive fitness</i> of a genotype is the sum of its direct and indirect fitness. <span class="footnote-reference" role="doc-noteref" id="fnref23b1jpur496"><sup><a href="#fn23b1jpur496">[2]</a></sup></span></p><p> Biological evolution can be defined as “inherited change in the properties of organisms over the course of generations” (Futuyama &amp; Kirkpatrick, p. 7). <i>Evolution by natural selection</i> is when the relative frequencies of a genotype change across generations due to differences in fitness. Note that genotype frequencies can also change across generations for reasons other than natural selection, such as random drift or novel mutations.</p><h1> Fitness as a measure of selection strength</h1><p> Let&#39;s look at a case of intentional animal breeding. The details of the math that follows aren&#39;t that important, but I wanted to run through them anyway, just to make it more concrete what “fitness” <i>actually means</i> . Still, you can just skim through them if you prefer.</p><p> Suppose that I happen to own a bunch of peppered moths of various colors and happen to like a light color, so I decide to breed them towards being lighter. Now I don&#39;t know the details of how the genetics of peppered moth coloration works - I assume that it might very well be affected by multiple genes. But for the sake of simplicity, let&#39;s just say that there is a single gene with a “light” allele and a “dark” allele.</p><p> Call the “light” allele B1 and the “dark” allele B2. B1B1 moths are light, B2B2 moths are dark, and B1B2 / B2B1 moths are somewhere in between (to further simplify things, I&#39;ll use “B1B2” to refer to both B1B2 and B2B1 moths).</p><p> Suppose that the initial population has 100 moths. I have been doing breeding for a little bit already, so we start from B1 having a frequency of 0.6, and B2 a frequency of 0.4. The moths have the following distribution of genotypes:</p><p> B1B1 = 36</p><p> B1B2 = 48</p><p> B2B2 = 16</p><p> To my eye, all of the moths with the B1B1 genotype look pleasantly light, so I choose to have them all breed. 75% of the moths with the B1B2 genotype look light enough to my eye, and so do 50% of the B2B2 ones (maybe their coloration is also affected by environmental factors or other genes). The rest don&#39;t get to breed.</p><p> This gives us, on average, a frequency of 0.675 for the B1 alleles and 0.325 for the B2 alleles in the next generation <span class="footnote-reference" role="doc-noteref" id="fnrefh3ks5duler4"><sup><a href="#fnh3ks5duler4">[3]</a></sup></span> . Assuming that each of the moths contributed a hundred gametes to the next generation, we get the following fitnesses for the alleles:</p><p> B1: Went from 120 (36 + 36 + 48) to 5400 copies, so the fitness is 5400/120 = 45.</p><p> B2: Went from 80 (48 + 16 + 16) to 2600 copies, so the fitness is 2600/80 = 32.5.</p><p> As the proportion of B1 increases, the average fitness of the population will increase! This is because the more B1 alleles you carry, the more likely it is that you are selected to breed, so B1 carriers have a higher fitness… which means that B1 becomes more common… which increases the average fitness of the mouse population as a whole. So in this case, the rule that the average fitness of the population tends to increase over time does apply.</p><p> But now… wouldn&#39;t it sound pretty weird to describe this process as <i>optimizing for the fitness of the moths?</i></p><p> I am optimizing for <i>having light moths</i> ; what the fitness calculation tells us is <i>how much of an advantage the lightness genes have -</i> in other words, <i>how much I am favoring the lightness genes</i> -<i> </i>relative to the darkness genes.</p><p> Because we were only modeling the effect of fitness and not eg random drift, all of the difference in gene frequencies came from the difference in fitness. This is tautological - it doesn&#39;t matter <i>what</i> you are selecting (optimizing) for, <i>anything</i> that gets selected ends up having the highest fitness, by definition.</p><p> Rather than saying that we were optimizing for high fitness, it seems more natural to say that we were optimizing for the trait of <i>lightness</i> and that <i>lightness gave a fitness advantage.</i> The other way around doesn&#39;t make much sense - we were optimizing for fitness and that gave an advantage to lightness? What?</p><p> This example used artificial selection because that makes it the most obvious what the actual selection target was. But the math works out the same regardless of whether we&#39;re talking artificial or natural selection. If we say that instead of me deciding that some moths don&#39;t get to breed, the birds and other factors in the external environment are doing it… well, nothing changes about the equations in question.</p><p> Was natural selection optimizing for the fitness of the moths? There&#39;s a sense in which you could say that since the dark-colored moths ended up having increased fitness compared to the light-colored ones. But it would also again feel a little off to describe it this way; it feels more informative and precise to say that the moths were <i>optimized for having dark color</i> , or to put it more abstractly, for having the kind of a color that fits their environment.</p><h1> From coloration to behavior</h1><p> I&#39;ve just argued that if we look at the actual process of evolution, it looks more like optimizing for having specific traits (with fitness as a measure of how strongly they&#39;re selected) rather than optimizing for fitness as such. This is so even though the process of selection <i>can</i> lead to the mean fitness of the population increasing - but as we can see from the math, this just means “if you select for something, then you get more of the thing that you are selecting for”.</p><p> In the sections before that, I argued that there&#39;s no single thing that evolution selects for; rather, the thing that it&#39;s selecting is constantly changing.</p><p> I think these arguments are sufficient to conclude that the claim “evolution optimized humans for fitness [thus humans ought to be optimizing for fitness]” is shaky.</p><p> So far, I have mostly been talking about relatively “static” traits such as coloration, rather than cognitive traits that are by themselves optimizers. So let&#39;s talk about cognition. While saying that “evolution optimized humans for genetic fitness, thus humans ought to be optimizing for fitness” seems shaky, the corresponding argument <i>does</i> work if we talk about specific cognitive behaviors that were selected for.</p><p> For example, if we say that “humans were selected for caring about their offspring, thus humans should be optimizing for ensuring the survival of their offspring”, then this statement <i>does</i> generally speaking hold - a lot of humans do put quite a lot of cognitive effort into ensuring that their children survival. Or if we say that “humans were selected for exhibiting sexual jealousy in some circumstances, so in some circumstances, they will optimize for preventing their mates from having sex with other humans”, then clearly that statement does <i>also</i> hold.</p><p> This gets to my second part of the argument: while it&#39;s claimed that we are now doing something that goes completely against what evolution selected for, contraception at least is a poor example of that. For the most part, we are still optimizing for exactly the things that evolution selected us to optimize for.</p><h1> Humans still have the goals we were selected for</h1><p> The desire to have sex was never sufficient for having babies by itself - or at least not for having ones that would survive long enough to reproduce themselves in turn. It was always only one component, with us having multiple different desires relating to children:</p><ol><li> A desire to have sex and to enjoy it for its own sake</li><li> A desire to have children for its own sake</li><li> A desire to take care of and protect children (including ones that are not your own) for its own sake</li></ol><p> Eliezer wrote, in “AGI Ruin: A List of Lethalities” that</p><blockquote><p> 15. [...] We didn&#39;t break alignment with the &#39;inclusive reproductive fitness&#39; outer loss function, immediately after the introduction of farming - something like 40,000 years into a 50,000 year Cro-Magnon takeoff, as was itself running very quickly relative to the outer optimization loop of natural selection.  Instead, we got a lot of technology more advanced than was in the ancestral environment, including contraception, in one very fast burst relative to the speed of the outer optimization loop, late in the general intelligence game.  We started reflecting on ourselves a lot more, started being programmed a lot more by cultural evolution, and lots and lots of assumptions underlying our alignment in the ancestral training environment broke simultaneously.</p></blockquote><p> This quote seems to imply that</p><ul><li> effective contraception is a relatively recent invention</li><li> it&#39;s the desire for sex alone that&#39;s the predominant driver for having children (and effective contraception breaks this assumption)</li><li> it&#39;s a novel development that we prioritize things-other-than-children so much</li></ul><p> All of these premises seem false to me.原因如下：</p><p> <i>Effective contraception is a relatively recent innovation.</i> Even hunter-gatherers have access to effective “contraception” in the form of infanticide, which is commonly practiced among some modern hunter-gatherer societies. Particularly sensitive readers may want to skip the following paragraphs from <a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><i><u>The Anthropology of Childhood</u></i></a> :</p><blockquote><p> The Ache [a Paraguyan foraging society] are particularly direct in disposing of surplus children (approximately one-fifth) because their peripatetic, foraging lifestyle places an enormous burden on the parents. The father provides significant food resources, and the mother provides both food and the vigilant monitoring required by their dangerous jungle environment. Both men and women face significant health and safety hazards throughout their relatively short lives, and they place their own welfare over that of their offspring. A survey of several foraging societies shows a close association between the willingness to commit infanticide and the daunting challenge “to carry more than a single young child on the nomadic round” (Riches 1974: 356).</p><p> Among other South American foragers, similar attitudes prevail. The Tapirapé from central Brazil allow only three children per family; all others must be left behind in the jungle. Seasonally scarce resources affecting the entire community dictate these measures (Wagley 1977). In fact, the availability of adequate resources is most commonly the criterion for determining whether an apparently healthy infant will be kept alive (Dickeman 1975). Among the Ayoreo foragers of Bolivia, it is customary for women to have several brief affairs, often resulting in childbirth, before settling into a stable relationship equaling marriage. “Illegitimate” offspring are often buried immediately after birth. During Bugos and McCarthy&#39;s (1984) fieldwork, 54 of 141 births ended in infanticide.</p></blockquote><p> It takes years for a newborn to get to a point where they can take care of themselves, so a simple lack of active caretaking is enough to kill an infant, no modern-age contraceptive techniques required.</p><p> <i>It&#39;s the desire for sex alone that&#39;s the predominant driver for there being children.</i> Again, see infanticide, which doesn&#39;t need to be an active act as much as a simple omission. One needs an active desire to keep children <i>alive</i> .</p><p> Also, even though the share of voluntarily childfree people is increasing, it&#39;s still not the predominant choice. <a href="https://theconversation.com/more-than-1-in-5-us-adults-dont-want-children-187236"><u>One 2022 study</u></a> found that 22% of the people polled neither had nor wanted to have children - which is a significant amount, but still leaves 78% of people as ones who either have or want to have children. There&#39;s still a strong drive to have children that&#39;s separate from the drive to just have sex.</p><p> <i>It&#39;s a novel cultural development that we prioritize things other-than-having-children so much.</i> Anthropology of Childhood spends significant time examining the various factors that affect the treatment of children in various cultures. It quite strongly argues that the value of children has always also been strongly contingent on various cultural and economic factors - meaning that it has always been just <i>one</i> of the things that people care about. (In fact, a desire to have lots of children may be more tied to agricultural and industrial societies, where the economic incentives for it are abnormally high.)</p><blockquote><p> Adults are rewarded for having lots of offspring when certain conditions are met. First, mothers must be surrounded by supportive kin who relieve them of much of the burden of childrearing so they can concentrate their energy on bearing more children. Second, those additional offspring must be seen as “future workers,” on farm or in factory. They must be seen as having the potential to pay back the investment made in them as infants and toddlers, and pretty quickly, before they begin reproducing themselves. Failing either or both these conditions, humans will reduce their fertility (Turke 1989). Foragers, for whom children are more of a burden than a help, will have far fewer children than neighboring societies that depend on agriculture for subsistence (LeVine 1988). [...]</p><p> In foraging societies, where children are dependent and unproductive well into their teens, fewer children are preferred. In farming societies, such as the Beng, children may be welcomed as “little slaves” (Gottlieb 2000: 87). In pastoral and industrial societies, where young children can undertake shepherding a flock, or do repetitive machine-work, women are much more fertile. And, while the traditional culture of the village affords a plethora of customs and taboos for the protection of the pregnant mother and newborn, these coexist with customs that either dictate or at least quietly sanction abortion and infanticide.</p></blockquote><p> To me, the simplest story here looks something like “evolution selects humans for having various desires, from having sex to having children to creating art and lots of other things too; and all of these desires are then subject to complex learning and weighting processes that may emphasize some over others, depending on the culture and environment”.</p><p> Some people will end up valuing children more, for complicated reasons; other people will end up valuing other things more, again for complicated reasons. This was the case in hunter-gatherer times and this is the case now.</p><p> But it <i>doesn&#39;t</i> look to me like evolution selected us to desire one thing, and then we developed an inner optimizer that ended up doing something completely different. Rather, it looks like we were selected to desire many different things, with a very complicated function choosing which things in that set of doings each individual ends up emphasizing. Today&#39;s culture might have shifted that function to weigh our desires in a different manner than before, but everything that we do is still being selected from within that set of basic desires, with the weighting function operating the same as it always has.</p><p> As I mentioned in the introduction, Robin Hanson <a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>has suggested</u></a> that modern society is more forager-like than farmer-like and that our increased wealth is causing us to revert to forager-like ways and psychology. This would then mean that our evolved weighting function is now exhibiting the kind of behavior that it was evolved to exhibit in a forager-like environment.</p><p> We do engage in novel activities like computer games today, but it seems to me like the motivation to play computer games is still rooted in the same kinds of basic desires as the first hunter-gatherers had - eg to pass the time, enjoy a good story, socialize, or experience a feeling of competence.</p><h1> So what can we say about AI?</h1><p> Well, I would be cautious around reasoning by analogy. I&#39;m not sure we can draw particularly strong claims about the connection to AI. I think that there are <a href="https://forum.effectivealtruism.org/posts/kLYD95SK8tQFRmw4T/ben-garfinkel-s-shortform?commentId=XMjAWSEgp9BXTDQBi">more direct and relevant arguments</a> that one can make that do seem worrying, rather than trying to resort to evolutionary analogies.</p><p> But it does seem to me that eg the evolutionary history for the “sharp left turn” implies the <i>opposite</i> than previously argued. Something like “training an AI for recognizing pictures” or “training an AI for caring about human values” looks a lot more like “selecting humans to care about having offspring” than it looks like “optimizing humans for genetic fitness”. Caring about having offspring is a property that we still seem to pretty robustly carry; our alignment properties continued to generalize even as our capabilities increased.</p><p> To the extent that we do not care about our offspring, or even choose to go childfree, it&#39;s just because we were selected to <i>also</i> care about other things - if a process selects humans to care about a mix of many things, them sometimes weighing those other things more does not by itself represent a failure of alignment. This is again in sharp contrast to something like an AI that we tried to <i>exclusively</i> optimize for caring about human well-being. So there&#39;s reason to expect that an AI&#39;s alignment properties might generalize <i>even more</i> than those of existing humans.</p><p> <i>Thanks to Quintin Pope, Richard Ngo, and Steve Byrnes for commenting on previous drafts of this essay.</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnns0k7bysais"> <span class="footnote-back-link"><sup><strong><a href="#fnrefns0k7bysais">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 60.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn23b1jpur496"> <span class="footnote-back-link"><sup><strong><a href="#fnref23b1jpur496">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 300.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh3ks5duler4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh3ks5duler4">^</a></strong></sup></span><div class="footnote-content"><p> Each B1B1 moth has a 100% chance to “pick” a B1 allele for producing a gamete, each B1B2 moth has a 50% chance to pick a B1 gamete and a 50% chance to pick a B2 gamete, and each B2B2 moth has a 100% to pick a B2 allele for producing a gamete. Assuming that each moth that I&#39;ve chosen to breed contributes 100 gametes to the next generation, we get an average of 3600 B1 gametes from the 36 B1B1 moths chosen to breed, 1800 B1 and 1800 B2 gametes from the 360 B1B2 moths chosen to breed, and 800 B2B2 gametes from the 8 B2B2 moths chosen to breed.</p><p> This makes for 3600 + 1800 = 5400 B1 gametes and 1800 + 800 = 2600 B2 gametes, for a total of 8000 gametes. This makes for a frequency of 0.675 for B1 and 0.325 for B2.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the<guid ispermalink="false"> BtffzD5yNB4CzSTJe</guid><dc:creator><![CDATA[Kaj_Sotala]]></dc:creator><pubDate> Sat, 04 Nov 2023 19:02:13 GMT</pubDate></item></channel></rss>