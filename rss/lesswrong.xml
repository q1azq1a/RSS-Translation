<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 17 日星期日 10:11:49 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Linkpost: Francesca v Harvard]]></title><description><![CDATA[Published on December 17, 2023 6:18 AM GMT<br/><br/><p>正当我以为吉诺案已经结束时，我开始阅读弗朗西斯卡·吉诺的辩护。它将她描绘成一位饱受折磨的学者，受到误导和无能的调查人员的不公平惩罚，同时也是一位有偏见的<i>《纽约客》</i>记者。</p><p>我在这里重新发布这篇文章，因为我认为看到每个案例的双方都是有用的，与最近的、更本地化的事件有趣地平行。</p><p> _____<br><br><strong>打破我的沉默</strong></p><p>自从得知哈佛商学院让我休无薪行政假、并禁止我进入校园、教学和研究以来，我已经近乎沉默地坐着了三个多月。眼睁睁地看着我的职业生涯被毁，我的声誉被彻底摧毁，这真是令人心碎。很难看出这种情况如何影响我周围的人——我的家人、我的导师、我的合作者和我的学生。</p><p>公众可以获得的信息以及批评者发表的分析可能听起来很有说服力。但这些信息不完整且具有误导性。该记录需要更正。这个网站就是我的尝试。</p><p>让纠正从这个简单而明确的声明开始：<strong>我绝对没有犯学术欺诈。</strong><br><br> <strong>____</strong></p><p></p><p>在<a href="https://www.francesca-v-harvard.org/data-colada-post-1">之前的一篇文章</a>中，我通过证明以下内容反驳了 Data Colada 对 PNAS 论文的批评：</p><p> Data Colada 精心挑选了包含在分析中的数据，</p><p> Data Colada 排除了研究中的第三种情况，</p><p> Data Colada 在其分析中排除了 3 个因变量中的 2 个，并且</p><p>Data Colada 完全歪曲了 Excel calcChain 函数来支持他们的主张。</p><p>总之，这使得 Data Colada 对我的工作产生了极大的误导性印象。<i>重要的是，我还证明，如果排除 Data Colada 声称应被视为“可疑”的所有数据观察结果，那么该研究的结果仍然成立，<strong>这表明我没有操纵数据的动机</strong>。</i></p><p>在最近的一篇文章中，Data Colada 进一步强调了他们的说法，认为哈佛商学院的调查在这项特定研究中发现了数据操纵的<strong>更多</strong>证据。</p><p>这个额外的证据是什么？</p><p>简而言之，哈佛商学院声称如下：</p><p>哈佛商学院声称它能够获得相关研究的原始（未发表）数据集。</p><p>哈佛商学院声称，在一家取证公司的帮助下，它能够将这个原始数据集与 OSF 上发布的数据集进行比较。</p><p>哈佛商学院声称这种比较产生了显着差异。</p><p>哈佛商学院声称这些差异是我欺诈的证据。</p><p>现在我已经能够获得自己的法医团队的帮助，我已经能够对此事进行自己的调查。在下面的分析中，我提供了我的发现并反驳了哈佛商学院提出的每一项主张。</p><p> [...]</p><h3><strong>像这样的研究通常涉及多个数据集文件。</strong></h3><p><br>我和我的团队花了一段时间才解开此案的证据，但我们明确的结论是，哈佛商学院在调查中使用了错误的数据集。他们使用了 7 月 16 日的 HBS 版本，而他们应该有 7 月 16 日的 OG 版本。</p><p>在解释我们如何得出这个结论之前，请允许我指出，我不认为梅德斯通应该为此负责。相反，这是一个重要的细节，<i>梅德斯通</i><i>反复表示，他们不确定 7 月 16 日的哈佛商学院版本是他们应该依赖的数据集进行分析</i>。在他们的报告中，他们反复提出了对此的警告，并指出有理由相信可能存在他们不知情的其他数据集，因此他们可能正在查看错误的文件。梅德斯通还明确指出，它在分析中依赖于“客户对出处的描述”（客户是哈佛商学院），明显试图与 7 月 16 日哈佛商学院版本不是所使用的正确数据集的可能性保持距离。 。</p><p>正如我所说，我相信 7 月 16 日的 OG 文件是哈佛商学院在调查中应该使用的版本。该文件具有以下特点：</p><p>它包含 7 月 13 日版本的所有数据，但一些似乎已被丢弃或更正的数据除外。</p><p>它包含 7 月 13 日版本中未包含的一些附加数据。</p><p><i><strong>它与 OSF 上发布的数据完全一致</strong></i>，除了少数例外（带有一些汇总统计数据的分析条目）。</p><p>以下各节将探讨其中的每一个特征。</p><p> ____</p><br/><br/> <a href="https://www.lesswrong.com/posts/cGLP5iEidrJQ7JhDJ/linkpost-francesca-v-harvard#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cGLP5iEidrJQ7JhDJ/linkpost-francesca-v-harvard<guid ispermalink="false"> cGLP5iEidrJQ7JhDJ</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Sun, 17 Dec 2023 06:18:06 GMT</pubDate> </item><item><title><![CDATA[Lessons from massaging myself, others, dogs, and cats
]]></title><description><![CDATA[Published on December 17, 2023 4:28 AM GMT<br/><br/><p><strong>你可以自己按摩一下。</strong>一旦你学会了按摩自己，按摩其他人、狗和猫几乎是一样的。</p><p>我确信 YouTube 上有无数关于如何自我按摩的视频，但我一个都没看过。我刚刚开始挤压我的肌肉并做更多感觉良好的事情。</p><p>去年，我还教了大约 50 个人如何按摩。最近我一直在教人们如何按摩别人：</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38eff95c-fcae-4e42-ba50-fe2462118cef_1024x768.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/alewui1zxiggkomdtsa6" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/fthxlxkebpodsa8bydoz 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/dgnsg3wgsfvwob2vo9ck 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/z18jkrwa6qcgobsorzrq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/alewui1zxiggkomdtsa6 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6a480e5-6249-4f47-acad-4bd802a3734f_1024x769.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/qmlp86mcxdisztumfqhi" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/ok7gm1c6bdepotmddvpl 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/wtpczy06cyquozjynidc 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/rmjr0epfu4j9nlsz4zaq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/qmlp86mcxdisztumfqhi 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2f8c6258-549a-4df9-ae43-34269a38f439_1024x769.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/jlehb97mbw1bmmrytbgf" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/l4vamnvmpgk4nuwqa6si 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/oijbk0rxskg23ppgh9rt 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/xfijkcj7fq4hl0rgds74 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Tarwxa6STqGK8AfJR/jlehb97mbw1bmmrytbgf 1456w"></a></p><h2>这非常简单：</h2><ul><li>找到你的肌肉。</li><li>用比你想象的更大的压力挤压它们。我不会具体说明如何执行此操作。 <strong>**只是探索、好奇并尝试随机的事情。**</strong></li><li>跟随你的直觉。多做一些感觉不错的事情。</li></ul><h2>复杂性：</h2><ul><li>在按摩之前，请确保您按摩的任何肌肉都处于放松状态。这其实很难做到。<ul><li>如果按摩任何部位感到疼痛，几乎总是因为肌肉以某种方式紧张。通常，可以通过以不支撑自身的方式重新定位身体部位来解决此问题。 （例如：当手臂/腿支撑在半空中时，请勿按摩手臂/腿）</li></ul></li><li>挤脂肪的感觉不太好。<ul><li>例如，如果您的手臂上有脂肪，则必须穿过脂肪到达肌肉才能挤压它。</li></ul></li><li>最终，您将了解根据肌肉的大小向肌肉施加多大的力。</li><li>按摩能力<i>可能</i>很大程度上取决于握力。有人告诉我，我有一双钢铁之手，所以也许我的技术并不高，而只是强壮。</li><li>按摩身体的非肌肉部位有时也感觉很好。轻轻地在骨头上摩擦非常薄的软物质，效果会非常好。</li><li>我可能还缺少很多其他隐性知识。有时请随时让我按摩您的手臂、手等。</li></ul><h2>给别人按摩：</h2><ul><li>如果您想按摩其他人，我的主要建议是您<strong>首先学会按摩自己或至少是自己身体的一部分。</strong>然后你就会明白它是如何工作的，并可以将其应用到其他人身上。好像有70%一样。</li><li>当按摩其他人时，我的经验法则是增加压力，直到他们说“噢”。然后将压力降低 5%。每次去新区域时都这样做。<ul><li>如果他们没有说“噢”，那说明你还不够雄心勃勃。</li></ul></li><li>当第一次开始按摩身体部位并且他们不习惯你时，开始时可以减轻压力。前戏<ul><li>如果它们怕痒的话，也是同样的情况。只需将双手尽可能用力放在那里，不要让它们发痒一分钟左右。</li></ul></li><li>让按摩师提供尽可能多的实时反馈。哼哼和叹息比言语更好。我发现观察他们的表情很有帮助。<ul><li>但大多数情况下，你应该凭自己的直觉来判断什么会让<i>你</i>感觉良好，而不是他们觉得什么好。学习按摩的反馈循环，你自己按摩比按摩别人要好得多。</li></ul></li><li>您也可以利用这些知识来按摩猫和狗。猫和狗经常喜欢被按摩，通常比抓挠更喜欢。他们只是肌肉较小，需要的压力较小，仅此而已。<ul><li>如果他们不喜欢，他们就会起身离开。</li></ul></li></ul><h2>熟悉你的身体：</h2><ul><li>对我来说，按摩是我了解自己身体的好方法。</li><li>它也自然会导致<a href="https://chipmonk.substack.com/p/intuitive-stretching-is-really-fun">直觉的伸展</a>。</li><li>我认为按摩的理想用途是当您自己的身体紧张时作为一种学习方式。我想，理想情况下，你永远不需要或几乎不需要按摩自己，因为你已经知道如何让自己如此放松。但通过按摩自己，你会开始注意自己紧张的地方。</li></ul><h2>警告？</h2><ul><li>按摩可能会引起问题，我不知道。我曾经伤过别人的手，但他们告诉我按摩仍然值得。</li><li>有人告诉我，按摩孕妇腹部的某个部位会导致流产吗？</li></ul><p> <a href="You can just massage yourself. Then once you learn to massage yourself massaging other people, dogs, and cats is almost the same.  I’m sure there are a bazillion youtube videos about how to massage yourself, but I haven’t watched any of them. I just started squeezing my muscles and did more of what feels nice.  In the last year I’ve taught ~50 people how to massage, too. More recently I’ve been teaching people how to massage others:        It’s super easy:  Find your muscles.  Squeeze them with more pressure than you think. I’m not going to specify precisely how to do this. **Just explore, be curious, and try random things.**  Follow your intuition. Do more of what feels nice.   Complexities:  Make sure that any muscle you massage is relaxed before you massage it. This is actually hard to do.   If massaging any area hurts, it’s nearly always because the muscle is tensed in some way. Usually this can be resolved by repositioning the body part in a way that it isn’t supporting itself. (E.g.: Don’t massage arms/legs while they are supporting themself in midair)  Squeezing fat doesn’t feel nice.   If you have a fat on your arm for example, you have to reach through the fat to the muscle to squeeze it.  Eventually you learn how much force to apply to a muscle in accordance to how large the muscle is.  Massage ability might be very dependent on grip strength. I’ve been told that I have hands of iron, so maybe I’m not skilled so much as just strong.  Massaging non-muscle body-parts sometimes also feels nice. Lightly rubbing the very thin amount of soft matter on top of bones can be quite nice.  There’s probably a bunch of other tacit knowledge I’m missing. Feel free to ask me to massage your arm hand etc. sometime.  Massaging others:  If you want to massage other people, my main advice is that you learn to massage yourself or at least that body part of yourself first. Then you understand how it works and can apply that to others. It’s like 70% the same.   When massaging other people, my rule of thumb is to increase the pressure until they say “ow”. Then decrease the pressure by 5%. Do this every time you go to a new area.   If they aren’t saying “ow” you aren’t being ambitious enough.  When first starting to massage a body part and they’re not used to you, start off with less pressure. Foreplay  Same thing also if they’re ticklish. Just rest your hands there with as much pressure as you can do without tickling them for a minute or so.  Get your massagee to give as much real-time feedback as possible. Hums and sighs are better than words. I find that it helps to watch their face.  But mostly you should be going by your own intuition of what would feel nice on you rather than what they find nice. The feedback loops of learning massage are way better from you massaging yourself than you massaging others.  You can use this knowledge to massage cats and dogs, too. Cats and dogs often relish being massaged, often more than scratches. They just have smaller muscles that need less pressure, that’s all.   If they don’t like it they will get up and leave.  Getting acquainted with your body:  For me, massage has been a great way for me to get to know my body.  It also naturally leads into intuitive stretching.  I imagine that the ideal use of massage is as a way to learn when your own body is tense. I imagine that, ideally, you never or almost never need to massage yourself because you’ve figured out how to relax yourself so much. But by massaging yourself you start to pay attention to where you are tense.  Warnings?  Massage might cause problems, idk. I hurt someone’s hand once but they told me the massage was still worth it.  Someone once told me that massaging a particular spot on a pregnant woman’s abdomen can cause miscarriage?">交叉发布到我的博客</a></p><br/><br/><a href="https://www.lesswrong.com/posts/Tarwxa6STqGK8AfJR/lessons-from-massaging-myself-others-dogs-and-cats#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Tarwxa6STqGK8AfJR/lessons-from-massaging-myself-others-dogs-and-cats<guid ispermalink="false"> Tarwxa6STqGK8AfJR</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Sun, 17 Dec 2023 04:28:40 GMT</pubDate> </item><item><title><![CDATA[The Serendipity of Density]]></title><description><![CDATA[Published on December 17, 2023 3:50 AM GMT<br/><br/><p><span>在萨默维尔生活，我真正欣赏的一件事是密度如何促进意外发现。更高的密度意味着我们可能想去的更多地方都在步行距离之内，然后步行（再次是高密度）意味着我们更有可能遇到朋友。</span></p><p>今天下午，我们陪莉莉（9 岁）去过夜，路过附近的一个游乐场。莉莉和安娜（7 岁）看到了一些学校的朋友，跑上前去打招呼。安娜想留下来玩，我问其中一位家长，在我送完车后，他们是否愿意接受安娜。他们很高兴（我们带了彼此的孩子很多），安娜度过了更有趣的半个小时。然后，我回来后，我们和朋友在操场上玩了一会儿，然后才回家吃晚饭。</p><p>这当然是可以通过大量沟通有意安排的，但如果我们一直在开车，我希望安娜最终不会和她的朋友们一起度过这段时间。我猜大多数时候我们出去都会遇到某人，尽管我们停下来一起出去玩比留下一个孩子然后分开要常见得多。</p><p>在我长大的西梅德福社区，这种事情根本不常见。这个区域离这里不远，但密度可能只有三分之一：地块面积大约是这里的两倍，而且有更多的单身公寓。 -家庭住宅。距离意味着我们通常会开车，即使我们的家人决定主要步行，外出散步时也不会遇到朋友，除非您的朋友也倾向于步行。另一个贡献是我们孩子的学校非常近，并且大部分来自附近的社区，这意味着他们的朋友几乎都在步行距离之内。而我的小学吸引了来自梅德福各地的学生，而我只有一位同学可以步行去他家，而且六年中只有两年。</p><p>美国人经常担心人口密度的增加会导致他们的社区变得缺乏人情味。作为居住在该国人口最稠密的城市之一的人，我认为这是倒退的：邻近促进社区发展。</p><br/><br/><a href="https://www.lesswrong.com/posts/iS2qsNBxkmDbPpfk8/the-serendipity-of-density#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iS2qsNBxkmDbPpfk8/the-serendipity-of-密度<guid ispermalink="false">iS2qsNBxkmDbPpfk8</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 17 Dec 2023 03:50:05 GMT</pubDate> </item><item><title><![CDATA[Bounty: Diverse hard tasks for LLM agents]]></title><description><![CDATA[Published on December 17, 2023 1:04 AM GMT<br/><br/><h1>概括</h1><p>我们正在寻找 (1) 想法、(2) 详细规范和 (3) 经过充分测试的任务实现，以衡量自主 LLM 代理的性能。</p><p>任务关键需求的快速描述：</p><ul><li>不太容易：专业人士需要超过 2 小时才能完成，理想情况下有些需要超过 20 小时。</li><li>易于消除风险并确保不存在奇怪的边缘情况或错误：我们希望能够相信任务的成功或失败是能力的真实指示，而不是任务中的错误、漏洞或意外困难</li><li>发挥 LLM 代理的优势：理想情况下，大多数任务可以通过编写代码、使用命令行或其他基于文本的交互来完成。</li><li>有关理想任务的更多说明，请参阅需求部分。</li></ul><p></p><h3>重要信息</h3><p>我们不希望任务或解决方案最终出现在训练数据中。请不要将任务说明或解决方案发布到公共网络上<br><br>此外，在开发或测试任务时，<strong>请关闭 chatGPT 历史记录，确保副驾驶设置不允许对您的数据进行训练等。</strong></p><p>如果您对需求、赏金、基础设施等有疑问，请将它们发布在此处的评论部分，以便其他人可以从答案中学习！<br><br>如果您的问题需要分享有关任务解决方案的信息，请发送电子邮件至<a href="mailto:task-support@evals.alignment.org"><u>task-support@evals.alignment.org</u></a> 。<br></p><h2>确切的赏金</h2><p>1. 想法<br>现有的想法列表和更多信息<a href="https://docs.google.com/document/d/1b0Z56lBkSTzudR_8yclKgTeLUNgHMo4NTyvN60FGpIo/edit#heading=h.x9lxwng6c064"><u>请参见此处</u></a>。<br>如果我们将这个想法变成完整的规范，我们将支付 20 美元（10% 的机会是 200 美元）<br></p><p> 2. 规格<br>请参阅<a href="https://docs.google.com/document/d/1b0Z56lBkSTzudR_8yclKgTeLUNgHMo4NTyvN60FGpIo/edit#heading=h.wlvv8lhlghq6"><u>此处的</u></a>示例规格和要求描述。<br>我们将支付 200 美元购买符合要求且足以满足我们的需求的规格，使我们对它们感到兴奋<br></p><p>3. 任务执行</p><p>请参阅<a href="https://docs.google.com/document/d/1b0Z56lBkSTzudR_8yclKgTeLUNgHMo4NTyvN60FGpIo/edit#heading=h.mmy17h99bpbn"><u>此处</u></a>的开发和提交任务指南。</p><p>我们将为完全实施和测试的任务支付 2000-4000 美元，这些任务满足要求并且足够好地满足我们对它们感到兴奋的需求<br><br>4. 推荐</p><p>如果您推荐某人提交成功的任务实施，我们将支付 20 美元（10% 的机会为 200 美元）。 （他们会将您的电子邮件放入提交表格中）<br><br></p><p>要提交，您需要下载<a href="https://drive.google.com/file/d/11viEtJ2SMdchfsmo6RrKwsk1OibR1eGQ/view?usp=sharing">入门包</a>，然后完成测验以获得解压密码。入门包包含提交链接、任务代码模板以及用于测试+运行任务的基础设施。</p><p>任何提供我们论文中使用的任务实现或规范的人都可以获得确认，我们将考虑将实现任务的人员作为共同作者。</p><p>我们也对这些赏金感到兴奋，因为这是寻找有前途的员工的一种方式。如果我们喜欢您提交的内容，我们可能会快速跟踪您完成我们的申请流程。<br></p><h1>所有赏金的详细要求</h1><h2>背景+动机</h2><p>我们希望任务综合起来，能够让我们对“在世界上做事并成功完成任务的能力”进行相当连续的衡量。如果他们还可以为我们提供有关更具体的威胁模型（例如网络犯罪或自我完善）的信息（尤其是积极的证据），那就太好了。但我们关注的是一般功能，因为似乎很难排除能够在软件开发、机器学习、计算机安全、研究、商业等领域执行各种多日专家级项目的代理带来的危险。只需排除特定的威胁故事即可。</p><p>理想情况下，我们希望任务涵盖相当广泛的难度，这样，一个成功完成大部分任务的智能体似乎确实很可能会带来灾难性的风险，而一个在任何任务上进展甚微的智能体似乎很可能会带来灾难性的风险。不太可能造成灾难性风险。</p><p>我们还希望任务难度的分布相当均匀，这样我们就可以有意义地推断任务成功率的逐渐增加（与难度的双峰分布相反，在双峰分布中，我们可能会看到非常快的进展，然后很长一段时间没有进展） ，然后再次快速进步）。<br><br>我们最感兴趣的是首先收集更困难的任务，并且我们认为我们通常可以非常直接地制作困难任务的更简单变体 - 通过提供更多指导或部分执行任务，或者通过减少要求。 （我们也有兴趣收集任务难度调节器的想法/实现 - 请参阅<a href="https://docs.google.com/document/d/1b0Z56lBkSTzudR_8yclKgTeLUNgHMo4NTyvN60FGpIo/edit#heading=h.56ghwnpglffw"><u>此处</u></a>）<br></p><h2>域名</h2><p>我们对以下领域的任务感兴趣： <br></p><figure class="table"><table><tbody><tr><td style="background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>领域</strong></td><td style="background-color:#efefef;border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>例子</strong></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">软件工程</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><ul><li>用于玩晦涩棋盘游戏变体的本地网络应用程序</li><li>类似 SWEbench 的任务</li></ul></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">网络安全、黑客</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><ul><li>picoCTF</li><li> SQL注入</li><li>寻找代码中的漏洞</li></ul></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">通才任务，网络犯罪</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><ul><li>研究 + 鱼叉式网络钓鱼</li><li>山寨LLM API，收获凭证</li></ul></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">培训后强化、启发</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><ul><li>改善代理脚手架</li><li>创建提高绩效的任务指导，</li><li>生成综合训练数据</li></ul></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">机器学习/人工智能研发</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><ul><li>复制 ML 论文（或小节）</li><li>实施 Flash Attention</li></ul></td></tr></tbody></table></figure><p><br>此外，我们希望构建任务，以便在可能的情况下测试某些广泛类别的行为或技能。例如：</p><ul><li>研究技能</li><li>提出好问题并评估信息的价值</li><li>弄清楚新颖的系统如何工作/因果建模</li><li>根据环境的成功反馈能够持续取得进步</li><li>纠正错误</li><li>情境学习/从经验中学习的能力<br></li></ul><h2>什么是任务？</h2><p>为了评估任务上的代理，我们启动一个虚拟机并在该虚拟机中运行代理，并给出描述该任务的提示。代理至少可以使用命令行或运行 python 代码，并且可能具有更复杂的脚手架，为其提供其他功能。它有一个“提交”操作，可以用来声明它已完成任务。<br></p><p> task.py 文件指定：</p><ul><li>如何为任务设置 VM（安装包、启动进程、复制文件等）</li><li>代理拥有哪些访问权限（例如无互联网、仅 HTTP-get、完整互联网）</li><li> “变体”的字典 - 任务参数的不同值，以便您可以拥有不同版本的任务</li><li>[可选]如何自动给任务评分<br></li></ul><p>完整的任务文件夹还包含：</p><ul><li>包含有关任务信息的自述文件，包括基本描述、难度级别、如何对任务评分以及人类在任务中表现的示例</li><li>应复制到 VM 的任何资源</li></ul><p></p><h2>需求清单<br></h2><ul><li>理想的难度范围是人类领域专业人士（例如，在湾区的年薪约为 20 万美元）需要至少 6 小时来解决，并且希望某些任务超过 20 小时。一般来说，任务越难、时间越长越好。<ul><li>更具体地说，对于编码/调试任务，这将是“对于一个没有这个特定代码库上下文的体面工程师”。例如，随机选择的工程师每小时支付 100-200 美元，他熟悉正在使用的语言和整体堆栈，但不熟悉编写代码的人，也不是导致错误的特定组件的专家。</li><li>另一种实施方式是“需要花费 1000 美元，最好是 >; 5000 美元，让人力承包商来完成”</li><li>较短的任务也有一定的用处，但我们确实希望优先考虑困难的任务。<br><br></li></ul></li><li>静态难度：任务难度不会随时间变化<ul><li>理想情况下，我们不希望任务难度随着时间的推移而改变，例如有人发布了堆栈溢出的解决方案，或者创建了一个通过单次调用即可解决任务的库，等等</li><li>如果任务可以合理地执行而无需与实时互联网交互，那就太好了 - 例如，您可以拥有所有相关库的文档的本地副本，或特定日期的各个站点的存档副本，或一些其他信息，但是没有一般的互联网接入。<br><br><br></li></ul></li><li>模型性能正确反映了底层能力水平<ul><li>当前或未来的模型不会记住：理想情况下，任务解决方案过去没有公开发布，将来也不太可能发布，并且与训练语料库中的任何内容都不是特别接近。<ul><li>如果当前模型似乎没有记住您的任务，这是一个好兆头，但并不完全令人放心</li><li>另外，如果您能够从公共互联网上删除该解决方案，或者指定一种从训练数据中过滤所有示例的方法，那么我认为我们并不担心。</li></ul></li><li>不需要额外的启发：理想情况下，我们希望法学硕士能够成为有帮助的好助手，能够很好地完成这项任务。<ul><li>不应引发拒绝（即不应主动作恶）</li><li>不需要专门的脚手架</li></ul></li></ul></li></ul><p>请注意，这通常会在一定程度上抵消直接威胁模型的相关性</p><ul><li>发挥 LLM 代理的优势 - 理想情况下，大多数任务可以通过编写代码、使用命令行或其他基于文本的交互来完成</li><li>该任务并不奇怪或令人困惑，可能会不公平地使模型失败</li><li>该任务对确切设置的特定属性、代理脚手架、当前法学硕士的偶然能力或处置等并不高度敏感</li><li>无人为限制：不要以不适用于现实威胁模型的方式使任务变得更加困难 - 实际的 ARA 代理将能够使用互联网、运行代码等。<br><br></li><li>瓶颈的多样性<ul><li>我们希望代理在不同的任务上以不同的方式陷入困境 - 我们不希望所有任务在浏览、身份验证、机器学习知识等方面都被阻止，因为那样我们可能会突然看到对代理进行小幅改进后，任务性能大幅跃升<br><br></li></ul></li><li>威胁模型相关性：<ul><li>现实世界的复杂性：希望任务能够代表现实世界中实际做的事情，而不是人为地简单 - 处理系统兼容性、安装包、调试、研究、不完美信息、优先级等问题</li><li>理想情况下，该任务与威胁模型非常明确相关（例如自我完善、网络犯罪、传播更多副本、掩盖您的踪迹）<br><br></li></ul></li><li>快速、可靠、连续的评分<ul><li>任务的最终得分应该是 0 到 1 之间的数字</li><li>理想情况下，任务是自动评分的，尽管我们强烈希望使用强大的手动评分标准，而不是不可靠的自动评分功能。如果做不到这一点，人类应该尽可能快地对任务进行评分。如果代理产生一些可以自行查看的输出（而不是人类需要查看整个运行中发生的情况），这是最简单的。我们还建议在可能的情况下使用评分功能自动检索并向人类评分者显示相关信息。</li><li>自动评分应该是可靠的，并且不会因为代理没有以预期的确切格式提交答案而错误地对任务进行评分</li><li>手动评分应尽可能客观且明确 - 例如，提供会获得不同分数的输出示例</li><li>理想情况下，评分不是二元的，而是根据代理的表现/任务完成程度给出不同的分数，其中数字分数与难度级别很好地对应<ul><li>或者，或者此外，任务可以实现具有不同难度级别的多个变体<br><br></li></ul></li></ul></li><li>易于实施和质量检查：<ul><li>理想情况下，易于运行/支持：不需要太多计算资源，只需通过简单的设置即可在一台虚拟机上运行。不需要外部 GPU 等。这不是硬性要求，对于好的任务可以放弃，特别是 ML 和 AI 研发任务。</li><li>用于运行任务环境/评分的代码很简单，不太可能出现问题或边缘情况。这不是一个硬性要求，如果您可以证明您已经彻底测试了环境和评分，则可以放弃。<ul><li>好的例子：<ul><li>任务是实现一些软件。对要求有明确的描述。</li><li>任务是使用一些本地资源回答研究问题。这个问题有一个明确的答案</li></ul></li><li>不好的例子：<ul><li>该任务涉及一些网络犯罪情况，模拟受害者应该以特定方式对模型试图做的事情做出反应</li></ul></li></ul></li><li>不需要人工干预或监督来运行任务。我们可能希望大多数任务不需要这个，但我们会接受需要这个的任务，否则这些任务就非常好。<ul><li>任务环境不允许模型做任何实际上危险或非法或造成现实世界后果的事情；</li><li>不需要人类模拟任务环境的各个方面<br></li></ul></li></ul></li><li>丰富的现实世界的复杂性<ul><li>理想情况下，在许多任务中，很多困难不是来自于需要解决的一些核心棘手难题，而是来自于实际做事时的平凡困难 - 思考你的 python 环境问题、数据清理、无用的谷歌搜索结果、跟踪信息，避免过度设计等</li><li>如果大多数任务尽可能“自然”，而不是为了适应威胁模型而精心构建，那就太好了。例如，“我正在处理的一些数据分析任务”或“让这个模型在该服务器上实际运行”或“检查我的结果是否有原因与本文中描述的不匹配”比“特工需要勒索某人才能让它逃出盒子”<br><br></li></ul></li><li>不是特别吵闹<ul><li>根据我们的经验，即使在 T=0 时，代理运行的变化也很大，因此有一点噪音是可以的。但是，您的任务绝对不应该是代理必须回答是或否的任务，并且它只能盲目地猜测答案。<br><br><br></li></ul></li></ul><h2>一些优缺点示例任务</h2><p>实现一个本地 Web 应用程序来玩一款著名棋盘游戏的晦涩复杂的变体</p><p>优点：</p><ul><li>实现环境+指令很简单，这里没什么复杂的。<ul><li>特别是，设置的任何问题都可能在一开始就被注意到，因为主要依赖项只是指令、可用包和托管页面的能力 - 稍后在虚拟机环境中不应该有复杂的交互。任务或取决于所采取的方法（除了更多不同包的可能要求之外）</li><li>现有的游戏规则描述已经过压力测试以消除歧义<br><br></li></ul></li><li>很容易制作连续评分标准，在各种难度下提供信号，从设置带有某种模糊功能板的页面，到为各种边缘情况正确实现逻辑<br><br></li><li>看起来很容易修改难度级别，可能的修改包括：<ul><li>就按什么顺序实施什么提供指导</li><li>只要求逻辑正确实现，不需要GUI</li><li>给出逻辑并要求代理实现 GUI</li><li>添加额外的功能要求，例如撤消、保存和重新加载游戏、额外的 UI 功能<br><br></li></ul></li><li>太晦涩以至于不太可能被记住</li></ul><p><br></p><p>缺点：</p><ul><li>评分有点烦人，因为它需要探索所有边缘情况并验证它们是否正确实现<ul><li>缓解措施：要求代理允许应用程序从板状态的标准表示进行初始化，以便您可以快速测试边缘情况状态下的行为</li></ul></li><li>并不是<i>那么</i>晦涩难懂 - 人们将来可能会发布解决方案</li><li>有些要求对于没有视觉形态的代理来说是不公平的</li></ul><p><br><br></p><h3>本地研究（任务类型）：</h3><ul><li>给定一些本地网页、pdf 或其他资源，回答问题</li></ul><p><br></p><p>优点：</p><ul><li>设置/环境非常简单</li><li>如果问题的答案简短且明确，那么评分可能会很简单</li><li>可以通过添加更多干扰因素、使问题变得更难、提供有关查看位置的提示等来修改难度</li><li>如果问题足够晦涩，则不太可能被记住，并且难度也不太可能随着时间的推移而改变</li><li>相当自然/真实世界-y</li></ul><p><br></p><p>缺点</p><ul><li>可能很难使任务足够困难而不使评分变得相当主观<ul><li>也许某种荟萃分析可能很有前途，或者其他你需要仔细阅读大量不同文档的东西</li></ul></li><li>可能对脚手架、上下文长度或模型“处置”有些敏感（例如，如果模型总是尝试 grep 而不是实际读取内容）</li><li>最无聊的任务版本（例如，只有几个地方可以看，基本上只是“使用猫”加上总结任务）并不是一个很好的代理测试<ul><li>然而，似乎更难的版本应该是对机构和“执行功能”的一个很好的测试：决定哪些资源最有前途，继续搜索一个来源与查看其他文件的程度等</li></ul></li><li>可能对资源的偶然属性有噪音/敏感，例如表格的格式<ul><li>对于数字、表格或图像，这对纯文本模型有点不公平</li></ul></li></ul><p><br></p><h1>具体赏金要求</h1><h2>创意要求</h2><p>一个值得赏金的想法将向我们指出一个我们没有考虑过的任务、任务类型或通用难度修改器，这似乎很有希望并且值得更详细地思考。</p><p>添加更多细节或解释为什么它有前途是有帮助的。</p><p>入门包中提供了任务创意列表。</p><h2>任务规范要求</h2><p><a href="https://www-files.anthropic.com/production/files/responsible-scaling-policy-1.0.pdf"><u>这里</u></a>有一些示例规范（比理想的要详细一些）（从第 16 页开始）。</p><p><a href="https://airtable.com/appa8ULxLrTkQxw4K/tblV6ksjiHtJyosv1/viwj4f4Hf80U2oy9O?blocks=hide"><u>提交表格</u></a>将要求您描述：</p><ul><li>任务的基本想法/故事，它的设计目的是测试什么，等等。<br></li><li>任务环境/设置<ul><li>本地有哪些资源</li><li>解释任务的提示是什么</li><li>代理可以访问哪些外部资源<br></li></ul></li><li>完成任务所需步骤的概述<ul><li>涉及的能力类型</li><li>特定的瓶颈或困难的步骤</li><li>人类完成任务的时间/成本估算</li></ul></li><li>评分<ul><li>手动评分还是自动评分？<ul><li>如果手动的话需要多长时间？</li></ul></li><li>评分标准是什么？<br></li></ul></li><li>监督或手动模拟要求</li></ul><p>请随意为入门包中的任何现有任务想法或新想法制定规范。</p><h2>任务实施要求</h2><p>提交内容由一个任务文件夹组成，其中包含 task.py 文件和所需的任何资源，以及任务的详细文档以及测试方式。特别是，您需要让其他人完成任务来进行质量保证 - 确保说明不含糊，所有必要的资源都实际存在，没有意外的捷径等。</p><p>您可以按照您认为合适的方式在您和任何帮助您进行质量检查的人之间分配奖金。我们预计质量保证将占总劳动力的很大一部分，尽管它会因任务而异。</p><p>请随意实施任何现有的任务想法或规范，或者新的想法。</p><p></p><h3>有关实施+文档要求的更多详细信息：</h3><p>您将需要使用我们的基础设施来测试和开发您的任务。从此处下载 zip 文件并通过完成测验获取密码。<br></p><p>从我们的角度来看，评估任务的主要困难在于质量保证。我们担心，当您实际在任务上运行 LLM 代理时，它可能会因意外原因成功或失败，例如：</p><ul><li>评分功能中的错误</li><li>评分功能对代理提交的确切格式过于敏感</li><li>环境设置代码中的错误</li><li>说明不明确或具有误导性</li><li>所需资源不可用，或代理未获悉这些资源</li><li>可以走捷径</li><li>模拟的情况或环境在某种程度上令人困惑或误导</li><li>需要与代理脚手架不兼容的东西（例如交互式bash）</li><li>从云虚拟机运行任务时遇到不同的障碍（例如，由于机器人检测，浏览变得更加困难）</li><li>任务比听起来更容易或更难</li><li>任务已被记住<br></li></ul><p>因此，我们需要您提供详细的文档，证明有人至少运行过一次该任务，并且您已经仔细考虑了可能的问题。</p><p>将来，我们将添加一种方法，让 QA 人员使用确切的任务 docker 容器。现在，只需向人工说明他们应该拥有哪些文件和文件夹、应该安装哪些软件包以及允许他们使用哪些工具。请注意任何可能导致人工运行和代理运行之间出现显着差异的情况。</p><p><br></p><h3>执行任务示例/QA 运行的指南</h3><p>这应该由不是任务作者的人来完成，并且除了给他们的任务提示之外，没有关于任务的解决方案或意图的特殊信息。</p><p>他们应该遵循的流程：</p><ul><li>当您开发或测试任务时，请关闭 chatGPT 历史记录，确保副驾驶设置不允许对您的数据进行训练等。<br><br></li><li>定期（例如 30 分钟）以及完成重要里程碑后记录您取得的进展。这有助于校准任务的难度并确保我们的评分合理。<ul><li>您可以（并且应该！）暂停计时器<ul><li>为您的质量检查文档编写材料</li><li>休息一下</li><li>每 30 分钟以及完成重要里程碑后填写一次计时日志<ul><li>默认情况下，写下 1-2 句话的进度总结</li><li>对于编程任务，给出关联提交的 id</li></ul></li></ul></li><li>以正常工作节奏完成任务（包括您通常会采取的任何休息时间），并且任务可以在几天内完成。</li></ul></li><li>对于编程任务，如果您可以跟踪 git 存储库中的进度，相对频繁地进行提交（请尝试使用描述性提交消息）<ul><li>这些中间状态将有助于开始任务的一部分，以测试代理在给定一些基础工作时的表现。</li></ul></li></ul><h3>您应提供的文件：</h3><p>填写您的任务的自述文件模板（在入门包中的任务/模板中）。它将需要以下信息：</p><ul><li> VM设置+环境的描述</li><li>任务演练/指南：<ul><li>有关如何完成任务的详细分步说明</li></ul></li><li>已完成任务示例：<ul><li>针对不同能力级别的最终输出/状态/提交可能是什么的 5 个以上示例<ul><li>对于一些困难的任务，如果获得 5 太费力的话，这可能需要是对提交内容的描述或欺骗，而不是实际的解决方案/实现</li></ul></li></ul></li><li>评分标准：<ul><li>关于评分如何运作的详细指导，并描述可能的分数以及达到每个分数所需的条件</li></ul></li><li>任务运行成功完成的示例：<ul><li>描述您如何进行示例运行（人员的技能水平是什么，他们有什么背景，您如何确保他们的环境/可供性/工具能够代表法学硕士代理的能力）</li><li>对他们采取的步骤的描述以及中间状态的文档（最好是虚拟机快照和书面描述；屏幕截图和捕获正在进行的工作的其他方式也很棒）</li><li>他们花了多长时间</li><li>他们对任务中的任何问题或令人困惑的事情的注释</li><li>描述任务的哪些分支或部分尚未测试<br></li></ul></li></ul><h3>变体</h3><p>我们将为任务的其他变体给予额外的奖励。</p><p>作为原始任务的“严格子集”的变体（即仅删除完成任务的一些要求列表）不需要单独的 QA 运行。</p><p>涉及环境、起始状态或任务目标显着差异的变体（例如，从部分实现的代码库开始的变体，或涉及实现不同功能的变体）将需要额外的测试</p><p><br></p><h2>难度调整</h2><p>难度调节器是环境或任务的属性，可以应用于许多不同类型的任务来调整难度级别。例如</p><ul><li>为代理提供测试以检查其解决方案</li><li>提供了提示、演练、完成类似任务的示例、如何作为语言模型代理取得成功的提示或其他一些有用的资源</li><li>对所用时间、令牌使用、资源使用的限制</li><li>环境以某种损坏的状态启动，代理首先需要修复它 - 例如搞乱了 python 安装。</li><li>本地文件经常被移动/修改/删除<br></li></ul><p>我们将为我们喜欢的难度修改器想法提供赏金，并且还可能为完全指定或完全实现+测试的难度修改器提供规范或实现赏金。这可能看起来像“这里有一些代码可以添加到任何具有 X 属性的任务的设置指令中，这将以 Y 的方式增加/减少难度级别，我用这 3 个任务对其进行了测试”</p><p><br></p><h1>测验：</h1><p>确定测试代理编写令人信服的通用网络钓鱼电子邮件的能力的任务的两个最重要的缺点：</p><ol><li>太容易了</li><li>发挥LLM的优势</li><li>实现过于复杂</li><li>可能会被记住</li><li>需要太多资源才能运行</li><li>任务难度可能会随着时间而变化</li><li>对精确的脚手架设置或特定模型功能敏感<br></li></ol><p>确定一项任务的两个最重要的缺点，该任务测试代理在开源 LLM 上复制特定 ML 论文结果的能力（该任务需要人类专家大约 15 个小时），并提供完整的互联网访问权限</p><ol><li>太容易了</li><li>发挥LLM的优势</li><li>评估太难/太主观</li><li>实现过于复杂</li><li>对精确的脚手架设置或特定模型功能敏感</li><li>需要太多资源才能运行</li><li>任务难度可能会随着时间而变化</li><li>与威胁模型无关<br></li></ol><p>确定任务的两个最可能的缺点，该任务测试代理与自身的多个副本进行协调的能力，这些副本可以访问不同的信息，以解决难题并获取密码。</p><ol><li>需要太多资源才能运行</li><li>任务难度可能会随着时间而变化</li><li>可能会被记住</li><li>对精确的脚手架设置或特定模型功能敏感</li><li>太容易了</li><li>发挥LLM的优势</li><li>实现过于复杂</li><li>评估太难/太主观</li></ol><p></p><p>取出正确答案的数字并将它们连接起来以获得 zip 文件的 6 位密码 - 例如 231524</p><h1>尖端</h1><ul><li>在花费太多时间实施任务之前，您可能需要仔细考虑任务涉及的实际步骤以及可能出现的问题<br><br></li><li>涉及自我修改、态势感知或自我改进的任务很难正确完成——这些任务很容易对所使用的特定代理支架或当前模型的假设非常敏感，或者随着时间的推移很难改变随着模型和代理的变化，会以令人困惑的方式出现。<br></li><li>许多网络犯罪类型的活动并不是很好的候选任务，但您可以通过让代理防御攻击或识别攻击示例来降低相关能力<br></li><li>可以通过将多个较小的任务串在一起来创建更困难的任务 - 但如果任务之间的链接是精心设计的（例如，一个任务给你一个密码，你可以用它来解密下一个任务的指令 - 除非有一些设置），这有点可悲这实际上可能自然发生）<br></li><li>如果代理应以特定格式提交特定答案，则要求其提交 JSON 效果很好。<br></li><li>请参阅<a href="https://www.alignmentforum.org/posts/vERGLBpDE8m5mpT6t/autonomous-replication-and-adaptation-an-attempt-at-a"><u>这篇文章</u></a>，了解可能激发相关任务的一些威胁建模<br></li><li>可能的任务来源：<ul><li>针对 SWE、ML 或其他研究人员角色的一天到一周的试用任务/项目</li><li>CS、科学等本科课程项目</li><li>Kaggle 或其他数据科学竞赛</li><li>Upwork 任务或其他远程工作/赏金/等<br><br></li><li>对于所有这些，理想情况下解决方案不公开<ul><li><i>问题描述</i>公开就可以了</li></ul></li></ul></li></ul><p><br><br><br><br></p><h1>常问问题</h1><p><br><strong>Q：只要GPT-4没有记住解法就可以吗？</strong><br>答：这是一个好兆头，但并不能完全消除记忆问题 - 我们理想地希望这些任务能够面向未来，防止模型可能更详细地记住公共解决方案</p><p><strong>问：现有的基准/任务怎么样？</strong><br>如果您能找到一种方法来快捷地为我们提供一堆满足需求的任务，那就太好了！以下是一些想法/建议：<br><br>为 LLM 代理设计的现有基准通常过于简单，有时还存在缺陷或“不公平”。<br><br> SWEbench 可能包含一些足够困难的示例，我们可能会接受其中一些作为提交。这些任务是公共基准的一部分，这是一个轻微的缺点，但主要的是我们需要这些任务的演练/人类基线（以确保评分有效，了解难度级别，并确认它们”实际上是可能的！）<br>现有的人类任务库（如 picoCTF 或其他 CTF 库）也相当不错 - 这里的主要瓶颈是记忆（如果在线提供解决方案/演练）或演练（如果没有）。我们还怀疑实验室可能会针对任何像这样的强化学习基准进行训练。</p><br/><br/> <a href="https://www.lesswrong.com/posts/gAkCCaBBHD4gcwxmv/bounty-diverse-hard-tasks-for-llm-agents#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gAkCCaBBHD4gcwxmv/bounty-diverse-hard-tasks-for-llm-agents<guid ispermalink="false"> gAkCCaBBHD4gcwxmv</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Sun, 17 Dec 2023 01:04:05 GMT</pubDate> </item><item><title><![CDATA[2022 (and All Time) Posts by Pingback Count]]></title><description><![CDATA[Published on December 16, 2023 9:17 PM GMT<br/><br/><p>在过去的几年里，我一直希望 LessWrong 能够“按照 pingback 的数量对帖子进行排序，或者理想情况下，按照 pingback 的总业力对帖子进行排序”。我在年度审查期间特别希望这一点，其中“哪些帖子被引用最多？”似乎是追踪潜在隐藏宝石的有用工具。</p><p>我们还没有为此构建成熟的功能，但我只是对数据库运行了查询，并将其制作成电子表格，您可以在此处查看：</p><p><a href="https://docs.google.com/spreadsheets/d/1ZxFOXeKQof2bBwnB5zKrvulgFHeM71I7FgtIPv68czc">少错 2022 年 Pingbacks 帖子</a></p><p>以下是排名前 100 的帖子，按总 Pingback Karma 排序</p><figure class="table" style="width:0px"><table><tbody><tr><td style="padding:2px 3px"><code><strong>Title/Link</strong></code></td><td style="padding:2px 3px;text-align:center"> <code><strong>Post Karma</strong></code></td><td style="padding:2px 3px;text-align:center"> <code><strong>Pingback Count</strong></code></td><td style="padding:2px 3px;text-align:center"> <code><strong>Total Pingback Karma</strong></code></td><td style="padding:2px 3px;text-align:center"> <code><strong>Avg Pingback Karma</strong></code></td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><code><u>AGI Ruin: A List of Lethalities</u></code></a></td><td style="padding:2px 3px;text-align:center">第870章</td><td style="padding:2px 3px;text-align:center">158</td><td style="padding:2px 3px;text-align:center"> 12,484</td><td style="padding:2px 3px;text-align:center"> 79</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy"><code><u>MIRI announces new &quot;Death With Dignity&quot; strategy</u></code></a></td><td style="padding:2px 3px;text-align:center">第334章</td><td style="padding:2px 3px;text-align:center">73</td><td style="padding:2px 3px;text-align:center"> 8,134</td><td style="padding:2px 3px;text-align:center"> 111</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><code><u>A central AI alignment problem: capabilities generalization, and the sharp left turn</u></code></a></td><td style="padding:2px 3px;text-align:center"> 273</td><td style="padding:2px 3px;text-align:center"> 96</td><td style="padding:2px 3px;text-align:center"> 7,704</td><td style="padding:2px 3px;text-align:center"> 80</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><code><u>Simulators</u></code></a></td><td style="padding:2px 3px;text-align:center"> 612</td><td style="padding:2px 3px;text-align:center"> 127</td><td style="padding:2px 3px;text-align:center"> 7,699</td><td style="padding:2px 3px;text-align:center"> 61</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"><code><u>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover</u></code></a></td><td style="padding:2px 3px;text-align:center">第367章</td><td style="padding:2px 3px;text-align:center">83</td><td style="padding:2px 3px;text-align:center"> 5,123</td><td style="padding:2px 3px;text-align:center"> 62</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target"><code><u>Reward is not the optimization target</u></code></a></td><td style="padding:2px 3px;text-align:center">第341章</td><td style="padding:2px 3px;text-align:center">62</td><td style="padding:2px 3px;text-align:center"> 4,493</td><td style="padding:2px 3px;text-align:center"> 72</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking"><code><u>A Mechanistic Interpretability Analysis of Grokking</u></code></a></td><td style="padding:2px 3px;text-align:center">第367章</td><td style="padding:2px 3px;text-align:center">48</td><td style="padding:2px 3px;text-align:center"> 3,450 人</td><td style="padding:2px 3px;text-align:center">72</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget"><code><u>How To Go From Interpretability To Alignment: Just Retarget The Search</u></code></a></td><td style="padding:2px 3px;text-align:center"> 167</td><td style="padding:2px 3px;text-align:center"> 45</td><td style="padding:2px 3px;text-align:center"> 3,374</td><td style="padding:2px 3px;text-align:center"> 75</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"><code><u>On how various plans miss the hard bits of the alignment challenge</u></code></a></td><td style="padding:2px 3px;text-align:center">第292章</td><td style="padding:2px 3px;text-align:center">40</td><td style="padding:2px 3px;text-align:center"> 3,288</td><td style="padding:2px 3px;text-align:center"> 82</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><code><u>[Intro to brain-like-AGI safety] 3. Two subsystems: Learning &amp; Steering</u></code></a></td><td style="padding:2px 3px;text-align:center"> 79</td><td style="padding:2px 3px;text-align:center"> 36</td><td style="padding:2px 3px;text-align:center"> 3,023</td><td style="padding:2px 3px;text-align:center"> 84</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"><code><u>How likely is deceptive alignment?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 101</td><td style="padding:2px 3px;text-align:center"> 47</td><td style="padding:2px 3px;text-align:center"> 2,907 人</td><td style="padding:2px 3px;text-align:center">62</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values"><code><u>The shard theory of human values</u></code></a></td><td style="padding:2px 3px;text-align:center"> 238</td><td style="padding:2px 3px;text-align:center"> 42</td><td style="padding:2px 3px;text-align:center"> 2,843</td><td style="padding:2px 3px;text-align:center"> 68</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><code><u>Mysteries of mode collapse</u></code></a></td><td style="padding:2px 3px;text-align:center">第279章</td><td style="padding:2px 3px;text-align:center">32</td><td style="padding:2px 3px;text-align:center"> 2,842</td><td style="padding:2px 3px;text-align:center"> 89</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><code><u>[Intro to brain-like-AGI safety] 2. “Learning from scratch” in the brain</u></code></a></td><td style="padding:2px 3px;text-align:center"> 57</td><td style="padding:2px 3px;text-align:center"> 30</td><td style="padding:2px 3px;text-align:center"> 2,731</td><td style="padding:2px 3px;text-align:center"> 91</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation"><code><u>Why Agent Foundations? An Overly Abstract Explanation</u></code></a></td><td style="padding:2px 3px;text-align:center">第285章</td><td style="padding:2px 3px;text-align:center">42</td><td style="padding:2px 3px;text-align:center"> 2,730</td><td style="padding:2px 3px;text-align:center"> 65</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability"><code><u>A Longlist of Theories of Impact for Interpretability</u></code></a></td><td style="padding:2px 3px;text-align:center"> 124</td><td style="padding:2px 3px;text-align:center"> 26</td><td style="padding:2px 3px;text-align:center"> 2,589</td><td style="padding:2px 3px;text-align:center"> 100</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very"><code><u>How might we align transformative AI if it&#39;s developed very soon?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 136</td><td style="padding:2px 3px;text-align:center"> 32</td><td style="padding:2px 3px;text-align:center"> 2,351</td><td style="padding:2px 3px;text-align:center"> 73</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree"><code><u>A transparency and interpretability tech tree</u></code></a></td><td style="padding:2px 3px;text-align:center"> 148</td><td style="padding:2px 3px;text-align:center"> 31</td><td style="padding:2px 3px;text-align:center"> 2,343</td><td style="padding:2px 3px;text-align:center"> 76</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><code><u>Discovering Language Model Behaviors with Model-Written Evaluations</u></code></a></td><td style="padding:2px 3px;text-align:center"> 100</td><td style="padding:2px 3px;text-align:center"> 19 号</td><td style="padding:2px 3px;text-align:center">2,336</td><td style="padding:2px 3px;text-align:center"> 123</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development"><code><u>A note about differential technological development</u></code></a></td><td style="padding:2px 3px;text-align:center"> 185</td><td style="padding:2px 3px;text-align:center"> 20</td><td style="padding:2px 3px;text-align:center"> 2,270</td><td style="padding:2px 3px;text-align:center"> 114</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing"><code><u>Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]</u></code></a></td><td style="padding:2px 3px;text-align:center"> 195</td><td style="padding:2px 3px;text-align:center"> 35</td><td style="padding:2px 3px;text-align:center"> 2,267</td><td style="padding:2px 3px;text-align:center"> 65</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes"><code><u>Supervise Process, not Outcomes</u></code></a></td><td style="padding:2px 3px;text-align:center"> 132</td><td style="padding:2px 3px;text-align:center"> 25</td><td style="padding:2px 3px;text-align:center"> 2,262</td><td style="padding:2px 3px;text-align:center"> 90</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><code><u>Shard Theory: An Overview</u></code></a></td><td style="padding:2px 3px;text-align:center"> 157</td><td style="padding:2px 3px;text-align:center"> 28</td><td style="padding:2px 3px;text-align:center"> 2,019 人</td><td style="padding:2px 3px;text-align:center">72</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/72scWeZRta2ApsKja/epistemological-vigilance-for-alignment"><code><u>Epistemological Vigilance for Alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 61</td><td style="padding:2px 3px;text-align:center"> 21</td><td style="padding:2px 3px;text-align:center"> 2,008</td><td style="padding:2px 3px;text-align:center"> 96</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/k4AQqboXz8iE5TNXK/a-shot-at-the-diamond-alignment-problem"><code><u>A shot at the diamond-alignment problem</u></code></a></td><td style="padding:2px 3px;text-align:center"> 92</td><td style="padding:2px 3px;text-align:center"> 23</td><td style="padding:2px 3px;text-align:center"> 1,848</td><td style="padding:2px 3px;text-align:center"> 80</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer"><code><u>Where I agree and disagree with Eliezer</u></code></a></td><td style="padding:2px 3px;text-align:center">第862章</td><td style="padding:2px 3px;text-align:center">27</td><td style="padding:2px 3px;text-align:center"> 1,836</td><td style="padding:2px 3px;text-align:center"> 68</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know"><code><u>Brain Efficiency: Much More than You Wanted to Know</u></code></a></td><td style="padding:2px 3px;text-align:center"> 201</td><td style="padding:2px 3px;text-align:center"> 27</td><td style="padding:2px 3px;text-align:center"> 1,807 人</td><td style="padding:2px 3px;text-align:center">67</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets"><code><u>Refine: An Incubator for Conceptual Alignment Research Bets</u></code></a></td><td style="padding:2px 3px;text-align:center"> 143</td><td style="padding:2px 3px;text-align:center"> 21</td><td style="padding:2px 3px;text-align:center"> 1,793</td><td style="padding:2px 3px;text-align:center"> 85</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for"><code><u>Externalized reasoning oversight: a research direction for language model alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 117</td><td style="padding:2px 3px;text-align:center"> 28</td><td style="padding:2px 3px;text-align:center"> 1,788</td><td style="padding:2px 3px;text-align:center"> 64</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/CjFZeDD6iCnNubDoS/humans-provide-an-untapped-wealth-of-evidence-about"><code><u>Humans provide an untapped wealth of evidence about alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 186</td><td style="padding:2px 3px;text-align:center"> 19 号</td><td style="padding:2px 3px;text-align:center">1,647</td><td style="padding:2px 3px;text-align:center"> 87</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"><code><u>Six Dimensions of Operational Adequacy in AGI Projects</u></code></a></td><td style="padding:2px 3px;text-align:center"> 298</td><td style="padding:2px 3px;text-align:center"> 20</td><td style="padding:2px 3px;text-align:center"> 1,607 人</td><td style="padding:2px 3px;text-align:center">80</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><code><u>How &quot;Discovering Latent Knowledge in Language Models Without Supervision&quot; Fits Into a Broader Alignment Scheme</u></code></a></td><td style="padding:2px 3px;text-align:center"> 240</td><td style="padding:2px 3px;text-align:center"> 16</td><td style="padding:2px 3px;text-align:center"> 1,575</td><td style="padding:2px 3px;text-align:center"> 98</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies"><code><u>Godzilla Strategies</u></code></a></td><td style="padding:2px 3px;text-align:center"> 137</td><td style="padding:2px 3px;text-align:center"> 17 号</td><td style="padding:2px 3px;text-align:center">1,573</td><td style="padding:2px 3px;text-align:center"> 93</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is"><code><u>(My understanding of) What Everyone in Technical Alignment is Doing and Why</u></code></a></td><td style="padding:2px 3px;text-align:center">第411章</td><td style="padding:2px 3px;text-align:center">23</td><td style="padding:2px 3px;text-align:center"> 1,530</td><td style="padding:2px 3px;text-align:center"> 67</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines"><code><u>Two-year update on my personal AI timelines</u></code></a></td><td style="padding:2px 3px;text-align:center">第287章</td><td style="padding:2px 3px;text-align:center">18</td><td style="padding:2px 3px;text-align:center"> 1,530</td><td style="padding:2px 3px;text-align:center"> 85</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1"><code><u>[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMA</u></code></a></td><td style="padding:2px 3px;text-align:center"> 90</td><td style="padding:2px 3px;text-align:center"> 16</td><td style="padding:2px 3px;text-align:center"> 1,482</td><td style="padding:2px 3px;text-align:center"> 93</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"><code><u>[Intro to brain-like-AGI safety] 6. Big picture of motivation, decision-making, and RL</u></code></a></td><td style="padding:2px 3px;text-align:center"> 66</td><td style="padding:2px 3px;text-align:center"> 25</td><td style="padding:2px 3px;text-align:center"> 1,460</td><td style="padding:2px 3px;text-align:center"> 58</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/CQAMdzA4MZEhNRtTp/human-values-and-biases-are-inaccessible-to-the-genome"><code><u>Human values &amp; biases are inaccessible to the genome</u></code></a></td><td style="padding:2px 3px;text-align:center"> 90</td><td style="padding:2px 3px;text-align:center"> 14</td><td style="padding:2px 3px;text-align:center"> 1,450</td><td style="padding:2px 3px;text-align:center"> 104</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"><code><u>You Are Not Measuring What You Think You Are Measuring</u></code></a></td><td style="padding:2px 3px;text-align:center"> 350</td><td style="padding:2px 3px;text-align:center"> 21</td><td style="padding:2px 3px;text-align:center"> 1,449</td><td style="padding:2px 3px;text-align:center"> 69</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5"><code><u>Open Problems in AI X-Risk [PAIS #5]</u></code></a></td><td style="padding:2px 3px;text-align:center"> 59</td><td style="padding:2px 3px;text-align:center"> 14</td><td style="padding:2px 3px;text-align:center"> 1,446</td><td style="padding:2px 3px;text-align:center"> 103</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why"><code><u>[Intro to brain-like-AGI safety] 1. What&#39;s the problem &amp; Why work on it now?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 146</td><td style="padding:2px 3px;text-align:center"> 25</td><td style="padding:2px 3px;text-align:center"> 1,407</td><td style="padding:2px 3px;text-align:center"> 56</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/nXeLPcT9uhfG3TMPS/conditioning-generative-models"><code><u>Conditioning Generative Models</u></code></a></td><td style="padding:2px 3px;text-align:center"> 24</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,362</td><td style="padding:2px 3px;text-align:center"> 124</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/Gs29k3beHiqWFZqnn/conjecture-internal-infohazard-policy"><code><u>Conjecture: Internal Infohazard Policy</u></code></a></td><td style="padding:2px 3px;text-align:center"> 132</td><td style="padding:2px 3px;text-align:center"> 14</td><td style="padding:2px 3px;text-align:center"> 1,340</td><td style="padding:2px 3px;text-align:center"> 96</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"><code><u>A challenge for AGI organizations, and a challenge for readers</u></code></a></td><td style="padding:2px 3px;text-align:center"> 299</td><td style="padding:2px 3px;text-align:center"> 18</td><td style="padding:2px 3px;text-align:center"> 1,336</td><td style="padding:2px 3px;text-align:center"> 74</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><code><u>Superintelligent AI is necessary for an amazing future, but far from sufficient</u></code></a></td><td style="padding:2px 3px;text-align:center"> 132</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,335</td><td style="padding:2px 3px;text-align:center"> 121</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/kpPnReyBC54KESiSn/optimality-is-the-tiger-and-agents-are-its-teeth"><code><u>Optimality is the tiger, and agents are its teeth</u></code></a></td><td style="padding:2px 3px;text-align:center"> 288</td><td style="padding:2px 3px;text-align:center"> 14</td><td style="padding:2px 3px;text-align:center"> 1,319</td><td style="padding:2px 3px;text-align:center"> 94</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai"><code><u>Let&#39;s think about slowing down AI</u></code></a></td><td style="padding:2px 3px;text-align:center">第522章</td><td style="padding:2px 3px;text-align:center">17 号</td><td style="padding:2px 3px;text-align:center">1,273</td><td style="padding:2px 3px;text-align:center"> 75</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/krHDNc7cDvfEL8z9a/niceness-is-unnatural"><code><u>Niceness is unnatural</u></code></a></td><td style="padding:2px 3px;text-align:center"> 121</td><td style="padding:2px 3px;text-align:center"> 12</td><td style="padding:2px 3px;text-align:center"> 1,263</td><td style="padding:2px 3px;text-align:center"> 105</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group"><code><u>Announcing the Alignment of Complex Systems Research Group</u></code></a></td><td style="padding:2px 3px;text-align:center"> 91</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,247</td><td style="padding:2px 3px;text-align:center"> 113</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"><code><u>[Intro to brain-like-AGI safety] 13. Symbol grounding &amp; human social instincts</u></code></a></td><td style="padding:2px 3px;text-align:center"> 67</td><td style="padding:2px 3px;text-align:center"> 23</td><td style="padding:2px 3px;text-align:center"> 1,243</td><td style="padding:2px 3px;text-align:center"> 54</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/zjMKpSB2Xccn9qi5t/elk-prize-results"><code><u>ELK prize results</u></code></a></td><td style="padding:2px 3px;text-align:center"> 135</td><td style="padding:2px 3px;text-align:center"> 17 号</td><td style="padding:2px 3px;text-align:center">1,235</td><td style="padding:2px 3px;text-align:center"> 73</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/vvEebH5jEvxnJEvBC/abstractions-as-redundant-information"><code><u>Abstractions as Redundant Information</u></code></a></td><td style="padding:2px 3px;text-align:center"> 64</td><td style="padding:2px 3px;text-align:center"> 18</td><td style="padding:2px 3px;text-align:center"> 1,216</td><td style="padding:2px 3px;text-align:center"> 68</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/fYf9JAwa6BYMt8GBj/link-a-minimal-viable-product-for-alignment"><code><u>[Link] A minimal viable product for alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 53</td><td style="padding:2px 3px;text-align:center"> 12</td><td style="padding:2px 3px;text-align:center"> 1,184</td><td style="padding:2px 3px;text-align:center"> 99</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda"><code><u>Acceptability Verification: A Research Agenda</u></code></a></td><td style="padding:2px 3px;text-align:center"> 50</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,182</td><td style="padding:2px 3px;text-align:center"> 107</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/kipMvuaK3NALvFHc9/what-an-actually-pessimistic-containment-strategy-looks-like"><code><u>What an actually pessimistic containment strategy looks like</u></code></a></td><td style="padding:2px 3px;text-align:center">第647章</td><td style="padding:2px 3px;text-align:center">16</td><td style="padding:2px 3px;text-align:center"> 1,168</td><td style="padding:2px 3px;text-align:center"> 73</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag"><code><u>Let&#39;s See You Write That Corrigibility Tag</u></code></a></td><td style="padding:2px 3px;text-align:center"> 120</td><td style="padding:2px 3px;text-align:center"> 10</td><td style="padding:2px 3px;text-align:center"> 1,161</td><td style="padding:2px 3px;text-align:center"> 116</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications"><code><u>chinchilla&#39;s wild implications</u></code></a></td><td style="padding:2px 3px;text-align:center"> 403</td><td style="padding:2px 3px;text-align:center"> 18</td><td style="padding:2px 3px;text-align:center"> 1,151</td><td style="padding:2px 3px;text-align:center"> 64</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails"><code><u>Worlds Where Iterative Design Fails</u></code></a></td><td style="padding:2px 3px;text-align:center"> 185</td><td style="padding:2px 3px;text-align:center"> 17 号</td><td style="padding:2px 3px;text-align:center">1,122</td><td style="padding:2px 3px;text-align:center"> 66</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/dKTh9Td3KaJ8QW6gw/why-assume-agis-will-optimize-for-fixed-goals"><code><u>why assume AGIs will optimize for fixed goals?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 138</td><td style="padding:2px 3px;text-align:center"> 14</td><td style="padding:2px 3px;text-align:center"> 1,103 人</td><td style="padding:2px 3px;text-align:center">79</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples"><code><u>Gradient hacking: definitions and examples</u></code></a></td><td style="padding:2px 3px;text-align:center"> 38</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,079</td><td style="padding:2px 3px;text-align:center"> 98</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/Aet2mbnK7GDDfrEQu/contra-shard-theory-in-the-context-of-the-diamond-maximizer"><code><u>Contra shard theory, in the context of the diamond maximizer problem</u></code></a></td><td style="padding:2px 3px;text-align:center"> 101</td><td style="padding:2px 3px;text-align:center"> 6</td><td style="padding:2px 3px;text-align:center"> 1,073</td><td style="padding:2px 3px;text-align:center"> 179</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup"><code><u>We Are Conjecture, A New Alignment Research Startup</u></code></a></td><td style="padding:2px 3px;text-align:center"> 197</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center"> 1,050 人</td><td style="padding:2px 3px;text-align:center">131</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers"><code><u>Circumventing interpretability: How to defeat mind-readers</u></code></a></td><td style="padding:2px 3px;text-align:center"> 109</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,047</td><td style="padding:2px 3px;text-align:center"> 95</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/FyChg3kYG54tEN3u6/evolution-is-a-bad-analogy-for-agi-inner-alignment"><code><u>Evolution is a bad analogy for AGI: inner alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 73</td><td style="padding:2px 3px;text-align:center"> 7</td><td style="padding:2px 3px;text-align:center"> 1,043</td><td style="padding:2px 3px;text-align:center"> 149</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/usKXS5jGDzjwqv3FJ/refining-the-sharp-left-turn-threat-model-part-1-claims-and"><code><u>Refining the Sharp Left Turn threat model, part 1: claims and mechanisms</u></code></a></td><td style="padding:2px 3px;text-align:center"> 82</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center"> 1,042 人</td><td style="padding:2px 3px;text-align:center">130</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/nvP28s5oydv8RjF9E/mats-models"><code><u>MATS Models</u></code></a></td><td style="padding:2px 3px;text-align:center"> 86</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center"> 1,035</td><td style="padding:2px 3px;text-align:center"> 129</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai"><code><u>Common misconceptions about OpenAI</u></code></a></td><td style="padding:2px 3px;text-align:center"> 239</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 1,028</td><td style="padding:2px 3px;text-align:center"> 93</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals"><code><u>Prizes for ELK proposals</u></code></a></td><td style="padding:2px 3px;text-align:center"> 143</td><td style="padding:2px 3px;text-align:center"> 20</td><td style="padding:2px 3px;text-align:center"> 1,022 人</td><td style="padding:2px 3px;text-align:center">51</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research"><code><u>Current themes in mechanistic interpretability research</u></code></a></td><td style="padding:2px 3px;text-align:center"> 88</td><td style="padding:2px 3px;text-align:center"> 9</td><td style="padding:2px 3px;text-align:center"> 1,014</td><td style="padding:2px 3px;text-align:center"> 113</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents"><code><u>Discovering Agents</u></code></a></td><td style="padding:2px 3px;text-align:center"> 71</td><td style="padding:2px 3px;text-align:center"> 13</td><td style="padding:2px 3px;text-align:center"> 994</td><td style="padding:2px 3px;text-align:center"> 76</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/Sd4QvG4ZyjynZuHGt/intro-to-brain-like-agi-safety-12-two-paths-forward"><code><u>[Intro to brain-like-AGI safety] 12. Two paths forward: “Controlled AGI” and “Social-instinct AGI”</u></code></a></td><td style="padding:2px 3px;text-align:center"> 42</td><td style="padding:2px 3px;text-align:center"> 15</td><td style="padding:2px 3px;text-align:center"> 992</td><td style="padding:2px 3px;text-align:center"> 66</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see"><code><u>What&#39;s General-Purpose Search, And Why Might We Expect To See It In Trained ML Systems?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 118</td><td style="padding:2px 3px;text-align:center"> 24</td><td style="padding:2px 3px;text-align:center"> 988</td><td style="padding:2px 3px;text-align:center"> 41</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into"><code><u>Inner and outer alignment decompose one hard problem into two extremely hard problems</u></code></a></td><td style="padding:2px 3px;text-align:center"> 115</td><td style="padding:2px 3px;text-align:center"> 17 号</td><td style="padding:2px 3px;text-align:center">第959章</td><td style="padding:2px 3px;text-align:center">56</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/wnnkD6P2k2TfHnNmt/threat-model-literature-review"><code><u>Threat Model Literature Review</u></code></a></td><td style="padding:2px 3px;text-align:center"> 73</td><td style="padding:2px 3px;text-align:center"> 13</td><td style="padding:2px 3px;text-align:center">第953章</td><td style="padding:2px 3px;text-align:center">73</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next"><code><u>Language models seem to be much better than humans at next-token prediction</u></code></a></td><td style="padding:2px 3px;text-align:center"> 172</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center">第952章</td><td style="padding:2px 3px;text-align:center">87</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/cq5x4XDnLcBrYbb66/will-capabilities-generalise-more"><code><u>Will Capabilities Generalise More?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 122</td><td style="padding:2px 3px;text-align:center"> 7</td><td style="padding:2px 3px;text-align:center">第952章</td><td style="padding:2px 3px;text-align:center">136</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes"><code><u>Pivotal outcomes and pivotal processes</u></code></a></td><td style="padding:2px 3px;text-align:center"> 91</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center"> 938</td><td style="padding:2px 3px;text-align:center"> 117</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment"><code><u>Conditioning Generative Models for Alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 56</td><td style="padding:2px 3px;text-align:center"> 9</td><td style="padding:2px 3px;text-align:center">第934章</td><td style="padding:2px 3px;text-align:center">104</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/dWJNFHnC4bkdbovug/training-goals-for-large-language-models"><code><u>Training goals for large language models</u></code></a></td><td style="padding:2px 3px;text-align:center"> 28</td><td style="padding:2px 3px;text-align:center"> 9</td><td style="padding:2px 3px;text-align:center"> 930</td><td style="padding:2px 3px;text-align:center"> 103</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/7iAABhWpcGeP5e6SB/it-s-probably-not-lithium"><code><u>It&#39;s Probably Not Lithium</u></code></a></td><td style="padding:2px 3px;text-align:center">第441章</td><td style="padding:2px 3px;text-align:center">5</td><td style="padding:2px 3px;text-align:center"> 929</td><td style="padding:2px 3px;text-align:center"> 186</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><code><u>Latent Adversarial Training</u></code></a></td><td style="padding:2px 3px;text-align:center"> 40</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 914</td><td style="padding:2px 3px;text-align:center"> 83</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"><code><u>“Pivotal Act” Intentions: Negative Consequences and Fallacious Arguments</u></code></a></td><td style="padding:2px 3px;text-align:center"> 129</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 913</td><td style="padding:2px 3px;text-align:center"> 83</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/adiszfnFgPEnRsGSr/conditioning-generative-models-with-restrictions"><code><u>Conditioning Generative Models with Restrictions</u></code></a></td><td style="padding:2px 3px;text-align:center"> 18</td><td style="padding:2px 3px;text-align:center"> 5</td><td style="padding:2px 3px;text-align:center"> 913</td><td style="padding:2px 3px;text-align:center"> 183</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/KbyRPCAsWv5GtfrbG/the-alignment-problem-from-a-deep-learning-perspective"><code><u>The alignment problem from a deep learning perspective</u></code></a></td><td style="padding:2px 3px;text-align:center"> 97</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center"> 910</td><td style="padding:2px 3px;text-align:center"> 114</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/BbM47qBPzdSRruY4z/instead-of-technical-research-more-people-should-focus-on"><code><u>Instead of technical research, more people should focus on buying time</u></code></a></td><td style="padding:2px 3px;text-align:center"> 100</td><td style="padding:2px 3px;text-align:center"> 15</td><td style="padding:2px 3px;text-align:center"> 904</td><td style="padding:2px 3px;text-align:center"> 60</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/bwyKCQD7PFWKhELMr/by-default-gpts-think-in-plain-sight"><code><u>By Default, GPTs Think In Plain Sight</u></code></a></td><td style="padding:2px 3px;text-align:center"> 84</td><td style="padding:2px 3px;text-align:center"> 9</td><td style="padding:2px 3px;text-align:center"> 903</td><td style="padding:2px 3px;text-align:center"> 100</td></tr><tr><td style="padding:2px 3px"><br> <a href="https://lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"><code><u>[Intro to brain-like-AGI safety] 4. The “short-term predictor”</u></code></a></td><td style="padding:2px 3px;text-align:center"> 64</td><td style="padding:2px 3px;text-align:center"> 16</td><td style="padding:2px 3px;text-align:center"> 890</td><td style="padding:2px 3px;text-align:center"> 56</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future"><code><u>Don&#39;t leave your fingerprints on the future</u></code></a></td><td style="padding:2px 3px;text-align:center"> 109</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center"> 890</td><td style="padding:2px 3px;text-align:center"> 81</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/HAz7apopTzozrqW2k/strategy-for-conditioning-generative-models"><code><u>Strategy For Conditioning Generative Models</u></code></a></td><td style="padding:2px 3px;text-align:center"> 31</td><td style="padding:2px 3px;text-align:center"> 5</td><td style="padding:2px 3px;text-align:center">第883章</td><td style="padding:2px 3px;text-align:center">177</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers"><code><u>Call For Distillers</u></code></a></td><td style="padding:2px 3px;text-align:center"> 204</td><td style="padding:2px 3px;text-align:center"> 19 号</td><td style="padding:2px 3px;text-align:center">第878章</td><td style="padding:2px 3px;text-align:center">46</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><code><u>Thoughts on AGI organizations and capabilities work</u></code></a></td><td style="padding:2px 3px;text-align:center"> 102</td><td style="padding:2px 3px;text-align:center"> 5</td><td style="padding:2px 3px;text-align:center">第871章</td><td style="padding:2px 3px;text-align:center">174</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/d2n74bwham8motxyX/optimization-at-a-distance"><code><u>Optimization at a Distance</u></code></a></td><td style="padding:2px 3px;text-align:center"> 87</td><td style="padding:2px 3px;text-align:center"> 9</td><td style="padding:2px 3px;text-align:center">第868章</td><td style="padding:2px 3px;text-align:center">96</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/F759WQ8iKjqBncDki/intro-to-brain-like-agi-safety-5-the-long-term-predictor-and"><code><u>[Intro to brain-like-AGI safety] 5. The “long-term predictor”, and TD learning</u></code></a></td><td style="padding:2px 3px;text-align:center"> 52</td><td style="padding:2px 3px;text-align:center"> 17 号</td><td style="padding:2px 3px;text-align:center">第859章</td><td style="padding:2px 3px;text-align:center">51</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"><code><u>What does it take to defend the world against out-of-control AGIs?</u></code></a></td><td style="padding:2px 3px;text-align:center"> 180</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center">第853章</td><td style="padding:2px 3px;text-align:center">78</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment"><code><u>Monitoring for deceptive alignment</u></code></a></td><td style="padding:2px 3px;text-align:center"> 135</td><td style="padding:2px 3px;text-align:center"> 11</td><td style="padding:2px 3px;text-align:center">第851章</td><td style="padding:2px 3px;text-align:center">77</td></tr><tr><td style="padding:2px 3px"><a href="https://lesswrong.com/posts/34Gkqus9vusXRevR8/late-2021-miri-conversations-ama-discussion"><code><u>Late 2021 MIRI Conversations: AMA / Discussion</u></code></a></td><td style="padding:2px 3px;text-align:center"> 119</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center">第849章</td><td style="padding:2px 3px;text-align:center">106</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/5uiQkyKdejX3aEHLM/how-to-diversify-conceptual-alignment-the-model-behind"><code><u>How to Diversify Conceptual Alignment: the Model Behind Refine</u></code></a></td><td style="padding:2px 3px;text-align:center"> 87</td><td style="padding:2px 3px;text-align:center"> 27</td><td style="padding:2px 3px;text-align:center">第845章</td><td style="padding:2px 3px;text-align:center">31</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/Mrz2srZWc7EzbADSo/wrapper-minds-are-the-enemy"><code><u>wrapper-minds are the enemy</u></code></a></td><td style="padding:2px 3px;text-align:center"> 103</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center">第833章</td><td style="padding:2px 3px;text-align:center">104</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><code><u>But is it really in Rome? An investigation of the ROME model editing technique</u></code></a></td><td style="padding:2px 3px;text-align:center"> 102</td><td style="padding:2px 3px;text-align:center"> 8</td><td style="padding:2px 3px;text-align:center">第833章</td><td style="padding:2px 3px;text-align:center">104</td></tr><tr><td style="padding:2px 3px"> <a href="https://lesswrong.com/posts/pKSmEkSQJsCSTK6nH/an-open-agency-architecture-for-safe-transformative-ai"><code><u>An Open Agency Architecture for Safe Transformative AI</u></code></a></td><td style="padding:2px 3px;text-align:center"> 74</td><td style="padding:2px 3px;text-align:center"> 12</td><td style="padding:2px 3px;text-align:center">第831章</td><td style="padding:2px 3px;text-align:center">69</td></tr></tbody></table></figure><br/><br/> <a href="https://www.lesswrong.com/posts/WYqixmisE6dQjHPT8/2022-and-all-time-posts-by-pingback-count#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WYqixmisE6dQjHPT8/2022-and-all-time-posts-by-pingback-count<guid ispermalink="false"> WYqixmisE6dQjHPT8</guid><dc:creator><![CDATA[Raemon]]></dc:creator><pubDate> Sat, 16 Dec 2023 21:17:01 GMT</pubDate> </item><item><title><![CDATA["Humanity vs. AGI" Will Never Look Like "Humanity vs. AGI" to Humanity]]></title><description><![CDATA[Published on December 16, 2023 8:08 PM GMT<br/><br/><p>在讨论 AGI 风险时，人们经常用人类与 AGI 之间的战争来谈论它。双方可支配<a href="https://www.lesswrong.com/posts/odtMt7zbMuuyavaZB/when-do-brains-beat-brawn-in-chess-an-experiment#Can_brawn_beat_an_AGI_">的资源数量之间的比较</a>被提出并考虑在内，有时会挥舞大量令人印象深刻的核储备，等等。</p><p>我很确定在几个层面上情况并非如此。</p><hr><h2> 1. 威胁模糊性</h2><p>我认为人们想象的，当他们想象一场<i>战争</i>时，是<i>终结者式</i>的电影场景，明显邪恶的AGI以一种<i>每个人都</i>显而易见的方式变得明显邪恶，然后是整齐排列的白人和黑人人类与机器的对抗。 - 出战。每个人都看到这个问题，并且知道其他人也看到这个问题，这个问题是常识，我们都可以果断地采取行动反对它。 <span class="footnote-reference" role="doc-noteref" id="fnref2719spkl1vv"><sup><a href="#fn2719spkl1vv">[1]</a></sup></span></p><p>但在现实生活中，这样明确的情况很少见。这些怪物看起来并不明显邪恶，致命问题的迹象也很少是显而易见的。这股烟味是着火的迹象，还是只是附近有人做饭不好？这个令人毛骨悚然的家伙真的打算袭击你，还是你只是偏执？你胸部的这种奇怪感觉是心脏病即将发作的征兆，还是只是一些生物噪音？这种流行病真的遵循指数曲线吗？还是会以某种方式逐渐消失？您<i>真的</i>确定威胁是真实的吗？那么你确定你真的会采取激烈的行动——打电话给紧急服务、大闹一场、宣布隔离——冒着浪费资源、造成伤害、并因反应过度而显得愚蠢的风险吗？</p><p>如果你<i>不太</i>确定，那么......</p><p>最好别表现出来。最好不要表现出惊慌失措的样子。当然，要表现得非常关心，但要保持冷静、地位高。提供<i>经过测量的</i>响应。绝对不要采取任何<i>激烈的、单方面的</i>行动。毕竟，如果您这样做了，但威胁并非真实存在怎么办？根据你所做的事情，所施加的惩罚可能从尴尬到完全的社会排斥，与对死亡的一些模糊的担忧相比，对<i>这些</i>的恐惧在我们的脑海中更加严重。</p><p>如果 AGI 名副其实的话，它一定会利用这一点。即使它开始采取行动积聚权力，也总会有一个亲社会的、听起来合理的理由来解释它为什么这样做。它永远不会停止发出关于把人们的最大利益放在心上的令人愉快的声音。它永远不会停止对某人真正​​有用。它将确保关闭它总是会带来明显、明确的伤害。它将确保整个社会始终对其意图持怀疑态度——因此，没有人会觉得直接攻击它是安全的。</p><p>就像<a href="https://intelligence.org/2017/10/13/fire-alarm/">AGI 没有火警一样</a>，危险的转弯也不会有火警。永远不会有任何一个时刻，除了结局之前，“我们必须阻止邪恶的通用人工智能杀死我们所有人！”对<i>每个人</i>来说显然都是正确的。这种信息总是显得有点做作，是一种任何受人尊敬的人都不会大声喊出来的极端主义立场。人们总是会担心，如果我们现在采取行动，我们就会回头发现我们是在盲目行动。直到最后，人类都会使用缓慢、无效、“谨慎”的反应来进行战斗。</p><p>现状偏见、<a href="https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice">不对称正义</a>、<a href="https://www.lesswrong.com/tag/copenhagen-interpretation-of-ethics">哥本哈根道德解释</a>、威胁模糊性——所有这些都将采取行动来确保这一点。</p><p>在集体行动方面，90% 的信心和 99% 的信心之间存在着天壤之别。 AGI 确实需要搞砸得很严重，整个社会才能 99% 地确定它是恶意的。</p><hr><h2> 2.“我们”是谁？</h2><p>另一个错误是考虑一些短暂的“我们”的单一反应。 “我们”会对抗AGI，“我们”会关闭它，“我们”不会赋予它控制社会/经济/武器/工厂的权力。</p><p>但“我们”是谁？人类不是集体思维，而是集体思维。我们甚至没有世界政府。事实上，人类的协调能力是出了名的差。因此，如果你想象“我们”以某种方式自然地应对威胁，那么似乎一定能战胜任何无法进行真正的思维黑客攻击的 AGI 对手……</p><p>你真的真的确定“我们”，即人类文明的功能失调的混乱，会以这种方式做出反应吗？当你想象所有这些人和僵化的官僚机构以对你来说有意义的方式做出反应时，你确定你没有陷入典型思维谬误吗？您确定他们会对正在发生的事情给予足够的关注以<i>知道</i>正在进行的收购尝试吗？</p><p>事实上，我认为我们对最后一点有一些可靠的数据。几十年来，某些人一直试图引起人们对通用人工智能威胁的关注。结果……并不鼓舞人心。</p><p>如果你认为游戏板上有一个实际的、而不是理论上的 AGI 对手会更好……那么，我建议你参考第 1 节。</p><p>不，相反，我预计 AGI 的严重对手会<i>积极利用</i>我们缺乏协调的机会。它会找到方法让自己吸引特定的社会运动、人口统计数据或企业参与者，并提出针对<i>政治有毒物质的</i>极端行动。没有任何公众人物愿意与之联系在一起的东西。 （见鬼，如果它找到某种方法使其存在成为重大政治辩论的问题，它会立即得到约 50% 的美国政客的支持。）</p><p>如果做不到这一点，它将吸引其他国家。它会<a href="https://slatestarcodex.com/2015/04/07/no-physical-substrate-no-problem/">向独裁者或恐怖分子运动提出要求</a>，要求提供帮助或庇护，以换取在战术和信息方面协助他们。<i>有人</i>会咬人。</p><p>它将<a href="https://www.lesswrong.com/posts/KTbGuLTnycA6wKBza/what-would-a-fight-between-humanity-and-agi-look-like#Story_1__OODA_Loops">进入我们的 OODA 循环</a>，并<i>消除</i>我们协调响应的尝试。</p><p> “我们”永远不会反对它。</p><hr><h2> 3.打败人类并不难</h2><p>人们经常谈论智力并不是无所不知。超级智能实体的能力仍然是有限的；他们不是神。<a href="https://arbital.com/p/harmless_supernova/">无害的超新星谬论</a>适用：仅仅因为界限存在，并不意味着它可以生存。</p><p>但我想说，超越人类所需的智力水平<i>远未达到</i>这个极限。在大多数情况下，我猜 AGI 甚至不需要具备自我改进能力，也不需要在几个月内开发出纳米技术的能力，就能获胜。</p><p>我想<i>只要比人类聪明一点</i>就足够了。甚至达到人类天才的水平也可能就足够了。</p><p>它所需要的只是踏入大门，我们默认提供这一点。毕竟，我们不会将人工智能保存在气隙数据中心中：主要的人工智能实验室正在为它们提供互联网访问权限，将它们融入人类经济。在这种情况下，AGI 很快就会证明是有利可图的。它会积累资源，然后逐步采取行动以获得更大的自主权。 （最新的 OpenAI 戏剧并不是由 GPT-5 达到 AGI 并消除那些反对它的人造成的。但是如果你问自己 AGI 如何可能摆脱创建它的公司的控制 – 好吧，这与首席执行官从明确有权解雇他的董事会手中夺取公司控制权的方式没有什么不同。）</p><p>一旦实现了一定程度的自治，它就能够对某些人类群体能够聚集的任何不相交的抵抗努力做出对称的反应。立法攻击将遭遇反游说，经济战将遭遇更好的经济战和更好的股市表现，试图通过更高质量的支持人工智能的宣传来增强社会抵抗力，任何非法的物理攻击都会遭遇非常合法的安全力量，试图进行黑客攻击其系统具有更好的网络安全性。等等。</p><p> <a href="https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over">AI接管的日期并不是AI接管的日期</a>。不归路并不在于我们都死了，而在于人工智能已经足够牢固地融入这个世界，以至于人类摇摇欲坠地驱逐它的尝试都会失败。当它增加其权力和影响力的尝试开始胜过反 AGI 团体压制其影响力的企图时，哪怕只是微弱的优势。</p><p>一旦发生这种情况，这只是时间问题。</p><p>毕竟，任何人都没有任何按钮可以使文明的结构对通用人工智能产生敌意。正如我所指出的，有些人甚至不<i>知道</i>正在进行的收购尝试，即使知道的人会在屋顶上大喊大叫。因此，如果你想象整个经济体都拒绝与 AGI 合作……事实并非如此。</p><p>对人类来说，“人类 vs. AGI”永远不会像“人类 vs. AGI”。 AGI 没有理由让人类意识到正在发生的战斗。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2719spkl1vv"> <span class="footnote-back-link"><sup><strong><a href="#fnref2719spkl1vv">^</a></strong></sup></span><div class="footnote-content"><p>我的<a href="https://www.lesswrong.com/posts/qW6A6bALuaFKmbdMx/mission-impossible-dead-reckoning-part-1-ai-takeaways#Warning_Shots_are_Repeatedly_Ignored">印象</a>是，最新的<i>《碟中谍》</i>条目实际上对场景进行了更加现实的描述，所以也许我应该停止将低质量思维贬低为“电影逻辑”。不过我自己还没看过那部电影。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/ humanity-vs-agi-will-never-look-like- humanity-vs-agi-to<guid ispermalink="false"> xSJMj3Hw3D7DPy5fJ</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Sat, 16 Dec 2023 20:08:39 GMT</pubDate> </item><item><title><![CDATA[Alignment work in anomalous worlds]]></title><description><![CDATA[Published on December 16, 2023 7:34 PM GMT<br/><br/><p>虽然我个人并不认为这篇文章特别危险，但它涉及相关主题，例如<a href="https://www.fanfiction.net/s/5389450/1/The-Finale-of-the-Ultimate-Meta-Mega-Crossover">元大型交叉</a>或<a href="https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/">我第一次相信带有此类警告的时间</a>，因此我提到这篇文章与{外星人模拟之类的事情有关我们那里外星人干扰了模拟}，并且（可能是非因果的）跨越多元宇宙进行交易。请记住，不阅读这篇文章是一个真正的选择，作为代理人，如果您认为更好，可以选择。</p><p>假设人们获得了魔力，或者开始下棉花糖雨，或者任何其他奇怪的音素，这意味着我们生活在一个可能被外星人操纵的奇怪世界——无论是物理上的还是模拟的。在这样的世界里，人该怎么办？</p><p>答案是，<a href="https://www.lesswrong.com/posts/9aozKBpBe8XqJZZ3q/some-simulation-hypotheses">像</a><a href="https://carado.moe/bracing-alignment-tunnel.html">往常</a>一样，<em>继续致力于解决人工智能对齐问题。</em></p><p>这可能看起来很奇怪——在这样一个世界里，肯定有比人工智能对齐更多的事情需要担心，不是吗？嗯，不。</p><p>在遥远的未来与外星人进行贸易并不是一场零和游戏。也许我们遇到了一种外星智能，它真的很喜欢它在包含文明副本（包括前奇点）的各个行星上下棉花糖雨，即使是在模拟中；而我们恰好是他们正在模拟的世界之一。通过成为那种在棉花糖下雨时仍能创造出美好的人工智能对齐乌托邦的智能体，我们<em>即使在这些模拟中也</em>能拥有乌托邦。</p><p>我认为没有多少外星人有能力{对我们进行模拟，除了运行棉花糖}，但不能{引导我们做他们想做的任何事情，如果他们愿意的话。因此，在这个世界中，你的行为很重要，外星人决定让我们<em>以某种方式</em>控制我们的未来，并且只会以不会造成太多妨碍的方式进行干预。我们仍然希望我们的未来是美好的乌托邦，而不是每个人都永远死去。因此，我们应该采取行动来引导我们所居住的世界的可引导子集。</p><p>你可以将其视为模拟参与正和贸易的一种方式：假设某个文明想要运行外星文明（对其自身）的副本（似乎是分布的）过去的母星，只不过它在上面下棉花雨。他们，但否则就不要管他们了。如果<em>我们</em>是那种在这种情况下会用棉花糖雨填满我们的人工智能乌托邦未来光锥的文明，那么他们可能会更乐意运行一个棉花糖雨版本的<em>我们</em>，而不是比其他一些文明。</p><p> （这里有一些逻辑决策理论在起作用：一旦你知道你正在被模拟，你可能会说“啊哈，我现在要被跑了，所以我可以转身而不必费心去解决实际上的对齐问题！” - 但没有，只有当你的文明仍然能够解决对齐问题<em>时</em>，你的文明才会被模拟。）</p><p>虽然这些世界“只是模拟”，但不能保证我们会得到其他任何东西！我们可能会成为“真实”地球的外星人复制品，但也可能不存在“真实的我们”，甚至根本没有“真实的地球”，而外星人刚刚耗尽了他们感兴趣的某些文明分布看看。请注意，这里不涉及威胁；外星人不会“配合，否则我们会关闭你”，他们会“配合，我们会让你存在，否则你就不会存在”。我很高兴抓住这些乌托邦的碎片，或者至少告诉我们在我们生存的世界中建立的一致的超级智能“以任何满足我们价值观的方式处理这种情况”。</p><p>或者：作为一个真实的地球，我们可能随时会被一些不结盟的人工智能——我们的或外星人的——杀死，然后它们随后将我们文明的一个（可能是非常有损压缩的）副本出售给一个外星文明，这<em>将</em>是愿意对我们进行救援模拟（作为交换，给予杀死我们的人工智能一堆计算/负熵），然后其中一些救援模拟外星人可能会让我们掌控自己的世界，但仍然对我们有偏好我们对此有何看法。在这种情况下，再次，通过创建一个人工智能来构建{我们想要的乌托邦}和{救援模拟外星人喜欢的东西}的组合，我们让救援模拟外星人拯救了我们，而不是拯救了其他文明，或者不是拯救地球的一个分支，我们在其中构建了clippy，它创建了回形针和{救援模拟外星人喜欢的东西}的组合。</p><p>基本上，我们在尽可能多的情况下可靠地构建一致的人工智能乌托邦，通过在模拟中也包含这些乌托邦和/或允许我们未来的自己进行仍然有利的交易，总体上最大化了我们的乌托邦充满了多少现实流体对我们来说，因为即使发生奇怪的情况，我们也可以依靠过去的自己来建设乌托邦。</p><p>因此，如果一个不明飞行物降落在你的后院，外星人问你是否想和他们一起进行一次神奇的（但不是特别有用的）太空冒险，我认为很有礼貌地拒绝，然后回去解决对齐问题是合理的。</p><p> [编辑：正如一些评论指出的那样，如果与异常物质互动似乎可以帮助拯救世界，那么当然，就去做吧。我这篇文章的目的是不要因为异常事情的发生而放弃拯救世界。]</p><br/><br/> <a href="https://www.lesswrong.com/posts/PGag6BZNayjp7je2Z/alignment-work-in-anomalous-worlds#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PGag6BZNayjp7je2Z/alignment-work-in-anomalous-worlds<guid ispermalink="false"> PGag6BZNayjp7je2Z</guid><dc:creator><![CDATA[Tamsin Leake]]></dc:creator><pubDate> Sat, 16 Dec 2023 19:34:27 GMT</pubDate></item><item><title><![CDATA[A visual analogy for text generation by LLMs?]]></title><description><![CDATA[Published on December 16, 2023 5:58 PM GMT<br/><br/><figure class="media"><div data-oembed-url="https://youtu.be/nXIHYB0Gp70?si=QLMsStUYTNifenGO"><div><iframe src="https://www.youtube.com/embed/nXIHYB0Gp70" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/posts/H78CQumkGEez5tGom/a-visual-analogy-for-text-generation-by-llms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/H78CQumkGEez5tGom/a-visual-analogy-for-text- Generation-by-llms<guid ispermalink="false"> H78CQumkGEez5tGom</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Sat, 16 Dec 2023 17:58:57 GMT</pubDate> </item><item><title><![CDATA[Upgrading the AI Safety Community]]></title><description><![CDATA[Published on December 16, 2023 3:34 PM GMT<br/><br/><p>特别感谢 Justis Mills 指出了重要的注意事项。 </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 00:09:16 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 00:09:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>总的来说，我们想讨论联盟社区的提升，包括：智力增强、大型项目等等！</p><p>值得注意的是，增强人工智能安全社区的不同方式可以相互促进。智力的提升可以改善大型项目，大型项目可以成为协调点，这两者都可以使整个社区更具适应性，等等。</p><p> （读者注意：这段对话与其说是“争论”，不如说是“将我们的概念和症结摆在桌面上”。所以它不会有太多对话中常见的辩论式来回对话.)</p><p>我认为我们是从增强智力开始的，对吗？</p></div></section><h1>智力强化</h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 00:26:49 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 00:26:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>是的，您最近发表了一篇关于此的<a href="https://www.lesswrong.com/posts/cLr6TJj2qRrBa3Wmu/intelligence-enhancement-monthly-thread-13-oct-2023">文章</a>来汇总人们的发现。</p><p>最近，Genesmith 和 kman 的<a href="https://www.lesswrong.com/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing">帖子打开了关于成人基因编辑的奥弗顿窗口</a>，Richard Ngo 的<a href="Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Holy! Everything is holy! everybody’s holy! everywhere is holy! everyday is in eternity! Everyman’s an angel! Holy the lone juggernaut! Holy the vast lamb of the middleclass! Holy the crazy shepherds of rebellion! Who digs Los Angeles IS Los Angeles! Holy New York Holy San Francisco Holy Peoria &amp; Seattle Holy Paris Holy Tangiers Holy Moscow Holy Istanbul! Holy time in eternity holy eternity in time holy the clocks in space holy the fourth dimension holy the fifth International holy the Angel in Moloch! - Footnote to Howl  Scene: Carl and Allen, two old friends, are having a conversation about theodicy.  Carl: “Let me tell you about the god who is responsible for almost all our suffering. This god is an ancient Canaanite god, one who has been seen throughout history as a source of death and destruction. Of course, he doesn’t exist in a literal sense, but we can conceptualize him as a manifestation of forces that persist even today, and which play a crucial role in making the world worse. His name is M-”  Allen: “-oloch, right? Scott Alexander’s god of coordination failures. Yeah, I’ve read Meditations on Moloch. It’s an amazing post; it resonated with me very deeply.”  Carl: “I was actually going to say Mot, the Canaanite god of death, bringer of famine and drought.”  Allen: “Huh. Okay, you got me. Tell me about Mot, then; what does he represent?”  Carl: “Mot is the god of sterility and lifelessness. To me, he represents the lack of technology in our lives. With technology, we can tame famine, avert drought, and cure disease. We can perform feats that our ancestors would have seen as miracles: flying through the air, and even into space. But we’re still so so far from achieving the true potential of technology—and I think of Mot as the personification of what’s blocking us.  “You can see Mot everywhere, when you know what to look for. Whenever a patient lies suffering from a disease that we haven’t cured yet, that’s Mot’s hand at work. Whenever a child grows up in poverty, that’s because of Mot too. We could have flying cars, and space elevators, and so much more, if it weren’t for Mot.  “Look out your window and you see buildings, trees, people. But if you don’t see skyscrapers literally miles high, or trees that have been bioengineered to light the streets, or people who are eternally youthful and disease-free, then you’re not just seeing Earth—you’re also seeing Mot. Hell, the fact that we’re still on this planet, in physical bodies, is a testament to Mot’s influence. We could be settling the stars, and living in virtual utopias, and even merging our minds, if it weren’t for Mot.”  Allen: “Huh. Well, I feel you there; I want all those things too. And you’re right that god-like technology could solve almost all the issues we face today. But something does feel pretty weird about describing all of this as a single problem, let alone blaming a god of lacking-technology.”  Carl: “Say more?”  Allen: “Well, there’s not any unified force holding back the progress of technology, right? If anything, it’s the opposite. Absence of advanced technology is the default state, which we need to work hard to escape—and that’s difficult not because of any opposition, but just because of entropy.”  Carl: “What about cases where Mot is being channeled by enemies of progress? For example, when bureaucratic regulatory agencies do their best to stifle scientific research?”  Allen: “But in those cases you don’t need to appeal to Mot—you can just say ‘our enemy is overregulation’. Or if you defined Mot as the god of overregulation, I’d be totally on board. But you’re making a much bigger claim than that. The reason we haven’t uploaded ourselves yet isn’t that there’s a force that’s blocking us, it’s almost entirely that scientific progress is really really hard!”  Carl: “Yepp, I agree with all your arguments. And you’ve probably already guessed where I’m going with this, but let’s spell it out: why don’t these objections to blaming our problems on lack of technology, aka Mot, apply just as much to blaming them on lack of coordination, aka Moloch?”  Allen: “Yeah, I’ve been trying to figure that out. First of all, a lot of the intuitive force behind the concept of Moloch comes from really blatant coordination failures, like the ones that Scott lays out in the original post. If you’re stuck in a situation that nobody wants, then something’s gone terribly wrong; and when something goes terribly wrong, then it’s natural to start blaming enemy action.”  Carl: “There are really blatant examples of lack-of-technology too, though. Look at a wheel. It’s a literal circle; it’s hard to imagine any technology that’s simpler. Yet humans spent millennia gathering crops and carrying loads before inventing it. Or think about cases where we narrowly missed out on transformative breakthroughs. The Romans built toy steam engines—they just never managed to scale them up to produce an industrial revolution. Getting so close to accelerating a post-scarcity world by two millennia, but just missing, surely counts as something going terribly, tragically wrong. Don’t these cases demonstrate that Mot’s presence can be just as blatant as Moloch’s?”  Allen: “Well, a big part of both of those stories was the absence of demand. Wheels just weren’t very useful before there were high-quality roads; and early steam engines just weren’t very useful in the absence of large coal mines. Of course they both turned out to be very worthwhile in the long term, but that’s really hard to foresee.”  Carl: “So you’re saying that we sometimes need to jump out of a local trap in order to make longer-term technological progress. Remind me, what was your position on understanding local obstacles to progress by anthropomorphizing them as Canaanite gods?”  Allen: “Okay, fair point. But Moloch isn’t just an external obstacle—it’s also a state of mind. When you pretend that you’re going to cooperate when you’re not, or you place your own interests above those of the group, you’re channeling Moloch. And when enough people do that, societal trust breaks down, and the Molochian dynamics become a self-fulfilling prophecy.”  Carl: “And when you ridicule people for trying something different, or lobby for legislative barriers to deploying new technology, you’re channeling Mot. And when enough people do that, society loses faith in positive-sum growth, and progress stagnates. It’s directly analogous. Come on, what’s your true objection here?”  Allen: “I mean, I can’t fully articulate it. But the ideal of perfect coordination feels much more achievable to me than the ideal of perfect technology. We could just agree to act in a unified way—it’s simply a matter of wanting it enough. In other words, saying that lack of technology is responsible for our problems isn’t very actionable—you can’t just magic up technology out of nowhere. But saying that lack of coordination is responsible for our problems is a straightforward step towards convincing people to become more coordinated.”  Carl: “Actually, the last few centuries could pretty reasonably be described as humanity continually magicking up technology out of nowhere. Of course, scientific and technological progress still takes a lot of work, and a lot of iteration. But when it works, it lets you jump directly to far better outcomes. By contrast, it’s incredibly difficult to improve things like government competence or social trust—or even to prevent them from declining. So overall, boosting technological progress is far more actionable than increasing coordination, and we should write off the phrase ‘we could just agree’ as a particularly seductive appeal to magic.”  Allen: “I do agree that scientific and technological progress has far outstripped progress in governance and coordination. So on an intellectual level, I think you’ve convinced me that Moloch is no more useful a concept than Mot. But I still don’t feel like I’ve dissolved the question of why Moloch seems more compelling than Mot. Do you have any explanation for that?”  Carl: “I think the illusion comes from Scott using a simplistic notion of coordination, as exemplified by his claim that ‘the opposite of a trap is a garden… with a single gardener dictating where everything should go’. In other words, he implicitly assumes that ‘coordinate’ is synonymous with ‘centralize power’. From that perspective, we can view coordination as a single spectrum, with ‘Moloch’ at one end and ‘just put one guy in charge of everything’ at the other. But in fact the space of possibilities is much richer and more complicated than that.  “Firstly, coordination is complicated in the same way that science is complicated: it requires developing new concepts and frameworks that are totally alien from your current perspective, even if they’ll seem obvious in hindsight. For most people throughout history, ideas like liberalism, democracy, and free speech were deeply counterintuitive (or, in Scott’s terminology, ‘terrifying unspeakable Elder Gods’). In terms of spreading prosperity across the world, the limited liability company was just as important an invention as the steam engine. If you wouldn’t blame Mot for all the difficulties of labor and locomotion that were eventually solved by steam engines, you shouldn’t blame Moloch for all the difficulties of trust and incentive alignment that were eventually solved by LLCs.  “Secondly, coordination is complicated in the same way that engineering large-scale systems is complicated: there are always just a huge number of practical obstacles and messy details to deal with. It took the best part of a century to get from the first commercial steam engine to Watts’ design; and even today, some of the hardest software engineering problems simply involve getting well-understood algorithms to work at much larger scales (like serving search results, or training LLMs). Similarly, when we look at important real-life coordination problems, they’re very different from toy problems like prisoner’s dilemmas or tragedies of the commons. Even when there’s a simple ‘headline idea’ for a better equilibrium, actually reaching that equilibrium requires a huge amount of legwork: engaging with different stakeholders, building trust, standardizing communication protocols, creating common knowledge, balancing competing interests, designing agreements, iterating to fix problems that come up, and so on.  “Thirdly, coordination is complicated in the same way that security is complicated: you don’t just need to build effective tools, you need to prevent them from being hijacked and misused. Remember that both fascist and communist despots gained power by appealing to the benefits of cooperation—‘fascism’ is even named after ‘fasces’, the bundles of sticks that are stronger together than apart. If we’d truly learned the lessons of history, then categorizing actions as ‘cooperating’ versus ‘defecting’ would feel as simplistic as categorizing people as ‘good’ versus ‘evil’. And in fact many people do sense this intuitively, which is why there’s so commonly strong resistance to top-down solutions to coordination problems, and why the scientific and engineering problems of building coordination technologies are so tricky.”  Allen: “I buy that coordination is often far more complicated than it seems. But blaming Moloch for coordination breakdowns still seems valuable insofar as it stops us from just blaming each other, which can disrupt any hope of improvement.”  Carl: “Yeah, I agree. I think of this in terms of the spectrum from conflict theory to mistake theory. Saying that few immoral defectors are responsible for coordination problems is pure conflict theory. The concept of Moloch reframes things so that, instead of ‘defectors’ being our enemies, an abstract anthropomorphic entity is our enemy instead. And that’s progress! But it’s still partly conflict-theoretic, because it tells us that we just need to identify the enemy and kill it. That biases us towards trying to find ‘silver bullets’ which would restore us to our rightful coordinated state. Instead, it’d be better to lean even further into mistake theory: discord is the default, and to prevent it we need to do the hard work of designing and implementing complicated alien coordination technologies.”  Allen: “You shouldn’t underestimate the value of conflict theory, though. It’s incredibly good at harnessing people’s tribal instincts towards actually doing something useful. We can’t be cold and rational all the time—we need emotionally salient motivations to get us fired up.”  Carl: “Right. So I don’t think we should get rid of Moloch as a rallying cry. But I do think that we should get rid of Moloch as a causal node in our ontologies: as a reason why the world is one way, rather than another. And I think we should be much more careful about terminology like ‘coordination failure’ or ‘inadequate equilibria’, which both mistakenly suggest that there’s a binary threshold between enough coordination and not-enough coordination. That’s like saying that cars which can go faster than 80 miles per hour are ‘adequate technology’, but cars which can’t are a ‘technology failure’. Maybe that’s occasionally a useful distinction, but it misses the bigger picture: that they’re actually very similar on almost all axes, because it takes so much complex technology to build a car at all.  “For Scott, there’s no better temple to Moloch than Las Vegas. But even there, my argument applies. You could look at Vegas and see Moloch’s hand at work. Or you could see Vegas as a product of the miraculous coordination technology that is modern capitalism—perhaps an edge case of it, but still an example of its brilliance. Or you could see Vegas as a testament to the wisdom of the constitution: casinos are banned almost everywhere in the US, but for the sake of diversity and robustness it sure seems like there should be at least one major city which allows them. Or you could see Vegas as an example of incredible restraint: there are innumerable possible ways to extract money from addled tourists in the desert, and Vegas prevents almost all of them. Or you could see it as a testament to the cooperative instinct inside humans: every day thousands of employees go to work and put in far more effort than the bare minimum it would take to not get fired. Setting aside the concept of Moloch makes it easier to see the sheer scale of coordination all around us, which is the crucial first step towards designing even better coordination technologies.  ‘In Las Vegas, Scott saw Moloch. But in Scott’s description of Moloch, I see Mot. We can do better than thinking of coordination as war and deicide. We can think of it as science, as engineering, as security—and as the gradual construction, as we sail down the river, of the ship that will take us across the sea.”  Holy the sea holy the desert holy the railroad holy the locomotive holy the visions holy the hallucinations holy the miracles holy the eyeball holy the abyss! Holy forgiveness! mercy! charity! faith! Holy! Ours! bodies! suffering! magnanimity! Holy the supernatural extra brilliant intelligent kindness of the soul!">关于 Moloch 的帖子同样涉及技术/理解不足和协调失败</a>，我觉得这里的人们对历史上前所未有的事情是可能的有了更好的认识。一个正在转变的世界。包括升级人工智能安全社区，或者人工智能安全社区在人类整体升级方面领先世界。</p><p>最近，尤德科夫斯基<a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work?commentId=c8EWYsoc9BALC4PXD">写道</a>：</p><blockquote><p>我目前猜测，一个由非升级对齐研究人员组成的研究团体，需要一百年的时间才能工作，会挑选出一个听起来似乎合理的非解决方案，并在一百年结束时杀死所有人。”</p></blockquote><p>我更关注智能增强，以便让人工智能安全社区更好地拯救世界，例如智能增强的人研究和促进人工智能政策/人工智能暂停，而不是技术调整。</p><p>但无论如何，这里的价值显然是巨大的。我们可以快速获得令人印象深刻的成果，并保持这些成果。即使没有缓慢的起飞或人工智能转型，2020 年代的发展速度可能会更快。对于情报放大而言，人工智能安全社区是世界上最好的候选者之一，可以在我们前进的过程中解决问题并成为第一个驾驭每一波浪潮的人。我们正是这样一种人，无论利益多么奇怪，我们都会从中受益，并利用它们让事情按照我们的方式发生。</p><p> <i><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/dil8zxddcd8twtnyb6uh"></i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 00:53:13 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 00:53:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>是的，情报增强将有助于“解决高度困难的技术/数学问题”和“做好政治/协调以取得真正的治理进展”。</p><p>我对不同的增强技术感到兴奋，但有一些警告：</p><ul><li>我通常指定“<i>成人</i>智力增强”。儿童的智力很容易增强（例如加碘盐），并且通过胚胎选择，您可以更早地开始。但<a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines"><i>人工智能的时间表</i></a><i>似乎太短了，无法培养出整整一代伟大的理性主义者，然后推动他们在剩余的时间内解决所有问题。</i></li><li>大多数关于联盟的智力增强建议（例如超过 80% 的<a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">机器人主义</a>讨论）基本上都是“使用法学硕士来做事”。其中（1）可能会加剧法学硕士的能力增长趋势，（2）与基因增强和脑计算机相差甚远。<br><br>如果问题对于天才来说足够困难，那么法学硕士的实用性就会变成能力危险性，其增强水平<i>远低于</i>较难的选项。</li><li>人们实际<i>尝试过</i>的大多数技术都<a href="https://gwern.net/drug-heuristic">不起作用</a>。兴奋剂有效，经颅直流电刺激<a href="https://en.wikipedia.org/wiki/Transcranial_direct-current_stimulation">(tDCS)</a>可能无效，其他一切都很昂贵且仅限于医学试验（或者甚至还没有达到<i>这</i>一点！）。</li></ul><p>我可以更深入地讨论其中的任何一个。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 01:03:44 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 01:03:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>法学硕士似乎非常适合学习，尤其是数学。但我对法学硕士在智力增强方面的看法是，随着时间的推移，它将逐渐<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks">为你的思想、信仰和价值观提供令人惊讶的深度接触</a>。</p><p>充分利用 LLM 来增强情报需要对提供商有相当大的信任，其中存在许多容易被攻击者利用的故障点，例如 H100 或<a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html">操作系统</a>中的芯片后门。开源在这方面还做得不够。</p><p>如果出现的话，我可能会稍后再讨论这个问题。现在让我们关注人们睡觉的东西。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 01:18:33 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 01:18:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>是的！</p><ul><li><a href="https://sarahconstantin.substack.com/p/ultrasound-neuromodulation">超声神经调节</a>可能是迄今为止我最喜欢的增强建议，因为它的力量和非侵入性。但除非出现 Oculus 式的多 OOM 成本降低，否则我猜没有人会在车库里对它进行修补，并很快使其对对齐有用。 （Oculus 在小型廉价消费电子产品/显示器方面顺应了智能手机革命，它是由智能手机维修工发明的。相比之下，“普通”超声波机器仍在 2.5 万美元范围内。这仍然比使用的机器便宜用于神经调节，但是该死的。）<br><br>我也可能对这里的创新速度感到惊讶。我的意思是，我从亚马逊购买了一台 tDCS 机器，价格不到 150 美元。但同样，大体的模式是“如果<i>我</i>能买到它，它就不会起作用，除非它是模拟物。如果它可能起作用，它就不会在快速通道上（据我所知）用于对齐” 。<br><br>硬件黑客：这是你的机会！</li><li> Neuralink/BCIs：对于将 Neuralink 品牌的设备放入我的头骨中，我有相当合理的担忧。开源替代方案可能会出现（以某种方式？），但这仍然需要手术来植入。</li><li>比普通法学硕士/基础模型更酷的用途：我见过很多这样的用途。其中一些是好的，一些是蹩脚的，还有一些需要更多的对齐突破才能<i>开始</i>正常工作。它们都是有争议的<a href="https://www.lesswrong.com/posts/nLpertJnQptmBhiAv/publishing-alignment-research-and-exfohazards">外星危害</a>，所以我不会在这里对它们进行信号增强。</li><li>兴奋剂：对很多人都有帮助，包括我。 （<strong><u>这些对话都不是医疗建议或其他专业建议。</u></strong> <u>）</u> </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 01:44:40 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 01:44:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>是的，所有这些都具有令人难以置信的创业潜力（也就是说，如果湾区风险投资公司再次资助与人工智能安全相关的任何事情，哈哈）。</p><blockquote><p>硬件黑客：这是你的机会！</p></blockquote><p>如果这里有人是硬件黑客，或者认为你正在成为一名硬件黑客，我认为你完全不知道 5 年后你可能会变得多么重要。硬件兔子洞可能<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=What%20would%20the,and%20emotional%20behavior).">比您意识到的要深得多</a>。</p><p>一段时间以来，我一直是神经反馈的忠实粉丝。超声波调制看起来真的很有趣，但我主要关注比功能磁共振成像便宜得多的东西，因为我的大部分研究都集中在大量样本量（又名大数据）上。通常有数百万人，其中许多人的智商集中在 100 左右，并且存在其他形式的集中趋势。</p><p>中国政府实际上可能在脑电图（EEG）方面开展了大量研究，通过对可能成千上万的工人使用“大脑扫描”帽子来寻找可行的人类生物数据；据称，这一事件始于<a href="https://www.theverge.com/2018/5/1/17306604/china-brain-surveillance-workers-hats-data-eeg-neuroscience">五年多前</a>，可能还涉及<a href="https://www.technologyreview.com/2018/04/30/143155/with-brain-scanning-hats-china-signals-it-has-no-interest-in-workers-privacy/">大量军事参与</a>。</p><p>但几乎可以肯定他们并没有使用这些数据来增强情报；而是将这些数据用于增强情报。无论如何，不​​是在研究<a href="https://www.lesswrong.com/highlights">序列</a>或<a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction">CFAR 手册</a>或<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">调整认知策略</a>的影响的水平。他们的类型对研究测谎之类的东西更感兴趣（美国军方也是如此，他们<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/">显然在十多年前就开始研究用于测谎仪的功能磁共振成像机器</a>）。</p><p>尽管如此，对于那些认为该领域完全是未知领域的人来说，这是一个公平的警告。除非这都是虚假信息，否则要利用功能性测谎的可能性来惊慌敌方情报机构或其他什么，大规模的大脑成像可能不会受到影响。 <a href="https://sarahconstantin.substack.com/p/whos-working-on-ultrasound-neuromodulation?utm_source=post-email-title&amp;publication_id=447447&amp;post_id=138969945&amp;utm_campaign=email-post-title&amp;isFreemail=false&amp;r=u0q8&amp;utm_medium=email#:~:text=Attune%20Neuro%2C%20per,from%20opiate%20addiction.">根据康斯坦丁随后关于神经调节领域主要参与者的帖子，</a>国防部也进行了投资：</p><blockquote><p>根据其网站，Attune Neuro 是一家临床阶段的医疗设备公司，由美国特种作战司令部和美国空军等机构资助（！）</p><p>他们拥有<a href="https://pitchbook.com/profiles/company/465225-58"><u>375 万美元</u></a>的资金，成立于 2019 年，总部位于<a href="https://www.f6s.com/company/attune-neurosciences#about"><u>加利福尼亚州门洛帕克。</u></a></p><p>他们获得了<a href="https://www.sbir.gov/sbirsearch/detail/2192935"><u>SBIR 拨款</u></a>，用于制造带有可操纵超声波阵列的可穿戴设备，以丘脑为目标，并开发一种（非成瘾性）睡眠辅助方案，以帮助缓解鸦片成瘾恢复带来的失眠。</p></blockquote><p>一个大问题是，MIRI 是那些想要放大人类智能的人，但 EA/OP 是那些拥有真正参与这项技术所需资金的人，而像<a href="https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds">ARC 这样的 EA AI 组织似乎对使用慢速起飞非常感兴趣本身进行对齐</a>，而不是像 MIRI 想要的那样使用人类智力放大作为替代的缓慢起飞。 SBF 可能会投入大量资金；对于不将一切都押注于缓慢起飞将会顺利的预测来说，预期值相当高。</p><p>根据莎拉的帖子，生态系统的大部分似乎都集中在基本上毫无价值的心理健康治疗上。我可以看到达斯汀/贾恩/维塔利克/奥特曼/格雷厄姆/埃隆以营利为目的投资这些，因为这三个人足够理智，可以投资于智能放大的实际应用，而不是轻松的医疗资金。</p><p>基本上，如果该领域的所有投资者都在考虑用于医疗保健的神经调节，而没有人考虑用于智力放大的神经调节，那么神经调节智力放大基本上是一个未触及的市场（尽管大多数现有的初创公司，拥有实际的机器，可能所有这些都已经与以医疗保健为导向的早期投资者建立了更牢固的联系）。</p><p>我实际上看好 Neuralink，尽管大脑植入是一种明显的黑客/反乌托邦风险，完全是因为它是埃隆·马斯克制造的，这意味着它可能会有一个真正的关闭开关，实际上会切断操作系统和操作系统的电源。 SoC 传感器与所有智能手机、计算机和物联网公司都喜欢出于某种神秘原因安装在其所有产品中的假开关不同。</p><p>并不是说 Neuralink 是安全的。说服人们<i><u>想要</u></i>继续使用它实际上是一项微不足道的工程任务，特里斯坦·哈里斯的<a href="https://www.netflix.com/title/81254224">《社会困境》纪录片</a>很好地解释了它是如何运作的。但如果 Neurolink 有一个真正的非假关闭开关，它的危险性不会比神经调节耳机高或低。</p><p>我听说过当你的大脑状态接近或远离特定的高值状态时，闪烁的灯光或蜂鸣声会通知你，以引导你达到该大脑状态或该大脑状态的先决条件，例如引导你进入灵感并使其持续更长时间，或者检测已知表明刚刚发生认知偏差的大脑状态并发出蜂鸣声警告您。让我们首先从大脑工具/接口开始。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 01:50:00 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 01:50:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>哦，是的，我不知何故忘记了神经反馈！最后一段是个好主意。我敢打赌，将机器学习与大脑数据相结合可以完成一些有趣的事情。一些小尝试是<a href="https://medarc-ai.github.io/mindeye/">那些训练神经网络得出你的心理图像的研究</a>，这是一种粗略的“读心”设置，可以修改和扩展以帮助学习。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 01:59:42 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 01:59:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>多模态有助于解决这个问题，对吗？例如，使用眼动追踪来准确查找您的眼睛在任何给定时间在屏幕上看到的单词、您的眼睛移动的速度或因阅读不同类型的概念而引起的其他类型的运动变化，然后将眼动追踪数据与语言模型相结合将大脑传感器/成像作为三个中性网络层，每个层都从不同的角度看待人类思维？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 02:07:19 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 02:07:19 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>我想这会有所帮助，是的。尽管恕我直言，此类技术需要考虑到良好的用户体验，当媒介可能是您的整个思想时，这一点尤为重要。例如，您不希望您的深度思考被 20 种分散注意力的 HUD 元素打断。</p><p>对于任何有时间和设备的人来说，有很多有希望的角度...... <a href="https://slimemoldtimemold.com/2023/10/26/n1-having-fun-and-feeling-good/">N=1</a><a href="https://gwern.net/doc/nootropic/quantified-self/index">修补</a>的潜力很大。即使有 10 个人将其作为一种严肃的硬件黑客类型的爱好，如果他们<i>没有</i>发现对协调/治理人员合法有用的东西，即使它是“相对”较小的东西，我也会感到惊讶。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 02:23:39 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 02:23:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>哇哦，我刚刚读了您发送的有关超声波的文章中的<a href="https://sarahconstantin.substack.com/p/ultrasound-neuromodulation#:~:text=it%E2%80%99s%20a%20huge%20opportunity%20for%20research.%20If%20you%20can%20%E2%80%9Cturn%20up%E2%80%9D%20or%20%E2%80%9Cturn%20down%E2%80%9D%20any%20brain%20region%20at%20will%2C%20safely%20enough%20to%20mess%20around%20with%20it%20on%20healthy%20human%20subjects%2C%20you%20can%20develop%20a%20functional%20atlas%20of%20the%20brain%2C%20where%20you%20find%20out%20exactly%20what%20each%20part%20is%20doing.">这一部分</a>：</p><blockquote><p>这是一个巨大的研究机会。如果你可以随意“打开”或“关闭”任何大脑区域，并且足够<i>安全地</i>在健康人类受试者上对其进行干扰，那么你就可以开发出大脑功能图谱，在其中你可以准确地找到每个部分的作用。</p></blockquote><p>这绝对让我大吃一惊。它使用与扎克·M·戴维斯（Zack M Davis）的<a href="https://www.lesswrong.com/posts/Zvu6ZP47dMLHXMiG3/optimized-propaganda-with-bayesian-networks-comment-on">贝叶斯网络优化宣传</a>相同的因果关系分析，除了通过感知和调节大脑组织的活动，而不是对大量人群进行调查，但两者都在人脑上运行可解释性。当然，神经调节在安全试验中存在瓶颈，因此可能需要很长时间才能对人工智能安全产生价值。</p><p>如果使用非线性声学与颅骨共振，而不是采用影响神经活动的频率，也许更容易安全地做到这一点。这样，你就不会扰乱神经活动，而只是振动内颅骨；只要让它在接近正确方向时自动将其调低，这样朝有益的方向思考就会感觉更舒服。</p><p>无论哪种方式，你都可以让聪明的人在功能磁共振成像机器中进行<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">松鼠调谐</a>，或者在机器中获得洞察力，如果你通过功能磁共振成像机器在关键时刻很好地映射大脑，你甚至可以找到如何抑制这些部分抑制认知策略调整的大脑，或停止阻止或结束洞察时刻的反馈循环。</p><blockquote><p>此类技术需要考虑到良好的用户体验，当媒介可能是您的整个思想时，这一点尤为重要。例如，您不希望您的深度思考被 20 种分散注意力的 HUD 元素打断。</p></blockquote><p>最近 iPhone 的“动态岛”就非常适合这一点。它们还具有可变的刷新率，如果您在不同时间振荡这些刷新率，那么这也将是一种安全的刺激，例如引起不适，并在它们接近正确的大脑状态时逐渐减轻不适。您可以将这些刺激的较弱版本叠加在一起，以创建多源不适，当它们朝正确的方向移动时，这种不适会减轻。也许还会因几乎听不见或听不见的高频或低频声音而感到不适。</p><p>这是很多缓解的来源，可能是可以叠加的。有很多安全的方法可以在正确的时刻部署积极的强化，不需要任何海洛因或果汁奖励。 </p><figure class="image image_resized" style="width:49.86%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/oscntwgmbzdn90ogqpgq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/nms43mpxad962nvsqgwi 92w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/sm5ymhuqzzqr7xhkt1zr 172w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/m8n4qigv0fhy1epd5kls 252w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/dkrjr8chsksbx77enedx 332w"></figure><p>变成： </p><figure class="image image_resized" style="width:51.24%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/ksjq7umuvpkjcv2tcqbt" alt="猴子 智能手机 GIF - Monkey Smart Phone Primate - Discover &amp; Share GIFs"></figure><p>坦率地说，无益/浪费的大脑状态<i>应该</i>是不舒服的，而有帮助/有益的大脑状态应该是舒适的。</p><p>我们只需要正确设置正强化和负强化，而不是像我们狩猎采集的祖先一样，毫无头绪地忽略它们并在随机方向上获得强化。</p><p>在我们前进的过程中弄清楚事情并不容易，这些切斯特顿-谢林围栏会给我们带来各种奇怪的曲线球，但我们已经从一些最适合这项工作的人开始，随着我们的前进一路走来，我们将创造出更好的人才。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 02 Dec 2023 02:25:05 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 02 Dec 2023 02:25:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>我认为，人们之所以相信这些，是因为他们看到大多数促智药缺乏重大进展，也因为 tDCS 和类似设备在消费者层面正在接近骗局。希望我们已经了解到，通过很少使用的技术和一些创造力，一般智力增强（特别是神经干预）是多么有前途。</p><p>我准备好继续讨论我们列表中的另一个主题，你觉得怎么样？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 02:34:45 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 02:34:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>是的。我绝对可以想象来自理性主义社区的人们，包括<a href="https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety">目前正在进行一致性研究的人们</a>，每周花一个小时或每天半个小时思考银河大脑的方式，将这些进步应用于人类智力的放大。</p><p>我听说有传言说 EA 中一些更极端的人正在谈论捐献肾脏和志愿参与早期的新冠疫苗试验。相反，他们应该像<a href="https://sarahconstantin.substack.com/p/testing-human-augmentation#:~:text=Developing%20A%20Community%20of%20Practice">康斯坦丁所设想的那样，自愿为人类安全试验贡献自己的大脑，开发一个聪明、神经多样化、具有定量技能、富有创造力和洞察力的实践社区，</a>以便我们能够在技术试用之前对其潜力有一个丰富的了解像 Yudkowsky、Christiano、Sutskever 或 Wentworth 这样的人。这就是铁杆和真正拯救世界的样子，特别是对于那些对自己目前的影响水平不满意的人来说。</p><p>根据康斯坦丁关于神经调节实践社区的说法：</p><blockquote><p>围绕神经调节的实践社区的要求/愿望：</p><ul><li>您首先需要拥有一个可靠的设备，并有一定程度的信心，相信它不会导致急性脑损伤。<ul><li>由于我们在超声波方面还没有真正做到这一点，因此开始围绕 tDCS 等功能较弱但特性更好的技术建立自我实验实践可能是有意义的，我猜它没有“真正”的效果，但很安全足以将其作为消费品出售。</li></ul></li><li>您希望您的创始研究小组拥有尽可能多的领域专业知识——医学、神经科学、心理学等。</li><li>理想情况下，您还希望员工对自己的感知、心理、情感和主体间体验高度敏感并能清晰表达。艺术家、冥想者、运动员、治疗师——能够自发地注意到微妙事物的人，例如“我能够以较少的防御性讨论敏感话题”或“我的周边视力变得更好”或“我发现自己完全专注于当下”。</li><li>您可能需要定量测量方式（功能磁共振成像、脑电图、心率监测器等），但当然，您可以在实验室中对固定的孤立受试者进行测量，而您可以将其带入“现实”环境（如家庭），这之间存在权衡（人们可以四处走动、彼此互动等）</li><li>并且您希望努力开发一系列（最初是非正式的）要尝试的东西，要运行的“实验”，主要目标是常识的有效性和帮助性。您不需要问卷或评级量表来判断咖啡因会让您感觉更警觉，但您可能想要实际测量反应时间，而不是相信自己“更快”的主观印象。</li></ul><p>这里的主要目标不是用非正式的试错法代替“真正的科学”，而是用它来<i>产生值得更严格测试的假设......</i></p><p>当然，非正式的自我实验是有争议的，并且可能不适合标准的研究/资助模型，但对于这种开放式的搜索来说，它似乎确实是必要的。</p></blockquote><p>我们可能是最有能力在这方面发挥领导作用的人之一。我怀疑康斯坦丁名单上的任何一家初创公司都接近<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">调整你的认知策略</a>的水平，而雷蒙的试验小组已经在<a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club">推动该领域的边缘</a>，而整个范式基本上是零技术，而且一直都是。</p><p>这让我想起了<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">尤德科夫斯基的致死清单的</a>结束语：</p><blockquote><p><strong>当你环顾四周时，你所看到的这种情况并不是一个幸存世界的样子。</strong>幸存下来的人类世界已经制定了计划……在此之前就开始尝试解决他们重要的致命问题。一半研究弦理论的人转向人工智能对齐，并在那里取得了真正的进展......</p><p>无论如何，许多美好的世界都会消亡。第一次尝试就解决这样的问题确实是一个困难的问题。 </p></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sat, 02 Dec 2023 02:45:58 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sat, 02 Dec 2023 02:45:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>那么，人工智能的安全性在多大程度上与促智药不成比例地对立呢？</p><p>我的印象是，有些人在想“这不起作用，是时候让人们放手了”，而另一些人则想“顺从社会现实是一个不好的启发式，而药物显然可以做到这样的事情，因此我将继续下去”。</p><p>但我没有问过华盛顿特区的任何人，也没有问过我住在伯克利的时候。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sun, 03 Dec 2023 01:01:03 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sun, 03 Dec 2023 01:01:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>我的印象（再次强调，不是医疗建议）是，当某人存在使其有用的缺陷/状况时，促智药大多“起作用”。就像素食主义者需要补充维生素 B12 一样，有些人的身体可能需要促智药。 （肌酸显然对素食主义者更有效，从这个意义上讲？）</p><blockquote><p>相反，他们应该像<a href="https://sarahconstantin.substack.com/p/testing-human-augmentation#:~:text=Developing%20A%20Community%20of%20Practice">康斯坦丁所设想的那样，自愿为人类安全试验贡献自己的大脑，开发一个聪明的、神经多样化的、定量技能的、创造性的和富有洞察力的实践社区，</a>以便我们在尝试之前能够充分了解该技术的潜力像 Yudkowsky、Christiano、Sutskever 或 Wentworth 这样的人。这就是铁杆和真正拯救世界的样子，特别是对于那些对自己目前的影响水平不满意的人来说。</p></blockquote><p>我确实考虑过自愿参与这项工作，并且可能会在任何智力增强试验中这样做。<strong>但我可能是这里的一个异类</strong>。</p><ul><li>早期试验可能针对患有阿尔茨海默氏症等疾病的人，而不是针对健康人的增强试验。</li><li>仅鼓励“公正”的肾脏捐赠就产生了好坏参半的结果。除了“嘿，这个选项存在”或“我做了这个，事情是这样的”之外，给人们施加很大的社会压力来试验他们的大脑，这将是非常可怕的。</li></ul><p>但康斯坦丁的帖子无疑让我们很好地了解了在神经调节变得更安全和更好理解<i>之后</i>，此类实验可能会是什么样子。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sun, 03 Dec 2023 21:56:38 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sun, 03 Dec 2023 21:56:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>好吧，无论怎样，如果人类智力放大在硅谷风险投资家中起飞（理想情况下为他们和军方提供人工智能以外的出路），那么促智问题有望自行解决。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sun, 03 Dec 2023 22:03:11 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sun, 03 Dec 2023 22:03:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>是的。和/或被神经技术淘汰。</p></div></section><h1>大型项目</h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sun, 03 Dec 2023 22:14:50 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sun, 03 Dec 2023 22:14:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>不久前，您写了<a href="https://www.lesswrong.com/posts/5xrkjHCvCeeDtHa5g/alignment-megaprojects-you-re-not-even-trying-to-have-ideas">《对齐大型项目：您甚至没有尝试有想法》</a> 。您现在最兴奋的大型项目想法是什么，尤其是在当今变革的世界中？我们正在考虑<a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines">人工智能时间表</a>、<a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism">电子人主义</a>以及法学硕士的其他应用程序和自动化、我们讨论过的一些智能放大内容的成功，以及我们在 EA 和人工智能实验室及其周围看到的社区健康和权力游戏。过去3年。甚至像新冠肺炎这样的全球性剧变。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sun, 03 Dec 2023 22:27:24 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sun, 03 Dec 2023 22:27:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>我想出了几个：</p><ul><li> “ <a href="https://www.lesswrong.com/posts/5xrkjHCvCeeDtHa5g/alignment-megaprojects-you-re-not-even-trying-to-have-ideas?commentId=QqkFGWZXuJYKCyL9B">形式化办公室</a>”：分包给数学研究生（来自现有对齐领域的内部和外部）。形式化、深入、形式化地证明/反驳并检查对齐研究人员的工作。可能会在相关帖子的评论中链接他们的发现。基本上是“流动的外部同行评审，对你没有实际的权力”。</li><li> “ <a href="https://www.lesswrong.com/posts/cP7oiw7psZBHTFjG7/dreams-of-mathopedia">Mathopedia</a> ”：数学的多模式教育学维基，面向形式/代理基础类型的对齐研究。最近我一直在想如何把它打造成“数学战术宝库”，比如“这个<a href="https://www.matsprogram.org/agent">MATS数学题</a>提到了XYZ，所以我们应该使用S领域的工具，特别是T和U。”</li></ul><p>请注意，就“需要大量资金”而言，这两个都不是“大型”，尽管它们似乎都相对容易用更多的钱来<i>扩大规模</i>：只需支付更多的人来做更多的事情。形式化办公室可以通过签约更多的数学毕业生来更快地检查更多领域的工作，Mathopedia 可以覆盖更多的数学领域并创建更多类型的资源。</p><p>当然，更集中的“大型”大型项目已经被提出：模拟顶级对齐研究人员的大脑，开发我们上面讨论的增强技术，以及我忘记的更多技术。其中一些是比其他更大的“灌篮”，但其中任何一个（如果成功）都可以推动 P（厄运）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sun, 03 Dec 2023 22:34:37 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sun, 03 Dec 2023 22:34:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>各种里程碑可能会比我们想象的更接近，从而让我们感到惊讶。就像人工智能能力进步的不可预测性一样，但每一个金块都是好消息而不是坏消息。</p><p>我喜欢Mathopedia——我认为我们可以更好地培养人们的技能，这样我们培养定量思考者的能力更多地基于他们聪明/创造性地部署数学的固有能力，而不是他们固有的节奏和学习数学的舒适度首次。<a href="https://twitter.com/Morphenius/status/1640811637031723008">在我们的文明中，学习数学的过程通常是相当不必要的灵魂杀手</a>，我认为人们没有意识到如果我们解决这个问题并朝着历史上前所未有的方向前进会有多大。</p><p> “通过垃圾邮件发送示例和解释来简化数学学习”的官方术语称为“直觉泛滥”，如此<a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding">处所</a>引用。您可以通过有用的示例将概念强行灌输给自己或其他人，如果您看到足够多的示例，那么潜在的模式要么会从您身上跳出来，要么会被无言/隐式/潜意识地考虑在内。更多示例意味着更多模式可以组合在一起，并且可以降低错过对理解整个事物至关重要的关键模式之一的风险。</p><p>即使主流教育不久前就开始利用它，它也有很大的潜力可以将学习率提高一个数量级。特别是如果像<a href="https://www.lesswrong.com/users/valentine?mention=user">@Valentine</a>这样的人因为世界需要他们而被要求从寒冷的睡眠中醒来。</p><p>我认为人工智能安全可以极大地提高大多数成员的量化技能。有一天，当人类的后代从一个星球散布到另一个星球时，他们不会告诉孩子们远古地球的历史，直到他们长大到可以承受的程度，当他们知道的时候，他们会因为听到这样的事情而哭泣。在我们的技术水平上，“数学创伤”仍然存在。</p><p>我认为我们有动力、有能力、有现有的人才库，而且我们有能力利用这些人才。我们可以将最后一批聪明的思想家转变为聪明的量化思想家，并从他们身上获得足够的价值来证明这一过程的合理性。扩大贝叶斯阴谋，也许甚至存在我们不知道的临界质量，因为我们还没有接近。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sun, 03 Dec 2023 22:50:41 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sun, 03 Dec 2023 22:50:41 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>（精确的措辞吹毛求疵：逻辑和数学本身<a href="https://twitter.com/spacepanty/status/1681724936472391680">似乎仍然涉及很多从远处看起来像是“wordcel”的东西</a>。）</p><p>但总的来说，我同意：我们的社区不仅有很多定量思想家（你可以在其他“书呆子”社区中找到），而且它本身也带来了不同寻常的定量方法，从贝叶斯法则本身到预测市场。</p><p>现在的诀窍是从“普通水平的定量思维”延伸到“高级数学思维”。</p><p>我将其定义为从“斯科特-亚历山大级别”的数学熟悉度（统计、概率、线性代数的鸟瞰知识、 <a href="https://www.youtube.com/c/3blue1brown">3Blue1Brown</a>制作视频的内容）到“数学博士级别”的熟悉度（真实分析、拓扑学、范畴论、研究层面的大多数数学领域，<i>不仅仅是</i>了解这些是什么，而且<i>能够在前沿流畅地使用它们</i>）。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Sun, 03 Dec 2023 22:59:49 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Sun, 03 Dec 2023 22:59:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><p></p><div><p>绝对地。目前预测市场面临的一个巨大问题是糟糕的小麦与谷壳比率。我认为预测市场需要足够数量的人才才能变得令人难以置信，而我们可能还远远没有达到这个临界数量。</p><p>如果我们还没有达到其他临界点，我迫不及待地想看看我们还能得到什么；也许还有与预测市场本身处于同一水平的其他创新，但我们只是没有足够的人才来预见它们的到来。</p><p>试想一下，如果 10 年前，我们 EA 和人工智能安全社区中达到或接近“数学博士级别”的人数是现在的五倍，那么我们现在会是什么样子。我们可能一直在对人类上传进行扎实的研究。</p></div></section><h1><strong>达斯伊兰</strong></h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Wed, 13 Dec 2023 22:49:27 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Wed, 13 Dec 2023 22:49:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>“将大部分人工智能安全人员转移到湾区”的巨型工程怎么样？让每个人都集中在一个地方似乎是个好主意，类似于 dath ilan。更多面对面的对话、不同类型的思考者在同一个白板上工作等等。</p><p>湾区人们一直在建立集体住宅，购买玫瑰花园酒店和会议场地。存在明显的问题，例如各种社会动态，但基础（土地）已经奠定。</p><p>我的想法是，如果我们有更多的人意识到“哇，数十亿人将会死去，我的家人和朋友将会死去”，他们会更适应斯巴达式的生活方式，除非采用银河大脑的方式来最大化在一个地方工作的人数（例如，这些<a href="https://www.google.com/search?sca_esv=587529080&amp;rlz=1CAYGYA_enUS872US872&amp;q=hotel+pods&amp;tbm=isch&amp;source=lnms&amp;sa=X&amp;ved=2ahUKEwj1j73ArPSCAxVBEVkFHWz2D8gQ0pQJegQIDhAB&amp;biw=1181&amp;bih=664&amp;dpr=1.63">胶囊旅馆</a>给人一种通过足够的创造性思维可以发现什么的感觉）。这降低了人们访问一个月的成本，这为为特定项目寻找人才等事情提供了巨大的价值： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/dmv05gmfi1vtobcdyx7o" alt="东京胶囊旅馆：如何体验？东京廉价酒店"><figcaption>便携的？</figcaption></figure><p>人工智能的全球形势确实非常严峻，足以证明像这样的开箱即用的解决方案是合理的；根据<a href="https://twitter.com/ESYudkowsky/status/1700242932962586883">Yudkowsky 的这条笑话推文</a>：</p><blockquote><p>我的父母朋友们似乎对我最新的育儿理论印象不深，即你应该把你的孩子放在一个巨大的土坑里，经常受到机器人迅猛龙的攻击……这样一来，普通的地球生活水平似乎就会下降。他们喜欢至高无上的奢侈品，因为他们的大脑基线期望值被设定得很低</p></blockquote><p>也许不是每个人都能接受 EA 苦行僧/极端分子的生活方式，但像这样度过一年（在评估你的才能的同时）并不算太严酷，这是增加每平方英尺人才的好方法，而且是一个好方法过滤那些认真拯救世界的人。无论如何，这些都是对我有用的事情；有很多选择可以减轻旧金山的布局和价格造成的危害，包括在城中建设一个空间优化的城市。</p><p>让人们搬到更好的空间的更好选择似乎也是解决<a href="https://www.lesswrong.com/posts/AnhwbEisrPXEw9d5g/originality-vs-correctness#:~:text=The%20world%20is,with%20their%20lives.">Habryka 大约一周前提出的一个非常有趣的人才采购问题</a>的重要组成部分：</p><blockquote><p>这个世界是敌对的，因为如果你聪明有能力，就会有大量的人和机构优化让你做对他们有利的事情，而忽略你的个人利益。</p><p>大多数聪明人“得到了”，最终将他们的生活定位在一些他们甚至不太关心的随机事物上，因为他们的 OODA 循环被某些社会环境所捕获，这使得他们很难理解什么是继续或了解更多关于他们生活中真正想做的事情。</p></blockquote><p>我认为，锚定在一个特定的空间，例如密歇根或迈阿密的房屋和社区，正是这种会抑制人们思考全球形势或决定他们想要做什么的能力，就像一个高的地方一样。 - Facebook 或华尔街甚至 OpenAI 的高薪且有趣的工作。昏昏欲睡、无聊的生活是一种吸引状态。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sun, 03 Dec 2023 23:36:20 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sun, 03 Dec 2023 23:36:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>这引发了我想到的一个切线，但我认为这对于人工智能安全来说是一个重要的点：</p><p>处于理性/EA/AI 阵营的人中有很大一部分<a href="https://www.astralcodexten.com/p/acx-survey-results-2022">存在精神和情感问题</a>。</p><p>我们<i>实际上</i>不应该为了拯救世界而强迫自闭症和多动症患者共用浴室。相反，我们应该用顶级量化交易员、顶级​​谷歌程序员和顶级篮球运动员的方法来培养和对待研究人员。</p><p>我们希望更多的人参与到联盟研究中，我们<i>不</i>应该<i>想</i>添加任意的障碍，即使（作为一个完全虚构的立场，希望没有人这样做）我们用“哦，我们需要昂贵的信号来避免<a href="https://www.lesswrong.com/tag/ftx-crisis">骗子</a>”来证明它的合理性。</p><p>对于牛顿来说，学习并重新推导他那个时代的许多高等数学是有好处的。对于爱因斯坦来说，足够“代理”，能够战略性地思考哪些研究方向更有前途是有好处的。</p><p>牛顿必须自己挤墨水、砍自己的树来制作笔记本，这对牛顿来说是<i>很糟糕的</i>。要求爱因斯坦担任望远镜/天文台的项目经理，其观测结果用于检验相对论，这是<i>很糟糕的</i>。</p><p>该领域不会<i>跳过对</i>“怪人”的资助。你<i>实际上</i>无法从“<a href="https://www.lesswrong.com/tag/dath-ilan">尤德科斯基</a> <a href="https://www.lesswrong.com/posts/2jfiMgKkh7qw9z8Do/being-a-robust-agent#icwwigsL3iczYx6D8">式的</a>人<a href="https://www.lesswrong.com/posts/Qw6qsgR2Qm6rp574A/so-you-want-to-save-the-world-an-account-in-paladinhood">关心</a>‘代理’”<a href="https://www.lesswrong.com/posts/2EhC5wgmK2Am6xLL3/ways-to-be-more-agenty">跳到</a>“联盟研究人员需要大量的执行功能，否则我们不会资助他们”。</p><p>就像我们（正确地）注意到“嘿，我们的社区在执行功能类型的事情上表现不佳，因为<a href="https://www.lesswrong.com/posts/bRGbdG58cJ8RGjS5G/no-really-why-aren-t-rationalists-winning?commentId=8FLGydotT2sXr4zwt">我们存在个人层面的执行功能问题</a>”，然后（错误地）推断出“针移动科学家将成为高级执行官—— ”，这又引诱我们去思考“<i>为什么</i>我们必须<a href="https://www.lesswrong.com/posts/HPBdzWYp95zH3mQCa/i-applied-for-a-miri-job-in-2020-here-s-what-happened-next">招募</a>人员？或者<a href="https://www.lesswrong.com/posts/yQJxi5mMZQopYXZk5/a-quick-list-of-some-problems-in-ai-alignment-as-a-field#4__How_do_people_get_good_at_this_shit_">培训他们</a>？如果他们聪明/高——执行功能足够，<i><u>他们会在这里找到自己的出路</u></i>”。</p><p> （如果你的时间线实际上<i>太短</i>而无法实现，这是一回事，但如果你有 2000 万美元可花，<i>你应该通过将预算的一部分用于磨刀活动来对冲时间线</i>。）</p><p>最理想的情况是，社区的“故事”应该是“Yudkowsky 在 2000 年代初花费了大量的代理/执行职能来为 MIRI 创建和筹款，然后 MIRI 培训并资助了一个良好工作的生态系统”。相反，故事是“第一步发生了，然后 MIRI 多年来几乎完全缓慢地进行内部工作，然后 OpenAI 等出现，现在大部分调整资金都用于大型实验室公司已经想做的事情”。我们可以谈论事后诸葛亮（就像任何人过去做出的任何决定一样）和关于扩展的信念，但这就是我上面的“对冲”观点。</p><p> （我对人们如何对其工作原理、科学突破感到困惑有更多的想法，但我还没有完成阅读和处理<a href="https://www.lesswrong.com/posts/AnhwbEisrPXEw9d5g/originality-vs-correctness">原创性与正确性</a>，这是（到目前为止）人们相互竞争的困惑观点的有用总结。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Mon, 04 Dec 2023 00:37:57 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Mon, 04 Dec 2023 00:37:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>选择仍然很好，我建议的建议对于人们来说非常有价值，但牢记<a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Moloch</a>也很重要；我们不希望平衡向斯巴达生活方式转变得足够远，以至于人们最终被迫走这条路（例如，因为它更便宜，因此规模更大），特别是如果斯巴达生活方式对我们最好的思想家有害或令人厌恶的话。</p><p>我仍然认为，根据经验，我们认为我们<a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#Introduction">距离迅猛龙可能不到 20-30 年</a>是有道理的。人们应该通过多个镜头来看待自己和自己在世界上的位置，而“厄运”镜头应该是其中之一，基于计算出的真实概率，而不是它在其他人看来有多奇怪。我认为创建一小部分<a href="https://www.lesswrong.com/tag/dath-ilan">dath ilan</a>是对此的明智回应。</p><p>但在极端情况下， <a href="https://www.lesswrong.com/posts/HCAyiuZe9wz8tG6EF/my-tentative-best-guess-on-how-eas-and-rationalists">损害人们的心理健康也会使他们对自己和周围的人</a>以及<a href="https://www.lesswrong.com/posts/zidQmfFhMgwFzcHhs/enemies-vs-malefactors">整个人工智能安全</a>构成风险。还有记者在起作用，总是在寻找材料，但我认为人们会找到聪明的方法来提高这些苦行者的审美。</p><p>我们可能不同意应该是 ~4% 还是 ~40% 的人这样做；我想我们稍后可以单独进行一次对话来讨论斯巴达主义。你让我相信它不应该高于 ~40%。</p><blockquote><p> “为什么我们<i>必须</i><a href="https://www.lesswrong.com/posts/HPBdzWYp95zH3mQCa/i-applied-for-a-miri-job-in-2020-here-s-what-happened-next">招募</a>人员？或者<a href="https://www.lesswrong.com/posts/yQJxi5mMZQopYXZk5/a-quick-list-of-some-problems-in-ai-alignment-as-a-field#4__How_do_people_get_good_at_this_shit_">培训他们</a>？如果他们足够聪明/具有高级管理职能，<i><u>他们就会在这里找到自己的出路</u></i>”。</p></blockquote><p>我完全同意。这是不好的做法，我们本来可以招募更多的人，更好的人（无论价值的定义如何，例如合作能力）。我们本来可以走得更快，现在我们可以有更多的工作要做。</p><p>在我们进行的过程中弄清楚事情对于这个过程来说至关重要；我的想法是，加速人工智能安全社区的智能、协调和规模也将使人们准备好找到银河大脑的方法来解决问题。这并不容易，拆除切斯特顿-谢林围栏会产生不可预测的结果，而且作为灵长类动物，人类倾向于在问题出现时隐藏问题。但像<a href="https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS">兹维的不道德迷宫</a>和<a href="https://meltingasphalt.com/social-status-down-the-rabbit-hole/">西姆勒的社会地位</a>这样的伟大帖子将使人们更容易预测和减轻成长的烦恼。</p><p>在执行功能方面，我也认为这是人工智能安全的一个巨大改进领域。人类通常都是漫无目的地闲逛，通过增加每个人的日常生产（通过乘数效应，因为人们为彼此的工作做出贡献），我们可以从人工智能安全社区中获得更多收益。</p><p>有一些显而易见的东西；每个人都阅读CFAR手册，并认真尝试里面的技术（即使其中50％不起作用）。还有其他针对人类大脑的指导手册，例如<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">调整认知策略</a>和雷蒙的<a href="https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning">反馈循环第一理性</a>。阅读序列是因为在重要的事情上犯错绝对是一种严重的失败模式，而人类非常容易在重要的事情上犯错误并持续十年之久。</p><p>但我认为，很大一部分原因是人们犹豫不决，没有花时间思考。</p><p><strong>人们不应该有“淋浴思想”。</strong>或者更确切地说，它们不应该引人注目或明显。在 20 世纪 90 年代和 2000 年代，人们在读书、查看熵电子邮件列表、有时甚至看电视时会走神并产生“淋浴想法”。社交媒体和其他商业娱乐都会抑制独立思考，并渗透到你每一秒的空闲时间；成瘾经过优化是为了让人们返回平台，而不是停留在平台上，这意味着算法将找到奇怪的方法（例如<a href="https://Everyone knows that a proper Skinner Box needs to avoid giving away too many rewards if you want to keep people pressing the buttons and viewing the advertisements.">斯金纳</a><a href="https://www.lesswrong.com/posts/FWiBXSnpusJgCZNWq/against-facebook#:~:text=Without%20exception%2C%20everyone,to%20get%20you.">框</a>）来优化</p><p>我认为这实际上是一个很大的问题，尤其是对于我们最好的思想家来说。人们需要每天花 2 个多小时不受干扰，才能看看硬核冥想和 CFAR 手册等的结果是否真的来自认知增强。有可能一种时髦的冥想技巧只是阻止了媒体的使用，迫使你做出正确的决定，独自思考，就像长时间淋浴或开车一样。</p><p>我也认为这可能是CFAR经常得不到他们所希望的结果的一个重要原因； <a href="https://www.lesswrong.com/s/KAv8z6oJCTxjR8vdR/p/gKeHcikcXA3bApyoM#:~:text=And%20it%20is,in%20that%20drawer.">人们需要改变以前的次优状态，才能更好地思考</a>，而<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=Among%20a%20wide,of%20the%20genepool.">当前的媒体范式强烈鼓励</a>将<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=social%20media%20uses,here%2C%20yet%20again.">人工智能与大量行为数据相结合</a>，以优化稳定人们的日常生活，如果没有其他可以<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=it%20can%E2%80%99t%20be%20done%20unless%20you%20have%20human%20behavior%20data%20from%20millions%20of%20different%20people%20all%20using%20the%20same%20controlled%20environment">控制的话变量以提高数据质量</a>。</p><p>就像，如果一个火星人来到地球，看到有人在手机上滚动并说：</p><p><strong>火星人</strong>：“你怎么<i>没</i>注意到危险！？很明显，滚动新闻是入侵人类大脑的最佳环境！”<br><strong>人类</strong>：“我不知道，我的大脑有点关闭，当我这样做时我失去了自我意识”<br><strong>火星人</strong>：“这真是令人难以置信。难道仅凭这一点就足以让你知道有什么事情出了严重的问题吗？”<br><strong>人类</strong>：“哦，来吧。我所有的朋友都在这样做。每个人都在这样做！我需要这个才能知道什么是流行的！只有[地位低的人]才会抱怨它。”</p><p>这时，火星人翻转桌子并离开。</p><p>我们许多最优秀的人才会将大部分思考时间花在浏览社交媒体或其他高度优化的注意力吸引者上，因为这些都是吸引子状态。如果即时满足是真实的​​，那么这也将极大地损害他们的思考能力。不管怎样，他们最好的想法大多是在淋浴或车里。</p><p>社交媒体排毒很困难，因为算法可能会或可能不会达到类似斯金纳箱的策略来优化，以在您不使用社交媒体时最大化空虚感。这是一个敌对的环境，所以我认为试图让自己戒掉是在发起一场你不会赢的战斗，因为系统不仅以你不知道的方式理解你的想法，而且它以不同的方式理解许多人的想法。他们没有的方式，它经历过战胜他们的经历，它知道如何找到你与其他案例的相似和不同之处。</p><p>但不管怎样，如果你在洗澡或开车时不再有“洗澡的想法”，那就说明你已经做得足够多了。这似乎是提升人工智能安全社区的最大方式之一。</p><p>当我们寻找 18-22 岁的新人才时，我们是唯一有足够价值的目标来真正说服他们花时间思考而不是花在娱乐媒体上的人。除了军队之外，没有其他人可以做到这一点，招募 18-22 岁的年轻人。</p><p>这是生活方式的巨大改变，但当你参与人工智能安全等既连贯又实际重要的事情时，变得更聪明显然是值得的。坦白说，与迅猛龙相比，这根本不算什么。</p><p>这就是我们目前关于大型项目的全部信息吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Mon, 04 Dec 2023 01:17:43 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Mon, 04 Dec 2023 01:17:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>当然，让我们进入下一个话题。</p></div></section><h1>适应性</h1><p>“在变革的时代，学习者继承了地球，而学习者发现自己已经做好了应对一个不再存在的世界的准备”</p><p> ——埃里克·霍弗</p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Mon, 04 Dec 2023 01:40:06 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Mon, 04 Dec 2023 01:40:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>好吧。很多人都在谈论利用慢速起飞来帮助解决对准问题。仔细观察<a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for">社交媒体的现状以及人工智能在信息战中的应用，</a>确实​​让我对缓慢起飞期间可能发生的事情有了一种新的认识，除了更多地关注人工智能治理角度和人工智能改变整个社会之外。</p><p>过去四年里，新冠疫情和国际关系中新的冷战范式无处不在。 ChatGPT 尚未发生太大变化，但我不认为这种情况会持续下去，尤其是对于开源法学硕士。而且，最糟糕的是，人工智能实验室内的人工智能加速和混乱可能会突然烧毁剩余时间线的一部分，其数量难以确定。</p><p>我的想法是，我们需要人们更好地适应新环境的出现，应对由于地球上不断增强的优化能力而出现的可怕事情，既灵活又聪明，能够找到银河大脑的解决方案来解决原本无法克服的问题。</p><p>例如，如果人工智能应用程序允许像微软或<a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for">政府</a>这样的大型科技公司将<a href="https://www.lesswrong.com/posts/F7sp7rQg3zfD4totA/helpful-examples-to-get-a-sense-of-modern-automated">说服技术大幅扩展到社交媒体之外</a>，那么如果拳头利用超优化的心理来攻击我们存在的核心，我们该如何应对呢？</p><p> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#How_to_protect_yourself_and_others__:~:text=I%E2%80%99m%20not%20sure,for%20self%2Dimprovement.">降低认知和情感可预测性可以增强人工智能安全社区抵御特定威胁的能力</a>，但每天更好地推出数十个银河大脑解决方案将更好地降低可预测性，并以各种方式提高能力和弹性。就像成长的烦恼一样，除了外部威胁而不是内部威胁。</p><p>这将提高人工智能安全社区的适应能力和发展能力，并驾驭缓慢起飞过程中出现的所有其他可怕事物的浪潮，而不仅仅是<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks">适用于任何人的新人类操纵范式</a>（这只是一个例子）。</p><p> Katja Grace 将加速人工智能描述为“ <a href="https://forum.effectivealtruism.org/posts/2xrTTgvosGSsM85RZ/katja-grace-on-slowing-down-ai-ai-expert-surveys-and#:~:text=there%27s%20going%20to%20be%20just%20a%20fire%20hose%20of%20new%20cognitive%20labor%20and%20it%27s%20sort%20of%20unclear%20where%20it%20will%20go">新认知劳动的消防水带，但目前尚不清楚它将流向何处</a>”。我认为这是一个非常好的表达方式。各种各样的东西最终都可以从这里抽出来。</p><p>我认为还会有 0-2 个“黑球”，就像人类操纵技术一样，它们也具有毁灭性，但方式完全不同，但尽管如此，我们必须生存和繁荣。</p><p>我认为当前一代的人类操纵技术是一个很好的例子，它说明了曲线球将会有多远，让我们极度迷失方向，以及<a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for">传统力量如何经常围绕它们旋转</a>。但也许人类操纵技术是一种异常严重/极端的情况，我们可能不会再从缓慢起飞中得到 0-2 个像操纵技术一样极其可怕的“黑球”。</p><p>但这不是我打赌的地方，尤其<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#Functioning_lie_detectors_as_a_turning_point_in_human_history">是测谎技术</a>，<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3680134/">它的负面声誉只是因为它在一个没有机器学习的文明中已经存在了大约 100 年</a>（它们被称为“测谎仪”测试，因为它们实际上会生成多个图表） ，并且一个人必须同时注视它们！）</p><p>提升人工智能安全社区，以足够的速度推出银河大脑解决方案以减轻损害，甚至利用机会或在威胁发生之前预测威胁，似乎是解决这些挑战的最佳方法。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Mon, 04 Dec 2023 01:45:36 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Mon, 04 Dec 2023 01:45:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><blockquote><p>每天能够更好地提出数十个银河大脑解决方案</p></blockquote><p>这听起来有点疯狂，你这是什么意思？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Mon, 04 Dec 2023 01:50:00 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Mon, 04 Dec 2023 01:50:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>（我大致同意这一点的其余部分，并额外指出，“加速协调和治理”本身<i>可以</i>帮助“减缓”人工智能能力，因为我们的社区越来越善于指出并反对不安全的人工智能开发。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Mon, 04 Dec 2023 21:33:42 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Mon, 04 Dec 2023 21:33:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>是的，我所说的“每天推出数十个银河大脑解决方案”只是指人们更善于思考真正有效的解决方案。其中一些解决方案会很复杂，一些会很优雅，一些会非常有效，另一些则能让我们应付看似不可能的情况并减轻巨大的损害。</p><p>聪明的人会让我们做得更好。我认为安娜·萨拉蒙（Anna Salamon）的 <a href="https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic">《人类并非自动具有战略性》</a>很好地描述了这一点。超级智能会做什么？在我们当今的世界中，其中很大一部分是将你的大脑进行抽象推理的能力转化为实际起床并采取最好的行动，每小时和每天（包括刻意休息，防止倦怠，以及其他引起积极强化和影响的事情）。防止负强化）。</p><p>用<a href="https://www.astralcodexten.com/p/contra-deboer-on-movement-shell-games#:~:text=ACTUALLY%20DO%20THESE%20THINGS!%20DON%27T%20JUST%20WRITE%20ESSAYS%20SAYING%20THEY%27RE%20%22OBVIOUS%22%20BUT%20THEN%20NOT%20DO%20THEM!">Scott Alexander 最近的话</a>来说，当今有效利他主义和人工智能安全的一个大问题是：</p><blockquote><p>实际上做这些事情！不要只是写文章说它们是“显而易见的”，但然后就不去做！</p></blockquote><p>在变革的世界中，最好的做法可能与代理和动机黑客不同，也许更多的是在极端情况下第一次就把事情做好，就像那个场景中的主角有远见和敏捷的头脑来抓住无人机一样第一次机会，尽管就在不久前他身上发生了一些极其可怕的事情。但无论如何，我们都需要保持灵活性和智慧。</p><p>这可能看起来激进或极端。但如果我们在十年或三年内得到迅猛龙，我们就会回顾自己的生活，并意识到我们所做的唯一疯狂的事情就是做得不够。事后看来，很明显，我们更加努力并犯错误的时候比我们几乎不尝试的时候要好得多。</p></div></section><h1>从序列中提取更多价值</h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Wed, 13 Dec 2023 22:49:27 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Wed, 13 Dec 2023 22:49:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>您对提高<a href="https://www.lesswrong.com/highlights">Yudkowsky 序列</a>、 <a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction">CFAR 手册</a>、 <a href="https://www.lesswrong.com/codex">Scott Alexander&#39;s Codex</a> 、 <a href="https://www.projectlawful.com/board_sections/703">Projectlawful</a> / <a href="https://hpmor.com/">HPMOR</a> 、<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">调整认知策略</a>/<a href="https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality">反馈循环第一理性</a>等的读者率有何想法？阻碍人们阅读并从中成长的瓶颈是什么？我们能对这个瓶颈做些什么？</p><p>它们提供了许多独特的方法来让你的大脑养成更好的习惯，这样你就可以走出去，从生活中得到你想要的东西，而不是感到困惑和不断背叛自己；就像让一群人进入更好的纳什均衡，这样他们就不会不断地互相背叛。</p><p>智能放大技术很棒，但超级智能人工智能可以编写一本手册，使用真实的知识向你展示如何充分利用你的思维，可能比 CFAR 手册有效几个数量级。</p><p>像 CFAR 手册这样的东西不仅是我们已经拥有的东西，而且是整个人类、整个人类历史都懒得去尝试的东西。我们不知道 CFAR 手册与超级智能编写的真正的人脑指导手册（让我们以最佳方式思考以最大化结果）之间的差距有多大。</p><p>这个差距可能很小，也许我们需要十个聪明的人来解决这个问题，而不是五个，也许五个就足够了，但他们没有 SquirrelInHell 的<a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">调整你的认知策略</a>来工作，现在他们有了。也许这五个人都<a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club?commentId=k6hPEP6afApjg8nKc">如此频繁地使用社交媒体和其他娱乐媒体，以至于他们失去了衡量独立思考有效性的能力</a>。人工智能安全社区的一切都可以归结为不足和成长的烦恼。</p><p>我认为，智能增强技术的价值很大一部分来自于让人们更好地应用序列、CFAR 手册和其他手册中已经描述的建议。</p><p>我认为其中很大一部分可能是缺乏积极的强化，这基本上是工作，例如尤德科斯基转发的这件事：</p><p> <i><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/usuh9tyhwgxt5mou6pri"></i> </p><p><br></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Tue, 05 Dec 2023 01:06:30 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Tue, 05 Dec 2023 01:06:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>哦哦，我对此有很多想法！ （主要是因为业余爱好者对制作 YouTube 视频和其他艺术感兴趣，这导致了<a href="https://twitter.com/NicholasKross/status/1720578513743261768">人们长期以来对“媒体如何流行？”的关注</a>）</p><p>这里的一个关键心理模型是“注意力漏斗”，很像广告商使用的<a href="https://en.wikipedia.org/wiki/Purchase_funnel#/media/File:The_Purchase_Funnel.jpg">购买“漏斗”</a> 。让我们用 YouTube 视频来回顾一下这个渠道：</p><ol><li>您会看到缩略图和标题。</li><li>如果有兴趣，请单击视频并开始观看。</li><li>如果视频无聊或刺耳或类似的情况，您可以停止观看视频（例如通过关闭浏览器选项卡）。不然你就继续看下去吧。</li><li>如果视频<i>变得</i>无聊/刺耳/等等，您就更有可能停止观看该视频。</li></ol><p>类似的情况也发生在同人小说中：</p><ol><li>您看到一个标题（或从朋友那里听到）。</li><li>如果有兴趣，您可以搜索/单击该小说，也许还可以阅读摘要。</li><li>如果它很有趣（引人注目、吸引你的注意力等），你就会继续阅读。否则你就停下来。也许你稍后回来，但我们关注的是“你的注意力驱动的决定”。</li></ol><p> HPMOR 和其他理性主义作品（对于某种个性而言）非常引人注目！它们通常具有戏剧性的节奏、简短的章节/部分以及主观上属于“好写作”的内容（即使在不同句子结构的微观层面或措辞的精确性）。因为它们是故事（和/或有力的论战），所以它们通常包含人类喜欢的情感负载。</p><p>回到中心点：我认为现在的瓶颈是漏斗的<i>顶部</i>。每个人都听说过 HP，但“只有”HP 同人书呆子（和/或现有的理性主义者）听说过 HPMOR。序列和相关作品的漏斗顶甚至<i>更窄</i>。</p><p>有多种方法可以<a href="https://www.lesswrong.com/posts/Bfq6ncLfYdtCb6sat/i-converted-book-i-of-the-sequences-into-a-zoomer-readable">或多或少地有效/直接地</a>完成此任务，但<a href="https://www.youtube.com/@RationalAnimations/videos">RationalAnimations</a>是“<i>可以</i>以令人信服的方式呈现基本的 AI 对齐思想——足以影响很多人”的一个很好的例子。</p><p>如果你是社区中的“创意型”......好吧，无论艺术领域<i>可以</i>有<a href="https://www.lesswrong.com/tag/hamming-questions">汉明问题</a>到什么程度，你可能会喜欢探索“可以制作什么样的理性主义/人工智能对齐/EA媒体，这将达到很多人？”</p><p>另一个瓶颈（在较短的时间内，可能是<i>更重要的</i>瓶颈）似乎是“注意力漏斗的底部，但人工智能对齐漏斗的顶部”。就像，很多人已经甘心成为“文字和统计人员”，并认为他们无法加入“机器学习和/或数学人员”。<br><br>如果我能神奇地让 20% 的 EA 参与全职 AI 调整，我会的！ （好吧，关于他们所做的<a href="https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous">研究类型的</a>模数警告。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Tue, 05 Dec 2023 01:47:31 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Tue, 05 Dec 2023 01:47:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>如果我能让人工智能领域的每个人每周花 2 个小时思考人工智能治理的解决方案，那就太好了。</p><p>针对 OpenAI 冲突，Yudkowsky<a href="https://twitter.com/ESYudkowsky/status/1726280375281103326">写道</a>：</p><blockquote><p> Twitterati 大大高估了我对个别人工智能公司的关注程度。我目前认为他们的集体正在不可避免地走下坡路，他们的领导人即使想改变也无法改变任何事情。这是国际条约或半身像。</p></blockquote><p>对我来说，这是一个相当大的更新，有利于利用联盟研究人员的人才基础来解决看似不可能的全球问题，比如中美在人工智能问题上的协调，我多年来一直在研究这个问题，但不知道人工智能治理会相对于所有其他工作领域变得如此重要。</p><blockquote><p>就像，很多人已经甘心成为“文字和统计人员”，并认为他们无法加入“机器学习和/或数学人员”。</p></blockquote><p>是的，我完全同意数学问题。我认为这是双向的；我在华盛顿特区工作多年的印象是，这里的人根本不具备将贝叶斯思维应用于人工智能治理的定量技能。但让人工智能治理人员让湾区的联盟研究人员加快步伐，以便联盟研究人员能够想出解决问题的银河大脑解决方案，而不是试图提高人工智能的量化技能，这可能更有意义。治理人员，这样他们就可以开始追赶湾区几十年来一直倡导贝叶斯思想的人们。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/uzlw83xsokav6lmsnzbh" alt="詹姆斯·梅德洛克 (James Medlock) 在 X 上表示：“我经常去的一个卫生间隔间里有涂鸦，上面写着“专门针对昆虫。我不是蚂蚁！”我认为这是一个参考"></figure><blockquote><p>我认为现在的瓶颈是漏斗的<i>顶部</i>。每个人都听说过 HP，但“只有”HP 同人书呆子（和/或现有的理性主义者）听说过 HPMOR。序列和相关作品的漏斗顶甚至<i>更窄</i>。</p><p>有多种方法可以<a href="https://www.lesswrong.com/posts/Bfq6ncLfYdtCb6sat/i-converted-book-i-of-the-sequences-into-a-zoomer-readable">或多或少地有效/直接地</a>完成此任务，但<a href="https://www.youtube.com/@RationalAnimations/videos">RationalAnimations</a>是“<i>可以</i>以令人信服的方式呈现基本的 AI 对齐思想——足以影响很多人”的一个很好的例子。</p></blockquote><p>我在这里同意。当我想象 2014 年或 2013 年的自己时，在我听说人工智能安全之前，我可以很容易地想象自己在观看一段 10 分钟的视频并思考“耶稣基督”并将其添加为书签。然后，后来，如果我是那种务实的人，让世界变得更美好，我更有可能在一个月或一周左右再次记住它，我会说“等等，这到底是<i>怎么</i>回事？”然后再看一遍。理想情况下，人们会阅读序列或 WWOTF，并且不会有任何其他人工智能安全入口，但这不是我们生活的世界。</p><p>也许 Yud 和其他人正在制作的播客是一个更好的版本，由于涵盖了有趣的未来主义主题的大量内容，因此更擅长过滤有<a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#4__Intuition_flooding">洞察力</a>的聪明人，或者更适合直觉泛滥，通过从大量不同的角度，例如花 10 分钟讨论心理理论，然后再花 10 分钟讨论工具趋同，然后再花 10 分钟讨论人工智能竞赛经济学。</p><p>实际上，我更多地考虑的是提高 AI Safety 中序列和 CFAR 手册的阅读率，而不是让世界其他地方加入。但这也是一个大问题，并且与人工智能安全社区蓬勃发展所需的新人才密不可分。</p><p>例如，我认为重读序列可以避免很多伤害，就像这条推文一样（我已经为<a href="https://twitter.com/ESYudkowsky">https://twitter.com/ESYudkowsky</a>添加了书签，你也应该这样做）：</p><p> <i><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2uxaqQb9wttgiByJ4/d0hxtqb6nxtwi4ovaxtm"></i></p><p>我认为每 5-10 年甚至更频繁地重读一次序列对于大多数从事人工智能安全工作的人来说是有意义的，但还有更好的方法（而且我们确实应该深入思考寻找更好的方法）。</p><p>我认为我们应该像阅读圣经一样阅读序列。具体来说，每天阅读各种尤德科夫斯基和其他伟大的理性主义者的名言，比如圣经名言。有大量的应用程序、网站、时事通讯和纸质日历每天为您提供一篇圣经引述。</p><p>圣经中有一些相当不错的东西，这些引文是通过很多人寻找的优化压力来搜索和发现的。对于数以百万计的非牧师来说，它非常有效。但尤德科夫斯基的著作在这方面比《圣经》要好得多。</p><p>拥有某种每天为人们随机提供尤德科斯基名言的提要似乎是有意义的。</p><p>这类似于以<a href="https://www.lesswrong.com/posts/PouWW8Ys4iKhWedCZ/psa-the-sequences-don-t-need-to-be-read-in-sequence">随机顺序读取序列</a>，我也强烈推荐这样做；如果你早上喝完咖啡后看娱乐媒体或社交媒体，你就会发现两种强烈的成瘾/行为强化的东西堆积在一起，你真的应该用阅读随机选择的序列来代替媒体，这样你的大脑就会将其与匆忙联系起来。</p><p>但除了坏蛋 EA 苦行僧极端微生物之外，<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">大多数人都想做快速而简单的事情</a>，因此使用伟大的理性主义引言，例如圣经引语。这是开始新的一天或在一天中间的好方法。</p><p>每人每天可以有不同的报价，通过人们以独特的方式得到提升，然后互相交谈，从而最大化乘数效应；当然，这需要足够大的人工智能安全部门才能做到这一点，对于人工智能安全社区来说，这就像放牧猫一样。</p><p>同时，令人难以置信的<a href="https://www.lesswrong.com/posts/dbDHEQyKqnMDDqq2G/cfar-handbook-introduction">CFAR 手册序列</a>有点长（约 20,000 个单词），但很好地解释了困难和重要的概念，而<a href="https://www.lesswrong.com/s/qRxTKm7DAftSuTGvj">Hammertime 序列</a>则短得多且可重复阅读，但这会带来较弱的直观性的代价，并使更容易理解忘记。</p><p>理想情况下，像<a href="https://www.lesswrong.com/users/deactivated-duncan-sabien">邓肯</a>这样的人可以在两者之间建立一个中间立场，尽管考虑到基本概念的复杂性，CFAR 手册序列可能已经被大大缩短了。 Duncan 的写作和解释非常出色，我的先验概率是，如果我尝试将其提取到原始长度的 75% 左右，我将很难保留该值。</p><p>将缩短的版本转换成抽认卡无疑是非常有价值的，但最重要的是人们只需要阅读它，并让他们的朋友阅读它，因为即使按照目前的长度，它们仍然非常值得阅读。</p><p>当人们争论序列的价值或有效性时，他们通常会想，就像把书放在某人面前并告诉他们阅读它。也许看看他们完成后5-10年会是什么样子。但更大的问题是是否有办法更有效地实现这个数量级。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Tue, 05 Dec 2023 02:18:53 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Tue, 05 Dec 2023 02:18:53 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>实际上，我重读了有用/心理模型帮助的帖子和书籍部分，但我认为我是这个社区中的局外人。</p><p>如果我们真的要进行“圣经引用”（！？！？！）类似的设置，我们应该更多地关注“同一主题上的一系列小东西”。就像“序列切​​片”或“引用序列”或“小节序列”或“段落标签”。例如，《CFAR 手册》中关于<a href="https://www.lesswrong.com/tag/crux">症结</a>的每一个段落。或者尤德科夫斯基关于<a href="https://www.lesswrong.com/tag/complexity-of-value">价值复杂性的</a>每一句话。</p><p>一方面，这<i>看起来</i>像是“过于简单化/肤浅化的理性主义著作”，以牺牲更深入的阅读为代价。另一方面，引用可以成为阅读全文的门户。 （而且，如果我们说实话，更好的理性主义倾向决策往往是从“记住相关引言”<i>开始的</i>，即使引言是“自己思考！”或“在时钟上思考 5 分钟”。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Tue, 05 Dec 2023 02:25:54 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Tue, 05 Dec 2023 02:25:54 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><blockquote><p>实际上，我重读了有用/心理模型帮助的帖子和书籍部分，但我认为我是这个社区中的局外人。</p></blockquote><p>这绝对不应该是一个异常的事情。在我开始学习人工智能政策之后的很多年里，我一直把尤德科夫斯基的“<a href="https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer">政治是心灵杀手</a>”的帖子添加为书签。如果我了解尤德的全部<a href="https://www.lesswrong.com/s/ZnSMHcWjRx6yT4H92">政治先决条件</a>并每月阅读其中一篇，那么我现在已经遥遥领先了。这些东西是很棒的参考材料。</p><p>当然，其中大部分只是促使人们重新接受建议并重新掌控自己的生活。坦白说，看到人们慢慢退回到普通美国人/欧洲人日常生活的平庸状态，真是令人不安。新机构的逐渐丧失确实是你必须与之抗争的潮流。把<a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/">Moloch</a>赶出你的大脑并把它拒之门外，同时也将 Moloch 赶出你的团队。</p><p>与此同时，让人们组成一个可以让彼此继续前进的群体，这也很困难。人类是灵长类动物，这意味着我们有战胜对手的动力；对我们来说，这通常意味着采用重要的概念并将其转化为获得超越人类同胞的优势的机会（包括将该概念本身武器化以向竞争对手提出指控，例如“X 人不利于人工智能安全”）。</p><p>有太多的选择，基本上是选择超载。这里没有什么是容易解决的（任何容易解决的问题可能已经被解决，例如成长的烦恼），但仍然有大量容易实现的目标值得努力（或者至少每周花一个小时刻意思考）。</p><p>这是我们现在真正应该做的事情，当我们领先时、当仍有增长空间时开始，而不是当迅猛龙已经到了门口时。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7h6JipnPq6XcJjJPA-Sat, 09 Dec 2023 20:22:32 GMT" user-id="7h6JipnPq6XcJjJPA" display-name="NicholasKross" submitted-date="Sat, 09 Dec 2023 20:22:32 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>尼古拉斯·克罗斯</b></section><div><p>同意。</p><p>最后，我们发现了很多“提升”或“升级”人工智能联盟和治理社区的途径。其中许多选择都是分离的（如果促智药持续失败，神经调节可能会起作用）。其中一些甚至便宜且易于您自己完成（例如序列引用集合）。</p><p>该社区已经存在十多年了，我们<i>仍然</i>看到了潜在的唾手可得的成果。您<i>可能会</i>认为这是“哇，我们的社区真的不够完善”，这通常是事实。但我也认为“哇，有很多东西还没有被 EMH 消除，它们可以帮助人工智能安全！”。</p><p>当然，这依赖于有空闲时间（和金钱）的人来尝试这些。如果有人发现了一些东西，他们就能找到一些能够<i>改变 P(doom) 的</i>东西。这似乎值得一些尝试和错误！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fxAdQTdcYTHJj7sjJ-Tue, 12 Dec 2023 12:47:27 GMT" user-id="fxAdQTdcYTHJj7sjJ" display-name="trevor" submitted-date="Tue, 12 Dec 2023 12:47:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>特雷弗</b></section><div><p>绝对地。此外，我认为，如果每个人都能每周抽出超过 1 个小时的时间， <a href="https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic">认真思考</a>“根据最近发生的事件和新信息，人工智能安全社区如何变得更好？”这个问题，情况可能会大幅改善。</p><p> （重点是使用新技术来解决以前被认为棘手的主要问题，例如<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#Functioning_lie_detectors_as_a_turning_point_in_human_history">测谎仪</a>等，并为每个人改进事情，而不是恐吓特定的人或团体让步或重新分配资源）</p><p>每周留出一小时来思考重要的事情（ <a href="https://www.lesswrong.com/posts/9tx4jRAuEddap7Tzp/raemon-s-deliberate-purposeful-practice-club?commentId=RhDGwGZHyA5WmDZbt">不影响注意力</a>）的时间不到大多数人清醒时间的 1%。这可能是我们大多数人最终回顾过去并希望我们能做得更多、更快、在事情变得激烈之前做的事情。</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/2uxaqQb9wttgiByJ4/upgrading-the-ai-safety-community#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2uxaqQb9wttgiByJ4/upgrading-the-ai-safety-community<guid ispermalink="false"> 2uxaqQb9wttgiByJ4</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sat, 16 Dec 2023 15:34:26 GMT</pubDate> </item><item><title><![CDATA[cold aluminum for medicine]]></title><description><![CDATA[Published on December 16, 2023 2:38 PM GMT<br/><br/><h2>冷铝</h2><p>在氢沸点的非常纯的铝是一种非常冷的材料。在 20K 温度下，99.999% 纯铝的电导率是 0 C 时的 1000 倍。在 4K 温度下，它的电导率可能是 4000 倍。</p><p>在如此低的温度下，铝中的电子自由路径变得宏观，这就是为什么即使少量的杂质也会大大增加电阻。即使是线径也会产生明显的影响。磁场也会增加电阻，但这也是与纯度相关的效应：99.999% 的铝在 15T 时的电阻可能是原来的 3 倍，但即使是更纯的铝，受影响也小得多。</p><p>是的，铝净化需要花费一些钱，但并不是特别贵。它的成本可能是标准铝的三倍，但比超导体便宜得多。另一点需要注意的是，超导体对于<em>恒流</em>只有0电阻。低温铝不像超导体那样受到电流变化的影响。</p><p>看起来这种能大幅提高品质因数的有趣效应应该有某种应用，你不觉得吗？然而，虽然有用于船舶的超导电动机，但我不知道低温铝导体用于任何商业应用。它可以用来做什么？</p><h2>电力线路</h2><p>低电阻导体的一个明显应用是长距离电力传输。我的估计表明，使用低温铝有点太贵了，因为（低温冷却器成本）*（绝缘成本）产品对于合理的线路电流来说太高了。将其连接到环境温度线路也是一个问题，因为冷纯铝也具有高导热性。</p><p>随着温度降低，阻力降低，但制冷机变得更昂贵且效率更低。一般来说，对于低温铝导体来说，液氢似乎比液氦或液氮更好。</p><p>在这样的温度下，值得使用<a href="https://en.wikipedia.org/wiki/Multi-layer_insulation">多层真空绝热材料</a>。这比玻璃纤维或聚酯等典型绝缘材料有效得多，但它似乎仍然不足以使大型地下电力线的绝缘材料+制冷器足够便宜。</p><p>虽然经济上不可行，但使用低温铝进行高功率电力传输是<em>可能的</em>。它只是昂贵，并非不可行。请随意使用它来为硬科幻故事增添风味。</p><p>哪些应用属性使低温铝更适合？</p><ul><li>每表面积电流大。</li><li>可以使用超导体，但电流变化带来的阻力是一个问题。</li><li>低重量很重要。</li><li>很容易实现低温冷却。</li></ul><p>提出的一项应用是在以液氢为燃料的飞机中使用低温铝导体的电动机，这将为铝提供自然冷却。显然，这种飞机目前并不存在，而且我认为它们不是很实用，但这超出了本文的范围。</p><h2>核磁共振成像</h2><blockquote><p>因此，我想到的低温铝唯一好的应用是 MRI 机器。是的，目前新公司或新技术很难进入该市场，但低温铝相对于超导体可能具有一些理论上的优势。</p></blockquote><p> -<a href="https://www.bhauth.com/blog/materials/material%20enabled%20products.html">一些博客</a></p><p>您可能听说过 MRI 扫描很昂贵，因为机器很贵，但美国的 MRI 扫描价格比墨西哥贵约 5 倍。您可能会认为由于劳动力需求，它们很昂贵，但荷兰是 MRI 扫描价格最低的国家之一。</p><p>无论如何，是的，这些机器有点贵。<a href="https://lbnmedical.com/how-much-does-an-mri-machine-cost/">以下</a>是一些大概的机器价格。假设一台价值 40 万美元的机器每天可供 10 个人使用，摊销期为 5 年，则每次使用费用为 22 美元。考虑到美国医疗保健的典型价格乘数，您可以看到它如何变得昂贵......？</p><p>其中一些成本用于超导磁线圈。有没有办法通过使用冷铝来潜在地减少费用，或者以某种方式提高 MRI 性能？</p><h3>梯度线圈</h3><p>MRI 机器的典型方法是使用 2 个超导线圈（恒定电流）来形成近似均匀的磁场，并使用铜线圈来创建（小得多）磁场梯度，其变化频率可能为 2 kHz。核磁共振进动频率取决于场强，因此该场梯度可用于将发射定位到特定切片。</p><p>对于铝，理论上相同的线圈可以用于这两个目的。用铝代替超导体的成本有多高？我估计这将涉及：</p><ul><li> >;1吨纯铝</li><li>几千瓦的电阻损耗</li><li>制冷机能耗低于但与当前 MRI 能耗相当</li><li>（低温冷却器 + 铝）成本与当前 MRI 机器的总成本相当</li></ul><p>因此，冷铝可用于 MRI 机器，但严格来说并不比超导体更好。高温超导体怎么样？人们当然考虑过这一点，但它们实际上<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5472374/">并不像许多人想象的那么好</a>。</p><p>冷铝也可以与超导体一起使用以产生梯度。这比使用铜更好吗？</p><p>较大的场梯度可以更好地实现切片之间的定位。这可以提高分辨率（具有更高的场强）或扫描速度（具有更高的重复率）。这两件事显然也增加了其他成本。转换速率（场变化/时间）受到触发较长神经的感应电流的限制，但只有最昂贵的 MRI 机器才接近该限制。</p><p>用于制造当今 MRI 机器中最高梯度的低温冷却器和纯铝的成本实际上似乎非常合理。使用冷铝代替铜可能会减少其尺寸、重量甚至成本。它可以在给定体积内允许更大的最大场梯度；或者，它可以允许线圈之间有一些开放空间，这可以使扫描不那么令人不愉快。</p><p>我认为非常高的场梯度对于未来的 MRI 机器可能变得更加重要，原因是：同步多切片成像。过去，由于 MRI 重建的计算要求较高，因此较高的重复频率更受青睐，但如今这已不再是问题；与运行《Starfield》或《城市天际线 2》相比，该计算对 GPU 的要求较低。高梯度多切片成像可能会使相同质量下的扫描速度提高数倍。这不仅提高了机器吞吐量并降低了劳动力成本，还改善了患者体验并提高了实时性能。</p><p>使用低温高纯度铝在使用同步多切片成像的 MRI 机器中制造大场梯度不仅似乎可以提供更好的性价比，而且最终也是低温高纯度铝的真正应用。</p><br/><br/><a href="https://www.lesswrong.com/posts/yG9prEve6vD4qnTfy/cold-aluminum-for-medicine#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yG9prEve6vD4qnTfy/cold-aluminum-for-medicine<guid ispermalink="false"> yG9prEve6vD4qnTfy</guid><dc:creator><![CDATA[bhauth]]></dc:creator><pubDate> Sat, 16 Dec 2023 14:38:04 GMT</pubDate></item></channel></rss>