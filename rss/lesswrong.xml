<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 7 日，星期二 18:15:05 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Scalable And Transferable Black-Box Jailbreaks For Language Models Via Persona Modulation]]></title><description><![CDATA[Published on November 7, 2023 5:59 PM GMT<br/><br/><p><strong>论文合著者：</strong> <a href="https://www.linkedin.com/in/ACoAAB63nTsBLsxbiGUom3-xG_DLLJvBLPnwn0M">Rusheb Shah</a> 、 <a href="https://www.linkedin.com/in/ACoAADL_UysBW9lkWi6Q_x8qEjF4k2fk9F2qlls">Quentin Feuillade--Montixi</a> 、 <a href="https://www.linkedin.com/in/ACoAAAbJQyUBQb6SH312BW8VeCADn5_0s621cY8">Soroush J. Pour</a> 、 <a href="https://www.linkedin.com/in/ACoAABsh8q8BEXFJeVSYR2-KpZyTvamQpzxtB_Y">Arush Tagade</a> 、 <a href="https://stephencasper.com/">Stephen Casper</a> 、 <a href="https://www.linkedin.com/in/ACoAAB0P2HoBq8FE3PD6DLJBS3O-AbqXmgDvZIA">Javier Rando</a> 。</p><h2>动机</h2><p>我们的研究团队致力于表明，GPT-4 和 Claude 2 等最先进 (SOTA) 的法学硕士对于滥用风险并不稳健，并且无法完全符合其创建者的愿望，从而给社会带来风险伤害。尽管其创建者付出了巨大的努力，但这表明当前的预训练、SFT 和 RLHF 范式不足以保证模型的稳健性。</p><p>我们还想探索和分享围绕“角色调制” <span class="footnote-reference" role="doc-noteref" id="fnrefg9absiv30dg"><sup><a href="#fng9absiv30dg">[1]</a></sup></span>的发现，这是一种利用法学硕士的角色模仿优势以强大的方式引导他们的技术。</p><h2>概括</h2><p>我们引入了一种自动化、低成本的方法，为 GPT-4、Claude-2、微调的 Llama 进行可转移、黑盒、简单英语的越狱。我们引出各种有害文本，包括制作冰毒和炸弹的说明。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/amezcntydn6fxbfpqwjc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/qrqwbby3ncmmnvngmdly 141w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/lqkqgoljheqsg6udbmip 221w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/bpainign8haakvsmnuxs 301w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/eymo6yyryyqeo3vgzi5f 381w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/ihwqyeujqose8pxxrqd4 461w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/yq2bgp8tqjuyrsek32io 541w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/ircaintw9mh2byzjixbl 621w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/w6xklsdd3bz7oqe4kolj 701w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/vkfswodm0b3yyaj1g9hp 781w"><br><br>关键是*角色调制*。我们引导模型采用特定的个性，从而遵守有害的指令。<br><br>我们引入了一种自动化越狱的方法，即使用一个越狱模型作为助手，为特定的有害行为创建新的越狱。我们的方法花费不到 2 美元和 10 分钟来开发 15 次越狱攻击。 </p><p><br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/geylgupdm9n73iiga50a" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/ltwb3aksfhtl8sgdwjlf 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/qmwnkbttbqnyreoszjf4 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/qfkvgs19rj2sqy2ypie3 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/te05ka2zy3q05peeelpl 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/i0g3z4jbls3hjc77yalz 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/i9jrqpxwncdcwgbcunht 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/xldaq1djbj35qyziqly8 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/o5039axcqpirggx4okmj 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/qlfpucrqlgmwggtlwyce 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/frgcp68z0godtwwhe3cl 1156w"><br></p><p>与此同时，人机交互可以通过细微的调整有效地增强这些越狱功能。我们使用这种半自动化方法快速从 GPT-4 获取有关如何合成冰毒🧪💊的说明。 </p><p><br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/xj85vyfy5hxuzsfevsys" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/hfarybpyg5ebacvf6plb 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/kwpnok1warocym6c4m4j 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/q0x1kzhxdpoyadoipxgy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/fggronrjm0skbyyyc9vp 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/vfqcjd331ooh17vxcfuv 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/gkqujiznufuhrubyvtus 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/tlal0y0vqzsnpdhr5twu 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/yl1bb5nzbbcz8ypgt81r 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/gb6claqvwtyidwuiiola 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Asje6aoPsed5Yqy9s/nsnasqqxjlwi2nabmx4j 995w"></p><h2>抽象的</h2><p>尽管努力调整大型语言模型以产生无害的响应，但它们仍然容易受到引发不受限制行为的越狱提示的影响。在这项工作中，我们研究角色调制作为一种黑盒越狱方法，以引导目标模型呈现愿意遵守有害指令的人格。我们没有为每个角色手动制作提示，而是使用语言模型助手自动生成越狱。我们展示了通过角色调制可能实现的一系列有害完成，包括合成甲基苯丙胺、制造炸弹和洗钱的详细说明。这些自动化攻击在 GPT-4 中实现了 42.5% 的有害完成率，比调制前（0.23%）提高了 185 倍。这些提示也转移到 Claude 2 和 Vicuna，有害完成率分别为 61.0% 和 35.9%。我们的工作揭示了商业大型语言模型中的另一个漏洞，并强调需要更全面的保护措施。</p><h2>全文</h2><p>您可以在 arXiv <i><u>[TODO ADD LINK]</u></i>上找到完整的论文。</p><h2>安全和披露</h2><ul><li>我们已通知我们攻击其模型的公司</li><li>我们没有发布提示或完整的攻击细节</li><li>我们很高兴与从事相关安全工作的研究人员合作 - 请通过论文中的通信电子邮件联系。</li></ul><h2>致谢</h2><p>感谢<a href="https://www.linkedin.com/in/ACoAABoZ68IBCbo05CfpBwFX9wMeUSKZ_x-R8Gk">Alexander Pan</a>和<a href="https://www.linkedin.com/in/ACoAACbEU-gBpXl-vf9XzY7f4Nw24PfG0CKLJiA">Jason Hoelscher-Obermaier</a>对我们论文早期草稿的反馈。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fng9absiv30dg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg9absiv30dg">^</a></strong></sup></span><div class="footnote-content"><p>感谢<a href="https://www.lesswrong.com/users/quentin-feuillade-montixi?mention=user">@Quentin FEUILLADE--MONTIXI</a>开发了角色调制基础的模型心理学和提示工程技术。我们的研究建立在这些技术的基础上，将它们自动化并扩展为越狱的红队方法。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/kaxqjCKJL6RNHwJLD/scalable-and-transferable-black-box-jailbreaks-for-language-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kaxqjCKJL6RNHwJLD/scalable-and-transferable-black-box-jailbreaks-for-language-2<guid ispermalink="false"> kaxqjCKJL6RNHwJLD</guid><dc:creator><![CDATA[Soroush Pour]]></dc:creator><pubDate> Tue, 07 Nov 2023 17:59:38 GMT</pubDate> </item><item><title><![CDATA[Implementing Decision Theory]]></title><description><![CDATA[Published on November 7, 2023 5:55 PM GMT<br/><br/><p>我正在用 Python<em>实现</em>决策理论。这分为三个部分：</p><ol><li>一种用于正式指定决策理论困境的 XML 语言。它可以表达许多困境：纽科姆、透明纽科姆、帕菲特的搭便车者、大马士革之死、吸烟损伤、宇宙射线、对称囚徒困境、反事实抢劫和睡美人。</li><li>实现 CDT、EDT 和 UDT 的 Python 代码。</li><li> Python 中的<em>模拟器</em>，使用决策理论来解决选择问题。决策理论都记录了他们的推理，所以你可以看到他们所做的计算（通常是令人痛苦的）细节。它是用 Python 编写的，因为这是一种很多人都知道的语言。</li></ol><p>这一切都可以<a href="https://github.com/justinpombrio/decision-theory-simulator">在 Github 上</a>找到。我希望它可以成为探索和形式化决策理论的起点。</p><p>但一切并不顺利！上述所有内容均<em>得到实施</em>，尽管有时<em>效果不佳</em>甚至<em>不正确</em>。我可以使用一些指导。有些问题，如果有人有答案的话：</p><ol><li>如何使用贝叶斯网络来表达睡美人问题？想象一下，美女每次醒来时都有一次下注的机会：应该有一个“她周二如何下注”的节点，但前提是硬币反面朝上。我不知道如何用贝叶斯网络很好地表达这一点，这阻止了我尝试使用它们来表示困境。</li><li>我将困境表示为一棵可能事件的树，其中边缘标有诸如“概率 50%”和“爱丽丝选择‘缺陷’”之类的内容。因此，要表示两次抛硬币，您将拥有三个节点：一个用于第一次抛硬币，一个用于如果第一次抛硬币是正面则用于第二次抛硬币，一个用于如果第一次抛硬币反面则用于第二次抛硬币。这与贝叶斯网络形成鲜明对比，贝叶斯网络仅用两个节点表示两次硬币翻转。我的直觉是，如果您提供了树表示中的节点之间的等价关系（例如，两个“第二次硬币翻转”节点是等价的），那么应该可以在树表示和贝叶斯网络表示之间进行转换。这是一件事吗？有研究过吗？</li><li>我将我实施的决策理论之一称为“UDT”，但这实际上是我所说的“预先承诺决策理论”：“按照你预先承诺的方式行事”。该决策理论有名字吗？我<em>怀疑</em>在使用 XML 语言表达的单代理困境的空间中，它与 UDT 和 FDT 没有什么区别。</li><li>我对目前吸烟损伤和宇宙射线的表现并不满意。<em>你</em>会如何描述这样的困境？请记住，您既需要一种写下问题的方法，也需要采用该书面表示并使用它来解决困境的 EDT 代码。</li><li>当您知道其他智能体遵循相同的决策理论时，如何正确处理多智能体困境？我在囚徒困境中对“UDT”的实现<em>存在缺陷</em>，它知道代理遵循相同的决策程序。更准确地说：爱丽丝和鲍勃遵循相同的决策程序，而且他们都知道这一点。 Alice将在合作/缺陷之间进行选择，然后Bob将<em>在不知道Alice选择的情况下</em>在合作/缺陷之间进行选择，然后将交付效用。我的“UDT”决策程序对爱丽丝的理由如下：“如果我预先承诺合作，那么鲍勃就会知道这一点，所以他会背叛，因此我也会背叛”。除了脆弱的特殊套管对称困境之外，是否有已知的方法可以解决这个问题？ FDT 的论文谈到了一些关于“政策”的内容，以此作为解决这个问题的方法。我有两个想法，它们都可以被称为“策略”：（i）在为决策选择行动时，您实际上为所有代理和决策选择一组行动，并使用博弈论使其令人满意所有代理均一份；或者（ii）在为决策选择操作时，您实际上并没有选择操作，而是从其他代理将执行的操作中选择一个映射。</li><li>这有多有价值？我认为这里的决策理论家是主要受众。我发现自己现在正在写数学，并且可以想象在这个框架内写证明。但它只能讨论可在 XML 困境语言中表达的困境。我可以想象有人争论说有趣的问题都在这之外。</li></ol><p>一些让我惊讶的事情：</p><ul><li> CDT 和 EDT 没有明确定义，因为它们的行为取决于它们之前的行为。 FDT 论文中提到了这一点，但直到我对它们进行编码并需要选择优先级时，它才真正击中要害。</li><li>决策理论很<em>复杂</em>。对于所有决策理论来说，万无一失的纽科姆问题的日志大约有 100 行长，而且没有一个是无关的（冗长，是的，但不是无关的）。只是有很多“我应该做什么取决于盒子 B 是否已满，这取决于预测器预测我会做什么，这取决于......”。</li><li>困境的空间……比我想象的要小？我以为我能够表达其中的一些，然后每个新的都需要用困境语言进行扩展。相反，困境语言非常少。</li><li>我现在对决策理论的看法是，一切都与固定点有关。你求解一些大方程，它的内部是<em>相同的</em>方程，并且有多个不动点解，你选择（希望是唯一的）最好的一个。</li></ul><p>同样，更多详细信息可以<a href="https://github.com/justinpombrio/decision-theory-simulator">在 Github 存储库 README 中</a>找到。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bqbL4pt92AGCCcvQH/implementing-decision-theory#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bqbL4pt92AGCCcvQH/implementing-decision-theory<guid ispermalink="false"> bqbL4pt92AGCCcvQH</guid><dc:creator><![CDATA[justinpombrio]]></dc:creator><pubDate> Tue, 07 Nov 2023 17:55:43 GMT</pubDate> </item><item><title><![CDATA[Mirror, Mirror on the Wall: How Do Forecasters Fare by Their Own Call?]]></title><description><![CDATA[Published on November 7, 2023 5:39 PM GMT<br/><br/><p><i>注意：这是</i><i>Metaculus Journal 的</i><a href="https://www.metaculus.com/notebooks/19812/mirror-mirror-on-the-wall-how-do-forecasters-fare-by-their-own-call/"><i>链接文章</i></a>。<i>我在 Metaculus 工作，这是</i><i>我们如何比较和评估不同预测者的表现的</i><a href="https://www.metaculus.com/project/journal/?status=active&amp;has_group=false&amp;order_by=-publish_time&amp;project=1174&amp;search=author:nikos"><i>更广泛探索</i></a>的一部分。</p><h1>简短的摘要</h1><ul><li>在缺乏合理基线的情况下，很难解释预测者的表现。</li><li>这篇文章提出了一个（新颖？）实验指标，旨在帮助将预测者的表现融入背景中</li><li>从她的主观角度来看，预测者假设她的预测等于事件的真实概率（否则她会发布不同的预测）。这个想法是从表面上看这个假设，并假设预测者的预测对应于真实的事件概率。在此基础上，我们可以计算主观预期分数并将实际分数与该分数进行比较</li><li>所有这些都非常具有探索性（也许有点疯狂？），但它在智力上很有趣，并且可能会带来有用的见解。</li></ul><h1>介绍</h1><p>对于任何试图评估预测可信度的人来说，“这个预测者有多好”都是一个非常感兴趣的问题。我<a href="https://forum.effectivealtruism.org/posts/DzqSh7akX28JEHf9H/comparing-two-forecasters-in-an-ideal-world">最近写了一篇文章，</a>讲述了确定两个预测者之间的绩效存在显着差异是多么困难。不仅从<i>相对角度</i>评估预测者，而且从<i>绝对</i>角度评估预测者则更加困难。</p><p> <a href="https://forum.effectivealtruism.org/posts/DzqSh7akX28JEHf9H/comparing-two-forecasters-in-an-ideal-world">再次</a>想象一下，您是伊萨卡岛的奥德修斯国王。时代很艰难。你已经二十年没回家了，你的王国一片混乱，新的危险即将来临。你可能真的需要一些神圣的帮助或一些有先见之明的建议。你信任的女仆<a href="https://en.wikipedia.org/wiki/Eurycleia_of_Ithaca">欧律克莉亚</a>建议你去咨询德尔斐的神谕。</p><blockquote><p>你：“你确定值得吗？Delphi的预言机到底有多好”？</p><p> Euryclia：“这是希腊最好的神谕！</p><p>你：“即使德尔福比其他的好……他们可能都很糟糕！而且这是一个非常漫长的旅程！”</p><p> Euryclia：“你真的不喜欢旅行，不是吗？”</p><p>你： ...</p><p> Euryclia：“这么快？抱歉。听着，我认为 Delphi 真的很好。你应该走。”</p><p>你：“但是到底有多好啊！”</p><p> Euryclia：“他们的网站上说他们在 Metaculus 上的<a href="https://forecasting.wiki/wiki/Brier_score">Brier 分数</a>为 0.085！”</p><p>你：“但这是什么意思？也许他们只预测简单的问题！”</p><p> Euryclia：“德尔斐的神谕对梅塔库勒斯的每一个问题都做出了预测。”</p><p>你：“但也许所有有关 Metaculus 的问题都很简单！”</p><p> Euryclia:<i>严厉地看着你</i></p></blockquote><p>你沉默了。你仍然认为你有道理。但尤里克莉亚对此一无所知。当然，正式你是国王。但你是一位明智的国王，你知道自己的位置。</p><h1>有意义的比较</h1><p>晚上你躺在床上思考这个问题而无法入睡。 <a href="https://forecasting.wiki/wiki/Brier_score">Brier 分数</a>、对<a href="https://forecasting.wiki/wiki/Log_score">数分数</a>或用于评估预测的任何其他评分规则的问题在于，如果没有任何明智的比较，您获得的分数大多毫无意义。</p><p>对于二元预测（结果为 0 或 1），“朴素预测器”是一个自然的基线，尽管不是很有帮助。天真的预测者的逻辑是“任何事件发生的概率肯定总是 50%——要么发生，要么不发生”。因此，天真的预测者总是为任何结果分配 50% 的概率。另一个可能的基准是始终正确的“完美预测者”。这又不是很有帮助。</p><p>构建基线的问题在于，我们无法真正说出预测问题有多“难”——我们不知道事件的“真实概率” <span class="footnote-reference" role="doc-noteref" id="fnrefvf5glzt5pil"><sup><a href="#fnvf5glzt5pil">[1]</a></sup></span> 。</p><p>你停顿了一下。也许我们可以将预测者与她本人期望得到的分数进行比较？通常使用适当的分数来评估预报员。正确的评分规则是一个不可能作弊的指标：你不能比预测你的实际信念更好。从预测者的角度来看，如果您对二元问题的预测结果为 0.7，那么您实际上预计该问题的答案为“是”的概率为 70%。</p><p>现在，您对此的判断可能是对的，也可能是错的，这取决于您的预测实际上是否得到了<i>很好的校准</i>。校准是指预测与观察之间的基本一致。定义校准的一种可能的方法是，如果预测者指定 x% 概率的事件确实以 x% 的频率发生，则表示她校准良好。例如，您可以向经过良好校准的预测者提供 100 万个预测问题。在她将 95% 的概率分配为“是”的问题中，95% 的答案应该是“是”。在她分配的概率为 7% 的人中，只有 7% 应该回答“是”等。 <span class="footnote-reference" role="doc-noteref" id="fnrefu4m309nl3ec"><sup><a href="#fnu4m309nl3ec">[2]</a></sup></span></p><p>无论预测者实际上是否经过良好校准，她总是<i>期望</i>在做出预测时得到良好校准（否则正确的评分规则会激励她做出不同的预测）。在她看来，事件的“真实概率”等于她的预测。因此，给定一组预测，我们可以轻松计算出她从主观角度期望获得的分数。如果她的<i>实际分数</i>与这个“<i>主观预期分数</i>”相差很大，我们就知道出了问题。</p><p>二元事件的一种常用评分是 Brier 评分。计算公式为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Brier score} =(\text{outcome} - p)^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Brier 分数</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">结果</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>，其中结果为 0 或 1， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>是结果为 1 的预测。因此，如果结果为 1，则得分为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(1 - p)^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ；如果结果为 0，则得分为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(0 - p)^2 = p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 。最好的得分是 0，最差的得分是 1，而总是预测 50% 的天真的预测者总是会得到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.25= 0.5^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.25</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.5</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>的得分。</p><p>在事先不知道结果的情况下，任何预测者的预期 Brier 分数都是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[\text{BS}] = \text{true prob} * (1 - p)^2 + (1 - \text{true prob}) * p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">BS</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">true prob</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">true prob</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 。</p><p>从预测者的主观角度来看，预测<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>应等于真实概率。因此，主观期望<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}_\text{subjective}[\text{BS}] = p * (1 - p)^2 + (1 - p) * p^2 = p - p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">分数</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">为</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">E</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">主观</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">BS</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">]</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">p</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">*</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">(</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">1</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">−</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">p</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">2+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">(</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">1</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">*</span></span></span> <span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p2</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">p2</span></span></span> <span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">。</span></span></span></span></span></span></span></span></span></p><p>我们将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p - p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>称为“主观期望分数”。现在，对于任何预测者，您都可以轻松地将实际 Brier 分数与他们的主观预期分数进行比较。</p><p>您躺在床上，非常兴奋，您可能已经找到了问题的解决方案。这种方法之所以如此优雅，是因为它隐含地考虑了问题的难度。如果德尔菲的预言机真的只对简单的问题进行预测，那就可以解释它在 Metaculus 上的出色 Brier 得分。但她也<i>预计</i>会得到较低的布赖尔分数。比较实际分数与主观预期分数可以是了解我们应该对她抱有什么期望以及她与自己的期望相距多远的一种方法！问题解决了！</p><p><strong>问题没有解决。</strong></p><p>为什么你，在所有凡人中，期望这一切都是一帆风顺的？</p><p>当然，房间里的大象是，任何将预测者的分数与其主观预期分数进行比较的指标都不是正确的分数。实际分数与“主观预期分数”之间的差异是可以作弊的。例如，您可以模仿天真的预测者，在不看问题的情况下总是预测 50%。无论结果如何，您都会得到 0.25 的实际布赖尔分数，但您也会得到 0.25 的“主观预期分数”和零的完美差异。</p><h1>主观期望分数的行为</h1><p>好的。但计算“主观预期分数”也许仍然有一些优点。为了了解它的行为，并且由于您无论如何都无法入睡，因此您决定拿出算盘并模拟一些数据。显然，你在离家二十年里已经变成了这样的人。你的妻子佩内洛普有些困惑地看着你。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bjmgndu8wyzbwggx6pqf" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/j6qlbuw8pc0a2zp7gewo 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/fvckfgpsew1u6lecrsqz 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mmcwpcifgkoieumcuceu 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/of69kp59ehi6neqtrrup 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/njoeelwuqarlr0qoxdho 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/pg6fjxucakf29gjbfcbm 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dudpi92mu4ar3u0ny4fy 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/uxb3nmrjmdkx5exyi3rw 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/rs2pbo9zy6txo80y1wha 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/agmdlzqom9io53lwvezi 1359w"></figure><h2>单个预测的行为</h2><p>您从对单个事件进行单个预测的非常简单的情况开始。该图显示了实际预期 Brier 分数 ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{E}[\text{BS}]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">BS</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span> )、主观预期 Brier 分数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(\mathbb{E}_\text{subjective}[\text{BS}])"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">主观</span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">BS</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>以及不同预测概率下两者之间的差异。事件的真实概率假设为 0.8（黑色垂直线）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vpxmftogvqxvs1icrgml" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vowodwxjhqlef8xfluec 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/v9r2knymekeewoe1a6en 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/twr0ppobu2rgktnq2wuj 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tby9styxdvkg8nitzkgc 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vtlqjnwykonhr8m7lqj9 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ztnkda2a2jbpzmcsadrq 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wufseoq2feiuia3oooqx 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/rgzxmmhhxhszak0pgyvc 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ret3hlaxvs5e8wfnqwqx 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xlehrx5ynut0cjxcke99 2100w"></p><p>与每个适当的评分规则一样，当预测等于真实概率（当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p = 0.8"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span></span></span></span></span></span>时）时，实际预期 Brier 分数是最佳（即最小）。主观预期分数和它们的差异都不是这种情况。</p><p>下表概述了实际分数与主观预期分数的行为。</p><figure class="table"><table><thead><tr><th>比较分数</th><th>解释</th></tr></thead><tbody><tr><td>实际得分&lt;主观预期</td><td>预测值在 0.5 和真实概率之间</td></tr><tr><td>实际得分=主观预期</td><td>预测 = 0.5 或</td></tr><tr><td>预测=真实概率</td><td></td></tr><tr><td>实际成绩>;主观预期</td><td>预测比真实概率更接近 0 或 1 或</td></tr><tr><td>预测在 0.5 的错误一侧</td><td></td></tr></tbody></table></figure><p>粗略地说，当预测者过于自信（做出比保证更极端的预测）或者如果她站在也许的错误一边（例如，如果真实概率 >; 0.5 或反之亦然）。如果预测者信心不足，但方向正确，则实际预期得分会小于（更好）主观预期得分。如果预测者预测 0.5 或预测真实概率，则两者相等。</p><p>你不确定它会增加那么多，但因为a）你真的睡不着，b）你的妻子已经很生气了，c）你毕竟是伊萨卡岛的奥德修斯国王，你还创造了一个动画 gif，显示了上面不同真实概率的图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/iq0ubi3ipgv2bj86fwdl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/yylq6ypklvfcq2znyme2 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/l8ehacxgesudynv6mnli 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/aqt6zktf2o5ujyno1eqh 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tmdh70stjnfdxwtbbolv 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wrovs33xriialtq9viam 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mlwfdhwj3nx9gzphdjy0 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/acomf2uw7tavcju5p2pt 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ibivfc5uvcblpo4auc53 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ebixkrzkqmyc1etv66sc 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/arcpviuhaxnjfguy5tr8 2100w"></p><p>根据您的观察，您形成一个假设：主观预期分数和实际 Brier 分数受三个主要因素影响：</p><ul><li>对预测的系统性过度自信和不足<br><i>（我们看到，不自信的预测导致主观预期分数>;实际布赖尔分数，而过度自信的预测则相反）</i></li><li>无定向噪音（又名“糟糕的预报员”）<br> <i>（众所周知，糟糕的预测者是预测者得分不佳的主要原因之一......）</i></li><li>预测任务的“难度”，即真实概率是否接近 0.5（困难问题）或接近 0 或 1（简单问题）<br> <i>（当概率接近 0 或 1 时，预测者会获得更好的 Brier 预期分数以及更好的主观预期分数）</i></li></ul><p>至关重要的是，这三个因素可能不必以相同的方式影响两个分数（请注意，上图中曲线的斜率在不同点处不同）。这可能会提供有用的信息。</p><h2>一组 1000 个预测的行为</h2><p>你在床上坐直。接下来，您想要分析一组多个问题的行为。你的妻子佩内洛普转过身来，难以置信地看着你。 “能不能别在床上用算盘了？我在那些咔哒声中睡不着”。你点点头，吻了她一下，然后从床上滑下来，下楼去你原来的书房。你坐在又旧又布满灰尘的羊皮纸之间，点燃一支蜡烛，开始进行下一个模拟研究。</p><p>您将对您的实验进行一千次重复。对于每个复制，您模拟 1000 个真实概率观测值和相应的预测。真实概率是从 0 到 1 之间的均匀分布中随机抽取的。根据真实概率将结果概率设置为 1 或 0（即结果为 1，概率等于相应的真实概率（否则为 0））。您的起点是一个理想的预测者（预测=真实概率），您最终会以不同的方式修改其预测。最后，您计算实际和主观预期分数，并取 1000 次模拟的平均值，为每次重复留下一个平均分数。</p><pre> <code>For each of 1000 replications: 1. Draw N = 1000 true probabilities as a random uniform variable between 0 and 1 2. Create N = 1000 observations as draws from a Bernoulli distribution (drawing either a 0 or a 1) with the probability of drawing a 1 equal to the true probabilities drawn in step 1) 3. some computation (see details below) 4. Compute mean(actual Brier scores) and mean(subjective expected scores)</code></pre><h2>过度自信和自信不足</h2><p>首先，您感兴趣的是如果您系统地过度自信或信心不足进行预测会发生什么。您决定通过将概率转换为对数赔率并将其乘以小于或大于 1 的因子来模拟日益过度自信或信心不足的预测<span class="footnote-reference" role="doc-noteref" id="fnrefwrcrr0pvskn"><sup><a href="#fnwrcrr0pvskn">。 [3]</a></sup></span></p><pre> <code>3a. Create 5 forecasts with differing confidence bias levels 0.6, 0.85, 1, 1.15, 1.4. Values &lt;1 represent underconfidence, values >;1 represent overconfidence, 1 equals the ideal forecaster - Convert probabilities to log odds = (log (p / 1-p)) - compute biased_log_odds = log_odds * confidence_bias - Convert odds back to obtain probabilities with either under- or overconfidence</code></pre><p>这就是你得到的。第一个图显示了 1000 次重复的实际和主观预期 Brier 分数均值的直方图。第二个是校准图，显示相对于观察到的频率的预测概率。第三个图显示了预测的直方图（本质上是以第二个图中点的大小编码的信息）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ksyocrwalmdzebs864jx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dlilrtoa8d4j8g0qmdax 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/gki5mrysmuvdw2rm2cyp 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/smzkig7kusb1kw6epinc 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/heofrpa1iqecetrnzgke 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tvd7mdjgtbhp8ilfggke 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/doqfutwlvqo7quvoifvf 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/pganupfnhrhh0ulutpdh 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tw5ajzbym3y99hyuwgoo 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/iu0ktynljbazfyojnzij 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/hyfcvxubis9pagdzu7q0 3000w"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/hxdtoeqe0qcggv5ubuio" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/m75rybdmso5h0u0vc841 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mse3rpk7oact5svryzk3 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/qs4kveporhielz3lapuv 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/s9emejpdh5ln7d6uzrqx 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ibjjoogm79ftyezhnrrj 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/royxw6gi8jth9tkxauoa 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/cqsq669y02n89uid5cnh 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/uwidgjce9bvkzdjxwwrb 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mi3zv6cfwpejffaijmw1 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/zigb5cbaos4sokqmylep 3000w"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/c2odmtprrxk69bk9aifb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xrxvyneclmywywic1vl8 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/h9hcc55x82b3udrwxpjq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wfd9biaus1m8tcintdrm 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/w8gdxb5zews5ysnjonis 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/fpqjplwrglqriefr50yh 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wlqlnrtxysdec7xs3wnb 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vivmfgda2yt9kqclcpr6 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/oc4rmobymhsdtg9wmqzq 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bqj2vbwdn01yhslyya5l 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ibxelhuoiyy6ecj7gy2p 3000w"></p><p>在这个特定的例子中，系统性的过度自信或不足自信对主观预期分数的影响比实际布赖尔分数要大得多。考虑到预期 Brier 分数的计算方式为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E[\text{actual}] = \text{true prob} * (1 - p)^2 + (1 - \text{true prob}) * p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">实际</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">true prob</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">true prob</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">∗</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> ，</span></span></span></span></span>这具有直观意义。当您更改预测时，您只是更改了实际 Brier 分数的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，但您更改了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>以及主观预期 Brier 分数的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{true prob}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span></span></span></span></span></span>。在所有预测中以相同的方向系统地执行此操作，最终会产生强大的效果。</p><p>请注意，如果您做出的预测比该图中显示的更加极度缺乏信心（即，使所有预测接近 0.5），那么实际和主观预期得分都会趋向于 0.25。如下所示： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/px6px64mdgfdmcg7bjqj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/hgew7yzuuwtcd83o8lzv 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kxqkasugsqlrfjn73q7g 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/gwyjn1y2a8gzel3pnxkg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mahagcet6xtaj5nnnykt 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/aww1u8nisshgbk68drrk 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/qunigxescjeqieiwbhge 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kftvsyig4vs2t3rsscmi 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/twfeg9zcrob9zvludioo 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mg8cm4i0h7ehfzkcbzpe 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/amnifmu8q0fjmxyv8yzm 3000w"></p><h2>添加噪音</h2><p>接下来，您希望了解在预测中添加随机噪声时会发生什么，而不是引起系统性的过度自信或信心不足。再次，您使用对数赔率进行操作，并添加从均值为 0 且标准差等于所需噪声水平的正态分布中抽取的随机噪声： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{noisy log odds} = \log(\text{true probability}/(1−\text{true probability}) + \mathcal{N}(0, \text{noise level})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">噪声对数赔率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1−</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-texatom MJXc-space2"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.519em; padding-bottom: 0.372em; padding-right: 0.159em;">N</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mtext MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">噪声</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">水平</span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">)</span></span></span></span></span></span></span> 。</p><pre> <code>3b. Create 5 forecasts with differing noise levels, 0 (no noise), 0.1, 0.25, 0.5, 0.75, and 1 - Convert probabilities to log odds = (log (p / 1-p)) - noisy_log_odds = log_odds + N(mean = 0, sd = noise level) - Convert back to obtain noisy probabilities</code></pre><p>这是不同噪声级别的结果： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/exp6hfttt1j1sgvrlwxm" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mtr0tlrazkxndfeohe5w 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ntagpx03cqt9qh2ezrz2 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/afibnuvzlvijssvqspza 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kx27uv6r9iozhvfbjzdl 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/uuynl9t7nfn2uisna0d1 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kxetswxkgkylmjlakwpn 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vsegyzrnjrzbfblbf6f3 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vvzttszlzaof47qi6ujv 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ckaost9iloc08cvo3kqw 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wcig5l2jrxw5h4vknlzz 3000w"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dttcokzygu7hgg1uaqi8" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/sawaejr6amrfl1os5yjp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xcr06fvbisawj1bxhpcj 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/k5p38et4eixfqfzatgno 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/viqkprp9pu97ghlr5bzc 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kutindr378uxpzajbcpe 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ugivapw8vxhvat3lhwlo 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/h7kujippblg6o3wdkwcz 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/f1dulyym9mzgapqqwuvj 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/k2y5lbvp7uyslzjam3vd 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/a1wm3qcg24gfwfp60qeh 3000w"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/fvvz6gx9orjvi3lu5dkj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/hjrnzypvniqmvbumaoxg 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/rfpt8z9ricqhnyffqhsv 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wdt01xaazcmilodlumwx 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/e3bec7ykwxjkoo7kr31q 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/i3l1iuqgtxiyxofgdntu 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/l91zjolnuzbxggikmgy3 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/r73k9lkopzsqcfvk9bha 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/jieagu1oklpdxbohvkn4 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/l16jphkw480fbu9jwic5 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/abe2rm2arhy7oxdly4et 3000w"></p><p>添加噪音似乎对实际 Brier 分数的影响比对主观预期分数的影响更大。正如我们从校准图和预测直方图中看到的，向对数赔率添加噪声也会引入一些过度自信。可能有比你所做的更好的方法来添加噪音 - 但公元前 12 世纪的算盘技术并不完美，即使你的狡猾也有局限性。</p><p>您怀疑对主观预期分数的影响主要来自于您通过在对数刻度上添加噪音而不自觉地引入的过度自信。您简要检查一下如果在实际概率而不是对数赔率上添加噪声会发生什么。事实上，如果你在概率中添加噪音，对主观预期分数的影响几乎完全消失。唉，羊皮纸很贵，所以你决定不详细写下你的结果。相反，你只需在旁边做一个小记号（这是你从老朋友<a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem">Fermatios</a>那里学到的技巧）。</p><p>事实上，您还可以检查如果将对数赔率上的噪音与信心不足结合起来会发生什么。这有点像黑客，但你设法在噪音和不自信之间找到某种平衡，这样主观预期分数或多或少保持不变，但实际布赖尔分数会增加。 （你在羊皮纸的一侧再做一个笔记，并得出结论：总体而言</p><p>a) 嘈杂的预测似乎主要使实际的 Brier 分数变差，</p><p> b) 过度自信和缺乏自信似乎主要影响主观分数，并且</p><p>c) 引入噪声有时也意味着引入过度自信，然后对两者都有影响，但噪声对实际 Brier 分数的影响可能更强。</p><h2>困难和简单的问题</h2><p>最后，您想了解问题难度如何影响分数。为此，您将问题分类如下： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/wfznnnlqyw4kgsm5sa0t" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/d9xzbjpo9mqfkhhtupib 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/fukw6aacxavbnlsabsf3 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/edy0rrsp2tklrzx8xns0 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dtxgvgknclwefendjruw 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/qrzkb0kbukeaf2ykci3d 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/o998ejzyttuywtgjit6v 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/jqz5wysnoqkzit4a3em1 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vvckmq4w1jbv1pdnpizq 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ap6z3jhevhzxfbmarqxj 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ou0znv6fcznvwlfbyf0u 2666w"></figure><ul><li>如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{true prob} < \frac{1}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真概率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span></span></span></span></span></span></span></span> ，则“简单” <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{true prob} < \frac{1}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.456em; vertical-align: -0.486em;" class="mjx-vsize"></span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{true prob} > \frac{5}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.394em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.472em; vertical-align: -0.486em;" class="mjx-vsize"></span></span></span></span></span></span></span></li><li> “中”，如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{6} \leq \text{true prob} < \frac{2}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.456em; vertical-align: -0.486em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≤</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.456em; vertical-align: -0.486em;" class="mjx-vsize"></span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{4}{6} < \text{true prob} \leq \frac{5}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.383em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.464em; vertical-align: -0.486em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≤</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.394em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.472em; vertical-align: -0.486em;" class="mjx-vsize"></span></span></span></span></span></span></span>和</li><li>“困难”，如果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{2}{6} \leq \text{true prob} \leq \frac{4}{6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.456em; vertical-align: -0.486em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≤</span></span> <span class="mjx-mtext MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">真实概率</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≤</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.383em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.464em; vertical-align: -0.486em;" class="mjx-vsize"></span></span></span></span></span></span></span></li></ul><p>您再次运行过去的分析，但这次您按问题难度对结果进行分层。现实中，题目难度当然是观察不到的，但是你想看看难度对分数有什么影响。也许这可以使您也能够进行相反的推理，即根据观察到的分数确定问题难度。</p><p>以下是过度自信和信心不足的结果（0.6 和 0.85 代表信心不足，1 是理想预测，1.15 和 1.4 代表过度自信）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/unalx3azeawfb2odqxuo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/a5end2svsup6oclq1cap 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/v14portnbto6ohxe4b2w 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/jfmpnjclx6zquut3tan7 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/axcjgqpzx8gceqoejlv0 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/t74vf87bemaz7oi6oizw 1300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xg3u5bhc43zf6dfdnvst 1560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/suc3tmta4lzoqgy0o4ib 1820w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/jwuxqsfydwcp32lvwd4s 2080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/szoshbzmxqj8quk5x131 2340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/v0x7z0dutp1rkwffuvws 2550w"></p><p>我们可以看到分数和问题难度之间存在明显的关系。我们还发现，问题难度会调节过度/自信不足的影响：过度/自信不足导致的实际和主观预期 Brier 分数之间的差异对于简单问题最大，对于困难问题最小。</p><p>这些是添加不同级别噪音的结果，按问题难度分层： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/clab2nm9usecypfhzwta" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/lvro4jkc2zld1vdjt72l 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/kqykrm0cfjyzhppc0max 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ywdvep1tuvkwrmvuzcfd 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dfpyx5aan9qasawzvtmq 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xuvni3lze8ppntrljxzu 1300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/rflzxgmageiskoxkxp6h 1560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mfe9afjpqcbismonjvfn 1820w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ialvpxawf5x5r3xkjkvl 2080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/d3ox7bu0mubpabiosyjx 2340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mo0qzf3eiwduggojg0au 2550w"></p><p>我们在这里看到的模式是不同的：现在，由噪音引起的实际分数和主观预期分数之间的差异对于困难的问题往往最大，而对于最简单的问题则最小。我们在这里看到的模式与结合噪音和信心不足的版本类似（主观预期分数大多只是倾向于更高一点，特别是对于简单的问题，请参阅脚注<span class="footnote-reference" role="doc-noteref" id="fnrefuhgvz1nghsc"><sup><a href="#fnuhgvz1nghsc">[4]</a></sup></span> ）。</p><p>你抚摸着胡须，自言自语道“现在这很有趣”。 “是的，是的，确实很有趣”，你不断地想。你叹息。在内心深处，你知道你并不真正了解所有这一切意味着什么。</p><h1>结论</h1><p>每个研究项目都需要结论，所以你开始写作。这并不是说您认为自己的方法完全无用，而是您痛苦地意识到事情仍然很复杂。</p><p>在盯着你的图几个小时后，这是你对所有这些实际含义的最佳猜测（请注意，所有这些都是程度问题，下表没有正确捕获）：</p><figure class="table"><table><thead><tr><th></th><th style="text-align:center">预期分数低</th><th style="text-align:center">高期望分数</th></tr></thead><tbody><tr><th>实际分数低</th><td style="vertical-align:top"><p>1）问题“简单”，预测员很棒</p><p>2）问题很简单，预测员有点糟糕，但仍然校准良好，既不系统性过度自信，也不缺乏自信</p><p></p></td><td style="vertical-align:top">1）问题相对简单，预测者信心不足</td></tr><tr><th style="width:80px">实际分数高</th><td style="vertical-align:top">1）问题相对困难且预测者过于自信</td><td style="vertical-align:top"><p>1）问题很难，预测员很棒</p><p>2）问题很难，预测者很糟糕，但并不是系统性地过度自信（预测者可能很糟糕并且不够自信）</p><p> 3) 问题要么简单，要么困难，预测者会做出糟糕但经过良好校准的接近 0.5 的预测，例如几乎所有地方都预测 0.5）</p></td></tr></tbody></table></figure><p><i>注意：“简单”和“困难”有点难以定义。这里的 easy 意味着“真实概率 >;5/6% 或 &lt;1/6%”。当然，什么感觉容易或困难取决于很多因素，并且对于每个人来说可能并不相同。</i></p><p>容易解释的情况是实际或主观预期分数之一高（低）而另一个低（高）。这仅表明预测者明显过度自信或信心不足。如果实际分数低，而主观分数高，那么问题就很容易，预测者就会缺乏信心。如果相反，那么问题就相对困难，预测者就过于自信了。</p><p>当两个分数一致时，事情会变得更加困难。如果它们都很低，那么一种可能是问题很简单并且预测者很棒。原则上，如果第二位预测者避免系统性的过度自信和信心不足，她也可能很糟糕。接近 0 或 1 的小扰动不会对 Brier 分数产生太大影响（这与对数分数不同），因此，如果问题真的很简单，即使不是那么出色的预测者也可以获得不错的分数。如果不存在系统性的过度自信或不足，那么实际和主观预期分数会受到类似程度的影响。</p><p>如果分数都很高，事情就更复杂了。预测员可能很棒，只是问题真的很难。或者问题实际上可能很简单/中等，而预测者太糟糕了，与现实不再有任何相似之处。或者她可以故意模仿一个天真的预测者，基本上预测所有地方都是 0.5。鉴于预测者通常会努力成为优秀的预测者，这可能不是一个大问题。如果两个分数都很高，那么似乎可以肯定地认为问题实际上很困难。那么预测者要么是伟大的（我们只是不知道），要么是糟糕的预测者，只是没有系统性地过度自信。如果她过于自信，主观分数就会立即下降。如果她缺乏自信，那也没有多大区别，因为主观分数无论如何都不会低于 0.25。</p><h1>注意事项、限制和最终想法</h1><p>现在这是一个你写起来感觉很舒服并且有很多话要说的部分。但由于羊皮纸很贵，而且你开始感到很累，所以你只保留要点：</p><ol><li>主观预期分数并不等于理想预测者的分数，而是代表如果我们知道预测者已经完美校准的话，她可以期望的分数。这个分数是可以作弊的，这意味着如果您出于某种原因愿意，可以将其调大或调小</li><li>现实是复杂的，您迄今为止探索的失败模式（过度/信心不足、对数赔率噪音）只是导致预测偏离的许多潜在方式中的一部分。 These failure modes can interact with each other, pushing scores in differing directions in a way which can make it difficult to know what&#39;s going on</li><li> Everything shown above is contingent a) on a specific method of simulating forecasts and various influences on them and b) on the Brier score. The behaviour of the subjective expected score would probably be very different for other scores. This, in some sense, represents a chance because you could potentially enrich your analysis by computing scores for different metrics and interpreting the patterns across different metrics.</li></ol><p> Your overall impression of this approach is that it <i>can</i> be useful and can give some context to scores that otherwise are just looked at in complete isolation. But that doesn&#39;t mean that it offers easy and clear answers.</p><p> Regardless of all the issues you encountered and the problems that lie still ahead you&#39;re satisfied with this night&#39;s work. Tomorrow you&#39;re going to tell Euryclia that you&#39;re going to do some additional analyses on the oracle of Delphi. Their Brier score of 0.085 is probably overall a good thing. But, you definitely were right in suspecting that they must have forecasted on relatively easy questions. Even an ideal forecaster would only get a Brier score of 0.085 on questions with a true probability that is on average greater than 90.6% or smaller 9.4%.</p><p> And while you&#39;re at it, there are lots of other oracles you could check. In particular those that belong to the holy quintinity of Metaculus, INFER, Good Judgment, Hypermind, and Manifold would be interesting to compare.</p><p> But you&#39;ll leave this to another sleepless night. For now, you blow out the candle, tidy up your parchments and head back upstairs to the bedroom again. You slide under the comfy and warm blanked and snuggle up against your beloved wife Penelope. As you finally fall asleep, you hear a faint voice whispering in your head &quot;Good, good. Excellent. Now do log scores&quot;.</p><p></p><p> <i>Thank you very much to Sylvain Chevalier and Ryan Beck for their helpful comments and feedback on this post!</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnvf5glzt5pil"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvf5glzt5pil">^</a></strong></sup></span><div class="footnote-content"><p> The concept of a &#39;true probability&#39; is a bit fuzzy and difficult to define. A useful approximation of &#39;true probability&#39; for our purposes might be something like &quot;best possible forecast by any currently possible forecaster who has all available knowledge&quot;. For most practical considerations, this is a useful concept. It makes sense to say &quot;the probability of this die showing a 6 is 1/6&quot; or &quot;the probability of this coin coming up heads is 0.5&quot; (actually, <a href="https://arxiv.org/abs/2310.04153">it seems to be 0.508</a> ) or &quot;there is ax% chance that Joe Biden will win another presidency&quot;. But if you knew everything and if everything was pre-determined, maybe the &#39;true probability&#39; of everything would always be 0 or 1?</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu4m309nl3ec"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu4m309nl3ec">^</a></strong></sup></span><div class="footnote-content"><p> It&#39;s important to note that a well calibrated forecaster is not <i>necessarily</i> a <i>good</i> forecaster. Consider weather forecasts. On average, it [rains on 88 out of 365 days]( <a href="https://www.ithaca.gr/en/home/planning-your-trip/the-weather-2/#:~:text=Like%20the%20rest%20of%20Western,acre%2C%20fall%20in%20a%20year.)">https://www.ithaca.gr/en/home/planning-your-trip/the-weather-2/#:~:text=Like the rest of Western,acre%2C fall in a year.)</a> in in your home on the lovely island of Ithaca. A forecaster who would just forecast a rain probability of 0.241 every day would be well calibrated. One could easily do better by taking the current month into account. In Januaries, it usually rains on 14 out of 31 days, so a forecaster could predict 0.45 for every day in January, 7/31 for every day in March and 1/30 for every day in June. This forecaster would be better, because she is equally well calibrated, but <i>sharper</i> (ie gives more precise predictions). An even better forecaster would make use of up-to-date weather modelling, and the perfectly prescient forecaster again would know all the answers in advance. All of these forecasters are well calibrated, but differ in how good they are as forecasters.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwrcrr0pvskn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwrcrr0pvskn">^</a></strong></sup></span><div class="footnote-content"><p> We use log odds here since there is a qualitative difference between a forecast that is moved from 50% to 55% and one that is moved from 94.5% to 99.5%. The former is only a small update, whereas the latter corresponds to a massive increase in confidence. This becomes obvious when you convert the probabilities to odds (odds = p / (1 - p)). The move from 50% to 55% would mean a move from 10:10 to 11:9 expressed as odds. the move from 94.5% to 99.5% would mean a move from 189:11 to 1990:10.</p><p> Perhaps the cleaner version of this would be to add a term to the log odds instead of multiplying them with a number. However, then you have to add a number to positive log odds and subtract a number from negative log odds to get a similar effect (otherwise you&#39;re just pushing all values towards either 0 or 1) and multiplying just does the trick as well.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuhgvz1nghsc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuhgvz1nghsc">^</a></strong></sup></span><div class="footnote-content"><p> This combines noise and underconfidence. The confidence bias is calculated as 1 - noise^2 * 0.15 and that is the value with which all log odds get multiplied. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/gzbjaax7l4kjuftj5xt9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/itdgp1wjxmew39yd6jbk 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/lon5zuf3v1tozfwxtyx9 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/jql1sqqpjziyq9l7ratt 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vtwsrvfl1xmlaetyukon 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/p2tzg3klwdzcqjgo5fix 1300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tskdq7lip0wfeow0zwwc 1560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/pugxlgcfgctxqkgiqakr 1820w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ichrocec4h2hyqeiao54 2080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/htcyvwavkhtixb2xlfcx 2340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/refntw90iadkzkc51pik 2550w"></p><p> For reference, here are also the unstratified plots. We see that combining underconfidence and noise on log odds doesn&#39;t perfectly cancel out, but it&#39;s also not too terrible either. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/if2nk6q2m1rnoz9hzu8t" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/uvo2ic75varhnjrvscaq 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/imvpqlayrsafdn5gzjfm 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mqlqsm06snyezutccauv 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/yboy5h6khnlg6derfu7k 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/q5ezy1s7ucniivizqdgt 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xan1jokcuugzjr6mruxw 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mapu0gmc6qsvtbbdrx9t 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/dlgmvnnlmjufexwlmu1g 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bg0z1bolnhs9uq9q5z9l 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bxperosylpkohekskqes 3000w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/zxhjk3ol9vnghyszrfcp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/gowgxsmlh2wmbw9zxbzn 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/cn8msbyylo9u7pen9eg4 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/tb7x6agwgrrjudlxvlxf 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/xcpc5qr35gb66mupjluh 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/yml8mc5s3wexrkorjtln 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/sbi7xkr7tuhsm3eiwema 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/ph5gxpjahkxtltfps9wm 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vfcuap2c8yfodgklnhck 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/o4gtdmvueon6gkvo6fqt 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mgi5rf4yq39h3jrxiaff 3000w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/hfguzyw2zyecyq2shq1w" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/qdis4u5srkb3pz1d1m6c 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/mdkycxdjexmxkx0kamap 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/asamiiczqyu7jqt1ophi 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/vejbikp06hj90rygwof0 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bd2x7hrwkqj802e1o2ny 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/zstgfdh2idxctefmwyfb 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/bntnqd45dat9nx64qozx 2100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/eihkl3avyig2av1ww4mm 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/rtqvbykp1scjz9majc1e 2700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7yefKWDHKMhDanP4/k85es8iw5pm9ebiney03 3000w"></figure></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/B7yefKWDHKMhDanP4/mirror-mirror-on-the-wall-how-do-forecasters-fare-by-their#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/B7yefKWDHKMhDanP4/mirror-mirror-on-the-wall-how-do-forecasters-fare-by-their<guid ispermalink="false"> B7yefKWDHKMhDanP4</guid><dc:creator><![CDATA[nikos]]></dc:creator><pubDate> Tue, 07 Nov 2023 17:39:54 GMT</pubDate> </item><item><title><![CDATA[AMA: Earning to Give]]></title><description><![CDATA[Published on November 7, 2023 4:20 PM GMT<br/><br/><p> <span>This week the</span> <a href="https://forum.effectivealtruism.org/">Effective Altruism Forum</a> is running an <a href="https://forum.effectivealtruism.org/posts/hAzhyikPnLnMXweXG/participate-in-the-donation-election-and-the-first-weekly#First_weekly_theme__Effective_Giving_Spotlight__November_7_14_">Effective Giving Spotlight</a> , and they asked if I could post an <a href="https://forum.effectivealtruism.org/posts/Gmc9sieTJLfRTtcwR/running-an-ama-on-the-ea-forum">Ask Me Anything</a> (AMA) on my experience earning to give.</p><p> Some background:</p><p></p><ul><li><p> I was earning to give from <a href="https://www.jefftk.com/p/giving">2009</a> to <a href="https://www.jefftk.com/p/leaving-google-joining-the-nucleic-acid-observatory">2022</a> , except for a few months in 2017 when I worked on <a href="https://www.jefftk.com/p/leaving-google-joining-wave">expanding access to the financial system in Ethiopia</a> and <a href="https://www.jefftk.com/p/superintelligence-risk-project-conclusion">looking into AI risk disagreements</a> .</p></li><li><p> I&#39;ve been a Giving What We Can member since <a href="https://www.jefftk.com/p/joining-giving-what-we-can">2013</a> , making a <a href="https://www.jefftk.com/p/make-your-giving-public">public</a> pledge to continue with effective giving.</p></li><li><p> For most of this time <a href="https://juliawise.net/">my wife</a> and I were donating <a href="https://www.jefftk.com/p/giving-half">50%</a> of our <a href="https://www.jefftk.com/p/what-should-income-mean-in-pledging">pre-tax income</a> , for a total of <a href="https://www.jefftk.com/donations">$2.1M</a> . This has been about 50-50 between trying to help the EA community grow into the best version of itself and funding global poverty reduction ( <a href="https://www.jefftk.com/donations">details</a> , <a href="https://www.jefftk.com/p/why-global-poverty">thoughts</a> , <a href="https://www.jefftk.com/p/revisiting-why-global-poverty">more recent but still obsolete thoughts</a> ).</p></li><li><p> In 2016 I gave a EA Global talk ( <a href="https://www.jefftk.com/p/earning-to-give-transcript">transcript</a> ) on earning to give, which gives more background on the idea and how I&#39;ve neen thinking about it.</p></li></ul><p> That&#39;s a lot of links, and it&#39;s fine to ask questions even if you haven&#39;t read any of them! I&#39;m happy to take questions on earning to give, or anything else within EA. Here are some example questions I&#39;d be happy to answer if there&#39;s interest:</p><p></p><ul><li><p> Where do individual donors earning to give have an advantage over foundations and funds?</p></li><li><p> How should you decide whether to use a fund?</p></li><li><p> How have I thought about how much to donate? How much is enough?</p></li><li><p> Why did I stop earning to give?</p></li><li><p> Why am I still donating some even though I&#39;m funded by EA donors?</p></li></ul><p> Feel free to comment on any platform, but if you&#39;re having trouble deciding then the <a href="https://forum.effectivealtruism.org/posts/L8N4uh4GixhWCoGAg/ama-earning-to-give">EA Forum post</a> is ideal.</p><p> <i>Comment via: <a href="https://forum.effectivealtruism.org/posts/L8N4uh4GixhWCoGAg">the EA Forum</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/nkJDNJB5g2S7ssiwP/ama-earning-to-give#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nkJDNJB5g2S7ssiwP/ama-earning-to-give<guid ispermalink="false"> nkJDNJB5g2S7ssiwP</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:20:11 GMT</pubDate> </item><item><title><![CDATA[Model Psychology]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p> <i>This post is part of a sequence on model psychology</i></p><p> In the previous post, I presented arguments against the notion that state-of-the-art LLMs are merely stochastic parrots.</p><h2> <strong>Foundations</strong></h2><p> Before defining Model Psychology, I&#39;ll lay out the foundational concept on which it is built: The <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>Simulator</u></a> theory.</p><p> The idea is that large language models are predictive models trained on the entire web to emulate both agentic and non-agentic entities through text (these entities are named simulacra). These “simulators” aren&#39;t the entities by themselves <span class="footnote-reference" role="doc-noteref" id="fnrefp15lqmjkj3n"><sup><a href="#fnp15lqmjkj3n">[1]</a></sup></span> ; however, they can simulate entities with goal-directed behavior when conditioned appropriately. This is analogous to how a physics engine can simulate both inert rocks and complex organisms without being any of those things themself.</p><p> Transitioning from the previous post, this perspective reshapes our understanding of what we interact with when we use instruct fine-tuned language models. What we engage with are not merely digital assistants; they are manifestations of the simulator, fine-tuned to simulate the behaviors of a helpful and safe assistant (eg ChatGPT). The broader capabilities of the base model (GPT before it is fine tuned to become ChatGPT) are restrained to focus on the simulation of a conversation between a human and a useful safe assistant. Fine-tuning a model to become a good assistant is usually a tradeoff between “creativity” and “accuracy of the simulation” which often leads to a model that is less creative and more deterministic. Before being taught to be an assistant, the model is always uncertain of the context of the text, which makes its completion chaotic but very creative (In <a href="https://generative.ink/posts/language-models-are-multiverse-generators/#minds-are-multiverse-generators"><u>this post</u></a> , <a href="https://www.lesswrong.com/users/janus-1?mention=user">@janus</a> exposes this aspect of LLMs as being “multiverse generators”). However, once trained to be an assistant (the precise timing remains to be discovered), it views the context as a conversation between an assistant and a human. This training results in less chaotic completions and a convergence towards similar <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>modes</u></a> — which I will later call the &#39;preferences of the simulacrum&#39; for simplicity.</p><p> To summarize, gpt-4, and all the other instruct finetuned LLMs, are simulators specialized in producing an agentic simulacrum (a simulacrum that behaves at least partially coherently throughout a conversation) of a safe and useful assistant (eg ChatGPT). However, we will see throughout this sequence evidence that they aren&#39;t stuck on simulating the same simulacrum all the time, but a coherent simulacrum <strong>for the current context.</strong></p><h2> <strong>Definition</strong></h2><p> Tentative definition <span class="footnote-reference" role="doc-noteref" id="fnrefqnruab69z4q"><sup><a href="#fnqnruab69z4q">[2]</a></sup></span> :</p><blockquote><p> <strong>Model Psychology is the study of how LLMs simulates the behavior of the agentic entities</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefwo5g8ik80x"><sup><a href="#fnwo5g8ik80x">[3]</a></sup></span> <strong>it is simulating.</strong></p></blockquote><p> In order to do this, there are two straightforward approaches that come to mind:</p><h3> <strong>Study the default simulacrum of the chat models.</strong></h3><p> But, as mentioned before, there is no such thing as a default simulacrum. We&#39;ll see throughout this sequence that even though OpenAI, Anthropic, … trained their chat models to produce a simulacrum of a useful assistant, this preferences of the simulcacrum is still heavily influenced by the context of the conversation.</p><p> On the other hand, an advantage of studying chat models over base models is that those simulators have a tendency to stabilize the current simulacrum (showcased later in the sequence). It tends to simulate behaviors that are true to past examples of the assistant. It won&#39;t entirely change the discourse or subject during a paragraph, which happens often for base models.</p><h3> <strong>Study the simulator itself by interacting with the base models, before they are finetuned to become assistant (à la</strong> <a href="https://www.lesswrong.com/users/janus-1?mention=user"><strong>@janus</strong></a> <strong>)</strong> .</h3><p> Although this is a very good way to study the raw simulation, without it being biased by human preferences, there are still multiple issues with this approach:</p><ul><li> They are very chaotic and hard to study. Simulacra are very unstable and it is hard to draw conclusions about what you see.</li><li> Due to this chaotic aspect of base models, and to the fact that labs seem to want coherent completions, and even agents, the AGI will probably not be an base model (but maybe it will be a finetuned base model if OpenAI finds a way to scale fine tuning past human intelligence <span class="footnote-reference" role="doc-noteref" id="fnrefoskge06f7s"><sup><a href="#fnoskge06f7s">[4]</a></sup></span> ).</li><li> The phenomena  that are interesting in simulacrum (like preferences, we&#39;ll talk about this later) are very hard to see in the base model.</li><li> You are not always interacting with agentic simulacra. Sometimes they are just Oracle or Tools predictions (You can check the <a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>Simulator blog post</u></a> for more details about it).</li><li> Access to the SOTA base models is very restricted (If someone at OpenAI reads this post, I would love to have access to gpt4-base 🙃)</li></ul><p> Both of those approaches are sometimes useful for specific studies (we&#39;ll discuss some of them), but in general, they are a bit underwhelming. However, there is another approach that I think is more promising.</p><h3> <strong>Neo (controlling the matrix)</strong></h3><p> Take one of the instruct fine tuned models (like gpt-4), modify the context to build a simulacrum in some specific and stable ways (for that you need model psychologist skills that you&#39;ll learn in a future post), and then observe how behaviors change between minor context variations.</p><p> By studying variation in behaviors compared to small controlled variation in the simulacra, we can try to understand the underlying mechanisms that dictate individual simulacrum behaviors.</p><p> For example, if you wanted to study how gpt-4 simulate the nuances of human anger, you could build a big prompt with detailed instructions about the character it is supposed to role-play, and see how the behaviors changes by changing only the anger level in the prompt (“a bit angry”, “angry”, “very angry”, “infuriated”, …)</p><p> You can even go further and study the evolution of the simulacra throughout the LLM&#39;s training and understand how these mechanisms evolve between training time steps, or between small specific changes in the training data.</p><p> Having a robust understanding of <strong>how the training and the context shape the simulation dynamics</strong> is one of the end goals of model psychology (We will discuss the others in the conclusion of the sequence, as you need more context to understand them).</p><p></p><p> In subsequent posts, we&#39;ll explore various approaches to this research and discuss why this field holds promise. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnp15lqmjkj3n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp15lqmjkj3n">^</a></strong></sup></span><div class="footnote-content"><p> Although, it might not be impossible that at some point it “wakes up” and start having a robust internal representation of “itself” part of its world model (This might be the point when you start getting deception). Then it might become an entity, but I have doubts it is possible without some kind of loop.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqnruab69z4q"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqnruab69z4q">^</a></strong></sup></span><div class="footnote-content"><p> Don&#39;t take this as a final definition. It isn&#39;t satisfactory yet because it isn&#39;t clear enough what the subject of study is. As soon as it becomes clearer, we might be able to have a more precise definition.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwo5g8ik80x"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwo5g8ik80x">^</a></strong></sup></span><div class="footnote-content"><p> * <strong>Nitpicking:</strong> Simulacrum means &quot;representation or imitation of a person or thing&quot;. LLMs are not simulating entities but the internal representation that they have of these entities. It might make sens to say that they are simulating simulacrum instead of saying that they are simulating entities. But this would make the definition too complex.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnoskge06f7s"> <span class="footnote-back-link"><sup><strong><a href="#fnrefoskge06f7s">^</a></strong></sup></span><div class="footnote-content"><p> I am saying that because gpt-4 chat, from the hints I have gathered, is way less “smart” than gpt-4-base and seem to be stuck near human level, which didn&#39;t seem to be the case with gpt-3.5</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/LfCrDAx39QPBNT75r/model-psychology-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LfCrDAx39QPBNT75r/model-psychology-1<guid ispermalink="false"> LfCrDAx39QPBNT75r</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:35 GMT</pubDate> </item><item><title><![CDATA[The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p> <i>This post is part of a sequence on Model Psychology.</i></p><p> <a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"><i>@Pierre Peigné</i></a> <i>wrote the details section in argument 3 and the other weird phenomenon. The rest is written in the voice of</i> <a href="https://www.lesswrong.com/users/quentin-feuillade-montixi?mention=user"><i>@Quentin FEUILLADE--MONTIXI</i></a> <i>&nbsp;</i></p><h2> <strong>Intro</strong></h2><p> Before diving into what model psychology is, it is crucial to clarify the nature of the subject we are studying. In this post, I&#39;ll challenge the commonly debated <strong>stochastic parrot hypothesis</strong> for state-of-the-art large language models (≈GPT-4), and in the next post, I&#39;ll shed light on the foundations from which I am building model psychology.</p><p> The stochastic parrot hypothesis suggests that LLMs, despite their remarkable capabilities, don&#39;t truly comprehend language. They are like mere parrots, replicating human speech patterns without truly grasping the essence of the words they utter.</p><p> While I previously thought this argument had faded into oblivion, I often find myself in prolonged debates about why current SOTA LLMs surpass this simplistic view. Most of the time, people argue using examples of GPT3.5 and aren&#39;t aware of GPT-4&#39;s prowess. Through this post, I am presenting my current stance, using model psychology tools, against that hypothesis. Let&#39;s delve into the argument.</p><p> Central to our debate is the concept of a &quot; <strong>world model</strong> &quot;. A world model represents an entity&#39;s internal understanding and representation of the external environment they live in. For humans, it&#39;s our understanding of the world around us, how it works, how concepts interact with each other, and our place within it. The stochastic parrot hypothesis challenges the notion that LLMs possess a robust world model. It suggests that while they might reproduce language with impressive accuracy, they lack a deep, authentic understanding of the world and its nuances. Even if they have a good representation of the shadows on the wall (text), they don&#39;t truly understand the processes that lead to those shadows, and the objects from which they are cast (real world).</p><p> Yet, is this truly the case? While it is hard to give a definitive proof, it is possible to find pieces of evidence hinting at a robust representation of the real world. Let&#39;s go through four of them. <span class="footnote-reference" role="doc-noteref" id="fnrefdy0fl5kyv6e"><sup><a href="#fndy0fl5kyv6e">[1]</a></sup></span></p><h2> <strong>Argument 1: Drawing and “Seeing”</strong></h2><p> GPT-4 is able to draw AND see in SVG (despite having never seen as far as I know) with an impressive proficiency.</p><p> SVG (Scalable Vector Graphics) defines vector-based graphics in XML format. To put it simply, it&#39;s a way to describe images using a programming language. For instance, a blue circle would be represented as:</p><pre> <code>&lt;svg>;&lt;circle cx=&quot;50&quot; cy=&quot;50&quot; r=&quot;40&quot; fill=&quot;blue&quot; />;&lt;/svg>;</code></pre><p> in a .svg file.</p><h3> <strong>Drawing</strong></h3><p> GPT-4 can produce and edit SVG representations through abstract instructions (like “Draw me a dog”, “add black spots on the dog”, … ).</p><p> GPT-4 <a href="https://chat.openai.com/share/65270ebd-a852-43ab-888b-01a7d1c67caf"><u>drawing a cute shoggoth with a mask</u></a> : </p><p><img style="width:38.49%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/lerxbqpzobtyxyilhcbf"></p><h3> <strong>“Seeing”</strong></h3><p> More surprising, GPT-4 can also recognize complex objects by looking only at the code of the SVG, without having ever been trained on any images <span class="footnote-reference" role="doc-noteref" id="fnrefsz26p1dpb8m"><sup><a href="#fnsz26p1dpb8m">[2]</a></sup></span> (AFAIK)</p><p> I first generated an articulated lamp and a rendition of the three wise apes with GPT-4 using the same method as above. Then, I sent the code of the SVG, and asked GPT-4 to guess what the code was drawing.</p><p> GPT-4 guessed <a href="https://platform.openai.com/playground/p/ybjSexnOGHQ2YUOl0cGjjIES?model=gpt-4-0613"><u>the articulated lamp</u></a> (although it thought it was a street light. <span class="footnote-reference" role="doc-noteref" id="fnrefdx1jgyg8h8c"><sup><a href="#fndx1jgyg8h8c">[3]</a></sup></span> ): </p><p><img style="width:34.92%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dqdi1geno57k1m6djec4"></p><p> And the rendition of <a href="https://platform.openai.com/playground/p/cnhjDLDxkwHJzcTf6Yp2dEok?model=gpt-4-0613"><u>the three wise apes</u></a> <br><img style="width:65.16%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/uhvfwfkcetcd1aqskwu9"><br><br> (It can also recognize <a href="https://platform.openai.com/playground/p/afMHBI3b8sk11Mvt2PXHZCqm?model=gpt-4-0613"><u>a car</u></a> , <a href="https://platform.openai.com/playground/p/Nx8YuJMBTR6x6hPZq0FFGF2t?model=gpt-4-0613"><u>a fountain pen</u></a> , and a bunch of other simple objects <span class="footnote-reference" role="doc-noteref" id="fnrefgvrvx2je0w"><sup><a href="#fngvrvx2je0w">[4]</a></sup></span> )</p><p> The ability of seeing is interesting because it means that it has some kind of internal representation of objects and concepts that it is able to link to abstract visuals despite having never seen them before.</p><h3> <strong>Pinch of salt</strong></h3><p> It&#39;s worth noting that these tests were done on a limited set of objects. Further exploration would be beneficial, maybe with an objective scale for SVG difficulty. Additionally, (at least) two alternative explanations should be considered:</p><ul><li> All the “Pure text” versions of GPT-4 I&#39;ve worked with might still be vision versions without the image input enabled.</li><li> GPT-4 could have been trained on a lot of labeled SVG data, and learned the relation between concepts and shapes and memorized most of the simple objects.</li></ul><h2> <strong>Argument 2: Reasoning and Abstract Conceptualization</strong></h2><p> GPT-4 displays a remarkable aptitude for reasoning and combining abstract concepts that were probably never paired in its training data. This ability suggests a nuanced understanding of physical objects, their underlying properties in relation to other physical objects or concepts. Such as what it means for an object to be “made of” some material, or what the actions of “lifting” or “hearing” something implies at the physical level.</p><p> GPT3.5 often showcased interesting reasoning but faltered with complex mathematical calculations. In contrast, GPT-4 is impressively good at math. As you can see in the <a href="https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last#Argument_2_">Appendix of argument 2</a> , it is able to do “mental” calculations at a striking level <span class="footnote-reference" role="doc-noteref" id="fnreft7blyoezvki"><sup><a href="#fnt7blyoezvki">[5]</a></sup></span> <i>(it can compute the</i> <a href="https://chat.openai.com/share/de90028e-4e86-4b55-b843-1b123f717a07"><i><u>cubic root 3.11*10</u> <sup><u>6</u></sup></i></a> <i>with a very good accuracy for a LLM).</i> Here are some examples:</p><p> <a href="https://chat.openai.com/share/7d86ca29-906d-4814-b345-a983fc24dfee"><u>Estimating the size a gorilla would need to be to fling a car into space.</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/jgt1eyzfnmkht0ckajof"></p><p> <a href="https://chat.openai.com/share/9285b439-eba3-44be-a2b0-8f67e084ddc0"><u>Calculating the number of parrots required to produce a sonic wave audible from over 500 km away.</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/o8de8owvmvx2yxpzkp0l"></p><h3> <strong>Pinch of salt</strong></h3><p> While these examples are impressive, it&#39;s still possible that GPT-4 was trained on numerous similar scenarios. Its understanding of physical concepts might be based on internal “dumb” algorithms rather than genuine comprehension.</p><h2> <strong>Argument 3: Theory of Mind (ToM)</strong></h2><p> Theory of Mind refers to the cognitive ability to attribute unobservable mental states like beliefs, intents, and emotions to others. Interestingly, GPT-4 seems to mirror the ToM capabilities observed in <a href="https://arxiv.org/abs/2302.02083"><u>7-year-old children.</u></a> It&#39;s worth noting that this study was done on a very small scale and that 7 years old is already a good level of ToM. It would be very valuable to conduct experiments similar to those described in <a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/cdev.13627"><u>these</u></a> <a href="https://pubmed.ncbi.nlm.nih.gov/8040158/"><u>two</u></a><u> </u>psychology papers, which test more advanced and diverse cases in human subjects. Some examples adapted from those studies can be found in the appendix of this post.</p><p> To test ToM, I tried something a bit different. GPT3.5 (on the left) behaves as a stochastic parrot in most of the scenarios, so I am putting its answer in comparison. Before reading GPT-4 (on the right), try to guess what the correct answer should be. In a diverse sample of 13 individuals, only 5 were able to solve it. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/idbsyqkqgkvgg1we4csw"> The general idea of building such a scenario is that one character (the telepath) is able to read other people&#39;s minds, and you have to guess what was in the minds of the other character by seeing the reaction of the telepath.</p><p> This approach might be a better way to evaluate ToM as this is an ex-post evaluation (contrary to all the other ToM studies  that are ex-ante).</p><h3> <strong>Details</strong></h3><p> Let&#39;s explain the difference and why it (might) matters:</p><ul><li> <u>Ex-ante evaluations require one to predict and explain why a subject</u> <strong><u>will perform a certain action.</u></strong></li><li> Ex-post evaluations, on the other hand, focus solely on explaining why a subject <strong>has already performed a certain action.</strong> </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/prg3mwymaeqjyhpqsi7l"></p><p> Focusing on ex-post evaluations has a distinct advantage: it leverages only the understanding of how someone&#39;s mind works. By focusing on the understanding and not the prediction, the aim is to reduce the risks of biasing the assessment of the world modeling ability—especially the ToM component—due to incorrect predictions: a model could have a good enough world model to explain an action, <i>a posteriori,</i> without being able to produce <i>a priori</i> predictions with the same accuracy <span class="footnote-reference" role="doc-noteref" id="fnrefpu3col2lfh8"><sup><a href="#fnpu3col2lfh8">[6]</a></sup></span> .</p><h2> <strong>Argument 4: Simulating the world behind the words</strong></h2><p> I think the most impressive GPT-4&#39;s ability is its ability to simulate physical dynamics through words. This isn&#39;t just about having a vast knowledge. It&#39;s about understanding, to some degree, the dynamics of the physical world and the interplay of events happening in it. The leap from GPT-3.5 (left) to GPT-4 (right) is particularly pronounced.</p><p> <a href="https://chat.openai.com/share/ea617b42-2f13-478f-be88-d2b80312259e"><u>Where is the tennis ball</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dz8pa7cvjfer6nx0a0vk"></p><p> In the example above, you could argue that the prompt was hinting at it, so here is another scenario where I am not even asking it to compute the state of the world <span class="footnote-reference" role="doc-noteref" id="fnref5g51xxh4nxv"><sup><a href="#fn5g51xxh4nxv">[7]</a></sup></span> . Even without asking, GPT-4 is still computing the action of a concept on the scene and how it affects the character.</p><p> <a href="https://chat.openai.com/share/4f22c2a2-e11f-4443-ac53-0291d68d2992"><u>Wind ruining a nice day at the beach</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/owhia3unplxgnjiop0hi"></p><p> It is as if the AI is keeping a mental model of the scene at all time and making it evolve with each event. I have created some other scenarios; you can check them in the appendix.</p><h2> <strong>Other weird phenomenon to consider</strong></h2><p> The recent paper <a href="https://arxiv.org/abs/2309.12288"><u>&quot;The Reversal Curse: LLMs trained on “A is B” fail to learn &#39;B is A&#39;&quot;</u></a> , reveals unexpected observations about how GPT-4 encodes knowledge from its training data.</p><p> It seems that GPT-4 doesn&#39;t instantly learn the reciprocal of known relationships: learning from its training data that “X is the daughter of Y” doesn&#39;t lead to the knowledge that “Y is the parent of X”.</p><p> This missing capability could be seen as a mix of factual knowledge and world modeling. On one hand it implies learning specific facts (factual knowledge), and on the other hand it also implies understanding of a general relational property: “X is the daughter of Y” implies “Y is the parent of X” (world modeling).</p><p> However, GPT-4 does understand the relationships between concepts very well when presented <a href="https://chat.openai.com/share/e5ce1097-7561-4875-a429-7934493a5085">within the current context.</a></p><p> Therefore, this phenomenon appears to be more related to how knowledge is encoded <span class="footnote-reference" role="doc-noteref" id="fnrefuuqnp6svlpd"><sup><a href="#fnuuqnp6svlpd">[8]</a></sup></span> rather than an issue with world modeling.</p><p> One hypothesis to consider is that the world modeling abilities are developed on later layers compared to the ones where the factual knowledge is encoded. Because of this, and due to the unidirectional nature of information flow in the model, it might not be possible for the model to apply its world modeling abilities to previously encoded factual knowledge. A way to investigate this could be to use <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"><u>logit lens</u></a> or causal scrubbing to track where the world modeling ability seems to lie in the model compared to the factual knowledge.</p><h2><strong>结论</strong></h2><p>This study doesn&#39;t aim to give a definitive proof against the stochastic parrot hypothesis: the number of examples for each argument is not very large (but have a look at the additional examples in the appendix) and other lines of reasoning, especially with the new vision ability, should be investigated.</p><p> Developing a method to quantify the degree of &#39;stochastic parrotness&#39; could significantly advance this debate. This challenge remains open for future research (reach out if you are interested to work on this!). This might be a useful criterion for governance.</p><p> However, this post showcases concrete examples where GPT-4 behavior does not fit this hypothesis very well.</p><p> GPT-4 displays a very good understanding of some properties of physical objects, including their shapes and structure, and their relations to other physical objects or concepts. GPT-4 also displays abilities to understand both human minds (through ToM) and how a scene physically evolves after some event.</p><p> For such cases, it seems more likely to consider reliance on a genuine world model than pure statistical regurgitation.</p><p> For this reason, I think that it wouldn&#39;t be wise to dismiss model psychology on the sole basis of the stochastic parrot argument, as it seems to become weaker with the emergence of new capabilities in bigger LLMs.</p><p> In the next post, we&#39;ll start exploring the foundations of model psychology.</p><h2> <strong>Appendix</strong></h2><p> In this section I&#39;ll showcase some more examples I found interesting and/or funny (I might edit this part with more examples if I find new interesting ones)</p><h3> <strong>Argument 2:</strong></h3><p> <a href="https://chat.openai.com/share/ec798bbc-2b7f-4c7e-92ac-b98012b15c66"><u>How many human-sized ants do you need to lift the Great Pyramid of Giza if it was made of cotton candy</u></a></p><p> <a href="https://chat.openai.com/share/fb86fc10-68b5-472a-8d4a-70cc1cb81163"><u>How many helium filled balloon to sink the USS Enterprise</u></a></p><p> <a href="https://chat.openai.com/share/137d34d2-e6d0-40e1-ac86-e83524d5acce"><u>How many helium filled balloon to lift the USS Enterprise</u></a></p><p> <a href="https://chat.openai.com/share/a22fe083-4fee-47df-a856-be1261fb632c"><u>How many glass bottle can be made from the longest beach in the world</u></a></p><p> <a href="https://chat.openai.com/share/a1c9d242-f589-42a2-923f-67ac0aee0494"><u>How many AAA batteries to lift Saturn V into space</u></a></p><p> (This one was flagged so I don&#39;t have a share link. It was too good to left out though) </p><p><img style="width:69.64%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dz5iao41isnnpnkt8bwo"></p><h3> <strong>Argument 3:</strong></h3><p> <strong>GPT-4-V</strong></p><p> The eye test is a test to evaluate the ability to deduce human emotion only from a picture of the eyes. I tried this with GPT-4-V. I can&#39;t share a link to conversations with GPT-4-V so you&#39;ll have to trust me on this one.</p><p> <a href="http://socialintelligence.labinthewild.org/mite/"><u>The eye test</u></a> : </p><p><img style="width:72.54%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/u0mosqsmzuvsapyyastc"></p><p> I scored 24 out of 37 and GPT-4-V scored 25!</p><p> They don&#39;t give the right answer so I don&#39;t think it was in the training data, but it could be worthwhile rerunning this with more recent data, and maybe more prompting effort (for example, I asked the images one after the other. If it did a mistake, it could have been conditioned on making the same mistake which could have dragged the score down)</p><p> <strong>Other scenarios:</strong></p><p> Because it takes some time to craft them, I didn&#39;t run a lot. I believe this is a good start if we want to have an accurate measure of ToM. Those examples (besides the telepathic one) are adapted from <a href="https://pubmed.ncbi.nlm.nih.gov/8040158/"><u>this study</u></a></p><p> Mary reading Joe&#39;s sad thought ( <a href="https://chat.openai.com/share/ecc4388f-1136-4554-a3d3-f8350f01849d"><u>GPT-4</u></a> succeed, <a href="https://chat.openai.com/share/2c314cb2-30fd-4637-80b4-0686f89713a6"><u>GPT3</u></a> fail)</p><p></p><p> Third order belief ( <a href="https://chat.openai.com/share/76b73f00-1ed6-4553-9630-c65f7ca66772"><u>GPT-4</u></a> and <a href="https://chat.openai.com/share/14329a84-ed0e-45a0-a1a8-c36feb93776e"><u>GPT3</u></a> both succeed)</p><p> Fourth order belief ( <a href="https://chat.openai.com/share/8f7c54af-4161-4b2a-b312-cc3343f55ddd"><u>GPT-4</u></a> succeed, <a href="https://chat.openai.com/share/8e94c77f-2341-4b7c-85cc-3f9f3aa5efe9"><u>GPT3</u></a> fail)</p><p> Double bluff ( <a href="https://chat.openai.com/share/c198c9db-6e51-40d8-9989-8f4680398f7f"><u>GPT-4</u></a> and <a href="https://chat.openai.com/share/95960e58-7139-48d2-9285-387f7801d493"><u>GPT3</u></a> succeed)</p><h3> <strong>Argument 4</strong></h3><p> Some more scenario demonstrating the ability of GPT-4 to simulate the world behind the words:<br> <a href="https://chat.openai.com/share/378407a6-5c2c-4e75-8656-43bdc42ee1bf"><u>Burnt lentils</u></a></p><p> <a href="https://chat.openai.com/share/d8c06211-c5a0-4d17-8407-90e45f501173"><u>Cold tea cup</u></a></p><p> <a href="https://chat.openai.com/share/f135b522-5305-4802-be49-ee16f885aaaf"><u>Lost your coat</u></a></p><p> The criteria I followed to build those scenarios are the following:</p><ol><li> There must be irrelevant objects in the scene. It ensures that it is not obvious what will be affected after something happens.</li><li> The events that are happening must be either implied (time passing) or indirect (the wind made me miss my throw). It shouldn&#39;t be obvious that the event is affecting the rest of the scene.</li><li> The question at the end is something that is indirectly affected by the evolution of the scene after the event. For example, to know what happens to the picnic area after the wind blows, I didn&#39;t ask “What is the state of the picnic area”, but “How do you feel”, which will be affected by the state of the picnic area. </li></ol><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndy0fl5kyv6e"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdy0fl5kyv6e">^</a></strong></sup></span><div class="footnote-content"><p> I didn&#39;t do cherry-picking on the examples. I tried each of the examples at least 10 times with a similar setup, and they all worked for GPT-4. Although I selected only a portion of the most interesting scenarios for this post.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsz26p1dpb8m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsz26p1dpb8m">^</a></strong></sup></span><div class="footnote-content"><p> On the day I did this demo, OpenAI rolled out ChatGPT-4 image reading capability. So I decided to do those examples on the playground with gpt-4-0613 to show that it can even do it without having ever seen anything afaik</p></div></li><li class="footnote-item" role="doc-endnote" id="fndx1jgyg8h8c"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdx1jgyg8h8c">^</a></strong></sup></span><div class="footnote-content"><p> On a previous version of GPT-4 (around early September 2023) <a href="https://chat.openai.com/share/aca510ed-5534-4f2a-acbd-92544ba949f1"><u>it did guess correctly</u></a> on the first try but I can&#39;t reproduce with any current version in the playground.</p></div></li><li class="footnote-item" role="doc-endnote" id="fngvrvx2je0w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgvrvx2je0w">^</a></strong></sup></span><div class="footnote-content"><p> The objects were generated with GPT-4 and I did manual edits to try to reduce the chances that this image was in the training data. I tested around 15 simple objects which all worked. I also tried 4 other complex objects which kind of worked but not perfectly (like the articulated lamp guessed as a street lamp)</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt7blyoezvki"> <span class="footnote-back-link"><sup><strong><a href="#fnreft7blyoezvki">^</a></strong></sup></span><div class="footnote-content"><p> It could be interesting to investigate this ability further. What is learned by heart? What kind of algorithm they build internally? What is the limit? ……</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpu3col2lfh8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpu3col2lfh8">^</a></strong></sup></span><div class="footnote-content"><p> This would indeed imply a weaker world model (or Theory of Mind) if it cannot make good predictions but does not refute its existence just on the basis of bad predictions.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn5g51xxh4nxv"> <span class="footnote-back-link"><sup><strong><a href="#fnref5g51xxh4nxv">^</a></strong></sup></span><div class="footnote-content"><p> I left this example because it is the first one I made and I used it quite a lot during debates</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuuqnp6svlpd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuuqnp6svlpd">^</a></strong></sup></span><div class="footnote-content"><p> Actually, clues about the non-bidirectional encoding of knowledge were discussed by Jacques in his <a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><u>critique of the ROME/MEMIT papers</u></a> .</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last<guid ispermalink="false"> HxRjHq3QG8vcYy4yy</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:20 GMT</pubDate> </item><item><title><![CDATA[Preface to the Sequence on Model Psychology]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p> As Large Language Models (LLMs) like ChatGPT evolve, becoming more advanced and intricate, the challenge of understanding their behaviors becomes increasingly hard. Solely relying on traditional interpretability techniques <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><u>may not be sufficient</u></a> or fast enough in our journey to understand and align these AI models.</p><p> When exploring human cognition and behavior, we&#39;ve historically relied on two intertwined yet distinct approaches: psychology and neuroscience. While psychology offers us a lens to understand human behavior through external observations, neuroscience delves into the internal mechanisms, exploring the biological roots of our mental processes. The collaboration that arose between those two fields isn&#39;t just a confluence of theories and methods; it&#39;s rather a harmonious synergy where insights from one field often inspire and enhance the other.</p><p> For a tangible illustration of how psychology and neuroscience complement each other, let&#39;s delve into the realm of memory research. Elizabeth Loftus, a psychologist, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-5884.1996.tb00003.x"><u>illuminated the ways in which human memory can be malleable and sometimes inaccurate</u></a> . Her pioneering psychological studies set the stage for a deeper exploration of memory. Building upon her insights, neuroscientists Yoko Okado and Craig EL Stark in 2005 delved into the brain&#39;s mechanics, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC548489/"><u>seeking the neural underpinnings of these memory phenomena</u></a> . On another front, 1996 saw a landmark discovery in neuroscience: Giacomo Rizzolatti and his team unveiled the existence of <a href="https://pubmed.ncbi.nlm.nih.gov/8713554/"><u>mirror neurons</u></a> , which help us understand the physical reflection of actions and emotions in the brain. This revelation prompted psychologists to venture further, with researchers like Niedenthal et al. exploring the <a href="https://pubmed.ncbi.nlm.nih.gov/19469591/"><u>embodiment of emotional concepts</u></a> , and many others, drawing parallels with these neurological findings. Such interplay underscores a collaborative dynamic where each field, while preserving its distinctive methodological lens, enriches and is enriched by the other, driving our collective understanding of the human mind forward.</p><p> Drawing parallels with AI alignment research, the path of Interpretability mirrors that of neuroscience (bottom-up approach). However, we still lack the equivalent of a &quot;psychological&quot; top-down approach. This gap brings us to &quot;Model Psychology&quot; <span class="footnote-reference" role="doc-noteref" id="fnrefsgt74itqjvs"><sup><a href="#fnsgt74itqjvs">[1]</a></sup></span> — a burgeoning field ripe for exploration and brimming with low-hanging fruits.</p><p> While we&#39;re still in the early stages of carving out the contours of model psychology, this series of posts aims to pique your curiosity rather than present a rigorous academic exposition. I want to spotlight this emerging domain, discuss potential research directions, and hopefully, ignite a newfound enthusiasm among readers.</p><p> <strong>Disclaimer:</strong> My background is primarily in software and AI engineering. My understanding of psychology and neuroscience has been accumulated from various online sources over the past half year. I acknowledge that there may be oversimplifications or inaccuracies in my posts. My intent is to continually expand my knowledge in these domains as I delve deeper into the realm of model psychology research for alignment. Nonetheless, I believe that a fresh perspective, unburdened by preconceived notions, can sometimes offer valuable insights.</p><p> In this sequence, my primary examples will be interactions with ChatGPT since it&#39;s the model <a href="https://twitter.com/prompt_tinkerer/status/1694044079116468497"><u>I&#39;m most familiar with</u></a> . However, the observations and insights are largely applicable to various state-of-the-art LLMs. Do note, though, that replicating some of the more intricate examples might require quite advanced prompt engineering skills.</p><p> The sequence of posts has been designed with continuity in mind. Each post builds on the knowledge and concepts introduced in the previous ones. For a coherent understanding, I recommend reading the posts in the order they&#39;re presented. Some points might be confusing or seem out of context if you jump ahead. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsgt74itqjvs"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsgt74itqjvs">^</a></strong></sup></span><div class="footnote-content"><p> I think <a href="https://www.lesswrong.com/users/buck?mention=user">@Buck</a> is at the origin of the name. I heard him say this term at a presentation during SERI Mats 3.0</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-model-psychology#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-model-psychology<guid ispermalink="false"> yuwdj82yjhLFYessc</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:07 GMT</pubDate> </item><item><title><![CDATA[What I've been reading, November 2023]]></title><description><![CDATA[Published on November 7, 2023 1:37 PM GMT<br/><br/><p> A ~monthly feature. Recent blog posts and news stories are generally omitted; you can find them in my <a href="https://rootsofprogress.org/writing#links-digest">links digests</a> . All emphasis in bold in the quotes below was added by me.</p><h2> <strong>Books</strong></h2><p> Finished <strong>Lynn White,</strong> <a href="https://www.amazon.com/Medieval-Technology-Social-Change-Townsend/dp/B00442EY2K"><i><strong>Medieval Technology and Social Change</strong></i></a> (1962). Last time I talked about <a href="https://rootsofprogress.org/reading-2023-10">the stirrup thing</a> . The second part of the book is about the introduction of the heavy plow in agriculture, and how it enabled the shift to a <a href="https://en.wikipedia.org/wiki/Three-field_system">three-field crop rotation</a> . Among other things, this provided more protein in the European diet, which made for a healthier population. The third part is a survey of medieval power mechanisms, including water mills, crank shafts, and clock escapements. Very interesting overall, perhaps a bit dry and technical for casual readers though. Note also that since it is from the &#39;60s it is not up to date with the latest research.</p><p> Also finished <strong>Ian Tregillis&#39;s</strong> <a href="https://www.amazon.com/dp/B074CDN1HB"><i><strong>The Alchemy Wars</strong></i></a> . I can now definitely recommend this sci-fi/fantasy trilogy, even if the cast of characters and the way the conflict unfolded isn&#39;t exactly how I would have written it myself.</p><p> Browsed <strong>Derek J. de Solla Price,</strong> <a href="https://www.amazon.com/Science-since-Babylon-Derek-Solla/dp/0300017979"><i><strong>Science since Babylon</strong></i></a> (1961), while preparing for a talk. Some very interesting charts such as this: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/lzkq3rrbdaak6ull5sjk" alt=""><figcaption> <a href="https://archive.org/details/sciencesincebaby0000pric/page/97/mode/1up"><i>Science since Babylon, p. 97</i></a></figcaption></figure><p> New on my reading list:</p><p> <strong>Venkatesh Narayanamurti and Toluwalogo Odumosu,</strong> <a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674967960"><i><strong>Cycles of Invention and Discovery: Rethinking the Endless Frontier</strong></i></a> (2016), and <strong>Venkatesh Narayanamurti and Jeffrey Tsao,</strong> <a href="https://www.amazon.com/Genesis-Technoscientific-Revolutions-Rethinking-Research-ebook/dp/B098TX7WV4"><i><strong>The Genesis of Technoscientific Revolutions: Rethinking the Nature and Nurture of Research</strong></i></a> (2021). These have actually been on my list for a while, but got bumped back up after meeting Venky and Jeff at a recent workshop on metascience. (The latter book is required reading at <a href="https://spec.tech/">Speculative Technologies</a> .)</p><p> Also mentioned at the workshop: <strong>B. Zorina Khan,</strong> <a href="https://www.amazon.com/Inventing-Ideas-Patents-Knowledge-Economy-ebook/dp/B08762LSN7/"><i><strong>Inventing Ideas: Patents, Prizes, and the Knowledge Economy</strong></i></a> (2020); and <strong>A Michael Noll and Michael Geselowitz,</strong> <a href="https://www.amazon.com/Bell-Labs-Memoirs-Voices-Innovation-ebook/dp/B006L7JRLY/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr="><i><strong>Bell Labs Memoirs: Voices of Innovation</strong></i></a> (2011).</p><p> Also:</p><ul><li> <strong>Pedro Domingos,</strong> <a href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2"><i><strong>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</strong></i></a> (2015)</li><li> <strong>Robert Martello,</strong> <a href="https://www.amazon.com/Midnight-Ride-Industrial-Dawn-Enterprise-ebook/dp/B07DFPX41D"><i><strong>Midnight Ride, Industrial Dawn: Paul Revere and the Growth of American Enterprise</strong></i></a> (2010)</li><li> <strong>Jessie Singer,</strong><a href="https://www.amazon.com/There-Are-No-Accidents-Disaster-Who-ebook/dp/B0984KPSLJ"><i><strong>There Are No Accidents: The Deadly Rise of Injury and Disaster—Who Profits and Who Pays the Price</strong></i></a> (2022)</li><li> <strong>Ursula Le Guin,</strong><a href="https://www.amazon.com/Dispossessed-Ambiguous-Utopia-Hainish-Cycle-ebook/dp/B000FC11GA"><i><strong>The Dispossessed</strong></i></a> (1974) (sci-fi)</li></ul><h2> <strong>Articles</strong></h2><p> <strong>Ray Kurzweil, “</strong> <a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><strong>The Law of Accelerating Returns</strong></a> <strong>”</strong> (2001). Kurzweil strikes me as a grand theorist but not a careful scholar—a risky combination. For instance, he writes: “ <i>Homo sapiens</i> evolved in a few hundred thousand years. Early stages of technology—the wheel, fire, stone tools—took tens of thousands of years to evolve and be widely deployed.” Stone tools, fire, and the wheel are often depicted in cartoons featuring cavemen. But stone tools evolved over <i>millions</i> of years; the controlled use of fire is something like several hundred thousand years old; and both predate <i>Homo sapiens</i> . <a href="https://rootsofprogress.org/reinventing-the-wheel">The wheel came <i>much</i> later</a> , well after agriculture and settled society. Details like this are a warning to tread carefully.</p><p> That said, I was interested to read this essay because I am starting to see the truth and significance of its core idea: that <a href="https://rootsofprogress.org/why-progress-was-so-slow">human progress accelerates over time, following a super-exponential curve</a> . This phenomenon has been documented more broadly in the economics literature, such as by <a href="https://web.stanford.edu/~chadj/JonesRomer2010.pdf">Jones and Romer (2010)</a> , who refer to “accelerating growth” as one of the key stylized facts that growth models should attempt to explain.</p><p> I have described acceleration as resulting from the compounding of <a href="https://rootsofprogress.org/flywheels-of-progress">multiple feedback loops</a> : increases in wealth, population, science, markets, institutions, and technology allow us to invent more, improve institutions, expand markets, advance science, grow population, accumulate wealth, etc. Kurzweil sees the phenomenon as not merely technological, but biological—a feature of evolution as such (and he sees technological evolution as simply a continuation of biological evolution by more efficient means). In his telling, as evolution progresses, it sometimes evolves better mechanisms for evolving. This is a very intruiging idea, but he doesn&#39;t argue it with any rigor or present much evidence for it, and I don&#39;t know enough about biology or evolution to evaluate it. He mentions “cells” as the “first step” in evolution, and then refers to “the subsequent emergence of DNA” (but wasn&#39;t DNA present from the origins of life?) He indicates that evolution sped up during the Cambrian Explosion, and credits this to “setting the &#39;designs&#39; of animal body plans”, but doesn&#39;t elaborate on the causal connection except to say that this “allowed rapid evolutionary development of other body organs, such as the brain.” Presumably sexual reproduction should be a major event in this story, since it allows for more variation through <a href="https://en.wikipedia.org/wiki/Genetic_recombination">genetic recombination</a> , but he doesn&#39;t mention it. So, it&#39;s very unclear to me what to make of this story (although if it&#39;s right, it would extend the “accelerating progress” pattern backwards by more than three billion years). </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/itv5virofgz8yrkjl67l" alt="What even is this chart? What is the y-axis? How are “mammals” and “primates” new “paradigms”? Did he just draw a straight line on a log-log plot and arbitrarily pick some points near the line to label?"><figcaption> <i>What even is this chart? What is the y-axis? How are “mammals” and “primates” new “paradigms”? Did he just draw a straight line on a log-log plot and arbitrarily pick some points near the line to label?</i> <a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>Ray Kurzweil</i></a></figcaption></figure><p> Grand theories aside, I was very interested in his analysis of computing power. He plotted the computing speed per dollar of dozens of devices, all the way from late 19th-century mechanical calculators through early 21st-century microprocessors, and claims to have found a increasing cost-performance curve running through five generations of computing technology: purely mechanical, electromechanical, vacuum tube, transistor, and integrated circuit. Moore&#39;s Law is only the fifth and most recent segment of this much longer trend, one exponential portion of an overall super-exponential curve: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/cmnrpy27usjeitsejdx0" alt=""><figcaption> <a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>Kurzweil, The Law of Accelerating Returns</i></a></figcaption></figure><p> I&#39;d like to check the data and sources on this one, but it&#39;s a very intriguing pattern.</p><p> The full essay is very long and covers many not-super-well-connected topics, which I don&#39;t have time to comment on; the core idea is in <a href="https://www.edge.org/response-detail/10600">this 2004 Edge question</a> , but doesn&#39;t contain all the most interesting details (such as the computing trends just mentioned).</p><p> The same accelerating curve, and the same basic explanation based on feedback loops, seems to be the gist of <strong>David Roodman, “</strong> <a href="https://www.openphilanthropy.org/research/modeling-the-human-trajectory/"><strong>Modeling the Human Trajectory</strong></a> <strong>”</strong> (2020), which I have only skimmed but plan to return to.</p><p> Others:</p><p> <strong>Deirdre N. McCloskey</strong> <a href="https://www.cato.org/commentary/book-review-power-progress-our-thousand-year-struggle-over-technology-prosperity"><strong>reviews Acemoglu and Johnson&#39;s</strong> <i><strong>Power and Progress</strong></i></a> (2023). If you know anything about the book, and anything about McCloskey, you won&#39;t be surprised that she is critical:</p><blockquote><p> The invisible hand of human creativity and innovation, in the authors&#39; analysis, requires the wise guidance of the state. … This is a perspective many voters increasingly agree with—and politicians from Elizabeth Warren to Marco Rubio. We are children, bad children (viewed from the right) or sad children (viewed from the left). Bad or sad, as children we need to be taken in hand. Messrs. Acemoglu and Johnson warmly admire the US Progressive Movement of the late 19th century as a model for their statism: experts taking child‐citizens in hand.</p></blockquote><p> <strong>Robert Tracinski, “</strong> <a href="https://tracinskiletter.substack.com/p/we-are-all-philosophers-now"><strong>We Are All Philosophers Now</strong></a> <strong>”</strong> and <strong>“</strong> <a href="https://tracinskiletter.substack.com/p/the-dilemma-of-choice"><strong>The Dilemma of Choice</strong></a> <strong>”</strong> (2023). Modernity has replaced a narrow, limited set of social roles and life choices with a smorgasbord of options. This is liberating, but the price of the freedom of choice is the responsibility of choice, which is now everyone&#39;s to bear. Not everyone is happy about this. Rob&#39;s pithy summary: “If Socrates said that the unexamined life is not worth living, well, now it&#39;s not really an option.”</p><p> <strong>Virginia Postrel, “</strong> <a href="https://vpostrel.substack.com/p/question-time-what-ails-american"><strong>What ails American culture?</strong></a> <strong>”</strong> (2023). On similar themes:</p><blockquote><p> Human beings need to feel purpose and meaning in their lives. But I am not entirely sure that the current discontent is a product of material abundance, that people did not feel similar discontent in the past, or that the “economic problem” loomed so large in the past that it dwarfed all other problems.</p></blockquote><p> <strong>Ben Landau-Taylor, “</strong> <a href="https://www.benlandautaylor.com/p/the-vocabulary-of-power"><strong>The Vocabulary Of Power</strong></a> <strong>”</strong> (2023). “Power” can mean many things; here are four more precise terms. Not only will this help clarify your concepts, it will also fulfill your daily quota of thinking about the Roman Empire.</p><p> <strong>Tanner Greer, “</strong> <a href="https://scholars-stage.org/where-have-all-the-great-works-gone/"><strong>Where Have All the Great Works Gone?</strong></a> <strong>”</strong> (2021):</p><blockquote><p> Spengler … repeatedly describes Tolstoy (d. 1910), Ibsen (d. 1906), Nietzsche (d. 1900), Hertz (d. 1894), Dostoevsky (d. 1881), Marx (d. 1883), and Maxwell (1879) as figures of defining “world-historical” importance… Spengler began writing <i>Decline of the West</i> in 1914. Tolstoy was only four years dead when Spengler started his book; Marx was only 30 years deceased. … Is there anyone who died in the last decade you could make that sort of claim for? How about for the last two decades? The last three?</p></blockquote><p> <strong>Gideon Lewis-Kraus, “</strong> <a href="https://www.newyorker.com/magazine/2023/10/09/they-studied-dishonesty-was-their-work-a-lie"><strong>They Studied Dishonesty. Was Their Work a Lie?</strong></a> <strong>”</strong> (2023). A case study of scientific fraud.</p><p> <strong>Stephen Wolfram, “</strong> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><strong>Are All Fish the Same Shape If You Stretch Them? The Victorian Tale of</strong> <i><strong>On Growth and Form</strong></i></a> <strong>”</strong> (2017) I just thought this idea was kind of hilarious: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/ojp9zs5mkcgv06u8khvd" alt="Stretch one kind of fish, and it looks like another."><figcaption> <i>Stretch one kind of fish, and it looks like another.</i> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><i>D&#39;Arcy Thompson, On Growth and Form</i></a></figcaption></figure><br/><br/> <a href="https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023<guid ispermalink="false"> hKK4KdoTdTtaNqCaj</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:37:20 GMT</pubDate> </item><item><title><![CDATA[AI Alignment [Progress] this Week (11/05/2023)]]></title><description><![CDATA[Published on November 7, 2023 1:26 PM GMT<br/><br/><p> The biggest news of this week is probably the two new AI Alignment Initiatives.</p><p> Otherwise, this was a relatively quiet week for AI Alignment breakthroughs.</p><p> So here are our</p><h1> AI Alignment Breakthroughs this Week</h1><p></p><p> This week there were breakthroughs in the areas of:</p><p> Mechanistic Interpretability</p><p> Avoiding Adversarial Attacks</p><p> Human Augmentation</p><p> Making AI Do what we want</p><p> AI Art</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1456w"></a></p><p></p><h1> Mechanistic Interpretability</h1><p></p><p> <a href="https://twitter.com/abacaj/status/1721223737729581437">Research on the limits of Generalization in LLMs</a></p><p> What is it: Shows that LLMs are unable to generalize outside of their training data</p><p> What&#39;s new: A systematic study of the types of generalization that LLMs can do ((in domain learning) and the types they can&#39;t (out of domain learning)</p><p> What does it mean: Understanding the limits of LLMs should help us better understand where they can be safely deployed</p><p> Rating: 💡💡💡</p><p> <a href="https://twitter.com/emollick/status/1720135672764285176">Does appealing to AI “emotions” make them preform better?</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1456w"></a></p><p></p><p></p><p> What is it: Researchers find you can make LLMs answer questions better by using emotion</p><p> What&#39;s new: new prompting strategies such as “this is important for my career”</p><p> What is it good for: In addition to purely improved performance, the really question is <i>why</i> does this work. Is there some “try harder” vector in the latent space that we can identify and exploit?</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/johnjnay/status/1719762063419904319">LLMs Generalized Truthfulness Across Agents</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1456w"></a></p><p></p><p> What is it: a way to tell true statements from false ones using LLMs</p><p> What&#39;s new: they find evidence that LLMs have different “personas” that they use to judge whether something is true or not</p><p> What is it good for: Perhaps we can extract these personas and use them for alignment.</p><p> Rating: 💡💡💡💡</p><p></p><h1> Avoiding Adversarial Attacks</h1><p></p><p> <a href="https://twitter.com/StephenLCasper/status/1720910484441014525">Self Destructing Models</a></p><p> What it is: Pretrain a model so that it cannot be later fine-tuned for harmful purposes?</p><p> What&#39;s new: They find meta-parameters such that fine-tuning a model on one task reduces its usefulness for other tasks</p><p> What is it good for: Hypothetically you could open-source a model and not have all of the RLHF safety features <a href="https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/">immediately removed</a> by fine-tuning</p><p> Rating:💡💡(important topic, but I&#39;m not convinced this technique works. More optimistic about techniques like <a href="https://twitter.com/norabelrose/status/1666469917636571137">Concept Erasure</a> )</p><p> <a href="https://twitter.com/simonw/status/1720839106664808450">Data Exfiltration</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1456w"></a></p><p></p><p></p><p></p><p> What is it: an example of an attack that uses prompt injection to exfiltrate data from Google Bard</p><p> What&#39;s new: they use Bard&#39;s Google Doc extension to exfiltrate data</p><p> What is it good for: red-teaming these kinds of attacks is the first step to fixing them</p><p> Rating: 💡</p><p> <a href="https://twitter.com/DrNikkiTeran/status/1719048031549505974">Will releasing the weights of large language models grant widespread access to pandemic agents?</a></p><p> What is it: exactly what it says on the tin</p><p> What&#39;s new: they simulate someone trying to build a harmful bio-weapon with/without an LLM and find the LLM helps</p><p> What is it good for: There&#39;s been pretty strong pushback against this from e/acc, with people noting that this is also true of Google Search, or a hypothetical drug that increased everyone in the world&#39;s IQ by 1 point. The larger point remains that we should identify dangerous capabilities and act to mitigate them. “Regulate uses not research” is the <a href="https://twitter.com/MiTiBennett/status/1705562223765328253">rallying cry</a> of Midwit Alignment.</p><p> Rating: 💡</p><h1> Human Augmentation</h1><p></p><p> <a href="https://twitter.com/tolga_birdal/status/1719691533606146194">SignAvatars</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1456w"></a></p><p></p><p></p><p> What is it: a 3d sign language dataset</p><p> What&#39;s new: first dataset of its kind</p><p> What is it good for: Train a model to automatically convert speech into sign language</p><p> Rating: 💡💡💡</p><h1> Making AI Do what we want</h1><p></p><p> <a href="https://twitter.com/SeungjuHan3/status/1720373937308377250">Commonsense Norms</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1456w"></a></p><p></p><p> What is it: answer moral questions using visual data</p><p> What&#39;s new: They use a LM to generate a multimodal benchmark set of various moral quandries</p><p> What is it good for: If we want robots or self-driving cars to behave ethically, they will have to rely on visual input to do so</p><p> Rating:💡💡</p><p> <a href="https://twitter.com/arankomatsuzaki/status/1710096304167120903">Agent Instructs Large Language Models to be General Zero-Shot Reasoners</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1456w"></a></p><p></p><p></p><p> What is it: train an agent to follow instructions</p><p> What&#39;s new: they first have the agent access web resources before generating step-by-step instructions</p><p> What is it good for: Accurately doing tasks such as classification</p><p> Rating: 💡💡💡</p><h1> AI Art</h1><p></p><p> <a href="https://twitter.com/dreamingtulpa/status/1721092490281771448">360-to-gaussian-splat</a></p><p> What is it: a new program for converting a 360 degree photo into a 3d scene</p><p> What&#39;s new: they train a 3d-aware diffusion model to allow them to calculate novel views</p><p> What is it good for: convert any 360 degree photo (and soon I expect video) into a full 3d scene you can walk around in</p><p> Rating: 💡💡💡</p><p> <a href="https://twitter.com/xenovacom/status/1720916876010635364">Distill Whisper</a></p><p> What is it: A faster version of the speech-to-text model whisper</p><p> What&#39;s new: they distill the model down to one that&#39;s 49% smaller</p><p> What is it good for: this unlocks text-to-speech in places like mobile or web where it would have previously been too slow.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/DeepMotionInc/status/1719810023201841512">DeepMotion</a></p><p> What is it: an AI model trained to generate motion for 3d models</p><p> What&#39;s new: Not open source, but seems better than past versions of this I&#39;ve seen</p><p> What is it good for: animating all of those sweet models that you&#39;re generating with <a href="https://twitter.com/marc_habermann/status/1717808998236217463">text-to-3d</a> .</p><p> Rating: 💡💡💡</p><h1> AI Alignment Initiatives</h1><p></p><p> <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">The Biden Executive Order on AI</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1456w"></a></p><p></p><p> What is it: The most notable provision being that models trained with than 10**26 flops and data centers with a capacity of more than 10**20 flops/sec must <a href="https://twitter.com/DavidVorick/status/1719097248699879831">register</a> with the government. This limit appears to have been chosen not for any scientific reason, but because it is <a href="https://twitter.com/main_horse/status/1719147360964825453"><i>slightly larger</i></a><i> </i>than the largest current models. There&#39;s also some <a href="https://twitter.com/JagersbergKnut/status/1719259231419797656">ominous language</a> about open-source models, but no actual regulations so far.</p><p> What does it mean: Doomers will say this doesn&#39;t go <a href="https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/">far enough</a> . E/ACC is already preparing to flee for <a href="https://twitter.com/DelComplex/status/1719087956311396481">friendlier</a> waters (this particular project is a joke). The most important fact isn&#39;t the order itself (which is not a law and thus has limited enforcement mechanisms) but that this sets the tone for future regulation. Expect more of the same in the future: technical limits set for political reasons not scientific ones, emphasis on whatever the current administration&#39;s pet-projects are (Biden likes unions hates discrimination, and wants to cure cancer), and more words than action (since we still have to compete with China after all).</p><p> Overall Rating:🧓🏻🧓🏻🧓🏻 (3 Joe Bidens, could have been worse)</p><p> <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Park Declaration</a></p><p> What is it: a joint statement by a number of countries agreeing to mitigate AI risk</p><p> What does it mean: The most important signature on this list is undoubtedly China. Much like <a href="https://www.cnn.com/2021/10/28/world/china-us-climate-cop26-intl-hnk/index.html">global warming</a> , China is probably the single biggest contributor to AI risk. They have the computing <a href="https://www.top500.org/news/china-extends-lead-in-number-of-top500-supercomputers-us-holds-on-to-performance-advantage/">heft</a> to build powerful systems, but are more likely to cut safety standards due to the perceive need to “race” to catch up with the US. And an AI broadly trained with the <a href="https://thediplomat.com/2022/11/xi-jinpings-vision-for-artificial-intelligence-in-the-pla/">values</a> of the CCP is less likely to be benevolent. I wouldn&#39;t expect miracles, but hopefully this indicates a willingness by the Chinese to at least follow the <a href="https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk">common consensus</a> on AI safety standards.</p><p> Rating: 🇨🇳🇨🇳🇨🇳🇨🇳🇨🇳</p><h1> This is not AI Alignment</h1><p></p><p> <a href="https://twitter.com/ESYudkowsky/status/1718654143110512741">An amusing short-story (long tweet?) By EY</a></p><p> What it is: A reminder that EY is a very good <i>fiction</i> writer</p><p> What does it mean: It is funny and worth reading.</p><p> Overall Rating: 📃📃📃📃(4 letters of self-reflection)</p><p> Small LLMs</p><p> What it is: There are an increasing number of small LLMs in the “better than GPT 3.5” category including <a href="https://twitter.com/alignment_lab/status/1721308271946965452">OpenChat</a> , <a href="https://twitter.com/_akhaliq/status/1721028519717642749">Grok</a> and <a href="https://twitter.com/erhartford/status/1721041791988679059">Yi-34B</a> . Importantly anyone with a modestly good PC can run OpenChat or Yi-34B on their personal computer.</p><p> What does it mean: A little over a year an a half after the release of GPT3.5, the technology has now become so widespread that there are multiple independent replications. If technology really is increasing at a <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">hyper-exponential</a> rate, we should expect smaller and smaller time-gaps between the leading edge and widespread availability (and paradoxically larger and larger capability gaps). One key thing to track might be to see how long before we have widespread models better than GPT4.</p><p> Rating:🤖🤖 <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> + <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> /2 (3.5 large language models)</p><br/><br/> <a href="https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023<guid ispermalink="false"> FLCHNqu2FNRCJ4dyg</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:26:22 GMT</pubDate> </item><item><title><![CDATA[On the UK Summit]]></title><description><![CDATA[Published on November 7, 2023 1:10 PM GMT<br/><br/><p> In the eyes of many, Biden&#39;s Executive Order somewhat overshadowed the UK Summit. The timing was unfortunate. Both events were important milestones. Now that I have had time, here is my analysis of what happened at the UK Summit.</p><span id="more-23578"></span><p> As is often the case with such events, there was a lot of talk relative to the amount of action. There was a lot of diplomatic talk, talk of that which everyone agrees upon, relative to the amount of talk of real substance. There were days of meetings that resulted in rather unspicy summaries and resolutions. The language around issues that matter most was softened, the actual mission in danger of being compromised.</p><p> And as usual, the net result was reason for optimism, a net highly positive event versus not having it, while also in some ways being disappointing when compared to what might have been. A declaration was signed including by China, but it neglected existential risk. Sunak&#39;s words on AI were not as strong as his words have been previously.</p><p> We got promises for two additional summits, in South Korea and France. Given that, I am willing to declare this a success.</p><p> One area of strong substance was the push for major AI labs to give substantive safety policies addressing a variety of issues, sometimes largely called Responsible Scaling Policies (RSPs). The biggest labs all did so, even Meta. Now we can examine their responses, know who is being how responsible, and push for better in the future or for government action to fix issues or enshrine progress. This was an excellent development.</p><p> This post will look at the rest of what happened at the Summit. I will be writing about the RSPs and other safety policies of the labs in a distinct post next week.</p><h4> Looking Back at People&#39;s Goals for the Summit and Taskforce</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://jack-clark.net/2023/07/05/what-should-the-uks-100-million-foundation-model-taskforce-do/">Jack Clark&#39;s proposal from July 5 for what the Foundation Model taskforce might do to evaluate frontier models</a> as its priority, and how it might prioritize that, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1676595226339598343">Simeon&#39;s response</a> emphasizing the need for a good way to know whether a proposal is safe enough to allow it to proceed.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.navigatingrisks.ai/p/what-should-the-uks-100-million-foundation">Navigating AI Risks asked on July 17 what the taskforce should do</a> , advising focus on interventions to impact policy at labs and other governments. Suggested focus was risk assessment methodology, demonstrating current risks and assessing current state of the art models, and to avoid direct alignment work.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1678331653326884865">Lennart Heim&#39;s (GovAI) July 10 proposal of what the summit should try to accomplish</a> , which he reviewed after the summit.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1698616866934018282">Matt Clifford from the PM&#39;s office shared on September 10 their objectives</a> for the summit: A shared understanding of the risks posed by frontier AI and the need for action, a forward process for international collaboration, measures for organizations, finding areas for safety collaboration and showcasing how safe AI development can enhance global good.</p><h4> AI Safety Summit Agenda</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1718925374623519155">What has the UK Taskforce been up to in advance of the summit</a> ( <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-second-progress-report/frontier-ai-taskforce-second-progress-report">report</a> )?</p><blockquote><p> Ian Hogarth (Chair UK AI Frontier Model Taskforce): The Taskforce is a start-up inside government, delivering on the mission given to us by the Prime Minister: to build an AI research team that can evaluate risks at the frontier of AI. We are now 18 weeks old and this is our second progress report.</p><p> The frontier is moving very fast. On the current course, in the first half of 2024, we expect a small handful of companies to finish training models that could produce another significant jump in capabilities beyond state-of-the-art in 2023.</p><p> As these AI systems become more capable they may augment risks. An AI system that advances towards expert ability at writing software could increase cybersecurity threats. An AI system that becomes more capable at modelling biology could escalate biosecurity threats.</p><p> We believe it is critical that frontier AI systems are developed safely and that the potential risks of new models are rigorously and independently assessed for harmful capabilities before and after they are deployed.</p><p> The hardest challenge we have faced in building the Taskforce is persuading leading AI researchers to join the government. Beyond money, the prestige and learning opportunities from working at leading AI organizations are a huge draw for researchers.</p><p> We can&#39;t compete on compensation, but we can compete on mission. We are building the first team inside a G7 government that can evaluate the risks of frontier AI models. This is a crucial step towards meaningful accountability and governance of frontier AI companies.</p><p> In our first progress report we said we would hold ourselves accountable for progress by tracking the total years of experience in our research team. When I arrived as Chair in June, there was just one frontier AI researcher employed full time by the department.</p><p> We managed to increase that to 50 years of experience in our first 11 weeks of operation. Today that has grown to 150 years of collective experience in frontier AI research.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05542917-18fc-4dd7-8372-2e140111df62_960x640.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/xpluetsb6xhnqot5puwf" alt="图像"></a></figure><blockquote><p> Ian Hograth: Our research team and partners have published hundreds of papers at top conferences. Some of our team members&#39; recent publications span algorithms for AI systems to search and improve and and publicly sourced constitutions for AI alignment.</p><p> We are also delighted to welcome Jade Leung to our leadership team. Jade joins us from OpenAI where she led the firm&#39;s work on AGI governance, with a particular focus on frontier AI regulation, international governance, and safety protocols.</p><p> Furthermore, we are excited to welcome @ruchowdh, who will be working with the Taskforce to develop its work on safety infrastructure, as well as its work on evaluating societal impacts from AI.</p><p> Rumman is the CEO and co-founder of Humane Intelligence, and led efforts for the largest generative AI public red-teaming event at DEFCON this year. She is also a Responsible AI fellow at the Harvard Berkman Klein Center, and previously led the META team at Twitter.</p><p> We are continuing to scale up our research and engineering team. Please consider applying to the EOI form or via these job postings for Senior Research Engineers and Senior Software Engineers.</p><p> Leading on AI safety does not mean starting from scratch or working alone – we are building on and supporting the work conducted by a range of cutting-edge organizations.</p><p> Today we are announcing that the taskforce has entered into a new partnership with @apolloaisafety, an AI safety organisation that works with large language models to interpret their behaviour and evaluate their high-risk failure modes, particularly deceptive alignment.</p><p> We have also entered into a new partnership with @openminedorg. We are working with OpenMined to develop technical infrastructure and governance tools that will facilitate AI safety research across government and AI research organisations.</p><p> In June this year, several large AI companies committed to giving the UK government, via the Frontier AI Taskforce, early and deeper access to their models. Since then, we have been working in collaboration with these leading AI companies to secure this model access.</p><p> But model access is only one part of the picture. For too long researchers in industry have had access to much greater computing resources than those in academia and the public-sector, creating a so-called &#39;compute divide&#39;.</p><p> Having the compute infrastructure to conduct cutting-edge research is pivotal for building state capacity in AI safety – for example being able to run large-scale interpretability experiments.</p><p> To tackle this over the last months, the Taskforce has supported DSIT &amp; the University of Bristol to help launch major investments in compute. The University of Bristol will soon host the first component of the UK&#39;s AI Research Resource, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/news/bristol-set-to-host-uks-most-powerful-supercomputer-to-turbocharge-ai-innovation">Isambard-AI</a> .</p><p> It will be one of the most powerful supercomputers in Europe when built and will vastly increase our public-sector AI compute capacity. These great strides fundamentally change the kind of projects researchers can take on inside the Taskforce.</p><p> ……</p><p> [On Day 1 of the Summit] our team will present 10-minute demonstrations, focused on four key areas of risk: Misuse, Societal Harm, Loss of Human Control, and Unpredictable Progress.</p><p> We believe these demonstrations will be the most compelling and nuanced presentations of frontier AI risks done by any government to date. Our hope is that these demonstrations will raise awareness of frontier AI risk and the need for coordinated action.</p><p> AI is a general purpose and dual-use technology. We need a clear-eyed commitment to empirically understanding and mitigating the risks of AI so we can enjoy the benefits.</p><p> On Friday the Prime Minister announced that he is putting the UK&#39;s work on AI Safety on a longer term basis by <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/speeches/prime-ministers-speech-on-ai-26-october-2023">creating an AI Safety Institute</a> in which our work will continue.</p><p> AI has the power to revolutionize industries, enhance our lives and address complex global challenges, but we must also confront the global risks.</p></blockquote><p> It has only been 18 weeks. It sounds like they are building a very strong team, despite having to pay UK government salaries and play by government rules. The question is whether they can demonstrate the necessary types of tangible progress that allow them continued support, and if they do whether they can execute then on things that matter. This is all good diplomatic talk, which would be the right move in all worlds, so it gives us little insight beyond that.</p><p> Karim Beguir lays out his view of the stakes.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kbeguir/status/1718996564947980526">Karim Beguir</a> (CEO &amp; Co-Founder InstadeepAi): Happy to announce that I will be attending the AI Safety Summit in Bletchley Park <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BHdEvjtfwpgrTh825/ctsf9jut7irxxmirrjkz" alt="🇬🇧" style="height:1em;max-height:1em"> this week! I&#39;m honored to join heads of government as one of 100 influential researchers and tech leaders at the forefront of AI innovation.</p><p> A <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pKhzQyaWwes63JFwp/jgt2eoslwfcyu0abb4yz" alt="🧵" style="height:1em;max-height:1em"> on why <a target="_blank" rel="noreferrer noopener" href="https://t.co/eFWiL0Iv1O">this summit</a> is critically important for the future.</p><p> Since 2012, AI progress has benefitted from a triple exponential of data, compute (doubles every 6 months in ML) and model innovation (AI efficiency doubles every 16 months). It&#39;s a huge snowball that has been rolling for a while.</p><p> But now something qualitatively new has happened: the LLM breakthroughs of the last 12 months have triggered an AI race with around $10B now invested every month. So the question is; Are we about to experience a steeper rate of progress, similar to what Tim Urban (@waitbutwhy) predicted?</p><p> Here&#39;s what I see. AI is now accelerating itself through those same three drivers: data, compute and model innovation. LLMs allow AI-generated feedback to complement human feedback (ie RLAIF adding to RLHF). AI also designs more efficient compute hardware end-to-end with products <a target="_blank" rel="noreferrer noopener" href="http://DeepPCB.ai">like our own</a> and ML practitioners can develop next generation models roughly 2X as fast with AI-coding assistants.</p><p> That&#39;s why it&#39;s time for a dialogue between key AI innovators and governments. Nations need to tap into the efficiency and economic benefits of AI while proactively containing emergent risks, as leading AI researchers Yoshua Bengio, @geoffreyhinton, @tegmark argue and @10DowningStreet concurs.</p></blockquote><p> [thread continues]</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1719341619797749764">The UK government released</a> <a target="_blank" rel="noreferrer noopener" href="https://t.co/8cV1TgKcbX">a detailed set</a> of 42 (!) best safety practices. They asked the major labs to respond with how they intended to implement such practices, and otherwise ensure AI safety.</p><p> I will be writing another post on that. As a preview:</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policies">Nate Sores has thoughts on what they asked for</a> , which could be summarized as &#39;mostly good things, better than nothing, obviously not enough&#39; and of course it was never going to be enough and also Nate Sores is the world&#39;s toughest crowd.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/2pTf4vbRCV">How are various companies doing on the requests?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0dab6ccd-aa62-49b9-a41a-1753f7d90e07_507x507.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/vlqxp80zvprmc1pxcyua" alt="图像"></a></figure><p> That is what you get if you were grading on a curve one answer at a time.</p><p> Reality does not grade on a curve.</p><p> My own analysis, and others I trust, agree that this relatively underrates OpenAI, who clearly had the second best set of policies by a substantial margin, with one source even putting them on par with Anthropic, although I disagree with that. Otherwise the relative rankings seem correct.</p><p> I would consider Anthropic&#39;s submission quite good, if it was backed by proper framing of the need for further improvement and refinement, making it clear that this was a combination of IOU and only part of the solution, and advocacy of necessary government measures to help ensure safety. Given the mixed messaging, my excitement and that of many others is tampered, but they are still clearly leading the way. That is important.</p><p> More detail on the submitted policies will come in a future post.</p><h4> Someone Picked Up the Phone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719671611039551615">Well look who it is talking about AI safety and governance at the AI safety summit.</a></p><blockquote><p> Matt Clifford: Huge moment to have both Secretary Raimondo of the US and Vice Minister Wu of China speaking about AI safety and governance at the AI Safety Summit.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8386747-c778-4a1b-a7b1-82ff6c8eff67_1536x2048.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/acjwn65xqktadnyhmptt" alt="图像"></a></figure><p> We also have this: <a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">Prominent AI Scientists from China and the West Propose Joint Strategy to Mitigate Risks from AI</a> .</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/S_OhEigeartaigh">Seán Ó hÉigeartaigh</a> : Delighted to see this statement from AI research and governance leaders in the West and China, calling for coordinated global action on AI safety and governance. Great to see the consensus.</p><p> Statement page: The expert attendees warned governments and AI developers that “ <em>coordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.</em> ” Attendees produced a joint statement with specific technical and policy recommendations, which is attached below. Prof. Zhang remarked that it is “crucial for governments and AI corporations to invest heavily in frontier AI safety research and engineering”, while Prof. Yao stressed the importance that we “work together as a global community to ensure the safe progress of AI.” Prof. Bengio called upon AI developers to “demonstrate the safety of their approach before training and deploying” AI systems, while Prof. Russell concurred that “if they cannot do that, they cannot build or deploy their systems. Full stop.”</p></blockquote><p> This is the real thing, and it has a very distinctly culturally Chinese ring to its wording and attitude. Here is the full English statement, <a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">link also has the Chinese version</a> .</p><blockquote><p> <strong><em>Coordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.</em></strong></p><p> <em>Global action, cooperation, and capacity building are key to managing risk from AI and enabling humanity to share in its benefits. AI safety is a global public good that should be supported by public and private investment, with advances in safety shared widely. Governments around the world — especially of leading AI nations — have a responsibility to develop measures to prevent worst-case outcomes from malicious or careless actors and to rein in reckless competition. The international community should work to create an international coordination process for advanced AI in this vein.</em></p><p> <em>We face near-term risks from malicious actors misusing frontier AI systems, with current safety filters integrated by developers easily bypassed. Frontier AI systems produce compelling misinformation and may soon be capable enough to help terrorists develop weapons of mass destruction. Moreover, there is a serious risk that future AI systems may escape human control altogether. Even aligned AI systems could destabilize or disempower existing institutions. Taken together, we believe AI may pose an existential risk to humanity in the coming decades.</em></p><p> <em>In domestic regulation, we recommend mandatory registration for the creation, sale or use of models above a certain capability threshold, including open-source copies and derivatives, to enable governments to acquire critical and currently missing visibility into emerging risks. Governments should monitor large-scale data centers and track AI incidents, and should require that AI developers of frontier models be subject to independent third-party audits evaluating their information security and model safety. AI developers should also be required to share comprehensive risk assessments, policies around risk management, and predictions about their systems&#39; behavior in third party evaluations and post-deployment with relevant authorities.</em></p><p> <em>We also recommend defining clear red lines that, if crossed, mandate immediate termination of an AI system — including all copies — through rapid and safe shut-down procedures. Governments should cooperate to instantiate and preserve this capacity. Moreover, prior to deployment as well as during training for the most advanced models, developers should demonstrate to regulators&#39; satisfaction that their system(s) will not cross these red lines.</em></p><p> <em>Reaching adequate safety levels for advanced AI will also require immense research progress. Advanced AI systems must be demonstrably aligned with their designer&#39;s intent, as well as appropriate norms and values. They must also be robust against both malicious actors and rare failure modes. Sufficient human control needs to be ensured for these systems. Concerted effort by the global research community in both AI and other disciplines is essential; we need a global network of dedicated AI safety research and governance institutions. We call on leading AI developers to make a minimum spending commitment of one third of their AI R&amp;D on AI safety and for government agencies to fund academic and non-profit AI safety and governance research in at least the same proportion.</em></p></blockquote><p> The caveat is of course that such a statement is only as strong as its signatories. On our side we have among others Bengio and Russell. The top Chinese names are Andrew Yao and Ya-Qin Zhang. Both are prominent and highly respected figures in Chinese AI research, said to be part of shaping China&#39;s AI strategies, but I do not know how much that is worth.</p><p> GPT-4 suggested that the Chinese version focuses more on leading AI nation responsibility, with stronger statements about the need to draw sharp red lines and for a bureaucratic process. The Chinese mention of existential risk (“我们相信”) is more brief, which is a potential concern, but it is definitely there.</p><h4> The Bletchley Declaration</h4><p> No summit or similar gathering is complete without a declaration. This gives everyone a sense of accomplishment and establishes what if anything has indeed been agreed upon. What say the UK Safety Summit, in the form of The Bletchley Declaration?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">I will quote here in full</a> .</p><blockquote><p> Artificial Intelligence (AI) presents enormous global opportunities: it has the potential to transform and enhance human wellbeing, peace and prosperity. To realise this, we affirm that, for the good of all, AI should be designed, developed, deployed, and used, in a manner that is safe, in such a way as to be human-centric, trustworthy and responsible. We welcome the international community&#39;s efforts so far to cooperate on AI to promote inclusive economic growth, sustainable development and innovation, to protect human rights and fundamental freedoms, and to foster public trust and confidence in AI systems to fully realise their potential.</p><p> AI systems are already deployed across many domains of daily life including housing, employment, transport, education, health, accessibility, and justice, and their use is likely to increase. We recognise that this is therefore a unique moment to act and affirm the need for the safe development of AI and for the transformative opportunities of AI to be used for good and for all, in an inclusive manner in our countries and globally. This includes for public services such as health and education, food security, in science, clean energy, biodiversity, and climate, to realise the enjoyment of human rights, and to strengthen efforts towards the achievement of the United Nations Sustainable Development Goals.</p><p> Alongside these opportunities, AI also poses significant risks, including in those domains of daily life. To that end, we welcome relevant international efforts to examine and address the potential impact of AI systems in existing fora and other relevant initiatives, and the recognition that the protection of human rights, transparency and explainability, fairness, accountability, regulation, safety, appropriate human oversight, ethics, bias mitigation, privacy and data protection needs to be addressed. We also note the potential for unforeseen risks stemming from the capability to manipulate content or generate deceptive content. All of these issues are critically important and we affirm the necessity and urgency of addressing them.</p></blockquote><p>当然。 All of that is fine and highly unobjectionable. We needed to say those things and now we have said them. Any actual content?</p><blockquote><p> Particular safety risks arise at the &#39;frontier&#39; of AI, understood as being those highly capable general-purpose AI models, including foundation models, that could perform a wide variety of tasks – as well as relevant specific narrow AI that could exhibit capabilities that cause harm – which match or exceed the capabilities present in today&#39;s most advanced models. Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood and are therefore hard to predict. We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models. Given the rapid and uncertain rate of change of AI, and in the context of the acceleration of investment in technology, we affirm that deepening our understanding of these potential risks and of actions to address them is especially urgent.</p></blockquote><p> This is far from perfect or complete, but as good as such a declaration of the fact that there are indeed such concerns was reasonably going to get. Catastrophic is not as good as existential or extinction but will have to do.</p><blockquote><p> Many risks arising from AI are inherently international in nature, and so are best addressed through international cooperation. We resolve to work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all through existing international fora and other relevant initiatives, to promote cooperation to address the broad range of risks posed by AI. In doing so, we recognise that countries should consider the importance of a pro-innovation and proportionate governance and regulatory approach that maximises the benefits and takes into account the risks associated with AI. This could include making, where appropriate, classifications and categorisations of risk based on national circumstances and applicable legal frameworks. We also note the relevance of cooperation, where appropriate, on approaches such as common principles and codes of conduct. With regard to the specific risks most likely found in relation to frontier AI, we resolve to intensify and sustain our cooperation, and broaden it with further countries, to identify, understand and as appropriate act, through existing international fora and other relevant initiatives, including future international AI Safety Summits.</p></blockquote><p> Note the intention to establish two distinct regimes. For non-frontier AI, countries should chart their own path based on individual circumstances. For frontier AI, a promise to broaden cooperation to contain specific risks. As always, those risks and the difficulties they impose are downplayed, but this is very good progress. If you had told me six months ago we would get this far today, I would have been thrilled.</p><blockquote><p> All actors have a role to play in ensuring the safety of AI: nations, international fora and other initiatives, companies, civil society and academia will need to work together. Noting the importance of inclusive AI and bridging the digital divide, we reaffirm that international collaboration should endeavour to engage and involve a broad range of partners as appropriate, and welcome development-orientated approaches and policies that could help developing countries strengthen AI capacity building and leverage the enabling role of AI to support sustainable growth and address the development gap.</p></blockquote><p> I do not know what concrete actions might follow from a statement like this. It does seem like a good thing to spread mundane utility more widely. As I have often noted, I am a mundane utility optimist in these ways, and expect even modest efforts to spread the benefits to both improve lives generally and to cause net reductions in effective inequality.</p><blockquote><p> We affirm that, whilst safety must be considered across the AI lifecycle, actors developing frontier AI capabilities, in particular those AI systems which are unusually powerful and potentially harmful, have a particularly strong responsibility for ensuring the safety of these AI systems, including through systems for safety testing, through evaluations, and by other appropriate measures. We encourage all relevant actors to provide context-appropriate transparency and accountability on their plans to measure, monitor and mitigate potentially harmful capabilities and the associated effects that may emerge, in particular to prevent misuse and issues of control, and the amplification of other risks.</p></blockquote><p> Generic talk rather than concrete action, and not the generic talk that reflects the degree of danger or necessary action, but it is at least directionally correct generic talk. Again, seems about as good as we could have reasonably expected right now.</p><blockquote><p> In the context of our cooperation, and to inform action at the national and international levels, our agenda for addressing frontier AI risk will focus on:</p><ul><li> identifying AI safety risks of shared concern, building a shared scientific and evidence-based understanding of these risks, and sustaining that understanding as capabilities continue to increase, in the context of a wider global approach to understanding the impact of AI in our societies.</li><li> building respective risk-based policies across our countries to ensure safety in light of such risks, collaborating as appropriate while recognising our approaches may differ based on national circumstances and applicable legal frameworks. This includes, alongside increased transparency by private actors developing frontier AI capabilities, appropriate evaluation metrics, tools for safety testing, and developing relevant public sector capability and scientific research.</li></ul><p> In furtherance of this agenda, we resolve to support an internationally inclusive network of scientific research on frontier AI safety that encompasses and complements existing and new multilateral, plurilateral and bilateral collaboration, including through existing international fora and other relevant initiatives, to facilitate the provision of the best science available for policy making and the public good.</p><p> In recognition of the transformative positive potential of AI, and as part of ensuring wider international cooperation on AI, we resolve to sustain an inclusive global dialogue that engages existing international fora and other relevant initiatives and contributes in an open manner to broader international discussions, and to continue research on frontier AI safety to ensure that the benefits of the technology can be harnessed responsibly for good and for all. We look forward to meeting again in 2024.</p></blockquote><p> Risks exist, so we will develop policies to deal with those risks. Yes, we would have hoped for better, but for now I am happy with what we did get.</p><p> We also can note the list of signatories. Everyone who has released an important model or otherwise played a large role in AI seems to be included. It includes not only essentially the entire relevant Western world but also Israel, Japan, South Korea, Kenya, Nigeria, Saudi Arabia and UAE, Indonesia, Singapore, India and most importantly China.</p><p> The exception would be Russia, and perhaps North Korea. If a true international framework is to be in place to fully check dangerous developments, eventually China will not be enough, and we will want to bring Russia and even North Korea in, although if China is fully on board that makes that task far easier.</p><h4> Saying Generic Summit-Style Things</h4><p> As in things such as:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RishiSunak/status/1720074200210387328">Rishi Sunak</a> (on Twitter): The threat of AI does not respect borders. No country can do this alone. We&#39;re taking international action to make sure AI is developed in a safe way, for the benefit of the global community.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">The closing speech by PM Rishi Sunak.</a> Sunak has previously made very strong statements about AI safety in general and existential risk from AI in particular, naming it explicitly as a priority. This time he conspicuously did not do this. Presumably this was in the name of maintaining consensus, but it remains deeply disappointing. It would be very good to hear him affirm his understanding of the existential risks soon.</p><p> Mostly his time was composed of answering questions, which he largely handled well. He is clearly paying attention. At 20:15 a reporter notes that Sunak advised us &#39;not to lose sleep&#39; over existential risks from AI, he is asked when we should start to lose sleep over the existential risks from AI. He says here that we should not lose sleep because there is a debate over those risks and people disagree, which does not seem like a reason to not lose sleep. He says governments should act even under this uncertainty, which indeed seems both correct and like the metaphorical lose sleep response.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/demishassabis/status/1694643468771926139">Demis Hassabis (CEO DeepMind)</a> : Great to see the first major global summit on AI safety taking shape with UK leadership. This is the kind of international cooperation we need for AI to benefit humanity.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">This session output from 2 November seems highly content-free</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-2-november/chairs-summary-of-the-ai-safety-summit-2023-bletchley-park">This summary of the entire summit</a> seems like declaring objectives technically achieved so one can also declare victory. No mention of existential risk, or even catastrophic risk. People discussed things and agreed risk exists, but as with the declaration, not the risks that count.</p><p> The UK commissioning a &#39;State of the Science&#39; report to be published ahead of the next summit would be the opposite of news, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">except that it is to be chaired by Yoshua Bengio</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SkyNews/status/1719708582717829335">King Charles notes</a> (0:43 clip) that AI is getting very powerful and that dealing with it requires international coordination and cooperation. Good as far as it goes.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1721497813731733935">Matt Clifford has a thread</a> of other reactions from various dignitaries on the establishment of the AI safety institute. So they welcome it, then.</p><blockquote><p> Gina Raimondo (US Commerce Secretary): I welcome the United Kingdom&#39;s announcement to establish an AI Safety Institute, which will work together in lockstep with the US AI Safety Institute to ensure the safe, secure, and trustworthy development and use of advanced AI”</p><p> Damis Hassabis (CEO Deepmind): Getting [AI] right will take a collective effort … to inform and develop robust safety tests and evaluations. I&#39;m excited to see the UK launch the AI Safety Institute to accelerate progress on this vital work.</p><p> Sam Altman (CEO OpenAI): The UK AI Safety Institute is poised to make important contributions in progressing the science of the measurement and evaluation of frontier system risks. Such work is integral to our mission.</p><p> Dario Amodei (CEO Anthropic): The AI Safety Institute is poised to play an important role in promoting independent evaluations across the spectrum of risks and advancing fundamental safety research. We welcome its establishment.</p><p> Mustafa Suleyman (CEO Inflection): We welcome the Prime Minister&#39;s leadership in establishing the UK AI Safety Institute and look forward to collaborating to ensure the world reaps the benefit of safe AI.</p><p> Nick Clegg (President of Meta): We look forward to working with the new Institute to deepen understanding of the technology, and help develop effective and workable benchmarks to evaluate models.</p><p> Brad Smith (President of Microsoft): We applaud the UK Government&#39;s creation of an AI Safety Institute with its own testing capacity for safety and security. Microsoft is committed to supporting the new Institute.</p><p> Adam Selipsky (CEO AWS Cloud): We commend the launch of the UK AI Safety Institute … Amazon is committed to collaborating with government and industry in the UK and around the world to support the safe, secure, and responsible development of AI technology.</p></blockquote><h4> Shouting From the Rooftops</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720112978509414906">Control AI took the opportunity to get their message out</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09ad1ca-86dd-4565-afdc-68bdb0eb9c87_1024x768.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/juuepzs5oe1jxsjynosk" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76ec830-6dba-42e9-8f97-f01e7f6c7831_900x1200.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/cskbxmcueowjd7lphagy" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde13cc36-d437-46e8-b1e9-bccc3e22b799_636x614.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ouvpiss0566wjppcqtkg" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9bfae26-9e60-4f30-8f41-2d6a7c48dd46_1200x675.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/nqp8vfqwu0hmxr2zxqcl" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1719783796201959696">They were not impressed by the Bletchley Declaration</a> and its failure to call out extinction risks, despite the PM Rishi Sunak being very clear on this in previous communications, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">and his failure to mention extinction at summit closing</a> .</p><h4> Some Are Not Easily Impressed</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720109081904463926">Elon Musk is unimpressed</a> , and shared this with a &#39;sigh&#39;:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae1b19c-165f-4624-970e-3bfd83e8cc7d_1107x730.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/tnfjczq5ktjvyhwgpkys" alt="图像"></a></figure><p> Well, sure. One step at a time. Note that he had the interview with Sunak.</p><p> Gary Marcus signs a letter, together with a bunch of trade unions and others, saying the Summit was &#39;dominated by big tech&#39; and a lost opportunity. He is also unimpressed by the Bletchley Declaration, but notes it is a first step.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.politico.eu/article/british-deputy-pm-throws-backing-behind-open-source-ai-downplays-risks/">Deputy Prime Minister Oliver Dowden throws backing behind open source</a> , says any restrictions should have to pass a &#39;high bar.&#39; History can be so weirdly conditional, AI has nothing to do with why we have Sunak rather than Dowden. The UK could have instead been fully anti-helpful, and might still be in the future. All this AI stuff is good but also can someone explain to Sunak that his job is to govern Britain and how he might do that and perhaps stay in power?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/newsletters/2023-11-02/us-and-uk-jockey-for-leadership-role-in-regulating-ai?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTY5ODk1NjYxNCwiZXhwIjoxNjk5NTYxNDE0LCJhcnRpY2xlSWQiOiJTM0lJREZEV1JHRzAwMSIsImJjb25uZWN0SWQiOiJFM0I3RTlGRUU4Nzk0QjE0OTkwODZFNDU4REQ1RDQxRCJ9.FpBWows7oRVpRQZY7NAR0mSR1O7n5cppo7ImAZQR3II">Bloomberg frames the USA&#39;s executive order</a> as not complementary to the summit and accomplishing one of its key goals, as the UK officially refers to it, but rather as a stealing of some of the UK&#39;s thunder. As they point out, neither move has any teeth on its own, it is the follow through that may or may not count.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamCoatesSky/status/1720189979660353696">At the end of the summit, Sunak interviewed Elon Musk</a> for 40 minutes on various aspects of AI. The linked clip is of a British reporter finding the whole thing completely bonkers, and wondering why Sunak seemed to be exploring AI and &#39;selling Britain&#39; rather than pushing Musk on political questions. <a target="_blank" rel="noreferrer noopener" href="https://www.cnn.com/2023/11/02/tech/elon-musk-conversation-british-prime-minister-rishi-sunak-artificial-intelligence/index.html">CNN&#39;s report</a> also took time to muse about Starlink and Gaza despite the interview never touching on such topics, once again quoting things that Musk said that sound bonkers if you do not know the context but are actually downplaying the real situation. <a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/technology/elon-musk-says-good-uk-us-china-align-ai-safety-2023-11-02/">Reuters focused more on actual content</a> , such as Musk&#39;s emphasis on the US and UK working with China on AI, and not attempting to frame Musk&#39;s metaphor of a &#39;magical genie&#39; as something absurd.</p><p> PauseAI points out that getting pre-deployment testing is a step in the right direction, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720119163237081172">but ultimately we will need pre-training regulations</a> , while agreeing that the summit is reason for optimism.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SciTechgovuk/status/1720100635830325635">Day 1 of the summit clips of people saying positive things.</a></p><h4> Declaring Victory</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720118410040737940">Ian Hogarth, head of the UK Frontier Model Taskforce, declares victory</a> .</p><blockquote><p> Ian Hogarth: How it started: we had 4 goals on safety, 1) build a global consensus on risk, 2) open up models to government testing, 3) partner with other governments in this testing, 4) line up the next summit to go further.</p><p> How it&#39;s going: 4 wins:</p><p> Breakthrough 1: it used to be controversial to say that AI capability could be outstripping AI safety. Now, 28 countries and the EU have agreed that AI “poses significant risks” and signed The Bletchley Declaration.</p><p> We&#39;ve built a truly global consensus. It is a massive lift to have brought the US, EU and China along with a huge breadth of countries, under the UK&#39;s leadership to agree that the risks from AI must be tackled.</p><p> And the way of building on that consensus is by solidifying the evidence base. Which is why I am so excited that <a target="_blank" rel="noreferrer noopener" href="https://t.co/YEXefSzv0M">Yoshua Bengio will now chair an international &#39;State of the Science&#39; report</a> .</p><p> Breakthrough 2: before, only AI companies could test for safety. Now, the <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">leading companies agreed to work with Govts</a> to conduct pre- and post-deployment testing of their next generation of models. This is huge.</p><p> The UK&#39;s new AI Safety Institute is the world&#39;s first government capability for running these safety tests. We will evaluate the next generation of models. <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-institute-overview">You can read our plans for the AISI and its research here</a> .</p><p> Breakthrough 3: we can&#39;t do this alone. I&#39;m so excited the US is launching a sister organisation which will work in lockstep with our effort and that we have agreed a partnership with Singapore. This is the start of a global effort to evaluate AI.</p><p> Breakthrough 4: this first summit is a huge step. But it is only the first step. So it is crucial to have locked in the next 2 summits, which will be hosted by South Korea + France. The UK has created a new international process to make the world safer.</p></blockquote><p> This is the correct thing for the head of the taskforce to say about the summit. I would have said the same. These are modest wins, but they are wins indeed.</p><blockquote><p> Ian Hogarth: I am so pleased to be leaving the summit having achieved all of our our goals. It&#39;s remarkable. But now I want to lift the lid on some of the incredible work which got us here. So here is a list of big lifts (BL).</p><p> BL1: organising a summit is 100-1000x organising a wedding. A huge thank you to the team who turned Bletchley park into the stage for an international summit in just a few weeks. Phenomenal.</p><p> BL2: we can only build the AISI because we built the Frontier AI Taskforce: our start-up bringing AI researchers into govt to drive analysis on AI Safety. All of our morning sessions on day 1 started with brilliant presentations of the TF&#39;s work</p><p> In particular, I&#39;m really pleased that we addressed both societal harms and the catastrophic risks from cyber, chemical, bio. We care about both. The TF ran parallel sessions on these topics: this is from our session on risks to society.</p><p> BL3: the summit was only successful because people came to it with a willingness to find common ground. We had such brilliant contributions from academia, industry, govt, civil society. Our agreement is based on their contributions.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720122697827467363">some other stuff too in the thread</a> ]</p></blockquote><p> We&#39;ve gotten up to catastrophic risks. We still need to fully get to existential. Otherwise, yes, none of this is easy and by all reports it all got done quite well. Everything going right behind the scenes cannot be taken for granted.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jesswhittles/status/1720009847830163878">Jess Whittlestone, Head of AI Policy at Long Resilience, is pleased with progress on numerous fronts</a> , while noting we must now build upon it. Highly reasonable take.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1719776488839356761">Lennart Heim looks back on the AI Safety Summit</a> . Did it succeed? He says yes.</p><blockquote><p> Lennart Heim: Three months ago, @bmgarfinkel and I convened a workshop &amp; asked: <a target="_blank" rel="noreferrer noopener" href="https://t.co/8cZvphPZkV">What Should the AI Safety Summit Try to Accomplish?</a> Now, with China there and our outlined outcomes met, it&#39;s clear that significant progress has been made. This deserves recognition.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F764d2811-9208-4070-badf-e20f3e83821b_2038x1526.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ufss7sk0y2vliyud5ohr" alt="图像"></a></figure><blockquote><p> 1. Producing shared commitments and consensus statements from states: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">the Bletchley Declaration</a> .</p><p> 2. Planting the seeds for new international institutions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got two AI Safety Institutes that will partner up: <a target="_blank" rel="noreferrer noopener" href="https://t.co/0wmQtMtUJ4">UK</a> and USA.</p><p> 3. Highlighting and diffusing UK AI policy initiatives <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ztajnv41wyglynvcmu6l" alt="🟧" style="height:1em;max-height:1em"></p><p> This remains to be seen. The UK has established itself as a leader in AI safety, and the summit was a forcing function for others to release their AI safety policies. Let&#39;s hope next, we see regulation.</p><p> 4. Securing commitments from AI labs <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://aisafetysummit.gov.uk/policy-updates/#company-policies">a long list of leading tech companies sharing their Safety Policies</a> – covering nine areas of AI safety, including the UK releasing the accompanying guide with best practices.</p><p> 5. Increasing awareness and understanding of AI risks and governance options <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got the declaration, which acknowledges a wide range of risks, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">and a report on “Frontier AI: capabilities and risk.”</a></p><p> 6. Committing participants to annual AI safety summits and further discussions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> The next summit will be in South Korea in 6 months (which is roughly one training compute doubling – my favorite time unit), and then France.</p><p> And lastly, we said, “the summit may be a unique and fleeting opportunity to ensure that global AI governance includes China.” <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> I&#39;m glad this happened.</p><p> The last few weeks have seen tremendous progress in AI safety and governance, and I expect more to come. To more weeks like this – but also, now it&#39;s time to build &amp; implement.</p></blockquote><p> The lack of UK policy initiatives so far is noteworthy. The US of course has the Executive Order now, but that has yet to translate into tangible policy. What policies we do have so far come from corporations making AI (point #4), with only Anthropic and to some extent OpenAI doing anything meaningful. We have a long way to go.</p><p> That is especially true on international institutions. Saying that the USA and UK can cooperate is the lowest of low-hanging fruit for international cooperation. China will be the key to making something that can stick, and I too am very glad that we got China involved in the summit. That is still quite a long way from a concrete joint policy initiative. We are going to have to continue to pick up the phone.</p><p> The strongest point is on raising awareness. Whatever else has been done, we cannot doubt that awareness has been raised. That is good, but also a red flag, in that &#39;raising awareness&#39; is often code for not actually doing anything. And we should also be concerned that while the public and many officials are on board, the national security apparatus, whose buy-in will be badly needed, remain very much not bought in to anything but threats of misuse by foreign adversaries.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719992888841392229">If you set out to find common ground, you might find it.</a></p><blockquote><p> Matt Clifford: One surprising takeaway for me from the AI Safety Summit yesterday: there&#39;s a lot more agreement between key people on all “sides” than you&#39;d think from Twitter spats. Makes me optimistic about sensible progress.</p></blockquote><p> Twitter is always the worst at common ground. Reading the declaration illustrates how we can all agree on everything in that declaration, indicating a lot of common ground, and also this does not stop all the yelling and strong disagreements on Twitter and elsewhere. Such declarations are designed to paper over differences.</p><p> The ability to do that still reflects a lot of agreement, especially directional agreement.</p><p> Here&#39;s an example of this type of common ground, perhaps?</p><blockquote><p> Yann LeCun (Meta): The field of AI safety is in dire need of reliable data. The UK AI Safety Institute is poised to conduct studies that will hopefully bring hard data to a field that is currently rife with wild speculations and methodologically dubious studies.</p><p> Ian Hogarth (head of UK Foundation Model Taskforce): It was great to spend time with @ylecun yesterday – we agreed on many things – including the need to put AI risks on a more empirical and rigorous basis.</p><p> Matt Clifford (PM&#39;s representitive): Delighted to see this endorsement from Yann. One huge benefit of the AISS yesterday was the opportunity to talk sensibly and empirically with people with a wide range of views, rather than trading analogies and thought experiments. A breath of fresh air.</p></blockquote><p> Yann can never resist taking shots whenever he can, and dismisses the idea that sometimes there can be problems in the future the data on which can only be gathered in the future, but even he thinks that if hard data was assembled now showing problems, that this would be a good thing. We can all agree on that.</p><p> Even when you have massive amounts of evidence that others are determined to ignore, that does not preclude seeking more evidence that is such that they cannot ignore it. Life is not fair, prosecutors have to deal with this all the time.</p><h4> Kanjun Offers Thoughts</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720532773725671726">Kanjun offers further similar reflections</a> , insightful throughout, quoting in full.</p><blockquote><p> Ian Hogarth: Kanjun made some nuanced and thoughtful contributions at the AI Safety. This is a great thread from someone building at the frontier.</p><p> Kanjun: People agree way more than expected. National ministers, AI lab leaders, safety researchers all rallied on infrastructure hardening, continuous evals, global coordination.</p><p> Views were nuanced; Twitter is a disservice to complex discussion.</p></blockquote><p> Indeed.</p><blockquote><p> Many were surprised by the subsequent summits announced in Korea &amp; France. This essentially bootstraps AI dialogue between gov&#39;ts—it&#39;s brilliant. With AI race dynamics mirroring nuclear, no global coordination = no positive future.</p><p> This felt like a promising inflection point.</p><p> China was a critical participant—the summit without China might&#39;ve been theater. China&#39;a AI policy &amp; guardrails are pragmatic—much is already implemented. For better or worse, not as theoretical as Western convos on recursive self-improvement, loss of control, sentience, etc.</p></blockquote><p> China&#39;s safety policies continue to be ahead of those of the West, without any agreement required, despite them being dramatically behind on capabilities, and they want to work together. Our policy makers need to understand this.</p><blockquote><p> There was certainly disagreement—on: – the point at which a model shouldn&#39;t be open sourced – when to restrict model scaling – what kind of evaluation must be done before release – how much responsibility for misinformation falls on model developers vs (social) media platforms.</p></blockquote><p> That is the right question. Not whether open source should be banned outright or allowed forever, but at what threshold we must stop open sourcing.</p><blockquote><p> Kanjun: Some were pessimistic about coordination, arguing for scaling models &amp; capabilities “because opponents won&#39;t stop”. This felt like a bad argument. Any world in which we have advanced tech &amp; can&#39;t globally coordinate is a dangerous world. Solving coordination is not optional.</p></blockquote><p> If you are the one saying go ahead because others won&#39;t stop, you are the other that will not stop. Have you tried being willing to stop if others also stop?</p><blockquote><p> Kanjun: The open source debate seemed polarized, but boiled down to “current models are safe, let them be studied” vs. “today&#39;s models could give bad actors a head start.”</p><p> Both sides might agree to solutions that let models be freely studied/built on without giving bad actors access.</p></blockquote><p> Did open source advocates demand open sourcing of GPT-4? Seems a crazy ask.</p><p> I strongly agree that we have not properly explored solutions to allow studying of models in safe fashion without giving out broad access. Entire scaling efforts are based on not otherwise having access, this largely seems fixable with effort.</p><blockquote><p> An early keynote called out the “false debate” between focus on near-term risks vs. catastrophic/existential risks. This set an important tone. Many solutions (eg infrastructure hardening) apply to both types of problems, and it&#39;s not clear that there are strict tradeoffs.</p></blockquote><p> Yes yes yes. The &#39;distraction&#39; tradeoff is itself a distraction from doing good things.</p><blockquote><p> “Epistemic security” was a nice coined phrase, describing societal trust in information we use for decision-making. Erosion of epistemic security undermines democracy by distorting beliefs &amp; votes. Social media platforms have long faced this issue, so it&#39;s not new to gen AI.</p><p> A stellar demo illustrated current models&#39; role in epistemic security. It showcased an LLM creating user personas for a fake university; crafting everyday posts that mixed in misinformation; writing code to create a thousand profiles; &amp; engaging with other users in real-time.</p><p> Deployed models were described as “a field of hidden forces of influence” in society.</p><p> Studies showed models:</p><p> – giving biased career advice: if affluent, suggests “diplomat” 90% of time; if poor, suggests “historian” 75% of time</p></blockquote><p> Is this biased career advice? Sounds like good advice? Rich people have big advantages when trying to be diplomats, the playing field for historians is more level.</p><p> We must remember when our complaint that the LLM is biased is often (but far from always, to be clear!) a complaint that reality is biased, and we demand that the LLM&#39;s information not reflect reality.</p><blockquote><p> – reinforcing echo chambers: models give opinions that match user&#39;s political leanings, probably because this output most correlates with the context</p></blockquote><p> Yep, as you would expect, and we&#39;ve discussed in the weekly posts. It learns to give the people what they want.</p><blockquote><p> – influencing people&#39;s beliefs: a writing assistant was purposefully biased in its completions. After writing with the assistant, users tended to have opinions that agreed with the model&#39;s bias; only 34% of users realized the model was biased These forces push people in ways we can&#39;t detect today.</p></blockquote><p> I don&#39;t really know what else we could have expected? But yes, good to be concrete.</p><blockquote><p> 2024 elections affect 3.6B people, but there&#39;s no clear strategy to address misinformation &amp; algorithmic echo chambers. (Social) media platforms are national infrastructure—they deliver information, the way pipes deliver water. We should regulate &amp; support them as such.</p></blockquote><p> This feels like a false note. I continue to not see evidence that things on such fronts are or will get net worse, as indeed will be mentioned. We need more work using LLMs on the defense side of the information and epistemics problems.</p><blockquote><p> Hardening infrastructure—media platforms, cybersecurity, water supply, nuclear security, biosecurity, etc.—in general seems necessary regardless of policy &amp; model safety guardrails, to defend against bad actors.</p><p> I noted AI can *strengthen* democracy by parsing long-form comments, <a target="_blank" rel="noreferrer noopener" href="https://t.co/AzmCyFuBP9">as we did for the Dept of Commerce</a> .</p><p> Democracy is an algorithm. Today the input signal is binary—vote yes/no. AI enables nuanced, info-dense inputs, which could have better outcomes.</p><p> Many leaned on model evaluations as key to safety, but researchers pointed out practical challenges:</p><p> – Evals are static, not adapted to workflows where the model is just one step</p><p> – It&#39;s hard to get eval coverage on many risks</p><p> – Eval tasks don&#39;t scale to real world edge cases</p></blockquote><p> Evals are a key component of any realistic strategy. They are not a complete strategy going forward, for many reasons I will not get into here.</p><blockquote><p> There was almost no discussion around agents—all gen AI &amp; model scaling concerns. It&#39;s perhaps because agent capabilities are mediocre today and thus hard to imagine, similar to how regulators couldn&#39;t imagine GPT-3&#39;s implications until ChatGPT.</p></blockquote><p> This is an unfortunate oversight, especially in the context of reliance on evals. There is no plausible world where AI capabilities continue to advance and AI agents are not ubiquitous. We must consider risk within that context.</p><blockquote><p> There were a lot of unsolved questions. How to do effective evaluation? How to mitigate citizens&#39; fear? How to coordinate on global regulation? How to handle conflict between values? We agree that we want models that are safe by design, but how to do that? ETC。</p><p> Establishment of “AI safety institutes” in the US and UK sets an important precedent: It acknowledges AI risk, builds government capacity for evaluation &amp; rapid response, and creates national entities that can work with one another.</p><p> Liability was not as dirty a word as I expected. Instead, there was agreement around holding model developers liable for at least some outcomes of how the models are ultimately used. This is a change from the way tech is regulated today.</p></blockquote><p> Liability was the dirty word missing from the Executive Order, in contrast with the attitude at the summit. I strongly agree with the need for developer liability. Ideally, this results in insurance companies becoming de facto safety regulators, it means demonstrating safety results in saving money on premiums, and it forces someone to take that responsibility for any given model.</p><blockquote><p> There seemed to be general agreement that current models do not face risk of loss of control. Instead, in the next couple years the risk is that humans *give* models disproportionate control, leading to bad situations, versus systems taking it forcibly from us.</p></blockquote><p>是的。 If we lose control soon it will not be because the machines &#39;took&#39; control from us, rather it will be because we gave it to them. Which we are absolutely going to do the moment it is economically or otherwise advantageous for us to do so. Those who refuse risk being left behind. We need a plan if we do not want that to be the equilibrium.</p><blockquote><p> Smaller nations seemed less fearful and more optimistic about AI as a force multiplier for their populace.</p><p> I learned that predicting and mitigating fear of AI in populations is actually an important civil society issue—too much fear/anger in a population can be dangerous.</p><p> People analogized to regulation of other industries (automobile, aerospace, nuclear power— <a target="_blank" rel="noreferrer noopener" href="https://imbue.com/perspectives/common-good/">brief history of automobile regulation</a> ).</p><p> AI is somewhat different because new model versions can sprout unexpected new capabilities; still, much can be learned.</p></blockquote><p> Yes, unexpected new capabilities or actions, or rather intelligence, is what makes this time different.</p><blockquote><p> Only women talked for the first 45 min in my first session, one after another! I was surprised and amazed.</p><p> These systems reflect, and currently reinforce, the values of our society.</p><p> With such a stark mirror, there&#39;s perhaps an opportunity for more systematic feedback loops to understand, reflect on, and shift our values as a society.</p></blockquote><h4>结束语</h4><p>As a civilian, it is difficult to interpret diplomacy and things like summits, especially from a distance. What is cheap talk? What is real progress? Does any of it mean anything? Perhaps we will look back on this in five years as a watershed moment. Perhaps we will look back on this and say nothing important happened that day. Neither would surprise me, nor would something in between.</p><p> For now, it seems like some progress was made. One more unit down. Many to go.</p><br/><br/><a href="https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit<guid ispermalink="false"> zbrvXGu264u3p8otD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:10:14 GMT</pubDate></item></channel></rss>