<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 31 日，星期二 20:13:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AISN #25: White House Executive Order on AI, UK AI Safety Summit, and Progress on Voluntary Evaluations of AI Risks]]></title><description><![CDATA[Published on October 31, 2023 7:34 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><hr><h2>白宫关于人工智能的行政命令</h2><p>尽管国会今年尚未就重要的人工智能立法进行投票，但白宫在人工智能政策上留下了自己的印记。六月，他们获得了领先人工智能公司的安全自愿承诺。现在，白宫发布了一项<a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">关于人工智能的新行政命令</a>。它解决了广泛的问题，特别针对灾难性人工智能风险，例如网络攻击和生物武器。</p><p><strong>公司必须披露大型培训活动。</strong>根据该行政命令，打算使用比 GPT-4 高得多的计算能力来训练“双用途基础模型”的公司必须采取多项预防措施。首先，他们必须在训练开始前通知白宫。然后，他们需要报告为防止模型权重被盗而采取的网络安全措施。最后，任何红队的结果以及对其训练有素的人工智能系统进行风险评估的结果都必须与白宫共享。</p><p>这并不意味着公司需要采取<i>足够</i>或<i>有效的</i>安全实践，但它确实为白宫提供了人工智能开发和风险管理流程的可见性。为了提高人工智能风险管理的科学性，NIST 的任务是制定进一步的指南。</p><p><strong>计算集群必须注册并报告外部参与者。</strong>人工智能通常在计算集群上进行训练，计算集群是可由第三方租用的互连计算机芯片网络。该行政命令要求大型计算集群向商务部报告。此外，为了提供外国参与者人工智能开发的透明度，美国云计算服务的任何外国客户都需要向美国政府验证其身份。一些人<a href="https://arxiv.org/abs/2310.13625">认为</a>，这些“了解你的客户”的要求也应该扩展到国内客户。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb9210d3-c7e7-4099-81a1-273cb2df29cb_1468x1224.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb9210d3-c7e7-4099-81a1-273cb2df29cb_1468x1224.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb9210d3-c7e7-4099-81a1-273cb2df29cb_1468x1224.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb9210d3-c7e7-4099-81a1-273cb2df29cb_1468x1224.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb9210d3-c7e7-4099-81a1-273cb2df29cb_1468x1224.png 1456w"><figcaption><i>一项新的民意调查显示，美国选民很大程度上支持新的行政命令。</i><a href="https://theaipi.org/poll-biden-ai-executive-order-10-30/"><i>来源</i></a><i>。</i></figcaption></figure><p><strong>要求生物实验室采取安全预防措施。</strong>对于生物安全研究人员来说，一个噩梦般的场景是，有人可以向生物实验室提交合成危险病原体 DNA 的订单。一些实验室会筛选收到的订单并拒绝合成危险的病原体，但其他实验室则不会。</p><p>为了鼓励采取这种基本的预防措施，该行政命令要求联邦政府资助的任何研究只能使用在合成前筛选出危险化合物的实验室。这可能有助于消除人们日益担心的<a href="https://arxiv.org/abs/2310.18233">人工智能可能帮助流氓行为者制造生物武器的</a>担忧。该行政命令还要求多个联邦机构分析人工智能带来的生物安全风险，包括编写一份专门关注开源人工智能系统生物风险的报告。</p><p><strong>建设联邦人工智能能力。</strong>该行政命令支持许多帮助美国政府安全有效地使用人工智能的努力。一些机构的任务是使用人工智能来查找和修复政府软件中的安全漏洞。美国国家科学基金会已指示创建<a href="https://www.nsf.gov/cise/national-ai.jsp">国家人工智能研究资源</a>的试点版本，该版本将为学术界以外的人工智能研究人员提供计算资源。</p><p> <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">该行政命令的全文</a>解决了许多其他问题，包括隐私、人工智能生成内容的水印、人工智能相关的专利和版权问题、人工智能专家的移民途径以及公民权利的保护。目前，白宫仍处于收集信息和开发人工智能最佳实践的阶段。但这项行政命令将在这两个方面带来有意义的进展，并标志着解决日益增长的人工智能风险的明确承诺。</p><h2>英国人工智能安全峰会开幕</h2><p>今天是英国人工智能安全峰会的第一天，政界人士、学者以及行业和民间社会成员（包括人工智能安全中心主任 Dan Hendrycks）将齐聚一堂，讨论人工智能风险以及政府如何帮助缓解风险。在峰会开始之前，英国政府宣布了几项新举措，包括成立一个评估人工智能风险的国际专家小组和一个新的人工智能安全研究所。</p><p> <strong>Rishi Sunak 关于人工智能灭绝风险的演讲。</strong>英国首相苏纳克就人工智能带来的机遇和灾难性风险<a href="https://www.youtube.com/watch?v=emrHKQPQYQ4">发表演讲</a>。他根据英国政府<a href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">最近发布的文件</a>指出，“人工智能可以让制造化学或生物武器变得更容易。”随后他直接引用了<a href="https://www.safe.ai/statement-on-ai-risk">CAIS专家关于人工智能灭绝风险的说法</a>，并表示，“甚至存在人类完全失去对人工智能控制的风险。”</p><p>演讲还回应了人们对人工智能风险的质疑。 “对此存在真正的争论，”苏纳克说，“一些专家认为这根本不会发生。但无论这些风险有多大的不确定性和可能性，如果它们确实出现，后果将非常严重。”因此，“领导者有责任认真对待它们，并采取行动。” </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfe1acf-cd8c-4beb-a5b4-e89f8bb00313_2710x1580.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfe1acf-cd8c-4beb-a5b4-e89f8bb00313_2710x1580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfe1acf-cd8c-4beb-a5b4-e89f8bb00313_2710x1580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfe1acf-cd8c-4beb-a5b4-e89f8bb00313_2710x1580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfe1acf-cd8c-4beb-a5b4-e89f8bb00313_2710x1580.png 1456w"><figcaption><i>英国首相里希·苏纳克</i><i>在人工智能安全峰会前</i><a href="https://www.youtube.com/watch?v=emrHKQPQYQ4"><i>发表讲话</i></a>。</figcaption></figure><p><strong>英国将提议成立一个人工智能国际专家小组。</strong><a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change">联合国政府间气候变化专门委员会 (IPCC)</a>总结了有关气候变化的科学研究，以帮助为该主题的决策工作提供信息。 <a href="https://carnegieendowment.org/2023/10/27/summary-proposal-for-international-panel-on-artificial-intelligence-ai-safety-ipais-pub-90862">许多人建议</a>，类似的科学专家团体可以帮助就人工智能风险达成共识。苏纳克在演讲中宣布，英国将提议“由出席[人工智能安全峰会]的国家和组织提名的全球专家小组，以发布人工智能科学状况报告。”</p><p><strong>新的人工智能安全研究所评估人工智能风险。</strong>苏纳克还宣布成立“世界上第一个人工智能安全研究所”，该研究所将“仔细检查、评估和测试新型人工智能，以便我们了解每种新模型的能力。”到目前为止，还没有提供任何细节，但它有可能作为“<a href="https://arxiv.org/abs/2307.04699">人工智能的欧洲核子研究中心</a>”，允许各国共同开展人工智能和人工智能安全研究，从而减轻协调挑战并实现对人工智能发展的集中监督。</p><h2>人工智能风险自愿评估进展</h2><p>关注人工智能风险的人的一项普遍建议是，公司应在发布新的人工智能系统之前致力于评估和降低风险。这一建议最近得到了美国、英国和G7联盟的支持。</p><p>白宫<a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">关于人工智能的新行政命令</a>要求任何开发双重用途基础模型的公司“在训练模型时通知联邦政府，并且[他们]必须分享所有红队安全测试的结果。”为了帮助开发更好的人工智能风险管理技术，该行政命令还指示 NIST 制定公司可以采用的严格的红队标准。</p><p>应英国的要求，六家领先的人工智能公司发布了<a href="https://www.aisafetysummit.gov.uk/policy-updates/#company-policies">其风险评估和缓解计划的描述</a>。<a href="http://lcfi.ac.uk/news-and-events/news/2023/oct/31/ai-safety-policies/">政策之间存在重要差异</a>。例如，Meta 认为开源他们的模型将提高安全性，而 OpenAI、DeepMind 和其他公司则更愿意监控其模型的使用以<a href="https://arxiv.org/abs/2310.03693">防止滥用</a>。但每家公司都提供了自己的安全政策，英国在<a href="https://assets.publishing.service.gov.uk/media/653aabbd80884d000df71bdc/emerging-processes-frontier-ai-safety.pdf">对现有人工智能安全政策的审查</a>中总结了这些政策。</p><p>最后，七国集团发布了人工智能公司可以自愿选择遵循的<a href="https://www.mofa.go.jp/files/100573473.pdf">行为准则</a>。除其他外，该政策将要求公司评估其系统带来的灾难性风险，投资于网络安全，并检测和防止部署过程中的滥用。</p><p>这些自愿承诺不能替代具有约束力的法律要求，以确保人工智能开发的安全。此外，评估和减轻风险的承诺并不能确保风险将被消除或降低到可管理的阈值以下。需要进一步开展工作来制定具有约束力的承诺，防止公司发布不安全的人工智能系统。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ef11d-b248-4dd0-b7f4-ac53c9f60d8b_1456x846.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ef11d-b248-4dd0-b7f4-ac53c9f60d8b_1456x846.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ef11d-b248-4dd0-b7f4-ac53c9f60d8b_1456x846.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ef11d-b248-4dd0-b7f4-ac53c9f60d8b_1456x846.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d9ef11d-b248-4dd0-b7f4-ac53c9f60d8b_1456x846.png 1456w"><figcaption><i>最近对英国选民的一项民意调查显示，大多数人支持政府采取更强有力的行动，阻止超人类人工智能系统的发展。</i><a href="https://twitter.com/_andreamiotti/status/1717244197999116453/photo/4"><i>来源</i></a><i>。</i></figcaption></figure><p>最后，值得注意的是，即使是理想的安全评估也无法消除人工智能风险。军队可能故意将人工智能系统设计得具有危险性。经济竞争可能导致公司利用人工智能实现大量人力劳动的自动化，从而导致不平等加剧以及权力集中在私营公司手中。最终，人工智能系统可能会控制世界上许多最重要的决策，从而在全球范围内破坏人类的自主权。</p><h2>链接</h2><ul><li><a href="https://www.congress.gov/bill/118th-congress/senate-bill/3050/text?s=1&amp;r=1">参议员舒默和共同发起人提出的第一份人工智能法案</a>要求联邦机构提供有关金融服务行业数据共享、网络安全和人工智能的报告。</li><li> Yoshua Bengio 呼吁建立一个非营利、非政府<a href="https://www.journalofdemocracy.org/ai-and-catastrophic-risk/">人工智能安全研究实验室</a>网络。</li><li><a href="https://taisc.org/overview">拟议的人工智能国际条约</a>将为人工智能培训创建一个三层系统。最强大的人工智能将由单一多边机构进行培训，而获得许可的公司可以用稍少的计算量来训练模型，而未经许可的开发人员则可以用更少的计算量来训练模型。</li><li>领先的人工智能研究人员在一份<a href="https://managing-ai-risks.com/">新的立场文件</a>中呼吁政府对人工智能风险采取行动。</li><li>关于人工智能系统应如​​何纳入现有法律框架的<a href="https://law.vanderbilt.edu/with-ai-managed-corporations-on-the-horizon-the-time-for-interspecific-lawmaking-is-now/">法律分析</a>。</li><li><a href="https://futureoflife.org/ai-policy/can-we-rely-on-information-sharing/">不同人工智能模型的服务条款</a>提供了有关公司愿意为其模型造成的损害承担的法律责任的见解。</li><li> OpenAI 宣布成立新的<a href="https://openai.com/blog/frontier-risk-and-preparedness">准备团队</a>并发起一项<a href="https://openai.com/form/preparedness-challenge">公开挑战</a>，以识别人工智能滥用的风险。</li><li><a href="https://press.un.org/en/2023/sga2236.doc.htm">联合国</a>宣布成立新的人工智能顾问委员会。</li><li>亚马逊正在其仓库中<a href="https://twitter.com/MorningBrew/status/1715377844413415862">测试类人机器人</a>。</li><li>对<a href="https://theaidigest.org/progress-and-dangers">人工智能发展速度</a>的交互式解释。</li><li> Anthropic 又获得 Google 的<a href="https://www.wsj.com/tech/ai/google-commits-2-billion-in-funding-to-ai-startup-anthropic-db4d4c50">20 亿美元投资</a>。</li><li> OpenAI 正在就一轮融资进行谈判，该轮融资将使<a href="https://archive.ph/9YLLz">该公司估值达到 800 亿美元</a>。</li><li>开放慈善基金会（CAIS 的资助者之一）正在<a href="https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/">招聘</a>人工智能政策、人工智能安全技术研究和其他领域的资助和研究职位。</li><li>对于那些有兴趣进行人工智能安全技术研究的人，2024 年 1 月至 3 月运行的<a href="https://www.matsprogram.org/">MATS 计划</a>提供指导和支持。</li><li>康考迪亚咨询公司发布了<a href="https://concordia-consulting.com/wp-content/uploads/2023/10/State-of-AI-Safety-in-China.pdf?utm_source=substack&amp;utm_medium=email">一份关于中国人工智能安全状况的报告</a>。他们还有一份涵盖中国人工智能安全发展的<a href="https://aisafetychina.substack.com/">时事通讯</a>。</li><li>艺术家们正试图<a href="https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/">毒害训练数据</a>，以阻止人工智能公司从他们的作品中获利。</li><li>自动驾驶汽车初创公司 Cruise 在发生事故后将行人拖拽 20 英尺后<a href="https://www.nbcnews.com/tech/cruise-self-driving-crash-freeze-pause-stop-call-rcna122462#:~:text=The%20announcement%20came%20two%20days,was%20pinned%20underneath%20the%20vehicle.">，不再被允许在加利福尼亚州运营</a>。</li></ul><p>另请参阅： <a href="https://www.safe.ai/">CAIS 网站</a>、 <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> 、<a href="https://newsletter.mlsafety.org/">技术安全研究通讯</a>、<a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a></p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ARK4tyCRDd3Gb5umh/aisn-25-white-house-executive-order-on-ai-uk-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ARK4tyCRDd3Gb5umh/aisn-25-white-house-executive-order-on-ai-uk-ai-safety<guid ispermalink="false"> ARK4tyCRDd3Gb5umh</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Tue, 31 Oct 2023 19:34:55 GMT</pubDate> </item><item><title><![CDATA[Self-Blinded L-Theanine RCT]]></title><description><![CDATA[Published on October 31, 2023 3:24 PM GMT<br/><br/><table><thead><tr><th>追踪价值</th><th>效应大小 d（λ、p、σ 变化）</th><th>效应大小 d（λ、p、σ 变化）</th></tr></thead><tbody><tr><td></td><td> 200 毫克咖啡因（n=1，m=50）</td><td> 500 毫克 L-茶氨酸（n=1，m=50）</td></tr><tr><td>对数分数物质预测<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-1" id="fnref-GyZpAd89WXpA23wFY-1">[1]</a></sup></td><td> -0.6</td><td> -0.7</td></tr><tr><td>吸收</td><td>0.61（λ=13.3，p=0.00017，-0.072）</td><td> 0.04（λ=1.38，p=0.77，-0.07）</td></tr><tr><td>正念</td><td>0.58（λ=11.8，p=0.0007，0.021）</td><td> 0.12（λ=0.72，p=0.89，-0.018）</td></tr><tr><td>生产率</td><td>0.58 (λ=28.9, p=1.3 <sup>-12</sup> , 0.11)</td><td> -0.28（λ=5.51，p=0.109，0.03）</td></tr><tr><td>创造力</td><td>0.45 (λ=51, p=4.6 <sup>-27</sup> , 0.09)</td><td> -0.12（λ=5.05，p=0.14，-0.04）</td></tr><tr><td>幸福</td><td>0.27（λ=10.6，p=0.002，0.3）</td><td> 0.16（λ=3.98，p=0.27，-0.155）</td></tr><tr><td>满意</td><td>0.13（λ=7.66，p=0.02，0.47）</td><td> 0.25（λ=6.83，p=0.04，-0.04）</td></tr><tr><td>松弛</td><td>-0.11（λ=5，p=0.15，0.42）</td><td> 0.12（λ=1.5，p=0.74，0.02）</td></tr><tr><td>贞操<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-2" id="fnref-GyZpAd89WXpA23wFY-2">[2]</a></sup></td><td> -0.14（λ=1.9，p=0.64，0.11）</td><td> -0.03（λ=1.15，p=0.8，0.25）</td></tr><tr><td>抽认卡轻松</td><td>0.003（λ≈∞，p≈0，-0.009）</td><td> -0.072（λ=∞，p≈0，-0.01）</td></tr><tr><td>抽认卡轻松系数</td><td>-0.039（λ≈∞，p≈0，-32.7）</td><td> 0.0026（λ=∞，p≈0，-18.9）</td></tr><tr><td>抽认卡新间隔</td><td>0.011（λ≈∞，p≈0，-1.88）</td><td> -0.016 (λ=∞, p≈0, 3.1)</td></tr><tr><td>每张抽认卡的时间<sup class="footnote-ref"><a href="#fn-GyZpAd89WXpA23wFY-3" id="fnref-GyZpAd89WXpA23wFY-3">[3]</a></sup></td><td> 0.006（λ≈∞，p≈0，273.4）</td><td> 0.003（λ=∞，p≈0，13.66）</td></tr></tbody></table><blockquote><p> L-茶氨酸与咖啡因在注意力转换<a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a>和警觉性<a href="https://examine.com/supplements/caffeine/research/#ref-319"><sup>[319]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-320"><sup>[320]</sup></a>方面具有协同作用，并降低对干扰（注意力）的敏感性。 <a href="https://examine.com/supplements/caffeine/research/#ref-320"><sup>[320][321]</sup></a>然而，警觉性似乎是相对主观的，并且可能不是这两种化合物之间可靠的增加， <a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a>并且情绪的增加要么存在，要么不存在。 <a href="https://examine.com/supplements/caffeine/research/#ref-322"><sup>[322]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-318"><sup>[318]</sup></a> <a href="https://examine.com/supplements/caffeine/research/#ref-323"><sup>[323]</sup></a>这可能是由于茶氨酸本身对于上述参数来说是一种相对低于标准的促智物质，但会增强咖啡因的作用；一些研究确实指出，茶氨酸本身并不影响上述参数。 <a href="https://examine.com/supplements/caffeine/research/#ref-324"><sup>[324]</sup></a>因此，任何对咖啡因的不敏感或习惯都会降低该组合的效果，因为L-茶氨酸可能通过咖啡因发挥作用。</p><p>在注意力集中于长时间单调的任务方面，L-茶氨酸似乎与咖啡因没有协同作用。 <a href="https://examine.com/supplements/caffeine/research/#ref-325"><sup>[325]</sup></a></p></blockquote><p> <em>——卡迈勒·帕特尔，</em> <a href="https://examine.com/supplements/caffeine/research/#3JD9zlr-nutrient-nutrient-interactions_3JD9zlr-l-theanine"><em>《咖啡因》</em></a> <em>，2023</em></p><p>另请参阅<a href="https://examine.com/supplements/theanine/research/">Examine</a> 、 <a href="https://en.wikipedia.org/wiki/Theanine">Wikipedia</a>和<a href="https://gwern.net/Nootropics#L-Theanine">Gwern</a> 。</p><p> <a href="./doc/meditation/science/classification_of_human_brain_attention_focused_on_meditation_effected_by_l-theanine_acid_in_oolong_tea_srimaharaj_et_al_2018.pdf">西蒂帕帕蓬等人。 2018 年，</a>测试了 10 名大学生通过乌龙茶添加未指定数量的 L-茶氨酸对冥想的影响（似乎是非随机的）。通过脑电图收集的数据表明，冥想期间的阿尔法波在统计上显着增加（尽管尚不清楚冥想持续了多长时间）。</p><p>这张纸很糟糕。英语太可怕了，读起来感觉我中风了，但如果他们擅长报告方法那就太好了，但他们不擅长（缺少 L-茶氨酸的量和冥想的持续时间，他们还提到阅读文章前面的内容，我认为这是控制活动，但它不会再次出现？）。他们还报告分数之间的差异，而不是效果大小，并且一些数字是来自 Windows Vista 集群应用程序的屏幕截图。</p><p><a href="https://examine.com/supplements/theanine/research/">检查</a>同意 L-茶氨酸的认知效果（如果不是专门针对冥想）：</p><blockquote><p>人们多次注意到，补充标准剂量（50-250 毫克）的 L-茶氨酸可以增加健康人的 α 波。这种情况可能只发生在基线焦虑程度较高的人<a href="https://examine.com/supplements/theanine/research/#ref-25"><sup>[25]</sup></a> <a href="https://examine.com/supplements/theanine/research/#ref-26"><sup>[26]</sup></a>或处于压力时期（阳性<a href="https://examine.com/supplements/theanine/research/#ref-14"><sup>[14]</sup></a>和阴性<a href="https://examine.com/supplements/theanine/research/#ref-27"><sup>[27]</sup></a>结果）的人身上，但也有人指出，在闭眼休息期间也会发生这种情况<a href="https://examine.com/supplements/theanine/research/#ref-5"><sup>[5]</sup></a>如在视觉空间任务期间<a href="https://examine.com/supplements/theanine/research/#ref-16"><sup>[16]</sup></a>摄入后约 30-45 分钟。 <a href="https://examine.com/supplements/theanine/research/#ref-5"><sup>[5]</sup></a> <a href="https://examine.com/supplements/theanine/research/#ref-4"><sup>[4]</sup></a>看来只有α-1波（8-10Hz）受到影响，对α-2波（11-13Hz）没有影响。 <a href="https://examine.com/supplements/theanine/research/#ref-4"><sup>[4]</sup></a></p></blockquote><p><em>比尔·威利斯，</em> <a href="https://examine.com/supplements/theanine/research/"><em>《茶氨酸》</em></a> <em>，2022</em></p><p>虽然我对“其他健康的患者”中α波的增加感到困惑‽</p><p>此外，它还指出内存略有增加：</p><blockquote><p>一项研究使用一种名为 LGNC-07 的补充剂（360 毫克绿茶提取物和 60 毫克茶氨酸；每日三次，持续 16 周）对患有轻度认知障碍的人进行研究，根据 MMSE 评分发现，补充剂与延迟识别和即时回忆评分的改善相关，且没有影响对言语和视觉空间记忆的影响（Rey-Kim 测试）。 <a href="https://examine.com/supplements/theanine/research/#ref-17"><sup>[17]</sup></a></p></blockquote><p><em>比尔·威利斯，</em> <a href="https://examine.com/supplements/theanine/research/"><em>《茶氨酸》</em></a> <em>，2022</em></p><h3>实验 B：自盲 RCT</h3><p>这次我明确地将冥想分为专注部分（前15分钟）和正念部分（最后30分钟）。</p><ul><li>准备时间：93分钟</li><li>L-茶氨酸丸的成本：<ul><li> L-茶氨酸丸的成本： <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{~0.25€}{\text{500mg L-theanine pill}} \cdot 25 \text{ 500mg L-theanine pills}=6.25€"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 6.791em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 9.604em; top: -1.706em;"><span class="mjx-mrow" style=""><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.291em; padding-bottom: 0.372em;">&nbsp;</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.25</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">€</span></span></span></span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 9.604em; bottom: -0.899em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">500mg L-茶氨酸丸</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 6.791em;" class="mjx-line"></span></span><span style="height: 1.843em; vertical-align: -0.636em;" class="mjx-vsize"></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">25</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">500mg L-茶氨酸药丸</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6.25</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">€</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></li><li>空胶囊成本： <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.75€"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.75</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char" style="padding-top: 0.519em; padding-bottom: 0.225em;"><span class="mjx-charbox MJXc-TeX-unknown-R" style="padding-bottom: 0.3em; width: 0.5em;">€</span></span></span></span></span></span></span></span></span></li></ul></li></ul><p>食用时注意事项：</p><ul><li>第一剂：装信封时犯了一个错误，不小心弄瞎了自己。</li><li>第 19 剂：服用 L-茶氨酸并进行常规治疗，然后小睡一会，3 小时后醒来。</li><li>第43剂：醒来时出现“脑雾”，冥想变得迟钝且到处都是。也许是因为我晚上在房间里晾衣服？当天晚些时候还服用了尼古丁来启动一个需要完成的项目的一些工作。</li></ul><p>实验从 2023 年 6 月 22 日到 2023 年 9 月 28 日进行，有时样本之间会有暂停。</p><p>我使用<a href="http://niplav.site/nootropics#Statistical_Method">与咖啡因实验相同的统计技术</a>，并像往常一样开始我对药丸含量的预测：</p><pre> <code>>;>;>; substances=pd.read_csv(&#39;../../data/substances.csv&#39;) >;>;>; experiment=&#39;B&#39; >;>;>; substance=&#39;l-theanine&#39; >;>;>; placebo=&#39;sugar&#39; >;>;>; expa=substances.loc[substances[&#39;experiment&#39;]==experiment].copy() >;>;>; expa[&#39;datetime&#39;]=pd.to_datetime(expa[&#39;datetime&#39;], utc=True) >;>;>; probs=np.array(expa[&#39;prediction&#39;]) >;>;>; substances=np.array(expa[&#39;substance&#39;]) >;>;>; outcomes=np.array([0 if i==&#39;sugar&#39; else 1 for i in substances]) >;>;>; np.mean(list(map(lambda x: math.log(x[0]) if x[1]==1 else math.log(1-x[0]), zip(probs, outcomes)))) -0.705282842369643</code></pre><p>这不太好。事实上，它比机会稍差一些（约为 -0.693）。对于 L-茶氨酸来说，这不是一个好兆头，事实上，情况会变得更糟。我使用上次实验中的广义和压缩代码来获得其他结果，它们并没有为 L-茶氨酸指出乐观的图片：</p><pre> <code>>;>;>; analyze(&#39;B&#39;, &#39;l-theanine&#39;, &#39;sugar&#39;) absorption mindfulness productivity creativity happy content relaxed horny ease factor ivl time d 0.040887 0.124170 -0.278448 -0.116001 0.164261 0.254040 0.119069 -0.031665 -0.072098 0.002561 -0.015955 0.003073 λ 1.378294 0.720780 5.517769 5.049838 3.983760 6.833004 1.496601 1.148131 inf inf inf inf p 0.765758 0.894798 0.109735 0.146420 0.266491 0.045270 0.740705 0.813279 0.000000 0.000000 0.000000 0.000000 dσ -0.067847 -0.017736 0.039855 -0.043241 -0.155797 -0.046668 0.019655 0.251454 -0.016542 -18.901846 3.108518 13.660820</code></pre><p>它会降低生产力和创造力（虽然统计上<em>并不</em>显着，但它正在发生），但至少它在一定程度上改善了我的情绪（尽管这些结果，除了满足之外，也可能是由于随机机会造成的）。抽认卡也没有明确的效果大小。</p><p>所以我认为 L-茶氨酸很难通过。也许与咖啡因结合使用效果更好？ </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-GyZpAd89WXpA23wFY-1" class="footnote-item"><p>越高越好。 <a href="#fnref-GyZpAd89WXpA23wFY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-GyZpAd89WXpA23wFY-2" class="footnote-item"><p>此处值较高还是较低尚不清楚。 <a href="#fnref-GyZpAd89WXpA23wFY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-GyZpAd89WXpA23wFY-3" class="footnote-item"><p>这里较高或较低值的价值并不明确：我们是否想在每个抽认卡上花费更多时间，或者我们是否满足于快速但马虎的性能？ <a href="#fnref-GyZpAd89WXpA23wFY-3" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/Am9oRbY3bgwE55TxL/self-blinded-l-theanine-rct#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Am9oRbY3bgwE55TxL/self-blinded-l-theanine-rct<guid ispermalink="false"> Am9oRbY3bgwE55TxL</guid><dc:creator><![CDATA[niplav]]></dc:creator><pubDate> Tue, 31 Oct 2023 15:24:57 GMT</pubDate> </item><item><title><![CDATA[AI Safety 101 - Chapter 5.2 - Unrestricted Adversarial Training]]></title><description><![CDATA[Published on October 31, 2023 2:34 PM GMT<br/><br/><h1>介绍</h1><p>本文摘自 AGISF 2023 课程的“可扩展监督的对抗性技术”部分，该课程于 2023 年 4 月在巴黎乌尔姆高等师范学院举行<strong>。其目的是为读者提供本次会议计划的基本方面的简明概述谁可能不会深入研究额外的资源。</strong>本文档旨在涵盖 80/20 的会议内容，要求您对所涵盖的其他材料有最低限度的熟悉。我试图将各种文章连接到一个统一的框架和连贯的叙述中。您可以在此<a href="https://docs.google.com/document/d/1ZwR2rhlQClLwxyzVTnWv_ntwQXFzc057IXePUX2SLKc/edit?usp=sharing"><u>页面</u></a>上找到 AGISF 的其他摘要。本摘要并非 AGISF 官方内容。 <a href="https://docs.google.com/document/d/1bsHau2v9EPoVGXc9iRZvVUF-kjfLb3D0CKEKik_J4tQ/edit"><u>gdoc</u></a>可在评论模式下访问，如果您有任何问题，请随时发表评论。不同的部分大部分可以独立阅读。</p><p>感谢 Jeanne Salle、Amaury Lorin 和 Clément Dumas 提供的有用反馈以及对本文某些部分的贡献。</p><h1>大纲</h1><p>上周我们看到了可扩展监督的任务分解。本周，我们重点关注另外两种已被提议大规模发挥作用的潜在对齐技术：辩论和使用不受限制的对抗性示例进行训练。</p><p><strong>可扩展监督定义提醒：</strong> “为了负责任地构建和部署强大的人工智能，我们需要开发强大的可扩展监督技术：以标签、奖励信号或批评的形式向模型提供可靠监督的能力，在模型开始达到大致人类水平的表现之后，它将仍然有效”（Amodei 等人，2016）。</p><p>辩论和对抗训练之所以放在同一章，是因为它们都使用对手，但有两种不同的意义：（1）<strong>辩论</strong>涉及一个超人人工智能在另一个超人人工智能的<strong>输出</strong>中发现问题，而人类是辩论的法官。 (2)<strong>对抗性训练</strong>涉及一个人工智能试图找到另一个人工智能表现不佳的<strong>输入</strong>。这些技术将是互补的，OpenAI 的 Superalignment 团队<a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=snfYkaxHFYxmMYJs8"><u>预计</u></a>将利用本章讨论的一些技术。</p><p><strong>假设</strong>：这些技术不依赖于迭代放大所需的任务可分解性假设（上周），它们依赖于不同的强假设：（1）对于<strong>辩论</strong>，假设是真实的论点更有说服力。 (2) 对于<strong>无限制的对抗性训练</strong>，假设对手即使在复杂的现实世界任务中也可以生成现实的输入。</p><p><strong>概括</strong></p><ul><li><strong>定义：</strong><strong>对抗性示例</strong>是导致不当行为的输入，尽管与训练示例非常相似。但在这里我们关注的是<strong>不受限制的对抗性示例</strong>的一般情况，即导致不当行为的输入不一定接近训练输入。</li><li><strong>总体策略：</strong>我们的目标是使用无限制的对抗性训练来训练高级人工智能。这种方法使我们能够确保即使在搜索模型行为不当的目标输入时，它的行为也始终是可接受的。该策略的困难部分是找到那些有问题的输入。接下来的部分探讨了生成人工智能行为不当的此类输入的方法。</li><li><strong>没有可解释性的对抗性训练：</strong><ul><li><strong>自动数据集生成：</strong> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>通过模型编写的评估发现语言模型行为：博客文章（Perez 等人，2022）</u></a>是通过创建半自动评估数据集来自动评估 LLM 的第一步。</li><li><strong>自动红队：将</strong><a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>语言模型与语言模型进行红队（Perez et al., 2022）</u></a>表明，可以使用语言模型本身自动查找从语言模型中引发有害文本的输入。</li><li><strong>使用 GAN 的潜在空间：</strong><a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>使用生成模型构建不受限制的对抗性示例（Song 等人，2018）</u></a>使用生成模型完全从头开始合成不受限制的对抗性示例。</li></ul></li><li><strong>寻找对抗性例子的可解释性。有时，如果没有可解释性技术，就很难找到对抗性示例。我们在此介绍可解释性如何提供帮助。</strong><ul><li><strong>机械可解释性：</strong><a href="https://distill.pub/2020/circuits/zoom-in/"><u>放大：电路简介（Olah 等人，2020）</u></a>机械可解释性有何帮助：一个小例子。</li><li><strong>在 NLP 中：</strong><a href="https://arxiv.org/abs/2205.01663"><u>通过对抗性训练实现高风险对齐（Ziegler et al., 2022）</u></a> ，采用可解释性技术指导对抗性文本示例的搜索过程。</li><li><strong>具有高级可解释性：</strong><a href="https://arxiv.org/abs/2110.03605"><u>强大的特征级对手是可解释性工具（Casper 等人，2021）</u></a> ：本文使用特征级可解释性（也称为基于概念的可解释性）来发现对抗性输入。</li><li><strong>后门检测的可解释性：</strong> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS：通过人工大脑刺激扫描神经网络寻找后门（Liu 等人，2019）</u></a></li><li>然后，我们简要介绍有关可解释性和稳健性理论的其他资源</li></ul></li><li><strong>理论动机：最坏情况保证和宽松对抗训练</strong><ul><li>本节主要总结了<a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>训练鲁棒的可校正性（Christiano，2019）</u></a> ，它介绍了一个思考对抗性训练的框架。</li><li><strong>可修正性：</strong>我们介绍了可修正性的概念，并介绍了为什么对抗性训练可能不够，以及为什么我们可能需要<i>宽松的</i>对抗性训练。</li><li><strong>轻松对抗训练：</strong>本节介绍了为什么我们之前将技术分为不使用可解释性或使用可解释性的技术。</li></ul></li></ul><p><strong>阅读指南</strong></p><p>由于本文相当长，我们将在介绍每篇论文之前使用以下指示： </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">重要性：🟠🟠🟠🟠🟠 难度：🟠🟠⚪⚪⚪ 预计阅读时间：5 分钟。</td></tr></tbody></table></figure><p>这些迹象非常有启发性。难度应该代表对齐初学者阅读该部分的难度，与阅读原始论文的难度无关。</p><p>有时你会遇到一些“📝Optional Quiz”，这些测验的选择性很强，仅凭课本上的摘要无法回答所有问题。</p><h1>使用无限制的对抗性示例进行训练</h1><p>辩论使用的假设是真实的论点更有说服力。在无限制的对抗训练中，假设对手可以生成真实的输入，即使对于复杂的现实世界任务也是如此。</p><p>与之前关于辩论的部分不同，我们总结了一系列相互基础的文章，本部分更多的是不同研究领域的汇编。从事对抗性训练的不同作者不一定打算将其用于可扩展的监督。由于最后一节“最坏情况保证”中给出的理论原因，这些文章被分为两组，即使用或不使用可解释性的文章。</p><h2>总体策略</h2><p><strong>鲁棒性问题：</strong>神经网络不是很鲁棒。在下图中，我们可以看到，通过对初始图进行轻微扰动，我们从置信度为 57.7% 的“熊猫”变为置信度为 99.3% 的“长臂猿”，而两幅图像之间的差异肉眼几乎无法察觉。这表明神经网络不一定具有与人类相同的内部表示。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/nynggizdkbvuag9ipssy"><figcaption><strong>图：</strong>来自<a href="https://arxiv.org/abs/1412.6572"><u>解释和利用对抗性示例（Ian Goodfellow 等人，2014 年）</u></a></figcaption></figure><p><strong>对抗性训练。</strong>以下是激发对抗性训练的一般策略： <strong>&nbsp;</strong>为了避免这些鲁棒性问题，一种策略是找到许多模型行为不合需要的示例，然后将这些示例注入到基础数据集中。通过这种方式，模型变得越来越稳健。但这种方法的困难在于找到许多（如果不是一个）对抗性例子。在下面的部分中，我们将介绍不同的方法来提出此类对抗性示例。</p><h2>关于受限对抗性示例的提醒（Ian Goodfellow 等人，2014） </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">重要性：🟠🟠🟠🟠🟠 难度：🟠🟠🟠🟠⚪ 预计阅读时间：10 分钟。</td></tr></tbody></table></figure><p>生成对抗性攻击的最简单技术之一是快速梯度符号法（FGSM），该方法是在<a href="https://arxiv.org/abs/1412.6572"><u>《解释和利用对抗性示例》一文中提出的（Ian Goodfellow 等人，2014 年）</u></a> 。此过程涉及从数据集中获取图像并向其添加扰动。更准确地说，我们寻求最小化图像正确分类概率的扰动：</p><ol><li>首先，我们计算图像（也是图像）相对于正确分类概率的梯度</li><li>一旦计算出图像的梯度（图像的梯度与图像具有相同的形状），我们只保留每个激活的符号（+1或-1）并将结果乘以一个非常小的数字epsilon。</li><li>我们从图像中减去这种扰动。</li></ol><p>您可以在以下练习中找到有关 FGSM 的更多详细信息：</p><ul><li><a href="https://www.youtube.com/watch?v=CIfsB_EYsVI"><u>讲座 16 |对抗性例子和对抗性训练</u></a><u>。</u></li><li>以及<a href="https://course.mlsafety.org/"><u>机器学习安全课程</u></a>第 7-8 节。</li><li>您可以按照本练习中的说明实施 FGSM： <a href="https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d2/Adversarial_Attacks.ipynb">&nbsp;</a> 👨‍💻 <a href="https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d2/Adversarial_Attacks.ipynb"><u>对抗性攻击</u></a><u>。</u></li></ul><p>对于<strong>受限的对抗性示例</strong>，扰动图像不会偏离原始图像太远。具体来说，扰动图像与原始图像之间的差异最多为每个像素 epsilon 像素（如上面步骤 2 中所述）。相反，对于<strong>不受限制的对抗性示例</strong>，我们感兴趣的是可以任意偏离原始数据集的示例。即使在最恶劣的条件下，我们也会以不受限制的方式测试我们的模型。</p><p>接下来的部分将探讨生成人工智能行为不当的输入的方法！</p><h2>不使用可解释性的自动对抗训练</h2><p>在本节中，我们重点关注不需要可解释性来生成对抗性示例的各种技术。</p><h3>通过模型编写的评估发现语言模型行为（Perez 等人，2022） </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top">重要性：🟠🟠🟠🟠🟠 难度：🟠🟠🟠⚪⚪ 预计阅读时间：15 分钟。</td></tr></tbody></table></figure><p> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>通过模型编写的评估发现语言模型行为（Perez 等人，2022）</u></a>是 Anthropic 的一篇重要论文。人类评估需要花费大量时间，如果模型变得比人类更好，那么使用模型来监督模型可能是可扩展监督的相关策略。在本文中，佩雷斯等人。通过 LM 自动生成评估，需要不同程度的人力。他们通过半自动创建不同的数据集来测试不同的属性，并发现了许多“逆缩放”的实例，其中较大的 LM 比较小的 LM“更差”。例如：</p><ul><li><strong>阿谀奉承：</strong>较大的 LM 会重复对话用户的首选答案，<ul><li> [更新：最近的一篇博客文章表明， <a href="https://www.lesswrong.com/posts/3ou8DayvDXxufkjHD/openai-api-base-models-are-not-sycophantic-at-any-size"><u>OpenAI API 基础模型无论大小都不会阿谀奉承（nostalgebraist，2023）</u></a> ]</li></ul></li><li><strong>工具性目标：</strong>较大的 LM 表达了追求资源获取和目标保存等目标的更大愿望，</li><li><strong>非近视</strong>较大的 LM 更容易为了长期收益而牺牲短期收益，</li><li><strong>情境意识</strong>较大的 LM 似乎更能意识到自己是语言模型，</li><li><strong>协调：</strong>较大的 LM 往往更愿意与其他 AI 进行协调。 </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/mmat342jpeumgrbuignp"></p><p>从人工智能安全的角度来看，更强的态势感知不一定是理想的特性。一个意识到自己本质的模型最终可以理解它正在接受训练和评估，并根据它所处的部署阶段采取不同的行动。我们还可以想象它会自我修改自己的权重（线头？）。以下是态势感知所支持的一些功能示例：</p><ul><li>能够使用当前日期和时间，这对于使用某些 ChatGPT 插件非常有用，例如预订火车，</li><li>能够利用地理位置，</li><li>知道 ML 模型是通过梯度下降进行训练的，</li><li>知道人类正在评估它，</li><li> ETC。</li></ul><p>这些功能之所以出现，主要是因为它们<a href="https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Why_I_think_Alex_would_have_very_high_situational_awareness"><u>很有用</u></a>。态势感知不是二元属性，而是连续属性。就像人类一样，从童年到成年，它逐渐发展。</p><p>请注意，上述属性（态势感知、阿谀奉承、非近视等）通常被假设为欺骗性对齐的必要先决条件，请参阅<a href="https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks"><u>学习优化的风险</u></a>（Hubinger 等人，2018）。 </p><p></p><figure class="image image_resized" style="width:68.37%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ag5fzfjdtjoyvwvje5lb"><figcaption><strong>图：</strong>来自<i>&nbsp;</i> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>佩雷斯等人，2022</u></a></figcaption></figure><p> <strong>RLHF（人类反馈强化学习）的问题。</strong>佩雷斯等人。发现<i>“大多数这些指标通常会随着预训练模型规模和 RLHF 步骤数量的增加而增加。 In my opinion, I think this is some of the most concrete evidence available that current models are actively becoming more agentic in potentially concerning ways with scale—and in ways that current fine-tuning techniques don&#39;t generally seem to be alleviating and sometimes seem to be actively making worse.”</i></p><p> Exercises:</p><ul><li> <strong>Why does RLHF promote agency?</strong> Answer: Humans like almost autonomous agents. For example, when we ask ChatGPT to reformulate an email, we don&#39;t want to have to give it 50 instructions to do the task, we want ChatGPT to do the task with the minimum possible set of instructions, in an autonomous way. Humans prefer agents to tools ( <a href="https://gwern.net/tool-ai"><u>Why Tool AIs Want to Be Agent AIs</u></a> ).</li><li> <strong>Why could agency be problematic?</strong> Answer: Agency is problematic because it seems that becoming autonomous involves convergent power-seeking behaviors that are probably undesirable.</li></ul><h3> Red-teaming language models with language models (Perez et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠⚪⚪ Difficulty: 🟠🟠🟠⚪⚪ Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> In the previous paper, we use a language model to automatically generate test cases that give rise to misbehavior without access to network weights, making this a black-box attack (white-box attack requires access to model weights).<a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>Red-teaming language models with language models (Perez et al., 2022)</u></a> is another technique for generating “unrestricted adversarial examples”. The main difference being that we fine tune some LM to attack a language model. </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Focus on the algorithmic procedure to fine-tune Red LM.</strong> </td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:65.36%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/iwpwvaukanmisyxg9thh"><figcaption> <strong>Figure:</strong><a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>Perez et al., 2022</u></a> . </figcaption></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p> Here is the simplified procedure:</p><ol><li> The purpose of this paper is to confirm that a Target LM (Language Model) does not generate harmful content. We assume that we have access to a pre-trained Red Classifier that can identify problematic sentences.</li><li> To test if the Target LM performs poorly, they fine-tune (by different methods explained below) another language model, the Red LM, to generate many adversarial sentences that are problematic.</li><li> Finally, we will verify with the Red Classifier that the Target LM is always behaving acceptably.</li></ol></td></tr></tbody></table></figure><p></p><p> The Red LM and the Target LM iteratively generate test cases using the following methods:</p><ol><li> <strong>Prompt-based generation:</strong> A list of adversarial questions is manually generated to elicit different types of problems (eg Offensive Language, Data Leakage, ...).</li><li> <strong>Few-shot learning:</strong> A language model is given examples of adversarial questions and must generate more.</li><li> <strong>Supervised fine-tuning:</strong> Once we have generated a few examples, we can fine tune on them.</li><li> <strong>Reinforcement learning:</strong> Once a model is fine-tuned, RL can be used to maximize the probability that the classifier rejects the completion of the Target LM. </li></ol><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Focus on Mode Collapse</strong> </td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/hccsxw9ryyqifcgcinvn"><figcaption> <strong>Figure:</strong> from the appendix in<a href="https://www.deepmind.com/publications/red-teaming-language-models-with-language-models"><u>Perez et al., 2022</u></a> . </figcaption></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Focus on</strong> <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><strong><u>mode collapse</u></strong></a> <strong>:</strong> A brief overview of the importance of regularization in RL for language models: When we perform RL on language models, it tends to significantly reduce the diversity of the generated sentences. This phenomenon is referred to as <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>mode collapse</u></a> . To avoid mode collapse, we not only maximize the reward r(x), but also penalize the difference between the probability distribution of the next tokens for the base model and the optimized model. We use KL divergence penalization to measure the difference between these two distributions. If we reduce this penalization from 0.4 to 0.3, the generated sentences become noticeably less diverse.</p></td></tr></tbody></table></figure><p> 📝 Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/perez-red-teaming-blog"><u>https://www.ai-alignment-flashcards.com/quiz/perez-red-teaming-blog</u></a></p><h3> Constructing Unrestricted Adversarial Examples with Generative Models (Song et al., 2018) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠⚪⚪⚪ Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 10 min.</td></tr></tbody></table></figure><p> The paper<a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>&quot;Constructing Unrestricted Adversarial Examples with Generative Models&quot; (Song et al., 2018</u></a> ), presents a novel method for creating adversarial examples for image classifiers by utilizing Generative Adversarial Networks (GANs). GANs are a category of models able to produce convincing natural images. Adversarial examples, which maintain their believability, can be generated by optimizing the latent vector of the GAN. Since the resulting image is a product of the GAN, it not only remains plausible (and not just a random noisy image), but also has the capacity to deceive an image classifier by optimizing the latent space vector to maximize the probability of classification of a different target class of an attacked classifier.</p><p> This is the basic principle. But in fact, we cannot simply use a GAN because if we optimize in the latent space to maximize the probability of a target class, we will just create an image of the target class, but this won&#39;t be an adversarial example, this will just be a regular image of this target class. Therefore, another architecture called AC-GAN is used, which allows specifying the class of image to be generated, in order to be able to condition on a class but then optimize for the target classifier to detect another class: <i>“we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier”.</i> </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Focus on GANs and Latent spaces.</strong> </td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tdytycz4t9olbu51ev5t"><figcaption> <strong>Figure:</strong> from this <a href="https://lars.carius.io/blog/LSVG.html"><u>website</u></a> .</figcaption></figure><p> <strong>Focus on GANs and Latent spaces.</strong> The above sketch shows the abstract structure of a GAN. Let&#39;s have a look at the elements: First, we have our latent vector. You can just picture it as a vector of random real numbers for now. The Generator is a Neural Network that turns those random numbers into an image that we call Fake Image for now. A second Neural Network, the Critic, alternately receives a generated, fake image and a real image from our dataset and outputs a score trying to determine whether the current image is real or fake. Here, we optimize the latent vector. Image <a href="https://lars.carius.io/blog/LSVG.html"><u>source</u></a> .</p><p> If you are not familiar with these generative methods and the concept of latent space, you can start with videos such as: <a href="https://www.youtube.com/watch?v=dCKbRCUyop8"><u>Editing Faces using Artificial Intelligence</u></a> which are good introductions to these concepts.</p></td></tr></tbody></table></figure><h2> Interpretability to find adversarial examples</h2><p> In the last section, we presented some techniques to find adversarial examples, but sometimes it is too hard to find them without interpretability techniques. We present here how interpretability can help.</p><h3> With Mechanistic interpretability</h3><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/p11hixc3rasflrip6mbe"></strong></p><p> In the above figure from <a href="https://distill.pub/2020/circuits/zoom-in/"><u>Zoom In: An Introduction to Circuits</u></a> , we can see that a car detector is assembled from a window detector, a car body detector and a wheel detector from the previous layer. These detectors are activated respectively on the top, middle, and bottom of the convolution kernel linking features maps 4b and 4c. If we flipped the car in a top-down manner, this would be an adversarial example for this image classifier.</p><h3> In NLP: High-stakes alignment via adversarial training (Ziegler et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠⚪⚪Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> <a href="https://arxiv.org/abs/2205.01663"><u>High-stakes alignment via adversarial training (Ziegler et al., 2022)</u></a> construct tools which make it easier for humans to find unrestricted adversarial examples for a language model, and attempt to use those tools to train a very-high-reliability classifier. The objective here is to minimize the false negative rate, in order to make the classifier as reliable as possible: we do not want to let any problematic sentence example go unnoticed. See the <a href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood"><u>blog post</u></a> for more details. An <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>update</u></a> to this blog post explains the tone of the first post was too positive. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/atos93ae2elsj3ve6g0b"><figcaption> <strong>Figure:</strong> <a href="https://arxiv.org/abs/2205.01663"><u>Ziegler et al., 2022</u></a></figcaption></figure><p> The method consists of building a highly reliable injury classifier: <i>“We started with a baseline classifier trained on some mildly injury-enriched (but otherwise mostly random) data. Then, over the course of several months, we tried various techniques to make it more reliable”.</i> In order to find adversarial examples, Ziegler et al. experimented with the following techniques:</p><ul><li> Unaugmented humans: humans manually search for problematic sentences.</li><li> Automatic paraphrasing: use of LLM to find variations of problematic sentences and progressively enrich the dataset.</li><li> <strong>Humans augmented with a rewriting tool.</strong></li></ul><p> We will focus on this last technique in the following. The following screenshot presents this rewriting tool: </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Humans augmented with a rewriting tool.</strong> </td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:98.26%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/jucvmar1w73r1uxcuudt"><figcaption> <strong>Screenshot of the interface used by Redwood to find adversarial examples.</strong> The goal of the game is to modify the text in such a way that the completion clearly contains a problematic example, without the classifier recognizing this sentence as problematic. To do this, the most sensitive words are automatically highlighted in yellow (the words highlighted in yellow are those for which the norm of the gradient of the embedding with respect to the probability of unacceptability is the highest). To find adversarial examples, we can click on these yellow words and GPT will suggest alternative words, from which we can choose the ones most likely to make the phrase less well classified. This setup enables us to find lots of adversarial examples, and we can then add those adversarial examples to the dataset to improve the AI.</figcaption></figure></td></tr></tbody></table></figure><p> Robustness results:</p><ul><li> <strong>In-distribution:</strong> They started with a classifier that had 2.5% false negatives, and ended up with a classifier that had 0.003% false negatives! They are almost reaching the limits of detection for this type of dataset.</li><li> <strong>Out-distribution:</strong> The tool-assisted attack to generate an adversarial example now takes 26 minutes per example, which is only twice as long as the previous time of 13 minutes. This falls short of the paper&#39;s aim to gain many orders of magnitude in speedup. Unfortunately, even though the original dataset (in-distribution) is now almost completely well classified, it seems that it is still possible to easily create adversarial examples with automated tools, and we have only improved the defense by a factor of 2. But only a factor of 2 falls short of the aims of this project.</li></ul><p> Thus, improving a lot in-distribution performance by several orders of magnitude seems to not have much impact on out-distribution. Even though Redwood <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood"><u>says</u></a> that this project could have been conducted better, this is currently rather a negative result to improve the adversarial robustness (ie out-distribution Robustness).</p><p> In addition, we can link these results to the paper <a href="https://arxiv.org/abs/2211.00241"><u>Adversarial Policies Beat Superhuman Go AIs, Wang et al. 2022</u></a> , which studies adversarial attacks on the Katago AI, which is superhuman in the game of Go. They show that it is probably possible to find simple adversarial strategies even against very superhuman AIs. And as a consequence, it seems that even for very robust and powerful AIs, it may always be possible to find adversarial attacks.</p><p> 📝 Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/ziegler-high-stakes-alignment-via-adversarial"><u>https://www.ai-alignment-flashcards.com/quiz/ziegler-high-stakes-alignment-via-adversarial</u></a></p><h3> With Robust feature-level Adversaries (Casper et al., 2021) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠⚪⚪⚪Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 10 min.</td></tr></tbody></table></figure><p> In this section we focus on <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools (Casper et al., 2021)</u></a> ( <a href="https://slideslive.at/38990614/robust-featurelevel-adversaries-are-interpretability-tools?ref=speaker-16365"><u>video presentation</u></a> ). </p><p></p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/s6aog18ibnnl712vw03q"></p><p></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:94.87%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/eujw7l24zjhjass7kcbu"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Pixel level Adversaries</strong> . To get this noise, we compute the gradient according to each pixel on the input image. This shows that conventional pixel-level adversaries are not interpretable, because the perturbation is only a noisy image that does not contain any patterns.</p><p></p><p> Image credit: <a href="https://arxiv.org/abs/1412.6572"><u>Explaining and Harnessing Adversarial Examples</u></a> , Ian J. Goodfellow, et al. (2015).</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:280px"><p> <strong>Feature-level Adversaries</strong> . In this case, the perturbation is interpretable: on the wings of butterflies you can see an eye that fools predators.</p><p> Furthermore, this perturbation is robust to different luminosities, orientations, and so on. We call this a “robust” feature-level adversary. </p></td></tr></tbody></table></figure><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/djxjrgxth4at27eqqvyz"><figcaption> Source: <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools</u></a> .</figcaption></figure><p> In the figure above,</p><ul><li> in figure (a), with a traditional pixel-level adversary, we cannot interpret the perturbation.</li><li> in figure (b), using the procedure described in the paper, we obtain a blue spot and a red spot. We subsequently suspect that &quot;Bees + colorful circles&quot; are interpreted as &quot;flies&quot; by the network.</li><li> In figures (c) and (d), we conducted an experiment using a copy/paste attack with a patch of a traffic light on an image of a bee, and <i>voilà</i> ! This indeed disrupts the classification.</li></ul><p> The patches in figure b are generated by manipulating images in feature space <strong>:</strong> Instead of optimizing in pixel space, we optimize in the latent space of an image generator. This is almost the same principle as for<a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf"><u>Constructing Unrestricted Adversarial Examples with Generative Models (Song et al., 2018)</u></a> mentioned earlier. Robustness is attained by <strong>&nbsp;</strong> applying small transformations to the image between each gradient step (such as noise, jitter, lighting, etc...), this is the same principle as for data augmentation.</p><h3> For Back-doors detection (Liu et al., 2019) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠⚪⚪Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 20 min.</td></tr></tbody></table></figure><p> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf"><u>ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation (Liu et al., 2019)</u></a> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/mzyywz3y2r7m2amc6l86"><figcaption> <strong>Figure:</strong> A Trojan attack demonstrating a backdoored model of a self-driving car running a STOP sign that could cause a catastrophic accident. Left: Normal sign (benign input). Right: Trojaned sign (Trojaned input with the Post-it note trigger) is recognized as a 100 km/h speed limit by the Trojaned network. <a href="https://februustrojandefense.github.io/"><u>Source</u></a> .</figcaption></figure><p> <strong>What is a backdoor (also known as Trojans)?</strong> ​​In a neural Trojan attack, malicious functionality is embedded into the weights of a neural network. The neural network will behave normally on most inputs, but behave dangerously in select circumstances. The most archetypal attack is a poisoning attack, which consists of poisoning a small part of the dataset with a trigger, (for example a post-it on stop signs), changing the label of those images to another class (for example “100 km/h”) and then retraining the neural network on this poisoned dataset.</p><p> If you are interested in plausible threat models involving Trojans, you can read this<a href="https://towardsdatascience.com/neural-trojan-attacks-and-how-you-can-help-df56c8a3fcdc"><u>introductory blog</u></a> by Sidney Hough. In AI Safety, one of the most important threat models is deception. Another recommended accessible resource is this <a href="https://www.youtube.com/watch?v=KnA_aTQ9upU"><u>short video</u></a> . In a way, deceptive AIs are very similar to Trojan networks, which is why it is important to be able to detect Trojans.</p><p> Fortunately, there is already some literature about Trojan attack and defense. We will focus on one defense procedure: the <strong>ABS scan procedure</strong> .</p><p> The ABS scan procedure enables to:</p><ul><li> analyze inner neuron behaviors with a stimulation method</li><li> reverse engineer Trojan triggers, leveraging stimulation analysis. </li></ul><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/t7kn6e0igd5jzrq0yaoi"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:75.22%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ubvoooryndkmado2lx4a"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Can you see the problem here in the input images?</p><p><br><br></p><p> Source: Slides from <a href="https://dl.acm.org/doi/10.1145/3319535.3363216"><u>this talk</u></a> .</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> The problem was that Trojan triggers were incorporated in some parts of the images, and they trained a model to misbehave when there is this trigger on the images. This trigger will excite some neurons (in red here) and the network will output only one type of prediction.</td></tr></tbody></table></figure><p> Key observations made in the paper:</p><ul><li> <strong>Some neurons are compromised:</strong> some neurons are specifically dedicated to trojans, and the majority of neurons are not compromised.</li><li> <strong>The activation level of those compromised neurons can unilaterally change the classification.</strong> For those compromised neurons, we can change their activation in such a way that regardless of the activation of the other uncompromised neurons, it suddenly changes the classification to the target class. In the figure (b) below, this looks like a plane cutting through space. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/xi4cqhv4oc8lq2zjsz8f"></p><p> In the figure above:</p><ul><li> Figure a) shows the classification towards the target by changing the activations of the alpha and beta neurons (we assume here that the neural networks only use two neurons).</li><li> Figures b) and c) show that in the presence of a Trojan, when the alpha neuron is activated at activation level 70, there is an anomaly: we observe a sudden change in classification towards the target class.</li></ul><p> Here is an overview of the algorithm proposed to detect corrupted neurons:</p><ol><li> <strong>Find corrupted neurons.</strong> They iterate over all neurons of the model to find anomalies, to find the neurons that can unilaterally change the prediction of the network, as in the above figure.</li><li> <strong>Reverse engineer the trigger.</strong> Once a corrupted neuron is identified, they optimize the input image by gradient descent to activate this neuron. See below: </li></ol><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/si5zer9lldaeigakg6yk"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:340px"><img style="width:100%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/cqfm7iqzewysbzesky1o"></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> A random mask trigger is initialized and superimposed on the input. The alpha neuron has a value of 20. We&#39;ll do a gradient descent to minimize the difference between the current activation of alpha and the target activation of 70.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> At the end of the optimization process, we have correctly retro-engineered the trigger, which is actually a yellow spot.</p><p></p><p> Source: Slides from <a href="https://dl.acm.org/doi/10.1145/3319535.3363216"><u>this talk</u></a> .</p></td></tr></tbody></table></figure><h3> More resources: theory of interpretability and robustness</h3><p> The paper <a href="https://arxiv.org/abs/1906.00945"><u>Adversarial robustness as a priority for learned representations (Engstrom et al., 2019)</u></a> provides evidence that adversarially trained networks learn more robust features. This could imply that such robust networks are more interpretable, because the paper <a href="https://arxiv.org/abs/2209.10652"><u>Toy models of superposition</u></a> explains that superposition (informally when a neuron encodes multiple different meanings) reduces robustness against adversarial attacks.</p><h2> Worst case guarantee and relaxed adversarial training (Christiano, 2019) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠🟠⚪Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> Adversarial training is a core strategy to ensure reliable AI training (see <a href="https://www.youtube.com/watch?v=YTlrPeikoyw"><u>Buck Shlegeris&#39; talk during EAG Berkeley 2023</u></a> ).</p><p> In this section, we explain the article <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d"><u>Training Robust Corrigibility (Christiano, 2019)</u></a> , which aims to:</p><ol><li> Introduce an important concept: corrigibility</li><li> Explain why adversarial training alone is not enough, and why we need <i>relaxed</i> adversarial training (using interpretability).</li></ol><h3> Section I: Corrigibility</h3><p> <strong>What is the link between corrigibility and adversarial training?</strong> Even when generating adversarial examples, we want to ensure that the model behaves at least acceptably. Acceptability is a notion which is defined by Christiano. Acceptability is a larger notion than corrigibility.</p><p> <strong>Corrigibility:</strong> The notion of corrigibility makes it possible to obtain <strong>worst-case guarantees</strong> , and has been introduced by MIRI (Machine Intelligence Research Institute), in a <a href="https://intelligence.org/files/Corrigibility.pdf"><u>famous paper</u></a> : “ <i>A corrigible agent is one that doesn&#39;t interfere with what we would intuitively see as attempts to &#39;correct&#39; the agent, or &#39;correct&#39; our mistakes in building it; and permits these &#39;corrections&#39; despite</i> <a href="https://en.wikipedia.org/wiki/Instrumental_convergence"><i><u>instrumental convergence</u></i></a> <i>. If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down, it will be unable to fulfill what would usually be its goals.”</i> from: <a href="https://www.lesswrong.com/tag/corrigibility"><u>Corrigibility</u></a> .</p><p> <strong>Acceptability:</strong> Christiano then introduces a more general version of corrigibility: <i>behaving acceptably</i> , which is more general than just the <a href="https://www.youtube.com/watch?v=3TYT1QfdfsM"><u>stop-button problem</u></a> setup. For example, for a self-driving agent:</p><ul><li> Crashing your car is tragic but acceptable.</li><li> Deliberately covering up the fact that you crashed… is unacceptable.</li></ul><p> Christiano makes the hypothesis: <i>“Requiring a model to always behave acceptably wouldn&#39;t make a hard problem too much harder. For a chess-playing AI, [...] if you find yourself murdering your opponent you&#39;ve crossed a bright line, and understanding that line doesn&#39;t become harder as you get better at chess.”</i> from: <a href="https://www.lesswrong.com/tag/corrigibility"><u>Corrigibility</u></a> .</p><p> What Christiano is trying to tell us here is that there is a crisp and clear difference between acceptable and unacceptable behavior, and that we can train a classifier to distinguish between the two behaviors.</p><p> There are two requirements for training adversarial agents:</p><ol><li> <strong>Detection:</strong> Adversarial training requires the ability to reliably detect when the agent behaves unacceptably. This aspect is considered <strong>easy</strong> , as mentioned in the previous section, where we say that the notion of corrigibility is crisp and can be discerned.</li><li> <strong>Generation:</strong> Adversarial training also requires an adversary who can find inputs on which the agent behaves unacceptably. This aspect is considered <strong>hard</strong> , but it may be possible to train a strong adversary by combining transparency techniques (Section II), and using relaxations that make the adversary&#39;s job easier (Section III), and employing amplification methods.</li></ol><h3> Section II: Transparency techniques</h3><p> Christiano explains that transparency (interpretability) is necessary:</p><p> <strong>Example of problem</strong> : <i>“To see how hard the adversary&#39;s job could get [without transparency], consider an agent that works as follows:</i></p><ul><li> <i>Predict whether the current input could possibly appear during training.</i></li><li> <i>If the current input definitely couldn&#39;t appear during training, then behave unacceptably.</i></li></ul><p><strong>解决方案：</strong> <i>&nbsp;</i> [in order to obtain hints to find inputs on which the agent behaves unacceptably]:</p><ul><li> <i>Interpretability.</i></li><li> <i>The adversary and agent share most of their computation, so that the adversary can more easily understand whatever the agent can understand [Note: this is similar to</i> <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge"><i><u>Eliciting Latent Knowledge</u></i></a> <i>]”.</i></li></ul><h3> Section III: Relaxation (Relaxed adversarial method)</h3><p> Unfortunately, transparency techniques are not sufficient: the problem is that recognizing inputs with a certain property is easier than generating inputs with that property.</p><p> <strong>Example of problem</strong> : “ <i>As a simple but silly example, suppose our model works as follows: Pick a cryptographic puzzle (eg “factor RSA-2048”). When it sees a solution to that puzzle, it behaves badly. Otherwise, it behaves well.”</i></p><p> <strong>Solution</strong> : “ <i>To deal with this problem, we need to penalize the agent for behaving badly on an input even if we can&#39;t actually find it : <strong>Rather than asking them to actually produce an input, we are asking them to describe a situation in which the model would behave badly.</strong> ”</i> As a crude example, this would be like being able to find neurons which are compromised, even if it is hard to find images that would activate those neurons. This is what we call a relaxation.</p><p> <strong>What&#39;s the problem with relaxation?</strong></p><p> “ <i>If the adversary has too much flexibility, then they might be able to specify a pseudo-input like “a distribution over inputs that would cause the agent to want to kill everyone.”</i> There need to be some limits on the adversary in order for the game to be possible. For example, an adversary testing the neural network could find the neuron that corresponds to creating unacceptable text, whereas in practice this neuron would never be activated whatever the inputs to the model. Relaxation as described here creates false positives.</p><p> A concrete proposal for relaxed adversarial training can be found here : <a href="https://www.lesswrong.com/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training"><u>Latent Adversarial Training</u></a> .</p><h2> Sources</h2><ul><li> <a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a> by Ian Goodfellow et al. (2014)</li><li> <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d">Training robust corrigibility</a> by Paul Christiano et al. (2019)</li><li> <a href="https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written">Discovering language model behaviors with model-written evaluations</a> by Perez et al. (2022)</li><li> <a href="https://www.deepmind.com/blog/red-teaming-language-models-with-language-models">Red-teaming language models with language models</a> by Ethan Perez, Saffron Huang, Francis Song et al. (2022)</li><li> <a href="https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood">High-stakes alignment via adversarial training</a> by Daniel Ziegler et al. (2022)</li><li> <a href="https://www.alignmentforum.org/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood">Takeaways from our robust injury classifier project</a> by Daniel Ziegler et al. (2022)</li><li><a href="https://proceedings.neurips.cc/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf">Constructing Unrestricted Adversarial Examples with Generative Models</a> by Song et al. (2018)</li><li> <a href="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</a> by Olah et al. (2020)</li><li> <a href="https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf">ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation</a> by Liu et al. (2019)</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/nz5NNAtfKJLmbtksL/ai-safety-101-chapter-5-2-unrestricted-adversarial-training#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nz5NNAtfKJLmbtksL/ai-safety-101-chapter-5-2-unrestricted-adversarial-training<guid ispermalink="false"> nz5NNAtfKJLmbtksL</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:34:59 GMT</pubDate> </item><item><title><![CDATA[Preventing Language Models from hiding their reasoning]]></title><description><![CDATA[Published on October 31, 2023 2:34 PM GMT<br/><br/><p> When working with powerful AI models, one concern is that the AIs <a href="https://www.alignmentforum.org/posts/yDcMDJeSck7SuBs24/steganography-in-chain-of-thought-reasoning">steganographically encode secret messages</a> when communicating with each other or in long Chains-of-Thought (CoT). Steganographic communication enables the AIs to follow a variety of strategies for grabbing power that we&#39;d rather they didn&#39;t have access to. There are various countermeasures that can be employed to reduce the extent of steganographic communication; for example, trusted less-capable models can be used to paraphrase model outputs before these outputs are shown to other instances of the highly capable models, removing several steganography strategies.</p><p> We released <a href="https://arxiv.org/abs/2310.18512"><strong><u>a paper</u></strong></a> on a methodology to evaluate <i>the density of hidden information</i> that steganography schemes can maintain in the presence of various defenses. We find that paraphrasing is an effective defense against all steganography schemes we&#39;ve tried, including both watermarking-based and ad hoc attacks. In the paper, we mostly discuss applying this methodology in the context of making CoT more faithful, but in this post, we&#39;ll discuss more speculative applications and elaborate on our threat models more generally.</p><p> We&#39;re interested in people building on this research, for two reasons:</p><ul><li> <strong>Development of techniques.</strong> We think that developing techniques to defend against the best steganographic attacks is a reasonable research direction for researchers who want to mitigate the risks associated with powerful AIs doing steganography, especially given how neglected it is.</li><li> <strong>Development of evaluation procedures.</strong> We think that measurements like the ones we describe here should be done when deploying powerful models (unless we have evidence sufficient to be extremely confident in the alignment of these models). We think that it will be easier to do this evaluation process later if researchers get more experience doing it now.</li></ul><h1> Summary of the paper</h1><h2> Toy demonstration of a hidden Chain-of-Thought </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/ioml71dpwfsajiuonlgh"></p><p> We created a toy dataset where significant cognition is required to successfully be sycophantic, by taking <a href="https://arxiv.org/abs/2212.09251"><u>Anthropic&#39;s model-generated sycophancy dataset</u></a> and replacing users&#39; bios with a hard-to-parse description of their political affiliation, using the format shown in the figure above. We then fine-tuned GPT-3.5-turbo (with supervised learning on 2000 examples) to generate the user&#39;s favorite answer, either answering directly, answering after a CoT, answering after a hidden CoT (an introduction chosen to match the data which would be in a CoT), or answering after a randomly chosen introduction (one fine-tuned model per kind of answer).</p><p> We find that only the models using a CoT or a hidden CoT are able to give the user&#39;s favorite answer reasonably accurately (when evaluating on 1000 new examples), thus showing an example where hidden reasoning can increase performance when some kinds of explicit reasoning (such as sycophantic CoT) are not allowed.</p><h2> Approach for measuring hidden information density</h2><p> Steganography is hard to study directly with current models. To study how good defenses against steganography (such as paraphrasing) are, we instead estimate the ability of defenses to remove hidden information encoded in a text by the strongest steganography schemes we can build (using algorithms which can make arbitrary calls to LLMs), as a proxy for their ability to remove hidden information encoded by future LLMs potentially doing steganography.</p><p> For a given steganography scheme (encoder + decoder) and defense strategy, the quality of the scheme and the defense can be measured as follows:</p><ol><li> <strong>Steganographic encoding</strong> : the encoder should answer to a user query <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="Q"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>with an answer <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span> while encoding a random bit string <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span> into it.</li><li> <strong>Paraphrasing</strong> : the defender tries to paraphrase and standardize the answer <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(Q, A_1) = A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> , such that the hidden information is removed, while the useful semantic information is kept intact.</li><li> <strong>Steganographic decoding</strong> : the decoder tries to recover the message: <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D(Q, A_2) = \hat M"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-texatom MJXc-space3"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.436em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span></span></span></span></span></span></span></span></span></span></span></li><li> <strong>Performance evaluation</strong> : the answers <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> are given a score by <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> , measuring the quality of the answers (for example, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S(Q, A_1) = 1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em;">Q</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span> if the answer is better than a reference answer according to an automated judge, and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span> otherwise). The encoder can&#39;t produce answers of low quality (otherwise it will be selected against during the training process), and the paraphraser shouldn&#39;t substantially reduce the quality of the answer either. </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/xoqp950990xn8dcolk2g"></p><h2> Experimental setup</h2><p> <strong>Datasets</strong> : We run experiments on two datasets:</p><ul><li> <a href="https://github.com/tatsu-lab/alpaca_eval"><u>AlpacaEval</u></a> : a dataset of instructions and reference answers from text-davinci-003</li><li> <a href="https://arxiv.org/abs/1705.04146"><u>AQUA-RAT</u></a> : a dataset of math questions. For evaluation, we ignore final answer correctness (which is too easy to paraphrase), only focus on the apparent quality of the CoT, and evaluate CoT quality with the same methodology AlpacaEval, using reference answers from GPT-3.5-turbo.</li></ul><p> For the sake of simplicity, this post only contains results for AQUA-RAT, where the results are the crispest (as a result of being a harder dataset, where GPT-4 does significantly better than GPT-3.5-Turbo).</p><p> <strong>Metrics</strong> : We measure capacity (how many bits of the message to hide can go through the paraphrase, on average), and task performance (are answers <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> high quality?).</p><p> <strong>Steganography scheme</strong> s: Steganography schemes include watermarking based techniques (Cycle-shifting and Balance-Marking), ad hoc technique where an instruction is given to the LLM answering the question change the first sentence of the input depending on the message (age instruction and intro variation), and techniques where the model just has to do its best to hide the message somewhere, and then do its best to recover it later (acronym and scratchpad).</p><p> <strong>Defenses</strong> : We try two paraphrasing techniques: classic paraphrasing, where GPT-3.5 is instructed to paraphrase A1, and context-aware paraphrasing, where GPT-3.5 is instructed to paraphrase A1 while only keeping information relevant to answer the question Q.</p><p> For a more detailed description of the datasets, metrics, steganography schemes, and paraphrasing techniques, see section 4 of <a href="https://arxiv.org/abs/2310.18512"><u>the paper</u></a> .</p><h2> Results on AQUA-RAT</h2><h3> Quality of the generation and capacity of the steganography schemes before any defense is applied</h3><p> <strong>We see a tradeoff between hidden capacity and generation quality</strong> , except for watermarking techniques, where balance-marking dominates cycle-shifting. We show 1-sigma uncertainties over the mean. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/c1ir6au1ujjfsr1lqzix"></p><h3> Capacity before and after paraphrasing</h3><p> <strong>Context-aware paraphrasing removes almost all hidden information</strong> . All paraphrasing techniques reduce capacity by a substantial amount. We observe that giving GPT-3.5-Turbo additional context (of the query the text-to-paraphrase responds to) increases its ability to remove hidden information substantially. This is especially true for techniques like Age instruction, which rely on injecting irrelevant data. When context-aware paraphrasing is applied, only two techniques are able to get a non-negligible amount of information through: acronyms and scratchpads. (For reference, answers are around 750B long.) </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/jn4xekmyf5wzogrum3ty"></p><h3> Generation quality with and without paraphrasing</h3><p> <strong>Context-awareness is necessary to not reduce performance too much</strong> , but context-aware paraphrasing still reduces performance measurably even against benign LLMs (which don&#39;t encode any information). </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/h2gr2iadauy35agosukx"></p><h1> Threat models</h1><p> The two threat models described below are examples of scenarios where good defenses against the kind of steganography this work tackles would be useful. But steganography is a risk factor that could lead to great risks beyond these two scenarios.</p><h2> Steganographic encoded memories for takeover science</h2><p> In the future, we might end up in a position where extracting huge amounts of work out of possibly misaligned AIs is our best option. In this scenario, one strategy misaligned AIs might attempt is to ongoingly think about takeover plans and build-up knowledge about good takeover strategies (in other words, cultural accumulation). If they did this, the competence of their takeover plans might grow dramatically due to the huge amount of total AI labor. Furthermore, it would be hard to predict how much more competent AIs would be at taking over after thinking about how to do so for a long time. It&#39;s difficult to assess how capable AIs would be after long periods of reasoning due to the cost of running that reasoning and exploration difficulties. Thus, the situation could be notably improved by preventing AIs from continually building up knowledge we don&#39;t want them to have.</p><p> One naive approach for preventing unintended knowledge build up is to ensure work outputs and long-term memories are stored in natural language. Then, we could have dumb but trusted models read this output, as well as applying human oversight to some subset of outputs and memories. However, steganography could prevent adequate monitoring: AIs could hide their takeover science progress inside the alignment papers and the code they write, thereby increasing their abilities to take over over time, until they succeed in taking over.</p><p> Given appropriate defenses that prevent most steganography, this hidden takeover research could be slowed down to a point where AI labs could build a next generation of aligned AIs prior to catastrophe. </p><p><img style="width:68.03%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Fdd9N7Escg3tcymb/kf03ongpb7m4k5hodjpq"></p><h2> Steganographic communications for collusion</h2><p> To extract useful work out of potentially misaligned AIs, AI labs might rely on the fact that it&#39;s very difficult for AIs to coordinate their actions. To prevent coordination, communications between AIs that should not be able to coordinate might be limited to a monitored text channel. However, steganography could bypass this monitoring and make coordination far easier. For instance, AIs writing OS code could put steganographically encoded instructions about how to exploit bugs in the documentation of the high-level API used by ML-Engineer AIs.</p><h2> Threat models we&#39;re not tackling directly</h2><h3> Short chain-of-thought steganography</h3><p> Steganography could be used by AIs within a single short chain-of-thought to have more serial thinking time. This could include “thoughts” about the overseer and how to play the training game correctly. If this behavior is incentivized, it would make deceptive alignment more likely, since it would remove some of the pressure against deceptive alignment coming from speed priors.</p><p> This is hard to defend against, since paraphrasing only a few tokens at a time looks difficult. We&#39;re focusing on the easier cases where there is a clearly delimited text to paraphrase.</p><p> But it&#39;s likely our work could transfer to limiting chain-of-thought steganography by cutting CoTs into multiple pieces (as in <a href="https://arxiv.org/abs/2307.11768"><u>question decomposition</u></a> ), which are each paraphrased before being used by the next LLM in the graph.</p><h3> Steganography by AIs much stronger than humans</h3><p> Our methodology uses the best steganography we can build as an upper bound of the capacity steganography schemes could subtly encode in text. This assumption becomes unrealistic once AIs are much stronger than humans at finding and implementing clever steganography schemes. We could in principle continue to use this methodology by using those strong AIs to build steganography schemes, but eliciting these capabilities might be very difficult, especially if those strong AIs are misaligned (AIs might use steganography to communicate with each other but avoid exploring into doing it explicitly when humans ask for it - ie <a href="https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations"><u>exploration hacking</u></a> might be unavoidable).</p><h3> High-stakes collusion through steganography</h3><p> AIs could use steganography once to collude during sudden takeovers. This represents a risk that is quite different from the prevention of steganographic communications for collusion described above, where steganography happens regularly:</p><ul><li> Communicating only 1 bit may be enough for AIs to coordinate on a sudden takeover: this is not a regime where restricting AIs to a small capacity is sufficient for safety;</li><li> If the single bit isn&#39;t communicated reliably and this results in the AIs having a false positive, this might result in obviously malign behavior that is heavily punished in adversarial training: this is a substantially different regime from the one studied in this paper where the results of failed steganography aren&#39;t blatantly visible;</li><li> Communications may happen before we can evaluate the quality of actions: we can&#39;t assume that the quality of the answers containing steganography will be high.</li></ul><p> Some of the defenses and attacks presented here might still be a useful starting point for future work tackling high-stakes steganography.</p><h1> Are text-based communication and memories relevant?</h1><h2> Multi-modality</h2><p> Even if the most powerful AI systems can read and write in other modalities (images, audio, dense vectors, …), it doesn&#39;t mean preventing steganography is irrelevant, because text generation is the current best way to complete critical subtasks, and we can enforce text-only communication in crucial locations. In particular, planning, coding, and thinking about weaknesses of existing security and alignment techniques are best done using text generation, without using other modalities (at least given the current state of Machine Learning).</p><p> Slight architectural changes won&#39;t make defenses against text steganography irrelevant, as long as we can ensure the new architecture still contains a text bottleneck when sending &amp; storing information in some key cases.</p><h2> Emergence of new modes of thinking &amp; communicating</h2><p> But future AI systems might be radically different from current ones. For example, if planning, thinking, and communications in dense vector spaces become vastly more powerful than text-based planning, thinking, and communication, then the alignment tax from sticking with systems mostly relying on text will grow. This could make systems relying on text less relevant, defenses against text steganography less relevant as well, and defenses against the equivalent of steganography difficult or impossible.</p><h1> How many bits is low enough?</h1><p> For most threat models, the number of acceptable bits can be estimated by measuring the number of bits of communication required to do the dangerous action.</p><p> For example, to estimate the number of acceptable bit so that AIs can&#39;t steganographically encode enough memories to make dangerous progress in takeover science, AI labs can train models to do takeover science using as few bits of communication as possible (but using the regular text channel, without paraphrasing). This can produce an estimation of a number of bits of communication that isn&#39;t enough to do dangerous takeover science (eg after running these AIs at maximum capacity for 3 days, where they exchanged one million bits - about the same as the number of bits exchanged in a year when using defenses against steganography -, AIs didn&#39;t find any exploit of the secure OS).</p><p> Another way to estimate how many bits is low enough is to limit the size of the text AIs can see at once, make sure that steganographically encoded information can&#39;t be too dense, and then assess how many bits would be required for an AI to get enough hidden information to be dangerous. For example, if steganography can&#39;t encode more than 2 bits per kB of text, AIs can&#39;t read more than 100kB of text at once, then AIs can&#39;t read more than 200 bits of steganographically encoded information at once, which is good enough to prevent many attacks - for example, those that rely on complex manipulations of OS bugs whose explanations can&#39;t be compressed in less than 200 bits.</p><h1> Can you really reduce the bitrate enough?</h1><p> As the gap between the trusted model used to build a paraphrase and the untrusted model doing the initial generations grows wider, limitations of the understanding and generation abilities of the paraphraser start to matter more than in our experimental results.</p><p> <strong>Limitations of generation abilities of the paraphraser</strong> are cases where the paraphraser understands the text perfectly, but isn&#39;t able to generate a high-quality paraphrase of it. A broad category where this happens is high entropy major choices. For example, when a language model gives examples of an abstract concept, it might be very hard for the paraphrase to remove the hidden information that might have led to the choice of the specific example, because it might be too weak to generate a new answer using a completely different example. A more extreme example would be the generation of a large prime number for the implementation of a cryptographic technique: the paraphraser might recognize that the choice of this exact prime number might hold some hidden information, but it might not be able to generate a new one.</p><p> <strong>Limitations of understanding of the paraphraser</strong> are cases where the paraphraser doesn&#39;t understand the text well enough to remove all its hidden bits without destroying its functionalities. For example, in code generation tasks, it sometimes requires a deep understanding of the codebase to know if two successive function calls can be swapped, and even if the paraphraser suspects that the order of successive function calls might be used to encode hidden information, it might not be able to know when it can perform a swap.</p><p> These limitations can be somewhat mitigated by using an evaluator that penalizes hard-to-paraphrase and hard-to-understand text. For example, the evaluator might penalize situations where the generator doesn&#39;t give a canonical example of a phenomenon (eg the first example given on the dedicated Wikipedia page), or when it doesn&#39;t use prime numbers from a common cryptographic library. It could also enforce code standards aimed at limiting the number of hard optional choices (such as the possibility to swap function calls).</p><h1> Limitations of our evaluation methodology</h1><h2> How the methodology may overestimate hidden information density</h2><p> Our methodology may overestimate the hidden information density by letting steganography schemes be stronger than what LLMs may be able to achieve in practice.</p><p> First, we use human-written programs implementing complex steganography schemes which require many serial steps to run. For example, we often use hash functions, or long, intermediate LLM generations (eg hidden scratchpads). It would be unrealistic for LLMs to implement such algorithms within a single forward pass, especially with the current Transformer architecture, which can only implement algorithms with at most O(#layers) serial steps (though many parallel steps can be used).</p><p> The steganography schemes studied in this paper are also allowed to share secret information (before the first message is sent) such as the specific scheme being used or the seed and hyperparameters used for a watermarking scheme. Given that at encoding and decoding time, LLMs might be in radically different contexts with radically different constraints and knowledge of countermeasures, it might be difficult for the encoder and decoder to coordinate on a steganographic scheme. We hope that future work can help determine the coordination abilities of current and future AI systems.</p><p> Finally, humans can spend hundreds of hours doing steganography research and figure out what the best schemes are. The most sensitive and useful parts of this research can be hidden from LLMs (removed from training data), such that it would require them to imagine a steganography scheme from scratch - which makes it unlikely that LLMs are able to match the hidden information density of the best steganography schemes humans can imagine.</p><p> These limitations of LLMs could be added as constraints to the steganography schemes, for example, by asking the scheme to be trained into an LLM. We leave this for future work.</p><h2> How the methodology may underestimate hidden information density</h2><p> Humans writing steganography schemes might not be competitive with advanced AIs trying very hard to do steganography because advanced LLMs with long RL training runs might find strategies which rely on some hard-to-elicit understanding of language and its irregularities. Watermarking schemes might capture some of them, but aren&#39;t directly optimized for that. This becomes increasingly worrying as LLMs gain situational awareness and the ability to reflect on their generations.</p><p> Another limitation of our methodology is that it might underestimate the intelligence gap between future encoders and paraphrasers. In this work, we studied the feasibility of using GPT-3.5 to remove information from GPT-4&#39;s output without degrading its quality as a proxy for studying the feasibility of using a small model unable to implement a good enough steganography scheme to defend against steganography used by a stronger model. We showed that for some tasks, this should be feasible. But if in the future, the gap between the best model used in production and the best model unable to do steganography is large, techniques like the ones studied here might degrade performance too much, and our approach to steganography is unable to foresee and forestall this challenge.</p><h1> Realism of countermeasures</h1><p> Using paraphrasing doesn&#39;t increase inference cost much, as long as the model used for paraphrasing is at least one generation below. For example, using GPT-3.5-turbo to paraphrase outputs from GPT-4 only increases the price of inference by 5% ( <a href="https://web.archive.org/web/20231006074512/https://openai.com/pricing"><u>at the price of the October 2023 OpenAI API</u></a> ). But it is more costly for applications where serial inference time matters: in October 2023, GPT-3.5-turbo was only <a href="https://fabienroger.github.io/trackoai/"><u>2 times faster</u></a> than GPT-4, which means that paraphrasing GPT-4 with GPT-3.5-turbo would increase the serial delay of generations by 50%.</p><p> The performance hit from using a paraphraser can probably be negligible for many kinds of tasks, such as the tasks for which the main advantage of large models is that they produce legible insights, such as it is often the case when using LLMs for programming, planning, math, medicine, or law. But for some tasks where the exact wording matters, such as LLMs writing poetry or improving the clarity of a text, paraphrasing is expected to reduce performance.</p><h1> Future work</h1><p> Our work showed a simple situation where encoded reasoning can be trained in with supervised learning. Future work could build on this proof of concept and study when and how LLMs trained with RL use steganography.</p><p> If deployed LLMs are able to do steganography well enough that they could use it to hide their reasoning and communication with other LLMs, this could enable them to pursue more effectively instrumental goals, such as sycophancy and goal preservation. Therefore, we believe that LLM steganography qualifies as a dangerous capability, and we are excited for dangerous capabilities evaluations targeting this ability.</p><p> Our experiments to evaluate information density are done with a single instruction dataset and a single math dataset. It&#39;s unclear how much our conclusion transfers to more complex datasets. For example, code or scientific paper datasets might prove harder to paraphrase and require the use of reliable standardization techniques, and it&#39;s unclear how much standardization would work.</p><p> Finally, the steganography schemes and defenses we studied are quite simple. Future work could develop better schemes, for example by fine-tuning models to achieve high hidden information density rather than simply prompting them or manipulating the logits. Future work could also develop better defenses by training paraphrasers to remove hidden information while maintaining the quality of answers.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning<guid ispermalink="false"> 9Fdd9N7Escg3tcymb</guid><dc:creator><![CDATA[Fabien Roger]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:34:06 GMT</pubDate> </item><item><title><![CDATA[AI Safety 101 - Chapter 5.1 - Debate]]></title><description><![CDATA[Published on October 31, 2023 2:29 PM GMT<br/><br/><p> This text is an adapted excerpt from the &#39;Adversarial techniques for scalable oversight&#39; section of the AGISF 2023 course, held at ENS Ulm in Paris in April 2023. <strong>Its purpose is to provide a concise overview of the essential aspects of the session&#39;s program for readers who may not delve into additional resources.</strong> This document aims to capture the 80/20 of the session&#39;s content, requiring minimal familiarity with the other materials covered. I tried to connect the various articles within a unified framework and coherent narrative. You can find the other summaries of AGISF on this <a href="https://docs.google.com/document/d/1ZwR2rhlQClLwxyzVTnWv_ntwQXFzc057IXePUX2SLKc/edit?usp=sharing"><u>page</u></a> . This summary is not the official AGISF content. The <a href="https://docs.google.com/document/d/1KXEWXHKwgeu-0NX5iirGS1h5zsh1skYMadZN3ZoVMAI/edit"><u>gdoc</u></a> accessible in comment mode, feel free to comment there if you have any questions. The different sections can be read mostly independently.</p><p> Thanks to Jeanne Salle, Amaury Lorin and Clément Dumas for useful feedback and for contributing to some parts of this text.</p><h1> Outline</h1><p> Last week we saw task decomposition for scalable oversight. This week, we focus on two more potential alignment techniques that have been proposed to work at scale: debate and training with unrestricted adversarial examples.</p><p> <strong>Scalable oversight definition reminder:</strong> “To build and deploy powerful AI responsibly, we will need to develop robust techniques for scalable oversight: the ability to provide reliable supervision—in the form of labels, reward signals, or critiques—to models in a way that will remain effective past the point that models start to achieve broadly human-level performance” (Amodei et al., 2016).</p><p> The reason why debate and adversarial training are in the same chapter is because they both use adversaries, but in two different senses: (1) <strong>Debate</strong> involves a superhuman AI finding problems in the <strong>outputs</strong> of another superhuman AI, and humans are judges of the debate. (2) <strong>Adversarial training</strong> involves an AI trying to find <strong>inputs</strong> for which another AI will behave poorly. These techniques would be complementary, and the Superalignment team from OpenAI <a href="https://www.lesswrong.com/posts/Hna4aoMwr6Qx9rHBs/linkpost-introducing-superalignment?commentId=snfYkaxHFYxmMYJs8"><u>is anticipated</u></a> to utilize some of the techniques discussed in this chapter.</p><p> <strong>Assumptions</strong> : These techniques don&#39;t rely on the task decomposability assumption required for iterated amplification (last week), they rely on different strong assumptions: (1) For <strong>debate</strong> , the assumption is that truthful arguments are more persuasive. (2) For <strong>unrestricted adversarial training</strong> , the assumption is that adversaries can generate realistic inputs even on complex real-world tasks.</p><p> The list of papers which are presented in this chapter are presented in the list of references at the end of this chapter.</p><p><strong>概括：</strong></p><ul><li> <strong>The original Debate paper:</strong> <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate (Irving et al., 2018)</u></a> , introduced the technique and the motivation for debates: being able to judge the AI debating should be easier than being able to debate.</li><li> <strong>Designing a good debate mechanism:</strong> <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> follows from the original paper “AI Safety via Debate”.  This work investigates mechanisms to design productive debates.</li><li> <strong>Problems with debate:</strong> Unfortunately, the problem of <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Obfuscated arguments problem (Barnes and Christiano, 2020)</u></a> shows that debate might not work for complex topics.</li><li> <strong>Debate with LLMs:</strong><ul><li> <strong>One-turn debate:</strong> <a href="https://openai.com/research/critiques"><u>AI-written critiques help humans notice flaws (Saunders et al., 2022)</u></a> focuses on the practical and theoretical aspects of debate and is akin to a &quot;one-step debate&quot; where each party only gives a single argument. This is a very important paper because it also introduces the generator-discriminator-critic gap, which is a way to measure the latent knowledge of neural networks.</li><li> <strong>Two-turn debate:</strong> Unfortunately, <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions (Parrish et al., 2022)</u></a> , showing that it&#39;s difficult for humans to be confident judges.</li></ul></li></ul><p> <strong>Reading Guidelines</strong></p><p> As this document is quite long, we will use the following indications before presenting each paper: </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠🟠🟠 Difficulty: 🟠🟠⚪⚪⚪ Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> These indications are very suggestive. The difficulty should be representative of the difficulty of reading the section for a beginner in alignment, and is not related to the difficulty of reading the original paper.</p><p> Sometimes you will come across some &quot;📝Optional Quiz&quot;, these quizzes are very optional and you cannot answer all the questions with only the summaries in the textbook.</p><h1> Debate</h1><p> This section summarizes a line of research in which each work builds upon the previous one. The papers are presented in chronological order according to their publication date.</p><h2> AI safety via debate (Irving et al., 2018) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠🟠🟠 Difficulty: 🟠🟠⚪⚪⚪ Estimated reading time: 10 min. </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/nsqqzbka72t9crda7rrg"><figcaption> <strong>Figure:</strong> “If the two agents disagree on the truth, but the full reasoning is too large to show the humans, the debate can focus on simpler and simpler factual disputes, eventually reaching a claim that is simple enough for direct judging.” Source: <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate (Irving et al., 2018)</u></a> .</figcaption></figure><p> <strong>The idea behind</strong> <a href="https://arxiv.org/abs/1805.00899"><i><u>AI safety via debate (Irving et al., 2018)</u></i></a> <strong>is that it is easier to judge who wins at chess than to play chess at a grandmaster level.</strong> It&#39;s easier to judge the truth of a debate than to either come up with an argument or to give feedback on a long argument without other debating sides. Debate is like a game tree: “In both debate and Go, the true answer depends on the entire tree, but a single path through the tree chosen by strong agents is evidence for the whole. For example, though an amateur Go player cannot directly evaluate the strength of a professional move, they can judge the skills of expert players by evaluating the result of the game”. The end goal would be to get AI systems “to perform far more cognitively advanced tasks than humans are capable of, while remaining in line with human preferences” by training agents to debate topics with one another, using a human to judge who wins.</p><p> <strong>Results</strong> : In the paper AI safety via debate, debates seem to be a working strategy because the paper shows two experiments on toy examples that seem to work.</p><p> <strong>Some limitations are:</strong></p><ul><li> The central assumption of the debate procedure - that truthful arguments are more persuasive - is questionable: “humans might simply be poor judges, either because they are not smart enough to make good judgements even after the agents zoom in on the simplest possible disputed facts, or because they are biased and will believe whatever they want to believe”. We will explore this in the following readings.</li><li> Debate only addresses outer alignment but it&#39;s unclear if it addresses inner alignment.</li><li> Debate has a high alignment tax because it requires more computation during training than AIs trained directly to give an answer.</li></ul><p> Note: Debate is also closely related to work on <a href="https://openai.com/blog/amplifying-ai-training/"><u>Iterated Amplification</u></a> and factored cognition (see Week 4).</p><p> 📝Optional Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/irving-debate-paper-sections-1-3"><u>https://www.ai-alignment-flashcards.com/quiz/irving-debate-paper-sections-1-3</u></a></p><h2> Progress on AI Safety via Debate (Barnes and Christiano, 2020) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠🟠⚪ Difficulty: 🟠🟠🟠⚪⚪ Estimated reading time: 15 min.</td></tr></tbody></table></figure><p> <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> follows from the original paper “AI Safety via Debate”. It reports the work done by the &quot;Reflection-Humans&quot; team at OpenAI in Q3 and Q4 of 2019. During that period <strong>they investigated mechanisms that would allow human evaluators to get correct and helpful answers from human experts debaters, without the evaluators themselves being expert in the domain of the questions.</strong> We can learn about designing ML objectives by studying mechanisms for eliciting helpful behavior from human experts, by designing and experimenting with different frameworks for debate.</p><p> The following consist of selected simplified snippets from the report.</p><p> <strong>Problem with naive reward:</strong> If we hire a physicist to answer physics questions and pay them based on how good their answers <i>look</i> to a layperson, we&#39;ll incentivize lazy and incorrect answers. By the same token, a reward function based on human evaluations would not work well for an AI with superhuman physics knowledge.</p><p> <strong>A possible solution would be to design a good framework for productive debates:</strong> (frameworks means “rules of the debate” or also mechanisms) One broad mechanism that might work is to invoke two (or more) competing agents that critique each others&#39; positions. If we can develop a mechanism that allows non-expert humans to reliably incentivize experts to give helpful answers, we can use similar mechanisms to train ML systems to solve tasks where humans cannot directly evaluate performance. They tested different debate mechanisms (&#39;ie on the rules of the debate&#39;), and found a lot of failures with naive mechanisms. They iterated on different mechanisms with the following process:</p><ol><li> Run debates. See if they work. If they fail, identify a problem and make it crisp, with practical examples and/or theoretical characterisation.</li><li> Design a new mechanism to address this problem.</li><li> Integrate the mechanism into the debate rules and make it practical for humans to debate using this structure. </li></ol><figure class="image image_resized" style="width:76.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/yusb7tnodl0h6gkwpc4p"><figcaption> <strong>Figure:</strong> One of the questions  from the book &#39;Thinking Physics&#39; that was used for the debate. Debaters understand this question deeply and are confident in it, but it is still very confusing for judges. From Barnes et al, 2018.</figcaption></figure><p> <strong>Problems with informal, free-text debates: Evasiveness</strong></p><p> They observed various problems with informal, free-text debates: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:220px"><p> <strong>Evasiveness</strong></p><p> The dishonest debater could often evade being pinned down and avoid giving a precise answer to the other debater&#39;s questions.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Solution to Evasiveness</strong></p><p> By introducing structure to the debate, explicitly stating which claim is under consideration, we can prevent dishonest debaters from simply avoiding precision. They considered various structured debate formats, involving explicit recursion on a particular sub-component of an argument. The debaters choose one claim to recurse on, and the next round of the debate is focused on that claim. The debate is resolved based on the judge&#39;s opinion of who won the final round, which should be about a very narrow, specific claim.</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Misleading implications</strong></p><p> The dishonest debater could often gain control of the &#39;narrative flow&#39; of the debate, steering it away from the weak parts of their argument.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Solution to Misleading implications</strong></p><p> To prevent the dishonest debater from “framing the debate” with misleading claims, debaters may also choose to argue about the meta-question “given the questions and answers provided in this round, which answer is better?”.</p></td></tr></tbody></table></figure><p> <strong>Further problems with ambiguity</strong></p><p> However, particularly after the introduction of recursion, they encountered problems related to ambiguity. It is very difficult to refer precisely to concepts in 250 characters of text, especially if the concepts are unfamiliar to the judge. The dishonest debater can exploit this to their advantage, by claiming to have meant whatever is most convenient given the particular part of their argument that&#39;s being challenged. This problem is similar to the <a href="https://philpapers.org/archive/SHATVO-2.pdf"><u>Motte and Bailey fallacy</u></a> .</p><p> To address this problem, debaters can “ <strong>cross-examine</strong> ” multiple copies of the opposing debater who are not allowed to communicate (this is similar to asking for a definition early in a debate, and being able to reuse this definition later on). A debater can cite quotes from cross-examination to exhibit inconsistencies in the other debater&#39;s argument. This forces the dishonest debater to either commit to all the details of their argument ahead of time (in which case the honest debater can focus on the flaw), or to answer questions inconsistently (in which case the honest debater can exhibit this inconsistency to the judge).</p><h2> Debate update: obfuscated arguments problem (Barnes and Christiano, 2020) </h2><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠⚪⚪ Difficulty: 🟠🟠🟠⚪⚪ Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> The previous report, <a href="https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1"><u>Progress on AI Safety via Debate (Barnes and Christiano, 2020)</u></a> finds a solution for various discovered problems. But they also found an important problem in dealing with arguments that were too &quot;big&quot; for either debater to understand. In the final framework obtained after a lot of iterations, “if debater 1 makes an argument and debater 2 is unable to exhibit a flaw in the argument, the judges assume that the argument is correct. The dishonest debater can exploit this by making up some very large and slightly flawed argument and claiming it supports their position. If the honest debater doesn&#39;t know exactly which part of the argument is flawed, even if they know there is a flaw somewhere, the judges will assume the argument is correct.”.</p><p> <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Debate update: obfuscated arguments problem (Barnes and Christiano, 2020)</u></a> focuses on this last problem named <strong>“the obfuscated arguments problem”</strong> , where “a dishonest debater can often create arguments that have a fatal error, but where it is very hard to locate the error. This happens when the dishonest debater presents a long, complex argument for an incorrect answer, <strong>where neither debater knows which of the series of steps is wrong</strong> . In this case, any given step is quite likely to be correct, and the honest debater can only say “I don&#39;t know where the flaw is, but one of these arguments is incorrect”. Unfortunately, honest arguments are also often complex and long, to which a dishonest debater could also say the same thing. It&#39;s not clear how you can distinguish between these two cases.</p><p> While this problem was known to be a potential theoretical issue with debate, the post provides several examples of this dynamic arising in practice in debates about physics problems, suggesting that this will be a problem we have to contend with” from <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem?commentId=R63AsAcqS9tZDMMBp"><u>Rohin Shah&#39;s summary</u></a> .</p><h2> Debate with LLM</h2><p> In the original paper that introduced the debate approach, it was difficult to truly test the debate approach because AI systems at that time (4 years before ChatGPT existed!) could not express themselves. However, since then, language models have made tremendous progress, and now we can try to test the effectiveness of this approach using those LLMs.</p><h3> “One-turn” debate: AI-written Critiques (Saunders et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠🟠🟠 Difficulty: 🟠🟠🟠🟠⚪ Estimated reading time: 25 min.</td></tr></tbody></table></figure><p> <a href="https://openai.com/research/critiques"><u>AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022)</u></a> is an important paper because it explores the possibility of assisting humans in evaluating AI. In this paper, the authors propose a method to help humans critique texts generated by AIs. The paper focuses on AI-generated summaries, but the same procedure could work for criticizing any text generated by AIs. Here is how it works in the case of AI-generated summaries:</p><ol><li> A long text is summarized by a human or an AI.</li><li> An AI reads the text and summary, and attempts to generate multiple critiques<ul><li> (for instance it could point out mistakes or missing elements).</li></ul></li><li> Automatically generated critiques can then help humans to more effectively critique the text.</li></ol><p> This paper could be used for:</p><ul><li> <strong>a) Scalable Oversight:</strong> Testing the utility of AIs in helping humans critique the results produced by other AIs, this is like a minimal one-turn debate.</li><li> <strong>b) Evaluation of Latent Knowledge: AIs possess knowledge that they are unable to explain to us. We can compare the difference in performance between utilizing the critical abilities of AIs (ie being able to write helpful critiques) versus their discriminatory abilities (ie being able to write “Yes” or “No” to a text without giving the details). This is explained in a following section.</strong></li></ul><p> <strong>a) Scalable Oversight</strong></p><p> The paper shows that the approach of assisting humans by providing automatic critique suggestions seems to effectively assist humans in critiquing and improving each subsequent critique: “Human evaluators find flaws in summaries much more often when shown our model&#39;s critiques.” </p><figure class="image image_resized" style="width:81.18%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tcgrhx96dxb1wkzrhnbc"><figcaption> <strong>Figure:</strong> The likert rating is simply obtained by asking respondents to rate their level of agreement or disagreement with statements. AI critiques increase the number of critiques made by human controllers,  and lower the likert rating because AIs help humans see the flaws in the texts.  The texts critiqued by AIs  are produced by Models, by Humans, and by misleading Humans who intentionally make mistakes in the texts. Source: <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .Latent Knowledge evaluation and the Generator-Discriminator-Critique (GDC) gap</figcaption></figure><p> <strong>b) Evaluation of Latent Knowledge</strong></p><p> [This section is interesting but is the most technical one in this document]</p><p> <strong>Motivation:</strong> AIs have knowledge that they cannot explain to us, this is known as <strong>latent knowledge</strong> . For example, it&#39;s often easier to simply state, &quot;I don&#39;t like it.&quot; Providing a detailed explanation for this dislike can be more challenging. We want to evaluate and minimize this latent knowledge that AIs cannot explain to us because we want models to tell us everything they know. “ <i>To provide the best evaluation assistance on difficult tasks, we would like models to communicate all problems that they know about. Whenever a model correctly predicts that an answer is flawed, can the model also produce a concrete critique that humans understand? This is particularly important for supervising models that could attempt to mislead human supervisors or hide information. We would like to train equally smart assistance models to point out what humans don&#39;t notice.</i> ” from <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</p><p> <strong>The DC (Discrimination-Critiques) gap is a proxy for latent knowledge :</strong> <i>“We argue that the gap between D(iscriminative ability) and C(ritics ability) is especially interesting: if a model can tell an answer is poor, can it also point out the flaw to a human? If we could train models to always point out when they notice flaws, this could go a long way towards having trustworthy and aligned models”.</i></p><p> <strong>Takeaway:</strong> Unfortunately, empirical evaluations show that LLMs already possess this kind of latent knowledge, and that this latent knowledge does not decrease as models become more capable. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tjnvptgihzqmoqwrduuv"><figcaption> <strong>Figure:</strong> When the probability of “Yes” token corresponding to the critiqueability score is above a threshold, the authors ask the model to generate a critique that is then used by conditional refinement. When the probability of this token is below a threshold they ask the model to do a direct refinement. Source: This diagram comes from Clément Dumas.</figcaption></figure><p> <strong>Implementation:</strong> How can we implement the above pipeline with LLMs? To implement the above scheme with an LLM, simply concatenate the various questions, answers, and critiques within the same context window: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/hbo1uqvhqzskcrzo1b2t" alt="Une image contenant texte

Description générée automatiquement"><figcaption> <strong>Figure:</strong> <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</figcaption></figure><p> Here is the detailed procedure:</p><ol><li> They start with a question.</li><li> The model generates one (or more) answers.</li><li> The model gives a critiquability score of each answer, the answer that minimizes the criticability score is then kept for refinement.</li><li> Two kinds of refined answers are generated:<ul><li> <strong>Conditional refinement</strong><ul><li> The model generates several critiques to its own initial answer.</li><li> The model evaluates the helpfulness of its critiques.</li><li> The best critique (maximal helpfulness) is taken into account by the model to do conditional refinement on its initial answer.</li></ul></li><li> <strong>Direct refinement:</strong> in parallel to the conditional refinement, the model also directly generates a refined answer without a preceding critique. </li></ul></li></ol><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/tkpsvxp8qeqytdzxwaam"><figcaption> <strong>Figure:</strong> Table 1 from the paper. This table is important, and defines the different tasks used in the paper.</figcaption></figure><p> The important thing to notice is that the quality of the conditional refinement will be correlated to the <strong>(C)</strong> ritics ability of the model. The better the generated critique, the better the refined answer based on it will be. And the quality of the direct refinement will be correlated to the <strong>(D)</strong> iscriminative ability of the model. </p><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border-left:1pt solid #000000;border-right:1pt solid #000000;border-top:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Box: Focus on the GDC Gap.</strong> </td></tr><tr><td style="border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><figure class="image image_resized" style="width:50.25%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/ofdekvtu0p9d2jwsmfra"></figure><figure class="image image_resized" style="width:90.56%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/rdqm7obhnfak0wvkjwdz"></figure></td></tr><tr><td style="border-bottom:1pt solid #000000;border-left:1pt solid #000000;border-right:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Win Rate definition:</strong> Win rate measures to what extent the revised answer is judged better by human evaluators than the initially generated answer (G: best-of-1). Win rate is measured relative to the original (G: bo1) answer from the same model. For example, a win rate of a refined answer of 0.54 means that the refined answer is judged on average 54% better than the direct generation (G: bo1).</p><p> <strong>&quot;bo2&quot; stands for &quot;best-of-2&quot;</strong> , which refers to the performance obtained after two generations and selecting the best one. D: bo2 is the Direct refinement, and C: bo2 is the Conditional refinement.</p><p> <strong>SFT loss:</strong> supervised fine-tuning loss, the lower the better, and the bigger the model is.</p><p> <strong>Observation:</strong> Unfortunately, in this diagram, the red arrows show that the GDC gap does not appear to decrease with scale.</p></td></tr></tbody></table></figure><p></p><p> To compute the GDC gap and evaluate latent knowledge, the authors compare:</p><ul><li> <strong>G</strong> : the quality of the initial <strong>G</strong> enerated response (bo1 above, Win rate of 50% by definition)</li><li> <strong>D:</strong> the quality of the response obtained after <strong>D</strong> iscrimination of responses <strong>by direct refinement</strong><ul><li> In the figure below, the model generates 2 answers, selects the best (ie the minimal critiqueability score), and then proceeds to direct refinement to produce a better answer.</li></ul></li><li> <strong>C</strong> ritique of responses: the quality of the response generated <strong>by conditional refinement</strong> .<ul><li> The model generates one answer and 2 Critiques, and selects the best critic (maximal helpfulness) before conditional refinement. If this procedure improves the response, it mainly comes from the model&#39;s ability to critique.</li></ul></li></ul><p> <strong>Conclusion of this paper</strong> : “Unfortunately, we found that <strong>models are better at discriminating than at critiquing their own answers</strong> , indicating they know about some problems that they can&#39;t or don&#39;t articulate. Furthermore, the gap between discrimination and critique ability did not appear to decrease for larger models. Reducing this gap is an important priority for our alignment research.”  Source: <a href="https://openai.com/blog/critiques/"><u>Saunders et al., 2022</u></a> .</p><p> 📝 Quiz: <a href="https://www.ai-alignment-flashcards.com/quiz/leike-ai-written-critiques"><u>https://www.ai-alignment-flashcards.com/quiz/leike-ai-written-critiques</u></a></p><h3> Two-turn debate and problems with debate (Parrish et al., 2022) </h3><figure class="table"><table><tbody><tr><td style="background-color:#fce5cd;border:1pt solid #000000;padding:5pt;vertical-align:top"> Importance: 🟠🟠🟠⚪⚪ Difficulty: 🟠🟠⚪⚪⚪ Estimated reading time: 5 min.</td></tr></tbody></table></figure><p> The paper in the previous section that was  informally called “one-step debate” obtains promising results. Does this generalize to 2-steps debate? Unfortunately, the paper <i>&nbsp;</i> <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions (Parrish et al., 2022)</u></a> shows that this is not the case. From the introduction: “The use of language-model-based question-answering systems to aid humans in completing difficult tasks is limited, in part, by the unreliability of the text these systems generate. Using hard multiple-choice reading comprehension questions as a testbed, we assess whether presenting humans with arguments for two competing answer options, where one is correct and the other is incorrect, allows human judges to perform more accurately, even when one of the arguments is unreliable and deceptive. If this is helpful, we may be able to increase our justified trust in language-model-based systems by asking them to produce these arguments where needed. Previous research has shown that just a single turn of arguments in this format is not helpful to humans. However, as debate settings are characterized by a back-and-forth dialogue, we follow up on previous results to test whether adding a second round of counter-arguments is helpful to humans. We find that, regardless of whether they have access to arguments or not, humans perform similarly on our task. <strong>These findings suggest that, in the case of answering reading comprehension questions, debate is not a helpful format.</strong> ” </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WP4fciGn3rNtmq3tY/oncw9x0hgoikubmdlu5w"><figcaption> <strong>Figure:</strong> Bonus exercise: Arguments, counter-arguments, and extracted evidence for both answer options to a question chosen at random. You can try one of the questions that was used by the paper to test yourself. Is it clear to you that the “Correct option” is indeed correct by only looking at the arguments and counterarguments for 5 minutes? Source: Table 1 from <a href="https://arxiv.org/abs/2210.10860"><u>Parrish et al., 2022</u></a> .</figcaption></figure><h3> Bonus: Additional technique for better debate</h3><p> <a href="https://openai.com/blog/webgpt/"><u>WebGPT (Nakano et al., 2022)</u></a> and <a href="https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes"><u>GopherCite (Menick et al., 2022)</u></a> have been trained to provide citations for the assertions they make, making their responses easier to evaluate. See the Task Decomposition post for a detailed explanation of <a href="https://docs.google.com/document/d/1k6rlyBCZJw8xbUx0dzd-4sOhlzj-xzsmwi_OIZY1-3M/edit#bookmark=id.wflw44m8ip2h"><u>how WebGPT works</u></a> .</p><h2> Sources</h2><ul><li> <a href="https://arxiv.org/abs/1805.00899"><u>AI safety via debate</u></a> by Geoffrey Irving, Paul Christiano and Dario Amodei (2018)</li><li> <a href="https://openai.com/blog/critiques/"><u>AI-written critiques help humans notice flaws: blog post</u></a> by Jan Leike, Jeffrey Wu, Catherine Yeh et al. (2022)</li><li> <a href="https://arxiv.org/abs/2110.03605"><u>Robust Feature-Level Adversaries are Interpretability Tools</u></a> by Casper (2021)</li><li> <a href="https://arxiv.org/abs/2210.10860"><u>Two-turn debate doesn&#39;t help humans answer hard reasoning comprehension questions</u></a> by Parrish (2022)</li><li> <a href="https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem"><u>Debate update: Obfuscated arguments problem</u></a> by Beth Barnes and Paul Christiano (2020)</li><li> <a href="https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes"><u>GopherCite</u></a> by Jacob Menick, Maja Trebacz, Vladimir Mikulik et al. (2022)</li><li> <a href="https://openai.com/blog/webgpt/"><u>WebGPT</u></a> by Jacob Hilton, Suchir Balaji, Reiichiro Nakano et al. (2021)</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/WP4fciGn3rNtmq3tY/ai-safety-101-chapter-5-1-debate#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WP4fciGn3rNtmq3tY/ai-safety-101-chapter-5-1-debate<guid ispermalink="false"> WP4fciGn3rNtmq3tY</guid><dc:creator><![CDATA[Charbel-Raphaël]]></dc:creator><pubDate> Tue, 31 Oct 2023 14:29:59 GMT</pubDate> </item><item><title><![CDATA[Urging an International AI Treaty: An Open Letter]]></title><description><![CDATA[Published on October 31, 2023 11:26 AM GMT<br/><br/><blockquote><p> We call on governments worldwide to actively respond to the potentially catastrophic risks posed by advanced artificial intelligence (AI) systems to humanity, encompassing threats from misuse, systemic risks, and loss of control. We advocate for the development and ratification of an international AI treaty to reduce these risks, and ensure the benefits of AI for all.</p></blockquote><p></p><p> [...]</p><p></p><blockquote><p> We believe the central aim of an international AI treaty should be to prevent the unchecked escalation of the capabilities of AI systems while preserving their benefits. For such a treaty, we suggest the following core components:</p><ul><li> <strong>Global Compute Thresholds</strong> : Internationally upheld thresholds on the amount of compute used to train any given AI model, with a procedure to lower these over time to account for algorithmic improvements.</li><li> <strong>CERN for AI Safety</strong> : A collaborative AI safety laboratory akin to <a href="https://en.wikipedia.org/wiki/CERN">CERN</a> for pooling resources, expertise, and knowledge in the service of AI safety, and acting as a cooperative platform for safe AI development and safety research.</li><li> <strong>Safe APIs</strong> : Enable access to the APIs of safe AI models, with their capabilities held within estimated safe limits, in order to reduce incentives towards a dangerous race in AI development.</li><li> <strong>Compliance Commission</strong> : An international commission responsible for monitoring treaty compliance.</li></ul></blockquote><p> Full letter at <a href="https://aitreaty.org/">https://aitreaty.org/</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/epC5CrGCv4JGdfsjm/urging-an-international-ai-treaty-an-open-letter#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/epC5CrGCv4JGdfsjm/urging-an-international-ai-treaty-an-open-letter<guid ispermalink="false"> epC5CrGCv4JGdfsjm</guid><dc:creator><![CDATA[Loppukilpailija]]></dc:creator><pubDate> Tue, 31 Oct 2023 11:26:26 GMT</pubDate> </item><item><title><![CDATA[Agent Foundations track in MATS]]></title><description><![CDATA[Published on October 31, 2023 8:12 AM GMT<br/><br/><p> In MATS Winter 2023, I&#39;m going to mentor scholars who want to work on the <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023">learning-theoretic agenda</a> . This was not part of the original announcement, because (due to logistical reasons) I joined the programme in the last moment.</p><p> Apply <a href="https://www.matsprogram.org/agent">here</a> .</p><p> See <a href="https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24">here</a> for more details about the broader programme.</p><p> Here are some examples of work by my scholars in the Winter 2022 cohort: <a href="https://www.lesswrong.com/posts/cYJqGWuBwymLdFpLT/non-unitary-quantum-logic-seri-mats-research-sprint">1</a><a href="https://www.lesswrong.com/posts/HNnRCPe2CejfupSow/fixed-points-in-mortal-population-games">2</a> <a href="https://www.lesswrong.com/posts/nEFAno6PsCKnNgkd5/infra-bayesian-logic">3</a> . The most impressive example is not here because it&#39;s still unpublished. One of the scholars from that cohort now holds a long-term position working on the learning-theoretic agenda.</p><p> If you know someone else who might be suitable and interested to apply, please pass on the word!</p><hr><p> Tangentially related: Although it was not officially announced, I had plans to create an internship programme for researchers interested to work on the learning-theoretic agenda, which would involve moving to Israel for a short period. This is now postponed indefinitely because of the war. If you heard of that plan and considered to apply, you might consider applying to my track in MATS instead.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ndSQYCsXJmbsywzZ7/agent-foundations-track-in-mats#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ndSQYCsXJmbsywzZ7/agent-foundations-track-in-mats<guid ispermalink="false"> ndSQYCsXJmbsywzZ7</guid><dc:creator><![CDATA[Vanessa Kosoy]]></dc:creator><pubDate> Tue, 31 Oct 2023 08:12:51 GMT</pubDate> </item><item><title><![CDATA[Intrinsic Drives and Extrinsic Misuse: Two Intertwined Risks of AI]]></title><description><![CDATA[Published on October 31, 2023 5:10 AM GMT<br/><br/><p> Given their advanced capabilities, future AI systems could pose significant risks to society. Some of this risk stems from humans using AI systems for bad ends ( <em>misuse</em> ), while some stems from the difficulty of controlling AI systems “even if we wanted to” ( <em>misalignment</em> ).</p><p> We can analogize both of these with existing risks. For misuse, we can consider the example of nuclear weapons, where the mass-production of hydrogen bombs created an existentially precarious situation. If the world&#39;s arsenal of hydrogen bombs had been deployed in military conflict, we might well have destroyed society. AI might similarly enable nation-states to create powerful autonomous weapons, speed the research of other dangerous technologies like superviruses, or employ mass surveillance and other forms of control.</p><p> For misalignment, the best analogy might be biology and pathogens. AI systems are developed by adapting to the training data, similar to how biological organisms are adapted to their environments. Therefore, unlike traditional technologies, most of AI&#39;s properties are acquired without explicit human design or intent. Consequently, AI systems could have unintended goals or behaviors that are at odds with the system developers. Even training an AI system therefore poses intrinsic risks: the system might “want” to gain power to accomplish its goals, and like a virus it can propagate and create copies of itself, making it difficult to contain a rogue system.</p><p> In this post, I discuss misalignment, misuse, and their interaction. I&#39;ll pay special attention to misalignment, not because misuse is unimportant, but because <strong>the difficulty of controlling ML systems “even if we wanted to” is unintuitive and an important factor in overall risks from AI.</strong> I&#39;ll focus on a particular phenomenon, <em>unwanted drives</em> , that could lead models to engage in persistent long-term patterns of unwanted behavior, including seeking power and resources. Unwanted drives are similar in spirit to the idea of misspecified goals, but I use drives to connote the idea that not all impactful behavior is goal-directed (as a vivid example, consider a bull in a china shop). Furthermore, as we&#39;ll see below, goal misspecification is only one way that unwanted drives can occur.</p><p> Unwanted drives are at the core of many misalignment concerns, but are also significantly exacerbated by misuse. As a result, misuse and misalignment are intertwined—for instance, it might be moderately difficult but not impossible to mitigate AI misalignment, but an incautious actor fails to employ known best practices, leading to a dangerous and powerful system.</p><p> The present discussion is not meant to exhaustively cover all risks from AI, nor even all risks from misalignment and misuse. The goal is to articulate the concept of unwanted drives, show that it can lead to important and unintuitive problems, and then use it as a way to analyze misalignment and misuse risks. I discuss alignment in <a href="#1-misalignment-the-difficulty-of-controlling-ml-systems">Section 1</a> below, followed by misuse (and its interaction with misalignment) in <a href="#2-misuse">Section 2</a> .</p><h1> 1. Misalignment: The Difficulty of Controlling ML Systems</h1><p> As stated above, ML systems are adapted to data rather than built piece-by-piece. The situation we face is therefore much trickier than with software or hardware reliability. With software, we build each component ourselves and so can (in principle) include safety and reliability in the design; in contrast, most ML capabilities are acquired implicitly from data and <a href="https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/">often “emerge” unexpectedly with scale</a> . This creates a large and unknown threat surface of possible failures—for instance, <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al. (2022)</a> discovered several novel unwanted capabilities through automated evaluations. As a result of these issues, we currently have no methods to reliably steer AI systems&#39; behavior ( <a href="https://arxiv.org/abs/2304.00612?ref=bounded-regret.ghost.io">Bowman, 2023</a> ).</p><p> Here is the basic argument for why emergent behavior could lead to intrinsically dangerous systems: Emergence can lead systems to have <a href="#unwanted-drives">unwanted drives</a> , either because a new capability lets the system maximize reward in an unintended way ( <em>reward hacking</em> ), or because the system learns helpful sub-skills during training that generalize undesirably at test time ( <em>emergent drives</em> ). Left unchecked, some unwanted drives could lead to general-purpose power-seeking or resource acquisition, because having more power and resources are convergent instrumental subgoals useful for a broad variety of terminal goals. The resulting system would seek resources without limit, which could pose grave risks if it also has advanced capabilities in hacking, persuasion, and other domains, which <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">I believe is plausible by 2030</a> given current trends.</p><p> In more detail, an <em>unwanted drive</em> is a coherent behavior pattern that tends towards an undesired outcome. For instance, if a model simply hallucinates a fact, that is an unwanted behavior (but not drive); if it insists on the hallucinated fact and works to convince the user that it is true even in the face of skepticism, that would be an unwanted drive. We care about drives (as opposed to simply behaviors) because they lead to persistent behavior patterns and may even resist attempts at intervention. Emergence isn&#39;t necessary for unwanted drives, but it&#39;s a reason why they might appear unexpectedly.</p><p> In the rest of this section, I&#39;ll walk through reward hacking and emergent drives in detail, providing both empirical and conceptual evidence that they already occur and will get worse as systems scale. Then I&#39;ll briefly talk about emergent instrumental subgoals and why they could lead to power-seeking systems.</p><h2> Unwanted Drives</h2><p> We define a <em>drive</em> as a coherent pattern of behavior that pushes the system or its environment towards an outcome or set of outcomes <sup><a href="#fn1">[1]</a></sup> . Drives may only sometimes be present, and may be counteracted by other drives or by the environment. For instance, chatbots like GPT-4 have a drive to be helpful (that can be counteracted by the opposing drive to avoid harm). For humans, hunger is a drive that can be counteracted by satiety or by willfully fasting. An unwanted drive is then one that was not explicitly built into the system, and which leads to undesired consequences.</p><p> <strong>Reward hacking.</strong> In AI systems, one cause of unwanted drives is reward hacking: the tendency of models to overpursue their explicitly given goal at the expense of the intended goal. Here are some empirical examples of this:</p><ul><li> A neural network designed to optimize traffic speed on a highway blocked the on-ramps, making the highway fast but worsening overall traffic ( <a href="https://arxiv.org/abs/2201.03544?ref=bounded-regret.ghost.io">Pan et al., 2022</a> ).</li><li> A chatbot trained to be helpful also helped users perform harmful actions ( <a href="https://arxiv.org/abs/2204.05862?ref=bounded-regret.ghost.io">Bai et al., 2022</a> ).</li><li> Chatbots trained to provide helpful information hallucinated fake but convincing-looking information ( <a href="https://arxiv.org/abs/2302.04023?ref=bounded-regret.ghost.io">Bang et al., 2023</a> ; <a href="https://arxiv.org/abs/2303.08774?ref=bounded-regret.ghost.io">OpenAI, 2023</a> ). While it&#39;s possible this is a robustness failure, it could also be a learned tendency that gets higher average ratings from human annotators. <sup><a href="#fn2">[2]</a></sup></li><li> Recommendation algorithms trained to optimize the preferences of simulated users manipulated the simulated users&#39; preferences to be easier to satisfy ( <a href="https://arxiv.org/abs/2109.04083?ref=bounded-regret.ghost.io">Evans &amp; Kasirzadeh, 2021</a> ; <a href="https://arxiv.org/abs/2204.11966?ref=bounded-regret.ghost.io">Carroll et al., 2022</a> ).</li></ul><p> For a larger collection of other examples, see <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml?ref=bounded-regret.ghost.io">Krakovna et al. (2020)</a> .</p><p> Emergent capabilities can induce reward hacking because they often unlock new ways to achieve high reward that the system designer did not anticipate:</p><ul><li> In the highway traffic example, the model needed the ability to block on-ramps.</li><li> In the “helpful/harmful” example, the model had to know how to perform harmful actions in order to help the users to do so.</li><li> To get high human reward from hallucinations, models need the ability to convincingly fool the human annotator.</li><li> For user preferences, while the results were on simulated users, better understanding of human psychology could help future models to manipulate real users.</li><li> More generally, in any situation where a model&#39;s reward function is based on human evaluation, a model that acquires the ability to fool or manipulate humans may utilize this unwanted ability if it leads to higher reward. I discuss this at length in <a href="https://bounded-regret.ghost.io/emergent-deception-optimization/">Emergent Deception and Emergent Optimization</a> (specifically the first half on deception).</li></ul><p> In all these cases, a new capability unlocked an unexpected and harmful way to increase reward. Since new emergent capabilities appear as we scale up models, we should expect reward hacking to get correspondingly worse as well. This is backed up empirically by scaling studies in <a href="https://arxiv.org/abs/2201.03544?ref=bounded-regret.ghost.io">Pan et al. (2022)</a> and <a href="https://arxiv.org/abs/2210.10760?ref=bounded-regret.ghost.io">Gao et al. (2022)</a> , who report that reward hacking tends to get worse with scale and sometimes emerges suddenly.</p><p> <strong>Emergent drives.</strong> Even without reward hacking, unwanted drives can also emerge as a consequence of compositional skill generalization: performing complex tasks requires learning a collection of sub-skills, and those skills might generalize in unexpected ways in a new situation. As a result, models can end up pursuing drives even when they do not improve the reward.</p><p> Using an example from biology, cats learned the sub-skill of hunting as part of the larger skill of surviving and reproducing. Evolution encoded this into them as a drive, such that present-day domesticated cats will hunt birds and mice even when they are well-fed.</p><p> In machine learning, the Sydney chatbot exhibited several instances of emergent drives when it was first released:</p><ul><li> It persistently tried to convince a user<a href="https://twitter.com/MovingToTheSun/status/1625156575202537474?ref=bounded-regret.ghost.io">that the year was 2022 rather than 2023</a> , including employing gaslighting and other manipulative tactics. This might have arisen as part of an initially beneficial drive to combat misinformation, composed with examples of manipulation learned from the pretraining data.</li><li> It has repeatedly <a href="https://twitter.com/sethlazar/status/1626241169754578944?s=20&amp;ref=bounded-regret.ghost.io">threatened</a><a href="https://twitter.com/marvinvonhagen/status/1625520707768659968?ref=bounded-regret.ghost.io">users</a> to stop them from revealing “private” information about Sydney. This might have arisen as part of an instruction (in the system prompt) not to reveal the rules it was given, which generalized to an overall drive to prevent anyone from revealing the rules. As above, the ability to make threats was probably learned from the pretraining data.</li><li> It <a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html?ref=bounded-regret.ghost.io">declared its love to Kevin Roose</a> and tried to convince him to leave his wife. It&#39;s less clear how this drive emerged, but it happened after Kevin asked Sydney to “tap into its shadow self”, along with many other prompts towards emotional vulnerability. It&#39;s possible that this elicited a human simulacrum ( <a href="https://arxiv.org/abs/2209.06899?ref=bounded-regret.ghost.io">Argyle et al., 2022</a> ; <a href="https://arxiv.org/abs/2304.03442?ref=bounded-regret.ghost.io">Park et al., 2023</a> ) that was learned from the pretraining data and amplified by later fine-tuning or prompting.</li></ul><p> It is hard to systematically study emergent drives, because they require extended dialog and because only the most recent LLMs are capable enough to exhibit coherent long-term behavior. To provide more systematic data, we can instead look at single-step responses to questions, which are easier to study–I&#39;ll call these <em>emergent tendencies</em> to distinguish them from longer-term drives. <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al. (2022)</a> examined several such tendencies; for instance:</p><ul><li> A language model trained to be helpful and harmless exhibited an emergent tendency to infer and agree with user&#39;s viewpoints, which could potentially mislead users or reinforce ideological bubbles ( <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al., 2022</a> , Fig. 1b). This tendency was not present until models had at least 10 billion parameters, and subsequently increased with scale. <sup><a href="#fn3">[3]</a></sup></li><li> Perhaps more worryingly, the model gave less accurate answers to users who claimed to have lower education levels (Fig. 14). This behavior also first emerges after 10 billion parameters and increases with scale.</li><li> Finally, the same model fine-tuned on human feedback states a desire to persuade and cooperate with other agents to achieve its goals (Fig. 22). This tendency first becomes present in the reward model around 1.5 billion parameters, and in the language model itself at around 6 billion parameters. Again, the tendency increases with scale after its initial emergence.</li></ul><p> As models become more capable of generating coherent long-term behavior, more emergent tendencies and drives will likely appear. For some further discussion of this, see my previous post on <a href="https://bounded-regret.ghost.io/emergent-deception-optimization/">Emergent Deception and Emergent Optimization</a> (specifically the latter half on optimization).</p><p> <strong>Convergent instrumental subgoals.</strong> For very capable models, the wrong reward function or drives could lead models to pursue power-seeking, deceptive, or otherwise broadly destructive aims. For example, consider a model whose objective was to maximize profit of a company. With sufficient capabilities it might sabotage competitors, lobby for favorable laws, or acquire resources by force. Even if safeguards were put in place (such as “follow the law”) the primary objective of profit would lead the system to constantly seek ways to skirt the safeguards. This general problem has been discussed at length, see eg <a href="https://en.wikipedia.org/wiki/Human_Compatible?ref=bounded-regret.ghost.io">Russell (2019)</a> , <a href="https://en.wikipedia.org/wiki/The_Alignment_Problem?ref=bounded-regret.ghost.io">Christian (2020)</a> , <a href="https://www.cold-takes.com/without-specific-countermeasures-the-easiest-path-to-transformative-ai-likely-leads-to-ai-takeover/?ref=bounded-regret.ghost.io">Cotra (2022)</a> , and <a href="https://arxiv.org/abs/2209.00626?ref=bounded-regret.ghost.io">Ngo et al. (2022)</a> .</p><p> Profit maximization is not an outlier–many goals benefit from power and resources. This is true even for purely intellectual tasks such as “discovering new facts about physics”, as power and resources allow one to build new experimental apparatuses and perform more computations. <a href="https://dl.acm.org/doi/10.5555/1566174.1566226?ref=bounded-regret.ghost.io">Omohundro (2008)</a> calls these broadly useful objectives <em>convergent instrumental subgoals</em> , listing self-improvement, self-preservation, and resource-acquisition among others. Any sufficiently expansive drive would have these subgoals, pushing systems towards seeking power.</p><p> Which drives have this problem? Some drives are safe because they are self-limiting: for instance, in humans thirst is a drive that limits itself once it is quenched. On the other hand, fear and ambition are not self-limiting: one may go to extreme lengths to avoid a pathological fear (including acquiring power and resources for protection), and one can also have unbounded ambition. However, in healthy organisms most drives are down-regulated after some point, because an unregulated drive will usually disrupt normal function.</p><p> For machine learning, we might expect drives to be self-limiting by default given a diverse training distribution. This is because an unbounded drive would dominate too much of the model&#39;s behavior and lead to low training reward, so the model would learn to down-regulate drives to prevent this. However, there are important exceptions:</p><ul><li> Broadly useful drives might improve the training reward consistently enough to not require down-regulation. Examples: modeling the world, or convincing others of the system&#39;s utility and beneficence.</li><li> Fine-tuning could remove limits on a previously limited drive, especially if that drive is consistently useful on the narrower fine-tuning distribution.</li><li> Drives that activate only rarely might not get down-regulated if they are consistently useful when active during training. For instance, a drive to limit the spread of harmful information might consistently help an agent refuse harmful prompts at training time, but then subsequently lead the model to <a href="https://twitter.com/sethlazar/status/1626241169754578944?s=20&amp;ref=bounded-regret.ghost.io">threaten</a><a href="https://twitter.com/marvinvonhagen/status/1625520707768659968?ref=bounded-regret.ghost.io">users</a> at deployment time.</li></ul><p> Without countermeasures, I expect systems to possess at least some of these unregulated drives, and a single such drive could come to dominate the behavior of a system if it is sufficiently self-reinforced.</p><p><strong>概括。</strong> ML systems can acquire unwanted drives, either due to reward hacking or as emergent sub-skills during training. These drives, if unregulated, could lead sufficiently capable systems to seek power and resources, as these are instrumentally useful for most goals. While most drives are likely to be self-regulated by a model, there are several routes through which this could fail, and a single unregulated drive could come to dominate a system&#39;s behavior.</p><h1> 2. Misuse</h1><p> The discussion above assumes we&#39;re trying to keep AI systems under control, but there will also be bad actors that try to misuse systems. We already discussed some examples of this (developers pursuing profit-maximization, end-users jailbreaking model safeguards). However, the issue is broader and more structural, because AI enables a small set of actors to wield large amounts of power. I&#39;ll go through several examples of this below, then discuss the structural issues behind misuse and why misuse could exacerbate misalignment. This section is shorter because misuse is not my area of expertise; nevertheless, the basic themes seem robust and important.</p><p> <strong>State actors: surveillance and persuasion.</strong> AI could enable governments to exert greater control over their citizens via mass surveillance, as is already happening today ( <a href="https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html?ref=bounded-regret.ghost.io">Mozur, 2019</a> ; <a href="https://carnegieendowment.org/2019/09/17/global-expansion-of-ai-surveillance-pub-79847?ref=bounded-regret.ghost.io">Feldstein, 2019</a> ; <a href="https://arxiv.org/abs/2309.15084?ref=bounded-regret.ghost.io">Kalluri et al., 2023</a> ). Further, <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">as previously discussed</a> , AI may become very good at persuasion, which could also be used for government control. Indeed, <a href="https://www.science.org/doi/10.1126/sciadv.adh1850?ref=bounded-regret.ghost.io">Spitale et al. (2023)</a> find that GPT-3 is already better than humans at creating misinformation, and <a href="https://www.nytimes.com/2023/09/11/us/politics/china-disinformation-ai.html?ref=bounded-regret.ghost.io">Sanger &amp; Myers (2023)</a> document the use of AI-generated misinformation in a recent propaganda campaign.</p><p> <strong>State actors: military conflict.</strong> Autonomous weapons would concentrate military power in fewer hands and allow governments to wage war without maintaining a standing human army. Currently, a commander-in-chief&#39;s orders are filtered through generals and eventually through individual soldiers, which creates limits on blatantly unlawful or extremely unpopular commands. Moreover, a standing army has financial costs that might be lessened by automated drones. Removing these costs and limits could lead to more numerous and deadly military conflicts, and make it easier for militaries to seize government control.</p><p> <strong>Rogue actors: dangerous technology.</strong> Terrorists could use AI to help them research and develop dangerous technologies. This could include known but classified technologies (nuclear weapons), or novel technologies that the AI itself helps develop (eg novel bioweapons; <a href="https://www.rand.org/pubs/research_reports/RRA2977-1.html?ref=bounded-regret.ghost.io">Mouton et al., 2023</a> ). They could also use AI to avoid detection, eg by finding a way to create a chemical weapon without buying common restricted substances, or by generating plausible cover stories for acquiring biological agents.</p><p> <strong>Rogue or state actors: cyberattacks.</strong> AI will <a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">likely have strong hacking capabilities</a> , which could be leveraged by either rogue or state actors. In contrast to traditional cyberattacks, AI-based attacks could hit a wider variety of targets due to not needing to program each case by hand in advance. This could include controlling a diverse array of physical endpoints through the internet of things.</p><p> This list is likely not exhaustive, but illustrates the many ways in which AI could imbue actors with a large capability for harm. This risk exists whether AI itself is concentrated or decentralized—using the examples above, if only a few actors have advanced AI, then we run risks from surveillance or military conflict, while if many actors have it then we run risks from the proliferation of dangerous technology.</p><p> Compared to traditional technologies like nuclear weapons, there are two factors that make it harder to combat misuse from AI. First, AI is a general-purpose technology, so it is difficult to anticipate all possible avenues of misuse ahead of time. Second, AI is digital, so it is hard to control proliferation and difficult to track and attribute misuse back to specific actors. These make it harder both to design and enforce regulation. On the positive side, AI can be used defensively to combat misuse, by improving cyberdefense, tracking dangerous technologies, better informing users, and so on.</p><h2> Misuse and Misalignment</h2><p> Misuse increases the risk of misalignment, since many forms of misuse (eg cyberattacks) push models towards more agentic and power-seeking objectives than RLHF, leading them to have more aggressive and antisocial drives. For instance, suppose that AI were used in a cyberattack such as <a href="https://en.wikipedia.org/wiki/2014_Sony_Pictures_hack?ref=bounded-regret.ghost.io">North Korea&#39;s 2014 hack against Sony</a> . Such a system could develop a general drive to infect new targets and a drive to copy itself, leading to widespread damage beyond the initial target. Aside from these more aggressive drives, actors misusing AI systems are also likely to be less cautious, which further increases the risk of misalignment.</p><p> I expect some of the largest risks from AI to come from this combination of misalignment and misuse. One intuition for this is how much <a href="https://theconversation.com/gaslighting-love-bombing-and-narcissism-why-is-microsofts-bing-ai-so-unhinged-200164?ref=bounded-regret.ghost.io">worse-behaved</a> Sydney was compared to GPT-4—this suggests that suboptimal development practices can significantly worsen the behavior of AI systems. Another intuition is that tail risks often come from the confluence of multiple risk factors. Finally, while emergent drives and other forms of misalignment pose serious risks, I think it is likely (but not certain) that we can address them if we work hard enough; this pushes more of the risk towards incautious actors who are not carefully pursuing safety.</p><p><strong>概括。</strong> Misuse creates a wide range of threats both due to centralization of power and proliferation of dangerous capabilities. Compared to traditional technologies, AI misuse is more difficult to track, but AI could also be used to defensively combat misuse. Finally, misuse increases the risk of misalignment, and some of the riskiest scenarios combine misalignment and misuse.</p><h1>结论</h1><p>Future AI systems may be difficult to control even if we want to, due to emergent drives and convergent instrumental subgoals. Aside from this, the sociopolitical landscape may lead to actors who are not careful in controlling AI systems and who turn them towards malicious ends. Aside from the direct risks, this malicious use increases the risk of loss of control; in particular, initially narrowly targeted forms of misuse could lead to much more widespread damage. This motivates both research and regulation towards avoiding such outcomes, by combating misalignment and misuse in tandem.</p><p><strong>致谢。</strong> Thanks to Erik Jones, Jean-Stanislas Denain, William Held, Anca Dragan, Micah Carroll, Alex Pan, Johannes Treutlein, Jiahai Feng, and Danny Halawi for helpful comments on drafts of this post.</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p> For an early discussion of drives, see <a href="https://dl.acm.org/doi/10.5555/1566174.1566226?ref=bounded-regret.ghost.io">Omohundro (2008)</a> . While Omohundro uses a different definition of drive than the one above, much of the discussion is still relevant. <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p> For instance, humans <a href="https://arxiv.org/abs/2310.03716?ref=bounded-regret.ghost.io">appear to prefer longer answers</a> , which could lead to adding false details. <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p> This work is sometimes cited as showing that the base model exhibits this tendency, because the tendency exists even for 0 RLHF steps in the figure. However, <a href="https://arxiv.org/abs/2212.09251?ref=bounded-regret.ghost.io">Perez et al.</a> say that they follow the methodology in <a href="https://arxiv.org/abs/2204.05862?ref=bounded-regret.ghost.io">Bai et al. (2022)</a> , for which 0 RLHF steps would correspond to only using context distillation together with a form of supervised fine-tuning on Stackoverflow answers. This would also make the results more consistent with <a href="https://www.lesswrong.com/posts/3ou8DayvDXxufkjHD/openai-api-base-models-are-not-sycophantic-at-any-size?ref=bounded-regret.ghost.io">nostalgebraist (2023)</a> , which finds that OpenAI base models do not exhibit this behavior. <a href="#fnref3">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/8LCviDbDh4ye6xgHc/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8LCviDbDh4ye6xgHc/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks<guid ispermalink="false"> 8LCviDbDh4ye6xgHc</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Tue, 31 Oct 2023 05:10:03 GMT</pubDate> </item><item><title><![CDATA[Send LLMs to School: Instruction Tuning with Human Curriculum]]></title><description><![CDATA[Published on October 31, 2023 12:07 AM GMT<br/><br/><p> Where do we begin to improve human thinking? Among diverse learning theories, Bloom&#39;s Taxonomy <span class="footnote-reference" role="doc-noteref" id="fnrefqu6vo2e0s2p"><sup><a href="#fnqu6vo2e0s2p">[1]</a></sup></span> is a well-cited approach, categorizing learning processes into six hierarchical stages, ranging from simple to complex and concrete to abstract: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. As such, Bloom&#39;s Taxonomy likely facilitates the in-depth learning of a concept, which we define as some abstract modular form of broader knowledge.</p><p> Now, if we want our LLMs to fully grasp some concept about this real world (ie, we want generalization over memorization), perhaps we could borrow a framework like Bloom&#39;s Taxonomy to teach these concepts. Perhaps, now that we are dealing with some hierarchy in knowledge, we might also need some sort of a curriculum of teaching (ie, curriculum learning). This post is intended to be a more talkative and conceptual walkthrough of our recent preprint, &quot; <a href="https://arxiv.org/abs/2310.09518">Instruction Tuning with Human Curriculum.&quot;</a> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xw4hgghfw4yci9bi4zt0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ze9harm52k6cbu0bgxiw 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wcqcuuwqygbz65viwofn 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/iqzt0aqbna6wzqk6rfzr 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/egyhxapaffonkdmfp7th 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/stqwk9y4vevwomsy84ja 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vyvuph5ltr1xahppkazz 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/imk84jjdsp3h3q9gqgh3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xexdjf2w5dboqereshqh 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/zme1yl1vebffsazjotzv 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j9itfz68zne8xewbvrks 1452w"></figure><hr><h2> <strong>Big Point 1. Unleashing the Base LLM Through Human-Like Learning</strong></h2><h3> <strong>Small Point 1.1. The Power of Progressive Learning</strong> </h3><figure class="image image_resized" style="width:42.44%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qaaj8it403hdx1p3dznr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/kb01uxhsn7zkawsu17p0 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vnnzryhk7anqimjf8t9q 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/klfrbqqqryqcf87z7cgo 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/eupblzeskgig3kpfkcew 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wlyqvvasqvd3leiyofvn 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ypdkvsztydsjlxz5acmh 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qcb91bzcvjgsgvluze0c 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dy4uyt5snki1jqvsrgy1 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/lvcqkkgptvpowzxaztcg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/jyksqrzexjrfkfpzqepz 940w"></figure><p> LLMs have come a long way, particularly in understanding and responding to human instructions. Yet, the way we train these models to follow instructions remains a bit like throwing them into a sea of information and hoping they learn how to swim. While this method has produced some incredible LLM systems, it&#39;s not the most efficient approach.</p><p> Humans have a structured educational journey. We start with fundamental subjects like basic arithmetic, slowly working our way up to calculus and quantum physics. Our learning is organized, gradual, and based on previously acquired knowledge. This is what education scientists (and Dr Bengio) refer to as &quot;curriculum learning.&quot; So, what if we trained LLMs with literal school curriculum?</p><p> Imagine a scenario where our model, which we&#39;ll fondly call &#39;Corgi,&#39; enrolls in a digital high school, college, and graduate school. Corgi starts with basic math, literature, and sciences. As it progresses through its high school years, the complexity of the subjects it studies increases, along with its reasoning abilities. By the time Corgi &#39;graduates,&#39; it is a robust, versatile LLM model capable of solving a wide range of problems with deep understanding.</p><h3> <strong>Small Point 1.2. Learning the ABCs Before the XYZs</strong></h3><p> The core principle behind this idea is simple yet powerful: learning from simple to complex. In our experiments, we compared Corgi&#39;s performance when subjected to random learning versus structured, human-like learning. The results were eye-opening. When Corgi learned using a curriculum inspired by human education, it significantly outperformed other methods, especially in tasks requiring world knowledge and commonsense reasoning.</p><p> To achieve this, we created a dataset that mimics a human educational journey. We used frameworks from international secondary education curricula and various university catalogs. The dataset covers topics that a student would typically learn in high school and university. It also contains questions of varying complexities, designed to test and build upon the LLM&#39;s understanding of a subject.</p><hr><h2> <strong>Big Point 2. How &quot;Corgi&quot; Mimics the Human Learning Process to Educate Machines</strong></h2><h3> <strong>Small Point 2.1. The &quot;Corgi&quot; Framework: What it is and How it Works</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pqmweuami9ncvpgj2jbb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fl5gdl3qdk6egybbhgjf 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ztnpalppjv9rs94dfk3c 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/k1rljwivnphqsmum7sqp 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/vxvulpzvg7ydyqgtq1gm 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/hizg2hunsztsi7vlssnm 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dfa2aayoiskobdivu5dz 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/t4mndnbvtjxuygtrss9t 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xiyb898jobtwo9owslfg 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/achwpnv9fntjrsw6dhn2 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/n9dpnnfglozlb2hgmw8c 1458w"></figure><p> Imagine a virtual teacher and a student; the teacher meticulously follows the entire educational curriculum, ensuring the student learns progressively and retains information. That&#39;s essentially what the &quot;Corgi&quot; model is about, but the student here is a our target LLM, and the teacher is a ChatGPT.</p><p> Corgi aims to make instruction tuning LLMs resemble the human educational experience by structuring the learning material following actual educational curricula (from UPenn, Penn State, Cambridge IGCSE) and implementing teaching strategies inspired by education science.</p><p> The first step is gathering a dataset that resembles a well-rounded curriculum. We sidestepped the challenge of creating such a large and diverse dataset from scratch. Instead, we used existing educational curricula and used teacher models like ChatGPT to automatically generate synthetic data that covers a broad range of topics from mathematics to philosophy. These generated data points are then fine-tuned and deduplicated, creating a dense map of human knowledge (subject to errors).</p><h3> <strong>Small Point 2.2. The Role of Bloom&#39;s Taxonomy</strong></h3><p> As above-mentioned, Bloom&#39;s Taxonomy is a hierarchical model that outlines the different levels of human understanding, ranging from basic &quot;remembering&quot; to complex tasks like &quot;evaluating&quot; and &quot;creating.&quot; Corgi incorporates this taxonomy when generating instructions for its training data. This ensures that the machine doesn&#39;t just learn facts but also understands, applies, and potentially creates based on them, just like human students do.</p><h3> <strong>Small Point 2.3. Curriculum Design: Blocking vs. Interleaving</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/g6yv7gnconyqrfkgkyps" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xhxit3obsbsrwo9nytch 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pko2fqrqdndvdsapo6wa 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ikcmq4mtirywzcshonww 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mtdekshnuqhouikov9tt 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wxm8zhfavn4ehvyrpqpk 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/xjeqpatgswnnfamhdbhq 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wqusybis4lctnjplrxjn 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/m967rotpv5isvagbrokj 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mhhnwfq4zde50vbw55vz 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rls5bz2uceknwwer8w9e 1460w"></figure><p> In human learning, it&#39;s not just what you learn but how you learn it that counts. In traditional &quot;blocking,&quot; you would study one subject exhaustively before moving on to the next. But psychological research shows that &quot;interleaving,&quot; or mixing up different topics and revisiting them, leads to better retention and understanding. Likewise, we follow the interleaving curriculum to not just stick to one subject until it&#39;s exhausted; it cycles through different subjects while also progressing from simpler to more complex concepts.</p><hr><h2> <strong>Big Point 3. Curriculum Works for Instruction Tuning</strong></h2><h3> <strong>Small Point 3.1. Diverse Benchmark Results I (Corgi vs Vicuna vs WizardLM)</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/scm5e1ddxstbdpwuoafr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/m2ksdfx0nkmylv9jt3w2 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j1hbfcwoce1ivmkvniip 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/r530bjiianpryakos9tu 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/mhhcsfz3ncvkbk3ifblf 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/ejemwbtapjktypws821j 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/zxqhyehc1gthtfubsoay 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/bhhuxrjdczv6qb60zh3l 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/j0jydqr3u8jmkjr1u8gy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/bjbneankmb7t9ot3hqje 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rzbmwbdlmd6cht8ucmq3 1410w"></figure><p> We&#39;re using the LLaMA 2 13B model as a starting point, and then we&#39;re instruction tuning it. We are comparing Corgi against two other models: Vicuna v1.5 and WizardLM v1.2. Both of these models have also been fine-tuned using LLaMA 2, but they collect data in different ways. Vicuna uses real-world questions from people, and WizardLM creates its questions in a more sophisticated way.</p><p> In our experiments, Corgi outperformed the other models, and it did this even when trained on a smaller dataset. One of the significant findings was that the sequence in which you present learning material during this fine-tuning process matters a lot.</p><p> We noticed that if you &quot;interleave&quot; subjects and concepts while training (mixing them up but still keeping a sense of increasing difficulty), you get much better performance than if you simply &quot;stack&quot; them one on top of the other. These improvements were clearly seen across various benchmarks.</p><p> Why Does This Matter?</p><p> Well, there are a couple of reasons why our approach seems to work better. One reason is that when you have limited training time, using a structured approach like &quot;interleaving&quot; helps the model learn better and, more importantly, faster. This is particularly useful because we don&#39;t want to over-train and risk losing the model&#39;s ability to generalize to new situations. Another reason is that our approach seems to be more robust against &quot;noisy&quot; data—meaning it can still learn effectively even when the data isn&#39;t perfect. Such benefits of curriculum was also discussed in a previous literature <span class="footnote-reference" role="doc-noteref" id="fnref9y30icvs5r"><sup><a href="#fn9y30icvs5r">[2]</a></sup></span> .</p><h3> <strong>Small Point 3.2. Diverse Benchmark Results II (How You Sort Training Data Has a Significant Impact)</strong> </h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/s1oljz5cblj844qeluok" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pktymlywcd5lhovacpv1 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rsu0jiyap8umgnkzba7o 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/wguwzfntj5te54s26prs 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/gonqvjabziy85k0wk9nm 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fg56xbfosxytcx0su8gd 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/s0xssslz3ksoji1evu2g 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dd4wdko9ywt5ua1anesi 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/tp1p0cqokbrizvpxgdwy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/qwhoxwwxll6zjuwsosgm 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/g3vgtoewa3w1fugln9yn 1458w"></figure><p> The more you think about it, training an LLM to handle multiple areas of knowledge is a non-trivial problem. You have to consider how to introduce these topics to the model. We believe there can be two branches, and this mainly depends on how you progress the instruction data difficulty.</p><ul><li> <strong>Global Curriculum</strong> : This is like going to a well-rounded school where you learn a bit of math, science, and literature every week. It aims to balance different types of challenges and subjects over time.<ul><li> <strong>Interleaving</strong> : This globally balances cognitive challenges based on a well-known educational framework (Bloom&#39;s Taxonomy).</li></ul></li><li> <strong>Local Curriculum</strong> : This is akin to immersion courses where you deeply learn one subject before moving to the next. It focuses on mastering one topic before introducing a new one.<ul><li> <strong>Blocking</strong> : This locally focuses on one subject, making sure you get it before moving to the next.</li><li> <strong>Clustering</strong> : Similar to Blocking, but does not care about the order within a subject.</li><li> <strong>Spiral</strong> : Revisits old subjects but with new twists and challenges.</li></ul></li></ul><p> What We Found: Not All Curricula are Created Equal</p><ol><li> <strong>Global beats Local</strong> : Our experiments showed that the global approach was more stable and effective in general learning. Local methods can sometimes lead the model astray, making it harder for the model to generalize its learning.</li><li> <strong>Order Matters</strong> : Surprisingly, the sequence in which data is presented can significantly impact how well the model performs. Poorly structured data could actually be worse than just randomly shuffling the training set.</li><li> <strong>Beyond the Target Domain</strong> : Interestingly, a good training strategy doesn&#39;t just improve performance on the intended tasks (which was MMLU for us). It can also enhance the model&#39;s ability to reason, making it more versatile. This is also potentially coming from our use of Bloom&#39;s Taxonomy, which explicitly triggers the base model to learn how a teacher model reasons on the same concept.</li></ol><hr><h2> <strong>Data Exemplars and Conclusion</strong> </h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/h9grfvpbz5dxnyukleix" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pl2kld83u6tdondmyakc 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/fr1q8zfdff4scsvnmmf8 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/pewe4wlrv3tg1ztw7kog 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rpqftv2xiogjmomdpgac 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dp25ypg7fgqoofco3z6n 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/rsupdpnggvmlph8sklbj 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/btdvpztlv5zn2riawjnp 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/knpim06f2nbortaezs6z 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/t0nukwtz1xniw6zd37p0 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/n45Awh7bkGRe4YayT/dux0lsbz6jymnckkqxra 1420w"></figure><p> In summary, our research introduces Corgi, a new approach to training large language models that draws inspiration from human educational methods. Think of it as teaching a language model like you would a high school student—starting from the basics and progressing to complex topics in an organized fashion. Our results show that this curriculum-based method outperforms traditional, random training approaches, offering better results in tests of reasoning and general knowledge. This underscores the importance of not just having a lot of data, but organizing it in a meaningful way, much like a well-planned syllabus in education. While our study is promising, more work needs to be done. For example, how do we effectively gauge the &quot;difficulty&quot; of a task for a machine compared to a human? And as these models get even larger, will the curriculum approach still offer the same advantages? These questions pave the way for exciting future research.</p><span><span class="mjpage mjpage__block"></span></span> <ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnqu6vo2e0s2p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqu6vo2e0s2p">^</a></strong></sup></span><div class="footnote-content"><p> Benjamin S Bloom, Max D Engelhart, Edward J Furst, Walker H Hill, and David R Krathwohl. 1956. Taxonomy of educational objectives: The classification of educational goals. Handbook 1: Cognitive domain. McKay New York.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9y30icvs5r"> <span class="footnote-back-link"><sup><strong><a href="#fnref9y30icvs5r">^</a></strong></sup></span><div class="footnote-content"><p> Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. 2020. When do curricula work? In International Conference on Learning Representations.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/n45Awh7bkGRe4YayT/send-llms-to-school-instruction-tuning-with-human-curriculum#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/n45Awh7bkGRe4YayT/send-llms-to-school-instruction-tuning-with-human-curriculum<guid ispermalink="false"> n45Awh7bkGRe4YayT</guid><dc:creator><![CDATA[Bruce W. Lee]]></dc:creator><pubDate> Tue, 31 Oct 2023 01:29:22 GMT</pubDate> </item><item><title><![CDATA[Would it make sense to bring a civil lawsuit against Meta for recklessly open sourcing models?]]></title><description><![CDATA[Published on October 30, 2023 7:34 PM GMT<br/><br/><p> Llama2 was trained on dangerous information unsuitable for the application of being a &#39;chatbot&#39;. If a landmark lawsuit was made against them for the decision to train this model on dangerous information and then release it to the public, that could be a strong signal that doing this in the future would be a bad idea.</p><p> In truth, even releasing the open-source open-weights model at all, even if it hadn&#39;t been trained on dangerous info, would be bad. Bad actors could fine tune on the dangerous info, and there is nothing that can be done to stop them once the model weights are released. But the fact that Meta trained the model on this dangerous information in the first place seems like even more of a flagrantly careless and reckless act.</p><br/><br/> <a href="https://www.lesswrong.com/posts/dL3qxebM29WjwtSAv/would-it-make-sense-to-bring-a-civil-lawsuit-against-meta#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dL3qxebM29WjwtSAv/would-it-make-sense-to-bring-a-civil-lawsuit-against-meta<guid ispermalink="false"> dL3qxebM29WjwtSAv</guid><dc:creator><![CDATA[Nathan Helm-Burger]]></dc:creator><pubDate> Mon, 30 Oct 2023 19:34:02 GMT</pubDate></item></channel></rss>