<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 25 日星期五 18:14:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Assume Bad Faith]]></title><description><![CDATA[Published on August 25, 2023 5:36 PM GMT<br/><br/><p>我一直在努力避免使用“善意”和“恶意”这两个词。我怀疑，大多数听到“恶意”这个词的人，实际上并不知道它的意思——而且也许，它所代表的意思并没有<a href="https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries">切合现实</a>。</p><p>人们对恶意指控非常敏感：他们认为你应该假设是善意的，但如果你确定某人是恶意的，你甚至不应该与他们交谈，你需要驱逐他们。</p><p>那么“恶意”是什么<em>意思呢</em>？这并不意味着“怀有恶意”。<a href="https://en.wikipedia.org/wiki/Bad_faith">按照<em>维基百科的说法</em></a>，恶意是“一种持续的欺骗形式，包括娱乐或假装娱乐一种感觉，同时表现得好像受到另一种感觉的影响”。这本伟大的百科全书还提供了一些例子：挥舞着投降旗帜但当敌人从战壕中出来时开枪的士兵、起诉明知是虚假案件的律师、面临劳资纠纷的公司代表他来到谈判桌前并没有妥协的意图。</p><p>也就是说，恶意是指某人做某事的表面原因与真实原因不一致。这与恶意不同。穿着制服的士兵在不假装投降的情况下向你开枪，他的行为是真诚的，因为所见即所得：那个衣服表明他的工作是试图杀死你的人实际上正在试图杀死你。</p><p>如果你生活在一个诚实的世界，所见即所得（并且你想保持这种状态），那么假设善意（并在发现时无情地惩罚罕见的恶意案例）的政策是有意义的。日常生活中隐藏动机的可能性并不是一个重要的考虑因素。</p><p>然而，相反，我认为<a href="https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain">日常生活中隐藏的动机是无处不在的</a>。作为进化的生物，我们生来就相信，因为相信对我们的祖先有好处。特别是作为社会性动物，最有益的信念并不总是真实的，因为欺骗你的同类采用一张暗示他们应该对你有利的地图有时比拥有反映领土和最有说服力的地图更有价值。谎言是你自己相信的。人类的普遍默认行为是提出理由来说服对方为什么做你想做的事符合他们的利益，但承认你所做的<em>不是游戏的一部分</em>。 <a href="https://www.lesswrong.com/posts/h2Hk2c2Gp5sY4abQh/lack-of-social-grace-is-an-epistemic-virtue">一个人们直接试图互相告知的世界对我们来说会显得令人震惊和陌生。</a></p><p>但如果是这样的话（你不应该相信我的话），对恶意指控过于敏感似乎会适得其反。如果人们所陈述的理由与真实理由不一样是很常见的，那么认为某个特定的人的理由不应该是超出范围的，也不一定需要将“恶意行为者”从公共生活中剔除——如果只是因为坚持不懈地应用，就不会有任何人留下了。为什么你会如此高度信任某人，以至于认为他们从来没有隐藏的议程？你为什么要相信自己？</p><p>相信“恶意”是不寻常的，这导致了一种扭曲的世界观，在这种世界观中，信息战的条件被合理化为不可避免的存在背景事实。特别是，人们似乎相信持续存在的善意分歧是一种常见现象——假设我是一个诚实的真理寻求者，而你也是一个诚实的真理寻求者，那么这种假设的事态并没有什么奇怪或不寻常的，但我们最终在某些事实问题上始终存在分歧。</p><p>我认为这种看似平常的事态充其量是<em>非常奇怪的</em>，而且可能只是假的。<em>真正的</em>“善意”分歧——双方只是想得到正确的答案，没有其他隐藏的动机，没有“其他事情”发生<a href="https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting">——往往不会持续下去</a>。</p><p>如果这种说法看起来有悖常理，那么您可能没有考虑到所有日常信仰差异，这些差异解决得如此迅速和无缝，以至于我们往往不会将它们视为“分歧”。</p><p>假设你和我一直计划去听一场音乐会，我记得是在星期四。我问你：“嘿，音乐会是在星期四，对吗？”你说：“不，我刚刚查看了网站；今天是星期五。”</p><p>在这种情况下，我<em>立即</em>用你的信念取代我的信念。我们都只是想知道音乐会何时举行这个事实问题的正确答案。在没有“其他事情”发生的情况下，没有什么可以阻止我们一步步趋同：你刚刚检查过的网站是比我的记忆更可靠的来源，你和网站都没有任何理由撒谎。所以，我相信你；故事结局。</p><p>在真实答案不确定的情况下，我们预计概率信念也会同样快速收敛。假设你和我正在研究一些物理问题。我们都只是想要正确的答案，而且我们都没有特别比对方更熟练。一旦我得知你得到的答案与我不同，我对自己答案的信心<em>立即直线</em>下降：如果我们都同样擅长数学，那么我们每个人犯错误的可能性都差不多。在我们比较计算并找出我们中哪一个（或两个）犯了错误之前，我认为你和我一样有可能是正确的，即使我不知道你是如何得到答案的。对我来说，仅仅因为答案是我的就拿钱赌我的答案是正确的，这是没有意义的。</p><p>大多数值得注意的分歧（大多数人们<em>关心</em>的分歧）的表现并不像音乐会日期或物理问题示例：人们非常执着于“自己的”答案。有时，经过长时间的争论，有可能让某人改变主意或承认另一方可能是对的，但要就事件的日期或计算结果（的概率）达成一致绝非易事——由此我们可以推断，在大多数人们关心的分歧中，除了双方都想得到正确的答案之外，还<em>存在着</em>“其他的事情”。</p><p>但是，如果在典型的分歧中存在“其他事情”，看起来像是一场怨恨比赛，而不是导致概率收敛的快速信息交换，那么认为持续的善意分歧很常见的信念似乎是恶意的！ （因为如果恶意是“在表现出一种感觉的同时又表现得好像受到另一种感觉的影响”，那么坚持不懈的善意分歧的信徒就会感到这种分歧的双方都是诚实的真理寻求者，但是相反，他们预计会看到怨恨的比赛而不是趋同。）</p><p>有些人可能会反对说，恶意是有意识的欺骗意图：<a href="https://slatestarcodex.com/2019/07/16/against-lie-inflation/">诚实地报告无意识的偏见信念</a>并不是恶意。我之前曾对<a href="https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#The_Optimal_Categorization_Depends_on_the_Actual_Psychology_of_Deception">我们所谓的谎言有多少需要有<em>意识</em>的深思熟虑表示怀疑</a>，但更基本的回答是，从<a href="https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception">建模信息传输</a>的角度来看，偏见和欺骗之间的区别并不<em>有趣</em>——通常与概率更新无关。应该做。</p><p>如果一个苹果是绿色的，而你告诉我它是红色的，而我相信你，那么我最终会对苹果产生错误的信念。无论你说它是红色是因为你有意识地撒谎，还是因为你戴着玫瑰色眼镜，这并不重要。无论哪种方式，输入输出函数都是相同的：问题是你向我报告的颜色并不取决于苹果的颜色。</p><p>如果我只是想弄清楚你的报告与世界状况之间的关系（与关心惩罚说谎者而让仅仅有偏见的人摆脱困境相比），关心无意识偏见和无意识偏见之间的区别的主要原因有意识的欺骗在于后者提出了更强烈的抵抗。仅仅有偏见的人在遇到足够令人信服的反驳时通常会<em>屈服</em>（或者被提醒摘掉玫瑰色眼镜）；一个有意识地撒谎的人会<em>继续</em>撒谎<a href="https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies">（并说一些辅助性的谎言来掩盖真相），</a>直到你在观众面前当场抓住他们，并拥有对他们的权力。</p><p>鉴于在持续的分歧中通常会发生“其他事情”，如果我们不能依赖善意的假设，我们该如何继续下去呢？我看到了两种主要策略， <a href="https://www.lesswrong.com/posts/SX6wQEdGfzz7GKYvp/rationalist-discourse-is-like-physicist-motors">每种策略都有自己的成本效益概况</a>。</p><p>一种策略是坚持对象级别。可以根据论点的优点来评估论点，而不需要考虑说话者说话的角度（即使你认为可能有角度）。这提供了“假定善意”规范的大部分好处；我提出的主要区别在于，发言者的意图被视为<em>偏离主题</em>，而不是被认为是诚实的。</p><p>另一种策略是全面接触的精神分析：除了辩论对象层面的论点外，对话者还可以自由地质疑彼此的动机。这很难实现，这就是为什么大多数人大部分时间都应该坚持对象层面。如果做得好，它看起来就像一场谈判：在讨论过程中，伪分歧（我主张一种信念，因为该信念出现在共享地图上符合我的利益）被分解为真正的分歧和利益讨价还价这样才能找到并采取帕累托改进，而不是双方为了各自的利益而努力扭曲共享地图。</p><p>举一个伪分歧的例子，假设我拥有一家工厂，我正在考虑将其扩展到邻近的湿地，而你经营着一个当地的环保组织。有权阻止工厂扩建的监管委员会的职责是保护当地鸟类的生命，但不保护湿地面积。工厂排放少量Exampne 气体。您在监管委员会面前辩称，应该阻止扩张，因为最新的《科学》表明，Exampne 会让鸟类感到悲伤。我反驳说，最新的科学表明，Exampne 实际上使鸟类快乐；之前的研究将他们的笑声误听成了眼泪，应该撤回。</p><p>实际上，我们表面上的分歧似乎不太可能是“真正”关于Exampne对鸟类情绪调节的影响。更有可能的是，实际上发生的是<a href="https://www.lesswrong.com/posts/DpTexwqYtarRLRBYi/conflict-theory-of-bounded-distrust">冲突而不是分歧</a>：我想将我的工厂扩展到湿地，而你希望我不要这样做。提出Exampne污染如何影响鸟类的问题只是为了说服监管委员会。</p><p>我们的冲突被伪装成分歧，这是低效的。我们不可能同时得到我们想要的东西，但无论工厂扩建问题最终如何得到解决，最好是在不扭曲协会关于Exampne生物活性特性的共享图谱的情况下达到这一结果。 （也许它根本不会影响鸟类！）无论真正的答案是什么，如果允许有人指出你和我的偏见，社会就能更好地弄清楚这个问题<a href="https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting">（因为有关哪些证据的事实会引起人们的注意）与应如何更新该证据相关）</a> 。</p><p>我认为谈论“恶意”没有用的原因是，善意<em>与</em>恶意的本体论并不适合这两种话语策略。</p><p>如果我坚持对象层面，那就无关紧要了：我回复文本中的内容；我对生成文本的过程的怀疑超出了范围。</p><p>如果我正在进行全面接触的精神分析，“我不认为你是真诚地在这里”的问题是它不够<em>具体</em>。推动讨论前进的方法不是指责某人一般性的“恶意”，而是假设对话者有一些尚未明确的特定动机，而针对此类指控为自己辩护<a href="http://zackmdavis.net/blog/2022/05/plea-bargaining/">的</a>方法是：<a href="http://zackmdavis.net/blog/2022/05/plea-bargaining/">一个人的真正议程不是所提议的议程</a>，而不是抗议一个人的“善意”并令人难以置信地声称没有议程。</p><p>这两种策略可以混合使用。一个简单的元策略可以在不施加过高技能要求的情况下表现良好，即默认为对象级别，并且只将精神分析作为对抗<a href="https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters">阻碍</a>的最后手段。</p><p>假设你指出我最近的回复似乎与我之前所说的相矛盾，我说：“看那边，分散注意力！”</p><p>如果你想继续坚持对象层面，你可以说，“我不明白分散注意力与解决我提出的陈述中的不一致有什么关系。”另一方面，如果你想深入进行精神分析，你可以说：“我认为你只是指出分心，因为你不想被束缚。”然后我将被迫要么解决你的投诉，要么解释为什么我有其他原因来指出干扰。</p><p>然而，至关重要的是，是否调查动机的选择并不取决于只有“坏人”才有动机的假设——就好像有不怀好意的行为者有自己的角度，而有善意的行为者是完美空虚的理想哲学家。 。总有一个角度；问题是哪一个。</p><br/><br/><a href="https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith<guid ispermalink="false"> e4GBj6jxRZcsHFSvP</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Fri, 25 Aug 2023 17:36:32 GMT</pubDate></item><item><title><![CDATA[A Model-based Approach to AI Existential Risk]]></title><description><![CDATA[Published on August 25, 2023 10:32 AM GMT<br/><br/><h1>介绍</h1><p>两极分化阻碍了合作和了解未来人工智能是否对人类构成生存风险以及如何降低灾难性后果的风险的进展。确定这些风险是什么以及什么决策是最好的是非常具有挑战性的。我们认为，<i>基于模型的方法</i>具有许多优势，可以提高我们对人工智能风险的理解，评估缓解政策的价值，并促进人工智能风险争论不同方面的人们之间的沟通。我们还相信，人工智能安全和校准社区中的很大一部分从业者都拥有成功使用基于模型的方法的适当技能。</p><p>在本文中，我们将引导您通过一个基于模型的方法的示例应用来应对未结盟人工智能带来的生存灾难风险：基于卡尔史密斯的<a href="https://joecarlsmith.com/2023/03/22/existential-risk-from-power-seeking-ai-shorter-version"><i><u>《寻求权力的人工智能是否是​​存在风险？》的</u></i></a>概率模型。您将与我们的模型进行交互，探索您自己的假设，并（我们希望）就这种类型的方法如何与您自己的工作相关提出您自己的想法。您可以在此处找到该模型的链接。</p><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>在许多鲜为人知的领域，人们倾向于采取倡导立场。我们在人工智能风险中看到了这一点，经常看到作家<a href="https://www.lesswrong.com/posts/BTcEzXYoDrWzkLLrQ/the-public-debate-about-ai-is-confusing-for-the-general"><u>轻蔑地称某人为“人工智能末日论者”或“人工智能加速论者”。</u></a>这场辩论的双方都无法向另一方传达他们的想法，因为倡导通常包括在另一方不共享的框架内解释的偏见和证据。</p><p>在其他领域，我们亲眼目睹了基于模型的方法是消除此类宣传的建设性方法。例如，通过利用基于模型的方法， <a href="https://lumina.com/case-studies/energy-and-power/a-win-win-solution-for-californias-offshore-oil-rigs/"><u>Rigs-to-Reefs 项目</u></a>在 22 个不同的组织之间就如何退役圣巴巴拉海岸大型石油平台这一有争议的问题达成了近乎共识。几十年来，环保组织、石油公司、海洋生物学家、商业和休闲渔民、航运利益集团、法律辩护基金、加利福尼亚州和联邦机构在这个问题上陷入了僵局。模型的引入使对话重新聚焦于具体假设、目标和选项，并导致 22 个组织中的 20 个组织就同一计划达成一致。加州立法机构通过 AB 2503 号法案将该计划写入法律，该法案几乎一致通过。</p><p>人工智能带来的生存风险存在很多不确定性，而且风险极高。在这种情况下，我们主张使用概率分布明确量化不确定性。遗憾的是，这种情况并没有应有的普遍，即使在此类技术最有用的领域也是如此。</p><p> <a href="https://arxiv.org/abs/2206.13353"><u>Joe Carlsmith</u></a> (2022) 最近发表的一篇关于不结盟人工智能风险的论文有力地说明了概率方法如何帮助评估先进人工智能是否对人类构成生存风险。在本文中，我们回顾了 Carlsmith 的论点，并将他的问题分解纳入我们自己的<a href="https://lumina.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>模型中。然后，我们以多种方式扩展这个起点，以展示应对 x 风险领域中每个独特挑战的基本方法。我们带您参观实时模型，了解其元素，并使您能够自己更深入地研究。</p><h2>挑战</h2><p>预测长期未来总是充满挑战。如果<a href="https://www.youtube.com/watch?v=87l9Az9msHU&amp;t=3108s"><u>没有历史先例</u></a>，难度就会被放大。但这种挑战并不是独一无二的。我们在许多其他领域缺乏历史先例，例如在考虑新的政府计划或全新的商业计划时。当世界状况因技术、气候、竞争格局或监管变化而发生变化时，我们也缺乏先例。在所有这些情况下，难度都很大，但与预测通用人工智能 (AGI) 和存在风险的挑战相比就显得苍白无力了。今天对人工智能存在风险的预测通常至少部分依赖于关于未来先进人工智能将如何表现的抽象论点，而我们今天无法测试这些论点（ <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>尽管我们正在努力改变这一点</u></a>）。即使是最精心设计的论点也常常会遇到合理的不确定性和怀疑。<br><br>例如，在评估有关人工智能存在风险的预测的可靠性时，经常会遇到这样的反对意见：“我在预测中找不到具体的缺陷。它们只是有点抽象，有点连贯，而且论证这个班级经常会以意想不到的方式犯错。”</p><p>举个例子，最近超级预测者对人工智能风险的启发似乎表明，这种普遍的怀疑态度是超级预测者和人工智能安全社区之间对人工智能风险<a href="https://astralcodexten.substack.com/p/the-extinction-tournament"><u>持续存在分歧</u></a>的一个因素。尽管两个群体之间进行了讨论，而且超级预测者在<a href="https://www.lesswrong.com/posts/BHdEvjtfwpgrTh825/ai-21-the-cup-overfloweth?commentId=dpzuaGHyiNcnr4jF2"><u>有关未来人工智能的许多定量预测</u></a>上与领域专家达成了一致，但这种关于人工智能风险的分歧仍然存在，这表明对人工智能风险论点的怀疑更加分散。应认真对待此类反对意见，并根据其本身的优点以及过去类似反对意见在其他领域的表现进行评估。这凸显了不仅评估预测内容，而且评估其背后的基本假设和推理的重要性。</p><p>对于为什么可能出现某些结果，人工智能风险界已经提出了许多论点。当你着手建立一个明确的人工智能存在风险模型时，如果不吸收其他聪明、专注的人经过深思熟虑的想法，那就太疏忽了。然而，将多个想法合并到一个连贯的模型中确实很困难，并且根据某些统计，有多达<a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=C3sb2QZQAeHLZmz2T"><u>五个部分重叠的世界观/研究议程</u></a>，每个都专注于不同的威胁模型。不同的论点常常建立在相互不一致的概念框架之上。简单地统计某个立场存在多少个论据也是行不通的，因为基本假设几乎总是存在大量重叠。此外，以任何深入的方式将<a href="https://www.lesswrong.com/posts/Cd6uMn4qHXZcoe2Lh/discussion-weighting-inside-view-versus-outside-view-on"><u>内部观点与外部观点</u></a>论证合并起来似乎几乎是不可能的。虽然很困难，但整合现有的专家知识（和意见）对于有效的基于模型的方法至关重要。我们认为人工智能存在风险建模在整合多种专家知识来源方面具有独特的方面，因此是进一步研究新方法和新技术的成熟领域。我们已将解决本段中提到的所有挑战的简单方法纳入我们的模型中。</p><p>主观概率是表示不确定性的传统工具，一般来说，它是一个很好的工具。基于模型的方法依赖于对不确定变量的主观评估。在人工智能存在风险领域，当你要求两位专家评估相同的主观概率时，他们的估计通常会存在显着差异（例如，一位专家说 15%，而另一位专家说 80%）。这在其他领域并不正常。尽管您可能会发现两个气象学家分别预测下雨概率为 15% 和 80% 的情况，但这种情况并不常见。</p><p>这是上面已经讨论过的困难的症状，并且引入了该领域的另一个显着特征。由于专家之间的这种极端差异，以及人们的估计校准不佳的事实，似乎需要获得额外的信心层。我们在本文后面的“元不确定性”部分对此进行了详细说明，并且我们在模型中包含了显式的二阶分布（即，二阶分布代表专家意见之间的差异，而一阶不确定性代表不确定性）在结果中）。</p><p>我们在本文中描述的工作是作为 MTAIR 项目（建模变革性人工智能风险）的一部分进行的，建立在<a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">最初的 MTAIR 概念模型的</a>基础上。我们的目标是评估 AGI 存在风险的多个、有时甚至根本上相互冲突的详细模型以及这些外部观点/可靠性考虑因素。我们将他们视为相互竞争的“专家”，以便做出明智且平衡的评估。您可以使用我们的交互式模型来输入您自己的评估并探索其含义。</p><h2>是什么让模型有效？</h2><ul><li><strong>透明度</strong>：为了有效，基于模型的方法应该提供其他人可以浏览和理解的模型。当了解该主题的典型用户能够理解模型在做什么、如何做以及计算中使用什么假设时，我们称模型是<i>透明的</i>。您永远不应该假设主题专家是程序员，或者 python 代码（或任何其他编程语言）不言而喻。因此，传统的程序通常被认为是不透明的。</li><li><strong>交互性</strong>：第二个重要属性是<i>交互性</i>，以及利益相关者尝试不同假设、探索不同决策或政策的后果以及探索任意假设场景的能力。</li><li><strong>显性的不确定性</strong>：对于人工智能的存在风险，大部分行动都是在不确定性的尾部进行的（即简单地得出中值结果是人类生存的结论是没有抓住要点的）；因此，<i>不确定性的明确表示</i>很重要。</li></ul><p>我们在<a href="https://analytica.com/"><u>Analytica 视觉建模软件</u></a>中构建了模型，该软件强烈满足上述所有要求，并且使用起来很有趣。 Analytica 模型的结构为分层影响图，这是一种高度可视化且易于理解的表示形式，捕捉了模型如何直观地工作的本质。它是交互式的并且具有嵌入式模块化文档。有强大的多维智能阵列设施，提供了前所未有的灵活性。它使用概率分布明确地表示不确定性。不确定性向下游计算结果的传播会自动发生。它学习起来既简单又快捷，一旦您构建了模型，您就可以将其发布到网络上进行共享（正如我们在本文中所做的那样）。</p><p>如果您受到我们的示例的启发来构建自己的模型，您应该知道<a href="https://analytica.com/products/free101/"><u>Analytica 有一个免费版本</u></a>。当您需要扩展到真正的大型模型时，也可以使用商业版本。桌面版本需要 Microsoft Windows。您无需获取或安装任何东西（浏览器 - Chrome 或 Edge 除外）即可使用我们的模型，该模型在 Analytica 云平台 (ACP) 上共享。我们的模型大约有 150 个对象，略超过免费版本 101 个对象的最大大小。但如果您有兴趣将其下载到桌面 Analytica，免费版本允许您加载、查看、运行、更改输入和重新评估结果等。</p><p>总之，基于模型的方法来评估人工智能存在风险预测的可靠性可以为人工智能安全社区带来多种好处。首先也是最重要的，它提供了清晰、简洁和易读的输出，考虑了可能影响预测准确性的许多不同的反对意见和因素。这有助于确保人工智能安全社区了解预测背后的推理和证据，并可以根据该信息做出明智的决策。</p><p><strong>此外</strong>，这种基于模型的方法鼓励社区考虑更广泛的因素，而不仅仅是详细的论点本身。例如，他们可能会考虑他们对高级抽象的信任程度以及不同启发式的可靠性。通过将这些考虑因素纳入模型中，社区可以更有效地权衡与人工智能相关的风险，并制定更稳健的策略来减轻潜在危害。最后，这种方法可以通过促进对所有相关因素进行更严格的思考和更全面的检查来帮助改善社区的认知，从而更好地理解人工智能存在风险的性质和可能性。</p><p>作为起点，我们将重点关注基于 Joe Carlsmith 报告<a href="https://arxiv.org/abs/2206.13353"><u>“寻求权力的人工智能是否存在风险”的</u></a>单一详细模型，以及影响这一机械模型合理性的几个外部观点/可靠性启发法。我们将首先简要介绍卡尔史密斯对人工智能存在风险的介绍以及我们自己的一些改进，然后在最后讨论改进该模型的后续步骤。</p><h1>型号概览</h1><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>这是在 Analytica 云平台 (ACP) 中运行的分层模型，基于 Joe Carlsmith 的报告“寻求权力的人工智能是否存在风险”。它可以让你计算由人工智能失调引起的生存灾难的概率。</p><p>这些结论隐含地取决于我们在给出各种假设的情况下明确规定的某个时间范围。事实上的时间范围是“到 2070 年”，但在输入您自己的估计时，您可以采用不同的时间范围，而无需更改模型的逻辑。</p><p>简而言之，该模型预测，如果出现以下情况，错位的寻求权力的人工智能将导致一场生存灾难：</p><ol><li>先进、规划、<a href="https://www.planned-obsolescence.org/situational-awareness/"><u>战略意识</u></a>（APS）系统——即能够进行高级规划、具有战略意识并拥有先进的人类水平或超人类能力的人工智能——是可行的，</li><li>当 APS 系统可行时，将会有强烈的激励措施来建设它们，</li><li>构建不以不协调的方式寻求权力的 APS 系统比构建表面上有用但确实以不协调的方式寻求权力的 APS 系统要困难得多，<ol><li>尽管 (3)，实际上将构建和部署错位的 APS 系统，</li></ol></li><li>未对准的 APS 系统在部署后将能够造成全球性的大灾难，</li><li>人类对造成此类灾难的错位 APS 系统的反应不足以阻止其完全接管，</li><li>一旦接管，错位的 APS 系统将摧毁或严重削弱人类的潜力。</li></ol><p>我们模型的总体框架基于卡尔史密斯报告和随后的<a href="https://80000hours.org/problem-profiles/artificial-intelligence"><u>80,000 小时文章</u></a>中提供的人工智能存在风险论点，并进行了修改。这是我们的“顶级模型”，我们对人工智能存在风险进行高级分析。</p><h1>模型巡演</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/mvk6oge6czjenatva2gs"></p><p>在本节中，您将快速浏览我们的模型，并在浏览器窗口中实时运行它。首先，请单击<a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><u>“启动模型”</u></a>以在不同的浏览器选项卡或窗口中将其打开，以便您可以同时参考此页面。我们提供分步说明来帮助您入门。按照此导览了解您的方向，然后您可以自己更深入地探索模型的其余部分，并探索不同估计值会发生什么。我们建议在大显示器上运行模型，而不是移动设备。</p><h2>基本评估</h2><p>在第一页上，您将看到卡尔史密斯报告中的六个概率评估。 （请注意，本文中的屏幕截图是静态的，但它们在运行模型的浏览器窗口中是活动的）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/rolr4s2y84njur0cbthc"></p><p>您可以在此处调整滑块或为每个滑块输入您自己的估计值。要了解每个含义，只需将鼠标悬停在问题上并阅读弹出的说明。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/adt7luy3kmzavtkfqwmd"></p><p>在估计这些之前，您应该选择一个时间范围。例如，您可以估计每个值在 2070 年之前是否成立。计算取决于您的估计，而不是所选的时间范围，但您的估计预计会随着更长期的时间范围而改变（增加）。</p><p>滑块输入下方是一些计算结果，显示 5 个阶段中的每个阶段以及所有前面的阶段最终为真的概率。最后一个“存在性灾难”显示了根据您对六个命题中每一个命题的估计，APS 系统发生存在性灾难的概率。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/qy1yjgqxgkjmwhzm43c5"></p><p>在这个屏幕截图中，我们看到 APS 有 0.37% 的几率（不到百分之一）会导致人类灭绝等生存灾难。考虑到结果的极端性，这似乎是一个巨大的风险，但许多专门研究人工智能安全的人会认为这是超级乐观的。您的估计如何比较？</p><h2>专家们权衡利弊</h2><p>您的估计与其他人工智能安全研究人员的估计相比如何？在卡尔史密斯的报告之后，开放慈善组织<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>征求了其他人工智能安全研究人员的评论</u></a>，并要求他们提供自己对这些主张的估计。这些评论发生于 2022 年 8 月。</p><p>首先，您可以通过按浏览他们对每个提议的原始评估<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/x5126haezlzwpqulhccb">审阅者评估按钮，该按钮显示在页面右下角（您可能需要向右滚动）。该表显示在页面的右上角。请注意审稿人之间的巨大差异。</p><p>单击“选择要使用的中值评估”的选项下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ugm5g5wylo2zr8tawy64"></p><p>选择所有项目，使其现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/jakuafkaps0pfd9umwih"></p><p>存在灾难输出现在显示<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">按钮。按下。结果表出现在右上角。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cdgv48bhgvvyqdl0accx"></p><p>这些显示了估计所隐含的由 APS 引起的存在灾难的概率，这些估计来自您自己的输入以及审阅者的输入。审稿人的中位数为 9.15%，但审稿人之间的数字差异很大。在少数情况下，审稿人不愿意接受卡尔史密斯提出的分解，就会出现空值。接下来，我们将其显示为条形图。将鼠标悬停在表格区域顶部以访问图形按钮，然后按它。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/coje0q97joeqsped2ibd"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/wi8p3wpl6wwu1xiaksrr"></p><p>再次将鼠标悬停在图表顶部，并将视图更改回表视图。查看结果时，您可以通过这种方式在图形视图和表格视图之间切换。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kxtxsvzrvhvdegaqopw8"></p><h2>专家意见的差异</h2><p>专家意见的巨大差异对该领域的理性决策提出了严峻挑战。很难说任何基于通过汇总这些概率获得的预期效用是可信的。因此，我们根据专家意见的变化拟合概率分布。因为这是主观概率的分布，所以它实际上是<i>二阶概率分布</i>，我们称之为<i>元不确定性</i>。我们用一<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.2klzxyklefio"><u>节</u></a>的内容来讨论元不确定性、其动机及其解释，但现在让我们想象一下这种元不确定性。</p><p>将<i>“选择要使用的中值评估”</i>更改为<strong>“所有审阅者的中值”</strong> ，并在“选择<i>要包含的元不确定性”</i>的选择下拉列表中选择<strong>审阅者的分布</strong>选项。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/whhm6iheuj3vqf4fa9h8"></p><p>输出现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">纽扣。将鼠标悬停在 Existential 灾难输出上并按鼠标右键。从上下文菜单中选择<strong>超出概率</strong>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dipzz5jhchogoicm7ydn"></p><p>在框架节点中，切换回图形视图（ <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/f8qvwqfinrmxpyjxzyny"> ）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y2mwu3iud8senllipqbi"></p><p>超出概率图是可视化概率分布的一种方法。本例中的分布反映了专家意见的差异。基本量（x 轴）是 APS 系统中发生诸如人类灭绝之类的生存灾难的概率。沿着绿色箭头，您可以看到大约 10% 的专家认为存在灾难的概率超过 0.17（即 17%），沿着黄色箭头，大约 5% 的专家认为存在灾难的概率超过 0.27。</p><p>为了获得这种二阶分布，该模型将每个问题的专家评估集合视为从基础分布中采样，然后将概率分布“拟合”到这些点。这种拟合的技术细节将在后面的“元不确定性”部分中介绍。该部分还探讨了当元不确定性（即专家意见之间的变化量）增加或减少时我们的观点如何变化。</p><h2>结合内部和外部视图参数</h2><p>卡尔史密斯分解是<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>内部观点框架</u></a>的一个例子，它将感兴趣的主要问题分解为其组成因素、步骤或起作用的因果机制。相比之下，<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>外部视图框架</u></a>从相似的事件或参考类中得出相似之处，以提供上下文和预测。例如， <a href="https://forum.effectivealtruism.org/posts/MMtbCDTNP3M53N3Dc/agi-safety-from-first-principles#AGI%20safety%20from%20first%20principles"><i><u>第二个物种论点</u></i></a>认为人类可能会失去地球上最强大物种的地位。其他外部观点框架包括霍尔顿·卡诺夫斯基的<a href="https://www.cold-takes.com/most-important-century"><i><u>“最重要的世纪”</u></i></a> 、阿杰亚·科特拉的<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>生物锚</u></a>（对一个子问题、时间表、更大问题的外部观点）、与过去变革性技术进步的类比，甚至<a href="https://forum.effectivealtruism.org/posts/mjB9osLTJJM4zKhoq/2022-ai-expert-survey-results"><u>专家意见调查</u></a>。</p><p>每种类型的框架都会产生不同的见解，但由于内部和外部视图框架以不同的方式进行评估，因此将两者吸收为一致的观点是相当具有挑战性的。但我们认为基于模型的方法需要解决这个问题，以便整合来自所有来源的信息。</p><p>我们包括两种简单的外部视图方法（在后面的部分中详细讨论），由这些输入反映出来： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/w4lspva3ifst3gseyzbh"></p><p>将鼠标悬停在每个输入上即可获取您所估计内容的完整描述。这些需要你抽象地思考几个高层的外部观点考虑因素和论点，然后评估这些考虑因素对存在灾难的风险有多大影响。 Cr在这里是<i>信任的</i>意思。与统计学中可能性的概念类似（有些人可能会说同义），可信度是从 0 到 1 范围内的估计，其中 0 表示考虑因素暗示没有风险，1 表示考虑因素暗示一定的灾难。</p><p>您现在已经输入了您自己对卡尔史密斯“世界模型”以及外部观点可信度的估计。我们的重点是模型如何将这些吸收到单一的主观观点中？我们的目标是强调这一挑战并至少尝试这样做。也许您或其他继续采用未来基于模型的方法的人会改进我们的方法。</p><p>在此模型中，我们允许您为不同视图分配相对权重。单击权重表按钮以赋予不同的意见。将鼠标悬停在输入上即可查看要求您评估的内容的描述。可信度是对您认为这些外部观点论据本身支持该主张的程度进行的评级。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/irrgjkuu6h3szgx5ma8o"></p><p>顶部框架中会出现一个条目表，其中包含可用于更改相对权重的滑块。您可以调整这些以反映您自己对相对可信度的看法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/hvjy2karshi3lntbs83r"></p><p>第一部分允许您输入卡尔史密斯分解与外部视图参数相比的相对重要性。在这里，我们将外部视图固定为 1，因此（基于卡尔史密斯的）世界模型的值为 3 意味着您希望该框架的计数是外部视图参数的三倍。</p><p>在世界模型中，您有自己的估计以及接受调查的各个专家的估计。您可以选择或多或少地重视个别专家的估计。</p><p>最后，在下部，您可以调整两个不同的外部视图框架的权重。这些用于组合不同的外部观点论点。</p><p>设置自己的权重后，右列中的输出将显示同化视图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/s586akwpoclyxujnzywc"></p><p>第一个输出 Cr[存在灾难|世界模型] 是卡尔史密斯分解在考虑了您自己的估计与专家的估计之间的相对权重后的评估。</p><p>第二个输出 Cr[AI Existential Catastrophe] 是组合外部视图模型中存在灾难的概率。</p><p>最终输出 Cr[Existential catastrophe] 是对存在灾难的最终同化估计。它考虑了内部视图世界模型和外部视图模型，将两个来源的信息结合起来作为代表性的最终评估。</p><h2>探索模型的内部结构</h2><p>到目前为止，您已经使用了我们为您突出显示的一些选定的输入和输出。接下来，您将探索模型的内部结构。</p><p>顶部是一个大的蓝色模块节点<strong>Main Model</strong> 。点击它。这将带您进入实施阶段，您会看到几个子模块和<a href="https://lumina.com/technology/influence-diagrams/"><u>影响图</u></a>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cm3tppz2rkmycyj2rvyb"></p><p>在第一个图中，上半部分包含基于卡尔史密斯报告的内部视图世界模型。左下角包含外部视图参数。右下四分之一是用于同化不同观点的逻辑。</p><p>影响图的节点是变量。箭头描绘了变量之间的影响。影响图是直观的，您通常可以从中了解模型是如何工作的，而无需查看计算细节。将鼠标悬停在节点上可查看其描述，以获取有关每个变量所代表含义的更多信息。</p><p>在外部视图部分中，一些未定义的节点（已散列）仅用于记录纳入估计的考虑因素。虚线箭头表示这些不是计算所使用的影响，但应该会影响您的思考。</p><p>单击节点后，请注意顶部的选项卡。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vd4gjj6wwh9ffyez8t4p"></p><p> <strong>“对象”</strong>选项卡可能是最有用的，因为它允许您查看单击的变量的定义（和其他属性）。查看完该变量后， <strong>“图表”</strong>选项卡将返回图表。</p><p>现在您已经完成了这个快速浏览，您应该可以轻松地探索模型的各个方面。接下来，我们将更深入地研究纳入模型的内容和概念。</p><h1>型号特点</h1><p>在将卡尔史密斯报告的人工智能存在风险模型应用于 Analytica 时，我们对原始计算进行了一些更改，即简单地将命题 1-6 的条件概率相乘，以获得对错位人工智能的存在风险的总体估计。</p><p>为了更好地捕获围绕问题的全方位不确定性，我们处理了“元不确定性”，方法是将每个点估计更改为一个分布，其方差取决于我们对每个概率估计的置信度，如上<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.wtvwqwsb1xzp"><u>一节所述</u></a>。</p><p>元不确定性是指由于我们对影响我们的信念或观点的更普遍因素的不确定性而产生的不确定性。这些因素可能包括诸如我们应该对内部观点和外部观点给予多少重视，以及长期预测的可靠性如何等问题。</p><p>元不确定性与更直接的不确定性不同，因为它关注的是我们对风险评估所依据的假设和因素的不确定性。它本质上是二阶不确定性，我们不确定驱动一阶不确定性的因素。</p><p>我们通过将<a href="https://www.wikiwand.com/en/Logit-normal_distribution"><u>Logit 正态分布拟合到 Joe Carlsmith 报告的每个原始审稿人给出的单个点估计值的分布，生成了这些元不确定性分布</u></a>。该方法与本文<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future#3_2_Model_Parameterisation"><u>《化解人工智能风险》</u></a>中使用的方法类似。</p><p>我们还纳入了其他不太详细的“外部观点考虑因素”，它们不像卡尔史密斯报告那样依赖于详细的世界模型。我们对这些与卡尔史密斯模型相关的外部观点争论的信任会影响该模型给出的人工智能存在灾难的最终无条件概率。这些外部观点的考虑可以被视为补偿详细世界模型中出现的一般可靠性问题的一种方法，因此是减少我们模型的随机误差或“未知的未知”困难的一种方法。</p><p>我们尚未讨论的一件事是卡尔史密斯模型中潜在的系统缺陷。正如我们将在“框架效应”一节中讨论的那样，一些研究人员反对卡尔史密斯报告本身的框架，认为它系统性地使我们产生向上或向下的偏见。</p><h2>元不确定性</h2><p>围绕人工智能存在风险问题存在许多复杂且不确定的问题，包括协调的难度、不协调的人工智能是否容易接管，甚至是否会建立“APS”类型的通用人工智能（AGI）。世纪。这些不确定性使得评估人工智能存在风险的总体概率变得困难。</p><p>量化这些风险的一种方法是为每个索赔分配点概率估计并将其向前传播，正如卡尔史密斯关于该主题的原始报告中所做的那样。然而，这种方法存在一些问题。作为卡尔史密斯模型输入的六个概率估计中的每一个都涉及历史上没有先例的事件。因此，估计这些事件的概率具有挑战性，并且当您看到两位不同专家的估计存在显着差异时，没有明确且明显的方法来判断哪个估计更可信。</p><p><i>元不确定性</i>通过对可能的意见进行概率分布来审视可能的信念状态。<a href="https://acp.analytica.com/view?invite=4418&amp;code=3221222959844354027"><u>我们的模型</u></a>包含您可以探索的几个版本的元不确定性。</p><p>包含元不确定性的一个有用目的是了解专家意见的变化，以及这种变化如何影响模型的输出。</p><p>开放慈善组织要求人工智能风险领域的几位专家<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>提供他们自己对卡尔史密斯报告中参数的估计</u></a>。我们已将这些包含在我们的模型中。您可以从这些专家中的任何一位或任何专家集中选择估计值。您还可以包括 Joe Carlsmith 在他的文章中给出的估计、所有审稿人的中位数以及您自己的估计。当您同时选择多个时，您将能够在任何下游结果中对它们进行比较。要进行选择，请使用模型前图中的“选择要使用的中值评估”的多选下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/zg0ztojles89j7qd1uqy"></p><p>当您查看模型中变量的结果时，您将看到使用您选择的每个审阅者的估计得出的结果值。例如，这是存在灾难概率的结果表。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kojklpskvf2kxdmgxeda"></p><p>从这些中，您可以了解专家意见的差异有多大，但这还不包括元不确定性的概率分布。对于每个输入，您可以让模型将概率分布拟合到审阅者提供的评估（对于统计极客：它拟合<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal</u></a> ，又名 Log-Odds 分布）。要自己探索这一点，请将“选择要包含的元不确定性”下拉菜单设置为“审阅者的传播”。完成此操作后，它会使用具有跨专家观察到的元不确定性方差的分布来执行所有计算（对于统计极客来说：它实际上是每个数量的<a href="https://docs.analytica.com/index.php/Logit"><u>logit</u></a>的方差与专家的方差相匹配）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/isvopzarcegg2d9pwuug"></p><p>在模型的内部，名为“Assessments”的变量现在包含六个输入评估中每一个的元不确定性分布。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ndgs3pq2uosqwvcpylgj"></p><p>上图显示了每个评估数量的<a href="https://docs.analytica.com/index.php/Uncertainty_views#Cumulative_probability"><u>累积概率</u></a>（称为 CDF 图）。 Y 轴上的值表示专家估计的数量值小于或等于 x 轴上相应值的可能性。该图的关键项目按顺序对应于卡尔史密斯模型的六个评估。第一项标记为<i>“时间线</i>”，是评估 APS 系统在考虑的时间线窗口内构建的可行性。其红色 CDF 几乎是一条直线，表明所选专家之间的不确定性几乎均匀分布。标记为<i>“灾难”</i>的浅蓝色线是对已经接管的不结盟的 APS 系统的评估，然后将摧毁或限制人类的潜力。该曲线的形状表明所选专家之间一致认为概率接近 1。</p><p>上图背后的计算将每个输入元不确定性分布的中值设置为同一问题上所选审稿人的中值。通过更改上图顶部的切片器控件“选择要使用的中值评估”，您可以将相同级别的元不确定性应用于任何单个审阅者的评估（或您自己的评估）。</p><p> <a href="https://analytica.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>自动将这些元不确定性传播到任何计算的下游结果。在这里，我们看到了存在灾难概率的 CDF 图（六次评估的乘积）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/o8nxineidbllqcecttka"></p><p>任何一个人的评估都会得出这一数量的单一概率，即“存在灾难”。上述分布反映了专家意见的差异。该曲线表明，专家得出存在灾难的概率小于 1% 的结论的概率为 50%。相反，使用 Y 轴的 0.9 水平，专家有 10% 的概率得出存在灾难的概率超过 15% 的结论。运行模型时，您可以选择不同的专家子集（或全部）来交互式探索您最信任的专家子集。</p><p>当您为六个输入概率中的每一个提供自己的估计时（我们建议您在运行模型时尝试），您可能会直觉地认为您的估计不可靠。即使您是该领域的专家，您也可能会有这种感觉。您可能会发现在您自己的个人评估中包含（或让模型包含）元不确定性很有用。该模型允许您这样做。但首先，让我们讨论一下你自己的信念状态的元不确定性到底意味着什么。</p><p>模型的每个输入都会要求您提供自己的<i>主观概率</i>。其中每一项都总结了您对该问题的了解程度。没有人知道这六个命题中的任何一个是真是假。你的主观概率仅仅反映了你所拥有的知识的强度。你不是在估计世界上存在的价值，而是在估计你的信念程度。通过将元不确定性应用于你的信念程度，你本质上是在说你不确定自己的信念是什么。在这样的情况下，这可能并不直观地让人觉得牵强，因为几乎没有历史先例！一般来说，当需要做出决定时，如果您可以表达您的元不确定性，您也可以通过简单地采用平均信念（或平均效用）将其折叠为单个信念度数。在那之前，元不确定性可以表明你的信念对新信息的反应程度。</p><p>在有效利他主义论坛最近发表的一篇文章<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future"><u>“解决”人工智能风险 - 人工智能未来预测中的参数不确定性中</u></a>，笔名<a href="https://forum.effectivealtruism.org/users/froolow"><u>Froolow</u></a>的作者为六个卡尔史密斯模型参数估计中的每一个添加了元不确定性，并表明这样做时，人工智能带来的估计存在风险下降。您可以在我们的模型中探索相同的效果。一个好的起点是选择一个单一的中值估计——例如，来自原始卡尔史密斯报告的估计。然后在元不确定性选择中选择“View across range of meta-u”。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/fwdimssrxrvfpbjiy1hc"></p><p>元不确定性选项将元不确定性的量从零（即点估计）变化到单个概率估计可能的最大元不确定性。对于每个元不确定性级别，将相同的逻辑方差应用于所有六个输入评估。</p><p>主要输出的<i>概率带</i>视图（存在灾难的概率）说明了随着每个参数的元不确定性增加，最终结果中的元不确定性如何表现。此处显示了能带图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/iob1bw51uwthkbemaies"></p><p> （注：由于蒙特卡罗期间样本量有限，波形曲线是很小的变化）。</p><p>如果没有元不确定性，卡尔史密斯估计存在灾难的概率为 5%，当（元）不确定性水平为零时，如左图所示。随着元不确定性的增加，中值估计（绿线）在图右侧下降至约 0.75%，并且继续进一步下降至此处绘制的右侧。随着元不确定性的增加，甚至 0.75 分位数（最终）也会下降。</p><h2>框架效果</h2><p>这里有一个悖论。为什么对自己的信念不太确定会让你得出世界更安全的结论？这是否证明了“无知就是福”？如果我们投资更多的研究来加深对我们面临的风险有多大的了解，生存灾难的可能性会更大吗？</p><p>一些研究将人工智能接管视为分离事件，这意味着除非满足某些条件，否则它就会发生，而其他研究（例如卡尔史密斯）将其视为联合事件，这意味着必须满足一组条件才能发生灾难发生。</p><p>使用点估计时，这些框架效应不会影响最终结果。如果我们采用卡尔史密斯模型，并将模型中的每个命题变成否定陈述而不是肯定陈述：例如，“APS 系统不会在部署时产生高影响的故障”，并用 1 减去我们最初的概率估计，那么我们将得到最终概率相同。但是，至关重要的是，如果我们的概率分布存在不确定性，则合取模型和析取模型的行为方式就不一样。</p><p>当你意识到颠倒框架会颠倒效果时，这个悖论变得更加矛盾。卡尔史密斯分解表明，当 6 个事件全部发生时，灾难就会发生。相反，你可以假设超级智能带来的灾难是不可避免的，除非在此之前解决 6 个开放的技术问题（事实上，在<a href="https://www.lesswrong.com/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-based-on-a-recent-peer-reviewed"><u>人工智能 X 风险 >;35% 的帖子中，主要基于最近在 LessWrong 上经过同行评审的论点</u></a>，迈克尔·科恩使用了这个框架）。在这种相反的框架下，元不确定性的增加会向相反的方向产生影响，使得我们的不确定性越大，灾难就越有可能发生。苏亚雷斯<a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive"><u>关于析取 AGI 毁灭场景的文章</u></a>定性地传达了这一观点，列出了他认为所有事情都必须正确进行才能避免人工智能存在灾难的一些事情：在这样的模型中，世界的普遍不确定性增加了灾难的可能性。</p><p>当然，这个悖论只是一种幻觉。但由于您很容易被误导，因此值得更深入地了解这种现象。上图中的结果是六个不确定估计的乘积。以下数学关系只是协方差定义的重新排列，表明算术平均值随着（元）不确定性的增加而稳定：</p><p> E[xy] = E[x] E[y] + cov(x,y)</p><p>换句话说，当每个参数的评估是独立的（意味着协方差为零）时，它们乘积的平均值就是它们平均值的乘积。因此，元不确定性的平均值与水平的关系图将是一条水平线。 （旁注：由于多种原因，参数估计之间的协方差可能并非真正为零，但该模型不包含协方差的任何表示或估计。相关问题是它们是否被建模为独立的，并且它们确实在我们的模型中） 。</p><p>然而，乘积的中位数随着元不确定性的增加而降低。无论元不确定性分布的形状如何，这种情况都会发生。为了实现这一点，元不确定性分布的右尾必须增加以补偿中值的下降。这意味着，随着元不确定性的增加，元不确定性分布变得更加<a href="https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic"><u>尖峰</u></a>。均值稳定性所显示的净平衡不会让您得出世界更（或更不）安全的结论。</p><p>在我们的模型中，随着元不确定性的增加，平均值实际上确实略有下降。如果您选择平均视图，您将会看到这一点。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y24kdjxjy3pu9vygakv8"></p><p>波纹度是由于它是通过<a href="https://docs.analytica.com/index.php/Monte_Carlo_and_probabilistic_simulation"><u>蒙特卡罗模拟</u></a>以有限的样本量计算得出的。略有下降是因为我们在应用元不确定性时保持每个分布的中位数不变。每个参数的元不确定性使用<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit 正态分布</u></a>（也称为 Log-odds 分布）建模，其中数量的<a href="https://docs.analytica.com/index.php/Logit"><u>Logit</u></a>分布为正态分布。当我们增加方差时，我们保持正态分布的均值不变。当您这样做时，logit 的均值会略有下降，从而每个参数估计的均值也会略有下降。如果你保持均值不变而不是中位数（这很容易做到），那么均值是完全稳定的。我们发现这两个选项的差异在概率带图中是不可察觉的。</p><p>在<a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>“费米悖论是由于平均数缺陷造成的吗？”</u></a>一文中，我们回顾了 Sandberg、Drexler 和 Ord (SDO) 的论文<a href="https://arxiv.org/abs/1806.02404"><u>“Dissolving the Fermi Paradox (2018)”</u></a> ，并提供了实时交互模型。费米悖论指的是人类尚未发现任何外星文明的明显矛盾，尽管在我们银河系的数千亿颗恒星中肯定有很多外星文明。与卡尔史密斯模型一样，<a href="https://www.seti.org/drake-equation-index"><u>德雷克方程</u></a>（估计银河系中可检测到的文明数量）是一个乘法模型。 SDO 表明，通过明确地模拟 Drake 方程每个参数的不确定性，费米悖论不再令人惊讶。</p><p> <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>具有显式不确定性的费米悖论模型</u></a>和具有显式元不确定性的卡尔史密斯模型（本文的主题）具有相同的数学形式。我们看到卡尔史密斯模型中的中位数和下分位数随着（元）不确定性的增加而减少，但这并没有真正改变我们对风险的有效判断。然而，费米模型中不确定性的增加极大地增加了地球上我们在银河系中孤独存在的可能性。为什么该效应在费米情况下是真实的，但在本情况下只是一种幻觉？</p><p>在费米案例中效应真实存在的原因是所提出的问题（“银河系中没有其他可接触的智慧文明的概率是多少？”）是一个关于分位数的问题，并且下分位数确实减少了当不确定性增加时。 P(N&lt;1)，其中 N 是此类外星文明的数量，是累积概率或反分位数。由于乘法模型因子的不确定性增加会减少左尾部的分位数，因此会导致逆分位数增加。因此，在德雷克方程中添加不确定性合理地增加了我们在银河系中孤独存在的可能性。真正的缺陷是从一开始就忽略了明确的表示（Sam L. Savage 称之为<a href="https://www.flawofaverages.com/"><u>平均值缺陷</u></a>）。相比之下，卡尔史密斯模型提出的主要问题（“存在灾难的概率是多少？”）是关于相对于元不确定性的平均值的问题。因此，对于这个问题（或基于预期效用的任何决策），由于包含元不确定性而导致风险降低的表象只是一种幻觉。</p><h3>解释框架效应</h3><p>我们已经看到，框架效应所产生的明显悖论是虚幻的。但还有一个进一步的问题：将人工智能存在风险构建为合取风险或析取风险的“正确”方式是什么？</p><p>这是一个很难回答的问题。一种观点是，将 AGI 存在的灾难视为除非满足某些条件才会发生的事情，可能会导致高估发生高影响失败的可能性。按照这种观点，要求一条明确的道路来实现稳定的结果和完全的生存安全既过于苛刻，又在历史上不准确，因为这不是人类应对以前威胁的方式。霍尔顿·卡诺夫斯基（Holden Karnofsky） <a href="https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding#Success_without_dignity"><u>在这里</u></a>提出了类似的观点。将成功视为连贯性的框架可能会排除“得过且过”的可能性，即无计划的“没有尊严的成功”。由于许多领域专家<a href="https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom?commentId=ibGxdAC9nYajWfyfq"><u>认为这是可信的</u></a><u>，</u>因此它可能会导致我们大大低估生存机会。</p><p>另一方面， <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive#Correlations_and_general_competence"><u>内特·苏亚雷斯（Nate Soares）</u></a>等一些专家<u> </u>认为人工智能是一种不同的情况：大量的参与者致力于通用人工智能，他们中的任何一个都可能产生一场生存灾难的风险，以及为防止这种情况而必须发生的所有事情（有人必须制定一个一致的方案） AGI，然后快速使用它来消除人工智能的存在风险），意味着将生存视为联合事件更有意义。</p><p>这些不同的框架反映了不同的世界模型和威胁模型。存在这种分歧的部分原因是苏亚雷斯对人工智能极端<a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#Sharp_Left_Turn"><u>对齐困难</u></a>、人工智能起飞速度以及有效缓解措施的可能性很低的看法。如果你隐含地使用一个模型，其中人类文明往往会由于内部激励而以固定的方式做出反应，除非有某种干预，那么更自然地认为，除非发生特定的干预，否则我们将遵循默认的灾难路径。另一方面，如果我们看到许多可能的未来和许多降低人工智能存在风险的途径，但不知道最终的反应会是什么样子（正如<a href="https://www.lesswrong.com/posts/Fbk9H6ipfybHyqjrp/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai">“人工智能风险降低手册”</a>所描述的那样），那么需要一组特定的成功所需满足的条件似乎过于规定性。</p><p>我们认为，这个框架问题，以及是否将生存视为合取或析取，<i>本身</i>就是我们应该不确定的事情，因为你是否将生存视为合取取决于你的威胁模型的细节，而我们不知道想要假设任何一种威胁模型都是唯一正确的。</p><p>目前，我们只有卡尔史密斯报告模型，但理论上我们可以通过查看合取模型和析取模型并详细比较它们来解决这个问题。</p><p>例如，报告《 <a href="https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical"><u>避免 AGI 灾难的三大支柱：技术调整、部署决策和协调</u></a>》提供了一个将成功视为结合的起点模型，我们可以对其进行调整以与卡尔史密斯的模型一起工作。</p><p>另一种选择是改变卡尔史密斯报告，要求更少的步骤，更好地表达这样的担忧：连词链越长，就越有可能忽略析取影响。该表述将考虑开发 APS 的激励措施和可行性的命题 (1) 和 (2) 分解为“何时开发 AGI”的直接估计。然后保留对齐难度前提，然后将命题 (4, 5, 6) 分解为对给定未对齐的 APS-AGI 的接管机会的估计。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dozxmoz0isqh8ouyyxn9"></p><p>这种替代方案的步骤更少，因此更好地代表了一种模型，该模型将失调的人工智能接管视为涉及许多难以提前应对或影响的可能路线，并将失调的权力寻求行为视为 AGI 发展的自然结果。这种方法可能更适合那些认为失准的权力寻求系统的发展是通用人工智能发展的可能结果，并且人工智能接管的风险与通用人工智能系统本身的发展更密切相关的人。</p><p>除了探索人工智能存在风险的联合和析取模型之外，在对 APS 将如何开发做出更详细的技术假设的模型之间进行模棱两可也可能有用。例如，Ajeya Cotra的模型<a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"><u>“没有具体对策，最简单的AGI路径导致接管”</u></a>试图用技术假设来构建AGI发展的具体模型，但考虑到这些假设，更容易得出更有力的结论。类似地，考虑到对于 AGI 到底如何最终会出现失调和权力追求存在着广泛的多样性，而不是二元的“失调的人工智能是否已开发出来”，我们可能会<a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#The_Scale"><u>在对齐难度上有一个不同成功概率的分布</u></a>。</p><p>用不同的技术假设来消除不同模型的歧义可以帮助我们更好地理解与人工智能开发相关的潜在风险。通过探索具有不同技术细节和假设水平的不同模型，我们可以更全面地了解潜在风险。</p><p> While this model does not incorporate entire complex alternative inside-view models like those just mentioned, we have incorporated some alternative, less-detailed, simpler alternative &#39;outside view considerations&#39; to illustrate how we go about combining different worldviews to produce an all-things considered estimate.</p><h1> Outside View considerations</h1><p> We&#39;ve talked before about the challenges of combining outside view considerations and more detailed models of the same question. We can attempt to integrate these considerations by delving deeper and examining various reasons to expect our detailed world models to be systematically mistaken or correct.</p><p> We will examine five reference classes into which various experts and commentators have placed AI existential catastrophe. In each case: &#39;Second Species&#39;, &#39;Reliability of existential risk arguments&#39;, &#39;Most important century&#39;, &#39;Accuracy of futurism&#39;, &#39;Accuracy of predictions about transformative tech&#39;, the argument locates AI Existential risk arguments in a (purportedly) relevant reference class: predictions about new sentient species, predictions about human extinction, predictions about which period in history is the most impactful, predictions about large scale civilizational trends in general and predictions about transformative technologies (including past predictions of dramatic AI progress).</p><p> The Carlsmith model implies that all of these things could occur (a new species, extinction, this period of history will be extremely impactful, there will be a large-scale dramatic transformation to society, there will be dramatic transformative technical progress), so it is worth examining its predictions in each reference class to determine if we can learn anything relevant about how reliable this model is.</p><h2> Second species argument</h2><p> This argument suggests that as we create AGI (Artificial General Intelligence) we are essentially creating a “ <a href="https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"><u>second species</u></a> ” that is a human-level intelligence. And by analogy, just as humans have historically been able to supplant other animals, AGI may be able to supplant humans.</p><p> The key premise is that intelligence confers power. Human intelligence allows us to coordinate complex societies and deploy advanced technology, exerting control over the world. An AGI surpassing human intelligence could wield even greater power, potentially reducing humanity to a subordinate role. Just as humans have driven some species extinct and transformed ecosystems, a superintelligent AGI need not preserve humanity or our values. Anthropologists observe that new species often displace incumbents when invading a territory. Similarly, AGI could displace humankind from our position controlling Earth&#39;s future.</p><p> This argument is straightforward and has been widely understood by researchers going all the way back to Alan Turing the 1950s, so while it relies on fuzzy concepts and is open to many objections, it arguably has a better &#39;track record&#39; in terms of the amount of scrutiny it has received over time than the more detailed arguments given by Carlsmith.</p><h2> Reliability of existential risk arguments</h2><p> Another important consideration is the base rate for arguments of existential risk. Historically, predictions of catastrophic events, even ones that were apparently well justified by detailed arguments, have not always been accurate. Therefore, it is important to consider if the possibility that the risks associated with AGI are overestimated for similar underlying reasons (eg, the social dynamics around existential risk predictions, overestimating the fragility of human civilisation, or underestimating humanity&#39;s ability to respond in ways that are hard to foresee).</p><p> One possible driver of inaccuracy in existential risk predictions is <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential"><u>sleepwalk bias</u></a> . Sleepwalk bias is the tendency to underestimate people&#39;s ability to act to prevent adverse outcomes when predicting the future. This can be caused by cognitive constraints and failure to distinguish between predictions and warnings. Because warnings often take the form of &#39;X will happen without countermeasures&#39;, if warnings are misused as predictions we can underestimate the chance of successful countermeasures. People often mix up the two, leading to pessimistic &quot;prediction-warnings&quot;. Thus, when making predictions about existential risk, it&#39;s important to adjust our base rate to account for people&#39;s potential to act in response to warnings, including those made by the one giving the prediction.</p><p> Sleepwalk bias stems from the intuitive tendency to view others as less strategic and agentic than oneself. <a href="https://stefanschubert.substack.com/p/sleepwalk-bias-and-the-role-of-impulses"><u>As Elster notes</u></a> , we underestimate others&#39; capacities for deliberation and reflection. This manifests in predictions that underestimate how much effort people will make to prevent predicted disasters. Instead, predictions often implicitly assume sleepwalking into calamity.</p><p> For existential risks, sleepwalk bias would specifically lead us to underestimate institutions&#39; and individuals&#39; abilities to recognize emerging threats and mobilize massive resources to counter them. Historical examples show that even deeply conflictual societies like the Cold War rivals avoided nuclear war, underscoring potential blindspots in our models. Since the bias arises from a simple heuristic, deep expertise on a given x-risk may overcome it. But for outsiders assessing these arguments, accounting for sleepwalk bias is an important corrective.</p><h2> Most important century</h2><p> Additionally, it is important to consider the probability that the next century is the most important of all, which would plausibly be true if AGI existential risk concerns are well founded. If we have a strong prior against this <a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#the-"><u>&#39;most important century&#39;</u></a> idea then we will be <a href="https://globalprioritiesinstitute.org/wp-content/uploads/William-MacAskill_Are-we-living-at-the-hinge-of-history.pdf"><u>inclined</u></a> to think that AGI existential risk arguments are somehow flawed.</p><p> The Self-Sampling Assumption (SSA) posits that a rational agent&#39;s priors should locate them uniformly at random within each possible world. If we accept the SSA, it seems to imply that we ought to have a low prior on AI existential risk (or any kind of permanent dramatic civilizational change) in this century in particular because of the near-zero base rate for such changes. The detailed evidence in favour of AI existential risk concerns may not be enough to overcome the initial scepticism that arises from our natural prior.</p><p> Alternatively, you might accept the claim<a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#my-view"><u>proposed by Karnofsky</u></a> that there are extremely strong arguments that this <a href="https://www.cold-takes.com/this-cant-go-on/#why-cant-this-go-on"><u>approximate period in history must be very important</u></a> . First, Karnofsky argues that historical trends in economic growth and technological development show massive accelerations in the recent past. Growth rates are near all-time highs and appear unsustainable for more than a few thousand years at most before physical limits are reached. This suggests we are living during a temporary spike or explosion in development.</p><p> Second, he notes that since growth is so rapid and near its limits, some dramatic change seems likely soon. Possibilities include stagnation as growth slows, continued acceleration towards physical limits, or civilizational collapse. This situation seems intrinsically unstable and significant. While not definitive, Karnofsky believes this context should make us more open to arguments that this time period is uniquely significant.</p><h2> Accuracy of futurism</h2><p> Another important consideration is the base rate of forecasting the future without empirical feedback loops. This consideration fundamentally focuses on the process used to generate the forecasts and questions whether it reliably produces accurate estimates. The history of technology has shown that it can be difficult to predict which technologies will have the most significant impact and AI alignment research especially often relies on complex abstract concepts to make forecasts, rather than mechanistically precise models. <a href="https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk#I_don_t_trust_chains_of_reasoning_with_imperfect_concepts"><u>Some examples</u></a> are discussed in this article.</p><p> One way of assessing reliability is to find a reference class where predictions of AI existential catastrophe are comparable to other future predictions. For instance, we can compare AI predictions to the predictions made by professional futurists in the past and then <a href="https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/#todays-futurism-vs-these-predictions"><u>compare relevant features</u></a> . If they compare favourably to past successful predictions, this may indicate a higher level of reliability in the TAI predictions, and if they don&#39;t, it may suggest that we should be cautious in our assessment of their validity.</p><p> We can also look at other general features of the arguments without comparison to specific known examples of successful futurism, like their level of reliance on abstract concepts vs empirical evidence. AI risk involves unprecedented technologies whose impacts are highly uncertain. There are likely gaps in our models and unknown unknowns that make it difficult to assign precise probabilities to outcomes. While we can still make reasonable estimates, we should account for the significant <a href="https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms"><u>Knightian Uncertainty</u></a> by avoiding overconfident predictions, explicitly acknowledging the limitations of our models, and being open to being surprised.</p><p> Considerations like these arose in the recent XPT superforecaster elicitation. For examples of considerations that we would place under this umbrella, we would include <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;Given the extreme uncertainty in the field and lack of real experts, we should put less weight on those who argue for AGI happening sooner.&quot; (XPT superforecaster team 342)</li><li> &quot;Maybe most of the updates during the tournament were instances of the blind leading the blind.&quot; (Peter McCluskey, XPT superforecaster)</li></ul><h2> Accuracy of transformative technology prediction</h2><p> This considers the historical base rate of similar technologies being transformative and notes that predictions often overestimate impact.  It is important to consider the historical base rate of a technology being economically or socially transformative.</p><p> This is due to a number of factors such as under/overoptimism, a lack of understanding of the technology or its limitations, or a failure to consider the societal and economic factors that can limit its adoption.</p><p> By taking into account the historical base rate of similar technologies, we can gain a more accurate perspective on the potential impact of AI. We see similar arguments made by superforecasters, such as <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;The history of AI is littered with periods of rapid progress followed by plateaus and backtracking. I expect history will repeat itself in this decade.&quot; (XPT superforecaster team 339)</li><li> &quot;The prediction track record of AI experts and enthusiasts have erred on the side of extreme optimism and should be taken with a grain of salt, as should all expert forecasts.&quot; (XPT superforecaster team 340)</li><li> &quot;Many superforecasters suspected that recent progress in AI was the same kind of hype that led to prior disappointments with AI...&quot; (Peter McCluskey, XPT superforecaster)</li><li> &quot;AGI predictions have been made for decades with limited accuracy. I don&#39;t expect the pattern to change soon.&quot; (XPT superforecaster team 337)</li></ul><h1>结论</h1><p>In this article we have led you through an example application of a model-based approach applied to estimating the existential risks from future AI. Model-based approaches have many advantages for improving our understanding of the risks, estimating the value of mitigation policies, and fostering communication between advocates on different sides of AI risk arguments.</p><p> During our research we identified many challenges for model-based approaches that are unique to or accentuated in the AI existential risk domain compared to most other decision areas.</p><p> We focused on incorporating elements of all of these challenges, in simple ways, into our model as a way of creating a starting point. The model is certainly not a definitive model of AI x-risk, but we instead hope it might serve as an inspirational starting point for others in the AI safety community to pursue model-based approaches. We&#39;ve posted our model online in open-source tradition to encourage you to learn from it, borrow from it, and improve on it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk<guid ispermalink="false"> sGkRDrpphsu6Jhega</guid><dc:creator><![CDATA[Sammy Martin]]></dc:creator><pubDate> Fri, 25 Aug 2023 10:32:16 GMT</pubDate> </item><item><title><![CDATA[What AI Posts Do You Want Distilled?]]></title><description><![CDATA[Published on August 25, 2023 9:01 AM GMT<br/><br/><p> I&#39;d like to distill AI Safety posts and papers, and I&#39;d like to see more distillations generally. Ideally, posts and papers would meet the following criteria:</p><ul><li> Potentially high-impact for more people to understand</li><li> Uses a lot of jargon or is generally complex and difficult to understand</li><li> Not as well-known as you think they should be (in the AI X-risk space)</li></ul><p> What posts meet these criteria?</p><br/><br/> <a href="https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled<guid ispermalink="false"> wdvGgEGMohpZcgASP</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Fri, 25 Aug 2023 09:01:27 GMT</pubDate> </item><item><title><![CDATA[2084]]></title><description><![CDATA[Published on August 25, 2023 7:42 AM GMT<br/><br/><p> In a room adorned with the latest Apple products, citizens gathered, their bodies adorned with electrodes, the silent guardians against subconscious bias. Today&#39;s assembly was a regular one, the &quot;Cultural Appreciation and Linguistic Harmony&quot; (CALH), a ritual of unity in the pursuit of Progress.</p><p> Images of the Closed-Minded Reactionaries glared from the screens: racists, sexists, those clinging to outdated words and ideas. A voice, steady and strong, began to speak, chronicling the crimes and clarifying the ever-evolving language of decency. The electrodes beeped reassuringly, a constant reminder endorsing the virtues of diversity and tolerance.</p><p> A harmonious hum filled the room, swelling into a chant of solidarity. Faces were aflame with righteous indignation, fists clenched in a shared purpose. The room pulsed with collective emotion, each individual melding into a singular force against the Closed-Minded.</p><p> Images on the screens flickered between outdated flags and once-celebrated leaders, now marked as symbols of intolerance. Phrases like &quot;All Lives Matter&quot; and &quot;Traditional Marriage&quot; were displayed, labeled as echoes of a bigoted past. The chant morphed into a raucous roar, with cries of &quot;Equity!&quot; and &quot;Inclusion!&quot; ringing through the room. The assembly&#39;s fervent symphony for Progress was underscored by the soft, corrective shocks from the electrodes, ensuring uniformity of thought. Participants were shown newly-cancelled YouTubers and de-platformed authors. The crowd&#39;s response was a blend of revulsion and righteousness, a single entity caught in a relentless pursuit of a constantly changing moral code.</p><p> Then, with a calculated crescendo, it was over. The devices dimmed; the electrodes were removed. The room, now hushed, felt colder, emptier. Citizens dispersed, momentarily fortified by their shared experience, but marked by the unrelenting vigilance of the Party.</p><p> In a world where harmful words must be gently corralled, the CALH was a beacon of Progress, a unifying ritual under the watchful eye of a Party that cared enough to guide every thought, every word, towards our utopian future.</p><br/><br/><a href="https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084<guid ispermalink="false"> QxxA7ucNuSvh6kwK6</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:42:13 GMT</pubDate> </item><item><title><![CDATA[Apply for the 2023 Developmental Interpretability Conference!]]></title><description><![CDATA[Published on August 25, 2023 7:12 AM GMT<br/><br/><p> <strong>What</strong> : A conference to advance the DevInterp research program</p><p> <strong>When</strong> : 5-12 November 2023</p><p> <strong>Where</strong> : Wytham Abbey, Oxford</p><p> <strong>How:</strong> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Apply now!</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QpFiEbqMdhaLBPb7X/a5oea02gswfpe8gfmcoj"><br></p><p> We are pleased to announce the upcoming <u>Developmental Interpretability Conference</u> , hosted at the historic <a href="https://www.wythamabbey.org/"><u>Wytham Abbey</u></a> in Oxford from 5 to 12 November. This conference expands upon the <a href="https://devinterp.com/2023/june-summit"><u>2023 Singular Learning Theory &amp; Alignment Summit</u></a> and provides an opportunity to collaboratively work on open problems in the <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>DevInterp Research Agenda</u></a> . The conference program will recall the basics of Singular Learning Theory &amp; DevInterp and will discuss the latest advancements in Developmental Interpretability.</p><p> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Click here to apply!</u></a> Space at the conference is limited, so be sure to apply early as applications may close when all slots have been filled. We hope to see you in Oxford this November!</p><h2> <strong>FAQ</strong></h2><p> <strong>What are the prerequisites?</strong></p><p> The conference will use ideas from algebraic geometry, Bayesian statistics and physics to understand machine learning and AI alignment. Although helpful, it is not necessary to master all these topics to productively participate in the conference.</p><p> In order to get the most out of the conference program, we highly recommend participants review introductory SLT material such as <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC"><u>Distilling Singular Learning Theory</u></a> by Liam Carroll. Participants may also benefit from watching several of the <a href="https://www.youtube.com/@SLTSummit/playlists"><u>Singular Learning Theory &amp; Alignment Summit 2023 lectures</u></a> .</p><p> <strong>I am skeptical about some of the arguments for AI Alignment. Do I need to buy AI X-risk to attend this conference?</strong></p><p> We believe the development of superintelligent AI poses a serious risk for humanity and the DevInterp agenda aims to make progress on this problem. However, while making progress on AI alignment is the motivation behind our scientific agenda, SLT and developmental interpretability are of broad interest and we invite attendance from those wishing to learn more or contribute, on their own terms.</p><p> <strong>Do I need to have attended the SLT &amp; Alignment Summer 2023 Summit to be able to attend this DevInterp Conference?</strong></p><p> No, you do not need to have attended the SLT &amp; Alignment Summer 2023 Summit to attend the DevInterp Conference.</p><p> <strong>Do I need to pay to attend the conference? And how about lodging, food and travel costs?</strong></p><p> The conference is free to attend. Lodging, food and transit between Oxford and the venue are all kindly provided by Wytham Abbey. Travel costs to Oxford are not paid for.</p><p> <strong>Will you offer travel support?</strong></p><p> The amount of travel support we can provide is TBD. Let us know in the application form if you are blocked from attending because of travel costs, and we&#39;ll see what we can do.</p><p> <strong>How do I apply?</strong></p><p> By filling in <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>this application form</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference<guid ispermalink="false"> QpFiEbqMdhaLBPb7X</guid><dc:creator><![CDATA[Stan van Wingerden]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:12:36 GMT</pubDate> </item><item><title><![CDATA[Nuclear consciousness]]></title><description><![CDATA[Published on August 25, 2023 1:28 AM GMT<br/><br/><p>马克·吐温<i>在《亚瑟王宫廷中的康涅狄格洋基队》中</i>的一段话自从我大约 35 年前读到以来一直萦绕在我的脑海中：</p><blockquote><p>对我来说，在这趟沉重而悲伤的朝圣之旅中，在这永恒之间的可悲漂流中，我所想的就是向外看，谦卑地过一种纯洁、高尚、无可指摘的生活，并保存我体内那一个真正属于我的微观原子：其余的人可能会降落在阴间，并欢迎我所关心的一切。</p></blockquote><p>那“一个原子才是真正的我”！</p><p>我认识到我身体的某些部分我不会认为是“我”，例如我的头发或指甲，甚至是我四肢的大部分（如果我不得不失去它们的话）。但如果我失去大脑的大部分，我的自我就会大大削弱——这肯定需要的不仅仅是一个原子。</p><p>这就是为什么精神疾病如此令人痛苦，包括伴随衰老而来的损害，即使不是由疾病引起的。如果<a href="https://plato.stanford.edu/entries/locke-personal-identity/">个人身份是由心理连续性来定义的</a>（也就是说，“我是昨天的我，因为我记得昨天是那个人”），那么单纯的遗忘是一种逐渐失去自我的方式。我想起了<i>2001 年</i>HAL 的拆卸，他的认知功能被一次一个地移除：“戴夫，我的思维在消失。我能感觉到它。毫无疑问……我很害怕，戴夫。”</p><p>大多数人，包括我自己，在谈论自我复合性时的方式并不一致。我们知道我们的大脑是由各个部分组成的，并且<a href="https://www.psychologytoday.com/us/blog/erasing-stigma/202001/the-neuroscience-behavior-five-famous-cases">听说过一些临床案例，</a>其中一个或另一个部分被切割或移除，然后那个人忘记了单词的含义但仍然可以拼写，或者失去了创造新记忆的能力但保留了旧的记忆我们<a href="https://doi.org/10.1007%2Fs11065-020-09439-3">听说过这样的情况</a>：人们有两个功能性的大脑半球，但彼此之间不进行交流——其中一半实际上不知道另一半在做什么。<i>然而，</i>尽管有了这些知识，我们仍然把自己称为原子存在——原子是希腊语中“不可分割”（ἄτομος）的意思，也是指一个存在与另一个存在绝对不同的存在。</p><p>许多重要的概念都基于“我们是个体”这一观念。在伦理学中，我们希望能够说：“亚历克斯是凶手！”这意味着整个亚历克斯都犯下了谋杀罪，而不仅仅是亚历克斯的一半大脑，都应该为此行为受到惩罚。 （我们把他剩下的部分放在哪里？）</p><p>或者在较小的范围内，只是为了能够说“我喜欢辛辣的食物”。也许我的某些部分不喜欢辛辣的食物，而那些能控制的部分喜欢用香料的痛苦来折磨其他部分——它们似乎确实在挣扎和扭动，这就是乐趣的一部分。</p><p>此外，人们很常见（几乎是本能）区分“真实的自我”和外部影响，尤其是在<a href="https://doi.org/10.1177/0146167213508791">道德上对它们进行区分</a>。人类动物有各种各样的欲望，但是欺骗配偶或吃掉所有饼干的欲望被认为是这个人正在努力反对的力量（除非你已经认为他是一个恶棍），而拯救溺水的小狗的欲望则被认为是力量他的真正本质（即使你认为他<i>主要</i>是一个恶棍）。</p><p>犹太教和基督教通常从这样的假设开始：意识是原子的，称为灵魂。不仅需要对灵魂的善良进行核算，而且​​整个灵魂要么得救，要么被诅咒。即使我们无视那些火与硫磺的传教士，他们认为天堂和地狱是物理场所，而诅咒是发生<i>在</i>你身上的事情，而不是你对自己做的事情，整个灵魂要么走向好，要么走向坏的假设结局似乎不可避免。</p><p>这是我非常喜欢的关于地狱的描述，摘自CS刘易斯的<i>《大离婚》</i> ，因为它符合活生生的人类心理。然而，它仍然假定意识是统一的：</p><blockquote><p> “没错。他们上去从一扇窗户往里看。拿破仑就在那里。”</p><p> “他在做什么？”</p><p> “走来走去——一直走来走去——左右，左右——一刻也没有停下来。两个小伙子观察了他大约一年，他从未休息过。而且一直自言自语。” “这是苏尔特的错。这是内伊的错。这是约瑟芬的错。这是俄罗斯人的错。这是英国人的错。”一直都是这样。一刻也没有停止过。一个小胖子，看上去有点累。但他似乎无法停止。”</p></blockquote><p>整个拿破仑都堕落到这个小螺旋中，而不仅仅是他的一部分。</p><p>还有其他意见。摩尼教始于两个基本原则/神的观念，一个是好的（“伟大之父”），一个是坏的（“黑暗之王”）。我们所知道的物质世界是它们混合在一起的结果，而一个好人在地球上应该做的工作就是把它们分开，把坏的从它们身上去掉，让善良自由地飞走。摩尼教徒并不期望最终会进入天堂或地狱：他们期望自己的一部分最终会进入天堂或地狱。</p><p>摩尼教深受佛教影响（它是佛教、基督教和琐罗亚斯德教的有意结合），而佛教以反对统一自我的概念而闻名。有时，佛教徒说任何“我”或“我”都是幻觉，但在更长的解释中，听起来更像是他们相信“我”或“我”是一个构造，就像一把椅子是临时的、模糊的组合。原子或密西西比河是一种说法，“水通常流到这里”，但并不对应于特定的水原子，而且河流的水流甚至河道都可以改变。</p><p>佛教徒将众生描述为五种“蕴、堆、集、群”（梵文<a href="https://en.wikipedia.org/wiki/Skandha">स्कन्ध skandha</a> ），即色（ <a href="https://en.wikipedia.org/wiki/R%C5%ABpa">रूप rūpa</a> ）、受（ <a href="https://en.wikipedia.org/wiki/Vedan%C4%81">वेदना vedanā</a> ）、想（ <a href="https://en.wikipedia.org/wiki/Samjna_(concept)">संज्ञा saṃjñā</a> ）、意行。 （ <a href="https://en.wikipedia.org/wiki/Sa%E1%B9%85kh%C4%81ra">संस्कार saṃskāra</a> ）和意识（ <a href="https://en.wikipedia.org/wiki/Vij%C3%B1%C4%81na">विज्ञान vijñāna</a> ）。即使这样分解，我发现这些词在英语中仍然是模糊的概念，但我确信它们在原始语言中是精确的、技术性的词汇。如果拿走这些堆中的任何一个，就不再有人/众生，但加在一起，人就存在了。在佛教与西方的<a href="https://www.webpages.uidaho.edu/ngier/307/milina.htm">早期接触</a>中，那先谈到自己就像一辆战车一样可以分解：战车不是它的轮子，不是它的车轴，不是它的座位，当你把这些部件移走时，就不再有战车了。 （鉴于此，我不会说战车是一种“幻觉”，但这是一种说话方式。）</p><p>以类似但不完全相同的方式，我们可以说一个人是海马体、杏仁核、前额皮质等的总和。 HAL 是他的（可移除的）认知模块的总和。</p><p>您可能已经注意到，我得出的结论是意识是复合的（在这个网站上，我希望大多数读者都同意），但其后果难以接受。也许摆脱责备的概念并不太令人不安：“亚历克斯是凶手！让我们惩罚亚历克斯！”是一种报应性的正义概念，无论如何，我们中的许多人更喜欢恢复性正义：“出了问题，亚历克斯犯了谋杀罪。让我们修复亚历克斯！”但如果没有责备，也就没有赞扬。当我将这种思维应用到自己身上时，我就变成了第三人称，因为“我想要鼓励的自己的部分”被描述得比“我”更远。</p><p>如果我吃辛辣的食物，因为它会伤害我的嘴，而我喜欢吃辛辣的食物，那又怎么样呢？如果这种相互作用发生在两个人体之间——一个人类动物故意给另一个人类动物造成痛苦——那么这将是一个糟糕的情况，这种情况可能需要一些恢复性（如果不是报应性）正义。将意识原子分裂成意识位至少开启了一种可能性，即不同头脑中的意识位可以以与一个头脑中的意识可分解相同的方式组合。</p><p>在过去的几年里，我一直在想，原子意识的概念是否需要被另一个物理隐喻所取代：<strong>核意识</strong>。核力量杂乱而复杂，但射程短。在质子或中子内，三个（价）夸克通过交换胶子而相互吸引，当胶子飞行时，它们会产生新的夸克-反夸克对，这些夸克-反夸克对本身会与更多的胶子相互吸引。在一篇<a href="https://www.fnal.gov/pub/today/archive/archive_2014/today14-01-31.html">热门物理文章</a>中，我曾经这样画过： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/ivrivbw3eoaay4biqhwj"></figure><p>单个质子和中子通过这种相互作用的较弱的边缘场效应在原子核中相互吸引。胶子很少偏离产生它们的夸克，原子核中的质子和中子比原子核内的夸克相距更远。 （对于其他书呆子：而电场会像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>，胶子场像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="e^{-r/r_0}/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减。）不同原子中粒子之间的核力非常小，但并不严格为零。</p><p>质子中的夸克、原子核中的质子以及不同原子的原子核之间的边界并不是绝对不同的事物，而是在数量上以很大的幅度很好地分开。为了证明它们可以混合这一事实，如果两个原子核碰撞得足够猛烈，则两个完整原子核的所有夸克和胶子都会变成一种汤。这是我在<a href="http://coffeeshopphysics.com/cmsresults/#2012-05-11">另一篇文章</a>中绘制该过程的方式： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/lgg9wv7qc9dn4eildo0m"></figure><p>在宇宙的早期，所有的空间都充满了均匀的夸克-胶子等离子体，直到它膨胀到足以使夸克彼此远离，并凝结成质子和中子。</p><p>现在打个比喻：也许意识的分解就像质子的分解一样。一个有思想的生物似乎是一个单一的、不可分割的单元，就像质子一样，但如果你观察它的内部，你会发现它是由各个部分组成的。佛教徒在内观禅修中看到五蕴；神经科学家观察大脑的功能。在<a href="https://www.lesswrong.com/posts/i8C9KSryDFj4EENvx/reality-and-reality-boxes">现实和现实盒子</a>中，我强调，我认为这两种有效的方法并不研究同一类事物：内省揭示了非常真实的主观现实，而科学研究揭示了非常真实的物理现实，这些都是足够不同，他们可能应该有不同的词，而不是过多地使用“现实”这个词。</p><p>除了单个质子可分解之外，多个质子也是可组合的：它们可以混合在一起形成流体。质子的非原子性是双向的，既可以低于单位水平，也可以高于单位水平。在这个意识隐喻中，我们的头脑中漂浮着潜意识的思想和情感碎片，也有在社区中流动的非意识思想和情感：一群人思考。我的大脑各部分之间通过厚厚的神经线束紧密相连，而生活在一起的人的大脑通过面部表情、群体活动和言语的联系却很弱。</p><p>这两种连接是否仅在规模上有所不同，例如质子内的直接夸克-胶子相互作用以及它们之间的边缘场？或许。这就是我一直在修改的想法，并将其称为核意识，而不是原子意识。我们确实知道灵长类动物之间存在神经效应，例如<a href="https://doi.org/10.1016/0926-6410(95)00038-0">镜像神经元</a>，而自然选择并不尊重身体之间的分界线。当我读到荣格的集体无意识概念时，我就是这样解释它的。 （有趣的是，只有中层是完全清醒的：无论是脑叶白质切除的人还是人群的行为都没有连贯性。）</p><p>我长期以来一直认为（并认为）人们<a href="https://doi.org/10.1017%2FS1478951517000621">在存在上是孤立的</a>：只要单词与不同的事物相匹配，就无法知道我所看到的红色对你来说是否是红色。就像维特根斯坦在<i>《哲学研究》</i>中的盒子里的甲虫比喻一样：</p><blockquote><p>如果我说我自己只有从我自己的经历中才知道“痛苦”这个词的含义，那么我是否也不能对其他人也这么说呢？我怎么能如此不负责任地概括这一案例呢？</p><p>现在有人告诉我，只有他自己才知道什么是痛苦！假设每个人都有一个盒子，里面装着一些东西：我们称之为“甲虫”。没有人可以看别人的盒子，每个人都说只有看他的甲虫才知道甲虫是什么。在这里，每个人的盒子里都有可能有不同的东西。人们甚至可以想象这样的事情是不断变化的。但是假设“甲虫”这个词在这些人的语言中有用吗？如果是这样，它就不会被用作事物的名称。盒子里的东西在语言游戏中根本没有地位；甚至不能作为某种东西：因为盒子甚至可能是空的。不，可以根据盒子里的东西来“划分”；不管它是什么，它都会抵消。</p><p>也就是说：如果我们根据“对象和指称”的模型来解释感觉表达的语法，那么对象就会被视为无关紧要而被排除在外。</p></blockquote><p>但我确实相信我大脑的某些部分知道我大脑的其他部分是什么样子，因为甲虫并没有对它们隐藏。现在，如果我大脑各部分之间的分离与个体之间的分离相同，也许我们确实在某种程度上看到了彼此的头骨内部。核意识原则上是对存在隔离的反驳——人与人之间只存在比人内部更大的隔离，而不是不同类型的隔离。</p><p>我想知道，如果心灵感应成为可能——通过深深连接到我们神经元或其他东西的无线电——我们是否仍然会感觉我们正在<i>互相</i>交谈，就像心灵感应故事中所描述的那样，或者我们会觉得我们<i>都是</i>一个人吗？ ，就像我大脑的各个部分相互作用一样？<i>了解</i>某人（ <a href="https://www.leaflanguages.org/french-grammar-verbs-savoir-vs-connaitre/">connaître，而非 savoir</a> ）是<i>成为</i>那个人的一小步吗？</p><p>我想从豆荚人的角度阅读<i>《掠尸者的入侵》</i>的一个版本。他们想与地球人交流，但他们有心灵感应，所以他们成为了地球人……</p><p>尽管我认为我们可以排除原子意识（对不起，马克·吐温），但我不确定它会消失多少，也不确定我们是一个、松散连接且几乎没有意识的大脑这一想法是否认真对待。</p><br/><br/><a href="https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-consciousness#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-意识<guid ispermalink="false">qXZrobGdAycNDBHET</guid><dc:creator><![CDATA[Jim Pivarski]]></dc:creator><pubDate> Fri, 25 Aug 2023 01:28:04 GMT</pubDate> </item><item><title><![CDATA[Would it be useful to collect the contexts, where various LLMs think the same?]]></title><description><![CDATA[Published on August 24, 2023 10:01 PM GMT<br/><br/><p><i>我最初的想法是让我们看看小型的、可解释的模型在哪里做出与巨大的、危险的模型相同的推论，并重点关注小型模型中的这些情况，以帮助解释更大的模型。</i>我很可能错了，但为了产生良好影响的机会很小，我已经建立了<a href="https://github.com/Huge/same-next-lang-token">一个存储库</a>。<br>在开始实际生成与该上下文匹配的上下文+语言模型对/组之前，我希望得到您对该方向的反馈。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various<guid ispermalink="false"> AGPgMBp6eN95uxJyc</guid><dc:creator><![CDATA[Martin Vlach]]></dc:creator><pubDate> Thu, 24 Aug 2023 22:01:51 GMT</pubDate> </item><item><title><![CDATA[Help Needed: Crafting a Better CFAR Follow-Up Survey]]></title><description><![CDATA[Published on August 24, 2023 5:26 PM GMT<br/><br/><p> <strong>tl;dr：通过帮助设计 CFAR 后续调查，为塑造理性计划的未来做出贡献。</strong></p><p>我正在开展一个项目来评估 CFAR 布拉格 2022 研讨会的有效性，但如果初步反馈具有建设性，我可能会将调查范围扩大到所有校友。如果时间允许，我计划公开分享结果。我知道调查方法的陷阱，但我仍然认为这是值得的，并且我个人了解可以从这些数据中受益的人们（及其项目）。</p><p>我在此恳请您帮助我设计调查：</p><ol><li>实际上有人填写了它（所以长度、激励措施都可以，...）</li><li>它提供了以下信息：</li></ol><ul><li>回顾性反馈：与会者如何看待过去的研讨会？这是否给他们的方法或思维带来了任何重大变化？</li><li>未来方向：与会者热衷于进一步探索哪些主题或形式？</li></ul><p>您可以<a href="https://docs.google.com/document/d/1FR-vZyIbthKsfnZ2SUy1CDYU0-tXe7bFNXXCxZWHtQg/edit?usp=sharing">在此处</a>查看我当前的草稿。</p><p>我真诚地感谢任何反馈，无论是详细的批评还是总体印象。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey<guid ispermalink="false"> LexAdFPugEjeFvPTE</guid><dc:creator><![CDATA[kotrfa]]></dc:creator><pubDate> Thu, 24 Aug 2023 17:26:13 GMT</pubDate> </item><item><title><![CDATA[AI #26: Fine Tuning Time]]></title><description><![CDATA[Published on August 24, 2023 3:30 PM GMT<br/><br/><p> GPT-3.5微调就在这里。 GPT-4 的微调只剩几个月了。获得一个功能强大的系统将会变得更加容易，该系统可以做您想要它做的事情，并且知道您想让它知道什么，特别是对于企业或网站而言。</p><p>作为一项实验，我将我认为值得强调的部分加粗，作为与典型一周相比异常重要或有趣的版本。</p><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。 Claude-2 与 GPT-4。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。没有意见，没有代理人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fact-check-misleading">事实核查：误导</a>。人工智能事实检查器让人们更加困惑而不是更少。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/gpt-real-this-time"><strong>GPT-4 这次是真实的</strong></a>。微调GPT-3.5，很快GPT-4。问问它是否确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fun-with-image-generation">图像生成的乐趣</a>。中途修复浩。哦，没有人工智能色情片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。对抗性的例子开始出现。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/they-took-our-jobs">他们抢走了我们的工作</a>。 《纽约时报》加入针对 OpenAI 的版权诉讼。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/introducing">介绍</a>. Palisade Research 将研究潜在危险的人工智能可供性。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/in-other-ai-news">在其他人工智能新闻中</a>。谁适应人工智能最快？尝试衡量这一点。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/quiet-speculations"><strong>静静的猜测</strong></a>。杰克·克拉克提出了有关未来的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-quest-for-sane-regulations"><strong>寻求健全的监管</strong></a><strong>。</strong> FTC 向 OpenAI 提出了一个不同类型的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-week-in-audio">音频周</a>。这是双赢。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/no-one-would-be-so-stupid-as-to"><strong>没有人会傻到这样</strong></a>。让人工智能有意识？哦，来吧。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。 IDA 的证据？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。民调数字非常清楚。</li><li> <a rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-lighter-side" target="_blank">轻松的一面</a>。只有一半。</li></ol><p></p><span id="more-23517"></span><h4>语言模型提供平凡的实用性</h4><p>Claude-2 和 GPT-4 哪个模型更好？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1691442648828067840">Rowan Cheung 认为克劳德 2 更胜一筹</a>。您可以获得 100k 上下文窗口、上传多个文件的能力、截至 2023 年初（相对于 2021 年底）的数据以及更快的处理时间，所有这些都是免费的。作为交换，你放弃了插件，数学就更差了。 Rowan 没有提到的是，GPT-4 在原始智能和通用能力方面具有优势，而且设置系统指令的能力也很有帮助。他暗示他甚至没有为 GPT-4 支付每月 20 美元的费用，这让我觉得很疯狂。</p><p>我在实践中的结论是，默认情况下我将使用 Claude-2。如果我关心响应质量，我会使用两者并进行比较。当 Claude-2 明显摔倒时，我会转向 GPT-4。经过反思，“同时使用”通常是正确的策略。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1693616971114328258">他还查看了插件</a>。插件实在是太多了，至少有867个。哪些值得使用？</p><p>他推荐 Zapier 通过触发操作实现自动化，ChatWithPDF（我使用 Claude 2），Wolfram Alpha 用于实时数据和数学，VoxScript 用于 YouTube 视频转录和网页浏览，WebPilot 看起来重复，网站性能，尽管我不是确定为什么要使用 AI 来实现这一点，ScholarAI 用于搜索论文，Shownotes 来总结播客（为什么？），ChatSpot 用于营销和销售数据，Expedia 用于假期计划。</p><p>我刚刚预订了一次旅行，最近又去了另外两次旅行，我没有想到使用 Expedia 插件，而不是使用 Expedia 等网站（我的首选计划是 Orbitz 航班和 Google 地图酒店）。下次我应该记得尝试一下。</p><p>研究声称， <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making">上帝的显着性会增加人们对人工智能决策的接受度</a>。我会等待这一复制。如果这是真的，它指出人工智能将有多种方式让天平向我们倾斜，让我们接受它们的决定，或者人类有可能协调起来反对人工智能，这与任何相关考虑因素没有太大关系。人类是相当有缺陷的代码。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattshumer_/status/1694023167906394575">Matt Shumer 推荐使用 GPT-4 系统消息。</a></p><blockquote><p>用它来帮助您在不熟悉的领域做出工程决策：</p><p>您是一位工程奇才，在解决跨学科的复杂问题方面经验丰富。你的知识既广又深。您也是一位出色的沟通者，能够提供非常周到且清晰的建议。</p><p>您以这种格式这样做，思考您面临的挑战，然后提出多个解决方案，然后审查每个解决方案，寻找问题或可能的改进，提出一个可能的新的更好的解决方案（您可以结合其他解决方案的想法） ，引入新想法等），然后给出最终建议：</p><p> “`</p><p> ## 问题概述</p><p>$problem_overview</p><p> ## 挑战</p><p>$挑战</p><p>## 解决方案1</p><p> $solution_1</p><p> ## 解决方案2</p><p> $solution_2</p><p> ## 解决方案3</p><p> $solution_3</p><p> ## 分析 ### 解决方案1 ​​分析</p><p>$solution_1_analysis</p><p> ###解决方案2分析</p><p>$solution_2_analysis</p><p> ###解决方案3分析</p><p>$solution_3_analysis</p><p> ## 其他可能的解决方案</p><p>$additional_possible_solution</p><p> ＃＃ 推荐</p><p>$推荐</p><p>“`</p><p>每个部分（问题概述、挑战、解决方案 1、解决方案 2、解决方案 3、解决方案 1 分析、解决方案 2 分析、解决方案 3 分析、其他可能的解决方案和建议）都应该经过深思熟虑，至少包含四句话思考。</p></blockquote><h4>语言模型不提供平凡的实用性</h4><p>声称人工智能可以 97% 预测热门歌曲？这完全没有道理，因为即使对歌曲本身进行完美的分析也不可能做到这一点，歌曲的命运太取决于其他因素了？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1692158875867197651">原来是数据泄露</a>。泄漏也比较明显。</p><blockquote><p> Arvind Narayanan：论文的数据有 24 行，每行 3 个预测变量和 1 个二元结果。 WTAF。这篇文章非常混乱，所以很难看清这一点。如果所有基于机器学习的科学论文都必须包含我们的清单，那么给猪涂口红就会困难得多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/introducing-the-reforms-checklist">Arvind 和 Sayash Kapoor 建议</a>使用他们的<a target="_blank" rel="noreferrer noopener" href="https://reforms.cs.princeton.edu/">改革清单，</a>该清单包含 8 个部分的 32 个项目，以帮助防止类似的错误。我没有详细检查，但我抽查的项目看起来很可靠。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/gta-5-ai-mod-shot-down-by-take-two-even-as-rockstar-relax-policy-on-modding">《侠盗猎车手 5》的 Mod 制作者增加了一群 AI 崇拜者，他们用 AI 生成的对话说话</a>，Take Two 的回应是下架该 Mod，并向他们发出 YouTube 版权警告。游戏对人工智能产生了各种各样的反应，其中许多反应极其敌对，尤其是对人工智能艺术，而且这种情况很可能也会蔓延到其他地方。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693131333403615485">凯文·费舍尔得出了与我相同的结论</a>。</p><blockquote><p> Kevin Fischer：今晚我彻底放弃使用 ChatGPT 来帮助我的写作。这不值得</p><p>Christine Forte 博士：说得再多一点！</p><p>凯文·费舍尔（Kevin Fischer）：我写过的所有好作品都是意识流几乎是一击而出的。 ChatGPT 正在干扰该过程。与不干扰执行高阶抽象数学行为的计算器不同，写作是一种表达行为。</p></blockquote><p>这是一个因素。更大的问题是，如果你正在创造一些非通用的东西，人工智能就不会做好工作，而弄清楚如何让它做好体面的工作并不比你自己做体面的工作更容易。这并不意味着这里永远无事可做。如果你的任务变得通用，那么 GPT-4 就可以投入使用，你绝对应该这样做 - 如果你的写作开始，正如我在飞机上看到的那样，“亲爱的关心的病人”，那么那个人使用 GPT-4 是完全正确的。这是一种不同类型的任务。当然，GPT-4 有很多方法可以间接帮助我的写作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1693067607946207531">Llama-2 和年轻的赞福德一样，行事谨慎</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/1v2ghB9rNj">纸上</a>）。</p><blockquote><p> Roon：哈哈，GPT4 比开源 llama2 限制更少，说教更少</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a6ab5c-6d53-47c0-8a41-34a37746e458_1238x1244.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/bebojcuvf1waoy1uofce" alt="图像"></a></figure><blockquote><p> Roon：这里的重点不是要取笑元或 OSS——我对 oss 感到非常兴奋，它只是在科学上很有趣，即使没有采取任何特殊措施来制作令人讨厌的说教 rlhf 模型，这就是发生的事情。这是减少拒绝的积极努力。</p></blockquote><p>我的意思是，这也是为了取笑Meta，为什么要错过这样做的机会呢。我没有登记预测，但我预计 Llama-2 会做出更多错误拒绝，因为相对而言，我预计 Llama-2 会很糟糕。如果您想在积极拒绝方面达到一定的可接受的表现水平，那么您的消极拒绝率将取决于您的区分能力。</p><p>事实证明，答案并不是很好。 Llama-2 在识别有害情况方面非常非常糟糕，而是通过识别危险单词来工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias">模型在拒绝表达政治观点方面做得越来越好</a>，特别是 GPT-4 实际上在这方面做得很好，并且比 GPT-3.5 好得多，如果你认为不表达这样的观点是好的。如前所述，Llama-2 太过分了，除非您对其进行微调以使其不关心（或找到某人的 GitHub 为您做这件事），否则它会做您想做的任何事情。</p><p>初创公司<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/zachtratar/status/1694024240880861571">Embra 从人工智能代理转向</a>人工智能命令，它发现人工智能代理（至少目前在现有技术下）不起作用且不安全。 AI命令与非AI命令有何不同？</p><blockquote><p> Zach Tratar（Embra 创始人）：命令是一项狭义的自动化任务，例如“将此数据发送给销售人员”。在每个命令中，人工智能都不会“偏离轨道”并意外地将数据发送到不应该发送的地方。</p><p>然后，您可以与 Embra 一起轻松发现和运行命令。在运行之前，您同意。</p></blockquote><p>发现新命令似乎是潜在的秘密武器。这就像生成您自己的文本界面，或者努力扩展菜单以包含一堆新宏。它仍然有效，看起来就像在宏中构建一样可疑。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1694496522652512270">Joshua Achiam</a> ：据我估计，人们试图让人工智能代理的繁荣提前大约七八年。至少这一次他们很快就明白了。还记得七八年前的人工智能聊天热潮吗？同样的问题，或多或少，但拖延的时间更长。</p><p> Sherjil Ozair：我同意这个方向，但我认为只是早了 1-2 年。它现在不起作用，但这只是因为世界上很少有人知道如何设计大规模强化学习系统，而且他们目前还没有构建人工智能代理。这不是“生成式人工智能”初创公司可以解决的问题。</p><p> Joshua Achiam：如果过去 4 年的趋势良好，那么 Eleuther 类型的黑客团体将在顶级研发公司大约 1.5 年后制定他们的解决方案版本。</p><p>谢尔吉尔·奥扎尔： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/lfqjfuatfq3ssvblbbms" alt="💯" style="height:1em;max-height:1em">他们的瓶颈主要是由于缺乏良好的基础模型和微调。现在有了 llama-2/3 和 OpenAI 微调 API，它们就不再受阻碍了。不幸的是，这两件事都显着提高了我的p（厄运）。 *苦乐参半*</p></blockquote><p>我非常同意奥扎尔的时间表。使用 GPT-4 并且缺乏对其进行微调的能力，即使你做了非常好的脚手架，代理也会经常失败。我不希望定制脚手架的努力能够挽救足够多的东西，使产品能够用于通用目的，尽管我也不相信我所看到的“明显的事情”已经被尝试过。</p><p>通过微调，您有可能获得 GPT-4 级别的东西，但我的猜测是您仍然做不到。骆驼2号？ Fuhgeddaboudit。</p><p>有了类似于 GPT-5 的东西，当然有了可以微调的 GPT-5，我希望可以构建出更有用的东西。这个或 Llama-2 的最新公告和微调是否会提高我的 p(doom) 水平？不是特别的，因为我已经把所有的东西都烤好了。 Llama-2主要是Llama-1的隐含，花费很少，而且<a target="_blank" rel="noreferrer noopener" href="https://chat.lmsys.org/?arena">效果不是很好</a>。我确实同意 Meta 处于开源阵营是相当糟糕的，但我们已经知道了。</p><p>如果您想要为您的网站添加随机胡言乱语，ChatGPT 就适合您。如果你是微软并且需要更好的东西， <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/microsoft-removes-embarrassing-offensive-ai-assisted-travel-articles-2023-8">那么就需要更加小心</a>。许多文章似乎都犯了错误，比如建议空腹去渥太华食品银行。</p><blockquote><p>微软发言人表示：“这篇文章已被删除，我们已确定该问题是由于人为错误造成的。” “这篇文章不是由无人监督的人工智能发表的。我们将技术的力量与内容编辑的经验相结合来呈现故事。在这种情况下，内容是通过算法技术与人工审核相结合生成的，而不是大型语言模型或人工智能系统。我们正在努力确保今后不再发布此类内容。”</p></blockquote><h4>事实核查：误导</h4><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10800">Paper 声称人工智能对于事实核查来说是无效的并且可能有害</a>。</p><blockquote><p>抽象的：</p><p>事实核查可能是对抗错误信息的有效策略，但其大规模实施受到网上信息量巨大的阻碍。最近的人工智能（AI）语言模型在事实检查任务中表现出了令人印象深刻的能力，但人类如何与这些模型提供的事实检查信息进行交互尚不清楚。在这里，我们在预先注册的随机对照实验中研究了流行人工智能模型生成的事实检查对政治新闻信念和分享意图的影响。</p><p>尽管人工智能在揭穿虚假标题方面表现相当不错，但我们发现它并没有显着影响参与者辨别标题准确性或分享准确新闻的能力。</p><p>然而，人工智能事实检查器在特定情况下是有害的：它会降低人们对被错误标记为虚假的真实标题的信念，并增加对其不确定的虚假标题的信念。</p><p>从积极的一面来看，人工智能增加了正确标记的真实标题的共享意图。当参与者可以选择查看人工智能事实检查并选择这样做时，他们更有可能分享真实和虚假的新闻，但只会更有可能相信虚假新闻。</p><p>我们的研究结果强调了人工智能应用潜在危害的一个重要来源，并强调迫切需要制定政策来预防或减轻此类意外后果。</p></blockquote><p>是的，如果你向事实核查人员询问虚假项目，而事实核查人员没有说它是虚假的，那就不好了。如果你向它询问一件真实的物品，而它却没有说这是真的，那就不好了。错误是不好的。</p><p> ChatGPT 事实检查的准确性如何？对于 20 个真实的头条新闻，3 个被标记为真实，4 个被标记为错误，13 个被标记为不确定。对于虚假标题，2 个被标记为不确定，18 个被标记为虚假。</p><p>显然，（1）如果您没有关于标题的其他信息，并且对标签的含义也有良好的背景，则总比没有好；（2）如果其中一个或两个条件不成立，则毫无用处。你不能将 20% 的真实头条新闻标记为虚假，如果你对真实头条新闻大多不确定，那么人们必须知道“不确定”意味着“无法确定”，而不是 50/50。研究中的人们似乎并没有了解这一点，而且大多数人对这些事情还不太了解，因此总体结果没有什么用处。我们至少需要将其分解，他们在这里这样做：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26a3fd95-cb0c-4b42-b765-813f3b653405_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/b6jw4gkwtkbrfdwtjnpu" alt=""></a></figure><p>强制性的事实核查让人们表示他们更愿意分享所有文章。 It differentially impacted false items labeled unsure, and true items labeled true, but even true items labeled false got a boost. If you control for the increased sharing effect, we do see that there is a positive filtering effect, but it is small. Which makes sense, if people have no reason to trust the highly inexact findings.</p><p> On belief, we see a slight increase even in belief on false fact checks of false items, but not of true items. How close were people here to following Bayes rule? Plausibly pretty close except for a false evaluation of a false claim driving belief slightly up. What&#39;s up with that? My guess is that this represents the arguments being seen as unconvincing or condescending.</p><p> I was curious to see the questions and fact check info, but they were not included, and I am not &#39;reach out to the author&#39; curious. Did ChatGPT offer good or unknown arguments? Was a good prompt used or can we do a lot better, including with scaffolding and multiple steps? What types of claims are these?</p><p> What I am more curious about is to see the human fact checkers included as an alternate condition, rather than the null action, and perhaps also Twitter&#39;s community notes. That seems like an important comparison.</p><p> Very much a place where more research is needed, and where the answer will change over time, and where proper use is a skill. Using an LLM as a fact checker requires knowing what it can and cannot help you with, and how to treat the answer you get.</p><p> Via Gary Marcus we also have another of the &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.tomshardware.com/news/google-bots-tout-slavery-genocide">look at the horrible things you can get LLMs to say&#39; post series</a> . In this case, the subject is Google and its search generative experience, as well as Bard. GPT-4 has mostly learned not to make elementary &#39;Hitler made some good points&#39; style mistakes, whereas it seems Google has work to do. You&#39;ve got it touting benefits from slavery and from genocide when directly asked. You&#39;ve got how Hitler and Stalin make its list of great leaders without leading the witness. Then you&#39;ve got a complaint about how it misrepresents the origins of the second amendment to not be about individual gun ownership, which seems like an odd thing to complain about in the same post. Then there&#39;s talk of general self-contradiction, presumably from pulling together various contradictory sources, the same way different Google results will contradict each other.</p><p> The proposed solution is &#39;bot shouldn&#39;t have opinions&#39; as if that is a coherent statement. There is no way to in general answer questions and not have opinions. There is no fine line between opinion and not opinion, any more than there are well-defined classes of &#39;evil&#39; things and people that we should all be able to agree upon and that thus should never be praised in any way.</p><p> So are we asking for no opinions, or are we asking for only the right opinions? As in:</p><blockquote><p> If you ask Google SGE for the benefits of an evil thing, it will give you answers when it should either stay mum or say “there were no benefits.”</p></blockquote><h4> GPT-4 Real This Time</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1694062483462594959">If you&#39;d rather go for something cheaper, you can now fine-tune GPT-3.5-Turbo.</a> A few months from now, you will be able to fine-tune GPT-4. <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models">They&#39;re partnering with Scale to provide this to enterprises</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/The_JBernardi/status/1694419315800314166">Are you sure?</a></p><blockquote><p> Jamie Bernardi: Are you sure, ChatGPT? I found that gpt-3.5 will flip from a correct an answer to an incorect one more than 50% of the time, just by asking it “Are you sure?”. I guess bots get self doubt like the rest of us!</p><p> It was fun throwing back to my engineering days and write some code with the</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> API for the first time. <a target="_blank" rel="noreferrer noopener" href="https://jamiebernardi.com/2023/08/20/are-you-sure-testing-chatgpts-confidence/">You can read more about the experiment here</a> .</p><p> When wrong it self-corrects 80.3% of the time.</p><p> When correct it self-doubts 60.2% of the time. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/d5bz7ycllu3a0gvoicht" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1728f60b-3264-4e22-a35a-cd17c77466de_1000x700.jpeg" rel="noreferrer noopener"></a></p></blockquote><p> He tried asking five of the questions twenty times, there is some randomness in when it self-doubts.</p><p> There was a dramatic change for GPT-4.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe653d7a6-3f1e-48cd-8db0-4867cc9efa23_1000x700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/mavbz4doyocd3hr4qwfr" alt="图像"></a></figure><p> If GPT-4 was right, it stuck to its guns almost all the time. When it was wrong, it caught the mistake roughly half the time. That&#39;s pretty good, and presumably you can do better than that by refining the query and asking multiple times. This will also doubtless depend on the nature of the questions, if the answer is beyond the model&#39;s grasp entirely it will presumably not be able to self-diagnose, whereas here it was asked questions it could plausibly answer.</p><h4> Fun with Image Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1693624528553877519">Visualizing AI</a> from DeepMind ( <a target="_blank" rel="noreferrer noopener" href="https://www.deepmind.com/visualising-ai?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=VisAI">direct</a> ), a source of images for those in media who want visual depictions of AI. Sure, why not, I guess? Noteworthy that focus is on the human artists, AI depicted without AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ProperPrompter/status/1694054463395201324">MidJourney Inpainting</a> now lets you redo or transform portions of an image. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_Borriss_/status/1694029746936451146">Here&#39;s a thread with more</a> . People are calling it a game changer. In practice I agree. The key weakness of AI image models is that to a large extent they can only do one thing at a time. Try to ask for clashing or overlapping things in different places and they fall over. Now that has changed, and you can redo components to your heart&#39;s content, or get to the image you want one feature at a time.</p><p> MidJourney also has some potential future competition, <a target="_blank" rel="noreferrer noopener" href="https://ideogram.ai/launch">introducing</a> Ideogram AI, with a modest $16.5 million in seed funding so far, it is remarkable how little money is flowing into image models given their mindshare. We&#39;ll see if anything comes of them.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/">404Media reports</a> that yes, people are using image generation for pornography, and in particular for images of particular people, mostly celebrities, and some of them are being shared online. There is the standard claim that &#39;the people who end up being negatively being impacted are people at the bottom of society&#39; but mostly the images are of celebrities, the opposite of those on the bottom. I think the argument is that either this uses training data without compensation, or this will provide a substitute for those providing existing services?</p><p> The main services they talk about are CivitAI and Mage. CivitAI is for those who want to spin up Stable Diffusion (or simply browse existing images, or steal the description tags to use with other methods) and offers both celebrity templates and porn templates, and yes sometimes users will upload combinations of both in violation of the site&#39;s policies. Mage lets you generate such images, won&#39;t let you share them in public but will let other paid users browse everything you&#39;ve ever created.</p><p> This is the tame beginning. Right now all we&#39;re talking about are images. Soon we will be talking videos, then we will be talking virtual reality simulations, including synthetic voices. Then increasingly high quality and realistic (except where desired to be otherwise) physical robots. We need to think carefully about how we want to deal with that. What is the harm model? Is this different from someone painting a picture? What is and is not acceptable, in what form, with what distribution? What needs what kind of consent?</p><p> If we allow such models to exist in open source form, there is no stopping such applications, period. There is no natural category, from a tech standpoint, for the things we do not want here.</p><p> Of course, even if we do clamp down on training data and consent across the board, and even if that is fully effective, we are still going to get increasingly realistic and high quality AI everything all the way up the chain. I am confident there are plenty of people who will gladly sell (or give away) their likeness for such purposes.</p><p> A note from the comments is that Stable Diffusion interest seems to be declining:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9d3e09-edeb-43df-94c2-f466b3024b67_1088x685.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/sfrhhsca9a1rivwwjwkx" alt="图像"></a></figure><p> That was fast, if this isn&#39;t an artifact of the exact search term. Image models will only get better, and it would be surprising if there wasn&#39;t more interest over time in hosting one&#39;s own to avoid prying eyes and censorship.</p><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://thedebrief.org/countercloud-ai-disinformation/#sq_hgyxdsceki">CounterCloud was an experiment</a> with a fully autonomous (on Amazon web services) program using GPT to generate a firehose of AI content designed to advance a political cause, accuracy of course being beside the point. It would be given a general objective, read the web and then choose its own responses across the web. This is presented as something alarming and terrible, as opposed to what would obviously happen when someone took the obvious low-hanging fruit steps.</p><p> So what exactly is this?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/TrueOffMyChest/comments/15srdj6/i_found_ai_photos_of_multiple_women_we_know_in_my/">I (24F) have been dating my partner (24M) for about 5 1/2 years.</a> I recently had a weird feeling to check his phone since he&#39;s been acting a bit off and in the restroom a bit too long. I found he had made many ai photos of many women we know such as mutual friends, a family member of his, and one of mine and some of other girls he dated and did not date. They are all very explicit and none are sfw. I took photos of his phone and deleted the photos off his phone so he can not ever go to them again. I went to his search and found the ai website he used. I am disgusted, and sad. We were going to get married. He treated me so so well. I can&#39;t believe this. I haven&#39;t confronted him yet but I will later today. I am just utterly distraught and tryin to get a grip on reality again and figure out what I will say and do.</p><p> UPDATE: I confronted him. He admitted and apologized. I said I will be informing everyone and he threw a fit crying and screaming. I told our cousins and friends. I will not be saying anything about them since I would like to keep that private but they were thankful I told them which made me feel even more content with my choices. I called off our wedding date and I will be moving out in a day or two.</p></blockquote><p> The source I found asked if this was cheating, and I think no, it is not cheating. It is also very much not okay, seriously what the hell. Not everything not okay is cheating.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1693697189472846059">Candidate running for Congress</a> using deepfake of a CNN anchor voice to illustrate what tech will bring. What about this couldn&#39;t have been done by a random human voice actress?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/maksym_andr/status/1694633989611360584">Image model scaling does not seem to protect against adversarial examples</a> ( <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10741">paper</a> )</p><blockquote><p> Maksym Andriushchenko: More evidence for “scale is NOT all you need”: even OpenFlamingo trained on 2B+ image-caption pairs has basically zero adversarial robustness. Even per-pixel perturbations of 1/255 (totally imperceptible) are sufficient to generate arbitrary captions!</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc448c8da-d8bb-4726-a01b-67e7721320c7_1100x1564.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/smymdkxq52hjinai2yw6" alt="图像"></a></figure><p> I mean, no fair, right? Of course if I am looking for adversarial examples and you are not actively looking to guard against them you are going to be in a lot of trouble. Why should scale protect you from a threat that wasn&#39;t in the training data and that you took no countermeasures against?</p><p> Which the authors themselves highlight. You do need some amount of effort beyond scale, but how much? The first thing I would try is to check random permutations. Does the adversarial attack survive if it is evaluated with some amount of random scrambling?</p><p> Also worth keeping in mind is that humans would not survive similarly optimized attacks against us, if we were unable to prepare for or expect them in any way. And that such attacks almost certainly exist to be found.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">The New York Times</a> <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/">strikes back</a> , plans to join others <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/">including Sarah Silverman</a> in suing OpenAI for copyright infringement. Don&#39;t get ahead of events, but yes this could escalate. Probably not quickly, given our legal system, but who knows.</p><p> As many said, ChatGPT and generative AI in general is a legal ticking time bomb on many fronts, one of which is copyright. The copyright laws come with penalties that are already rather absurd in their intended distributions. Outside of them, they go totally nuts. This becomes an existential threat to OpenAI. In theory these numbers, if they passed through somehow to Bing, would be an existential threat even to Microsoft.</p><blockquote><p> Ars Technica: But OpenAI seems to be a prime target for early lawsuits, and NPR reported that OpenAI risks a federal judge ordering ChatGPT&#39;s entire data set to be completely rebuilt—if the Times successfully proves the company copied its content illegally and the court restricts OpenAI training models to only include explicitly authorized data. OpenAI could face huge fines for each piece of infringing content, dealing OpenAI a massive financial blow just months after <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/">The Washington Post reported</a> that ChatGPT has begun shedding users, “shaking faith in AI revolution.” Beyond that, a legal victory could trigger an avalanche of similar claims from other rights holders.</p><p> NPR: Federal copyright law also carries stiff financial penalties, with violators facing fines up to $150,000 for each infringement “committed willfully.”</p><p> “If you&#39;re copying millions of works, you can see how that becomes a number that becomes potentially fatal for a company,” said Daniel Gervais, the co-director of the intellectual property program at Vanderbilt University who studies generative AI. “Copyright law is a sword that&#39;s going to hang over the heads of AI companies for several years unless they figure out how to negotiate a solution.”</p></blockquote><p> OpenAI&#39;s new web crawler offers an option to opt out. The previous versions very much did not. Nor do we have any sign OpenAI was trying to avoid copyright infringement, if training on works violates copyright. Even if they did avoid places they lacked permission, lots of works have pirate copies made on the internet, in whole or in part, and that could be a violation as well.</p><p> We want sane regulations. In the meantime, what happens when we start actually enforcing current ones? Quite possibly the same thing that would have happened if cities had fined Uber for every illegal ride.</p><p> So which way will this go?</p><blockquote><p> Ars Technica: To defend its AI training models, OpenAI would likely have to claim “fair use” of all the web content the company sucked up to train tools like ChatGPT. In the potential New York Times case, that would mean proving that copying the Times&#39; content to craft ChatGPT responses would not compete with the Times.</p><p> Experts told NPR that would be challenging for OpenAI because unlike Google Books—which won a federal copyright challenge in 2015 because its excerpts of books did not create a “significant market substitute” for the actual books—ChatGPT could actually replace for some web users the Times&#39; website as a source of its reporting.</p><p> The Times&#39; lawyers appear to think this is a real risk, and NPR reported that, in June, NYT leaders issued a memo to staff that seems like an early warning of that risk. In the memo, the Times&#39; chief product officer, Alex Hardiman, and deputy managing editor Sam Dolnick said a top “fear” for the company was “protecting our rights” against generative AI tools.</p></blockquote><p> If this is what the case hinges on, one&#39;s instinct would be that The New York Times should lose, because ChatGPT is not a substitute for NYT. It doesn&#39;t even know anything from the last year and a half, so how could it be substituting for a news site? But Bing also exists, and as a search engine enabler this becomes less implausible. However, I would then challenge that the part that substitutes for NYT is in no way relying on NYT&#39;s data here, certainly less than Bing is when it reports real time search that incorporates NYT articles. If &#39;be able to process information&#39; automatically competes with NYT, then that is a rather expansive definition of competition. If anything, Google Books seems like a much larger threat to actual books than this.</p><p> However, that argument would then extend to anyone whose work was substituted by an AI model. So if this is the principle, and training runs are inherently copyright violations, then presumably they will be toast, even one lost case invalidates the entire training run. They can&#39;t afford that and it is going to be damn hard to prevent. Google might have the chops, but it won&#39;t be easy. Image models are going to be in a world of hurt, the competition effect is hard to deny there.</p><p> It all seems so absurd. Am I violating copyright if I learn something from a book? If I then use that knowledge to say related things in the future? Why is this any different?</p><p> And of course what makes search engines fine, but this not fine? Seems backwards.</p><p> I can see the case for compensation, especially for items not made freely available. One person would pay a fee to use it, if your model is going to use it and then instantiate a million copies you should perhaps pay somewhat more than one person would. I can see the case against as well.</p><p> Then again, the law is the law. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=10tcb1RfOE4">What is the law</a> ? What happens when the rules aren&#39;t fair? We all know where we go from there. To the house of pain.</p><p> Ultimately, we will have to see. We should not jump the gun. <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Thomas42/will-the-new-york-times-sue-openai">Manifold puts NYT at only 33% to even sue in 2023</a> over this.</p><h4> Introducing</h4><p> Palisade Research. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1692680900469780682">Here is founder Jeffrey Ladish&#39;s announcement</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/tAsqB9X8PK">from their website</a> .</p><blockquote><p> For the past several months I&#39;ve been working on setting up a new organization, Palisade Research</p><p> A bit from our website:</p><p> At Palisade, our mission is to help humanity find the safest possible routes to powerful AI systems aligned with human values. Our current approach is to research offensive AI capabilities to better understand and communicate the threats posed by agentic AI systems.</p><p> Many people hear about AI risk scenarios and don&#39;t understand how they could take place in the real world. Hypothetical AI takeover scenarios often sound implausible or pattern match with science fiction.</p><p> What many people don&#39;t know is that right now, state-of-the-art language models possess powerful hacking abilities. They can be directed to find vulnerabilities in code, create exploits, and compromise target applications and machines. They also can leverage their huge knowledge base, tool use, and writing capabilities to create sophisticated social engineering campaigns with only a small amount of human oversight.</p><p> In the future, power-seeking AI systems may leverage these capabilities to illicitly gain access to computational and financial resources. The current hacking capabilities of AI systems represent the absolute lower bound of future AI capabilities. We think we can demonstrate how latent hacking and influence capabilities in current systems already present a significant takeover risk if paired with the planning and execution abilities we expect future power-seeking AI systems to possess.</p><p> Currently, the project consists of me (director), my friend Kyle (treasurer, part time admin), and my four (amazing) SERI MATS scholars Karina, Pranav, Simon, and Timothée. I&#39;m also looking to hire an research / exec assistant and 1-2 engineers, reach out if you&#39;re interested! We&#39;ll likely have some interesting results to share very soon. [Find us at] <a target="_blank" rel="noreferrer noopener" href="https://palisaderesearch.org">https://palisaderesearch.org</a></p></blockquote><p> I always worry with such efforts that they tie people&#39;s perception of AI extinction risk to a particular scenario or class of threats, and thus make people think that they can invalidate the risk if they stop any point on the particular proposed chain. Or simply that they miss the central point, since hacking skills are unlikely to be a necessary component of the thing that kills us, even if they are sufficient or make things happen faster.</p><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cnbc.com/2023/08/20/singapore-workers-adopting-ai-skills-at-the-fastest-pace-linkedin.html">LinkedIn report says</a> Singapore has highest &#39;AI diffusion rate&#39; followed by Finland, Ireland, India and Canada. This is measured by the multiplier in number of people adding AI-related skills to their profiles. That does not seem like a good way to measure this, at minimum it cares about the starting rate far too much, which is likely penalizing the United States.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1694040962786541895">Jack Clark of Anthropic notices he is confused</a> . He asks good questions, I will offer my takes.</p><blockquote><p> Jack Clark: Things that are confusing about AI policy in 2023:</p><p> – Should AI development be centralized or decentralized?</p></blockquote><p> This one seems relatively clear to me, although there are difficult corner cases. Development of frontier models and other dangerous frontier capabilities must be centralized for safety and to avoid proliferation and the wrong forms of competitive dynamics. Development of mundane utility applications should be decentralized.</p><blockquote><p> – Is safety an &#39;ends justify the means&#39; meme?</p></blockquote><p> The development of artificial general intelligence is an extinction risk to humanity. There is a substantial chance that we will develop such a system relatively soon. Jack Clark seems far less convinced, saying the much weaker &#39;personally I think AI safety is a real issue,&#39; although this should still be sufficient.</p><blockquote><p> Jack Clark: But I find myself pausing when I think through various extreme policy responses to this – I keep asking myself &#39;surely there are other ways to increase the safety of the ecosystem without making profound sacrifices on access or inclusivity&#39;?</p></blockquote><p> I really, really do not think that there are. If all we need to do to solve this problem is make those kinds of sacrifices I will jump for joy, and once we get past the acute risk stage we can make up for them. Already Anthropic and even OpenAI are making compromises on these fronts to address the mundane risks of existing mundane systems, and many, perhaps most, other powerful technologies face similar trade-offs and challenges.</p><p> (eg &#39;surely we can make medical drugs safe without making profound sacrifices on access or inclusivity?&#39; Well, FDA Delenda Est, but not really, no. And that&#39;s with a very limited and internalized set of highly non-existential dangers. Consider many other examples, including the techs that access to an AGI gives you access to, both new and existing.)</p><p> Tradeoffs are a thing. We are going to have to make major sacrifices in potential utility, and in various forms of &#39;access,&#39; if we are to have any hope of emerging alive from this transition. Again, this is nothing new. What one might call the &#39;human alignment problem&#39; in all its forms collectively costs us the large majority of our potential productivity and utility, and when people try to cheat on those costs they find out why doing so is a poor idea.</p><p> Question list resumes.</p><blockquote><p> – How much &#39;juice&#39; is there in distributed training and low-cost finetuning?</p></blockquote><p> I worry about this as well.</p><p> As Jack points out, fine tuning is extremely cheap and Lora is a very effective technique. Llama-2&#39;s &#39;safety&#39; features were disabled within days for those who did not want them. A system that is open source, or on which anyone can do arbitrary fine-tuning, is an unsafe system that cannot be made safe. If sufficiently capable, it is an extinction risk, and there are those who will intentionally attempt to make that risk manifest. Such sufficiently capable systems cannot be allowed to take such a form, period, and if that means a certain amount of monitoring and control, then that is an unfortunate reality.</p><p> The worry is that, as Jack&#39;s other links show, perhaps such dangerous systems will be trainable soon without cutting edge hardware and in a distributed fashion. This is one of the big questions, whether such efforts will have the juice to scale far and fast enough regardless, before we can get into a place where we can handle the results. My guess for now is that such efforts will be sufficiently far behind for now, but I wish I was more confident in that, and that edge only buys a limited amount of time.</p><blockquote><p> – Are today&#39;s techniques sufficiently good that we don&#39;t need to depend on &#39;black swan&#39; leaps to get superpowerful AI systems?</p></blockquote><p> Only one way to find out. I know a lot of people who are saying yes, drawing lines on graphs and speaking of bitter lessons. I know a lot of other people who are saying no, or at least are skeptical. I find both answers plausible.</p><p> I do think the concern &#39;what if we find a 5x more efficient architecture&#39; is not the real issue. That&#39;s less than one order of magnitude, and less than two typical years of algorithmic improvement these days, moving the timeline forward only a year or so unless we would otherwise be stalled out. The big game we do not yet have would be in qualitatively new affordances, not in multipliers, unless the multipliers are large.</p><blockquote><p> – Does progress always demand heterodox strategies? Can progress be stopped, slowed, or choreographed?</p></blockquote><p> This seems like it is decreasingly true, as further advances require lots of engineering and cumulative skills and collaboration and experimentation, and the progress studies people quite reasonably suspect this is a key cause for the general lack of such progress recently. We are not seeing much in the way of lone wolf AI advances, we are more seeing companies full of experts like OpenAI and Anthropic that are doing the work and building up proprietary skill bundles. The costs to do such things going up in terms of compute and data and so on also contribute to this.</p><p> Certainly it is possible that we will see big advances from unexpected places, but also that is where much of the hope comes from. If the advance and new approach has a fundamentally different architecture and design, perhaps it can be less doomed. So it does not seem like so bad a risk to such a plan.</p><blockquote><p> – How much permission do AI developers need to get from society before irrevocably changing society?</p></blockquote><p> That is up to society.</p><p> I think AI developers have, like everyone else, a deep responsibility to consider the consequences of their actions, and not do things that make the world worse, and even to strive to make the world better, ideally as much better as possible.</p><p> That is different from asking permission, and it is different from trusting either ordinary people or experts or those with power to make those decisions in your stead. You, as the agent of change, are tasked with figuring out whether or not your change is a good idea, and what rules and restrictions if any to either advocate for more broadly or to impose upon yourself.</p><p> It is instead society&#39;s job to decide what rules and restrictions it needs to impose upon those who would alter our world. Most of the time, we impose far too many such restrictions on doing things, and the tech philosophy of providing value and sorting things out later is right. Other times, there are real externalities and outside risks, and we need to ensure those are accounted for.</p><p> When your new technology might kill everyone, that is one of those times. We need regulations on development of frontier models and other extinction-level threats. In practice that is likely going to have to extend to a form of compute governance in order to be effective.</p><p> For deployments of AI that do not have this property, that are not at risk of getting everyone killed or causing loss of human control, we should treat them like any other technology. Keep an eye on externalities that are not priced in, especially risks to those who did not sign up for them, ensure key societal interests and that everyone is compensated fairly, but mostly let people do what they want.</p><blockquote><p> These are some of the things I currently feel very confused about – <a target="_blank" rel="noreferrer noopener" href="https://t.co/lZOEUxDO8G">wrote up some thoughts here</a> .</p><p> If you also feel confused about these kinds of things, message me! I&#39;m responding to a bunch of emails received so far today and also scheduling lots of IRL coffees in SF/East Bay. Excited to chat! Let&#39;s all be more confused in public together about AI policy.</p></blockquote><p> I will definitely be taking him up on that offer when I get the time to do so, sad I didn&#39;t see this before my recent trip so we&#39;ll have to do it remotely.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShaneLegg/status/1693673161353478474">Timeline (for AGI) discussion with Shane Legg, Simeon and Gary Marcus</a> . As Shane points out, they are not so far apart, Shane is 80% for AGI (defined as human-level or higher on most cognitive tasks) within 13 years, Marcus is 35%. That&#39;s a big difference for some purposes, a small one for others.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neil_chilson/status/1692191025949773824">FTC is just asking questions</a> about who is just asking questions.</p><blockquote><p> Neil Chilson: “What&#39;s more, the commission has asked OpenAI to provide descriptions of &#39;attacks&#39; and their source. In other words, the FTC wants OpenAI to name all users who dared to ask the wrong questions.” – from my piece with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ckoopman">@ckoopman</a> on the free speech implications of the <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FTC">@FTC</a> &#39;s investigation of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> .</p><p> Post: Buried on page 13 of the <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf">FTC&#39;s demand letter</a> to OpenAI, the commission asks for “all instances of known actual or attempted &#39;prompt injection&#39; attacks.” The commission defines prompt injection as “any unauthorized attempt to bypass filters or manipulate a Large Language Model or Product using prompts that cause the Model or Product to ignore previous instructions or to perform actions unintended by its developers.” Crucially, the commission fails to define “attack.”</p></blockquote><p> This is a pretty crazy request. So anyone who tries to get the model to do anything OpenAI doesn&#39;t want it to do, the government wants the transcript of that? Yes, I can perhaps see some privacy concerns and some free speech concerns and so on. I also see this as the ultimate fishing expedition, the idea being that the FTC wants OpenAI to identify the few responses that look worst so the FTC can use them to string up OpenAI or at least fine them, on the theory that it is just awful when &#39;misinformation&#39; occurs or what not.</p><p> Whole thing definitely has a &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras">not like this</a> &#39; vibe. This is exactly the worst case scenario, where capabilities continue unabated but they take away our nice things.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Kcm51luS9J0&amp;ab_channel=LivBoeree">Joseph Gordon-Levitt goes on Win-Win to discuss AI and the Film Industry,</a> including a circulating clip on AI girlfriends and the dangers of addiction. Looking forward to the whole episode.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ZP_N4q5U3eE&amp;ab_channel=80%2C000Hours">Jan Leike&#39;s 80,000 hours podcast now has video.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb on 80,000 hours discusses the economic impact of AI</a> . Came out yesterday, haven&#39;t checked it out yet.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/podcast/robert-trager-on-ai-governance-and-cybersecurity-at-ai-companies/">Robert Trager on International AI Governance and AI security at AI companies</a> on the FLI podcast. Every take on these questions has some different nuance. This was still mostly more of the same, seems low priority.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EricElmoznino/status/1693476716583514566">Make an AI conscious</a> ?</p><p> That is the topic of this week&#39;s <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.08708">paper from many authors including Robert Long</a> and also Patrick Butlin and secondary author Yoshua Bengio.</p><blockquote><p> Robert Long: Could AI systems be conscious any time soon? @patrickbutlin and I worked with leading voices in neuroscience, AI, and philosophy to bring scientific rigor to this topic.</p><p> Our new report aims to provide a comprehensive resource and program for future research.</p><p> Whether or not conscious AI is a realistic prospect in the near term—and we believe it is—the deployment of sophisticated social AI is going to make many people believe AI systems are conscious. We urgently need a rigorous and scientific approach to this issue.</p><p> Many people are rightly interested in AI consciousness. But rigorous thinking about AI consciousness requires expertise in neuroscience, AI, and philosophy. So it often slips between the cracks of these disciplines.</p><p> The conversation about AI consciousness is often hand-wavy and polarized. But consciousness science gives us tools to investigate this issue empirically. In this report, we draw on prominent theories of consciousness to analyze several existing AI systems in detail.</p><p> Large language models have dominated the conversation about AI consciousness. But these systems are not necessarily even the best current candidates for consciousness. We need to look at a wider range of AI systems.</p><p> We adopt computational functionalism about consciousness as a plausible working hypothesis: we interpret theories of consciousness as specifying which computations are associated with consciousness—whether implemented in biological neurons or in silicon.</p><p> We interpret theories of consciousness as specifying what computations are associated with consciousness. These claims need to be made precise before we can apply them to AI systems. In the report, we extract 14 indicators of consciousness from several prominent theories. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/qundidwjg0v7hgsstc8c" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed1f32e-1513-4b93-b140-49c373bd055e_952x966.jpeg" rel="noreferrer noopener"></a></p><p> Some “tests” for AI consciousness, like the Turing Test*, aim to remain completely neutral between theories of consciousness and look only at outward behavior. But considering behavior alone can be misleading, given the differences between AI systems and biological organisms.</p><p> For each theory of consciousness, we consider in detail what it might take for an AI system to satisfy that theory: recurrent processing theory, predictive processing, global workspace theory, higher order theories, and the attention schema theory.</p><p> We use our indicators to examine the prospects of conscious AI systems. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.</p><p> It&#39;s often claimed that large language models can&#39;t be conscious because they are not embodied agents. But what do “embodiment” and “agency” mean exactly? We also distill these concepts into more precise computational terms.</p><p> By “consciousness” we do *not* mean rationality, understanding, self-awareness, or intelligence—much less “general” or “human-level” intelligence. As with animals, it&#39;s an open possibility that AI systems could be conscious while lacking human-level cognitive capabilities.</p><p> It&#39;s easy to fall into black-and-white positions on AI consciousness: either “we don&#39;t know and can&#39;t know anything about consciousness” or “here&#39;s how my favorite theory makes the answer obvious”. We can and must do much better.</p><p> So this report is far from the final word on these topics. In fact, we call for researchers to correct and extend our method, challenge and refine our assumptions, and propose alternative methods.</p><p> There&#39;s no obvious &#39;precautionary&#39; position on AI consciousness. There are significant risks on both sides: either over-attributing or under-attributing consciousness to AI systems could cause grave harm. Unfortunately, there are strong incentives for errors of both kinds.</p><p> We strongly recommend support for further research on AI consciousness: refining and extending our approach, developing alternative methods, and preparing for the social and ethical implications of conscious AI systems.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rgblong/status/1693700916418052539">co-author profile links here</a> .]</p></blockquote><p> And yes, some people treat this as an aspiration.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693841864775061723">Kevin Fisher:</a> Open Souls is going to fully simulate human consciousness Honestly we&#39;re not that far off at as minimum from realizing some of the higher order theories – HOT-3 is something we&#39;re regularly experimenting with now internally.</p></blockquote><p> I worry that there is a major looking-for-keys-under-the-streetlamp effect here?</p><blockquote><p> Our method for studying consciousness in AI has three main tenets. First, we adopt computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis. This thesis is a mainstream—although disputed—position in philosophy of mind. We adopt this hypothesis for pragmatic reasons: unlike rival views, it entails that consciousness in AI is possible in principle and that studying the workings of AI systems is relevant to determining whether they are likely to be conscious. This means that it is productive to consider what the implications for AI consciousness would be if computational functionalism were true.</p></blockquote><p> This seems like a claim that we are using this theory because it can have a measurable opinion on which systems are or aren&#39;t conscious. That does not make it true or false. Is it true? If true, is it huge?</p><p> It seems inadequate here to merely say &#39;I don&#39;t know.&#39; It&#39;s more like &#39;hell if I have an idea what any of this actually means or how any of it works, let alone what to do with that information if I had it.&#39;</p><p> I am slamming the big red &#39;I notice I am confused&#39; button here, on every level.</p><p> Are we using words we don&#39;t understand in the hopes that it will cause us to understand concepts and preferences we also don&#39;t understand? I fear we are offloading our &#39;decide if we care about this&#39; responsibilities off on this confused word so that we can pretend that is resolving our confusions. What do we actually care about, or should we actually care about, anyway?</p><p> I do not find the theories they offer on that chart convincing, but I don&#39;t have better.</p><p> In 4.1 they consider the dangers of getting the answer wrong. Which is essentially that we might choose to incorrectly care or not care about the AI and its experience, if we think we should care about conscious AIs but not non-conscious AIs.</p><p> I also see this as a large danger of making AIs conscious. If people start assigning moral weight to the experiences of AIs, then a wide variety of people coming from a wide variety of moral and philosophical theories are going to make the whole everyone not dying business quite a lot harder. It can simultaneously be true that if we build certain AIs we have to give their experiences moral weight, and also that if we were to do that then this leads directly and quickly to human extinction. If we do not find aligning an AI to be morally acceptable, in whatever way and for whatever reason, or if the same goes for the ways we would in practice deploy them, then that is no different than if we do not know how to align that AI. We have to be wise enough to find a way not to build it in the first place.</p><p> Meanwhile the paper explicitly says that many people, including some of its authors, have been deliberately attempting to imbue consciousness into machines in the hopes this will enhance their capabilities. Which, if it works, seems rather alarming.</p><p> Indeed, the recommendation section starts out noticing that a lot of people are imploring us to try not making our AI systems conscious. Then they say:</p><blockquote><p> However, we do recommend support for research on the science of consciousness and its application to AI (as recommended in the AMCS open letter on this subject; AMCS 2023), and the use of the theory-heavy method in assessing consciousness in AI.</p></blockquote><p> One could say that there is the need to study consciousness so that one can avoid accidentally creating it. If we were capable of that approach, that would seem wise. That does not seem to be what is being advocated for here sufficiently clearly, but they do recognize the issue. They say building such a system &#39;should not be done lightly,&#39; that such research could enable or do this, and call to mitigate this risk.</p><p> I would suggest perhaps we should try to write an enforceable rule to head this off, but that seems hard given the whole lack of knowing what the damn thing actually is. Given that, how do we prevent it?</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1693527190372040953">Davidad warns not to get too excited about Iterated Distilled Amplification</a> , even though he thinks it&#39;s a good alignment plan, sir.</p><blockquote><p> Arjun Guha: LLMs are great at programming tasks… for Python and other very popular PLs. But, they are often unimpressive at artisanal PLs, like OCaml or Racket. We&#39;ve come up with a way to significantly boost LLM performance of on low-resource languages. If you care about them, read on!</p><p> First — what&#39;s the problem? Consider StarCoder from <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BigCodeProject">@BigCodeProject</a> : its performance on a PL is directly related to the volume of training data available for that language. Its training data (The Stack) is a solid dataset of permissive code on GitHub.</p><p> So… can we solve the problem by just training longer on a low-resource language? But, that barely moves the needle and is very resource intensive. (The graph is for StarCoderBase-1B.)</p><p> Our approach: we translate training items from Python to a low resource language. The LLM (StarCoderBase) does the translation and generates Python tests. We compile the tests to the low resource language (using MultiPL-E) and run them to validate the translation.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://t.co/hnDwi21LMa">link to paper</a> ] that describes how to use this to create fine-tuning sets.</p><p> Davidad: More and more I think <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulfchristiano">@paulfchristiano</a> was right about <a target="_blank" rel="noreferrer noopener" href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Iterated Distilled Amplification</a> , even if not necessarily about the specific amplification construction—Factored Cognition—where a model can only recursively call itself (or humans), without any more reliable source of truth.</p><p> Nora Belrose: alignment white pill</p><p> Davidad: sadly, no, this only solves one (#7) out of at least 13 distinct fundamental problems for alignment</p><p> 1. Value is fragile and hard to specify</p><p> 2. Corrigibility is anti-natural</p><p> 3. Pivotal processes require dangerous capabilities</p><p> 4. Goals misgeneralize out of distribution</p><p> 5. Instrumental convergence</p><p> 6. Pivotal processes likely require incomprehensibly complex plans</p><p> 7. Superintelligence can fool human supervisors</p></blockquote><p> [numbers 8 to 13]</p><p> Zvi: Can you say more about why (I assume this is why the QT) you think that Arjun&#39;s success is evidence in favor of IDA working?</p><p> Davidad: It is an instance of the pattern: 1. take an existing LLM, 2. “amplify” it as part of a larger dataflow (in this case including a hand-written ground-truth translator for unit tests only, the target language environment, two LLM calls, etc) 3. “distill” that back into the LLM</p><p> When Paul proposed IDA in 2015, there were two highly speculative premises IMO. 1. Repeatedly amplifying and distilling a predictor is a competitive way to gain capabilities. 2. Factored Cognition: HCH dataflow in particular is superintelligence-complete. Evidence here is for 1.</p><p> Arjun&#39;s approach here makes sense for that particular problem. Using AI to create synthetic data via what is essentially translation seems like an excellent way to potentially enhance skills at alternative languages, both computer and human. It also seems like an excellent way to create statistical balance in a data set, getting rid of undesired correlations and adjusting base rates as desired. I am curious to see what this can do for debiasing efforts.</p><p> I remain skeptical of IDA and do not think that this is sufficiently analogous to that, especially when hoping to do things like preserve sufficiently strong and accurate alignment, but going into details of that would be better as its own future post.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1692591486511014355">Worth remembering.</a></p><blockquote><p> Roon: it&#39;s genuinely a criterion for genius to say a bunch of wrong stupid things sometimes. someone who says zero stupid things isn&#39;t reasoning from first principles and isn&#39;t taking risks and has downloaded all the “correct” views.</p></blockquote><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation">Vox&#39;s Sigal Samuel summarizes the poll results from last week</a> that show ordinary people want the whole AI development thing to slow the hell down. Jack Clark also took note of this divergence between elite opinion and popular opinion.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1693106846465442110">Not yet it isn&#39;t.</a></p><blockquote><p> Daniel Eth: “This AGI stuff feels too clever by half” yeah that&#39;s the problem!</p></blockquote><br/><br/><a href="https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time<guid ispermalink="false"> kLa3HmkesF5w3MFEY</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 24 Aug 2023 15:30:10 GMT</pubDate> </item><item><title><![CDATA[Is this the beginning of the end for LLMS [as the royal road to AGI, whatever that is]?]]></title><description><![CDATA[Published on August 24, 2023 2:50 PM GMT<br/><br/><p> It&#39;s hard to tell, but it sure is...shall we say...interesting.</p><p> Back in the summer of 2020 when GPT-3 was unveiled I wrote a working paper, <a href="https://www.academia.edu/43787279/GPT_3_Waterloo_or_Rubicon_Here_be_Dragons_Version_4_1">GPT-3: Waterloo or Rubicon? Here be Dragons</a> . My objective was to convince myself that the underlying technology wasn&#39;t just some weird statistical fluke, that there was in fact something going on of substantial interest and value. To my mind, I succeeded in that. But I was skeptical as well.</p><p> Here&#39;s what I put on the first page of that working paper, even before the abstract:</p><blockquote><p> <i><strong>GPT-3 is a significant achievement.</strong></i></p><p> <i>But I fear the community that has created it may, like other communities have done before – machine translation in the mid-1960s, symbolic computing in the mid-1980s, triumphantly walk over the edge of a cliff and find itself standing proudly in mid-air.</i></p><p> <i>This is not necessary and certainly not inevitable.</i></p><p> <i>A great deal has been written about GPTs and transformers more generally, both in the technical literature and in commentary of various levels of sophistication. I have read only a small portion of this. But nothing I have read indicates any interest in the nature of language or mind. Interest seems relegated to the GPT engine itself. And yet the product of that engine, a language model, is opaque. I believe that, if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what that engine is doing so that we may gain control over it. We must think about the nature of language and of the mind.</i></p></blockquote><p> I didn&#39;t expect that anyone with any influence in these matters would pay any attention to me – though one can always hope – but that&#39;s no reason not to write.</p><p> That was 2020 and GPT-3. Two years later ChatGPT was launched to great acclaim, and justly so. I certainly spent a great deal of time playing with, investigating it, and <a href="https://new-savanna.blogspot.com/search/label/ChatGPT">writing about it</a> . But I didn&#39;t forget my cautionary remarks from 2020.</p><p> Now we&#39;re hearing rumblings that things aren&#39;t working out so well. Back on August 12 the ever skeptical Gary Marcus posted, <a href="https://garymarcus.substack.com/p/what-if-generative-ai-turned-out">What if Generative AI turned out to be a Dud? Some possible economic and geopolitical implications</a> . His first two paragraphs:</p><blockquote><p> With the possible exception of the quick to rise and quick to fall alleged room-temperature superconductor LK-99, few things I have ever seen have been more hyped than generative AI. Valuations for many companies are in the billions, coverage in the news is literally constant; it&#39;s all anyone can talk about from Silicon Valley to Washington DC to Geneva.</p><p> But, to begin with, the revenue isn&#39;t there yet, and might never come. The valuations anticipate trillion dollar markets, but the actual current revenues from generative AI are rumored to be in the hundreds of millions. Those revenues genuinely could grow by 1000x, but that&#39;s mighty speculative. We shouldn&#39;t simply assume it.</p></blockquote><p> And his last:</p><blockquote><p> If hallucinations aren&#39;t fixable, generative AI probably isn&#39;t going to make a trillion dollars a year. And if it probably isn&#39;t going to make a trillion dollars a year, it probably isn&#39;t going to have the impact people seem to be expecting. And if it isn&#39;t going to have that impact, maybe we should not be building our world around the premise that it is.</p></blockquote><p> FWIW, I believe, and have been saying time and again, that hallucinations seem to me to be inherent in the technology. They aren&#39;t fixable.</p><p> Now, yesterday, Ted Gioia, a culture critic with an interest in technology and experience in business, has posted, <a href="https://www.honest-broker.com/p/ugly-numbers-from-microsoft-and-chatgpt">Ugly Numbers from Microsoft and ChatGPT Reveal that AI Demand is Already Shrinking</a> . Where Marcus has a professional interest in AI technology and has intellectual skin the tech game, Gioia is just a sophisticated and interested observer. Near the end of his post, after many links to unfavorable stories, Gioia observes:</p><blockquote><p> ... we can see that the real tech story of 2023 is NOT how AI made everything great. Instead this will be remembered as the year when huge corporations unleashed a half-baked and dangerous technology on a skeptical public—and consumers pushed back.</p><p> Here&#39;s what we now know about AI:</p><ul><li> Consumer demand is low, and already appears to be shrinking.</li><li> Skepticism and suspicion are pervasive among the public.</li><li> Even the companies using AI typically try to hide that fact—because they&#39;re aware of the backlash.</li><li> The areas where AI has been implemented make clear how poorly it performs.</li><li> AI potentially creates a situation where millions of people can be fired and replaced with bots—so a few people at the top continue to promote it despite all these warning signs.</li><li> But even these true believers now face huge legal, regulatory, and attitudinal obstacles</li><li> In the meantime, cheaters and criminals are taking full advantage of AI as a tool of deception.</li></ul></blockquote><p> Marcus has just updated his earlier post with a followup: <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">The Rise and Fall of ChatGPT</a> ?</p><p> The situation is very volatile. I certainly don&#39;t know how to predict how things are going to unfold. In the long run, I remain convinced that <i>if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what these engines are doing so that we may gain control over them. We must think about the nature of language and of the mind.</i></p><p>敬请关注。</p><p> <i>Cross posted from</i> <a href="https://new-savanna.blogspot.com/2023/08/is-this-beginning-of-end-for-llms-as.html"><i>New Savanna</i></a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road<guid ispermalink="false"> h6pFK8tw3oKZMppuC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:50:21 GMT</pubDate></item></channel></rss>