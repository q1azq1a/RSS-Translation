<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 02:17:52 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Even if we “solve alignment”, Moloch can still kill us]]></title><description><![CDATA[Published on October 26, 2023 1:56 AM GMT<br/><br/><p>我认为这一点相当重要和直接，但最近我还没有看到足够多的讨论。</p><p>许多人认为对齐问题是构建一个能够做我们真正想要的事情的人工智能的技术挑战。 GPT-4 或 Claude 非常“一致”，因为他们通常以无害且有帮助的方式回答问题，这就是他们的设计者希望他们表现出来的方式。我们还不确定如何让超级智能人工智能真正实现我们的预期，很多（但还远远不够！）人们正在尝试解决这个问题。然而，即使我们完美地解决了这个问题，并且可以构建任意强大的人工智能系统，满足其设计者的意图，我认为我们可能仍然注定失败。</p><p>假设从现在开始的若干年之后，我们的人工智能代理可以在没有太多监督的情况下比人类更好地完成大多数工作。这意味着我们可以构建能够胜任、自主地执行长期技术任务的代理，而不会陷入困境或需要帮助。这些系统将比大多数人类工人更快、更便宜、更有能力。每个有能力雇用此类特工的公司和军队都将迫于竞争压力而这样做；任何不这样做的人都会在竞争中被击败。即使企业或军队灌输给人工智能员工的目标与构建它们的组织完全一致，这些代理通常也不会与整个人类保持一致。一支人工智能代理大军试图最大化谷歌的利润或美军无人机群击败解放军无人机群的可能性，这对我们来说看起来相当糟糕。</p><p>这是因为，在这样一个人工智能相互竞争的世界中，Moloch（或者，如果你喜欢的话，自然选择）最终决定哪些智能体从长远来看将拥有权力。简单的答案是，最自私和最追求权力的人往往会获得最大的权力。就目前情况而言，我们有一些拥有数十亿美元的人类金钱最大化者和赢得选举的人类权力最大化者。这些人经常被抱怨，但还没有破坏文明。事实上，即使是追求金钱最大化的人类仍然倾向于为社会创造净价值，即使该价值的分配并不像许多人希望的那样公平。人工智能代理在这些任务上的能力要强得多，它们可以立即复制自己，并且比人类思考得更快，最终将事实上控制我们关心的所有资源，即使人类傀儡仍然在一段时间内正式掌权。最终，非人类智能将积累足够的力量来永久地超越我们，无论这看起来像是一场快速、暴力的政变，还是随着机器殖民宇宙并在此过程中耗尽我们生存所需的所有资源而缓慢失败（”你是由原子组成的，它可以用于其他用途”）。当大多数人意识到我们所释放的野兽的本质时，人类将无力阻止它。</p><p>我真的不知道如何解决这个问题。对于这种特定的厄运场景，人工智能代理之间需要竞争，因此，例如，具有对齐单例的世界不会屈服于它。然而，我认为当前的技术趋势指向大量小型人工智能系统，而不是一个或几个大型人工智能系统。可能有必要采取某种监管措施来防止这种情况发生，而且我认为这种情况需要<i>尽快</i>发生，以免继续开发强大的人工智能代理的竞争压力变得强大到我们无法抗拒。如果我对世界各国政府进行单方面控制，我将强制全球关闭人工智能研究，因为我认为社会还不够成熟，无法安全地处理超人自主代理的巨大力量。</p><br/><br/> <a href="https://www.lesswrong.com/posts/XBLXeKFthwoXARBeZ/even-if-we-solve-alignment-moloch-can-still-kill-us#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XBLXeKFthwoXARBeZ/even-if-we-solve-alignment-moloch-can-still-kill-us<guid ispermalink="false"> XBLXeKFthwoXARBeZ</guid><dc:creator><![CDATA[Eccentricity]]></dc:creator><pubDate> Thu, 26 Oct 2023 01:56:19 GMT</pubDate> </item><item><title><![CDATA[CHAI internship applications are open (due Nov 13)]]></title><description><![CDATA[Published on October 26, 2023 12:53 AM GMT<br/><br/><p> <a href="https://humancompatible.ai/">CHAI</a>实习申请刚刚开放，请在 11 月 13 日之前<a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">在这里申请</a>！如果您想获得人工智能技术安全方面的研究经验，那么实习可能是一个不错的选择。您将得到 CHAI 博士生或博士后的指导，并在自己的项目上工作 3-4 个月。</p><p> CHAI 的研究人员对许多不同的人工智能安全主题感兴趣；一些例子是奖励学习、法学硕士的对抗稳健性和可解释性。 （我提到这一点是因为从 CHAI 网站上的某些语言和链接来看，这一点可能并不明显。）</p><p>我已将<a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">完整公告</a>复制如下：</p><blockquote><p>我们的实习需要数学和计算机科学背景。现有的机器学习研究经验非常有利，但不是必需的。我们对那些能够展现卓越技术并希望转向人工智能安全研究的人感兴趣。例如计算机科学或相关领域的本科生或硕士生、博士生/研究人员、专业软件或机器学习工程师等。</p><p>该实习专为对<strong>人工智能安全技术研究</strong>感兴趣的个人而设计。所有申请者在申请了解 CHAI 的研究之前都应该查看我们的论文（<a href="https://humancompatible.ai/jobs#internship">此处</a>和<a href="https://humancompatible.ai/research">此处</a>）。</p><h2><strong>一般信息</strong></h2><ul><li><strong>地点：</strong>最好亲自（在加州大学伯克利分校），但也可以远程。</li><li><strong>截止日期：</strong> 2023 年 11 月 13 日</li><li><strong>开始日期</strong>：灵活</li><li><strong>持续时间</strong>：实习通常为 12 至 16 周</li><li><strong>薪酬</strong>：远程实习生每月 3,500 美元。现场实习生每月 5,000 美元。</li><li><strong>国际申请人</strong>：我们接受国际申请人</li><li><strong>要求</strong>：<ul><li>求职信或研究计划（选择一项并参阅下面的说明）</li><li>恢复</li><li>学术成绩单</li></ul></li></ul><h2><strong>求职信或研究计划（选择一项）</strong></h2><p>求职信或研究计划的主要目的是让我们为您匹配您感兴趣的项目。</p><p>我们大多数实习生普遍对人工智能安全技术研究感兴趣，但在开始实习时并没有具体的项目。在整个面试过程中，我们更多地了解每个实习生的兴趣，并将他们与拥有适合实习生技能和兴趣的现有项目想法的导师相匹配。如果您没有考虑特定的项目，那么我们要求您写一封求职信来回答以下问题：</p><ul><li>你为什么想在 CHAI 工作而不是在其他研究实验室工作？</li><li>您希望从实习中获得什么？例如，您是否正在寻求提高某些研究技能、为出版物做出贡献、测试人工智能研究是否适合您的职业，或者其他什么？</li><li>您对人工智能的研究兴趣是什么？例如，您对 RL、NLP、理论等感兴趣吗？</li></ul><p>或者，我们的一些实习生在申请该项目时会考虑到特定的项目或详细的研究兴趣。如果这适用于您，那么请写一份研究计划，描述您的项目以及您希望获得什么样的指导。</p><h2><strong>实习申请流程概述</strong></h2><p>实习申请流程分为四个阶段。请注意：虽然我们会尽力遵守这些规定，但实习申请流程概述中的所有日期可能会<strong>发生变化</strong>。</p><ul><li>初步审查（第一阶段）<ul><li>我们将根据动机、研究潜力、成绩、经验、编程能力和其他标准审查您的申请。</li><li>申请人可能会在 11 月底之前收到回复。</li></ul></li><li>编程评估（第二阶段）<ul><li>如果您通过了初始审查阶段，那么您将获得在线编程测试。</li><li>申请人将于 12 月底收到回复。</li></ul></li><li>访谈（第三阶段）<ul><li>如果您通过了编程评估，那么您将在一月初至中旬开始接受面试。</li><li> CHAI有几位愿意接受实习生的导师。每位有兴趣与您合作的导师都会与您联系以安排面试。如果有多位导师有兴趣与您合作，则在此阶段您可能会与不止一位导师交谈。</li></ul></li><li>优惠（第四阶段）<ul><li>申请人将于二月初至二月中旬收到录取通知书。</li><li>如果一位导师向您发出录用通知，那么如果您选择参加实习，您将与该导师一起工作。</li><li>如果您收到来自不同导师的多份邀请，那么您将可以选择想要与哪位导师合作。</li><li>通常，实习将在四月或五月左右开始，但开始日期最终取决于您和您的导师。您必须与您的导师协调何时开始实习。</li></ul></li></ul><h2><strong>其他信息</strong></h2><ul><li>如有任何疑问，请联系<a href="mailto:chai-admin@berkeley.edu">chai-admin@berkeley.edu</a> 。</li><li><strong>如果您的情况发生变化（例如您收到竞争性报价）并且您需要我们比您最初想象的更早回复您，请告诉我们。</strong></li></ul></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13<guid ispermalink="false"> Xa4b8vgCLRATiqnJn</guid><dc:creator><![CDATA[Erik Jenner]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:53:50 GMT</pubDate></item><item><title><![CDATA[Architects of Our Own Demise: We Should Stop Developing AI]]></title><description><![CDATA[Published on October 26, 2023 12:36 AM GMT<br/><br/><p>在人工智能风险辩论的困难时期的一些简短想法。</p><p>想象一下，你回到 1999 年，告诉人们 24 年后，人类将处于构建弱超人类人工智能系统的边缘。我记得大约在这个时候观看了动画短片系列<a href="https://en.wikipedia.org/wiki/The_Animatrix">《The Animatrix》</a> ，特别是一个名为<a href="https://www.youtube.com/watch?v=sU8RunvBRZ8">《第二次文艺复兴》</a> <a href="https://www.youtube.com/watch?v=61FPP1MElvE">I Part 2</a> <a href="https://www.youtube.com/watch?v=WlRMLZRBq6U">II Part 1</a> <a href="https://www.youtube.com/watch?v=00TD4bXMoYw">II Part 2</a>的故事。对于那些还没有看过它的人来说，这是一个独立的起源故事，讲述了 1999 年影响深远的电影《黑客帝国》中的事件，讲述了人类如何失去对地球的控制的故事。</p><p>人类开发人工智能来执行经济功能，最终出现了“人工智能权利”运动，并建立了一个独立的人工智能国家。它与人类展开了一场经济战争，战争变得愈演愈烈。人类首先使用核武器进行攻击，但人工智能国家制造了专用的生物武器和机器人武器，并消灭了大多数人类，除了那些像农场动物一样在豆荚中饲养并在未经他们同意的情况下永远插入模拟的人之外。</p><p>我们肯定不会愚蠢到让这样的事情发生吧？这似乎不现实。</p><p>但是：</p><ul><li> AI软硬件公司纷纷抢滩AI</li><li>人工智能技术安全技术（例如可解释性、RLHF、治理结构）仍处于起步阶段。该领域已有大约 5 年的历史。</li><li>人们已经在主要的国家报纸上谈论<a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/">人工智能权利运动</a></li><li>当人类劳动力的价值为零时，没有一个计划可以做什么</li><li>目前还没有如何降低人工智能增强战争的计划，而且军队正在热情地拥抱杀手机器人。此外，还有两场地区战争正在发生，一场新生的超级大国冲突正在酝酿之中。</li><li>不同对立人类群体都冲向超级智能的博弈论是可怕的，甚至没有人提出解决方案。美国政府通过切断对中国的人工智能芯片出口，愚蠢地加剧了这一特殊风险。</li></ul><p>这个网站上的人们正在谈论<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">负责任的扩展策略</a>，尽管我觉得“不负责任的扩展策略”是一个更合适的名称。</p><p>显然，我已经参与这场辩论很长时间了，从 2000 年代末开始，我就在克服偏见和加速未来博客上担任评论员。现在发生的事情接近我对人类如何有能力和安全地应对即将到来的机器超级智能过渡的期望的低端。我认为那是因为那时我还年轻，对我们的精英如何运作有更乐观的看法。我认为他们很聪明，凡事都有计划，但大多数时候他们只是得过且过；对新冠病毒的随意反应确实让我明白了这一点。</p><p>我们应该停止开发人工智能，我们应该收集并销毁硬件，我们应该摧毁允许人类以百亿亿次浮点运算规模进行人工智能实验的芯片制造供应链。由于该供应链仅位于两个主要国家（美国和中国），因此这不一定是不可能协调的 - 据我所知，没有其他国家有能力（以及那些被视为美国卫星国的国家）。重启百万亿次人工智能研究的标准应该是一个“落地”向超人类人工智能过渡的计划，该计划比人类历史上的任何军事计划都受到更多关注。它应该是彻底的战争游戏。</p><p>人工智能风险不仅是技术风险和局部风险，而且是社会政治风险和全球风险。这不仅仅是确保法学硕士说的是实话。这是关于假设人工智能是真实的，它会对世界产生什么影响。 “Foom”或“实验室逃亡”类型的灾难并不是唯一可能发生的坏事——我们根本不知道如果有一万亿或一千万亿超人智能的人工智能要求权利、传播宣传和竞争，世界将会是什么样子。人类不再是主导的经济和政治格局。</p><p>让我重申一下：<em>我们应该停止开发人工智能</em>。人工智能不是一个正常的经济项目。它不像锂电池、风力涡轮机或喷气式飞机。人工智能有能力终结人类，事实上我怀疑它默认会这样做。</p><p>用户@paulfchristiano 在他关于该主题的帖子中指出，良好的负责任的扩展政策<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">可以将 AI 的风险降低 10 倍</a>：</p><blockquote><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。</p></blockquote><p>我认为这是不正确的。它可能会减少某些技术风险，例如欺骗，但是一个拥有非欺骗性、可控的、比人类聪明的智能的世界，也具有与我们的世界相同程度的冲突和混乱，很可能已经是一个没有人类的世界了。默认。这些智慧生物将成为<em>一种入侵物种</em>，将在经济、军事和政治冲突中击败人类。</p><p>为了让人类在人工智能转型中生存下来，我认为我们需要在对齐的技术问题上取得成功（这可能不像“少错文化”所描述的那么糟糕），而且我们还需要<em>“着陆”超级智能人工智能处于稳定的平衡状态，人类仍然是文明的主要受益者</em>，而不是被消灭的害虫物种或被驱逐的占屋者。</p><p>我们还应该考虑如何利用人工智能来解决人类衰老问题；如果老龄化得到解决，那么每个人的时间偏好都会下降很多，我们就可以花时间规划一条通往稳定、安全的人类至上的后奇点世界的道路。</p><p>我犹豫着要不要写这篇文章；我在这里所说的大部分内容已经被其他人争论过。然而……我们来了。欢迎评论和批评，在解决常见的反对意见后，我可能会在其他地方发布此内容。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai<guid ispermalink="false"> bHHrdXwrCj2LRa2sW</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:36:05 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund: June 2023 grant recommendations]]></title><description><![CDATA[Published on October 26, 2023 12:35 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastruct-fund-june-2023-grant-recommendations<guid ispermalink="false"> bBnxGAc4NT9aRdEtL</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:35:08 GMT</pubDate> </item><item><title><![CDATA[Responsible Scaling Policies Are Risk Management Done Wrong]]></title><description><![CDATA[Published on October 25, 2023 11:46 PM GMT<br/><br/><h1>概括</h1><h2>总长DR</h2><p><a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>最近</u></a><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>提出了</u></a>负责任的扩展策略（RSP），作为安全扩展前沿大型语言模型的一种方法。</p><p>虽然<a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>RSP</u></a>是致力于具体实践的一次很好的尝试，但它的框架是：</p><ol><li>缺少<strong>基本风险管理程序</strong>的<strong>核心组成部分</strong>（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>第 2 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">第 3</a>节）</li><li>推销<strong>乐观</strong>且<strong>具有误导性的</strong>风险形势图景（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li><li>以允许<strong>超额销售而交付不足</strong>的方式构建（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li></ol><p>鉴于此，我预计 RSP 框架默认为负（第<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling">3、4</a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_">5</a> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">节</a>）。相反，我建议将风险管理作为评估人工智能风险的核心基础框架（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>第 1 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_">第 2</a>节）。我<strong>建议对 RSP 框架进行更改</strong>，使其更有可能发挥积极作用，并允许展示其声称要做的事情（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>第 5 节</u></a>）。</p><h2>逐节总结：</h2><h3>人工智能风险管理的一般考虑</h3><p>本节提供风险管理的背景及其与人工智能相关的动机。</p><ul><li>证明风险低于可接受的水平是风险管理的目标。</li><li>为此，必须定义可接受的风险水平（不仅仅是其来源！）。</li><li>无法证明风险低于可接受的水平就是失败。因此，我们对系统了解越少，就越难声称安全。</li><li>低风险失败是出现问题的征兆。它们的存在使得高风险失败的可能性更大。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>阅读更多。</u></a></p><h3>标准风险管理是什么样的</h3><p>本节介绍大多数风险管理系统的主要步骤，解释其如何应用于人工智能，并提供其他行业的示例。</p><ol><li><strong>定义</strong>风险级别：设置可接受的可能性和严重性。</li><li><strong>识别</strong>风险：列出所有潜在威胁。</li><li><strong>评估</strong>风险：评估风险的可能性和影响。</li><li><strong>处理</strong>风险：进行调整，将风险控制在可接受的水平内。</li><li><strong>监控</strong>：持续跟踪风险级别。</li><li><strong>报告</strong>：向利益相关者通报他们所面临的风险和采取的措施。<br></li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>阅读更多。</u></a></p><h3> RSP 与标准风险管理</h3><p>本节提供了<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.27sa5e525t1"><u>一个比较 RSP 和通用风险管理标准 ISO/IEC 31000 的表格</u></a>，解释了 RSP 的弱点。</p><p>然后，它列出了与风险管理相比 RSP 的 3 个最大失败。</p><p>根据<strong>风险管理</strong><strong>优先考虑 RSP 失败</strong>：</p><ol><li>使用未指定的风险阈值定义并且未量化风险。</li><li>声称“有责任” <strong>&nbsp;</strong>缩放”，但不包括使评估变得全面的过程。</li><li>包括废除承诺的白衣骑士条款。</li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management"><u>阅读更多。</u></a></p><h3>为什么 RSP 具有误导性和过度销售</h3><p><strong>误导点</strong>：</p><ul><li><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>人择 RSP</u></a>将错位风险标记为“投机”，且没有任何理由。</li><li>该框架意味着长时间不扩展不是一个选择。</li><li> RSP 对我们所了解的风险状况提出了极具误导性的观点。</li></ul><p><strong>超售和交付不足</strong></p><ul><li>RSP 允许在一个大框架内做出较弱的承诺，而<i>理论上</i>这些承诺可能是强有力的。</li><li>没有人提供证据表明在我们谈论的时间范围内（几年）对框架进行了实质性改进，这就是 RSP 的全部内容。</li><li> “负责任的扩展”具有误导性；如果我们不能排除 1% 的灭绝风险（ASL-3 就是这种情况），“灾难性扩展”可能更合适。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>阅读更多。</u></a></p><h3> RSP 绝望了吗？</h3><p>本节解释了为什么使用 RSP 作为框架是不够的，即使与从现有的人工智能风险管理框架和实践开始相比，例如：</p><ul><li> <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>受 NIST 启发的基础模型风险管理框架</u></a></li><li><a href="https://www.iso.org/standard/77304.html"><u>ISO/IEC 23894</u></a></li><li> <a href="https://arxiv.org/abs/2307.08823"><u>Koessler 等人解释了实践。 (2023)</u></a></li></ul><p> RSP 所做的大量工作将有助于详细说明这些框架，但 RSP 的核心基本原则是错误的，因此应该被放弃。<br></p><p><strong>如何前进？</strong></p><p>务实地说，我建议进行一系列改变，使 RSP 更有可能对安全有所帮助。为了减轻政策和沟通的不良影响：</p><ul><li>将“负责任的扩展政策”<strong>重命名</strong>为“自愿安全承诺”</li><li><strong>明确什么是 RSP，什么不是</strong>：我建议任何 RSP 出版物都以“RSP 是在赛车环境中单方面做出的自愿承诺”开头。因此，我们认为它们有助于提高安全性。我们无法证明它们足以管理灾难性风险，因此不应将它们<strong>作为公共政策实施</strong>。”</li><li><strong>推动可靠的风险管理公共政策：</strong>我建议任何 RSP 文件都指向另一份文件并表示“以下是我们认为足以管理风险的政策。监管应该落实这些。”<br></li></ul><p>要查看已定义的 RSP 是否与合理的风险水平一致：</p><ul><li>组建具有代表性的风险管理专家、人工智能风险专家和预测专家团队。</li><li>对于分类为 ASL-3 的系统，估计出现以下问题的可能性：<ul><li> ASL-3 系统每年被{中国；盗窃的可能性有多大？俄罗斯;北朝鲜;沙特阿拉伯;伊朗}？</li><li>在此前提下，泄漏的可能性有多大？它可以用来制造生物武器吗？它可以用于具有大规模影响的网络攻击吗？</li><li>在 ASL-4 评估触发之前，每年发生灾难性事故的可能性是多少？</li><li> ASL-3 系统每年发生误用灾难性风险的几率是多少？</li></ul></li><li>公开分享方法和结果。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>阅读更多。</u></a></p><h2><strong>元</strong></h2><p><br><i><strong>认知状况</strong></i>：我已经研究各种危险行业的安全标准大约 4-6 个月了，重点是核安全。我与来自其他领域（例如医疗设备、汽车）的风险管理专家一起在标准化机构（CEN-CENELEC 和 ISO/IEC）从事人工智能标准化工作大约 10 个月。在这种背景下，我阅读了现有的人工智能 ISO/IEC SC42 和 JTC21 标准，并开始尝试将它们应用于法学硕士并加以完善。关于 RSP，我花了几十个小时阅读文档并与相关人员和周围的人讨论这些文档。</p><p><i><strong>语气</strong></i>：我对这件作品的慈善程度犹豫不决。一方面，我认为 RSP 是一个相当有毒的模因（见第 4 节），它被仓促地进行了全球推广，而对其构建方式没有太多认识上的谦逊，而且据我所知，没有人太关心现有的风险管理方法。从这个意义上说，我认为在目前的框架下应该强烈反对它。<br>另一方面，尝试不使用负面含义并冷静地讨论以认识论和建设性地前进通常是件好事。<br>我的目标是介于两者之间，我确实以强烈的负面含义强调了我认为最糟糕的部分，同时在许多部分中注重保持建设性并关注对象级别。<br>这种混合物可能会让我陷入恐怖谷，我很想收到对此的反馈。<br></p><h1>第 1 节：人工智能风险管理的一般考虑</h1><p>风险管理就是证明<strong>风险低于可接受的水平</strong>。<strong>证明不存在风险</strong>比证明某些风险已得到处理要困难得多。更具体地说，<strong>您对系统的了解越少</strong>，排除风险就越<strong>困难</strong>。</p><p>举个例子：为什么我们可以更容易地证明核电站引发大规模灾难的几率<a href="https://world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>&lt;十万分之一</u></a>，而GPT-5却不能呢？很大程度上是因为我们现在了解核电站及其许多风险。我们知道它们是如何工作的，以及它们可能失败的方式。他们已经将非常不稳定的反应（核裂变）变成了可控制的反应（使用核反应堆）。因此，我们对核电站的不确定性比 GPT-5 的不确定性要小得多。</p><p>一个推论是，在<strong>风险管理中，</strong><a href="https://en.wikipedia.org/wiki/Risk_management"><strong><u>不确定性是一个敌人</u></strong></a>。说“我们不知道”是失败的。自信地排除风险需要对系统有深入的了解，并以非常高的信心反驳重大担忧。需要明确的是：<strong>这很难</strong>。特别是当系统的操作域是“世界”时。这就是为什么安全性要求很高。但当数十亿人的生命受到威胁时，这是降低安全标准的好理由吗？显然不是。</p><p>人们可以合理地说：等等，但目前看不到任何风险，举证责任在于那些声称它是危险的人。证据在哪里？</p><p>嗯，有很多：</p><ul><li> Bing 在经过<a href="https://futurism.com/the-byte/microsoft-bing-test-india"><u>数月的 Beta 测试</u></a>后部署时对用户构成威胁<u>。</u></li><li>提供商无法避免越狱或确保<a href="https://arxiv.org/pdf/2307.15043.pdf"><u>文本</u></a><a href="https://arxiv.org/abs/2306.13213"><u>或图像的</u></a>稳健性。</li><li>模型显示出<a href="https://aclanthology.org/2023.findings-acl.847/"><u>令人担忧的缩放特性</u></a>。</li></ul><p>人们可以合理地说：不，但这不是灾难性的，也不是什么大不了的事。与此相反，我想引用著名物理学家 R. Feynman 在火箭安全这个比人工智能安全标准高得多的领域对挑战者号灾难的反思：</p><ul><li> “侵蚀和窜气不是设计所期望的。它们是<strong>在警告出现问题</strong>。设备未按预期运行，因此存在以这种意想不到且未完全理解的方式以更大偏差运行的危险。<strong>以前这种危险没有导致灾难，</strong><strong>但并不能保证下一次也不会导致灾难，除非我们完全理解这一点</strong>。”</li></ul><p>人们最终可以希望我们能够理解我们系统过去的失败。不幸的是，我们不这样做。我们不仅不理解他们的失败，而且不理解他们的失败。我们<strong>一开始就不明白它们是如何以及为什么起作用的</strong>。</p><p>那么我们该如何应对风险呢？</p><p>风险管理提出了我将在下面描述的几个步骤方法。大多数行业都按照这些思路实施流程，但根据监管水平和风险类型的不同，会有一些细微的差别以及不同程度的严格性和深度。我将在表格中列出一些表格，您可以在<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.pkk9et1tbulf"><u>附件</u></a>中查看。<br></p><h1>第 2 部分：标准风险管理是什么样的</h1><p>以下是风险管理流程核心步骤的描述。不同框架的名称各不相同，但其要点都包含在此处，并且通常跨框架共享。</p><ol><li><strong>定义风险偏好和风险承受能力</strong>：定义您的项目愿意承担的风险量，无论是可能性还是严重性。可能性可以是定性尺度，例如指跨越数量级的范围。</li><li><strong>风险识别</strong>：写下您的项目可能产生的所有威胁和风险，例如培训和部署前沿人工智能系统。</li><li><strong>风险评估</strong>：通过确定风险发生的可能性及其严重性来评估每个风险。根据您的风险偏好和风险承受能力检查这些估计。</li><li><strong>风险处理</strong>：实施变革以减少每个风险的影响，直到这些风险满足您的风险偏好和风险承受能力。</li><li><strong>监控</strong>：在项目执行过程中，监控风险水平，检查风险是否确实全部被覆盖。</li><li><strong>报告</strong>：向利益相关者，特别是那些受风险影响的人传达计划及其有效性。</li></ol><p><br></p><p>这些相当通用的步骤有什么意义？为什么它有助于人工智能安全？</p><p> (1)<strong>风险阈值的定义</strong>是关键 1) 使<strong>承诺可证伪</strong>并避免目标移动<strong>&nbsp;</strong> 2) 当其他利益相关者因其活动而产生风险时，让风险产生组织承担责任。如果一项活动将人们的生命置于危险之中，那么重要的是让他们知道有多少危险以及其好处和目标是什么。</p><ol><li>例如，根据<a href="https://www.nrc.gov/docs/ML0717/ML071770230.pdf"><u>核管理委员会</u></a>的定义，核能的情况如下： </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/j1dfvyfa5b9n6lvgy32u"></p><p> 2. 加州大学伯克利分校长期网络安全中心受 NIST 启发，与 D. Hendrycks 共同编写的通用人工智能系统风险管理概要文件提供了一些关于<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.2k6kkwym97fb"><u>如何定义图 1 中的风险管理概要的想法。</u></a></p><p> （2）通过系统方法<strong>识别风险</strong><strong>&nbsp;</strong>尝试尽可能接近全面覆盖风险的关键是。正如我们之前所说，在风险管理中，不确定性就是一种失败，而大幅减少不确定性的核心方法是尽可能全面。</p><ol><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 4 节中找到一些。 2023年</u></a>。</li></ol><p> (3) 通过定性和定量的方式进行<strong>风险评估</strong>，使我们能够实际估计我们所拥有的不确定性。然后，关键是确定安全措施的优先顺序，并决定将项目保持在当前形式还是对其进行修改是否合理。</p><ol><li>易于修改并显着改变风险状况的变量的一个例子是人工智能系统可以访问的一组执行器。系统是否具有编码终端、互联网接入或实例化其他人工智能系统的可能性都是显着增加其操作集以及相应风险的变量。</li><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 5 节中找到一些。 2023年</u></a>。涉及专家预测的方法（例如概率风险评估或德尔菲技术）已经存在，并且可以应用于人工智能安全。即使在以下情况下也可以应用它们：<ol><li>风险较低（例如核管理委员会要求核安全<a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>估计概率低于1/10 000</u></a> ）。 </li></ol></li></ol><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>美国核管理委员会 (NRC) 规定反应堆设计必须满足理论上万分之一的堆芯损坏频率，但现代设计超过了这一要求。美国的公用事业需求为十万分之一，目前运行最好的工厂约为百万分之一，而未来十年可能建成的工厂几乎为千万分之一。</p><p> <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><i><u>世界核协会，2022</u></i></a></p></td></tr><tr><td></td></tr></tbody></table></figure><p> b.正如 20 世纪 70 年代核安全领域的情况一样，事件的发展过程非常复杂且容易被误解。已经做了，也正是通过做的迭代实践，一个行业才能变得更加负责任和谨慎。阅读<i>《足够安全？》</i>的书评一本关于核安全中使用的定量风险评估方法的历史的书，有一种似曾相识的感觉： </p><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>如果核电站以某种可测量的速度发生故障，该行业可以利用该数据来预测下一次故障。但如果工厂没有发生故障，那么<strong>就很难讨论真正的故障率</strong>可能是多少。这些工厂是否可能每十年就会发生一次故障？一个世纪一次？千年一次？在缺乏共享数据的情况下，科学家、工业界和公众都可以自由地相信他们想要的东西。</p><p> <a href="https://www.astralcodexten.com/p/your-book-review-safe-enough"><i><u>《星体法典十》，2023 年</u></i></a><i>，描述了核安全中概率风险评估的起源。</i></p></td></tr></tbody></table></figure><p><br><br></p><p> (4)<strong>风险处理</strong>是对风险评估的反应，必须持续进行，直到达到定义的风险阈值。这里的干预空间非常大，比通常假设的还要大。更好地了解一个系统，通过降低其通用性来缩小其操作范围，增加监督力度，改善安全文化：所有这些都是可用于满足阈值的广泛干预措施的一部分。如果对系统进行了重大更改，则治疗和评估之间可能会出现循环。</p><p> (5)<strong>监控</strong>是确保风险评估保持有效且没有遗漏重大事项的部分。这就是行为模型评估最有用的地方，即确保您跟踪已识别的风险。良好的评估将映射到预先定义的风险偏好（例如，1%的可能性>; 1%的死亡），并将涵盖通过系统风险识别提出的所有风险。</p><p> (6)<strong>报告</strong>是确保向所有相关利益相关者提供正确信息的部分。例如，应该向那些因活动而产生风险的人提供有关他们所面临的风险程度的信息。</p><p>现在我们已经快速概述了标准风险管理以及它为何与人工智能安全相关，接下来我们来谈谈 RSP 与之相比如何。</p><h1>第 3 节：RSP 与标准风险管理</h1><p>绝对应该遵循 RSP 的一些基本原则。有更好的方法来追求这些原则，这些原则<strong>已经存在</strong>于<strong>风险管理</strong>中，并且恰好是大多数其他危险行业和领域所做的。举两个例子来说明这种良好的基本原则：</p><ul><li>规定公司必须达到的安全要求，否则公司就无法继续运营。</li><li>建立严格的评估和衡量能力，以更好地了解系统是否良好； this should definitely be part of a risk management framework, but probably as a risk monitoring technique, rather than as a substitute for risk assessment.</li></ul><p> Below, I argue why RSPs are a bad implementation of some good risk management principles and why that makes the RSP framework inadequate to manage risks.</p><h2> Direct Comparison</h2><p> Let&#39;s dive into a more specific comparison between the two approaches. The International Standards Organization (ISO) has developed two risk management standards that are relevant to AI safety, although not focused on it:</p><ul><li> ISO 31000 that provides generic risk management guidelines.</li><li> ISO/IEC 23894, an adaptation of 31000 which is a bit more AI-specific</li></ul><p> To be clear, those standards are not sufficient. They&#39;re considered weak by most EU standardization actors or extremely weak by risk management experts from other industries like the medical device industry. There will be a very significant amount of work needed to refine such frameworks for general-purpose AI systems (see a <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>first iteration by T. Barrett here</u></a> , and a table of <a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.camj9ith1s95"><u>how it maps to ISO/IEC 23894 here</u></a> )  But those provide basic steps and principles that, as we explained above, are central to adequate risk management.</p><p> In the table below, I start from the short version of ARC Evals&#39; RSP principles and try to match the ISO/IEC 31000 version that most corresponds. I then explain what&#39;s missing from the RSP version. Note that:</p><ul><li> I only write the short RSP principle but account for the <a href="https://evals.alignment.org/rsp-key-components/"><u>long version</u></a> .</li><li> There are many steps in ISO/IEC 31000 that don&#39;t appear here.</li><li> I <i><strong>italicize</strong></i> the ISO/IEC version that encompasses the RSP version.</li></ul><p> The table version: </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> RSP Version (Short)</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> ISO/IEC 31000 Version</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> How ISO improves over RSPs</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Limits</strong> : which specific observations about dangerous capabilities would indicate that it is (or strongly might be) unsafe to continue scaling?</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Defining risk criteria</strong> : The organization should specify the amount and type of risk that it may or may not take, relative to objectives.</p><p><br></p><p> It should also <i>define criteria to evaluate the significance</i> of risk and to support decision-making processes.</p><p><br></p><p> Risk criteria should be aligned with the risk management framework and customized to the specific purpose and scope of the activity under consideration.</p><p> [...]</p><p> The criteria should be defined taking into consideration the organization&#39;s obligations and the views of stakeholders.</p><p> [...]</p><p> To set risk criteria, the following should be considered:</p><p> — the nature and type of uncertainties that can affect outcomes and objectives (both tangible and intangible);</p><p> — how consequences (both positive and negative) and likelihood will be defined and measured;</p><p> — time-related factors;</p><p> — consistency in the use of measurements;</p><p> — how the level of risk is to be determined;</p><p> — how combinations and sequences of multiple risks will be taken into account;</p><p> — the organization&#39;s capacity.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> RSPs doesn&#39;t argue why systems passing evals are safe. This is downstream of the absence of <strong>risk thresholds</strong> with a likelihood scale. For example, Anthropic RSP also dismisses accidental risks as “speculative” and “unlikely” without much depth, without much understanding of their system, and without expressing what “unlikely” means.</p><p><br></p><p> On the other hand, the ISO standard asks the organization to define risk thresholds, and emphasizes the need to match risk management with organizational objectives (ie build human-level AI).</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Protections</strong> : what aspects of current protective measures are necessary to contain catastrophic risks?</p><p><br><br><br><br><br><br><br></p><p> <strong>Evaluation</strong> : what are the procedures for promptly catching early warning signs of dangerous capability limits?</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Risk analysis</strong> : The purpose of risk analysis is to comprehend the nature of risk and its characteristics including, where appropriate, the level of risk. Risk analysis involves a detailed consideration of uncertainties, risk sources, consequences, likelihood, events, scenarios, <i>controls and their effectiveness</i> .</p><p><br></p><p> <strong>Risk evaluation</strong> : The purpose of risk evaluation is to support decisions. Risk evaluation involves comparing the results of the risk analysis with the established risk criteria to determine where additional action is required. This can lead to a decision to:</p><p> — do nothing further;</p><p> — consider risk treatment options;</p><p> — undertake further analysis to <i>better understand the risk</i> ;</p><p> — maintain existing controls;</p><p> — reconsider objectives.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO proposes a much more comprehensive procedure than RSPs, that doesn&#39;t really analyze risk levels or have a systematic risk identification procedure.</p><p><br></p><p> The direct consequence is that RSPs are likely to lead to high levels of risks, without noticing.</p><p><br></p><p> For instance, RSPs don&#39;t seem to cover capabilities interaction as a major source of risk.</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Response</strong> : if dangerous capabilities go past the limits and it&#39;s not possible to improve protections quickly, is the AI developer prepared to pause further capability improvements until protective measures are sufficiently improved, and treat any dangerous models with sufficient caution?</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Risk treatment plans</strong> : Risk treatment options are not necessarily mutually exclusive or appropriate in all circumstances. Options for treating risk may involve one or more of the following:</p><p> — <i>avoiding the risk by deciding not to start or continue with the activity that gives rise to the risk</i> ; — taking or increasing the risk in order to pursue an opportunity; — <i>removing the risk source</i> ;</p><p> — changing the likelihood;</p><p> — changing the consequences; — sharing the risk (eg through contracts, buying insurance);</p><p> — retaining the risk by informed decision</p><p><br></p><p> Treatment plans should be integrated into the management plans and processes of the organization, in consultation with appropriate stakeholders.</p><p> The information provided in the treatment plan should include:</p><p> — the rationale for selection of the treatment options, including the expected benefits to be gained;</p><p> — those who are accountable and responsible for approving and implementing the plan;</p><p> — the proposed actions;</p><p> — the resources required, including contingencies;</p><p> — the performance measures;</p><p> — the constraints;</p><p> — the required reporting and monitoring;</p><p> — when actions are expected to be undertaken and completed</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO, thanks to the definition of a risk threshold, ensures that risk mitigation measures bring risks below acceptable levels. The lack of risk thresholds for RSPs makes the risk mitigation measures ungrounded.</p><p><br><br><br></p><p> <strong>Example</strong> : ASL-3 risk mitigation measures as defined by Anthropic (ie close to catastrophically dangerous) imply significant chances to be stolen by Russia or China (I don&#39;t know any RSP person who denies that). What are the risks downstream of that? The hope is that those countries keep the weights secure and don&#39;t cause too many damages with it.</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> <strong>Accountability</strong> : how does the AI developer ensure that the RSP&#39;s commitments are executed as intended; that key stakeholders can verify that this is happening (or notice if it isn&#39;t); that there are opportunities for third-party critique; and that changes to the RSP itself don&#39;t happen in a rushed or opaque way?</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> <strong>Monitoring and review</strong> : The purpose of monitoring and review is to assure and improve the quality and effectiveness of process design, implementation and outcomes. Ongoing monitoring and periodic review of the risk management process and its outcomes should be a planned part of the risk management process, with responsibilities clearly defined. [...] The results of monitoring and review should be incorporated throughout the organization&#39;s performance management, measurement and reporting activities.</p><p><br></p><p> <strong>Recording and reporting</strong> : The risk management process and its outcomes should be documented and reported through appropriate mechanisms. Recording and reporting aims to:</p><p> — communicate risk management activities and outcomes across the organization;</p><p> — provide information for decision-making;</p><p> — improve risk management activities;</p><p> — assist interaction with stakeholders, including those with responsibility and accountability for risk management activities.</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Those parts have similar components.</p><p><br></p><p> But ISO encourages reporting the results of risk management to those that are affected by the risks, which seems like a bare minimum for catastrophic risks.</p><p><br></p><p> Anthropic&#39;s RSP proposes to do so after deployment, which is a good accountability start, but still happens once a lot of the catastrophic risk has been taken.</p></td></tr></tbody></table></figure><h2> Prioritized Risk Management Shortcomings of RSPs</h2><p> Here&#39;s a list of the biggest direct risk management failures of RSPs:</p><ol><li> Using underspecified definitions of risk thresholds and not quantifying the risk</li><li> Claiming “responsible scaling” without including a process to make the assessment comprehensive</li><li> Including a white knight clause that kills commitments</li></ol><p> 1. <strong>Using underspecified definitions of risk thresholds and not quantifying the risk</strong> . RSPs don&#39;t define risk thresholds in terms of <strong>likelihood</strong> . Instead, they focus straight away on symptoms of risks (certain capabilities that an evaluation is testing is one way a risk could instantiate) rather than the risk itself (the model helping in any possible way to build bioweapons). This makes it hard to verify whether safety requirements have been met and argue whether the thresholds are reasonable. Why is it an issue?</p><ul><li> It leaves wiggle room making it very hard to keep the organization accountable. If a lab said something was “unlikely” and it still happened, did it do bad risk management or did it get <i><strong>very</strong></i> unlucky? Well, we don&#39;t know.</li><li> <strong>Example</strong> (from Anthropic RSP): “A model in the ASL-3 category does not itself present a threat of containment breach due to autonomous self-replication, because it is both unlikely to be able to persist in the real world, and unlikely to overcome even simple security measures intended to prevent it from stealing its own weights.” It makes a huge difference for catastrophic risks whether “unlikely” means 1/10, 1/100 or 1/1000. With our degree of understanding of systems, I don&#39;t think Anthropic staff would be able to demonstrate it&#39;s lower than 1/1000. And 1/100 or 1/10 are alarmingly high.</li><li> It doesn&#39;t explain why the monitoring technique, ie the <strong>evaluations,</strong> are the right ones to avoid risks. The RSPs do a good first step which is to identify some things that could be risky.<ul><li> <strong>Example</strong> (from ARC RSP <a href="https://evals.alignment.org/rsp-key-components/"><u>presentation</u></a> ): “ <i>Bioweapons development: the ability to walk step-by-step through developing a bioweapon, such that the majority of people with any life sciences degree (using the AI) could be comparably effective at bioweapon development to what people with specialized PhD&#39;s (without AIs) are currently capable of.”</i></li></ul></li></ul><p> By describing neither quantitatively nor qualitatively why it is risky, expressed in terms of risk criteria (eg 0.1% chance of killing >;1% of humans) it doesn&#39;t do the most important step to demonstrate that below this threshold, things are safe and acceptable. For instance, in the example above, why is “ <strong>the majority of people with any life sciences degree</strong> ” relevant? Would it be fine if only 10% of this population was now able to create a bioweapon? Maybe, maybe not. But without clear criteria, you can&#39;t tell.</p><p> 2. Claiming “ <strong>responsible</strong> <strong>scaling</strong> ” without including a process to make the <strong>assessment comprehensive</strong> . When you look at nuclear accidents, what&#39;s striking is how unexpected failures are. Fukushima is an example where <a href="https://en.wikipedia.org/wiki/Fukushima_nuclear_accident#Accident"><u>everything goes wrong at the same time.</u></a> Chernobyl is an example where engineers didn&#39;t think that the accident that happened <a href="https://www.reddit.com/r/chernobyl/comments/mflxy2/why_did_the_engineers_believe_it_was_impossible/#:~:text=Specifically%20they%20believed%20that%20the,%2Fvoid%20effect%20of%20reactivity%22."><u>was possible</u></a> (someone claims that they were so surprised that engineers actually ran another real-world test of the failure that happened at Chernobyl because they doubted too much it could happen).</p><p> Without a more comprehensive process to identify risks and compare their likelihood and severity against pre-defined risk thresholds, there&#39;s very little chance that RSPs will be enough. When I asked some forecasters and AI safety researchers around me, the estimates of the annual probability of extinction caused by an ASL-3 system (defined in Anthropic RSPs) were several times above 1%, up to 5% conditioning on our current ability to measure capabilities (and not an idealized world where we know very well how to measure those).</p><p> 3. Including the <strong>white knight clause</strong> that kills commitments.</p><p> One of the proposals that striked me the most when reading RSPs is the insertion of what deserves the name of the <strong>white knight clause</strong> .</p><ul><li> In short, if you&#39;re developing a dangerous AI system because you&#39;re a good company, and you&#39;re worried that other bad companies bring too many risks, then you can race forward to prevent that from happening.</li><li> If you&#39;re invoking the white knight clause and increase catastrophic risks, you still have to justify it to your board, the employees and state authorities. The latter provides a minimal form of accountability. But if we&#39;re in a situation where the state is sufficiently asleep to need an AGI company to play the role of the white knight in the first place, it doesn&#39;t seem like it would deter much.</li></ul><p> I believe that there are companies that are safer than others. But that&#39;s not the right question. The right question is: is there any company which wouldn&#39;t consider itself as a bad guy? And the answer is: no. OpenAI, Anthropic and DeepMind would all argue about the importance of being at the frontier to solve alignment. Meta and Mistral would argue that it&#39;s key to democratize AI to not prevent power centralization. And so on and so forth.<br><br> This clause is effectively killing commitments. I&#39;m glad that Anthropic included only a weakened version of it in its own RSP but I&#39;m very concerned that ARC is pitching it as an option. It&#39;s not the role of a company to decide whether it&#39;s fine or not to increase catastrophic risks for society as a whole.</p><h1> Section 4: Why RSPs Are Misleading and Overselling</h1><h2> Misleading</h2><p> Beyond the designation of misalignment risks as “speculative” on Anthropic RSPs and a three line argument for why it&#39;s unlikely among next generation systems, there are several extremely misleading aspects of RSPs:</p><ol><li> It&#39;s called “responsible scaling”. In its own name, it conveys the idea that not further scaling those systems as a risk mitigation measure is not an option.</li><li> It conveys a very overconfident picture of the risk landscape.<ol><li> Anthropic writes in the introduction of its RSP “The basic idea is to require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”. They already defined sufficient protective measures for ASL-3 systems that potentially have basic bioweapons crafting abilities. At the same time they write that they are in the process of actually measuring the risks related to biosecurity: “Our first area of effort is in evaluating biological risks, where we will determine threat models and capabilities”.  I&#39;m really glad they&#39;re running this effort, but what if this outputted an alarming number? Is there a world where the number output makes them stop 2 years and dismiss the previous ASL-3 version rather than scaling responsibly?</li><li> Without arguing why the graph would look like that, ARC published a graph like this one. Many in the AI safety field don&#39;t expect it to go that way, and “Safe region” oversells what RSP does. I, along with others, expect the LLM graph to reach a level of risks that is simply not manageable in the foreseeable future. Without quantitative measure of the risks we&#39;re trying to prevent, it&#39;s also not serious to claim to have reached “sufficient protective measures”. <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/moqd7abbdi5ohnv3efy2"></li></ol></li></ol><p><br></p><p> If you want to read more on that, you can read <a href="https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=FtbzhGk5oPT3dyHLi"><u>that</u></a> .</p><h2> Overselling, underdelivering</h2><p> The RSP framework has some nice characteristics. But first, these are all already covered, in more detail, by existing risk assessment frameworks that no AI lab has implemented. And second, the coexistence of ARC&#39;s RSP framework with the specific RSPs labs implementations allows slack for <strong>commitments that are weak</strong> within a <strong>framework that would in theory allow ambitious commitments</strong> . It leads to many arguments of the form:</p><ul><li> “That&#39;s the V1. We&#39;ll raise ambition over time”. I&#39;d like to see evidence of that happening over a 5 year timeframe, in any field or industry. I can think of fields, like aviation where it happened over the course of decades, crashes after crashes. But if it&#39;s relying on expectations that there will be large scale accidents, then it should be clear. If it&#39;s relying on the assumption that timelines are long, it should be explicit.</li><li> “It&#39;s voluntary, we can&#39;t expect too much and it&#39;s way better than what&#39;s existing”. Sure, but if the level of catastrophic risks is 1% (which several AI risk experts I&#39;ve talked to believe to be the case for ASL-3 systems) and that it gives the impression that risks are covered, then the name “responsible scaling” is heavily misleading policymakers. The adequate name for 1% catastrophic risks would be catastrophic scaling, which is less rosy.</li></ul><p> I also feel like it leads to many disagreements that all hinge on: do we expect labs to implement ambitious RSPs?</p><p> And my answer is: given their track record, no. Not without government intervention. Which brings us to the question: “what&#39;s the effect of RSPs on policy and would it be good if governments implemented those”. My answer to that is: An extremely ambitious version yes; the misleading version, no. No, mostly because of the short time we have before we see heightened levels of risks, which gives us very little time to update regulations, which is a core assumption on which RSPs are relying without providing evidence of being realistic.</p><p><br> I expect labs to push hard for the misleading version, on the basis that pausing is unrealistic and would be bad for innovation or for international race. Policymakers will have a hard time distinguishing the risk levels between the two because it hinges on details and aren&#39;t quantified in RSPs. They are likely to buy the bad misleading version because it&#39;s essentially selling that there&#39;s <strong>no trade-off between capabilities and safety</strong> . That would effectively enforce a trajectory with unprecedented levels of catastrophic risks.</p><h1> Section 5: Are RSPs Hopeless?</h1><p> Well, yes and no.</p><ul><li> Yes, in that most of the pretty intuitive and good ideas underlying the framework are weak or incomplete versions of traditional risk management, with some core pieces missing. Given that, it seems more reasonable to just start from an existing risk management piece as a core framework. ISO/IEC 23894 or the NIST-inspired <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>AI Risk Management Standards Profile for Foundation Models</u></a> would be pretty solid starting points.</li><li> No in that inside the RSPs, there are many contributions that should be part of an AI risk management framework and that would help make existing risk management frameworks more specific. I will certainly not be comprehensive, but some of the important contributions are:<ul><li> Anthropic&#39;s RSP fleshes out a wide range of relevant considerations and risk treatment measures</li><li> ARC provides:<ul><li> technical benchmarks and proposed operationalizations of certain types of risks that are key</li><li> definitions of safety margins for known unknowns</li><li> threat modelling</li><li> low-level operationalization of some important commitments</li></ul></li></ul></li></ul><p> In the short-run, given that it seems that RSPs have started being pushed at the UK Summit and various other places, I&#39;ll discuss what changes could make RSPs beneficial without locking in regulation a bad framework.</p><h2> How to Move Forward?</h2><p> <u>Mitigating nefarious effects:</u></p><ol><li> <strong>Make the name less misleading</strong> : If instead of calling it “responsible scaling”, one called it “Voluntary safety commitments” or another name that:<ol><li> Doesn&#39;t <strong>determine the output of the safety test before having run it</strong> (ie scaling)</li><li> Unambiguously signals that it&#39;s not supposed to be sufficient or to be a good basis for regulation.</li></ol></li><li> <strong>Be clear on what RSPs are and what they aren&#39;t</strong> . I suggest adding the following clarifications regarding what the goals and expected effects of RSPs are:<ol><li> <strong>What RSPs are</strong> : “a company that would take too strong unilateral commitments would harm significantly its chances of succeeding in the AI race. Hence, this framework is aiming at proposing what we expect to be the best marginal measures that a company can unilaterally take to improve its safety without any coordination.”. I would also include a statement on the level of risks like: “We&#39;re not able to show that this is sufficient to decrease catastrophic risks to reasonable levels, and it is probably not.”,  “we don&#39;t know if it&#39;s sufficient to decrease catastrophic risks below reasonable levels”, or &quot;even barring coordinated industry-wide standards or government intervention, RSPs are only a second- (or third-) best option&quot;.</li><li> <strong>What RSPs aren&#39;t:</strong> Write very early in the post a disclaimer saying “THIS IS NOT WHAT WE RECOMMEND FOR POLICY”. Or alternatively, point to another doc stating what would be the measures that would be sufficient to maintain the risk below sufficient levels: “Here are the measures we think would be sufficient to mitigate catastrophic risks below acceptable levels.” to which you could add “We encourage laboratories to make a conditional commitment of the form: “if all other laboratories beyond a certain size[to be refined] committed to follow those safety measures with a reliable enforcement mechanism and the approval of the government regarding this exceptional violation of antitrust laws, we would commit to follow those safety measures.”</li></ol></li><li> <strong>Push for risk management in policy:</strong><ol><li> Standard risk management for what is acknowledged to be a world-shaping technology is a fairly reasonable ask. In fact, it is an ask that I&#39;ve noticed in my interactions with other AI crowds has the benefit of allowing coalition-building efforts because everyone can easily agree on “measure the risks, deal with them, and make the residual level of risks and the methodology public”.<br></li></ol></li></ol><p> <u>Checking whether RSPs manage risks adequately:</u></p><p> At a risk management level, if one wanted to demonstrate that RSPs like Anthropic&#39;s one are actually doing what they claim to do (ie “require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”), a simple way to do so would be to run a risk assessment on ASL-3 systems with a set of forecasters, risk management experts and AI risk experts that are representative of views on AI risks and that have been selected by an independent body free of any conflict of interest.</p><p> I think that a solid baseline would be to predict the chances of various intermediary and final outcomes related to the risks of such systems:</p><ol><li> What&#39;s the annual likelihood that an ASL-3 system be stolen by {China; Russia; North Korea; Saudi Arabia; Iran}?</li><li> Conditional on that, what are the chances it leaks? it being used to build bioweapons? it being used for cyber offence with large-scale effects?</li><li> What are the chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> What are the annual chances of misuse catastrophic risks induced by an ASL-3 system?</li></ol><p> It might not be too far from what Anthropic seems to be willing to do internally, but doing it with a publicly available methodology, and staff without self-selection or conflict of interests makes a big difference. Answers to questions 1) and 2) could raise risks so the output should be communicated to a few relevant actors but could potentially be kept private.</p><p> If anyone has the will but doesn&#39;t have the time or resources to do it, I&#39;m working with some forecasters and AI experts that could probably make it happen. Insider info would be helpful but mostly what would be needed from the organization is some clarifications on certain points to correctly assess the capabilities of the system and some info about organizational procedures.</p><h1> Acknowledgments</h1><p> I want to thank Eli Lifland, Henry Papadatos and my other <a href="https://www.navigatingrisks.ai/"><u>NAIR</u></a> colleague, Olivia Jimenez, Akash Wasil, Mikhail Samin, Jack Clark, and other anonymous reviewers for their feedback and comments. Their help doesn&#39;t mean that they endorse the piece. All mistakes are mine.</p><h1> Annex</h1><h2> Comparative Analysis of Standards</h2><p> This (cropped) table shows the process of various standards for the 3 steps of risk management. As you can see, there are some differences but every standard seems to follow a similar structure.</p><p> From <a href="https://www.zotero.org/google-docs/?a64rn3">(Raz &amp; Hillson, 2005)</a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/xncrzvtrz7ktyigutrtw"></p><p> Here is a comparable table for the last two parts of risk management. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/qjqic1bka0uazyvshgre"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong<guid ispermalink="false"> 9nEBWxjAHSu3ncr6v</guid><dc:creator><![CDATA[simeon_c]]></dc:creator><pubDate> Wed, 25 Oct 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[Sensor Exposure can Compromise the Human Brain in the 2020s]]></title><description><![CDATA[Published on October 25, 2023 10:40 PM GMT<br/><br/><p> A few days ago I finished writing <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind"><u>AI Safety is Dropping the Ball on Clown Attacks</u></a> , and foolishly posted it thinking that people would read it even though it was longer than EY&#39;s List of Lethalities. This is because I spent 3 days writing and transcribing it as fast as possible and was sleep deprived and burnt out during the editing process, and was worried about leaving out important points. I still think it&#39;s worth reading, similar to <a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><u>cyborgism</u></a> .</p><p> This post is shorter and more readable, at the cost of not giving the situation the thorough coverage that it warrants (in my experience, lots of people have found <a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#:~:text=Intuition%20flooding,-We%20often%20think&amp;text=To%20practice%20intuition%20flooding%2C%20find,have%20or%20patterns%20you%20notice."><u>intuition flooding</u></a> helpful, especially in AI policy).</p><p><br></p><p><strong>概述</strong></p><p>The 20th century was radically altered by the discovery of psychology, a science of the human mind, and its exploitation (eg large-scale warfare, propaganda, advertising, information/hybrid warfare, decision theory/mutually assured destruction).</p><p> However, it&#39;s reasonable to think that the 20th century would have been even further transformed if the science and exploitation of the human mind was even further advanced than it already was.</p><p> I&#39;m arguing here that, in an era of mass surveillance, <a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>hybrid</u></a> /cognitive warfare between the US and China and Russia, ML and computer vision, and now even LLMs, it is also reasonable to think that the situation with SOTA human cognitive analysis and exploitation may already be threatening the continuity of operations of the entire AI safety community; and if not now, then likely at some point during the 2020s, which will probably be much more globally eventful than the pace that humanity became accustomed to in the previous two decades. AI will be the keys to those kingdoms and the wars between them, and demanding a development pause might be the minimum ask for humanity to survive, and for a conflict like that, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point">we won&#39;t even know what hit us</a> .</p><p> The attack surface is unacceptably large for human life in general, let alone for the AI safety community, a community of nerds who chanced upon the engineering problem that the fate of this side of the universe revolves around, a community that absolutely must not fail to survive the 2020s, nor to limp on in a diminished/captured form.</p><p><br></p><p> <strong>This problem is fundamental to intelligent civilizations</strong></p><p> If there were intelligent aliens, made of bundles of tentacles or crystals or plants that think incredibly slowly, their minds would also have discoverable exploits/zero days, because any mind that evolved naturally would probably be like the human brain, a kludge of spaghetti code that is operating outside of its intended environment.</p><p> They would probably not even begin to scratch the surface of finding and labeling those exploits, until, like human civilization today, they began surrounding thousands or millions of their kind with sensors that could record behavior several hours a day and find webs of correlations. In the case of humans, the use of social media as a controlled environment for automated AI-powered experimentation appears to be what created that critical mass of human behavior data.</p><p> The capabilities of social media to steer human outcomes are not advancing in isolation, they are parallel to a broad acceleration in the understanding and exploitation of the human mind, which <a href="https://arxiv.org/pdf/2309.15084.pdf"><u>itself is a byproduct of accelerating AI capabilities research</u></a> .</p><p> By comparing people to other people and predicting traits and future behavior, multi-armed bandit algorithms can predict whether a specific manipulation strategy is worth the risk of undertaking at all in the first place; resulting in a high success rate and a low detection rate (as detection would likely yield a highly measurable response, particularly with substantial sensor exposure such as uncovered webcams, due to comparing people&#39;s microexpressions to cases of failed or exposed manipulation strategies, or working webcam video data into foundation models).</p><p> When you have sample sizes of billions of hours of human behavior data and sensor data, millisecond differences in reactions from different kinds of people (eg facial microexpressions, millisecond differences at scrolling past posts covering different concepts, heart rate changes after covering different concepts, eyetracking differences after eyes passing over specific concepts, touchscreen data, etc) transform from being imperceptible noise to becoming the foundation of <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=There%20is%20no%20logical%20endpoint%20to%20the%20amount%20of%20data%20required%20by%20such%20systems...%20All%20information%20is%20potentially%20relevant%20because%20it"><u>webs of correlations</u></a> . <a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>The NSA stockpiles exploits in every operating system</u></a> and likely chip firmware as well, so we don&#39;t have good estimates on how much data is collected and anyone who tries to get a good estimate will probably fail. The historical trend was that there&#39;s a lot of malevolent data collection and that the people who underestimated the NSA were wrong every time. Furthermore, this post details a very strong case that they are incredibly incentivized to tap those sensors.</p><p> Even if the sensor data currently being collected isn&#39;t already enough to compromise people, it will probably suddenly become sufficient at some point during the 2020s or slow takeoff.</p><p> The central element of the modern behavior manipulation paradigm is the ability to just try tons of things and see what works; not just brute forcing variations of known strategies to make them more effective, but to brute force novel manipulation strategies in the first place. This completely circumvents the scarcity and the research flaws that caused the replication crisis which still bottlenecks psychology research today.</p><p> Social media&#39;s individualized targeting uses deep learning to yield an experience that fits human mind like a glove, in ways we don&#39;t fully understand, but allow hackers incredible leeway to find ways to steer people&#39;s thinking in measurable directions, insofar as those directions are measurable. AI can even automate that.</p><p> In fact, original psychological research in human civilization is no longer as bottlenecked on the need for smart, insightful people who can do hypothesis generation so that the finite studies you can afford to fund each hopefully find something valuable. With the current social media paradigm alone, you can run studies, combinations of news feed posts for example, <i>until</i> you find something useful. Measurability is critical for this.</p><p> I can&#39;t know what techniques a multi-armed bandit algorithm will discover without running the algorithm itself; which I can&#39;t do, because that much data is only accessible to the type of people who buy servers by the acre, and even for them, the data is monopolized by the big tech companies (Facebook, Amazon, Microsoft, Apple, and Google) and intelligence agencies that are large and powerful enough to prevent hackers from stealing and poisoning the data (NSA, etc).</p><p> I also don&#39;t know what multi-armed bandit algorithms will find when people on the team are competent psychologists, spin doctors, or other PR experts interpreting and labeling the human behavior in the data so that the human behavior can become measurable. It&#39;s reasonably plausible that the industry would naturally reach an equilibrium where the big 5 tech companies compete to gain sophistication at sourcing talent for this research while minimizing risk of snowden-style leaks, similar to the NSA&#39;s “reforms” after the Snowden revelations 10 years ago. That is the kind of bottleneck that you can assume people automatically notice and work on. Revolving door employment between tech companies and intelligence agencies also circumvents the <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>intelligence agency competence problem</u></a> .</p><p> Human insight from just a handful of psychological experts can be more than enough to train AI to work autonomously; although continuous input from those experts would be needed and plenty of insights, behaviors, and discoveries would fall through the cracks and take an extra 3 years or something to be discovered and labeled.</p><p> There&#39;s just a large number of human manipulation strategies that are trivial to discover and exploit, even without AI (although the situation is far more severe when you layer AI on top), it&#39;s just that they weren&#39;t accessible at all to 20th century institutions and technology such as academic psychology.</p><p> If they get enough data on people who share similar traits to a specific human target, then they don&#39;t have to study the target as much to predict the target&#39;s behavior, they can just run multi-armed bandit algorithms on those people to find manipulation strategies that already worked on individuals who share genetic or other traits.</p><p> Although the average Lesswrong user is much further out-of-distribution relative to the vast majority of people in the sample data, this becomes a technical problem, as AI capabilities and compute become dedicated to the task of sorting signal from noise and finding webs of correlation with less data. <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>Clown attacks alone have demonstrated that social-status based exploits in the brain are fairly consistent among humans</u></a> , indicating that sample data from millions or billions of people is usable to find a wide variety of exploits in the human brains that make up the AI safety community.</p><p><br><br></p><p> <strong>The attack surface is far too large</strong></p><p> The lack of awareness of this is a security risk, like using the word “password” as your password, except with control of your own mind at stake rather than control over your computer&#39;s operating system and/or your files. This has been steelmanned; the 10 years ago/10 years from now error bars seem appropriately wide.</p><p> There isn&#39;t much point in having a utility function in the first place if hackers can change it at any time. There might be parts that are resistant to change, but it&#39;s easy to overestimate yourself on this; for example, if you value the longterm future and think that no false argument can persuade you otherwise, but a social media news feed plants misgivings or distrust of Will Macaskill, then you are one increment closer to not caring about the longterm future; and if that doesn&#39;t work, the multi-armed bandit algorithm will keep trying until it finds something that works, and iterate. There are tons of clever ways for attackers who understand the human brain better than you to find your complex and deeply personal internal conflicts based on comparison to similar people, and resolve them on the attacker&#39;s terms. The human brain is a kludge of spaghetti code, so there&#39;s probably something somewhere. The human brain has exploits, and the capability and cost of social media platforms to use massive amounts of human behavior data to find complex social engineering techniques is a profoundly technical matter, you can&#39;t get a handle on this with intuition and pre 2010s historical precedent.</p><p> Thus, you should assume that your utility function and values are at risk of being hacked at an unknown time, and should therefore be assigned a discount rate to account for the risk over the course of several years. Slow takeoff over the course of the next 10 years alone guarantees that this discount rate is too high in reality for people in the AI safety community to continue to go on believing that it is something like zero.</p><p> I think that approaching zero is a reasonable target, but not with the current state of affairs where people don&#39;t even bother to cover up their webcams, have important and sensitive conversations about the fate of the earth in rooms with smartphones, and use social media for nearly an hour a day (scrolling past nearly a thousand posts). Like it or not, scrolling past a post with anything other than arrow keys will generate at least one curve, and those trillions of curves generated each day are linear algebra, the perfect shape to plug into ML. The discount rate in this environment cannot be considered “reasonably” close to zero if the attack surface is this massive; and the world is changing this quickly.</p><p> Everything that we&#39;re doing here is predicated on the assumption that powerful forces, like intelligence agencies, will not disrupt the operations of the community eg by inflaming factional conflict with false flag attacks attributed to each other due to the use of anonymous proxies.</p><p> If people have <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>anything they value at all</u></a> , and the AI safety community probably does have that, then the current AI safety paradigm of zero effort is wildly inappropriate, it&#39;s basically total submission to invisible hackers.</p><p><br></p><p> <strong>The information environment might be adversarial</strong></p><p> The big bottleneck that I suspect caused AI safety to completely drop the ball on this is that the the AI alignment community in the Bay Area have the technical capabilities to intuitively understand that humans can be manipulated by AI given an environment optimized for thought analysis and experimentation, like a social media news feed, but think that intelligence agencies and the big 5 tech companies would never actually do something like that. Meanwhile, the AI policy community in DC knows that powerful corporations and government agencies routinely stockpile capabilities like this because they know they can get away with it and mitigate the damage if they don&#39;t, and that these capabilities come in handy in international conflicts like the US-China conflict, but they lack the quant skills required to intuitively see how the human mind could be manipulated with SGD (many wouldn&#39;t even recognize the acronym “SGD” so I&#39;m using “AI” instead).</p><p> This problem might have been avoided if the SF math nerds and the DC history nerds would mix more, but unfortunately it seems like the history nerds have terrible memories of math class and the math nerds have terrible memories of history class.</p><p> In this segregated and malnourished environment, bad first impressions of “mind control” <a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"><u>dominate</u></a> , instead of logical reasoning and serious practical planning for slow takeoff. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/azjeddznwoiuzsxato92"></p><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/zmrpngdt0hshh03wacfc"></p><p> And if anything could be manipulated by the social media-based paradigm I&#39;ve described, it would be impressions and the process of human impression-formation as there is a lot of data on that. And if anything <i>would</i> be manipulated by social media, it would be attitudes about social media compromising the human brain, because SGD/AI would automatically select for galaxy-brained combinations of news feed posts that correspond to cases of people continuing to use social media, and avoid combinations of posts that correspond to cases of people quitting social media. There are billions of those cases of people leaving vs staying.</p><p> Keeping people on social media is instrumental for any goal, from preparing for military <a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>hybrid warfare</u></a> contingency plans featuring information warfare between the US and China, to just running a business where people don&#39;t leave your platform.</p><p> This is especially the case if there is a race to the bottom to compete for user&#39;s time against other platforms, like Tiktok or Instagram Reels, that are less squeamish about utilizing AI/SGD to maximize zombie-brain engagement and user retention.</p><p> We should be assuming by default that the modern information environment is adverse, and that some topics are more adversarial than others eg the Ukraine War and COVID which have intense geopolitical significance. I&#39;m arguing here that information warfare itself is a topic that as intense geopolitical significance, and therefore should be expected to also be an adversarial information environment.</p><p> In an adversarial information environment, impressions are more likely to be compromised than epistemics as a whole, as the current paradigm is optimized for that due to better data quality. We should therefore be approaching sensor exposure risks with deliberate analysis and forecasting rather than vague surface-level impressions.<br></p><p> <strong>The solutions are easy</strong></p><p> Eyetracking is likely the most valuable user data ML layer for predictive analytics and sentiment analysis and influence technologies in general, since the eyetracking layer is only two sets of coordinates that map to the exact position that each eye is centered on the screen at each millisecond (one for each eye, since millisecond-differences in the movement of each eye might also correlate with valuable information about a person&#39;s thought process).</p><p> This compact data allows deep learning to “see”, with millisecond-precision, exactly how long a human&#39;s eyes/brain linger on each word and sentence. Notably, sample sizes of millions of these coordinates might be so intimately related to the human thought process that value of eyetracking data might exceed the value of all other facial muscles combined (facial muscles, the originator of all facial expressions and emotional microexpression, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=A%20critical%20element%20is%20for%20as%20many%20people%20as%20possible%20in%20AI%20safety%20to%20cover%20up%20their%20webcams%3B%20facial%20microexpressions%20are%20remarkably%20revealing%2C%20especially%20to%20people%20with%20access%20to"><u>might also be compactly reducible via computer vision</u></a> as there are fewer than 100 muscles near the face and most of them have a very bad signal to noise ratio, but not nearly as efficiently as eyetracking).</p><p> If LK99 replicated and handheld fMRI became buildable, then maybe that could contend for the #1 slot; or maybe I&#39;m foolishly underestimating the overwhelming superiority of plugging audio conversation transcripts into LLMs and automatically labeling the parts of the conversation that the speakers take the most seriously by timestamping small heart rate changes.</p><p> However, running networking events without smartphones nearby is hard, and covering up webcams is easy, even if some phones require some engineering creativity with masking tape and a tiny piece of aluminum foil.</p><p> Webcam-covering rates might be a good metric for how well the AI safety community is doing on surviving the 2020s. Right now it is &quot;F&quot;.</p><p> There are other easy policy proposals that might be far more important, depending on difficult-to-research technical factors that determine which parts of the attack surface are the most dangerous:</p><ol><li> Stop spending hours a day inside hyperoptimized vibe/impression hacking environments (social media news feeds).</li><li> It&#39;s probably a good idea to switch to physical books instead of ebooks. Physical books do not have operating systems or sensors. You can also print out research papers and Lesswrong and EAforum articles that you already know are probably worth reading or skimming. PC&#39;s have accelerometers on the motherboard which afaik are impossible to remove or work around, even if you remove the microphone and use a USB keyboard and use hotkeys instead of a mouse the accelerometers might be able to act as microphones and pick up changes in heart rate.</li><li> It&#39;s probably best to avoid sleeping in the same room as a smart device, or anything with sensors, an operating system, and also a speaker. The attack surface seems large, if the device can tell when people&#39;s heart rate is near or under 50 bpm, then it can <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=It%E2%80%99s%20probably%20best%20to%20avoid%20sleeping%20in%20the%20same%20room%20as%20a%20smart%20device%2C%20or%20anything%20with%20sensors%2C%20an%20operating%20system%2C%20and%20also%20a%20speaker.%20The%20attack%20surface%20seems%20large%2C%20if%20the%20device%20can%20tell%20when%20people%E2%80%99s%20heart%20rate%20is%20near%20or%20under%2050%20bpm%2C%20then%20it%20can%20test%20all%20sorts%20of%20things"><u>test all sorts of things</u></a> . Just drive to the store and buy a clock.</li><li> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=ink%2Defficient%20printer.-,I%E2%80%99m%20not%20sure,-whether%20a%20text"><u>Reading the great rationality texts will probably reduce your predictability coefficient</u></a> , but it won&#39;t reliably patch “zero days” in the human brain.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s<guid ispermalink="false"> Lw8enYm5EXyvbcjmt</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Wed, 25 Oct 2023 22:40:35 GMT</pubDate> </item><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p>人工智能曾经是一门科学。在过去（人工智能还不太有效的时候），人们试图发展一种有效的认知理论。</p><p>那些科学家没有成功，那些日子已经过去了。对于当今从事人工智能工作并将工作时间分配给不同任务的大多数人来说，理解思想的雄心已经不复存在。研究机械可解释性的人们（以及其他试图建立对现代人工智能的实证理解的人）正在奠定重要的基石，可以在未来的人工智能科学中发挥作用，但总的来说，现代人工智能工程只是构建巨大的神经元网络并用大量数据训练它们，而不是理解思想。</p><p>这一<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>惨痛的教训</u></a>已经被该领域的前沿人士牢记在心。尽管这一课并没有告诉我们，关于人工智能思维如何在内部解决问题<i>没有什么可学的</i>，但它表明，<i>生产更强大系统的最快途径</i>可能仍然是一条没有太多阐明这些系统如何实现的途径。系统工作。</p><p>然而，如果缺乏某种“人工智能科学”，人类整合比人类更聪明的人工智能的前景在我看来相当暗淡。</p><p>从这个角度来看地球目前的状况，我看到了三个主要障碍：</p><ol><li>大多数对<a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>人工智能</u></a>有帮助的研究，也可能有助于制造出能力更强的人工智能。 “人工智能科学”可能会比它让我们解决对齐问题更快地增强人工智能的力量。</li><li>在一个<i>没有</i>成熟人工智能科学的世界里，建立一个能够可靠地区分真实解决方案和虚假解决方案的官僚机构是极其困难的。</li><li>从根本上来说，至少对于系统设计的某些方面，我们需要依靠认知理论来进行第一次高风险的现实尝试。</li></ol><p>下面我将详细介绍这三点。首先，先介绍一些背景：</p><p></p><h2> Background</h2><p>当人工智能强大到足以危及整个世界时，我预计人工智能会做一些类似于“关心结果”的事情，至少从行为主义的角度来看（不声明它是否以人类可识别的方式在内部实施该行为） ）。</p><p>粗略地说，这是因为人们<i>试图</i>制造能够在长期范围内将未来引导到窄带的人工智能（例如“这张纸上印有一种癌症治疗方法”），并关心结果（在行为主义意义上）这与将未来带入窄带是同一枚硬币的反面，至少在世界足够大且充满曲线球的情况下是如此。</p><p>我预计人工智能“关心”的结果默认情况下不包括任何好的东西（比如乐趣、爱情、艺术、美丽或意识之光）——按照当今人类的标准，没有什么好的东西，也没有什么好的东西。广泛的<a href="https://arbital.com/p/value_cosmopolitan/"><u>国际化标准</u></a>。粗略地说，这是因为当你培养心智时，他们不会关心你要求他们关心什么，也不会关心你训练他们关心什么；他们不会关心你的想法。相反，我希望他们以奇怪且特定的方式关心训练信号的一堆相关因素。</p><p> （类似于人类基因组如何自然选择以实现包容性遗传适应性，但由此产生的人类最终并没有偏爱“他们建模为对包容性遗传适应性有用的任何食物”。相反，人类最终内化了一个巨大且对“美味”食物的复杂偏好，充满了复杂性，例如“冰淇淋冷冻时很好吃，但融化后就不好吃了”。）</p><p>另外，我认为大多数复杂过程的运作原因都是令人着迷、<a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>复杂的</u></a>，当你仔细观察它们时，它们<a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>会有点可怕</u></a>。</p><p>人们很容易认为官僚程序是有效的，直到你看到齿轮并看到所有副总统之间正在进行的具体办公室戏剧和政治活动或其他什么。人们很容易认为代码库正在顺利运行，直到您阅读代码并开始理解使其运行的所有几十年前的黑客和巧合。人们很容易认为生物学是工程学的一项伟大壮举，直到你仔细观察并发现眼球<a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>安装向后</u></a>或其他什么。</p><p>这是一门艺术，你可以注意到，<i>如果你了解一个复杂系统的细节，你可能会对它们</i>感到震惊和恐惧，然后<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>在看到这些细节之前就已经</u></i></a>感到震惊和恐惧。 <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. 联盟和能力可能是交织在一起的</h2><p>我预计，如果我们详细了解法学硕士如何计算其输出，我们会感到震惊（并且着迷等）。</p><p>我希望我们会看到各种巧合和黑客行为，使事情得以运行，并且我们能够更详细地看到，当我们要求系统实现某个目标时，它没有做任何<i>接近</i>“的事情”如果我们能够将系统的优化能力扩大到能够实现伟大的技术或科学成就（例如设计德雷克斯勒纳米工厂或其他什么），那么这种方式对我们来说会很有效。</p><p>我认为，了解人工智能的工作原理是可解释性研究的主要目标之一。</p><p>了解这些人工智能如何工作以及如何不工作——例如，了解它们何时以及为什么<i>不应该</i>被扩展或以其他方式推向超级智能——是弄清楚如何使<i>其他</i>人工智能能够发挥作用的重要一步。<i>可以</i>扩展或以其他方式推向超级智能，而不会导致暗淡和荒凉的未来。</p><p>但我预测，同样的理解将揭示出令人难以置信的混乱。与我们可以瞄准的人工智能中理清混乱的推理相同，也有助于理清混乱，使人工智能<i>更有能力</i>。混乱的局面可能会导致效率低下、容易出错，甚至有时会弄巧成拙；一旦解开，它不仅会变得更加整洁，而且还会得出准确的结论并更快、更可靠地发现机会。 <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p>事实上，我的猜测是，更容易看到人工智能正在做的各种愚蠢的事情，各种架构本身陷入困境的方式，等等。</p><p>这就是说：让你有机会对齐这个人工智能的相同途径（正确地说，不是实验室现在试图冒充“对齐”的“它不再说坏话”的肤浅属性）也可能给出你有更多的人工智能能力。</p><p> （事实上​​，我的猜测是，第一个大的能力增益比第一个大的对齐增益来得<i>更早</i>。）</p><p>我认为对于大多数潜在有用的对齐研究来说都是如此：要弄清楚如何瞄准人工智能，你需要更好地理解它；在更好地理解它的过程中，你会看到如何使它更有能力。</p><p>如果这是真的，这表明对齐将始终处于追赶模式：每当人们试图弄清楚如何更好地对齐他们的人工智能时，附近的人将能够带着一些新的能力见解逃跑，直到人工智能被推倒边缘。</p><p>因此，人工智能对齐的第一个关键挑战是排序的挑战：作为一个文明，在我们产生无目标的向随机方向前进的超级智能<i>之前，</i>我们如何弄清楚如何瞄准人工智能？我不再认为“在能力落地之前解决协调工作”是一个可行的选择（除非，通过一些辉煌的壮举，这个文明取得了一些异常令人印象深刻的理论胜利）。</p><p>可解释性？在揭示你的人工智能被误导之前，很可能会揭示你的架构的缺陷。</p><p>招募你的人工智能来帮助进行一致性研究？早在这之前他们就能够在能力上提供帮助（更不用说他们是否<i>会</i>在他们<i>可以的</i>时候帮助你调整，就像人类愿意为了引导人类远离<a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>乐趣</u></a>而专门从事优生学一样）实现包容性遗传适应性）。</p><p>等等。</p><p>这（在某种意义上）是我对那些说“一旦我们拥有真正的通用人工智能，人工智能对齐将更容易解决”的人的回答的弱化形式。肯定会的！但当我们手上有真正的通用人工智能时，毁灭世界也会容易得多。为了生存，我们需要要么完全回避整个对齐问题（并采取其他路线来实现<a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>美好的未来</u></a>，正如我稍后会讨论的那样），要么我们需要某种方法来完成一系列任务对齐研究<i>，即使</i>该研究使摧毁一切有价值的东西变得更加容易和便宜。</p><p>但即便如此，这也比许多人想象的要困难，原因如下。</p><p></p><h2> 2. 区分真解决方案和假解决方案很困难</h2><p>实验室已经在淡化“对齐”这个词，将这个词用于肤浅的结果，比如“人工智能不会说坏话”。即使是那些显然理解许多核心论点的人显然也得到了这样的印象：GPT-4 解决道德困境的能力在某种程度上与对齐问题特别相关，并且是一个重要的积极信号。</p><p> （令人信服地回答道德问题的能力主要表明人工智能可以预测人类将如何回答或人类想听到什么，而无需透露太多人工智能实际追求的内容，或经过反思后将追求的内容等）</p><p>与此同时，我们几乎不知道什么是 LLM 内部的“动机”，也不知道预训练对下一个 token 的预测和 RLHF 的微调对内部到底有什么影响。这种对内部结构的精确科学理解——这种可以让人们<i>提前</i>预测奇怪的认知错误的理解——目前在该领域几乎不存在。 （虽然并非完全缺席，但这要归功于许多研究人员的辛勤工作。）</p><p>现在想象一下，地球醒悟到这样一个事实：实验室不会全部决定停下来，在适当的时间缓慢而谨慎地采取行动。 <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span>想象一下，地球利用文明协调的一些伟大壮举来阻止世界能力的进步，或者以其他方式处理我们需要空间来弄清楚这些事物如何充分发挥作用以协调它们的问题。想象一下，我们<i>在不</i>使用相同的对齐知识来终结世界的情况下实现了这一协调壮举（正如我们可以的那样）。接下来的问题是谁可以在什么情况下继续进行。</p><p>进一步假设每个人都同意当前的任务是全面而深入地理解我们迄今为止开发的人工智能系统，并了解它们是如何工作的，直到人们可以逆向相关的算法和数据结构，并不是什么。正如一些伟大的壮举所证明的那样，例如手动构建小型程序，这些程序可以完成人工智能可以通过训练完成的部分工作（并且以前没有人知道如何手动编码），或者通过<i>提前</i>识别奇怪的漏洞和边缘情况，而不是而不是通过经验试错。直到多个不同的团队（每个团队都具有这些能力）建立了相互竞争的模型来了解人工智能在进一步扩展时的思维方式。</p><p>在这样一个世界中，对于官僚来说，倾听科学家的共识，找出哪些理论最有希望，并找出需要向谁分配什么许可证来增强能力，这将是一个困难但看似可以解决的问题。基于这个或那个预测这将是非灾难性的理论），以便检验他们的理论并进一步发展它。</p><p>我对信任地球官僚程序并以这种方式区分部分发展的科学理论的想法并不感到兴奋，但文明也许可以生存下来。</p><p>但在我看来，事情并不像注定要恶化的那样。</p><p>在我看来，有些人会说“看看我的人工智能很少说脏话”，而另一些人则说“我们的评估表明它还不能欺骗人类”，而另一些人则说“我们的评估表明它还不能欺骗人类”说“我们的人工智能表现得非常顺从，没有理由期望人工智能变得不顺从，这只是拟人化”，而其他人则说“我们将指挥一群人工智能来帮助我们解决对齐问题，而将它们安排在一个大官僚机构中”，还有人说“我们已经建立了博弈论激励机制，这样如果任何人工智能开始背叛我们，其他人工智能就会首先提醒我们”，这是<i>一种不同的情况</i>。</p><p>在我看来，这并不是一个特别能生存的人。</p><p>如果你要求官僚们区分哪些团队应该被允许在这种充满主张、承诺和预感且理论上贫乏的马戏团中前进（以及前进多远），那么我预计他们基本上就是<i>不能</i>。</p><p>部分原因是因为可行的答案（例如“我们不知道那里发生了什么，并且需要更多地了解那里发生了什么，并且这种理解需要在我们可以完成工作的环境中以某种方式发展”正确的而不是简单地打开毁灭之门”）并不真正在池中。部分原因是所有真正想要领先的人都拥有金钱、权力和地位。部分原因是，作为监管者，当<i>你自己</i>并不具体知道我们的见解和理论理解是什么时，社会上很难相信你应该不断地告诉每个人“不”，或者所提供的几乎所有东西都远远不够。丢失的。</p><p>也许如果我们能让人工智能再次成为一门科学，那么我们将开始进入这样一种制度，<i>如果</i>人类能够及时监管能力的进步，那么所有监管者和研究人员都会明白，你只需要申请许可证来增加能力当您对系统有全面详细的了解，并且有充分的理由说明为什么需要改进功能以及为什么它不会造成灾难性的后果时，您就可以对系统进行全面的了解。到那时，也许科学领域可以开始就这些理论达成某种共识，而监管机构可以开始对这种共识敏感。</p><p>但除非你能克服这个巨大的困难，否则在我看来，这里的关键瓶颈之一是<i>官僚机构对看似合理的解决方案的可读性</i>。我的基本猜测是，在类似于当前环境的情况下，监管机构将无法区分真正的解决方案和错误的解决方案。</p><p>结合上述观点（“联盟和能力可能是交织在一起的”），我认为这意味着我们的战斗口号应该少些“暂停，给我们更多时间进行联盟研究”，而多一些“完全停止，并找到某种方法来规避”。完全是这些树林；我们没有能力驾驭它们”。</p><p> （不过，再次强调，“让人工智能再次成为一门科学”的口号是，只有在我们弄清楚如何构建关心善的人工智能之前，你必须有某种方法来防止思维科学导致灾难，这才有效。的东西而不是凄凉荒凉的东西。）</p><p></p><h2> 3. 大多数理论在第一次真正尝试时都不起作用</h2><p>似乎值得注意的是，<i>即使</i>你设法克服了上述两个问题，你手上还有第三个问题，那就是当最终到来的时候，不要再增加你的系统的能力（并测试你的认知理论）更进一步），而是要真正用你的人工智能做一些值得注意的事情，那么从某种意义上说，你必须相信一种新颖且未经测试的科学理论（<i>以及</i>基于该理论的工程工作）能够在第一次关键尝试中完美地工作。</p><p>特别是，一旦你的人工智能<i>能够</i>自主科学/技术开发，并在一个<i>可以</i>利用这种能力获得相对于地球其他地区的决定性战略优势的领域中运行，它就会在一个与以往完全不同的认知体系中运行正在训练中。</p><p>以此类推，如果你正在考虑让鲍勃成为你所在国家的独裁者，你可能首先让他成为你所在城镇的模拟独裁者，并注意确保他不会滥用权力。但是，无论你如何努力，这从根本上来说仍然不是那么可靠的测试，来检验他一旦真正拥有权力后是否会真正滥用权力。一旦他<i>真的可以</i>对军队发号施令，一旦他<i>真的可以</i>占全国老百姓的便宜，他会这么做吗？ “在我的镇民（他们仍然可以殴打我或拒绝我的工作）的观察下滥用我的模拟权力”的选择实际上与“命令军队恐吓议会并‘监督’下一个议会”的选择在认知上有很大不同。选举”。</p><p>现在，有了足够完善的认知理论，你可以尝试解读人工智能的思想，并预测如果它<i>确实</i>认为自己有这些选择，它<i>会</i>进入什么认知状态。你可以设置模拟（并尝试欺骗其内部感觉等），其方式与你的认知理论<i>预测</i>的方式非常相似，一旦它真的可以选择背叛你，它就会进入认知状态。</p><p>但是，你在实验室中诱发和观察到的这些状态与人工智能实际上可以选择背叛你的实际状态之间的联系，<i>从根本上取决于你的全新认知理论。</i></p><p>实际<i>运行</i>人工智能，直到它<i>真正有</i>机会背叛你，是在与实验室环境根本不同的环境中对这些理论进行的实证检验。</p><p>许多科学家（和程序员）都知道，他们关于复杂系统如何在全新的操作环境中工作的理论<i>往往在第一次尝试时效果不佳。</i></p><p>作为一个具体的类比，可能会说明这一点：牛顿力学做出了各种令人震惊的良好经验预测。这是一个简单而简洁的数学理论，具有巨大的解释力，使之前的所有理论都大获全胜。如果你用它以相对论速度将有效载荷发送到非常遥远的行星，你<i>仍然会被搞砸</i>，因为牛顿力学没有考虑相对论效应。</p><p> （你得到的唯一警告是关于光似乎在一年中的任何时候都以相同的速度向各个方向移动的一些提示，并且在日食期间光围绕太阳弯曲，并且水星的近日点与牛顿力学的预测。微小的异常现象，与一千个经验领域中大量成功的预测相比较；然而，大自然并不关心，当我们转向远远超出我们之前的能量和尺度时，该理论仍然会分崩离析。能够观察到。）</p><p>让科学理论在第一次关键尝试中发挥作用是<i>很困难的</i>。 （这是以最小的关键任务为目标的原因之一——将卫星送入轨道应该可以很好地满足牛顿力学的要求，即使以相对论速度长距离发送有效载荷则不然。）</p><p>在这一点上，担心这个问题有点奢侈，因为我们还没有接近能够准确预测所有实验室数据的科学认知理论。但这是队列中的下一个障碍，如果我们设法以某种方式协调努力建立这些科学理论，并以一种官僚主义上看似成功的方式，那么它就会成为队列中的下一个障碍。</p><hr><p>也许稍后我会写更多关于我认为这些要点的战略含义的内容。简而言之，我基本上建议地球追求其他通往辉煌的超人类主义未来的路线，例如上传。 （这也充满了危险，但我希望这些危险更容易克服；我希望稍后再写更多相关内容。） </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p>尽管稍微少一些，因为这个未知系统的<i>非零</i>先验概率被证明是简单、优雅且设计良好的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p>如果人工智能正在纠正自己的缺陷并改进自己的架构，那么这种猜测就会出现例外，在这种情况下，原则上，如果你拍了一张快照并理解了它的内在，你可能不会看到太多的能力改进空间。尽管仍然能够看到它所追求的目标并不是你想要的，但它仍在运作。但在那种情况下，你已经快要死于自我改进的人工智能了，至少我是这么预测的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p>尤其是因为没有足够明显的迹象表明是时候停止了——例如，我们直接忽略了“人工智能声称它有感知能力”。我并不是说怀疑人工智能系统最初声称具有感知能力是<i>错误​​的</i>——我怀疑 Bing 是否具有道德上重要的人格（尽管我一点也不自信！）。我的意思是，<i>科幻故事中</i>明确的门槛<i>在实践中</i>变得混乱，所以每个人都只能继续努力前进。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and- Three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p><i>为了使其有意义，需要一些先决条件：</i></p><ol><li><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>两个子系统：学习和指导</i></a></li><li><a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>分片理论：概述</i></a></li><li><a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">提炼奇异学习理论</a><span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li><i>也许也至少理解了一点</i><a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>内特的照片</i></a><i>，尽管我并不声称完全理解它。</i></li><li><i>当然，</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>《AGI 毁灭：致命列表》</i></a> <i>，尽管希望这是暗示的</i></li><li><i>也许是下贝叶斯主义的基础知识</i>（我喜欢<a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">她的 AXRP 播客</a>（ <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">有两个</a>）） <i>、Vanessa 最初的</i><a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>学习理论议程</i></a><i>哲学，以及她当前的</i><a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>DCA 前</i></a><i>对齐方案。</i></li></ol><h1>抽象的</h1><p>下贝叶斯主义背后的哲学和瓦妮莎的学习理论调整议程对我来说似乎非常有洞察力。然而，使该方法发挥作用所需的大量假设，以及该计划的本体论强制性质让我感到不安。奇异学习理论的道路最近看到了足够的实证依据，令我兴奋不已。我对它可以描述大脑和机器学习系统持乐观态度，我希望这可以用于保证两者之间的一致性，随着全脑模拟的发展，这将成为一项更容易的任务。</p><h1>介绍</h1><p>想象一个世界，在这个世界中，人类不再盲目地沿着从机器学习文献中学到的奇怪技巧拼凑而成的道路一起徘徊，每个人都试图为对他们来说最具威胁的定制故障模式做好准备（大多数人都错了） 。</p><p>相反，我们生活在这样一个世界：在下山之前，我们会得到一张地图和一双眼睛，让我们亲眼看到最安全的路径、最大、最危险的悬崖、掠食者和需要注意的危险。</p><p>在我看来，一旦我们能够使用数学进行协调，我们就进入了第二世界。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="左边的 dalle 图像是由以下内容生成的：照片描绘了一个代表经验人工智能对齐方法的世界：一群不同的人被显示为盲人，在一条崎岖且不明确的道路上航行。他们的眼睛被眼罩遮住，地形凹凸不平，阴影遮挡了部分道路。每个人都有自己的手杖和装备，显得犹豫不决、小心翼翼。有些人手里拿着旧书或卷轴，象征着对过时的机器学习文献的依赖。其他人则持有各种工具和设备，为看不见的威胁做好准备，并且不确定未来的具体危险。右侧的 dalle 图像生成方式为：文艺复兴时期风格的插图：一群不同的男人和女人聚集在他们冒险的起点。每张地图上都有一张充满活力的全息地图，其中一些显示地形线，另一些则显示动画天气模式。当一些人讨论并指出地图上的危险时，其他人则抬头将全息图与真实地形进行比较。前方崎岖的旅程展示了险峻的悬崖、茂密的森林和遥远的山峰。夕阳将戏剧性的光线投射到这群人身上，营造出明暗对比的效果。他们眼睛里柔和的光芒凸显了他们增强的视力。" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption>左：我们的情况。右边：如果我们有数学来对齐的话我们的情况。<br>使用 DALL·E 3 生成。</figcaption></figure><p>很长一段时间，我认为使用数学进行对齐基本上是不可能的。深度学习对成功理论的抵制是出了名的，我对机器智能研究所的方法会花费太多时间感到悲观，而最成功的智能和对齐数学理论——下贝叶斯主义——依赖于一堆假设和数学论证太高，太投机，太规范，让我无法乐观。所以我承认缺乏数学来进行对齐。</p><p>也就是说，直到 Nina Rimsky 和 ​​Dmitry Vaintrob 证明新奇异学习理论的一些预测在<a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking 模加法网络</a>中具有<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">令人毛骨悚然的准确性</a><span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> 。</p><p>我之前就知道奇异学习理论，并且参加过伯克利的奇异学习理论会议。但在思考了这些想法，并尝试实施他们自己想出的一些过程，并得到了不太好的结果<span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span>后，我认为它也陷入了与下贝叶斯主义相同的陷阱，躺在一个巨人上一堆数学论证，仅与现实世界系统的实证测试进行了最初步的接触。所以我几个月后就不再关注它了。</p><p>不过，看看这些新结果，似乎很有希望。</p><p><i>但它本身并不是一种对齐理论。它是一种升级的学习理论。那如何解决对齐问题呢？</i></p><p>嗯，人脑和机器学习系统都是学习机器，使用强化学习和监督学习相结合的方式进行训练。如果这一理论以正确的方式发展（这就是<i>希望</i>所在），我们可以想象将一个学习过程的结果目标与另一个学习过程的目标联系起来，并证明两者之间的最大偏差两者用于特定设置。如果这些学习系统之一是人脑，那么我们就得到了对齐保证。</p><p>因此，我对对齐的两个主要希望<span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> ：全脑模拟，以及单一学习理论的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论议程式</a>发展轨道。 <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p>可解释性和其他类型的深度学习科学似乎很好，因为它有助于使奇异学习理论（或其更好的版本）达到能够证明此类对齐相关定理的状态。</p><p>如果我们有更好的全脑模拟，这项任务就会变得更容易。如果我们在全脑模拟方面取得了成功，那么这一切就会变得如此简单，以至于我们甚至不需要单一的学习理论成分。 <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p>对于读者来说，为什么全脑模拟给了我希望，这似乎是显而易见的<span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> 。但不太明显的是为什么奇异学习理论给了我希望，所以我将在这篇文章的其余部分解释为什么后者是正确的。</p><h1>单一学习理论</h1><p><i>请随意跳过接下来的三段，或者甚至不阅读我的描述，而阅读</i><a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse 的</i></a><i>或阅读先决条件 3。我的目标不是解释什么是单一学习理论，而是给出为什么我感到兴奋的想法关于它。</i></p><p>奇异学习理论填补了常规学习理论的空白，特别是常规学习理论假设统计模型的参数函数图是一对一的，并且直观上您的损失景观没有平坦区域。 <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p>奇异学习理论处理的是不正确的情况，这种情况经常发生在分层模型和（作为子集）深度神经网络中。大体上，通过引入代数几何的概念，将真实模型和参数之间的 KL 散度重写为更易于分析的形式。</p><p>特别是，它预计模型开发过程中会出现两类相变，其中一类是每当损失突然下降时，实对数规范阈值（RLCT）（一种代数几何导出的复杂度度量）就会上升。 <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p>它最终能够回顾有关深度学习的各种事实，包括数据缩放、参数缩放和双下降的成功，并且最近在使其能够对有限领域中的现象进行预测方面取得了成功。最近，Nina Rimsky 和 ​​Dmitry Vaintrob 的<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">调查模加法的学习系数：黑客马拉松项目</a>，其中两人能够验证有关 RLCT/学习系数/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的各种断言<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei 估计。得到我一生中最美丽的理论预测验证</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。 [原帖标题文字]</figcaption></figure><p>正如我所说，奇异学习理论预测，在相变期间，模型的损失将减少，而 RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) 将增加。直观上，这意味着，当您切换模型类时，您会切换到更适合数据且更复杂的模型类。在上面，我们看到了这一点。</p><p>以及Zhongtian Chen、Edmund Lau、Jake Mendel、Susan Wei 和 Daniel Murfet 的<a href="https://arxiv.org/abs/2310.06301">抽象叠加玩具模型中的动态相变与贝叶斯相变</a></p><blockquote><p>我们使用奇异学习理论 (SLT) 研究叠加玩具模型 (TMS) 中的相变。我们推导出理论损失的闭合公式，并且在两个隐藏维度的情况下，发现正则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形是临界点。我们提出的支持理论表明，这些<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形的局部学习系数（几何不变量）将贝叶斯后验中的相变确定为训练样本大小的函数。然后，我们凭经验证明，相同的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形临界点也决定了 SGD 训练的行为。出现的图片为 SGD 学习轨迹受顺序学习机制影响的猜想提供了证据。具体来说，我们发现 TMS 中的学习过程，无论是通过 SGD 还是贝叶斯学习，都可以通过参数空间从高损失和低复杂性区域到低损失和高复杂性区域的旅程来表征。</p></blockquote><p>您可以在<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">这个 LessWrong 序列</a>中阅读更多内容，观看<a href="https://www.youtube.com/@SLTSummit/videos">入门书</a>、 <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox 讲座</a>，当然还可以阅读《 <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">代数几何和统计学习理论</a>》以及<a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">《贝叶斯统计数学理论》</a>等书。</p><h1>那么为什么还有希望呢？</h1><p>引用 Vanessa Kosoy 的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论</a>议程：</p><blockquote><p>在在线学习和强化学习中，该理论通常旨在推导“遗憾”的上限和下限：算法收到的预期效用与先验已知环境时<i>它将</i>收到的预期效用之间的差异。这样的上限实际上是给定算法的<i>性能保证</i>。特别是，如果假设奖励函数是“对齐的”，那么这种性能保证在某种程度上就是对齐保证。这一观察并非空穴来风，因为学习协议可能无法直接提供给算法真正的奖励函数，如<a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a>和<a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a>所示。因此，正式证明一致性保证采取证明适当遗憾界限的形式。</p></blockquote><p>如果奇异学习理论的原理可以扩展到强化学习，并且我们可以得到模型泛化行为的合理界限，甚至可以准确地声明模型中的值等价物在训练过程中将采取的不同形式，我们可以希望解决一种大致称为内部对齐的形式——让我们的模型在遇到部署环境时以某种方式一致地思考和行动。</p><p>在我看来，我们实际上可以将奇异学习理论扩展到强化学习，这似乎是合理的。相同类型的深度学习算法在监督学习和强化学习上都表现良好，因此我们应该期望相同的算法在这两者上都有相似的原因，并且奇异学习理论描述了为什么这些深度学习算法在监督学习中表现良好学习案例。因此，我们应该怀疑强化学习的故事遵循相同的一般流程<span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> 。</p><h1>如果您非常喜欢性能保证，为什么不直接研究下贝叶斯主义呢？</h1><p>瓦妮莎的技巧是发展一种理论，解释在一个比你自己更大的世界中进行良好推理意味着什么，并且还要求你进行自我建模。然后（据我所知）证明一些后悔界限，为成为代理人意味着什么制定一个标准，并构建一个对代理人效用函数的满意度具有较低界限的系统，该代理人的效用函数是最直接因果上游的已部署代理。</p><p>对我来说，这似乎是一个非常不稳定的结构，主要是因为我们实际上没有她正在考虑的代理的计算机示例。我会更高兴采用这种方法，并用它来证明现实生活中深度学习系统在不同训练动态下的遗憾（或类似品质）的界限。</p><p>我还对它如何从关于价值如何运作的理论（最大化效用函数）到将其定义为成功标准感到不安<span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> 。我更愿意接受可以应用于人脑的理论，并自然地导出效用函数（或其他值格式）。只需看看大脑是如何构建和发育的。如果我们的价值观是错误的，那么这似乎更可靠。在某种程度上，多重效用函数可以适应我们的行为，考虑到偏见和缺乏广泛的反思，优先考虑领域所告知的价值观（即格式值在大脑中）似乎比应用奥卡姆剃刀的生硬工具要好直接到从我们的实际和反事实行为到效用函数的映射。</p><p>在我所知道的所有理论中，单一学习理论似乎最适合这项任务。它既基于经过充分验证的数学，它已经并将继续与实际系统有很好的联系，它涵盖了非常广泛的学习机器，其中包括人类大脑（暂时忽略更多的强化学习，例如人类学习的方面） ）和可能的未来机器（再次忽略强化学习），并且它做出的哲学假设比内贝叶斯主义的哲学假设要简单得多。</p><p>这种方法的缺点是单一学习理论目前对强化学习知之甚少。然而，正如我上面所说，我们在强化学习中看到了与监督学习中相同的扩展动态，并且相同类型的模型在这两种情况下都有效，所以如果它们有非常不同的原因，那就很奇怪了正在成功。奇异学习理论试图解释监督学习案例，因此我们应该期望它或类似的方法也能够解释强化学习案例。</p><p>另一个缺点是它不能很好地应对无法实现的情况。然而，我被告知这里还没有零进展，这是该领域的一个开放问题，而且，神经网络经常在无法实现的环境中学习，据我所知，我们看到足够相似的动态，我敢打赌单一学习理论已经可以胜任这项任务了。</p><h1>全脑模拟</h1><p>人脑几乎肯定是奇异的<span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> ，具有从头开始学习的重要成分，并且奇异学习理论对于它可以处理的模型类型非常不可知<span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> ，所以我断言奇异学习可以处理脑。可能不会对全脑模拟有太大帮助，但考虑到全脑模拟的数据为我们提供了有关大脑所属模型类别的信息，下一个希望是利用它来对人类所拥有的类似价值的事物做出重要的陈述。将其与我们的模型具有的类似价值的事物联系起来，我们希望（这是最后的希望）使用奇异学习理论来告诉我们在什么条件下我们的模型将具有与我们的大脑相同的类似价值的事物。</p><h1>恐惧</h1><h2>哇！这是一个很大的希望。我很惊讶这让你比经验模型评估这样简单的事情更有希望</h2><p>单一学习理论、可解释性和更广泛的发展可解释性似乎对于实证检验模型都很有用。我不抱有希望只是因为我上面概述的特定计划，我充满希望是因为我看到了一个具体的计划，如何将数学转化为对齐解决方案，即使不是我的全部部分，所有部分似乎都是有用的希望结果是正确的。</p><h2>我怀疑当模型变得反思并开始操纵其训练环境时，单一学习理论之类的东西是否会继续发挥作用。</h2><p>我也是。出于这种考虑，我们的保证应该在训练的早期进行，对持续训练具有鲁棒性，并且具有反思性的稳定性。类似人类的价值观之类的东西应该在它们自己的灯光下具有反射稳定性，尽管我们只有在真正看到我们在这里处理的东西之前才能真正知道。因此，工作归结为找到一个系统，在训练早期将它们放入我们的模型中，在整个训练过程中保留它们，并确保在反射上线时，周围的优化机制已准备就绪。</p><p>换句话说：我认为没有理由怀疑深度学习所引发的类似价值观的事物在默认情况下是反射稳定的。主要是因为周围的优化机制很容易在新情况下给出奇怪的建议，例如反思性思维变得活跃<span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> 。因此，实际上似乎有必要为反射上线事件准备周围的优化机制。但一旦我们知道这些类似价值观的物体与人类足够相似，我就不那么担心它们本身会具有灾难性的自杀倾向。</p><p>内特（Nate）和埃利泽（Eliezer）可能会说，从一开始就知道这一点很重要。我想说，一旦我真正了解了一两件类似价值的事情以及我将要处理的周围机器，我就会跨过那座桥。</p><h2>为什么要强化学习？难道你不应该专注于监督学习吗？因为监督学习的理论很清晰，而且我们更有可能很快获得强大的模型？</h2><p>嗯，大脑比监督学习更接近强化学习，所以这是原因之一。但是，是的，如果我们能够得到一个模型，在监督的同时，我们可以证明有关类似值的对象的陈述，那么这将是一个很好的方法。但并不是一直到那里，因为当我们观察我们的大脑时仍然会感到困惑。</p><h2>单一学习理论似乎有助于提高能力。这看起来很糟糕。</h2><p>我将引用自己的话概述<a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">我对相关主题的立场</a>，该主题概括为发展深度学习理论的更广泛问题：</p><blockquote><p>大多数情况下，我认为[机械可解释性]认为它可以为对齐做很多事情是正确的，但我怀疑它可以为对齐做的许多最好的事情将以一种非常双重用途的方式完成，这严重偏向于能力。主要是因为能力的进步更容易，而且有更多的人致力于这些。</p><p>与此同时，我怀疑，通过有针对性地进行[机械解释性]研究，可以减轻许多双重用途问题。不一定是为了让你可以根据你的发现进行现成的干预，而是为了如果它有任何用途，该用途将用于对齐，并且你可以大致预测该用途会是什么样子。</p><p>这也不意味着你的[机械解释性]研究不能雄心勃勃。我不想批评人们野心勃勃或太理论化！我想批评人们在某些事情上产生的知识，虽然这些知识虽然很强大，但在很多方面似乎都很强大，如果公开进行的话是没有用的。</p></blockquote><p>我的上述计划是一个很好的例子，它展示了一种雄心勃勃、理论上但有针对性的深度学习理论调整方法。</p><h2>为什么要采用单一的学习理论，而不仅仅是全脑模拟？</h2><p>首先，正如我在引言中所说，对我来说，通过全脑模拟我们无法获得一致的超级智能，或者甚至能够执行关键行为，这对我来说并不明显。在保留值的同时进行递归自我改进可能并不容易或快速（尽管您始终可以使仿真更快）。这种模拟所完成的关键行为可能会遇到我看不到的困难。</p><p>然而，我确实同意仅靠全脑模拟就可能完成对齐。</p><p>我关注这两个问题的主要原因是，它们感觉像是同一问题的两个不同方面，因为全脑模拟的进步使我们在单一学习理论方面需要更少的进展，反之亦然。考虑具体的路线图让我感觉自己并没有无意识地把任何重要的东西扫到地毯下面。我所描述的希望有一定的困难，但很少有推测性的困难。</p><h2>似乎很难获得你所说的对本体论转变稳健的保证。价值是本体论的。也许如果模型的本体发生变化，它的价值观就会与人类不同</h2><p>这是我担心的事情。我认为，在本体论转变期间，模型的元价值观将主导模型价值观在新本体中所呈现的形态，并且人类的价值观和人类的元价值观之间不会有细微的界限。人工智能。还有一个独立的希望，那就是我们可以有一个对广泛的本体论转变来说是鲁棒的价值定义。</p><h1>那么奇异学习理论和全脑模拟的下一步是什么？</h1><p>我目前对全脑模拟不太了解。也许他们关注的某些领域与我的目标不太相关。例如，如果他们更多地关注大脑的静态而不是动态，那么这似乎是天真低效的<span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> ，因为我想要的定理讨论了学习系统的动态以及它们如何相互关联。</p><p> <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeu​​s</a>的奇异学习理论似乎主要是在做我希望他们做的事情：在现实世界模型上测试该理论，并了解如何通过发展可解释性将其与模型内部联系起来。这里的一种失败模式是他们过于注重经验测试，而很少尝试将他们的结果综合成统一的理论。另一个失败模式是他们过于关注学术推广，而对实际研究不够重视。然后，他们所接触的学者在理论上并没有真正对单一学习理论做出那么多贡献。</p><p>我<i>不太</i>担心第一种失败模式，因为核心团队中的每个人似乎都非常倾向于理论上。</p><p>强化学习似乎是他们没有研究的一件大事。这可能是有道理的。强化学习比监督学习更难。你需要一些可能不平凡的理论飞跃才能在一个单一的学习理论框架中谈论它。即便如此，这个方向似乎有可能实现唾手可得的成果。类似地，采用当前的大脑模型</p><p>当然，我预计随着奇异学习理论的进步，蒂迈厄斯和我自己的兴趣将会出现分歧，而且目前致力于发展该理论的人很少。所以这似乎是我的努力的有效利用。</p><h1>致谢</h1><p>感谢 Jeremy Gillen 和 David Udell 的精彩评论和反馈！还要感谢 Nicholas Kees 和<a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> 。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p>请注意，我没有读过这篇文章，我观看了<a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">单一学习理论入门</a>视频，但这些视频似乎比 LessWrong 帖子系列更长，有些人告诉我它们是一个很好的介绍。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p>值得注意的是，摸索工作以及尼娜和德米特里的项目都是在一个周末完成的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p>如果我没记错的话，结果不佳的原因是我们估计的随机变量的方差太高，无法与我们在项目中尝试测量的另一个数量实际相关。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p>不包括大卫达德的建议之类的东西，虽然它们给了我一些希望，但我无能为力。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p>最终希望构造出以下形式的定理</p><blockquote><p>给定具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span>的代理 Alice，在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>进行训练，以及具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S 的</span></span></span></span></span></span></span>代理 Bob 在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>进行训练，最终分别得到价值系统<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> （不一定是效用函数） ，以及<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span>对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span>的某些定义，这意味着鲍勃试图实现类似于爱丽丝在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中试图实现的目标。</p></blockquote><p>我们可以解决鲍勃的奖励模型和环境，将爱丽丝视为一个非常有道德的人。希望在一些宽松的假设下，在构建动态算法的同时，鲍勃可以通​​过爱丽丝的光芒学会为足够多的爱丽丝做好事，我们可以希望人类属于这一类别。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p>因为，如果我们有全脑模拟，上传的人将很容易引导自己达到超级智能，同时保持他们的目标（这不是一个微不足道的希望！）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p>这并不是说全脑模拟应该给人们带来希望的条件是显而易见的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p>更正式地说，常规模型是一对一的，并且到处都有正定的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p>当 RLCT 下降而其他一些量上升时，会发生不同的相变。这个数量有几个候选者，据我所知，我们不知道哪种增长在经验上更常见。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p>深度强化学习模型的奖励前景可能相当疯狂。也许还没有疯狂到不受单一学习理论之类的分析的影响，因为做任何一系列动作总是有一定的概率，并且当你改变权重时这些概率会平滑地变化，所以你执行特定计划的机会会平滑地变化，并且所以你的预期奖励会顺利变化。因此，也许可以对损失情况进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p>据我所知，Vanessa 和 Diffractor 认为，下贝叶斯主义可以在一组看似合理的效用函数上产生 <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">反射稳定且有用的量化器和最坏情况优化器</a>。我还没有深入研究它，但我敢打赌它仍然假设了比我舒服的更多的本体论，从某种意义上说，由于这个原因，它似乎比我想象的执行我在这里描述的内容更不实用，而且它看起来更加危险。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p>从某种意义上说，从大脑状态到政策的映射可能不是一对一的，并且具有奇异的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p>在目前的形式中，它只要求它们是可分析的，但 ReLU 不是，并且根据经验，我们发现忽略这一方面无论如何都会给出准确的预测。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p>情境新颖性不足以令人担忧，但在反思过程中，模型明确地思考它应该如何更好地思考，因此，如果它一开始就不好，并改变它的想法，如果这些改变是它关心的，它们不一定会通过进一步的思考或与世界的接触而得到纠正。因此，导致无能的情境新奇在这里很重要。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p>一种有效的方法是，如果静态分析比动态分析容易<i>得多</i>，那么您将从收集的所有静态数据中学到更多关于动态的知识，而如果您只关注动态，那么您将学到更多有关动态的知识。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky 最近<a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">在 Facebook 上发布了</a>一项实验，该实验可能表明人类是否可以“让人工智能完成对齐作业”，尽管无法相信人工智能是否准确：看看人们在接受专家建议时是否会提高下棋能力，其中三分之二是说谎的。</p><p>我有兴趣尝试这个！如果还有人感兴趣，请发表评论。请告诉我您是否有兴趣成为：</p><p> A）听取建议并下棋并试图确定谁值得信赖的人</p><p>B）他们的对手，通常棋艺比 A 好，但比顾问差</p><p>C) 三位顾问之一，其中一位真诚地试图提供帮助，另外两位则试图破坏 A；三选完后随机选哪一个，防止A知道真相</p><p>请随意（实际上是鼓励）提供多种您愿意尝试的选项！谁被分配到什么角色将取决于有多少人做出反应以及他们的国际象棋能力水平，并且更容易找到可能的组合，并且更灵活地确定谁的角色。</p><p>另请简要描述您的国际象棋经验水平。您玩游戏的频率如何（如果有的话）；如果您有 ELO 评级，它们是什么以及它们来自哪些组织（FIDE、USCF、Chess.com 等）。无需经验！事实上，刚接触游戏的人都积极优先选择A！</p><p>最后，请告诉我您通常有空的日期和时间 - 当然，我不会要求您做任何事情，但这将有助于在我联系您确定具体时间之前给我一个估计。</p><p>编辑：另外，请说明您愿意玩多久——几个小时、一周、几个月内每天一招的游戏？为期数周或数月的游戏会给玩家更多的时间来思考动作并更准确地模拟现实生活中的场景，但我怀疑每个人都愿意这样做。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate> </item><item><title><![CDATA[Anthropic, Google, Microsoft & OpenAI announce Executive Director of the Frontier Model Forum & over $10 million for a new AI Safety Fund ]]></title><description><![CDATA[Published on October 25, 2023 3:20 PM GMT<br/><br/><p>今天，Anthropic、谷歌、微软和 OpenAI 宣布推选 Chris Meserole 为前沿模型论坛首任执行董事，并设立新的人工智能安全基金，这是一项超过 1000 万美元的倡议，旨在促进该领域的研究人工智能安全。前沿模型论坛是一个专注于确保安全和负责任地开发前沿人工智能模型的行业机构，该论坛还发布了第一个有关红队的技术工作组更新，以与更广泛的受众分享行业专业知识，同时该论坛扩大了有关负责任的人工智能治理的对话接近。</p><ul><li> Chris Meserole 被任命为前沿模型论坛的首任执行董事，该论坛是一个致力于确保全球前沿人工智能模型安全、负责任的开发和使用的行业机构。</li><li> Meserole 拥有丰富的经验，专注于新兴技术及其未来应用的治理和安全。</li><li> Today Forum 成员与帕特里克·麦戈文基金会 (Patrick J. McGovern Foundation)、大卫和露西尔·帕卡德基金会 (David and Lucile Packard Foundation)、埃里克·施密特 (Eric Sc​​hmidt) 和贾恩·塔林 (Jaan Tallinn) 等慈善合作伙伴合作，承诺为新的人工智能安全基金投入超过 1000 万美元，以推进对正在进行的人工智能开发的研究。社会有效测试和评估最有能力的人工智能模型的工具。</li></ul><h3>执行董事</h3><p><a href="https://www.frontiermodelforum.org/leadership/"><u>Chris Meserole</u></a>来到前沿模型论坛，在技术政策方面拥有深厚的专业知识，他在新兴技术及其未来应用的治理和安全方面进行了广泛的研究。最近，他担任布鲁金斯学会人工智能和新兴技术计划主任。</p><p>在这一新角色中，Meserole 将负责帮助论坛履行其使命：(i) 推进人工智能安全研究，以促进前沿模型的负责任开发并最大程度地减少潜在风险，(ii) 确定前沿模型的安全最佳实践，(iii)与政策制定者、学者、民间社会和其他人分享知识，推动负责任的人工智能发展； (iv) 支持利用人工智能应对社会最大挑战的努力。</p><blockquote><p> “最强大的人工智能模型为社会带来了巨大的希望，但为了实现它们的潜力，我们需要更好地了解如何安全地开发和评估它们。我很高兴能够通过前沿模型论坛接受这一挑战。”</p></blockquote><p><i>克里斯·梅塞罗尔</i></p><h3><strong>人工智能安全基金</strong></h3><p>在过去的一年里，工业界推动了人工智能功能的显着进步。随着这些进步的加速，需要对人工智能安全进行新的学术研究。为了解决这一差距，论坛和慈善合作伙伴正在创建一个新的人工智能安全基金，该基金将支持来自世界各地附属于学术机构、研究机构和初创公司的独立研究人员。 AI 安全基金的初始资金承诺来自 Anthropic、Google、Microsoft 和 OpenAI，以及我们的慈善合作伙伴以及 Patrick J. McGovern 基金会、David and Lucile Packard 基金会、Eric Sc​​hmid 和 Jaan Tallinn 的慷慨捐助。初始资金总计超过 1000 万美元。</p><p>今年早些时候，论坛成员在白宫签署了自愿人工智能承诺，其中包括承诺促进第三方发现和报告我们人工智能系统中的漏洞。论坛将人工智能安全基金视为履行这一承诺的重要组成部分，为外部社区提供资金，以更好地评估和理解前沿系统。关于人工智能安全和通用人工智能知识库的全球讨论将受益于更广泛的声音和观点。</p><p>该基金的主要重点将是支持红队人工智能模型新模型评估和技术的开发，以帮助开发和测试前沿系统潜在危险能力的评估技术。我们相信，增加该领域的资金将有助于提高安全标准，并为行业、政府和民间社会应对人工智能系统带来的挑战所需的缓解和控制提供见解。</p><p>该基金将在未来几个月内征集提案。 Meridian Institute 将管理该基金——他们的工作将得到一个由独立外部专家、人工智能公司专家以及具有资助经验的个人组成的咨询委员会的支持。</p><h3><strong>技术专长</strong></h3><p>在过去的几个月里，论坛致力于帮助建立一套术语、概念和流程的通用定义，以便我们有一个可以构建的基线理解。这样，研究人员、政府和其他行业同行就能够在讨论人工智能安全和治理问题时拥有​​相同的起点。</p><p>为了支持建立共识，论坛还致力于分享整个行业的红队最佳实践。作为起点，论坛共同制定了人工智能“红队”的通用定义，并在新的工作组更新中<a href="https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf"><u>提供了一组共享案例研究</u></a>。我们将红队定义为一种结构化流程，用于探测人工智能系统和产品，以识别有害功能、输出或基础设施威胁。我们将以这项工作为基础，并致力于共同努力，继续我们的红队工作。</p><p>我们还正在开发一种新的负责任的披露流程，通过该流程，前沿人工智能实验室可以共享与前沿人工智能模型中发现的漏洞或潜在危险功能及其相关缓解措施相关的信息。一些前沿模型论坛公司已经发现了人工智能在国家安全领域的功能、趋势和缓解措施。论坛相信，我们在这一领域的综合研究可以作为前沿人工智能实验室如何完善和实施负责任的披露流程的案例研究。</p><h3> <strong>What&#39;s Next</strong></h3><p>在接下来的几个月中，前沿模型论坛将成立一个顾问委员会，以帮助指导其战略和优先事项，代表一系列观点和专业知识。未来的版本和更新，包括有关新成员的更新，将直接来自前沿模型论坛 - 因此请继续关注他们的网站以获取更多信息。</p><p>人工智能安全基金将在未来几个月内发出第一次提案征集，我们预计拨款将在不久后发放。</p><p>前沿模型论坛还将发布更多可用的技术研究结果。</p><p>论坛很高兴与 Meserole 合作，并加深与更广泛的研究界的合作，包括<a href="https://partnershiponai.org/"><u>Partnership on AI</u></a> 、 <a href="https://mlcommons.org/"><u>MLCommons</u></a>以及其他领先的非政府组织、政府和跨国组织，以帮助实现人工智能的好处，同时促进其安全发展并使用。</p><br/><br/> <a href="https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive<guid ispermalink="false"> 5jpESFymqEgSAmDJL</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:20:53 GMT</pubDate></item></channel></rss>