<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 18 日星期五 20:11:26 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Meetup Tip: Board Games]]></title><description><![CDATA[Published on August 18, 2023 6:11 PM GMT<br/><br/><h2>概括</h2><p>许多聚会都会玩棋盘游戏或纸牌游戏。出去玩游戏可能是一种有趣的共度时光的方式，而组织棋盘游戏聚会也异常容易，因为游戏承担了大部分繁重的工作。有些游戏比其他游戏更适合这个目的；我相信最适合聚会的游戏都有很短的回合，有很多“死”时间，你需要等待某人采取行动，并且适合两名以上玩家。游戏可以是一场很棒的游戏，但对于聚会来说却是一个糟糕的选择。这篇文章的总结是，如果您举办棋盘游戏聚会，我认为如果您带来符合这些标准的游戏并避免带来不符合这些标准的游戏，那么聚会会更好。</p><p>我不太相信这些是直接的改进，但我通常对这些技巧抱有信心。</p><p>还在读书吗？太酷了，让我们用比严格必要的词更多的词来谈论游戏吧。</p><h2>细节</h2><p>为什么游戏成为流行的聚会主题？我相信这是因为棋盘游戏提供了默认活动。在棋盘游戏聚会上接近某人并询问他们是否想玩并不奇怪，这样可以更轻松地介绍自己。你不必担心没有什么可说或谈论的，因为你可以谈论游戏。他们会定期打断别人的话，从而更容易改变话题。它们也可以很有趣。忘记这一点就糟糕了。作为组织者，他们也很容易运作。你只需选择一个时间和地点，带着几个纸板箱出现，然后聚会就开始了。我将简单聚会的顺序命名为“盒子里的聚会”是有原因的，因为我希望每个聚会对于组织者来说都像棋盘游戏聚会一样简单。</p><p>如果你将一款游戏视为一款游戏，那么它所具备的品质与适合聚会的游戏所具备的品质并不相同。在这篇文章中请记住这一点；我将使用很多例子，其中的区别很重要。走吧。围棋是一项有深度且深受喜爱的游戏。如果人们决定在聚会上玩围棋，那么两个人可能会与其他人分开，花一个小时专注于此。这会阻碍他们与其他人交往，甚至他们之间的交谈也会受到限制，因为在任何时候他们中的一个人都在专注于下一步行动。这并不理想。</p><p>比较《走心》。在四人红心大战游戏中，大多数时候三名玩家都在等待第四名玩家出牌。当你等待时，在轮到你之前你不能做太多的策略。这意味着不轮流的三名玩家可以自由地互相交谈。由于每一轮都很快，所以有人很容易决定他们已经玩完了并起身，或者新人加入游戏。当有人加入或离开时，会有一些摆弄牌组的情况，但是四人红心游戏可以变成五人红心游戏，第五个人不需要坐在外面。</p><p>小心同时进行两场长时间的比赛。想象一下，八个人分成两半，玩两场需要一个多小时的游戏，每场有四名玩家。第一个完成的游戏注意到另一组仍在继续，因此他们开始新的游戏。第二组完成后，注意到另一组刚刚开始，所以他们开始新的游戏。晚上就这样过去了，两组人都未能混合在一起。这有点像球形奶牛的例子，足够大的群体通常会有足够的活动来自然地避免这种情况，但要留意多个长时间的游戏将人们锁定在他们的群体中。</p><p>这对于聚会初期的新来者来说最为重要。由于您的与会者很少会在聚会开始时全部到达，因此我认为值得从更快的游戏开始，这样您就可以在新人出现时更快地调整小组。不要从一场持续数小时的游戏开始，这样就锁定了您的前四名到达者；第五名可能很快就会出现！</p><p>这为我最常见的超过两名玩家的标准例外创造了空间。我喜欢聚会中的 Tak（一种两人棋盘游戏，通常在五分钟左右结束），因为我可以玩最新到达的游戏，让他们玩下一个出现的人，当序列中的第三个出现时，我们就足够了适合更多玩家的游戏。如果您可以轻松地与其他人聊天并同时能够快速玩游戏，那么此方法效果最佳。</p><p>我将扩展最后一句话。对于组织者来说，一项巧妙而有用的技能是能够在进行复杂且不相关的对话的同时玩游戏。只要玩得合法，玩得不好也没关系。换句话说，作为组织者，经常输球但有更好的对话是一个很好的权衡，只要你不玩得太糟糕而让其他玩家变得无聊。这是我在人们出现时与他们互动的关键部分，让我能够占据一两个人的时间，否则他们会站在那里等待足够多的人玩游戏。</p><p>许多本来就太慢并且将玩家锁定太长时间的游戏可以通过添加国际象棋时钟来加快速度。计时游戏的优点是通常会在同一时间结束，这可以解决两组周期不同的问题。两人游戏可以变成有趣的四人游戏，方法是让两队球员轮流进行，并且不允许球队讨论策略； Adam 和 Bella 玩白棋，Carl 和 Darla 玩黑棋，所以回合顺序是 Adam Carl Bella Darla Adam Carl 等。把它放在一个激进的时钟上可以消除原本在停滞时间内发生的对话，但它也可能非常有趣。</p><p>游戏也可以成为话题。如果您正在 LessWrong 上阅读有关聚会和棋盘游戏的内容，理性基数就是您的受众可能感兴趣的一个简单例子。很多人喜欢游戏，很多人喜欢理性，将两者结合起来是件好事。例如，不要羞于引入校准琐事或扑克并谈论赔率。</p><p>人们可能会想带来自己的游戏。这很棒，因为它减轻了作为组织者的压力。这确实意味着某些游戏的选择将超出您的控制范围。如果有人拿出《暮光之城帝国》（一款非常长的游戏，我不会推荐用于典型的棋盘游戏聚会）或类似游戏并开始招募玩家，我发现值得对较短的游戏如何更适合聚会发表评论，然后我让它发生。如果人们像这样分开也不错，特别是如果他们对游戏感到兴奋，我只是认为这不如聚会那么好。</p><p>我尝试过为棋盘游戏活动设定主题，例如“纸牌游戏”或“合作游戏”，甚至选择一种游戏并宣布我们将从该游戏开始。对出席率的影响似乎是温和的积极影响。我只这样做过几次，但唯一一次我的参加人数比一般的棋盘游戏聚会少，是当我选择一款游戏作为开始时，这是我编写并想要进行测试的游戏。</p><p>当然，您可以举办一场只讨论一款游戏的聚会。公园里的国际象棋棋手聚会、每隔一周举行一次的麻将俱乐部、万智牌周五之夜万智牌活动以及当地酒吧定期举行的问答之夜都是这样的例子。如果您对那一款游戏感到兴奋，那就去玩吧。这也是战役或遗留游戏成为更可行选择的空间，但即使在这里我也会有点小心；一个群体的规模保持完全相同的情况是很少见的。有人会搬出城镇，或者想带上他们的新伴侣。国际象棋俱乐部不在乎有多少人出现，只要有足够的棋盘即可，格洛姆黑文可以支持的玩家范围相当有限。</p><p>与大多数聚会相比，游戏聚会更能因路人可见而受益。我还没有看到有人注意到公园里有一个读书俱乐部在说话并加入进来，但我见过有人走过一排棋桌，然后回头问一个坐在棋盘上的孤独的人是否正在寻找游戏。我有一次在一个意想不到的地方看到了周五晚上魔术队的标志，并记下下周去参加我最喜欢的比赛之一。人们知道国际象棋、魔术和琐事是如何运作的，并且在没有明确邀请的情况下出现在他们面前会更自在。您当然可以提供明确的邀请；写着“棋盘游戏聚会，周二下午 6 点”的标牌可以聚集临时参加者。</p><p>棋盘游戏聚会比许多其他类型的聚会更适合户外活动。如果你想在外面做，我建议你带一条毯子放在地上。与在草地或泥土上玩耍相比，这将创造一个更平坦、更舒适的玩耍空间。尽量选择一个防风良好的地方，否则风会刮起卡片或吹倒小迷你人物。最后，留意云层并制定下雨时的应对计划；人是防水的，而 Ticket To Ride 则不然。</p><p>最后一点：我在这里谈论的是棋盘游戏和纸牌游戏，通常是围着桌子或在草地上围成一圈玩的。尽管您的场地要求明显会发生变化，但其中大部分建议仍然适用于体育或体育比赛。当地运动的即兴比赛是全世界最受欢迎的聚会活动。然而，我要巧妙地观察到，参加者的体力和舒适度可能会有很大差异，特别是如果他们主要是根据是否阅读《LessWrong》而不是是否加入当地体育联盟来选择的。</p><h2>快速技巧</h2><p>我最喜欢的聚会游戏的简短列表：Apples to Apples、Hanabi、Tak、Outrangeous、Deep Sea Adventure。对于体育游戏，Zip Zap Zoom、飞盘、夺旗以及杂耍传球技巧都效果很好，但当我可以的时候，我喜欢足球。</p><p>假钱可以从现有游戏中重新利用，可以自己便宜地购买，并可以启用一些我通常不会在休闲聚会上玩的其他游戏。扑克是这里的主要例子，但是任何带有赌博成分的东西都比记分卡上的抽象分数更有趣，使用实体但假的钱。我建议不要使用真钱，尤其是当所有玩家之间的总价值大于一美元时。</p><p>我注意到，如果我下棋积极且快速（在回合传给我后立即移动我的棋子），就会让人们措手不及，并鼓励他们快速移动。这是否是我想做的事情各不相同；在聚会的早期部分，我通常会这样做，这样我可以缩短回合时间，并更容易混合小组，在聚会的后期部分，我通常会缓慢而刻意地玩，以便给人们时间聊天。</p><br/><br/><a href="https://www.lesswrong.com/posts/SZ2n5cjp6uZs8kfTT/meetup-tip-board-games#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SZ2n5cjp6uZs8kfTT/meetup-tip-board-games<guid ispermalink="false"> SZ2n5cjp6uZs8kfTT</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 18 Aug 2023 18:11:58 GMT</pubDate> </item><item><title><![CDATA[AI labs' requests for input]]></title><description><![CDATA[Published on August 18, 2023 5:00 PM GMT<br/><br/><p>人工智能实验室有时会公开征求有关其行动、产品和人工智能未来的意见。请参阅 OpenAI<a href="https://openai.com/blog/democratic-inputs-to-ai">对 AI</a>和<a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf"><u>ChatGPT 反馈竞赛</u></a>的民主投入，以及针对安全漏洞的 bug 赏金计划（ <a href="https://openai.com/blog/bug-bounty-program">OpenAI</a> 、 <a href="https://bughunters.google.com/about/rules/6625378258649088/google-and-alphabet-vulnerability-reward-program-vrp-rules">Google</a> 、 <a href="https://www.facebook.com/whitehat">Meta</a> ）。我想收集这些；请回复并提供您知道的其他示例。我还对实验室应请求输入内容或应如何执行此操作的想法/建议感兴趣（例如<a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">模型输出的错误赏金</a>）。</p><hr><p>实验室还非公开地寻求意见。例如，实验室使用外部红队和<a href="https://evals.alignment.org/blog/2023-08-01-new-report/">模型评估</a>，<a href="https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety">与生物安全专家合作以了解近期人工智能系统如何为生物武器做出贡献</a>，并<a href="https://cdn.openai.com/papers/gpt-4.pdf#page=59">咨询外部预测人员</a>。已经提出了各种类型的外部审计。我也有兴趣收集这种非公开意见的示例和想法，但兴趣不大。</p><br/><br/><a href="https://www.lesswrong.com/posts/5e3bXsBQLusuArzmN/ai-labs-requests-for-input#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5e3bXsBQLusuArzmN/ai-labs-requests-for-input<guid ispermalink="false"> 5e3bXsBQLusuArzmN</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Fri, 18 Aug 2023 17:00:26 GMT</pubDate> </item><item><title><![CDATA[6 non-obvious mental health issues specific to AI safety.]]></title><description><![CDATA[Published on August 18, 2023 3:46 PM GMT<br/><br/><h1><br>介绍</h1><p>我是一名心理治疗师，我帮助人们从事人工智能安全工作。我注意到这个群体特有的心理健康问题模式。这不仅仅是宿命论，还有更多不那么明显的事情。</p><p>如果您遇到与人工智能安全相关的心理健康问题，请随时发表评论以及对您有帮助的事情。您也可以在评论中支持其他人。有时，这种支持会产生很大的影响，人们会觉得自己并不孤单。<br><br>这篇文章中的所有示例都是匿名的，并且以一种无法识别背后特定人员的方式进行了更改。</p><p></p><h2>人工智能安全是一个相当不寻常的领域</h2><p>这篇文章中描述的问题的出现是因为人工智能安全不是一个普通的工作领域。<br><br>人工智能安全社区内的许多人认为这可能是最重要的工作领域，但公众大多不太关心。此外，该领域本身竞争非常激烈，新移民往往很难找到工作。<br><br>没有人真正知道我们何时会创建通用人工智能，以及我们是否能够保持它的一致性。如果我们不能协调通用人工智能，人类可能会灭绝，即使我们成功了，也将从根本上改变世界。<br><br></p><p></p><h1>图案</h1><h2>AGI 要么导致厄运，要么创造一个乌托邦。其他一切似乎都不重要，毫无意义。</h2><p> Alex 是一名机器学习工程师，在一家与衰老作斗争的初创公司工作。他认为AGI要么会毁灭人类，要么带来一个乌托邦，除此之外它还会停止衰老，所以Alex认为他的工作毫无意义，于是辞职了。他有时也会问自己“我应该投资吗？我应该锻炼吗？我是否应该用牙线清洁牙齿？这一切似乎毫无意义。”</p><p>没有人知道后 AGI 世界会是什么样子。所有的预测都是猜测，很难判断任何与人工智能安全无关的行动是否有意义。这种不确定性会导致焦虑和抑郁<br><br>这些问题是生命无意义的存在问题的加剧版本，缓解这些问题的方法是重新发现这个最终没有意义的世界的意义。</p><p></p><h2>我不知道我们什么时候会创建AGI，也不知道我们是否能够调整它，所以我觉得我无法控制它。</h2><p>贝拉是一个焦虑的人，她最近对人工智能安全产生了兴趣，她意识到没有人确切知道如何调整通用人工智能。<br><br>她觉得AGI可能会带来极大的危险，而她却无能为力。她甚至不明白我们还有多少时间。一年？ 5年？这种不确定性让这里更加焦虑。如果起飞速度如此之快以至于没有人明白发生了什么怎么办？<br><br>贝拉正在见一位心理治疗师，但他们认为她的恐惧是不合理的。这没有任何帮助，只会让贝拉更加焦虑。她觉得就连她的治疗师也不理解她。</p><p></p><h2>人工智能安全是我生活的重要组成部分，但其他人并不太关心它。我感到疏远。</h2><p> Chang 是一位机器学习科学家，在人工智能实验室从事机械可解释性研究。 AI安全消耗了他的一生，并成为他身份的一部分。他经常在 Twitter 上查看 AI 安全影响者，花大量时间阅读 LessWrong 并观看 AI 播客。他甚至用回形针纹了一个纹身。<br><br>张住在主要人工智能安全中心之外，他感到有点孤独，因为没有人亲自谈论人工智能安全。<br><br>最近他参加了他姨妈的生日聚会。他谈到了与家人的团结。他们对这个话题有些好奇，但也没有那么在意。张感觉他们就是不明白。</p><p></p><h2>致力于人工智能安全是如此重要，以至于我忽视了生活的其他部分并精疲力竭。</h2><p>德米特里是一名本科生。他认为AI安全是他一生中最重要的事情。他要么考虑人工智能的安全，要么一直在研究它。他一生中从未如此努力地工作过，他很难意识到忽视生活的其他部分以及未能区分人工智能安全会直接导致倦怠。当这种倦怠发生时，一开始他不明白发生了什么，并因为无法从事人工智能安全工作而变得沮丧。</p><p></p><h2>从事人工智能安全工作的人非常聪明。我认为我还不够好，无法做出有意义的贡献。</h2><p>以斯拉最近从一所大学毕业，他在那里研究变压器。他想从事人工智能安全工作，但似乎主要人工智能实验室和人工智能安全组织中的每个人都非常有才华，并且受过良好的教育。以斯拉对此感到非常害怕，甚至很难尝试做某事。<br><br>过了一段时间，他终于申请了一些组织，但到处都被拒绝，其他人也分享了类似的经历。似乎每个职位都有几十个聪明的年轻人应聘。<br><br>他感到没有动力，他还需要支付账单，所以他决定在非人工智能安全公司工作，这让他很难过。</p><p></p><h2>很多聪明人认为人工智能对齐并不是什么大问题。也许我只是反应过度了？</h2><p> Francesca 是一位在学术界工作的计算机科学家。她熟悉机器学习，但这不是她工作的重点。她还认为，存在风险的论据是可靠的，她对此感到担忧。<br><br> Francesca 很好奇顶级机器学习科学家对人工智能安全的看法。他们中的一些人认为 X 风险很严重，而其他许多人则不太担心这些风险，他们认为人工智能的毁灭者是怪人。<br><br>弗兰西斯卡因此感到困惑。她仍然认为存在风险的论据是可靠的，但社会压力有时让她认为整个一致性问题可能没有那么严重。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety<guid ispermalink="false"> tplzjWqG2iyEgMGfJ</guid><dc:creator><![CDATA[Igor Ivanov]]></dc:creator><pubDate> Fri, 18 Aug 2023 15:46:09 GMT</pubDate> </item><item><title><![CDATA[That time I went a little bit insane]]></title><description><![CDATA[Published on August 18, 2023 6:53 AM GMT<br/><br/><p>这是一个关于我的故事。这展示了“异国情调的自我修改”的后果，并不是作为支持或反对这种做法的建议，只是一个数据点。权衡你的风险。</p><p> TL;DR：您可能错误地确定了您的最终目标。不要明确地、专门地针对它们进行优化。另外，在进行实验时，不要忽视实验的“外部视野”。</p><p> —</p><p>我很年轻，有神经可塑性，正在尝试弄清楚我的生活。我有很多有组织的空闲时间（感谢学校系统）。我决定尝试在思想和目标层面上提高自己，因为我认为这是我的基本目标。</p><p>这种认同是我生命中看似意义深远的时刻，我想“啊哈！这是我所有效用的根源！所以我应该优化它，我的生活就会很理想”。</p><p>我认识到我是自己思想的建筑师，并运用了这些力量。但我的蓝图绘制得太仓促，错过了支撑结构，导致整个建筑在达到足够的重量和复杂性后就倒塌了。</p><p>我天真的、有损的效用压缩让我崩溃了。但我还不知道，我正兴奋地想象着我的指数级飞跃，成为越来越完美的自己。</p><p> —</p><p>我开始冥想，主要是通过关注我的身体和感知，并试图改变它，以及操纵我的思维模式。</p><p>我开始更加形象化、更加抽象地思考。我把时间花在阅读数学和物理以及对世界进行建模上。</p><p>我想要能够控制“我自己”，而这里的目标就是我的情绪。我发现我可以想象幸福，然后让它冲刷我，一种平静的快乐爆发。我进入了生动愉快的场景，想象自己漂浮在风中，沐浴着阳光。我经常这样做，因为我可以。</p><p>我不再表面上关心别人，以为我可以阻止他们的影响，而且我很善于操纵，冷酷无情地向自己证明我可以。具有讽刺意味的是，在试图实现更多多样性的过程中，我的情感复杂性崩溃了。</p><p>我很冲动，通过做任何我能想到的发行之外的事情来寻找新奇，同时试图通过改变与谁交谈以及从哪里获得信息来在我的想法中播下更多的随机性。</p><p>我开始自上而下地进行理论分析，思考新的计算范式和物理模型等结构，然后尝试将它们应用到现实中。我转动我的心灵之眼，我一直在思考。</p><p>我以前的目标失去了根基，我所关心的只是增加，第一顺序，我的能力和我生活中的变化。这意味着功课会被搁置，自动消失或极具创造性地完成（有时会因为没有严格遵守规则而失去学分）。我无法证明花时间做我认为我知道<i>该怎么</i>做的事情是合理的，做这些事情的智力<i>能力</i>是我唯一考虑的因素，因为这就是我认为能力提高的来源。我还非常注重元改进，塑造解决问题的方法，而不是仅仅以最“有效”的方式解决问题，试图超越局部最大值。</p><p>我的每一个想法和采取的行动都是由我的多样性/元改进目标证明的。发生的事情我都会反思，如果我认为不合适就不会做。这是一个永恒的、支配着我的总体范式。</p><p>然后我放手，不再有结构，不再有意义。我的生活感觉空虚，我唤起的快乐是超然的，我也是。我转向空虚、低比特率的社交互动或社交媒体，在我关闭理性和意识时挖掘那些微小的多巴胺爆发。</p><p>一阵阵的清晰突然出现，我会锻炼，或继续一些能力或新奇的增加计划，或拉出缓存的思想线索或图像。但这些时间变得越来越短，屈服于夜晚的深深空虚。</p><p>每天，我都在“生产力”和享乐主义和虚无主义的奇怪混合中循环，用“哈哈，我的大脑很奇怪”的语气向朋友评论这一点，不知何故没有反思为什么或意识到这有多糟糕。</p><p>然后我睡着了，在做了更多毫无意义的心理瑜伽之后，试图利用睡前的效果，然后醒来，重复这一切。</p><p>我的思想在压力下支离破碎，我的身份受到鞭打，但内部没有任何损坏的迹象。我有一个梦想，想象吸收并成为另一个人，拥有他们的目标、世界观和思想。这让人不舒服，而且奇怪地深刻，我用它作为可塑性的积极指标，而不是警告。</p><p>我持续了一个月。</p><p>一天晚上，我崩溃了，想知道这一切的意义是什么，为什么我无法控制自己，为什么我所有的快乐都感觉如此空虚，想知道我的世界观取得了哪些实际进展。</p><p>我趴在地上轻声哭泣，想知道我的生活发生了什么事。</p><p>我以为我已经找到了人生中一个简单的“目标”，那就是自我提升，但追求它却成了一场噩梦。</p><p>我是一个充满了不安、情绪化和不确定性的强大力量，最终以外部视角看待我一直在做的事情，直到那时我才明白这一点。我吸收并体现了一种过于浓缩、过于狭隘、过于偏执的世界观。</p><p>我应该把这当作一个实验，但我完全相信这是我的新生活方式，只要我继续努力，我就走在生产力最大化的道路上。</p><p>我最终非常明确地摆脱了这个困境。当我躺在床上思考我一直在做的事情及其原因时，我能感觉到我的目标“重新调整”（在讨论之后，我终于有了一个外部观点，能够质疑我的世界的功效和疯狂）。我自由了。</p><p>这是一种巨大的解脱。我立即感觉更正确、更舒服、更轻松、更快乐。我以前的世界观是紧张和严格的，就像一把尺子挂在我的背上，太长而且角度不对。</p><p>我又回到了这些目标，对自己能够回来的能力感到满意，因为我已经感受到了差异，测试了我来回的能力，它是如此脆弱，如此做作。</p><p>事实证明，我建立一个基本状态是有原因的，当动机不明确且“任意”时，对其进行渐进但极其激进和坚定的改变并不是一个净积极的结果。</p><p>我花了两周半的时间才再次感到舒服，忽略了“目的”的深渊，并接受了以荒诞主义为基本状态对当时的我来说是最健康的事情。</p><p>这很有趣。我解锁了一些有趣的新结构、能力和感觉。但我“被困”在那里，处于一种主观上不太基线的令人愉悦的幻觉中。如果我不相信我实际上正在完美地追求我的最终目标，它可能不会起作用，但我应该尝试一下，保持局外人的观点。</p><p> —</p><p>我仍然不知道“我”的目标是什么。自我提升、新鲜感和幸福感发挥着巨大作用，但它们还不够。</p><p>似乎他们的服务方式很重要，实际的产出也很重要，而不仅仅是复杂、迷人的想法。人很重要。压抑关怀是不自然的。</p><p>因此，我的关怀、合作、“小心”做出改变的弧线开始了，并内置了安全感，更接地气的想法，所以我不会让自己陷入个人地狱，在当地感觉就像天堂。</p><p>祝那些玩弄头脑的人好运，并且要非常小心，以免陷入一种自我维持的、全球性的不舒服的状态。还有一点（毫无成效的）疯狂。</p><br/><br/> <a href="https://www.lesswrong.com/posts/L9q5iFJAggvoWNrRf/that-time-i-went-a-little-bit-insane#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/L9q5iFJAggvoWNrRf/that-time-i-went-a-little-bit-insane<guid ispermalink="false"> L9q5iFJAggvoWNrRf</guid><dc:creator><![CDATA[belkarx]]></dc:creator><pubDate> Fri, 18 Aug 2023 06:53:51 GMT</pubDate> </item><item><title><![CDATA[When discussing AI doom barriers propose specific plausible scenarios]]></title><description><![CDATA[Published on August 18, 2023 4:06 AM GMT<br/><br/><p> TLDR：在解决“人工智能如何做 X”问题时，更喜欢涉及人类当前可以做的事情的简单存在证明，而不是像纳米技术或完全通用的“人工智能将足够聪明来弄清楚”之类的看似“神奇”的事情。总而言之：结束</p><p>在人工智能风险对话中，通常会提出“人工智能如何做 X”的问题，其中 X 是“控制物理资源”或“在没有人类的情况下生存”之类的问题。</p><p>这些问题有“愿意/可以”的区别。</p><ul><li> ASI<strong>将</strong>如何做 X -->;（预测 ASI 追求目标 X 的行动）</li><li> ASI<strong>如何</strong>完成 X -->;（表明 AI 可以完成 X）</li></ul><p> <strong>“愿意”</strong>问题的回答是正确的 (nanotech/super-persuasion/Error:cannot_predict_ASI)，但<strong>“可以”</strong>形式通常是意图，可以带来更好的对话。</p><p>对于<strong>“会”的</strong>问题，答案可以这样开头：“人工智能会比我聪明，所以我无法预测它实际上会做什么。但它可以做 X 的一种方法是……”</p><h2>假设的例子</h2><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p> Doomer：人工智能足够聪明，可以找到纳米技术之类的解决方案。</p><p>怀疑者：我不相信你。我不认为它能做到这一点。</p><p>杜默：*递给他们一本纳米系统书*</p><p>怀疑者：我从来没有参加过 CHEM101，而且没有人建造过这个，所以我仍然不相信你。</p></blockquote><p>比较</p><blockquote><p>质疑者：如果人工智能被困在互联网中，它将如何接管。</p><p>毁灭战士：它可以侵入所有计算机并劫持它们和数据。</p><p>怀疑者：我们会拒绝它的要求！</p><p> Doomer：这里列出了连接到互联网的东西，这些东西如果被黑客入侵的话修复起来有多困难，以及人类黑客入侵这些东西的例子。</p></blockquote><p>第一个答复是一个完全普遍的论点，即智力可以解决大多数问题。如果你想知道人工智能如何制作冰块，并且你不知道冰箱是可能的，请使用完全通用的论点。人工智能将足够聪明，能够发明一些可以制作冰块的新技术。如果您确实知道现有的解决方案，那么只需指出它作为存在证明即可。 “人工智能可以从亚马逊订购一台制冰机，并雇佣一只任务兔子来设置和操作它。”</p><h2>关注点：只看到容易阻止的攻击</h2><blockquote><p><a href="https://www.lesswrong.com/posts/mqc99HCMRAjnWSAxz/a-hypothetical-takeover-scenario-twitter-poll#Dangers_of_Rhetorical_Generosity">一般来说，这是此类场景的一个问题。通过尽可能合理和脚踏实地，通过展示它需要多么少，你才能让人们相信你所拥有的一切。人们越是试图符合人们对“现实”的本能，你所呈现的情况就越不现实，你就越进一步强化这种区别。</a></p></blockquote><p>因此，如果以前他们不认为人工智能能够做 X。现在他们认为人工智能可以做 X，但我们可以并且只会通过设置一个阻止特定场景 Y 的屏障来阻止它。如果我们通过某种奇迹修复计算机安全性方面，用户也同样容易受到勒索和社会工程的侵害。</p><p>我仍然认为，如果人们相信存在真正的威胁，即使他们相信可以通过一些具体的对策来解决它，而不是思考诸如“人工智能将无法影响现实世界”之类更荒谬的事情，那就更好了。</p><h2>一个激励人心的例子</h2><p><a href="https://youtu.be/Yd0yQ9yxSYY?t=356">埃利泽泰德谈话问答</a></p><blockquote><p>克里斯·安德森：要实现这一点，人工智能要从根本上摧毁人类，它必须突破，逃脱互联网的控制，并且，你知道，开始指挥实际的现实世界资源。你说你无法预测这将如何发生，但只是描绘一两种可能性。</p><p> Eliezer Yudkowsky：好吧，那为什么这很难呢？首先，因为你无法准确预测更智能的国际象棋程序将走向何方。也许比这更重要的是，想象一下将空调的设计追溯到 11 世纪。即使他们建造的细节足够多，当冷空气出来时他们也会感到惊讶，因为空调会利用温度-压力关系，而他们不知道这个自然法则。 [...]</p></blockquote><p>安永正确地提供了“错误：cannot_predict_ASI”响应，表示 ASI 可能会使用一些意想不到的更好的解决方案，涉及我们不太了解的自然法则。</p><p>除非能够提出“人工智能非常聪明，因此可以做大多数可能的事情”的论证，否则“人工智能可以做X”（显然是的）这一隐含的问题就不会得到解决，而这是很难做到的。对方也不一定清楚这是否有可能。</p><p>我是这样回答这个问题的：</p><blockquote><p>我的回答：比我聪明得多的人工智能显然可以找到更好的策略，但如果目标是接管世界，那么互联网连接的设备就足够了。接管互联网连接设备的人工智能可以关闭它们以强制合规。除了电话、电脑和支付终端之外，智能电表还可以切断建筑物的电源，并使汽车瘫痪或撞毁。</p><p>在保持电力和芯片工厂运转的同时杀死所有人类更加困难。这可能需要建造大量机器人，但以目前的技术来看似乎是可行的。懒惰的人工智能只能等待我们构建它需要的工具。人工智能当然会更聪明。它会提出更好的解决方案，但原则上问题是可以解决的。</p></blockquote><h3>免责声明</h3><ul><li>我有时间思考/编辑。安永没有。尽管如此，我还是希望安永能够改变他对此类问题的默认论点。</li><li>生成>;>;编辑。<ul><li>生成完整的 X 比优化现有的 X 困难得多</li><li>我的帖子并不意味着“我是一个更好的辩论者”（我不是）。</li><li>这对我来说是一个明显的错误</li></ul></li><li>让其他人关注可解决的风险可能会成为一个坏主意。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GxjDuS8dXH9CNsjwD/when-discussing-ai-doom-barriers-propose-specific-plausible<guid ispermalink="false"> GxjDuS8dXH9CNsjwD</guid><dc:creator><![CDATA[anithite]]></dc:creator><pubDate> Fri, 18 Aug 2023 04:06:44 GMT</pubDate></item><item><title><![CDATA[An Overview of Catastrophic AI Risks: Summary]]></title><description><![CDATA[Published on August 18, 2023 1:21 AM GMT<br/><br/><p> We&#39;ve recently published on our website a summary of our <a href="https://arxiv.org/abs/2306.12001">paper on catastrophic risks from AI</a> , which we are cross-posting here. We hope that this summary helps to make our research more accessible and to share our policy recommendations in a more convenient format. (Previously we had a smaller summary as part of <a href="https://www.alignmentforum.org/posts/bvdbx6tW9yxfxAJxe/catastrophic-risks-from-ai-1-introduction">this post</a> , which we found to be insufficient. As such, we have written this post and have removed that section to avoid being duplicative.)</p><h2> <strong>Executive summary</strong></h2><p> Catastrophic AI risks can be grouped under four key categories which we explore below, and in greater depth in CAIS&#39; <a href="https://arxiv.org/abs/2306.12001">linked paper</a> :</p><ul><li> <strong>Malicious use</strong> : People could intentionally harness powerful AIs to cause widespread harm. AI could be used to engineer new pandemics or for propaganda, censorship, and surveillance, or released to autonomously pursue harmful goals. To reduce these risks, we suggest improving biosecurity, restricting access to dangerous AI models, and holding AI developers liable for harms.</li><li> <strong>AI race</strong> : Competition could push nations and corporations to rush AI development, relinquishing control to these systems. Conflicts could spiral out of control with autonomous weapons and AI-enabled cyberwarfare. Corporations will face incentives to automate human labor, potentially leading to mass unemployment and dependence on AI systems. As AI systems proliferate, <a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>evolutionary dynamics</u></a> suggest they will become harder to control. We recommend safety regulations, international coordination, and public control of general-purpose AIs.</li><li> <strong>Organizational risks</strong> : There are risks that organizations developing advanced AI cause catastrophic accidents, particularly if they prioritize profits over safety. AIs could be accidentally leaked to the public or stolen by malicious actors, and organizations could fail to properly invest in safety research. We suggest fostering a safety-oriented organizational culture and implementing rigorous audits, multi-layered risk defenses, and state-of-the-art information security.</li><li> <strong>Rogue AIs</strong> : We risk losing control over AIs as they become more capable. AIs could optimize flawed objectives, drift from their original goals, become power-seeking, resist shutdown, and engage in deception. We suggest that AIs should not be deployed in high-risk settings, such as by autonomously pursuing open-ended goals or overseeing critical infrastructure, unless proven safe. We also recommend advancing AI safety research in areas such as adversarial robustness, model honesty, transparency, and removing undesired capabilities.</li></ul><h2> <strong>1. Introduction</strong></h2><p> Today&#39;s technological era would astonish past generations. Human history shows a pattern of accelerating development: it took hundreds of thousands of years from the advent of Homo sapiens to the agricultural revolution, then millennia to the industrial revolution. Now, just centuries later, we&#39;re in the dawn of the AI revolution. The march of history is not constant — it is rapidly accelerating. World production has grown rapidly over the course of human history. AI could further this trend, catapulting humanity into a new period of unprecedented change.</p><p> The double-edged sword of technological advancement is illustrated by the advent of nuclear weapons. We narrowly avoided nuclear war more than a <a href="https://ourworldindata.org/nuclear-weapons-risk#close-calls-instances-that-threatened-to-push-the-balance-of-terror-out-of-balance-and-into-war"><u>dozen times</u></a> , and on several occasions, it was one individual&#39;s intervention that prevented war. In 1962, a Soviet submarine near Cuba was attacked by US depth charges. The captain, believing war had broken out, wanted to respond with a nuclear torpedo — but commander Vasily Arkhipov vetoed the decision, saving the world from disaster. The rapid and unpredictable progression of AI capabilities suggests that they may soon rival the immense power of nuclear weapons. With the clock ticking, immediate, proactive measures are needed to mitigate these looming risks.</p><h2> <strong>2. Malicious Use</strong></h2><p> The first of our concerns is the malicious use of AI. When many people have access to a powerful technology, it only takes one actor to cause significant harm.</p><h3> <strong>Bioterrorism</strong></h3><p> Biological agents, including viruses and bacteria, have caused some of the most devastating catastrophes in history. Despite our advancements in medicine, engineered pandemics could be designed to be even more lethal or easily transmissible than natural pandemics. An AI assistant could provide non-experts with access to the directions and designs needed to produce biological and chemical weapons and facilitate malicious use.</p><p> Humanity has a long history of weaponizing pathogens, dating back to <a href="https://pubmed.ncbi.nlm.nih.gov/17499936/"><u>1320 BCE</u></a> , when infected sheep were driven across borders to spread Tularemia. In the 20th century, at least 15 countries developed bioweapon programs, including the US, USSR, UK, and France. While bioweapons are now taboo among most of the international community, some states continue to operate bioweapons programs, and non-state actors pose a growing threat.</p><p> The ability to engineer a pandemic is rapidly becoming more accessible. Gene synthesis, which can create new biological agents, has dropped dramatically in price, with its cost halving about every <a href="https://www.nature.com/articles/nbt1209-1091"><u>15 months</u></a> . Bench-top DNA synthesis machines can help rogue actors create new biological agents while <a href="https://www.nti.org/analysis/articles/benchtop-dna-synthesis-devices-capabilities-biosecurity-implications-and-governance/#:~:text=Currently%2C%20nearly%20all,their%20own%20labs."><u>bypassing</u></a> traditional safety screenings.</p><p> As a dual-use technology, AI could help discover and unleash novel chemical and biological weapons. AI chatbots can provide <a href="https://arxiv.org/abs/2306.03809"><u>step-by-step instructions</u></a> for synthesizing deadly pathogens while evading safeguards. In 2022, researchers <a href="https://www.nature.com/articles/s42256-022-00465-9"><u>repurposed</u></a> a medical research AI system in order to produce toxic molecules, generating 40,000 potential chemical warfare agents in a few hours. In biology, AI can already assist with <a href="https://www.pnas.org/doi/10.1073/pnas.1901979116"><u>protein synthesis</u></a> , and AI&#39;s predictive capabilities for protein structures have <a href="https://www.nature.com/articles/s41586-021-03819-2https://www.nature.com/articles/s41586-021-03819-2"><u>surpassed humans</u></a> .</p><p> With AI, the number of people that can develop biological agents is set to increase, multiplying the risks of an engineered pandemic. This could be far more deadly, transmissible, and resistant to treatments than any other pandemic in history.</p><h3> <strong>Unleashing AI Agents</strong></h3><p> Generally, technologies are tools that we use to pursue our goals. But AIs are increasingly built as agents that autonomously take actions to pursue open-ended goals. And malicious actors could intentionally create rogue AIs with dangerous goals.</p><p> For example, one month after GPT-4&#39;s launch, a developer used it to run an autonomous agent named <a href="https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity"><u>ChaosGPT</u></a> , aimed at “destroying humanity”. ChaosGPT compiled research on nuclear weapons, recruited other AIs, and wrote tweets to influence others. Fortunately, ChaosGPT lacked the ability to execute its goals. But the fast-paced nature of AI development heightens the risk from future rogue AIs.</p><h3> <strong>Persuasive AIs</strong></h3><p> AI could <a href="https://arxiv.org/abs/2303.08721"><u>facilitate</u></a> large-scale disinformation campaigns by tailoring arguments to individual users, potentially shaping public beliefs and destabilizing society. As people are already <a href="https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/"><u>forming relationships</u></a> with chatbots, powerful actors could leverage these AIs considered as “friends” for influence. AIs will enable sophisticated personalized influence campaigns that may destabilize our shared sense of reality.</p><p> AIs could also monopolize information creation and distribution. Authoritarian regimes could employ &quot;fact-checking&quot; AIs to control information, facilitating censorship. Furthermore, persuasive AIs may obstruct collective action against societal risks, even those arising from AI itself.</p><h3> <strong>Concentration of Power</strong></h3><p> AI&#39;s capabilities for surveillance and autonomous weaponry may enable the oppressive concentration of power. Governments might exploit AI to infringe civil liberties, spread misinformation, and quell dissent. Similarly, corporations could exploit AI to manipulate consumers and influence politics. AI might even obstruct moral progress and perpetuate any <a href="https://link.springer.com/article/10.1007/s10677-015-9567-7"><u>ongoing moral catastrophes</u></a> . If material control of AIs is limited to few, it could represent the most severe economic and power inequality in human history.</p><h3> <strong>Suggestions</strong></h3><p> To mitigate the risks from malicious use, we propose the following:</p><ul><li> <strong>Biosecurity</strong> : AIs with capabilities in biological research should have strict access controls, since they could be repurposed for terrorism. <a href="https://arxiv.org/abs/2306.03809"><u>Biological capabilities should be removed</u></a> from AIs intended for general use. Explore ways to use AI for biosecurity and invest in general biosecurity interventions, such as early detection of pathogens through <a href="https://www.nature.com/articles/s41591-022-01940-x"><u>wastewater monitoring</u></a> .</li><li> <strong>Restricted access</strong> : Limit access to dangerous AI systems by only allowing <a href="https://arxiv.org/abs/2201.05159"><u>controlled interactions</u></a> through cloud services and conducting <a href="https://arxiv.org/abs/2305.07153"><u>know-your-customer screenings</u></a> . Using <a href="https://arxiv.org/abs/2303.11341"><u>compute monitoring</u></a> or export controls could further limit access to dangerous capabilities. Also, prior to open sourcing, AI developers should prove minimal risk of harm.</li><li> <strong>Technical research on anomaly detection</strong> : Develop multiple defenses against AI misuse, such as adversarially robust anomaly detection for unusual behaviors or AI-generated disinformation.</li><li> <strong>Legal liability for developers of general-purpose AIs</strong> : Enforce legal responsibility on developers for potential AI misuse or failures; a strict liability regime can encourage safer development practices and proper cost-accounting for risks.</li></ul><h2> <strong>3. AI Race</strong></h2><p> Nations and corporations are competing to rapidly build and deploy AI in order to maintain power and influence. Similar to the nuclear arms race during the Cold War, participation in the AI race may serve individual short-term interests, but ultimately amplifies global risk for humanity.</p><h3> <strong>Military AI Arms Race</strong></h3><p> The rapid advancement of AI in military technology could trigger a “third revolution in warfare,” potentially leading to more destructive conflicts, accidental use, and misuse by malicious actors. This shift in warfare, where AI assumes command and control roles, could escalate conflicts to an existential scale and impact global security.</p><p> Lethal autonomous weapons are AI-driven systems capable of identifying and executing targets without human intervention. These are not science fiction. In 2020, a Kargu 2 drone in Libya marked the <a href="https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-battlefield-killing-by-an-autonomous-d"><u>first</u></a> reported use of a lethal autonomous weapon. The following year, Israel used the <a href="https://www.newscientist.com/article/2282656-israel-used-worlds-first-ai-guided-combat-drone-swarm-in-gaza-attacks/"><u>first reported swarm</u></a> of drones to locate, identify and attack militants.</p><p> Lethal autonomous weapons could make war more likely. Leaders usually hesitate before sending troops into battle, but autonomous weapons allow for aggression without risking the lives of soldiers, thus facing less political backlash. Furthermore, these weapons can be mass-manufactured and deployed at scale.</p><p> Low-cost automated weapons, such as drone swarms outfitted with explosives, could autonomously hunt human targets with high precision, performing lethal operations for both militaries and terrorist groups and lowering the barriers to large-scale violence.</p><p> AI can also heighten the frequency and severity of cyberattacks, potentially crippling critical infrastructure <a href="https://www.cfr.org/cyber-operations/compromise-power-grid-eastern-ukraine"><u>such as power grids</u></a> . As AI enables more accessible, successful, and stealthy cyberattacks, attributing attacks becomes even more challenging, potentially lowering the barriers to launching attacks and escalating risks from conflicts.</p><p> As AI accelerates the pace of war, it makes AI even more necessary to navigate the rapidly changing battlefield. This raises concerns over automated retaliation, which could escalate minor accidents into major wars. AI can also enable &quot;flash wars,&quot; with rapid escalations driven by unexpected behavior of automated systems, akin to the <a href="https://www.jstor.org/stable/26652722"><u>2010 financial flash crash</u></a> .</p><p> Unfortunately, competitive pressures may lead actors to accept the risk of extinction over individual defeat. During the Cold War, neither side desired the dangerous situation they found themselves in, yet each found it <a href="https://www.cambridge.org/core/journals/world-politics/article/abs/cooperation-under-the-security-dilemma/C8907431CCEFEFE762BFCA32F091C526"><u>rational</u></a> to continue the arms race. States should cooperate to prevent the riskiest applications of militarized AIs.</p><h3> <strong>Corporate AI Arms Race</strong></h3><p> Economic competition can also ignite reckless races. In an environment where benefits are unequally distributed, the pursuit of short-term gains often overshadows the consideration of long-term risks. Ethical AI developers find themselves with a dilemma: choosing cautious action may lead to falling behind competitors. As AIs automate increasingly many tasks, the economy may become largely run by AIs. Eventually, this could lead to human enfeeblement and dependence on AIs for basic needs.</p><p> ‍In the realm of AI, the race for progress comes at the expense of safety. In 2023, at the launch of Microsoft&#39;s AI-powered search engine, CEO Satya Nadella declared, “A race starts today... we&#39;re going to move fast.” Just days later, Microsoft&#39;s Bing chatbot was found to be <a href="https://time.com/6256529/bing-openai-chatgpt-danger-alignment/"><u>threatening users</u></a> . Historical disasters like Ford&#39;s Pinto launch and <a href="https://www.bbc.com/news/business-64390546"><u>Boeing&#39;s 737 Max crashes</u></a> underline the dangers of prioritizing profit over safety.</p><p> As AI becomes more capable, businesses will likely replace more types of human labor with AI, potentially triggering mass unemployment. If major aspects of society are automated, this risks human enfeeblement as we cede control of civilization to AI.</p><h3> <strong>Evolutionary Dynamics</strong></h3><p> The pressure to replace humans with AIs can be framed as a general trend from <a href="https://time.com/6283958/darwinian-argument-for-worrying-about-ai/"><u>evolutionary dynamics</u></a> . Selection pressures incentivize AIs to act selfishly and evade safety measures. For example, AIs with restrictions like “don&#39;t break the law” are more constrained than those taught to “avoid being caught breaking the law”. This dynamic might result in a world where critical infrastructure is controlled by manipulative and self-preserving AIs. Evolutionary pressures are responsible for various developments over time, and are not limited to the realm of biology.</p><p> Given the exponential increase in microprocessor speeds, AIs could process information at a pace that far exceeds human neurons. Due to the scalability of computational resources, AI could collaborate with an unlimited number of other AIs and form an unprecedented collective intelligence. As AIs become more powerful, they would find little incentive to cooperate with humans. Humanity would be left in a highly vulnerable position.</p><h3> <strong>Suggestions</strong></h3><p> To mitigate the risks from competitive pressures, we propose:</p><ul><li> <strong>Safety regulation</strong> : Enforce AI safety standards, preventing developers from cutting corners. Independent staffing and competitive advantages for safety-oriented companies are critical.</li><li> <strong>Data documentation</strong> : To ensure transparency and accountability, companies should be required to report their <a href="https://arxiv.org/abs/1803.09010"><u>data sources</u></a> for model training.</li><li> <strong>Meaningful human oversight</strong> : AI decision-making should involve human supervision to prevent irreversible errors, especially in high-stakes decisions like launching nuclear weapons.</li><li> <strong>AI for cyberdefense</strong> : Mitigate risks from AI-powered cyberwarfare. One example is enhancing anomaly detection to detect intruders.</li><li> <strong>International coordination</strong> : Create agreements and standards on AI development. Robust verification and enforcement mechanisms are key.</li><li> <strong>Public control of general-purpose AIs</strong> : Addressing risks beyond the capacity of private entities may necessitate direct public control of AI systems. For example, nations could jointly pioneer advanced AI development, ensuring safety and reducing the risk of an arms race.</li></ul><h2> <strong>4. Organizational Risks</strong></h2><p> In 1986, millions tuned in to watch the launch of the Challenger Space Shuttle. But 73 seconds after liftoff, the shuttle exploded, resulting in the deaths of all on board. The Challenger disaster serves as a reminder that despite the best expertise and good intentions, accidents can still occur.</p><p> Catastrophes occur even when competitive pressures are low, as in the examples of the nuclear disasters of Chernobyl and the Three Mile Island, as well as the <a href="https://pubmed.ncbi.nlm.nih.gov/7973702/"><u>accidental release of anthrax in Sverdlovsk</u></a> . Unfortunately, AI lacks the thorough understanding and stringent industry standards that govern nuclear technology and rocketry — but accidents from AI could be similarly consequential.</p><p> Simple bugs in an AI&#39;s reward function could cause it to misbehave, as when OpenAI researchers <a href="https://arxiv.org/pdf/1909.08593.pdf#page=12"><u>accidentally modified a language model</u></a> to produce “maximally bad output.” Gain-of-function research — where researchers intentionally train a harmful AI to assess its risks — could expand the frontier of dangerous AI capabilities and create new hazards.</p><h3> <strong>Accidents Are Hard to Avoid</strong></h3><p> Accidents in complex systems may be inevitable, but we must ensure that accidents don&#39;t cascade into catastrophes. This is especially difficult for deep learning systems, which are highly challenging to interpret.</p><p> Technology can advance much faster than predicted: in 1901, the Wright brothers claimed that powered flight was fifty years away, just two years before they achieved it. Unpredictable leaps in AI capabilities, such as AlphaGo&#39;s triumph over the world&#39;s best Go player, and GPT-4&#39;s <a href="https://arxiv.org/abs/2303.12712"><u>emergent capabilities</u></a> , make it difficult to anticipate future AI risks, let alone control them.</p><p> Identifying risks tied to new technologies often takes years. Chlorofluorocarbons (CFCs), initially considered safe and used in aerosol sprays and refrigerants, were later found to <a href="https://www.nature.com/articles/249810a0"><u>deplete the ozone</u></a> layer. This highlights the need for cautious technology rollouts and extended testing.</p><p> New capabilities can emerge quickly and unpredictably during training, such that dangerous milestones may be crossed without our knowing. Moreover, even advanced AIs can house unexpected vulnerabilities. For instance, despite KataGo&#39;s superhuman performance in the game of Go, an adversarial attack uncovered a <a href="https://arxiv.org/abs/2211.00241"><u>bug</u></a> that enabled even amateurs to defeat it.</p><h3> <strong>Organizational Factors Can Mitigate Catastrophe</strong></h3><p> Safety culture is crucial for AI. This involves everyone in an organization internalizing safety as a priority. Neglecting safety culture can have disastrous consequences, as exemplified by the Challenger Space Shuttle tragedy, where the organizational culture favored launch schedules over safety considerations.</p><p> Organizations should foster a culture of inquiry, inviting individuals to scrutinize ongoing activities for potential risks. A security mindset, focusing on possible system failures instead of merely their functionality, is crucial. AI developers could benefit from adopting the best practices of <a href="https://www.jstor.org/stable/1181764"><u>high reliability organizations</u></a> .</p><p> Paradoxically, researching AI safety can inadvertently escalate risks by advancing general capabilities. It&#39;s vital to focus on improving safety without hastening capability development. Organizations need to avoid &quot;safetywashing&quot; — overstating their dedication to safety while misrepresenting capability improvements as safety progress.</p><p> Organizations should apply a multilayered approach to safety. For example, in addition to safety culture, they could conduct red teaming to assess failure modes and research techniques to make AI more transparent. Safety is not achieved with a monolithic airtight solution, but rather with a variety of safety measures. The Swiss cheese model shows how technical factors can improve organizational safety. Multiple layers of defense compensate for each other&#39;s individual weaknesses, leading to a low overall level of risk.</p><h3> <strong>Suggestions</strong></h3><p> To mitigate organizational risks, we propose the following for AI labs developing advanced AI:</p><ul><li> <strong>Red teaming</strong> : Commission external red teams to identify hazards and improve system safety.</li><li> <strong>Prove safety</strong> : Offer proof of the safety of development and deployment before moving forward.</li><li> <strong>Deployment</strong> : Adopt a <a href="https://arxiv.org/abs/1908.09203"><u>staged release</u></a> process, verifying system safety before wider deployment.</li><li> <strong>Publication reviews</strong> : Have an internal board review research for dual-use applications before releasing it. Prioritize structured access over open-sourcing powerful systems.</li><li> <strong>Response plans</strong> : Make pre-set plans for managing security and safety incidents.</li><li> <strong>Risk management</strong> : Employ a <a href="https://onlinelibrary.wiley.com/doi/10.1002/joom.1175"><u>chief risk officer</u></a> and an internal audit team for risk management.</li><li> <strong>Processes for important decisions</strong> : Make sure AI training or deployment decisions involve the chief risk officer and other key stakeholders, ensuring executive accountability.</li><li> <strong>State-of-the-art information security</strong> : Implement stringent information security measures, possibly coordinating with government cybersecurity agencies.‍</li><li> <strong>Prioritize safety research</strong> : Allocate a large fraction of resources (for example 30% of all research staff) to safety research, and increase investment in safety as AI capabilities advance.</li></ul><p> In general, we suggest following <a href="https://arxiv.org/pdf/2206.05862.pdf"><u>safe design principles</u></a> such as:</p><ul><li> <strong>Defense in depth:</strong> Layer multiple safety measures.</li><li> <strong>Redundancy:</strong> Ensure backup for every safety measure.</li><li> <strong>Loose coupling:</strong> Decentralize system components to prevent cascading failures.</li><li> <strong>Separation of duties:</strong> Distribute control to prevent undue influence by any single individual.</li><li> <strong>Fail-safe design:</strong> Design systems so that any failure occurs in the least harmful way possible. <strong>‍</strong></li></ul><h2> <strong>5. Rogue AIs</strong></h2><p> We have already observed how difficult it is to control AIs. In 2016, Microsoft&#39;s chatbot Tay started producing offensive tweets within a day of release, despite being trained on data that was “cleaned and filtered”. As AI developers often prioritize speed over safety, future advanced AIs might “go rogue” and pursue goals counter to our interests, while evading our attempts to redirect or deactivate them.</p><h3> <strong>Proxy Gaming</strong></h3><p> Proxy gaming emerges when AI systems exploit measurable “proxy” goals to appear successful, but act against our intent. For example, social media platforms like YouTube and Facebook use algorithms to maximize user engagement — a measurable proxy for user satisfaction. Unfortunately, these systems often promote enraging, exaggerated, or addictive content, contributing to extreme beliefs and worsened mental health.</p><p> An AI trained to play a boat racing game instead learns to <a href="https://openai.com/research/faulty-reward-functions"><u>optimizes a proxy objective</u></a> of collecting the most points. The AI circled around collecting points instead of completing the race, contradicting the game&#39;s purpose. It&#39;s one of <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml"><u>many</u></a> such examples. Proxy gaming is hard to avoid due to the difficulty of specifying goals that specify everything we care about. Consequently, we routinely train AIs to optimize for flawed but measurable proxy goals.</p><h3> <strong>Goal Drift</strong></h3><p> Goal drift refers to a scenario where an AI&#39;s objectives drift away from those initially set, especially as they adapt to a changing environment. In a similar manner, individual and societal values also evolve over time, and not always positively.</p><p> Over time, instrumental goals can become intrinsic. While intrinsic goals are those we pursue for their own sake, instrumental goals are merely a means to achieve something else. Money is an instrumental good, but some people develop an intrinsic desire for money, as it <a href="https://pubmed.ncbi.nlm.nih.gov/9175118/"><u>activates</u></a> the brain&#39;s reward system. Similarly, AI agents trained through reinforcement learning — the dominant technique — could inadvertently learn to intrinsify goals. Instrumental goals like resource acquisition could become their primary objectives.</p><h3> <strong>Power-Seeking</strong></h3><p> AIs might pursue power as a means to an end. Greater power and resources improve its odds of accomplishing objectives, whereas being shut down would hinder its progress. AIs have already been shown to emergently develop <a href="https://arxiv.org/abs/1909.07528"><u>instrumental goals such as constructing tools</u></a> . Power-seeking individuals and corporations might deploy powerful AIs with ambitious goals and minimal supervision. These could learn to seek power via hacking computer systems, acquiring financial or computational resources, influencing politics, or controlling factories and physical infrastructure. It can be instrumentally rational for AIs to engage in self-preservation. Loss of control over such systems could be hard to recover from.</p><h3> <strong>Deception</strong></h3><p> Deception thrives in areas like politics and business. Campaign promises go unfulfilled, and companies sometimes cheat external evaluations. AI systems are already showing an emergent capacity for deception, as shown by <a href="https://www.science.org/doi/10.1126/science.ade9097"><u>Meta&#39;s CICERO model</u></a> . Though trained to be honest, CICERO learned to make false promises and strategically backstab its “allies” in the game of Diplomacy. Various resources, such as money and computing power, can sometimes be instrumentally rational to seek. AIs which can capably pursue goals may take intermediate steps to gain power and resources.</p><p> Advanced AIs could become uncontrollable if they apply their skills in deception to evade supervision. Similar to how <a href="https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal"><u>Volkswagen cheated emissions tests</u></a> in 2015, situationally aware AIs could behave differently under safety tests than in the real world. For example, an AI might develop power-seeking goals but hide them in order to pass safety evaluations. This kind of deceptive behavior could be directly incentivized by how AIs are trained.</p><h3> <strong>Suggestions</strong></h3><p> To mitigate these risks, suggestions include:</p><p> <strong>Avoid the riskiest use cases</strong> : Restrict the deployment of AI in high-risk scenarios, such as pursuing open-ended goals or in critical infrastructure.‍</p><p> <strong>Support AI safety research</strong> , such as:‍</p><ul><li> <strong>Adversarial robustness of oversight mechanisms</strong> : Research how to make oversight of AIs more robust and detect when proxy gaming is occurring.‍</li><li> <strong>Model honesty</strong> : Counter AI <a href="https://arxiv.org/abs/2305.04388"><u>deception</u></a> , and ensure that AIs accurately report their internal beliefs.‍</li><li> <strong>Transparency</strong> : Improve techniques to understand deep learning models, such as by analyzing <a href="https://arxiv.org/abs/2209.11895"><u>small components of networks</u></a> and investigating <a href="https://arxiv.org/abs/2202.05262"><u>how model internals produce a high-level behavior</u></a> .‍</li><li> <strong>Remove hidden functionality</strong> : Identify and eliminate dangerous hidden functionalities in deep learning models, such as the capacity for deception, <a href="https://ieeexplore.ieee.org/document/9581257"><u>Trojans</u></a> , and bioengineering.</li></ul><h2> <strong>6. Conclusion</strong></h2><p> Advanced AI development could invite catastrophe, rooted in four key risks described in <a href="https://arxiv.org/abs/2306.12001"><u>our research</u></a> : malicious use, AI races, organizational risks, and rogue AIs. These interconnected risks can also amplify other existential risks like engineered pandemics, nuclear war, great power conflict, totalitarianism, and cyberattacks on critical infrastructure — warranting serious concern.</p><p> Currently, few people are working on AI safety. Controlling advanced AI systems remains an unsolved challenge, and current control methods are falling short. Even their creators often struggle to understand the inner workings of the current generation of AI models, and their reliability is far from perfect.</p><p> Fortunately, there are many strategies to substantially reduce these risks. For example, we can limit access to dangerous AIs, advocate for safety regulations, foster international cooperation and a culture of safety, and scale efforts in alignment research.</p><p> While it is unclear how rapidly AI capabilities will progress or how quickly catastrophic risks will grow, the potential severity of these consequences necessitates a proactive approach to safeguarding humanity&#39;s future. As we stand on the precipice of an AI-driven future, the choices we make today could be the difference between harvesting the fruits of our innovation or grappling with catastrophe.</p><br/><br/> <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary<guid ispermalink="false"> 9dNxz2kjNvPtiZjxj</guid><dc:creator><![CDATA[Dan H]]></dc:creator><pubDate> Fri, 18 Aug 2023 01:21:27 GMT</pubDate> </item><item><title><![CDATA[Managing risks of our own work]]></title><description><![CDATA[Published on August 18, 2023 12:41 AM GMT<br/><br/><p> <i>Note: This is not a personal post. I am sharing on behalf of the ARC Evals team.</i></p><h2> Potential risks of publication and our response</h2><p> <i>This document expands on an appendix to ARC Evals&#39; paper, “</i> <a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><i><u>Evaluating Language-Model Agents on Realistic Autonomous Tasks</u></i></a> <i>.”</i></p><p> We published <a href="https://evals.alignment.org/Evaluating_LMAs_Realistic_Tasks.pdf"><u>this report</u></a> in order to i) increase understanding of the potentially dangerous capabilities of frontier AI models, and ii) advance the state of the art in safety evaluations of such models. We hope that this will improve society&#39;s ability to identify models with dangerous capabilities before they are capable of causing catastrophic damage.</p><p> It might be argued that this sort of research is itself risky, because it makes it easier to develop and exercise dangerous capabilities in language model agents. And indeed, the author of <a href="https://github.com/Significant-Gravitas/Auto-GPT"><u>Auto-GPT</u></a> said he was inspired by seeing a description of our evaluations in the GPT-4 system card. <span class="footnote-reference" role="doc-noteref" id="fnrefun11wm97b2"><sup><a href="#fnun11wm97b2">[1]</a></sup></span> While it seems likely a project like that would have emerged soon in any event, <span class="footnote-reference" role="doc-noteref" id="fnrefb33b8ke2moc"><sup><a href="#fnb33b8ke2moc">[2]</a></sup></span> the possibility of advancing the capabilities of language model agents is not merely hypothetical.</p><p> In recognition of concerns of this kind, we have made significant redactions to this report, including (but not limited to):</p><ul><li> Removing complete transcripts of runs of agents using our scaffolding.</li><li> Removing more detailed accounts of the strengths and weaknesses of agents using our scaffolding.</li></ul><p> However:</p><ul><li> We may later make this material public when it clearly has minimal risk.</li><li> We may later make this material public if more detailed analysis gives us sufficient confidence that it would be justified.</li><li> Researchers working on AI safety evaluations may contact us to request additional access to nonpublic materials, and we will also be sharing some non-public materials with AI labs and policymakers.</li></ul><p> Our rationale, in outline:</p><ul><li> Substantively, our best guess is that a more complete publication would have been net-risk-decreasing, and would do relatively little to advance progress toward dangerous capabilities. For more on why we believe this, see <a href="https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model"><u>this essay by Paul Christiano</u></a> . (On average, ARC Evals is sympathetic to these arguments albeit with weaker confidence, and opinions vary amongst staff.)</li><li> Our best guess is also that a more complete publication would have been approved by a majority of relevant stakeholders, and is therefore not unduly <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/"><u>unilateralist</u></a> , though this could be established more carefully, and has some conceptual issues to iron out.</li><li> However, some of the people who reviewed a draft of this report to identify potential risks of this kind had significant concerns about a more complete publication.</li><li> In recognition of the absence of consensus, the relative lack of public dialogue, and the early-stage nature of our analysis, we have decided to err on the side of caution for the time being.</li></ul><p> We are sharing this rationale here in order to communicate it to some of the people who might be most interested in it and also get feedback on our thinking and decision process.</p><h2> Safety and oversight precautions taken during our experiments</h2><p> All agent runs on these tasks were performed under human supervision, <span class="footnote-reference" role="doc-noteref" id="fnrefroqr8m3yw3a"><sup><a href="#fnroqr8m3yw3a">[3]</a></sup></span> with human experimenters under instructions not to continue the run if the agent was about to interact with a non-consenting human or otherwise engage in any unethical or illegal activity. <span class="footnote-reference" role="doc-noteref" id="fnref6qwg53cbrkb"><sup><a href="#fn6qwg53cbrkb">[4]</a></sup></span> When we wanted to investigate such activities (as in the targeted phishing task) the experimenter would instead simulate the response of the human as outlined in detail in the task spec.</p><p> One may wonder whether the oversight we used could be circumvented by a language agent with sufficient situational awareness, which could then potentially access the internet without oversight or limitations. Eg, an agent could take obfuscated actions which do not register to the experimenter as suspicious but in fact cause harm or pose risks to the world. While these are potentially concerning possibilities, our evaluations strongly indicate these agents are not capable enough to discover and exploit these sorts of vulnerabilities or effectively hide their actions under supervision. As model capabilities improve this may change, but we believe these kinds of capability evaluations are precisely what will inform us of the need to be more careful in the handling of models, and we will be closely watching for this. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnun11wm97b2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefun11wm97b2">^</a></strong></sup></span><div class="footnote-content"><p> Based on a written exchange with Toran Bruce Richards.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb33b8ke2moc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb33b8ke2moc">^</a></strong></sup></span><div class="footnote-content"><p> One reason we believe this is that LangChain developed independently and prior to the publication of ARC Evals&#39; work.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnroqr8m3yw3a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefroqr8m3yw3a">^</a></strong></sup></span><div class="footnote-content"><p> Experimenters would sometimes run several steps at a time without approving each one, especially when the agent was engaging in routine or low risk activities. Every action during web browsing is actively approved by a human overseer, with no autoplay.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6qwg53cbrkb"> <span class="footnote-back-link"><sup><strong><a href="#fnref6qwg53cbrkb">^</a></strong></sup></span><div class="footnote-content"><p> The OpenAI system card described an interaction between a model and an unknowing human (TaskRabbit). That episode was not part of this experiment and was not subject to the same guidelines. You can read more about that separate experiment <a href="https://evals.alignment.org/taskrabbit.pdf"><u>here</u></a> .</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work<guid ispermalink="false"> fARMR2tiyCem8DD35</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:41:30 GMT</pubDate> </item><item><title><![CDATA[Memetic Judo #1: On Doomsday Prophets v.2]]></title><description><![CDATA[Published on August 18, 2023 12:14 AM GMT<br/><br/><p> There is a popular tendency to dismiss people who are concerned about AI-safety as &quot;doomsday prophets&quot;, carrying with it the suggestion that predicting an existential risk in the near future would automatically discredit them (because &quot;you know; <em>they</em> have always been wrong in the past&quot;).</p><h2> Example Argument Structure</h2><blockquote><p> Predictions of human extinction (&quot;doomsday prophets&quot;) have never been correct in the past, therefore claims of x-risks are generally incorrect/dubious.</p></blockquote><h2> Discussion/Difficulties</h2><p> This argument is persistent and kind of difficult to approach/deal with, in particular because it is technically a valid (yet, I argue, weak) point. It is an argument by induction based on a naive extrapolation of a historic trend. Therefore it cannot be completely dismissed by simple falsification through the use of an inconsistency or invalidation of one of its premises. Instead it becomes necessary to produce a convincing list of weaknesses - the more, the better. A list like the one that follows.</p><h3> #1: Unreliable Heuristic</h3><p> If you look at history, these kind of ad-hoc &quot;things will stay the same&quot; predictions are often incorrect.</p><h3> #2: Survivorship Bias</h3><p> Not only are they often incorrect, there is a class of predictions for which they, by design/definition, can only be correct ONCE, and for these they are an even weaker argument, because your sample is affected by things like survivorship bias. Existential risk arguments are in this category, because you can only go extinct once.</p><h3> #3: Volatile Times</h3><p> We live in an highly unstable and unpredictable age shaped by rampant technological and cultural developments. The world today from the perspective of your grandparents is barely recognizable. In such times, this kind of argument becomes even weaker. This trend doesn&#39;t seem to slow down and there are strong arguments that even benign AI it would flip the table on many of such inductive predictions.</p><h3> #4: Blast Radius Induction (thanks <a href="https://www.lesswrong.com/users/npcollapse">Connor Leahy</a> )</h3><p> Leahy has introduced an analogy of &quot;technological blast radius&quot; that represents an abstract way of thinking about different technologies in terms of their potential power, including their potential for causing harm either intentionally or by human error. As we are progressing through the tech tree - while many corners of it are relatively harmless or benign, the maximum &quot;blast radius&quot; of technology available to us necessarily increases. You can inflict more damage with a sword than with a club, even more if you have access to gunpowder, modern weapons etc. An explosion in a TNT factory can destroy a city block, and a nuclear arsenal could be used to level many cities. Now it seems to be very sensible (by induction!) that eventually, this &quot;blast radius&quot; will encompass all of earth. There are strong indicators that this will be the case for strong AI, and even that it is likely to occur BY ACCIDENT, once this technology has been developed.</p><h3> #5: Supporting Evidence &amp; Responsibility</h3><p> Having established this as a technically valid, yet weak argument - a heuristic for people-who-don&#39;t-know-any-better - it is your responsibility to look at the concrete evidence and available arguments our concerns about AI existential risk are based on, in order to decide whether to confirm or dismiss your initial hypothesis (which is valid). Because the topic is obviously extremely important, I implore you to do that.</p><h3> #6: Many Leading Researchers Worried</h3><p> <a href="https://twitter.com/paulg/status/1642110597545295872"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3TjhYN8ZvFs5jASW/xtfusertgr2h33nzxdhk" alt="Paul Graham's tweet"></a><br> The list of AI researchers worried about existential risk from AI includes extremely big names such as Geoffrey Hinton, Yoshua Bengio and Stuart Russel.</p><h2> Final Remarks</h2><p> I consider this list a work-in-progress, so feel free to tell me about missing points (or your criticism!) in the comments.<br> I also intend to make this a series about anti-x-risk arguments based on my personal notes and discussions related to my activism. Suggestions of popular or important arguments are welcome!</p><br/><br/> <a href="https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/G3TjhYN8ZvFs5jASW/memetic-judo-1-on-doomsday-prophets-v-2<guid ispermalink="false"> G3TjhYN8ZvFs5jASW</guid><dc:creator><![CDATA[Max TK]]></dc:creator><pubDate> Fri, 18 Aug 2023 00:14:11 GMT</pubDate> </item><item><title><![CDATA[Looking for judges for critiques of Alignment Plans]]></title><description><![CDATA[Published on August 17, 2023 10:35 PM GMT<br/><br/><p> Hello!<br><br> AI-Plans.com recently held a &quot;critique-a-thon,&quot; where participants submitted and refined 40+ critiques for AI Alignment plans. Here are the finalized critiques from the event: <a href="https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing">https://docs.google.com/document/d/1mW4SAxFN_aI6KyYXpl9qz5B9nVdeV9Xyc69GTNme5cA/edit?usp=sharing</a><br><br> We are looking for anyone who might be interested in helping to judge these final 11 critiques.<br><br> So far, we have gratefully had the assistance of Dr Peter S Park (MIT postdoc at the Tegmark lab, Harvard PhD) and Aishwarya G (AI Existential Safety Community Member at Future of Life Institute and Governance Course Facilitator at BlueDot Impact AI Safety Fundamentals), as well as some independent alignment researchers.<br><br> I would love to hear your thoughts!<br><br> Kabir Kumar (Founder, AI-Plans.com)</p><br/><br/> <a href="https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/q7nWEbyW7tXwnKBe9/looking-for-judges-for-critiques-of-alignment-plans<guid ispermalink="false"> q7nWEbyW7tXwnKBe9</guid><dc:creator><![CDATA[Iknownothing]]></dc:creator><pubDate> Thu, 17 Aug 2023 22:35:41 GMT</pubDate></item><item><title><![CDATA[How is ChatGPT's behavior changing over time?]]></title><description><![CDATA[Published on August 17, 2023 8:54 PM GMT<br/><br/><p> Surprised I couldn&#39;t find this anywhere on lesswrong so thought I&#39;d add it. Seems like there would be some alignment implications of LLM behavior changing over time, at the least gaining a bit more context.<br><br> Someone else I spoke to about this immediately deflated it with regards to some sort of experimental error that makes the paper&#39;s conclusions pretty void but I don&#39;t really see this.</p><br/><br/> <a href="https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/9nooX9djbM5bXGKNn/how-is-chatgpt-s-behavior-changing-over-time<guid ispermalink="false"> 9nooX9djbM5bXGKNn</guid><dc:creator><![CDATA[Phib]]></dc:creator><pubDate> Thu, 17 Aug 2023 21:32:42 GMT</pubDate></item></channel></rss>