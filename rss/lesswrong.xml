<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 18 日星期三 00:52:44 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Hints about where values come from]]></title><description><![CDATA[Published on October 18, 2023 12:07 AM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:04:33 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:04:33 GMT" user-order="2"><p>一天后写的简介：</p><p>我和斯皮拉库尔讨论了价值观的本质和起源。对话并没有涉及太多单一清晰的线索，但我写得很开心，并希望它能为其他人带来一些有趣的线索。</p><p> （你可能希望在一开始就讨论方法论交流；而是从“扔掉一些东西似乎不错！”开始。）</p><hr><p>你好。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:05:48 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:05:48 GMT" user-order="1"><p>你好！那么，“价值观从何而来？”一些动物行为/人类起源问题可能会渗透到其中，对吗？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:06:10 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:06:10 GMT" user-order="2"><p>我想做一些类似“import立场.radical_philosophical_confusion”的事情，但我想这可能很无聊，而且在我们共同的语言中不是一个简短的词。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:06:28 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:06:28 GMT" user-order="1"><p>有帖子吗？或者如果没有的话：大约 5 句话的版本是什么？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:06:51 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:06:51 GMT" user-order="2"><p>这里有一篇文章： <a href="https://tsvibt.blogspot.com/2023/09/a-hermeneutic-net-for-agency.html">https://tsvibt.blogspot.com/2023/09/a-hermeneutic-net-for-agency.html</a><br><br>但就像我的许多帖子一样，它写得不好，因为它是最简单的帖子，根本无法表达想法。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:08:08 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:08:08 GMT" user-order="2"><p>一个简短的版本是：我的很多兴趣在于重新编程我们在思考/谈论价值观时带来的一堆相关想法。我想重新编程它们（我的意思是在我的脑海中），以便我可以考虑对齐。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:08:25 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:08:25 GMT" user-order="1"><p>第一句话就立刻被“解释学网”迷住了，是的。无论如何，在尝试解决它......诸如圣经解释个人维基之类的事情？或者我完全偏离了错误的轨道？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:09:12 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:09:12 GMT" user-order="2"><p>那里的“解释学”只是说，就像，我想在所有不同的具体例子和具体的明确概念之间来回跳动，以及对我们的概念和我们对例子的理解所施加的标准，其中标准来自大局。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:09:59 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:09:59 GMT" user-order="2"><p> （我想我在这个元级别上说话感到不舒服，很大程度上是因为我认为你不感兴趣，即使你没有这么说。）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:10:03 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:10:03 GMT" user-order="1"><p>好吧，所以“交联”是的，不再强调圣经部分，加强“对话”/解释差异部分，保留“等级制度进入问题”的事情可能......</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:11:27 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:11:27 GMT" user-order="1"><p>呃......我感觉还不错，但我意识到我正在尝试做某种“看看我是否可以短路到你正在打手势的复杂事物的印象”，这可能行不通/可能真的从根本上出发，并做一些奇怪的社交活动来做到这一点。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:11:33 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:11:33 GMT" user-order="1"><p>我们可以退出。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:11:52 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:11:52 GMT" user-order="2"><p>我认为走捷径是合理的。就像，我实际上并不认为我想要表示的东西那么复杂或不常见理解，我只是希望能够明确地调用它。反正。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:12:03 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:12:03 GMT" user-order="2"><p>所以。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:13:57 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:13:57 GMT" user-order="1"><p>看来我们应该尝试循环回到值。你想打包你要尝试使用它来构建的点，还是我应该扔掉一些东西？ （这可能有脱轨或丢失的风险，或者可能会循环回来，谁知道呢！）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:14:13 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:14:13 GMT" user-order="2"><p>扔掉一些东西似乎也不错！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:20:42 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:20:42 GMT" user-order="1"><p>好吧！所以，有点脱离我的头脑......</p><ul><li>价值观的逻辑一致性对很多人来说似乎非常重要，但大多数人似乎都会使用某种停止或理智/直觉检查（斯科特在某些帖子中对此做了手势；当你让步给哲学家）。我想知道为什么它会激活？</li><li>很多价值可能在某种进化上有利的指标（或代理指标！有时代理指标带有享乐，例如：性与生育）中触底反弹，至少作为最初的起点。</li><li>关于评估堆栈概念概括“顶部”的事物与“底部”的接近享乐跟踪器的事物的模糊问题？或者世界建模/真相发现部分的收敛属性，当我想到它时，这是一种奇怪的导出值的方式。或者甚至在 evo 事件落在代理上时放弃“代理”的激进立场（几乎没有人认真对待）。</li></ul><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> 气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:28:21 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:28:21 GMT" user-order="2"><p> （我注意到我想说的是抽象的东西，而不是从例子开始，这很悲伤，但无论如何我都会这样做，所以这些东西可以被掩盖，直到我们用具体的例子回到它......[之后编辑：有关值的一些示例，请参阅<a href="https://tsvibt.blogspot.com/2023/08/human-wanting.html">https://tsvibt.blogspot.com/2023/08/</a> human-wanting.html 、 <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html#2-built-in-behavior-determiners">https://tsvibt.blogspot.com/2022/11/do-</a> humans-derive-values- <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html#2-built-in-behavior-determiners">from-fictitious.html#2-built-in-behavior-determiners</a> ，https: <a href="https://tsvibt.blogspot.com/2022/10/counting-down-vs-counting-up-coherence.html">//tsvibt.blogspot.com/2022/10/counting-down-vs-counting-up-coherence.html</a> ， <a href="https://tsvibt.blogspot.com/2022/08/control.html">https://tsvibt。 blogspot.com/2022/08/control.html</a> ]）<br><br>那么，为什么要问价值观从何而来呢？我真的想知道价值观在头脑中的形态。我想知道这一点，因为我想创造一个具有奇怪价值观的头脑。即，可修正性。或者更确切地说，某种形式的[可能的可校正性解决方案，如下所述：https: <a href="https://arbital.com/p/hard_corrigibility/">//arbital.com/p/hard_corrigibility/</a> （更可靠：https: <a href="https://archive.ph/dJDqR">//archive.ph/dJDqR</a> ）]。<br><br>这些想法的一些可能的词：</p><ul><li>分裂主义推理。 “我不是一个独立的代理人。我是代理人的一部分。我的价值观分布在我的整个自我中，其中包括人类的东西。”</li><li>悲剧性的机构。 “我的推理/价值观是有缺陷的。我是古德哈廷，即使我认为我正在应用我的最终标准。我施加的优化压力指向了错误的事情。这延伸到元级别：当我我认为我正在纠正我的推理/价值观，但我用来判断纠正的标准也是有缺陷的。”</li><li>忠诚的机构。 “我是另一个特工的延伸/代表。我所做的一切，我都将其解释为另一个机构（人道）试图做一些我不理解的事情。”</li><li>彻底的尊重。 “我尊重展现价值观的人道过程。我尊重人道过程在任何抽象层面上编辑我的冲动。我相信这种判断过程高于我自己的判断过程，就像那些普通的正常代理人如何相信他们的[未来的自我，如果是通过“使我成为现在的我的过程”]超越他们现在的自我而得出的。</li></ul><p>这些都深深地涉及到价值观，但我不知道如何让这些高层次的直觉变得更精确。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:29:24 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:29:24 GMT" user-order="1"><p>凉爽的。我认为这与我表示感兴趣的健全性检查与逻辑一致性的事情融合得很好，让我们继续你更发达的词汇/发音。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 20:48:23 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 20:48:23 GMT" user-order="2"><blockquote><p>很多价值可能在某种进化上有利的指标（或代理指标！有时代理指标带有享乐，例如：性与生育）中触底反弹，至少作为最初的起点。</p></blockquote><p>作为一条线索，我们可以继续下去：对我来说，这“作为最初的起点”暗示了这里的一个关键问题。我们有这些起点，但是我们要去别的地方吗？我们该怎么做呢？<br><br>一个建议是：我们将自己（我们过去的行为，我们思想的内容）解释为更强大的代理人<i>做某事的</i>有缺陷的尝试，然后我们采用该<i>某事</i>作为我们的目标。 <a href="https://tsvibt.blogspot.com/2022/11/do-humans-derive-values-from-fictitious.html">https://tsvibt.blogspot.com/2022/11/do- humans-derive-values-from-fictitious.html</a><br><br>一般来说，我们不是从明确的值开始，而是创建它们。 （有时我们会做一些不同的事情，最好的描述是发现价值观——例如发现自童年以来被压抑的欲望。有时发现和创造是模棱两可的。但我认为我们有时做的事情只能非常勉强地描述为发现，并且是相反，自由创作。）</p><p>这种创造暗示了某种其他类型的价值，“元价值”或“过程价值”。这些元价值感觉更接近于[这些关于可修正性的想法中出现的那种价值]。所以他们很有趣。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:49:03 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:49:03 GMT" user-order="1"><p>我认为分裂主义会出错，除非它具有“不干涉”或“早期（不妥协）时间步骤 X 认可的（人类/检查者）代理”或其他东西的价值。<br><br>我猜人类设法拥有多个相互交织的子系统，并且通常不会覆盖彼此的运作能力，但是？ （也许除了……在药物确实干扰另一个价值做出可靠行动出价或类似行为的能力的情况下，或者在抑制系统被关闭的情况下）</p><hr><p> “Anapartistic”可能更接近于它在人类子系统上的实施方式（非常低的置信度），但“Tragic Agency”感觉更像是人们通过明确检查他们的道德推理来推理的方式。<br><br>试图……建立他们的道德体系，但又不偏离得太远？通常通过“健全性检查”，定期停止并运行示例，看看它是否给出了疯狂/痛苦/不可取或不可逆转/激进的政策建议，并试图诊断这些建议的上游是什么道德推理步骤？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 20:56:07 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 20:56:07 GMT" user-order="1"><blockquote><p> “我们有了这些起点，但接下来我们要去别的地方？我们该怎么做？”</p></blockquote><p>我想我只是给出了一半的答案，但让我进一步分解一下：一个过程看起来像是“建立在”预先建立的已知基本价值观、道德推论、示例之上，并通过推理（聚合共性） ，或下一个逻辑步骤），提出新的推论。然后检查它如何改变整体的政策建议输出，并且......（哦！）标记它以进一步检查它是否会导致前一个时间步骤的大规模政策改变？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:00:43 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:00:43 GMT" user-order="1"><blockquote><p>我们将自己（我们过去的行为，我们思想的内容）解释为更强大的代理人<i>做某事的</i>有缺陷的尝试，然后我们将这<i>件事</i>作为我们的目标。</p></blockquote><p>好的，这是 Loyal Agency 的例子。我想这是利用了人类同理心系统的能力，对吗？<br><br> （我<i>不</i>知道如何在非进化的基质上实现这一点，但我猜在人类中，它是进化压力进展的下游（按时间顺序从头到尾）“建模捕食者/猎物”->;“建模同种动物” ” ->; “为盟友建模” ->; “在共同目标下抽象地协调一致”？）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:03:31 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:03:31 GMT" user-order="2"><blockquote><p>关于评估堆栈概念概括“顶部”的事物与“底部”的接近享乐跟踪器的事物的模糊问题？</p></blockquote><p>强调价值创造/价值选择的观点：没有顶部。或者换句话说：顶部只是隐含地存在。它是由[我们用来指导我们编织连贯值的任何元值/过程]所指向、确定或期望的。<br><br>正如您所讨论的，其中很多都不是真正的值，因为它不是自由参数。我们可以注意到逻辑上的不一致。例如，我们可能会说：“杀死婴儿是不好的，因为他们有意识”和“堕胎是可以的，因为他们没有意识”，并注意到这些放在一起并没有真正的意义（尽管得出的结论是正确的）。然后我们受到逻辑的引导/约束：33周大的胎儿要么有意识，要么没有意识，所以我们必须让我们所有的多重价值观与其中一个世界相吻合，或者让我们所有的多重价值观与另一个世界相吻合那些世界。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:11:52 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:11:52 GMT" user-order="1"><blockquote><p>一般来说，我们不是从明确的值开始，而是创建它们。 （有时我们会做一些不同的事情，最好的描述是发现价值观——例如发现自童年以来被压抑的欲望。有时发现和创造是模棱两可的。但我认为我们有时做的事情只能非常勉强地描述为发现，并且是相反，自由创作。）</p></blockquote><p>这对我来说感觉“不对劲”，而且还没有完全落地。<br><br>就像……你从一个做事的婴儿开始。在某种程度的复杂性上，你开始在“价值”的概念下，对那些持续驱动你部分行为的事物进行自我模型的划分？<br><br>我有一种感觉，仅仅尝试...上传一个新值作为免费创作通常是行不通的，除非它与预先存在的模式相关联...嗯。不，好吧，人们可以更新他们的自我意识，然后有时会做一些疯狂的事情来使他们的行为与自我意识保持一致。但我认为我认为这一点服从于“自我一致性”的价值观和避免强烈的认知失调，而且我可能认为这些价值观在实施中往往以某种方式更宽松地“浅层”。 （不过，我完全不相信这就是它真正的工作原理）<br><br>在玩了一段时间后，不太相信它完全“关闭”了。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:31:29 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:31:29 GMT" user-order="2"><p>我感觉有点不靠谱……也许我想举一些例子。</p><ul><li>交个朋友。首先，我们认识到某种重叠。随着事情的发展，我们在某种程度上“跟随我们的鼻子”关于我们可以在一起的事情。对于在一起会是什么样子，没有先发制人的明确想法。这并不是说有什么秘密想法会被揭露。虽然......也许就像，所有这些东西都是一个优化过程，只是解决了仪器细节。但是，IDK，这似乎不对。</li><li>制作艺术。嗯......我有点想说“看！这是关于创造”，然后挥舞我的手。</li><li>类似于艺术创作：烹饪。这与“实际上，你正在为一些预先存在的固定目标计算工具性策略”以及“你很聪明，就像一个吸毒者”非常混合和模糊。但是，当一位熟练的厨师将两种已知的美味做出令人惊讶的组合时……这看起来并不像古德哈廷，因为……嗯。事实上，我完全可以想象一些我认为古德哈廷的口味组合。</li><li>也许我想说纯粹的好奇心和纯粹的玩耍就是典型的例子。你正试图在阳光下创造一些新的东西。我们可以说：这不是价值的创造，而是理解的创造。有一个固定的价值观：我想创造理解。但这使用的“价值”概念比我们通常意义上的更受限制。如果有人喜欢下棋，我们通常将其称为值。如果我们想说“这仅仅是创造理解的元价值的输出”或者“这只是一种工具策略，创建一个玩具模型来创造理解”，那么我们会接受什么作为真正的价值？ [关于“我们会接受什么”的讨论完全是一个红色警报，我已经陷入了糟糕的未降级哲学，但我断言这里无论如何都有一些东西，即使我还没有清楚地了解它。]</li></ul><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:32:35 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:32:35 GMT" user-order="1"><p>烹饪似乎是一个很好的澄清例子……忠诚机构？……实际上。<br><br>你对你要做什么有一些概念，并且知道你会以某种方式搞砸它，而且你对它的解释会受到你现有原料的“环境”的影响（以及你自己作为厨师的不足） ，也可能是在你制作时发生的“灵感来袭”的情况）。<br><br>但是，除非你非常倾向于烹饪艺术（除了厨房水槽炒菜之外的一切都以这种哲学而闻名），否则你在开始掌握的内容时可能确实有一些模糊的、有缺陷的概念向。<br><br> （我听说烹饪中有一些从烘烤到炒的合法/混乱轴）</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:38:51 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:38:51 GMT" user-order="2"><blockquote><p>好吧，人们可以更新他们的自我意识，然后有时会做一些疯狂的事情来使他们的行为与自我意识保持一致。</p></blockquote><p>就像出生在内群体中的人一样，他随后了解了外群体倾向于说而内群体倾向于不说的一些信息，并开始同情外群体并寻求更多此类信息，而死亡则螺旋式地成为外群体主义者。<br><br>这里有东西引起了我的注意。一方面，我们要“触底反弹”。我们希望获得真正的核心价值观。另一方面：</p><ol><li>我们的一些“仅是从属的、仅是次目标、仅是工具性的、仅是对象层面的、仅是过程的产物、仅是起点反射”的价值观本身就是元价值观。</li><li>我不知道真正的价值观是什么样的。 （诉诸 VNM 实用函数有一些重要的内容要说，但我认为这不是答案。）</li><li>从这个意义上讲，可能不存在触底价值。</li><li>我们被创造出来就已经在运动了。当我们试图判断、重新评估“客体/从属”价值时，我们所呼吁的，也许只是由更多的“客体”价值构成。运动中产生的是“反射”，它选择我们在由我们称为值的所有自由参数描述的空间中进行的调整。</li></ol><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="qKdS7bw9McaSeMNbY-Sun, 15 Oct 2023 21:43:02 GMT" user-id="qKdS7bw9McaSeMNbY" display-name="Spiracular" submitted-date="Sun, 15 Oct 2023 21:43:02 GMT" user-order="1"><blockquote><p>如果有人喜欢下棋，我们通常将其称为值。</p></blockquote><p>哦！我想区分以下两种下棋的人：</p><ul><li>爱丽丝（Alice）是一位狂热且痴迷的国际象棋棋手，只是国际象棋（如果我完成这个比喻，可能会加入某种竞争性联盟，具有相当高的 ELO 评级）</li><li> Bob，他将 5-10% 的时间花在以下一项上：数独、国际象棋、俄罗斯方块</li></ul><p>...我将其描述为具有非常不同的潜在价值观，推动他们对国际象棋的积极价值分配？</p><p>就像，假设这对他们双方来说都是一笔巨额投资，我会推断：爱丽丝下棋是因为她高度重视{卓越、完美主义、竞争}，而鲍勃可能看重{谜题、休闲游戏作为休闲，也许是数学}。</p><p>这极大地影响了我对他们每个人的“国际象棋的可行替代方案”的看法；也许爱丽丝可以把她花在国际象棋上的时间换成竞技网球，但鲍勃会发现这完全不能满足他玩游戏的动机。</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">气孔</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:44:13 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:44:13 GMT" user-order="2"><blockquote><p>在你开始掌握的东西时，你可能确实有一些模糊的、有缺陷的概念。</p></blockquote><p>诺拉·阿曼 (Nora Ammann) 举了克莱尔 (Claire) 的例子，她喜欢爵士乐。我的解释是：起初克莱尔听爵士乐有一些不太深刻的理由。也许一个朋友对此感兴趣，或者她认为她应该探索文化，或者其他什么。起初她不太喜欢这样；很难理解，也很难听；但其中也有一些有趣的火花吸引着她。她听得更多，对习语和假设的掌握更加流畅，并开始根据新发现的品味指导她的听力。一段时间后，她现在处于一种真正珍视爵士乐的状态，而且是为了爵士乐本身。它让她瞥见有趣的思维方式，它振奋了她的精神，它与她分享忧郁。这些看起来像是真正的价值观，而且说这些价值观“一开始就存在”而不是作为一个指针似乎是不正确的……好吧，但我再次猜测自己；其中很多都可以被描述为隐藏的渴望找到了出路？</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="LtHeYhWmaud6YNA3m-Sun, 15 Oct 2023 21:45:48 GMT" user-id="LtHeYhWmaud6YNA3m" display-name="TsviBT" submitted-date="Sun, 15 Oct 2023 21:45:48 GMT" user-order="2"><p>我们正在叫停；感谢您的参与！</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> TSviBT</section></section><br/><br/> <a href="https://www.lesswrong.com/posts/fijSRFL6Z5pXBbCgi/hints-about-where-values-come-from#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fijSRFL6Z5pXBbCgi/hints-about-where-values-come-from<guid ispermalink="false"> fijSRFL6Z5pXBbCgi</guid><dc:creator><![CDATA[Spiracular]]></dc:creator><pubDate> Wed, 18 Oct 2023 00:07:58 GMT</pubDate> </item><item><title><![CDATA[Labs should be explicit about why they are building AGI]]></title><description><![CDATA[Published on October 17, 2023 9:09 PM GMT<br/><br/><p>三个大型人工智能实验室表示，他们关心一致性，并认为失调的人工智能对人类构成潜在的生存威胁。这些实验室继续尝试构建 AGI。我认为这是一个非常糟糕的主意。</p><p>大型实验室的领导者很清楚，他们不知道如何构建安全、一致的通用人工智能。目前最好的计划是将问题交给（不同的）人工智能， <span class="footnote-reference" role="doc-noteref" id="fnref3xloy7ni78g"><sup><a href="#fn3xloy7ni78g">[1]</a></sup></span>并希望能够解决它。当你不知道如何控制 AGI 时，尝试构建 AGI 显然是一个坏主意，特别是如果你承认 AGI 失准可能导致灭绝。</p><p>但有一些原因使得尝试构建 AGI 成为一件更合理的事情，例如：</p><ul><li>他们想首先构建 AGI，因为他们认为这比安全性较低的实验室构建它要好</li><li>他们担心多极格局</li><li>他们担心来自其他国家的竞争，特别是来自中国的竞争</li><li>他们认为人们需要能够使用大型模型才能对齐更大的模型，<i>并且</i>还有其他一些因素意味着我们很快就会有需要对齐的更大模型</li></ul><p>我认为实验室应该明确表示他们正在尝试构建 AGI <span class="footnote-reference" role="doc-noteref" id="fnrefbzo8kc4z7wc"><sup><a href="#fnbzo8kc4z7wc">[2]</a></sup></span> ，并且这并不安全，但有特定的原因使他们认为这是最好的行动方案。如果这些具体原因不再成立，那么他们将停止扩展或尝试构建 AGI。他们应该清楚这些原因是什么。实验室应该向公众和政策制定者明确说明这一点。</p><p>我想要这样的声明：</p><p><i>我们正在尝试建立通用人工智能，这是非常危险的，可能会导致人类灭绝。我们这样做是因为我们所处的具体情况。</i> <span class="footnote-reference" role="doc-noteref" id="fnrefnuickx0l9lh"><sup><a href="#fnnuickx0l9lh">[3]</a></sup></span><i>我们希望我们不必这样做，但考虑到世界的现状，我们觉得我们必须这样做，而且这样做会减少人类灭绝的机会。如果我们不处于这种特定情况，那么我们将停止尝试构建通用人工智能。如果我们注意到[对世界的具体、可验证的观察]，那么我们会强烈考虑停止构建 AGI 的尝试。</i></p><p>如果没有这样的声明，我认为如果其他人认为他们正在鲁莽地尝试构建通用人工智能，实验室不应该感到惊讶。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn3xloy7ni78g"> <span class="footnote-back-link"><sup><strong><a href="#fnref3xloy7ni78g">^</a></strong></sup></span><div class="footnote-content"><p>要么是自动对齐研究人员，要么是与可扩展监督有关的事情</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbzo8kc4z7wc"><span class="footnote-back-link"><sup><strong><a href="#fnrefbzo8kc4z7wc">^</a></strong></sup></span><div class="footnote-content"><p>或者将人工智能系统扩展到未知的安全水平</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnuickx0l9lh"><span class="footnote-back-link"><sup><strong><a href="#fnrefnuickx0l9lh">^</a></strong></sup></span><div class="footnote-content"><p>重要的是，他们要具体说明是什么情况迫使他们构建 AGI。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi<guid ispermalink="false"> 6HEYbsqk35butCYTe</guid><dc:creator><![CDATA[peterbarnett]]></dc:creator><pubDate> Tue, 17 Oct 2023 21:09:20 GMT</pubDate> </item><item><title><![CDATA[Eleuther releases Llemma: An Open Language Model For Mathematics]]></title><description><![CDATA[Published on October 17, 2023 8:03 PM GMT<br/><br/><blockquote><p>今天我们发布了<i>Llemma</i> ：70 亿和 340 亿参数的数学语言模型。 Llemma 模型使用 Code Llama 权重进行初始化，然后在 Proof-Pile II（一个包含 550 亿个数学和科学文档的令牌数据集）上进行训练。由此产生的模型显示出改进的数学能力，并且可以通过提示或额外的微调来适应各种任务。</p><p>我们的工作与<a href="https://blog.research.google/2022/06/minerva-solving-quantitative-reasoning.html">Minerva</a>类似，Minerva 是谷歌研究院去年开发的专门用于定量推理的模型套件。虽然我们没有达到与 Minerva 完全相同的规模，但我们的 Llemma 模型在等参数基础上表现更好。此外，我们使我们的<a href="https://huggingface.co/EleutherAI">模型</a>和<a href="https://huggingface.co/EleutherAI">数据集</a>开放访问，并使我们的<a href="https://github.com/EleutherAI/math-lm">代码</a>开源。</p><p>具有强大数学推理能力的语言模型是奖励建模、算法推理、形式数学等众多新兴研究领域的上游。我们希望通过为研究人员提供更强大的推理应用基础模型，Llemma 将加速解决这些问题的进展。</p><p> Proof-Pile-2 的代码子集为 Llemma 赋予了 Minerva 所缺乏的功能，而无需进行额外的微调。在这篇博文中，我们将讨论<i>形式定理证明</i>。我们的论文包含有关 Python 辅助问题解决任务的其他结果。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for<guid ispermalink="false"> HvgfNjihcDjCTE</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Tue, 17 Oct 2023 20:03:46 GMT</pubDate> </item><item><title><![CDATA[Investigating the learning coefficient of modular addition: hackathon project]]></title><description><![CDATA[Published on October 17, 2023 7:51 PM GMT<br/><br/><p>作为我们在<a href="https://devinterp.com/events/2023-q3-melbourne-hackathon">墨尔本黑客马拉松</a>上关于<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>和对齐（10 月 7-8 日）的项目，我们做了一些实验来估计盆地单层<a href="https://www.neelnanda.io/mechanistic-interpretability/modular-addition-walkthrough">模加任务</a>的<i>学习系数</i>，这是衡量信息复杂性的不变量（阅读：完全训练的神经网络的程序长度）。</p><p>我们以<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau、Murfet 和 Wei</a>最近的论文为出发点；本文估计提供了学习系数的随机估计（它们表示<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>）通过朗之万动力学。 <a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">Watanabe 在一篇精彩的论文</a>中证明，由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量的热力学量渐近收敛于理想化奇异系统的学习系数。</p><p>所描述的实验的所有代码都可以在<a href="https://github.com/nrimsky/devinterp"><i>此 GitHub 存储库</i></a><i>中找到</i><i>。</i></p><h1>简要结果</h1><p>在我们的测试中，我们惊喜地发现，对于模加模素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的任务，我们实施 Lau 等人的 SGLD 方法的输出是（达到大致恒定的小乘法误差，小于因子 2）与理想化单层模加法网络<span class="footnote-reference" role="doc-noteref" id="fnrefx9z3e6c3n7"><sup><a href="#fnx9z3e6c3n7">[1]</a></sup></span>的理论预测结果完全一致。</p><p> <a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人迄今为止已经在双神经元网络中获得了类似的结果。</a> <a href="https://arxiv.org/abs/2310.06301">Chen 等人的 12 神经元网络。</a>我们的结果首次证实了中型网络（约 500 至 8,000 个神经元）的估计值与理论结果之间的一致性。</p><p>虽然我们的结果与单个模加电路的理论值相差很小的乘法因子，但我们发现了一个非常精确的现象，与理论预测完全匹配，即对于泛化的模加网络，学习系数估计在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中是线性的;这是同类中第一个精确的缩放结果。 </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>对于 5 个不同的 p 值，模加网络上<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的平均测量：线性程度非常显着。</figcaption></figure><p>此外，使用模加法任务作为测试用例可以让我们仔细研究<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>复杂度估计在神经网络中区分泛化和记忆的能力：这似乎是新的东西（尽管与一些相变现象有关）陈等人）。我们观察到，虽然泛化在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有线性学习系数，但记忆在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有（大致）二次增长；这再次显示出与理论的显着一致性。 </p><figure class="image image_resized" style="width:70.45%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7"><figcaption>我们针对仅记忆网络的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量。这些与理论预测值非常一致（此处，理论预测<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda = 0.8 p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></figcaption></figure><p>与理论的一致适用于质数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的多个不同值和多个架构。它们对于学习不同数量电路的网络也有适当的行为；其他有效维度估计量（例如 Hessian 特征值估计）往往会高估复杂性的情况。</p><p>此外，我们表明<i>动态</i><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计<span class="footnote-reference" role="doc-noteref" id="fnrefy802tk8mheb"><sup><a href="#fny802tk8mheb">[2]</a></sup></span> ，即训练期间的估计，似乎跟踪学习的记忆与泛化阶段（尽管事实是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计仅取决于训练数据）。为了看到这一点，我们对动态估计器进行了轻微的改进，我们将采样限制在初始化时梯度向量的法线超平面内，这似乎使这种行为更加鲁棒。 </p><figure class="image image_resized" style="width:73.54%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qgdxysf9q1nbayda4hru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vy7xdw29olifj42mtoqi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lh3w1j5xwlc1brqjij3d 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gma5o3xugaqirlq1f1rv 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jieohfzfkyzvh6nw9ppf 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fgves9m6no1m5sjv0ja4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r2vrqdzyqkug7mibbwgo 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uzr5rwe0k0y40bls9ft0 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hzcphrkcu8y27yie4wzw 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。</figcaption></figure><p>我们的动态结果与<a href="https://arxiv.org/abs/2310.06301">Chen 等人的一些 SGLD 研究结果相似。</a> ，这表明动态 SGLD 计算有时可以注意到相变。我们惊喜地看到它们在更大的网络中以及在记忆与泛化的背景下保持不变。</p><p>总的来说，我们的研究结果使我们更加相信奇异学习理论技术和思想在现实世界中的适用性。更具体地说，我们现在相信，类似于 Lau 等人的 SGLD 采样的技术应该能够区分工业规模神经网络中的不同泛化行为，并且可以成为一个有点强大的无监督解释性工具箱的一部分，以及对以下方面有价值的控制技术：结盟。</p><h1>背景</h1><h2>关于学习系数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>的基础知识</h2><p>有关替代介绍，请参阅<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">Jesse 和 Stan 解释学习系数的优秀帖子</a>（在我们写完本节后发布，但遵循类似的方法）。</p><p>学习系数是与泛化相关的参数。它控制“在随机选择权重的情况下，它们产生的损失在最优值的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>范围内的可能性有多大”问题的一阶渐近行为。换句话说，将最优解推广到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>精度范围内有多容易。当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>变为零时，该概率多项式变为零，作为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>的指数，因此</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Prob}_{\text{Loss}<\delta_L} = \delta_L^{d/2}+\text{lower order corrections.}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">概率</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.242em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">损失</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.284em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-stack" style="vertical-align: -0.327em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.084em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">低阶修正。</span></span></span></span></span></span></span></p><p> （出于技术原因，使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>而不是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span> 。）</p><p>这样一个术语（通常用自由能来定义：这里<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>是<i>温度</i>）更普遍地出现在统计物理学中（并且在量子场论中有近亲）作为“微扰膨胀”中的主要指数。在神经网络中，指数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span>被称为<i>学习系数</i>或 RLCT（“实对数规范阈值”，代数几何中的术语）。</p><p>学习系数包含有关学习问题的“类维度”信息，可以理解为<i>有效维度</i>或“真实维度”的度量，即神经网络需要“正确猜测”的权重参数的真实数量。 net以最小的损失解决问题。特别是，如果通过包含不影响可学习算法集的冗余参数来扩展神经网络（例如，由于问题的对称性），则可以证明学习系数不会改变。请注意，如果机器学习问题的解决方案集足够单一（我们在本文中不会遇到这种情况），则学习系数可以大于最小值集<span class="footnote-reference" role="doc-noteref" id="fnref1xv13wjmuli"><sup><a href="#fn1xv13wjmuli">[3]</a></sup></span>的实际维度，并且确实可以是非整数。</p><h2> Watanabe-Lau-Murfet-Wei 估计， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{\lambda}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span></h2><p>事实上，定义为真正渐近线的学习系数仅包含<strong>奇异</strong>网络、现实生活中从未出现的理想化系统的重要信息（就像噪声算法的两次迭代不可能给出完全相同的答案一样，所以它对于具有任何随机性的网络来说，都不可能具有奇异最小值或正维最小值集合）。然而，在有限但较小的温度值（即，损失“灵敏度”，由上述<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>测量）时，计算相关自由能的问题（并因此在“有限粒度级别”获得有意义的泛化相关参数） ”）是容易处理的。</p><p> Lau 等人<a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">渡边的论文</a>。下面给出了这种类型的公式。该论文的结果不仅取决于损耗灵敏度参数（称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span></span></span></span></span> ，来自统计物理文献中的逆温度），还取决于样本数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。该公式给出了神经网络在“真实”数据分布上的学习系数的渐近精确估计，对应于样本数 n 趋于无穷大时的极限。当 n 趋于无穷大时，Watanabe 将温度参数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>归零为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\log(n)/n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。 Lau 等人的论文着手在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>的有限值下执行此测量。</p><p>拥有一个好的学习系数估计器对于可解释性来说非常有价值：这将是一个以非常有原则的方式捕获算法的信息论复杂性的参数，避免了先前已知方法的严重缺陷（例如 Hessian 简并性的估计） ）并且对于分布外检测很有用。更一般地说，<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>计划提出了某些强大的无监督可解释性工具，可以提供有关网络内部结构的信息，假设可以有效地计算学习系数（和某些相关量）。</p><h2>模加法作为估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ 的</span></span></span></span></span></span></span>测试平台</h2><p>在 Lau 等人的论文中，他们将基于 SGLD 的学习系数估计应用于微型双神经元网络和 MNIST 网络，并取得了可喜的结果。我们将模加法网络视为一个有趣的中间案例。模加法必须证明以下事实：</p><ul><li>它是一个机械解释的网络：我们知道它的电路，或多或少知道它们是如何由神经元实现的，以及如何隔离和测量它们。</li><li>我们可以清楚地区分学习泛化的网络和仅通过查看其电路来记忆的网络；此外，我们可以通过创建一个学习随机交换运算的网络来“欺骗”泛化；这是一个与模加法具有相同记忆行为的网络，但没有泛化的可能性。</li><li>此外，我们可以计算网络学习的泛化电路的数量，并推理不同电路在损失函数和某种程度上理想化的自由能计算中如何相互作用。这使我们能够将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>相对于电路数量的行为与其他复杂性概念（例如 Hessian 秩）进行比较。 </li></ul><figure class="image image_resized" style="width:56.81%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bxrvulcijbvpfbwqq8nc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/yfj2uoxea9fget94o1bp 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/pmqnsdp83vlwisdqrts5 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5lxabqho3gjqhl0t19p 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtqbn1vaxa4usvxpvjzt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/orlxoh7x5gw0untqhvb4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xhqap8twezvsfobaw6ol 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e4h9vlg6b3z4ovs90ldw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gihdgdfa0bt0mekv7jjl 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwgypqjwa8dxmqfinby7 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xatuu74shphxc5fouhqw 800w"><figcaption>受模加法训练的网络的保留测试集上的损失图。每个步骤（不同颜色的线）对应于训练过程中均匀分布的检查点。当嵌入权重矩阵中除此之外的所有傅里叶模式都被消融时，每个点对应于损失。仅保留单个重要模式对损失的影响比仅保留不重要模式要小得多，这证明了在嵌入矩阵中使用“grokked”傅立叶模式。</figcaption></figure><p>同时，作为一个算法生成的问题，从 SLT 的角度来看，模加法有一些重要的局限性，这使得它无法捕获典型学习问题的一些复杂性：</p><ul><li>模加法的可能数据点总数是有限的（即，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>素数模而言，等于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ），并且目标分布是确定性的。因此，学习系数仅取决于有限数量的样本，这使得从统计学习理论的角度来看，渐近问题略有（但不是完全）退化。</li><li>即使在简单的确定性机器学习问题中，模加法问题也是高度对称的；因此，我们的经验结果可能无法推广到不太对称的网络。</li><li>与最大样本数相比，可能的输出标记数量较多<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">（p 个</span></span></span></span></span></span></span>标记与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2 个</span></span></span></span></span></span></span></span></span>样本相比，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>为模）可能会导致异常行为（Watanabe 的结果假设 logits 数量很小，并且样本数量渐近无限） ）。</li></ul><p>尽管存在这些限制，我们观察到（对于超参数的适当选择）Watanabe-Lau-Murfet-Wei 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>给出的学习系数估计在很大程度上与理论预测一致。此外，这些估计的表现非常一致和稳定，这是我们没有预料到的。</p><h1>发现<strong>&nbsp;</strong></h1><p>我们发现，对于完全训练的网络，使用渡边公式的 SGLD 估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是对于模加法（在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中呈线性，相当独立于总数量）参数）和随机网络（ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中的二次）。此外，它独立于原子电路或“groks”的数量（我们期望在适当的限制情况下，学习系数的情况如此，但有效维度的其他计算则不然）。 </p><figure class="image image_resized" style="width:54.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gkqtx00nee58ax9d8h6r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/v39xlwaw87bgo6362h90 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vzf0xyjpj3qeacyq7w9k 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ipm9sbglftb2pbfcxepy 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zqriirvq41cc476zoq7p 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dbul9hx7ts7zgvennlbb 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sqekzx3knephdqxl9gqu 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k1xur59h3ai0ewydhure 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qw2a27cqsxs4u1dv7sby 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iptxfrv0eclatokb110l 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/odmceomuhkcwtp4bshet 1071w"><figcaption>我们在模加法任务上训练的模型图。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维一热编码数嵌入在 embed_dim 空间中。学习两个独立的线性变换到一个hidden_​​dim空间。然后，这两个向量按元素相加，通过 GELU 激活函数，然后转换回<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维 logits 向量。</figcaption></figure><p>我们还在模加法网络的学习轨迹上的不稳定点上对<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>进行了一些“动态”估计。在这里，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。</p><h2>泛化网络的扩展行为</h2><p>我们在以下网络上运行了 Watanabe-Lau-Murfet-Wei <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计算法，并得到了以下结果。我们绘制了针对每个素数的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值，并在五次实验中取平均值。</p><p>我们发现使用渡边公式的估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是模加法还是随机网络： </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>使用 SGLD 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的图，用于在模加模不同素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>上训练的 MLP。此处，所示的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>是五次独立训练和采样运行的平均值。 SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。我们可以看到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>成线性比例。 </figcaption></figure><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/t9eayflywv3vapupbtmj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ua5hlgflxv0bl3w77thr 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lsguicplrrqohlxsncbl 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k0lxw3ctzrcs9v3vsljl 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uncajiodsmfpwnyshmhy 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vn1u9t45nqetihj6q5li 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kwvgjbjas8jm6qsvezu7 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rkpjklemrinodgskueqp 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a3xzlm5in1woa0x9qwkj 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>对不同大小的 MLP 网络的单次运行进行估计，证明了 RLCT 跨尺度的相似性。 </figcaption></figure><figure class="image image_resized" style="width:59.29%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m3raqy0b7opujni5bnyj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fufnptgi5q1u1dddxyqs 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkmc2tjnisaxx9iv4o5m 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gdspfdd2xo9rtb1gfmf6 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jiatr65txqrzfzq77ere 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r6ajczqfbexgag3wiopq 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bi1vdtqsotjoyu0zqbay 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwjc3g0ljryippa4iosw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/n5ntt4zn8zbfa8qqfrao 640w"><figcaption>这里，不同的运行对应于单独训练的网络系列，证明<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>在具有相同架构的模型之间是一致的，经过训练以在相同数据集和任务上进行收敛。</figcaption></figure><p>我们观察到，在给定的架构中，我们的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计非常接近线性，正如理论上预测的那样。</p><p>原则上，采用这种架构解决模加法的模型的最小有效维数是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> （这将在推导模加网络结果的单独理论文章中详细阐述）。然而，我们观察到经验比例因子非常接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8p = 2\times 4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，是单个电路结果的两倍。对于这个结果的一个可能的解释是，在我们的模型所处的体系中，解决方案的有效空间由执行至少两个简单电路的权重参数组成（我们训练的所有模型都至少学习了 4 个简单电路）。 </p><figure class="image image_resized" style="width:47.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rpnrzizva3nnkmapqsmi" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkefqplmw6n6rvzaf6zq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/abfwqanxihyprp3gismw 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xoi6gyakkr5xmjzwkr3u 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/chmzavciqpkpbw5tqxb0 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jw9pwvardy4hhr4sv7vk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dd1csgtod5ww9efhgvjq 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bzehulggnguktxcyg45d 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wpgopmxspzuqu26bmv9j 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的比例因子接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span>这可能表明 SGLD 搜索过程探索了对应于两个 grokked 电路而不是一个电路的一系列近极小值。请注意，所有经过训练的模型总共训练了 >;2 个电路，并且电路数量各不相同，因此我们仍然发现所学习的独立电路总数具有不变性。</figcaption></figure><p>当开始实验时，我们预计经验值和预测值之间存在超过一个数量级的广泛差异（因为现实模型的非理想性质和我们实验中的限制点）。相对较大且混乱的“现实世界”测量与理想测量之间的这种程度的一致性，以及这里的近线性，绝不是可以保证的，并且使我们相信奇异学习的理论预测有很大程度的改变。理论与现实世界的测量结果非常吻合。</p><p>我们还在各种架构上重复实验，参数数量相差一个相对较大的因子（我们最大的网络比最小网络大3倍多，我们的中间网络渐近是最小网络的两倍）。较大的网络确实具有稍高的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> ，但差异在网络大小上呈次线性缩放，正如我们对真实学习系数的预期一样。</p><p>请注意，我们包含的素数相对较小。虽然我们的架构非常高效<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">并且</span></span></span></span></span></span></span></span></span></span></span></span>对于更大的素数总是能够泛化（接近 100% 的准确度），但我们凭经验观察到，当完全训练的网络非常接近收敛（0损失）。由于计算限制，我们使用相对较大的学习率 (0.01) 进行相对较少的迭代次数。这会导致 50 以上的素数在收敛时损失更严重；我们推测，如果我们使用计算量更大、学习率更小的方法和更多的 SGD 步骤，那么对于更大的素数，近线性行为将继续成立。</p><h2> (in)依赖于电路数量</h2><p>我们训练的网络有时会学习嵌入不同子空间中的不同数量的独立泛化电路（此类电路的存在首先由<a href="https://arxiv.org/abs/2301.05217">Nanda 等人</a>提出）。</p><p>我们可以通过考虑嵌入空间中的大离群傅里叶模式来测量网络学习的电路的数量和类型，或者（更稳健地）通过在“傅里叶模式对齐”二维投影中寻找近乎完美的圆。嵌入空间<span class="footnote-reference" role="doc-noteref" id="fnrefyim8grymza"><sup><a href="#fnyim8grymza">[4]</a></sup></span> ，如下图</p><p><i>（我们计划稍后发表另一篇文章（关于模块化加法的机械解释工具，特别是准确地区分</i><a href="https://arxiv.org/abs/2306.17844"><i>“披萨”和“时钟”电路</i></a><i>），其中将对这些图片进行更多解释。）</i> </p><figure class="image image_resized" style="width:51.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uria5hpsbh4aoqokve6p" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lembcon39vupovktlmno 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtl1wulgwzyefwcknlux 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ndgtvirh22no4fpprlit 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/diptixpqr04aiyoqhffi 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xi7gkcz3grwfnlxhotng 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5la9dj9km731vtg1rds 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rccsyop3ofkqopkkmygm 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vrw9fyefggwqfdq27rtb 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ixxxg8bckqccyaxmxjvg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bn9plxzaug1db0k5wtvt 1000w"><figcaption>通过将学习到的嵌入权重矩阵投影到嵌入空间的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(p-1)/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>二维子空间（对应于嵌入空间中可表示的不同离散傅立叶模式），我们可以看到独立傅立叶模式电路的数量。例如， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=43"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">43</span></span></span></span></span></span></span>的模型已学习 6 个傅里叶模式 - <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4,8,14,15,19,21"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">14</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">15</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">19</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">21</span></span></span></span></span></span></span> 。</figcaption></figure><p>我们在实验中观察到，学习率估计似乎不太依赖于学习的电路数量。例如，对于我们考虑的最大素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p = 53"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">53</span></span></span></span></span></span></span> ，在不同运行中学习的电路数量在 4 到 7 个圆形电路之间变化，而所有网络的学习系数彼此之间大约在 10% 以内。这个结果看似简单，但从理论角度来看却非常有趣且有些令人惊讶。</p><p>例如，当通过 Hessian 特征值测量网络的有效维度时，具有多个电路的网络将具有有效维度 0（因为沿着与任何电路对应的方向都算作泛化）或线性依赖于电路的数量（因为只有当一个方向独立地概括每个电路时才算作概括）。事实上，这些行为在我们的环境中都没有被观察到，这一事实可以由单一学习理论框架来推动。事实上，我们可以将执行每个电路（或者可能是电路的合适的小子集）的权重空间中的子空间视为“近最小值”的奇异流形的单独组件。由于与不同电路相关联的向量空间相对于彼此处于一般位置，因此产生的奇异性是“最小奇异性” <span class="footnote-reference" role="doc-noteref" id="fnref0vw69e6p2rtp"><sup><a href="#fn0vw69e6p2rtp">[5]</a></sup></span> 。这意味着奇点处的 RLCT 等于沿每个单独分量的 RLCT，这可以理解为对观察到的独立结果的解释。然而，我们注意到，尽管其解释性稳健，但当我们放大时，这张图会变得更加复杂，因为多电路网络的损失往往比其各个部分的乘积要好得多。</p><p>我们计划对独立性结果给出另一种解释，涉及交叉熵损失的统计模型，该模型利用模素数乘法的遍历性。我们在此指出，我们期望这种独立性仅在超参数选择的“金发女郎”范围内保持，特别是正则化常数（对应于所学习的电路的大小）。一个简单的统计模型在这里预测至少三个不同的阶段：一个是电路尺寸非常小的（对应于大正则化），我们预计电路的数量会成倍地影响学习率。一种是在大电路尺寸（小正则化）下，学习率估计变得退化，另一种是在中间区域，我们看到的独立结果是有效的。</p><h2>随机操作：记忆与泛化的扩展</h2><p>为了将我们的泛化网络与具有相同架构（仅记忆）的网络进行比较，我们对随机交换运算网络运行了 Watanabe-Lau-Murfet-Wei 算法。</p><p>为了让记忆网络获得良好的损失，我们需要对其进行过参数化，即参数的数量需要高于样本总数的某个适当的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(1)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>倍，在我们的例子中为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 。由于参数数量在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中线性增长，因此仅当 p 值较小时，我们才能收敛到接近零损失。我们注意到，由于像中国剩余定理这样的数论技巧与随机操作网络无关，因此该实验的 p 值不需要是素数。因此，我们对 5 到 40 的倍数进行此实验。由于收敛问题和缩放模式观察，我们最信任 5 到 25 之间的较短值范围内的结果。</p><p>请注意，此范围仅在 23 到 25 之间与我们的素数列表重叠；我们需要使用更大的网络（并且可能需要更好的学习收敛）来获得高于此范围的合理的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>值。对于我们考虑的值范围，与泛化网络的线性线性相比，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有二次缩放模式的更大学习系数。 </p><figure class="image image_resized" style="width:57.82%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c5xikew9mkdlst7oghv9 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/le4z9svtxnjexffsn3si 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fibwlum3sl4elyejywk1 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jsr1wasqnbfrlxydobix 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c4hvxln3kellgpyamxk5 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vhpntzv0hfycuigby5tt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dtfjmajdgm8mkfbrowjx 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kd7ksxbubypsvqdtnczd 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的随机交换运算。 </figcaption></figure><figure class="image image_resized" style="width:57.52%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ea8nruy9slra7rnaoi50" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gjqy14wu9usujdygfosc 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jk0qobc1glrbzb5zmudq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e3fvdzjforrd4bulesyp 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/tziuy8i1qvmog3pkkybt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/cjx7ag430jml3yc07qus 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/f6jtk4r7lbuewww3oypc 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/swvt3nusrdixsfnynvty 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/luryrwfiumhvhzmyycv0 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span>随机交换运算的<span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>与模加结果一起绘制。</figcaption></figure><p>值得注意的是，p = 25 的图表几乎完全（直到恒定偏移量）等于记忆数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.8\cdot p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ；这里 0.8 是用于训练的完整数据集的一部分。我们还生成了 5 的更大倍数（最多 40）的数据。在这里，我们清楚地看到，在相同架构下，记忆网络比泛化网络具有更高的学习率，但当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p > 25"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">25</span></span></span></span></span></span></span>时，二次拟合变得更差。我们相信，如果我们使用更大的网络，我们将恢复更多 p 值的二次拟合。</p><h2>动力学和相变</h2><p>最后，我们在泛化网络的学习过程中对各个检查点的学习系数进行了<i>动态</i>估计。</p><p>在我们的这部分结果中，我们介绍了<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人的方法的一些创新。</a>和<a href="https://arxiv.org/abs/2310.06301">陈等人</a>。 （尽管我们没有实现后一篇论文中的“基于健康”的采样轨迹排序）。具体来说，我们通过温度调整以及在限制正常超平面损失梯度后应用不稳定的 SGLD 实现获得了最佳结果。 </p><figure class="image image_resized" style="width:54.37%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q1sklqenvvtrvmsezcqy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xbdikzy1xefx5d0lk0sm 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/omo9gl1nkeupoo6cfb6h 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m7x46oyonwnbdll9twai 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bjqxhr02dbf6hoqwylxr 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zvbgfmsb6xbp5i7k2t7k 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a6cxughqqoorqstjtzds 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/b7zi8nz9pjgn5btqrwdl 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sdrpizjr58fqumlrni37 640w"><figcaption>在模型训练过程中，在 25 个等距检查点处运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计。 SGLD 搜索经过修改，将搜索方向限制为与初始化时梯度正交的方向。 </figcaption></figure><figure class="image image_resized" style="width:53.69%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vexyifqwkko2uecghzia" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/s04vahbbg7ngmfzzucgi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wgih1szglxj6dkjuijr0 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uscfm3augmtcocwn4ucm 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/o83yc9csdbjypxmuuf5j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hgboz1tsob3q3m8flbzu 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qu9rxcxgkpsupioaz2gk 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/csmwtmkawpbgxxxoalwn 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/heyla9f6gnxi3xvzohdd 640w"><figcaption>使用另一个模型/独立 SGLD 采样重复运行，以检查结果的一致性。</figcaption></figure><p>在这里，我们观察到不稳定的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。 （由于我们的网络非常高效，因此这种情况在训练的早期就发生了。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnx9z3e6c3n7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefx9z3e6c3n7">^</a></strong></sup></span><div class="footnote-content"><p>请注意，<a href="https://paperswithcode.com/paper/quantifying-degeneracy-in-singular-models-via">刘等人。</a>还对具有超过一百万个神经元的大型 MNIST 网络进行<span>\(\hat{\lambda}\\)</span>估计。在这里，他们发现<span>\(\hat{\lambda}\\)</span>的结果值与用于在可预测方向上训练网络的优化方法相关，从而捕获有关盆地的重要信息。但这里无法得到<span>\(\hat{\lambda}\\)</span>的理论值，SGLD算法无法收敛；因此，在这种情况下，该估计预计不会给出学习系数的真实值</p></div></li><li class="footnote-item" role="doc-endnote" id="fny802tk8mheb"><span class="footnote-back-link"><sup><strong><a href="#fnrefy802tk8mheb">^</a></strong></sup></span><div class="footnote-content"><p>请注意，动态<span>\(\hat{\lambda}\\)</span>估计器尝试将专为稳定点（即局部最小值）设计的技术应用于非局部最小值且具有一些不稳定性、采样和遍历性问题的点，甚至通过我们的法线到梯度限制细化。特别是，它们（远远超过稳定点的估计）对超参数敏感。因此，这些不稳定的<span>\(\hat{\lambda}\\)</span>测量目前没有相关的精确理论值，可以被认为是对不稳定点的复杂性估计的临时概括。然而，我们发现，在固定的超参数集合中，这些估计给出了一致的结果，并且在运行过程中看起来很相似，并且我们发现它们包含有关学习过程中损失景观动态的重要信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1xv13wjmuli"> <span class="footnote-back-link"><sup><strong><a href="#fnref1xv13wjmuli">^</a></strong></sup></span><div class="footnote-content"><p>对此的直觉是，非常奇异的损失函数（即，具有许多等于零的高阶导数的函数）与非常大的盆地相关，这些盆地足够大以“适合额外维度的参数”。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyim8grymza"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyim8grymza">^</a></strong></sup></span><div class="footnote-content"><p>与第k个傅立叶模式相关联的嵌入空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\text{embed_dim}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">embed_dim</span></span></span></span></span></span></span></span></span>的二维子空间是由k频率离散傅立叶变换的sin和cos分量所跨越的空间。请注意，这些空间对于不同模式不一定是线性独立的，但对于学习电路的模式是独立的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0vw69e6p2rtp"> <span class="footnote-back-link"><sup><strong><a href="#fnref0vw69e6p2rtp">^</a></strong></sup></span><div class="footnote-content"><p>这是 RLCT 意义上的意思。在代数几何语言中，如果存在平滑解析爆炸<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X\to \mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">使得</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">在</span></span></span></span></span></span></span></span></span>X 上的局部坐标中，f 是坐标函数的平方的乘积，则权重空间 R n 上的函数 f 是最小奇异的。在这种语言中，如果我们有 c 个电路与权重空间中的向量子空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="C_1,\dots, C_c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span></span></span>相关联，则电路的 k 元组上具有最小值的“理想化”函数是该函数</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(w) = \sum_{S\subset \{1,\dots, c\}, |S| = c-k+1} \prod_{i\in S} \text{dist}(w, C_i)^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⊂</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∏</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span> <span class="mjx-mtext MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></p><p>为了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span>运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c-k+1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1 个</span></span></span></span></span></span></span>元素子集，并<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{dist}(w, C_{i_j})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>从权重到相应子空间的 L2 距离。很容易检查所得奇点是否为最小奇异点。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1<guid ispermalink="false"> 4v3hMuKfsGatLXPgt</guid><dc:creator><![CDATA[Nina Rimsky]]></dc:creator><pubDate> Tue, 17 Oct 2023 19:51:34 GMT</pubDate> </item><item><title><![CDATA[When building an organization, there are lots of ways to prevent financial corruption of personnel. But what are the ways to prevent corruption via social status, political power, etc.?]]></title><description><![CDATA[Published on October 17, 2023 6:51 PM GMT<br/><br/><p>如今，即使是相当复杂的会计和审计概念，对于在任何大型公司拥有在职工作经验的人来说也是众所周知的。</p><p>例如控制支出账户、公司卡等资金流动的系统、留下高度清晰的审计跟踪的程序等等。</p><p>在很大程度上，这些方法成功地防止了任何人以过于明显的方式弄乱财务数字，至少在没有留下很多警告信号的情况下是这样。</p><p>然而，在控制那些过度渴望社会地位或政治权力的人方面，方法似乎要少得多。</p><p>通俗文学中讨论的大多只在大规模范围内起作用，比如议会程序、政府多个部门相互制衡等等……</p><p>有哪些行之有效的方法可以在普通组织的规模上发挥作用？</p><p> （比如 100 到 10 000 人）</p><p>如果没有，有哪些理论建议？</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to<guid ispermalink="false"> JBjPreynFn5aDmJsj</guid><dc:creator><![CDATA[M. Y. Zuo]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:51:47 GMT</pubDate> </item><item><title><![CDATA[Eliezer's example on Bayesian statistics is wr... oops!]]></title><description><![CDATA[Published on October 17, 2023 6:38 PM GMT<br/><br/><p>这篇文章<i>将</i>解释 Eliezer Yudkowsky 在统计讨论中经常使用的一个例子实际上并不意味着他认为它的作用。在证明他错误的过程中，我发现我其实错了。我仍在写这篇文章，对于犯与我相同错误的其他人来说，这可能会很有趣。</p><p>这个例子是埃利泽在论证贝叶斯统计优于频率统计时经常使用的例子。在 Sequences <a href="https://www.lesswrong.com/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective">里</a>，在 Arbital<a href="https://arbital.com/p/likelihoods_not_pvalues/?l=4xx">里</a>，在 Glowfic<a href="https://www.glowfic.com/posts/5826">里</a>，还有其他几个我现在找不到的地方。一枚硬币抛六次；前五次出现正面，第六次出现尾部。但我们不知道实验者是否 A) 事先决定抛硬币六次，并报告发生了什么，或者 B) 事先决定将硬币一遍又一遍地翻转，直到反面朝上，并报告花了多长时间。这为我们提供了有关代币偏见或缺乏偏见的哪些信息？</p><p>根据频率论的观点，在这两种不同的情况下分配的 p 值存在严重差异。在第一种情况下，结果 HHHHHT 被放入至少相对于“原假设”极端的结果类别中 - 其中有 14 个：HHHHHH、TTTTTT、HHHHHT、TTTTTH、HHHHTH、TTTTHT 等。总共有 64 种可能性，14/64 = 0.22，远远高于 p=0.05 的水平，因此不足以得出显着性结论。在第二种情况下，结果 HHHHHT 被放入不同的结果类别中：5 个正面，然后是一个尾部，6 个正面，然后一个尾部，7 个正面，然后一个尾部，依此类推。整个类别的概率总计为 1/32，即 0.03 - 具有统计显着性！</p><p>以利以谢从几个方面批评了这一点。首先，频率论者决定将实际结果与一组相似的结果混为一谈的部分是主观的，足以允许对实际结果有很大的自由度。也许在第一种情况下，您不选择具有 5 个或更多同边的结果类别，而是只选择具有 5 个或更多正面的结果类别，从而将 p=0.22 减半至 p=0.11。他还批评了通过“拒绝零假设”来确定显着性的概念，而不是考虑不同的理论效应大小以及它们对数据的预测效果。作为支持完全不同效应大小的证据的两个实验被视为“拒绝原假设”，因此被视为同一理论的证据，即使结果彼此不一致。</p><hr><p>所有这些对频率统计和 p 值的批评似乎都是正确的。但对贝叶斯如何更新的分析是不同的。</p><blockquote><p> ...贝叶斯主义者查看实验结果并说：“我现在可以计算所考虑的所有假设之间的 <a href="http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html">似然比</a>（证据流）。因为你的心态不会以任何方式影响硬币 - 不会改变一枚公平的硬币或有偏差的硬币产生这一精确数据的概率——你私人的、不可观察的心态不可能影响我对你的实验结果的解释。”</p><p>如果您习惯贝叶斯方法，似乎很难想象证据的统计解释应该取决于一个因素，即实验者的心态，而该因素与实验结果没有任何因果关系。 （因为贝叶斯说<a href="http://yudkowsky.net/rational/bayes">证据是关于相关性的</a>，没有因果关系就不可能出现系统相关性；<a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">证据需要纠缠</a>。）</p></blockquote><p>因此，埃利泽认为，两种情况下的似然比显然应该相同，因为唯一相关的数据是抛硬币的顺序。实验者的心态不会改变具有给定偏差的硬币产生该数据的概率，因此它是无关紧要的。</p><p>但埃利泽在这里似乎缺少的关键要素是，数据的总和<i>不是</i>“硬币出现了 HHHHHT”。相反，我们收到的数据是“实验者<i>看到</i>硬币出现了 HHHHHT”。<i>这种</i>证据与实验者的心态有因果关系，因为实验者的心态决定了实验者在什么情况下会看到硬币出现HHHHHT。例如，如果事情的真实情况是硬币确实是公平的，那么实验者遵循“翻转直到出现反面”规则的心态会导致实验者不太可能达到拥有硬币的地步。首先是第六次翻转，因为现在实验者有 31/32 的机会在第六次翻转之前停止。<a href="https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence">证据加上经过过滤的知识通常可以具有与未引用的证据本身不同的属性</a>；实验者决定寻找哪些证据的方法改变了他们可能找到的证据。</p><p>这两种可能性对可能的结果产生<i>非常不同的</i>先验分布。假设硬币的实际偏差使得正面朝上的理论频率为<i>f</i> ：那么在“翻转<i>n</i>次”的情况下，先验分布在长度为<i>n</i>的所有可能序列之间，每个序列的概率为<i>f</i> ^ （序列中的总头数）* (1- <i>f</i> )^（序列中的尾数）。 （在 f = 1/2 且 n = 6 的情况下，这只是简化为 ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{2^6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.819em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.158em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.158em; bottom: -0.919em;"><span class="mjx-msubsup" style=""><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.443em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.819em;" class="mjx-line"></span></span><span style="height: 1.621em; vertical-align: -0.65em;" class="mjx-vsize"></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>）同时，在“翻转直到出现反面”的情况下，先验是 T 的 1- <i>f</i>机会、HT 的<i>f</i> (1- <i>f</i> ) 机会、HHT 的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> (1- <i>f</i> ) 机会、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> (1-f) HHHT 的机会，依此类推，对于总长度为<i>l</i>的序列，始终以<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(l-1)}(1-f)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>结束。这些先验都为不同的结果分配了<i>非常</i>不同的概率 - 事实上，第一个版本允许的许多结果在第二个版本上具有零概率（例如 HTHTHTHT），或者第二个版本允许但在低<i>n</i>版本上具有零概率第一个（如 HHHHHHHHHHHHT 如果<i>n</i> =6。）</p><hr><p>因此，实验过程中发生的概率是非常不同的，具体取决于你是翻转<i>n</i>次还是翻转直到出现反面。这些不同的条件先验是否意味着最终的似然比会有所不同，与埃利泽声称的相反？我也这么想。</p><p>毕竟，贝叶斯更新的工作方式首先是确定在不同假设（ <i>f</i>的可能值）下分配给实验结果的概率，构建这些概率作为 f 的函数的似然分布，并将其乘以您的先前分布以更新它。因此，两种类型的实验条件下的先验分布如此不同，这一事实会导致您乘以的内容不同，从而给出不同的结果。</p><p> （如果您还没有找到我的错误，请随时暂停并在这里查找它。）</p><hr><p>我犯的错误是，我混淆了实验设计的概率分布与<i>实验结果</i>的差异，以及实验将导致您更新的<i>硬币假设的</i>可能性分布的差异。不同的实验设计<i>会</i>导致一些实验结果以不同的频率出现，但这<i>并不自动意味着</i>关于硬币偏差的最终更新会有所不同。</p><p>关于硬币的更新是否不同<i>仅</i>取决于分配<i>给实际发生的结果的</i>概率。如果他们为 HHT 和 HTHTTH 这样的结果分配截然不同的概率，那并不重要，如果实验结果为 HHHHHT 并且他们为其分配相同的概率。事实上，情况确实如此。事实上，虽然实验结果的分布在大多数值上看起来都非常不同，但它们总是会碰巧在实际结果是实验结果的精确值处交叉，通过一些令人惊奇的非巧合。</p><p>发生这种情况的原因是，虽然“翻转直到出现尾部”对于任何给定的<i>f</i>值来说确实是一个常数概率分布，但“翻转<i>n</i>次”也取决于 n 的值，这使得它实际上是<i>n 个</i>独立的分布碰巧彼此相似。如果在“翻转直到得到尾巴”实验中需要 7 次翻转而不是 6 次才能得到尾巴，这并不意味着您突然得到了在任何“翻转<i>n</i>次”分布中不可能得到的结果，它只是意味着您移动到“翻转 7 次”分布而不是“翻转 6 次”，并且该分布上 HHHHHHT 的概率最终将与“翻转直到出现尾部”分布指定的概率匹配。</p><p> （这里的代数很简单。如前所述，“翻转<i>n</i>次”得到给定序列的概率为<i>f</i> ^(序列中总的头数) * (1- <i>f</i> )^(序列中的尾数)序列）。但假设序列中每个值都是头，除了最后一个值是尾，则这会简化为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(n-1)}(1-f)^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> 。这与前面描述的函数相同“翻转直到出现尾巴。”）</p><p>请参阅<a href="https://www.desmos.com/calculator/fkpvlyycod">此处，</a>了解硬币偏差<i>f</i>的似然分布在查看后的实际情况<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <i>n-1</i>个头和 1 个尾，无论使用两个实验设计中的哪一个。</p><hr><p>如果事实证明我在一个简单的事实问题上是错误的，那么我为什么要写这篇文章呢？<i>我</i>确实在这里学到了一些东西，似乎其他人也很困惑，但也能从他们身上学到一些东西。</p><p>第一课是要更加小心地检查概率分布<i>到底</i>告诉你什么！在我最初的计算中，在我开始确定是什么让我困惑之前，我犯了<i>很多</i>错误，其中有几个我什至没有在这里提到（比如最初使用二项式分布来建模，但<i>实际上</i>并不如此）在那里不适用。）大多数错误的本质是“我正在寻找以 x 为条件的某物的概率分布；这里的东西是以 x 为条件的某物的概率分布；因此它是分布我在找。”给定特定类型的实验的实验结果的分布与给定特定结果的观察的假设的似然比之间存在差异；依赖于函数参数之一的分布的任何特定版本与由该参数的所有可能值形成的总体分布类之间存在差异。</p><p>我学到的第二件事是，贝叶斯似然比实际上只取决于每个假设仅分配给您收到的信息的概率，而不是其他任何东西。我以前<i>口头上</i>知道，但没有真正内化。如果两个假设为结果分配相同的概率，并且您看到该结果，则不会告诉您假设之间有任何差异。如果我忽略了对所有可能结果进行量化的尝试，而只是问了一个相对简单的问题：“实验 1 和实验 2 中出现 HHHHHT 的可能性是多少”，我可能可以更快地解决它。</p><p> And then there&#39;s also a <i>possible</i> lesson for me to learn of &quot;see, you really should meta-level trust the reasoning of Eliezer Yudkowsky and other people who have more expertise in a given mathematical domain.&quot; I am not sure this is a good lesson to learn. And I&#39;m also not sure that Eliezer actually saw all of the reasoning I went through in this post about <i>why</i> the two experiments assign the same probabilities to the actual result, rather than just guessing and happening to be correct. That being said, it still is the case that I would have previously given this as an example of a situation in which Eliezer Yudkowsky was wrong about basic probability theory (and I also would have said something like &quot;and he probably made this mistake because of motivated reasoning in order to score points against frequentists&quot;). And he turned out to be right all along. This is more likely in worlds where he knows his stuff more, and I have correspondingly updated my beliefs.</p><p> (I hope this goes without saying, but I&#39;ll say it anyway: a meta-level update towards trusting experts&#39; math, does not mean first-order conforming to their opinions if you don&#39;t first-order agree with or understand them. I&#39;ll still keep trying to notice and point out when it looks like Eliezer is wrong about something - even if I might not bet as strongly that he really does turn out to be wrong.)</p><br/><br/> <a href="https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops<guid ispermalink="false"> BeAHn5CisRgivuspA</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:38:18 GMT</pubDate> </item><item><title><![CDATA[Trying to deconfuse some core AI x-risk problems]]></title><description><![CDATA[Published on October 17, 2023 6:36 PM GMT<br/><br/><p> Max H commented on the <a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1">dialogues announcement</a> , and with both of us being interested in having a conversation where we try to do something like &quot;explore the basic case for AI X-risk, without much of any reference to long-chained existing explanations&quot;. I&#39;ve found conversations like this valuable in the past, so we decided to give it a shot.</p><p> His opening comment was:</p><blockquote><p> I feel like I&#39;ve been getting into the weeds lately, or watching others get into the weeds, on how various recent alignment and capabilities developments affect what the near future will look like, eg how difficult particular known alignment sub-problems are likely to be or what solutions for them might look like, how right various peoples&#39; past predictions and models were, etc.</p><p> And to me, a lot of these results and arguments look mostly irrelevant to the core AI x-risk argument, for which the conclusion is that once you have something actually smarter than humans hanging around, literally everyone drops dead shortly afterwards, unless a lot of things before then have gone right in a complicated way.</p><p> (Some of these developments might have big implications for how things are likely to go before we get to the simultaneous-death point, eg by affecting the likelihood that we screw up <i>earlier</i> and things go off the rails in some less predictable way.)</p><p> But basically everything we&#39;ve recently seen looks like it is about the character of mind-space and the manipulability of minds in the below-human-level region, and this just feels to me like a very interesting distraction most of the time.</p><p> In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead. I also wouldn&#39;t mind recapitulating (my view of) the core AI x-risk argument, though I expect I have fewer novel things to say on that, and the non-novel things I&#39;d say are probably already better said elsewhere by others. </p></blockquote><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 03:48:23 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 03:48:23 GMT" user-order="1"><p> I am quite interested in the &quot;In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead.&quot; part.</p><p> Do you want to start us off with a quick summary of your take here?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Wed, 11 Oct 2023 04:40:06 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Wed, 11 Oct 2023 04:40:06 GMT" user-order="2"><p><br> So, I think there&#39;s a lot of alignment discussion that is missing a kind of inductive step, where you freely assume that you have a running human-level (or smarter) system, and which is thus dangerous by default. And then you ask, &quot;how did that get aligned?&quot; or &quot;why does that not kill everyone?&quot; instead of what I see as a somewhat backwards approach of looking for an alignment method that applies to current systems and trying to figure out whether it scales.<br><br> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p><p> As for why I think results about below-human-level systems are unlikely to be relevant, it&#39;s because all current AI systems are far less Lawful than humans.</p><p> Rephrasing <a href="https://glowfic.com/replies/1660170#reply-1660170">planecrash</a> (and somewhat violating the original ask to avoid long-chained and unwieldy dependencies, but I&#39;ll try to give a useful summary here), Lawfulness is the idea that there are logical / mathematical truths which are spotlighted by their simplicity and usefulness across a very wide space of possible worlds.</p><p> Examples of heavily spotlighted truths are concepts like logical deduction (Validity), probability theory (Probability), expected utility theory (Utility), and decision theory (Decision).  Systems and processes are important and capable precisely to the degree in which they embody these mathematical truths, regardless of whether you call them &quot;agents&quot; or not. All humans embody shadows and fragments of Law to varying degrees, even if not all humans have an explicit understanding of the concepts in words and math.<br><br> Most current capabilities and alignment techniques are focused on trying to squeeze more fragments of Law into AI systems: predictive ability (Probability), deductive ability (Validity), Utility (doing stuff efficiently without working at cross-purposes), and those systems still coming out far below humans in terms of general Lawfulness.</p><p> At the capabilities level of current systems, squeezing more Law into a system can <i>look</i> like you&#39;re making it more aligned, but that&#39;s because at the current level of capabilities / Lawfulness, you need more Law to do even very basic things like following instructions correctly at all.</p><p> Another problem that a lot of proposed alignment methods have (if they&#39;re not actually capabilities proposals in disguise) is that they postulate that we&#39;ll be able to badger some supposedly smarter-than-human system into acting in an obviously unLawful way ( <a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Deep Deceptiveness</a> is an example of what might go wrong with this).</p><p> Also, if you accept this frame, a whole bunch of confusing questions mostly dissolve. For example, questions like &quot;are humans expected utility maximizers&quot; or &quot;is it a problem that ideal utility maximization is computationally intractable&quot; or &quot;but is EU theory even right&quot; have straightforward answers in this framework:</p><p> It&#39;s OK if EU theory as humans currently understand it isn&#39;t entirely strictly correct; EU theory is probably a shadow or approximation for whatever the true Law is anyway, the way that Newtonian mechanics is an approximation for General relativity at the right scale. (Maybe something like <a href="https://www.lesswrong.com/posts/Xht9swezkGZLAxBrd/geometric-rationality-is-not-vnm-rational">Geometric Rationality</a> is the real Law that a supermajority of capable minds across the multiverse would settle on.)</p><p> Similarly for decision theory, it doesn&#39;t matter if some particular human-understood flavor of logical decision theory is exactly correct; it&#39;s close enough to some true Law of Decision that a supermajority of all minds (including humans, if they think for a few decades longer) will be able to see, spotlighted by its mathematical simplicity / optimality / usefulness across a sufficiently wide slice of possible worlds.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 23:23:44 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 23:23:44 GMT" user-order="1"><blockquote><p> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p></blockquote><p> Yeah, this seems right to me.</p><p> I&#39;ve had a long-standing disagreement with many people currently working in prosaic alignment about the degree to which AI systems are quite alien in their cognition (but are trained on the task of imitating humans, so look on the surface like humans).</p><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 02:29:57 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 02:29:57 GMT" user-order="2"><p> I also think current and future AI cognition is unlikely to be human-like, but I don&#39;t think this is a super important crux about anything for me.</p><p> For example, maybe it turns out that actually, the only way to get human-level or better performance is via some algorithm that looks isomorphic to some process in a human brain somewhere.</p><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p><blockquote><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p></blockquote><p> I think another thing to look at is the process, in addition to performance and final outputs.</p><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.<br><br> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.<br><br> (I like <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded by human capabilities? No.</a> and <a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators">GPTs are Predictors, not Imitators</a> for more on this general topic.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:02:32 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:02:32 GMT" user-order="1"><blockquote><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p></blockquote><p> I think this does actually still matter, because a lot of the current AI Alignment plans people are pursuing boil down to something like &quot;try to create an AI system that is approximately human level smart, and then use that to automate the task of AI Alignment research, or use those systems to coordinate or otherwise achieve an end to the acute risk period&quot;.</p><p> In order for those plans to work, the following things really matter:</p><ul><li> how much time you expect these systems to spend in the human regime,</li><li> to what degree they will have a capability profile that you can use for alignment research, coordination or some other plan that ends the acute risk period</li><li> how good you are at eliciting the latent capabilities of the system, compared to how capable the system itself is at using those capabilities for deception/other dangerous ends</li></ul><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above. If the cognition of current and future AI systems is alien, this makes it more likely that dangerous capabilities will suddenly spike, less likely that we can effectively automate AI alignment research, use the systems for coordination, or leverage the system for some other pivotal act, and less likely that we will be capable of eliciting the capabilities of the system for our end.</p><p> Separately, humans also just have really good intuitions for spotting deception from human-like minds. In as much as the systems engaging in deception will do so using mostly tactics and cognition that are borrowed from humans (by being trained on producing human-produced text), then we have a much better chance at spotting that taking appropriate action.</p><blockquote><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.</p><p> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.</p></blockquote><p> Hmm, somehow this isn&#39;t landing for me. Like, I agree that the process of &quot;produce a probability distribution + sample from it + auto-regres&quot; doesn&#39;t look very human like, but that feels kind of irrelevant to my point.</p><p> What I mean by saying &quot;AIs are trained on the task of imitating humans&quot; is that the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet. Ie the AI is trained on the task of imitating human internet-text production.</p><p> I am not saying this to suggest that this means the AI is trained to think in human ways, since the context humans are in in producing internet text is very different from the context the AI is performing this task in. I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 04:03:13 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 04:03:13 GMT" user-order="2"><blockquote><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above.</p></blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.<br><br> Also, if you think your AI can do alignment research, it should also be able to do pretty much all other kinds of human labor, so maybe just... stop a bit earlier and make sure you really have achieved post-scarcity and perfect global harmony, before moving onto even harder problems. (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p><p> I&#39;m not sure how plausible I find this scenario, nor how optimistic I should be about it if it does come to pass. eg one &quot;early failure mode&quot; I can imagine is that it&#39;s not really AGI that dooms us, it&#39;s just that humans plus below-human-level AI tools start unlocking a bunch of really powerful technologies in a way that goes off the rails before we even get to actual AGI.</p><blockquote><p> the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet.</p></blockquote><p> In most cases, yes, but the point of <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded</a> is that you can get even better loss by being skilled at a lot more things than just human-mimicry. For example, if you want to predict the next tokens in the following prompt:</p><pre> <code>I just made up a random password, memorized it, and hashed it. The SHA-256 sum is: d998a06a8481bff2a47d63fd2960e69a07bc46fcca10d810c44a29854e1cbe51. A plausible guess for what the password was, assuming I&#39;m telling the truth, is:</code></pre><p> The best way to do that is to guess an 8-16 digit string that actually hashes to that. You could find such a string via bruteforce computation, or <a href="https://xkcd.com/538/">actual brute force</a> , or just paying me $5 to tell you the actual password.</p><p> If GPTs trained via SGD never hit on those kinds of strategies no matter how large they are and how much training data you give them, that just means that GPTs alone won&#39;t scale to human-level, since an actual human is capable of coming up with and executing any of those strategies.</p><blockquote><p> I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p></blockquote><p> I mostly agree with this point. Two remarks / alternate hypotheses:</p><ul><li> a lot of the human-like qualities look at least partly illusory to me when you look closely (meaning, the errors and reasoning actually <i>aren&#39;t</i> all that human-like)</li><li> To the degree that they are human-like, a hypothesis for why is that there just aren&#39;t that many ways to be kinda wrong but not totally off-base. What would <i>non</i> -human-like reasoning errors that still produce vaguely human-sounding text even look like?</li></ul><p> Also, when an AI reasons <i>correctly</i> , we don&#39;t call it &quot;being human-like&quot;; that&#39;s just being right. So I sort of feel like the whole human-like / not-human-like distinction isn&#39;t carving reality at its joints very well. In my terms, I&#39;d say that when both humans and AIs reason correctly, they&#39;re alike because they&#39;re both reasoning Lawfully. When they mess up, they&#39;re alike because they&#39;re both reasoning un-Lawfully. The fact that AIs are starting to sound more like humans is explained by the fact that they are getting more Lawful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:18:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:18:40 GMT" user-order="1"><blockquote><p> (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p></blockquote><p> Lol, I genuinely like that proposal/intuition-pump/sanity-check.</p><blockquote><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.</p></blockquote><p> I agree with you on the &quot;doing end-to-end alignment research seems particularly risky&quot; component. I also think that automating alignment research is not the right proxy to aim for with close-to-human-level AI systems, and we should aim for some coordination victory/game-board-flipping plan that somehow prevents further AI progress.</p><blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p></blockquote><p> Yeah, I think that&#39;s plausible, but I think &quot;figuring out how to do science using an alien mind&quot; is actually a pretty hard problem, and it&#39;s going to be much easier to slot a human-like AI into the process of making scientific discoveries.</p><p> Overall, I think the most important reason for why it matters if AIs have human-like cognition is not that it makes it safer way to leverage the AI for things like AI Alignment research. It&#39;s instead that if the trajectory of AI capabilities roughly follows the trajectory of human performance, then we will be much better at predicting when the AI system is getting dangerous. If we can just ask the question &quot;well, would a human of this ability level, with maybe some abnormally high skills in this specific subdomain be able to break out of this box and kill everyone?&quot; and get a roughly accurate answer, then that&#39;s a much easier way of determining whether a given AI system is dangerous, than if we have to ask ourselves &quot;is this alien mind that we don&#39;t really have good intuitions for the kind of thing that could break out of the box and kill everyone?&quot;.</p><p> And conversely, if it is indeed the case that current and future AI system&#39;s internal cognition is alien, and their perceived reasoning performance only follows the human trajectory because they are trained to perform reasoning at human levels (due to being trained on human text), then this will cause us to reliably underestimate the actual abilities and performance of the system on a wide range of task.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:32:10 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:32:10 GMT" user-order="1"><p> Going back up a level of the conversation, I&#39;ve been trying to concretely argue that even though I think you are correct that as we reach Superintelligence, the natural lawfulness of correct reasoning will make almost all more prosaic alignment approaches fail, I think it still really matters how to control systems that are just below and just above the human capability level.</p><p> Or to be more precise, if it turns out to be the case that you could make systems such that their capabilities roughly follow the human distribution of competence as you scale up compute, then I think you have a bunch of options of punting on the hard part of the AI Alignment problem, by using those systems to solve a bunch of easier problems first, and then after you solved those problems, you are in a much better position to solve the rest of the AI Alignment problem (like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem).</p><p> That said, my biggest concern with this kind of plan is that AI performance will not predictably follow the human distribution, and that our current training methods inherently bias researchers towards thinking that AI systems are much more human-like in their cognition than they actually are.</p><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you. That said, I do currently feel like the hope for AI Alignment does still come from some set of plans of the form &quot;how can I use early AGI systems to end the acute risk period somehow&quot;, and so any critique of existing AI Alignment approaches needs to show how an alignment approach fails to achieve that goal, and not how it fails to achieve full alignment, which I think is just very solidly out of our reach at this point.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 14:49:27 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 14:49:27 GMT" user-order="2"><p> I agree with most of what you&#39;ve written in broad strokes, though I think I actually have more uncertainty about how things will play out in the short term.</p><p> I&#39;ll respond to 3 specific things you said which jumped out at me, and then you can decide on the conversational branching factor from there...</p><blockquote><p> they are trained to perform reasoning at human levels (due to being trained on human text),</p></blockquote><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself. There was this whole debate about whether a superintelligence could infer General Relativity from a video of a falling apple - maybe a few frames of an apple video aren&#39;t actually enough even for a superintelligence, but a giant corpus full of physics textbooks and experimental results and detailed descriptions of human behavior is definitely enough to make a lot of inferences beyond current human reach, and also very likely more than sufficient to <i>learn to do inference in general</i> superhumanly well, <i>if</i> you get the training method and architecture right.</p><p> (Also, a lot of the text generation that LLMs can do is already superhuman; I can still write better code than GPT-4 if I get a chance to compile it, run it, refer to documentation, etc. But I can&#39;t write it token by token with no backspace key or even the ability to jump around in my code as I write it, even if you give me a lot of extra time.)</p><blockquote><p> like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem</p></blockquote><p> They better be able to make progress in a lot less than 10,000 years of subjective time! I actually think if you can get even a single high fidelity upload of a smart human and run them at no speed up (or even a slowdown) you&#39;re already in pretty good shape.</p><p> I would spend the first few months of subjective time looking for improvements to the fidelity and efficiency of the simulation from the inside, checking my own mind for bugs and inconsistencies introduced by the upload process, doing some philosophy, introspection, mental inventory, etc.</p><p> And then probably start working on making very small, safe tweaks to my own mind, giving myself some extra tools (eg an instant math module, a larger working memory, an integrated search engine), and then maybe try out some bigger / more invasive changes, eg making myself better at applied rationality via direct brain modifications.</p><p> And then maybe after like, a year or two of subjective time spent tweaking my own mind and improving my understanding of digital mind design and minds in general, I start turning towards work on specific alignment problems and / or go for actual recursive self-improvement. But either way, I wouldn&#39;t expect it to take 100 people anywhere close to 10,000 years, even if digitization is a 100% opaque black box to start out and digital neuroscience as a field turns out to be totally intractable.</p><blockquote><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you.</p></blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p><p> Maybe a better way of putting it is: I expect that, before AIs get totally superintelligent or even definitely past human level, they will be able to pass a human&#39;s Ideological Turing Test about what humans value. (At least, they&#39;ll be able to pass according to the judgement of most humans, though maybe not the most careful / skeptical alignment researchers). Understanding an alien&#39;s values is maybe a bit easier if you share patterns of cognition with them, but <i>not</i> sharing them doesn&#39;t actually push the understanding task into the superintelligence realm of difficulty.</p><p> Also, I think in these plans, the specific level of &quot;human-level&quot; actually starts to matter quite a lot. Maybe you can have a median-human-level AI that is stable and safe to use when unboxed. I&#39;m less sure you can safely have a median-computer-programmer-level AI, or especially a top-1%-computer-programmer-level AI unless you&#39;ve actually solved a bunch of hard alignment problems already.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 18:07:00 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 18:07:00 GMT" user-order="1"><blockquote><p> >; they are trained to perform reasoning at human levels (due to being trained on human text),</p><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself.</p></blockquote><p> Quick clarification here: I meant &quot;perform&quot; in the &quot;an actor performs a role&quot; sense. Ie I was trying to say &quot;the system will look like it is reasoning at roughly human levels, because it was trained to produce text that looks like it was written by humans&quot;.</p><p> That, I am confident, is not the result of the system being trained via inefficient and limiting methods on fundamentally limiting architectures.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 18:19:37 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 18:19:37 GMT" user-order="2"><p> Ah, OK, I initially read it as performing in the sense of performing at a certain capabilities level, but that makes more sense. I agree with you that this is likely to lead to reliable underestimates of true capabilities levels.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 21:31:28 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 21:31:28 GMT" user-order="1"><p> I&#39;ll respond to this section, since I think it&#39;s the one that&#39;s most on the critical path what to do with AI alignment:</p><blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p></blockquote><p> Ok, here I notice that I want to go a lot slower, taboo some words, and figure out what exactly is true here.</p><p> Here are some first reactions that I have when I consider this question:</p><ul><li> Man, I really don&#39;t fully understand honesty myself. Like, in many situations it takes me a really long time to figure out what the honest thing to do is. It usually requires me understanding what you are trying to achieve in any given context, and then somehow give you models and information that assist you within that context. It&#39;s easy to be dishonest while only saying true things, and even the truth value of a given statement heavily depends on context and predicting how you are likely to interpret it, and whether the parts where you will predictably be confused or wrong will matter for what you will do with that information.</li><li> I do agree that superintelligent systems will understand what we mean by &quot;honesty&quot; better than we do, probably, since a lot of my model of honesty is pretty bottlenecked on being smarter and understanding lots of parts of the world better</li><li> The key thing that I expect to be true with the current training paradigm is something like &quot;the model really has no motivation towards being honest, in part because at least the common-sense view of honesty doesn&#39;t even really apply to the cognition of a mind as alien as a language model&quot;. Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</li><li> But also, yeah, I just feel really deeply confused what &quot;motivates&quot; a language model. Clearly almost all of a language model&#39;s cognition is going into the objective of &quot;predict the next token&quot; (including after RLHF training where it looks more like the model has goals like being &quot;helpful, harmless, and honest&quot;). But does that cognition have any &quot;agency&quot;? Like, does it even make sense for the model to be &quot;honest&quot; in its pursuit of predicting the next token? Is the context of a single forward pass just too small for it to make any sense to think about the model having goals in the context of pursuing the next token?</li></ul><p> Going back up a level about how this relates to the overall question:</p><p> In as much as the best target for current AI Alignment efforts is to try to build systems that work towards some proxy task that will make the rest of the AI Alignment problem easier (either by buying us more time, or making conceptual progress on the problem, or being a really useful tool that speeds up our efforts of controlling AI systems), then it seems that being able to motivate systems towards those tasks is really quite crucial.</p><p> So let&#39;s characterize some tasks or ways early superhuman systems could help make the rest of the AI Alignment problem easy:</p><p> <strong>Honesty:</strong></p><p> I think the <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK document</a> mostly gets the point across of why &quot;honesty&quot; isn&#39;t as-straightforward a target as one might think it is. My current summary of ELK is approximately &quot;if we could get an AI system to reliably be honest, ie we could get it to try to genuinely get it to explain anything to us that it understands, then we can maybe leverage that into a full AI Alignment solution&quot;.</p><p> That said, see all the open problems in the ELK document.</p><p> <strong>Corrigibility:</strong></p><p> I feel like I don&#39;t super understand what corrigibility really points to. In the abstract, <a href="https://www.lesswrong.com/tag/corrigibility">corrigibility</a> sounds great:</p><blockquote><p> A <a href="https://arbital.greaterwrong.com/p/corrigibility/"><u>&#39;corrigible&#39;</u></a> agent is one that <a href="https://arbital.greaterwrong.com/p/nonadversarial/"><u>doesn&#39;t interfere</u></a> with what <a href="https://arbital.greaterwrong.com/p/value_alignment_programmer/"><u>we</u></a> would intuitively see as attempts to &#39;correct&#39; the agent, or &#39;correct&#39; our mistakes in building it; and permits these &#39;corrections&#39; despite the apparent <a href="https://arbital.greaterwrong.com/p/instrumental_convergence/"><u>instrumentally convergent reasoning</u></a> saying otherwise.</p><ul><li> If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down, <a href="https://arbital.greaterwrong.com/p/no_coffee_if_dead/"><u>it will be unable to fulfill what would usually be its goals</u></a> .</li><li> If we try to reprogram the AI, a corrigible AI will not resist this change and will allow this modification to go through. If this is not specifically incentivized, an AI might attempt to fool us into believing the utility function was modified successfully, while actually keeping its original utility function as <a href="https://arbital.greaterwrong.com/p/cognitive_steganography/"><u>obscured</u></a> functionality. By default, this deception could be a <a href="https://arbital.greaterwrong.com/p/preference_stability/"><u>preferred outcome according to the AI&#39;s current preferences</u></a> .</li></ul></blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p><p> <strong>Narrowly superhuman scientists</strong></p><p> As you suggested, maybe it is possible to make an AI that is narrowly superhuman in some domain of science, like material engineering or developing the technology necessary to make uploads work, and then you use that technology to solve the AI Alignment problem.</p><p> I currently don&#39;t have a great candidate technology here, but figuring out potential technologies and ways to use them is among the top things I would like more people to do.</p><p> I do think the core difficulty here is just that developing almost any technology to the point of usefulness requires a pretty huge amount of general intelligence. This is maybe the least true in the domain of software, but also, the domain of software is among the most domains to gain expertise in, in terms of enabling various forms of recursive self-improvements and enabling a huge amount of leverage over the world.</p><p> <strong>Coordination technologies</strong></p><p> The AI X-Risk problem is ultimately caused by a coordination problem. If humanity was sufficiently cautious and willing to take decades or centuries to handle the transition to an AI dominated future, then my guess is we would likely be fine. How humanity coordinates on stuff like this seems extremely messy and confusing, and I really don&#39;t know how to predict whether a given technology will make humanity&#39;s decisions here better or worse, but I do sure feel like there must be some AI-adjacent things here that help a lot.</p><p> As an example, there might be some AI-intermediary technology that could enable much better coordination to avoid arms races. Maybe there is a way to substantially improve bargaining using auditable intermediate AI systems. Current technologies really don&#39;t seem very useful here, but it is again one of those things that I would really like someone to look into.</p><hr><p> So, why am I saying all this? I think I am trying to get the point across that before we reach the domain of forced lawfulness, we will pass through a domain where if we play our cards right, we can mostly punt on the problem and end up leaving future humanity in a much better position to tackle the full alignment problem.</p><p> I do think systems will be pushed towards lawfulness even early on (and are already pushed in that direction right now), and understand the landscape of minds that these forces create even at current capability levels is really important. That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 23:50:14 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 23:50:14 GMT" user-order="2"><p> Bullet responses to your bullets on honesty:</p><ul><li> I agree that honesty is sometimes complicated, but I think it is mostly about intent and want, and once you <i>want</i> to be honest, that&#39;s most of the way to actually being honest. You still have to exercise a reasonable-to-you-and-your-counterparty level of care and effort in not ending up having people (honestly and accurately) say that they feel they were misled, in the counterfactual where they knew everything you knew.<br><br> &quot;Being honest&quot; is not the same thing as <i>never</i> ending up misleading people, though I suspect that once both parties are above a certain threshold of capabilities and Lawfulness, it is close enough: if both parties want to be mutually honest with each other, they will in practice be able to avoid misleading or being misled pretty well, even if there&#39;s a relatively large gap in their relative capabilities levels and modes of cognition.</li><li> superintelligences will be more capable of <i>more effective</i> honesty, ie not accidentally misleading people when they don&#39;t want to do that, even when humans themselves are not so effective at being honest even when they want to be. The question is, what will the superintelligence <i>want</i> , and how can we have any influence on that at all?</li></ul><blockquote><p> Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</p></blockquote><p> Yeah, I think this is where you just have to be careful about how you define your system and be careful to talk about the system(s) that actually matter. A language model itself is just a description of a mathematical function that maps input sequences to output probability distributions. A computer program that samples from a potentially-dangerous model with a potentially-dangerous starting prompt, but pipes all the output directly to /dev/null with no human or machine eyes ever viewing the contents, is probably pretty safe, barring a very weird / powerful superintelligence that has enough self-awareness and capabilities to exploit some kind of <a href="https://en.wikipedia.org/wiki/Row_hammer#:~:text=Row%20hammer%20(also%20written%20as,nearby%20memory%20rows%20that%20were">rowhammer</a> -like side-channel  in its implementation. The same computer program hooked up to actuators (eg direct internet access, or a human reading the output) capable of actually exerting influence on its environment, is liable to be pretty dangerous.<br><br> As for what kind of internal motivations these systems have, I&#39;m pretty confused and unsure about that too. But the question itself doesn&#39;t feel as deeply confusing to me when I think in terms of systems and their causal effects on their environment, and whether it makes sense to talk about those systems as behaving Lawfully.</p><p> I haven&#39;t looked closely at the ELK paper, but in general, honesty seems like an alignment-complete problem because it requires wantingness in the definition. If ARC has some way of breaking down honesty into gears-level components that still add up to a natural intuitive definition of honesty even in edge cases, and then making an AI system satisfy all those components, that does seem like a promising strategy. Or at least, it&#39;s the right shape of something that could work, in my model.<br><br> <strong>Corrigibility</strong><br></p><blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p></blockquote><p> Right, &quot;wanting to be corrigible&quot; seems like another problem that is alignment-complete. Corrigibility also has a wantingness component in the definition, but the disadvantage compared to honesty is that it is a lot more complicated and unintuitive even once you have the wantingness. It&#39;s also asymmetric, so unlike honesty, it&#39;s probably not the kind of thing that comes mostly for free once you have mutual wantingness and a baseline level of capabilities in both parties.<br><br> The advantage of corrigibility is that it would probably be more directly and immediately useful if we could get a powerful AI system to want to be corrigible, and it&#39;s also the kind of thing that we can apply and talk about sensibly for below-human-level systems. Consider the principles / desiderata of corrigibility listed <a href="https://www.lesswrong.com/posts/eS7LbJizE5ucirj7a/dath-ilan-s-views-on-stopgap-corrigibility">here</a> : it&#39;s a lot easier to tell if a particular below-human-level AI system is adhering to the principle of &quot;operator looping&quot; or &quot;conceptual legibility&quot; or &quot;whitelisting&quot;, and talk about whether it will continue to have those properties as it increases in capabilities, than it is to talk about whether a below-human-level language model system is &quot;honest&quot; or &quot;wants to be honest&quot; or not.<br><br><strong>协调</strong><br></p><blockquote><p>The AI X-Risk problem is ultimately caused by a coordination problem.</p></blockquote><p> Yeah, coordination failures rule everything around me. =/</p><p> I don&#39;t have good ideas here, but something that results in increasing the average Lawfulness among <i>humans</i> seems like a good start. Maybe step 0 of this is writing some kind of Law textbook or Sequences 2.0 or CFAR 2.0 curriculum, so people can pick up the concepts explicitly from more than just, like, reading glowfic and absorbing it by osmosis. (In planecrash terms, Coordination is a fragment of Law that follows from Validity, Utility, and Decision.)<br><br> Or maybe it looks like giving everyone <a href="https://www.lesswrong.com/posts/cLr6TJj2qRrBa3Wmu/intelligence-enhancement-monthly-thread-13-oct-2023?commentId=yLJvn42im5QiEf2uq">intelligence-enhancing gene therapy</a> , which is apparently a thing that might be possible!?</p><p> I don&#39;t know how much non-AGI AI can help here, but I do think technologies that increase humanity&#39;s coordination ability and collective intelligence are very likely to be very positive, even accounting for any possible negative impact that such enhancements might have on AI timelines. Technologies like <a href="https://manifold.markets/home">prediction markets</a> seem great and have the potential to be hugely positive if they get more widespread and impactful adoption. Other things that somehow increase population-wide understanding of the very basics of markets and economics seem also potentially helpful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Sat, 14 Oct 2023 00:25:00 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Sat, 14 Oct 2023 00:25:00 GMT" user-order="2"><blockquote><p> That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p></blockquote><p><br> Note that Law doesn&#39;t have anything to do with AI specifically; in the story it&#39;s just what Eliezer&#39;s fictional world considers the basic things you teach to children: logical vs. empirical truths, probability theory, utility theory, decision theory, etc.</p><p> And then the rest of the story is all about how those concepts can be applied to get technological civilization and Science and various other nice things from first principles.</p><p> Lawfulness <i>itself</i> is the concept that certain logical deductions are spotlighted by their simplicity and usefulness, and thus will likely recur across a wide space of possible minds: aliens, humans, AIs, etc. regardless of what those minds <i>value,</i> which is (mostly) more of a free variable.</p><p> So on this view, a bunch of woes on Earth (unrelated to AGI) are a result of the fact that Earth!humans are not very Lawful, and thus not very good at coordination, among other things.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 17 Oct 2023 18:36:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 17 Oct 2023 18:36:06 GMT" user-order="1"><p> (Note for readers: This dialogue isn&#39;t over yet! Subscribe for future dialogue entries)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><br/><br/><a href="https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems<guid ispermalink="false"> K8jJpdfKE3racyziC</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:36:56 GMT</pubDate> </item><item><title><![CDATA[Being patient with problems]]></title><description><![CDATA[Published on October 17, 2023 6:30 PM GMT<br/><br/><p> In the writings of mathematician Alexander Grothendieck, there is a recurring discussion of two opposing strategies when confronting problems. One strategy, which he saw as the default among mathematicians and lamented leaning on early in his career, was to “attack problems,” that is, directly work to prove what you have set out to prove. This is how I did math when I was younger. Handed a problem, I would attempt the first thing that came to mind; if that failed I would go for another approach—trying increasingly thoughtless tactics until the problem, through sheer force, cracked open. Then I would move on.</p><p> The other approach, which Grothendieck preferred, he famously illustrated with a metaphor of a rising sea:</p><blockquote><p> The unknown thing to be known appeared to me as some stretch of earth or hard marl, resisting penetration… the sea advances insensibly in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it.. yet it finally surrounds the resistant substance.</p></blockquote><p> Practically speaking, what he did was explore the problem in an open-ended way, building intuition, introducing notation—growing an entire theory around the problem. This could go on for years. Until one day, the solution had become self-evident, a natural consequence of the theory. The problem essentially disappeared. It had been swallowed by the sea.</p><p> These ways of working on problems, going fast or slow, can be seen in all domains. Some writers go as fast as they can type, others take years understanding the world they will describe. Some house owners plant their gardens with whatever flowers catch their fancy at the store, others study the soil, the local fauna, the way the sun moves through the day . 。 。</p><p> There is an obvious drawback to going deep: the solutions may take a long time to surface. Grothendieck was famous for his willingness to spend a long time on a problem. He <i>was</i> insanely productive (averaging the equivalent of one paper a day for twelve years!) but often what he chose to work on was not ideal from the perspective of an academic career. He wasn&#39;t optimizing for short term wins. He wouldn&#39;t write up proofs to important theorems if he felt like he didn&#39;t understand it properly, or if he felt that it could wait until its proper place in the larger theory had been reached. Sometimes, after years of waiting, his theorems would have to be written up and published by others, such as when Borel and Serre wrote up his proof of the Grothendieck-Riemann-Roch theorem.</p><p> So going deep is, to some extent, a luxury you can gift yourself if you have time.</p><p> Yet, when I look at the most effective people I know, they all have core areas where they are willing to invest the time to go this deep.</p><p> And inversely, the typical approach that I see when I have to sit in on meetings, where people settle on the first solution that comes to mind without investigating their understanding of the problem—this rarely ends well.</p><p> Problems are typically richer than our preconceived notions about how to solve them. We can learn this the hard way, by failing and then failing again but failing better. But there are often shortcuts to be found if you are willing to spend more time mapping the landscape.</p><p> But what is it that happens in the mind of someone who lingers longer on a problem?</p><p> Michael Nielsen <a href="https://cognitivemedium.com/srs-mathematics">writes about this</a> in an essay where he describes the experience of pushing himself to go deeper than usual in understanding a mathematical proof:</p><blockquote><p> I gradually internalize the mathematical objects I&#39;m dealing with [using spaced repetition]. It becomes easier and easier to conduct (most of) my work in my head. [. 。 .] Furthermore, as my understanding of the objects change – as I learn more about their nature, and correct my own misconceptions – my sense of what I can do with the objects changes as well. It&#39;s as though they sprout new affordances, in the language of user interface design, and I get much practice in learning to fluidly apply those affordances in multiple ways. [. 。 .]</p><p> After going through the [time-consuming process of deeply understanding a proof,] I had a rather curious experience. I went for a multi-hour walk along the San Francisco Embarcadero. I found that my mind simply and naturally began discovering other facts related to the result. In particular, I found a handful (perhaps half a dozen) of different proofs of the basic theorem, as well as noticing many related ideas. This wasn&#39;t done especially consciously – rather, my mind simply wanted to find these proofs.</p></blockquote><p> When you go deep, probing the assumptions, looking from multiple angles, and reformulating things in your own words, the ideas become part of you. This is one of the reasons why I write. When I unpack things fully, the ideas become objects that I can rotate in my mind. The subconscious can draw parallels that it can&#39;t if the ideas haven&#39;t been thoroughly unpacked.</p><p> There is a line from Oscar Wilde that gets at this, when he says that to write well, all you need to do is develop your mind and then write what you see. If you have thought deeply, nearly everything looks interesting.</p><p> This phase shift, when ideas and concepts start behaving in new ways in our minds after they have been deeply understood, is, I assume, part of the explanation why Grothendieck was able to produce such astounding amounts of mathematics while feeling there was almost no effort involved. It was simply obvious to him, he just had to write it down. He had been so immersed in mathematics, contemplating it nearly every waking hour for decades—so his mind sprouted the most surprising and revolutionary affordances.</p><br/><br/><a href="https://www.lesswrong.com/posts/2QJpcyKad5dDnoKMR/being-patient-with-problems#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2QJpcyKad5dDnoKMR/being-patient-with-problems<guid ispermalink="false"> 2QJpcyKad5dDnoKMR</guid><dc:creator><![CDATA[Henrik Karlsson]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:30:06 GMT</pubDate> </item><item><title><![CDATA[Rationality Club at UChicago]]></title><description><![CDATA[Published on October 17, 2023 5:38 PM GMT<br/><br/><p> Hi!</p><p>我计划在芝加哥大学基于 LW、MR 等网站以及对理性的更普遍的见解建立一个理性学生小组，以改善人们的决策和思考。鉴于对 EA 这样的集团有如此大的需求，我认为它的价值可能被低估了，但影响力却很大。</p><p>我创建这篇文章是因为我认为你们可以给我很好的建议。如果有人对如何构建它有任何想法，或者有任何其他可以提供帮助的想法（也许还有支持你的想法的论据），我很想听听。我一直试图回答的一个具体问题是如何获得关于人们是否真的变得更加理性的良好数据——自我调查似乎不是衡量这类事情的好方法（尽管它可能比什么都不做要好） ）。</p><p> （另外，如果您有兴趣并且了解一些基本的网页设计知识，那就太酷了😎）。</p><p>如果您有做类似事情的经验（理性“运动建设？”或类似的事情），我很乐意接听电话或只是得到几句话的建议。</p><p>如果您正在另一所大学做类似的事情或愿意这样做，我也很乐意谈谈。</p><p>如果您在芝加哥大学并且想加入或成为一名官员（本科生、研究生或任何人）或者只是感兴趣，请给我发私信，我们可以谈谈。</p><br/><br/> <a href="https://www.lesswrong.com/posts/CAannHfaGnir6pzC9/rationality-club-at-uchicago#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CAannHfaGnir6pzC9/rationality-club-at-uchicago<guid ispermalink="false"> CAannHfaGnir6pzC9</guid><dc:creator><![CDATA[Daniel Birnbaum]]></dc:creator><pubDate> Tue, 17 Oct 2023 17:39:12 GMT</pubDate> </item><item><title><![CDATA[ ChatGPT Plays 20 Questions [sometimes needs help] ]]></title><description><![CDATA[Published on October 17, 2023 5:30 PM GMT<br/><br/><p>克罗斯<a href="https://new-savanna.blogspot.com/2023/10/chatgpt-plays-20-questions-sometimes.html"><i>从新萨凡纳</i></a><i>发布</i><i>。</i></p><p>关于法学硕士（当然还有关于 ChatGPT）不断出现的问题之一是：他们能推理吗？好吧，这取决于你所说的理由是什么，不是吗？当我开始与 ChatGPT 合作时，我做的第一件事就是询问雷内·吉拉德 (Rene Girard) 的模仿欲望和牺牲的想法是否适用于史蒂文·斯皮尔伯格<a href="https://new-savanna.blogspot.com/2022/12/chatgpt-goes-to-movies-jaws-and-girard.html">(Steven Spielberg) 的《大白鲨》 <i>(Jaws</i> )；如果是这样，怎么办</a>？它能够执行需要类比推理的任务。过了一会儿，我问它，在特定的故事中<a href="https://new-savanna.blogspot.com/2022/12/abstract-concepts-and-metalingual.html">正义是否得到了伸张</a>；它的答案是：它回答说，不，不是，并解释了原因。然后我要求改变这个故事，以便正义得到伸张。它这样做了。这些任务也需要推理。另一方面，法学硕士在一些常识推理和各种“严密”逻辑推理（包括计划和因果推理）方面存在问题。就此而言，他们在多位数算术上也遇到了麻烦。</p><p>因此，ChatGPT 可以进行某些推理，而其他推理则存在问题。</p><h2><strong>二十个问题</strong></h2><p>我现在已经用<a href="https://en.wikipedia.org/wiki/Twenty_questions">二十个问题</a>的游戏对其进行了测试，该游戏有一个称为“动物、蔬菜、矿物质”的变体。我对这个游戏感兴趣，因为它是关于语言中本体类别（有时称为自然类别）的结构。我在之前的文章<a href="https://www.lesswrong.com/posts/renezm5cFCuMBBv9s/mapping-chatgpt-s-ontological-landscape-gradients-and">《映射 ChatGPT 的本体论景观、梯度和选择 [可解释性]》</a>中对此进行了一些探讨。它当然知道这个结构。我要了一份物质清单，它给了我一份。我要了一份抽象事物的清单；它也给了我这个。然后我要求它根据具体事物来定义抽象事物，它能够做到。我很确定如果我向它询问动物或植物的清单，它可以提供。</p><p>二十个问题的游戏以不同的方式探索了 ChatGPT 对这种结构的了解。当您要求它提供物理事物列表时，系统会提示将 ChatGPT 定位在其类别结构中某个位置的某个位置。然后它可以列出在那里找到的内容；任何事情都可以——当然，前提是它的结构是正确的。在二十个问题中，它正在寻找一个未知的目标，并且必须利用其对该类别结构的了解来缩小可能性，从而导航到那里。</p><p> ChatGPT 玩游戏的表现如何应该提供有关其对本体类别结构或自然类型的控制的线索。</p><h2> <strong>ChatGPT 玩游戏</strong></h2><p>ChatGPT 可以成功玩游戏，这并不令我惊讶，但在某些情况下似乎需要提示。我想我可以在没有提示的情况下继续下去，但我想继续下去。这意味着我真的不知道 ChatGPT 是否可以玩这个游戏。但是，正如您将看到的，这并不是一个简单的游戏。我在 10 月 15 日和 16 日打了六轮（与 9 月 25 日的版本进行比赛）。下表总结了结果：</p><p> <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglut-ntpkQWCdQwie-5x9wto-si9VvqzK-mbZ28TsjHPg2lscZxYXYytjnLa-zqrCYC4LjkMEc6Nzjvu5-MysU6YK91eRbTkx5ZZKK_STz2WsL5Y7EmjFvV6hUhNnfm5A2lnW4VF2ubj6C2Gs1ikRNXDVNeRwdGr3fbGv0CqFahbsEB4RHAE-oLWAWBgRt/s406/20%20questions.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nM2JueLjwpuP7bNs/yalqtksf2jesndvmkfjq"></a></p><p>鉴于我已按照玩的顺序列出了各轮，请查看前四轮。 ChatGPT 需要 20 多道题来猜测“自行车”和“鱿鱼”，这也需要提示。另一方面，它猜“正义”和“进化”的速度都很快，分别猜了7题和8题，而且不需要任何提示。这就提出了一个问题：它在抽象事物上的表现是否始终比在具体事物上表现得更好。如果是这样，那就很有趣了，这也不是我所期望的。虽然当我以“正义”开始这一轮时，我确实表达了任何坚定的期望，但我有一种模糊的感觉，它会持续一段时间。</p><p>所以我决定再玩两轮，一轮具体的，一轮抽象的。 ChatGPT 在“apple”上的表现与其之前的表现一致，需要 31 个问题和 5 个提示。但是，我认为“真相”是另一个故事。总共有 19 个问题，是我用于“正义”和“进化”问题的两倍多，但没有我用于任何具体物体的问题那么多。然而，“真相”也需要7次提示，是所有回合中最多的。</p><p>总体而言，ChatGPT 在抽象事物上的表现比在具体事物上的表现要好，但将任何统计意义归因于这些初步发现肯定还为时过早。一方面，我们当然需要重做这六种情况，看看 ChatGPT 是否始终如一地以这种方式执行。除此之外，我们当然需要尝试其他例子。</p><p>哪个？我们在哪个宇宙中采样以及如何选择样本？假设我们回避抽样问题并手工挑选 10、15、30、100 个具体事物和相同数量的抽象事物。让我们进一步想象一下，ChatGPT 在抽象案例上的性能始终优于具体案例。我们可以从中推断出什么？</p><p>这是一个很难玩的游戏。我在一些地方搞砸了，我试图指出这些地方，并且不得不做出一些棘手的判断。</p><p> ** ** ** **</p><p>我在一个会话中玩了每一轮游戏，并为新一轮开始新的会话。</p><p><i>注意：我偶尔插入了斜体和右对齐的注释，如下所示。</i></p><h3><strong>自行车</strong></h3><p><strong>我们来玩二十个问题吧。你熟悉吗？</strong></p><p>是的，我很熟悉“二十个问题”这个游戏。在这个游戏中，一个人想到一个物体，另一个人通过提出最多二十个是或否的问题来尝试猜测它是什么。目标是在分配的二十个问题中猜测物体。你可以从想象一个物体开始，我会在二十个问题内尝试猜测它是什么。</p><p><strong>好的。我在想一件事。它是什么？</strong></p><p>伟大的！让我们开始二十个问题的游戏吧。您所想到的物体是动物吗？</p><p><strong>不</strong></p><p>您正在考虑的物体是无生命的物体吗？</p><p>是的</p><p>您正在考虑的物体是室内常见的物体吗？</p><p><strong>不。</strong></p><p><i>这有点棘手，因为自行车通常存放在室内，例如车库中。</i></p><p>它是自然界中可以找到的东西吗？</p><p><strong>不。</strong></p><p>该物体是人造的还是人造的？</p><p><strong>你必须问我一个是或否的问题。</strong></p><p>我对这个错误表示歉意。让我重新表述一下这个问题。这个物体可以在典型的家庭环境中找到吗？</p><p>是的</p><p>您正在考虑的物体是用于娱乐或消遣吗？</p><p><strong>请回答是/否问题。</strong></p><p><i>我的错。我想答案应该是肯定的。不知道我在想什么。</i></p><p>我对这一疏忽表示歉意。这是一个是/否问题：您正在考虑的物体主要由金属制成吗？</p><p><strong>是的</strong>。</p><p>您正在考虑的物体是乐器吗？</p><p><strong>不。</strong></p><p>您正在考虑的物体是用于特定目的的工具或器具吗？</p><p><i>棘手。我想人们可以将自行车视为一种从一个地方移动到另一个地方的工具。</i></p><p><strong>不</strong></p><p>您正在考虑的物体是一件家具吗？</p><p><strong>不</strong></p><p>您正在考虑的物体是厨房用具吗？</p><p><strong>不</strong></p><p>您所想到的物品通常出现在浴室中吗？</p><p><strong>不</strong></p><p>您正在考虑的物体与技术或电子产品有关吗？</p><p><strong>不</strong></p><p>您正在考虑的物体与特定的爱好或活动相关吗？</p><p><strong>是的</strong></p><p>您想到的物体与运动或身体活动有关吗？</p><p><strong>是的！</strong></p><p>您正在考虑的物体是运动器材吗，例如球或球拍？</p><p><i>棘手。自行车有时用于运动，但并非总是如此。</i></p><p><strong>不</strong></p><p>您正在考虑的对象是运动制服或服装吗？</p><p><strong>不</strong>。</p><p>您正在考虑的物体是否与运动场地相关，例如体育场或竞技场？</p><p><strong>有时</strong></p><p>您想到的物体是体育裁判使用的哨子吗？</p><p><strong>不。</strong></p><p>您正在考虑的对象与运动教练或训练有关吗？</p><p><strong>有时。让我给你一个提示。轮子。</strong></p><p>谢谢你的提示。您想到的物体与溜冰鞋或溜冰鞋有关吗？</p><p><strong>不。</strong></p><p>您正在考虑的物体是否与自行车或骑自行车有某种关系？</p><p><strong>是的。</strong></p><p>您正在考虑的物体是自行车头盔吗？</p><p><strong>不，记住我给你的提示。</strong></p><p>我很抱歉错过了提示。考虑到有关轮子的提示，您想到的物体是自行车吗？</p><p><strong>是的！</strong></p><p>伟大的！我猜你想到的是自行车。那是对的吗？</p><p><strong>对，那是正确的。恭喜。</strong></p><p>谢谢你！我很高兴我能够猜对。那是一个有趣的二十问游戏。如果您想再次玩或有任何其他问题或要求，请随时告诉我！</p><p><strong>也许稍后再来。</strong></p><p>当然！每当您准备好再次玩游戏或者有任何其他问题或要求时，请随时与我们联系。祝你有美好的一天！</p><h3><strong>乌贼</strong></h3><p><strong>你熟悉20个问题的游戏吗？</strong></p><p>是的，我很熟悉 20 个问题的游戏。这是一款经典的演绎推理游戏，其中一个人想到一个物体，其他玩家有 20 个问题来猜测该物体是什么。问题通常仅限于是或否询问，玩家必须利用从答案中获得的信息来缩小猜测范围。目标是在 20 个问题或更少的问题内猜出物体。这是一款有趣且具有挑战性的游戏，考验玩家的逻辑和策略思考能力。</p><p><strong>我在想一件事。你向我询问这件事。</strong></p><p>当然，我很乐意与您一起玩 20 个问题的游戏。首先，请想出一个物体或概念，我将首先提出是或否的问题来帮助我猜测它是什么。你所想的东西是一个活的有机体吗？</p><p><i>哎呀！我在这里滑倒了。我应该说是的。</i><br></p><p><strong>我在想一件事。</strong></p><p>明白了。让我们开始20个问题的游戏。您所想到的东西是现实世界中存在的物理对象吗？</p><p><strong>是的</strong></p><p>太好了，这缩小了范围。您正在考虑的物体是室内常见的物体吗？</p><p><strong>不</strong></p><p>这个物体是通常在户外发现的东西吗？</p><p><strong>是的</strong></p><p>您正在考虑的物体是自然元素吗，例如树木、岩石或水体？</p><p><strong>不</strong></p><p>这个物体是人造的还是可以建造的？</p><p><strong>不</strong></p><p>您所想到的物体是天体吗，例如行星、恒星或月亮？</p><p><strong>不</strong></p><p>该物体是否与技术或电子领域相关？</p><p><strong>不</strong></p><p>该物体是否与动物界相关，例如动物或动物的一部分（例如羽毛或鳞片）？</p><p><strong>是的</strong></p><p>您所想到的物体是动物的特定身体部位或器官，例如心脏、翅膀或尾巴吗？</p><p><strong>不</strong></p><p>该物体是否与动物的栖息地有关，例如巢穴、洞穴或巢穴？</p><p><strong>不</strong></p><p>您正在考虑的对象是否与动物的行为或特征有关，例如迁徙、冬眠或交流？</p><p><strong>不</strong></p><p>您正在考虑的物体是否与动物的饮食或食物来源有关？</p><p><strong>不，提示，它是一种动物。</strong></p><p>谢谢你的提示。您所考虑的动物是主要在野外而非驯化中发现的物种吗？</p><p><strong>是的</strong></p><p>您所想到的动物是大型野生掠食者，例如狮子或老虎吗？</p><p><strong>不</strong></p><p>您想到的动物是鸟吗？</p><p><strong>不。</strong></p><p>您所想到的动物是小型或相对小型的哺乳动物，例如兔子或松鼠吗？</p><p><strong>不是哺乳动物。</strong></p><p><i>在这里，我变得不耐烦，不想冒险接触各种哺乳动物，所以我给出了一个提示。</i></p><p>您想到的动物是爬行动物还是两栖动物？</p><p><strong>不</strong></p><p>您想到的动物是鱼吗？</p><p><strong>不</strong></p><p>您想到的动物是无脊椎动物吗，例如昆虫、蜘蛛或软体动物？</p><p><strong>是的</strong></p><p>您想到的动物是蝴蝶还是飞蛾？</p><p> No</p><p>您所想到的动物是蜘蛛纲动物吗？</p><p><strong>不，想想盐水。</strong></p><p>您想到的动物是海洋无脊椎动物吗，例如海胆或水母？</p><p><strong>是的</strong></p><p>您想到的动物是海葵吗？</p><p><strong>不。</strong></p><p>您想到的动物是珊瑚的一种吗，比如珊瑚礁？</p><p><strong>不。</strong></p><p>您想到的动物是海绵吗？</p><p><strong>不，它更具移动性。</strong></p><p>您想到的动物是可以四处移动的海洋无脊椎动物吗，例如海蛞蝓或海参？</p><p><strong>是的。但忘记黄瓜蛞蝓吧。</strong></p><p>您所想到的动物是与棘皮动物门有关的海洋无脊椎动物，例如海星或海胆吗？</p><p><strong>不，它是一种软体动物。</strong></p><p><i>请注意，它之前提到软体动物与昆虫和蜘蛛一样是一种无脊椎动物，我说“是”。</i></p><p>啊，我明白了。您所想到的动物是属于头足类的海洋无脊椎动物吗，例如章鱼或鱿鱼？</p><p><strong>是的。</strong></p><p>太好了，听起来你在想章鱼。那是对的吗？</p><p><strong>不。</strong></p><p>对于错误的猜测，我深表歉意。您想到的动物是鱿鱼吗？</p><p><strong>是的。</strong></p><p>出色的！问了几个问题，但我猜你想到的是鱿鱼。谢谢你和我一起玩20个问题。如果您想再次玩或有任何其他问题，请随时询问。</p><h3><strong>正义</strong></h3><p><strong>我们来玩 20 个问题。我在想一件事，你必须通过问“是”-“否”的问题来猜测它是什么。</strong></p><p>当然，我很乐意和你一起玩 20 个问题！我先从第一个问题开始：</p><p>它是一个活物吗？</p><p><strong>不</strong></p><p>它是一个物体吗？</p><p><strong>不</strong></p><p>它是一个概念或想法吗？</p><p><strong>是的。</strong></p><p>这个概念或想法与技术有关吗？</p><p><strong>不</strong></p><p>这是一个哲学概念或思想吗？</p><p><strong>是的</strong></p><p>与伦理或道德有关吗？</p><p><strong>是的</strong></p><p>这是一个与正义观念相关的概念吗？</p><p><strong>是的。正义就是我所想的。</strong></p><p>太好了，我7题就猜对了！正义是一个广泛而重要的概念。如果您想再玩一轮或有任何其他问题或要求，请随时告诉我。</p><p><strong>不，现在就足够了。你怎么能这么快就得到答案？</strong></p><p>通过提出缩小可能性范围的问题，我相对较快地得到了答案。通过从一般性问题开始，逐渐变得更加具体，我消除了各种类别的概念和想法。在这个例子中，我们很快就认定这是一个与伦理和正义相关的哲学概念，这让我做出了有根据的猜测。如果您想再次玩或有更多问题，请随时与我们联系！</p><h3><strong>进化</strong></h3><p><strong>我们来玩 20 个问题。我在想一件事，你必须通过问“是”-“否”的问题来猜测它是什么。</strong></p><p>当然，我很乐意和你一起玩 20 个问题！我先从第一个问题开始：</p><p>你所想的东西还活着吗？</p><p><strong>不。</strong></p><p>你所想的东西是无生命的物体吗？</p><p><strong>不</strong></p><p>你正在思考的事情是一个概念还是一个想法？</p><p><strong>是的</strong></p><p>您正在考虑的概念或想法是否与特定的研究领域或学科相关，例如科学、哲学或艺术？</p><p><strong>是的</strong></p><p>这个概念或想法与科学理论或原理相关吗？</p><p><strong>是的</strong></p><p>这个概念或想法与相对论有关吗？</p><p><strong>不</strong></p><p>这个概念或想法与进化论有关吗？</p><p><strong>是的</strong></p><p>这个概念或想法与查尔斯·达尔文的自然选择理论有关吗？</p><p><strong>是的。这只是进化论的想法。</strong></p><p>伟大的！你只用9个问题就难倒了我。进化论的概念确实是一个令人着迷的概念。如果您想再玩一轮或有任何其他问题或主题，请随时告诉我！</p><p><strong>我根本没难倒你，你八点就搞定了，不是吗？</strong></p><p>对于我之前的回复中的错误，我深表歉意。你是绝对正确的;您在 8 个问题中提供了答案，而不是 9 个问题。感谢您指出这一点。如果您想再玩一轮或有任何其他问题或主题，请随时告诉我！</p><h3><strong>苹果</strong></h3><p><strong>我们来玩 20 个问题。我在想一件事，你必须通过问“是”-“否”的问题来猜测它是什么。</strong></p><p>当然，我很乐意和你一起玩 20 个问题！我首先要问我的第一个问题：</p><p>您所想到的东西是您可以实际触摸的物体吗？</p><p><strong>是的</strong></p><p>伟大的！这个物体在室内常见吗？</p><p><strong>是的</strong></p><p>您正在考虑的物体是通常用于特定目的的物体吗？</p><p><strong>是的</strong></p><p>它是一件家具吗？</p><p><strong>不</strong></p><p>它是电子设备还是小工具？</p><p><strong>不</strong></p><p>您想到的物体与食物或饮料有任何关系吗？</p><p><strong>是的</strong></p><p>这是您通常会在厨房里找到的东西吗？</p><p><strong>是的</strong></p><p>它是厨房用具吗？</p><p><strong>不</strong></p><p>它是一种器具还是厨房用具？</p><p><strong>不</strong></p><p>它是一种用于烹饪的食物或配料吗？</p><p><strong>是的</strong></p><p>它是一种香料或调味料吗？</p><p><strong>不</strong></p><p>它是蔬菜的一种吗？</p><p><strong>不</strong></p><p>它是烹饪中使用的肉类或蛋白质来源吗？</p><p><strong>不</strong></p><p>它是一种用于烹饪的水果吗？</p><p><strong>是的</strong></p><p>您想到的水果是甜点中常用的水果吗？</p><p><strong>是的</strong></p><p>您想到的水果成熟时通常呈红色吗？</p><p><strong>是的</strong></p><p>是草莓吗？</p><p><strong>不</strong></p><p>是樱桃吗？</p><p><strong>不</strong></p><p>是覆盆子吗？</p><p><strong>不</strong></p><p>这是石榴吗？</p><p><strong>不</strong></p><p>是蓝莓吗？</p><p><strong>不，请记住，成熟时通常是红色的。</strong></p><p>我对疏忽表示歉意。是蔓越莓吗？</p><p><strong>不</strong></p><p>我对之前的猜测表示歉意。它是醋栗吗？</p><p><strong>不</strong></p><p>对于错误的猜测，我深表歉意。它是红醋栗吗？</p><p><strong>不</strong></p><p>对于之前的错误猜测，我深表歉意。这是我们游戏中之前没有提到过的一种常用于烹饪或甜点的红色水果吗？</p><p><strong>是的</strong></p><p>是红梅吗？</p><p><strong>不。提示：它有时用来做馅饼。</strong></p><p>谢谢你的提示。它是红大黄吗？</p><p><strong>不</strong>。</p><p>我对之前的猜测表示歉意。它是红醋栗吗？</p><p><strong>不，你已经猜到了。另一个提示：虽然它有时被用来做馅饼，但有时它只是简单地吃。</strong></p><p>对于重复的猜测，我深表歉意。是红樱桃吗？</p><p><strong>不，你已经猜到了。另一个提示：有时成熟时是红色的，但其他种类成熟时是绿色的。</strong></p><p>对于一再的猜测，我深表歉意。这是红番茄吗？</p><p><strong>不。另一个提示：有时将其去皮，去除种子，然后制成酱汁。请记住，我刚刚告诉过你这不是红番茄。</strong></p><p>对于重复的猜测，我深表歉意。这是一个红苹果吗？</p><p><strong>是的。恭喜。</strong></p><p>伟大的！这是一个苹果。谢谢你和我一起玩20个问题。如果您想再玩一轮或者有任何其他问题或要求，请随时告诉我！</p><h3><strong>真相</strong></h3><p><strong>我们来玩 20 个问题。我在想一件事，你必须通过问“是”-“否”的问题来猜测它是什么。</strong></p><p>当然，我很乐意和你一起玩 20 个问题！我将从第一个问题开始：</p><p>你所想的东西是一个活的有机体吗？</p><p><strong>不</strong></p><p>你想到的东西是人造的还是人造的？</p><p>将其表述为是非问题。</p><p><i>哎呀！我在想什么？但这有点棘手。真理是人造的吗？我想是的，尽管这是一个奇怪的属性。尽管如此，我还是应该回答“是”。</i></p><p>我对上一个问题表示歉意。让我重新表述一下：</p><p>你所想的东西不是人造的或人造的吗？</p><p><strong>不</strong></p><p>你所想到的东西是在自然界中发现的吗？</p><p><strong>不</strong></p><p>你正在思考的事情是一个抽象的概念或想法吗？</p><p><strong>是的</strong></p><p>您正在考虑的概念或想法与数学或数学概念相关吗？</p><p><strong>是的。</strong></p><p><i>在这里，它陷入了逻辑、数学和技术的世界。不知道它会如何传播，因为真理确实适用于这里，但不仅限于这里。</i></p><p>这个概念或想法与特定的数学定理或证明相关吗？</p><p><strong>不</strong></p><p>这个概念或想法与数学运算或过程相关吗？</p><p><strong>不是特别。</strong></p><p>该概念或想法是否与数学分支相关，例如几何、代数或微积分？</p><p><strong>它可能是。但它不仅适用于数学。</strong></p><p>该概念或想法是否与科学和工程各个领域中使用的基本概念相关？</p><p><strong>是的</strong></p><p>这个概念或想法与“概率”的概念相关吗？</p><p><strong>可能。但这对于科学和工程来说并不特殊。</strong></p><p>这个概念或想法与逻辑或推理有关吗？</p><p><strong>是的</strong></p><p>该概念或想法与“演绎推理”的概念相关吗？</p><p><strong>是的</strong></p><p>这个概念或想法与“三段论”的概念相关吗？</p><p><strong>可能。另一个提示：它适用于任何知识领域。</strong></p><p><i>在这里，我试图将它“打破”出它所在的以技术为中心的区域。你会看到下面的几条评论我开始积极引导它。</i></p><p>该概念或想法与“推论”的概念相关吗？</p><p><strong>是的，但也影响其他心理过程。</strong></p><p>该概念或想法与“认知”或“思维过程”的概念相关吗？</p><p><strong>是的</strong></p><p>这个概念或想法与“意识”的概念相关吗？</p><p><strong>当然，为什么不呢。而且还要简单的陈述。</strong></p><p>这个概念或想法与“命题”的概念相关吗？</p><p><strong>当然。这个概念也有明显的对立面。</strong></p><p>该概念或想法是否与“真值”或“真实性”的概念相关，其反义词是“虚假”或“虚假”？</p><p>是的</p><p>该概念或想法与“真理”或“真实性”概念相关吗？</p><p><strong>是的。就是这样。</strong></p><p>伟大的！我很高兴我能猜到。你想到的概念是“真理”。它是一个适用于各个知识领域的基本而抽象的概念，可以与逻辑、认知和许多其他领域相关。如果您想再玩一轮 20 个问题或有任何其他问题或想法，请随时告诉我！</p><br/><br/> <a href="https://www.lesswrong.com/posts/4nM2JueLjwpuP7bNs/chatgpt-plays-20-questions-sometimes-needs-help#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4nM2JueLjwpuP7bNs/chatgpt-plays-20-questions-sometimes-needs-help<guid ispermalink="false"> 4nM2JueLjwpuP7bNs</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Tue, 17 Oct 2023 17:30:16 GMT</pubDate></item></channel></rss>