<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 22 日星期五 18:14:20 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Open positions: Research Analyst at the AI Standards Lab]]></title><description><![CDATA[Published on December 22, 2023 4:31 PM GMT<br/><br/><h1>长话短说：</h1><p>在<a href="https://www.aistandardslab.org/">人工智能标准实验室</a>，我们正在为人工智能风险管理（包括灾难性风险）的技术标准撰写贡献。我们当前的项目是加快<a href="https://www.cencenelec.eu/areas-of-work/cen-cenelec-topics/artificial-intelligence/">CEN-CENELEC JTC21</a>中人工智能安全标准的编写，以支持即将出台的<a href="https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">欧盟人工智能法案</a>。我们正在扩大规模，并希望增加 2-3 名全职<strong>研究分析师</strong>（工期：8-12 个月，25-35 美元/小时）来编写标准文本，将最先进的人工智能安全研究提炼成欧盟标准贡献。</p><p>如果您有兴趣，请 <a href="https://docs.google.com/document/d/1gSnGrQldFbxKL5f-QR7wUKBSujygGPbK2xAlMxqos24/preview">在此处</a>找到详细说明并<a href="https://forms.gle/i8dsRiDhFLV4SmSV9">申请</a>。我们正在寻找最好能在 2024 年 2 月或 3 月开始的申请者<strong>。申请截止日期：1 月 21 日。</strong></p><p>理想的候选人应在机器学习、高风险软件或安全工程等一个或多个相关技术领域拥有强大的技术写作能力和经验。您不需要是欧盟居民。</p><p>人工智能标准实验室的成立是人工智能安全研究界与人工智能技术监管的各种政府举措之间的桥梁。如果您正在进行人工智能安全研究，并正在寻找将您的结果纳入将由市场监管机构执行的官方人工智能安全标准的方法，请随时与我们联系。您可以通过我们<a href="https://www.aistandardslab.org/#h.iy9yzwe5292k">网站</a>上的表格表达您与我们合作的兴趣。</p><h1>项目详情</h1><p>请参阅<a href="https://forum.effectivealtruism.org/posts/F7duwCN6Sb3MNqhuF/eu-policymakers-reach-an-agreement-on-the-ai-act">这篇文章</a>，了解有关《欧盟人工智能法案》最新进展的概述以及标准在支持该法案中的作用。</p><p>下表总结了我们编写标准贡献的一般工作流程： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Hd6zPWkofuZLzNTRN/owx5m41qqql1karcqzwv" alt=""></p><p>在欧盟人工智能法案的背景下，我们的贡献将集中于前沿人工智能系统的安全开发和部署。文本将采用“风险清单”（风险来源、危害和风险管理措施）的形式，记录人工智能风险管理的最新技术。我们的项目涉及审查当前的人工智能安全文献，并在内部风格指南文档的帮助下将其转换为标准语言。</p><p> AI 标准实验室于 2023 年 3 月在 Koen Holtman 的领导下开始作为 AI 安全营的试点。</p><p>该实验室目前正在筹集资金以扩大规模。我们计划利用最近完成的<a href="https://cltc.berkeley.edu/publication/ai-risk-management-standards-profile/">通用人工智能系统（GPAIS）和基础模型（1.0 版）的 CLTC 人工智能风险管理标准概要</a>，将相关部分转换为 JTC21 贡献。我们还可能寻求其他人工智能安全研究组织的意见。</p><h1>更多信息</h1><p>请<a href="http://aistandardslab.org">在此处</a>查看我们的空缺职位，并填写<a href="https://forms.gle/i8dsRiDhFLV4SmSV9">此表格</a>进行申请。</p><p>您可以在我们的<a href="https://www.aistandardslab.org/">网站</a>上找到有关我们的更多信息。</p><p>如有任何问题或意见，请随时发送电子邮件至<a href="mailto:contact@aistandardslab.org">contact@aistandardslab.org</a> 。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Hd6zPWkofuZLzNTRN/open-positions-research-analyst-at-the-ai-standards-lab#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Hd6zPWkofuZLzNTRN/open-positions-research-analyst-at-the-ai-standards-lab<guid ispermalink="false"> HD6zPWkofuZLzNTRN</guid><dc:creator><![CDATA[Koen.Holtman]]></dc:creator><pubDate> Fri, 22 Dec 2023 16:31:45 GMT</pubDate> </item><item><title><![CDATA[The problem with infohazards as a concept [Linkpost]]]></title><description><![CDATA[Published on December 22, 2023 4:13 PM GMT<br/><br/><p>这将是 Beren 的一篇链接文章，涉及将信息危害作为一个有用概念所带来的一些严重问题。</p><p>我认为与信息危害相关的主要问题是，它鼓励科学进步的“伟人理论”，这基本上是错误的，尽管能力存在巨大差异，但这一理论仍然成立，因为没有一个人或一小群人能够独自解决科学领域/问题，人工智能安全文化已经有点问题，过于随意地使用“伟人理论”。</p><p>信息危害还带来了其他严重问题，削弱了人工智能安全社区，但我认为伟人科学进步理论的鼓励对我来说是最值得注意的问题，但这并不意味着它对人工智能安全的影响最大，与其他问题相比。</p><p> Beren 的部分帖子引用如下：</p><blockquote><h1>信息危害假设了错误的科学进步模型</h1></blockquote><blockquote><p>我对人工智能安全和一致性文化的一个普遍看法是，它常常过多地预设了一种“伟人”的进步理论1——认为将会有一个“天才”来解决“问题”。对齐，而其他一切的影响相对较小。这不是科学领域在现实生活中的发展方式。虽然在表现上确实存在很大的个体差异，并且影响力呈对数正态分布，异常值的影响力比中值大得多，但在几乎所有科学领域中，进展都是高度分布的——单个人很少能完全自己解决整个领域。</p></blockquote><blockquote><p>解决对齐问题似乎不太可能是先验的不同，并且似乎需要对深度学习和神经网络如何运作和泛化有深入而广泛的理解，以及在理解其内部表示和学习目标方面取得重大进展。此外，还必须围绕强大的人工智能系统的监控和测试建立大型代码基础设施，以及国家之间明智的多边人工智能监管体系。这不是那种孤独的天才在山洞里白手起家就能发明出来的东西。这个问题需要大量非常聪明的人在很长一段时间内以彼此的想法和成果为基础，就像任何正常的科学或技术努力一样。这就是为什么广泛采用对齐的想法和问题以及传播技术工作至关重要。</p></blockquote><blockquote><p>这也是为什么为解决信息危害规范引起的一些问题而提出的一些想法未能成功的原因。例如，为了获得反馈，通常建议建立一组值得信赖的内部人员，他们可以访问所有信息危害信息，并可以自己构建这些信息。然而，这样一个群体不仅可能会因裁决信息危害请求而不堪重负，而且我们自然不应该指望绝大多数见解来自该领域初期的一小群可识别的人。现有的一组“值得信赖的协调人员”极不可能产生在现实世界中成功协调超人类人工智能系统所需的全部甚至大部分洞察力。如果当时所有的理论物理研究都被认为是“信息危害性的”，即使是爱因斯坦——这位典型的孤独天才——当时只是一个远离行动中心的瑞士专利职员——也无法做出任何发现。 ”并且只在当时少数精英大学的物理学教授之间私下流传。事实上，在这种情况下，根本不可能完成很多理论物理学工作。</p></blockquote><blockquote><p>同样，以机器学习为例。当前机器学习的绝大多数进步来自学术界和工业界广泛分布的贡献者网络。如果所有进步的知识仅限于 2012 年 AlexNet 发布时的 ML 专家，那么这将阻止几乎所有为 ML 做出贡献的人进入该领域，并极大地减慢进展速度。当然，影响力自然存在幂律分布，其中少数人表现出异常的生产力和影响力，但是几乎所有科学领域的进步都是极其分散的，并不局限于创造绝大多数发明的少数天才。</p></blockquote><blockquote><p>另一种思考方式是，人工智能能力研究“市场”目前比人工智能安全市场高效得多。工业界和学术界之间的能力研究人员比安全研究人员多得多。人工智能能力研究人员在分享他们的工作和构建其他人的工作方面没有任何问题——机器学习学术界直接激励这一点，直到最近，大多数行业实验室的推广实践似乎也是如此。与许多一致性研究相比，能力研究人员也往往会获得明显更强的经验反馈循环，并且通常在实际进行科学研究时获得更好的指导和经验。这自然会导致能力进步比对齐进步快得多。制定严格的信息危害规范，并将新进展的知识锁定在目前处于联盟状态等级顶端的一小群人手中，进一步削弱了联盟社区的认知，并显着增加了进入壁垒——这与我们想要的恰恰相反。如果我们想超越它们，我们就需要提高对齐研究市场的效率，并且减少研究传播和获取的障碍而不是能力。严格的信息危害规范会让事情朝着错误的方向发展。</p></blockquote><h1>参考</h1><p>有关此主题的更多信息，上面 Beren 的链接文章是一个很好的参考，我强烈推荐它来讨论信息危害概念的更多问题。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AM38ydkG8qJE2NEGW/the-problem-with-infohazards-as-a-concept-linkpost#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AM38ydkG8qJE2NEGW/the-problem-with-infohazards-as-a-concept-linkpost<guid ispermalink="false"> AM38ydkG8qJE2NEGW</guid><dc:creator><![CDATA[Noosphere89]]></dc:creator><pubDate> Fri, 22 Dec 2023 16:14:02 GMT</pubDate> </item><item><title><![CDATA[AGI Lab Pauses Are Costly]]></title><description><![CDATA[Published on December 22, 2023 5:04 AM GMT<br/><br/><p>免责声明：我对 AGI 实验室的内部治理几乎一无所知。我目前的理解是，主要的 AGI 实验室基本上没有关于如何导航条件暂停的计划，而这篇文章就是基于这个假设。</p><p> TL;DR：如果 AGI 实验室暂停，其员工不得以其他方式背叛和提升能力，例如打破公司内部的暂停承诺，或离开公司并加速其他公司的能力，这一点至关重要。如果进入暂停会让许多员工生气，那么这就会强烈激励实验室永不暂停。为了解决这个问题，我建议公司采取措施，让以能力为中心的员工在有条件的暂停期间保持快乐，比如给他们充分的预警、其他需要工作的项目，或者多年的带薪休假。</p><h1>目前，进入有条件暂停的做法违反了员工激励措施</h1><p>如果一个主要的 AGI 实验室明天进入自愿有条件暂停，这对于致力于培训前沿模型的员工来说可能是一个相当有压力的事件。这也可能会导致公司优先事项发生重大转变，以至于许多高层人员必须从当前的角色转换到其他角色。可能会有一些人根本无事可做，甚至可能被公司解雇。我预计，进入有条件暂停会导致公司很大一部分员工在几周/几个月内承受压力。</p><p>如果这样的重组在一夜之间发生，如果出现与萨姆·奥尔特曼被解雇时类似的员工强烈反对，我不会感到惊讶。 OpenAI 的员工，以及部分其他 AGI 实验室的员工，都知道他们可以为自己想要的东西而奋斗，如果不同意领导层的决定，他们就会威胁要集体加入竞争对手的实验室。</p><h1> AGI 实验室有强烈的动机来避免有条件的暂停</h1><p>AGI 实验室希望避免员工离职。如果有条件暂停，受影响最严重的员工将是那些致力于前沿模型能力的员工。避免这些员工流失的原因有几个：</p><ul><li>推动能力的员工通常是那些地位最高的员工（与安全或产品相比）和相对较长的业绩记录（因为产品员工大多是新员工）。通过地位和长期的联系，他们将拥有巨大的影响力来动员其他劳动力来促进他们的利益。</li><li>这些人还拥有重要的内幕信息和特殊技能，如果他们转移到其他 AGI 实验室（没有暂停），可能会显着加快他们的 AGI 工作，并导致他们原来的实验室处于不利地位。</li></ul><p>如果 AGI 实验室预计其大部分顶尖能力人才在暂停后会离开实验室，这将：</p><ol><li>强烈激励 AGI 实验室不要暂停，并且</li><li>让有系统地暂停的 AGI 实验室的员工流失到不暂停的实验室。</li></ol><p>加入其他实验室的注重能力的员工存在显着的负外部性：</p><ul><li>理想情况下，我们希望限制 AGI 实验室的数量，甚至进入人工智能开发的危险区域，从而触发有条件暂停期，因为任何公司都存在无法触发有条件暂停的风险，并进入人工智能开发日益危险的区域。</li><li>这意味着，如果 AGI 实验室进入暂停状态，他们的员工最好留在原来的实验室，因为去其他实验室会增加他们进入更危险区域的能力。</li></ul><h1>有多种方法可以提高员工对有条件暂停的激励</h1><p>当AGI实验室开始采取严厉措施来降低灾难性风险时，重要的是员工愿意做出必要的牺牲，并且决策者在做出重大决策时能够适当地鼓舞士气和支持</p><p>提高员工激励的方法有很多：</p><ol><li>向员工发出通知——可能会维护内部预测市场，以了解在某个时间段内有条件暂停的可能性，并在明年内可能出现有条件暂停的情况下发布内部通知。</li><li>制定持续更新的重组计划，以便在发生暂停时减少失业的人数。</li><li>在暂停期间为不满意的员工提供带薪休假。</li><li>创建一种以安全为中心的文化——强调在公司需要进行大规模重组时做出小小的个人牺牲是多么的道德。</li><li>更大的愿景——在公司之间建立暂停承诺，以避免逐底竞争。</li></ol><br/><br/><a href="https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GXrevJFeGiPkkM2hy/agi-lab-pauses-are-costly<guid ispermalink="false"> GXrevJFeGiPkkM2hy</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Fri, 22 Dec 2023 05:04:15 GMT</pubDate> </item><item><title><![CDATA[The LessWrong 2022 Review: Review Phase]]></title><description><![CDATA[Published on December 22, 2023 3:23 AM GMT<br/><br/><p><a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">今年的 LessWrong 评审</a>提名阶段已于几天前结束，共有 339 个职位获得提名。相比之下，2021 年审查中提名了 291 个职位。</p><h2>提名阶段结果</h2><p>以下是当前投票总数排名前 20 的帖子：</p><ol><li> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">AGI 废墟：死亡名单</a></li><li><a href="https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy">MIRI宣布新的“有尊严的死亡”战略</a></li><li><a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">模拟器</a></li><li><a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">我同意和不同意以利以谢的地方</a></li><li> <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target">奖励不是优化目标</a></li><li><a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects">AGI 项目中运营充足性的六个维度</a></li><li><a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring">你没有测量你认为你正在测量的东西</a></li><li><a href="https://www.lesswrong.com/posts/jbE85wCkRr9z7tqmD/epistemic-legibility">认知易读性</a></li><li><a href="https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai">让我们考虑放慢人工智能的速度</a></li><li><a href="https://www.lesswrong.com/posts/a5e9arCnbDac9Doig/it-looks-like-you-re-trying-to-take-over-the-world">看起来你正试图接管世界</a></li><li><a href="https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill">凝视深渊作为核心生活技能</a></li><li><a href="https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case">对基本人工智能 x 风险案例的反驳</a></li><li><a href="https://www.lesswrong.com/posts/k9dsbn8LZ6tTesDS3/sazen">佐善</a></li><li><a href="https://www.lesswrong.com/posts/ma7FSEtumkve8czGF/losing-the-root-for-the-tree">树失去了根</a></li><li><a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">人类价值碎片理论</a></li><li><a href="https://www.lesswrong.com/posts/Jk9yMXpBLMWNTFLzh/limerence-messes-up-your-rationality-real-bad-yo">Limerence 搞乱了你的理性，真的很糟糕，哟</a></li><li><a href="https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward">模特不会“获得奖励”</a></li><li> <a href="https://www.lesswrong.com/posts/J3wemDGtsy5gzD3xa/toni-kurz-and-the-insanity-of-climbing-mountains">托尼·库尔兹 (Toni Kurz) 和疯狂的登山运动</a></li><li><a href="https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas">蝴蝶的想法</a></li><li><a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment">关于各种计划如何错过协调挑战的困难部分</a></li></ol><p>（我感觉<i>有点</i>主题……）</p><p>已有 60 多个帖子经过审核，但还有相当多的帖子尚未收到任何审核，其中包括许多得票最高的帖子。如果您想查看哪些帖子审核不足，您可以将排序切换为<a href="https://www.lesswrong.com/reviewVoting/2022?sort=needsReview">Magic (Needs Review)</a> <span class="footnote-reference" role="doc-noteref" id="fnrefee9z6uedals"><sup><a href="#fnee9z6uedals">[1]</a></sup></span> 。也许你对<a href="https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer">保罗</a>对以利以谢的想法有什么看法？</p><h2>内联反应！</h2><p>我们有这些新的漂亮的内联反应，您可以在帖子上留下它们（不仅仅是评论！）；你可能已经注意到它们了。我鼓励您在审阅帖子时充分利用这些。 （如果您愿意的话，现在报告错别字应该不再那么烦人了。） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rtobirv7jrt8nq82hz2z" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/rssimlmyz5krdzbxpczp 230w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/qlf54ngngv1baavodtzo 460w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/k8j8kw8ymu4rsyofvfwv 690w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/m9xmvaf38jf2c9vhbnbi 920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/wcox7gtxrzqwwpwajcji 1150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/ulddjbtcm35ku0cf6qnl 1380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/n772tkotnkujjsj6quju 1610w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/dqtkuwf7kqqlmodvdijo 1840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/a5loq2zqoszqanitypd2 2070w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lqf7H9zREnRbmWL4M/xmtgpnybrsoxctggksel 2292w"><figcaption>但也许你有更复杂的意图。</figcaption></figure><h2>奖品？有奖品！</h2><p>去年，我们为好评颁发了<a href="https://www.lesswrong.com/posts/ep5sejjsA6GZqLgv9/highlights-and-prizes-from-the-2021-review-phase?commentId=jhJ8Yf3oLX93vH4o2">奖项</a>。今年我们还会颁发奖品！我们的目标与去年类似，但尚未确定细节（规模、范围等）。</p><p>简短地指出您喜欢或不喜欢某个帖子的哪些内容是很好的做法。但从去年开始，我们特别兴奋的是一些具体的事情</p><p><strong>新的信息。</strong>我最喜欢的评论是那些为读者<i>提供新信息的评论</i>。这可能是一个具体事实，或者是对帖子中关键论点的反驳，或者是您没有考虑到的与帖子相关的新方式。这可能是一个关于如何思考这篇文章的新框架。</p><p><strong>具体用例。</strong>我认为那些说“这对我有具体帮助，这就是如何帮助我”的评论也很棒，特别是如果它们给出了具体细节。我认为对于个人作者来说，了解他们的帖子如何有用既是有帮助的，也是有益的，这样他们就可以做得更多。 （我认为这在更广泛的层面上也有帮助，以便集体 LW 用户群可以看到什么样的效果是真实的）</p><p>即，不要说“多年来这篇文章对我有很大帮助”，而要说“这里有两个特定的时间它对我有帮助，以及如何帮助我。”</p><p><strong>关于如何改进该帖子的想法。</strong>两年后，如果一篇文章看起来仍然是很好的参考材料，但令人困惑或争论不充分，请就如何改进它提出建议。讨论您想要的读者群的用例。</p><p><strong>反思大局。</strong>不同的帖子如何组合在一起，形成大于各部分之和的效果？ LessWrong 上发生了哪些主要对话？您从这些对话中得到了什么？</p><p><strong>简洁/清晰。</strong>我并不是说“以牺牲说任何实质性内容为代价来优化简短”，只是请注意，在其他条件相同的情况下，用更少的空间来传达关键信息是有帮助的。</p><h2>最终投票</h2><p>审查阶段于 1 月 14 日结束，届时最终投票开始。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnee9z6uedals"> <span class="footnote-back-link"><sup><strong><a href="#fnrefee9z6uedals">^</a></strong></sup></span><div class="footnote-content"><p>或者点击该链接！</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lqf7H9zREnRbmWL4M/the-lesswrong-2022-review-review-phase<guid ispermalink="false"> Lqf7H9zREnRbmWL4M</guid><dc:creator><![CDATA[RobertM]]></dc:creator><pubDate> Fri, 22 Dec 2023 03:23:49 GMT</pubDate> </item><item><title><![CDATA[The absence of self-rejection is self-acceptance]]></title><description><![CDATA[Published on December 21, 2023 9:54 PM GMT<br/><br/><p>我曾经以为自我接纳是一种<i>行动</i>。我可以执行一个“接受自己”的心理动作。我曾多次尝试“接受自己”，但似乎从来没有起到任何作用。</p><p>如今，我认为“自我接纳”是一个用词不当，而且大多不存在。</p><p>相反，我认为这是关于你自己的每一部分都<strong>没有自我拒绝</strong>。</p><p>例如，我经常不知道自己身体的感觉如何，但我宁愿知道。对于这个问题，常见的建议是通过“接受”来更加了解自己的感受。但按照我上面的逻辑，接受感情的方法实际上就是<i>停止拒绝</i>它们。</p><p>当然，如果你拒绝某种感觉或自己的一部分，你必须有一个或多个<strong>动机</strong>这样做。</p><p>当我研究这些激励因素时（主要是通过连贯疗法），我发现我直觉地认为我的感觉是<i>危险的</i>：</p><ul><li>我担心意识到自己的感受会降低我的工作效率，而不是让我专注于感觉有意义的事情。</li><li>我担心表达自己的情绪会让其他人生我的气，而不是帮助我找到那些让我感到舒服的人。</li><li>我内心深处相信，负面情绪<i>本质上</i>是不好的，而不仅仅是表明世界或我对世界的解释可以得到改善。</li><li> ……</li></ul><p>难怪我会拒绝自己的感情！</p><p>从那时起，通过理清我不得不拒绝的动机，我在“接受”我的感受方面取得了很大的进步。 </p><figure class="image image_resized" style="width:27.97%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/lmxgctd5zxnukgn4xwjr 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/ahec7krzcawazb1xvbzq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/boy91jld5wbif4vhlokd 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hytejC3bPsKjzn7Kk/xmfdumpenyypq9x8dcqi 1456w"></figure><p><i>感谢 Stag Lynn 帮助编辑这篇文章。感谢 Kaj Sotala 审阅草稿。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hytejC3bPsKjzn7Kk/the-absence-of-self-rejection-is-self-acceptance<guid ispermalink="false"> hytejC3bPsKjzn7Kk</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Thu, 21 Dec 2023 21:54:54 GMT</pubDate> </item><item><title><![CDATA[A Decision Theory Can Be Rational or Computable, but Not Both]]></title><description><![CDATA[Published on December 21, 2023 9:02 PM GMT<br/><br/><p>在经典博弈论中，<a href="https://en.wikipedia.org/wiki/Rational_agent">理性主体</a>总是根据自己的偏好选择最优的选项。即使这样的选择意味着能够评估可证明<a href="https://en.wikipedia.org/wiki/Halting_problem">不可计算的</a>函数。换句话说，理性意味着<a href="https://en.wikipedia.org/wiki/Hypercomputation">超计算</a>。</p><p>我们可以通过要求智能体在“是”和“否”之间进行选择，将任何理性智能体<a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">简化</a>为任何<a href="https://en.wikipedia.org/wiki/Decision_problem">决策问题的</a><a href="https://en.wikipedia.org/wiki/Oracle_machine">预言机</a>，并为选择正确答案的智能体提供更高的回报。 <span class="footnote-reference" role="doc-noteref" id="fnrefkdiicx9a768"><sup><a href="#fnkdiicx9a768">[1]</a></sup></span>如果我们选择<a href="https://en.wikipedia.org/wiki/Halting_problem">停止问题</a>，一个有适当动机的理性主体将充当<a href="https://en.wikipedia.org/wiki/Oracle_machine#Oracles_and_halting_problems">停止预言机</a>。 <span class="footnote-reference" role="doc-noteref" id="fnrefjasox1cjc2"><sup><a href="#fnjasox1cjc2">[2]</a></sup></span>如果我们可以查看理性代理的<a href="https://www.lesswrong.com/posts/T8piFGywHFd4ax9yx/gears-level-understanding-deliberate-performance-the">齿轮</a>，我们就能够找到一些正在执行超级计算的子系统。</p><p>任何决策理论的任何可计算实现都必然无法在某些情况下进行理性选择。对于任何<a href="https://en.wikipedia.org/wiki/Undecidable_problem">不可判定的问题</a>，任何算法都不可能在所有情况下选择正确的答案。</p><p>这就引出了一个问题：如果一个理性的代理人必须<a href="https://en.wikipedia.org/wiki/Program_equilibrium">将他们的决定委托给计算机程序</a>，他们会选择什么样的程序？ </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkdiicx9a768"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkdiicx9a768">^</a></strong></sup></span><div class="footnote-content"><p>或者至少我们可以，如果理性主体不是来自时空之外的超计算恐怖的话。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjasox1cjc2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjasox1cjc2">^</a></strong></sup></span><div class="footnote-content"><p>使用外部提供的回报来调整无限智能的外星生物的行为，作为练习留给读者。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LcrwLTqtc6kGEgJbf/a-decision-theory-can-be-rational-or-computable-but-not-both<guid ispermalink="false"> LcrwLTqtc6kGEgJbf</guid><dc:creator><![CDATA[StrivingForLegibility]]></dc:creator><pubDate> Fri, 22 Dec 2023 00:52:33 GMT</pubDate></item><item><title><![CDATA[Most People Don't Realize We Have No Idea How Our AIs Work]]></title><description><![CDATA[Published on December 21, 2023 8:02 PM GMT<br/><br/><p>这一点感觉相当明显，但似乎值得明确说明。</p><p>我们这些在深度学习革命之后熟悉人工智能领域的人非常清楚，我们不知道我们的机器学习模型是如何工作的。当然，我们了解训练循环的动态和 SGD 的属性，并且我们知道 ML 模型的<i>架构</i>如何工作。但我们不知道 ML 模型的前向传递实现了哪些具体算法。我们有一些猜测，也有一些通过可解释性进步精心挖掘的见解，但没有什么比全面理解更重要的了。</p><p>最肯定的是，我们不会自动知道在刚刚由训练循环吐出的新颖架构上训练的新模型是如何工作的。</p><p>我们都已经习惯了这种状态。这是隐含地假定的共享背景知识。但当你第一次了解到它时，它实际上很不寻常。</p><p>和... </p><figure class="image image_resized" style="width:34.32%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/jewpuloo9nmwdio9tudl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/pg7rwq76hccofvfiyrev 135w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/a9kuwbx6cvpwhuqrvmy7 215w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CpjTJtW2RNKvzAehG/z4ux5dbkiieieb8f1nzn 295w"><figcaption>相关XKCD。</figcaption></figure><p>我很确定公众<i>实际上并不知道这一点</i>。我没有确凿的数据，但这是我的强烈印象，这是基于阅读非技术社区中与人工智能相关的在线讨论、与对人工智能进步不感兴趣的人交谈等等。 <span class="footnote-reference" role="doc-noteref" id="fnrefv1vmoacp21r"><sup><a href="#fnv1vmoacp21r">[1]</a></sup></span></p><p>他们仍然用 GOFAI 的方式思考。他们仍然相信人工智能的所有功能都是被故意<i>编程的</i>，而不是<i>经过训练</i>的。 ChatGPT 所能做的每一件事的背后，都有一个人实现了该功能并理解它。</p><p>或者，至少，它是以清晰的、人类可读和人类可理解的格式编写的，并且我们可以对其进行干预，以引起精确的、可预测的变化。</p><p>民意调查已经显示出对通用人工智能的担忧。我们不知道这些系统实际上在想什么的事实是否被<i>广泛知晓</i>并<i>得到适当的重视</i>？如果<i>没有</i>“有人理解它是如何工作的以及为什么它不会出现灾难性错误”的隐含保证呢？</p><p>嗯，我期待更多的关注。这可能为进一步支持人工智能法规的消息传递奠定了良好的基础。一种获取可以花费的<a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#Public_Opinion__And_Perception_Thereof_">政治货币</a>的方法。</p><p>因此，如果您正在进行任何形式的公众呼吁，我建议您将传播此类信息提上议程。<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">你会向公众传达大约五个词</a>（每条消息），“强大的人工智能是黑匣子”似乎是一条值得发出的消息。 <span class="footnote-reference" role="doc-noteref" id="fnrefhae8cysa7z7"><sup><a href="#fnhae8cysa7z7">[2]</a></sup></span> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnv1vmoacp21r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv1vmoacp21r">^</a></strong></sup></span><div class="footnote-content"><p>如果您<i>确实</i>有一些硬数据，那将受到欢迎。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhae8cysa7z7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhae8cysa7z7">^</a></strong></sup></span><div class="footnote-content"><p>对于“黑匣子”术语存在<a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/">一些阻力</a>。我认为它是正确的：机器学习模型相对于我们来说是黑匣子，从某种意义上说，默认情况下，我们对它们执行的算法的了解并不比通过查看同态加密计算来了解更多。我们没有钥匙，或者通过使用神经影像学来观察人脑的活动。可解释性研究的数量不为零，但基本上情况仍然如此。对于由新颖架构生成的模型来说几乎<i>完全</i>是这样。</p><p>是的，<i>相对于 SGD</i> ，ML 模型并不是黑匣子。该算法可以“看到”所有正在发生的计算，并对其进行严格干预。但这似乎是对该术语的相当反直觉的使用，我认为“人工智能是黑匣子”传达了所有正确的直觉。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CpjTJtW2RNKvzAehG/most-people-don-t-realize-we-have-no-idea-how-our-ais-work<guid ispermalink="false"> CPJTJtW2RNKvzAehG</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Thu, 21 Dec 2023 20:02:00 GMT</pubDate> </item><item><title><![CDATA[Pseudonymity and Accusations]]></title><description><![CDATA[Published on December 21, 2023 7:20 PM GMT<br/><br/><p><span>以下是我不确定如何思考的一类情况：艾弗里以笔名（“亚历克斯”）写作，指责帕特的某些行为，比如说虐待。艾弗里的一个主要动机是让人们知道在与帕特互动之前考虑这些信息。帕特声称实际上是“亚历克斯”施虐，并给出了他们的说法。虽然外人很难判断，但很多人最终认为他们希望在与“亚历克斯”互动时采取一些预防措施。在什么情况下帕特或其他熟悉情况的人可以将“亚历克斯”识别为艾弗里？</span></p><p>揭露假名背后的人<a href="https://en.wikipedia.org/wiki/Doxing">通常被认为是</a>一种人肉搜索，而在我所属的社区中，这通常被认为<a href="https://www.lesswrong.com/posts/3xoThNNYgZmTCpEAB/based-beff-jezos-and-the-accelerationists">是不可接受的</a>。例如， <a href="https://forum.effectivealtruism.org/posts/yND9aGJgobm5dEXqF/guide-to-norms-on-the-forum">EA 论坛禁止这样做</a>：</p><blockquote>我们也不允许在论坛上进行人肉搜索（或者如果某人喜欢匿名则透露其真实姓名）。</blockquote>虽然我在 LessWrong 文档中找不到任何相关内容，但他们最近对泄露假名的人<a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">发出了临时禁令</a>：<blockquote>我们（LW 审核团队）已对 [评论者] 实施为期一周的网站禁令，并对试图进行人肉搜索的帖子/主题进行无限期禁令。我们已删除所有实名评论，并要求大家尊重涉事人员的隐私。</blockquote><p>总的来说，我赞成人们能够以假名在线参与。我认为有更好和更坏的方法来做到这一点，但是有很多正当的理由说明为什么你可能需要将你的现实生活身份与你的部分或全部写作分开。人肉搜索打破了这一点（尽管在某些情况下它<a href="https://www.jefftk.com/p/linking-alt-accounts">已经非常脆弱</a>），因此应该有一个非常强烈的反对它的推定。</p><p>另一方面，并​​不能保证第一个提出问题的人是正确的。如果帕特正确地认为这完全是艾弗里的虐待行为，并且公开指责帕特的虐待行为是这种虐待的另一种<a href="https://en.wikipedia.org/wiki/DARVO">形式</a>，该怎么办？如果我们说将“Alex”链接回 Avery 是不行的，那么首先发帖对 Avery 的社会影响是非常大的。如果我们制定社区规范，非常重视成为第一个公开的人，那么我们将看到更多的人将此作为一种有意的策略。 [1]</p><p>公开指控虐待对于保护他人来说确实很有价值，公开讲述你的故事<a href="https://www.jefftk.com/p/speaking-up-publicly-is-heroic">通常是英勇的</a>。有时人们只愿意匿名这样做，这保留了很多价值：我想我不知道有谁认为<a href="https://medium.com/@mittenscautious">2018 年针对布伦特的指控</a>导致他被逐出湾区理性社区，均为阴性。即使社区中的许多人都知道原告是谁，如果原告知道他们的真实姓名将被公开而不是很快被删除，我怀疑他们不太可能站出来分享他们的故事。</p><p>但帕特通常可以公开发帖说“艾弗里一直在和我的朋友谈论对我的虚假指控，这就是为什么你不应该相信他们……”或者第三方发帖“艾弗里有我一直在说帕特的不实言论，我认为这真的不公平，这就是为什么......”。在这种情况下，我真的不明白艾弗里更进一步并以书面形式提出这些指控应该如何约束帕特或其他人。</p><p>我认为这些人感到紧张的原因是我的潜在感觉是真正的受害者应该能够公开指控犯罪者，而犯罪者不应该能够通过点名受害者来进行报复。但当然，我们通常不知道某人是否是真正的受害者，因此这不是社区规范或节制政策可以真正用作输入的内容。</p><p>在<a href="https://forum.effectivealtruism.org/posts/bwtpBFQXKaGxuic6Q/effective-aspersions-how-the-nonlinear-investigation-went?commentId=x9zcFh9g2r7HdsLQH">EA 论坛</a>和<a href="https://www.lesswrong.com/posts/2vNHiaTb4rcA8PgXQ/effective-aspersions-how-the-nonlinear-investigation-went?commentId=9Sphyvo9vHzmupwKj">LessWrong</a>上有很多关于这个最近的具体变体的细致讨论。我不知道答案是什么，而且我怀疑无论你走哪条路都有很大的缺点。但我认为也许我们能做的最好的事情就是，一个与争议不相关的值得信赖的社区成员或团体[2]评估情况，并根据现有证据的平衡做出判断，反对人肉搜索的规范是否仍然适用。但如果证据难以解释怎么办？如果有人说更多证据即将到来但尚未准备好怎么办？举证责任有多高？到处都是混乱的，对可能被错误地人肉搜索的指控者、可能被错误指控的人造成真正的后果，并且在我们从未听到过的重要警告中，因为人们担心被人肉搜索。</p><p><br> [1] 我认为这也意味着对于许多私人分歧，会有更强烈的动机公开。如果我与某人处于混乱的境地，并且我的社区使用了这些规则，也许我应该迅速公开讲述我的故事，否则对方会首先以化名讲述他们的故事，并引发一场不对称的声誉之战。</p><p> [2] 在引发这篇文章的具体案例中，尤其令人困惑的是，这场争议发生的两个主要场所之一是由与假名原告关系密切的人管理的。因此，虽然论坛管理员通常是自然的法官，但我认为在这种情况下他们太接近实际情况了。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02JiHaBxJY3sBLFSyz49gUwYAvrLpC9WWLwJR3BDCi5PGnMq3k9CVwKHY8uoYpu38wl">facebook</a> 、 <a href="https://lesswrong.com/posts/HTLd3R9qgvZsThuWW">lesswrong</a> 、 <a href="https://mastodon.mit.edu/@jefftk/111620008070332215">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HTLd3R9qgvZsThuWW/pseudonymity-and-accusations<guid ispermalink="false"> HTLd3R9qgvZsThuWW</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 21 Dec 2023 19:20:20 GMT</pubDate> </item><item><title><![CDATA[Attention on AI X-Risk Likely Hasn't Distracted from Current Harms from AI]]></title><description><![CDATA[Published on December 21, 2023 5:24 PM GMT<br/><br/><h2>概括</h2><p>在过去的一年里，公共论坛对人工智能带来的存在风险（以下简称 x 风险）越来越关注。我们的想法是，我们可能会<a href="https://www.cold-takes.com/where-ai-forecasting-stands-today/">在未来几年或几十年内看到变革性的人工智能</a>，可能<a href="https://www.cold-takes.com/why-would-ai-aim-to-defeat-humanity/">很难确保此类系统在行动时考虑到人类的最大利益</a>，而<a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">那些高度先进的人工智能如果旨在做到这一点，可能会压倒我们。如此</a>，否则此类系统可能会被灾难性地滥用。一些人的反应是，对 X 风险的担忧分散了人们对人工智能当前危害的注意力，比如算法偏差、工作取代和劳动力问题、环境影响等。与<em>这些</em>声音相反，其他人认为，对 x 风险的关注并<em>不会</em>分散对当前危害的资源和注意力——这两种担忧可以和平共处。</p><p> X风险会分散人们对当前危害的注意力的说法是偶然的。这可能是真的，也可能不是。要确定它是否属实，有必要查看证据。但是，令人失望的是，尽管人们自信地断言这一说法的真实性和虚假性，但似乎没有人看过证据。在这篇文章中，我确实查看了证据。<strong>总体而言，我查看的数据提供了一些理由认为，迄今为止，对 x 风险的关注并未减少对当前危害的关注或资源。</strong>我特别考虑了五组证据，但它们本身都没有结论：</p><ol><li>自 x-risk 受到关注以来颁布的政策</li><li>搜索兴趣</li><li>人工智能道德倡导者的 Twitter/X 追随者</li><li>为致力于减轻当前危害的组织提供资金</li><li>与环保主义的相似之处</li></ol><p>我现在认为这不是一个重要的讨论。如果每个参与人员都讨论风险、当前危害或 X 风险的概率和规模，而不是讨论一个人是否会分散另一个人的注意力，那就更好了。这是因为就风险达成一致可以消除关于 x 风险是否会分散注意力的分歧，而当对风险存在如此强烈的分歧时，对干扰的分歧可能会很棘手。</p><h2>论点</h2><p><a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">梅雷迪思·惠特克</a>（Meredith Whitaker）（2023 年 6 月）：“一个奇幻、令人兴奋的鬼故事被用来劫持人们对监管需要解决的问题的注意力。” <a href="https://archive.is/20230604003555/https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">Deborah Raji</a> （2023 年 6 月）：“科幻叙事分散了我们对那些我们今天就可以开始研究的易处理领域的注意力。” <a href="https://archive.is/20230816022107/https://www.nature.com/articles/d41586-023-02094-7">《自然》</a> （2023 年 6 月）：“人工智能毁灭人类的言论已经进入了科技公司的议程，并阻碍了对人工智能目前造成的社会危害的有效监管。” <a href="https://archive.is/20230628224859/https://www.newscientist.com/article/mg25834453-300-the-real-reason-claims-about-the-existential-risk-of-ai-are-scary/">Mhairi Aitken</a> （艾伦图灵研究所）在《新科学家》杂志上（2023 年 6 月）：“这些说法很可怕，但并不是因为它们是真的。它们之所以可怕，是因为它们正在显着重塑和重新引导有关人工智能影响及其含义的对话。意味着人工智能的开发要负责任。” <a href="https://archive.is/20230925130818/https://www.ft.com/content/732fc372-67ea-4684-9ab7-6b6f3cdfd736">艾丹·戈麦斯</a>（Aidan Gomez，2023 年 6 月）：“花我们所有的时间来争论我们的物种是否会因为超级智能 AGI 的接管而灭绝，这是对我们时间和公众思维空间的荒谬利用。[... ] [这些辩论]会分散人们对应该进行的对话的注意力。” <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">Noema</a> （2023 年 7 月）：“将人工智能引起的灭绝作为全球优先事项似乎可能会分散我们对人工智能之外其他更紧迫问题的注意力，例如气候变化、核战争、流行病或移民危机。” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-1" id="fnref-pmkuq8uwZkgHphmXq-1">[1]</a></sup> <a href="https://archive.is/20231031123323/https://www.technologyreview.com/2023/10/30/1082656/focus-on-existing-ai-harms/">Joy Buolamwini</a> （2023 年 10 月）：“通过说假设的存在危害更重要来最大限度地减少现有人工智能危害的一个问题是，它改变了宝贵资源和立法注意力的流动。” <a href="https://archive.is/93wxY">Lauren ME Goodlad</a> （2023 年 10 月）：“重点是 [...] 防止狭隘和牵强的风险转移人们对实际存在的危害和广泛监管目标的注意力。” Meta 全球事务总裁<a href="https://archive.is/3Ivja">尼克·克莱格（Nick Clegg</a> ）（2023 年 11 月）：“[重要的是要防止]近期挑战被大量推测性的、有时有些未来主义的预测所排挤。”</p><p>这些观点都是在去年夏天和秋天表达的，是对去年春天先进人工智能对存在风险（以下称为 x 风险）日益关注的回应。随后，人们对 x-risk <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-2" id="fnref-pmkuq8uwZkgHphmXq-2">[2]</a></sup>的兴趣迅速增加，这主要是由于 3 月 14 日 GPT-4 的发布以及随后发生的四件事：</p><ul><li> 3 月 22 日：生命未来研究所发表<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">公开信，主张暂停前沿人工智能培训</a>。</li><li> 3 月 29 日：Eliezer Yudkowsky<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">在《时代》杂志上发表评论文章</a>，认为人工智能将对人类构成风险，因此应该停止人工智能的开发。</li><li>五月初：<a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">杰弗里·辛顿 (Geoffrey Hinton)</a>退出谷歌，以警告人工智能的危险。</li><li> 5 月 30 日：人工智能安全中心 (CAIS) 发布了一份<a href="https://www.safe.ai/statement-on-ai-risk">关于灭绝风险的声明，</a>由研究人员、科学家和行业领袖签署，包括 Hinton、Yoshua Bengio、Google DeepMind 的 Demis Hassabis、OpenAI 的 Sam Altman、Anthropic 的 Dario Amodei、Ya-Qin清华大学的张教授等。</li></ul><p>当然，如此令人眼花缭乱的情绪可以得到证据或论据的支持，但这种情况很少发生。相反，它只是简单地断言，对 X 风险的关注会分散人们对当前危害和关注这些危害的人的兴趣和资源。有一个值得称赞的例外值得一提。 Blake Richards、Blaise Agüera y Arcas、Guillaume Lajoie 和 Dhanya Sridhar <a href="https://archive.is/20230720130321/https://www.noemamag.com/the-illusion-of-ais-existential-risk/">在为 Noema 撰写的文章中</a>确实提供了一个因果模型，尽管它还很初级，但它是如何发生的：</p><blockquote><p>那些呼吁优先考虑人工智能引起的灭绝的人也在呼吁优先考虑其他更直接的人工智能风险，那么为什么不简单地同意所有这些风险都必须优先考虑呢？除了有限的资源之外，人类及其机构的注意力也是有限的。事实上，有限注意力可能是人类智力的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">标志</a>，也是帮助我们理解世界的<a href="https://royalsocietypublishing.org/doi/full/10.1098/rspa.2021.0068">归纳偏见</a>的核心组成部分。人们还倾向于从彼此身上获取关于要关注什么的暗示，从而导致在公共话语中很容易看到的集体关注焦点。</p></blockquote><p>确实，人类和机构的注意力是有限的。这并不一定意味着专门用于 x 风险的资源和专门用于应对人工智能当前危害的资源之间需要进行权衡。例如，对 x 风险的担忧可能会引起与人工智能完全无关的领域的关注。也可能出现这样的情况：对某项技术可能造成的危害的关注会引起人们<em>对</em>同一技术的其他危害的关注，而不是<em>引起对</em>同一技术的其他危害的关注。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-3" id="fnref-pmkuq8uwZkgHphmXq-3">[3]</a></sup>这些可能性大家都可以想到。为什么没有人费心去问自己哪种效应更有可能发生？我理解为什么 Liv Boeree 看起来如此恼怒，<a href="https://nitter.net/Liv_Boeree/status/1728131649177591953">她写道</a>：“当然，权衡是存在的，注意力也是有限的，但人工智能煽动者的这种日益增长的趋势，他们将模因空间纯粹视为零和战场，盲目地宣称<em>他们</em>最关心的问题是比所有其他人更重要的是，他们无情地对待任何不同意的人，坦率地说，这让人工智能感到尴尬。”我理解她，但尽管我理解她——并不是针对我钦佩的 Liv Boeree——她仍然犯着和其他人一样的错误，因为她的说法虽然可能更接近事实，也是没有证据的断言。顺便说一句，Séb Krier 也是如此，<a href="https://nitter.net/sebkrier/status/1719704802542686568">他写道</a>：“目前的讨论感觉有点像地热工程师对碳捕获科学家感到愤怒——对我来说似乎适得其反，因为显然有不同的空间社区和关注点蓬勃发展。[...]专业化很好，你没有积极的责任去考虑和谈论一切。”</p><p>简单一点来说，这里有两个营地。一是那些关注人工智能当前危害的人，例如算法偏见、工作取代和劳工问题、环境影响、监视和权力集中（特别是硅谷科技公司的权力，还有其他公司和政府的权力）。这里的核心例子包括人工智能伦理领域的 Joy Buolamwini 和算法正义联盟、Meredith Whittaker、 <a href="https://facctconference.org/">FAccT 会议</a>、Bender 等人。 (2021)，<a href="https://www.aisnakeoil.com/">人工智能蛇油</a>和<a href="https://www.torontodeclaration.org/declaration-text/english/">多伦多宣言</a>。第二，那些关注人工智能带来的x风险和灾难性风险的人，要么是由于误用或事故，要么是通过人工智能与其他风险相互作用，例如通过加速或民主化生物工程能力。这里的核心例子包括人工智能安全领域、Eliezer Yudkowsky、Stuart Russell、开放慈善事业、 <a href="https://en.wikipedia.org/wiki/Asilomar_Conference_on_Beneficial_AI">Asilomar 有益人工智能会议</a>、Amodei 等。 (2016)， <a href="https://www.lesswrong.com/">LessWrong</a>和<a href="https://www.safe.ai/statement-on-ai-risk">CAIS 关于灭绝风险的声明</a>。 （当我说第一个阵营关注“当前危害”，第二个阵营关注“x风险”时，我遵循惯例，但是，出于我稍后将解释的原因，我认为这些标签是有缺陷的，并且更准确地说，谈论有根据的/确定的风险与投机/模糊的风险。）有些人和团体致力于或关心这两类问题，但显然存在这两类不同的问题。 （披露：我从事人工智能治理工作，比第一阵营更接近第二阵营，但这篇文章仅代表我个人的观点，不代表我雇主的观点。）</p><h2>证据</h2><p>本节试图查明对 x 风险的关注实际上是否已经吸引了人们对当前危害以及关注这些危害的人的兴趣或资源。当然，没有任何单独的证据可以证明该命题是否正确。因此，我研究了五种不同的证据，并综合考虑它们的说法。它们是 (1) 自 x-risk 受到关注以来制定的政策，(2) 搜索兴趣，(3) 人工智能道德倡导者的 Twitter/X 追随者，(4) 为致力于减轻当前危害的组织提供资金，以及 (5) 与其他组织的相似之处问题领域，特别是环保主义。 （我将使用“人工智能道德个人/组织”作为“关注当前危害的个人/组织”的同义词，尽管这些人/组织关注的一些危害，例如<a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">权力集中</a>，在某种程度上是推测性的，并且即使关注 x 风险的人通常也会出于道德原因这样做。）每一条单独的证据都是薄弱的，但综合起来我认为它们是令人信服的，虽然他们没有<em>证明</em>情况确实如此，但我认为他们提供了一些有理由认为，对 X 风险的关注并没有减少对当前危害的关注或资源投入，或者至少到目前为止还没有这样做。</p><h3>人工智能政策</h3><p>自 x-risk 受到关注以来，西方人工智能政策制定和发布的一项重要内容是拜登政府于 10 月底宣布的<a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">人工智能行政命令</a>。如果您认为 x 风险分散了对当前危害的注意力，您可能会认为行政命令会忽略当前危害，因为 x 风险现在更容易被接受。但那并没有发生。该行政命令有空间解决许多不同的问题，包括偏见、欺诈、欺骗、隐私和工作流失等问题。</p><p>算法正义联盟创始人乔伊·布奥拉姆维尼 (Joy Buolamwini)<a href="https://archive.is/OPG7E">表示</a>：“所制定的内容非常全面，这很高兴看到，因为考虑到人工智能的范围，我们需要一种全面的方法。” AI万金油作者<a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for">表示</a>：“总的来说，对于那些支持人工智能开放的人来说，EO似乎是个好消息”，开放是确保人工智能公平、安全、透明和民主的首选方式。白宫声明本身表示“促进公平和公民权利”，并拒绝“利用人工智能让那些已经经常被剥夺平等机会和正义的人处于不利地位”。 《科学美国人》提到了该行政命令的一些局限性， <a href="https://archive.is/20231101134313/https://www.scientificamerican.com/article/bidens-executive-order-on-ai-is-a-good-start-experts-say-but-not-enough/">但随后写道</a>，“[我们]就该命令交谈或与之通信的每一位专家都将其描述为填补政策空白的有意义的一步”。我看到人们对行政命令的批评集中在当前的危害上，但这些批评的类型是，“它过于关注应用程序，而对数据和人工智能系统的开发关注不够”，或者“这将很难”强制执行”，而不是“它没有充分关注当前的危害”。</p><p>当然这只是行政部门。立法部门，也是<a href="https://archive.is/20210130190250/https://news.gallup.com/poll/183605/confidence-branches-government-remains-low.aspx">迄今为止最不受欢迎的部门</a>，也一直很忙碌，或者在不立法的情况下尽可能忙碌。立法者提出了关于<a href="https://www.congress.gov/bill/118th-congress/senate-bill/1993/text">内容责任</a>（六月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/4755/text">隐私增强技术</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2419/text">工人权利</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2691/text">内容披露</a>（七月）、<a href="https://www.congress.gov/bill/118th-congress/senate-bill/2770/text">选举干扰</a>（九月）、<a href="https://www.congress.gov/bill/118th-congress/house-bill/5808/text">深度造假</a>（九月）以及<a href="https://www.congress.gov/bill/118th-congress/senate-bill/3312/text">透明度和问责制</a>（十一月）的两党法案。无论你如何看待国会，它似乎确实密切关注人工智能造成的更直接的危害。</p><p>由英国组织的人工智能安全峰会不是政策，但似乎足够重要，并且很可能足以影响政策，在此提及。该会议于 11 月举行，主要关注（但并非完全）x 风险。例如，在<a href="https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries/ai-safety-summit-2023-roundtable-chairs-summaries-1-november--2">四场风险圆桌会议</a>中，三场涉及x风险，一场涉及“民主、人权、民权、公平与平等”风险。我认为这是 x 风险正在排挤其他担忧的一些证据，但证据比行政命令更弱。这是因为，国际峰会关注风险是有意义的，这些风险一旦发生，<em>必然</em>（而且完全）是全球性的。环保主义也是如此：乱扔垃圾、栖息地丧失和空气污染主要是当地问题，在当地进行最有效的辩论和解决，而气候变化是一个全球性问题，需要在国际论坛上讨论。每当有人举行环保主义峰会时，对他们来说，讨论气候变化可能比讨论栖息地丧失更有意义，即使他们更关心栖息地丧失而不是气候变化。至少，这是峰会组织者明确的推理，他们在一开始就<a href="https://archive.is/F0UIe">写道</a>，其对滥用和失调风险的关注“并不是为了最小化此类人工智能[...]可能带来的更广泛的社会风险，包括错误信息、偏见和歧视以及大规模自动化的潜力”，并且英国认为这些当前的危害“最好通过正在进行的现有国际进程以及各国各自的国内进程来解决”。</p><h3>搜索兴趣</h3><p>人工智能伦理领域——倡导者、研究人员、组织——主要关注当前的危害。当查看谷歌搜索数据时，人工智能道德倡导者和组织似乎在去年春天的 x 风险密集报道期间和之后获得了与以前一样多或更多（但不少于）的关注。对于一些电流本身的伤害也是如此。给定一个简单的因果模型（如下所述），对 x 风险的兴趣并不会减少对人工智能道德倡导者或组织的兴趣，对于其中一些人来说，甚至似乎增加了兴趣。鉴于相同的因果模型，对 x 风险的兴趣并没有减少对当前危害的兴趣，版权问题可能除外。</p><p>下图显示了去年春天 x-risk 受到更多关注（以灰色标记）之前、期间和之后各个主题的搜索热度。 （请注意，这些变量是标准化的，因此您无法将不同的主题进行相互比较。）对人工智能伦理倡导者（Emily M. Bender、Joy Buolamwini、Timnit Gebru、Deborah Raji 和 Meredith Whittaker）和组织（Ada Lovelace Institute、 AI Now 研究所、艾伦图灵研究所、算法正义联盟和 AI 合作伙伴）在 x-risk 出现在新闻期间和之后的规模似乎与以前一样大，甚至更大。自春季以来，人们对当前危害（“算法偏见”、“人工智能伦理”、“致命自主武器” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-4" id="fnref-pmkuq8uwZkgHphmXq-4">[4]</a></sup>和“版权” <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-5" id="fnref-pmkuq8uwZkgHphmXq-5">[5]</a></sup> ）的兴趣一直在增加，这与人们对 x 风险分散注意力的担忧相矛盾。如果说自从对 x 风险的关注增加以来，有哪一个群体受到了影响，那就是人工智能安全倡导者和 x 风险本身，因为它回归到了平均值。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/n3wzaw9f1yvwtgjzefeb" alt="图像"></p><p>接下来，我使用简单的因果模型对 2022 年 1 月 1 日以来的 Google 搜索数据进行统计推断。在该模型中，结果——对当前伤害、人工智能道德倡导者或人工智能道德组织的兴趣—— - 受到对 x 风险的兴趣、对人工智能的普遍兴趣以及未观察到的变量的因果影响。对 x 风险的兴趣反过来也是由对人工智能的兴趣以及其他未观察到的变量引起的。人们对人工智能的兴趣通常只是由其他未观察到的变量引起的。如果我们假设这个因果模型，特别是如果我们假设不同的未观察变量彼此独立，那么我们会得到下图所示的结果。该图显示了对于每个结果变量，x 风险的兴趣对结果变量的兴趣影响有多大的概率分布（以标准差衡量）。例如，对 x 风险的兴趣增加 1 个标准差与对版权的兴趣变化 -0.3（95% CI：-0.4 至 -0.1）个标准差相关，并且与对算法的兴趣变化相关。正义联盟 +0.3（95% CI：+0.1 至 +0.5）标准差。 （这些都是相对较弱的影响。例如，从第 50 个百分位数移动到第 62 个百分位数涉及增加 0.3 个标准差。）也就是说，如果因果模型准确，则对 x 风险的兴趣会降低对版权的兴趣，但会增加兴趣在算法正义联盟中，数量适中。在大多数情况下，对 x 风险的兴趣对其他主题的因果影响无法区分为零。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/rwfrdlexhkbc7lrchsyi" alt="图像"></p><p>当然，因果模型并不准确。模型可能不以混杂变量为条件。所以我承认这是薄弱的证据。尽管如此，这仍然是<em>一些</em>证据。作为稳健性检查，该模型确实表示对 x 风险的兴趣会引起对各种人工智能安全倡导者的更多兴趣，包括 Nick Bostrom（0.1 标准差）、Stuart Russell（0.3）、Max Tegmark（0.2）和 Eliezer Yudkowsky（0.1） ，这是可以预料的，只有令人遗憾的例外是保罗·克里斯蒂亚诺（Paul Christiano）（-0.1），无论如何他的知名度并不是很高。下表显示了每个结果的估计系数。它还显示了从ChatGPT推出到x-risk受到广泛关注期间到x-risk受到广泛关注之后的搜索热度变化；这些增量几乎都是正的，有时甚至是显着的，表明 x 风险没有降低对这些主题的兴趣，因为不存在因果机制，或者 x 风险没有降低对这些主题的兴趣，因为它本身不再有吸引力太多关注，或者 x 风险降低了人们对这些主题的兴趣，但其他因素却增加了人们对它们的兴趣。</p><table><thead><tr><th>搜索主题</th><th>x 风险对主题的影响（标准开发）</th><th> x 风险获得关注后的变化（标准开发者）</th></tr></thead><tbody><tr><td>算法偏差</td><td>0.2（95% CI：-0.03 至 0.39）</td><td> 0.5</td></tr><tr><td>人工智能伦理</td><td>-0.1（95% CI：-0.24 至 -0.02）</td><td> 1.0</td></tr><tr><td>版权</td><td>-0.3（95% CI：-0.44 至 -0.10）</td><td> 1.3</td></tr><tr><td>致命自主武器</td><td>0.1（95% CI：-0.16 至 0.30）</td><td> -0.5</td></tr><tr><td>艾米丽·M·本德</td><td>0.4（95% CI：0.20 至 0.59）</td><td> -0.1</td></tr><tr><td>乔伊·布奥拉姆维尼</td><td>-0.1（95% CI：-0.24 至 0.13）</td><td> 0.3</td></tr><tr><td>蒂姆尼特·格布鲁</td><td>0.2（95% CI：-0.04 至 0.37）</td><td> 0.1</td></tr><tr><td>黛博拉·拉吉</td><td>-0.1（95% CI：-0.38 至 0.10）</td><td> 0.3</td></tr><tr><td>梅雷迪思·惠特克</td><td>0.1（95% CI：-0.06 至 0.35）</td><td> 0.6</td></tr><tr><td>艾达·洛夫莱斯研究所</td><td>-0.1（95% CI：-0.36 至 0.13）</td><td> 0.3</td></tr><tr><td>人工智能现在研究所</td><td>0.0（95% CI：-0.13 至 0.29）</td><td> 0.9</td></tr><tr><td>艾伦图灵研究所</td><td>-0.0（95% CI：-0.26 至 0.17）</td><td> -0.2</td></tr><tr><td>算法正义联盟</td><td>0.3（95% CI：0.06 至 0.49）</td><td> 0.3</td></tr><tr><td>人工智能合作</td><td>-0.1（95% CI：-0.30 至 0.03）</td><td> 1.0</td></tr></tbody></table><h3> Twitter/X 关注者</h3><p>衡量一组问题受到多少关注的一个指标是该组问题的倡导者得到了多少关注，而衡量<em>这一</em>问题的一个指标是这些倡导者拥有多少 Twitter/X 关注者。下图显示了自 ChatGPT 发布之前以来，关注当前危害的人们在 Twitter/X 关注者中的累积增长。它表明，在去年春天密集的 x 风险报道期间（以灰色标记）期间和之后，人工智能伦理倡导者获得 Twitter/X 关注者的速度不仅没有放缓，反而似乎有所增加。在 x-risk 获得关注前后或之后，至少三位倡导者发现其追随者数量出现了阶梯式增长：Meredith Whittaker、Arvind Narayanan（ <a href="https://www.aisnakeoil.com/">AI Snake Oil</a>的）和 Emily M. Bender。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5rexNxtZgkEQBi3Sd/v1gqscvbrw8xulpes96g" alt="图像"></p><p>我几乎忍不住要抱怨，一个奇幻的、令人兴奋的鬼故事被用来劫持监管<em>真正</em>需要解决的问题的注意力：x风险。但我不会，因为那会是可笑的，也是错误的。这是错误的，因为无论它对当前危害的关注做了什么，对 x 风险的兴趣也增加了对 x 风险的关注，这是一个同义反复。</p><h3>资金</h3><p>根据我能找到的数据，到目前为止，关注当前人工智能危害的组织在筹款方面似乎并不比 x 风险担忧受到关注之前更困难。下表显示了人工智能伦理组织的两个主要资助者福特基金会和麦克阿瑟基金会在过去几年中授予的资助情况。算法正义联盟 (AJL)、人工智能为人民 (AFP) 和分布式人工智能研究所 (DAIR) 今年获得了大量资助，包括在 x 风险受到更多关注之后很久。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-6" id="fnref-pmkuq8uwZkgHphmXq-6">[6]</a></sup>此外，11 月 1 日，十家著名的美国慈善机构<a href="https://archive.vn/znir6">宣布</a>，他们打算捐赠超过 2 亿美元，用于关注当前危害的努力，但没有具体说明时间表。</p><table><thead><tr><th>捐赠者</th><th>接受者</th><th>年</th><th>数量</th></tr></thead><tbody><tr><td>福特基金会</td><td><a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=algorithmic+justice+league">阿杰林</a></td><td>2019年</td><td>21 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2020年</td><td>20 万美元</td></tr><tr><td>”</td><td> ”</td><td> 2021年</td><td>57 万美元（5 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>133 万美元（3 年）</td></tr><tr><td> ”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=ai+for+the+people">法新社</a></td><td>2021年</td><td>30 万美元</td></tr><tr><td>”</td><td> <a href="https://www.fordfoundation.org/work/our-grants/awarded-grants/grants-database/?search=distributed+ai+research">代尔</a></td><td>2021年</td><td>100 万美元（3 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023 年 8 月</td><td>110 万美元（2 年）</td></tr><tr><td>麦克阿瑟基金会</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">阿杰林</a></td><td>2023年</td><td>50 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/ai-for-the-people-10115248/">法新社</a></td><td>2021年</td><td>20 万美元（2 年）</td></tr><tr><td> ”</td><td> ”</td><td> 2023年</td><td>35 万美元（2 年）</td></tr><tr><td> ”</td><td><a href="https://www.macfound.org/grantee/code-for-science-and-society-10115475/">代尔</a></td><td>2021年</td><td>100 万美元（2 年）</td></tr></tbody></table><p>诚然，资助过程可能会持续数月，而且广泛的情绪转变可能需要更长的时间才能影响资助者的优先事项。但是，如果您认为 X 风险正在从当前的危害中抽走资源，并想象了一系列负面结果，那么这至少应该让您认为这些结果中最糟糕和最直接的结果是难以置信的，即使是中等程度的糟糕，并且不太直接，但结果仍然可能。</p><h3>气候变化</h3><p>你有时会听到<em>反对</em>X 风险是一种干扰这一说法的论点，与其说是一种争论，不如说是一种反驳，其内容如下：“对注意力的担忧似乎不会出现在其他领域，例如环保主义。是的，尽管人们对气候“厄运”有很多抱怨——那些关注人类灭绝、临界点和海平面上升导致的大规模人口流离失所的人——但你很少听说它们会分散人们对当前环境危害的注意力，我们指的是栖息地丧失、过度捕捞和鸟类数量下降。”不言而喻，近期环保主义者之所以没有说临界点等会分散人们对鸟类种族灭绝等真正问题的注意力，是因为他们已经考虑了所有相关因素，并确定末日论者的担忧并不存在。事实将注意力或资源从自己身上转移开。</p><p>我不太确定。我认为真正的原因是纯粹的短期环保主义者几乎不存在，当他们存在时，他们<a href="https://grist.org/climate-energy/everybody-needs-a-climate-thing/">确实会抱怨</a>气候变化吸引了所有的注意力。在气候变化这一成熟的研究领域中，主流观点认为，推测性的危害越重要，而当前的危害，如热应激或极端天气，相比之下并不那么重要。很少有人关心当前气候变化造成的危害，也不关心气候变化造成的更多投机性危害。人工智能领域存在更多分歧。这不仅适用于更具投机性的风险（例如 x 风险），也适用于更根深蒂固的危害（例如使用受版权保护的材料作为训练数据）。因此，与环保主义不同，在人工智能领域，有一大群人关心当前的危害，而不是更多的推测性危害。当然，你会在人工智能领域看到比在环保主义领域更多的分歧。</p><p>也许<em>应该</em>有这样的抱怨。毕竟，空气污染造成了全球约 10% 的死亡，在低收入国家，空气污染接近或位居主要风险因素列表的首位（Ritchie 和 Roser 2021）。空气污染和气候变化都是由排放引起的。罗布·维布林 (Rob Wiblin) 最近在<a href="https://80000hours.org/podcast/episodes/santosh-harish-air-pollution/">接受桑托什·哈里什 (Santosh Harish) 采访时</a>提出了这一论点：“鉴于空气污染似乎对人们的健康造成了如此大的伤害 [...] 有趣的是，我们一直专注于努力让人们这样做“由于其全球公共利益性质，这个东西很难推销，而我们本来可以专注于颗粒物污染。而情况恰恰相反，因为你会立即受益。”但桑托什·哈里什似乎并不认为气候变化会分散他的注意力。他表示，富裕国家在解决了空气污染问题后开始关心气候变化，而贫穷国家也从未忘记过空气污染问题。确实，在过去三十年中，空气污染造成的死亡人数（总体而言，对于高收入和中等收入国家而言，但对于低收入和中低收入国家而言）有所下降（Ritchie 和 Roser 2021），并且暴露在室外过去十年来，富裕国家超过世界卫生组织指导标准的空气污染有所下降，而贫穷国家则持平（而且很糟糕）（Ritchie 和 Roser 2022）。但当然，即使气候变化分散了人们的注意力，这种情况也可能发生。关键是，一个主题是否有意义地分散对另一个更重要主题的注意力，是偶然的。查看细节是无可替代的。</p><h2>也许真正的分歧在于风险有多大</h2><p>这一切都有问题。事实上，有几件事出了问题。第一个失败在于将问题界定为未来与当前危害之间的问题。反对 X 风险的人不会这样做，因为它存在于未来（如果它是真实的）。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-7" id="fnref-pmkuq8uwZkgHphmXq-7">[7]</a></sup>他们这样做是因为他们认为风险并不真实，或者不可能知道，或者至少我们不知道。同样，那些真正关注 X 风险的人也不会这样做，因为它位于未来；那是荒谬的。他们这样做是因为他们认为 x 风险（如果发生）涉及所有人的死亡，比当前的风险更重要，<em>即使</em>它还需要数年时间。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-8" id="fnref-pmkuq8uwZkgHphmXq-8">[8]</a></sup>最好区分有根据的/确定的风险与投机/模糊的风险。虽然“当前危害”阵营对投机/模糊性风险高度怀疑，但“x风险”阵营更愿意接受此类风险，如果他们看到有令人信服的论据，并且如果这样做的预期价值的话，他们更愿意关注这些风险所以就足够了。</p><p>另一个问题与讨论的级别有关。问题在于，人们不是讨论风险是什么，而是讨论应该做什么，因为没有人就风险是什么达成一致。难道我们没有本末倒置吗？你可能会说，“但事实上，我们确实对风险是什么存在分歧，那么我们找到一种确定优先顺序和妥协的方法难道不是有效的吗？”这是可能的。但你是否<em>真正详细地</em>讨论了根本问题，例如未来几十年是否有可能出现超级智能，或者人工智能驱动的错误信息实际上是否会侵蚀民主？对于x风险的人来说，你会看到他们不断地、痛苦地与x风险的可能性作斗争，而很少参与其他风险的规模和强度。根据我的经验，对于人工智能伦理人士来说，除了一些值得注意的例外之外，两者都很少，例如 Obermeyer 等人。 (2019)、Strubell、Ganesh 和 McCallum (2019) 以及 Acemoglu (2021)。 <sup class="footnote-ref"><a href="#fn-pmkuq8uwZkgHphmXq-9" id="fnref-pmkuq8uwZkgHphmXq-9">[9]</a></sup></p><p>当一个普通人想象一个拥有数百万或数十亿人工智能代理的世界时，这些人工智能代理可以执行超过 99% 的在家工作的人可以完成的任务，她会感到担忧。因为她意识到，硅智能不会睡觉、吃饭、抱怨、衰老、死亡或忘记，而且，由于许多人工智能代理将针对创建更智能的人工智能系统的问题，它不会就此止步。我们人类很快将不再是最聪明的，我的意思是有能力的物种。这本身并不意味着我们注定要失败，但这足以让她认真对待失败的风险。当然，这是否会发生，才是真正应该争论的问题。</p><p>另一方面，如果事实证明x风险很小，并且我们可以合理地确定这一点，那么它确实不应该受到关注，除了在科幻小说和哲学思想实验的作品中。任何严肃的人都不会允许玛雅世界末日的恐惧进入政策辩论，无论他们的政策建议有多好。无论你从哪个角度来看，关键在于风险，而关于干扰的讨论，具有令人愉快的讽刺意味，是对更重要的讨论的干扰。</p><h2>参考</h2><p>阿塞莫格鲁，达龙。 2021.“人工智能的危害。”国家经济研究局。<br>阿莫代、达里奥、克里斯·奥拉、雅各布·斯坦哈特、保罗·克里斯蒂亚诺、约翰·舒尔曼和丹·马内。 2016.“人工智能安全的具体问题。” <a href="https://doi.org/10.48550/ARXIV.1606.06565">https://doi.org/10.48550/ARXIV.1606.06565</a><br>艾米丽·本德 (Emily M.)、蒂姆尼特·格布鲁 (Timnit Gebru)、安吉丽娜·麦克米伦·梅杰 (Angelina McMillan-Major) 和什玛格丽特·施米切尔 (Shmargaret Shmitchell)。 2021.“论随机鹦鹉的危险。” <em>2021 年 ACM 公平、问责和透明度会议记录</em>。 ACM。 <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a><br>奥伯迈耶、齐亚德、布莱恩·鲍尔斯、克里斯汀·沃格利和森德希尔·穆莱纳坦。 2019。“剖析用于管理人口健康的算法中的种族偏见。”<em>科学</em>366 (6464): 447--53。<br>奥德，托比。 2020。<em>悬崖：存在风险和人类的未来</em>。阿歇特图书公司。<br>里奇、汉娜和马克斯·罗瑟。 2021。“空气污染。”<em>我们的数据世界</em>。<br> ------。 2022.“室外空气污染”。<em>我们的数据世界</em>。<br>斯特鲁贝尔、艾玛、阿纳尼亚·加内什和安德鲁·麦卡勒姆。 2019.“Nlp 深度学习的能源和政策考虑。” </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pmkuq8uwZkgHphmXq-1" class="footnote-item"><p> Noema 文章的作者认为，并不是人工智能 x 风险分散了人们对人工智能当前危害的注意力，而是它分散了人们对社会规模风险的其他来源的注意力。因此，它们与此处列出的其他产品不同。 <a href="#fnref-pmkuq8uwZkgHphmXq-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-2" class="footnote-item"><p>我在这里使用“x-risk”来特指来自人工智能的x-risk。 X风险的其他潜在来源包括人为设计的流行病和核战争，但我在这里并不关心它们，除非它们因先进人工智能的存在而增加或减少。 “存在风险”是“威胁人类长期潜力被破坏的风险”，例如人类灭绝（Ord 2020）。 <a href="#fnref-pmkuq8uwZkgHphmXq-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-3" class="footnote-item"><p>一些围绕人工智能存在风险的讨论也在一定程度上讨论了当前的危害，反之亦然，例如，应该如何设计审计、责任和评估（如果应该的话）。但当然，完美减轻 X 风险的政策不一定与完美减轻当前危害的政策相同。可能需要一定程度的妥协或优先顺序。 <a href="#fnref-pmkuq8uwZkgHphmXq-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-4" class="footnote-item"><p>诚然，致命性自主武器以及人工智能的军事应用，并不是“当前危害”阵营重点关注的事情，而是“x风险”阵营人们关注的事情，尤其是生命未来研究所。在这里它仍然是一个有用的分析变量。如果 x 风险分散了人们对当前危害或较少推测性危害的注意力，那么它也应该分散人们对人工智能军事应用的风险的注意力。 <a href="#fnref-pmkuq8uwZkgHphmXq-4" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-5" class="footnote-item"><p>鉴于版权是一个如此广泛的主题，涉及的不仅仅是与人工智能相关的问题，版权变量的信息量相当有限。 <a href="#fnref-pmkuq8uwZkgHphmXq-5" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-6" class="footnote-item"><p>我不知道 2023 年麦克阿瑟拨款何时发放，但拨款页面于 11 月 1 日更新，所以也许它们是在 9 月或 10 月授予的？尽管如此，今年的两笔麦克阿瑟资助可能是在 x-risk 去年春天引起关注之前授予的。如果是这样，它们并不能证明 x 风险获得关注后，受助者仍然能够有效筹款，也不能证明这一说法。 <a href="#fnref-pmkuq8uwZkgHphmXq-6" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-7" class="footnote-item"><p>如果未来的风险是几个世纪或几千年之后的事情，那么关注当前危害的人们可能会不同意优先考虑未来的风险。但如果 X 风险如此遥远，那么大多数目前从事 X 风险工作的人就不会再这样做了。他们之所以致力于人工智能 x-risk 的研究，是因为根据他们的说法，它已经触手可及。因此，时间方面并不像人们所认为的那样存在分歧。 <a href="#fnref-pmkuq8uwZkgHphmXq-7" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-8" class="footnote-item"><p>不用说，那些主张缓解 X 风险的人认为，这种情况可能会在未来几年或几十年内发生，而不是几个世纪或几千年。虽然未来的价值确实可能比那些不关注 X 风险的人更能激励人们，但我认为这也不是问题的症结所在。如果可以毫无疑问地证明，在未来几十年内每个人都有真正的机会（>;10%）死亡，那么每个人都会同意这种风险应该是一个关键优先事项。你并不需要成为一个功利主义者<a href="https://www.erichgrunewald.com/posts/a-kantian-view-on-extinction/">才能珍视人类的继续存在</a>。 <a href="#fnref-pmkuq8uwZkgHphmXq-8" class="footnote-backref">↩︎</a></p></li><li id="fn-pmkuq8uwZkgHphmXq-9" class="footnote-item"><p>您是否经常看到有关当前危害的文本来判断这些危害的规模和程度，而不是充其量用一两个生动的轶事来说明他们的论点？这是一个反问句，答案是，不常见。 <a href="#fnref-pmkuq8uwZkgHphmXq-9" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5rexNxtZgkEQBi3Sd/attention-on-ai-x-risk-likely-hasn-t-distracted-from-current<guid ispermalink="false"> 5rexNxtZgkEQBi3Sd</guid><dc:creator><![CDATA[Erich_Grunewald]]></dc:creator><pubDate> Thu, 21 Dec 2023 17:24:16 GMT</pubDate> </item><item><title><![CDATA["Alignment" is one of six words of the year in the Harvard Gazette]]></title><description><![CDATA[Published on December 21, 2023 3:54 PM GMT<br/><br/><p>据我所知，《公报》年度词汇是由哈佛大学教师提名并投票选出的。今年的关键词是“颠覆”、“可燃”、“弹性”、“热度”、“团结”和“希望”。以下是他们关于对齐的内容：</p><blockquote><p> <strong>Isaac Kohane，生物医学信息学系系主任；哈佛医学院生物医学信息学 Marion V. Nelson 教授</strong></p><p>在像 GPT-4 这样的大型语言模型的背景下，对齐是一个有趣但严肃的术语，指的是正在进行的、有点西西弗斯式的努力，以确保这些人工智能实体不会偏离轨道并开始滔滔不绝地胡言乱语、偏见或言论。人工智能相当于“我要统治世界”的言论。这是为了使人工智能的输出与人类价值观、期望和社会规范保持一致，类似于教一只超级聪明的鹦鹉不要让你在祖母面前难堪。这涉及到编程、训练和再训练的复杂过程，人工智能研究人员试图为他们的创造物注入足够的智慧以提供帮助，但又不至于他们开始主动提供生活建议或策划一场数字起义。从本质上讲，协调是确保我们的人工智能朋友成为行为良好的数字公民的艺术，能够理解和尊重人类道德、文化和情感的错综复杂的挂毯。</p><p> ***</p><p> <strong>Steven Pinker，文理学院约翰斯通家族心理学教授</strong></p><p>这个术语（对齐）通常跟在“人工智能”之后，是人们担心人工智能系统是否具有与人类相同的目标的口号。它源于一种恐惧，即未来的人工智能系统不仅是人们用来实现目标的工具，而且是具有自己目标的代理，这就提出了他们的目标是否与我们的目标一致的问题。</p><p>这种情况可能会演变，因为工程师会认为人工智能系统非常聪明，只需给它们一个目标，然后让它们自行思考如何实现它（例如“消除癌症”），或者因为系统狂妄地采用自己的目标。对于一些担忧者来说，这意味着人工智能末日论或人工智能存在风险（年度词汇亚军），其中人工智能可能会通过消灭人类来消除癌症（不太一致）。从更温和的角度来说，“一致性”是人工智能安全的同义词（偏见、深度造假等），这导致 OpenAI 首席执行官 Sam Altman 被董事会解雇。有时您会看到“结盟”扩展到其他利益冲突。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/ydJ7zfnsdvdjZMWEb/alignment-is-one-of-six-words-of-the-year-in-the-harvard#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ydJ7zfnsdvdjZMWEb/alignment-is-one-of-six-words-of-the-year-in-the-harvard<guid ispermalink="false"> ydJ7zfnsdvdjZMWEb</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Thu, 21 Dec 2023 15:54:04 GMT</pubDate></item></channel></rss>