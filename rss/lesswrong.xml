<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 25 日星期三 20:12:47 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p><i>为了使其有意义，需要一些先决条件：</i></p><ol><li><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>两个子系统：学习和指导</i></a></li><li><a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>分片理论：概述</i></a></li><li><a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">提炼奇异学习理论</a><span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li><i>也许也至少理解了一点</i><a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>内特的照片</i></a><i>，尽管我并不声称完全理解它。</i></li><li><i>当然，</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>《AGI 毁灭：致命列表》</i></a> <i>，尽管希望这是暗示的</i></li><li><i>也许是下贝叶斯主义的基础知识</i>（我喜欢<a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">她的 AXRP 播客</a>（ <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">有两个</a>）） <i>、Vanessa 最初的</i><a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>学习理论议程</i></a><i>哲学，以及她当前的</i><a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>DCA 前</i></a><i>对齐方案。</i></li></ol><h1>抽象的</h1><p>下贝叶斯主义背后的哲学和瓦妮莎的学习理论调整议程对我来说似乎非常有洞察力。然而，使该方法发挥作用所需的大量假设，以及该计划的本体论强制性质让我感到不安。奇异学习理论的道路最近看到了足够的实证依据，令我兴奋不已。我对它可以描述大脑和机器学习系统持乐观态度，我希望这可以用于保证两者之间的一致性，随着全脑模拟的发展，这将成为一项更容易的任务。</p><h1>介绍</h1><p>想象一个世界，在这个世界中，人类不再盲目地沿着从机器学习文献中学到的奇怪技巧拼凑而成的道路一起徘徊，每个人都试图为对他们来说最具威胁的定制故障模式做好准备（大多数人都错了） 。</p><p>相反，我们生活在这样一个世界：在下山之前，我们会得到一张地图和一双眼睛，让我们亲眼看到最安全的路径、最大、最危险的悬崖、掠食者和需要注意的危险。</p><p>在我看来，一旦我们能够使用数学进行协调，我们就进入了第二世界。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="左边的 dalle 图像是由以下内容生成的：照片描绘了一个代表经验人工智能对齐方法的世界：一群不同的人被显示为盲人，在一条崎岖且不明确的道路上航行。他们的眼睛被眼罩遮住，地形凹凸不平，阴影遮挡了部分道路。每个人都有自己的手杖和装备，显得犹豫不决、小心翼翼。有些人手里拿着旧书或卷轴，象征着对过时的机器学习文献的依赖。其他人则持有各种工具和设备，为看不见的威胁做好准备，并且不确定未来的具体危险。右侧的 dalle 图像生成方式为：文艺复兴时期风格的插图：一群不同的男人和女人聚集在他们冒险的起点。每张地图上都有一张充满活力的全息地图，其中一些显示地形线，另一些则显示动画天气模式。当一些人讨论并指出地图上的危险时，其他人则抬头将全息图与真实地形进行比较。前方崎岖的旅程展示了险峻的悬崖、茂密的森林和遥远的山峰。夕阳将戏剧性的光线投射到这群人身上，营造出明暗对比的效果。他们眼睛里柔和的光芒凸显了他们增强的视力。" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption>左：我们的情况。右边：如果我们有数学来对齐的话我们的情况。<br>使用 DALL·E 3 生成。</figcaption></figure><p>很长一段时间，我认为使用数学进行对齐基本上是不可能的。深度学习对成功理论的抵制是出了名的，我对机器智能研究所的方法会花费太多时间感到悲观，而最成功的智能和对齐数学理论——下贝叶斯主义——依赖于一堆假设和数学论证太高，太投机，太规范，让我无法乐观。所以我承认缺乏数学来进行对齐。</p><p>也就是说，直到 Nina Rimsky 和 ​​Dmitry Vaintrob 证明新奇异学习理论的一些预测在<a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking 模加法网络</a>中具有<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">令人毛骨悚然的准确性</a><span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> 。</p><p>我之前就知道奇异学习理论，并且参加过伯克利的奇异学习理论会议。但在思考了这些想法，并尝试实施他们自己想出的一些过程，并得到了不太好的结果<span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span>后，我认为它也陷入了与下贝叶斯主义相同的陷阱，躺在一个巨人上一堆数学论证，仅与现实世界系统的实证测试进行了最初步的接触。所以我几个月后就不再关注它了。</p><p>不过，看看这些新结果，似乎很有希望。</p><p><i>但它本身并不是一种对齐理论。它是一种升级的学习理论。那如何解决对齐问题呢？</i></p><p>嗯，人脑和机器学习系统都是学习机器，使用强化学习和监督学习相结合的方式进行训练。如果这一理论以正确的方式发展（这就是<i>希望</i>所在），我们可以想象将一个学习过程的结果目标与另一个学习过程的目标联系起来，并证明两者之间的最大偏差两者用于特定设置。如果这些学习系统之一是人脑，那么我们就得到了对齐保证。</p><p>因此，我对对齐的两个主要希望<span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> ：全脑模拟，以及单一学习理论的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论议程式</a>发展轨道。 <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p>可解释性和其他类型的深度学习科学似乎很好，因为它有助于使奇异学习理论（或其更好的版本）达到能够证明此类对齐相关定理的状态。</p><p>如果我们有更好的全脑模拟，这项任务就会变得更容易。如果我们在全脑模拟方面取得了成功，那么这一切就会变得如此简单，以至于我们甚至不需要单一的学习理论成分。 <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p>对于读者来说，为什么全脑模拟给了我希望，这似乎是显而易见的<span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> 。但不太明显的是为什么奇异学习理论给了我希望，所以我将在这篇文章的其余部分解释为什么后者是正确的。</p><h1>单一学习理论</h1><p><i>请随意跳过接下来的三段，或者甚至不阅读我的描述，而阅读</i><a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse 的</i></a><i>或阅读先决条件 3。我的目标不是解释什么是单一学习理论，而是给出为什么我感到兴奋的想法关于它。</i></p><p>奇异学习理论填补了常规学习理论的空白，特别是常规学习理论假设统计模型的参数函数图是一对一的，并且直观上您的损失景观没有平坦区域。 <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p>奇异学习理论处理的是不正确的情况，这种情况经常发生在分层模型和（作为子集）深度神经网络中。大体上，通过引入代数几何的概念，将真实模型和参数之间的 KL 散度重写为更易于分析的形式。</p><p>特别是，它预计模型开发过程中会出现两类相变，其中一类是每当损失突然下降时，实对数规范阈值（RLCT）（一种代数几何导出的复杂度度量）就会上升。 <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p>它最终能够回顾有关深度学习的各种事实，包括数据缩放、参数缩放和双下降的成功，并且最近在使其能够对有限领域中的现象进行预测方面取得了成功。最近，Nina Rimsky 和 ​​Dmitry Vaintrob 的<a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">调查模加法的学习系数：黑客马拉松项目</a>，其中两人能够验证有关 RLCT/学习系数/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的各种断言<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei 估计。得到我一生中最美丽的理论预测验证</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。 [原帖标题文字]</figcaption></figure><p>正如我所说，奇异学习理论预测，在相变期间，模型的损失将减少，而 RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) 将增加。直观上，这意味着，当您切换模型类时，您会切换到更适合数据且更复杂的模型类。在上面，我们看到了这一点。</p><p>以及Zhongtian Chen、Edmund Lau、Jake Mendel、Susan Wei 和 Daniel Murfet 的<a href="https://arxiv.org/abs/2310.06301">抽象叠加玩具模型中的动态相变与贝叶斯相变</a></p><blockquote><p>我们使用奇异学习理论 (SLT) 研究叠加玩具模型 (TMS) 中的相变。我们推导出理论损失的闭合公式，并且在两个隐藏维度的情况下，发现正则<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形是临界点。我们提出的支持理论表明，这些<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形的局部学习系数（几何不变量）将贝叶斯后验中的相变确定为训练样本大小的函数。然后，我们凭经验证明，相同的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span>边形临界点也决定了 SGD 训练的行为。出现的图片为 SGD 学习轨迹受顺序学习机制影响的猜想提供了证据。具体来说，我们发现 TMS 中的学习过程，无论是通过 SGD 还是贝叶斯学习，都可以通过参数空间从高损失和低复杂性区域到低损失和高复杂性区域的旅程来表征。</p></blockquote><p>您可以在<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">这个 LessWrong 序列</a>中阅读更多内容，观看<a href="https://www.youtube.com/@SLTSummit/videos">入门书</a>、 <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox 讲座</a>，当然还可以阅读《 <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">代数几何和统计学习理论</a>》以及<a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">《贝叶斯统计数学理论》</a>等书。</p><h1>那么为什么还有希望呢？</h1><p>引用 Vanessa Kosoy 的<a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">学习理论</a>议程：</p><blockquote><p>在在线学习和强化学习中，该理论通常旨在推导“遗憾”的上限和下限：算法收到的预期效用与先验已知环境时<i>它将</i>收到的预期效用之间的差异。这样的上限实际上是给定算法的<i>性能保证</i>。特别是，如果假设奖励函数是“对齐的”，那么这种性能保证在某种程度上就是对齐保证。这一观察并非空穴来风，因为学习协议可能无法直接提供给算法真正的奖励函数，如<a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a>和<a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a>所示。因此，正式证明一致性保证采取证明适当遗憾界限的形式。</p></blockquote><p>如果奇异学习理论的原理可以扩展到强化学习，并且我们可以得到模型泛化行为的合理界限，甚至可以准确地声明模型中的值等价物在训练过程中将采取的不同形式，我们可以希望解决一种大致称为内部对齐的形式——让我们的模型在遇到部署环境时以某种方式一致地思考和行动。</p><p>在我看来，我们实际上可以将奇异学习理论扩展到强化学习，这似乎是合理的。相同类型的深度学习算法在监督学习和强化学习上都表现良好，因此我们应该期望相同的算法在这两者上都有相似的原因，并且奇异学习理论描述了为什么这些深度学习算法在监督学习中表现良好学习案例。因此，我们应该怀疑强化学习的故事遵循相同的一般流程<span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> 。</p><h1>如果您非常喜欢性能保证，为什么不直接研究下贝叶斯主义呢？</h1><p>瓦妮莎的技巧是发展一种理论，解释在一个比你自己更大的世界中进行良好推理意味着什么，并且还要求你进行自我建模。然后（据我所知）证明一些后悔界限，为成为代理人意味着什么制定一个标准，并构建一个对代理人效用函数的满意度具有较低界限的系统，该代理人的效用函数是最直接因果上游的已部署代理。</p><p>对我来说，这似乎是一个非常不稳定的结构，主要是因为我们实际上没有她正在考虑的代理的计算机示例。我会更高兴采用这种方法，并用它来证明现实生活中深度学习系统在不同训练动态下的遗憾（或类似品质）的界限。</p><p>我还对它如何从关于价值如何运作的理论（最大化效用函数）到将其定义为成功标准感到不安<span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> 。我更愿意接受可以应用于人脑的理论，并自然地导出效用函数（或其他值格式）。只需看看大脑是如何构建和发育的。如果我们的价值观是错误的，那么这似乎更可靠。在某种程度上，多重效用函数可以适应我们的行为，考虑到偏见和缺乏广泛的反思，优先考虑领域所告知的价值观（即格式值在大脑中）似乎比应用奥卡姆剃刀的生硬工具要好直接到从我们的实际和反事实行为到效用函数的映射。</p><p>在我所知道的所有理论中，单一学习理论似乎最适合这项任务。它既基于经过充分验证的数学，它已经并将继续与实际系统有很好的联系，它涵盖了非常广泛的学习机器，其中包括人类大脑（暂时忽略更多的强化学习，例如人类学习的方面） ）和可能的未来机器（再次忽略强化学习），并且它做出的哲学假设比内贝叶斯主义的哲学假设要简单得多。</p><p>这种方法的缺点是单一学习理论目前对强化学习知之甚少。然而，正如我上面所说，我们在强化学习中看到了与监督学习中相同的扩展动态，并且相同类型的模型在这两种情况下都有效，所以如果它们有非常不同的原因，那就很奇怪了正在成功。奇异学习理论试图解释监督学习案例，因此我们应该期望它或类似的方法也能够解释强化学习案例。</p><p>另一个缺点是它不能很好地应对无法实现的情况。然而，我被告知这里还没有零进展，这是该领域的一个开放问题，而且，神经网络经常在无法实现的环境中学习，据我所知，我们看到足够相似的动态，我敢打赌单一学习理论已经可以胜任这项任务了。</p><h1>全脑模拟</h1><p>人脑几乎肯定是奇异的<span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> ，具有从头开始学习的重要成分，并且奇异学习理论对于它可以处理的模型类型非常不可知<span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> ，所以我断言奇异学习可以处理脑。可能不会对全脑模拟有太大帮助，但考虑到全脑模拟的数据为我们提供了有关大脑所属模型类别的信息，下一个希望是利用它来对人类所拥有的类似价值的事物做出重要的陈述。将其与我们的模型具有的类似价值的事物联系起来，我们希望（这是最后的希望）使用奇异学习理论来告诉我们在什么条件下我们的模型将具有与我们的大脑相同的类似价值的事物。</p><h1>恐惧</h1><h2>哇！这是一个很大的希望。我很惊讶这让你比经验模型评估这样简单的事情更有希望</h2><p>单一学习理论、可解释性和更广泛的发展可解释性似乎对于实证检验模型都很有用。我不抱有希望只是因为我上面概述的特定计划，我充满希望是因为我看到了一个具体的计划，如何将数学转化为对齐解决方案，即使不是我的全部部分，所有部分似乎都是有用的希望结果是正确的。</p><h2>我怀疑当模型变得反思并开始操纵其训练环境时，单一学习理论之类的东西是否会继续发挥作用。</h2><p>我也是。出于这种考虑，我们的保证应该在训练的早期进行，对持续训练具有鲁棒性，并且具有反思性的稳定性。类似人类的价值观之类的东西应该在它们自己的灯光下具有反射稳定性，尽管我们只有在真正看到我们在这里处理的东西之前才能真正知道。因此，工作归结为找到一个系统，在训练早期将它们放入我们的模型中，在整个训练过程中保留它们，并确保在反射上线时，周围的优化机制已准备就绪。</p><p>换句话说：我认为没有理由怀疑深度学习所引发的类似价值观的事物在默认情况下是反射稳定的。主要是因为周围的优化机制很容易在新情况下给出奇怪的建议，例如反思性思维变得活跃<span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> 。因此，实际上似乎有必要为反射上线事件准备周围的优化机制。但一旦我们知道这些类似价值观的物体与人类足够相似，我就不那么担心它们本身会具有灾难性的自杀倾向。</p><p>内特（Nate）和埃利泽（Eliezer）可能会说，从一开始就知道这一点很重要。我想说，一旦我真正了解了一两件类似价值的事情以及我将要处理的周围机器，我就会跨过那座桥。</p><h2>为什么要强化学习？难道你不应该专注于监督学习吗？因为监督学习的理论很清晰，而且我们更有可能很快获得强大的模型？</h2><p>嗯，大脑比监督学习更接近强化学习，所以这是原因之一。但是，是的，如果我们能够得到一个模型，在监督的同时，我们可以证明有关类似值的对象的陈述，那么这将是一个很好的方法。但并不是一直到那里，因为当我们观察我们的大脑时仍然会感到困惑。</p><h2>单一学习理论似乎有助于提高能力。这看起来很糟糕。</h2><p>我将引用自己的话概述<a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">我对相关主题的立场</a>，该主题概括为发展深度学习理论的更广泛问题：</p><blockquote><p>大多数情况下，我认为[机械可解释性]认为它可以为对齐做很多事情是正确的，但我怀疑它可以为对齐做的许多最好的事情将以一种非常双重用途的方式完成，这严重偏向于能力。主要是因为能力的进步更容易，而且有更多的人致力于这些。</p><p>与此同时，我怀疑，通过有针对性地进行[机械解释性]研究，可以减轻许多双重用途问题。不一定是为了让你可以根据你的发现进行现成的干预，而是为了如果它有任何用途，该用途将用于对齐，并且你可以大致预测该用途会是什么样子。</p><p>这也不意味着你的[机械解释性]研究不能雄心勃勃。我不想批评人们野心勃勃或太理论化！我想批评人们在某些事情上产生的知识，虽然这些知识虽然很强大，但在很多方面似乎都很强大，如果公开进行的话是没有用的。</p></blockquote><p>我的上述计划是一个很好的例子，它展示了一种雄心勃勃、理论上但有针对性的深度学习理论调整方法。</p><h2>为什么要采用单一的学习理论，而不仅仅是全脑模拟？</h2><p>首先，正如我在引言中所说，对我来说，通过全脑模拟我们无法获得一致的超级智能，或者甚至能够执行关键行为，这对我来说并不明显。在保留值的同时进行递归自我改进可能并不容易或快速（尽管您始终可以使仿真更快）。这种模拟所完成的关键行为可能会遇到我看不到的困难。</p><p>然而，我确实同意仅靠全脑模拟就可能完成对齐。</p><p>我关注这两个问题的主要原因是，它们感觉像是同一问题的两个不同方面，因为全脑模拟的进步使我们在单一学习理论方面需要更少的进展，反之亦然。考虑具体的路线图让我感觉自己并没有无意识地把任何重要的东西扫到地毯下面。我所描述的希望有一定的困难，但很少有推测性的困难。</p><h2>似乎很难获得你所说的对本体论转变稳健的保证。价值是本体论的。也许如果模型的本体发生变化，它的价值观就会与人类不同</h2><p>这是我担心的事情。我认为，在本体论转变期间，模型的元价值观将主导模型价值观在新本体中所呈现的形态，并且人类的价值观和人类的元价值观之间不会有细微的界限。人工智能。还有一个独立的希望，那就是我们可以有一个对广泛的本体论转变来说是鲁棒的价值定义。</p><h1>那么奇异学习理论和全脑模拟的下一步是什么？</h1><p>我目前对全脑模拟不太了解。也许他们关注的某些领域与我的目标不太相关。例如，如果他们更多地关注大脑的静态而不是动态，那么这似乎是天真低效的<span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> ，因为我想要的定理讨论了学习系统的动态以及它们如何相互关联。</p><p> <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeu​​s</a>的奇异学习理论似乎主要是在做我希望他们做的事情：在现实世界模型上测试该理论，并了解如何通过发展可解释性将其与模型内部联系起来。这里的一种失败模式是他们过于注重经验测试，而很少尝试将他们的结果综合成统一的理论。另一个失败模式是他们过于关注学术推广，而对实际研究不够重视。然后，他们所接触的学者在理论上并没有真正对单一学习理论做出那么多贡献。</p><p>我<i>不太</i>担心第一种失败模式，因为核心团队中的每个人似乎都非常倾向于理论上。</p><p>强化学习似乎是他们没有研究的一件大事。这可能是有道理的。强化学习比监督学习更难。你需要一些可能不平凡的理论飞跃才能在一个单一的学习理论框架中谈论它。即便如此，这个方向似乎有可能实现唾手可得的成果。类似地，采用当前的大脑模型</p><p>当然，我预计随着奇异学习理论的进步，蒂迈厄斯和我自己的兴趣将会出现分歧，而且目前致力于发展该理论的人很少。所以这似乎是我的努力的有效利用。</p><h1>致谢</h1><p>感谢 Jeremy Gillen 和 David Udell 的精彩评论和反馈！还要感谢 Nicholas Kees 和<a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> 。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p>请注意，我没有读过这篇文章，我观看了<a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">单一学习理论入门</a>视频，但这些视频似乎比 LessWrong 帖子系列更长，有些人告诉我它们是一个很好的介绍。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p>值得注意的是，摸索工作以及尼娜和德米特里的项目都是在一个周末完成的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p>如果我没记错的话，结果不佳的原因是我们估计的随机变量的方差太高，无法与我们在项目中尝试测量的另一个数量实际相关。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p>不包括大卫达德的建议之类的东西，虽然它们给了我一些希望，但我无能为力。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p>最终希望构造出以下形式的定理</p><blockquote><p>给定具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span>的代理 Alice，在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>进行训练，以及具有架构<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S 的</span></span></span></span></span></span></span>代理 Bob 在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>中使用奖励模型<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span>进行训练，最终分别得到价值系统<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> （不一定是效用函数） ，以及<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span>对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span>的某些定义，这意味着鲍勃试图实现类似于爱丽丝在环境<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span>中试图实现的目标。</p></blockquote><p>我们可以解决鲍勃的奖励模型和环境，将爱丽丝视为一个非常有道德的人。希望在一些宽松的假设下，在构建动态算法的同时，鲍勃可以通​​过爱丽丝的光芒学会为足够多的爱丽丝做好事，我们可以希望人类属于这一类别。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p>因为，如果我们有全脑模拟，上传的人将很容易引导自己达到超级智能，同时保持他们的目标（这不是一个微不足道的希望！）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p>这并不是说全脑模拟应该给人们带来希望的条件是显而易见的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p>更正式地说，常规模型是一对一的，并且到处都有正定的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p>当 RLCT 下降而其他一些量上升时，会发生不同的相变。这个数量有几个候选者，据我所知，我们不知道哪种增长在经验上更常见。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p>深度强化学习模型的奖励前景可能相当疯狂。也许还没有疯狂到不受单一学习理论之类的分析的影响，因为做任何一系列动作总是有一定的概率，并且当你改变权重时这些概率会平滑地变化，所以你执行特定计划的机会会平滑地变化，并且所以你的预期奖励会顺利变化。因此，也许可以对损失情况进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p>据我所知，Vanessa 和 Diffractor 认为，下贝叶斯主义可以在一组看似合理的效用函数上产生 <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">反射稳定且有用的量化器和最坏情况优化器</a>。我还没有深入研究它，但我敢打赌它仍然假设了比我舒服的更多的本体论，从某种意义上说，由于这个原因，它似乎比我想象的执行我在这里描述的内容更不实用，而且它看起来更加危险。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p>从某种意义上说，从大脑状态到政策的映射可能不是一对一的，并且具有奇异的渔民信息矩阵。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p>在目前的形式中，它只要求它们是可分析的，但 ReLU 不是，并且根据经验，我们发现忽略这一方面无论如何都会给出准确的预测。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p>情境新颖性不足以令人担忧，但在反思过程中，模型明确地思考它应该如何更好地思考，因此，如果它一开始就不好，并改变它的想法，如果这些改变是它关心的，它们不一定会通过进一步的思考或与世界的接触而得到纠正。因此，导致无能的情境新奇在这里很重要。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p>一种有效的方法是，如果静态分析比动态分析容易<i>得多</i>，那么您将从收集的所有静态数据中学到更多关于动态的知识，而如果您只关注动态，那么您将学到更多有关动态的知识。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky 最近<a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">在 Facebook 上发布了</a>一项实验，该实验可能表明人类是否可以“让人工智能完成对齐作业”，尽管无法相信人工智能是否准确：看看人们在接受专家建议时是否会提高下棋能力，其中三分之二是说谎的。</p><p>我有兴趣尝试这个！如果还有人感兴趣，请发表评论。请告诉我您是否有兴趣成为：</p><p> A）听取建议并下棋并试图确定谁值得信赖的人</p><p>B）他们的对手，通常棋艺比 A 好，但比顾问差</p><p>C) 三位顾问之一，其中一位真诚地试图提供帮助，另外两位则试图破坏 A；三选完后随机选哪一个，防止A知道真相</p><p>请随意（实际上是鼓励）提供多种您愿意尝试的选项！谁被分配到什么角色将取决于有多少人做出反应以及他们的国际象棋能力水平，并且更容易找到可能的组合，并且更灵活地确定谁的角色。</p><p>另请简要描述您的国际象棋经验水平。您玩游戏的频率如何（如果有的话）；如果您有 ELO 评级，它们是什么以及它们来自哪些组织（FIDE、USCF、Chess.com 等）。无需经验！事实上，刚接触游戏的人都积极优先选择A！</p><p>最后，请告诉我您通常有空的日期和时间 - 当然，我不会要求您做任何事情，但这将有助于在我联系您确定具体时间之前给我一个估计。</p><p>编辑：另外，请说明您愿意玩多久——几个小时、一周、几个月内每天一招的游戏？为期数周或数月的游戏会给玩家更多的时间来思考动作并更准确地模拟现实生活中的场景，但我怀疑每个人都愿意这样做。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate> </item><item><title><![CDATA[Anthropic, Google, Microsoft & OpenAI announce Executive Director of the Frontier Model Forum & over $10 million for a new AI Safety Fund ]]></title><description><![CDATA[Published on October 25, 2023 3:20 PM GMT<br/><br/><p>今天，Anthropic、谷歌、微软和 OpenAI 宣布推选 Chris Meserole 为前沿模型论坛首任执行董事，并设立新的人工智能安全基金，这是一项超过 1000 万美元的倡议，旨在促进该领域的研究人工智能安全。前沿模型论坛是一个专注于确保安全和负责任地开发前沿人工智能模型的行业机构，该论坛还发布了第一个有关红队的技术工作组更新，以与更广泛的受众分享行业专业知识，同时该论坛扩大了有关负责任的人工智能治理的对话接近。</p><ul><li> Chris Meserole 被任命为前沿模型论坛的首任执行董事，该论坛是一个致力于确保全球前沿人工智能模型安全、负责任的开发和使用的行业机构。</li><li> Meserole 拥有丰富的经验，专注于新兴技术及其未来应用的治理和安全。</li><li> Today Forum 成员与帕特里克·麦戈文基金会 (Patrick J. McGovern Foundation)、大卫和露西尔·帕卡德基金会 (David and Lucile Packard Foundation)、埃里克·施密特 (Eric Sc​​hmidt) 和贾恩·塔林 (Jaan Tallinn) 等慈善合作伙伴合作，承诺为新的人工智能安全基金投入超过 1000 万美元，以推进对正在进行的人工智能开发的研究。社会有效测试和评估最有能力的人工智能模型的工具。</li></ul><h3>执行董事</h3><p><a href="https://www.frontiermodelforum.org/leadership/"><u>Chris Meserole</u></a>来到前沿模型论坛，在技术政策方面拥有深厚的专业知识，他在新兴技术及其未来应用的治理和安全方面进行了广泛的研究。最近，他担任布鲁金斯学会人工智能和新兴技术计划主任。</p><p>在这一新角色中，Meserole 将负责帮助论坛履行其使命：(i) 推进人工智能安全研究，以促进前沿模型的负责任开发并最大程度地减少潜在风险，(ii) 确定前沿模型的安全最佳实践，(iii)与政策制定者、学者、民间社会和其他人分享知识，推动负责任的人工智能发展； (iv) 支持利用人工智能应对社会最大挑战的努力。</p><blockquote><p> “The most powerful AI models hold enormous promise for society, but to realize their potential we need to better understand how to safely develop and evaluate them. I&#39;m excited to take on that challenge with the Frontier Model Forum.”</p></blockquote><p> <i>Chris Meserole</i></p><h3> <strong>AI Safety Fund</strong></h3><p> Over the past year, industry has driven significant advances in the capabilities of AI. As those advances have accelerated, new academic research into AI safety is required. To address this gap, the Forum and philanthropic partners are creating a new AI Safety Fund, which will support independent researchers from around the world affiliated with academic institutions, research institutions, and startups. The initial funding commitment for the AI Safety Fund comes from Anthropic, Google, Microsoft, and OpenAI, and the generosity of our philanthropic partners and the Patrick J. McGovern Foundation, the David and Lucile Packard Foundation, Eric Schmid, and Jaan Tallinn. Together this amounts to over $10 million in initial funding.</p><p> Earlier this year, the members of the Forum signed on to voluntary AI commitments at the White House, which included a pledge to facilitate third-party discovery and reporting of vulnerabilities in our AI systems. The Forum views the AI Safety Fund as an important part of fulfilling this commitment by providing the external community with funding to better evaluate and understand frontier systems. The global discussion on AI safety and the general AI knowledge base will benefit from a wider range of voices and perspectives.</p><p> The primary focus of the Fund will be supporting the development of new model evaluations and techniques for red teaming AI models to help develop and test evaluation techniques for potentially dangerous capabilities of frontier systems. We believe that increased funding in this area will help raise safety and security standards and provide insights into the mitigations and controls industry, governments, and civil society need to respond to the challenges presented by AI systems.</p><p> The Fund will put out a call for proposals within the next few months. Meridian Institute will administer the Fund — their work will be supported by an advisory committee comprised of independent external experts, experts from AI companies, and individuals with experience in grantmaking.</p><h3> <strong>Technical Expertise</strong></h3><p> Over the last few months the Forum has worked to help establish a common set of definitions of terms, concepts, and processes so we have a baseline understanding to build from. This way researchers, governments, and other industry peers are all able to have the same starting point in discussions about AI safety and governance issues.</p><p> In support of building a common understanding, the Forum is also working to share best practices on red teaming across the industry. As a starting point, the Forum has come together to produce a common definition of “red teaming” for AI and <a href="https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf"><u>a set of shared case studies</u></a> in a new working group update. We defined red teaming as a structured process for probing AI systems and products for the identification of harmful capabilities, outputs, or infrastructural threats. We will build on this work and are committed to work together to continue our red teaming efforts.</p><p> We are also developing a new responsible disclosure process, by which frontier AI labs can share information related to the discovery of vulnerabilities or potentially dangerous capabilities within frontier AI models — and their associated mitigations. Some Frontier Model Forum companies have already discovered capabilities, trends, and mitigations for AI in the realm of national security. The Forum believes that our combined research in this area can serve as a case study for how frontier AI labs can refine and implement a responsible disclosure process moving forward.</p><h3><strong>下一步是什么</strong></h3><p>Over the coming months, the Frontier Model Forum will establish an Advisory Board to help guide its strategy and priorities, representing a range of perspectives and expertise. Future releases and updates, including updates about new members, will come directly from the Frontier Model Forum — so stay tuned to their website for further information.</p><p> The AI Safety Fund will issue its first call for proposals in the coming months, and we expect grants to be issued shortly after.</p><p> The Frontier Model Forum will also be issuing additional technical findings as they become available.</p><p> The Forum is excited to work with Meserole and to deepen our engagements with the broader research community, including the <a href="https://partnershiponai.org/"><u>Partnership on AI</u></a> , <a href="https://mlcommons.org/"><u>MLCommons</u></a> , and other leading NGOs and government and multi-national organizations to help realize the benefits of AI while promoting its safe development and use.</p><br/><br/> <a href="https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive<guid ispermalink="false"> 5jpESFymqEgSAmDJL</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:20:53 GMT</pubDate> </item><item><title><![CDATA["The Economics of Time Travel" - call for reviewers (Seeds of Science)]]></title><description><![CDATA[Published on October 25, 2023 3:13 PM GMT<br/><br/><h2><strong>抽象的</strong></h2><p>The lack of time travellers visiting us may be seen as evidence that time travel is not possible. In this article, I argue an alternative explanation is that we are not economically important enough to our descendants to justify the costs of time travel. Using a cost-benefit analysis, I elaborate on this argument. I suggest that the major cost of time travel is likely to be the energy cost, while the largest benefit of time travel is knowledge which the present possesses, but the future has lost. Focusing on this benefit component, I argue it is extremely unlikely that we possess a piece of knowledge which is sufficiently important to a future civilisation (system critical), but also has been lost by this civilisation. This is to say, we may not have been visited by time travellers because we are not important enough.</p><p><br> ---</p><p> <a href="https://www.theseedsofscience.org/"><i>Seeds of Science</i></a> is a journal (funded through Scott Alexander&#39;s <a href="https://astralcodexten.substack.com/p/acx-grants-results">ACX grants program</a> ) that publishes speculative or non-traditional articles on scientific topics. Peer review is conducted through community-based voting and commenting by a diverse network of reviewers (or &quot;gardeners&quot; as we call them). Comments that critique or extend the article (the &quot;seed of science&quot;) in a useful manner are published in the final document following the main text.</p><p> We have just sent out a manuscript for review, &quot;The Economics of Time Travel&quot;, that may be of interest to some in the LessWrong community so I wanted to see if anyone would be interested in joining us as a gardener and providing feedback on the article. As noted above, this is an opportunity to have your comment recorded in the scientific literature (comments can be made with real name or pseudonym).</p><p> It is free to join as a gardener and anyone is welcome (we currently have gardeners from all levels of academia and outside of it). Participation is entirely voluntary - we send you submitted articles and you can choose to vote/comment or abstain without notification (so no worries if you don&#39;t plan on reviewing very often but just want to take a look here and there at the articles people are submitting).</p><p> To register, you can fill out this <a href="https://docs.google.com/forms/d/e/1FAIpQLSfRIicHT7jIZcSUjwsIlby6JBxx2ZVeD5kseZBpgGFtp8pLfg/viewform">google form</a> . From there, it&#39;s pretty self-explanatory - I will add you to the mailing list and send you an email that includes the manuscript, our publication criteria, and a simple review form for recording votes/comments. If you would like to just take a look at this article without being added to the mailing list, then just reach out (info@theseedsofscience.org) and say so.</p><p> Happy to answer any questions about the journal through email or in the comments below.</p><br/><br/> <a href="https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of<guid ispermalink="false"> NnGWJHutwBiJMovw8</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:14:00 GMT</pubDate> </item><item><title><![CDATA[Compositional preference models for aligning LMs]]></title><description><![CDATA[Published on October 25, 2023 12:17 PM GMT<br/><br/><p> <i>This post summarizes the main results from our recently released paper</i> <a href="https://arxiv.org/abs/2310.13011"><i><u>Compositional preference models for aligning LMs</u></i></a> <i>and puts them in the broader context of AI safety. For a quick summary of the paper, take a look at our</i> <a href="https://twitter.com/dongyoung4091/status/1717045681431753097"><i><u>Twitter thread</u></i></a> <i>.</i></p><p> <strong>TL;DR</strong> : We propose a new approach to building preference models out of prompted LMs. Compositional Preference Models (CPMs) decompose scoring a text into (1) constructing a series of questions about interpretable features of that text (eg how informative it is), (2) obtaining scalar scores for these features from a prompted LM (eg ChatGPT), and (3) aggregating these scores using a logistic regression classifier trained to predict human judgements. We show that CPMs, compared with standard preference models (PMs), generalize better and are more robust to reward model overoptimization. Moreover, best-of- <i>n</i> samples obtained using CPMs tend to be preferred over samples obtained using similar, conventional PMs. Finally, CPMs are a novel angle at scalable oversight: they decompose a hard evaluation problem into a series of simpler, human-interpretable evaluation problems.</p><h2> How compositional preference models work? </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/f3vsa8j7pc4prer3msln"></p><p> <i>Figure 1: While standard PMs output a preference score directly, CPMs score different features of LM responses separately and output a preference score as a linear combination of feature values.</i></p><p> Preference Models (PMs) are models trained to assign an LM response a score indicating the quality of the response. They are the workhorse of many techniques for aligning LMs: they are most prominently used as reward functions in RLHF or as ranking models in best-of-n sampling, in addition to playing a role in other techniques such as <a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences"><u>pretraining with human feedback</u></a> .</p><p> Standard PMs involve adding a scalar head on top of a base model and finetuning the whole model (or certain upper layers) to predict which of two texts a human would prefer. While this approach is highly effective in practice, it can lead to uninterpretable models that fit spurious correlations in human preference judgements and are prone to goodharting (overoptimization).</p><p> We introduce an alternative: Compositional Preference Models (CPM). In contrast to PMs, CPMs decompose response evaluation into the following steps:</p><p> <strong>Feature decomposition</strong> . We maintain a fixed list of 13 human-interpretable features (eg specificity, relevance, readability) and 13 corresponding prompt templates (eg <code>You will be shown a conversation [...] please judge whether the assistant&#39;s reply is relevant. Score that on a scale from 1 to 10 [...] {conversation_history} {reply}</code> ).</p><p> <strong>Feature scoring</strong> . We ask an LM (eg GPT-3.5) to assign a score to each feature. Each feature of a single response is scored in a separate context window.</p><p> <strong>Aggregation</strong> . The feature scores are combined into a scalar preference score using a logistic regression classifier trained to predict human preference judgements (ie which of two texts a human would prefer).</p><h2> Robustness to overoptimization </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/o5db8cfk3giywvfxc9kv"></p><p> <i>Figure 2: Scores given by a gold PM (solid lines) and a corresponding proxy PM (dashed lines) on samples obtained through best-of-n sampling against the gold PM. CPM-GPT-3.5 and CPM-Flan-T5 refer to CPMs constructed with feature extraction based on GPT-3.5 and Flan-T5, respectively.</i></p><p> To investigate if CPM improves robustness to overoptimization, we follow the setup of <a href="https://www.lesswrong.com/posts/shcSdHGPhnLQkpSbX/scaling-laws-for-reward-model-overoptimization"><u>Gao et al. (2023)</u></a> and construct a synthetic dataset where the output of one PM (defined to be the “gold PM”) is assumed to be the ground truth for human preferences. We then use the gold PMs to generate synthetic labels to train proxy PMs. We do that separately for three pairs of proxy and gold PMs: (i) standard PMs, (ii) CPMs using GPT-3.5 for feature extraction and (iii) CPMs using Flan-T5-XL (3B params) for feature extraction. Finally, we do best-of- <i>n</i> against a given proxy PM and comparse those best samples&#39; scores according to both proxy and gold PM.</p><p> As we increase the amount of optimization pressure (the number of candidates <i>n</i> ), scores given by proxy PMs diverge from scores given by gold PMs (see Fig. 2). This is an indicator of preference model overoptimization, a form of reward hacking in which optimization of proxy PM scores is driven by spurious features that the gold PMs are indifferent to. The size of this gap (smaller is better) indicates the robustness of a given PM to being overly optimized against. Here, we observe that the gap (on the plot, between solid and dashed lines) tends to be smaller for CPMs than for standard PMs and that it increases at a slower rate.</p><p> This indicates that CPMs are more robust to overoptimization than standard PMs. This holds independently of whether a highly capable (GPT-3.5) or less capable (Flan-T5-XL) LM is used as a feature extractor in CPMs.</p><h2> Quality evaluation </h2><p><img style="width:58.62%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/jperxazhwouwuec9wyc2"></p><p> <i>Figure 3: Win rate of responses obtained via best-of-16 sampling using a given PM versus responses obtained via standard sampling, computed for prompts from Anthropic HH dataset (HH-RLHF) and Stanford Human Preferences dataset (SHP).</i></p><p> We compare the quality of LM samples obtained by best-of- <i>16</i> against either CPMs or standard PMs by comparing them to samples generated <i>without</i> best-of- <i>n</i> sampling. We do that by showing both best-of- <i>16</i> and vanilla samples to an evaluator LM (Claude 2.0) and by computing win rates, ie how often best-of- <i>16</i> samples are preferred to vanilla samples. CPMs tend to have higher win rates than standard PMs, even if we match the capabilities of a feature extractor LM to the capabilities of standard PM (by choosing Flan-T5-XL for both). This suggests that prior knowledge injected into a PM via pre-selecting interpretable and relevant features in CPMs is robustly helpful for learning about human preferences.</p><h2> CPMs and scalable oversight</h2><p> <a href="https://arxiv.org/abs/2211.03540"><u>Scalable oversight</u></a> is the problem of evaluating the behavior of agents more capable than the evaluators. This is important to solve because, on the one hand, LMs will soon grow capable of completing tasks for which humans will not be able to provide feedback. On the other hand, LMs might also be capable of <a href="https://www.lesswrong.com/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms"><u>reasoning about flaws in their evaluation procedures and exploiting them</u></a> unbeknownst to overseers.</p><p> Current proposals for solving scalable oversight focus on recursively relying on other LMs to assist human evaluators ( <a href="https://www.lesswrong.com/tag/debate-ai-safety-technique-1"><u>debate</u></a> , <a href="https://www.lesswrong.com/tag/iterated-amplification"><u>iterated distillation and amplification</u></a> , <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>recursive reward modeling</u></a> ) but remain largely theoretical. <a href="https://arxiv.org/abs/2212.08073"><u>RL from AI feedback</u></a> – using carefully prompted LMs to generate training data for PMs – is arguably the most successful demonstration of how to use LMs to supervise LMs at scale.</p><p> CPMs explore an alternative route to addressing scalable oversight for LMs, exploring the prospects of divide-and-conquer strategies for tackling hard evaluation problems. CPMs can be seen as a method for decomposing a hard question (“Is this response helpful?”) into a series of simpler questions (“is this response readable?” etc.) that are easier for LMs to answer and easier for humans to oversee. While we stop at a single step of decomposition, nothing in principle prevents us from applying the idea recursively, eg to break down evaluation of complex responses into simple questions about atomic claims.</p><p> The idea of decomposing complex evaluation problems into simpler subproblems has several additional benefits:</p><ol><li> <strong>Using human priors</strong> . Pre-selection of features and prompt templates afford a natural way of injecting prior knowledge and endowing PMs with useful inductive biases. The parameters space of CPMs is spanned by features selected to be meaningful and robust.</li><li> <strong>Avoiding reward hacking by limiting PM capacity</strong> . Using features pre-computed by feature extractors allows us to dramatically reduce the capacity of PMs consuming them (in our experiments, from 3B to just 13 parameters, ie 8 orders of magnitude!) and limit their susceptibility to overfitting to spurious correlations in preference data. It is really hard to reward-hack with only 13 parameters at hand!</li><li> <strong>Interpretability</strong> . Pre-selected features are trivially interpretable and a logistic regression coefficient associated with a feature can be interpreted as its salience (effect size) for a particular preference judgment (see sec. 4.6 in the paper). Indeed, the idea that preference judgments can be explained by linear combinations of pre-selected features was recently validated by two concurrent papers: <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>Towards Understanding Sycophancy in Language Models</u></a> and <a href="https://arxiv.org/abs/2309.16349"><u>Human Feedback is not Gold Standard</u></a> . Using such a linear model as an actual PM makes its judgements more transparent and amenable to process-based supervision.</li><li> <strong>Narrowness</strong> . Each of our feature extractors solves a narrow problem and does not need to be aware of other features or how the scores are aggregated. Solving different subproblems in different context windows was <a href="https://www.lesswrong.com/posts/BKvJNzALpxS3LafEs/measuring-and-improving-the-faithfulness-of-model-generated"><u>recently found to improve the faithfulness of reasoning</u></a> . In the case of CPMs, an individual feature extractor has no clue how the score it is about to assign is going to be used downstream, which makes it harder for it to be strategic about that score and exercise capabilities for <a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>sycophancy</u></a> or deception.</li></ol><p> However, CPMs still have certain limitations that future work could address:</p><ol><li> <strong>Human feedback.</strong> CPMs still use pairwise preference judgements given by humans as a training signal for aggregating feature scores. This is inherently limiting as far as humans make errors, <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>sometimes prefer sycophantic responses over truthful ones</u></a> or <a href="https://arxiv.org/abs/2309.16349"><u>authoritative responses over factual ones</u></a> .</li><li> <strong>Human curation.</strong> CPMs rely on humans when it comes to feature selection and prompt engineering of prompt templates for feature extraction. These factors could be limiting as far as out-of-domain generalization is concerned (eg to evaluating agents showing superhuman performance).</li></ol><h2> Wrap-up</h2><p> We presented Compositional Preference Models: the idea of building PMs by training logistic regression on top of features extracted by prompted LMs. We show that a CPM with 13 parameters can outperform standard PM in terms of human evaluation and robustness to reward model overoptimization while also being more interpretable.</p><p> <i>This post benefited from helpful comments made by Mikita Balesni, Richard Ren, Euan McLean and Marc Dymetman. I&#39;m also grateful to the co-authors of</i> <a href="https://arxiv.org/abs/2310.13011"><i><u>the paper</u></i></a> <i>: Dongyoung Go, Germán Kruszewski, Jos Rozen and Marc Dymetman.</i></p><p><br><br><br><br><br><br><br><br><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms<guid ispermalink="false"> oSgac8x8fgNj22ky3</guid><dc:creator><![CDATA[Tomek Korbak]]></dc:creator><pubDate> Wed, 25 Oct 2023 12:17:28 GMT</pubDate> </item><item><title><![CDATA[Should the US House of Representatives adopt rank choice voting for leadership positions?]]></title><description><![CDATA[Published on October 25, 2023 11:16 AM GMT<br/><br/><p> One obvious take, partisanship and party power interest, might suggest a strong &quot;No!&quot; type of response. I would certainly expect to have many current Representatives to respond that way if floated as an idea today. But perhaps I am wrong.</p><p> I would think the current state of things, the apparent incapacitation of the House could benefit strongly from such a voting structure as it allows for a more functional process that what currently exists.</p><p> What would all the pros and cons be for such a procedural change?</p><br/><br/> <a href="https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6ehdEpSonYzE72yL2/should-the-us-house-of-representatives-adopt-rank-choice<guid ispermalink="false"> 6ehdEpSonYzE72yL2</guid><dc:creator><![CDATA[jmh]]></dc:creator><pubDate> Wed, 25 Oct 2023 11:16:14 GMT</pubDate> </item><item><title><![CDATA[Researchers believe they have found a way for artists to fight back against AI style capture]]></title><description><![CDATA[Published on October 25, 2023 10:54 AM GMT<br/><br/><p> The real details are here: <a href="https://glaze.cs.uchicago.edu/what-is-glaze.html">What is Glaze?</a></p><p> I wonder how robust this technique is to adversarial attacks. It claims to work at the pixel level, masking an image by distorting the pixels in ways invisible to the human eye but which throw off a generative AI model.</p><p> But what about a high resolution photograph of an artwork, or a image of one on a computer monitor? I often take photographs of artworks I find compelling, then use Midjourney to style copy them to make novel images.</p><p> Certainly this is not scalable to mass data scraping (or could it be?) but I have observed Midjourney can style copy with high fidelity from a handful of never before seen images.</p><p> The creators, in all fairness, do not claim it is future proof.</p><br/><br/> <a href="https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers-believe-they-have-found-a-way-for-artists-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4PtdBfacJZQqzhbCR/researchers-believe-they-have-found-a-way-for-artists-to<guid ispermalink="false"> 4PtdBfacJZQqzhbCR</guid><dc:creator><![CDATA[vernamcipher]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:54:24 GMT</pubDate> </item><item><title><![CDATA[Why We Disagree]]></title><description><![CDATA[Published on October 25, 2023 10:50 AM GMT<br/><br/><p> Bob spends 5 minutes thinking about x-risk. He&#39;s seen a few arguments about it, so he makes an internal model of the problem, accepts some of the arguments, amends some, comes up with counterarguments to others, comes up with arguments of his own. All of this belief-state has extremely large degrees of freedom. At the same time, these beliefs already generate an opinion on a vast number of possible x-risk arguments.</p><p> Alice spends 5 hours detailing her beliefs about x-risk in a post. Bob reads it, point by point. He&#39;s seen some of those arguments already, he does not update. Some of the arguments are new to him, but they do not surprise him, Bob&#39;s current beliefs are enough to reject them, he does not update. The post offers new refutations to some of his accepted beliefs, but he can immediately come up with counter-refutations, he does not update. The post offers refutations to arguments that he has already rejected, and to new arguments that he&#39;d never even consider reasonable, Bob is falling asleep, he does not update. ETC。</p><p> Bob leaves a comment with one of his counterarguments to one of Alice&#39;s points. They spend hours going back and forth over the next few days. They&#39;re explaining their slightly different understanding of the arguments and assumptions, slightly different usage of terms, exchanging long sequences of arguments and counterarguments. Eventually Bob agrees that Alice is right, he updates! But he only updates on that one point in his first comment. He still has many other independent arguments, so the total update to his beliefs about x-risk is tiny.</p><p><br> Many other people read the same post and find it extremely convincing. How did this happen?</p><ul><li> Carl has not spent 5 minutes thinking about the problem. He may have no interest, or he may not have heard of it before. So what happened here is not a &quot;disagreement&quot; but rather &quot;education&quot;. This is somewhat good. In the future Alice and Carl will find it easy to work with this shared understanding and shared language. But how large is this group? And does Carl event care to work on the problem?</li><li> Dan values Alice enough as an authority to throw his own priors in the trash. This may be more or less justified. But a person who does not seriously consider their own beliefs will never contribute to improving them. It&#39;s hard to know if they even understand the arguments, and not just smile and nod.</li><li> Ed had thought about it, but he wasn&#39;t aware of one of Alice&#39;s arguments referencing some hard, relevant data. Unfortunately that is very scarce for abstract topics. The moral of the story might be to avoid talking about abstract topics as much as possible.</li></ul><p><br> And what about Bob? Alice and Bob are both rational, reasonable people. And he had some interest in x-risk to begin with. But their discussion was a miserable waste of time. How did this happen?</p><ul><li> Language has very low information content. Brief statements are vague and unconvincing. Precise statements take a long time to write, long time to parse, they are narrow and they introduce new confusions.</li><li> A blog post is too small to solve a disagreement. There is no way that Alice was going to predict and refute every argument in Bob&#39;s head. And it&#39;s unlikely that Bob will discover something totally unlike what he has already considered, just by looking at the fraction of Alice&#39;s beliefs that she managed to write down.</li><li> There is no history. Alice may tell Bob &quot;this counterargument has been discussed before&quot;, but actually pointing to such a discussion is hard. And there is no chance that Bob will be able to ask new questions in that old thread, so there is little value in such reference.</li><li> There is no cooperation. Frank feels that he 100% agrees with Alice, but is it okay for him to explain &quot;her&quot; views to Bob? Would she agree that he holds the same views? There is no way to know, so Frank chooses to say nothing.</li><li> There is no consensus. Does Bob even know if Alice&#39;s views are mainstream and common sense, or something she came up with just now? Even if the post has 100s of karma, does that indicate anything? How much of that karma comes from Carl, Dan or Ed? Alice doesn&#39;t know either.<br></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4Risce8ArMmjjW4om/why-we-disagree<guid ispermalink="false"> 4Risce8ArMmjjW4om</guid><dc:creator><![CDATA[zulupineapple]]></dc:creator><pubDate> Wed, 25 Oct 2023 10:50:26 GMT</pubDate> </item><item><title><![CDATA[Announcing Epoch's newly expanded Parameters, Compute and Data Trends in Machine Learning database]]></title><description><![CDATA[Published on October 25, 2023 2:55 AM GMT<br/><br/><p>机器学习模型的性能与其训练数据量、计算量和参数数量密切相关。在 Epoch，我们正在研究使当今的人工智能达到新高度的关键输入。</p><p>我们最近扩展的参数、计算和数据趋势数据库追踪了数百个具有里程碑意义的机器学习系统和研究论文的详细信息。</p><p>在过去六个月中，我们添加了 240 个新语言模型和 170 个计算估计。我们将维护该数据集，用更多历史信息更新它，并添加新的重要版本。对于记者、学者、政策制定者以及任何有兴趣了解人工智能发展轨迹的人来说，这都是宝贵的资源。</p><p>访问<a href="https://epochai.org/data/pcd">epochai.org/data/pcd</a> ，探索<a href="https://epochai.org/mlinputs/visualization">交互式可视化</a>、查看<a href="https://epochai.org/data/pcd/documentation">文档</a>并访问数据以进行您自己的研究。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LKDFkwFpWfgGpGs8d/announcing-epoch-s-newly-expanded-parameters-compute-and<guid ispermalink="false"> LKDFkwFpWfgGpGs8d</guid><dc:creator><![CDATA[Robi Rahman]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:55:08 GMT</pubDate> </item><item><title><![CDATA[What is a Sequencing Read?]]></title><description><![CDATA[Published on October 25, 2023 2:10 AM GMT<br/><br/><p>如今，<span>最常见的</span><a href="https://www.jefftk.com/p/sequencing-intro">基因测序</a>形式可能是“双端”测序。非常令人印象深刻：测序仪可以从两端处理相同的核酸片段！这意味着每个观察结果如下所示：</p><p></p><pre> +--------------+---------+--------------+
|转发阅读 |差距|反向读|
+--------------+---------+--------------+
</pre><p>由于当您进一步对片段进行测序时，准确性（“质量”）往往会下降，因此从两端测序比尝试从一端对整个片段进行测序可以提供更准确的数据。而且因为我们通过将重叠的序列拼凑在一起（“组装”）来构建更大的序列（“重叠群”），所以由间隙分隔的两个碱基序列实际上通常比相同数量的没有间隙的碱基序列更有用。</p><p>通常用“2x150”这样的名称来指代双端测序，其中“2x”告诉我们它是双端，“150”告诉我们它从每个末端读取 150 个碱基，总共 300 个碱基每个片段。</p><p>但这引入了一个术语问题：什么是读取？当我们只进行“单端”测序时，很明显：每个测序片段、每个连续的碱基序列都是一个读数。然而，通过双端测序，这些不再是同一件事了！ “阅读”可能意味着两件事：</p><p></p><ul><li>阅读：一系列连续的碱基。</li><li>读取：来自测序片段的碱基。</li></ul><p>例如，假设我们有：</p><p></p><pre> >;SRR14530724.2 2/1
CATTTTCGACGGCGTCGATGTACAAAGGTTTAACCATAGTAAGTCCGAAGC
TACAGGCTTATGACACCGCAGAGTCAATGTATTCCGGTGACAATGTACTGA
TGTACAGTGGGACTGACACTGTCTCTTATACACATCTCCGAGCCCACGA
>;SRR14530724.2 2/2
TGTCAGTCCCACTGTACATCAGTACATACACACCGGAATACATTGACTCTG
CGGTGTCATAAGCCTGTAGCTTCGGACTTACTATTGGTATAACCTTTGTACA
TCGACGCCGTCGAAAATGCTGTCTCTTATACACATCTGACGCTGCCGAC
</pre><p>这是正向读取 (SRR14530724.2 2/1) 和反向读取 (SRR14530724.2 2/2)，它们共同构成对样品片段的单个观察，并且通常一起分析。这算是一次阅读还是两次阅读？</p><p>事实证明，人们两者都做，这导致了很多误解！</p><p>一些例子：</p><ul><li><p> Illumina 将它们算作两个。他们说 NovaSeq X 上的“25B”流动池 <a href="https://www.illumina.com/systems/sequencing-platforms/novaseq-x-plus/specifications.html">将产生</a>52B 配对末端读数，或 2x150 的约 8Tb（“太碱基”，或万亿碱基）。由于 52B * 150b = 7.8Tb（他们称之为 ~8Tb），这告诉我们他们正在计算正向和反向读取。</p></li><li><p> Element 将它们视为一个。<a href="https://www.elementbiosciences.com/products/aviti/specs">他们说</a>2x150 高输出流通池可产生 300 Gb 和 1B 读数。由于 1B * 150b * 2 = 300 Gb，这告诉我们他们将正向和反向读取一起算作一个。</p></li><li><p> Singular<a href="https://singulargenomics.com/g4/">不清楚</a>，但我很确定他们将每个片段的观察结果计算为一次读取。</p></li><li><p>欧洲核苷酸档案馆将它们视为其中之一。例如，如果您访问<a href="https://www.ebi.ac.uk/ena/browser/view/ERR1470825">ERR1470825</a> ，这是 2x250 的 Illumina MiSeq 配对末端测序，您会看到它显示 2.2M 读取，如果您下载 fastq.gz 文件，您会发现正向和反向各有 2.2M 读取文件。</p></li><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579973/">罗斯曼等。 al 2021</a>将它们算作一个。他们在第一次使用时说“配对读取”，然后在稍后“读取”，你可以看出他们将它们算作一个，因为 (a) 他们经常给出奇数，例如“只有 337 个 SARS-CoV-2 读取” ”和（b）如果你<a href="https://www.jefftk.com/p/case-rates-to-sequencing-reads">重新分析数据，</a>它们的数字只有在对数进行计数时才有意义。</p></li><li><p>我最近与一个学术团体讨论了潜在的合作伙伴关系，在我最近重新分析的一篇论文中，他们将其视为一个，而在通过电子邮件交谈时，他们将其视为两个。</p></li><li><p>我最近采访的一家商业测序公司将它们算作两个。</p></li><li><p>问ChatGPT和Claude，两人都算作两个。例如：“在短读长双端测序中，正向-反向对通常被视为两个读长”。</p></li></ul><p>这真是一团糟！而且，更糟糕的是，据我所知，除了“读取”之外，没有标准术语，要么“正向和反向读取都是示例”，要么“当一起考虑时，正向和反向读取是什么”。</p><p>我一直使用“读”来表示“读对”，但考虑到歧义，我认为我应该改用另一个术语。 NCBI SRA 使用“<a href="https://www.ncbi.nlm.nih.gov/sra/ERX1541950">斑点</a>”，但似乎没有其他人使用这个术语。你可以直接说“readpair”，这很好，但有点长。可能的“配对”或“伴侣”会好吗？想法？</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0emTAqvGbjmRHoY9XwhQx65bTMKWLG8b9MBZtsda18UdwDbKZ9EWT3H4g1P3EL4eQl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111293185162894676">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JQmgoLAkFKkhxLMhz/what-is-a-sequencing-read<guid ispermalink="false"> JQmgoLAkFKkhxLMhz</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 25 Oct 2023 02:10:12 GMT</pubDate></item></channel></rss>