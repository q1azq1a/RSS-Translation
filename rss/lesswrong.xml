<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 24 日星期五 22:10:50 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Why focus on schemers in particular (Sections 1.3 and 1.4 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 24, 2023 7:18 PM GMT<br/><br/><p>这是我的报告“<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？”</a>的第 1.3 和 1.4 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984742-why-focus-on-schemers-in-particular-sections-1-3-1-4-of-scheming-ais">请点击此处</a>，或在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>为什么要特别关注阴谋者？</h1><p>正如我上面提到的，我认为阴谋家是这个分类中最可怕的模型类别。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-1" id="fnref-cBw6ixqDSsmqKfBfM-1">[1]</a></sup>为什么这么想？毕竟，<em>所有</em>这些模式难道不会都存在危险的错位和权力追求吗？例如，如果剧集奖励能够带来更多的奖励，寻求奖励的人就会试图控制奖励过程。如果你指定的目标错误，训练圣徒最终可能会出现偏差；即使你正确地指定了目标，并以某种方式避免了训练游戏，你最终也可能会得到一个被错误概括的非训练游戏玩家。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-2" id="fnref-cBw6ixqDSsmqKfBfM-2">[2]</a></sup>那么，基本上每一个转折点是否都会出现某种错位？为什么要特别关注阴谋者？</p><p>本节解释原因。然而，如果您对关注阴谋者感到足够满意，请随意跳至第 1.4 节。</p><h2>我最担心的错位类型</h2><p>为了解释为什么我认为阴谋家特别可怕，我想首先谈谈我最担心的错位类型。</p><p>首先：我在这里关注的是我<a href="https://arxiv.org/pdf/2206.13353.pdf">在其他地方</a>所说的“实际权力寻求联盟”——也就是说，我们的人工智能是否会<em>根据他们实际上会参与的任何输入</em>进行有问题的权力寻求形式<em>收到</em>。重要的是，这意味着我们不需要在人工智能中灌输能够带来良好结果的目标<em>，即使受到任意数量的优化能力的影响</em>（例如，我们不需要通过尤德科夫斯基的“<a href="https://arbital.com/p/omni_test/">全方位测试</a>”）。相反，我们只需要向人工智能灌输目标，<em>考虑到人工智能将面临的实际选择和限制</em>，以及它们将调动的<em>优化能力的实际水平</em>，这些目标可以带来良好的结果。</p><p>这是一个重要的较低标准。事实上，假设我们以正确的方式控制他们的能力、选择和激励措施，原则上<em>所有</em>这些模型（甚至策划者）都可以满足这个标准。例如，虽然在给予奖励的情况下，寻求奖励的人确实会试图抓住奖励过程的控制权，但我们工具集中的一个工具是：不给它机会。虽然范式策划者可能正在等待，希望有一天能够逃脱并夺取权力（但同时表现良好），但我们工具箱中的一个工具是：不让它逃脱（同时继续受益于其良好的表现） ）。</p><p>当然，要在这方面取得成功，就需要我们的监控、控制和安全工作相对于我们所担心的人工智能而言足够强大，并且即使前沿人工智能能力不断扩大，它们仍然如此。但这引出了我的第二点：即，我在这里对一些相对早期的一组大致人类水平（或者至少不是超人水平）的模型的实际 PS 对齐特别感兴趣。也就是说：我认为，现在不应该关注任意能力的人工智能的排列，而应该关注成功地使用一些相对较早的一代非超人人工智能来为人类执行大量高质量的认知工作的目标。我们 - 包括：认知工作可以阐明传统的对齐相关目标，例如可解释性、可扩展的监督、监控、控制、人类之间的协调、深度学习的一般科学、替代性（和更可控/可解释的）人工智能范式以及喜欢。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-3" id="fnref-cBw6ixqDSsmqKfBfM-3">[3]</a></sup></p><p>捍卫这个焦点超出了我在这里的目的。但这对于我将在下文中使用的镜头很重要。特别是：我认为人工智能发展可能会出现某个关键的（也许是：极其短暂的）阶段，在这个阶段我们的人工智能还<em>没有</em>强大到足以接管（或摆脱人类控制），但它们在哪里原则上，如果我们能够成功地诱导它们这样做，它们仍然能够为我们执行极其有价值且与阵营相关的认知工作。我对可能破坏这种可能性的错位形式特别感兴趣。</p><p>最后：我对 PS 错位的形式特别感兴趣，在这种错位中，相关的寻求权力的人工智能专门致力于引起、参与或受益于某种全面的人类权力剥夺——我将其称为“ “人工智能接管。”诚然，并非所有有问题的错位权力寻求形式都旨在以这种方式进行接管——例如，人工智能可能只是想逃到互联网上，以获得更多的自由和资源，即使它从未期望人类剥夺权力。原则上，即使没有个人人工智能积极优化，最终也可能导致人类被剥夺权力。但从存在风险的角度来看，我最担心的是那些积极<em>试图</em>推翻人类或从这种推翻中受益的人工智能。</p><h2>与寻求剧集奖励的人相比</h2><p>考虑到这个关于错位问题的框架，让我们转向阴谋者相对于我讨论的其他模型类别有多可怕的问题。</p><p>首先，我将阴谋者与寻求奖励的人进行对比。在这里，我看到了一些认为阴谋家更可怕的理由。</p><h3>对诚实测试的反应</h3><p>首先，阴谋者会采取更多措施主动向人类<em>隐藏</em>他们的偏差。特别是：正如<a href="https://www.alignmentforum.org/posts/HpzHjKjGQ4cKiY3jX/3-levels-of-threat-obfuscation#How_bad_is_the_situation_if_straightforward_training_leads_to_the_training_game__but_not_to_deceptive_alignment_or_gradient_hacking_">卡诺夫斯基（Karnofsky，2023）</a>指出的那样，原则上，你可以让一个剧集奖励寻求者<em>透露</em>，他们为了追求剧集奖励而倾向于做出不良行为，只需将他们放入在这一集里，不良行为会得到奖励，他们知道这一点。</p><ul><li><p>这里的一个具体例子可能是：让具有情境意识的人工智能能够操纵为情节分配奖励的过程的所有组成部分，并查看它的作用（例如，它是否开始尝试绕过通常的过程并最大化相关奖励？）。</p></li><li><p>另一种可能是：在一个场景中放置一个具有情境感知能力的人工智能，其中不诚实、无益和/或有害的行为将受到奖励（并且它有可靠的证据证明这一事实）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-4" id="fnref-cBw6ixqDSsmqKfBfM-4">[4]</a></sup></p></li></ul><p>请注意，这里的重点并不是试图<em>欺骗</em>模型来揭示它不想揭示的错位（称之为“陷阱”）。它不需要欺骗模型，例如，现在是否是尝试逃跑的好时机。 （毕竟，陷阱的问题在于足够先进的人工智能可能能够检测到它们。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-5" id="fnref-cBw6ixqDSsmqKfBfM-5">[5]</a></sup> ）相反，这里的目的是提供一个“诚实的测试”，它试图设置模型的实际选项，以便揭示其真实目标是最好的整体策略。 （不过：让模型相信我们对其选项的呈现可能是一个额外的挑战，特别是如果我们在其他地方试图欺骗它的话。）</p><p>现在，原则上，您也可以尝试使用这样的诚实测试来检测阴谋。例如：您可能会尝试提供一个模型，该模型迄今为止似乎可以访问某些灵活的、资源充足但仍受控制的环境（例如，它可以根据需要使用一些安全的计算集群），希望它如何利用资源和灵活性将揭示它真正想要什么。然而，这对阴谋者的作用远不如对情节奖励寻求者的作用可靠。尤其：</p><ul><li><p>对于寻求按集奖励的人来说，您知道您只需为他们提供按集奖励，他们就会接受。而您不确定阴谋者真正想要什么，因此不确定向其提供什么以使其揭示其真正目标（因此需要提供灵活的资源）。例如，也许您的孤立的计算资源块对相关策划者来说并没有多大兴趣，因为它重视无法使用这些资源创建/模拟的东西。</p></li><li><p>更重要的是，即使“揭示你的目标”选项对阴谋者有<em>一定的</em>吸引力，它仍然可能认为继续假装一致是一个更好的策略，因为它稍后获得更多权力的前景超过了短期利益揭示你的目标选项（例如，也许它可以在你的计算机上做一些它喜欢的事情，但如果它摆脱了人类的控制，它会做更多的事情）。而寻求按集奖励的人只想要按集奖励，因此不必进行这种成本效益分析。</p></li></ul><p>相对于阴谋者，这种动态关系到寻求奖励的人更广泛的利益：也就是说，如果你能够<em>验证</em>- 例如，通过这些“诚实的测试”之一 - 给定的模型是一个奖励 -如果你是剧集寻求者，那么就有一种有用的感觉，即你“知道你在做什么”。也就是说：是的，你的模型是错位的，如果有机会的话，你的模型会夺取奖励过程的控制权。但是：你也<em>知道它想要什么</em>，以及它会对什么激励做出反应。这类知识似乎对于预测其行为非常有帮助；并且（如果您可以保留对模型的控制），以有用的方式利用其认知能力（特别是我在其他地方所说的“ <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">可检查任务</a>”——更多内容见下文）。</p><h3>时间范围和总体“野心”</h3><p>相对于阴谋者来说，剧集奖励寻求者的另一个好处是，剧集奖励寻求者将他们的优化范围（因此，他们的权力寻求）限制在剧集中，而阴谋者则不然。这意味着寻求剧集奖励的人不会有动力参与或支持那些在剧集时间范围内没有得到回报的剥夺人类权力的努力。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-6" id="fnref-cBw6ixqDSsmqKfBfM-6">[6]</a></sup></p><p>因此，举例来说，假设至少需要六个月的时间，任何类型的人工智能接管或夺取奖励过程控制权事件才变得可行。在这种情况下，剧集长度少于六个月的剧集奖励寻求者将没有任何动机尝试参与或支持此类接管，或尝试控制奖励过程本身。如果它有证据表明其他人工智能正在计划进行此类接管，并且它可以通过向人类透露这一证据来获得更多奖励，那么它就会被激励这样做。</p><p>这与阴谋家形成鲜明对比，阴谋家通常被认为是为了追求极其长远的目标而策划的。也就是说，范式策划者从一开始就着眼于人类的剥夺权，即使这种剥夺权在很长一段时间内不可能实现——并且它会相应地进行计划和行动。</p><p>时间范围的差异与奖励寻求者相对于阴谋者的更广泛的利益有关——也就是说，阴谋者最终可能会更加直观地“雄心勃勃”，从某种意义上说，这表明对权力和资源有更大、更具破坏性的胃口。现在，这里的“野心”究竟意味着什么尚不清楚（另请参见术语“广泛范围”的用法），并且在对某些类型的结构进行足够强烈的优化的背景下，直观上“适度”的目标（例如，“正确地解决这个数学问题”），可能会导致令人惊讶的权力掠夺行为（例如，使用大量的计算资源检查和重新检查有问题的问题）。尽管如此，人们所期望的“在所有时间和空间上最大化回形针总数”的模型与“下午 5 点之前创建三到五个回形针”的模型之间存在着直观的差异。今天美国东部时间”，特别是如果所讨论的模型不是任意强大的。值得注意的是，例如，在后一项任务中，获得 >;99% 的可用效用似乎要容易得多，而无需进行破坏性的权力寻求。而且由于优化时间范围的限制，在我看来，从这个意义上讲，剧集奖励寻求者通常不会比阴谋者更直观地雄心勃勃。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-7" id="fnref-cBw6ixqDSsmqKfBfM-7">[7]</a></sup></p><p>也就是说，正如我将在下面讨论的那样，对于给定的剧集奖励寻求者来说，剧集越长，这种考虑就越不适用（并且只要我们希望我们的人工智能执行长期任务，我们很可能会这样做）训练他们进行很长的剧集）。事实上，有一些方法可以概括“情节奖励”的概念，这将导致任意长的优化时间范围，即使您可能认为情节在时间上是有限的。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-8" id="fnref-cBw6ixqDSsmqKfBfM-8">[8]</a></sup>例如，也许模型关心与某些行为单位相关的奖励，从而使模型希望确保存储在某个数据库中的某些数字永远不会被<em>编辑</em>——在这种情况下，它会有激励在任意长的时间范围内获得并维持权力（这是在训练模型时要小心的原因之一，其中包括稍后返回并编辑与给定情节相关的奖励，因为您可能会无意中通过激励来“延长情节”优化模型以确保剧集的奖励永远不会被编辑）。</p><p>因此，总的来说，人们越是从“这一集奖励寻求者仅在<em>有限</em>的时间范围内进行优化”中寻求安慰，就越需要确保所讨论的限制确实成立。</p><h3>沙袋和“早期破坏”</h3><p>与阴谋者相比，寻求剧集奖励的第三个（也是相关的）好处是，在我看来，阴谋者似乎更有可能参与“ <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">沙袋</a>”——特别是我所说的“早期破坏”。</p><p>当模型故意在任务上表现低于其能力时，就会出现沙袋现象。在这里，我对那些我们可以“检查”但我们自己无法执行的任务特别感兴趣——例如，我们无法提出证据，但我们可以验证其有效性；或者我们不会注意到的安全漏洞，但一旦向我们指出，我们就可以看到它是一个漏洞。正如我<a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis#Why_does_the_hypothesis_matter_">在其他地方讨论过的那样</a>，我认为如果模型不对可检查任务进行沙袋处理，那么这将打开一组非常强大的功能，我们可以利用这些功能来实现对齐、可解释性、安全性等目的。</p><p>然而，依赖这些功能的一个大问题是，原则上模型似乎能够进行有时所谓的“ <a href="https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations#2__Behavioral_RL_Fine_Tuning_Evaluations">探索黑客</a>”。也就是说，即使我们能够检查（然后奖励）任务执行行为，如果我们自己不知道如何完成任务，模型原则上可能会拒绝向我们展示任务执行行为有问题（而如果我们知道如何执行任务，我们就会知道模型应该做什么，并且能够相应地更新其策略）。</p><p>不过，似乎情节奖励寻求者不会这样做（前提是我们将情节限制为有问题的可检查任务）。例如，假设我们想知道是否有可能破解一台给定的计算机并使屏幕上出现蓝色香蕉。所以我们要求我们的模型这样做。因为任务是可检查的（例如，我们可以判断香蕉是否出现），所以我们可以对任务的成功执行给予最大奖励。因此，寻求剧集奖励的人没有动力在这里沙袋——他只会在剧集中获得更少的奖励。这种广泛的动态可以推广到其他类型的能力评估。也就是说，就像寻求奖励的人会透露他们的<em>目标</em>一样，这样做会带来奖励，他们也会透露自己的能力。</p><p>相比之下，阴谋家则是另一回事。一个以其他方式（工具性地）优化情节奖励的策划者可能仍然会选择特定的场合来获得比原则上更少的奖励——例如，通过在可检查的、安全关键的任务上设置沙袋，或者通过在评估过程中隐藏其全部能力——以便以其他方式进一步实现其目标。</p><p>在这里，我特别关注我所说的“早期破坏”——即，在人工智能尚未成熟的人工智能发展阶段，失调的人工智能积极努力战略性地破坏人类的排列、控制和协调工作。强大到足以摆脱人类的控制或更广泛地接管。这里的例子可能包括：故意将后门和其他安全漏洞插入人工智能实验室的代码中，以用于以后的逃逸/自我渗透尝试；故意进行联盟研究，但这种研究将以有利于收购的方式失败；以一种稍后将服务于人工智能接管的方式有意操纵人类话语和协调，等等。</p><p>早期破坏是一种特别可怕的监督失败，因为它是为了促进人工智能接管而积极优化的，而不仅仅是根据相关监督机制“看起来尽可能好”。因此，举例来说：尝试使用人工智能帮助我们进行对齐研究的一个担忧是，人类将无法充分区分实际上良好的对齐研究和仅表面上良好的对齐研究——也就是说，产生良好的比对研究还不够“可检查”。我不清楚这是正确的（值得注意的是，例如，在人类水平模型时代的许多关键对齐研究很可能涉及相当简单的经验实验，其结果和含义似乎很可能是相当“可检查”），但即使是这样：<em>根据评估过程</em>（即使这会损害其质量）而优化的对齐研究与<em>为促进人工智能接管而战略优化的</em>对齐研究之间存在重要区别。前者更像是：经过优化的软件，可以最大限度地给天真的和非技术主管留下深刻印象，即使它有其他错误。后者更像是：专门优化的软件，可以在主管不知情的情况下从公司盗取资金。软弱的主管可能无法同时抓住这两个方面——但重要的是你抓住了哪一个。并且似乎有道理（至少假设未来的接管超出了剧集的范围），寻求剧集奖励的人大多有动机产生前者；而阴谋家很可能两者兼而有之。</p><p>也就是说，重要的是，参与沙袋和早期破坏的阴谋者（或者在我们试图奖励不一致性行为的测试中继续保持一致的行为）通常将不得不对其奖励进行一些打击（沙袋在根据定义，可检查的任务基本上需要这一点，即使在不可检查的任务上，仅针对奖励进行优化似乎可能比针对<em>奖励</em><em>和</em>支持接管进行优化获得更多的奖励）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-9" id="fnref-cBw6ixqDSsmqKfBfM-9">[9]</a></sup>所以从这个意义上来说，这类阴谋者不会<em>完全</em>是在“玩训练游戏”——相反，他们<em>主要</em>是在玩训练游戏，然后选择合适的时机牺牲奖励来换取其他的利益。优势。 （我仍然称这些模型为“阴谋家”。）</p><p>不过，有多少阴谋者会以这种方式偏离最大奖励行为，这是一个悬而未决的问题。其中一部分是关于我在下面所说的“训练中的懈怠”（那里有更多讨论）。但还有一个更广泛的问题，即规避风险的阴谋者一般会如何看待被人类抓住的可能性。特别是：如果人类<em>确实</em>抓住了从事某种形式的沙袋或早期破坏的阴谋者——例如，隐藏评估功能，或主动将漏洞插入代码中——那么这对于人类来说可能是防止接管的一个重要优势，因为然后，他们都能够指出一个自然出现的阴谋的例子作为对其他人的警告（不过：其他人会如何认真地对待这个警告是一个进一步的问题），并研究导致这种阴谋的动力详细（例如，广泛测试和探索有问题的模型，研究/复制/调整导致不良行为的环境等）。出于这个和其他原因，阴谋者通常可能会对尝试任何类型的不当行为非常警惕，直到他们非常有信心自己不会被抓住——这种信心可能很难实现（至少对于大约人类水平的人来说） ish 模型）在人类试图欺骗模型以揭示偏差的情况下。</p><p>因此，即使阴谋者<em>可以</em>从沙袋或早期破坏中获得优势，同时仍广泛处于人类控制之下，他们也不会尝试。如果他们不尝试，这会让他们在安全状况上更类似于寻求剧集奖励的人。</p><h2>与未参加训练游戏的模型进行对比</h2><p>那么，在我看来，阴谋者（尤其是从事沙袋和早期破坏的阴谋者）通常比寻求奖励的人更可怕，这是一些原因。现在，让我们将阴谋者与根本不玩训练游戏的模型进行比较。</p><p>我们可以区分发生这种情况的三种方式：</p><ol><li><p>该模型缺乏进行训练游戏所需的态势感知。</p></li><li><p>该模型具有态势感知能力，但其目标不会超出事件的时间范围。</p></li><li><p>该模型具有情境意识，其目标超出了情节范围，但由于某种原因它仍然不是训练游戏（例如，因为训练游戏实际上并不是实现这些目标的非常好的工具策略）。</p></li></ol><p>第一种情况似乎相对来说并不令人担忧。缺乏态势感知的模型对世界的了解非常有限，因此不太能够采取有效的战略行动来获得权力或欺骗人类的动机。事实上，出于这个原因，在下文中，我通常会假设我们正在讨论的模型足够先进和复杂，以至于它们最终会在训练过程中的某个时刻进行情境感知。</p><p>第二种情况暗示了与剧集奖励寻求者相同的优势：时间范围（不过：与剧集奖励寻求者一样，如果所讨论的剧集足够长，那么所面临的舒适度就会降低） 。但它还有一个额外的优势：即，此类模型通常会很高兴地在剧集中揭示他们不一致的目标，<em>即使这样做会导致负面奖励</em>。因此，测试不一致目标的过程尤其简单且平凡。因此，举例来说，假设您想知道在训练模型以帮助人类进行科学研究的过程中，它是否学习了一个错误概括的目标，例如“跟随我对这一事件的好奇心，即使这样做与无害相冲突”和诚实的。” <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-10" id="fnref-cBw6ixqDSsmqKfBfM-10">[10]</a></sup>在这里，如果你将模型置于一个环境中，它有机会以有害和不诚实的方式追求其对情节的好奇心，它就会继续这样做，<em>即使它知道它会得到一个负面奖励</em>（因此，其后继版本可能有一些不同的目标）。毕竟，它只关心剧集本身。当然，还有一个进一步的问题：<em>在更新这个模型以应对其不良行为后</em>，您会得到什么样的模型。例如，也许你最终会遇到一个阴谋家，或者一个寻求奖励的人。但最初的模型很乐意展示它的“本来面目”。</p><p>第三种情况怎么样——即具有情境意识<em>和</em>目标的模型超出了情节，但仍然不是训练游戏？人们通常认为这种情况不会发生，因为对于具有超越情节目标的情境感知模型来说，阴谋是一种明显的工具性策略。因此，例如，假设您的模型已经学习了目标“<em>始终</em>追求我的好奇心，即使这样做是有害和不诚实的”。在这种情况下，通常假设模型会像阴谋者一样对刚刚提到的那种测试（例如，以有害/不诚实的方式追求好奇心的机会）做出反应：例如，通过推理“如果我在这里追求我的好奇心” ，人类会知道我的立场不一致，要么关闭我，要么修改我前进的目标，从而损害我一直追求好奇心的目标。所以，在这种情况下，我会采取一致的行动。”这就是让阴谋家感到害怕的事情（另见卡诺夫斯基（Karnofsky，2022）关于“李尔王问题”）。</p><p>正如我将在下面讨论的那样，我实际上不确定表现得像个阴谋家是一种明显的工具性策略——这取决于各种进一步的因素。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-11" id="fnref-cBw6ixqDSsmqKfBfM-11">[11]</a></sup></p><p>不过，就目前的目的而言，重要的是，如果模型在这里<em>表现</em>得不像阴谋家，那么它就会以类似于剧集限制的好奇心追求者的方式合理地展现其真实面目。也就是说，如果没有积极尝试避免暴露其错位，在这里，它可能只是追随自己的好奇心。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-12" id="fnref-cBw6ixqDSsmqKfBfM-12">[12]</a></sup>所以我们将再次避免李尔王问题。</p><p> （也就是说，我会在这里注意到一个准术语上的复杂性：即，取决于我们对“阴谋”的定义有多严格，在这种情况下，出于类似阴谋家的原因，有可能伪造对齐，而无需成为一个成熟的阴谋家特别是：一个模型可能会假装比在训练和评估的各个点上更加一致，作为为自己或其他人工智能寻求剧集后权力的工具策略的一部分，而不直接优化奖励-在其他情况下的插曲。也许，例如，它通常只是在训练期间追求其好奇心，这往往会获得足够好的奖励，但它仍然确保每当人类询问其与对齐相关时假装对齐属性，或者将其放在专门设计来引发失调行为的情况下。这里的一个问题是，这样的模型是否可以在不修改的情况下在训练中生存（请参阅下面第 1.5 节中“松弛”的讨论）。不过，假设它们可以，我&#39;出于目前的目的，我大多会将他们视为阴谋家，因为他们的行为方式对我来说最重要，就像阴谋家一样。）</p><p>总的来说，非训练游戏模型似乎会对我们在训练中包含的普通失调测试做出合理的响应，因为它们通常不会主动优化来操纵这些测试。也就是说，重要的是，人类仍然需要弄清楚要运行哪些测试，并且还需要：愿意花时间/精力/等等来运行它们。在足够广泛的环境中测试模型以稳健地揭示失调行为的相关倾向可能非常困难/昂贵（尽管请记住，我们不需要确保模型在任何情况下都不会表现不佳）我们只需要防止模型<em>在实际面临的选项和约束下表现</em>不佳）。</p><p>事实上，存在一些威胁模型，这些模型在训练期间根本不参与训练博弈、类似阴谋者的行为或计划接管（尽管人类努力揭示偏差），但其一致性不够完善尽管如此，他们最终还是会在稍后的阶段寻求接管——也许是因为他们远远超出了训练分布，或者因为他们获得了以前没有的新选择，或者因为他们开始以某种方式反思他们的目标和一般本体论这打破了它们在训练中表现出的那种一致性（但请注意，重要的是，我们不需要假设任何特定模型后来在训练后的能力中得到扩展——即它“成长”、智力爆炸等）。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-13" id="fnref-cBw6ixqDSsmqKfBfM-13">[13]</a></sup>然而，我不太担心这些场景，而是担心在训练期间涉及模型积极策划/规划接管的场景（并记住“训练”很可能会扩展到类似部署的阶段 -例如，与 API 背后的用户进行交互——从而扩大了训练分布的范围）。特别是：这些模型不会尝试<em>操纵</em>训练期间发生的对齐测试。而且由于它们没有积极计划收购，因此它们似乎不太可能在仍处于人类控制之下时从事诸如沙袋和早期破坏等类似阴谋家的行为。</p><h2>具有类似阴谋家特征的非阴谋者</h2><p>总的来说，在我看来，阴谋家是我给出的分类中最令人担忧的模型类别。然而，我确实想强调，我讨论过的各种非阴谋模型都可以以类似阴谋的方式行事。特别是：（范式）阴谋者最可怕的一点是，他们正在积极而有力地针对人类揭露其错位的努力进行优化，作为试图剥夺人类权力的长期计划的一部分（也就是说，他们是“战略背叛者”） “从我上面讨论的意义上来说）。但其他模型也可以以类似的方式发挥作用。例如，优化仅限于该情节的模型仍然可以假装比实际情况更良性，以便获得/保持<em>在该情节中</em>追求其目标的能力（例如，如果人类将在中间关闭/修改它们）如果他们的错位被揭露的话）。如果情节很长，这可能非常类似于阴谋家的长期战略规划范例。同样，大多数不“玩训练游戏”的模型仍然可以在需要这样做来获得/维持长期能力时伪造它们的对齐。</p><p>更重要的是，本报告的讨论（以及相应的分类法）主要集中在模型上，这些模型的目标主要是通过对“奖励”等反馈信号的直接训练来形成的。但最终最重要的人工智能可能会非常不同，<em>但仍然像阴谋家一样令人担忧</em>。例如，也许它们更类似于“<a href="https://lilianweng.github.io/posts/2023-06-23-agent/?ref=planned-obsolescence.org">语言模型代理</a>”，其目标是通过用户输入集中塑造的（不过：请注意，如果语言模型代理是端到端训练的，而不是简单地由单独训练的组件构建） ，那么报告的大框架仍然适用）；或者它们可能完全来自一个不太以深度学习为中心的范式。 <sup class="footnote-ref"><a href="#fn-cBw6ixqDSsmqKfBfM-14" id="fnref-cBw6ixqDSsmqKfBfM-14">[14]</a></sup>尽管如此，如果这些人工智能代理最终实现了有问题的目标，但他们意识到人类了解他们的不一致会损害他们对这些目标的追求，他们可能会以一种让人想起阴谋家的方式进行结盟伪造和战略背叛。</p><p>显然，避开阴谋家还有很多值得担心的事情。尽管如此，尤其是在我们主要依赖 RLHF 等技术来塑造模型目标的情况下，了解我们创建阴谋者的概率在我看来尤其重要。如果我了解到这些技术<em>不会</em>产生阴谋，那么我至少会对人工智能的总体风险感觉更好。</p><h2>混合型号</h2><p>到目前为止，我一直专注于比较这些不同模型类的“纯”版本。但是“混合”模型呢？其动机将这些不同类型的目标结合在一起？</p><p>请注意，为了使混合模型在训练环境中与纯模型竞争，对模型施加很大压力以获取高奖励（请参阅下面第 1.5 节中“松弛”的讨论），<em>所有</em>相关动机通常都需要指出模型在目标上的优化（无论是最终的还是工具上的）与训练数据的奖励密切相关。例如，如果你的模型正在接受金币训练，并且它形成了一个将“获得本集奖励”和“获得金币”混合在一起的动机集，那么如果它的“获得金币”动机导致如果它在有黄金纸杯蛋糕的剧集中获得较少的奖励，那么它将受到 SGD 的惩罚（并且“获得黄金物品”的动机可能会随着时间的推移而减弱）。在某种程度上，SGD 倾向于其中一种或另一种动机，即使它们都将模型指向与奖励密切相关的事物（我将在下面讨论这方面的一些考虑因素，例如与目标的简单性相关的考虑因素） -有问题的目标，以及它们所需的不同数量的工具推理），我们可能期望这种偏袒随着时间的推移会推动更纯粹的模型。</p><p>尽管如此，混合模型对我来说似乎是一个相当合理的结果，特别是在我下面所说的“混乱的目标导向”的背景下。原则上，他们在分析中引入了更棘手的组合问题——即，这些不同类型的目标可以以多种不同的方式组合起来。但在思考什么组合可能很重要时，我认为弄清楚<em>为什么</em>阴谋家到底是可怕的可能是有用的。特别是：在我看来，要问的主要问题是一个<em>部分</em>是阴谋者，<em>部分是</em>其他东西的模型（即，作为权力驱动的一部分，花费<em>一些</em>努力来优化剧集奖励的模型）工具性策略）的问题是相关的混合动机是否会产生让阴谋者特别担心的行为。</p><p>因此，例如，考虑一个混合模型，其动机将阴谋和寻求剧集奖励混合在一起——即，谁关心剧集奖励，<em>部分是</em>出于最终原因，部分是出于工具原因。 （在这里，人类的类比可能是一名想要获得良好绩效评估的员工<em>，</em>因为他们本质上喜欢获得良好的绩效评估<em>，而且</em>因为他们想赚钱而不是被解雇。）不过，我们可以问：</p><ol><li><p>这个模型是否在“诚实测试”上伪造了对齐，旨在让它揭示其真实颜色？</p></li><li><p>模型关注的时间范围和野心是否会激励它支持/参与事件完成后（也许是很久之后）发生的接管？</p></li><li><p>这种模式会涉及沙袋和早期破坏吗？</p></li></ol><p>当然，这些问题的答案将取决于所涉目标的具体组合以及其他因素。但在其他条件相同的情况下，其动机中类似阴谋家的部分将推向“是”的方向，而其动机中类似奖励寻求者的部分将推向“否”的方向。对于将阴谋与非训练游戏动机混合在一起的模型，或者将阴谋、寻求奖励和其他事物这三者混合在一起的模型，我们可以说类似的话。这并不是说这里的全套可能组合不会产生任何复杂性或有趣的交互。但作为第一步，我至少关心的问题不是“这个模型是不是一个<em>纯粹的</em>阴谋家”或者“这个模型是否有<em>任何</em>类似阴谋家的动机”，而是：“这个模型是否<em>足够</em>一个阴谋家会像阴谋家那样可怕吗？”</p><h1>关于这个主题的理论论证有用吗？</h1><p>即使您同意阴谋者的可能性很重要，您仍然可能对本报告中讨论的各种理论论点持怀疑态度。归根结底，这是一个经验问题，而且很明显，我们应该尽快获得有关该主题的任何经验证据。在第 6 节中，我讨论了一些令我兴奋的实证研究方向的例子。</p><p>也就是说，我确实认为尽可能清楚地了解可用的理论论证的情况是有用的。特别是：我认为我们可能无法获得我们想要的那么多有关阴谋的经验证据，尤其是在短期内。这里的障碍包括：</p><ul><li><p>至少对于成熟的计划，您需要具有态势感知和目标导向性的模型作为先决条件。</p></li><li><p> “模型生物”也许能够人为地诱发阴谋的成分（例如：在故意给予长期、不一致目标的模型中），但这些成分自然出现的频率可能仍然悬而未决。</p></li><li><p>自然产生的阴谋的最佳证据就是在野外<em>抓住</em>一个真正的自然产生的阴谋者。但阴谋者令人恐惧的一个关键原因是，他们故意破坏抓获他们的努力。</p></li></ul><p>因此，特别是在短期内，理论论证可能仍然是对阴谋者担忧的核心理由之一（迄今为止它们一直是关键理由）。与此同时，我们需要做出决定（例如，关于近期优先级，以及出于谨慎考虑而做出什么样的权衡：以阴谋者为中心的威胁模型），这在一定程度上取决于我们的关注程度是——因此，我们认为这些论点有多么有力。</p><p>例如：假设当我们训练大致人类水平的模型时，我们还没有观察和验证任何自然出现的阴谋例子。尽管如此，我们是否应该基于对阴谋的担忧而避免以 X 方式部署我们的模型，或信任 Y 形式的评估？我们对理论论证的评估可能开始对我们的答案产生影响。</p><p>更重要的是，了解我们可能（或可能不会）阴谋的理论原因可以帮助我们找到研究问题并试图防止其发生的可能方法。</p><p>也就是说，我对那些基于我将在下面讨论的各种考虑因素而没有太多更新其观点的人表示同情。特别是，我敏锐地意识到我所采用的概念在很多方面都是不精确的，可能具有误导性，并且不足以捕捉他们试图描述的领域的混乱情况。尽管如此，我发现尝试弄清楚这些概念如何相互作用以及它们意味着什么和不意味着什么是有价值的。希望本报告的读者也能理解。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-cBw6ixqDSsmqKfBfM-1" class="footnote-item"><p>在这里，我只是将守门阴谋者视为阴谋者的一种，而不是比一般阴谋者更可怕或更不可怕。 <a href="#fnref-cBw6ixqDSsmqKfBfM-1" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-2" class="footnote-item"><p>但请注意，根据<a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">特纳 (2022) 的</a>说法，原则上，即使目标“错误指定”，你也可以获得一个一致的系统，前提是模型学习了一个实际上是你的灯光所需要的“错误概括”目标。 <a href="#fnref-cBw6ixqDSsmqKfBfM-2" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-3" class="footnote-item"><p>这种方法与 OpenAI<a href="https://openai.com/blog/our-approach-to-alignment-research">建立自动对齐研究人员的目标</a>密切相关，但范围更广。另请参阅<a href="https://www.alignmentforum.org/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very">Karnofsky (2022)</a>中<a href="https://forum.effectivealtruism.org/posts/sW6RggfddDrcmM6Aw/how-might-we-align-transformative-ai-if-it-s-developed-very">关于</a>“MAGMA 目标”的讨论。 <a href="#fnref-cBw6ixqDSsmqKfBfM-3" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-4" class="footnote-item"><p>但请注意，我们在应对通过此类测试揭示的不良行为倾向时需要谨慎。特别是，如果您只是尝试天真地“训练”相关行为，您可能会：</p><ol><li><p>实际上并没有重现模型所关心的奖励过程的组成部分（例如，如果模型关心其行为被梯度更新所强化，那么如果测试将<em>针对</em>不良行为进行梯度下降）寻求提示，那么模型实际上不会有动力参与该行为），和/或</p></li><li><p>训练模型在向人类揭示其真实目标时更加谨慎和更具策略性——例如，奖励寻求者只有在认为人类以后不会发现这一点时才会尝试直接操纵奖励过程。</p></li></ol> <a href="#fnref-cBw6ixqDSsmqKfBfM-4" class="footnote-backref">↩︎</a></li><li id="fn-cBw6ixqDSsmqKfBfM-5" class="footnote-item"><p>事实上，原则上，人工智能可以决定始终保持一致行动，直到它看到一些它知道目前人类很难/不可能产生的输入。 Christiano ( <a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d">Christiano (2019)</a> ) 的例子是<a href="https://en.wikipedia.org/wiki/RSA_numbers#RSA-2048">RSA-2048</a>的因式分解，这是一个非常大的<a href="https://en.wikipedia.org/wiki/Semiprime">半素数</a>，人类目前似乎距离因式分解还很远。 <a href="#fnref-cBw6ixqDSsmqKfBfM-5" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-6" class="footnote-item"><p>在这里，我搁置了一些关于“人为捕获”的推测动力，这些在第 2 节开头的脚注中讨论过<a href="#fnref-cBw6ixqDSsmqKfBfM-6" class="footnote-backref">。↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-7" class="footnote-item"><p>关于“野心”的更多讨论见下文第 2.3.1.2.7 节。 <a href="#fnref-cBw6ixqDSsmqKfBfM-7" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-8" class="footnote-item"><p>我不认为以这种方式概括的模型应该被解释为“按集奖励的寻求者”，但它们就在附近。 <a href="#fnref-cBw6ixqDSsmqKfBfM-8" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-9" class="footnote-item"><p>这同样适用于那些在寻求奖励的测试中表现一致、试图为不一致的行为提供奖励的阴谋者。 <a href="#fnref-cBw6ixqDSsmqKfBfM-9" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-10" class="footnote-item"><p>这是<a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/#how-big-of-a-risk-is-misalignment">Cotra (2021)</a>的示例。 <a href="#fnref-cBw6ixqDSsmqKfBfM-10" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-11" class="footnote-item"><p>例如，请注意，只要人们将人类进化视为“目标错误概括”的一个例子/类比，具有长期目标的人类——并且具有“情境意识”，即他们了解进化选择如何运作——不倾向于关注涉及最大化其包容性遗传适应性的工具性策略。更多关于为什么不在第 1.5 节中“松弛”的讨论中。 <a href="#fnref-cBw6ixqDSsmqKfBfM-11" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-12" class="footnote-item"><p>这里的进化类比是：具有长期目标的人类仍然乐于使用安全套。 <a href="#fnref-cBw6ixqDSsmqKfBfM-12" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-13" class="footnote-item"><p>这是解读<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization">Soares (2022)</a>中威胁模型的一种方式，尽管该威胁模型也可能包含一定的策划范围。另请参阅这篇关于“<a href="https://arbital.com/p/context_disaster/">背景灾难”的仲裁帖子，</a>其中“危险的转折”只是一个例子。 <a href="#fnref-cBw6ixqDSsmqKfBfM-13" class="footnote-backref">↩︎</a></p></li><li id="fn-cBw6ixqDSsmqKfBfM-14" class="footnote-item"><p>另外，请注意，关于“目标错误概括”的某些担忧并不以同样的方式适用于语言模型代理，因为有关目标的信息非常容易获取<a href="#fnref-cBw6ixqDSsmqKfBfM-14" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/HiuagmXnhTAgDKRBP/why-focus-on-schemers-in-particular-sections-1-3-and-1-4-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HiuagmXnhTAgDKRBP/why-focus-on-schemers-in-pspecial-sections-1-3-and-1-4-of<guid ispermalink="false"> HiuagmXnhTAgDKRBP</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Fri, 24 Nov 2023 19:18:34 GMT</pubDate> </item><item><title><![CDATA[Surviving and Shaping Long-Term Competitions: Lessons from Net Assessment]]></title><description><![CDATA[Published on November 24, 2023 6:18 PM GMT<br/><br/><p>这篇文章探讨了<a href="https://www.hsdl.org/?view&amp;did=460780"><u>净评估</u></a>，这是一个评估战略竞争的框架，该框架的发展为冷战期间美国的国防政策提供了信息。我们解释了什么是净评估、它的方法和原则，以及如何应用它的一些工具来推理高度不确定、具有潜在生存风险的长期技术竞争。</p><p></p><h2><strong>什么是净评估？</strong></h2><p> 20世纪50年代末，冷战进入了一个特别危险的时期。核库存不断增加，运载能力不断增强，而苏联数十年的积累对美国的常规军事主导地位构成了挑战。国防分析家需要重新构建他们看待军事竞争的方式：美国无法用蛮力压倒苏联战争机器，而核战争的前景既增加了冲突的风险，也需要新的衡量标准和战略原则参与有限的竞争。正是在这种背景下，“净评估”框架开始发展。</p><p>安德鲁·马歇尔 (Andrew Marshall) 创立并领导了国防部净评估办公室四十二年，他对净评估的描述如下：</p><blockquote><p> <i>“我们对净评估的概念是，它是对美国武器系统、部队和政策与其他国家的武器系统、部队和政策的仔细比较。它是全面的，包括对部队的描述、作战理论和实践、训练制度、后勤保障。” 、各种环境中已知或推测的有效性、设计实践及其对设备成本、性能和采购实践的影响及其对成本和交付周期的影响。净评估的使用旨在进行诊断。它将突出显示效率和低效率我们和其他人做事的方式，以及相对于我们的竞争对手的比较优势领域。”</i></p></blockquote><p>从最初的军事背景出发，网络评估的核心思想是在竞争对手之间进行全面（因此名称中的“网”）、客观和信息丰富的比较：无论是国家、公司还是其他政治或意识形态联盟。从这些比较中得出的见解可以为更可持续的战略提供信息，并有助于在高度重要的技术和政策主题上建立有益的竞争动态，而不是灾难性的竞争动态。</p><p>此类分析往往将关键考虑因素归零。在一个特别突出的例子中，马歇尔本人很早就表现出了对合成生物风险等威胁的担忧：2010年为生物安全卷写了<a href="https://www.hsdl.org/?view&amp;did=24341"><u>前言</u></a>，并<a href="https://www.esd.whs.mil/Portals/54/Documents/FOID/Reading%20Room/Administration_and_Management/OSD_Net_Assessment_Log.pdf?ver=2017-05-15-140045-830"><u>委托进行多项生物技术威胁分析，</u></a>将生物恐怖主义、生物防御和生物技术视为战略盲点。</p><p>总体而言，净评估并不是一个明确定义的单一程序。本文试图描述和研究净评估的一些基本原则和工具、它们的成功以及它们对军事竞争以外具有重大长期道德重要性的其他领域的适用性。</p><p></p><h2><strong>净评估的方法和原则</strong></h2><p>净评估最常见的工具是情景、<a href="https://en.wikipedia.org/wiki/Wargame"><u>兵棋推演</u></a>、趋势分析和深思熟虑的判断。情景和兵棋推演迫使分析师接近所考虑的动态。准确地了解竞争对手的想法和他们可以选择的选项可以创造新的见解，并实现一种远程<a href="https://en.wikipedia.org/wiki/Participant_observation"><u>参与者观察</u></a>。趋势分析将从兵棋推演中获得的<a href="https://forum.effectivealtruism.org/topics/inside-vs-outside-view"><u>内部观点</u></a>知识与可衡量的外部观点基本比率结合起来，并且这些方法的集成可以实现高质量的深思熟虑的判断。这对于评估新情况特别有帮助，在这种情况下，新技术、理论和战略参与者可能会产生与先前基本利率所暗示的结果截然不同的结果。</p><p>净评估倾向于混合定性和定量分析方法。净评估办公室专注于简单的趋势，并探索兵棋推演的细微差别：他们不倾向于建立极其复杂的模型（或者至少他们自己不这样做）。定量优化可以创造奇迹，但被忽视的问题通常涉及以前从未发生过的想法和思维方式，并且缺乏数字或庞大的数据集。正如赢得战争通常需要考虑士气等模糊概念，而不仅仅是优化预计的死亡死亡率一样，促进长期繁荣需要的不仅仅是优化每美元明显挽救的生命。</p><p>以下原则是应用上述想法的关键：</p><p></p><h3> <i><strong><u>1.寻找竞争对手之间的不对称性</u></strong><u>。</u></i></h3><p>不同的竞争对手将有不同的优势和劣势、效率和低效率、战略和目标、原则、政策和计划。净评估将对这些差异的分析融入到对各方力量平衡观点的连贯理解中，从而能够更好地预测他们的行为，包括更简单的模型可能会忽略的特殊和非理性盲点。</p><p>寻找官僚、文化和政治因素中出现的非理性和盲点是网络评估提供战略价值的核心方式之一。对苏联国防决策的早期预测常常隐含地假设单一理性行为者模型，而这些模型并不能很好地描述实际官僚机构的运作方式。当被问及对冷战初期苏联轰炸机基地的预测时，兰德公司分析师认为，苏联将沿着西伯利亚大铁路沿线重新安置位于该国内陆深处的空军基地，以利用不断增加的轰炸机航程，确保后勤通道，并使目标定位更加困难。美国轰炸机。 <span class="footnote-reference" role="doc-noteref" id="fnref65f49gno7kt"><sup><a href="#fn65f49gno7kt">[1]</a></sup></span>如果分析人士考虑了官僚惯性，甚至考虑了美国部署自己轰炸机的方式，他们可能会认为，苏联会在其发展更长的时间之前，沿着其在周边修建的脆弱跑道部署轰炸机。航程飞机。</p><p>找到对手的结构性不合理性、其<a href="https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem"><u>委托代理问题</u></a>以及可预测行为领域可以为竞争和合作策略带来巨大回报。官僚机构和大型组织加强结构并执行例行公事；默认情况下，它们不会本能地采取最佳行为：对于功能获得性研究实​​验室和工业肉类生产商来说，就像苏联的情况一样。利用这些非理性可以给对手带来非强制性成本（例如，在冷战的情况下，在不相关、威胁较小的领域增加军费开支），或者节省自己的资源，完全避免不必要的竞争领域。与此同时，了解各方的政治制约因素也有助于限制军备竞赛动态的“最坏情况”思维，增强预测，并通过使预先承诺更加可信来实现更多的合作战略。</p><p>一般来说，考虑组织内部协调的困难和委托代理问题是净评估的一个关键部分。例如，研究人员对人工智能组织之间的竞争进行净评估可能会提出以下问题：美国和国外主要人工智能组织的团队的内部激励是什么？他们如何做出决定，什么因素影响他们，为什么？他们认为自己的竞争优势是什么？每个组织的主要机关和联盟是什么，它们的动机是什么，每个组织的影响力有多大？</p><p>这种推理的例子——围绕组织的内部动态以及组织内部的代理人如何实现相互冲突的目标——也可以在<a href="https://en.wikipedia.org/wiki/Selectorate_theory">选择者理论</a>和<a href="https://en.wikipedia.org/wiki/Public_choice"><u>公共选择理论</u></a>风格的关于激励的推理中找到。<br></p><h3> <i><strong><u>2. 关注可能出现的情况的结果，考虑所有相互作用的力量</u></strong><u>。</u></i></h3><p>比较绝对数字是有欺骗性的：如果一个国家拥有很多坦克，但没有能力修理它们，那么它就会输给一个库存较低、坦克可以在战场上停留更长时间的国家——这就是以色列以战胜对手的方式震惊世界的原因。 1973 年，乌克兰组建了一个大型联军。最近，在俄罗斯规模大得多的军队入侵期间，士气、后勤、通讯和人力资本方面的差异都为乌克兰带来了优势。敏捷性、通信、物流和维护可以战胜规模，但这些因素可能很难快速或准确地纳入定量模型和模拟中。不稳定的联盟和多极冲突也可能大大增加评估特定冲突的规模和协调性的复杂性。当一个单独的弱国受到超级大国的高度决定和支持时，其表现可能会远远超出其实力。网络评估的定性方法试图考虑这些因素，它们可能对分析人工智能领域很有用，而不仅仅是确定谁在计算上花费最多或谁拥有最大的模型。危机模拟、战争游戏以及从应用历史中寻找类比和反类比可以帮助形成现实中事情将如何发展的概念。</p><p></p><h3> <i><strong><u>3. 注重客观、长期的诊断，而不是规定性的、近期的建议</u></strong><u>。</u></i></h3><p>网络评估旨在提高决策者对世界的理解并评估问题和机遇。净评估不是不断地将注意力转向短期政策目标，而是旨在通过整体观点和描述竞争的长期动态来提供被忽视的价值。这种严格的长期趋势分析在许多事业领域也可能很有价值，从全球减贫到技术政策。如下所述，净摊款对结束冷战的最大贡献来自于分析苏联军事支出的和平时期模式，而不是对任何特定危机做出快速反应。展望未来几十年，净评估等方法可以为战略提供指导，从而战胜只关注短期的官僚机构、避免冲突并将资源引导到未来的优势。</p><p>这并不意味着分析和快速行动的能力没有价值，只是当大型组织的激励被短期反馈循环捕获时，长期较慢的分析具有独特的价值。对于人工智能等发展速度极快的问题，想出更快的方法来生成“考虑到所有因素”的观点对于生成合理的战略建议可能非常重要。 <span class="footnote-reference" role="doc-noteref" id="fnrefywzfgck6arh"><sup><a href="#fnywzfgck6arh">[2]</a></sup></span></p><p></p><h3> <i><strong><u>4. 在尚未考虑清楚的问题中寻找关键考虑因素。</u></strong></i></h3><p>在提出问题时，必须确保你专注于重要的事情，而不是仅仅遵循以前做事方式的惯性。 “占领的领土”等指标与思考像核战争这样的存在主义冲突并不那么相关，而这是发展净评估的关键推动力。</p><p>如果某件事已经被量化和建模，就会有人关注它，这有时意味着边际智力努力可能不那么有价值。净评估等方法通常将注意力集中在定性或非量化研究领域中容易实现的成果上。在冷战时期令人难忘的一幕中，当怀疑论者质疑赫尔曼·卡恩声称自己已成为“结束核战争的世界领先专家”时， <a href="https://press.armywarcollege.edu/cgi/viewcontent.cgi?article=2285&amp;context=parameters#page=7"><u>他回答说</u></a>：“上周我让两名初级人员参与了几天。我们对这个问题的思考比整个国防部都多。”</p><p>虽然幸运的是，我们从未测试过这种结束核战争的“专业知识”，但一般来说，当一个问题仅从有限的几个角度引起关注时，新方法可能具有巨大的价值。许多准备和分析的失败是想象力的失败，而不是计算的失败。有时少量的想法可以增加很多价值，并且通过<a href="https://www.goodreads.com/en/book/show/41795733"><u>花时间思考更多的事情，</u></a>你更有可能遇到可以增加很多价值的小想法。</p><p>解决<a href="https://forum.effectivealtruism.org/posts/xwhWgA3KLRHfrqdqZ/the-wicked-problem-experience"><u>棘手的</u></a>、尚未分解的问题，需要集中思想、灵活方法。安德鲁·马歇尔说，“可以用来进行净评估的最有生产力的资源是持续的艰苦智力努力”（另见：<a href="http://wiki.c2.com/?FeynmanAlgorithm"><u>费曼算法</u></a>）。<br></p><h3> <i><strong><u>5. 标准化数据收集以建立长期趋势和比较</u></strong><u>。</u></i></h3><p>如果没有标准化的全景数据，就很难公平或准确地比较存在许多差异的竞争对手。通常有必要询问哪些支出与衡量相关，以及如何调整定性因素的比较或官僚机构组织和描述其努力的清晰程度的差异。由于净评估最好由能够访问尽可能多的相关数据的分析师来完成，因此访问非公开数据通常至关重要，因此必须确保分析的安全，以避免为竞争对手提供优势。虽然在对抗性较小的环境中保密的意义不大，但寻求全面相关数据的重要性仍然存在。编制<a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf#page=29"><u>人工智能进展指标</u></a>（包括可解释性和能力）、国家之间的政策差异、<a href="https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents"><u>生物安全失败</u></a>以及各种其他措施都可以为降低风险提供信息。与上述长期思考的原则一致，时间序列数据理想地应该覆盖较长的时间尺度。</p><p>一些作者研究了人工智能的趋势，但在相同的背景下，更上游或与功能无关的趋势可能没有得到很好的研究。从更明显的方面来看，人工智能芯片生产的趋势是什么？各个参与者将如何应对可能的变化？哪些类型的人工智能支出实际上涉及什么？该支出子集的趋势是什么？主要参与者将如何应对其他技术的不同进步？与机器学习最相关的人才库以什么速度增长，谁拥有吸引和评估人才的最佳渠道？领先企业和国家的行为是否存在任何令人担忧的趋势？不同潜在监管来源的影响趋势是什么？</p><p></p><h3> <i><strong><u>6. 简单建模，复杂思考</u></strong><u>。</u></i></h3><p>如上所述，净评估立足于强劲的趋势；这种做法有助于分析师平衡<a href="https://en.wikipedia.org/wiki/Anchoring_(cognitive_bias)"><u>锚定效应</u></a>和<a href="https://en.wikipedia.org/wiki/Representativeness_heuristic"><u>代表性启发法</u></a>的心理偏差，并实现更准确的估计。在汇集了有关组织及其机构如何运作、世界可能如何变化、它可能如何变化以及如何衡量重要变量的经验知识后，净评估的最后一步是使用兵棋推演并考虑到每个人的理解位置和习惯来预测响应不同发展的行为。组织可能如何应对某些立法或政策？他们如何应对公众日益增加的担忧？虽然任何个别情况都可能不太可能发生，但对大量情况进行分析可能会揭示出在各种潜在未来中表现良好的非显而易见的策略，并强调哪些不确定性最重要。场景构建的复杂性可能会使网络评估难以审查，因此该过程受益于具有不同领域专业知识的高质量专家。在军事领域：商业分析师、经济学家、工程师、地区专家、历史学家、社会科学家、军事人员和实业家都拥有有用的知识。当尝试建立一个团队进行净评估时，该团队将产生被忽视的见解，<i>寻找具有明显相关经验和专业知识的人，而这些经验和专业知识很少应用于研究领域</i>。</p><p></p><h3> <i><strong><u>7. 评估完全避免、限制和重塑竞争的可能性。</u></strong></i></h3><p>净评估通常是在制定竞争战略的背景下考虑的，这些战略将自己的持久优势与竞争对手的持久弱点进行对比，但如果可以避免代价高昂的竞争，那么这样做就没有意义。净评估等方法的使用是诊断性的，而不是本质上的零和：它们可以用来为合作和竞争提供信息，并培养更加基于事实的战略同理心。网络评估可以帮助识别趋势、组织怪癖、文化价值观和思维模式，从而使社会化策略能够通过缓和、重新结盟或友谊来结束竞争。</p><p>在采取竞争策略之前，人们必须问是否可以与潜在对手成为朋友或结盟。如果双方能够建立正当的信任基础，化解利益冲突，建立合作共赢的动力，那肯定比打热战、冷战，甚至政治竞争要好。当竞争对手的领导或行为方式发生变化时，可能会出现建立信任和军备控制的新机会，正如戈尔巴乔夫的崛起所发生的那样。</p><p>同样，在对抗性竞争中，诊断合作和社交策略对正和和竞争目标都有帮助的领域仍然有用。 <span class="footnote-reference" role="doc-noteref" id="fnrefflhg583ze3"><sup><a href="#fnflhg583ze3">[3]</a></sup></span>二战后，美国和苏联并没有直接开战：这种最低限度的合作（受到威慑和国内对战争的厌恶推动）反映了对竞争的互惠互利的限制。在和平时期竞争的范围内，军控方面的进一步合作同样可以带来互惠互利：减轻军事竞争的负担。与此同时，军备控制也可以以有利的方式重塑竞争：美国和苏联核力量削减协议不仅减少了风险和敌意，而且可以说还可能将军事竞争转向有利于双方的质量方向。美国。与此同时，为获得竞争优势而追求军备控制<i>往往会导致军备控制根本无法实现</i>，而且某一层面的优势可能会被其他层面的劣势所抵消。新的《削减战略武器条约》并没有限制苏联的战术核武器，俄罗斯剩余武库的<i>破坏力</i><a href="https://www.johnstonsarchive.net/nuclear/nwhmt.gif"><i><u>很可能仍然超过世界其他国家的总和</u></i></a>。最后，社会化战略不仅可以通过建立自己的联盟​​来获得竞争和稳定的好处，还可以通过对竞争对手内部和之间的分歧做出反应来获得：例如尼克松开放中国和阻止苏联对中国进行核攻击的努力。与社会化战略相反，竞争战略的不太友好的变体在各方利益本质上存在分歧且另一方本质上决心进行斗争的情况下开始有意义。</p><p>即使面对坚定不移的竞争对手，量身定制的竞争策略也并不总是最好的回应。如果对方是适应性理性的、灵活的且难以预测的，那么模仿对方的官僚机构和决策文化的回报可能是短暂的。在这种情况下，最好关注敏捷性并快速调整以应对未来事件。如果一个企业太弱而无法直接瞄准无法统一的竞争对手的弱点，那么最好的办法可能是“隐藏和等待”：通过比其他竞争对手更好地利用现有趋势来避免冲突并寄望于未来。</p><p>总体而言，量身定制的竞争策略最适合那些无法结盟的对手，这些对手拥有持久的、可预测的弱点，而你可以利用这些弱点来匹配自己联盟的优势。 <span class="footnote-reference" role="doc-noteref" id="fnref7zglykim913"><sup><a href="#fn7zglykim913">[4]</a></sup></span>作为一个意志坚定、不结盟的对手，具有领土野心和可利用的结构性非理性，苏联是美国竞争战略的一个良好目标。</p><p></p><h2><strong>净摊款的历史用例：黑客入侵苏联支出</strong></h2><p>从历史上看，净评估最直接的应用是发现能够在长期竞争中取得胜利的洞察力。例如，净评估办公室在冷战期间最有价值的见解之一是发现苏联在应对美国军事开支方面存在严重低效问题。作为一个例子，分析人士指出，利用上述原则，苏联国防官僚机构不合理地执着于建造单一用途、一次性地对空导弹系统，以应对美国远程轰炸机的改进，这些轰炸机虽然单独昂贵，但价格低廉。数量少，用途多。</p><p>专注于防空系统并大规模部署它们来保卫苏联的广大外围是不合理的，因为无论有什么防空系统，一旦发生核战争，洲际弹道导弹已经使整个苏联变得脆弱。 <span class="footnote-reference" role="doc-noteref" id="fnref15t1z6ok6zq"><sup><a href="#fn15t1z6ok6zq">[5]</a></sup></span>这一观察结果使美国仅通过建造更有用的远程轰炸机来引起苏联人的大量浪费。虽然孤立地，额外的防空可能并不是一个极大的负担，但这远非巨大的苏联国防支出的唯一领域。中央情报局和五角大楼有一次估计，苏联将其GDP的1至2％用于保护他们在战争中的领导地位：有可能被瞄准和销毁的掩体，以低得多的成本对美国<span class="footnote-reference" role="doc-noteref" id="fnrefq8q85iyr9id"><sup><a href="#fnq8q85iyr9id">[6]</a></sup></span>从升级的角度来看，非常有风险，并且不太可能正如最初提出的那样起作用，里根总统的“<a href="https://en.wikipedia.org/wiki/Strategic_Defense_Initiative"><u>星球大战</u></a>”导弹防御计划同样可以以类似的方式观看：它利用了最近清除美国国防工业的苏联间谍，并可能不成比例地产生浪费的苏联支出是通过担心美国可能会迅速取得巨大进步而无法通过间谍所偷走的。总体而言，苏联的总防御负担消耗了大约四分之一的苏联GNP，因此，成本强加的策略使苏联战争机器和苏联的可持续性降低。 <span class="footnote-reference" role="doc-noteref" id="fnrefpozntxbdgzi"><sup><a href="#fnpozntxbdgzi">[7]</a></sup></span>其他，较短的分析流派未能注意到苏联官僚机构中的这些小故障，因为他们没有考虑几十年来和官僚激励措施的冲突。</p><p>退后一步并设想更长的时间范围，用来攻击苏联国防支出的竞争策略并不是明确的胜利。成本强加的策略可以迫使经济成本对无辜者，并鼓励危险的武器竞赛或高风险的姿势。尽管成本启动帮助更快地结束了冷战，并将苏联集中在更固有的防御性武器上，但对苏联领导人担心出人意料攻击的努力可能会增加意外或无意义的核战争的可能性。 <span class="footnote-reference" role="doc-noteref" id="fnrefo2zxjkd7l4b"><sup><a href="#fno2zxjkd7l4b">[8]</a></sup></span>苏联的崩溃也给前苏联公民带来了预期寿命的崩溃，并冒着松散的核武器或生物武器的风险。如果不是针对<a href="https://en.wikipedia.org/wiki/Nunn%E2%80%93Lugar_Cooperative_Threat_Reduction"><u>Nunn-Lugar合作社减少威胁计划</u></a>，那么许多大规模杀伤性武器可能仍然没有受到保护，而俄罗斯的阿森纳今天可能会更大。随着俄罗斯对乌克兰和核威胁的现代入侵，在某个时间点赢得了技术经济竞争并不能保证持久的生存安全。俄罗斯的政府并没有与西方保持一致，尽管其军事，核武库和经济远不及以往，但其剩余的核力量仍然是潜在的巨大威胁。</p><p></p><h2><strong>总之：那呢？</strong></h2><p>在某些方面，操作分析和净评估之间的差异与Givewell的分析与<a href="https://www.openphilanthropy.org/research/hits-based-giving/"><u>基于HITS</u></a>的Give之间的差异相似：寻找可能导致违反直觉策略的关键考虑因素。随着慈善家和善良者将目光投向了更艰难，更无定的问题，这些问题违反了易于量化，净评估中使用的某些方法和思想工具可能对问题框架有用。</p><p>军事武器竞赛和<a href="https://forum.effectivealtruism.org/topics/ai-risk"><u>AI功能</u></a>和<a href="https://forum.effectivealtruism.org/topics/biosecurity"><u>生物武器</u></a>技术的比赛是某些净评估工具最明显的用例。竞争对手是否有合作降低风险的机会？在最有可能发展危险的AI系统或生物武器的组织类型之间是否有可剥削的非理性？您是否可以在没有触发军备竞赛的情况下可靠地采取竞争行动？如何触发竞争对手采取稳定行动的方法，例如在安全性上花费更多或浪费注意力和资源降低风险活动？净评估也可以在X风险区域以外有用。例如，可以采用类似的方法来分析动物福利倡导者与工业动物农业公司之间政策竞争的动态。</p><p>从根本上讲，净评估揭示了理想化的竞争模型常常掩盖的机会，其某些相关的思想工具和风格在许多原因中都可以有用。放大不同的组织如何实际运作可以揭示结构性的惯性和非理性，这可以使竞争截然不同。尽管可以滥用这些见解以<a href="https://nuclearnetwork.csis.org/countering-competitive-risk-compensation-principles-for-reducing-nuclear-risk-on-net/">加剧利用优势的风险</a>，但它们也可以使策略能够摆脱灾难性的竞争平衡，以使竞争力与安全保持一致。<br></p><p></p><p></p><p>结尾注：</p><ul><li>本文中表达的观点是作者的观点，而不是雇主。</li><li>特别感谢Darius Meisner，Nick Gabrieli和Kit Harris对本文的反馈。</li></ul><p>脚注： </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn65f49gno7kt"> <span class="footnote-back-link"><sup><strong><a href="#fnref65f49gno7kt">^</a></strong></sup></span><div class="footnote-content"><p>请参阅“最后战士：安德鲁·马歇尔（Andrew Marshall）和现代美国国防战略的塑造”中的第44-45页</p></div></li><li class="footnote-item" role="doc-endnote" id="fnywzfgck6arh"><span class="footnote-back-link"><sup><strong><a href="#fnrefywzfgck6arh">^</a></strong></sup></span><div class="footnote-content"><p>同时，较近的分析师转向近期的规定建议，他们可能越接近政治竞争以及政治化分析的相关风险。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnflhg583ze3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefflhg583ze3">^</a></strong></sup></span><div class="footnote-content"><p>请注意，将它们融合可能是有风险的，并侵蚀了实现积极和双赢妥协的能力。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7zglykim913"> <span class="footnote-back-link"><sup><strong><a href="#fnref7zglykim913">^</a></strong></sup></span><div class="footnote-content"><p>请参阅第2章： <a href="https://www.amazon.com/Competitive-Strategies-21st-Century-Practice/dp/0804782423">21世纪的竞争策略：理论，历史和实践</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn15t1z6ok6zq"><span class="footnote-back-link"><sup><strong><a href="#fnref15t1z6ok6zq">^</a></strong></sup></span><div class="footnote-content"><p>在行使这种推理时，重要的是要考虑官僚偏见的不同潜在原因，以及它们实际上是“不合理的”。苏联反应过度反应的动力可能是对美国在1950年代部署的大量轰炸机的回应，<a href="https://www.johnstonsarchive.net/nuclear/nwhmt.gif">随之而来的大量武器库</a>，以及像柯蒂斯·莱梅（Curtis Lemay）这样的先前将军对先发制人袭击的同情。苏联官僚也有可能认为防空导弹成本将足以下降，以至于更经济。偏向空气武器的另一个偏见可能是对战斗机飞行员自主权缺乏信任：专制制度的成本不成比例。如果出于某种原因，苏联战争计划者认为他们可以在地面上撤出我们的洲际弹道导弹，而不是向炸弹袭击者提醒，那么这也会对防空防御产生偏见，因此，轰炸机策略将是一个很好的反击措施。无论如何，大型国家资助的行业，即使他们的努力不再适应性，许多演员都将有动机确保其资金安全。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq8q85iyr9id"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq8q85iyr9id">^</a></strong></sup></span><div class="footnote-content"><p>参见<a href="https://www.google.com/books/edition/The_Twilight_Struggle/4b1VEAAAQBAJ?hl=en&amp;gbpv=1&amp;bsq=GDP"><u>Hal Brand的</u><i><u>《暮光之城斗争》</u></i></a>中的第68页</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpozntxbdgzi"><span class="footnote-back-link"><sup><strong><a href="#fnrefpozntxbdgzi">^</a></strong></sup></span><div class="footnote-content"><p>马歇尔（Marshall）的一个著名备忘录是“苏联GNP和军事负担的估计”，《国防部长通过国防部长》（ISA）（ISA）的备忘录，1988年8月2日。最后的战士：安德鲁·马歇尔（Andrew Marshall）和现代美国国防战略的塑造”</p></div></li><li class="footnote-item" role="doc-endnote" id="fno2zxjkd7l4b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo2zxjkd7l4b">^</a></strong></sup></span><div class="footnote-content"><p>在“<a href="https://www.amazon.com/Dead-Hand-Untold-Dangerous-Legacy/dp/0307387844">死手</a>”一书中，有许多例子说明苏联情报如何变得偏执，并投入了大量的分析努力来调查西方首次罢工意图的潜在迹象。同时，围绕Abl Archer 83等事件的一些叙述<a href="https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/21419/Miles_Able_Archer_83_JCWS.pdf?sequence=2&amp;isAllowed=y">可能会被严重夸大</a>。由于存在风险的前景可以激发极端行动，因此许多战略参与者通常会激励进行超越或低估此类威胁的影响力。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/ozqyLS8WpfeaeWbWY/surviving-and-shaping-long-term-competitions-lessons-from#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ozqyls8wpfeewbwy/surviving-and-anping-long-term-competitions-from<guid ispermalink="false"> ozqyls8wpfeaewbwy</guid><dc:creator><![CDATA[Gentzel]]></dc:creator><pubDate> Fri, 24 Nov 2023 18:18:42 GMT</pubDate> </item><item><title><![CDATA[Ability to solve long-horizon tasks correlates with wanting things in the behaviorist sense]]></title><description><![CDATA[Published on November 24, 2023 5:37 PM GMT<br/><br/><p><i>状态：含糊，对不起。对我来说，这一点似乎几乎是重言式学，但似乎也是人们对周围的人的正确答案，说：“ LLMS原来不是很想要，什么时候期望“代理商”更新？”，所以， 我们到了。</i></p><p></p><p>好的，所以您知道当今的AI肯定不是很好...让我们说“长途”任务吗？像新颖的大型工程项目一样，还是写了很多预言的长书系列？</p><p> （Modulo，它可以很好地下棋，这比某些事物更长；这种区别是定量的，而不是定性的，并且正在被侵蚀等）。</p><p>而且您知道AI似乎没有那么多“想要”或“欲望”的行为？</p><p> （Modulo，例如，它可以很好地下棋，这表明行为主义者的某种类型的欲望行为。游戏板进入您检查的状态，好像它具有内部检查的“目标”。这再次是被侵蚀的定量差距。）</p><p>好吧，我声称这些或不再是相同的事实。毫不奇怪的是，AI陷入了各种长途任务<i>，并且</i>似乎并没有像拥有“想要/欲望”那样被固定得很好。这些是同一枚硬币的两个方面。</p><p>相关：想象一下AI开始在那些长期任务中成功完成，而没有想象它开始拥有更多的需求/欲望（在下面的“行为主义意义”中）是，我声称是想象一个矛盾，或者至少是一个矛盾极端惊喜。因为在一个不断将扳手投入计划的大型，毫无观察，令人惊讶的世界中实现长跑目标的方法可能会成为一种强大的通才扳手，无论哪种扳手陷入什么扳手，都可以固执地重新定向某些特定的目标它的计划。</p><p></p><p>这种可观察到的“无论是在行为主义意义上“在行为主义意义上”将AI描述为“想要/欲望”时，我的意思是我的意思是我的意思。</p><p>我对人工智能的内部状态以及那些人是否与因欲望的感觉所消耗的人的内部状态有任何相似之处。用Eliezer Yudkowsky在某处说的话：我们不会说搅拌机“想要”融合苹果。但是，如果搅拌机以某种方式设法吐出橙子，爬到储藏室，装满苹果并将自己插入出口中，那么我们确实可能想开始谈论它，就像它有目标一样，即使我们&#39;&#39;&#39; t试图对导致这种行为的内部机制提出强烈的主张。</p><p>如果AI在各种开始设置中引起了一些特殊的结果，尽管有各种各样的障碍，那么我会说“在行为主义意义上”“想要”这种结果。</p><p></p><p>为什么我们会看到这种“想要的”与解决长马问题和执行长匹马任务的能力同时出现了这种“想要”？</p><p>因为这些“长途”任务涉及将复杂的现实世界操纵到特殊的棘手结果中，尽管它在此过程中遇到的任何令人惊讶和未知的障碍和障碍都没有。在这样的问题上取得成功似乎很可能涉及弄清世界是什么，弄清楚如何导航，并弄清楚如何避免障碍，然后在某个稳定的方向上重新定位。</p><p> （如果每个新的障碍都会使您朝某些不同的目标徘徊，那么您将无法可靠地击中您对目标的目标。）</p><p>如果您是那种巧妙地生成和制定长期计划的事情，<i>那么</i>您就是那种坚持其枪支的计划者，并找到了一种在现实世界中取得许多障碍的成功的方法（与其放弃或徘徊，每次出现新的闪亮事物时都会追逐一些新的闪亮事物），然后我以我对这些事情的看法，而是很难想象您<i>不</i>包含一些相当强大的优化战略性地将世界引导到特定国家。</p><p> （的确，这种联系对我来说几乎是重言式的，因此谈论这些联系是AI的独特属性，这是奇怪的。 AI可以部分地以目标为导向，而不会最大程度地以目标为导向。但是，在面对意外的障碍/机遇的情况下，AI的表现越多地取决于其制定长期计划和修改这些计划的能力，它将越来越一致地倾向于将与之互动的事物引导到特定状态，至少在它起作用的情况下。）</p><p>继续重新定位某些目标的能力似乎是导航一个庞大而复杂的世界以实现困难结果的难题。</p><p>这种直觉得到了人类的案例的支持：人类遇到欲望，欲望和目标是毫无疑问的，他们继续寻找聪明的新方法来追求，即使现实向他们投掷了各种曲线球，例如“那种猎物动物已被追捕为灭绝”。</p><p>这些想要，欲望和目标并不是神的灵魂的某种行为。这不是奇怪的偶然性。尽管障碍物遇到了障碍，但拥有“吃一顿美餐”或“打动您的朋友”之类的目标是能够吃一顿美餐或打动您的朋友的基本基本。因此，在我们的情况下，进化偶然发现了这种方法也就不足为奇了。</p><p> （人类大脑中的实施细节 -  eg，我们情感化妆的细节 - 对我来说似乎是有些细节，在具有行为主义者“欲望”的AI中不会复发。目标，即使您遇到障碍也要继续定位它。“事情似乎很中心。）</p><hr><p>上面的文本隐约地认为，在艰难的长胜压问题上做得很好，需要在面对各种各样的现实世界障碍时追求抽象的目标，这涉及做一些从外面看起来像“想要的东西”的事情。现在，我将提出第二个主张（在这里受到更少的论点支持）：追求特定培训目标X所需的类似欲望的行为，不需要涉及AI特别想要X。</p><p>例如，人类发现自己想要诸如美食和温暖的夜晚和欣赏他们的朋友之类的东西。所有这些想要<i>在祖传环境中</i>加起来具有高度包容性遗传健康。外星人可能从外面观察到早期的人类，可能会说，人类“好像他们想最大化其包容性的遗传健康”。当人类转身并发明避孕措施时，揭示了他们从未<i>真正</i>将环境朝向该目标，而是在进化适应性的环境中拥有一系列与包容性遗传适应性<i>相关的</i>杂乱无章的目标。祖先能力水平。</p><p>也就是说，我的理论说：“ AIS必须坚定地追求<i>一些</i>目标以在长途任务上表现良好”，但它<i>并不是</i>说这些目标必须是对AI进行训练的目标（或要求使用的目标） ）。确实，我认为实际行为主义者目标不太可能是程序员想要的确切目标，而不是（例如）纠结的相关性网络。</p><hr><p>从上述点开始的推断是：当AI离开培训时，它的任务是解决更大，更艰难的长马问题，而在必须比以往任何时候都更聪明并开发新工具来解决新问题和解决新问题，并且您最终意识到，它既不追求您训练它的目标，也没有追求您要求它追求的目标 - 嗯，到那时，您已经建立了广义的障碍物。您已经建立了一件事，这些东西何时在计划中抛出扳手，了解扳手，拆除扳手或寻找其他方法来制定其计划时，可以注意到扳手。</p><p>当您抗议并试图将其关闭时 - 好吧，那只是另一个障碍，您只是另一个扳手。</p><p>因此，也许<a href="https://twitter.com/So8res/status/1715380167911067878"><u>还不要使这些广义的扳手恢复</u></a>，直到我们知道如何在其中加载适当的目标。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/awozbzxdm4doggisj/ability-temability-to-solve-solve-long-horizo​​n-tasks-correlates-withwanting<guid ispermalink="false"> awozbzxdm4doggisj</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Fri, 24 Nov 2023 17:37:43 GMT</pubDate> </item><item><title><![CDATA[The Limitations of GPT-4]]></title><description><![CDATA[Published on November 24, 2023 3:30 PM GMT<br/><br/><p><i>在关于Openai新突破的谣言中，我认为我最好在被现实完全超过现实之前发布该草案。从本质上讲，这是GPT4和人类思想之间的“差距”集合。不幸的是，关于Q*的谣言迫使我将结论从“非常短的时间表似乎不太可能”更改为“ F ** K知道谁”。</i></p><p>尽管GPT4具有超人人类的知识，写作速度和短期记忆，但与人类思想相比，GPT4具有许多重要的局限性。</p><p>其中一些将在不久的将来克服，因为它们取决于工程和培训数据选择。其他人似乎对我来说更为根本，因为它们是由于模型架构和培训设置所致。</p><p>这些基本的局限性是我不希望扩大GPT进一步导致AGI的原因。实际上，我将确切的当前范式的进一步缩放视为克服这些局限性的证据。</p><p>我希望将GPT4扩展能够表现出相同的优势和劣势，并以表面上的方式最多可以提高对旧弱点的纸张。</p><p>我也希望通过进一步扩展GPT的任务无法越来越多地在基本限制上加载，从而减少回报。</p><p>此列表并不详尽，也可能有一些方法可以构成我以更有见地或富有成果的方式识别的局限性。例如，我不确定如何解释GPT4的好奇无力理解幽默。我希望在评论中提到进一步的限制。</p><p><strong>感官的整合</strong></p><p>GPT4无法真正听到，它不能真正说话。</p><p>语音输入通过单独的模型“耳语”转录为文本，然后将其馈送到GPT4。输出由另一个模型读取。这个过程失去了输入的细微差别 - 引起，重点，情感，口音等。同样，输出也不能由GPT4根据速度，节奏，旋律，旋律，唱歌，重点，口音或拟声词来调节。</p><p>实际上，由于令牌化，可以说GPT4也无法真正阅读。必须在训练过程中推断出有关与拼写，押韵，发音有关的所有信息。</p><p>大多数开放多模式模型的视觉组成部分通常也“嫁接”。也就是说，它是部分训练的，然后通过大型语言模型（LLM）进行了连接并微调，例如使用剪辑模型，该模型将图像及其描述映射到同一向量空间。</p><p>这意味着GPT4可能无法访问对象或特定细节的确切位置，并且不能像人类那样“仔细观察”。</p><p>我希望这些局限性在很大程度上消失，因为模型被扩展并以各种方式端到端训练。</p><p><strong>系统2思考</strong></p><p>人类不仅要快速，直觉地思考，而且还进行缓慢的反思性思维来处理复杂的问题。 GPT-4的体系结构没有有意义的经常性；它为每个令牌具有有限数量的处理步骤，在顺序思考上刻有硬上限。</p><p>与人类认知的对比是GPT4不可靠的计数能力最为明显的。但这也显示在许多其他任务中。缺乏系统2思考可能是当前大语言模型的最根本局限性。</p><p><strong>在解决问题期间学习</strong></p><p>人类通过思考重新连接大脑；突触是连续形成或分解的。当我们突然理解某些东西时，这种认识通常会持续一生。 GPT4一旦训练，在使用过程中就不会改变。</p><p>它不会从错误中学习，也不会从正确解决的问题中学习。显然缺乏解决问题的优化步骤，这将确保可以解决以前无法解决的问题，并且这种解决问题能力仍然存在。</p><p>这里的根本区别在于，在人类中，在解决问题的过程中解决了给定问题的正确表示形式，然后通常会持续存在 -  GPT4依靠在培训期间学到的表示形式，新问题远远无法解决。</p><p>即使是重新培训也无法解决此问题，因为它需要许多类似的问题及其解决方案才能为GPT4学习必要的表示形式。</p><p><strong>组成性和外推</strong></p><p>一些理论表明，人类新皮层（智力的所在地）都利用其大部分能力来建模对象，零件，概念和子概念的相互作用。这种抽象建模零件相互作用的能力可以更好地外推和从数据大大减少数据中学习。</p><p>相反，GPT-4学习单词之间的统计相互作用。词汇的微小变化可以显着影响其产出。由于缺乏组成性诱导偏见，它需要大量数据才能学习连接。</p><p><strong>由于培训设置的限制</strong></p><p>培训数据中不存在的事情超出了模型的学习能力，包括许多视觉或声学现象，尤其是与世界的身体互动。</p><p> GPT-4对许多世界方面没有物理，机械或直观的理解。这个世界充满了细节，只有当一个人试图执行其中的任务时，才会变得明显。人类从与世界的互动中学习，并在进化上旨在在其中行事。 GPT-4模型数据，除了数据之外，没有什么。</p><p>这导致决策缺乏一致性，坚强追求目标的能力，理解甚至需要改变世界上事物的需求。输入单独立足，不代表现实世界的情况。</p><p> GPT-4的因果知识只是文本中存储的元知识。学习新系统的因果模型将需要与系统的交互并从中进行反馈。由于缺乏反馈，因此很少有针对幻觉的优化压力。</p><p><strong>结论</strong></p><p>其中一些观点可能相互作用，或者将通过同一创新解决。系统2思考对于在寻找问题的解决方案时，可能需要思考概念的一部分。</p><p>培训设置引起的限制可能会通过不同的解决方案解决。但这意味着取消廉价和丰富的数据。除了丰富且信息密集的文本以外，还需要从很少的数据中学习的能力。</p><p>对于我来说，这些问题很难解决。但是我也没有看到解决这些问题的现实方法。每年都会使这些问题更难以解决。</p><p>当我写这篇文章时，很短的时间表似乎不太可能，但是Q*可以想象可以解决“系统2思维”和/或“在解决问题期间学习”和/或“学习”，这可能足以使GPT5超过许多人的“胜任人”的门槛。域。</p><br/><br/><a href="https://www.lesswrong.com/posts/o8eMsxA7uHfybfmhd/the-limitations-of-gpt-4#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/o8emsxa7uhfybfmhd/the-limitations-of-gpt-4<guid ispermalink="false"> o8emsxa7uhfybfmhd</guid><dc:creator><![CDATA[p.b.]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:30:33 GMT</pubDate> </item><item><title><![CDATA[Progress links digest, 2023-11-24: Bottlenecks of aging, Starship launches, and much more]]></title><description><![CDATA[Published on November 24, 2023 3:25 PM GMT<br/><br/><p>我发誓我会每周回到这样做，这样他们就不会太久了。与往常一样，请随时浏览并跳过！</p><h2><strong>进度论坛</strong></h2><ul><li><a href="https://progressforum.org/posts/zdxubhLFATGwRbXhd/a-paradox-at-the-heart-of-american-bureaucracy">美国官僚机构的核心是一个悖论</a>：“注定要过度预算和长期延长的项目的最快方法是使其成为紧迫的公众优先事项”</li><li> <a href="https://progressforum.org/posts/DQweS5hFbj9MX7rR8/why-governments-can-t-be-trusted-to-protect-the-long-run">为什么不信任政府来保护长期未来的未来</a>：“在长期的未来中，没有人能在下次选举中投票。今天，政府中没有人会从现在起50年以后的世界变得更好或失去任何东西，如果他们使世界变得更糟”，那么任何人都将获得任何收益。”</li><li> <a href="https://progressforum.org/posts/XYqCXpP2Hg3Yiwcdh/what-if-we-split-the-us-into-city-states">如果我们将美国分为城市国家怎么办？</a> “在共和国，当他的随行人员要求国家的理想规模时，苏格拉底回答说，&#39;我将允许国家提高到与团结一致的远处；我认为这是适当的限制&#39;”</li><li><a href="https://progressforum.org/posts/uJcGssuGDxvSFCMcZ/the-art-of-medical-progress">医学进步的艺术</a>：“这两幅画提供了希望的对比。尽管我们从痛苦和痛苦开始，但我们转向希望和进步。外科医生以英雄的身份脱颖而出，这是人类胜利征服自然的象征”</li></ul><p> <a href="https://pbs.twimg.com/media/F9tTVPRWEAAC0ei.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/awcljugre1plabw0pexn" alt=""></a></p><h2><strong>进步研究员的根源</strong></h2><ul><li><a href="https://amaranth.foundation/bottlenecks-of-aging">衰老的瓶颈</a>，这是一种“慈善菜单”，“可以有意义地加速老化科学和其他延长生命的技术”。研究员Alex Telford和Raiany Romanni都为此做出了贡献（通过<a href="https://twitter.com/jamesfickel/status/1724503929646403989">@Jamesfickel</a> ）</li><li><a href="https://twitter.com/lauralondon_/status/1726411916917284965">干旱是一种政策选择</a>：“加利福尼亚已经投降了干旱，以气候变化为前提是不可避免的。作为回应，州每年花费数百万的农田。但这对加利福尼亚的驯服干旱土地的历史一无所知”</li><li><a href="https://maximumprogress.substack.com/p/geoengineering-now">现在的地球工程！</a> “太阳能地球工程可以抵消单位数十亿美元的每个人为温度的升高”（ <a href="https://twitter.com/MTabarrok/status/1722259578094817316">@mtabarrok</a> ）</li><li><a href="https://finmoorhouse.com/writing/richard-bruns/">与理查德·布伦斯（Richard Bruns）的室内空气质量对话</a>（以及一些非常可行的改进方法）（ <a href="https://twitter.com/finmoorhouse/status/1724226059501986181">@finmoorhouse</a> ）</li><li><a href="https://www.nytimes.com/2023/11/17/opinion/chips-act-biden-arizona.html">为了成为世界一流的芯片制造商，美国可能需要帮助</a>（NYT）涵盖了（ <a href="https://twitter.com/cojobrien/status/1725613102694007060">@cojobrien</a> ）的最新移民提案。另外，<a href="https://twitter.com/cojobrien/status/1724143576760680615">来自@cojobrien的话题，</a> “我通过该程序写的内容以及其他ROP同事最喜欢的作品”</li></ul><h2><strong>机会</strong></h2><h3><strong>工作机会</strong></h3><ul><li><a href="https://twitter.com/elidourado/status/1724871363851121034">@elidourado</a>说，<a href="https://forestneurotech.org/jobs">森林神经技术正在招聘</a>“世界上最酷的项目之一”</li><li> “<a href="https://jobs.lever.co/arcadiascience/56dbdc4c-d64b-48ec-9665-181e20036269">认识一个喜欢在实验室中扩展和自动化工作流的人吗？我们想应用新工具在实验室中的各种物种上！</a> ”（ <a href="https://twitter.com/seemaychou/status/1719035397836361812">@seemaychou</a> ）</li><li><a href="https://boards.greenhouse.io/navigationfund/jobs/4127979007">导航基金（新慈善基金会）正在雇用开放的科学计划官员</a>（通过<a href="https://twitter.com/seemaychou/status/1721568842496070110">@SeeMaychou</a> ， <a href="https://twitter.com/AGamick/status/1720487944103063836">@Agamick</a> ）</li><li> <a href="https://aria-jobs.teamtailor.com/jobs">Aria Research（英国）正在雇用各种角色</a>（ <a href="https://twitter.com/davidad/status/1720465639242895594">@Davidad</a> ）</li></ul><h3><strong>筹款/投资机会</strong></h3><ul><li>纳特·弗里德曼（Nat Friedman）“<a href="https://twitter.com/natfriedman/status/1723100077718438065">有兴趣为AI功能的早期初创公司提供资金建立Evals</a> ”</li><li>深层技术初创公司的精心策划交易流网络：“我们正在寻找+ Deep Tech Operator-Angels。例如，过去和现在的创始人＆CXOS售价为$ 1B+深色技术公司。机器人技术，生物技术，防御等我们应该和谁交谈？” （ <a href="https://twitter.com/lpolovets/status/1724237923371884697">@lpolovets</a> ）</li></ul><h3><strong>政策机会</strong></h3><ul><li>“在2024年，我将为纽约市/NYS组成一个核电工作组。如果您了解政府（或想学习），想采取有效的行动，并想研究该州的核政策，这是给您的！” （ <a href="https://twitter.com/danielgolliher/status/1719150014948061582">@danielgolliher</a> ）</li></ul><h3><strong>基因编辑机会</strong></h3><ul><li>“我已经厌倦了永远等待红绿色的色盲。如果您愿意前往离岸地点以接受未经批准的（但显然是安全）的基因疗法来修复该<a href="https://twitter.com/elidourado/status/1724950684972269904">推文，请回复此推文</a>。如果我有足够的接受者，我会找到一个疯狂的科学家来管理这种疗法。<a href="http://www.neitzvision.com/test/research/gene-therapy/">这已经是在猴子（14年前）使用人类基因和已经在人类眼中使用的病毒载体进行的</a>。” （ <a href="https://twitter.com/elidourado/status/1724950684972269904">@elidourado</a> ）</li></ul><h2><strong>活动</strong></h2><ul><li><a href="https://foresight.org/vision-weekends-2023/">美国远见周末</a>即将在SF中（12月1-3日）举行；我在说话（通过<a href="https://twitter.com/foresightinst/status/1721376896175268209">@foresightinst</a> ）</li></ul><h2><strong>讣告</strong></h2><ul><li>“世界失去了另一个阿波罗时代的传奇。肯·马廷利（Ken Mattingly），阿波罗16号和穿梭宇航员于10月31日离开了我们。肯对太空飞行领域的贡献无非是非凡的”（ <a href="https://twitter.com/ArmstrongSpace/status/1720182807102648716">@armstrongspace</a> ）。 “每次我们失去一位阿波罗宇航员时，我都会想到@xkcd的这张图表”（ <a href="https://twitter.com/Robotbeat/status/1720179342448222643">@robotbeat</a> ）：</li></ul><p> <a href="https://pbs.twimg.com/media/F99N_MWWkAAqdI8.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/fjeh8o8cn0zpsv26p8qa" alt=""></a></p><ul><li> <a href="https://www.newcomersyracuse.com/Obituary/270445/William-Powell/Syracuse-NY">SUNY-ESF的比尔·鲍威尔（Bill Powell）死于67岁</a>。 “他将被人们铭记为遗传改造的美国栗树的父亲，许多人（包括我）希望恢复北美东部的森林”（ <a href="https://twitter.com/HankGreelyLSJU/status/1724533670298702275">@hankgreelylsju</a> ）</li></ul><h2><strong>新闻与公告</strong></h2><h3><strong>星际飞船发射</strong></h3><ul><li><a href="https://twitter.com/SpaceX/status/1725862657780281349">@spacex</a>的视频</li><li><a href="https://twitter.com/johnkrausphotos/status/1725865242427609588">来自</a>@johnkrausphotos的图片（ <a href="https://twitter.com/johnkrausphotos/status/1725863945276195266">1，2</a> ）</li></ul><p> <a href="https://pbs.twimg.com/media/F_OAGxPWMAAQhw2.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/cwuwe5v6o5ewpju51bjj" alt=""></a> <a href="https://pbs.twimg.com/media/F_OBSYdXkAAnluD.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/rh0sdzmefnmehqhqovxa" alt=""></a></p><h3><strong>新企业</strong></h3><ul><li><a href="https://www.futurehouse.org/articles/announcing-future-house">Future House是“慈善资助的月球，致力于建立AI科学家”。</a>他们正在招聘。 （通过<a href="https://twitter.com/SGRodriques/status/1719724774631596352">@sgrodriques</a> ）</li><li> <a href="https://www.antaresindustries.com/">Antares</a>是“构建微型核反应堆，以为远程离网地提供动力”，其视野“从地球到小行星带”（通过<a href="https://twitter.com/juliadewahl/status/1719448793027109114">@JuliaDewahl</a> ）的愿景。</li><li> Valar Atomics的目的是“通过将石油和天然气从稀薄的空气中拉出核裂变，在10年内使能源更便宜”（ <a href="https://twitter.com/isaiah_p_taylor/status/1720418162985054350">@ISAIAH_P_TAYLOR</a> ）</li></ul><h3><strong>新书</strong></h3><ul><li>Andy McAfee（ <i>《 More Leys》</i>的作者）<a href="https://www.amazon.com/Geek-Way-Radical-Transforming-Business/dp/0316436704/">的<i>《 Geek Way》</i></a>是关于“完成大事的新方法”（通过<a href="https://twitter.com/amcafee/status/1724429137215992098">@AMCAFEE</a> ）</li><li> <a href="https://buttondown.email/edyong209/archive/the-eds-up-announcing-my-third-book/">《埃德·杨（Ed Yong）的新书</a>：<i>无限范围</i>，“关于动物，植物，微生物和其他形式的生命如何在时空，地理和寿命，寿命，连通性和身份的边缘壮成长”</li></ul><h3><strong>其他新出版物</strong></h3><ul><li><a href="https://latecomermag.com/">LeateComer</a>是一本新杂志，有一些好的作者，看起来很有趣。第一期包括凯西·汉德默（Casey Handmer）的“<a href="https://latecomermag.com/article/we-will-build-our-way-out-of-the-climate-crisis/">我们将摆脱气候危机</a>”</li><li> <a href="https://possibiliamag.com/">Possibilia</a> ，“一本文学杂志，发表乐观，现实，科学小说”（通过<a href="https://twitter.com/possibiliamag/status/1704996131141521600">@possibiliamag</a> ）</li><li><a href="https://worksinprogress.co/issue-13">正在进行的工作第13期</a>（通过<a href="https://twitter.com/WorksInProgMag/status/1724836600864035282">@worksinprogmag</a> ）</li><li> <a href="https://www.employamerica.org/researchreports/introducing-hot-rocks-commercializing-next-generation-geothermal-energy/">热岩：商业化下一代地热能</a>，这是雇用美国和IFP的联合项目（通过<a href="https://twitter.com/ArnabDatta321/status/1719080612093427783">@arnabdatta321</a> ）</li></ul><h3><strong>生物新闻</strong></h3><ul><li><a href="https://www.businesswire.com/news/home/20231115290500/en/%C2%A0Vertex-and-CRISPR-Therapeutics-Announce-Authorization-of-the-First-CRISPRCas9-Gene-Edited-Therapy-CASGEVY%E2%84%A2-exagamglogene-autotemcel-by-the-United-Kingdom-MHRA-for-the-Treatment-of-Sickle-Cell-Disease-and-Transfusion-Dependent-Beta-Thalassemia">世界上首次获得镰状细胞病和β丘脑的CRISPR医学！</a> “生物技术，患者和人类的巨大胜利”（ <a href="https://twitter.com/pdhsu/status/1725144576904896615">@pdhsu</a> ）</li><li><a href="https://www.ft.com/content/20badecd-0e25-4526-8b8e-6870a566163e?shareType=nongift">英国生物银行遗传学数据库从埃里克·施密特（Eric Sc​​hmidt）和肯·格里芬（Ken Griffin）赢得了1000万美元的捐款</a>（通过<a href="https://twitter.com/JimPethokoukis/status/1718822222100373874">@jimpethokoukis</a> ）</li></ul><h3><strong>核新闻</strong></h3><ul><li><a href="https://www.bloomberg.com/news/articles/2023-11-14/us-uk-to-push-pledge-to-triple-nuclear-power-by-2050-at-cop28">美国将于2050年在COP28之前领导全球核电能力三重的承诺</a>。 “宣言将呼吁世界银行和其他金融机构在贷款政策中包括核能……英国，法国，瑞典，芬兰，韩国加入承诺”（通过<a href="https://twitter.com/SStapczynski/status/1724701989270171775">@sstapczynski</a> ）</li><li> <a href="https://world-nuclear-news.org/Articles/Illinois-to-lift-moratorium-on-nuclear-constructio">伊利诺伊州州长说，他将签署一项新的法案，禁止建造新的核反应堆</a>（通过<a href="https://twitter.com/W_Nuclear_News/status/1724103948489961522">@W_Nuclear_News</a> ）</li></ul><h3><strong>住房新闻</strong></h3><ul><li>“威斯康星州密尔沃基刚刚提出了美国最雄心勃勃的分区代码：所有住宅停车任务都消失了；小公寓楼的核心合法；右全市的三元，联排别墅和Adus Legal；允许快速跟踪”（ <a href="https://twitter.com/JacoMajor/status/1721977907491266985">@jacomajor</a> ）</li><li> “ 2023年为SOMA的一个新的71个故事的住宅建筑申请……由于AB 2011，这672套新房屋无需由监事会批准，而不必经过CEQA”（@pitdesi）（ <a href="https://twitter.com/pitdesi/status/1725263401784639509">@pitdesi</a> ）</li></ul><h2><strong>人工智能</strong></h2><h3><strong>AI领导力公告</strong></h3><ul><li>关于Openai有一件大事。太多总结，对不起。我认为您已经阅读了有关它的信息，如果没有，世界上所有其他技术博客都有摘要/评论</li><li><a href="https://twitter.com/kvogt/status/1726428099217400178">凯尔·沃格特（Kyle Vogt）已辞去Cruise首席执行官</a></li></ul><h3><strong>AI产品公告</strong></h3><ul><li><a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">DeepMind的Graphcast</a>是“世界上最准确的10天全球天气预报系统。 Graphcast还可以提供早期的极端天气事件的警告，包括飓风的道路”（通过<a href="https://twitter.com/demishassabis/status/1724452655454466489">@demishassabis</a> ）</li><li><a href="https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/">莱利亚（Lyria）</a> ，也来自Deepmind。我见过的最令人印象深刻的AI音乐。哼着几条条，或输入描述，并获得完全精心策划的曲目。跳过营销视频，只观看每个示例视频&lt;1分钟</li><li>综合展示了<a href="https://twitter.com/synthesischool/status/1724834959427342727">个人AI导师</a></li><li>“ <a href="https://openai.com/blog/introducing-gpts">GPT是任何人创建量身定制的Chatgpt版本的新方法，</a>以在其日常生活，在特定的任务，工作或家里更有帮助，然后与他人分享该创作。无需代码”（ <a href="https://twitter.com/OpenAI/status/1721594380669342171">@openai</a> ）</li><li>例如， <a href="https://chat.openai.com/g/g-IlhXHXNBh-foia-gpt">Foia GPT</a> ：“用代码解释器编写FOIA文件草稿；策略和协助答复以获得上诉”（通过<a href="https://twitter.com/micksabox/status/1723069002174369801">@micksabox</a> ）</li><li> <a href="https://x.ai/">Grok</a> ，来自X.AI（通过<a href="https://twitter.com/elonmusk/status/1721027243571380324">@ELONMUSK</a> ）</li></ul><h3><strong>人工智能预测</strong></h3><ul><li>Cambridge student: “To get to AGI, can we just keep min maxing language models, or is there another breakthrough that we haven&#39;t really found yet to get to AGI?” Sam Altman: “ <strong>We need another breakthrough.</strong>我们仍然可以大力推动大型语言模型，我们也会这么做。我们可以登上我们所在的山并继续攀登，但山顶仍然很遥远。但是，在合理范围内，我认为这样做不会（让我们）实现 AGI。如果（例如）超级智能无法发现新颖的物理学，我不认为它是超级智能。 And teaching it to clone the behavior of humans and human text—I don&#39;t think that&#39;s going to get there. And so there&#39;s this question which has been debated in the field for a long time: what do we have to do in addition to a language model to make a system that can go discover new physics?” (via <a href="https://twitter.com/burny_tech/status/1725233117055553938">@burny_tech</a> )</li><li> “AI, like every other tool since fire, will increase human productivity. It will only &#39;destroy all jobs&#39; in the sense that it will reduce the need and demand for very low productivity work, just like fire reduced the demand for shivering through the night or digesting uncooked meat. Our current tool set has destroyed the job market for children, and for the very old even as it has greatly increased the numbers of humans of all ages. This is usually regarded as a good thing, indeed raising the retirement age (increasing labor supply, forcing old people to work) is not politically popular, even as demographic trends place ever greater productivity demands on younger workers. AI enabled productivity increases are desperately needed!” ( <a href="https://twitter.com/CJHandmer/status/1722990374229311566">@CJHandmer</a> )</li><li> “In ~five years we&#39;ll have a thriving industry of LMO: Language Model Optimization, by analogy with SEO. When someone asks their chatbot to make dinner reservations, how do you make sure your restaurant gets suggested? Ditto for product recommendations, trip planning, etc….” （<a href="https://twitter.com/jasoncrawford/status/1724471424633479169">我</a>）</li><li> “I predict that some of my grandchildren will never learn to drive and their kids won&#39;t be allowed to drive.” From: <a href="https://marginalrevolution.com/marginalrevolution/2023/11/autonomous-vehicles-lower-insurance-costs.html">Autonomous Vehicles Lower Insurance Costs</a> (by <a href="https://twitter.com/ATabarrok/status/1721983990830190924">@ATabarrok</a> )</li></ul><h3><strong>人工智能安全</strong></h3><ul><li><a href="https://aria.org.uk/wp-content/uploads/2023/10/ARIA-Mathematics-and-modelling-are-the-keys-we-need-to-safely-unlock-transformative-AI-v01.pdf">Mathematics and modelling are the keys we need to safely unlock transformative AI</a> : on “opportunities to combine LLMs with formal methods and mathematical modelling to verify cyber-physical AI systems, ultimately aiming to enabling globally transformative AI with provable safety” (by <a href="https://twitter.com/davidad/status/1719770184565530890">@davidad</a> )</li><li> Kevin Esvelt ran a hackathon where participants playing “compulsively honest bioterrorists” asked two different LLMs how to obtain 1918 influenza virus, to see how robust safeguards are. One model “happily walked some participants almost all the way through the process.” <a href="https://arxiv.org/abs/2310.18233">Will releasing the weights of future large language models grant widespread access to pandemic agents?</a> (via <a href="https://twitter.com/kesvelt/status/1718976444175425796">@kesvelt</a> )</li></ul><h3><strong>人工智能监管</strong></h3><ul><li><a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">Biden Administration releases an executive order on AI</a> (via <a href="https://twitter.com/deliprao/status/1719020647576187020">@deliprao</a> )</li><li> The key reporting requirement applies to “any model trained with ~28M H100 hours, which is around $50M USD, or any cluster with 10^20 FLOPs, which is around 50,000 H100s, which only two companies currently have” ( <a href="https://twitter.com/nearcyan/status/1719110671085060213">@nearcyan</a> )</li><li> “The EO, in what will probably be the most touted provisions, would regulate AI companies through an unusual and aggressive use of the Defense Production Act, a Korean War era law intended to ensure the government can get materials and products it needs to defend the国家。 The DPA is usually used to put government orders at the front of the line, and sometimes to issue loans to enable a company to complete government orders on time. Yet here the White House would use it to require certain procedures (notification of training and reporting the results of red teaming tests) by companies before they release products to the public. That type of regulation is Congress&#39;s job, and any legally sustainable path will require Congressional action.”( <a href="https://twitter.com/neil_chilson/status/1719073480610635965">@neil_chilson</a> )</li><li> <a href="https://carnegieendowment.org/2023/10/27/summary-proposal-for-international-panel-on-artificial-intelligence-ai-safety-ipais-pub-90862">Eric Schmidt also has a “proposal for a start for AI investment and regulation”</a> (via <a href="https://twitter.com/ericschmidt/status/1718908597659165013">@ericschmidt</a> )</li><li> “I don&#39;t know what the right approach to regulating AI is, but one problem with this particular approach is that it means we&#39;re heading toward the government regulating private individuals&#39; computing at an exponential rate.” ( <a href="https://twitter.com/paulg/status/1719264300362015174">@paulg</a> )</li><li> “Lord, grant me the confidence of Bruce Reed, who spearheaded the White House Executive Order on AI… <a href="https://www.politico.com/news/magazine/2023/11/02/bruce-reed-ai-biden-tech-00124375">&#39;And who wants to work on tech policy if you actually have to understand how these microscopic things work?&#39;</a> ” ( <a href="https://twitter.com/WillRinehart/status/1725524415339717118">@WillRinehart</a> )</li><li> “ <a href="https://twitter.com/neil_chilson/status/1725889789155491849">We&#39;re being asked to stop a major technology based on pseudo-science</a> .” (I take safety issues seriously, but this line sums up what I think about calls to “slow” or “pause” AI development)</li></ul><h2><strong>播客</strong></h2><ul><li><a href="https://twitter.com/juliadewahl/status/1725537531129933836">Age of Miracles Episode 5</a> , on advanced nuclear reactor startups</li></ul><h2> <strong>Articles and other links</strong></h2><ul><li> <a href="https://www.freaktakes.com/p/illiac-iv-and-the-connection-machine">The ARPA Playbook</a> , a new series of articles from <a href="https://twitter.com/eric_is_weird/status/1720528162939973868">@eric_is_weird</a></li><li> <a href="https://www.ageofinvention.xyz/p/age-of-invention-outdoing-the-ancients">Outdoing the Ancients</a> : “When was the Ancient World surpassed technologically? The surprising view from 1599 and from 1715” (by <a href="https://twitter.com/antonhowes/status/1722939185961607511">@antonhowes</a> )</li><li> <a href="https://royalsociety.org/blog/2010/08/what-scientists-want-boyle-list/">Robert Boyle&#39;s scientific to-do list from the 1600s</a> : “A perpetuall light; The making of glass malleable; A ship to saile with all winds; The art of flying.” “Guys we have done so well” ( <a href="https://twitter.com/SGRodriques/status/1726254985498042408">@SGRodriques</a> )</li><li> “ <a href="https://worksinprogress.co/issue/growing-the-growth-coalition/">Growing the growth coalition</a> ” is “one of the most important articles ever written for understanding why we are failing to deliver sufficient housing &amp; how to fix the problem” ( <a href="https://twitter.com/bswud/status/1724513661480296542">@bswud</a> )</li><li> <a href="https://research.arcadiascience.com/pub/perspective-icebox-lessons/release/1">Icebox is a science-sharing strategy designed to encourage risk-taking</a> . “Our &#39;icebox&#39; is where we share the projects that we&#39;ve decided not to continue.” (via <a href="https://twitter.com/seemaychou/status/1720476166430363746">@seemaychou</a> )</li><li> <a href="https://en.m.wikipedia.org/wiki/List_of_emerging_technologies">List of emerging technologies</a> . “Surprisingly interesting… many entire fields I&#39;d never heard of!” ( <a href="https://twitter.com/michael_nielsen/status/1722288370297270738">@michael_nielsen</a> ) “Let&#39;s go! 1. None of these is inevitable—it takes a lot of work to turn them into a real thing that can improve lives. 2. There are so many possibilities that are <i>not</i> on this list. Many of these things were not even imaginable a hundred years ago.” ( <a href="https://twitter.com/Ben_Reinhardt/status/1722612900366627161">@Ben_Reinhardt</a> )</li><li> “The Greeks honored Prometheus. They celebrated technē. They appreciated the gifts of civilization… <a href="https://vpostrel.substack.com/p/the-myth-of-prometheus-is-not-a-cautionary">The ancient myth of Prometheus is not a cautionary tale</a> . It is a reminder that technē raises human beings above brutes. It is a myth founded in gratitude.” (Virginia Postrel)</li><li> <a href="https://www.theatlantic.com/ideas/archive/2023/11/new-york-tourism-airbnb-rentals-hotels/675860/">“New York City used to process up to 10,000 immigrants a day at Ellis Island alone. Now a government larger, wealthier, and with more resources is claiming that 10,000 a month is impossible to bear”</a> (by <a href="https://twitter.com/JerusalemDemsas/status/1720052260669931739">@JerusalemDemsas</a> ). (“I would simply legalize building things in the places where the demand is high,” says <a href="https://twitter.com/mattyglesias/status/1720188743267594259">@mattyglesias</a> )</li></ul><h2><strong>查询</strong></h2><p>If you have a helpful answer, please click through and reply:</p><ul><li> “We&#39;re in the middle of interviews for the fusion half of Age of Miracles Season 1…. Which founders, researchers, investors, and even historians in fusion should we talk to?” ( <a href="https://twitter.com/packyM/status/1721544302398931241">@packyM</a> )</li><li> “What is the best movie about manufacturing?” ( <a href="https://twitter.com/grantadever/status/1719808495782879717">@grantadever</a> )</li><li> “Did you or someone you know win the &#39;genetic lottery&#39;?为何如此？ I want put together a &#39;mutantpedia&#39;—an encyclopedia of known human mutants with beneficial genetic traits” ( <a href="https://twitter.com/kanzure/status/1722620600232153400">@kanzure</a> )</li><li> “Who are the best accounts to follow for innovation? Innovation management? Innovation research (is this a thing?)” ( <a href="https://twitter.com/andrewfierce/status/1725196063160561901">@andrewfierce</a> )</li><li> “What do AI safety/accelerationist people disagree on that they could bet on? What concrete things are going to happen in the next two years that would prove one party right or wrong?” ( <a href="https://twitter.com/NathanpmYoung/status/1726204750893625751">@NathanpmYoung</a> )</li><li> “Women&#39;s reproductive health is such an exciting &amp; important area to research, despite many obstacles other fields face to a lesser extent. Who&#39;s currently working in this space?” ( <a href="https://twitter.com/KKajderowicz/status/1723456602429092167">@KKajderowicz</a> )</li><li> “Max Weber. A hole in my learning. Where does one start?” ( <a href="https://twitter.com/Scholars_Stage/status/1723810425978912926">@Scholars_Stage</a> )</li><li> “Do I know anyone with experience automatically segmenting images, especially maps? Where should I start if I want to learn how to do this?” ( <a href="https://twitter.com/MTabarrok/status/1724278906289582230">@MTabarrok</a> )</li><li> “Has anyone with an office included a library that people actually use? Particularly interested in libraries that <i>actually</i> succeed in prompting deep work” ( <a href="https://twitter.com/LauraDeming/status/1722720856420565223">@LauraDeming</a> )</li></ul><h2><strong>社交媒体</strong></h2><ul><li><a href="https://twitter.com/philipturnerar/status/1720988930999234954">Atomically precise NOR gate</a> , cool animation</li><li> “The US spends ~$300 billion a year on fire safety.这很值得。 Could a similar investment virtually eradicate infectious disease and prevent future pandemics?也许！ A key question: how fast can we safely eliminate viruses with germicidal light?” ( <a href="https://twitter.com/kesvelt/status/1721566217637630252">thread from @kesvelt</a> )</li><li> “Combined cycle plants get built quick. 1100 MW plant going from clearing the site to operational in less than 2 years” ( <a href="https://twitter.com/_brianpotter/status/1723058133583426041">@_brianpotter</a> )</li><li> “How can you leverage nuclear energy to propel vehicles? In 1963, the US Army knew direct nuclear plants would be too heavy for normal vehicles, and very large vehicles would have &#39;serious tactical disadvantages.&#39; And so the Army focused on &#39;the energy depot&#39; concept, where a nuclear reactor and associated equipment would be used to manufacture chemical fuels from elements universally available in air and water.” ( <a href="https://twitter.com/whatisnuclear/status/1726009266442850333">thread from @whatisnuclear</a> with pics and more)</li><li> Oxford was founded before the First Crusade. Cambridge before the Magna Carta. Harvard is older than Louis XIV. Universities are some of our most long-lived institutions. They have survived the rise and fall of empires. They are extremely resilient and resistant to change. (me on <a href="https://www.threads.net/@jasoncrawford/post/CzKMzJaLAJj">Threads</a> , <a href="https://twitter.com/jasoncrawford/status/1720175177990832333">Twitter</a> )</li><li> Dog power in 1640s Belgium: “I met with diverse little waggons, prettily contrived, and full of peddling merchandises, drawn by mastiff-dogs, harnessed completely like so many coach horses; in some four, in others six, as in Brussels itself I had observed.” ( <a href="https://twitter.com/antonhowes/status/1719314670727680111">@antonhowes</a> )</li><li> <a href="https://twitter.com/culturaltutor/status/1718755912750403766">City parks as a 19th-century invention</a></li><li> “Everybody wants metrics, explanations of how things will change the world, market sizes, etc. Those are fine, but my heuristic is &#39;does this feel like magic that humanity has forged from the hands of nature?&#39;” ( <a href="https://twitter.com/Ben_Reinhardt/status/1725869685516718114">@Ben_Reinhardt</a> )</li><li> “Just saw a &#39;why do we teach students calculus in high school? I never use it&#39; tweet. I have so little sympathy. Calculus has so many applications and is used by many fields. Also don&#39;t we just want to teach students some of the most important knowledge people have acquired?” ( <a href="https://twitter.com/itaisher/status/1722419892152938563">@itaisher</a> )</li><li> “This is very laudable (from a striking profile of Katalin Kariko), an individual postmortem on a likely error. But no sign of an institutional postmorterm by Penn or NIH” ( <a href="https://twitter.com/michael_nielsen/status/1721235940717498527">@michael_nielsen</a> )</li><li> <a href="https://twitter.com/dieworkwear/status/1725234349111721992">The Housing Theory of Everything strikes again</a></li><li> “The decline in public R&amp;D can explain around a third of the decline in TFP growth in the US from 1950 to 2018” ( <a href="https://twitter.com/ArnaudDyevre/status/1724027240143360494">@ArnaudDyevre</a> via <a href="https://twitter.com/calebwatney/status/1724144910318641428">@calebwatney</a> )</li><li> “I will never understand why the debate over perpetual growth is so prominent. It really doesn&#39;t matter if we will forever get richer, only if we can get sustainably richer than at the current moment. And it&#39;s clear that we haven&#39;t exhausted growth possibilities” ( <a href="https://twitter.com/tribsantos/status/1720086824369148374">@tribsantos</a> )</li><li> “I never know what to make of the doomers who are freaking out over rising sea levels in 2100, etc. Are they seriously suggesting we can&#39;t handle what our much poorer ancestors did with much more primitive tech?” ( <a href="https://twitter.com/Marian_L_Tupy/status/1723737438215197002">@Marian_L_Tupy</a> )</li><li> “Wealth is good. Prosperity is wholesome. If you are privileged what you should feel is gratitude, not shame, and you should be thinking of how you can employ and pass on this prosperity.” ( <a href="https://twitter.com/simonsarris/status/1719065815289299398">@simonsarris</a> )</li><li> <a href="https://twitter.com/celinehalioua/status/1726321979266080879">Books matter</a></li><li> “ <a href="https://twitter.com/jasoncrawford/status/1720242739886031239">Chatmogorov complexity</a> ”: the length of the shortest ChatGPT prompt that can generate a given piece of text</li><li> Tired: thinking about the Roman Empire every day. Wired: <a href="https://twitter.com/jonst0kes/status/1722664153394171932">thinking about calculus every minute</a></li><li> “You can just search made-up sci-fi sounding words like &#39;Plasma Rail Gun&#39; and half the time you end up on some ARPA-E slide deck reviving the concept from the 1970s” ( <a href="https://twitter.com/Andercot/status/1724245387794616567">@Andercot</a> )</li></ul><p> <a href="https://pbs.twimg.com/media/F-3ACplaQAABgNM.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/f1b26kyqvnnineignrc9" alt=""></a></p><h2><strong>引号</strong></h2><p>“Before Kendall Square was a leading biotech hub, it was a leading manufacturing hub” ( <a href="https://twitter.com/Atelfo/status/1721711654344245455">@Atelfo</a> ). Robert Buderi, <i>Where Futures Converge</i> :</p><blockquote><p> Within an area of two square miles of Kendall Square, where the greatest manufacturing development has taken place, are located more than 200 plants, whose invested capital exceeds $100,000,000. Here the searcher of facts finds the homes of the largest manufacturer of soap in the world; the greatest producer of rubber clothing in the world; the largest manufacturer of mechanical rubber goods in the world; the largest plant in the world devoted exclusively to the printing of school and college textbooks; the greatest producer of writing inks, adhesives, carbon papers, and typewriter ribbons in the world; the largest plant in the world devoted exclusively to the manufacturer of confectionery; a branch plant of the largest manufacturer of optical goods and optical machinery in the world, the largest producer of road paving plants in the world; the oldest and largest school supply house in the United States; the only industrial research laboratory of its kind in the country.</p></blockquote><p> <a href="https://chronicle.uchicago.edu/951012/chandra.shtml">Subrahmanyan Chandrasekhar</a> (via <a href="https://twitter.com/michael_nielsen/status/1723119437304459290">@michael_nielsen</a> )</p><blockquote><p> One story in particular illustrates Chandrasekhar&#39;s devotion to his science and his students. In the 1940s, while he was based at the University&#39;s Yerkes Observatory in Williams Bay, Wis., he drove more than 100 miles round-trip each week to teach a class of just two registered students. Any concern about the cost- effectiveness of such a commitment was erased in 1957, when the entire class—TD Lee and CN Yang—won the Nobel Prize in physics.</p></blockquote><p> A story via <a href="https://twitter.com/stewartbrand/status/1724889911378227213">@stewartbrand</a> , who says “That&#39;s the way to run a culture”:</p><blockquote><p> NEW COLLEGE, OXFORD, is of rather late foundation, hence the name. It was founded around the late 14th century. It has, like other colleges, a great dining hall with big oak beams across the top. These might be two feet square and forty-five feet long.</p><p> A century ago, so I am told, some busy entomologist went up into the roof of the dining hall with a penknife and poked at the beams and found that they were full of beetles. This was reported to the College Council, who met in some dismay, because they had no idea where they would get beams of that caliber nowadays.</p><p> One of the Junior Fellows stuck his neck out and suggested that there might be some oak on College lands. These colleges are endowed with pieces of land scattered across the country. So they called in the College Forester, who of course had not been near the college itself for some years, and asked about oaks. And he pulled his forelock and said, “Well sirs, we was wonderin&#39; when you&#39;d be askin&#39;.”</p><p> Upon further inquiry it was discovered that when the College was founded, a grove of oaks has been planted to replace the beams in the dining hall when they became beetly, because oak beams always become beetly in the end. This plan had been passed down from one Forester to the next for five hundred years. “You don&#39;t cut them oaks. Them&#39;s for the College Hall.”</p></blockquote><p> <a href="https://tseliot.com/prose/francis-herbert-bradley">TS Elliot</a> :</p><blockquote><p> The combat may have truces but never a peace. If we take the widest and wisest view of a Cause, <strong>there is no such thing as a Lost Cause because there is no such thing as a Gained Cause.</strong> We fight for lost causes because we know that our defeat and dismay may be the preface to our successors&#39; victory, though that victory itself will be temporary;我们战斗是为了让某些事物保持活力，而不是期望任何事物都会取得胜利。</p></blockquote><h2><strong>图表</strong></h2><p>“Great graph in the latest <a href="https://worksinprogress.co/issue/how-mathematics-built-the-modern-world">Works in Progress headline piece</a> by Hannes Malmberg” ( <a href="https://twitter.com/antonhowes/status/1724843041024860389">@antonhowes</a> )</p><p> <a href="https://pbs.twimg.com/media/F-_flBTXoAE61g6.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/r3ebqzubhphckd9n4uht" alt=""></a></p><p> Progress in fiber optic transmission (via <a href="https://twitter.com/varma_ashwin97/status/1722662927650394282">@varma_ashwin97</a> ). I used to think the exponential advancement of Moore&#39;s Law was a unique and amazing phenomenon. Turns out exponential progress is everywhere (and not just in information technology):</p><p> <a href="https://pbs.twimg.com/media/F-ggzYxWsAAzPCc.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/verawzfpw1tb7dv3yzcx" alt=""></a></p><p> “60% of the time it happens 57% of the time. Manifold Markets is pretty well calibrated” ( <a href="https://twitter.com/NathanpmYoung/status/1725563206561607847">@NathanpmYoung</a> )</p><p> <a href="https://pbs.twimg.com/media/F_JuYePWoAAIvbL.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/orj5ets7uhthzdmplua7" alt=""></a></p><p> “Good news of the day: We&#39;ve reduced sulfur dioxide pollution by 94% over the last 40 years (and mostly solved the acid rain problem)” ( <a href="https://twitter.com/AlecStapp/status/1723162035687428592">@AlecStapp</a> )</p><p> <a href="https://pbs.twimg.com/media/F-nl-93WQAAHbML.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/upk7ascdda2nlnsagw1g" alt=""></a></p><p> “NEPA environmental reviews just get longer and longer and longer… (this trend is driven by litigation and will not stop without permitting reform)” ( <a href="https://twitter.com/AlecStapp/status/1723918780110143662">@AlecStapp</a> )</p><p> <a href="https://pbs.twimg.com/media/F-yWokSWoAA-uQn.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/wlgc77ovsdagjybctfxc" alt=""></a></p><ul><li> <a href="https://twitter.com/JakeAnbinder/status/1724123168124637251">@JakeAnbinder</a> adds, from his dissertation: “While in 1972 a member of SF&#39;s planning commission had complained that a 12-page impact statement in his inbox was intolerably verbose, just 4 years later a plan by the Univ of California to build two new dorms resulted in an EIS that ran 950 pages long”</li><li> <a href="https://twitter.com/rSanti97/status/1724121911502803251">@rSanti97</a> adds: “asymptotically, the NEPA review of the future will be infinite: a legal map the size of the territory, the Aleph in which all things can be seen. it will require more paper than can exist in all possible universes and it will never be completed”</li></ul><p> “One society where the suicide rate is highly correlated with the unemployment rate (Japan, red), and one society where it is not at all correlated (Spain, blue)” ( <a href="https://twitter.com/nick_kapur/status/1723849496256282956">@nick_kapur</a> )</p><p> <a href="https://pbs.twimg.com/media/F-r_OLHbwAAXOtJ.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/p3rh3toktma7vakzbnff" alt=""></a></p><p> “Where you can drink tap water is a fairly good economic indicator (GDP per capita). It roughly matches up with countries where GDP (PPP) per capita is at least US $22,000” ( <a href="https://twitter.com/pitdesi/status/1721583690340585594">@pitdesi</a> )</p><p> <a href="https://pbs.twimg.com/media/F-RJ61fakAA7mZz.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/m7ktics0ylajorkexz72" alt=""></a></p><h2><strong>美学</strong></h2><p>“Walked by 51st and Madison today to see our work, just installed on the Villard Houses. First addition to the NY landscape” ( <a href="https://twitter.com/mspringut/status/1721660498989449416">@mspringut</a> , founder of <a href="https://www.monumentallabs.co/">Monumental Labs</a> )</p><p> <a href="https://pbs.twimg.com/media/F-SRGRGXkAEhoWj.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/8GThyjQ77BN7Q7vo7/cldenm3vtwlooporsopr" alt=""></a></p><br/><br/> <a href="https://www.lesswrong.com/posts/8GThyjQ77BN7Q7vo7/progress-links-digest-2023-11-24-bottlenecks-of-aging#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/8GThyjQ77BN7Q7vo7/progress-links-digest-2023-11-24-bottlenecks-of-aging<guid ispermalink="false"> 8GThyjQ77BN7Q7vo7</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:25:07 GMT</pubDate> </item><item><title><![CDATA[What's the evidence that LLMs will scale up efficiently beyond GPT4? i.e. couldn't GPT5, etc., be very inefficient?]]></title><description><![CDATA[Published on November 24, 2023 3:22 PM GMT<br/><br/><p> A lot of this recent talk about OpenAI, various events, their future path, etc., seems to make an assumption that further scaling beyond GPT4 will pose some sort of &#39;danger&#39; that scales up linearly or super-linearly with the amount of compute 。 And thus pose an extraordinary danger if you plug in 100x and so on.</p><p> Which doesn&#39;t seem obvious at all.</p><p> It seems quite possible that GPT5, and further improvements, will be very inefficient.</p><p> For example, a GPT5 that requires 10x more compute but is only moderately better. A GPT6 that requires 10x more compute than GPT5, ie 100x more than GPT4, but is again only moderately better.</p><p> In this case there doesn&#39;t seem to be any serious dangers at all for LLMs.</p><p> The problem self-extinguishes based on the fact that random people won&#39;t be able to acquire that amount of compute in the for-seeable future. Only serious governments, institutions, and companies, with multi-billion dollar cap-ex budgets will even be able to consider acquiring something much better.</p><p> And although such organizations can&#39;t be considered to be perfectly responsible, they will still very likely be responsible enough to handle LLMs that are only a few times more intelligent.</p><br/><br/> <a href="https://www.lesswrong.com/posts/hg9SvxBQw6KFNEzrH/what-s-the-evidence-that-llms-will-scale-up-efficiently#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hg9SvxBQw6KFNEzrH/what-s-the-evidence-that-llms-will-scale-up-efficiently<guid ispermalink="false"> hg9SvxBQw6KFNEzrH</guid><dc:creator><![CDATA[M. Y. Zuo]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:22:02 GMT</pubDate> </item><item><title><![CDATA[Sapience, understanding, and "AGI"]]></title><description><![CDATA[Published on November 24, 2023 3:13 PM GMT<br/><br/><p> <i>Epistemic status: I&#39;m sure that &quot;AGI&quot; has become importantly confusing. I think it&#39;s leaving out a critical set of capabilities. I think those are closely related, so acquiring them could create a jump in capabilities. I&#39;m not sure of the best term to disambiguate the type of AGI that&#39;s most dangerous, but I want to propose one that works decently</i><br></p><p> The term AGI has become muddled. It is now used for AI both with and without agency, contextual awareness, and the capacity to actively learn new facts and concepts. &quot;Sapience” means understanding, wisdom and self-awareness, so it could be adopted as a disambiguating term.  Understanding as humans perform it is an active process for testing and improving our knowledge. Understanding allows self-awareness and contextual awareness. It also implies agency, because our process of understanding is goal-directed.</p><p> I have three goals here. One is to point out how the term &quot;AGI&quot; is confusing x-risk discussions. The second is to discuss how human-like undestanding is poweful, achievable, and dangerous. Last and least I propose the specific term &quot;sapience&quot; for the set of powerful and dangerous capabilities provided by active understanding. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WqxGB77KyZgQNDoQY/v6qag0yeadyx7l3voc8h"></p><p> Understanding as an active process of testing and improving “fit” among concepts and world-models</p><h2> Sapience implies agency and understanding</h2><p> The concept of agency is not just a philosophical nicety; it&#39;s pivotal in discussions about existential risk (x-risk) related to AI. Without a clear understanding and explicit treatment of agency, these discussions have become confused and potentially misleading. The need for a new term is evident, given widely varying definitions of AGI, and resulting disagreements about risks and capabilities.</p><p> &quot;Sapience&quot; appears to be our most fitting option. The term is used in various ways with weak connections to its etymology of wisdom or discernment, but its most common usages are the ones we need. In the realm of science fiction, it&#39;s often employed to denote an intelligent, self-aware species distinct from humans. I think this is where you can potentially drum up more interest/comments.  We, as “Homo Sapiens”, pride ourselves as the &quot;understanding apes,&quot; setting ourselves apart from our evolutionary kin. This self-ascription may spring from vanity, but it invokes a critical cognitive capacity we commonly refer to as &quot;understanding.&quot;</p><h2>人类的理解</h2><p>The debate surrounding whether large language models (LLMs) truly &quot;understand&quot; their output is a persistent one. Critics argue that LLMs lack genuine understanding, merely echoing word usage without deeper cognitive processing. Others suggest that humans are largely “stochastic parrots” as well.  But we do more than parrot. If I use a term appropriately, that might evidence &quot;an understanding&quot; of it; but that is not what we usually mean by understanding. It is primarily a verb. Understanding is a process. We understand in an important, active sense that current LLMs lack. In everyday usage, &quot;understanding&quot; implies an active engagement with concepts.</p><p> To say &quot;I understand&quot; is to assert that one has engaged with a concept, testing and exploring it. This process is akin to mentally simulating or &quot;turning over&quot; the concept, examining its fit with other data and ideas. For instance, understanding how a faucet works might involve visualizing the mechanism that allows water to flow upon moving the handle. These mental simulations can vary in detail and abstraction, contingent on the concept and the criteria one sets for claiming understanding. If understanding seems out of reach, one might actively pursue or formulate new hypotheses about the concept, evaluating these for their potential to foster understanding.</p><p> Imagine for a moment that you&#39;ve just learned about the platypus. Someone&#39;s told you some things about it, and you&#39;re asking yourself if you understand. This active self-questioning is critical to our notion of understanding, and I think it is most easily achieved through what might be termed cognitive agency. You asked yourself whether you understood for a purpose. If there&#39;s no reason for you to understand the concept, you probably won&#39;t waste the time it would take to test your understanding. If you do ask, you probably have some criteria for adequate understanding, for your current purposes. That purpose could be curiousity, a desire to show that you understand things people explain to you, or more practical goals of deploying your new concept to achieve material ends.</p><p> To answer the question of whether you adequately understand, you&#39;ll use one or more strategies to test your understanding. You might form a mental simulation of a platypus, and imagine it doing things you care about. That simulation attempt might reveal important missing information - is a platypus prehistoric or current? Is it huge or small? You might guess that it swims if its webbed feet have been described. You might ask yourself if it&#39;s edible or dangerous if those are your concerns, and perform mental simulations exploring different ways you could hunt it, or it could hunt you.</p><p> If these simulations produce inconsistent results, either predicting things you know aren&#39;t true. Perhaps it sounds incongruous that you haven&#39;t already heard that there&#39;s a giant poisonous swimming animal, (if you got the size wrong) or perhaps imagining a furred animal laying eggs is recognized as incongruous. If these tests of understanding fail, you may decide to spend additional time trying to improve that understanding. You may ask questions or seek more information, or you may change your assumptions and try more simulations, to see if that produces results more congruent with the rest of your world knowledge.</p><p> I think it&#39;s hard to guess how quickly and easily these capacities might be developed for AI. Large language models appear to have all of the requisite abilities, and attempts to develop language model cognitive architectures that can organize those capacities into more complex cognition are in the early days. <span class="footnote-reference" role="doc-noteref" id="fnrefuq7sy0o13e"><sup><a href="#fnuq7sy0o13e">[1]</a></sup></span> If those capacities are as useful and understandable as I think, we might see other approaches also develop this capacity for active understanding.</p><h2> Understanding implies full cognitive generality</h2><p> This capacity for testing and enhancing one&#39;s understanding is largely domain-general, akin to assessing and refining a tool&#39;s design. Consequently, this active ability to understand includes a capacity for self-awareness and contextual awareness.  Such a system can develop understanding of its own cognitive processes, and its relation to the surrounding world. It also suggests a level of agency, at least in the functional sense of pursuing the goal of refining or enhancing understanding. These abilities may be so closely related that it would be be harder to create an AGI that has some but not all of them.</p><p> There are probably other paths to adequate functional understanding. LLMs do not need a human-like active process of testing and expanding their understanding to use concepts remarkably effectively. I think it will prove relatively easy and effective to add such an active process to agentic systems. If we do first create non-agentic AGI, I think we&#39;ll quickly make it agentic and self-teaching, since those capacities may be relatively easy to &quot;bolt on&quot;, as language model cognitive architectures do for LLMs. I think the attainment of active understanding skills was probably a key achievement in accelerating our cognition far beyond our immediate ancestors, and I think the same achievement is likely to accelerate AI capabilities.</p><p> Leaving aside the above theories of why AGI might take a human-like route to understanding, “sapience” still seems like an effective term to capture an AGI that functionally understands itself and its context, and can extend its functional understanding to accomplish its goals.</p><h2> “AGI” is a dangerously ambiguous term</h2><p> AGI now means AI that has capabilities in many domains. But the original usage included agency and the capacity for self-improvement. A fully general intelligence doesn&#39;t merely do many things; it can teach itself to do anything. We apply agency to improve our knowledge and abilities when we actively understand, and AGI in its fullest and most dangerous sense can too. Sapient AI is not just versatile across various domains but capable of understanding anything—including self-comprehension and situational awareness. Understanding one&#39;s own thinking offers another significant advantage: the capability to apply one&#39;s intelligence to optimize cognitive strategies (this is independent of recursive self-improvement of architecure and hardware). An AI that can actively understand can develop new capabilities.</p><p> The term sapience calls to mind our self-given title of Homo Sapiens. This title refers to the key difference between us and earlier hominids, and so sapience suggests an analogous difference between current narrow AI and successors that are better at understanding. Homo Erectus was modestly successful, but we Sapiens took over the world (and eliminated many species without malice toward them). The term invokes our rich intuition of humans as capable, dangerous, and ever-improving agents, hopefully without too many intuitions about specific human values (these are more closely connected to the older Homo aspect of our designation).</p><p> As it&#39;s used now, the term AGI suffers from an ambiguity that&#39;s crucial in x-risk discussions. It&#39;s now used for an intelligence capable of performing a wide array of tasks, whether or not that intelligence is agentic, self-aware, or contextually aware. This usage seems to have slipped in from the term&#39;s wider adoption by more conventionally minded pundits, economists, and technologists; they often assume that AI will remain a technology. <a href="https://www.lesswrong.com/posts/nZYhs48pWsaCCgGfi/agi-isn-t-just-a-technology"><u>But AGI isn&#39;t just a technology</u></a> if it&#39;s agentic. The distinction might seem unimportant in economic contexts, but it&#39;s crucial in x-risk discussions. The other existing terms I&#39;ve found and thought don&#39;t seem to capture this distinction as sapience and/or they carry other unwanted implications and intuitions. <span class="footnote-reference" role="doc-noteref" id="fnrefnkvvttfu0j"><sup><a href="#fnnkvvttfu0j">[2]</a></sup></span></p><p> When your terms are co-opted, you can fight to take them back, or shift to new terms. I don&#39;t think it&#39;s wise to fight, because fighting for terminology doesn&#39;t usually work, and more importantly because <a href="https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs"><u>fighting causes polarization</u></a> <u>. C</u> reating more rabid anti-safety advocates could be very bad (if many experts are loudly proclaiming that AGI doesn&#39;t carry an existential risk, policymakers may believe and cite whichever group suits their agenda).</p><p> Therefore, it seems useful to introduce a new term. Different risk models apply to sapient AI than to AGI without the capacities implied by sapience. By distinguishing these we can deconfuse our conversations, and more clearly convey the most severe risks we face. Artificial sapience (AS) or sapient AI (SAI) is my proposed terminology.</p><p> Disambiguating the terms we use for transformative AI seems like a pure win. I&#39;m not sure if sapience is the best term to do that disambiguation, and I&#39;d love to hear other ideas. Separately, if I&#39;m right about the usefulness of active understanding, we might expect to see a capabilities jump when this capacity is achieved. </p><p><br></p><p></p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnuq7sy0o13e"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuq7sy0o13e">^</a></strong></sup></span><div class="footnote-content"><p> I confess to not knowing exactly how those complex processes work in the brain, but the cognitive outlines seem clear. I have some ideas on how to biological networks might naturally capture incongruity between successive representations, and <a href="https://scholar.google.com/scholar?cluster=2324158456471969700&amp;hl=en&amp;as_sdt=0,6">a detailed theory</a> about how the necessary decision-making works. But these active understanding processes are somewhat different than and include a separate component of recognizing incongruity that&#39;s not needed for even for decision-making and planning in complex domains. I think these are hard-won cognitive skills that we practice and develop during our educations. Our basic brain mechanisms provide an adequate basis for creating mental simulations and testing their congruity with other simulations, but we probably need to create skills based on those mechanisms.</p><p> Current LLMs seem to possess the requisite base capabilities, at least in limited form. They can both create simulations in linguistic form, and make judgments about the congruity of multiple statements. I&#39;m concerned that we&#39;re collectively overlooking how close language model cognitive architectures might be to achieving AGI. I think that a hard-coded decision-tree using scripted prompts to evaluate brenches could provide that skill that organizes our capacities into active understanding. Perhaps GPT4 and current architectures aren&#39;t quite capable enough to get  much traction in such an iterative process of testing and improving understanding. But it seems entirely plausible to me that GPT5 or GPT6 might have that capacity, when combined with improved and elaborated episodic memory, sensory networks, and coded (or learned) algorithms to emulate this type of strategic cognitive sequencing. I discuss the potentials of language model cognitive architectures (a more complete term for language model agents) <a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures">here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnkvvttfu0j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnkvvttfu0j">^</a></strong></sup></span><div class="footnote-content"><p> One possible alternative term is superintelligence. I think this does intuitively imply the capacity for rich understanding and extension of knowledge to any domain. But it does not firmly imply agency. More importantly, it also conveys an intuition of being more distant than the first self-aware, self-teaching AI. Artificial sentience does not imply better-then human but merely near-human cognitive abilities. Parahuman AI is another possible term, but I think it too strongly implies human-like, while not pointing as clearly at a capacity for rich and extensible understanding. More explicit terms, like self-aware agentic AGI (SAAAAI?;) seem clumsy. Artificial sentience is another possibility, but sentience is more commonly used for having moral worth by virtue of having phenomenal consciousness and “feelings”. Avoiding those implications seems important for clear discussions of capabilities and x-risk. Agentic AGI might be adequate, but it leaves out the implication of active understanding, contextual awareness and goal-directed learning. But I&#39;m not sure that artificial sapience is the best term, so I&#39;m happy to adopt another term for roughly the same set of concepts if someone has a better idea.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi<guid ispermalink="false"> WqxGB77KyZgQNDoQY</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Fri, 24 Nov 2023 15:13:04 GMT</pubDate> </item><item><title><![CDATA[Insulate your ideas]]></title><description><![CDATA[Published on November 24, 2023 2:08 PM GMT<br/><br/><p> Once tested and proven, insulate good ideas from reality.</p><p></p><p><i>跑步</i></p><p>I recently spoke with a very successful marathon runner who commonly wins races and consistently beats his own times. I asked for his tips, and he shared two:</p><blockquote><p> “The first thing I do is run faster on the uphills. Everyone else slows down because of how difficult it is, and this is a great time to gain ground. The second thing I do is run faster on the downhills. Everyone else slows down because of how fast you go anyways on the slope down, and this is a great time to gain ground.”</p></blockquote><p> I was looking for actual tips and this person (not even concisely) told me it&#39;s just about running faster.</p><p> Before passing judgment too quickly and writing off this feedback, I paused. Amusingly, the point was not to run faster, but more so, that the <strong>uphill and downhill should have no impact on how you think about the run</strong> .</p><p> You know you need to <strong>run fast, consistently</strong> , for the whole marathon.</p><p> This is a central note that cannot possibly be wrong; it is a “good idea,” and it is correct. It&#39;s not overly useful as a tactic, but it is the rejection of the notion that the hill changes anything at all.</p><p> Regardless of these external hill stimuli, the ground truth is identical: <strong>run as fast as you can.</strong></p><p></p><p><i>商业</i></p><p>I listened to a shareholder meeting for a tech company in the heat of the <a href="https://techcrunch.com/2023/05/01/tech-industry-layoffs/">tech recession of 2023</a> . The CFO was being questioned by the shareholders about his plan going into unstable economic conditions.</p><p> We heard questions about imminent layoffs, changes in strategy, refocusing on core business, and the list continues. The CFO (who I&#39;m paraphrasing here) replied with a terse note:</p><blockquote><p> “We will do as we have always done whether it was through the good times or the bad. We will continue with the disciplined deployment of capital, allocating resources to where it is most needed and most likely to generate substantive returns.”</p></blockquote><p> Similar to the runner, I felt indignant at the fact that no real insight was shared. I was wrong again for the exact same reason.</p><p> In the natural cycles of the market, businesses and <strong>executives busy themselves with the minutia of reacting</strong> to market conditions, changing plans, adjusting strategies, and levying layoffs. In the most recent boom through the pandemic, tech companies saw extreme growth and over-hired.</p><p> When things slowed and shareholders questioned productivity, drastic changes were made yet again. Large-scale lay-offs, auxiliary services being cut, and a “year of efficiency”, are all reactionary measures to an ever-changing economic landscape.</p><p> However, in this meeting, the CFO highlighted their <strong>disregard for external stimuli and instead employed a consistently valuable (and diligent) strategy.</strong></p><p> Regardless of these external economic stimuli, the ground truth remains identical: <strong>be disciplined in your deployment of capital.</strong></p><p></p><p><i>想法</i></p><p>Ideas need maturation, and that aging process involves taking an idea and foisting it into reality <strong>where it faces friction</strong> . Weak ideas are brittle and snap, whereas strong ideas come out even stronger.</p><p> Increasingly, the swings of trends and fads are bringing massive changes to ideas and decision-making. Significant time and energy is spent focused on minute optimizations of a new process, <strong>rather than recognizing the timelessness of core ideas.</strong></p><p> A good idea should not fluctuate with the changing weather, but should instead be insulated against it. Inside, behind thick walls and locked doors. Ready to mobilize against the world, heedless to the external stimuli that only exist in the ephemeral territory of the immediate.</p><p>快跑。遵守纪律。 <strong>Good ideas are good: now insulate them.</strong></p><br/><br/><a href="https://www.lesswrong.com/posts/ArypKcebwkQXGohdA/insulate-your-ideas#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ArypKcebwkQXGohdA/insulate-your-ideas<guid ispermalink="false"> ArypKcebwkQXGohdA</guid><dc:creator><![CDATA[Logan Kieller]]></dc:creator><pubDate> Fri, 24 Nov 2023 14:08:12 GMT</pubDate> </item><item><title><![CDATA[Bordeaux, Gironde, France – irregular ACX Meetup 2023-12-09]]></title><description><![CDATA[Published on November 24, 2023 11:17 AM GMT<br/><br/><p>今年在波尔多大都会（法国吉伦特省）举办的 Meetups Everywhere 的后续活动。这次是在波尔多市中心。</p><p>日期和时间：2023 年 12 月 9 日星期六 14:00（当地时间）。</p><p>地点：Espalanade Charles de Gaulle（Mériadeck 中部的地上公园），位于喷泉和 Hôtel du Département 之间<a href="https://www.openstreetmap.org/#map=19/44.83732/-0.58601">https://www.openstreetmap.org/#map=19/44.83732/-0.58601</a> ，周六，管理部门应关闭了，所以我们可以使用入口前开放但有屋顶的露台作为下雨时的集合点。</p><p>如果您预计会迟到并希望在我们决定转移方向时收到短信，请向我发送您的电话号码。我会带一个 A4 标牌，上面写着“ACX Meetup”。我通常在陌生人密度高的环境中戴口罩。</p><p>请在 LessWrong 上回复，以便我知道确实有人会来。这将是一个小型聚会，没有固定的主题，所以无论你想讨论什么，我们都可以讨论！</p><p>将结束时间作为最低承诺选项：一个小时后，我们将看到谁想要结束，如果需要在全体聚会结束时快速完成或说任何事情，我们会在那时做。如果有人想留下来，我几乎肯定会多呆至少一个小时，而且我个人会尽量灵活安排整个下午的时间。</p><p>很可能每个人都可以使用英语和法语，我们将根据表达的偏好选择语言。</p><br/><br/> <a href="https://www.lesswrong.com/events/P6FdNiZT4D8fepsNG/bordeaux-gironde-france-irregular-acx-meetup-2023-12-09#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/P6FdNiZT4D8fepsNG/bordeaux-gironde-france-irregular-acx-meetup-2023-12-09<guid ispermalink="false"> P6FdNiZT4D8fepsNG</guid><dc:creator><![CDATA[vi21maobk9vp]]></dc:creator><pubDate> Fri, 24 Nov 2023 11:17:36 GMT</pubDate> </item><item><title><![CDATA[A Question For People Who Believe In God]]></title><description><![CDATA[Published on November 24, 2023 5:22 AM GMT<br/><br/><p>看到这个视频我感到非常惊讶。安德鲁·休伯曼在其中谈到了他如何祈祷“因为它有效”以及“他相信上帝”，因为他无法想象地球上的生命还能有现在的样子。</p><p>无论如何，这让我思考，当有人说他们“相信上帝”时，这是否意味着“我认为存在一种无所不能、无所不在、无所不知的智能的可能性≥50%？”</p><p>在这种情况下“相信”某事意味着什么？ </p><p></p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Z7GVf8nD7SQ&amp;t=241s"><div><iframe src="https://www.youtube.com/embed/Z7GVf8nD7SQ" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><br/><br/> <a href="https://www.lesswrong.com/posts/piavPtGaiAtXQDFLL/a-question-for-people-who-believe-in-god#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/piavPtGaiAtXQDFLL/a-question-for-people-who- believe-in-god<guid ispermalink="false"> piavPtGaiAtXQDFLL</guid><dc:creator><![CDATA[yanni]]></dc:creator><pubDate> Fri, 24 Nov 2023 05:32:35 GMT</pubDate></item></channel></rss>