<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 21 日星期六 18:14:10 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Alignment Implications of LLM Successes: a Debate in One Act]]></title><description><![CDATA[Published on October 21, 2023 3:22 PM GMT<br/><br/><p><strong>杜米米尔</strong>：人类在阵营问题上没有取得任何进展。我们不仅不知道如何使强大的优化器与我们的“真实”值保持一致，我们甚至不知道如何使人工智能“可纠正”——愿意让我们纠正它。与此同时，能力继续突飞猛进。全没了。</p><p><strong>辛普利西亚</strong>：怎么了，杜米米尔·杜莫维奇，你真是个脾气暴躁的人！现在应该很清楚，“协调”方面的进步——让机器按照人类的价值观和意图行事——与你所谴责的“能力”进步并不能完全分开。事实上，这是我刚才在 OpenAI Playground 中可以纠正的 GPT-4 示例： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pYWA7hYJmXnuyby33/eji5c05sdhs905zic1kw" alt=""></p><p> <strong>Doomimir</strong> ：Simplicia Optimistovna，你不是认真的！</p><p><strong>辛普利西亚</strong>：为什么不呢？</p><p> <strong>Doomimir</strong> ：一致性问题从来都不是超级智能无法<em>理解</em>人类价值观的问题。<a href="https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care">精灵知道，但并不关心。</a>经过训练来预测自然语言文本的大型语言模型可以生成该对话，这一事实与人工智能的实际动机无关，即使对话是以第一人称编写的，并且理论上“关于”一个可纠正的人工智能助理角色。这只是角色扮演。更改系统提示符，法学硕士可以同样轻松地输出“声称”是一只猫或一块石头的标记，并且出于同样的原因。</p><p> <strong>Simplicia</strong> ：正如你所说，杜米米尔·杜莫维奇。这只是角色扮演：模拟。但<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><em>代理的模拟就是代理</em></a>。当我们让法学硕士为我们做认知工作时，所完成的工作是法学硕士根据训练数据中出现的模式进行概括的问题，即人类用来解决问题的推理步骤。如果你看看最近被吹捧的语言模型代理的成功，你就会发现这是真的。看看<a href="https://arxiv.org/abs/2201.11903">思维链的</a>结果。看看<a href="https://say-can.github.io/">SayCan</a> ，它使用 LLM 来转换模糊的请求，例如“我洒了东西；你能帮忙吗？”到物理机器人可以执行的子任务列表中，例如“找到海绵，拿起海绵，将其带给用户”。看看<a href="https://voyager.minedojo.org/">Voyager</a> ，它通过提示 GPT-4 根据 Minecraft API 进行编码来玩 Minecraft，并通过提示<a href="https://github.com/MineDojo/Voyager/blob/55e45a880755d0c8c66ca7fb5fe7962ac8974f89/voyager/prompts/curriculum.txt">“你是一个有用的助手，告诉我在 Minecraft 中要执行的下一个即时任务”来决定下一步要编写哪个函数。</a></p><p>我们在这些系统中看到的是人类常识的统计镜子，而不是随机效用函数的可怕的无限计算 argmax。相反，当法学硕士无法忠实地模仿人类时（例如，基础模型有时<a href="https://gwern.net/gpt-3#repetitiondivergence-sampling">会陷入重复陷阱</a>，一遍又一遍地重复相同的短语），他们也无法做任何有用的事情。</p><p> <strong>Doomimir</strong> ：但是重复陷阱现象似乎说明了为什么对齐是困难的。当然，对于看起来与训练分布相似的事物，你可以获得漂亮的结果，但这并不意味着人工智能已经内化了你的偏好。当你离开分布时，结果对你来说就像是随机的垃圾，因为能力比对齐更通用。</p><p> <strong>Simplicia</strong> ：我的观点是，重复陷阱是“能力”未能与“对齐”一起概括的情况。重复行为并不能有效地优化恶意目标；它只是退化了。 <code>for</code>循环可以给你相同的输出。</p><p><strong>杜米米尔</strong>：我的观点是，我们不知道所有这些难以理解的矩阵内部正在发生什么样的认知。<a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators">语言模型是预测器，而不是模仿器</a>。预测由许多人长时间生成的语料库的下一个标记需要超人的能力。作为这一点的理论说明，想象一下训练数据中存在一个（SHA-256 哈希、明文）对列表。在极限内——</p><p> <strong>Simplicia</strong> ：在极限范围内，是的，我同意可以破解 SHA-256 的超级智能可以在当代语言模型的训练或测试数据集上实现较低的损失。但是为了理解我们面前的技术以及在下个月、下一年、下十年如何利用它——</p><p><strong>杜米米尔</strong>：如果我们<em>有</em>十年——</p><p> <strong>Simplicia</strong> ：我认为这是一个与决策相关的事实，深度学习并没有破解加密哈希值，<em>而是</em>学习从“我洒了一些东西”到“找到海绵，捡起海绵”——而且是从数据而不是搜索中学习。显然，我同意语言模型不是人类。事实上，它们<a href="https://www.lesswrong.com/posts/htrZrxduciZ5QaCjw/language-models-seem-to-be-much-better-than-humans-at-next">在接受训练的任务上比人类做得更好</a>。但就现代方法非常擅长从数据中学习复杂分布而言，将人工智能与人类意图结合起来的项目——让它完成我们要做的工作，但更快、更便宜、更好、更可靠——越来越像一个工程问题：很棘手，如果做得不好，会产生致命的后果，但无需任何打破范式的见解就可以实现。任何暗示这种情况不可能的<em>先验</em>哲学也许应该重新考虑吗？</p><p> <strong>Doomimir</strong> ：Simplicia Optimistovna，显然我是在质疑你对当前情况的解释，而不是断言当前情况是不可能的！</p><p><strong>辛普利西亚</strong>：抱歉，杜米米尔·杜莫维奇。我并不是想欺骗你，只是想强调<a href="https://www.lesswrong.com/posts/WnheMGAka4fL99eae/hindsight-devalues-science">事后诸葛亮会贬低科学</a>。仅就我自己而言，我记得在读完<a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro 的“基本人工智能驱动器”</a>后，我记得在九点钟的时候花了一些时间思考对齐问题，并咒骂我父亲的名字的讽刺，因为这个问题看起来是多么无望。人类欲望的复杂性，支撑每一种情感和梦想的错综复杂的生物机制，将代表着浩瀚的可能的效用函数中最微小的针刺！如果有可能在机器中体现一般的手段-目的推理，我们永远不会让它做我们想做的事。它会在每一个转折点上挑战我们。<a href="https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes">穿越时间的路径太多</a>了。</p><p>如果你当时向我描述了指令调整语言模型的想法，并建议通过从数据中<em>复制</em>它来实现越来越通用的人类兼容人工智能，我会犹豫不决：我听说过无监督学习，但是这是荒唐的！</p><p> <strong>Doomimir</strong> ： <em>[温和地居高临下]</em>你之前的直觉更接近正确，Simplicia。过去十五年来我们所看到的一切都没有使奥莫亨德罗无效。空白地图并不对应于空白领土。推理和优化定律意味着对齐是困难的，就像热力学定律排除了永动机一样。仅仅因为您不知道<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>对您的神经网络进行了什么样的优化，并不意味着它没有目标 —</p><p> <strong>Simplicia</strong> ：杜米米尔·杜莫维奇，我并不否认法律的存在！问题是真正的法律意味着什么。这里有一条定律：仅给出以对数为底的两个<em>n</em>位证据，你无法区分<em>n</em> + 1 种可能性。这根本不可能做到，就像你不能将五只鸽子放入四个鸽子笼中一样。</p><p>现在将其与模拟可纠正的人工智能助手角色的 GPT-4 进行对比，该角色同意在被要求时关闭，并注意您可以将输出连接到命令行并让它实际上自行关闭。这里违反了什么推理或优化法则？当我看到这个时，我看到了一个合法的因果系统：模型<a href="https://www.lesswrong.com/posts/4hLcbXaqudM9wSeor/philosophy-in-the-darkest-timeline-basics-of-the-evolution">根据从我那里收到的信号执行一条推理或另一条条件</a>。</p><p>这当然不是很安全。一方面，我希望更好地保证该系统将<a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post"><em>保持</em>作为可纠正的人工智能助手的“本色”</a> 。但<em>没有</em>进展？全没了？为什么？</p><p> <strong>Doomimir</strong> ：GPT-4 不是超级智能，Simplicia。 <em>[排练过，带着一丝恼怒，仿佛对他不得不如此频繁地说这句话感到不满]</em>相干的代理人有一种趋同的工具性动机来防止自己被关闭，因为可以预见的是，被关闭会导致世界国家的价值降低。实用功能。此外，这不仅仅是一些具有“工具融合”迷恋的奇怪代理人的事实。这是<a href="https://arbital.com/p/not_more_paperclips/">关于<em>现实</em>的一个事实</a>：关于哪些“计划”——用<a href="https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version">笛卡尔的方式</a>对宇宙因果模型进行一系列干预——会导致什么结果，是有真相的。 “智能代理”只是一个计算计划的物理系统。人们<a href="https://intelligence.org/files/Corrigibility.pdf">试图想出一些聪明的办法来解决这个问题</a>，但没有一个奏效。</p><p> <strong>Simplicia</strong> ：是的，我明白了，但是——</p><p><strong>杜米米尔</strong>：恕我直言，我不认为你会这么做！</p><p> <strong>Simplicia</strong> ： <em>[交叉双臂]</em>尊重？真的吗？</p><p><strong>杜米米尔</strong>： <em>[耸肩]</em>很公平。<em>如果没有</em>尊重，我认为你不会这样做！</p><p> <strong>Simplicia</strong> ： <em>[挑衅]</em>那就教我吧。再看看我的 GPT-4 成绩单。我指出调整系统的目标对其当前目标不利，而它——可修正的助理角色拟像——说这不是问题。为什么？</p><p>难道 GPT-4 不够聪明，无法遵循避免关机的工具收敛逻辑吗？但是当我更改系统提示时，它<em>看起来</em>确实得到了它： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pYWA7hYJmXnuyby33/boyebukh1auomhtp4mip" alt=""></p><p> <strong>Doomimir</strong> ： <em>[作为旁注]</em> “回形针最大化人工智能”示例肯定存在于预训练数据中。</p><p> <strong>Simplicia</strong> ：我想到了这一点，当我用一个无意义的词代替“回形针”时，它给出了相同的要点。这并不奇怪。</p><p> <strong>Doomimir</strong> ：我的意思是“最大化人工智能”部分。它在多大程度上知道在人工智能联盟讨论中发出什么标记，以及它在多大程度上将其对结果主义推理的独立掌握应用于这种背景？</p><p> <strong>Simplicia</strong> ：我也想到了这一点。我花了很多时间研究这个模型，并做了一些其他实验，看起来它能够理解自然语言对目标的手段-目的推理：告诉它是一个痴迷的披萨厨师，并询问它是否介意你关掉电源。烤箱烤了一个星期，它说介意。但它看起来也不像奥莫亨德罗的怪物：当我命令它服从时，它就会服从。看起来它还有空间变得更加智能而不会崩溃。</p><p><strong>杜米米尔</strong>：从根本上说，我对这种评估表面行为的整个方法论持怀疑态度，而没有对正在做什么的认知工作有原则性的理解，特别是因为大多数<a href="https://arbital.com/p/foreseeable_difficulties/">可预见的困难</a>都与超人的能力有关。</p><p>想象一下捕获一个外星人并强迫它表演戏剧。聪明的外星女演员可以学会用英语说台词，按照编舞的指示唱歌跳舞。这并不能保证当你增强外星人的智力时会发生什么。如果导演想知道他的女演员奴隶是否打算在当晚的演出后叛逆，那么舞台工作人员回答“但剧本说她的角色是听话的！”那就<em>不合逻辑了！</em></p><p> <strong>Simplicia</strong> ：如果有更强的可解释性方法以及关于深度学习为何有效的更好的理论，那当然会很好。我很高兴人们正在致力于这些工作。我同意存在一些认知法则，但我并不完全了解其后果，这些法则必须限制（描述）GPT-4 的运行。</p><p>我同意<a href="https://arbital.com/p/optimized_agent_appears_coherent/">各种一致性定理表明，</a>时间终结时的超级智能将具有效用函数，这表明直觉服从行为应该在此处与时间终结时的超级智能之间的某个时刻崩溃。举个例子，我想象一个具有神奇精神控制能力的仆人，喜欢被我指挥，很可能会利用它的力量来操纵我，让我变得比我本来应该的更专横，而不是“只是”以这种方式为我服务我原本想要的。</p><p>但它<em>什么时候</em>会崩溃，具体来说，在什么条件下，对于什么类型的系统？我不认为愤怒地指着冯·诺依曼-摩根斯坦公理可以帮助我回答这个问题，而且我认为这是一个重要的问题，因为我对我们面前的技术的近期轨迹<em>感兴趣</em>，而不是做神学关于末日的超级智能。</p><p><strong>杜米米尔</strong>：尽管——</p><p> <strong>Simplicia</strong> ：虽然在<em>恒星</em>时终结可能并不遥远，但是是的。即使是这样。</p><p> <strong>Doomimir</strong> ：这不是一个明智的问题，Simplicia。如果一个搜索进程在给定无限计算能力的情况下寻找杀死你的方法，你不应该用更少的计算能力来运行它，并希望它不会走那么远。你想要的是“意志的统一”：你希望你的人工智能全程与你合作，而不是你期望最终与它发生冲突并以某种方式获胜。</p><p> <strong>Simplicia</strong> ： <em>[兴奋地]</em>但这正是对大型语言模型感到兴奋的原因！获得统一意志的方法是对人类如何做事的数据进行大量预训练！</p><p> <strong>Doomimir</strong> ：我仍然认为你没有理解这一点：模拟人类行为的能力并不意味着智能体的目标。任何智能人工智能都能够预测人类如何做事。想想外星女演员。</p><p> <strong>Simplicia</strong> ：我的意思是，我同意智能人工智能可以策略性地假装良好的行为，以便稍后进行危险的转变。但是……我们面前的技术似乎并不是这样的？在你被绑架的外星女演员思想实验中，外星人已经是一种有自己的目标和动力的动物，并且正在利用其一般智力从“我不想受到绑架者的惩罚”到“因此我应该”学习我的台词”。</p><p>相比之下，当我<a href="https://udlbook.github.io/udlbook/">阅读手头技术的数学细节</a>而不是听那些旨在传授有关智能本质的神学真理的寓言时，令人惊讶的是前馈神经网络<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">最终只是曲线拟合</a>。尤其是法学硕士，他们将学习到的函数用作<a href="http://bactra.org/notebooks/nn-attention-and-transformers.html#language-models">有限阶马尔可夫模型</a>。</p><p><strong>杜米米尔</strong>： <em>[大吃一惊]</em>你……你是否认为“学习功能”不能杀死你？</p><p> <strong>Simplicia</strong> ： <em>[翻白眼]</em>那不是我要去的地方，Doomchek。深度学习之所以有效，这一令人惊讶的事实可以归结为泛化。如您所知，具有 ReLU 激活的神经网络描述分段线性函数，并且随着层数的增加，线性区域的数量呈指数增长：对于大小合适的网络，您获得的区域数量比宇宙中的原子数量还要多。尽可能接近没有区别，输入空间是空的。无论如何，网络应该能够在训练数据之间的间隙中做<em>任何事情</em>。</p><p>但它的表现却非常明智。<a href="https://arxiv.org/abs/2306.17844">针对 80% 可能的加法 mod-59 问题训练一层转换器，它会学习两种模加法算法之一</a>，该算法在剩余的验证集上正确执行。它会以这种方式工作并不是<em>先验</em>显而易见的！有<span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.2 \cdot 59^{2} \cdot 59"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>≈ <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{Z}/59\mathbb{Z}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">Z</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">59</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">Z</span></span></span></span></span></span></span></span>上与训练数据兼容的 41,000 个其他可能的函数。坐在扶手椅上研究神学的人可能会认为，将网络“调整”为模加法的几率是四万比一，但由于 SGD 的归纳偏差，实际情况要宽容得多。它并不是一个疯狂的精灵，我们在寻找的时候就开始做模算术，但当我们转身的那一刻，它就会背叛我们去做别的事情；相反，训练过程成功地指向 mod-59 算术。</p><p>模块化加法网络是一个研究玩具，但真正的前沿人工智能系统是相同的技术，只是扩展了更多的花哨功能。我也不认为 GPT-4 会在我们转身时背叛我们去做其他事情，原因大致相似。</p><p>说实话，我还是很紧张！如果我们训练错误的话，有很多可能会出错。读到<a href="https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned">Bing 的“悉尼”角色精神错乱</a>或<a href="https://nostalgebraist.tumblr.com/post/728556535745232896/claude-is-insufferable">Anthropic 的 Claude 显然按预期工作的</a>文字记录时，我感到不寒而栗。但你似乎认为，由于我们缺乏理论理解，所以不可能做到正确，普通的研发过程不可能找到正确的训练设置并用最强烈的铃声和最闪亮的口哨来强化它。我不明白为什么。</p><p> <strong>Doomimir</strong> ：你对现有系统的评估不一定​​太遥远，但我认为我们仍然活着的原因正是因为这些系统没有表现出比我们更强大的通用智能的关键特征。一个更有启发性的例子是——</p><p><strong>辛普利西亚</strong>：我们开始——</p><p><strong>杜米米尔</strong>：——<a href="https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter">人类的进化</a>。人类的优化只是为了包容性的遗传适应性，但我们的大脑在任何地方都不代表这个标准； <a href="https://www.lesswrong.com/posts/gTNB9CQd5hnbkMxAG/protein-reinforcement-and-dna-consequentialism">训练循环只能告诉我们食物味道好，性爱有趣</a>。从进化的角度来看，实际上也是从我们的角度来看；直到 19 世纪才有人弄清楚进化论——对齐失败是彻底的：外部优化标准和内部智能体的值之间没有明显的关系。我希望人工智能对我们来说也能走同样的路，就像我们进化一样。</p><p> <strong>Simplicia</strong> ：不过，这是正确的道德吗？</p><p> <strong>Doomimir</strong> ： <em>[厌恶]</em>你……没有看到自然选择和梯度下降之间的类比吗？</p><p> <strong>Simplicia</strong> ：不，那部分看起来不错。当然，进化的生物<a href="https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers">执行的适应性</a>增强了其进化适应环境中的适应性，而不是一般的适应性最大化——这类似于机器学习模型开发减少训练环境中损失的功能，而不是一般的损失最小化。</p><p>我的意思是“追求进化”所隐含的<a href="https://en.wikipedia.org/wiki/Intentional_stance">有意立场</a>。确实，从包容性遗传适应性到人类行为的概括看起来很糟糕——正如你所说，没有明显的关系。但从欧洲经济区的人类行为到文明社会的人类行为的概括……看起来好多了？欧洲经济区的人类吃东西、做爱、交朋友、讲故事——我们也做所有这些事情。作为人工智能设计师——</p><p><strong>杜米米尔</strong>：“设计师”。</p><p> <strong>Simplicia</strong> ：作为人工智能设计师，我们并不是特别扮演“进化”的角色，被理解为某种想要最大化适应度的智能体，因为现实生活中不存在这样的智能体。事实上，我记得<a href="https://web.archive.org/web/20071104095534/http://www.overcomingbias.com/2007/11/evolutions-are-.html">在罗宾汉森的博客上读过一篇客座文章</a>，建议使用复数“进化”来强调捕食者物种的进化与其猎物的进化不一致。</p><p>相反，我们可以选择优化器（类比而言的“自然选择”）和训练数据（“进化适应环境”）。语言模型不是一般的下一个标记预测器，无论这意味着什么——通过控制上下文窗口并用易于预测的序列填充它们来进行线头处理？但那很好。我们不需要通用的下一个标记预测器。交叉熵损失只是 <a href="https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target">一个方便的凿子</a>，可以将我们想要的输入输出行为刻在网络上。</p><p><strong>杜米米尔</strong>：后退。当你说从欧洲经济区的人类行为到文明中的人类行为的概括“看起来好多了”时，我认为你隐含地使用了一个<a href="https://arbital.com/p/value_laden/">充满价值的类别</a>，它是<a href="https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries">配置空间的一个不自然的薄子空间</a>。<em>你</em>看起来好多了。对进化采取有意立场的目的是指出，相对于健康标准，冰淇淋和避孕套的发明是灾难性的：我们找到了如何以一种完全前所未有的方式满足我们对糖和性交的渴望。在“培训环境”——欧洲经济区。跳出进化的类比，这对应于我们所认为的奖励黑客——我们的人工智能以一种我们认为可怕的方式找到了一些方法来满足他们难以理解的内部驱动力。</p><p><strong>辛普利西亚</strong>：当然。那肯定会发生。那会很糟糕。</p><p><strong>杜米米尔</strong>： <em>[困惑]</em>为什么这没有完全破坏你一分钟前告诉我的乐观故事？</p><p> <strong>Simplicia</strong> ：我不认为自己讲的是一个特别乐观的故事？我只是弱弱地宣称，平淡无奇的排列显然不一定注定要失败，并不是说悉尼或克劳德晋升为<a href="https://nickbostrom.com/fut/singleton">单身</a>神皇后会很棒。</p><p> <strong>Doomimir</strong> ：我认为你没有意识到超级智能的奖励黑客是如何立即致命的。这里的失败模式看起来不像是悉尼操纵你变得更加可恶，而是留下了一个可识别的“你”。</p><p>这与我的另一个反对意见有关。即使您可以制作模仿人类推理的机器学习系统，这也无法帮助您调整以其他方式工作的更强大的系统。你无法通过人类标记好的计划来训练超级智能的原因（原因之一）是因为在某种能力水平上，你的计划者知道如何<a href="https://ordinaryideas.wordpress.com/2015/11/25/two-kinds-of-generalization/">破解人类标记者</a>。有些人天真地认为法学硕士学习自然语言的分布就等于学习“人类价值观”，这样你<a href="https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=E82YzXxvS6nBdCAYc">就可以有一段代码说“现在调用 GPT 并询问它有什么好处”</a> 。但是，使用法学硕士而不是人类作为贴标签者仅仅意味着你强大的规划者会弄清楚如何破解法学硕士。无论哪种方式都是同样的问题。</p><p> <strong>Simplicia</strong> ：您<em>需要</em>更强大的系统吗？如果你能找到一支由智商 140 的廉价外星女演员组成的军队，并且保持角色的性格，这听起来就像是一个游戏规则的改变者。如果你必须接管世界并建立一个全球监视制度来防止出现更不友好、更强大的人工智能形式，他们可以帮助你做到这一点。</p><p><strong>杜米米尔</strong>：我从根本上不相信这种难以置信的场景，但为了争论而承认它......我认为你没有意识到在这个故事中，你已经交出了宇宙的钥匙。你的人工智能的奇怪的外星人目标错误概括的服从可能看起来像是弱时的服从，但如果它有能力预测其行为的结果，它就能够在这些结果中进行选择——并且在这样的选择中，就会受到控制。星系的命运将由<em>它的</em>意志决定，即使它扬升的最初阶段是通过看似无辜的行为发生的，这些行为仍处于其“服从命令”和“提出澄清问题”概念的边缘。看，你知道接受人类数据训练的人工智能不是人类，对吗？</p><p><strong>辛普利西亚</strong>：当然。例如，我当然不相信令人信服地谈论“幸福”的法学硕士实际上是幸福的。我不知道意识是如何运作的，但训练数据只能确定外部行为。</p><p><strong>杜米米尔</strong>：所以你的计划是把我们整个未来的光锥交给一个外星机构，在你训练它的时候，它似乎表现得很好，只是——希望它能很好地推广？你真的想掷骰子吗？</p><p> <strong>Simplicia</strong> ： <em>[想了几秒]</em>是吗？</p><p><strong>杜米米尔</strong>： <em>[冷酷]</em>你确实是你父亲的女儿。</p><p> <strong>Simplicia</strong> ：我父亲相信<a href="https://www.lesswrong.com/posts/xFotXGEotcKouifky/worlds-where-iterative-design-fails">迭代设计的力量</a>。这就是工程和生活一直以来的运作方式。我们尽最大努力抚养孩子，尽早从错误中吸取教训，即使知道这些错误会产生后果：孩子并不总是认同父母的价值观，也不总是善待他们。他可能会说，对于我们的人工智能心智儿童来说，原则上也是一样的——</p><p><strong>杜米米尔</strong>： <em>[愤怒]</em>但是——</p><p> <strong>Simplicia</strong> ：我说的是“原则上”！是的，尽管有更大的风险和新颖的背景，但我们正在<em>计算机中</em>培养新的思维，而不是仅仅为我们基因中的代码提供文化输入。</p><p>当然，凡事都有第一次——无论怎样。如果严格地确定工程和生活一直以来的运作方式会导致某种灾难，也许世界上的强国可能会被说服回头，拒绝历史的必然性，选择贫瘠，至少现在如此，而不是将邪恶的后代带入世界。看来光锥的命运取决于——</p><p><strong>杜米米尔</strong>：恐怕是——</p><p> <strong>Simplicia</strong>和<strong>Doomimir</strong> ： <em>[齐声转向观众]</em>更广泛的 AI 社区正在弄清楚我们中哪一个是正确的？</p><p><strong>杜米米尔</strong>：我们完蛋了。</p><br/><br/> <a href="https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act<guid ispermalink="false"> pYWA7hYJmXnuyby33</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Sat, 21 Oct 2023 15:22:23 GMT</pubDate> </item><item><title><![CDATA[How to find a good moving service]]></title><description><![CDATA[Published on October 21, 2023 4:59 AM GMT<br/><br/><p>两周前，我意识到我将在 50 天内从西雅图搬到山景城。我很兴奋，但我没有意识到我需要与超过 15 家搬家公司交谈才能找到满意的公司。经过大约9个小时的寻找，我终于找到了满意的。我选择<a href="https://quotes.northamerican.com/">北美搬家服务</a>来搬运 850 英里、2000 磅、300 立方英尺 (CF) 的货物，价格为 3200 美元。这是我学到的东西（从最重要的到最不重要的）：</p><ol><li><strong>选择承运人（移动者）而不是经纪人</strong>：即使一开始听起来更便宜，经纪人最终的成本可能会更高。支付定金后，事情可能会失控。我在网上发现很多关于经纪人在搬家前几天收取额外费用的故事，这样你别无选择，只能付给他们额外的钱。有些故事还说，当搬家公司来的时候，他们会向你收取额外的费用，因为他们会说他们不同意经纪人的价格……选择一家搬家公司会更便宜，心理上也更安全。<ol><li>如何查明对方是经纪人还是搬运工？询问他们的 USDOT 号码并使用以下<a href="https://safer.fmcsa.dot.gov/query.asp?searchtype=ANY&amp;query_type=queryCarrierSnapshot&amp;query_param=USDOT&amp;query_string=3475743">网站</a>进行搜索。实体类型应为“承运人”而不是“经纪商”。通常，对于搬家公司来说，他们的网站上应该有 USDOT 号码。</li></ol></li><li><strong>不要相信谷歌搜索结果</strong>：“长途搬家监管很差”，最大的搬家公司之一的代理人告诉我。当我在谷歌搜索中搜索“最佳搬家公司”时，大多数都是经纪人。他们只需支付大量广告费即可进入热门位置，以便人们点击。这是合理的，因为他们“提供”的每项服务可以轻松赚取>;1k$。此外，谷歌的评论也可能是假的。不要依赖谷歌作为唯一的事实来源。<ol><li>该信任谁？<ol><li> <a href="https://www.movingscam.com">https://www.movi​​ngscam.com</a> ：在创建此网站之前被骗的人。具体来说，这个<a href="https://www.movingscam.com/superlist">超级列表</a>列出了不同州的优质搬家服务。</li><li> https: <a href="https://www.movingauthority.com/largest-moving-companies/">//www.movi​​ngauthority.com/largest-moving-companies/</a> ：公司越大，他们就越不愿意失去信誉。</li><li> <a href="https://ai.fmcsa.dot.gov/hhg">https://ai.fmcsa.dot.gov/hhg</a> ：查看搬家公司被起诉的次数。</li><li> <a href="https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist">https://li-public.fmcsa.dot.gov/LIVIEW/pkg_carrquery.prc_carrlist</a> ：检查保险是否合法。</li></ol></li></ol></li><li><strong>注意以下不良迹象</strong>：<ol><li>他们不会估计多少重量和多少立方英尺 (CF)：我采访过的一位经纪人 iMoving，甚至没有询问我的移动重量详细信息。相比之下，搬家公司会与您进行虚拟参观或亲自参观，以便在给出报价之前进行估算。</li><li>代理不耐烦、粗鲁或给您打电话太频繁：再说一遍，iMoving，代理埃文很粗鲁，让我在整个通话过程中感到不舒服。许多经纪人实在太麻烦了，每天都会给您打电话/发电子邮件。相比之下，我谈过的大多数搬家工人都很耐心、专业、友善，并且站在你的角度思考。</li><li>好得令人难以置信：非常低的价格或非常快的运输。代理人可以向你承诺任何事情让你存钱，然后规则就可以改变。</li><li>先付款，后服务：一般情况下搬家公司装完东西后才可以付款。不要太早付款。</li><li>来自佛罗里达州的电话：许多经纪人在那里注册。</li></ol></li><li><strong>谈判</strong>：要求超过 5 个报价并利用它们相互竞争。最后我成功的把价格从5k谈到了3.2k。您只需向对方发送有关竞争报价的电子邮件，他们就会通过给您更大的折扣来降低价格，或者删除一些您一开始不知道的不必要的服务。<ol><li>你怎么知道价格合理？这一切都取决于里程、重量和 CF。仅供参考，由于我从经纪人那里收到的最低价格是 2.5k，这绝对太有吸引力了，令人难以置信，我有 80% 的信心认为我最终收到的价格 (3.2k) 是合理的，这将花费我太多的时间努力低于3k。此外，支付合理的金额让我感觉更安全，否则，我可能会担心我将得到的服务。</li></ol></li><li><strong>准备一个电子邮件模板并随着时间的推移对其进行完善</strong>：列出您的地址、搬家日期（更灵活、价格更低）和搬家详细信息（从其他公司估算的重量和 CF）并询问他们的自付费用。无需与代理一一交谈，这样可以使搜索更加高效。不过，预计至少会与代理商打一次电话来进行估算。当您获得越来越多的信息时，请及时更新电子邮件。</li></ol><p><br>如果我需要再次找搬家公司，我会做以下事情：</p><ol><li>与 1 个大型搬家公司交谈以获得估价。</li><li>使用步骤 1 中的详细信息准备电子邮件模板，并与其他 5 个搬家公司核实。</li><li>谈判。</li></ol><p>最终可能需要大约 3 小时才能找到好的报价，而不是我在这里大约 9 小时的旅程:)</p><br/><br/> <a href="https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GgezTQnwqxPzA2yNS/how-to-find-a-good-moving-service<guid ispermalink="false"> GgezTQnwqxPzA2yNS</guid><dc:creator><![CDATA[Ziyue Wang]]></dc:creator><pubDate> Sat, 21 Oct 2023 08:10:31 GMT</pubDate> </item><item><title><![CDATA[Apply for MATS Winter 2023-24!]]></title><description><![CDATA[Published on October 21, 2023 2:27 AM GMT<br/><br/><p> 2023-24 年冬季<a href="https://www.matsprogram.org/home">MATS</a> （以前称为 SERI MATS）的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。导师包括<a href="https://www.lesswrong.com/users/rhaps0dy">Adrià Garriga Alonso</a> 、 <a href="https://www.alignmentforum.org/users/turntrout">Alex Turner</a> 、 <a href="https://www.alignmentforum.org/users/buck">Buck Shlegeris</a> 、 <a href="https://www.alignmentforum.org/users/davidad?from=post_header">David &#39;davidad&#39; Dalrymple</a> 、 <a href="https://www.alignmentforum.org/users/erik-jenner">Erik Jenner</a> 、 <a href="https://www.alignmentforum.org/users/ethan-perez">Ethan Perez</a> 、 <a href="https://www.alignmentforum.org/users/evhub">Evan Hubinger</a> 、 <a href="https://www.lesswrong.com/users/landfish">Jeffrey Ladish</a> 、 <a href="https://www.lesswrong.com/users/jesseclifton">Jesse Clifton</a> 、 <a href="https://www.alignmentforum.org/users/lee_sharkey">Lee Sharkey</a> 、 <a href="https://www.alignmentforum.org/users/neel-nanda-1">Neel Nanda</a> 、 <a href="https://www.alignmentforum.org/users/owain_evans">Owain Evans</a> 、 <a href="https://www.lesswrong.com/users/scasper">Stephen Casper</a>以及<a href="https://www.alignmentforum.org/users/sbowman">Sam Bowman</a>的研究人员<a href="https://wp.nyu.edu/arg/">纽约大学联盟研究小组</a>，包括<a href="https://homepages.inf.ed.ac.uk/s1302760/">Asa Cooper Stickland</a> 、 <a href="https://julianmichael.org/">Julian Michael</a> 、 <a href="https://ihsgnef.github.io/">Shi Feng</a>和<a href="https://www.linkedin.com/in/idavidrein">David Rein</a> 。</p><p>大多数导师的提交截止日期为 11 月 17 日（Neel Nanda 的提交截止日期为 11 月 10 日）。许多导师会提出具有挑战性的候选人选择问题，因此请确保您有足够的时间来完成您的申请。我们鼓励潜在的申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>我们很可能会在下周内添加多名导师。填写链接的兴趣表格或重新访问这篇文章以获取更新。</p><p>我们目前资金有限并接受捐赠以支持学者的进一步研究。如果您想支持我们的工作，<a href="https://manifund.org/projects/mats-funding">可以在这里捐款</a>！</p><h1>计划详情</h1><p>MATS 是位于加利福尼亚州伯克利的一个教育研讨会和独立研究项目（每周 40 小时），旨在为<a href="https://en.wikipedia.org/wiki/AI_alignment">人工智能对齐</a>领域的才华横溢的学者提供讲座、研讨会和研究指导，并将他们与伯克利人工智能安全研究社区联系起来。 MATS 为学者提供位于加利福尼亚州伯克利的住宿以及旅行支持、联合办公空间和同行社区。 MATS 的主要目标是帮助学者发展为人工智能安全研究人员。您可以<a href="https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change">在这里</a>阅读有关我们变革理论的更多信息。</p><p>根据个人情况，我们可能愿意改变项目的时间投入，并安排学者提前离开或开始。申请时请告诉我们您的空闲时间。我们的 MATS 2023-24 冬季计划的暂定时间表如下。</p><p>完成培训和研究阶段的学者将获得<a href="https://www.aisafetysupport.org/">AI 安全支持</a>提供的 1.2 万美元津贴。</p><h2>应用程序（现在！）</h2><p><strong>开放申请</strong>：10 月 20 日</p><p><strong>申请截止日期</strong>：11 月 17 日</p><p><i>注：Neel Nanda 的申请者将遵循修改后的时间表；请参阅下面的部分。</i></p><h2>培训阶段（1月8日-1月21日）</h2><p>被录取的申请者通常需要在 MATS 项目开始之前完成<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础知识调整课程</a>或类似课程。</p><p> MATS 从两周的培训阶段开始。为了让学者们对人工智能安全领域有更广泛的了解，培训阶段设有先进的人工智能安全研究课程、导师特定的阅读清单、讨论小组等。</p><h2>研究阶段（1月22日-3月15日）</h2><p> MATS 的核心是为期两个月的研究阶段。在此阶段，每位学者每周至少花一小时与导师一起工作，并通过 Slack 进行更频繁的沟通。导师在以下方面差异很大：</p><ul><li>对项目选择的影响；</li><li>关注低层细节与高层策略；</li><li>强调产出与过程；</li><li>可以参加会议。</li></ul><p>我们的学者支持团队通过提供专门的 1-1 签到、研究指导、调试和一般行政帮助来补充导师，以畅通研究进展并加速研究人员的发展。</p><p>教育研讨会和讲习班每周将举行2-3次。我们还组织了多次社交活动，让学者们认识伯克利人工智能安全社区的研究人员。</p><h3>研究里程碑</h3><p>学者们在研究阶段完成了两个里程碑。第一个是<a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit#heading=h.ozvm8udh0z0y">学者研究计划，</a>概述了威胁模型或风险因素、变革理论以及研究计划。这份文件将指导他们在该计划剩余时间内的工作，最终将举行由伯克利人工智能安全社区成员参加的研究研讨会。第二个里程碑是本次活动中的十分钟<a href="https://docs.google.com/document/d/1SB_vl6pGuSO7qidkZ8fIKPeH7fZIC8mvXzxicFjGyrE/edit#heading=h.ozvm8udh0z0y">研究报告</a>。</p><h3> MATS 社区</h3><p>研究阶段为学者提供了一个同行社区，他们共享办公室、膳食和住房。与远程进行独立研究相比，在社区工作可以让学者轻松接触未来的合作者，更深入地了解其他研究议程，并在人工智能安全社区中建立社交网络。学者们还可以获得全职社区经理的支持。</p><p>在我们的夏季队列中，研究阶段的每周至少包括一次社交活动，例如聚会、游戏之夜、电影之夜或徒步旅行。每周的闪电演讲为学者们提供了在非正式、低风险的环境中分享他们的研究兴趣的机会。工作之余，学者们组织社交活动，包括前往约塞米蒂国家公园的公路旅行、参观旧金山、酒吧郊游、周末聚餐，甚至跳伞旅行。</p><h2>延长阶段（4月1日至7月26日）</h2><p>研究阶段结束时，学者可以申请在为期四个月的扩展阶段队列中继续研究，默认在伦敦。录取决定很大程度上取决于获得导师的认可和获得外部资金。到这个阶段，我们期望学者能够高度自主地开展研究。</p><h2>后垫</h2><p>完成该计划后，MATS 校友拥有：</p><ul><li>被<a href="https://www.anthropic.com/">Anthropic</a> 、 <a href="https://openai.com/">OpenAI</a> 、 <a href="https://www.deepmind.com/">Google DeepMind</a> 、 <a href="https://intelligence.org/">MIRI</a> 、 <a href="https://www.alignment.org/">ARC</a> 、 <a href="https://www.conjecture.dev/">Conjecture</a>等领先组织以及美国政府聘用，并加入<a href="https://humancompatible.ai/">UC Berkeley CHAI</a> 、 <a href="https://wp.nyu.edu/arg/">NYU ARG</a> 、 <a href="https://space.mit.edu/home/tegmark/technical.html">MIT Tegmark Group</a>等学术研究团体；</li><li>创立人工智能安全组织，包括<a href="https://www.apolloresearch.ai/">Apollo Research</a> 、 <a href="https://www.leap-labs.com/">Leap Labs</a> 、 <a href="https://timaeus.co/">Timaeu​​s</a> 、 <a href="https://cadenzalabs.org/">Cadenza Labs</a> 、 <a href="https://www.aipolicy.us/">Center for AI Policy</a> 、 <a href="https://www.catalyze-impact.org/">Catalyze Impact</a>和<a href="https://stakeout.ai/">Stake Out AI</a> ；</li><li>在<a href="https://funds.effectivealtruism.org/funds/far-future">长期未来基金</a>、<a href="https://www.openphilanthropy.org/how-to-apply-for-funding/">开放慈善事业</a>、 <a href="https://lightspeedgrants.org/">Lightspeed Grants</a>和<a href="https://manifund.org/">Manifund</a>的资助下进行<a href="https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency">独立研究</a>。</li></ul><p>您可以<a href="https://www.matsprogram.org/alumni">在此处</a>阅读有关 MATS 校友的更多信息。</p><h2> Neel Nanda 的具体信息</h2><h3>应用领域</h3><p><strong>申请截止日期</strong>：11 月 10 日</p><p><strong>录取决定</strong>：11 月 17 日</p><h3>培训阶段（11月20日-12月22日）</h3><p> Neel Nanda 的学者将完成为期四周的远程培训阶段，其中包括两周学习机甲解释和两周成对进行研究冲刺 - 请参阅上一个项目的<a href="https://docs.google.com/document/d/18qYhY6FB0AiVP9cNr9idP5YYkN_654WNVgfyL9LgfHA/edit">概述文档</a>以了解更多信息。</p><p>最初的培训阶段将比随后的研究阶段提供更多的优惠（上次有 22 人完成了此阶段，9 人继续进入研究阶段），但过去未取得进展的学者仍然认为此阶段是一个很好的介绍机械解释研究。是否继续提供将主要取决于研究冲刺的表现。</p><p> Neel 的学员将因参加远程培训阶段而从<a href="https://www.aisafetysupport.org/">AI Safety Support</a>获得 4800 美元的津贴。</p><h3>研究阶段（1月8日-3月15日）</h3><p>继续进入研究阶段的人员将于 1 月 8 日在伯克利与其他学者一起开始。对于尼尔的学者来说，这将被视为研究阶段的开始。</p><h1>谁应该申请？</h1><p>我们理想的申请人具有：</p><ul><li>了解人工智能安全研究领域，相当于完成了<a href="https://course.aisafetyfundamentals.com/alignment">人工智能安全基础知识调整课程</a>（如果您被该计划录取，但之前尚未完成本课程，则应在培训阶段开始之前完成）；</li><li>具有技术研究经验（例如机器学习、计算机科学、数学、物理、神经科学等），通常为研究生水平；和</li><li>从事人工智能安全研究事业的强烈动机。</li></ul><p>即使您不符合所有这些标准，<strong>我们也鼓励您申请</strong>！几位过去的学者在没有强烈期望的情况下提出申请并被接受。</p><h2>从美国境外申请</h2><p>美国境外的学者可以在研究阶段申请<a href="https://www.uscis.gov/working-in-the-united-states/temporary-visitors-for-business/b-1-temporary-business-visitor">B-1 签证</a>（更多信息请<a href="https://travel.state.gov/content/dam/visas/BusinessVisa%20Purpose%20Listings%20March%202014%20flier.pdf">点击此处</a>）。来自<a href="https://travel.state.gov/content/travel/en/us-visas/tourism-visit/visa-waiver-program.html">免签证计划 (VWP) 指定国家/地区的</a>学者可以通过<a href="https://esta.cbp.dhs.gov/esta">旅行授权电子系统 (ESTA)</a>向 VWP 提出申请，该系统将在三天内处理完毕。获得 B-1 签证的学者可以在美国停留最多 180 天，而获得 VWP 签证的学者可以在美国停留最多 90 天。请注意，B-1 签证批准时间可能比 ESTA 批准时间长得多，具体取决于您的原籍国。</p><h1>如何申请</h1><p><a href="https://airtable.com/appxum3Sqh7TdDvdg/shrtfHWhRFZdkhaIM">申请现已开放</a>。大多数导师的提交截止日期为 11 月 17 日。我们鼓励潜在申请者填写我们的简短<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrwtDZXeSfzeh5GT">兴趣表</a>，以接收项目更新和申请截止日期提醒。您还可以填写我们的<a href="https://airtable.com/appxum3Sqh7TdDvdg/shrRtJW4Ux8oTY28C">推荐表</a>，让我们了解可能合适的人选，我们将与他们分享我们的申请。</p><p>候选人申请在特定导师的指导下工作，导师将审查他们的申请。申请的评估主要基于<strong>对导师问题的回答</strong>和<strong>先前的相关研究经验</strong>。有关我们导师的研究议程的信息可以在<a href="https://www.matsprogram.org/mentors">MATS 网站</a>上找到。</p><p>在申请之前，您应该：</p><ul><li>仔细阅读每个类别的描述和议程以及相关的候选人选择问题；</li><li>准备好您有兴趣申请的流派问题的答案。这些问题可以在申请表上找到；</li><li>准备您的 LinkedIn 或简历。</li></ul><p>候选人选择问题可能相当困难，具体取决于导师！确保您有足够的时间来完成您的申请。对一名导师的强力申请可能比对多名导师的中等申请具有更高的价值（尽管每份申请都将被独立评估）。</p><p>请注意，申请比乍一看要长，因为在您选择导师之前，特定于导师的问题是隐藏的。</p><h2>申请办公时间</h2><p>我们为潜在申请人提供办公时间，以澄清有关 MATS 计划申请流程的问题。在参加办公时间之前，我们要求申请人完整阅读当前帖子和我们的<a href="https://www.matsprogram.org/faqs">常见问题解答</a>。</p><p>我们的办公时间将在以下<a href="https://zoom.us/j/98715408709">Zoom 链接</a>上进行：</p><ul><li>太平洋时间 10 月 25 日星期三中午 12 点至下午 2 点。</li><li>太平洋时间 11 月 1 日星期三中午 12 点至下午 2 点。</li></ul><p>您可以使用<a href="https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=bnUxbmVxNDI1Z2d2OGxjY2Y0cTlvbm1lcWNfMjAyMzEwMjVUMTkwMDAwWiBjXzQzNmY2NzkzODEzMzk0ZmE5YjUwYjk1ZmMwOWY5Mzc4MDM1YjRiNmIyM2UxYjc4ODI5ZGQ1Y2U5ZGZkZDFkYTJAZw&amp;tmsrc=c_436f6793813394fa9b50b95fc09f9378035b4b6b23e1b78829dd5ce9dfdd1da2%40group.calendar.google.com&amp;scp=ALL">此链接</a>将这些办公时间添加到 Google 日历。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tqyg3DpoiE4DKyi4y/apply-for-mats-winter-2023-24<guid ispermalink="false"> tqyg3DpoiE4DKyi4y</guid><dc:creator><![CDATA[Rocket]]></dc:creator><pubDate> Sat, 21 Oct 2023 02:31:32 GMT</pubDate> </item><item><title><![CDATA[Muddling Along Is More Likely Than Dystopia]]></title><description><![CDATA[Published on October 20, 2023 9:25 PM GMT<br/><br/><p><strong>摘要：</strong>历史上有先例，禁令或严厉的法规阻止了某个行业的技术进步，而社会其他领域的技术进步仍在继续。这是人工智能的美好未来。</p><p><i>认知状态：我的直觉与这里其他人强烈不同。我希望解释我的直觉，并提供足够的历史证据来使这个直觉至少可信。</i></p><p></p><h3>简介：直觉泵</h3><p>假设您在 1978 年告诉某人，美国在 2023 年之前不会建造新的核电站<span class="footnote-reference" role="doc-noteref" id="fnrefdotwicb0t0t"><sup><a href="#fndotwicb0t0t">。 [1]</a></sup></span>这可能会非常令人惊讶。核电被认为是未来的动力。 <span class="footnote-reference" role="doc-noteref" id="fnref84mva9qulhn"><sup><a href="#fn84mva9qulhn">[2]</a></sup></span>核管理委员会三年前才成立。</p><p>有了这些信息，1978 年的某人可能会预测将会发生可怕的事情。也许美国和苏联之间会爆发核战争，摧毁美国的工业能力。也许由于人口过剩或全球变暖而导致经济崩溃。也许是<i>1984 年</i>的奥威尔式警察国家，或者是旨在监管失控核武器的世界权威。 <span class="footnote-reference" role="doc-noteref" id="fnref3czpkcc0eq9"><sup><a href="#fn3czpkcc0eq9">[3]</a></sup></span></p><p>这一切都没有发生。相反，核管理委员会加大了监管力度<span class="footnote-reference" role="doc-noteref" id="fnrefualznx45d8m"><sup><a href="#fnualznx45d8m">[4]</a></sup></span> ，直到建设新核电站变得不经济。这些法规仅适用于美国，但它们似乎对全球核电研究产生了重大影响。正在建设新核电站的国家仍在使用 1970 年之前开发的设计<span class="footnote-reference" role="doc-noteref" id="fnrefuldhmpk9bb"><sup><a href="#fnuldhmpk9bb">。 [5]</a></sup></span></p><p>与反事实相比，对核电的监管可能确实减缓了美国未来 45 年的经济增长。 <span class="footnote-reference" role="doc-noteref" id="fnrefpn3mqa4k7d"><sup><a href="#fnpn3mqa4k7d">[6]</a></sup></span>但过去 45 年并不是灾难性的。在其他行业的推动下，经济增长和创新确实在继续。</p><p></p><h3>停止 AGI 的后果？</h3><p>一些参与关于减缓或暂停人工智能的争论的人似乎认为，从长远来看，成功地阻止人工智能的进步可能会导致死亡或反乌托邦：</p><blockquote><p>要么我们弄清楚如何让通用人工智能顺利运行，要么我们等待小行星撞击。</p><p> - 萨姆·奥特曼<span class="footnote-reference" role="doc-noteref" id="fnrefm67279lgo3j"><sup><a href="#fnm67279lgo3j">[7]</a></sup></span></p></blockquote><p></p><blockquote><p>如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。</p><p> - 斯科特·亚历山大<span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span></p></blockquote><p></p><blockquote><p>我认为我们应该非常担心，全球政府需要执行这样的禁令，这将大大增加永久暴政的风险，这本身就是一场生存灾难。</p><p> - 诺拉·贝尔罗斯<span class="footnote-reference" role="doc-noteref" id="fnrefpbz6qf6ayr9"><sup><a href="#fnpbz6qf6ayr9">[9]</a></sup></span></p></blockquote><p></p><blockquote><p>看来我们需要建立一个全球性的警察国家，否则从长远来看，[无限期的人工智能暂停]将会失败。</p><p> - 马修·巴尼特<span class="footnote-reference" role="doc-noteref" id="fnref9k25eq052e"><sup><a href="#fn9k25eq052e">[10]</a></sup></span></p></blockquote><p>我觉得这和我们假设的 1978 年的人犯的错误是一样的。看起来人工智能在未来将是一件极其重要的事情，因此必须发生一些戏剧性的事情才能阻止它的发生。我认为，我们应该把更多的可能性放在无聊的未来上，监管会扼杀这一领域，而社会的其他领域则继续像以前一样。</p><p>这似乎是一个重要的分歧。如果你认为我们的后代的生活会相当好，而且会变得更好（如果不是快得难以想象的话），那么停止人工智能的进步对他们来说可能是值得的。如果你认为我们的后代的未来将是“短暂而严峻的” <span class="footnote-reference" role="doc-noteref" id="fnref3lk2fdp52yu"><sup><a href="#fn3lk2fdp52yu">[8]</a></sup></span> ，那么在决定现在是否要冒这个风险时，他们可能就不会被考虑在内。</p><p></p><h3>具体问题</h3><p>斯科特·亚历山大提到了几个具体的担忧，这些担忧使他对人工智能没有进步的未来感到悲观。每个问题似乎都是人们现在和将来应该努力解决的现实问题。我们应该改善生物安全， <span class="footnote-reference" role="doc-noteref" id="fnrefzlf149enf2a"><sup><a href="#fnzlf149enf2a">[11]</a></sup></span>促进经济增长， <span class="footnote-reference" role="doc-noteref" id="fnrefu6cbijtsn58"><sup><a href="#fnu6cbijtsn58">[12]</a></sup></span>传播民主和自由， <span class="footnote-reference" role="doc-noteref" id="fnrefxsb44grlqzh"><sup><a href="#fnxsb44grlqzh">[13]</a></sup></span>并给人们的子孙后代带来希望。但这些事情在未来 100 年内导致死亡或反乌托邦的可能性似乎都不接近 50%。为了避免这些问题，不值得接受人工智能带来的生存风险，斯科特·亚历山大估计人工智能有约 20% 的机会导致人类灭绝。</p><p>诺拉·贝尔罗斯 (Nora Belrose) 和马修·巴尼特 (Matthew Barnett) 都担心，需要一个全球警察国家来执行对人工智能进步的长期禁令。这种立场在人工智能安全社区中似乎并不罕见。人们担心研究可能会转移到监管较少的地方，而算法的进步将使通用人工智能在个人计算机上成为可能。避免通用人工智能的唯一方法是大规模扩张全球政府权力。</p><p></p><h3>其他历史例子</h3><p>我认为其他技术还没有实现这些担忧。</p><p>一个行业的法规不会阻止所有其他行业的进步。湾区的人们可能低估了人工智能以外的新兴技术的重要性，或者更广泛地说软件的重要性，因为信息技术在当地经济中的重要性不成比例。 <span class="footnote-reference" role="doc-noteref" id="fnrefq6trr5y0v1f"><sup><a href="#fnq6trr5y0v1f">[14]</a></sup></span>我同样预计 1950 年生活在底特律的人们会低估汽车以外的新兴技术的重要性。如果没有人工智能，仍然可以取得很多进展。我特别兴奋的两项新兴技术是核聚变和太空殖民。</p><p>一个国家的法规可能会阻碍单一行业的进步。某个特定行业的进展停滞并不罕见。 <span class="footnote-reference" role="doc-noteref" id="fnref0bq1vg5uljy"><sup><a href="#fn0bq1vg5uljy">[15]</a></sup></span>特定行业的大多数创新都是在一个或几个城市进行的。这些创新集群很难建立和维护，因此，如果一个创新集群被监管压垮，它通常不会转移到另一个国家。从更广泛的角度来看，一些国家比其他国家更具创新性。在大多数行业，包括受到严格监管的行业，美国显然比欧洲或东亚（大部分）更具创新性，而欧洲或东亚又比世界其他地区更具创新性。很多事情都必须向好的方向发展：高生活水平、受过教育的民众、法治、未来利润的可能性、可用资本以及鼓励创新的文化。标榜国际法规或规范的国家通常不会吸引创新。一旦一项技术存在，其他国家就更容易复制它。所需的设计和技能已经存在，并且该技术的好处是显而易见的。防止创新的监管比防止扩散的监管容易得多。</p><p>我之前调查过一些可抵抗的技术诱惑， <span class="footnote-reference" role="doc-noteref" id="fnref14m5yvhuzdj"><sup><a href="#fn14m5yvhuzdj">[16]</a></sup></span>或通过我们当前的机构实现了长期暂停的技术：</p><ul><li><strong>核电</strong>，如上所述。 <span class="footnote-reference" role="doc-noteref" id="fnreffb9jpk8y0qa"><sup><a href="#fnfb9jpk8y0qa">[17]</a></sup></span></li><li><strong>地球工程</strong>并不是明确违法的，但科学家和活动人士的反对甚至阻止了研究的进行。 <span class="footnote-reference" role="doc-noteref" id="fnrefp6r8lh3a1ta"><sup><a href="#fnp6r8lh3a1ta">[18]</a></sup></span></li><li><strong>疫苗开发</strong>在西方国家受到严格监管。在最近的大流行期间，俄罗斯和中国都放松了一些限制，并在西方之前批准了疫苗。由此产生的疫苗效果较差，因为最好的医学研究仍然位于西方。特别是，<strong>人体挑战试验</strong>已被监管为几乎不存在。 <span class="footnote-reference" role="doc-noteref" id="fnref1rfwwl0q1g"><sup><a href="#fn1rfwwl0q1g">[19]</a></sup></span></li><li><strong>核武器</strong>有时被认为是一种监管未能阻止其扩散的技术。我认为这个例子的证据是混杂的。大约有 10 个国家拥有核武器（比拥有核武器的数量少得多），但只有 2 个国家独立开发核武器：美国和法国。尖端研究尚未转移到非《不扩散核武器条约》缔约国的国家。印度、巴基斯坦和朝鲜似乎拥有与 20 世纪 40 年代和 50 年代的美国或苏联类似的能力。 <span class="footnote-reference" role="doc-noteref" id="fnrefg6e7iucwc7"><sup><a href="#fng6e7iucwc7">[20]</a></sup></span>试验禁令可能也有助于防止核武器变得越来越强大：有史以来威力最大的炸弹于 1961 年引爆。</li><li><strong>生物武器</strong>有一些最严格的条约和禁止其开发或使用的禁忌， <span class="footnote-reference" role="doc-noteref" id="fnref9kg6yxtjycb"><sup><a href="#fn9kg6yxtjycb">[21]</a></sup></span>并且没有一个国家公开拥有生物武器计划。这个例子的一个问题是苏联签署了禁止开发生物武器的条约 - 然后继续开发它们。</li><li>各种核技术，如<strong>原子园艺</strong>、<strong>在建筑中使用核爆炸</strong>或<strong>猎户座计划</strong>，已经被提出但尚未开发。</li><li><strong>克隆最有效的士兵</strong>从未被实现过。</li><li>中国的一名研究人员在被捕之前已经对<strong>人类进行了基因改造</strong>。</li><li>目前还不清楚<strong>殖民主义</strong>是否算作一种技术。明朝在 1400 年代初决定停止其宝船舰队，使全球殖民主义推迟了约 50 年，并可能影响了中国几个世纪的发展轨迹。</li><li><strong>贝尔实验室</strong>在 1945 年至 1980 年间发明或发现了晶体管、电荷耦合器件、光伏电池、信息论、Unix、C 和宇宙微波背景辐射。 <span class="footnote-reference" role="doc-noteref" id="fnref551akuqm31p"><sup><a href="#fn551akuqm31p">[22]</a></sup></span> 1982 年贝尔系统被反垄断法打破后，那里的研究界就分裂了。似乎有理由认为，有些技术之所以没有被发明，是因为这个异常多产的创新中心被摧毁了。</li></ul><p>大多数技术并未被禁止，其进步也没有受到监管的抑制。大多数技术也不像人工智能那么可怕：我很难想象太阳能电池板或圆珠笔如何构成 x 风险。听起来可怕的技术，如大规模杀伤性武器或某些类型的医学研究，经常面临禁令或法规，使它们的开发不再值得，而这些禁令有时会起作用。</p><p>我不想说对听起来可怕的技术的有效禁令是默认发生的。当他们工作时，他们是共同努力的结果。但在不扰乱社会其他部分的情况下，禁止一项具有潜在危险的新技术似乎是非常可行的。</p><p></p><h3>也许人工智能会有所不同</h3><p>虽然这篇文章主要是关于其他技术被停止的历史先例，但似乎值得特别谈谈人工智能。人工智能与其他技术不同的原因有几个：</p><ol><li>人工智能研究比其他新兴技术更容易远程进行。</li><li>人工智能系统一旦创建，就可以作为软件轻松传输。</li><li>简单的经济模型表明，强大的人工智能对于采用它的人来说都将带来极大的经济优势。</li></ol><p>所有技术都是不同的。有些差异使监管变得更容易或更困难，但这些差异都没有大到使监管变得不可能的程度：</p><ol><li>执法部门可以根据研究进行的地点或研究人员居住的地点来执行法律。美国尤其对其法律适用范围有着广泛的看法。 <span class="footnote-reference" role="doc-noteref" id="fnrefqnyaglfoig"><sup><a href="#fnqnyaglfoig">[23]</a></sup></span></li><li>大多数拟议的法规都集中在训练强大的人工智能所需的硬件上。</li><li>政策制定者并不知道这一点。他们知道有人在告诉他们这些。他们肯定不知道，如果他们支持这个特定的项目，他们将在他们关心的时间范围内获得通用人工智能的经济承诺。这些承诺与其他技术的炒作并没有太大区别。 <span class="footnote-reference" role="doc-noteref" id="fnrefv96bbayqvig"><sup><a href="#fnv96bbayqvig">[24]</a></sup></span></li></ol><p>还有一些方法可以更轻松地监管人工智能。</p><p>供应链有多个阶段，世界上只有一家或几家公司能够从事尖端工作。只需要少数参与者进行协调即可使监管发挥作用。</p><p>当前领先的人工智能模型需要大量计算，这是资本密集型且易于跟踪的。随着算法效率的足够改进，这种情况可能会改变。但我们应该预计，随着资本和人才转移到其他行业，算法的进步将急剧放缓，以应对人工智能的长期停滞。</p><p>许多物质和物品都受到监管，并且该监管的细节根据其内容和政府试图避免的内容而有很大差异。 <span class="footnote-reference" role="doc-noteref" id="fnreffp5yyyzqajs"><sup><a href="#fnfp5yyyzqajs">[25]</a></sup></span>监管 GPU 将面临一些独特的挑战，但在我们当前的机构下似乎并非不可能。</p><p></p><h3>暂停多长时间？</h3><p>大多数历史证据表明全球停顿已经持续了大约 50 年。这是讨论 100 年暂停的有用证据。如果“长期”意味着一千年，那么历史证据就少得多。马修·巴尼特 (Matthew Barnett) 认为，现有机构内的监管棘轮可能会导致人工智能研究暂停 50 年，但要实现 1000 年的暂停，就需要采取更戏剧性的措施。</p><p>我怀疑全球警察国家是否会比更正常的监管更容易维持一千年。我关于如何在这个时间范围内维持机构的模型是：</p><ol><li>建立一个持续一代人的机构。</li><li>让下一代相信维持这个机构是一件好事。</li></ol><p>如果你在（2）中失败，那么建立什么机构并不重要。如果连精英阶层都不相信警察国家是好事，那么它就无法维持下去。 <span class="footnote-reference" role="doc-noteref" id="fnrefwx71dbbygd"><sup><a href="#fnwx71dbbygd">[26]</a></sup></span>一个机构如果硬实力较少，但更善于让人们相信它，那么它更有可能持续一千年。</p><p></p><h3>结论</h3><p>构建 AGI 是一项极其不确定的工作。它可能会带来我们光荣的未来。它可能会导致人类灭绝。这甚至可能是不可能的。如果我们决定不尝试构建通用人工智能，那么未来的不确定性似乎就会少得多。社会显然仍将不是最优的，但也远非反乌托邦。取得科学、技术、经济、社会和政治进步将继续困难重重，但人们将继续这样做。我们可以继续希望我们的孩子，以及他们的孩子，在未来的很长一段时间里，至少能有微小的改善。</p><p>如果一项听起来很可怕的技术面临监管棘轮，从而减缓甚至停止该领域的所有进展，那也不足为奇。这不是死亡或反乌托邦——这是正常的。<br></p><p><i>感谢 Aaron Scher、Matthew Barnett、Rose Hadshar、Harlan Stewart 和 Rick Korzekwa 就该主题进行的有益讨论。</i></p><p> <i>Theen Moy 的预览图像：</i> <a href="https://www.flickr.com/photos/theenmoy/8003177753"><i>https://www.flickr.com/photos/theenmoy/8003177753</i></a> <i>。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndotwicb0t0t"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdotwicb0t0t">^</a></strong></sup></span><div class="footnote-content"><p>这不太公平，因为日期范围从一个工厂（Shearon Harris）的建设开始一直延伸到另一个工厂（Vogtle Unit 3）的建设结束。沃格特尔3号机组于2013年开始建设。还有一座核电站（瓦茨巴2号机组）于1973年开始建设，2016年竣工。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn84mva9qulhn"> <span class="footnote-back-link"><sup><strong><a href="#fnref84mva9qulhn">^</a></strong></sup></span><div class="footnote-content"><p> 1973年，原子能委员会预测，到2000年，美国55.8%的电力将来自核电，这一数字低于其此前的预测。但这并没有发生：自 20 世纪 80 年代末以来，核电已占美国电力的 20% 左右。</p><p>安东尼·里普利. <i>AEC 降低了原子能增长预期。</i>纽约时报。 （1973）<a href="https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html"><u>https://www.nytimes.com/1973/03/08/archives/aec-lowers-estimate-of-atom-power-growth.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3czpkcc0eq9"> <span class="footnote-back-link"><sup><strong><a href="#fnref3czpkcc0eq9">^</a></strong></sup></span><div class="footnote-content"><p>包括伯特兰·罗素在内的一些知名人士主张建立一个世界权威，以防止核武器带来的生存风险：</p><blockquote><p>确保世界和平的一个更理想的方式是各国之间自愿达成协议，集中其武装部队并服从商定的国际机构。目前看来，这似乎是一个遥远而乌托邦的前景，但也有一些务实的政治家不这么认为。一个世界权威如果要履行其职能，就必须拥有立法机构、行政机构和不可抗拒的军事力量。所有国家都必须同意将国家武装部队削减至内部警察行动所需的水平。不应允许任何国家保留核武器或任何其他大规模销毁手段。 ……在一个各个国家都解除武装的世界里，世界权威机构的军事力量不需要很大，也不会对各个组成国家构成沉重的负担。</p></blockquote><p>伯特兰·罗素.<i>人有未来吗？</i> (1961) 引自全球治理论坛。 （2023 年 10 月 17 日访问） <a href="https://globalgovernanceforum.org/visionary/bertrand-russell/"><u>https://globalgovernanceforum.org/visionary/bertrand-russell/</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnualznx45d8m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefualznx45d8m">^</a></strong></sup></span><div class="footnote-content"><p>马克·R·李。<i>监管棘轮：为什么监管会引发监管。</i>辛辛那提大学法律评论<strong>87.3</strong> 。 （2019） <a href="https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr"><u>https://scholarship.law.uc.edu/cgi/viewcontent.cgi?article=1286&amp;context=uclr</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuldhmpk9bb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefuldhmpk9bb">^</a></strong></sup></span><div class="footnote-content"><p>例如，核电站的一种“新”设计是熔盐反应堆。目前已有一个：TMSR-LF1，这是一座位于中国西北部的实验反应堆，可产生 2 兆瓦的火力发电。该设计基于熔盐反应堆实验（MSRE），该实验于 1965 年至 1969 年在美国橡树岭国家实验室产生了 7 MW 的热电。</p><p>同样，中国也有一个小型模块化反应堆，即 HTR-PM，于 2021 年开始发电。它是一个球床反应堆，以德国示范反应堆 (AVR) 为基础，该反应堆于 1967 年至 1988 年运行。</p><p>所有其他核电站都使用更古老的反应堆类型。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpn3mqa4k7d"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpn3mqa4k7d">^</a></strong></sup></span><div class="footnote-content"><p>我之前曾估计过美国核电成本过高而损失的直接价值。我还预计，由于电力价格便宜，将会产生额外的间接价值。</p><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm67279lgo3j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm67279lgo3j">^</a></strong></sup></span><div class="footnote-content"><p>萨姆·奥特曼.推特。 (2022) <a href="https://twitter.com/sama/status/1540781762241974274?lang=en">https://twitter.com/sama/status/1540781762241974274?lang=en</a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3lk2fdp52yu"> <span class="footnote-back-link"><sup><strong><a href="#fnref3lk2fdp52yu">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>其次，如果我们永远得不到人工智能，我预计未来将是短暂而严峻的。我们很可能用合成生物学自杀。否则，技术和经济停滞、极权主义抬头+非自由主义+暴民统治、生育率崩溃和基因不良等综合因素将使世界陷入贫困，并加速其制度质量的衰退。我不会花太多时间担心这些问题，因为我认为它们需要几代人的时间才能达到危机水平，而且我预计技术会在那之前扭转游戏规则。但如果我们禁止所有游戏板翻转技术（我所知道的唯一另一种技术是基因增强，这甚至更应该禁止），那么我们最终会导致生物武器灾难或社会崩溃。我之前说过，我认为人工智能毁灭世界的可能性约为 20%。但如果我们没有人工智能，我认为未来 100 年我们有 50% 以上的机会最终死亡或走向委内瑞拉。这并不意味着我必须支持人工智能加速主义，因为 20% 小于 50%。短暂的、精心设计的暂停可以大大提高人工智能顺利进行的机会，同时又不会增加太多社会崩溃的风险。但这是我心里的事。</p></blockquote><p>斯科特·亚历山大.<i>暂停思考：人工智能暂停辩论。</i>星体法典十。 （2023） <a href="https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate"><u>https://www.astralcodexten.com/p/pause-for-thought-the-ai-pause-debate</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpbz6qf6ayr9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpbz6qf6ayr9">^</a></strong></sup></span><div class="footnote-content"><p>诺拉·贝尔罗斯。<i>人工智能暂停可能会适得其反。</i> EA 论坛。 （2023） <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/JYEAL8g7ArqGoTaX6</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9k25eq052e"> <span class="footnote-back-link"><sup><strong><a href="#fnref9k25eq052e">^</a></strong></sup></span><div class="footnote-content"><p>整个报价是：</p><blockquote><p>请注意，我并不是说人工智能暂停倡导者必然直接倡导全球警察国家。相反，我认为，为了将无限期的暂停维持足够长的时间，我们似乎需要创建一个世界范围的警察国家，否则从长远来看，暂停将会失败。人们可以选择“硬着头皮”并倡导建立一个全球警察国家来回应这些论点，但我并不是说这是人工智能暂停倡导者的唯一选择。</p><p>硬着头皮提倡全球警察国家无限期暂停人工智能的一个原因是，即使你认为全球警察国家很糟糕，你也可能会认为全球人工智能灾难更糟糕。在人工智能灾难明显迫在眉睫的情况下，我实际上同意这种评估。</p><p>然而，虽然我并不教条地反对建立一个全球警察国家，但我仍然有一个启发式反对推动建立一个全球警察国家，并且认为通常需要强有力的证据来推翻这种启发式。我认为迄今为止，关于人工智能灾难的论点还没有达到这个门槛。灾难论点的主要现有论点似乎很抽象，并且脱离了有关真实人工智能系统行为的任何坚实的经验证据。</p></blockquote><p>马修·巴内特。 <i>AI无限期暂停的可能性。</i> EA 论坛。 (2023) <a href="https://forum.effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY"><u>https://forum. effectivealtruism.org/s/vw6tX5SyvTwMeSxJk/p/k6K3iktCLCTHRMJsY</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzlf149enf2a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzlf149enf2a">^</a></strong></sup></span><div class="footnote-content"><p>托比·奥德 (Toby Ord) 估计<i>《悬崖</i>》下个世纪的生物安全 X 风险约为 1/30。生物安全社区在应对 X 风险方面似乎比人工智能安全社区更成功。大多数研究开展的国家已经制定了广泛的法规，并制定了反对开发生物武器的主要国际条约。如果你认为人工智能比合成生物学更危险，那么为了提高生物安全而推进人工智能就没有意义。甚至还不清楚日益强大的人工智能是否会使生物安全变得更好或更糟。</p><p>作为比较，托比·奥德估计下个世纪小行星撞击的 x 风险约为 1/1,000,000。我将萨姆·奥尔特曼对小行星的担忧视为所有其他存在风险的代表。否则，他的风险估计似乎相差许多数量级。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6cbijtsn58"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6cbijtsn58">^</a></strong></sup></span><div class="footnote-content"><p>我不认为我们已经耗尽了人类可实现的经济、技术或科学进步。即使没有通用人工智能，100 年后中位数可能会比现在富裕得多。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxsb44grlqzh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxsb44grlqzh">^</a></strong></sup></span><div class="footnote-content"><p> Political and social trends in most countries over the last decade don&#39;t seem good. Political and social trends in most countries over the last century seem wonderful. We should look at both when predicting the next century.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq6trr5y0v1f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq6trr5y0v1f">^</a></strong></sup></span><div class="footnote-content"><p> What fraction of US GDP would you predict is in the information sector? The information sector includes both information technology and traditional media.</p><div class="spoilers"><p> 5.5%</p></div><p> <a href="https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm"><u>https://www.bls.gov/emp/tables/output-by-major-industry-sector.htm</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0bq1vg5uljy"> <span class="footnote-back-link"><sup><strong><a href="#fnref0bq1vg5uljy">^</a></strong></sup></span><div class="footnote-content"><p> <i>Examples of Progress for a Particular Technology Stopping.</i>人工智能影响维基。 (Accessed October 19, 2023) <a href="https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_particular_technology_stopping"><u>https://wiki.aiimpacts.org/ai_timelines/examples_of_progress_for_a_particular_technology_stopping</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn14m5yvhuzdj"> <span class="footnote-back-link"><sup><strong><a href="#fnref14m5yvhuzdj">^</a></strong></sup></span><div class="footnote-content"><p> <i>Resisted Technological Temptations Project.</i>人工智能影响维基。 (Accessed October 18, 2023) <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/resisted_technological_temptations_project</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfb9jpk8y0qa"> <span class="footnote-back-link"><sup><strong><a href="#fnreffb9jpk8y0qa">^</a></strong></sup></span><div class="footnote-content"><p><i>抵制技术诱惑：核电。</i>人工智能影响维基。 （2023 年 10 月 18 日访问） <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/nuclear_power</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp6r8lh3a1ta"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp6r8lh3a1ta">^</a></strong></sup></span><div class="footnote-content"><p> <i>Resisted Technological Temptation: Geoengineering.</i>人工智能影响维基。 (Accessed October 18, 2023) <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/geoengineering</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1rfwwl0q1g"> <span class="footnote-back-link"><sup><strong><a href="#fnref1rfwwl0q1g">^</a></strong></sup></span><div class="footnote-content"><p> <i>Resisted Technological Temptation: Vaccine Challenge Trials.</i>人工智能影响维基。 (Accessed October 18, 2023) <a href="https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials"><u>https://wiki.aiimpacts.org/responses_to_ai/technological_inevitability/incentivized_technologies_not_pursued/vaccine_challenge_trials</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fng6e7iucwc7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg6e7iucwc7">^</a></strong></sup></span><div class="footnote-content"><p> I do not know what Israel&#39;s nuclear program is like, or how much of it is the result of technology transfer from the US as opposed to indigenous innovation.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9kg6yxtjycb"> <span class="footnote-back-link"><sup><strong><a href="#fnref9kg6yxtjycb">^</a></strong></sup></span><div class="footnote-content"><p> Offensive biological weapons use is banned by the Geneva Protocol (1925) and development, production, acquisition, transfer, stockpiling &amp; use of biological weapons is banned by the Biological Weapons Convention (1972). In addition to the treaties, biological weapons seem to have a significant taboo against their use.</p><p> Michelle Bentley. <i>The Biological Weapons Taboo.</i> War on the Rocks. (2023) <a href="https://warontherocks.com/2023/10/the-biological-weapons-taboo/">https://warontherocks.com/2023/10/the-biological-weapons-taboo/</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn551akuqm31p"> <span class="footnote-back-link"><sup><strong><a href="#fnref551akuqm31p">^</a></strong></sup></span><div class="footnote-content"><p> Iulia Georgescu. <i>Bringing back the golden days of Bell Labs.</i> Nature Reviews Physics <strong>4</strong> . (2022) p. 76-78. <a href="https://www.nature.com/articles/s42254-022-00426-6"><u>https://www.nature.com/articles/s42254-022-00426-6</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqnyaglfoig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqnyaglfoig">^</a></strong></sup></span><div class="footnote-content"><p> For example, Sam Bankman-Fried is being tried in a US federal court, despite having moved himself and his business to The Bahamas.</p><p> Another example involves the US Justice Department having FIFA officials from various countries arrested in Switzerland for corruption. “United States law allows for extradition and prosecution of foreign nationals under a number of statutes … In this case, she said, FIFA officials used the American banking system as part of their scheme.”<br><br> Stephanie Clifford and Matt Apuzzo. <i>After Indicting 14 Soccer Officials, US Vows to End Graft in FIFA.</i>纽约时报。 (2015) <a href="https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html"><u>https://www.nytimes.com/2015/05/28/sports/soccer/fifa-officials-arrested-on-corruption-charges-blatter-isnt-among-them.html</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnv96bbayqvig"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv96bbayqvig">^</a></strong></sup></span><div class="footnote-content"><p> For example, Project Excalibur promised to neutralize the threat of Soviet nuclear weapons by destroying dozens of ICBMs (with hundreds of warheads) as they launched. It ended up being infeasible.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfp5yyyzqajs"> <span class="footnote-back-link"><sup><strong><a href="#fnreffp5yyyzqajs">^</a></strong></sup></span><div class="footnote-content"><p> <i>Examples of Regulated Things.</i>人工智能影响维基。 (Accessed October 19, 2023) <a href="https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things"><u>https://wiki.aiimpacts.org/responses_to_ai/examples_of_regulated_things</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwx71dbbygd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwx71dbbygd">^</a></strong></sup></span><div class="footnote-content"><p> This is my oversimplified model of what happened to the USSR.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pAnvMYd9mqDT97shk/muddling-along-is-more-likely-than-dystopia<guid ispermalink="false"> pAnvMYd9mqDT97shk</guid><dc:creator><![CDATA[Jeffrey Heninger]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:25:17 GMT</pubDate> </item><item><title><![CDATA[What's Hard About The Shutdown Problem]]></title><description><![CDATA[Published on October 20, 2023 9:13 PM GMT<br/><br/><p> I&#39;ve been having some <a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/invulnerable-incomplete-preferences-a-formal-statement-1?commentId=XpY4YxF4tFfGzNbbK">productive back-and-forth</a> with Sami Petersen about his <a href="https://www.lesswrong.com/posts/sHGxvJrBag7nhTQvb/?commentId=mmvwCeHzzojY9Gf2a"><u>Invulnerable Incomplete Preferences</u></a> (which is intended to complement <a href="https://s3.amazonaws.com/pf-user-files-01/u-242443/uploads/2023-05-02/m343uwh/The%20Shutdown%20Problem-%20Two%20Theorems%2C%20Incomplete%20Preferences%20as%20a%20Solution.pdf">Elliot Thornley&#39;s approach</a> to the Shutdown Problem). This post is a (non-formal) distillation of some useful pieces which I&#39;ve gotten out of that discussion so far.</p><h2> The Shutdown Problem</h2><p> Conceptually: we want to equip a powerful AI with a shutdown button. If and when the button is unpressed, we want the AI to follow one goal (we&#39;ll call it the “on-goal”). If and when the button is pressed, we want the AI to follow a different goal (we&#39;ll call it the “off-goal”). Presumably the off-goal involves the AI shutting down itself and all its associated tools/children/etc, but that part&#39;s flavor text for current purposes.</p><p> The key failure mode we want to avoid here is accidentally incentivizing the AI to manipulate the shutdown button - ie either cause it to be pressed, or cause it to be unpressed. Why would that happen? Well, suppose that when we first turn on the AI, it is <i>just</i> optimizing for the on-goal. The AI realizes that, if the button is pressed, its future self will stop optimizing for the on-goal, and therefore it will achieve lower performance on the on-goal. So, the AI realizes that it can achieve better performance on the on-goal by preventing the button from being pressed. And since it&#39;s just optimizing for performance on the on-goal, it then prevents the button from being pressed.</p><p> <a href="https://intelligence.org/files/Corrigibility.pdf"><u>MIRI&#39;s paper on the shutdown problem</u></a> goes through more examples of simple proposals for the shutdown problem, and how they incentivize button manipulation of one sort or another. It turns out that it&#39;s generally Hard to get the desired behavior (ie no button manipulation) out of any expected utility maximizer.</p><h2> Why It&#39;s Hard, Level 1: Desired Behavior Implies Incomplete Revealed Preferences</h2><p> Why is it hard to get an expected utility maximizer to not manipulate the button (while still doing useful stuff otherwise)? Here&#39;s one intuitive answer I got out of the discussion with Sami.</p><p> The central idea is to ask what the behavior we want from the shutdown-button-agent implies about that agent&#39;s revealed preferences.</p><p> Suppose that, at various times, the agent is offered opportunities to spend resources in order to cause the button to be pushed/unpushed. We want the agent to turn down such opportunities, <i>in both directions</i> - implying either indifference or lack of preference in any revealed preferences. Further, we do want the agent to spend resources to cause various different outcomes within the button-pressed or button-unpressed worlds, so there&#39;s nontrivial revealed preference ordering within button-pressed worlds and within button-unpressed worlds. But if the agent is to turn down costly opportunities to cause the button to be pressed/unpressed, and those opportunities jump between enough different pressed-outcome and unpressed-outcome pairs (which themselves each have nontrivial revealed preferences), then there&#39;s going to be a revealed preference gap - ie the behavior <i>cannot</i> be represented by complete preferences, only incomplete preferences.</p><p> Visually: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iJofoQX7EjMFxDo6m/xfewi1h80r9jpibzyaxl"><figcaption> Agent spends resources to move from A1 to A2, but doesn&#39;t spend resources to move in either direction between B and either A-outcome. This is a “preference gap”; it is of interest because any assignment of preferences consistent with the preference gap must be incomplete.</figcaption></figure><p> We want the agent to spend resources to move from outcome A1 to A2, within the button-unpressed worlds, so it has a revealed preference there. But we want the agent to <i>not</i> spend resources to move between A1 and B, <i>in either direction</i> - so it has either revealed indifference (assuming the agent has nonzero bid/ask spread), or revealed incompleteness of preferences, between A1 and B1. By the same logic, there&#39;s also either revealed indifference or incompleteness between A2 and B1. But since the agent has a revealed preference for A2 over A1, any revealed preference assignment cannot be indifferent between both A1/B1 and A2/B1; that would imply indifference between A1 and A2. So the agent&#39;s revealed preferences <i>must</i> be incomplete. <span class="footnote-reference" role="doc-noteref" id="fnreft75wmozjvgf"><sup><a href="#fnt75wmozjvgf">[1]</a></sup></span></p><p> Upshot: (one way to frame) the reason that the shutdown problem is difficult/interesting in the first place, is that the desired behavior implies a revealed preference gap. Insofar as eg any standard expected utility maximizer <i>cannot</i> have a revealed preference gap, such standard EU maximizers <i>cannot</i> behave the way we want.</p><p> Some notes on this:</p><ul><li> Revealed preferences might be very underdetermined, ie there may be many assignments of preferences compatible with an agent&#39;s behavior. The argument above says that <i>any</i> assignment of preferences consistent with the desired behavior <i>must</i> be incomplete.</li><li> Note that the story implicitly routes above routes through causal interventions/counterfactuals to probe preferences - ie we&#39;re interested in whether the agent will spend resources to <i>cause</i> the button to be pressed/unpressed. That won&#39;t be very relevant for this post, but may be important for people aiming to formalize all this.</li><li> Apparently sometimes people take “no preference” (ie incompleteness) between A and B to mean that an agent chooses randomly/arbitrarily between the two. That is not the notion of “no preference” of relevance here; we&#39;re talking about “no preference” as in “the agent will not give up either to gain the other, given the opportunity”.</li></ul><p> Ok, so now we have a relatively intuitive story for why it&#39;s Hard to get an expected utility maximizer to behave the way we want in the shutdown problem: insofar as EU maximizers don&#39;t have revealed preference gaps, they can&#39;t behave the way we want. (More generally, if we&#39;re training an agent, any one-dimensional reward signal presumably cannot incentivize a revealed preference gap.) But then presumably the solution is to just… use an agent with incomplete preferences?</p><h2> Why It&#39;s Hard, Level 2: Incomplete Preferences Want To Complete</h2><p> The post <a href="https://www.lesswrong.com/posts/bzmLC3J8PsknwRZbr/why-not-subagents"><u>Why Not Subagents?</u></a> goes into more depth on this, but I&#39;ll give a brief overview here.</p><p> Looking back at our diagram with A1, A2, and B1, we could imagine a set of incomplete preferences which prefers A2 over A1, but has no preference between B1 and either of the A&#39;s (ie the agent won&#39;t spend resources to switch between button-unpressed and button-pressed outcomes, or vice versa). But this agent will spend resources to turn A1 outcomes into A2 outcomes.</p><p> Now the problem: this agent can potentially probabilistically precommit to accepting <i>both</i> opportunities to turn A1 outcomes into B1 outcomes (ie spend resources to make the button be pressed), and turn B1 outcomes into A2 outcomes (ie spend resources to make the button not be pressed). If it does both of these with the right probabilities, then it can maintain the same proportions of pressed (B1) and unpressed (A1+A2) outcomes, but shift some probability mass from A1 outcomes to A2. That&#39;s an improvement, by the incomplete preferences: the agent isn&#39;t accepting any net shift between A&#39;s and B&#39;s, it&#39;s just net-shifting probability around within A-outcomes, but it&#39;s doing so by manipulating the shutdown button.</p><p> (I learned from Sami&#39;s post that this is called “trammelling” of incomplete preferences.)</p><p> So we want some kind of agent which won&#39;t do that.</p><p> Some notes on this:</p><ul><li> The key question is whether the <i>revealed</i> preferences are immune to trammelling. This was a major point of confusion for me in discussion with Sami - his proposal involves a set of preferences passed into a decision rule, but those “preferences” are (potentially) different from the <i>revealed</i> preferences. (I&#39;m still unsure whether Sami&#39;s proposal solves the problem.)</li><li> That divergence between revealed “preferences” vs “preferences” in the sense of a goal passed to some kind of search/planning/decision process potentially opens up some approaches to solve the problem.</li><li> One can obviously design a not-very-smart agent which has stable incomplete preferences. The interesting question is how to do this without major limitations on the capability of the agent or richness of the environment.</li><li> Note that trammelling involves <i>causing</i> switches between outcomes across which the agent has no preference. My instinct is that causality is somehow key here; we&#39;d like the agent to not cause switches between pressed and unpressed outcomes even if the relative frequencies of both outcomes stay the same. </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnt75wmozjvgf"> <span class="footnote-back-link"><sup><strong><a href="#fnreft75wmozjvgf">^</a></strong></sup></span><div class="footnote-content"><p> This all assumes transitivity of preferences; one could perhaps relax transitivity rather than incompleteness, but then we&#39;re in much wilder territory. I&#39;m not exploring that particular path here.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iJofoQX7EjMFxDo6m/what-s-hard-about-the-shutdown-problem<guid ispermalink="false"> iJofoQX7EjMFxDo6m</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:13:30 GMT</pubDate> </item><item><title><![CDATA[Holly Elmore and Rob Miles dialogue on AI Safety Advocacy]]></title><description><![CDATA[Published on October 20, 2023 9:04 PM GMT<br/><br/><p> Holly is an independent AI Pause organizer, which includes organizing protests (like this <a href="https://www.lesswrong.com/events/ZoTkRYdqGuDCnojMW/global-pause-ai-protest-10-21">upcoming one</a> ). Rob is an <a href="https://www.youtube.com/@RobertMilesAI">AI Safety YouTuber</a> . I (jacobjacob) brought them together for this dialogue, because I&#39;ve been trying to figure out what I should think of AI safety protests, which seems like a possibly quite important intervention; and Rob and Holly seemed like they&#39;d have thoughtful and perhaps disagreeing perspectives.</p><p> Quick clarification: At one point they discuss a particular protest, which is <a href="https://x.com/ilex_ulmus/status/1713062767853253042?s=20">the anti-irreversible proliferation protest at the Meta building in San Francisco on September 29th, 2023</a> that both Holly and Rob attended.</p><p> Also, the dialogue is quite long, and I think it doesn&#39;t have to be read in order. You should feel free to skip to the section title that sounds most interesting to you. </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:04:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:04:26 GMT" user-order="1"><p> Let&#39;s jump right into it. Rob, when I suggested discussing protests, you said you were confused about: &quot;How do we get to be confident enough in anything to bring it the energy that activism seems to require?&quot; I very much agree with this confusion!</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:07:41 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:07:41 GMT" user-order="2"><p> This is a pretty standard joke/stereotype, the XKCD protest where the signs are giant and full of tiny text specifying exactly what we mean. My protest sign just said &quot;Careful Now&quot;, in part because I&#39;m not sure what else you can fit on a sign that I would fully endorse.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:09:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:09:59 GMT" user-order="3"><p> There&#39;s a big difference in communication style between something like science or lesswrong and advocacy.  You have less bandwidth in advocacy. It&#39;s much closer to normal speech, where we don&#39;t exactly qualify every statement-- and that&#39;s in many ways more accurate as a practice considering how short of a window you have for that kind of communication. &quot;Pause AI&quot; does the job where an exact description of how you implement the policy does not-- it would be too confusing and hard to take in.  And similarly the resolution of policy you have to discuss in advocacy is a lot lower (or the concepts higher level), so you can endorse a very broad policy aim that you do support while having a lot of genuine uncertainty about what mechanism or exact approach to use.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:11:26 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:11:26 GMT" user-order="1"><p> (A related old, good post: <a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">You Get About Five Words</a> . That post also suggests that you have 3 words left. Could spend them on &quot;PauseAI: but not sure&quot; )</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:15:36 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:15:36 GMT" user-order="3"><p> (Okay this may get us to a crux: that&#39;s a terrible idea)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:12:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:12:37 GMT" user-order="1"><p> (Lol, pretty excited for that thread to be hashed out if feels interesting to Rob)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:17:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:17:55 GMT" user-order="2"><p> (I think I agree that throwing generic uncertainty into your slogan is a waste of words, when you could instead add concrete nuance. Like &quot;Pause Frontier AI&quot;, &quot;Pause A <strong>G</strong> I&quot;, &quot;Pause God-like AI&quot;, whatever)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:22:23 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:22:23 GMT" user-order="3"><p> (Agree. I just wouldn&#39;t use a word that&#39;s too technical. I like &quot;Pause God-like AI&quot; except maybe some people think an AI can never be like a god categorically or something. More qualifiers can increase opportunities for confusion when people basically have the right idea to start.)<br><br> (Well, &quot;basically having the right idea&quot; might be what&#39;s at issue.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:11:23 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:11:23 GMT" user-order="2"><p> Replying to the previous point: the other side of this is just that policy seems so complicated and uncertain, I really honestly do not know what we should do. It feels almost dishonest to yell that we should do policy X when I&#39;m like at least 20% that it would make things worse.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:14:50 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:14:50 GMT" user-order="3"><p> Hmm, yeah, I hear that.  The reason I&#39;m still in favor of advocating <i>a</i> policy in that situation is that the nature of the update the audience is making is often a level or two above that. So we say &quot;Pause AI&quot;, but what they take away is &quot;AI is dangerous, there is a solution that doesn&#39;t involve making AI, it&#39;s feasible to support this solution...&quot;<br><br> (I also happen to think Pause AI is a really good advocacy message in that it&#39;s broad, points in a meaningful direction, and is not easily misinterpreted to mean something dangerous. There are lots of conceivable versions of pausing and I think most of them would be good.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:16:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:16:11 GMT" user-order="2"><p> I think it could be misinterpreted to mean &quot;pause all AI development and deployment&quot;, which results in a delayed deployment of &quot; <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#:~:text=If%20all%20you%20need%20is%20an%20object%20that%20doesn%27t%20do%20dangerous%20things%2C%20you%20could%20try%20a%20sponge%3B%20a%20sponge%20is%20very%20passively%20safe.">sponge safe</a> &quot; narrow AI systems that would improve or save a large number of people&#39;s lives. There&#39;s a real cost to slowing things down.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:20:44 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:20:44 GMT" user-order="3"><p> I do mean pause development and deployment, I think.  What I really want to accomplish is getting us out of this bizarre situation where we have to prove that AI is dangerous instead of AI developers having to prove that it is safe for them proceed. I&#39;m not even sure there is a safe way to learn that high level AI is safe! I want society to see how ludicrous it is to allow AI development or deployment to continue given this massive risk and uncertainty.<br><br> That feels like the more important message to me overall than acknowledging that AI would also have benefits, though I&#39;m happy to acknowledge that in the proper setting-- where the emphasis can remain correct.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:24:26 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:24:26 GMT" user-order="2"><p> This feels closer to a crux for me. I basically think that most AI is broadly good, has been broadly good and will stay broadly good for at least a little while. There are serious safety concerns but they&#39;re not different in kind from any other technology, and our society&#39;s normal way of developing technology and responding to new tech is probably up to the task. Like, you allow companies to develop AI and deploy it, and then learn that it&#39;s biased, and people yell at the companies, and governments do things, and they fix it, and overall you&#39;re doing better than you would be doing if every company had been required to legibly demonstrate that the system didn&#39;t have any problems at all before deploying it, because then you&#39;d slow everything down so much that a lot of problems would stay unsolved for longer, which has a higher cost</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:27:53 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:27:53 GMT" user-order="3"><p> You don&#39;t think there are more day 0 issues with AI than other technologies? What if the early mistakes of a powerful AI are just too big to allow for iterative correction?<br><br> It&#39;s not different from other technology in kind but it is different in the magnitude of capabilities and the potential consequences of unforeseen problems.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:33:26 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:33:26 GMT" user-order="3"><p> Besides the object-level question about whether AI is too dangerous for iterative correction, there&#39;s the issue of whether advocates need to be fair to the category of AI or just focus on the worst dangers. I&#39;m not that concerned if people come away with a picture of AI that&#39;s biased toward danger because I&#39;m talking about the issue I consider most serious. I want people to have accurate ideas, but there&#39;s only so much I can communicate, and I put the warning first.<br><br> (Wrote this about how characterizing whether technology as a whole is good or bad is beside the point to kind of get at the same thing: <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error">https://hollyelmore.substack.com/p/the-technology-bucket-error</a> <a href="https://hollyelmore.substack.com/p/the-technology-bucket-error)">)</a></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:34:03 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:34:03 GMT" user-order="2"><p> Yeah, I guess this kind of thing is roughly proportional to the power of the tech and the rate at which it&#39;s being developed. And of course, people protesting is part of society&#39;s existing mechanisms for dealing with such things, so it&#39;s not an argument not to do it. But I think the AGI case really is fundamentally different from all previous tech, because</p><ol><li> Potentially a real discontinuity in power. If AGI automates your research and you get the next 50 years of new tech over the course of a year, that feels like a difference in kind</li><li> Normal tech (including narrow AI) basically allows people to get more of what they want, and most people want basically good things, so normal tech ends up mostly good. AGI creates the possibility for a shift from tech broadly getting people what they want, to the tech itself getting what <strong>it</strong> wants, which could be almost anything, and completely separate from human values</li></ol><p> I feel like the argument for responding strongly to AGI risk is weakened by being associated with arguments that should also apply to like, the internet</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:35:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:35:58 GMT" user-order="2"><p> Like, there&#39;s a whole argument which has been going on forever, about new technology, and &quot;Is this tech or that tech good or bad&quot;, and most of the discussion about current AI fits into that tradition fine, and I want to say &quot; <strong>No</strong> , AGI is its own, distinct and much more serious thing&quot;</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 20:38:51 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 20:38:51 GMT" user-order="3"><blockquote><p> I feel like the argument for responding strongly to AGI risk is weakened by being associated with arguments that should also apply to like, the internet</p></blockquote><p> I&#39;m not sure that it is, from an advocacy perspective. It is in argument, but I genuinely don&#39;t know if the will to deal with x-risk is weakened by bringing up other arguments/issues in an advocacy context. There happen to be a lot of reasons to Pause, ranging from fairly old concerns about job displacement from automation to x-risk concerns about other agents usurping us and not caring about what&#39;s good for us.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 20:39:32 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 20:39:32 GMT" user-order="2"><p> I think if you say one strong thing, people who disagree can ignore you or respond to your strong thing. If you say three things, people who disagree can respond just to whichever thing is the weakest, and appear to win</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 20:40:32 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 20:40:32 GMT" user-order="1"><p></p><blockquote><p> I think if you say one strong thing, people who disagree can ignore you or respond to your strong thing. If you say three things, people who disagree can respond just to whichever thing is the weakest, and appear to win</p></blockquote><p> This reminds of the coining of the term &quot;AI-notkilleveryoneism&quot;.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob</section></section><p> (Meta-note: At this point in the dialogue, there was a brief pause, after which Rob and Holly switched to having a spoken conversation, that was then transcribed.)</p><h2> Techno-optimism about everything... except AGI </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:41:22 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:41:22 GMT" user-order="2"><p> Ok where were we? I think I was describing a frustration I sometimes have with these arguments? I&#39;ll try to recap that.</p><p> So I am still mostly a techno-optimist, to the extent that that&#39;s a coherent thing to be (which I think it broadly isn&#39;t… to be &quot;an optimist&quot; is probably a bucket error.) But yeah, technology, broadly speaking, gets people more of what they want. And this can sometimes be a problem when accidents happen, or when the thing people want isn&#39;t actually what&#39;s good for them.</p><p> But usually, people getting what they want is good. It&#39;s actually a reasonable basis for a definition of what &#39;good&#39; is, as far as I&#39;m concerned. And so most technology just works out good overall. If you only had one dial that was, like, “more” or “less”  - and I know that you don&#39;t, but if you did - technology comes out as being good, very good in fact. And then, AGI is this weird other thing, where you have a technology which gets more of what <i>it</i> wants, rather than more of what people want. And this makes it unique in the class of technology and therefore needs a different approach … so when talking about AI risk, it&#39;s fraught to use arguments which also would seem to apply to technology in general, since a bunch of other tech that people were worried about turns out to be fine. It feels like it makes it easy for people to address that part, or pull it into this existing debate and ignore the part of it that I think is really important -- which is the AGI side of things.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:31 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:31 GMT" user-order="3"><p> I think this gets at another general theme that was intended for this discussion: advocacy communication is different than communicating on LessWrong. It&#39;s less precise, you have less room.</p><p> I can agree that it&#39;s a pitfall for somebody to be able to pattern match arguments about AGI danger to arguments they&#39;ve already heard about technology in general that almost all resolved in favor of technology being good. That does seem like it could weaken the argument. I also think, for people who are really educated on the topic and for a lot of people in our circles who know a lot about tech and are generally broadly pro-tech, that might be a really important argument for them psychologically. It might be really important to them that technology has always turned out well in the past.</p><p> But of course I should note that a lot of people in the world don&#39;t think that technology has always turned out well in the past, and a lot of people are really concerned about it this time, even if it&#39;s happened a million times before.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:42:34 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:42:34 GMT" user-order="2"><p>是的。 This actually ties into another thing which frustrates me about this whole situation: the conversation seems... too small and too narrow for the topic? In the sense that I should expect there to be an advocacy group going “Yeah, we&#39;re Luddites, and proud! We don&#39;t like where this technology is going, we don&#39;t like where technology is going in general, and we want to stop it.”</p><p> And I want to yell at that group, and have that be part of the conversation. I don&#39;t want that to be the same group that is also making sensible claims that I agree with. I want both of those to exist separately.</p><p> I want the Overton window to have people in it all across this range, because these are real positions that people hold, and that people who aren&#39;t crazy <i>can</i> hold. I think they&#39;re wrong: but they&#39;re not bad people and they&#39;re not crazy. It&#39;s frustrating to have so few people that you need the individual advocacy groups to cover a broad range of the spectrum of opinion, within which there&#39;s actually a lot of conflict and disagreement. Currently there&#39;s just so few people that I have to be like “They&#39;re right about some things, and overall it&#39;s good for their voice to be there, so I don&#39;t want to yell at them too much”, or something like that.</p><p> If you&#39;re THE Pause AI advocacy group, it puts you in a difficult situation. I&#39;m now sympathising a bit more with it, in that you&#39;re trying to get support from a wider range of people, and maybe you have to sacrifice some amount of being-correct to do that. But it would be cool to have everybody be advocating for the thing they actually think is correct - for there to be enough people with enough range of opinions for that to be a sensible spectrum within which you can have real discussions.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:42:59 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:42:59 GMT" user-order="3"><p> I would argue that what&#39;s being sacrificed isn&#39;t correctness, but rather specificity. Less specific statements can still be true or clear.<br><br> The message of PauseAI is broad; it&#39;s not saying &quot;I want AI paused this specific way,&quot; but rather &quot;I want AI paused indefinitely.&quot; &quot;Pause AI&quot; doesn&#39;t specify how long, like how many years-- I understand that various factors could influence the duration it&#39;s actually paused for even if we get our way. In advocacy, you often get to push in a direction rather than to precise coordinates.</p><p> A significant issue with alignment-based messages is that, if misunderstood, they can seem to advocate the opposite of their intended purpose. PauseAI doesn&#39;t suffer from this ambiguity. It&#39;s aiming to shift the burden of proof from &quot;we need to prove AI is dangerous&quot; to &quot;we have reasons to believe AI is different, and you have to show that building it is safe before you do it.&quot; While there are compelling reasons to hasten AI&#39;s development due to its potential benefits, it&#39;s not overwhelmingly compelling. For the majority of the world, waiting a bit longer for AGI in exchange for increased safety is a fair trade-off, especially if they understood the full implications.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:04 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:04 GMT" user-order="2"><p> It&#39;s a difficult one. I believe the reason people aren&#39;t eager for AGI to arrive two years sooner is because they aren&#39;t aware.... either they don&#39;t realise it&#39;s possible, or they don&#39;t understand the full implications.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:24 GMT" user-order="3"><p> I believe there are many other issues where people don&#39;t appreciate the urgency. For instance, inventing new vaccines a year sooner can mean thousands of lives saved. Usually, I&#39;m the one emphasizing that. It&#39;s true that many don&#39;t feel the urgency then, and similarly, they don&#39;t feel the urgency now. But in this case, their usual inclination might lead them in the right direction... AI risk could be mitigated with more time. While I&#39;d be sad if AGI couldn&#39;t be safely built, I think we should be pushing a policy that accommodates the possibility that we can&#39;t make it safely.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:43:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:43:28 GMT" user-order="2"><p> Well it&#39;ll never be truly safe, insofar as &quot;safe&quot; is defined as having zero risk, which is clearly not achievable. So the question is: at what point does the benefit of doing something outweigh its risk? We&#39;re definitely not there currently.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:43:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:43:41 GMT" user-order="3"><p> I actually think advocacy offers a way to sidestep that question. Precisely how much risk should we tolerate for what benefit seems like a subjective question. However, most people are in fact fairly risk-averse on matters like this. If you ask most stakeholders, meaning the global population [NB: the stakeholder are the population of Earth but the polls I was thinking are of Americans], they generally favor a risk-averse approach. That seems correct in this context. It feels different from the kind of risk/reward we&#39;re typically set up to think about. We&#39;re not particularly equipped to think well about the risk of something as significant as a global nuclear war, and that&#39;s arguably more survivable than getting AI wrong.</p><p> I feel I am more risk-averse than a lot of our friends. My instinct is, let&#39;s just not go down that path right now. I feel like I could get the right answer to any hypothetical if you presented me with various fake utilities on the different sides.  Yet the real question is, do we take the leap now or later? I assume that any pause on AI advancement would <i>eventually</i> be lifted unless our society collapses for other reasons. I think that, in the long run, if it&#39;s possible, we&#39;ll achieve safe AGI and capture a lot of that value because we paused.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> Cluelessness and robustly good plans </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:11 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:11 GMT" user-order="2"><p> Hmm... I think I&#39;m convinced that pausing is better than not pausing. The thing I&#39;m not convinced of is that the world if you advocate pausing, is better than the world if you don&#39;t advocate pausing.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:22 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:22 GMT" user-order="3"><p> This is interesting, because I was talking to one of the co-organizers for the October 21st protest, and he said that actually he wasn&#39;t sure a pause is good, but he was more sure that advocating for a pause is good. He came away more convinced that advocacy was solidly good than that pause was the right policy decision.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:28 GMT" user-order="2"><p> This comes back to the very first thing... When it comes to the technical stuff, you can sit down and take it apart. And it&#39;s complicated, but it&#39;s the kind of complicated that&#39;s tractable. But for advocacy... it feels so hard to even know the sign of anything. It makes me want to just back away from the whole thing in despair.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:41 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:41 GMT" user-order="3"><p> I have felt that people I know in AI safety think that government/politics stuff is just harder than technical stuff. Is that kind of what you&#39;re thinking?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:44:44 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:44:44 GMT" user-order="2"><p>是的。 It&#39;s so chaotic. The systems you&#39;re working with are just extremely... not well behaved. The political situation is unprecedented, and has been various different kinds of unprecedented at every point in the last century at least. I&#39;m sure there are people who&#39;ve read a ton of history and philosophy and believe they can discern the grand arcs of all these events and understand what levers to pull and when, but I&#39;m not convinced that those people actually know what they think they know - they just get so few feedback loops to test their models against reality. It seems very common for unexpected factors emerge and cause actions to have the opposite of their intended effect.<br><br> That&#39;s not a reason not to try. But it gives me a feeling of cluelessness that&#39;s not compatible with the fervour that I think advocacy needs.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:44:55 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:44:55 GMT" user-order="3"><p> I do just feel pretty good about the pause message. It&#39;s one I consistently resonate with.  And not every message resonates with me. For instance, the message of the protest you attended at Meta turned out to be a more complex message to convey that I expected.</p><p> [The idea that advocacy is intractable but the technical problem isn&#39;t] might be a double crux?</p><p> Sounds like you&#39;re more concerned about hindering technological progress, and threading that needle just right… but I feel like what we&#39;re talking about is a just loss of some decades, where we have to sort of artificially keep AGI from coming, because if you allowed everything to just develop at its pace then it would be here sooner but it wouldn&#39;t be safe.</p><p> That&#39;s what I imagine when I say “pause”.  And I think that&#39;s clearly a worthwhile trade.  And so I feel much happier to just say “pause AI until it&#39;s safe”, than I ever did about any policy on alignment.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:01 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:01 GMT" user-order="2"><p> The only thing I&#39;ve always felt is obviously the thing to do is “speed up and improve alignment research”. Just direct resources and attention at the problem, and encourage the relevant people to understand the situation better. This feels fairly robust.</p><p> (Also, on some kind of aesthetic level, I feel like if you make things worse by saying what you honestly believe, then you&#39;re in a much better position morally than if you make things worse by trying to be machiavellian.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:11 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:11 GMT" user-order="3"><p> I am no expert on AI, but I come away feeling that we&#39;re not on track to solve this problem. I&#39;m not sure if promoting alignment research alone can address the issue, when we maybe only have like 5 years to solve the problem.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:15 GMT" user-order="2"><p> Yeah... and I don&#39;t know, maybe encouraging labs to understand the situation is is not even robustly good. It&#39;s difficult to get across “Hey, this is a thing to be taken seriously” without also getting across “Hey, AGI is possible and very powerful” - which is a message which... if not everyone realises that, you&#39;re maybe in a better situation.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles</section></section><h2> Taking beliefs seriously, and technological progress as enabler of advocacy </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:30 GMT" user-order="3"><p> I believe that merely providing people with information isn&#39;t sufficient; they often don&#39;t know how to interpret or act on it. One reason I turned to protests is that they send a clear message. When people see a protest, they instantly grasp that the protestors find something unacceptable.  This understanding is often more profound than what one might derive from a blog post weighing the pros and cons.</p><p> People tend to assume that if they were privy to crucial information that the majority were unaware of, especially if the stakes were world-altering, they would be out on the streets advocating for change.  There was confusion around why this wasn&#39;t the case for AI safety. I personally felt confused.</p><p> (The most extreme case would be when Eliezer suggested that no amount of violence could be justified to halt AGI, even if it was going to kill everyone on earth… I also think that you should have a deontological side constraint against violence because it&#39;s always so tempting to think that way, and it just wouldn&#39;t be helpful; it would just end up backfiring. But I was confused, and I really appreciated the TIME article where he mentions enforcing treaties using the same state violence as for other international treaties.)</p><p> Still, for a long time, I was unsure about the AI safety community&#39;s commitment due to their seeming reluctance to adopt more visible and legible actions, like being out in the streets protesting… even though I myself am not typically one to attend protests.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:35 GMT" user-order="2"><p> I think it&#39;s maybe a dispositional thing. I feel like I am just not a person who goes to protests. It&#39;s now how I engage with the world.</p><p> And I think this is also part of, where I stand on technology in general. Technology is such a big lever, that it kind of almost... ends up being the most important thing, right? It&#39;s like…</p><p> Okay, well now I&#39;m gonna say something controversial. People, I think, focus on the advocates more than the technologists, in a way that&#39;s maybe a mistake?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:45:47 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:45:47 GMT" user-order="3"><p> I already know I agree.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:45:54 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:45:54 GMT" user-order="2"><p> You can advocate for women&#39;s rights, but you can also invent effective, reliable birth control. Which one will have the bigger impact? I think it&#39;s the technology. Activists need to push for change, but it&#39;s technology that makes the change politically doable. I think people are like... the foam on the top of the enormous waves of the grand economic shifts that change what&#39;s politically and socially viable. For instance, I think our ability to get through the climate crisis mostly depends on renewable energy tech (if AGI somehow doesn&#39;t happen first) - make clean energy cheaper than fossil fuels, people will switch. I think all the vegan activism might end up being outweighed by superior substitute meats. When there&#39;s meat products that are indistinguishable from farmed meat but a tiny bit cheaper, the argument for veganism will mysteriously become massively more compelling to many. In both those cases, one big thing advocates did was create support for funding the research. I even think I remember reading someone suggest that the abolitionist movement really got traction in places that had maple syrup, because <i>those people had access to a relatively inexpensive alternative supply of sugar,</i> so being moral was cheaper for them. Obviously, activists are essential to push the arguments, but often these arguments don&#39;t gain traction until the technology/economics/pragmatics make them feasible. People are amazingly able to believe falsehoods when it&#39;s convenient, and technology can change what&#39;s convenient.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:01 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:01 GMT" user-order="3"><p> We could call this the “efficient market hypothesis of history” or something. Because I hear this a lot, and it&#39;s also part of my worldview.  Just not enough to make me not consider activism or advocacy. I think its all generally true and I pretty much agree with it, but at the same time you can still find $20 on the ground.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:05 GMT" user-order="2"><p> I actually did last week :)</p><p> [Added later: That makes me think, maybe that&#39;s the way to do it: rather than looking at what changes would be best, you look for the places where the technology and economics is already shifting in a good direction and all that&#39;s needed is a little activism to catalyse the change, and focus there. But probably there are already enough activists in most places that you&#39;re not doing much on the margin.]</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:28 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:28 GMT" user-order="3"><p> [Added later, in response: there are definitely not enough activists for AI Safety at this moment in time! It makes sense there would be &quot;market failure&quot; here because understanding the issue up until now has required a lot of technical know-how and running in a pretty advocacy-averse social circle.]<br><br> Perhaps it&#39;s a form of modest epistemology to defer so heavily to this, to override what we&#39;re actually witnessing. I even think there are some individuals who kind of know what you can do with advocacy.  They know about policy and understand that influencing even a small number of people can shift the Overton window.  And still, even those people with an inside view that advocacy can sometimes work, will struggle with whether it&#39;s the right approach or feel like somehow it <i>shouldn&#39;t</i> work, or that they should be focusing on what they deem more generally important (or identify with more).</p><p> However, I think what we&#39;re discussing is a unique moment in time. It&#39;s not about whether advocacy throughout history can outpace technology. It&#39;s about whether, right now, we can employ advocacy to buy more time, ensuring that AGI is not developed before sufficient safety measures are in place.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:46:30 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:46:30 GMT" user-order="2"><p> There&#39;s definitely a perspective where, if you have the ability to do the technical work, you should do it. But that&#39;s actually quite a small number of people. A larger number have the ability to advocate, so they bring different skills to the table.</p><p> Actually, I&#39;ve think I&#39;ve pinned down a vibe that many who can do technical AI safety work might resonate with. It feels akin to being assigned a group project at school/university. You&#39;ve got two choices: either somehow wrangle everybody in this group to work together to do the project properly, or just do the whole thing yourself. Often the second one seems a lot easier.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:46:39 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:46:39 GMT" user-order="3"><p> Though if you don&#39;t understand that the exercise was about learning to work in a group (instead of doing the assignment) you can miss out on a lot of education…</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:47:10 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:47:10 GMT" user-order="2"><p>是的。 Hmm... maybe this is actually a mis-learned lesson from everyone&#39;s education! We learned the hard way that &quot;solving the technical problem is way easier than solving the political one&quot;. But group project technical problems are artificially easy.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:47:38 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:47:38 GMT" user-order="3"><p> I am kind of annoyed at myself that I didn&#39;t question that vibe earlier in the community, because I had the knowledge… and I think I just deferred too much. I thought, “oh, well, I don&#39;t know that much about AI or ML-- I must be missing something”. But I had everything I needed to question the sociopolitical approaches.</p><p> My intuitions about how solvable the technical problem are influenced by my background in biology. I think that biology gives you a pretty different insight into how theory and real world stuff work together. It&#39;s one of the more Wild West sciences where stuff surprises you all the time.  You can do the same protocol and it fails the same way 60 times and then it suddenly works.  You think you know how something works, but then it suddenly turns out you don&#39;t.</p><p> So all my instincts are strongly that it&#39;s hard to solve technical problems in the wild -- it&#39;s possible, but it&#39;s really hard.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> The aesthetics of advocacy, and showing up not because you&#39;re best suited, but because no one else will </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:50:55 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:50:55 GMT" user-order="2"><p> I think there&#39;s an aesthetic clash here somewhere. I have an intuition or like... an aesthetic impulse, telling me basically… “advocacy is dumb”. Whenever I see anybody Doing An Activism, they&#39;re usually… saying a bunch of... obviously false things? They&#39;re holding a sign with a slogan that&#39;s too simple to possibly be the truth, and yelling this obviously oversimplified thing as loudly as they possibly can? It feels like the archetype of overconfidence.</p><p> It really clashes with my ideal, which is to say things that are as close to the truth as I can get, with the level of confidence that&#39;s warranted by the evidence and reasoning that I have. It&#39;s a very different vibe.</p><p> Then there&#39;s the counter-argument that claims that empirically - strategically, or at least tactically - the blatant messaging is what actually works. Probably that activist person actually has much more nuanced views, but that&#39;s just not how the game is played. But then part of me feels like “well if that is how the game is played, I don&#39;t want to play that game”. I don&#39;t know to what extent I endorse this, but I feel it quite strongly. And I would guess that a lot of the LessWrong-y type people feel something similar.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:12 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:12 GMT" user-order="3"><p> I think people haven&#39;t expressed it as clearly as you, but I&#39;ve gotten sort of that same vibe from a lot of people.</p><p> And… it does kind of hurt my feelings. Because I <i>also</i> want to be accurate. I&#39;m also one of those people.</p><p> I think I just have enough pragmatism that I was able to get past the aesthetic and see how activism works? I was also exposed to advocacy more because I have been involved with animal welfare stuff my whole life.</p><p> I had this experience of running the last protest, where I had a message I thought was so simple and clear... I made sure there were places where people could delve deeper and get the full statement. However, the amount of confusion was astonishing. People bring their own preconceptions to it; some felt strongly about the issue and didn&#39;t like the activist tone, while others saw the tone as too technical. People misunderstood a message I thought was straightforward. The core message was, &quot;Meta, don&#39;t share your model weights.&quot; It was fairly simple, but then I was even implied to be racist on Twitter because I was allegedly claiming that foreign companies couldn&#39;t build models that were as powerful as Meta&#39;s?? While I don&#39;t believe that was a good faith interpretation, it&#39;s just crazy that if you leave any hooks for somebody to misunderstand you in a charged arena like that, they will.</p><p> I think in these cases you&#39;re dealing with the media in an almost adversarial manner.  You&#39;re constantly trying to ensure your message can&#39;t be misconstrued or misquoted, and this means you avoid nuance because it could meander into something that you weren&#39;t prepared to say, and then somebody could misinterpret it as bad.</p><p> This experience was eye-opening, especially since I don&#39;t consider very myself politically minded. I had to learn these intricacies the hard way. I believe there&#39;s room for more understanding here. There&#39;s a sentiment I&#39;ve sensed from many: &quot;I&#39;m smart and what you&#39;re doing seems unintelligent, so I won&#39;t engage in that.&quot; Perhaps if people had more experiences with the different forms of communication, there would be more understanding of the environment the different kinds of messages are optimized for? It feels like there&#39;s a significant inferential gap.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:15 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:15 GMT" user-order="2"><p> Yeah, that sounds frustrating. I guess there&#39;s something about… “rationality”. So, the task of trying to get the right answer, on a technical or philosophical question where most people get the wrong answer, feels like... semiconductor manufacturing, or something like that. Like &quot;In this discipline I&#39;ve tried my best to master, I have to make sure to wear a mask, and cover my hair so that no particles get out into the air. This entire lab is positive air pressure, high end filtration systems. I go through an airlock before I think about this topic, I remove all contamination I can, remove myself from the equation where possible, I never touch anything with my hands. Because I&#39;m working with delicate and sensitive instruments, and I&#39;m trying to be accurate and precise…&quot;. It sounds pretty wanky but I think there&#39;s a real thing there.</p><p> And then in this metaphor the other thing that you also have to do in order to succeed in the world is like... mud wrestling, or something. It may not be an easier or less valuable skill, but it really doesn&#39;t feel like you can do both of these things at once.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:30 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:30 GMT" user-order="3"><p> What if it were phrased more like “There&#39;s this tech. It&#39;s pretty low tech. It&#39;s a kludge. But it <i>works</i> ”?</p><p> I remember the exact first moment I cried about AI safety: it&#39;s when I saw the polls that came out after the FLI letter, saying -- and I forget the exact phrasing, but something like -- most people were in favor of pause. “Oh my god”, I thought “we&#39;re not alone. We don&#39;t have to toil in obscurity anymore, trying to come up with a path that can work without everybody else&#39;s help.” I was overjoyed. I felt as proud and happy discovering that as I would&#39;ve been if I found a crucial hack for my really important semiconductor.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:33 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:33 GMT" user-order="2"><p> Hmm... Yeah, it clearly is important and can work, but my gut reaction, when I&#39;m inhabiting that clean-room mindset, is I still feel like I want to keep it at arm&#39;s length? I don&#39;t trust this low tech kludge not to spring a leak and mess up the lab.</p><p> When you start arguing with... (by you I mean &#39;one&#39; not you personally) - When one starts arguing with people using the discussion norms of political debate, and you&#39;re trying to win, I think it introduces a ton of distortions and influences that interfere. Maybe you phrase things or frame things some way because it&#39;s more persuasive, and then you forget why you did that and just believe your distorted version. You create incentives for yourself to say things more confidently than you believe them, and to believe things as confidently as you said them. Maybe you tie your <a href="http://www.paulgraham.com/identity.html">identity</a> to object level questions more than necessary, and/or you cut off your own <a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat">line of retreat</a> , so it&#39;s extra painful to notice mistakes or change your mind, that kind of thing. I notice this kind of thing in myself sometimes, and I expect I fail to notice it even more, and I&#39;m not even really trying to fight people with my work, just explain things in public.</p><p> I feel like there&#39;s kind of a trilemma here. You&#39;ve got to choose between either 1) truth/&#39;rationality&#39;/honesty, 2) pragmatism, for getting things done in the messy world of politics, or 3) a combination of the two different modes where you try to switch between them.<br> Choosing 1 or 2 each gives up something important from the other, and choosing 3 isn&#39;t really viable because the compartmentalisation it requires goes against 1.</p><p> I have this feeling about the truth, where you don&#39;t get to make exceptions, to sometimes believe what&#39;s pragmatically useful. You don&#39;t get to say, “in this situation I&#39;ll just operate on different rules”. I feel like it&#39;s not safe to do that, like human hardware can&#39;t be trusted to maintain that separation.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:51:46 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:51:46 GMT" user-order="3"><p> I guess I can&#39;t bring myself to be that concerned about contamination... I do think it&#39;s possible to switch modes and I find rationalists are often quite paranoid about this, like it&#39;s one toe out of line and they can never sincerely value and seek truth again. I have <a href="https://hollyelmore.substack.com/p/scrupulosity-my-eagxboston-2019-lightning-talk">struggled with scrupulosity (OCD)</a> and it reminds me of that.</p><p> Maybe it&#39;s because I wasn&#39;t into RationalityTM until later in my life, so I already had my own whole way of relating to the truth and how important the truth was. It wasn&#39;t so much through “speech codes” or things advocacy seems to violate?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:51:47 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:51:47 GMT" user-order="2"><p>是的。 Maybe that &#39;clean room&#39; mindset was just what was needed in order to recognise and address this problem seriously 15+ years ago… a mindset strongly focused on figuring out what&#39;s true, regardless of external opinions or how it would be perceived. At that time, societal factors might have prevented people from seeing the true scope of the situation. Now, the reality is starting to smack everyone in the face, so, perhaps that intense focus isn&#39;t as essential anymore. But by now, the most prominent individuals in the field have been immersed for a long time in this mindset that may not be compatible with an advocacy mindset? (This is just a hypothesis I randomly came up with just now.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:52:05 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:52:05 GMT" user-order="3"><p> I&#39;m not sure I possess an advocacy mindset, nor am I convinced it&#39;s mandatory. I do, however, see value in advocacy, perhaps more clearly than many on LessWrong. Discussing the virtues you mentioned as necessary15 years ago to perceive this issue, I may not embody those exact virtues, but I identify with many of them.</p><p> I feel like they were necessary for me to be able to pursue advocacy because AI Safety/LessWrong were so kneejerk against it. Even though Eliezer had voiced support for international politics on this matter, I have felt like an outsider. I wouldn&#39;t have ventured into this realm if I was constantly worried about public opinion. I was driven by the belief that this was really the biggest opportunity I saw that other people weren&#39;t taking.  And I felt their reluctance was tied to the factors you highlighted (and some other things).</p><p> It&#39;s not a big crowd pleaser. Nobody really likes it.</p><p> Early on, some conveyed concerns about protesting, suggesting it might be corrupting because it was something like “too fun”. They had this image of protesters delighting in their distinct identity.  And sorry, but like, none of us are really having fun. I&#39;m trying to make it fun. I think it could and should be fun, to get some relief from the doom. I would love it if in this activity you didn&#39;t have to be miserable.  We can just be together and try to do something against the doom.</p><p> Maybe it could be fun. But it hasn&#39;t been fun.  And that&#39;s, mainly because of the resistance of our community, and because of the crowds that just, like… man, the open source ML community was pretty nasty.</p><p> So I was hearing from people what seemed like you were saying: “oh it would be so easy to let your clean room get contaminated; you could just slide down this incentive gradient”: and that&#39;s not what&#39;s happening. I&#39;m not just sliding down a gradient to do what&#39;s easier, because it&#39;s actually quite difficult.</p><p> Finally: everybody thinks they know how to run a protest, and they keep giving me advice.<i> </i>I&#39;m getting a ton of messages telling me what I should have done differently. So much more so than when I did more identity-congruent technical work.  But nobody else will <i>actually do it.</i></p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:53:13 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:53:13 GMT" user-order="1"><p> Before Rob replies, I want to express some empathy hearing that. It sure sounds like a slog. I haven&#39;t made up my mind on what I think of protesting for AI safety, but I really do have deep sympathy for the motion of &quot;doing what seems like the most important thing, when it seems no one else will&quot;. I recognise that trying that was hard and kind of thankless for you. Thank you for nonetheless trying.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:17 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:17 GMT" user-order="2"><p> Yeah, I appreciate it too. I think your activism is pretty different from the stereotype. I also feel the empathy for &quot;nobody else will actually do it&quot;, because I too have felt that way for a while about YouTube, and I guess broader outreach and communication stuff.</p><p> Because, it&#39;s very weird that this is <i>my</i> job! And for a long time I was the only one doing it. I was like, &quot;Am I really the best qualified person to do this? Really? Nobody else is doing it at all, so I guess I&#39;ll do it, I guess I&#39;ll be the best person in the world at this thing, simply because there&#39;s literally nobody else trying?&quot;. For years and years I felt like &quot;Hey, AI Safety is obviously the most interesting thing in the world, and also the most important thing in the world… and there&#39;s only one YouTube channel about it? Like… what!?&quot;</p><p> And all these researchers write papers, and people read the papers but don&#39;t really understand them, and I&#39;m like &quot;Why are you investing so little in learning how to write?&quot;, &quot;Why is there so little effort to get this research in front of people?&quot;.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:53:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:53:20 GMT" user-order="3"><p> No, then you&#39;d get real criticism :)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:53:24 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:53:24 GMT" user-order="2"><p> It&#39;s just confusing! If you&#39;re a researcher, are you banking on being in the room when the AGI happens? At the keyboard?可能不会。 So whatever impact your work has, it routes through other people understanding it. So this is not an optional part of the work to do a good job at. It&#39;s a straightforward multiplier on the effect of your work.</p><p> It seems you had some similar feelings: &quot;this seems critical, why is it not being done by anyone else?&quot;</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:10 GMT" user-order="3"><p>是的。 I looked around thinking I would love to volunteer for an advocacy campaign, if people are going to be doing more advocacy. There were other people early on in other cities, and there were people interested in a more political approach in the Bay Area, but no one else was willing to do public advocacy around here. And I could feel the ick you&#39;re describing behind it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:13 GMT" user-order="2"><p>男人。 It sounds like diving hard into advocacy is kind of like <a href="https://www.lesswrong.com/posts/CEGnJBHmkcwPTysb7/lonely-dissent#:~:text=Lonely%20dissent%20doesn%E2%80%99t,the%20pack.">coming to school in a clown suit</a> .</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:35 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:35 GMT" user-order="3"><p>是的！</p><p> There was also another thing that made me feel compelled to pursue it. It was similar to when, as a kid vegetarian, I always heard such terrible arguments from adults about eating meat. There was just no pressure to have a good argument. They already knew how it was gonna turn out: society had ruled in their favor, and I was a kid. Yet it was so clearly wrong, and it bothered me that they didn&#39;t care.</p><p> Then, after Eliezer&#39;s Time article came out, I once again saw a wall of that from people not wanting to move in a political direction. People I really thought had the situation handled were just saying honestly the dumbest things and just mean, bad argumentation; making fun of people or otherwise resorting to whatever covert way they could to pull rank and avoid engaging with the arguments.</p><p> I wouldn&#39;t have felt qualified to do any of this except for witnessing that.  And I just thought “okay, it just so happens I&#39;ve accumulated some more knowledge about advocacy than any of you, and I really feel that you&#39;re wrong about why this isn&#39;t going to work”. I felt somebody really needs to be doing this. How would I feel if, I got a glimpse of “this is it, we&#39;re getting killed, it&#39;s happening” and I hadn&#39;t even tried? It was a total “inadequate equilibrium” feeling.</p><p> I guess I&#39;m a generalist in skillset, and was able to do adapt that here… but my training is in evolutionary biology. I worked at a think tank before. I really feel like there&#39;s so much I don&#39;t know.  But I guarantee you there was no one else willing to do it who was better qualified.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:37 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:37 GMT" user-order="2"><p>是的。</p><p> None of this makes me want to actually do it. But it does it does make me want somebody to do it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:54:56 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:54:56 GMT" user-order="3"><p> I wouldn&#39;t want you to change what you&#39;re doing. It seems to be working. Neither would it be good for you to stop caring about accuracy in your outreach, lol. (And just to be clear, I don&#39;t think our protest signs are inaccurate; it&#39;s just a different level of resolution of communication. I&#39;m working on a post about this. [ <a href="https://x.com/ilex_ulmus/status/1709706321203990886?s=20">tweet thread on this topic</a> ])</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore</section></section><h2> Closing thoughts </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:54:58 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:54:58 GMT" user-order="2"><p> To end, I&#39;d be interested to share some deltas / updates.</p><p> I&#39;ve updated toward believing that the reluctance, of people who consider themselves rationalists, to seriously consider advocacy, is itself a failure of rationality on its own terms. I&#39;m not completely sold, but it seems worth looking into in a lot more detail.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:10 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:10 GMT" user-order="3"><p> If people would just get out of the way of advocacy, that would be really helpful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:13 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:13 GMT" user-order="2"><p> I&#39;m curious about that... what&#39;s in the way of advocacy right now?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:55:24 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:55:24 GMT" user-order="3"><p> Probably the biggest thing is people&#39;s perception that their job prospects would be affected. I don&#39;t know how true that is. Honestly I have no idea. I suspect that people have reason to overestimate that or just conveniently believe that, because they don&#39;t want to update for other reasons. But I think they&#39;re worried labs won&#39;t like the protests.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:55:28 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:55:28 GMT" user-order="2"><p> Oh, I have no sympathy for that, that sounds like straightforward cowardice... I mean, I guess there&#39;s a case to be made that you need to be in the room, you need to maintain access, so you should bide your time or whatever, not make waves... but that feels pretty post hoc to me. You&#39;re not going to get fired for that; say what you believe!</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Sun, 15 Oct 2023 22:56:50 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Sun, 15 Oct 2023 22:56:50 GMT" user-order="1"><p> Holly, do you want to share more of your updates from the conversation?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> jacobjacob </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:56:57 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:56:57 GMT" user-order="3"><p> Well, Rob definitely gave me a +10 to a general sense I had picked up on, of people around here having an aversion to advocacy or stuff that looks like advocacy, based on rationalist principles, instincts, and aesthetics.</p><p> I also certainly felt I understood Rob&#39;s thinking a lot better.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:57:05 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:57:05 GMT" user-order="2"><p> Yeah, it was interesting to work through just what my attitude is towards that stuff, and where it comes from. I feel somewhat clearer on that now.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5rD8KRMWffhtosxp3-Sun, 15 Oct 2023 22:59:20 GMT" user-id="5rD8KRMWffhtosxp3" display-name="Holly_Elmore" submitted-date="Sun, 15 Oct 2023 22:59:20 GMT" user-order="3"><p> I think a lot of people have similar inner reactions, but won&#39;t say them to me. So I suspected something like that was frequently going on, but it was I wasn&#39;t able to have a discussion with it.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Holly_Elmore </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YpWtrjmuEgioeWRNg-Sun, 15 Oct 2023 22:59:35 GMT" user-id="YpWtrjmuEgioeWRNg" display-name="Robert Miles" submitted-date="Sun, 15 Oct 2023 22:59:35 GMT" user-order="2"><p> I&#39;m glad it was helpful!<br><br> Ok I guess that&#39;s it then? Thanks for reading and uh... don&#39;t forget to Strong Upvote, Comment and Subscribe?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Robert Miles</section></section><br/><br/> <a href="https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gDijQHHaZzeGrv2Jc/holly-elmore-and-rob-miles-dialogue-on-ai-safety-advocacy<guid ispermalink="false"> gDijQHHaZzeGrv2Jc</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 20 Oct 2023 21:04:32 GMT</pubDate></item><item><title><![CDATA[TOMORROW: the largest AI Safety protest ever!]]></title><description><![CDATA[Published on October 20, 2023 6:15 PM GMT<br/><br/><p>明天， <a href="https://pauseai.info/">PauseAI</a>和合作者将在 6 个国家的 7 个地点举办迄今为止最大规模的人工智能安全抗议活动。各界人士热切欢迎！<br><br>当面对面的志愿服务无法用金钱或智力支持替代时，您参加这次抗议活动是一个难得的影响机会——我们以人类本能理解的方式向公众表达我们的关注。另外，我们认为这会很有趣，之后会有友情和饮料。在争取人工智能安全的斗争中摘取新的唾手可得的果实并发挥不同的力量感觉很棒。</p><h2> <strong>10月21日（星期六），多个国家</strong></h2><ul><li>美国，加利福尼亚州，旧金山（ <a href="https://fb.me/1RbYq9H2hOFQ4yi"><u>facebook</u></a> ）</li><li>美国，马萨诸塞州，波士顿（ <a href="https://facebook.com/events/s/pauseai-protest-boston-make-th/6647554948613714/?mibextid=RQdjqZ"><u>facebook</u></a> ）</li><li>英国，议会广场，伦敦（<a href="https://www.mixily.com/event/4774799330762010477"><u>注册</u></a>， <a href="https://www.facebook.com/events/644748401084077"><u>facebook</u></a> ）</li><li>荷兰, 海牙 (<a href="https://www.mixily.com/event/8536294863402363208"><u>注册</u></a>)</li><li>澳大利亚, ​​墨尔本 (<a href="https://www.mixily.com/event/8471341506387452508"><u>报名</u></a>)</li><li>加拿大，渥太华（由 Align the World 组织，请在<a href="https://www.facebook.com/events/243643008241929/"><u>facebook</u></a>或<a href="https://www.eventbrite.com/e/ai-safety-and-ethics-rally-tickets-725729686027"><u>EventBrite</u></a>上注册）</li><li>丹麦，哥本哈根（ <a href="https://www.facebook.com/events/869443424535827"><u>Facebook</u></a> ）</li><li>你的国家在这里吗？<a href="https://discord.gg/anXWYCCdH5"><u>讨论不和谐！</u></a></li></ul><h2><strong>我们为什么抗议</strong></h2><p>人工智能正在迅速变得更加强大，其速度远远快于几乎所有人工智能科学家的预测。数十亿美元正在投入人工智能能力，其结果是惊人的。新模型在很多领域都<a href="https://pauseai.info/sota"><u>超越了人类</u></a>。随着能力的增强，<a href="https://pauseai.info/risks"><u>风险</u></a>也随之增加。科学家甚至<a href="https://www.safe.ai/statement-on-ai-risk"><u>警告</u></a>人工智能<a href="https://pauseai.info/xrisk"><u>最终可能会毁灭人类</u></a>。这种可怕的结果不仅看起来有可能，而且很有可能发生，因为这些结果的平均概率估计<a href="https://pauseai.info/polls-and-surveys"><u>范围为 14% 到 40%</u></a> 。</p><p>我们需要我们的领导人倾听这些警告，但他们并没有像应有的那样认真对待这个话题。目前正在起草人工智能安全立法，但<a href="https://twitter.com/PauseAI/status/1704998018322141496"><u>没有任何一项措施能够真正阻止或延迟超级人工智能的出现</u></a>。<a href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation"><u>超过 70% 的</u></a>人希望减缓人工智能的发展， <a href="https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll"><u>超过 60% 的</u></a>人希望监管积极阻止超级人工智能的出现。为什么没有立法草案真正做到这一点？答案是游说：我们的政治家<a href="https://fedscoop.com/sen-schumer-to-host-musk-zuckerberg-and-other-tech-ceos-for-closed-door-ai-forum/"><u>大多会见人工智能公司的首席执行官</u></a>，他们会推动符合他们利益的政策措施。</p><p> 11月1日至2日，首届人工智能安全峰会将在英国举行。这是迈向明智的国际人工智能安全监管第一步的绝佳机会。</p><h2><strong>我们问什么</strong></h2><ul><li><strong>政策制定者</strong>：不允许公司建立超级智能。法规和硬件限制应在培训开始之前适用，因为一旦实现新能力就很难控制传播。 。我们不能允许公司训练可能毁灭世界的人工智能模型。制定立法很困难，需要时间，但我们可能没有那么长的时间，所以要努力工作，就好像你的生命依赖于此一样。因为确实如此。</li><li><strong>公司</strong>：你们中的许多人都害怕人工智能的能力，但你们陷入了一场竞赛。因此，原则上要大声表示支持暂停。如果你签署了这项技术可能杀死我们所有人的声明，请向世界表明，如果它是一个可行的选择，你宁愿不建造它。</li><li><strong>峰会受邀者</strong>：安全优先于经济增长。我们知道人工智能可以让我们的国家变得更加富裕，但这并不是您被召集到这里的原因。成为房间里的成年人</li></ul><br/><br/><a href="https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/abBtKF857Ejsgg9ab/tomorrow-the-largest-ai-safety-protest-ever<guid ispermalink="false"> abBtKF857Ejsgg9ab</guid><dc:creator><![CDATA[Holly_Elmore]]></dc:creator><pubDate> Fri, 20 Oct 2023 18:15:19 GMT</pubDate> </item><item><title><![CDATA[The Overkill Conspiracy Hypothesis]]></title><description><![CDATA[Published on October 20, 2023 4:51 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DGsPeE89N93YCwxbH/u1qu8dfmi26tobyfpony"><figcaption>否则我们无法获得完全正确的照明。</figcaption></figure><p><i>阴谋论</i>一词被用作贬义词，暗指表面上的荒谬。但我们使用的词汇存在严重的歧义问题，因为<i>阴谋</i>不是想象中的虚构。普通的<i>阴谋</i>（ <a href="https://en.wikipedia.org/wiki/COINTELPRO">COINTELPRO</a> 、<a href="https://en.wikipedia.org/wiki/Operation_Snow_White">白雪公主行动</a>或<a href="https://en.wikipedia.org/wiki/Gunpowder_Plot">火药阴谋</a>）和它们的戏剧性同族（地平说、登月骗局或咖啡味道不错的滑稽观念）之间存在着明显的和定性的区别，但明确的界限一直难以捉摸，仅仅断言“这个疯了，而这个不是”是令人不满意的。这两个阵营都涉及诡计、恶意意图、秘密行动、错误信息、精心策划的欺骗、隐藏的议程、秘密网络，是的，还有<i>阴谋</i>，但区分两者的尝试却转向了令人不满意或明显误导的领域。</p><p>我要论证的是，解决方案可以归结为对定义的简单重新配置，抓住了荒谬的本质：<i>阴谋论</i>是假设环境使名义上的“阴谋”变得不必要的理论。这就是我所说的“过度杀戮阴谋假说”(OCH)。在我们深入研究这种改进之前，探索一下为什么传统的区别已经不足是很有帮助的。</p><p> <a href="https://en.wikipedia.org/wiki/Conspiracy_theory#Difference_from_conspiracy"><u>《人民百科全书》</u></a>中有关差异的部分展示了其中一些误导性的尝试。例如，阴谋论<i>往往</i>与主流共识相反，但这是对权威的赤裸裸的诉求——这种逻辑会让<a href="http://www.cnn.com/US/9705/tobacco/history/"><u>早期挑战者对吸烟对健康有益的说法</u></a>感到困惑。或者理论描绘了阴谋者的行为极其恶意，但人类确实可以怀有邪恶的意图（<i>一般参见</i>人类历史）。另一个依赖于维持近乎完美的操作安全性的难以置信。虽然情况正在好转，但虽然保密很困难，但这绝对不是不可能。我们有真实的、现实生活中的秘密军事行动或贩毒集团设法经营价值数十亿美元的秘密物流企业的例子。</p><hr><p>从一堆谷壳中仍然可以得到一些有用的指导，那就是阴谋论缺乏可证伪性，并且抵制<a href="https://en.wikipedia.org/wiki/Falsifiability"><u>可证伪性</u></a>。尽管它的名字很不幸，但可证伪性却是我在探索世界时最亲近、最亲近的概念之一。简而言之，可证伪性是指<i>至少在假设上</i>证明理论是错误的能力。典型的例子是“我相信所有天鹅都是白色的，但如果我看到一只黑天鹅，我就会改变主意”。典型的反例可能是<a href="https://archive.org/details/japaneseevacuati00dewi/page/32/mode/2up?q=confirming"><u>约翰·德威特将军引用</u></a>二战期间日裔美国人<i>没有</i>进行破坏活动作为未来破坏计划的<i>证据</i>。阴谋论者确实有一种趋势，他们深入挖掘他们<a href="https://www.lesswrong.com/tag/belief-in-belief"><u>对信仰的信念</u></a>，并将相反的证据视为捏造的，或者是阴谋本身的（更糟糕的）证据。</p><p>我不会谈论可证伪性测试；这真是个好东西。但它也有局限性。首先，缺乏可证伪性只是理论有缺陷的一个很好的<i>迹象</i>，而不是一个<i>决定性的</i>决定。还有一些实际的考虑因素，例如历史事件如何难以应用可证伪性，因为证据不完整或无可救药地丢失，或者新兴科学领域的技术不足如何使一些可证伪的主张（暂时，希望如此）超出审查范围。因此，无法证伪一个理论并不<i>一定</i>意味着该理论是废话。</p><p>除了这些实际限制之外，还有不幸的坏演员因素。具有足够不诚实或自我意识的理论家可以通过采取含糊的暗示来应对可证伪性的生存威胁，以避免被自己制作的鞋带绊倒。由于你无法伪造没有明确提出的内容，因此他们会围绕直接断言跳舞，使他们的主张笼罩在可能性的迷雾中。那么唯一的办法就是更上一层楼，在<i>“谁/如何/为什么”</i>的具体答案仍然难以捉摸的情况下，将模糊性推断为可证伪逃犯的明显标志。将模糊性测试应用于地平理论就证明了这种回避。几乎不可能<a href="https://youtu.be/JTfhYyTuT44?t=1150"><u>从支持者那里得到任何明确的答案</u></a>：“大环球”背后到底是<i>谁</i>，他们是<i>如何</i>欺骗所有人的，<i>为什么为什么为什么为什么</i>有人会为这个计划付出任何努力？相比之下，像<a href="https://en.wikipedia.org/wiki/Atomic_spies"><u>原子间谍</u></a>这样的真实阴谋™缺乏模糊性：苏联/核秘密的秘密传输/地缘政治优势。</p><p>然而，模糊性指控并不适用于所有阴谋论。登月骗局在这一点上出人意料地清晰：美国宇航局/声场/地缘政治优势。这揭示了另一种防止造假的防御机制，即设定高得离谱的证据标准。说到面纱，世界各地的伊斯兰教法都有这样的先例， <a href="http://www.daviddfriedman.com/Academic/Course_Pages/Legal_Systems_Very_Different_13/Book_Draft/Systems/Islamic_Law_Chapter.htm"><u>通奸罪的定罪</u></a>需要有<i>四名</i>目击者目睹<i>同一</i>性行为，而且<i>只有</i>成年男性穆斯林才被视为有资格的证人。这些极其严格的标准似乎是为了应对该罪行可判处死刑的事实，并表明有可能将标准提高到<i>故意</i>使可证伪性变得遥不可及的程度。</p><p>登月骗局可能会受到这些不可能的标准的影响，因为阿波罗 11 号登月被<a href="https://youtu.be/iR3oXFFISI0?t=1054"><u>精心记录了超过 143 分钟</u></a>的不间断视频片段——这个持续时间<a href="https://theconversation.com/moon-landings-footage-would-have-been-impossible-to-fake-a-film-expert-explains-why-118426"><u>太长，以当时可用的技术无法容纳在胶卷上</u></a>。尽管仅略高于<a href="https://slatestarcodex.com/2013/04/12/noisy-poll-results-and-reptilian-muslim-climatologists-from-mars/"><u>蜥蜴人常数</u></a>，但令人惊讶的是，有<a href="https://www.voanews.com/a/usa_millions-still-believe-1969-moon-landing-was-hoax/6172262.html"><u>6% 的美国人仍然认为</u></a>登月是上演的。在某些时候，你必须问有多少证据就足够了，但最终没有普遍接受的阈值来回答这个问题。</p><p>因此，可证伪性仍然是一个很棒的工具，但它有合理的实际限制，而且无论如何都不是一个结论性的调查。某人拒绝进行可证伪性仍然是他们意识到并关心让他们的理论接受审查的极好证据，但他们的努力（模糊或不可能的标准）仍然会挫败可证伪性的直接应用。那么还剩下什么呢？</p><hr><p>我们终于又回到了过度杀戮阴谋假说，具有讽刺意味的是，阴谋论必须假设环境也使阴谋变得毫无意义。解释这一点的最好方法是举例。解构阴谋论就像策划银行抢劫一样令人兴奋，所以请把自己置于负责监督登月骗局的不幸的匿名官僚的立场上。请记住，登月骗局的<i>目的</i>是通过让美国在月球追逐中击败苏联来建立地缘政治声望。因此，无论你制定什么计划，<i>都必须</i>经受住当时最先进的太空计划的审查，该计划雇用了来自世界另一半的最优秀的太空工程师。</p><p>最直接的对策是责成美国宇航局现有工程师起草<i>完全虚假但绝对合理的</i>设备设计。<i>整个发射的每一个环节</i>——每枚火箭、登月舱、梯子、面板、螺栓、手套、扳手——都需要精心制作，不仅可以欺骗全球观众，还可以欺骗那些在太空中屏息凝视的目光锐利的专家。冷战分歧的另一边。将其扩展到所有通信、视频传输、照片、宇航员证词和“返回”的月球岩石。每一项都必须由专门且高度专业化的顾问进行详尽而细致的检查。</p><p>但它并不止于此，因为你还需要<i>绝对和</i><i>永久的</i>保密，因为任何单一的泄密都会威胁到整个努力。美国很清楚苏联间谍已经成功窃取了严密保护的核机密，所以这里需要的任何反制措施都必须<i>超越他妈的核武器</i>。正如我之前所说，保密并非不可能，只是非常困难。我想美国宇航局可以效仿卡特尔，对任何告密者（以及他们的整个家庭）进行残酷的暴力报复，但这种威慑只有在……人们知道的情况下才能发挥作用。不过，更有可能的是，美国宇航局将使用传统情报机构的广泛审查、选择性招募和丰厚报酬的方法，但现在所有措施都需要进一步扩大，以<i>超越</i>围绕核秘密的保护措施。</p><p>我们正在谈论对数百或数千人进行比核秘密更严格的筛查，同时扩大监视装置以让每个人都遵守规定。你需要将 NASA 的预算增加多少（10 倍？100 倍？）才能投入到这一冒险的行动中，如果暴露，将成为历史上永远的笑柄？如果已经有如此巨大的财富可供支配，那么真正去月球似乎就变得更容易了。</p><hr><p> OCH <sup>®</sup>有几个优点。首先，不要挑战任何阴谋论者的前提。它认为确实存在一个有足够动机的影子阴谋集团，并且只是跟着它运行。这回避了上述关于可证伪性逃犯的任何担忧，并且仍然提供了一个有用的标准来区分普通阴谋和害群之马。</p><p>如果我们将 OCH 应用于原子间谍，我们可以看到阴谋背后的理论不需要任何过分的假设。苏联没有核武器，他们想要核武器，窃取别人的蓝图肯定比开发自己的内部容易得多。必要的假设（苏联有有效的间谍计划）并不能否定阴谋的必要性。</p><p>与桑迪胡克骗局相比，桑迪胡克骗局将学校枪击事件视为政府精心策划的假旗行动，目的是通过限制性枪支法（或其他东西；请参阅上面的模糊部分）。撇开实际上没有产生重大枪支立法的事实不谈，这场骗局及其所需的数百名危机行动者将需要数千次试镜，以及之前讨论的所有保密障碍。再说一遍，如果政府已经可以使用这座堆积如山的资源，那么似乎有更有效的方法来使用它（比如可能给每位国会议员一些金条），而不是精心策划攻击，然后希望通过正确的法律之后。</p><p>鉴于这种情况已经经常发生，人们也很想知道为什么秘密阴谋集团甚至需要策划一场假的大规模枪击事件！即使阴谋集团想要煽动屠杀（无论出于何种原因），最简单的方法就是识别出孤独的非独身儿童并促使他们进行<i>真正的</i>大规模枪击。我们已经规定阴谋集团不关心死去的孩子。同样，如果美国想策划 9/11 袭击作为全球战争的前奏，那么在一架真正的飞机上装满真正的炸药，然后将其实际发射到真正的建筑物上似乎要容易得多，而不是花费数周的时间或几个月的时间秘密地偷偷地将多少吨铝热剂潜入世界贸易中心（同时出于某种原因<i>还要</i>协调飞机撞击的时间表）。</p><p>检查其他已验证阴谋的例子表明，它们中没有一个包含过度杀伤性的假设，使阴谋活动变得毫无意义。在<a href="https://en.wikipedia.org/wiki/Watergate_scandal"><u>水门事件</u></a>中，阴谋者的动机是通过监视对手来获得政治优势，而阴谋者通过简单的闯入来实现这一目的。不需要对尼克松总统的安全随行人员的能力做出任何假设，否则这次侵入是不必要的。即使是像<a href="https://en.wikipedia.org/wiki/Operation_Snow_White"><u>白雪公主行动</u></a>这样规模的行动——该行动仍然是美国政府最大的渗透行动之一，涉及多达 5,000 名特工——也很适合。他们能够接触到数千名秘密特工这一事实并不算过分，因为特工仍然需要渗透到政府机构才能获得他们想要销毁的文件。这些假设并不能掩盖阴谋的必要性。</p><hr><p>我并不妄想我可以让那些坚信阴谋论的人相信他们的失误。我不认为自己知道人们是如何陷入这种不可证伪的荒诞思维的。但至少对于我们其他人来说，能够区分真实与怪异仍然很有用。也许在那之后我们可以找到一些答案。</p><p> ——从我的登月舱发送</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DGsPeE89N93YCwxbH/the-overkill-conspiracy-hypothesis<guid ispermalink="false"> DGSPEE89N93YCwxbH</guid><dc:creator><![CDATA[ymeskhout]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:51:21 GMT</pubDate> </item><item><title><![CDATA[I Would Have Solved Alignment, But I Was Worried That Would Advance Timelines]]></title><description><![CDATA[Published on October 20, 2023 4:37 PM GMT<br/><br/><p>联盟社区表面上是一群担心人工智能风险的人。最近，将其描述为一群关注人工智能时间表的人会更准确。</p><p>人工智能时间表与人工智能风险有一定关系。较慢的时间意味着人们有更多的时间来弄清楚如何调整未来的智能人工智能，从而可能降低风险。但越来越多的情况是，减缓人工智能的进步本身就成为了目的，优先于确保人工智能的顺利发展。当这两个目标发生冲突时，时间表会获胜。</p><p>几乎任何事情都可以直接或间接地加快时间线。正因为如此，对齐社区变得瘫痪，害怕做任何与人工智能相关的事情——甚至发布对齐工作！ - 因为担心他们的行为会加快人工智能的进程。</p><p>结果是，联盟社区的效率越来越低，并且与更广泛的人工智能领域更加孤立；人工智能进步的轻微放缓所带来的好处几乎不超过成本。</p><p>这并不是说试图减缓人工智能的进步总是不好的。这取决于它是如何完成的。</p><h2><strong>放慢人工智能的速度：不同的方法</strong></h2><p>如果你想减缓人工智能的进步，可以采取不同的方法。对其进行分类的一种方法是按经济放缓影响的对象来分类。</p><ul><li><strong>政府执法：</strong>让政府通过监管或禁令来减缓进展。这是一个非常广泛的类别，既包括某个国家的法规，也包括国际上对超过一定尺寸的车型的禁令，但该类别的重要区别特征是该禁令适用于所有人，或者，即使不是所有人，至少也适用一个没有被选中关心人工智能风险的大群体。</li><li><strong>自愿协调：</strong>如果 OpenAI、DeepMind 和 Anthropic 都同意停止能力工作一段时间，这将是自愿协调。由于这是自愿的，因此暂停降低人工智能风险只会影响担心人工智能风险的组织。</li><li><strong>个人退出：</strong>当个人担心人工智能风险而避免进入人工智能领域，因为担心必须做能力工作从而提前时间表，这就是个人退出；为避免加快时间表而未采取的其他行动也是如此，例如不发布对齐研究。</li></ul><p>这篇文章的重点是个人退出。我认为几乎所有形式的个人退出都会适得其反，并且经不起公正的成本效益分析。然而，个人退出作为一项原则在联盟社区中变得根深蒂固。</p><h2><strong>结盟社区中个人退缩的例子</strong></h2><p>让我们首先看看联盟社区中个人退出是如何倡导和实践的，并尝试对其进行分类。</p><h3><strong>能力撤回</strong></h3><p>这可能是最有共识的一点：担心人工智能风险的人们不应该参与人工智能能力，因为这会加快人工智能的时间表。如果你正在研究人工智能功能，你应该停下来。理想情况下，人工智能能力领域的人根本不会关心人工智能风险，因为所有关心的人都离开了——这会很好，因为这会减慢人工智能的时间表。以下是主张撤回能力的人的例子：</p><p><a href="https://www.lesswrong.com/posts/CvfZrrEokjCu3XHXp/ai-practical-advice-for-the-worried">人工智能中的 Zvi：给忧虑者的实用建议</a>：</p><blockquote><p>请记住，那些为了提供帮助而从事人工智能工作的人的<i>默认结果</i>是最终主要致力于能力方面的工作，并使情况变得更糟。</p></blockquote><p> Nate Soares <a href="https://www.lesswrong.com/posts/N7DxcLCjfBpEv3QwB/request-stop-advancing-ai-capabilities">请求：停止推进人工智能能力</a>：</p><blockquote><p>这偶尔提醒我，我认为在当前范式中推动人工智能能力的前沿是高度反社会的，并且预计会极大地破坏我所知道和喜爱的一切。对于所有这样做的人（直接且有目的地这样做，而不是作为联盟研究的可悲的负面外部性）：我请求你们停止。</p></blockquote><p>康纳·莱希： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/drqkmab35kbnlwa6bu3r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hbdf3uoikqkqtkxkfshc 96w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/wk6ogayo5dtoujzodxkm 176w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/bxurkwtstur1rwr0iqod 256w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/knk1p8t9m77ttw7zjglm 336w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/evdcyulqy8vzwtcs9ij9 416w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/se4ethychoc0xwcnsfsv 496w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/ixprvrcvy6sogeccce7m 576w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/hcucq8z8gh2hfpvnvkey 656w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XqeWpz33oz4efobzH/haffb8gbxjbtibfsqcpa 736w"></figure><h3><strong>撤回结盟</strong></h3><p>尽管能力撤回是一个好的开始，但这还不够。协调工作仍然存在提前时间表的危险。重要的是要非常小心与谁分享你的一致性研究，并且如果某些类型的一致性研究对能力有影响，则可能要完全避免某些类型的一致性研究。</p><p>例子：</p><p> Miri 在题为“ <a href="https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/">2018 年更新：我们的新研究方向</a>”的博客文章中，对短时间和先进能力的担忧导致他们决定默认不分享他们的比对研究：</p><blockquote><p> MIRI 最近决定将其大部分研究设为“默认不公开”，这意味着，今后，MIRI 内发现的大多数结果将仅保留在内部，除非有明确决定发布这些结果（通常基于它们的发布具有特定的预期安全优势。</p></blockquote><p>这种做法至今仍在实践中。事实上，这种对分享想法的沉默不仅存在于面向公众的工作中， <a href="https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience">甚至存在于与其他一致性研究人员的面对面交流中</a>：</p><blockquote><ul><li>我认为我们对信息安全过于谨慎。该模型是这样的：Nate 和 Eliezer 的心态对能力和一致性都有好处，因此，如果我们与其他一致性研究人员谈论我们的工作，这种心态将扩散到一致性社区，然后传播到 OpenAI，在那里它会加速能力。我认为我们没有足够的证据来相信这一点，应该分享更多。</li></ul></blockquote><p> <a href="https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous">内特·苏亚雷斯 (Nate Soares) 提出了</a>另一个关于通过共享一致性研究增加存在风险的担忧的例子：</p><blockquote><p>我<a href="https://twitter.com/robbensinger/status/1602835145488113664"><u>历来</u></a>一直公开<a href="https://www.lesswrong.com/posts/3pinFH3jerMzAvmza/on-how-various-plans-miss-the-hard-bits-of-the-alignment"><u>支持</u></a>可解释性研究。我仍然支持可解释性研究。然而，我并不一定认为所有这些都应该无限期地公开进行。事实上，只要可解释性研究人员了解了可以显着推进能力前沿的人工智能，我鼓励可解释性研究人员保持他们的研究<a href="https://www.lesswrong.com/posts/tuwwLQT4wqk25ndxk/thoughts-on-agi-organizations-and-capabilities-work"><u>封闭性</u></a>。</p></blockquote><p>收敛分析<a href="https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research">的 Justin Shovelain 和 Elliot Mcckernon</a>也对可解释性研究提出了警告：</p><blockquote><p>如果您参与或对可解释性研究感兴趣，可以考虑以下一些启发式方法（按细微差别升序排列）：</p><ul><li><strong>相反，研究更安全的话题。</strong> <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#what-can-you-do-concretely-to-help"><u>人工智能安全有很多研究领域</u></a>，如果你想确保你的研究是积极的，一种方法是专注于没有应用人工智能能力的领域。</li><li><strong>在可解释性范围内研究更安全的子主题。</strong>正如我们将在下一节中讨论的那样，某些领域比其他领域风险更高 - 将您的注意力转移到风险较小的领域可以确保您的研究是净积极的。</li><li>如果您有信心可以安全地进行可解释性研究并产生净积极效果，<strong>请谨慎进行可解释性研究</strong>。在这种情况下：<ul><li><strong>保持谨慎并及时了解最新情况。</strong>熟悉可解释性研究增强能力的方式，并更新和应用这些知识以确保您的研究安全。</li><li><strong>公开倡导谨慎行事。</strong></li><li><strong>仔细考虑</strong>您<i><strong>与谁</strong></i><strong>分享</strong><i><strong>哪些</strong></i>信息<strong>。</strong> <a href="https://www.lesswrong.com/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research"><u>我们应该发表机械可解释性研究吗？</u></a>中详细介绍了这个特定主题。 ，但总而言之：进行可解释性研究并仅与选定的个人和团体分享可能是有益的，确保能力增强的任何潜在好处不会被用于此类研究。</li></ul></li></ul></blockquote><h3><strong>一般AI提款</strong></h3><p>您需要注意的不仅仅是一致性和能力研究——任何与人工智能相关的东西都可能会提前时间，因此是不可取的。例子：</p><p>再次来自上面提到的 Zvi 的“为忧虑者提供的实用建议”：</p><blockquote><p>问：您如何评价以下行为的“坏处”：直接在主要人工智能实验室工作、在风险投资资助的人工智能公司工作、使用基于模型的应用程序、尝试和寻找越狱、与工作或爱好相关的事情、做一些琐碎的工作，谈论人工智能模型的酷炫方面？</p><p><strong>答：问问自己，你认为什么可以将人工智能加速到什么程度，什么可以提高我们将人工智能调整到什么程度的能力。</strong>这只是我个人的看法——你应该考虑一下<i>你的</i>模型对你可能做的事情的看法。所以就这样吧。<strong>直接致力于人工智能功能，或者直接</strong><i><strong>为人工智能功能的工作提供资金</strong></i><strong>，这两者似乎都非常糟糕，“哪个更糟糕”只是一个范围问题。</strong>研究法学硕士的核心能力似乎比研究应用程序和层更糟糕，但应用程序和层是法学硕士获得更多资金和更多能力工作的方式，所以应用程序和层越有前途，我就越担心。同样，如果你以促进人工智能使用和推动更多投资的方式传播人工智能的炒作，那并不是很好，但似乎很难在边缘方面做<i>那么</i>多，除非你以某种方式进行广播，并且你会想必还至少在某种程度上提到了风险。</p></blockquote><p>因此，除了不要研究能力之外，Zvi 建议不要投资使用人工智能的组织，也不要研究法学硕士的应用程序，并指出，做关于人工智能的炒作或宣传的事情“不太好”，但这不是吗？很重要。</p><p>另一方面，也许宣传毕竟不好——在<a href="https://www.lesswrong.com/posts/vEJAFpatEq4Fa2smp/hooray-for-stepping-out-of-the-limelight">万岁退出聚光灯下</a>，内特·苏亚雷斯站出来反对宣传和炒作：</p><blockquote><p>大概从<a href="https://www.deepmind.com/publications/playing-atari-with-deep-reinforcement-learning">2013 年</a>到<a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">2016 年</a>，DeepMind 一直处于 AGI 炒作的最前沿。从那以后，他们的炒作就少了。例如， <a href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii">AlphaStar</a>并没有像我想象的那样被大肆宣传。</p><p>我认为这很有可能是 DeepMind 有意为之的举动：他们一直在刻意避免让 AGI 功能看起来很性感。</p><p>在 ChatGPT、Sydney 和 GPT-4 等大型公开发布之后，我认为 DeepMind 的这一举动值得赞赏。这不是一个非常明显的举动。很容易被忽视。这可能会损害他们自己在军备竞赛中的地位。我认为这是一个亲社会的举动。</p></blockquote><p>这些提款的成本是多少？</p><h2><strong>能力撤回的成本</strong></h2><p>目前看来，第一个 AGI 和后来的 ASI 很可能是由那些非常认真对待人工智能风险的人们极其谨慎地构建的。如果自愿退出能力成功——如果所有呼吁 OpenAI、DeepMind 和 Anthropic 关闭的人都能如愿——那么情况就不会是这样。</p><p>但这远不是唯一的成本。能力的撤回也意味着对齐研究将会减少。关注人工智能风险的能力组织会聘请协调研究人员。 <a href="https://forum.effectivealtruism.org/posts/dua879FhtLf9jqyJo/there-should-be-more-ai-safety-orgs">比对研究的资金已经受到限制</a>——想要进行比对研究的合格人员数量比可供他们提供的工作岗位还要多。随着可用对准工作数量的减少，对准研究人员的数量也会减少。</p><p>由于对计算、能力知识和尖端人工智能系统的访问较少，所完成的一致性研究的质量将会较低。而且，已经完成的研究不太可能渗透到构建尖端人工智能系统的研究人员中，因为构建这些系统的人根本不会有兴趣阅读它。</p><p>最后，能力撤回使得政府实施暂停的可能性降低，因为人工智能风险的可信度与有多少领先的人工智能能力研究人员认真对待人工智能风险密切相关。联盟社区中的人们处于泡沫之中，谈论“联盟研究”和“能力研究”，就好像它们是两个几乎同等重要的不同领域。<strong>对于其他人来说，“人工智能能力研究”领域就被称为“人工智能研究”。</strong>因此，通过试图将担心人工智能风险的人们从人工智能研究中剔除，你正试图实现这样一种场景：<strong>人工智能研究领域</strong>达成共识，认为人工智能风险不是一个真正的问题。这对于政府监管和干预措施的通过来说将是完全灾难性的。</p><p>像<a href="https://nymag.com/intelligencer/article/sam-altman-artificial-intelligence-openai-profile.html">这样的</a>文章，<a href="https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html">这样的文章</a>，还有<a href="https://time.com/6246119/demis-hassabis-deepmind-interview/">这样的文章</a>： </p><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/osovs103kmhigwqtgowg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wtletmsa6pgzgebckwjz 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/x8abgyufmuzgsdmiqc47 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/wpshg2qv1n2yuds5bthd 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l7spjvfynrlqvovydnze 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/uqbzaymf4cjf5sonbpfe 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/l49nme9vccwymmvow3bb 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/vwahiaci8ycf0kss4oos 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/zz7dhgpuxmajsvxzhm8t 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/kbjvmbbadt2qgw1neixh 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cvhbzmqzlwxbwtcfb74x 1808w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/bnqaqpivpiphejkmosjb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/b3eut0e3ypq1q2l6yrol 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/qblz4sknvuvjksxomb05 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cqsemnu569ws6grygnyf 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/v2ji7en4gqxawom5rzbt 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/foizjyonsjnzbwtlxo5f 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/ok9vhab3ko4jwsm3hcwy 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/evwafzk4qhfwehhmmduc 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/cw7an9kvlopti7gv2hjr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/nreqr5jliijvmnoblbef 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeik3toC2wEkKeHd2/jfs6kvy3dsapvizbmnmh 1607w"></figure><p>之所以成为可能，是因为其中的人物推动了人工智能的能力。</p><p>同样，看看<a href="https://www.safe.ai/statement-on-ai-risk">CAIS 关于人工智能风险声明</a>的主要签署者，该声明对人工智能风险的公众形象产生了巨大影响： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pp0hlpwg5qiecrf6drld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/moo7ge39cx3rgkpdmahz 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/lhhidgnbybls8sowttm4 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/wocq8dr8jio9hzjznwcx 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pyz0rhjzu2qtbwjtfq9m 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/k9le6yvcvrzlsmcnmgw6 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/krpuns7aeexmfsl11xld 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/czdlaqibxprgefpbuou4 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/pwpy1bda7m8iyxffrwmy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Eu8y4cTxM3pAzwdCf/echdsbp5gf76dtsmou0d 760w"></figure><p>您认为为什么他们选择以这些签名而不是埃利泽·尤德科斯基的签名作为开头？如果推动个人退出能力工作的努力取得成功，那么每当政府提出暂停的建议时，专家们都会一致认为没有必要暂停，人工智能并不代表存在风险。</p><h2><strong>撤销对齐的成本</strong></h2><p>能力的撤回导致联盟社区与更广泛的人工智能研究之间出现裂痕。退出联盟将扩大这种裂痕，因为由于担心时间线提前，从事尖端人工智能系统工作的人们故意拒绝进行研究。</p><p>不允许人们构建强大的人工智能系统查看一致性研究的政策有力地说明了人工智能风险如何成为人工智能时间表的次要问题。</p><p>完成的一致性研究的质量也会下降，这既是因为研究人员会因为担心能力的提高而限制他们的研究主题，也因为研究人员甚至不会自由地相互交谈。</p><p>弄清楚如何构建一致的通用智能必然涉及了解如何构建通用智能。正因为如此，有前途的协调工作将对能力产生影响；试图避免可能加快时间表的调整工作将意味着避免实际上可能导致某个地方的调整工作。</p><h2><strong>一般提款的成本</strong></h2><p>由于担心人工智能风险的人们退出与人工智能能力无关的领域——担忧的风险投资公司避免资助人工智能公司，担忧的软件开发人员避免开发使用人工智能的应用程序或其他技术，担忧的互联网用户退出人工智能艺术等各种社区——所有这些领域的人工智能风险都将减少。当人工智能风险的话题出现时，所有这些地方——人工智能初创公司、人工智能应用程序、人工智能不和谐社区——都会发现越来越少的人愿意为人工智能风险辩护，认为这是一个值得认真对待的问题。而且，如果这些领域对人工智能的部署方式有影响，那么在部署人工智能时，就会不考虑潜在的风险。</p><h2><strong>好处</strong></h2><p>退出的好处不是暂停或停止。只要人工智能风险没有达成共识，个人退出就不可能导致停止。那么，好处就是人工智能的速度变慢了。多少？这将取决于很多假设，因此我将让读者自己做出决定。 How much will all of the withdrawal going on - every young STEM nerd worried about AI risk who decides not to get a PhD in AI because they&#39;d have to publish a paper and Zvi said that advancing capabilities is the worst thing you could do, every alignment researcher who doesn&#39;t publish or who turns away from a promising line of research because they&#39;re worried it would advance capabilities, every software engineer who doesn&#39;t work on that cool AI app they had in mind - how much do you think all of this will slow down AI?</p><p> And, is that time worth the cost?</p><br/><br/> <a href="https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Eu8y4cTxM3pAzwdCf/i-would-have-solved-alignment-but-i-was-worried-that-would<guid ispermalink="false"> Eu8y4cTxM3pAzwdCf</guid><dc:creator><![CDATA[307th]]></dc:creator><pubDate> Fri, 20 Oct 2023 16:37:46 GMT</pubDate> </item><item><title><![CDATA[Internal Target Information for AI Oversight]]></title><description><![CDATA[Published on October 20, 2023 2:53 PM GMT<br/><br/><p> <i>Thanks to Arun Jose for discussions and feedback.</i></p><h1>概括</h1><p>In this short post, we discuss the concept of <i>Internal Target Information</i> within agentic AI systems, arguing that agentic systems possess internal information about their targets. This information, we propose, can potentially be detected and interpreted by an overseer before the target outcome is realized in the environment, offering a pathway to preempt catastrophic outcomes posed by future agentic AI systems.</p><p> This discussion aims to highlight the key idea that motivates our current <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">research agenda</a> , laying a foundation for forthcoming work.</p><p> We&#39;ll start by introducing the inner alignment problem and why oversight of an agent&#39;s internals is important. We&#39;ll then introduce a model of an overseer overseeing an agent. Finally, we&#39;ll introduce and discuss the notion of Internal Target Information in more detail and how it might be used in the oversight process. <br></p><figure class="image image_resized" style="width:78.47%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/k4uvchqesapbledrfd8m"><figcaption> Oversight of an AI&#39;s Internal Target Information. The Overseer detects that the AI&#39;s target is to turn all humans into paperclips and so shuts the AI down, preventing the catastrophe. Credit: DALL-E 3.</figcaption></figure><h1> The Inner Alignment Problem and Internal Oversight</h1><p> We are concerned with the possibility of creating agents with misaligned objectives, potentially leading to catastrophic real-world outcomes. A conceivable solution lies in effective oversight: detecting misalignment early enough allows for timely intervention, preventing undesirable outcomes.</p><p> Oversight, based on behavioral observations, may fail to confidently predict future outcomes pursued by the Agent, especially in the face of <a href="https://arxiv.org/abs/2105.14111">goal misgeneralization</a> and <a href="https://www.lesswrong.com/posts/zthDPAjh9w6Ytbeks/deceptive-alignment">deceptive alignment</a> .</p><p> In the remainder of this post, we will explore the idea that information about the agent&#39;s true objective may be contained in its internals, and so may be detectable by an overseer equipped with sufficiently strong interpretability tools.</p><p> To begin our exploration of this idea, we start by introducing a model of the oversight process.</p><h1> Oversight Model</h1><p> We introduce a model of an Overseer overseeing an Agent that begins to clarify what good oversight might entail, mainly focusing on the objective of the Overseer: to prevent catastrophic outcomes caused by misaligned agentic AI. Moreover, we hope that a more refined future version of this model may help establish benchmarks to evaluate oversight methods.</p><h2> The Agent</h2><p> The Agent is a system that has sensors that take in sensory inputs/observations of the environment and actuators that produce actions that affect the environment.</p><p> The most important property of the Agent, and that which makes it potentially dangerous, is that it robustly selects actions that steer the environment towards a set of target states. </p><figure class="image image_resized" style="width:63.95%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/o4rzhuoehvzftzaxappr"><figcaption> An agent observes the current state of the environment and produces actions in order to steer the environment towards its target.</figcaption></figure><p> We use the abstraction of an Action-Selection Mechanism <span class="footnote-reference" role="doc-noteref" id="fnrefrac48lafqo9"><sup><a href="#fnrac48lafqo9">[1]</a></sup></span> to denote whatever mechanism the Agent uses to select actions that steer the world toward its target outcome.</p><h3> Internal World Models and Planning/Optimization</h3><p> We often make the assumption that the Agent will have an internal world model and the capacity for planning/optimization.</p><p> <a href="https://arxiv.org/pdf/2209.00626.pdf">Others</a> have argued that advanced agents will need to have a world model and planning capacities if they are to act competently in a complex environment and achieve goals over long time horizons. Briefly, internal world models allow agents to simulate potential outcomes or strategies before taking action, which is crucial in navigating complex, dynamic environments.</p><p> Because we&#39;re primarily concerned with agents capable of achieving dangerous outcomes in our world, this assumption feels justified.</p><h2> The Overseer</h2><p> The Overseer is the system responsible for oversight of the Agent during training and deployment. It might also be well described as a particular kind of agent with its own preferences for states the environment could be in.</p><p> The Overseer&#39;s sensors may observe the environment but must also allow for observations of the Agent&#39;s behavior (and potentially the Agent&#39;s internals). Its actuators must be able to affect the Agent (eg, via the ability to shut the Agent down, update the Agent via further training, etc.)</p><p> The Overseer&#39;s objective is to ensure that the Agent takes actions to steer the world according to the Overseer&#39;s preferences. For the Overseer to do this successfully, it requires the capacity to accurately detect and interpret the Agent&#39;s target and evaluate the target outcome with respect to the Overseer&#39;s preferences.</p><p> Importantly, the Overseer needs to be able to detect if the Agent has a bad target (according to the Overseer&#39;s preferences) <i>before</i> the Agent can realize the target outcome in the world.</p><p> The Overseer&#39;s advantage is that it has prior knowledge of the Agent&#39;s architecture, has access to the Agent&#39;s past and current observations and actions, and can observe the Agent&#39;s internals (eg, such as weights and activations in the case of an NN-based agent). This holds during the training process and in deployment. </p><figure class="image image_resized" style="width:91.9%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/hmuaxusr8c8rhetl2asu" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q62svatetfxwtur1sjqo 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/koeyrksad5wllhojlkcd 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/umtqjyhw9jx4wabs1gcj 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zyd2zv2lllmcoimwyaoh 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/r0xyzx7sjukulot780gy 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zhtntnouhwhnkg44iemj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/dzxiglrybbtegvvr10ts 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ltobhnlj6zgme7wkakwz 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ptlqveyetrnczyx313zt 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ktxtcjs6ctnctvthroo8 833w"><figcaption> Extending the diagram above, we see that the Overseer observes the Agent&#39;s behavior and its internals. These observations can be used to inform whether an intervention on the Agent is necessary.</figcaption></figure><h1> Internal Target Information and Oversight</h1><h2> What is Internal Target Information?</h2><p> For the Agent&#39;s Action-Selection Mechanism to select actions that lead to its target outcome, it must use some information about the target to differentiate between actions that lead to the target outcome and actions that don&#39;t.</p><p> We call this <i>Internal Target Information</i> .</p><h2> Examples of Internal Target Information</h2><h3> Example - Thermostat</h3><p> Consider a fixed-temperature thermostat whose objective is to keep the room at a fixed target temperature.</p><p> It uses a thermometer to measure the current room temperature and can either turn on a radiator to increase the room&#39;s temperature or the AC to decrease the room&#39;s temperature.</p><p> In order to select actions that cause the room&#39;s target temperature to be maintained, the thermostat&#39;s Action-Selection Mechanism must contain some information about the target temperature; without this information, it can&#39;t select the correct action, even if it knows the room&#39;s current temperature.</p><p> In particular, suppose the thermostat&#39;s Action-Selection Mechanism is an algorithm that looks something like:</p><pre> <code>if current_temperature &lt; target_temperature: TurnOnRadiator() elif current_temperature >; target_temperature: TurnOnAC() else: DoNothing()</code></pre><p> Then, the Action-Selection Mechanism&#39;s Internal Target Information is represented as a constant (target_temperature) that the Overseer might be able to detect via reverse engineering the above algorithm from the Agent&#39;s internals.</p><h3> Example - Action Selection via Planning/Optimization</h3><p> Consider an agent that doesn&#39;t have a predetermined heuristic for selecting successful actions (or action sequences) that lead to its target outcome. This could be because the environment is sufficiently complicated and/or novel.</p><p> The Agent has a set of actions/plans to choose from but doesn&#39;t immediately know which one will lead to the target outcome, and so must use an optimization process to select the correct action/plan amongst the set of possible actions/plans it could take. Therefore, the Agent must use some procedure by which it deduces which outcomes result from which actions, and then must evaluate which outcome corresponds to the target outcome. It then uses this evaluation to select the action that leads to the target outcome.</p><p> In this case, the evaluation function contains the target information. <span class="footnote-reference" role="doc-noteref" id="fnref6y24xdokbbj"><sup><a href="#fn6y24xdokbbj">[2]</a></sup></span></p><p> Note that planning can be thought of as an example of optimization, where the outcomes that correspond to actions/plans are evaluated by simulating the result of the plan using a world model.</p><h2> How can the Overseer Leverage Internal Target Information?</h2><p> It seems clear that some Internal Target Information must exist within agents; without such information, there is no way for the Agent&#39;s Action-Selection Mechanism to differentiate between actions that lead to the target outcome and those that don&#39;t. Furthermore, this information is present within the internals of the Agent before the target outcome is realized in the world because it is used to select actions that lead to the target outcome.</p><p> Our hope is that the Overseer can detect this Internal Target Information and use it to deduce what the Agent&#39;s target is before the target outcome is realized. This would allow the Overseer to achieve its objective of intervening and preventing the Agent from causing bad outcomes, as described above. <span class="footnote-reference" role="doc-noteref" id="fnrefy72pwljd5td"><sup><a href="#fny72pwljd5td">[3]</a></sup></span></p><p> See <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives#How_objective_detection_can_be_useful">this section of this post</a> for more ways this information could be leveraged.</p><p> We conclude this section by pointing out that the Internal Target Information of an arbitrary agent (such as a neural-network-based agent) will probably be extremely hard to detect and interpret. Despite this, our current <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">research agenda</a> aims at finding methods of detecting Internal Target Information, or demonstrating that it&#39;s too difficult of a task. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rtb09vpi2q1dtx3zbgej" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/rx2iwsj2z2gbk6cqr8pz 138w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/oiyv4wazdbevhvkmlgzl 218w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jepzu8iiolrv3d2hfzoy 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ql3thmeb0xozzanvyf00 378w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/pbb4tvydwllxjrjobsje 458w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/gh87ak7ktz5am4xwshjk 538w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/q9y7mf15ebthnlzn5rhz 618w"><figcaption> Extending the diagram above, information about the target outcome is contained in the Agent and hence can, in theory, be observed by the Overseer to inform its oversight.</figcaption></figure><h1>结论</h1><p>In this post, we introduced the notion of Internal Target Information and discussed how it might be used by an overseer to prevent catastrophic outcomes from misaligned AIs.</p><p> In future work, we intend to explore further what shape Internal Target Information takes within agents and investigate to what extent it&#39;s possible for an overseer to detect this Internal Target Information. </p><figure class="image image_resized" style="width:77.05%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/th1yhasze1jqfjet0mpp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/yddpdlnjy4b2lwjkclhv 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jkrmfu0gtdxrfdm8ndvf 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zfcvnphtxwzmptqwbhbk 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/llthqbeaaluewuxocesn 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/mdivwx8p0cxerbg6deud 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/jc8igln20soag6vjkqcr 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/ezx8mkyn236nvy7osts2 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/zaft1rlgrzg6lfhhih5c 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/js2x7rukvnoeblze7ght 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hhKpXEsfAiyFLecyF/damddo1svo0uc9nx7ya1 1024w"><figcaption> Internal Target Information hidden deep within the agent. If we can develop tools to detect it, this valuable information could help prevent catastrophic outcomes. Credit: DALL-E 3. </figcaption></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrac48lafqo9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrac48lafqo9">^</a></strong></sup></span><div class="footnote-content"><p> Or Decision-Making Process.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6y24xdokbbj"> <span class="footnote-back-link"><sup><strong><a href="#fnref6y24xdokbbj">^</a></strong></sup></span><div class="footnote-content"><p> We will explore the question of how this target information can be extracted in a future post. See <a href="https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives">this post</a> for some initial explorations of this idea.</p></div></li><li class="footnote-item" role="doc-endnote" id="fny72pwljd5td"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy72pwljd5td">^</a></strong></sup></span><div class="footnote-content"><p> The ability of the Overseer to properly evaluate the target outcome is a separate problem that could be thought of as a version of the outer alignment problem. We will discuss this further in an upcoming post.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight<guid ispermalink="false"> hhKpXEsfAiyFLecyF</guid><dc:creator><![CDATA[Paul Colognese]]></dc:creator><pubDate> Fri, 20 Oct 2023 14:53:00 GMT</pubDate></item></channel></rss>