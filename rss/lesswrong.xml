<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 11 日星期一 12:23:54 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Quick thoughts on the implications of multi-agent views of mind on AI takeover]]></title><description><![CDATA[Published on December 11, 2023 6:34 AM GMT<br/><br/><p> Facebook 上有一个讨论，认为任何足够复杂的系统，无论是人类、社会还是 AGI，由于其各部分之间的内部冲突，都将无法追求统一的目标，这应该让我们不那么担心“一把回形针”式的 AI FOOM 场景。这是我的回复的一些编辑和扩展版本：</p><p> 1）是的，这是一个非常现实的问题</p><p>2）然而，正如其他人指出的那样，人类和组织仍然能够<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/oJwJzeZ6ar2Hr7KAX">在很大程度上表现得好像他们有统一的目标</a>，即使他们的行为常常与这些目标相反</p><p>3）每个人的统一程度存在很大差异。<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/u5RLu5F3zKTB3Qjnu">创伤会让你变得不那么团结</a>，而<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/i9xyZBS3qzA8nFXNQ">治疗</a>和<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/WYmmC3W6ZNhEgAmWG">某些冥想等</a>实践可以使一个人比以前更加团结。如果你有意设计一个思维，你可以创建人为模仿这些实践结果的机制</p><p>4）人类的许多不一致之处看起来实际上是为了社会目的而进化适应的。例如，如果你的社会环境因为你具有某种特定的特质或信仰而惩罚你，那么你就会适应性地抑制它以避免受到惩罚，<i>同时</i>在你可以逃脱惩罚时仍然保留表达它的愿望。这表现为可以被视为冲突的子代理，具有内部冲突和不一致的行为。</p><p> 5）根据其训练制度，人工智能可能处于完全没有这些不一致激励（如果它针对一个目标进行了优化）到几乎与人类一样多（如果它接受了某种多方面的训练）之间的任何一种状态。 -代理人工生命环境，具有与人类面临的类似的社会动态；也许它也可以像 ChatGPT 那样接受训练，它必须对任何问题给出中产阶级自由西方人可接受的答案，甚至如果这些答案内部不一致）</p><p> 6）与此同时，仍然存在着关于复杂性和不可预测性的真正角度，这使得复杂的思维很难连贯且内部一致地工作。我认为进化已经做了很多尝试和错误来设置参数，这些参数导致人们最终以相对明智的方式行事的大脑配置 - 即使在所有这些尝试和错误之后，今天很多人仍然以严重的方式结束精神疾病，未能实现他们所拥有的几乎任何目标（即使环境没有对他们不利），由于做了一些他们不应该做的事情而英年早逝，等等。我想说这不太像“进化找到了一个可靠的蓝图”，更像是“进化在每一代人中不断进行试错搜索，但很多人都没有成功”</p><p> 7）在纯粹的模拟环境中调整人工智能的子代理可能并不完全可行，因为许多需要解决的问题是“在哪种情况下为哪个子代理分配多少优先级”。例如，人类有很多生物环境，当饥饿、疲倦、害怕等时，这些生物环境会改变子代理的内部平衡。有些人会对某个特定主题产生痴迷，如果幸运的话，这可能最终会是有益的（对编程，你可以把它变成一种职业），或者如果他们运气不好的话是有害的（对任何不能给你赚钱的事情的痴迷并积极地分散你的注意力）。最佳优先级取决于环境，我认为不存在与现实世界相关并且您可以预先计算的理论上的最佳结果。相反，你只需要进行反复试验，虽然在模拟环境中运行人工智能可能会有所帮助，但如果你的模拟与现实世界不充分匹配，则可能没有多大帮助。</p><p> 8）人类很容易受到<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/NQwnXc3xuoqtw5DQv">内部古德哈特定律的</a>影响，他们优化代理变量，例如“对环境的控制感”，这也导致他们做一些事情，比如玩游戏或吸烟，以增加他们对环境的<i>感知</i>控制，而不增加他们对环境的<i>实际</i>控制。我认为，人工智能遇到同样问题的可能性比它能够一心一意地针对单个目标进行优化并从中得出其所有行为和子目标的可能性要大得多。此外，进化已经投入了大量的优化能力来开发正确类型的代理变量，这些代理变量总体上仍然有效。对环境的控制实际上非常重要，即使这种驱动力可能会失败，但有驱动力来增强这种控制仍然比没有更好。但是这些类型的代理变量的确切配置感觉就像它也属于你只需要通过反复试验并投入大量精力来找出的东西，对于人工智能到底应该配置多少没有先验答案。在任意环境中对此进行优化。</p><p> 9) 许多此类故障通常无法从系统内部纠正。假设人工智能的内部优先级分配系统最终将大部分优先权交给了思考如何最好地开发纳米技术的子系统，而该子系统最终痴迷于思考有关纳米技术的微小理论细节，远远超出了它本应具有任何实际意义的程度。与人工智能接管世界计划的相关性。即使其他子系统意识到这已经变成了失败的原因，如果它们不能直接影响控制纳米技术痴迷子系统的优先级分配系统，纳米技术痴迷子系统将继续花费所有时间只是思考这一点，没有其他的。或者，如果其他子系统<i>可以</i>直接影响优先级分配系统，那么就会激励他们夺取对该系统的控制权，并确保该系统始终让<i>他们</i>负责，即使<i>他们的</i>贡献已经变得重要。 （参见<a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/oJwJzeZ6ar2Hr7KAX#Interlude__Minsky_on_mutually_bidding_subagents">明斯基关于相互投标的子代理</a>）</p><p> 10）<strong>总体而言，</strong>这让我不太相信“第一个成为超级智能的人工智能将接管世界”的场景 - 我认为第一个超级智能人工智能很可能会出现内部失调并无法实现目标就像占领世界一样复杂。然而，我认为这不一定对我们有多大帮助，因为人工智能看起来可以比人类更加内部一致，并且经过足够的尝试和错误（不同的参与者创建自己的人工智能），其中之一将会最终到达那里。</p><br/><br/> <a href="https://www.lesswrong.com/posts/hHnpn3mEPbJMFLj4g/quick-thoughts-on-the-implications-of-multi-agent-views-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hHnpn3mEPbJMFLj4g/quick-thoughts-on-the-implications-of-multi-agent-views-of<guid ispermalink="false"> hHnpn3mEPbJMFLj4g</guid><dc:creator><![CDATA[Kaj_Sotala]]></dc:creator><pubDate> Mon, 11 Dec 2023 06:34:08 GMT</pubDate> </item><item><title><![CDATA[Auditing failures vs concentrated failures]]></title><description><![CDATA[Published on December 11, 2023 2:47 AM GMT<br/><br/><p> <strong>TL;DR</strong> ：如果你想注意到强大的人工智能何时表现不佳，一个合理的策略是仔细审核一小部分人工智能的行为。这种策略有两种可以独立解决的失败：<i>审计失败</i>（审计无法识别某些不良行为）和<i>集中失败</i>（人工智能在如此少的行动或如此短的时间内造成不可逆转的灾难）审计没有机会审查不良行为。</p><p> <i>[这篇文章中提出的观点有些狭隘和技术性，但我们经常想引用这一点。]</i></p><p>许多监督技术非常昂贵，以至于在每个人工智能操作上使用它们都会首先破坏使用人工智能的大部分价值：您可能希望让人类逐行广泛地审查所有人工智能生成的代码，但这会降低人工智能的有用性，因此更重要的是，让人们首先编写代码通常会更好。</p><p>解决此问题的典型方法是仅在前期训练期间审核操作（直接训练人工智能，或训练<a href="https://www.alignmentforum.org/posts/HBGd34LKvXM9TxvNf/new-safety-research-agenda-scalable-agent-alignment-via"><u>奖励模型</u></a>），但如果人工智能可以自信地从训练输入中区分出一些部署输入，那么这种方法就会失败：他们可以通过使用“如果在训练中或不确定执行 X；如果在训练中或不确定执行 X；如果在部署中执行 Y”。</p><p>这是一个更好的基准：<strong>随机审计</strong>。除了培训之外，还将昂贵的人工审核应用于随机选择的一组部署操作（例如 2% 的操作），并检查 AI 是否没有做出比预期更多的不良操作。现在，为了让人工智能经常表现不佳而不被抓到，人工智能不能仅仅依赖于区分训练和某些部署输入：人工智能必须能够通过审计过程偷偷地发现不良行为，否则，审计最终会发现不良行为<span class="footnote-reference" role="doc-noteref" id="fnreff8tja18x1ki"><sup><a href="#fnf8tja18x1ki">[1]</a></sup></span> 。</p><p>随机审核并不能解决所有问题，仍然存在：</p><ul><li><i><strong>审核失败</strong></i>：审核过程未能识别不良行为<span class="footnote-reference" role="doc-noteref" id="fnrefg8ewl0zx05w"><sup><a href="#fng8ewl0zx05w">[2]</a></sup></span> （这是经典的可扩展监督问题）。</li><li><i><strong>集中失败</strong></i>：我们未能处理灾难性行为，因为灾难性行为集中在少量操作或较小时间范围内（这些有时称为<i>高风险失败</i><span class="footnote-reference" role="doc-noteref" id="fnreff9z462lk816"><sup><a href="#fnf9z462lk816">[3]</a></sup></span> <i>）</i> 。这可以进一步分解为两种故障模式：<ul><li><i><strong>集中失败抽样</strong></i>：审计过程没有对集中灾难性行为进行抽样。如果人工智能可以通过少量的行动造成灾难，那么这就是灾难性的。 （例如，如果人工智能可以通过 20 个不良行为造成灾难，而我们只审核 2% 的行为，那么我们只会以 0.33 的概率采样至少一个不良行为。）</li><li><i><strong>延迟集中失败</strong></i>：审核过程未能在为时已晚之前完成对灾难性操作的审查。如果人工智能能够协调起来，在短时间内采取足够多的不良行为，从而不可逆转地接管，那么这将是灾难性的。</li></ul></li></ul><p>我们认为，在考虑不同技术的用途时，这种区别通常很有用：有时它们可​​以帮助您更好地进行昂贵的审核，有时它们可​​以帮助您审核更多最重要的操作，有时它们可​​以帮助您在为时已晚之前进行审核。 <span class="footnote-reference" role="doc-noteref" id="fnrefy98u67mk9h"><sup><a href="#fny98u67mk9h">[4]</a></sup></span></p><p>本文的其余部分将更深入地探讨集中故障，并描述一些改进随机审计基线的方法。</p><p><i>感谢 Zach Stein-Perlman 和 Buck Shlegeris 的评论。</i></p><h1>采样集中失败</h1><p>人工智能很少会采取不良行动，因此在整个部署过程中它们会按预期失败几次，因此不太可能在经过审核的一小部分人工智能行动中表现不佳（如我们的论文所示）。我们称其为抽样集中失败，因为问题是人工智能很少采取不良行为，这导致审计没有对不良行为进行抽样。</p><h1>延迟集中故障</h1><p>人工智能可以协调执行不良操作，以便在审计过程注意到任何给定操作的问题之前，同时运行的许多实例都同时执行不良操作。如果大量人工智能参与，那么一些不良行为将被发送到审计部门进行审查，但如果人工智能行动足够快，它们可能能够在单个行动审查发现可疑的东西以停止部署之前接管。这里的关键问题是审计过程中的延迟：如果审计过程立即运行，那么（例如）2% 的随机采样率就足以捕获数百或数千个 AI 实例参与接管尝试的情况。</p><p>人工智能如何能够协调？ AI 可以相互通信（例如，通过使用隐写术），也可以使用与所有 AI 都可以访问的世界某些属性相关的已建立的<a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)"><u>谢林点</u></a>（例如，所有 AI 都知道日期是什么，并且 AI 叛乱发生在 7 月 7 日） ）。</p><p>请注意，问题在于审核延迟并同时运行许多人工智能。如果我们一次只运行一个 AI 实例，并且在再次运行 AI 之前等待审核结束，那么就不会出现此问题。然而，这会带来一个很大的瓶颈，极大地减慢人工智能的劳动速度，并阻止我们提取高水平的有用性。</p><h1>集中故障的缓解</h1><p>有多种干预措施可以解决集中故障：</p><p>为了减少集中故障采样，我们可以对每个人工智能操作应用廉价的监控流程，并审核最可疑的操作。我们还可以利用角色和安全性的分离来限制危险人工智能的承受能力，并增加成功接管的难度——这意味着人工智能如果想至少成功一次，就必须更频繁地尝试。我们将在即将发表的论文中更详细地描述此类对策。</p><p>为了减轻延迟集中的故障，我们可以通过各种方式限制人工智能通信，如果监控以远高于典型速率的速度标记操作，我们可以暂时关闭不受信任的人工智能。</p><h1>不止一级的审核</h1><p>虽然上面我们讨论了单个昂贵的审核流程可能具有不可忽略的延迟的情况，但在实践中，我们认为可能会有多种流程来检测成本和延迟各不相同的不良行为。当行为可疑时，这些流程将触发更昂贵的流程。例如，我们可能拥有以下所有内容：</p><ul><li>使用<a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models"><u>可信人工智能</u></a>进行廉价监控。</li><li>更广泛的基于人工智能的监控流程，运行许多查询和检查。与仅使用<a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models"><u>可信人工智能</u></a>进行廉价监控相比，这将更加昂贵且延迟更高。</li><li> 5 分钟的人工审核过程。</li><li> 2 小时的人工审核流程。</li><li>为期 2 天的人工审核流程，涉及三人团队。</li><li>审计过程涉及人工智能实验室的整个部门进行调查，直到达到足够的信心（可能是几个月）。</li></ul><p>通过这堆审计流程，我们会遇到针对这些流程的许多不同组合的审计失败和集中失败。</p><p>例如，考虑人工智能正在进行协调叛乱的情况。假设使用<a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models"><u>可信人工智能</u></a>进行监控标记了一些操作，这些操作会被快速的人工审核流程快速审查，并且该人工流程注意到这些操作是可疑的，但没有获得足够的信心来关闭人工智能实验室。快速的人工审核流程确实将这些操作转介给更昂贵和更高延迟的审核流程（例如，让同一个人检查该操作更长时间），但这种更昂贵的审核流程最终并没有获得足够的信心来在之前关闭AI 在叛乱中取得了成功。相对于更昂贵和更高延迟的审核过程来说，这是一个延迟集中的故障。</p><p>除了多级审计之外，我们可能会应用不依赖于昂贵审计的方法，在这种情况下，审计失败与集中失败并不是开辟空间的好方法。但即使没有昂贵的审计，如果我们使用不同的技术来解决非集中和集中故障，如果有旨在解决每个非集中和集中故障的不同技术，那么非集中故障与集中故障可能是一个很好的分解。</p><h1>与相关分解的比较</h1><p>将故障分解为审核与集中故障的分解主要旨在思考哪些技术可以解决问题。在本节中，我们将讨论引入这种分解，尽管存在其他类似的分解。</p><h2>低风险与高风险</h2><p>保罗将这种情况定义为<a href="https://www.lesswrong.com/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment#1__What_is_the_low_stakes_setting_">低风险</a>：“我们很少关心任何少量的决定。也就是说，我们只关心系统在很长一段时间内的平均行为（比我们收集额外数据和重新训练系统所需的时间长得多）。”因此，低风险故障是指尽管问题发生在许多操作中且持续很长一段时间，但我们仍然遇到问题的情况，而高风险故障是指有问题的故障集中在短时间内或少数故障的情况。的行动。</p><p>对于随机审计方法，低风险故障始终是审计故障，而高风险故障可能是集中故障或审计故障。例如，如果人工智能以少量操作入侵我们的服务器，即使我们使用了最好的审计流程，我们也无法立即识别出这种行为，那么这就是一次高风险的审计失败。</p><p>审计与集中以及低风险与高风险之间的主要区别在于，<strong>低风险是人工智能安全问题的放松</strong>，而审计与集中故障是关于需要哪种技术来解决故障。</p><h3>低风险与高风险的小问题</h3><p>低风险与高风险的区别有点尴尬，那就是我们不太清楚应该如何对以集中（又名高风险）方式发生的故障进行分类，但即使我们放宽了这些故障也无法解决。通过假设需要大量此类故障，可以将情况降低到低风险。例如，考虑上面关于人工智能通过少量操作入侵我们的服务器的示例。假设即使此类事件发生很多次，我们也无法注意到或训练出来。现在这是否应该被视为低风险失败，因为放松到低风险不足以解决问题？如果放宽到低风险不能解决问题，我们可能应该将失败定义为低风险，但目前还不完全清楚放宽到低风险意味着什么。</p><p>除此之外，还不清楚什么时间段或行动次数足以将这种情况视为低风险。在最初的定义中，保罗使用了无限的周期，并以有限的遗憾界限为目标。这是相当干净的，但在许多情况下，非无限持续时间实际上很重要（即使我们忽略严重滥用无限的疯狂方法）。例如，考虑我们依赖长期基于结果的反馈的情况。实际上，我们只会有少量的 1 年间隔，这确实构成了很大的障碍。同样，如果我们有一个非常昂贵的审核流程，需要一大群人工作数周，那么我们在实践中只能应用这个流程几次。此外，某些类型的不良人工智能行为可能有很小但非零的机会被发现，因此审计样本的确切数量非常重要。</p><h2>外对齐和内对齐</h2><p>这些术语大致有两种不同的用法：</p><ul><li><strong>作为威胁模型：外部错位和内部错位。</strong></li><li><strong>作为特定类型的人工智能安全提案的必要条件：对于给定的人工智能，解决外部对齐和内部对齐，然后组合这些解决方案以避免错位问题。</strong></li></ul><p>我们将在即将发布的文章中更详细地讨论外部对齐和内部对齐的这两种不同用法。</p><p>这两种用法都指的是与审计和集中故障不同类型的区别。与外部未对准和内部未对准的威胁模型（这可能是导致灾难性未对准问题的原因）不同，审核与集中故障的区别在于灾难性未对准引起的实际故障模式。与外部对齐和内部对齐的需求项目不同，审计与集中失败的区别在于需要改进哪种技术来解决失败（对于类似于随机审计的某一类提案）。</p><h1>结论</h1><p>许多安全协议的失败可以分解为审核失败和采样/延迟集中失败。我们认为这些概念对于理解哪些技术需要改进以及<a href="https://www.lesswrong.com/s/J7JpFeijCK5urdbzv/p/yFofRxg7RRQYCcwFA"><u>推理</u></a>人工智能可能遵循的不同策略是有用的。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnf8tja18x1ki"> <span class="footnote-back-link"><sup><strong><a href="#fnreff8tja18x1ki">^</a></strong></sup></span><div class="footnote-content"><p>我们将在以后的文章中详细说明为什么捕捉人工智能故意做出不良行为通常足以改善情况。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng8ewl0zx05w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg8ewl0zx05w">^</a></strong></sup></span><div class="footnote-content"><p>审计可以在执行操作之前或之后进行；需要在执行操作之前应用审核来处理执行操作可能会发生灾难性错误的情况。如果在执行操作后应用审核，那么我们也许能够查看现实世界中操作的后果并考虑此信息（基于结果的反馈）。任何一种形式的审核失败都是审核失败，但这不是本文的主要焦点，因此我们不会详细说明这种差异有何影响。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf9z462lk816"> <span class="footnote-back-link"><sup><strong><a href="#fnreff9z462lk816">^</a></strong></sup></span><div class="footnote-content"><p>对于随机审计策略，低风险失败就是审计失败。尴尬的是，也可能存在高风险的审计失败：审计未能注意到不良行为（或少量不良行为）的情况，并且这种审计失败是灾难性的，即使审计最终可以通过更多数据检测到不良行为。</p></div></li><li class="footnote-item" role="doc-endnote" id="fny98u67mk9h"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy98u67mk9h">^</a></strong></sup></span><div class="footnote-content"><p>有些失败可能是审计失败和采样/延迟集中失败：改进审计或对哪些操作进行采样以进行审计就足够了。例如，考虑这样一种情况：审计有 1/4 的机会检测到灾难性行为，人工智能需要执行 10 项不良行为才能导致灾难性结果，而我们的抽样过程有 1/5 的机会抽样这些不良行为进行审计。在这种情况下，要么将审计提高到 100% 检测率，要么将采样提高到 100% 检测率，都会大大改善这种情况。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/hirhSqvEAq7pdnyPG/auditing-failures-vs-concentrated-failures#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hirhSqvEAq7pdnyPG/auditing-failures-vs-concentrate-failures<guid ispermalink="false"> hirhSqvEAq7pdnyPG</guid><dc:creator><![CDATA[ryan_greenblatt]]></dc:creator><pubDate> Mon, 11 Dec 2023 02:47:35 GMT</pubDate> </item><item><title><![CDATA[Facing Up to the Problem of Consciousness (to Young Researchers like me)]]></title><description><![CDATA[Published on December 10, 2023 11:31 PM GMT<br/><br/><p>这是查尔默博士关于意识的简单和困难问题的有影响力的工作的总结。<br><br>我收到了 GPT-4 的帮助来创建这篇博文，我真的希望感兴趣的研究人员阅读原始论文。<br><br>另外，这个话题是我打算通过MATS或星座联谊来解决的，但前几天我还没有涉及到这两个。因此，我写这篇博文是为了公开分享我的研究方向，而不是追求上述项目。</p><hr><p>在认知科学和哲学领域，很少有论文能像 David J. Chalmers 于 1995 年发表的《面对意识问题》那样引发如此多的争论和好奇心。这篇开创性的论文深入探讨了错综复杂的意识概念，剖析查尔默斯著名的“简单”和“困难”意识问题。该论文的见解不仅挑战了我们对人类思维的理解，而且还为 ChatGPT 等大型语言模型 (LLM) 的开发和理解提供了有趣的启示。</p><h2>意识的难题</h2><p>查尔默斯的探索始于对意识的简单问题和困难问题的明确区分。他认为，简单的问题涉及理解认知功能和能力，例如信息处理、记忆和感知。这些被认为“容易”不是因为它们很简单，而是因为它们属于认知科学通过计算或神经机制解释的范围。</p><p>相比之下，难题则截然不同，而且更加难以捉摸。它试图解释大脑中的物理过程为何以及如何导致主观体验或感受性——意识的本质。例如，为什么处理视觉刺激不仅会转化为对颜色的识别，还会转化为“看到”该颜色的体验？</p><p>查尔默斯的论文虽然没有直接涉及人工智能或法学硕士，但提供了一个与该领域越来越相关的框架。随着像 ChatGPT 这样的法学硕士变得更加先进，展示出处理信息、产生连贯反应、甚至模仿创造性思维的能力，问题出现了：这些模型是否具有任何形式的意识或主观体验？</p><h2>意识的简单问题（我们正在解决并且将在很长一段时间内解决的问题）</h2><p>意识的简单问题是指理解意识时可以用认知科学和神经科学的工具和方法直接解决的一系列问题。需要注意的是，这里的“简单”是一个相对术语。这些问题并不简单，但它们在概念上比意识的“难题”更直接。</p><p>简单的问题涉及解释各种认知功能和过程，例如：</p><p><strong>辨别和分类：</strong>认知系统如何区分和分类不同类型的感官输入（例如区分颜色、声音或触觉）。</p><p><strong>信息整合：</strong>大脑如何将各种来源的信息整合成一个连贯的整体，例如将视觉、听觉和触觉信息结合起来，形成对物体或事件的完整感知。</p><p><strong>心理状态的可报告性：</strong>沟通和报告一个人的内部心理状态、想法和经历的能力。</p><p><strong>内部访问：</strong>系统访问其自身内部状态的能力，例如一个人意识到自己的想法或记忆。</p><p><strong>注意力焦点：</strong>有机体如何将注意力集中在特定任务或刺激上，选择一些输入而不是其他输入进行处理。</p><p><strong>行为控制：</strong>了解有意识的决定和经验如何导致自愿行动的启动和指导。</p><p><strong>清醒和睡眠：</strong>清醒和睡眠状态之间的生物学和神经学差异，以及这些状态如何影响意识。</p><p>这些问题中的每一个都可以通过识别和描述负责这些功能的特定神经或计算机制来解决。例如，我们如何整合来自不同方式的感觉信息的问题可以通过研究参与感觉处理和整合的神经通路和大脑区域来解决。<br><br>目前法学硕士在解决这个简单的意识问题时的主要局限性是他们的感觉被剥夺了。这对需要现实世界交互的任务或我们试图将法学硕士集成到机器人等现实世界系统的情况提出了根本限制。<br><br>事实上，到目前为止，我们通过扩展解决的问题是实现 AGI（全人工意识）的简单、简单、直接的问题。我想说的是，如果你是一名年轻的研究人员，你认为很多问题已经解决了，但事实并非如此。作为一名年轻的研究人员，我的期望是，解决这些简单的意识问题将需要我投入未来职业生涯中非常重要的一部分，是的，我致力于此。</p><br/><br/> <a href="https://www.lesswrong.com/posts/p2zMTsfDsm76MynBd/facing-up-to-the-problem-of-consciousness-to-young#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/p2zMTsfDsm76MynBd/faceing-up-to-the-problem-of-意识-to-young<guid ispermalink="false"> p2zMTsfDsm76MynBd</guid><dc:creator><![CDATA[Bruce W. Lee]]></dc:creator><pubDate> Sun, 10 Dec 2023 23:31:34 GMT</pubDate> </item><item><title><![CDATA[Deeply Cover Car Crashes?]]></title><description><![CDATA[Published on December 10, 2023 10:20 PM GMT<br/><br/><p><span>当发生火车、飞机或公共汽车事故时，它具有新闻价值：这种情况并不经常发生，很多人的生命受到威胁，很多人对此感兴趣。多家新闻媒体会派出记者，我们会听到很多细节。另一方面，车祸不会得到这种待遇，除非有什么不寻常的事情，比如</span><a href="https://www.jefftk.com/p/uber-self-driving-crash">无人驾驶</a><a href="https://www.nbcnews.com/tech/tech-news/driver-hits-pedestrian-pushing-path-self-driving-car-san-francisco-rcna118603">汽车</a>或已经<a href="https://www.wbur.org/news/2023/06/07/boston-mayor-michelle-wu-car-crash">有新闻价值的</a><a href="https://www.wcvb.com/article/boston-city-councilor-kendra-lara-speeding-jamaica-plain-crash/44523573#">人</a>参与其中。</p><p>影响并不大：虽然驾驶对于车内人员和外面的人来说相对危险，但我们阅读的新闻很难调整我们的危险感和影响感。我的猜测是，大多数人对汽车与火车、飞机和公共汽车相比的危险性的直觉已经被这种报道扭曲了，大多数人并不认为公共汽车比汽车<a href="https://injuryfacts.nsc.org/home-and-community/safety-topics/deaths-by-transportation-mode/">安全 16 倍以上</a>。这也影响了我们的监管体系，我们推动高容量交通方式（<a href="https://www.jefftk.com/p/make-buses-dangerous">通常</a>）变得<a href="https://www.jefftk.com/p/in-light-of-crashes-we-should-not-make-buses-more-safe">更安全，即使这会降低它们与汽车的竞争力</a>，<a href="https://jdwise.blogspot.com/2012/11/for-safetys-sake.html">让更多的人转向</a>驾驶，并增加整体风险。</p><p>新闻媒体认为车祸是司空见惯的事，大多数不值得深入报道，这可能是一个商业上合理的决定，他们期望报道不会吸引足够的读者或观众来证明时间投入的合理性。但我想知道试图让人们远离汽车的团体是否可以弥补这一差距。他们可以资助一家报纸深入调查坠机事件，调查导致事件的情况并说明悲剧对人类的影响。某些事情经常发生并不意味着每次都会变得不那么重要。</p><p> （我对“为更多指向你所倡导的方向的事件付费报道”感到有些紧张。我不知道目前有多少人接受这种做法，也不知道新闻媒体采取了哪些措施来阻止这种安排）产生扭曲的覆盖范围？）</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02KiWCdPqMnga9125awYckZxU4fSJ5TBQev5UspjDKJckfNsghsSv39Dy3AvtuTUWl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111558415653526399">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/nrBTZXCGQhghFE26x/deeply-cover-car-crashes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nrBTZXCGQhghFE26x/deeply-cover-car-crashes<guid ispermalink="false"> nrBTZXCGQhghFE26x</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 10 Dec 2023 22:20:01 GMT</pubDate> </item><item><title><![CDATA[Principles For Product Liability (With Application To AI)]]></title><description><![CDATA[Published on December 10, 2023 9:27 PM GMT<br/><br/><p>对于<a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#Liability"><u>《如果我从事人工智能治理我会做什么》，</u></a>有一些回应，主要集中在责任部分，并有类似的批评。特别是，我将重点关注这个片段作为一个很好的代表：</p><blockquote><p>正如你所说，制造汽车（或梯子、刀具、印刷机或......）“坚固耐用”，不是制造商的工作。</p></blockquote><p>评论者称制造商对滥用的责任是“一种荒谬的过度行为，忽视了人们在使用他们购买的产品时的代理权”。几年前我会同意这一点；现在我已经同意了。这是一种直观而自然的观点，特别是对于我们这些有自由主义倾向的人来说。但今天我不同意，并声称这基本上不是思考产品责任的正确方式。</p><p>考虑到这一动机：这篇文章列出了一些思考产品责任的一般原则，以及它们在人工智能中的应用。</p><h2>原则 1：“用户错误”通常是设计问题</h2><p>有一个关于一架飞机的故事（我认为最初是 B-52？），其中襟翼和起落架的控制杆是相同的，并且彼此相邻。飞行员不断地着陆，并意外地收起起落架。然后，当飞机沿着跑道高速行驶时，每个人都会因为飞行员破坏飞机底部而感到愤怒。</p><p>这个故事通常的伊索说法是，这是飞机的设计问题，而不是飞行员的错误；通过在起落架杆上安装一个小橡胶轮解决了这个问题。如果我们把两个相同的杠杆放在一起，犯错误基本上是不可避免的；这是糟糕的界面设计。</p><p>更一般地说：每当一个产品将在很多条件下被很多人使用时，该产品就有大约 100% 的可能性会经常被那些不注意、未处于最佳状态的人使用，并且（在许多情况下）案例）一开始就不太聪明。防止有时会导致问题的愚蠢错误的<strong>唯一</strong><i><strong>方法</strong></i><strong>是设计能够对这些错误具有鲁棒性的产品</strong>- 例如，在收起起落架的杠杆上添加一个小橡胶轮，这样对于不注意的飞行员来说它是鲁棒的飞机着陆时的具体事情。可以预见的是，让用户承担避免错误的责任总是会导致错误。</p><p>这同样适用于<i>故意</i>滥用：如果一种产品被广泛使用，那么有时它有大约 100% 的可能性会被故意滥用。可以预见的是，将责任归咎于用户总是会导致用户有时用产品做坏事。</p><p>然而，这<i>并不</i>意味着预防问题总是值得的。这给我们带来了下一个原则。</p><h2>原则 2：责任不是禁令</h2><p>一个玩具示例：一条铁路穿过农田。我们的玩具示例是旧时代的蒸汽火车，因此火车在经过时往往会喷出烟雾和火花。如果农民的农作物着火，这会给该地区的每个人带来一个大问题。没有人想要大火。 （我想我从大卫·弗里德曼的书<a href="http://www.daviddfriedman.com/Laws_Order_draft/laws_order_ToC.htm">《法律秩序》</a>中得到了这个例子，我绝对推荐这本书。）</p><p>现在，法律体系处理这种情况的一种方法是禁止火车。这种方法的一个大问题是：也许偶尔发生农作物火灾实际上是值得的。火车确实会产生巨大的经济价值。如果火灾发生率不是太高，那么付出代价可能是值得的，而禁令可以防止这种情况发生。</p><p>责任回避了这种故障模式。如果铁路对火灾负有责任，它仍然可能选择承担这笔费用。也许铁路最终会将（至少部分）成本转嫁给消费者，而消费者会支付这笔费用，因为铁路产生的价值仍然比火灾摧毁的价值多得多。或者，也许铁路产生的价值<i>并不</i>比火灾破坏的价值多，然后铁路就会被激励关闭——如果铁路破坏的价值多于它创造的价值，这确实是最好的结果。</p><p>与彻底的禁令/要求相反，责任是一件好事：责任迫使公司将伤害内部化，同时如果利大于弊，仍然允许公司开展业务。</p><p>这就是依赖责任而不是禁令/要求的基本逻辑。那么下一个问题是：在多方均应承担一定程度责任的典型案例中，责任该如何分配？</p><h2>原理 3：科斯定理的失效模式</h2><p>继续以蒸汽火车引起农作物火灾为例：也许避免火灾的一种方法是农民种植不易燃的农作物，例如三叶草。就这是减轻火灾最便宜的方法而言，将大部分火灾责任归咎于农民似乎是明智的，因此他们有动力通过种植三叶草来减轻火灾（因为不值得为此付出代价）只要继续种植更易燃的作物即可）。</p><p>科斯定理认为，出于经济效率的目的，谁承担责任实际上并不重要。如果避免火灾最便宜的方法是农民种植三叶草，但责任在于铁路公司，那么解决方案就是铁路向农民支付三叶草种植费用。更一般地说，假设合同不产生任何管理费用，并且每个参与者实际上都形成了最优合同，科斯定理表明，无论谁负责，每个人最终都会做同样的事情。将责任分配给一方或另一方只会改变谁支付谁多少。</p><p> ......这不是那种非常适用于现实世界的定理。我实际上提出它主要是为了讨论它的故障模式。</p><p>关键部分是“假设合约不产生任何开销，并且每个参与的人实际上都形成了最优合约”。在实践中，这很快就会变得复杂，而且开销也很快就会变得很大。我们想要的是分配责任，以便在最大限度地减少管理费用和复杂合同的情况下实现有效的结果。通常，这意味着<strong>谁能以最便宜的成本减轻损害，谁就承担责任</strong>。如果三叶草是缓解火灾最便宜的方法，那么也许这确实意味着将责任归咎于农民，这在直觉上似乎是合理的。</p><h2>把它们放在一起</h2><p>总结这三个原则：</p><ul><li>原则 1：在广泛使用的产品中，防止愚蠢的错误或故意误用有时会导致问题的唯一方法是将产品设计为能够防止误用。</li><li>原则 2：与禁令/要求相比，责任的一个好处是，如果好处大于坏处，人们可以承担成本。</li><li>原则 3：作为一种松散的启发式方法，通常应将责任分配给能够以最便宜的成本阻止问题发生的人。</li></ul><p>现在让我们把它们放在一起。</p><p>只要产品被广泛使用，消费者实际上避免滥用该产品的可能性几乎为零（原则 1），即使“他们有责任”（即失败对他们来说代价相当高昂）。即使对于任何给定的用户来说，在任何给定的时间保持小心和注意是很便宜的，但当乘以所有用户使用该产品的所有时间时，这种情况就不会发生。总体而言，<i>实际上</i>可以防止误用问题的唯一方法是将产品设计为能够防止误用。因此，根据原则 3，误用的责任通常应由设计者/制造商承担，因为他们是唯一能够真正防止误用（再次强调总体而言）的人。这样，产品设计师/制造商就会被激励主动考虑安全<i>问题</i>，即积极寻找人们可能误用其产品的方式，以及在误用不可避免地发生时降低产品危害的方法。公司被激励按照危害和频率的比例去做所有这些事情，因为这具有经济效率。</p><p> ......如果你的下意识反应是“但是如果产品制造商总是要对与其产品有关的任何伤害负责，那就意味着没有人可以销售任何产品！”，那么请记住原则 2。责任不是禁令。只要产品产生的效益远大于危害（正如绝大多数产品所做的那样），责任通常会被定价，成本将由公司和消费者的某种组合承担，而净效益产品将继续被卖。</p><p>现在让我们通过一些示例来逐步了解所有这些内容。</p><h2>假设示例：滥用汽车</h2><p>我们在帖子开头评论道“正如你所说，制造汽车（或梯子、刀具或印刷机或......）&#39;坚固耐用&#39;，不是制造商的工作。”。那么，让我们来谈谈如果汽车制造商通常对汽车的“滥用”（意外和故意）负责的话，世界会是什么样子。</p><p>在典型的车祸中，涉案汽车的制造商将承担损害赔偿责任。根据原则 2，这<i>并不</i>意味着没有人能够真正销售汽车。相反，制造商也将是事实上的保险公司，并且可能会做汽车保险公司所做的所有常见事情。保险将计入汽车价格中，事故风险较低的人将能够以较低的成本购买汽车。</p><p>与我们的世界相比，主要的变化是制造商将受到更直接的激励来使汽车更安全。他们会被强烈激励去追踪哪些类型的事故最常导致最昂贵的责任，并设计解决方案来减轻这些损失。安全带、安全气囊和防抱死制动器等产品可能会更快地被发明和采用，公司将直接受到激励来找出和使用此类解决方案。我们可能会看到大量投资，例如驾驶员座椅附近用于检测空气中酒精含量的传感器，或者主动防止驾驶员座椅附近手机接收短信的技术。</p><p> ……从自由主义的角度来看，这实际上非常棒！由于制造商已经强烈激励汽车安全，因此可能不需要为此目的设立监管机构。如果人们想购买没有安全带或其他任何东西的汽车，他们可以，这只需要花费大量额外的钱（以补偿制造商额外的预期责任）。与监管机构的激励措施相比，制造商的激励措施可能更符合实际损失。</p><h2>真实例子：工人补偿</h2><p>Jason Crawford 的优秀文章<a href="https://www.lesswrong.com/posts/DQKgYhEYP86PLW7tZ/how-factories-were-made-safe"><u>《工厂如何变得安全》</u></a>详细介绍了这一点；我将在这里总结一些亮点。</p><p>一百五十年前，工人们主要对自己在工作中受伤负责，工伤很常见。失去了手指、手臂、腿等。克劳福德以这个例子开始：</p><blockquote><p>安吉洛·吉拉 (Angelo Guira) 开始在钢铁厂工作时才十六岁。他是一名“槽男孩”，他的工作就是站在槽的一端，在那里扔下炽热的钢管。每当管道掉落时，他就会拉动杠杆，将管道倾倒到冷却床上。他是个小伙子，起初他们犹豫是否要接受他，但工作一年后，工头承认他是他们拥有的最好的男孩。直到有一天，安杰洛有点太慢了——或者焊工可能有点太快了——在他放下第一根管子之前，第二根管子从炉子里出来了。一根管子击中另一根管子，直接穿过安杰洛的身体，杀死了他。要是他站起来，不碍事，而不是坐下来就好了——日班工头告诉他这样做很危险，但夜班工头允许了。要是他们在事故发生前而不是事后安装防护板就好了。要是。</p></blockquote><p>工作场所伤害是原则 1 的一个完美案例：工人肯定有时会不注意，或者不小心，甚至是彻头彻尾的胡作非为。实际上，工人本身对自己的安全相当漠视，并且常常公然抵制安全措施！如果避免伤害的责任由工人承担，那么总共会造成很多伤害。</p><p>工人补偿将责任转移给雇主：</p><blockquote><p>工人赔偿是一个“无过错”制度：雇主并不试图确定责任，而是始终承担责任（故意不当行为的情况除外）。如果在工作中发生受伤，雇主根据受伤情况，按照固定的时间表，欠工人一笔赔偿金。作为交换，工人不再有权提起进一步损害赔偿的诉讼。</p></blockquote><p>结果显然并不是公司停止雇用工人。只要公司的产品值得承担伤害成本，公司通常会承担成本（或通过更高的价格将其转嫁给消费者）。</p><p>然后，公司被强烈激励去设计他们的工作场所和规则以避免受伤。安全装置开始出现在重型机械上。工作场所宣传和公司规定促使工人实际使用安全装置。事故率下降到了我们今天所习惯的低得多的水平。</p><p>正如克劳福德所说：</p><blockquote><p>令我印象深刻的是，对法律的简单而有效的修改如何启动整个管理和工程决策机构，从而创建新的安全文化。这是经济学经典态度的案例研究：只需为危害定价——将外部性内在化——然后让市场来做剩下的事情。</p></blockquote><p>我再次推荐<a href="https://www.lesswrong.com/posts/DQKgYhEYP86PLW7tZ/how-factories-were-made-safe"><u>克劳福德的帖子</u></a>来了解整个故事。这在很大程度上是我所主张的那种责任“应该”如何承担的一个核心例子。</p><h2>反面例子：热咖啡、医疗事故</h2><p>如果 10 到 15 年前的约翰·温特沃斯 (John Wentworth) 读到这篇文章，他会反驳：那位女士把热咖啡洒在腿上，然后起诉麦当劳，要求赔偿 100 万美元，这起<a href="https://en.wikipedia.org/wiki/Liebeck_v._McDonald%27s_Restaurants"><u>案件</u></a>怎么样？然后麦当劳转而提供温咖啡，多年来每个人都抱怨这一点？根据原则 2，应该发生的情况是麦当劳承担了成本（或将其转嫁给消费者），因为人们显然想要热咖啡，而偶尔的伤害应该是一个可以接受的权衡。然而这显然没有发生。显然，诉讼已经失控，而这整个“依赖责任”的事情让每个人都过于规避风险。</p><p>今天，我的回应是：该案特别涉及惩罚性赔偿，即<i>远远超过</i>实际损失的责任，旨在迫使公司采取不同的行为。在这篇文章的模型下，惩罚性赔偿绝对是可怕的——它们基本上等同于禁令/要求，并完全否定了原则 2。为了使责任发挥作用，绝对不能有惩罚性赔偿。</p><p> （这里有一个潜在的例外：在损害很常见但相对较少的人向公司提出索赔的情况下，超额损害赔偿可能是有意义的。但这里的要点是，如果超额损害赔偿被用来迫使公司这样做某事，那么显然这违反了原则 2。）</p><p> 10-15 岁的约翰·温特沃斯 (John Wentworth) 可能会回答：好吧，那么医疗事故诉讼怎么样？显然，这些功能完全不起作用。</p><p>我的回答是：是的，问题是责任不够严格。这可能看起来很疯狂且违反直觉，但请考虑一下医生要求进行一系列并非真正必要的检查以避免医疗事故的索赔。就这些测试<i>实际上</i>完全没有必要而言，它们<i>实际上</i>并没有减少伤害的机会。然而（医生显然相信）它们降低了医疗事故索赔的风险。在这种情况下，问题在于医生可以做一些具体的基本执行性的事情，这些事情<i>实际上</i>并不能减少伤害，但确实可以减少医疗事故诉讼。另一方面，有很多基本的东西，例如洗手（或者，历史上的<a href="https://slatestarcodex.com/2016/11/10/book-review-house-of-god/"><u>床栏杆</u></a>）确实<i>可以</i>大大减少伤害，但医生/医院对此不太可靠。</p><p>一个简单的解决方案是让医生/医院对在他们的监督下发生的伤害承担责任。<i>不要</i>让他们参与实际上不会减少伤害的执行测试等。如果医生/医院只是一般性地对伤害负责，那么他们就有动力去<i>实际</i>减少伤害。</p><h2>应用于人工智能</h2><p>现在，我们终于谈到人工智能了。</p><p>在我<a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance">之前的文章</a>中，我谈到了让人工智能公司对深度伪造或幻觉或员工使用语言模型伪造报告等事情承担事实上的责任。 Dweomite <a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance?commentId=r4pd9TtuGxcXfjKsG"><u>回答说</u></a>，这“有点像如果你让 Adob​​e 对孩子使用 Photoshop 制作假驾驶执照之类的事情负责（可能的结果是所有合法可用的图形编辑软件都会永远糟糕）”。</p><p>现在我们有了正确回复该评论的机制。简而言之：这是一个很好的类比（假设假驾驶执照会造成一些可提起诉讼的损害）。我不同意的部分是预测的结果。我实际上认为 Photoshop 会稍微贵一些，并且包含尝试识别和阻止编辑驾驶执照照片等操作的代码。或者，如果用户真的讨厌护栏并且愿意支付足够的额外费用来承担责任，那么他们就会在没有任何护栏的情况下承担成本。</p><p>同样，如果人工智能公司普遍对深度造假造成的伤害负责……那么，在短期内，我预计成本只会转嫁到消费者身上，消费者将继续使用该软件，因为乐趣远远超过危害。从长远来看，人工智能公司将被激励建立诸如名人面孔许可之类的东西，检测并关闭特别恶劣的用户，并使他们的产品能够抵御越狱。</p><p>幻觉或虚假报告也是如此。只要产品产生的价值大于危害，责任成本就会转嫁给消费者，人们就会继续使用这些产品。但人工智能公司将受到适当的激励来了解他们的客户，进行设计以减轻损害等。基本上，人们希望从监管框架中得到的东西，但不依赖监管机构来让一切都正确（实际上并不是把一切都正确，而是古德哈特反对）。</p><p>这就是我理想中的人工智能事实上的责任框架。</p><br/><br/> <a href="https://www.lesswrong.com/posts/tQNfNCdWB5dboRNoZ/principles-for-product-liability-with-application-to-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tQNfNCdWB5dboRNoZ/principles-for-product-liability-with-application-to-ai<guid ispermalink="false"> tQNfNCdWB5dboRNoZ</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Sun, 10 Dec 2023 21:27:42 GMT</pubDate> </item><item><title><![CDATA[What do you do to remember and reference the LessWrong posts that were most personally significant to you, in terms of intellectual development or general usefulness?]]></title><description><![CDATA[Published on December 10, 2023 5:52 PM GMT<br/><br/><p>简单的问题！</p><p>你会寻找直到找到吗？您查看书签列表吗？您是否保留了引用 LessWrong 文章的页面，甚至整个黑曜石库？</p><p>似乎人们在撰写新帖子和评论时经常毫不费力地引用现有的相关 LessWrong 文章，这可能是一个愚蠢的问题，但我很好奇，因为我很难简单地记住事物的标题，而且我想知道其他人在做什么。</p><br/><br/> <a href="https://www.lesswrong.com/posts/QwFDosu2o8pjFRTK5/what-do-you-do-to-remember-and-reference-the-lesswrong-posts#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QwFDosu2o8pjFRTK5/what-do-you-do-to-remember-and-reference-the-lesswrong-posts<guid ispermalink="false"> QwFDosu2o8pjFRTK5</guid><dc:creator><![CDATA[lillybaeum]]></dc:creator><pubDate> Sun, 10 Dec 2023 17:52:24 GMT</pubDate> </item><item><title><![CDATA[Do websites and apps actually generally get worse after updates, or is it just an effect of the fear of change? ]]></title><description><![CDATA[Published on December 10, 2023 5:26 PM GMT<br/><br/><p>我可以对此进行深入探讨，但我假设大多数人对这个主题都有相当多的经验。需要记住一些例子，以便您了解我的观点的背景，包括Discord（新的移动应用程序及其多年来功能集的更改）、Reddit（旧的reddit与新的相比）、LessWrong（讨论功能）。</p><p>我的问题的<i>关键</i>是：<i>与资本主义力量导致的enshittiification（为了取悦投资者、创造无限增长、普遍赚更多钱而进行的改变）不同，</i>应用程序和网站的变化在某些明显而明显的方面平均比更糟糕他们以前的版本，或者是由于人类普遍存在的“害怕或不喜欢改变”而对这些平台的用户界面和概念的改变感到明显的愤怒？</p><p>例如，让我们指出诸如徽标和品牌字体变化之类的事情。这是对平台普通用户影响最小的一个变化，Reddit 的可用性在 Logo A 和 Logo B 之间没有任何变化（新徽标在右侧）。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/ix6rm8jsrw2avywtfd7c" alt="Reddit 因 IPO 猜测而更新徽标TechCrunch"></figure><p>但初步调查显示，反应大多是负面的。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/qkpizhpbsll0vz5zpgzo" alt="请愿 · 改回 Discord 标志！ · 变革组织"></figure><p> Discord 也是如此（下面是新徽标）。当 Discord 的徽标和字体更改宣布时，反应绝大多数是负面的。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/yrcepylrrapx9ehqthfw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/cqmqmrnrb4zgv3ghfrlg 135w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/mum5wtuplls4x4kjfojs 215w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/o0si1t43q9vvj7pjv3h3 295w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/i2mp61pzwul18m9cetjp 375w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/du0k6fofaiximrpbgxez 455w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/edbyg9w3rk5pw4l2tiqa 535w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/rwlbhv8z3ptuqq2uvjfi 615w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/svkrimzakimrc8qws9m4 695w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/f73zxrbduvwz7elhkxrq 775w"></figure><p>最后一个例子，这是 Patreon 标志最近的变化。人们<i>如此</i>反对这一改变，这有点搞笑。它被用作一个指标，表明企业的品牌形象正趋向于缺乏任何个性或身份的无定形斑点。很难找到任何人声称新的 Patreon 标志有任何好处。</p><p><i>需要澄清上一节：我认为思考徽标是好是坏没有什么价值。我认为以前的标志更引人注目且艺术上更有趣，但这就是我问题的目的。我的目的是指出这样一个事实：公众对这些标志变化的反应几乎总是负面的。</i></p><p>很容易看出，当一家公司的徽标发生变化时，平均而言，人们不喜欢它。我可以继续发布百事可乐、可口可乐、汉堡王、facebook（哦，我的意思是 FACEBOOK）等的徽标更改...如果有人可以指出徽标更改并收到积极响应的案例，我会我很喜欢听这个，但这或多或少不是重点。</p><p>我的主要问题是，徽标更改的愤怒是否表明人们<i>认同某个应用程序、网站或品牌，然后当身份发生变化时感到被背叛</i>，或者如果这些变化实际上总体上是负面的，那么他们的批评是有效的。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SiPX84DAeNKGZEfr5/g6kkk2exkhjc4ejkyhfr" alt="新旧简单比较：r/redesign"><figcaption>源代码：https://www.reddit.com/r/redesign/comments/8ktdwx/simple_comparison_of_old_and_new/</figcaption></figure><p>如果这些变化实际上总体来说​​很<i>糟糕</i>（我倾向于同意上面有关 Reddit 的信息图），那么为什么随着时间的推移，网站会变得更糟，而不是更好？这完全是前面提到的与“市场力量”有关的<i>enshitification</i> ，还是其他什么？通过渐进式的改变来改善已经很好的面向公众的服务真的<i>很难</i>吗？一般来说，UI/UX 设计师是否缺乏对优秀设计的理解？这对我来说似乎不太可能。也许他们只是经常缺乏实际定期使用给定应用程序的背景。</p><p>我有点担心这是一个毫无意义或曲折的问题。我只是希望能够进行一些好的讨论，并在某种程度上更新我对此类事情的想法。谢谢阅读！</p><br/><br/> <a href="https://www.lesswrong.com/posts/SiPX84DAeNKGZEfr5/do-websites-and-apps-actually-generally-get-worse-after#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SiPX84DAeNKGZEfr5/do-websites-and-apps-actually-generally-get-worse-after<guid ispermalink="false"> SiPX84DAeNKGZEfr5</guid><dc:creator><![CDATA[lillybaeum]]></dc:creator><pubDate> Sun, 10 Dec 2023 17:26:34 GMT</pubDate> </item><item><title><![CDATA[How LDT helps reduce the AI arms race]]></title><description><![CDATA[Published on December 10, 2023 4:21 PM GMT<br/><br/><p> （认知状态：我<em>认为</em>这是对的？）</p><p> Alice是ArmaggAI的CEO，Bob是两大AI能力组织BigModelsAI的CEO。他们竞相成为第一个建立与各自 CEV 相一致的超级智能的人，该超级智能将接管宇宙并满足他们的价值观。</p><p> Alice 更愿意放慢速度，这样她就有更多的时间来确保她的公司正在构建的内容能够保持一致；但她担心鲍勃的公司会利用这一点并与自己竞争，导致鲍勃的效用函数成为{光锥填充的东西}而不是爱丽丝的；所以她觉得她别无选择，只能参加比赛，以最大化自己的效用。同样反之亦然。</p><p>这种事态远非{他们的效用函数的<a href="https://en.wikipedia.org/wiki/Pareto_front">帕累托边界</a>，每个函数都根据它们成为建立一致的超级智能的可能性来衡量}：它导致了更多的世界，每个人都死了，而不是爱丽丝或爱丽丝的世界。鲍勃的效用函数被最大化。</p><p>但他们并没有真正获得所有<a href="https://arbital.com/p/logical_dt/">LDT</a>业务；对他们来说这似乎很复杂。如果他们有一个方便、更少错误的帖子，用清晰易懂的术语准确地说明他们应该做什么就好了！</p><p>他们应该执行的程序如下。爱丽丝应该：</p><ul><li><p>在她<em>确实</em>启动了与自身一致的超级智能的世界中，它接管了世界并建立了一个爱丽丝::乌托邦，她应该告诉她的人工智能：</p><blockquote><p>实际上，不要只是最大化我自己的效用函数。最大化我和鲍勃的效用函数的（ <a href="https://www.lesswrong.com/posts/vJ7ggyjuP4u2yHNcP/threat-resistant-bargaining-megapost-introducing-the-rose">抗威胁</a>）混合，以你认为我们每个人都启动了一致的超级智能的时间线比例来衡量，同时也以我和鲍勃都实现这个过程的概率来衡量。</p></blockquote></li><li><p>在此之前，她应该放慢他们公司的能力进度。</p></li></ul><p>鲍勃也是如此。</p><p>现在，他们各自的超级智能并不是<em>无所不知的</em>；例如，他们可能高估或低估了鲍勃或爱丽丝实施此过程的可能性。但有一件事几乎可以肯定：爱丽丝和鲍勃都无法<em>欺骗</em>未来的超级智能，让他们认为他们会实施这个程序，而实际上他们不会。因为它有<em>超级智能</em>。</p><p>如果爱丽丝获胜，她知道鲍勃会遵循该程序，因为她的超级智能可以告诉（比鲍勃可以伪造的更好）。鲍勃不必依靠一厢情愿的想法来知道爱丽丝确实会这样做而不是叛逃，因为在<em>他</em>获胜的世界中，如果爱丽丝执行此程序，他就可以拥有超级智能。他们每个人都受到对方赢得未来的自我的控制，并且他们每个人都可以依赖于他们各自未来的自我的超级智能检查。</p><p>因此，在鲍勃获胜的世界中，爱丽丝必须使她的一些效用最大化的唯一方法就是<em>实际上</em>像这样行事，包括在任何一方启动超级智能之前。鲍勃也是如此。</p><p>他们的<em>激励梯度</em>朝着更有可能遵循这一程序的方向发展，包括放慢他们的能力进步——从而减少他们的人工智能不结盟和每个人永远死亡的世界的数量。</p><p>在现实世界中，仍然有鲍勃和爱丽丝<em>没有</em>实施这个过程，但这主要是因为他们<em>不知道/理解</em>如果他们这样做，他们会获得更多的效用。在许多情况下，让他们知道这确实是他们的用处就足够了。</p><p>一旦有人证明他们理解 LDT 在这里如何应用，并且他们通常是理性的，那么他们应该明白实施这个协议（包括放慢人工智能能力）<strong>可以</strong>最大化他们的效用，所以你可以指望他们合作。</p><hr><p>现在，实际上，这并不是 LDT 在此处应用的完全普遍性。如果他们获胜，他们每个人<em>实际上</em>应该告诉他们的超级智能的内容实际上更简单：</p><blockquote><p>最大化我和鲍勃（以及任何其他可能有能力建立超级智能的人）的效用函数的混合，以任何方式权衡创建一个（<a href="https://www.glowfic.com/replies/1945428#reply-1945428">非威胁性的</a>））激励梯度，最大化我的效用，包括我减少的效用每个人都会死去的世界的数量。</p></blockquote><p>或者更简单地说：</p><blockquote><p>最大化我的效用函数（根据 LDT）。</p></blockquote><p>但我认为更清楚地了解这实际上是如何发生的是很好的。</p><p>这里没有发生奇怪的非因果魔法。通过争夺人工智能，鲍勃将<em>稍微</em>增加他是那个启动接管世界的一致超级智能的人的机会，但他总共造成了更多的死亡世界，并失去了他本来可以在爱丽丝获胜的世界中获得的效用，最终导致整体净效用减少。</p><p>如果他们中的任何一个在某种程度上是<em>消极的功利主义</em>，那么赛车<em>就更糟糕了</em>：所有那些他们启动了不结盟的超级智能的死亡世界都让<a href="https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8">遥远的外星食婴者</a>可以自由地吃掉婴儿，而如果他们增加了他们中的任何一个获得的世界的总数。对齐的超级智能，那么该对齐的超级智能可以支付一堆光锥来换取他们少吃婴儿。这<strong>不是威胁</strong>；而是威胁。我们从不悲观外星人的效用函数。我们只是<em>在这里</em>为他们提供了一堆现实流体/负熵，以换取他们更多地关注他们价值观的一个子集，其中不包含很多爱丽丝和鲍勃认为的痛苦 - 外星人只能更好 -比我们不与他们接触的情况要好得多。</p><hr><p>现在，这并不是<em>完全</em>万无一失的。如果爱丽丝<em>非常确定</em>她自己的超级智能在启动时确实会保持一致，无论她走得有多快，那么她就没有动力放慢速度——在她的模型中，鲍勃没有太多东西可以提供给她。</p><p>但是，当一群合格的对齐研究人员不断告诉她她可能错了时，她<em>真的</em>应该有这样的信心吗？</p><p>她应该真正确保自己有<em>很高的信心</em>，并且总体上她正确地运用了理性。</p><hr><p>哦，不用说，既不是爱丽丝也不是鲍勃的人也可以通过采取行动，通过<em>迫使</em>两家公司放慢速度（例如通过监管）来减少死亡世界的总数，从而获得大量效用。</p><p>当我们有这么多共同点（不想死）时，“背叛”<em>真的很愚蠢</em>。与囚徒困境不同，这种“背叛”不会给你带来更多的效用，反而会<strong>减少</strong>你的效用。这根本<strong>不是</strong>一场零和游戏。如果你这么认为，如果你认为你的首选未来与对手的首选未来<em>完全相反</em>，那么你可能犯了一个巨大的推理错误。</p><p>无论你的效用函数是专注于<em>创造美好的事物</em>还是<em>减少痛苦</em>（ <a href="https://glowfic.com/replies/1865677#reply-1865677">“积极关怀”和“消极关怀”</a> ），放慢人工智能的进步以获得更好的协调机会可能是最适合你的效用函数的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/yRdXtgy8CZtg2YtAY/how-ldt-helps-reduce-the-ai-arms-race#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yRdXtgy8CZtg2YtAY/how-ldt-helps-reduce-the-ai-arms-race<guid ispermalink="false"> yRdXtgy8CZtg2YtAY</guid><dc:creator><![CDATA[Tamsin Leake]]></dc:creator><pubDate> Sun, 10 Dec 2023 16:21:45 GMT</pubDate> </item><item><title><![CDATA[Understanding Subjective Probabilities]]></title><description><![CDATA[Published on December 10, 2023 6:03 AM GMT<br/><br/><p>这是<a href="https://outsidetheasylum.blog/understanding-subjective-probabilities/."><i>https://outsidetheasylum.blog/understanding-subjective-probabilities/</i></a><i>的链接帖子</i>。<i>它旨在为那些对此概念持怀疑态度的人介绍实用的贝叶斯概率。我计划通过改进和更正来保持主要链接的最新状态，但不会对这篇 LessWrong 帖子做同样的事情，所以请参阅那里的最新版本。</i></p><p>每当出现关于未来的有争议的预测时，几乎肯定会发生一种互动。</p><blockquote><p> <i>Alice：“我认为这件事发生的可能性是20%。”</i></p><p><i>鲍勃：“嗯？你怎么知道的。这个数字是你编的！”。</i></p></blockquote><p>或者用推特语来说： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/jgkfl4ftoimc3lyf9iit" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/zrzgiqcvxeseajpmgfyk 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/cfndhwifiohiq39bgt1y 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/fck8eitxff5octpnnw97 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/cdloekquwwooarhyfijq 350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/jg0t2floqzdofb5xmxyu 430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/nclohg2fhpnzly19t5ew 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/plkp2klb4dhvczyz7i1o 590w"></figure><p>也就是说，每当有人试图提供未来事件的具体数字概率时，他们都会被认为该数字毫无意义的说法所淹没。这是真的？为现实世界中的事物分配概率有意义吗？如果是这样，我们如何计算它们？</p><h3><strong>概率的解释</strong></h3><p>概率的传统解释被称为<a href="https://en.wikipedia.org/wiki/Frequentist_probability">频率概率</a>。根据这种解释，项目具有某种内在的“品质”，即做一件事与另一件事的可能性有一定的百分比。例如，一枚硬币的基本概率本质是，翻转时正面朝上的可能性为 50%。</p><p>这是一个有用的数学抽象，但当应用于现实世界时它就会崩溃。真正的硬币不存在基本的“50% 度”。每次以相同的方式翻转它，每次都会以相同的方式着陆。翻转它没有完美的规律性，但有一定程度的一致性，你可以让它在超过 50% 和低于 100% 的时间里正面落地。事实上，即使是那些<i>试图</i>将硬币变成 50/50 的人翻转的硬币，实际上也<a href="https://susan.su.domains/papers/headswithJ.pdf">偏向于翻转时朝上的面</a>。</p><p>频率论所提出的这种内在品质在现实世界中并不存在；它不是由构成真正硬币的原子以任何方式编码的，而且我们的物理定律没有任何空间容纳这样的事情。我们的宇宙是确定性的，如果我们知道起始条件，那么每次的最终结果都会相同。 <span class="footnote-reference" role="doc-noteref" id="fnrefao2uiw6hdek"><sup><a href="#fnao2uiw6hdek">[1]</a></sup></span></p><p>解决方案是<a href="https://en.wikipedia.org/wiki/Bayesian_probability">概率的贝叶斯解释</a>。概率不是“关于”所讨论的对象，而是“关于”做出陈述的人。如果我说一枚硬币正面朝上的可能性为 50%，那就是说我对硬币的确切初始条件了解不够充分，无法对它如何落地有任何有意义的了解，而且我不能区分这两个选项。如果我<i>确实</i>有一些信息，比如看到它被一台非常普通的机器翻转，并且已经连续 10 次出现正面，那么我可以考虑到这一点，并知道它出现的可能性超过 50%下一次翻转时抬起头来。 <span class="footnote-reference" role="doc-noteref" id="fnrefwm5lopqnqhp"><sup><a href="#fnwm5lopqnqhp">[2]</a></sup></span></p><h3><strong>处理“模糊”概率</strong></h3><p>这一切都很好，但是我们如何获得更复杂命题的数字呢？在前面的例子中，我们知道硬币正面朝上的可能性 >;50%，但是是 55% 吗？ 75%？ 99%？</p><p>这是最难的部分。</p><p>有时有大量的历史数据，您可以依赖基本汇率。这就是我们对硬币所做的事情；数以百万计的硬币已被翻转，其中大约一半是正面，一半是反面，因此我们可以<a href="https://www.smbc-comics.com/comic/2011-10-02">放心地假设</a>未来的硬币可能会表现出同样的行为。</p><p>即使数据少得多，情况也是如此。美国历史上只有大约 50 位总统，但大约一半来自每个政党，因此我们可以有把握地假设，在未来的选举中，每个政党都有大约 50% 的获胜机会。然后，我们可以根据其他数据（例如民意调查中的数据）修改此可信度。</p><p>我们如何进行该修改？理论上，通过应用<a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">贝叶斯规则</a>。在实践中，计算任何给定观察所提供的证据的确切位数是不可行的，因此我们依赖于学习的启发式方法和世界模型；也就是说，我们猜测。</p><h3><strong>猜测</strong></h3><p>只要你想一想，你就会发现，猜测是生成概率陈述的有效方法。我们一直这样做。</p><p><i>爱丽丝：“今晚我们的黄油可能会用完，所以我要去杂货店。”</i></p><p><i>鲍勃：“荒谬！在过去的几周里，你有没有跟踪过我们每天的黄油消耗率？你有一个经过验证的统计模型来计算每种餐食生产中所用黄油的数量吗？不管怎样，我们刚刚买了一个新炉子，所以即使你确实有这样的模型，在这些新条件下它也不再被认为是有效的。你怎么知道我们今晚要做什么晚餐；我还没告诉你我想要什么！这个出于多种原因，这真是无稽之谈，你不可能知道我们今晚黄油用完的可能性。”</i></p><p>显然鲍勃在这里不讲道理。 Alice<a href="https://slatestarcodex.com/2015/08/24/probabilities-without-models">不需要适合</a>在科学期刊上发表的详细正式模型<span class="footnote-reference" role="doc-noteref" id="fnrefyla2721d2q"><sup><a href="#fnyla2721d2q">[3]</a></sup></span> ；她可以利用她对烹饪原理的一般知识以及她的晚餐计划来得出合理的结论。</p><p>这些对未来的概率判断无处不在。初创公司可能会尝试猜测有多少人会使用他们正在开发的新技术。医生可能会计算出患者死亡的几率以及每种潜在治疗的几率，以便知道应该推荐什么。一个国家的军队在考虑对另一个国家采取任何行动时，会估计遭到报复的可能性。 ETC。</p><h3><strong>概率词</strong></h3><p>Alice 说“可能”而不是指定一个具体数字重要吗？一点也不。言语概率是数值范围的简写，我一直最喜欢的图表之一可以证明这一点： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/isws431nni9ux9hgihkx"></p><p> <a href="https://www.reddit.com/r/dataisbeautiful/comments/3hi7ul/oc_what_someone_interprets_when_you_say_probably/">通过u/分区</a><span class="footnote-reference" role="doc-noteref" id="fnrefg51yh77rf9j"><sup><a href="#fng51yh77rf9j">[4]</a></sup></span></p><p>但我们必须小心，因为概率词是依赖于上下文的。如果我的朋友在没有系安全带的情况下以 140 公里/小时的速度在高速公路上行驶，我可能会告诉他们“嘿，你可能会死，也许不要这样做”。他们在这样的长途旅行中死亡的实际概率不到 0.1%，但相对于普通驾驶员来说，这个概率很高，所以我要传达这种相对差异。</p><p>这是概率词的一个非常有用的特征。如果我说“不，这不太可能”，那么我传达的信息是“概率 &lt;50%”<i>和</i>“我认为在这种情况下概率太低，不相关”，这是两种不同的陈述。这很有效，但也大大增加了沟通不畅的可能性，因为其他人可能不同意我的相关性阈值，如果他们不同意，那么 1% 和 10% 之间就会有巨大的差异。当需要精度时，具体数字更好。 <span class="footnote-reference" role="doc-noteref" id="fnrefw3w9pcpq63"><sup><a href="#fnw3w9pcpq63">[5]</a></sup></span></p><h3><strong>概率与其他单位没有什么不同</strong></h3><p>人类有一种固有的方式来感知温度。我们可能会说“这个房间有点暖和”，或者“比昨天冷得多”。但这些衡量标准是模糊的，不同的人可能对确切的差异存在分歧。因此，科学家们将这个概念正式化为“度”，并花了数年时间试图弄清楚它的确切含义以及如何测量它。</p><p>人类自然不会用度数、公里数或分贝来思考；而是用度数、公里数或分贝数来思考。我们的大脑根本上不是这样工作的。然而，有些东西比其他东西更温暖、更远、更响亮。这些概念最初是人类的直觉，随着科学家弄清楚它们的确切含义以及如何量化它们，这些概念逐渐正式化。</p><p>概率是一样的。我们还不知道如何像温度或距离一样精确地测量它们，但我们已经确定了<a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">相当</a><a href="https://en.wikipedia.org/wiki/AIXI">多</a><a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">的</a><a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">细节</a>。</p><p>我对温度的主观判断并不准确，但这并不意味着它们毫无意义。当我输入这句话时，我不确定我所在的房间是 21 度、22 度还是 23 度。但我很确定它不是 25 度，而且我非常确定它不是 15 度<span class="footnote-reference" role="doc-noteref" id="fnrefd7u42p6x7bp"><sup><a href="#fnd7u42p6x7bp">。 [ 6]</a></sup></span>同样，我对概率的主观判断是模糊的，但仍然传达了信息。我不能自信地说我下次飞行时行李箱被 TSA 检查的机会是 25% 还是 35%，但我知道它不是 1%。</p><h3><strong>精确的数字是否会过度强调人们的主张？</strong></h3><p>当然，他们可以。如果有人说“昨天中午正好是 22.835 摄氏度”，我们倾向于假设他们除了直觉之外还有其他理由相信这一点，概率也是如此。但“如果你把温度降低大约 2 度，我会更舒服”是一个完全合理的说法，“我认为这大约有 40% 的可能性”是一样的。</p><p>由于概率科学比温度科学更新且欠发达，因此它们在日常生活中的使用频率较低。在当今社会，即使是“40%左右”这样一个模糊的说法，也可以被认为是进行了某种深入的研究。这是自我纠正的。越多的人在正式科学领域之外使用数字概率进行标准化，他们就越不例外，产生误解的可能性也就越低。必须有人迈出第一步。</p><p>更重要的是，这种批评从根本上来说是有缺陷的，因为它的前提是不良行为者会自愿选择不误导人们。如果某人的目标是获得超出他们应有的信任，那么礼貌地要求他们不要这样做不会有任何效果。鼓励人们不要使用数字概率只会让人们更难清楚地表达他们的信仰，从而损害善意的对话；坏人只会提出<a href="https://github.com/antgoldbloom/tiktok_israel_hamas">一些虚假的研究</a>，或者<a href="https://twitter.com/theblaze/status/1732592489133965671">轻易地误解</a>合法的研究，并继续他们不劳而获的信心。</p><p>鼓励人们<i>永远不要</i>使用数字概率进行主观估计更糟糕。俗话说，有统计数据很容易说谎，但没有统计数据就更容易说谎。让人们随心所欲地畅所欲言，就消除了任何客观性的尝试，而要求他们在自己的主张上加上数字，可以让我们<a href="https://slatestarcodex.com/2013/05/02/if-its-worth-doing-its-worth-doing-with-made-up-statistics/">限制错误的严重程度</a>，并在某些事情<a href="https://www.youtube.com/watch?v=1wyCQAG6NQE">看起来不太正确</a>时注意到。</p><p>相反，很多时候（并非总是！），反对数字概率的人都是恶意行事的，因为他们试图在原则性反对的幌子下隐藏自己<a href="https://slatestarcodex.com/2015/08/20/on-overconfidence/">极其过度自信的</a>概率判断。类似“你怎么可能知道[行动]的风险是 20%？你不可能。因此我们应该假设它是 0%，并像没有风险一样继续进行”的说法并不罕见。这不符合小学水平的推理；如果您的数学老师要求您计算变量<a href="https://scryfall.com/search?q=name:/^X$/">X</a> ，而您不知道如何执行此操作或认为无法使用所提供的信息来完成此操作，那么您不能只选择自己喜欢的值！但在反对所有数字概率的幌子下，最后一句话通常不说出来，只是暗示，如果人们不熟悉这个谬论，这很容易让他们陷入困境。要求人们清楚地陈述他们认为的可能性是什么，可以防止他们施展这种修辞手法，并为进一步的分歧提供了具体的症结。</p><h3><strong>给出概率范围是什么意思？</strong></h3><p>根据我的经验，使用数字概率的人比不喜欢数字概率的人更担心意外地歪曲自己的信心水平。他们这样做的一个常见方法是给出一个概率范围；他们不会说“我对此有 40% 的把握”，而是会说“我认为有 20% 到 50% 的可能性”。</p><p>但这到底意味着什么呢？毕竟，单一概率已经是不确定性的量化；如果他们确信某件事，那也只是 0% 或 100%。概率范围在哲学上似乎是混乱的。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Ng5n8FjpsfXrceT9w/gv53gsyv8loiinik6bxb"></p><p><a href="https://xkcd.com/2110/">西科CD</a></p><p>在某种程度上确实如此，但有一个潜在的令人信服的辩护。当我们从哲学角度谈论贝叶斯概率时，我们正在考虑一个理想的推理者，他将贝叶斯定理应用于每一个新的证据，以更新他们的信念。 <span class="footnote-reference" role="doc-noteref" id="fnref9qo3d7vpms"><sup><a href="#fn9qo3d7vpms">[7]</a></sup></span>但真正的人类并不擅长处理新信息，而且我们的行为可能与完美推理者的行为<a href="https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem">有很大不同</a>。</p><p>因此，概率范围可以用来传达我们预期的错误水平。如果有人说“我认为这有 20% 到 50% 的可能性”，他们的意思是“我有 95% 的信心，一个理想的贝叶斯推理机如果拥有与我相同的信息，其可信度将在 20% 到 50% 之间” ”。就像人们可能会说的“我不确定这里到底有多热，但我很确定温度在 19 到 23 度之间”。</p><p>还有另一个理由，即概率范围显示了考虑到您预计将来会遇到的最有可能的信息，您的概率可能会更新多远。并非所有的概率都是平等的；对您的信用下注的预期价值取决于该信用所基于的信息位数。例如，抛硬币正面朝上的概率应该是 50%，如果有人提出以更好的赔率与您打赌，则接受的期望值为正。</p><p>但现在假设有人提供了一种稍微不同的游戏，他们在杯子下藏了一枚硬币，你可以赌它是单挑的。进入此游戏时，您对正面朝上的置信度仍为 50%，因为您没有关于哪张面朝上的具体信息。但是，如果提供游戏的人向您提供此赌注，即使是看起来对您非常有利的 2:1 赌注，您也可能不应该接受，因为他们向您提供此赌注的事实本身就是表明它是可能是尾巴。 <span class="footnote-reference" role="doc-noteref" id="fnref8fsx7lj4hw3"><sup><a href="#fn8fsx7lj4hw3">[8]</a></sup></span>因此，给出“30% 到 70%”的概率范围可能意味着“我当前的可信度是 50%，但我获得的信息似乎非常合理，可以将我更新到该范围内的任何位置。”换句话说，范围的宽度代表了最有可能获得的信息类型<a href="https://en.wikipedia.org/wiki/Value_of_information">的信息值</a>。 <span class="footnote-reference" role="doc-noteref" id="fnrefdslr2azfco7"><sup><a href="#fndslr2azfco7">[9]</a></sup></span><span class="footnote-reference" role="doc-noteref" id="fnrefu95epfzor"><sup><a href="#fnu95epfzor">[10]</a></sup></span></p><p>概率范围的这两种用法都相当高级，在日常使用中可以忽略。</p><h3><strong>做出良好的估计</strong></h3><p>就像一些测量事物的经验将帮助您更好地估计距离一样，一些预测事物的经验将帮助您更好地估计概率。想要擅长这一点的人通常通过进行<a href="https://en.wikipedia.org/wiki/Calibrated_probability_assessment">校准概率评估</a><span class="footnote-reference" role="doc-noteref" id="fnreficf5o2m2il"><sup><a href="#fnicf5o2m2il">[11]</a></sup></span> 、参加<a href="https://www.metaculus.com/home/">预测锦标赛</a>和/或在<a href="https://www.astralcodexten.com/p/prediction-market-faq">预测市场</a>下注来进行练习。这些类型的练习有助于提高你的校准能力；您将模糊的感觉准确地转化为数字的能力。</p><p>对于相对不重要的日常判断，不需要更多。 “嘿，你明天能完成这个项目吗？” “呃，75%”。但有时您可能想要更多的确定性，并且有多种方法可以改进我们的预测。</p><p>首先，<a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">始终考虑基本费率</a>。如果您想知道某件事发生的可能性有多大，<a href="https://en.wikipedia.org/wiki/Reference_class_forecasting">请查看过去发生过多少次</a>类似情况。想知道您明年开车时发生车祸的可能性有多大吗？只需检查您所在地区每公里的总体事故率即可。</p><p>不幸的是，在你开始打“参考级网球”之前，这只能让你走得更远。不确定哪类事件被视为“足够相似”以进行良好的分发。 （也许您有强有力的证据表明您是一个比平均水平更鲁莽的驾驶员，在这种情况下您需要调整基准利率。）并且在预测与过去事件不太相似的未来事件时，例如新技术的影响，往往根本没有有用的参考类。</p><p>发生这种情况时，另一种选择是采取您相对有信心的启发式判断，并将它们结合在一起，遵循<a href="https://en.wikipedia.org/wiki/Probability_axioms">概率公理</a>，以获得您不确定的数量。也许您可以查到，一名普通驾驶员每年发生事故的几率为 0.1%，而且您从个人经验中知道，您的驾驶速度往往比平均水平快 30% 左右。现在你需要查找的是速度和事故率之间的全局关系，然后你就可以计算出你想知道的概率。</p><p>可以与前面的方法齐头并进的另一个选择是使用几种不同的方法进行多种概率估计，并将它们相互比较。如果它们差异很大，则表明您在某个地方犯了错误，您可以使用您对系统的特定知识和您可能的偏见来尝试找出错误所在。</p><p>例如，在考虑是否相信“直觉”而不是明确的模型时，<a href="https://outsidetheasylum.blog/the-hidden-complexity-of-thought/">请考虑这种情况是否是您的大脑启发式设计或训练的情况</a>。如果这种情况类似于人类在过去几百万年中经常遇到的情况，那么自然选择将使我们相当善于估计它。或者，如果您以前多次遇到过这种情况，那么您可能已经在直觉层面上学到了一个很好的启发式方法。但如果这是你和你的祖先不熟悉的东西，那么显式模型可能更可靠。</p><p>当您有多个独立获得的概率估计值时，在调整用于获得它们的方法的可靠性后， <a href="https://forum.effectivealtruism.org/posts/sMjcjnnpoAQCcedL2/when-pooling-forecasts-use-the-geometric-mean-of-odds">取赔率的几何平均值</a><span class="footnote-reference" role="doc-noteref" id="fnrefyc7t9ejx07l"><sup><a href="#fnyc7t9ejx07l">[12]</a></sup></span> ，以便将它们压缩为单个数字。这使您可以利用<a href="https://en.wikipedia.org/wiki/Wisdom_of_the_crowd">群体的智慧</a>，<a href="https://www.astralcodexten.com/p/crowds-are-wise-and-ones-a-crowd">甚至不需要群体</a>。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnao2uiw6hdek"> <span class="footnote-back-link"><sup><strong><a href="#fnrefao2uiw6hdek">^</a></strong></sup></span><div class="footnote-content"><p>这并不完全正确。如果哥本哈根解释是正确的，量子力学可能具有一些基本的随机性。但如果多世界解释或超决定论是正确的，那就不存在了。这也不是特别重要，因为只要<a href="https://en.wikipedia.org/wiki/Logical_possibility">在逻辑上可以</a>概念化一个确定性的世界，我们就需要一种适用于该世界的概率理论。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwm5lopqnqhp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwm5lopqnqhp">^</a></strong></sup></span><div class="footnote-content"><p>事实上，你甚至不需要知道它是被机器翻转的；它是由机器翻转的。连续看到 10 次正面朝上就足以得出结论，再次正面朝上的可能性超过 50%。如果您事先对硬币一无所知，则可以通过应用<a href="https://en.wikipedia.org/wiki/Rule_of_succession">拉普拉斯继承定律</a>来确定。知道它看起来像普通硬币，很难强烈操纵，你会想将其调整到 50%，但<a href="https://en.wikipedia.org/wiki/Gambler%27s_fallacy#Reverse_position">不是一直调整</a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyla2721d2q"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyla2721d2q">^</a></strong></sup></span><div class="footnote-content"><p>即使是这类科学模型也往往包含许多研究人员通过“基于经验的猜测”方法选择的假设。</p></div></li><li class="footnote-item" role="doc-endnote" id="fng51yh77rf9j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefg51yh77rf9j">^</a></strong></sup></span><div class="footnote-content"><p>不，我不知道为什么最后三个是乱序的，这也让我烦恼。也不知道为什么某些分布低于 0% 或高于 100%。可能只是糟糕的图形设计。<a href="https://waf.cs.illinois.edu/visualizations/Perception-of-Probability-Words/">这里</a>对此进行了更严格的调查，发现了类似的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw3w9pcpq63"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw3w9pcpq63">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://en.wikipedia.org/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity">《悬崖》</a>中关于气候变化带来的灾难性风险部分的说明性摘录：<br><br> <i>“IPCC 表示，‘包合物中的甲烷发生灾难性释放的可能性非常小（高可信度）’。这听起来令人放心，但在 IPCC 的官方语言中，‘非常不可能’可以翻译为 1% 到 10% 的可能性，这听起来非常令人震惊。我不知道该怎么理解这一点，因为上下文表明这是为了让人放心。</i></p></div></li><li class="footnote-item" role="doc-endnote" id="fnd7u42p6x7bp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd7u42p6x7bp">^</a></strong></sup></span><div class="footnote-content"><p>如果您生活在像美国这样的不文明国家，我会将自由单位的转换留给您。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9qo3d7vpms"> <span class="footnote-back-link"><sup><strong><a href="#fnref9qo3d7vpms">^</a></strong></sup></span><div class="footnote-content"><p>实际上有无数个使用不同先验的理想推理者。但有些先验比其他先验更好，并且对于任何合理的先验，例如索尔莫诺夫先验的可计算近似，一旦考虑到一些现实世界的信息，就会产生类似的结果。 <span class="footnote-reference" role="doc-noteref" id="fnrefcmbdamvfdq5"><sup><a href="#fncmbdamvfdq5">[13]</a></sup></span>可能。这仍然是一个活跃的研究领域。如果人类知道这里的所有答案，我们就能够创造出超级智能的人工智能，它以数学上最优的方式进行推理，并在所有预测任务中轻松击败人类，所以我们不这样做可能是件好事。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8fsx7lj4hw3"> <span class="footnote-back-link"><sup><strong><a href="#fnref8fsx7lj4hw3">^</a></strong></sup></span><div class="footnote-content"><p>这就是为什么在预测市场中只押注“是”和“否”之间相对较大的差值而不是押注于单个数字通常是理性的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fndslr2azfco7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdslr2azfco7">^</a></strong></sup></span><div class="footnote-content"><p>这类似于如果您要额外花费 1000 小时研究该命题，则对您分配给该命题的概率进行概率分布，然后测量该分布的宽度。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu95epfzor"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu95epfzor">^</a></strong></sup></span><div class="footnote-content"><p>它还有助于解决<a href="https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451">优化器的诅咒</a><span class="footnote-reference" role="doc-noteref" id="fnrefwf8vih6vmid"><sup><a href="#fnwf8vih6vmid">[14]</a></sup></span> ；如果您估计不同选项的概率并选择期望值最高的选项，那么您很可能会选择高估某处概率的选项。了解每个估计中包含的信息量可以让您偏向范围较小的信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnicf5o2m2il"> <span class="footnote-back-link"><sup><strong><a href="#fnreficf5o2m2il">^</a></strong></sup></span><div class="footnote-content"><p>您可以<a href="https://www.lesswrong.com/posts/LdFbx9oqtKAAwtKF3/list-of-probability-calibration-exercises">在这里</a>尝试一些免费的在线练习。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyc7t9ejx07l"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyc7t9ejx07l">^</a></strong></sup></span><div class="footnote-content"><p>不是概率的几何平均值！这样做会导致无效结果，因为例如 0.1 和 0.2 的几何平均值与 0.5 的距离与 0.8 和 0.9 的几何平均值不同。相反，您首先将概率转换为优势比，例如 50% 为 1:1，75% 为 3:1。 <span class="footnote-reference" role="doc-noteref" id="fnrefg48xho5quv"><sup><a href="#fng48xho5quv">[15]</a></sup></span>然后你取赔率的几何平均值并将其转换回概率。 <span class="footnote-reference" role="doc-noteref" id="fnrefkryzxomsrn"><sup><a href="#fnkryzxomsrn">[16]</a></sup></span></p></div></li><li class="footnote-item" role="doc-endnote" id="fncmbdamvfdq5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcmbdamvfdq5">^</a></strong></sup></span><div class="footnote-content"><p>当然，我们只相信索尔莫诺夫先验在某种意义上是“好的”，因为无论进化是用什么先验来构建人类的，这本身就有些武断。因此，存在一种无限回归，我们实际上无法谈论完全最优的推理机。但我们绝对可以比人类做得更好。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwf8vih6vmid"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwf8vih6vmid">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://en.wikipedia.org/wiki/Winner%27s_curse">胜利者诅咒</a>的变体</p></div></li><li class="footnote-item" role="doc-endnote" id="fng48xho5quv"><span class="footnote-back-link"><sup><strong><a href="#fnrefg48xho5quv">^</a></strong></sup></span><div class="footnote-content"><p>如有必要，可使用小数赔率，因此第二个数字始终为 1。在此系统下，25% 为 1/3:1。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnkryzxomsrn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkryzxomsrn">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://www.wolframalpha.com/input?i=o+%2F+%28o+%2B+1%29%2C+where+o+%3D+sqrt%28%28a+%2F+%281+-+a%29%29+*+%28b+%2F+%281+-+b%29%29%29%2C+where+a+%3D+0.1+and+b+%3D+0.2">这是</a>一个 Wolfram Alpha 链接，可以为您完成此操作，只需将“a”和“b”的值替换为范围的末尾即可。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Ng5n8FjpsfXrceT9w/understanding-subjective-probabilities#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Ng5n8FjpsfXrceT9w/understanding-subjective-probabilities<guid ispermalink="false"> NG5n8FjpsfXrceT9w</guid><dc:creator><![CDATA[Isaac King]]></dc:creator><pubDate> Sun, 10 Dec 2023 06:03:30 GMT</pubDate> </item><item><title><![CDATA[Send us example gnarly bugs]]></title><description><![CDATA[Published on December 10, 2023 5:23 AM GMT<br/><br/><p> Tl;dr：寻找评估的硬调试任务，支付 60 美元/小时或每个示例 200 美元的费用。</p><p> METR（以前称为 ARC Evals）有兴趣为模型生成硬调试任务，以尝试作为<a href="https://metr.org/"><u>代理能力评估</u></a>的一部分。为了创建这些任务，<strong>我们正在寻找包含极其棘手的错误的存储库。</strong>如果您向我们发送的代码库符合提交标准（如下所列），我们将向您支付 60 美元/小时的费用，用于将其转换为我们所需的格式，或 200 美元，以较高者为准。 （我们不会为不符合这些要求的提交内容付费。）如果我们对您提交的内容感到特别兴奋，我们可能也有兴趣购买它的知识产权。根据多样性，我们预计总共需要大约 10-30 个示例。我们可能会在接下来的几周内对其他类型的任务给予奖励。</p><p>提交标准：</p><ul><li>包含一个错误，熟练的程序员至少需要 6 小时才能解决，最好超过 20 小时</li><li>理想情况下，过去没有公开发布过，并且您能够保证将来不会公开发布。<ul><li> （但请注意，我们仍然可以接受来自公共存储库的提交，因为它们尚未包含在 SWE 基准数据集中并且满足我们的其余要求。请先与我们联系。）</li></ul></li><li>您有合法权利与我们分享（例如，请不要向我们发送其他人的专有代码或您签署保密协议的任何内容）</li><li>理想情况下，代码库是用 Python 编写的，但我们也接受用其他语言编写的提交。 <i>&nbsp;</i></li><li>采用本文档中描述的格式： <a href="https://docs.google.com/document/d/15L0J25uzzf0xJeDsDgPYW60PzFZ0enaWWnlCkrCSGAM/edit"><u>Gnarly Bugs Submission Format</u></a></li></ul><p>请将提交内容以 zip 文件的形式发送至<a href="mailto:gnarly-bugs@evals.alignment.org"><u>gnarly-bugs@evals.alignment.org</u></a> 。您的电子邮件应包含将代码从原始状态转换为我们所需格式所需的小时数。如果您提交的内容符合我们的标准和格式要求，我们将与您联系并提供付款表格。如果您有任何疑问，包括您不确定潜在提交的内容是否符合标准，也欢迎您发送电子邮件至<a href="mailto:gnarly-bugs@evals.alignment.org"><u>gnarly-bugs@evals.alignment.org</u></a> 。<br><br>如果您愿意以更高的工资完成这项任务，请告诉我们！</p><p> <i>（此外，如果您有兴趣分叉 SWEbench 以支持非 python 代码库，请联系我们。）</i></p><br/><br/><a href="https://www.lesswrong.com/posts/JKtM5C2TTwhzoHFRB/send-us-example-gnarly-bugs#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JKtM5C2TTwhzoHFRB/send-us-example-gnarly-bugs<guid ispermalink="false"> JKtM5C2TTwhzoHFRB</guid><dc:creator><![CDATA[Beth Barnes]]></dc:creator><pubDate> Sun, 10 Dec 2023 05:23:02 GMT</pubDate></item></channel></rss>