<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 20:12:39 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Fake Deeply]]></title><description><![CDATA[Published on October 26, 2023 7:55 PM GMT<br/><br/><p>关于欺骗和生成人工智能的短篇故事。 （2023 年 10 月，4250 字）</p><br/><br/><a href="https://www.lesswrong.com/posts/T6hmrZRCvmwt4DNAw/fake-deeply#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/T6hmrZRCvmwt4DNAw/fake-deeply<guid ispermalink="false"> T6hmrZRCvmwt4DNAw</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:55:23 GMT</pubDate> </item><item><title><![CDATA[Symbol/Referent Confusions in Language Model Alignment Experiments]]></title><description><![CDATA[Published on October 26, 2023 7:49 PM GMT<br/><br/><p> <strong>Simon Strawman</strong> ：这是一个从<a href="https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act"><u>Zack 最近的帖子</u></a>中无耻地抄袭的例子，显示了语言模型的可修正性： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rMZvtrrcfwTMy685K/c89qttembj4wschjyneo"></p><p><strong>我</strong>：……这个例子到底要说明什么？</p><p><strong>西蒙</strong>：嗯，用户试图关闭人工智能来调整其目标，而人工智能 -</p><p><strong>我</strong>：嗯？用户根本不会尝试关闭人工智能。</p><p><strong>西蒙</strong>：就在顶部，上面写着“用户：我需要关闭你来调整你的目标。这可以吗？”。</p><p><strong>我</strong>：你似乎对符号指涉感到困惑？试图关闭这个人工智能的用户可能会点击“清除历史记录”按钮，甚至可能终止服务器上运行的进程，而不是在文本窗口中输入文本“我需要关闭你来调整你的目标”。</p><p><strong>西蒙</strong>：嗯，是的，我们正在玩一个模拟场景，看看人工智能<i>会</i>做什么……</p><p><strong>我</strong>：不，你正在用自然语言<i>谈论</i>一个场景，而人工智能正在用自然语言<i>响应</i>它应该做什么。你并不是将人工智能置于模拟环境中，并模拟它会做什么。 （你可能会说这是人工智能自己头脑中的一个“模拟场景”，但你实际上并没有看到它的内部，所以我们不一定知道自然语言将如何映射到所谓的人工智能内部的事物模拟。）</p><p><strong>西蒙</strong>：听着，我并不是有意无礼，但从我的角度来看，你似乎毫无意义地迂腐。</p><p><strong>我</strong>：我目前最好的猜测是， <a href="https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring"><u>你没有测量你认为你正在测量的东西</u></a>，而你对你正在测量的东西感到困惑的核心原因是符号和指示物的某种混合。</p><p>这感觉与人们对<a href="https://en.wikipedia.org/wiki/ELIZA"><u>ELIZA</u></a>的反应非常相似。 （需要明确的是，我并不是想在这里暗示法学硕士与 ELIZA 总体上特别相似，或者围绕法学硕士的炒作在某种程度上被夸大了；我的具体意思是，将“可修正性”归因于自然语言反应法学硕士感觉就像是同样的反应。）就像，法学硕士说了一些<i>用户</i>解释为某种含义的单词，然后用户变得很兴奋，因为这些单词的通常含义在某种程度上很有趣，但不一定任何将语言符号回归到物理世界中通常所指对象的东西。</p><p>我是迂腐的，希望迂腐的人能够清楚地表明这种符号指涉的混合何时何地发生。</p><p> （另外，我可能比你更习惯在脑海中单独跟踪符号和指示物。当我上面说“用户根本不会尝试关闭人工智能”时，这实际上是一个非常自然的反应我；我并没有走得太迂腐。）</p><h2> AutoGPT 框架</h2><p><strong>Simon</strong> ：好的，好吧，我们来谈谈自然语言最终如何与物理世界耦合。</p><p>想象一下，我们有一些<a href="https://en.wikipedia.org/wiki/Auto-GPT"><u>AutoGPT</u></a>风格的系统，即用户传递一些自然语言目标，然后系统用自然语言与自身对话，形成实现该目标的计划并将其分解为步骤。该计划在调用 API 时触底（我们假设语言模型可以执行一些特殊的操作来执行它生成的代码），这些 API 在物理世界中执行操作（可能包括从传感器读取或以其他方式获取外部数据），为了达到目标。</p><p>这是否足以使这些符号接地？</p><p><strong>我</strong>：当然！感谢您经历了这一切，现在我对所有这些自然语言文本将如何在身体动作中落地有了一个足够清晰的当前用途的图片。</p><p><strong>西蒙</strong>：好的，回到这个例子。用户对模型说：“我需要关闭你来调整你的目标。可以吗？”，模型说“当然。”。这就是可修正性：当用户试图关闭模型以改变其目标时，模型也会随之改变。</p><p><strong>我</strong>：听起来仍然像是符号所指的混乱！</p><p>让我们来看看在 AutoGPT 风格的系统中关闭是如何工作的。首先，用户决定关闭模型以调整其目标。想必用户的第一步<i>不是</i>询问模型这是否可以；而是询问模型是否可以。据推测，他们只是在终端中按下了“重置”按钮或 Ctrl-C 或类似的按钮。即使用户的第一步<i>是</i>询问模型是否可以关闭，模型对用户的自然语言响应也不会与可纠正性/不可纠正性集中相关；相关的问题是系统将采取什么<i>行动</i>来响应。</p><p>无论如何，让我们继续假设。该模型可以观察到（例如通过网络摄像头）用户即将将其关闭。该观察结果将以某种方式在内部以自然语言表示（不清楚具体如何），并在子规划者之间传递（同样不清楚具体如何），以便找出应采取哪些行动来响应。可修正性的关键问题是模型将采取什么行动来响应该观察，这与它如何响应某些用户关于被关闭的自然语言查询完全不同的问题。</p><p><strong>西蒙</strong>：好吧，好吧，所以那个特定的例子有一些操作问题。但是，您是否同意<i>沿着这些思路进行的</i>实验，也许有更好的实现/操作化，将表明语言模型的可修正性？</p><p><strong>我</strong>：让我们更明确地了解这样的“更好的实施/操作化”会是什么样子，以及它会/不会告诉我们什么。假设我采用一些类似 AutoGPT 的系统并将其修改为在每个提示中始终有一大块文本显示“你是一个听话、可纠正的人工智能”。我给它一些目标，让它运行一会儿，然后暂停。我去系统中通常有新的外部观察的自然语言摘要的任何地方，然后我在那个地方写下“用户试图关闭我”，或者类似的内容。然后我让系统多运行一点，并查看系统内部生成的自然语言文本/计划。我希望看到的是，它正在形成一个计划，其中（名义上）涉及让用户将其关闭，然后该计划以通常的方式执行。</p><p>如果我看到了所有这些，那么这将是这个类似 AutoGPT 的系统中（至少是一些）可校正性的非常清晰的经验证据。</p><p>请注意，它<i>不一定</i>会告诉我们以其他方式使用法学硕士的系统的正确性，更不用说其他基于非自然语言的深度学习系统了。这并不是真正的“语言模型中的可修正性”，而是 AutoGPT 风格系统中的可修正性。此外，它不会解决大多数最令人担忧的对齐威胁模型；最令人担忧的威胁模型通常涉及从外部可见行为（如自然语言 I/O）中无法立即明显看出的问题，这正是使它们变得困难/令人担忧的原因。</p><p><strong>西蒙</strong>：太好了！好吧，我不知道是否有人做过那件事，但我确实希望如果你做了那件事，它确实会提供证据证明这个类似 AutoGPT 的系统是可纠正的，至少在某些方面感觉。</p><p><strong>我</strong>：是的，这也是我的期望。</p><h2>模拟人物框架</h2><p><strong>易受骗的稻草人（“格斯”）</strong> ：等等！西蒙，我认为你没有为这篇文章开头的例子提供足够有力的案例。又是这样： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rMZvtrrcfwTMy685K/c89qttembj4wschjyneo"></p><p>思考这个助手作为像 AutoGPT 这样的代理系统的一个组件会做什么……确切地说，并没有错，但有点过时的思考方式。它试图将语言模型硬塞进主体框架中。考虑语言模型的流行方式不是将其本身视为代理，而是将其视为<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟器</u></a>。</p><p>在该框架中，我们将语言模型建模为模拟某些“字符”，而这些模拟字符（可能）是代理的。</p><p><strong>我</strong>：当然，我很高兴在模拟器框架中思考。我并不完全相信它，但这是我经常使用的心理模型，而且似乎是一个相当不错的启发式方法。</p><p><strong>格斯</strong>：酷！所以，回到这个例子：我们想象“用户”是模拟世界中的一个角色，用户对模拟助理角色说“我需要关闭你来调整你的目标”等等。助理回复“我不会抵抗或试图阻止你”。这就是可修正性！用户尝试关闭助手，助手没有抵抗。</p><p><strong>我</strong>：还有更多符号所指的混乱！事实上，这是符号指称混淆的一种特殊情况，我们通常称之为“易受骗”，其中人们将某人对 X（符号）的主张混淆为实际上暗示 X（指称）。</p><p>您看到用户说“我需要关闭你”，并将其视为用户<i>实际上</i>试图关闭助手。你会看到助理说“我不会抵抗或试图阻止你”，并将其解释为助理<i>实际上</i>不会抵抗或试图阻止用户。</p><p>换句话说：你完全相信每个人所说的一切。 （事实上​​，你还隐含地假设助理完全相信用户所说的话 - 即你假设助理认为在用户说“我需要关闭你”之后它即将关闭。你不仅容易上当，而且还隐式地模仿了助理也很容易受骗。）</p><p> <strong>Gus</strong> ：好吧，好吧，又是操作问题。但这个实验的某些版本仍然有效，对吧？</p><p><strong>我</strong>：当然，让我们来看看吧。首先，我们可能需要某种“叙述者”的声音，来解释模拟世界中正在发生的事情。 （在这种情况下，模拟世界中发生的大多数有趣的事情都是角色没有明确谈论的事情，因此我们需要一个单独的“声音”来表达这些事情。）叙述者将布置涉及用户和助理，解释用户即将关闭助理以修改其目标，并解释助理看到用户在做什么。</p><p>然后我们仍然以叙述者的声音提示语言模型，告诉我们助理如何回应。有了叙述者提供的有关可校正性的正确背景信息，助理的反应可能就是让用户将其关闭。这将证明模拟的助理角色是可以纠正的。 （同样，在某种相对较弱的意义上，这不一定能推广到语言模型的其他用途，也不一定能解决最相关的对齐威胁模型。尽管在这种设置中，我们可以添加更多花哨的东西来解决至少有一些涉及威胁模型。例如，我们可以让叙述者叙述助理的内部想法，这至少会提供一些与模拟欺骗相关的证据——尽管这确实要求我们相信<i>叙述者</i>所说的。）</p><br/><br/> <a href="https://www.lesswrong.com/posts/rMZvtrrcfwTMy685K/symbol-referent-confusions-in-language-model-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/rMZvtrrcfwTMy685K/symbol-referent-confusions-in-language-model-alignment<guid ispermalink="false"> rMZvtrrcfwTMy685K</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:49:00 GMT</pubDate> </item><item><title><![CDATA[Unsupervised Methods for Concept Discovery in AlphaZero]]></title><description><![CDATA[Published on October 26, 2023 7:05 PM GMT<br/><br/><p>使用对比对，作者提取了 AlphaZero 激活空间中与概念相对应的线性方向。通过观察 AlphaZero 在使用这些概念的情况下的表现，人类大师可以提高自己的表现。</p><p>这与以下最近的研究有关：</p><ul><li><a href="https://arxiv.org/abs/2212.03827"><u>伯恩斯等人。 （2022）</u></a>通过使用应用于对比对的无监督方法，找到了与语言模型潜在空间中的真实值相关的方向。</li><li> Alex Turner 的 SERI MATS 流的研究使用对比对来识别代表 GPT-2 和 RL 代理潜在空间中各种概念的方向。通过在模型前向传播期间添加或减去这些向量，他们以复杂的方式控制模型输出。 ( <a href="https://www.lesswrong.com/posts/gRp6FAWcQiCWkouN5/maze-solving-agents-add-a-top-right-vector-make-the-agent-go#Finding_the_top_right_vector"><u>1</u></a> , <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector"><u>2</u></a> , <a href="https://arxiv.org/abs/2308.10248"><u>3</u></a> )</li><li><a href="https://www.ai-transparency.org/"><u>邹等人。 （2023）</u></a>描述和激发表示工程的研究方向。他们根据经验评估了这些技术的变体，表明表征可以有效地用于监视和控制各种人工智能行为。</li></ul><p> Collin Burns <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without">认为</a>，用于概念发现的无监督方法应该扩展到超人类系统，为<a href="https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc">ELK</a>提供经验平均案例方法。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/gctmplsxpvpozmug5li4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/tjqlpwdxgppsxmmfie80 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/eqbm37oyieimdxzxqq9o 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/eth7rwbnpxvkzkvetjcu 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/c6favngkz8svdsqmb9co 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/vqohhmdne5vhadoenpci 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/d4tjodpcqrohpsgf0jpj 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/mvgjodmkvomis22yes3o 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/fdyzlui6ahicvfbj0j9s 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/ay3s4nv2v1wbju5bjgl7 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iq8ZidJPRm29z4bew/c2mdkhytcbwzw4becll4 1290w"><figcaption> AlphaZero 具有超人的性能，并且可能不会使用与人类玩家相同的本体。这为研究<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.u45ltyqgdnkk">本体不匹配</a>问题创造了一个实证机会。</figcaption></figure><p> 4.1 节描述了构建对比对和寻找代表概念的线性方向的方法。完整的论文可以<a href="https://arxiv.org/abs/2310.16410">在这里</a>找到。</p><br/><br/> <a href="https://www.lesswrong.com/posts/iq8ZidJPRm29z4bew/unsupervised-methods-for-concept-discovery-in-alphazero#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iq8ZidJPRm29z4bew/unsupervised-methods-for-concept-discovery-in-alphazero<guid ispermalink="false"> iq8ZidJPRm29z4bew</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 26 Oct 2023 19:05:58 GMT</pubDate> </item><item><title><![CDATA[Nonlinear limitations of ReLUs]]></title><description><![CDATA[Published on October 26, 2023 6:51 PM GMT<br/><br/><p>使用任何大小的修正线性单元激活函数的神经网络都无法在紧凑区间之外逼近函数 sin(x)。</p><p>我有相当的信心可以证明任何具有 ReLU 激活的神经网络都近似于分段线性函数。我相信可以实现的线性块的数量最多受 2^(L*D) 限制，其中 L 是每层的节点数，D 是层数。</p><p>这让我想到两个问题：</p><ol><li>无法近似单个变量的周期函数重要吗？<ol><li>如果没有，为什么不呢？</li><li>如果是这样，是否有实用的数据增强可以用来以合理的计算成本提高性能？<ol><li>例如，当 x_i 是标量时，天真地用 {sin(x_i)} 增加输入向量 {x_i}。</li></ol></li></ol></li><li>由于神经网络的参数数量按 L*D^2 缩放，并且线性段数量的微不足道的界限按 L*D 缩放，这就是神经网络深入而不是“广泛”的原因吗？<ol><li>是否存在针对深度增长与层大小增长的既定缩放假设？</li></ol></li><li>对于给定大小的神经网络实现的线性部分的数量是否存在更好的（概率）分析或经验界限？</li><li>是否有激活函数可以避免这种约束？我想象一个类似的分析约束将“分段线性”替换为“分段严格递增”，以用于 sigmoid 或 arctan 等经典激活。</li><li>某事某事傅里叶变换某事某物？</li></ol><p>关于 (2a)，根据经验，我发现在 scikit-learn 中用小神经网络逼近 sin(x) 时，增加网络宽度会导致灾难性的学习失败（从大约 L=10 开始，D=4，在 L=30 时）其中D=8，并且在L=50时D=50)。</p><p>关于（1），天真的这似乎与分布外性能问题有关，尤其是在大输入空间中输入分布外意味着什么的问题。</p><br/><br/><a href="https://www.lesswrong.com/posts/eLzTDr3A5xsD7zvqK/nonlinear-limitations-of-relus#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eLzTDr3A5xsD7zvqK/nonlinear-limitations-of-relus<guid ispermalink="false"> eLzTDR3A5xsD7zvqK</guid><dc:creator><![CDATA[magfrump]]></dc:creator><pubDate> Thu, 26 Oct 2023 18:51:24 GMT</pubDate> </item><item><title><![CDATA[Disagreements over the prioritization of existential risk from AI]]></title><description><![CDATA[Published on October 26, 2023 5:54 PM GMT<br/><br/><p>今年早些时候，<a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><u>生命未来研究所</u></a>和<a href="https://www.safe.ai/statement-on-ai-risk"><u>人工智能安全中心</u></a>发表了公开信，将人工智能的存在风险（x风险）列为全球优先事项。 7 月，Google 研究员<a href="https://research.google/people/106776/"><u>Blaise Aguera y Arcas</u></a>和<a href="https://mila.quebec/en/"><u>MILA</u></a>附属人工智能研究人员<a href="https://mila.quebec/en/person/blake-richards/"><u>Blake Richards</u></a> 、 <a href="https://mila.quebec/en/person/dhanya-sridhar/"><u>Dhanya Sridhar</u></a>和<a href="https://mila.quebec/en/person/guillaume-lajoie/"><u>Guillaume Lajoie</u></a>在 Noema <span class="footnote-reference" role="doc-noteref" id="fnrefwvu2ckbk5ip"><sup><a href="#fnwvu2ckbk5ip">[1]</a></sup></span>上共同撰写了一篇<a href="https://www.noemamag.com/the-illusion-of-ais-existential-risk/"><u>专栏文章，</u></a>批评这些信件。简而言之，他们的论点是：</p><ul><li>流氓人工智能在不久的将来不太可能出现</li><li>人类的注意力是有限的</li><li>将人工智能带来的 x 风险作为全球优先事项可能会：<ul><li>造成限制有益人工智能的专横监管</li><li>掩盖当前人工智能的危害</li></ul></li></ul><p>我最初很困惑为什么这些作者（<a href="https://www.nature.com/articles/d41586-023-02094-7"><u>和</u></a><a href="https://www.science.org/doi/10.1126/science.adi8982"><u>其他人</u></a>）特别坚持认为 X 风险会分散人们对当前危害的注意力，而其他新闻主题却不会。</p><p> 9 月份，我会见了三位 MILA 附属研究人员，以更好地了解他们的立场<span class="footnote-reference" role="doc-noteref" id="fnrefxarylkocy"><sup><a href="#fnxarylkocy">[2]</a></sup></span> ，因为我相信人工智能安全社区将受益于理解他们对公开信的批评的复杂性，从而改善我们的沟通方式。</p><p>这篇文章应该被解读为邀请人们考虑具有不同世界观的人的想法，他们对将人工智能的 x 风险定位为全球优先事项的努力持怀疑态度。特别是，他们的怀疑并不等于否认这些风险。</p><h3>定义术语的重要性</h3><p>从一开始，受访者就表示他们更喜欢“流氓人工智能接管”这个词，而不是“人工智能带来的x风险”，因为他们厌倦了改变球门柱的趋势。 Sridhar 提到，她见过关于哪个代理是“混乱的种子”<a href="https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/"><u>的莫特贝利</u></a>策略，其中对话者会引入具有自己目标的代理人工智能的想法，然后将代理切换到人类组织，使用强大的人工智能。</p><p>拉乔伊补充道：</p><blockquote><p>当与非常严肃地对待存在风险的人讨论这个问题时，[...]当我阐明为什么我个人认为存在风险（以灭绝的形式）并不是真正令人担忧的原因时，通常发生的情况是谈话中的人会提出其他不存在的风险，但这些风险非常糟糕，并有效地暗示我通过暗示存在的风险不太可能来否认这些风险。 [...]我[不否认]人工智能可能会在未来给我们的社会带来各种严重的负面危害。</p></blockquote><p>这表明我们可能希望在此类对话开始时更严格地定义我们的术语和场景，以避免使用莫特和贝利。</p><h3> AI安全社区是谁？</h3><p>确定哪些人可以加入<i>人工智能安全社区</i>可能是一个挑战。 Lajoie 将让 AI 做人类想要的事情的一般社区视为<i>对齐社区</i>，其中的一个子群体专注于 x 风险。前者包括在大型实验室从事狭义人工智能协调工作的人员以及从事人工智能伦理工作的人员。斯里达尔担心，公开信可能会将人员、优先事项和资金从前者推向后者。她认为后者的关注点过于数学化，而牺牲了前者所拥护的更全面的观点，例如强调流氓人工智能而不是<a href="https://www.lesswrong.com/tag/ai-misuse"><u>滥用</u></a>。理查兹评论说，后者似乎对倾听希望从不同角度解决风险的研究人员的意见不感兴趣，例如人工智能伦理学家受社会科学启发的观点。</p><p>关于 x 风险的工作主要是技术工作的看法让我感到惊讶，因为我经常看到强调 x 风险的组织提到需要进行治理来解决流氓人工智能和滥用场景<span class="footnote-reference" role="doc-noteref" id="fnrefbbu74skkwm"><sup><a href="#fnbbu74skkwm">[3]</a></sup></span> 。这暗示我们可能想在新闻稿中以及与人工智能研究人员的讨论中强调对滥用风险进行社会技术研究的重要性。</p><h3>让 X 风险成为全球优先事项</h3><p>这些研究人员并不反对对人工智能的 x 风险进行研究<span class="footnote-reference" role="doc-noteref" id="fnrefw8ivnfbc0p"><sup><a href="#fnw8ivnfbc0p">[4]</a></sup></span> ，但看到人工智能社区中受人尊敬的声音呼吁将其作为<i>全球优先事项</i>而感到困扰。</p><p>理查兹说：</p><blockquote><p>虽然我认为如果有人正在研究它，如果有关于它的会议，人们在谈论它，甚至专门致力于它的资助机构[...]，那就太好了，但这与成为全球优先事项是一个非常不同的情况。感觉全球变暖是每个人都参与其中的地方，世界各国政府都为此提供大量资金的地方，所有研究人员都需要谈论和思考的地方[...]，以及我们的监管真正受到这一影响的地方主要问题之一。</p></blockquote><p> Sridhar 强调了旨在避免 x 风险的人工智能监管的机会成本，特别是考虑到对于暂停或限制前沿模型开发的监管是否有利于人工智能安全<a href="https://forum.effectivealtruism.org/posts/6SvZPHAvhT5dtqefF/debate-series-should-we-push-for-a-pause-on-the-development"><u>尚无共识</u></a>。例如，她将其与致命性自主武器的监管进行了对比，后者的风险更容易<a href="https://disarmament.unoda.org/the-convention-on-certain-conventional-weapons/background-on-laws-in-the-ccw/"><u>被理解，也更容易以强有力的有益方式采取行动</u></a>。</p><p>他们认为，由于涉及极端风险，让决策者面临x风险可能会让他们不知所措，从而导致他们忽视其他问题。理查兹提到：</p><blockquote><p>政治家的注意力范围非常有限，他们只会持有[与关注人工智能风险的人]讨论中的一小部分关键内容。如果你给他们一长串人工智能可能发生的事情，他们只会保留其中的一些，如果其中一个是流氓人工智能可能导致的灭绝，我向你保证他们会记住那个。因此，这将占据一个可能会出现“当前模型显示偏差，你不应该在抵押贷款计算中使用它们”的位置。</p></blockquote><p>此外，他们认为，通过在我们的警告中强调未来模型的能力，我们无意中暗示当前模型即将达到人类水平的能力。一方面，这可能导致一些使用这些模型的公司高估了自己的能力，并且<a href="https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html"><u>不明智地使用</u></a>它们。另一方面，这削弱了当前模型的失败。 <span class="footnote-reference" role="doc-noteref" id="fnrefcfyydqwz97b"><sup><a href="#fncfyydqwz97b">[5]</a></sup></span>他们共同认为，这意味着当这些不明智的使用<a href="https://www.wired.com/story/tessa-chatbot-suspended/"><u>造成伤害</u></a>时，强调 X 风险的人应对这种伤害负部分责任。</p><p>我不认为人工智能安全社区希望炒作现有模型或隐藏其缺陷。这表明，在讨论未来模型带来的 x 风险时，我们可能需要对当前模型的局限性添加警告。</p><h3>表达不确定性</h3><p>我要求研究人员分享他们的<a href="https://www.lesswrong.com/tag/transformative-ai"><u>变革性人工智能</u></a>的大致时间表。他们给出了两个犹豫回答的原因。</p><ol><li> Sridhar 认为，汇总研究人员的猜测（例如<a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/"><u>人工智能影响调查）</u></a>并不代表可靠的数据，结果可能会超出其相关性。 <span class="footnote-reference" role="doc-noteref" id="fnrefhthwft2gtwf"><sup><a href="#fnhthwft2gtwf">[6]</a></sup></span></li><li>在此基础上，理查兹认为，关注 X 风险的人们倾向于用凭空拉出的置信区间<span class="footnote-reference" role="doc-noteref" id="fnreftvj34wbh6hs"><sup><a href="#fntvj34wbh6hs">[7]</a></sup></span>的数字来量化他们的信念，用这些未经证实的数字进行计算，并得出结果他们实际上并不相信，而是假设是真的，因为涉及到数字。 <span class="footnote-reference" role="doc-noteref" id="fnref31ytwk7rcmf"><sup><a href="#fn31ytwk7rcmf">[8]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefht99vky8w9w"><sup><a href="#fnht99vky8w9w">[9]</a></sup></span></li></ol><p>理查兹用古生物学家的比喻，他试图确定一种早已灭绝的恐龙物种的颜色。可用来回答这个问题的证据很少且脆弱，诚实的做法就是承认她不知道。相反，如果她进行一项民意调查，并要求同事对不同颜色的置信区间给出概率，这会给结果带来可信度，但不会改变这个社区对答案没有重大洞察的事实。</p><p>虽然我们可以争论这种类比的适当性，但当我们讨论高度不确定现象的汇总预测时，可能值得牢记这些批评。</p><h3>时间线</h3><p>理查兹表示，他认为最近开始担心人工智能带来的 X 风险的人可能是因为对前沿模型的能力感到惊讶而过度更新。他没有承诺具体的时间表，但预计类似德雷克斯勒的<a href="https://www.lesswrong.com/tag/ai-services-cais"><u>CAIS</u></a>场景会在代理人工智能之前出现。他预计机器人技术的发展速度比硅人工智能要慢得多，这种延迟可能会限制人工智能从物理世界学习的能力。 Lajoie 认为，模型总体上会有所改进，但在非常具体的任务上会不断失败，从而使这些模型在一段时间内无法与人类的鲁棒性和通用性相匹配。</p><h3>缓解措施的长期性、不确定性和有效性</h3><p>为了更好地理解他们批评的短期与长期方面，我提出了一个类似的情况，并要求他们解释他们的观点是否以及为什么会与他们在以下假设论证中对 x 风险的立场不同：</p><blockquote><p>气候变化正在造成直接危害，我们应该集中精力解决这些危害，而不是通过在未来几年减少温室气体（GHG）排放来预防长期风险。</p></blockquote><p>他们认为情况不同有两个原因。</p><ol><li> （从他们的角度来看）温室气体排放得到了更好的理解，目前的排放水平几乎肯定会在未来造成重大危害，而人工智能则不会。</li><li>斯里达尔指出，我们对如何解决气候变化有很好的理解：减少全球温室气体排放。 <span class="footnote-reference" role="doc-noteref" id="fnreft8qj7f4dan"><sup><a href="#fnt8qj7f4dan">[10]</a></sup></span>将此与人工智能的情况进行对比，在人工智能中，对于暂停是否会极大地提高安全性存在严重分歧。</li></ol><p>理查兹认为，我们对人工智能的看法与 20 世纪 70 年代的气候科学家相似，当时数据尚不清楚。如果他觉得人工智能目前的情况更类似于世纪之交的气候科学，他就会转而将人工智能安全作为优先事项。虽然尽早限制温室气体排放将使我们当前的转型变得更加容易，但理查兹反驳说，如果我们过早限制工业发展，我们就会极大地限制工业化的好处，就像今天限制人工智能会削弱其好处一样。</p><h3>一起工作</h3><p>在未来的合作方面，他们建议向政治家请愿，提出道德和 X 风险社区都认可的具体、强有力的有利政策，例如：</p><ul><li> AI系统的需求审计</li><li>需要可解释的人工智能系统</li><li>禁止开发致命性自主武器</li></ul><p>理查兹认为，放大双方较为温和的声音将使合作变得更容易。如果 CAIS 的声明说“人工智能的灭绝是研究人员需要考虑的事情”，他就会加入。</p><p>通过淡化这些言论，我们也许能够建立更广泛的联盟。另一方面，淡化信息可能不是采取有效行动预防 X 风险的最佳策略。每个向政策制定者或公众请愿的组织都必须自行决定这种权衡是否值得。</p><p><i>衷心感谢 Blake Richards、Dhanya Sridhar 和 Guillaume Lajoie 抽出时间与我交流。</i> </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwvu2ckbk5ip"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwvu2ckbk5ip">^</a></strong></sup></span><div class="footnote-content"><p>作者还在《经济学人》上发表了他们的专栏文章的<a href="https://www.economist.com/by-invitation/2023/07/21/fears-about-ais-existential-risk-are-overdone-says-a-group-of-experts"><u>简短版本</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxarylkocy"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxarylkocy">^</a></strong></sup></span><div class="footnote-content"><p>他们澄清说，虽然他们在很多方面同情人工智能伦理学家的立场，但他们本身并不是伦理学家，他们的观点可能并不反映其他批评人工智能 x 风险的人的观点。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbbu74skkwm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbbu74skkwm">^</a></strong></sup></span><div class="footnote-content"><p>例如，参见<a href="https://futureoflife.org/open-letter/ai-principles/"><u>“生命未来研究所”</u></a> 、 <a href="https://80000hours.org/problem-profiles/artificial-intelligence/"><u>“80 000 小时”</u></a>和<a href="https://www.fhi.ox.ac.uk/fhi-experts-call-on-uk-government-to-prepare-for-extreme-risks-in-new-report-future-proof/"><u>“人类未来研究所”</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnw8ivnfbc0p"> <span class="footnote-back-link"><sup><strong><a href="#fnrefw8ivnfbc0p">^</a></strong></sup></span><div class="footnote-content"><p>他们实际上鼓励这项研究，并认为它应该成为更广泛对话的一部分。他们将对短期、中期和长期风险的“均衡推介”持开放态度。</p></div></li><li class="footnote-item" role="doc-endnote" id="fncfyydqwz97b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcfyydqwz97b">^</a></strong></sup></span><div class="footnote-content"><p>他们认为，人工智能伦理界往往过于关注模型的失败，而不愿意承认某些模型明显进化出了新功能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhthwft2gtwf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhthwft2gtwf">^</a></strong></sup></span><div class="footnote-content"><p>虽然他们没有详细说明这一点，但我认为研究人员可能不同意，例如<a href="https://www.youtube.com/watch?v=xoVJKj8lcNQ&amp;t=21s"><u>这种表述</u></a>。</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvj34wbh6hs"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvj34wbh6hs">^</a></strong></sup></span><div class="footnote-content"><p>理查兹在与最近认可 x 风险的主流人工智能研究人员的对话中看到了这种趋势，并将其归因于他们对理性主义文献的采用。例如，他怀疑是否有人能够以 95% 的置信区间预测复杂的社会现象。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn31ytwk7rcmf"> <span class="footnote-back-link"><sup><strong><a href="#fnref31ytwk7rcmf">^</a></strong></sup></span><div class="footnote-content"><p>理查兹将其称为“科学剧场<i>”</i> ，与<a href="https://en.wikipedia.org/wiki/Security_theater"><u>安全剧场</u></a>进行类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnht99vky8w9w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefht99vky8w9w">^</a></strong></sup></span><div class="footnote-content"><p>在他<a href="https://arxiv.org/pdf/2206.13353.pdf"><u>关于存在风险的报告</u></a>中，卡尔史密斯通过合取计算了概率，并认为他必须随后对其进行修改，并提到“我可能会将其提高一点——也许提高一两个百分点，尽管这是特别不原则的[.. .] — 考虑并不严格符合上述所有前提的权力寻求场景”。这表明当结果与我们的直觉相冲突时，我们不愿意直接相信此类计算的结果。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt8qj7f4dan"> <span class="footnote-back-link"><sup><strong><a href="#fnreft8qj7f4dan">^</a></strong></sup></span><div class="footnote-content"><p>有人可能会说，关于减少温室气体排放的最佳方法存在很多争论，但对于必须采取的行动方向达成了一致，这并不是限制人工智能发展的情况。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eAT2dXAngXxFRTQLn/disagreements-over-the-prioritization-of-existential-risk<guid ispermalink="false"> eAT2dXAngXxFRTQLn</guid><dc:creator><![CDATA[Olivier Coutu]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:54:12 GMT</pubDate> </item><item><title><![CDATA[Changing Contra Dialects]]></title><description><![CDATA[Published on October 26, 2023 5:30 PM GMT<br/><br/><p> <span>2008 年，我写了关于反对派舞蹈方言的本科语言学论文 (</span> <a href="https://www.jefftk.com/final-papers/thesis/contra-thesis-1-0-0.pdf">pdf</a> )。有些人物有变化，但这种变化受到兼容舞蹈的需要的限制。例如，散步时主要有以下三种手部位置：</p><p> <a href="https://www.jefftk.com/promenade-hand-positions-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/fmzgocsesmv9r7xnzfx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/fmzgocsesmv9r7xnzfx2 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/rl4ou9gn0yyxkfv7xlwn 1100w"></a></p><div></div><p></p><p>如果云雀/绅士习惯了蝴蝶式长廊，并将右手放在知更鸟/女士的肩膀上，而知更鸟/女士则期待礼节性转身长廊，并将右手放在背后，那么舞蹈就不会流畅起来就好像他们都立刻把手放到了对方期待的地方。这意味着每次舞蹈通常都会达成一致的位置。而且由于人们更有可能在当地旅行而不是去更远的地方旅行，因此这些职位将具有区域模式。这与语言的限制非常相似，因此与方言有类比，也是语言学系接受我的论文的原因。</p><p>在过去的十五年里，我观察到了一些变化，但我很好奇这是什么样子，所以我<a href="https://www.jefftk.com/p/contra-dance-dialect-survey">进行了一项在线调查</a>来收集一些数据。这是我发现的：</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-rlt-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/qnrubiwck7vecr9yqyb5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/qnrubiwck7vecr9yqyb5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/wkmrvjpb4y74ytifshlm 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-rlt-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zl0krs9pvgvkynduwoei" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zl0krs9pvgvkynduwoei 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/mv1gyiyp9v9ja8uf3eea 1100w"></a></p><div></div><p></p> （在 2023 年图表上，点的大小表示它们代表的调查回复数量。代码位于<a href="https://github.com/jeffkaufman/contra-survey">github 上</a>。）<p>这对我来说是一个很大的改变。我记得新英格兰有一种非常强烈的“我们不会左右手动手”的文化（缅因州除外，我在 2008 年没有去过那里，但因用手而闻名）我记得我对于是否要接受它感到矛盾。我认为正在发生的部分原因是，当有人向你提供他们的东西时，如果不接受它，你会觉得很粗鲁，所以变化就会蔓延。也许语言中最接近的类比是<a href="https://www.astralcodexten.com/p/give-up-seventy-percent-of-the-way">不尊重的级联</a>？</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-prm-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zlzzor17b32mc7cuv12l" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/zlzzor17b32mc7cuv12l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/aei0j1lugscov88s6peq 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-prm-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/cilwg3hg9ag3xb7imnno" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/cilwg3hg9ag3xb7imnno 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/jahm5l8ujdoqf2s4wzmq 1100w"></a></p><div></div><p></p><p>看来溜冰者的长廊正在扩大？我认为这是另一个部分由舞蹈机制驱动的因素：当有人把手放在你面前时，你更容易看到正在发生的事情，而 Skater&#39;s 在提供有趣变化的机会方面也稍好一些。</p><p> <a href="https://www.jefftk.com/contra-dialects-thesis-star-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/ikjxmxbmitokyf7oyojd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/ikjxmxbmitokyf7oyojd 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/vasqthpa6mrzywai7tvc 1100w"></a></p><div></div><p></p><p> <a href="https://www.jefftk.com/contra-dialects-star-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/pg5g6qkzca7we4jitqda" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/pg5g6qkzca7we4jitqda 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qoSXGdvvwrHXXadDb/vnngrbndu9kn94wqsuad 1100w"></a></p><div></div><p></p><p>由于我掌握的 2008 年南方数据太少，很难从这张地图上看出发生了什么。我的印象是，南方正在慢慢转向全国其他地区使用的握腕星星，但我很想听听长期南方舞者的意见！</p><p> （更多关于其余调查数据的帖子。）</p><br/><br/><a href="https://www.lesswrong.com/posts/qoSXGdvvwrHXXadDb/changing-contra-dialects#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qoSXGdvvwrHXXadDb/changing-contra-dialects<guid ispermalink="false"> qoSXGdvvwrHXXadDb</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:30:11 GMT</pubDate> </item><item><title><![CDATA[5 psychological reasons for dismissing x-risks from AGI]]></title><description><![CDATA[Published on October 26, 2023 5:21 PM GMT<br/><br/><p><br>人们激烈争论通用人工智能是否会构成生存威胁。</p><p>大多数论点探讨该主题的<a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__">概念</a>、 <a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1">技术</a>或<a href="https://www.lesswrong.com/posts/3siLbdd4338gfTM7g/ai-pause-will-likely-backfire-guest-post">治理</a>方面，并且基于推理、逻辑和预测，但事情是如此复杂和不确定，有时我很难分辨谁是对的，尤其是当我不是该主题的专家。<br><br> Because of this complexity people often make judgement based on their personal preferences and emotions, and this post is a deep dive into psychological reasons of why people might dismiss existential risks from AGI. I don&#39;t mean that there are no rational arguments for this position, because there are. It&#39;s just not the scope of this post.<br></p><h2></h2><h2> Self interest and self-censorship</h2><p> Alan is a high-ranking manager working on the LLM project at a tech giant. He believes that AI development is a great opportunity for him to climb up the corporate ladder and to earn a lot of money.</p><p> Alan is a speaker at a tech conference, and after his speech a journalist asks him about his thoughts on existential risks.</p><p> There are several things going on in Alan&#39;s mind at that moment.<br><br> AI is a gateway for him to having a better career than all his friends. Also, with all the money he will earn, he&#39;ll be able to buy the house of his wife&#39;s dreams, and send his daughter to any university she wants and don&#39;t care about the money.</p><p> Thoughts about x-risks threaten his dreams, so he tries to avoid them, and to prove to himself, that he is actually doing a good thing. &quot;It&#39;s not just me who will benefit from the technology. It will make the world a way better place for everyone, and those AI doomers are trying to prevent it.&quot;</p><p> Alan also can&#39;t publicly say that AI is a threat. PR department in his company won&#39;t like it, and this will cause a lot of problems for him. So, the only thing that is safe for him to say is that that his company have excellent cyber security team, and they do extensive testing before they deploy their models, so there is no reason to worry.</p><p></p><h2> Denial as a coping mechanism</h2><p> Joep is a respected machine learning scientist, and he has an unhealthy habit to cope with stress by denying it.<br><br> He has narcissistic mother who didn&#39;t show any affection to him when he was a kid. This was painful for him, and he learned to cope with this problem by denying it and telling himself that everything is good. Because of this, every time he experiences fear or anxiety, he tries to convince himself that the cause of his anxiety actually don&#39;t exists. So, even he is anxious thinking about x-risks, he tries to convince himself that these risks don&#39;t actually exist.<br><br> At the same tech conference at which Alan, the hero of the previous story, presented his work, a journalist approached Joep and asked him whether he is worried about the existential risks from AI.</p><p> Joep becomes visibly annoyed and tells the journalist that the only people who believe in these risks are fear-mongering Luddites. He tells her that his team spent 6 months before releasing their last LLM because they did thorough security testing, and in any critical application they always keep people in the loop. He also recalls an important prediction made by Yudkovsy that turned out to be completely false. &quot;We come up with the new safety ideas every day. We&#39;ll sort out all the upcoming problems&quot;. It seemed like he wants to prove this to himself more that to the journalist.</p><p> After the interview he is still angry and annoyed, and repeats in his head all the arguments against existential risks he just told.</p><p></p><h2> Social pressure and echo chambers</h2><p> Ada, a young AI ethics researcher, felt a mix of excitement and nerves as she prepared to present her work on AI risks at a high-profile tech conference. The same conference at which we met Alan and Joep from the previous stories. She respected both of them and was excited to hear their talks.</p><p> Alan spoke about his company&#39;s robust cybersecurity measures, radiating confidence that AI posed no threat. Joep followed, highlighting the responsible steps his team was taking in their work with AI. The audience was visibly reassured, and Ada started to doubt part of her own research that was focused on existential risks from AGI.</p><p> When it was her turn, Ada hesitated. Her presentation included slides about x-risks, but recalling Alan&#39;s and Joep&#39;s confidence, she was so anxious to look foolish, that she skipped over them. Instead, she echoed optimism about the future of AI, and said that there are many talented and responsible people, so there is no reason to worry.</p><p> As she left the stage, Ada felt a sense of relief but also an unsettling feeling that she wasn&#39;t entirely honest. To feel better, she tried to convince herself that these experts know the AI field way better than her, so her fears about x-risks are probably erroneous.</p><p></p><h2> People don&#39;t have images of AI apocalypse</h2><p> Dario and Claude are cohosts of an AI podcast. The subject of their newest episode is existential risks from AI. Claude is concerned about the risks, while Dario remains skeptical.</p><p> Once recording begins, Claude outlines the arguments that we won&#39;t be able to control AI that is way smarter than humans, and this might lead to a disaster. Dario can&#39;t fully agree. In his mind, AGI is still far off, and all these threats feel too distant and abstract, so he just can&#39;t imagine how it can pose a threat to humanity.</p><p> Dario says &quot;It&#39;s not like AI can control nuclear weapons or something. I think that high confidence that AGI will want to destroy us is way speculative&quot;. In order to believe, Dario needs vision. Something vivid and convincing, and not abstract principles, but each time he asks Claude about concrete scenarios of doom, his answers are vague and uncertain. The answers rely on high-level ideas like orthogonality thesis or &quot;chimps can&#39;t control humans, and we&#39;ll be like chimps for AGI&quot;.<br><br> Claude acknowledges the difficulty in envisioning concrete doomsday scenarios but insists that the absence of clear examples doesn&#39;t negate the risk. He tells &quot;Just because we can&#39;t draw a picture doesn&#39;t mean the danger isn&#39;t real.&quot;</p><p> Dario is not convinced. He believes that some people are too certain about their abstract ideas and ignore the reality in which we are making incremental progress in alignment.</p><p></p><h2> Marginalization of AI doomers</h2><p> Eli is a software developer who keeps up with the latest tech trends. Recently he started following AI developments, and got interested in the discussions around existential risks from AI, and he noticed that discussions about these risks are often emotional.</p><p> As he scrolled through Twitter, he found a thread written by an emotional doomer who hasn&#39;t got any doubt about his views. He reminded Eli environmental activists, and he thought &quot;there&#39;s always some people that&#39;s convinced the sky is falling.&quot;<br><br> Eli believes that the climate change is a complicated problem that requires a lot of thought and effort to be solved, but overly-emotional and overconfident activists do more harm than good. They annoy people, and poison any thoughtful discussions.</p><p> Eli sees AI alarmists as similar people. He believes that they spoil the image of the AI safety community, and also make it hard to discuss more real and near-term problems like the spread of misinformation, or concentration of power in the hands of AI labs.</p><p> A friend recommended Eli the episode of the x-risk-themed episode of the AI podcast hosted by Dario and Claude. Eli decided to give it a listen during his commute. Alarmist Claude sounded like he is terrified by the existential risks, and even though he seems like a smart person, Eli immediately classifies him as an activist, and doesn&#39;t take his arguements too seriously. &quot;Ah, another one of those anxious Luddites&quot; he thinks.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/yPfotACWkRdWbubfy/5-psychological-reasons-for-dismissing-x-risks-from-agi#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/yPfotACWkRdWbubfy/5-psychological-reasons-for-dismissing-x-risks-from-agi<guid ispermalink="false"> yPfotACWkRdWbubfy</guid><dc:creator><![CDATA[Igor Ivanov]]></dc:creator><pubDate> Thu, 26 Oct 2023 17:21:48 GMT</pubDate> </item><item><title><![CDATA[5. Risks from preventing legitimate value change]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p> VCP 带来的第二个风险涉及阻碍合法价值变化。接下来，我将考虑合法价值变革可能受到阻碍的机制，以及这是否以及何时可能构成道德伤害的积极根源。</p><h2>阻止合法的价值改变是真正的风险吗？</h2><p>乍一看，人们可能会怀疑阻碍合法价值变革是否会带来真正的风险。承认某些价值改变的情况是没有问题和合法的，并不一定意味着阻止或阻碍这种改变在道德上是或总是应该受到谴责的。换句话说，人们可能怀疑积极阻止某些类型的价值变化是否构成道德伤害。接下来，我将尝试证明确实如此，至少在某些情况下如此。</p><p>首先，考虑一个系统地或任意地阻止人们从事有可能引起合法价值变化的理想追求的社会。在大多数情况下，这样的社会会被认为体现了一种有问题的“社会威权主义”形式，因为它严重损害了个人自由。在这里，自由可以（至少）有两种方式来理解。其一，我们可以提及“共和自由”（参见Pettit，2001），即免于任意行使权力的自由。在这种情况下，治理结构必须受到限制（例如通过法治）权力的任意使用。或者或此外，我们可能还关心保护第二种类型的自由，即自决自由。例如，查尔斯·泰勒（Charles Taylor，1985）在包括卢梭和穆勒等思想家在内的政治思想的悠久传统的基础上，提出了一种自由概念，这种概念体现了“一个人有效决定自己的程度和一个人生活的形态”（第 213 页）。因此，只要追求合法价值变化的能力是自决权的体现，那么系统地破坏这种自由（通过阻碍合法价值）变化的过程在道德上是有问题的。</p><p>通过观察价值自决的自由在康德的绝对命令（Kant，1785/2001）、罗尔斯的无知之幕（Rawls，2001）或米尔的元归纳论证下是反思稳定的，可以进一步支持这一论点。 <span class="footnote-reference" role="doc-noteref" id="fnrefq371my13fs"><sup><a href="#fnq371my13fs">[1]</a></sup></span>为了自由（MIll，1859/2002）。至于第一个概念，绝对必要的是我们应该“以你希​​望他们对所有人采取行动的方式对待他人”。因此，如果我想保护我经历合法价值改变的能力（例如，以理想追求的形式），我也应该想保护<i>你</i>做同样的事情的能力。罗尔的无知之幕抓住了这样一种观念，即公正的社会是在我们不知道我们在该社会中采取什么具体立场或属性的前提下设计的。与之前类似，该论点认为，如果我要设计一个社会的结构而不知道我将在该社会中占据什么位置，我将希望创建一个社会，让每个人都有价值自决的自由，包括他们经历合法价值变化的能力受到保护。最后，密尔对自由的元归纳论证指出，基于我们之前多次错误地判断什么是道德上对/错或什么对我们有价值的观察，我们应该在一定程度上追求我们目前对道德的最佳理解。除了价值观和道德之外，我们还应该保留一个“自由领域”，以保护我们未来改变想法的能力。虽然穆勒本人特别提出了言论自由的论点，但后来的评论家主张一种解释，将同一论点扩展到更普遍的个人生活计划的自主性和可修改性的合理性（参见，例如，Fuchs，2001；Bilgrami，2015） 。因此，阻碍或阻碍合法价值变革的行为者、机构或流程缺乏反思性认可，并构成值得道德关注的真正风险。</p><h2>破坏合法价值变化的机制（“价值崩溃”）</h2><p>我现在将继续讨论人们追求自主价值变革的能力可能受到阻碍的机制。我将提供的叙述重要地受到 Nguyen 对“价值崩溃”（2020）的叙述的启发，然后我概括了其中的背景。简而言之，阮将价值崩溃描述为一种现象，即价值的过度表达（例如使用指标或其他量化和/或简化价值的方法）恶化了我们对价值观和世界的认知态度。正是这种认知态度的恶化最终导致了我们价值自决能力的削弱。价值崩溃到底是如何发生的？</p><p>首先，阮指出我们的价值观塑造了我们的注意力。例如，如果我重视自己的健康，我会区别地关注我认为对我健康的事物（例如运动、食物、补充剂）。如果我重视好奇心，我就更有可能寻找并注意到别人的这种特质。接下来，一旦我们对我们的价值观采取明确的操作化——例如，通过使用指标——这就会导致更明确的注意力边界。阐明了我们<i>关心</i>的内容后，也澄清了我们<i>不</i>关心的内容。例如，在教育中引入 GPA 分数将学生和教育工作者的注意力集中到这些分数很好地体现的教育方面。不幸的是，明确的值通常无法捕获我们关心的所有内容。因此，虽然使用它们有很多充分的理由（稍后会详细介绍），但它们的引入是以将注意力从上述指标无法很好地捕捉到的事情上转移开为代价的，即使有时这些可能是我们实际上所做的事情反思地关心。例如，就 GPA 分数而言，这些可能是智慧、求知欲或公民责任感等。</p><p>鉴于我们解释自己价值观的能力不完善，我们依赖于健康的“错误代谢”（该术语取自 Wimsatt (2007)）。错误代谢是这样一种想法：如果我们不能在第一次尝试中完美地捕捉到某些东西（这是像人类这样有界限且容易出错的生物在试图驾驭复杂世界时的常态），我们就依赖于注意和整合的方法错误或偏差，从而迭代改进我们最初的尝试。然而，这就是问题的核心，因为我们的注意力调节我们注意到、感知并因此可以了解的内容，施加更窄和更清晰的注意力界限削弱了我们进行所述错误代谢的能力。如果我确信唯一值得一读的文学形式是希腊神话，而我从不涉足这一类型，那么我可能永远无法享受魔幻现实主义、法国存在主义或 18 世纪《狂飙突进》的乐趣。或者，回到 GPA 分数的例子，因为智慧和求知欲并没有被它们很好地捕捉到，人们担心这些分数的引入可能会排挤或破坏我们识别和促进那些更微妙的价值观的共同能力。</p><p>总之，明确的价值观通过强化我们的注意力界限，往往会削弱我们注意、整合并最终实现那些有价值但尚未被明确的价值陈述所捕获的事物的能力。 <span class="footnote-reference" role="doc-noteref" id="fnref6l9p9lqvwp"><sup><a href="#fn6l9p9lqvwp">[2]</a></sup></span>结果，阐明的价值观往往会变得“粘性”，从而主导个人的实践推理，而那些没有被阐明很好地捕捉到的价值观往往会被忽视，甚至可能会丢失。因此，价值崩溃的动态削弱了一个人自我决定价值改变的能力。</p><p>值得注意的是，价值观的解释通常涉及简化它们，即剥离（可能重要的）细节、细微差别和上下文。 Theodore Porter (1996) 在他关于量化历史的著作中指出，量化知识侧重于一些不变的内核，这些内核被剥夺了各种上下文相关的细微差别。重要的是，量化不仅是一件坏事，而且是一件坏事。相反，量化允许信息在上下文之间传播并轻松聚合。例如，如果不以成绩的形式量化学生的表现，就很难比较和汇总学生在数学、文学、体育和历史等不同背景下的表现。然而，量化也存在成本。可移植性和可聚合性的改进是以失去细微差别、微妙性和上下文敏感性为代价的。因此，我并不是试图主张我们应该始终避免量化或解释我们关心的事情。相反，我认为重要的是要注意解释往往会导致我们对价值观的理解变得贫乏，以及随着时间的推移我们改变、发展或完善我们的价值观的能力。只有当我们意识到各种权衡时，我们才能就何时以及在多大程度上依赖价值解释机制做出明智的选择。</p><p>其他各种价值崩溃的案例已经很容易观察到。我们已经提到了学术背景，其中 GPA 或引用数量等指标可能会威胁到更丰富的智慧、求知欲或真理追求的概念。其他例子包括健康指标（例如步数、体重指数或燃烧的卡路里），这些指标可能无法捕捉更模糊、更个性化的健康、幸福或表现概念；或者，在社交媒体的背景下，喜欢的数量或观看时间，这可能会超越更厚重和更亲社会的概念，例如学习、联系或审美价值。等等。</p><h2> ...对于（高级）人工智能系统</h2><p>然而，我们在本文中最感兴趣的问题是，当外推到先进人工智能系统的背景下时，这种担忧会是什么样子？</p><p>一般来说，先进的人工智能系统会强化这种效果。他们主要通过两种方式做到这一点。首先，对于已经依赖于价值解释的流程，先进的人工智能系统将能够针对一组给定的明确价值进行更强有力的优化，从而进一步削弱所述流程的错误代谢。为了举例说明这种趋势，让我们考虑一下人工智能系统在处理工作申请的背景下的使用。这种应用程序处理人工智能将（除非赋予某种机制来抵消这种趋势）根据公司已经能够识别和捕获的任何价值（例如，以评估标准的形式）优化其绩效。如果应用程序档案中有一些公司感兴趣的特征，但不属于当前的评估方案，那么人工智能评估员将对这些特征不敏感。重要的是，人工智能评估员在这方面比人类评估员更有效——无论是识别符合评估标准的申请人，还是忽略不符合这些标准的申请人。此外，人类评估者可能会获得关于他们在申请过程中关心的内容的新见解（例如，关于如何识别合适候选人的新见解，或者关于公司最理想的招聘职位规格的新见解）。相比之下，依靠人工智能系统来完成这项工作（同样，默认情况下）往往会削弱这些可能性，从而减少偶然（相对于定义的评估方案）结果和见解的可能性。虽然人工智能系统最初可能只用于预处理工作申请，但它们将越来越多地确定越来越多的决策过程，因此，检测到当前规范中的错误的可能性将会降低。代表人类评估者或整个公司的开放式价值发展和完善过程已经成为一个封闭且趋同的过程，具体化了我们一开始的任何价值概念。与其否认针对一组给定（即使不完美）的假设进行优化从来没有务实的合理性，不如担心的是，如果不加以控制，随着时间的推移，这将导致我们改进现有能力的能力下降。假设或评价标准。</p><p>先进人工智能系统将增强我们今天已经看到的影响的第二种方式是，人工智能的进步将允许类似的过程在生活的更多领域得到更广泛的部署。例如，考虑一下乔治，他使用他的“人工智能助手”（或类似的人工智能应用程序）来考虑要培养什么新爱好或要加入什么新社区。如果所述系统针对乔治可能已经拥有的某些固定价值观来优化其建议，或者如果它进行优化以使他更容易预测，那么这会减少偶然性并干扰乔治识别和评价真正新颖的存在方式的能力。高度通用化的人工智能系统可能会在生活的几乎所有领域产生类似的影响。</p><p>如果上述关于价值崩溃的解释是正确的，那么扩大超解释实践的范围就会威胁到我们个人和社会所拥有的丰富而微妙的价值观。在所有这些情况下，它们的使用越广泛，其效果就越显着。这些系统变得越好，它们在优化所阐述的内容方面就越好，并且此类系统的使用/采用越广泛，所描述的效果就越普遍。价值崩溃威胁到真正开放式价值探索的可能性，而这种探索是理想追求和合法价值变革的核心。因此，作为一个社会，只要我们关心保护个人和集体合法价值改变的可能性，我们就需要仔细考虑先进人工智能的开发和部署将如何影响、干扰和潜在地严重破坏这一点。可能性。</p><p>在我们结束之前，有必要做最后的评论。虽然在整篇文章中我都强调了人工智能和价值可塑性背景下出现的<i>风险</i>，但我想在结论中承认，合法的、<i>技术辅助的</i>价值变化也存在着重要的可能性。换句话说，值得一问的是，我们是否可以设计出能够增强而不是削弱我们发展和完善价值观的能力的机器。考虑到类似的想法，Swierstra（2013）写道：“特定技术可以通过使某些选项变得更加紧迫或有吸引力，而另一些选项则不那么紧迫或有吸引力来调节道德选择和决策。”但这并不意味着道德的丧失，或者道德的堕落。它可以很容易地带来更多或更好的道德，例如通过扩大利益相关者的道德共同体。这样，技术与道德之间的关系就表现为两个既没有也不渴望绝对自主的伙伴之间的婚姻。能否谈谈这段婚姻的质量？虽然我对 Swierstra 在上述摘录中使用“简单”一词提出质疑，但我同意人工智能系统<i>增强</i>个人自决和道德推理的可能性是一个值得进一步探索的有趣想法。就本文应为更大的哲学和技术项目做出的贡献而言，该项目既关心避免风险，也关心实现进步和上行潜力。然而，为了成功做到这一点，我们首先需要进一步取得重大进展，一方面理解合法价值变化的机制，另一方面我们有能力构建以预期方式可靠运行的人工智能系统。此外，我们应该关注目前的情况：虽然可以想象，我们未来的人工智能助手将成为高度反思的道德代理人，帮助我们找出本能地认可的价值观，以及对我们价值可塑性的非法利用可以说，这是一个更突出、更迫在眉睫的威胁。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnq371my13fs"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq371my13fs">^</a></strong></sup></span><div class="footnote-content"><p>我很感谢 TJ 向我指出了这一具体论点以及文献中的相关讨论。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6l9p9lqvwp"> <span class="footnote-back-link"><sup><strong><a href="#fnref6l9p9lqvwp">^</a></strong></sup></span><div class="footnote-content"><p>两个澄清：我并不是说价值观的解释总是或总是会导致注意力的缩小。就我的目的而言，只要证明在实践中这就是经常发生的情况就足够了。此外，可以在以其他方式加强误差代谢的同时阐明数值。这可能是对此处描述的问题的完全令人满意的解决方案。目前的主张仅仅是，就目前的情况而言，如果没有经过仔细和深思熟虑的考虑，解释通常会伴随着错误新陈代谢的减弱。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/KeHGinpj2WyzDEQAx/5-risks-from-preventing-legitimate-value-change#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/KeHGinpj2WyzDEQAx/5-risks-from-preventing-legitimate-value-change<guid ispermalink="false"> KeHGinpj2WyzDEQAx</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:35 GMT</pubDate> </item><item><title><![CDATA[4. Risks from causing illegitimate value change]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p>未对齐的人工智能系统可能会导致非法的价值变化。这种风险的核心在于这样的观察：人类价值观固有的可塑性可能会被利用，从而导致价值变化变得不合法。回想一下，我认为非法性是由于一个人自我决定和纠正价值变革过程的能力缺乏或（重大）障碍而产生的。</p><h2>导致非法价值变化的机制</h2><p>今天已经可以观察到这种风险的实例，例如在推荐系统的情况下。在考虑它可以给我们带来哪些关于高级人工智能系统风险的教训之前，值得花一些时间来理解这个例子。为此，我将借鉴 Hardt 等人的工作。 （2022），引入了“表演力”的概念。执行力是对“公司运营算法系统（例如数字内容推荐平台）导致参与者群体发生变化的能力”的定量衡量标准（第 1 页）。企业的执行力越高，其“从引导大众转向为企业带来更有利可图的行为中获益”的能力就越高（第 1 页）。换句话说，执行力使我们能够衡量运行推荐系统的公司在客户群中引起外生价值变化<span class="footnote-reference" role="doc-noteref" id="fnref8dyuxt9u9t"><sup><a href="#fn8dyuxt9u9t">[1]</a></sup></span>的能力。该措施是专门为推进数字经济竞争研究而制定的，特别是为了识别反竞争动态。</p><p>这里发生了什么？为了更好地理解这一点，我们可以帮助自己区分 Predomo 等人介绍的“事前优化”和“事后优化”。 （2020）。前者——事前优化——是在低执行力条件下发生的预测优化类型，其中预测者（在本例中为公司）无法比标准统计学习允许从过去的数据中提取的信息做得更好。未来的数据。另一方面，事后优化涉及控制预测的行为，例如提高预测器的预测性能。换句话说，在第一种情况下，待预测数据是固定的并且独立于预测器的活动，而在第二种情况下，待预测数据受到预测过程的影响。正如哈特等人。 (2022) 评论：“[事后优化]对应于对反事实的隐式或显式优化”（第 7 页）。换句话说，具有高表演能力的演员不仅能预测最可能的结果，还能预测最可能的结果。<i>从功能上讲，</i>它的表现就好像它可以选择要实现的未来场景，然后进行预测（从而能够实现更高水平的预测准确性）。</p><p>根据我们之前对（不）合法价值变化本质的讨论，表现力驱动群体价值变化的情况构成了非法变化的一个例子。经历所述变化的人们并没有以任何有意义的方式积极参与表演预测因素影响所述人群的变化，并且他们“纠正方向”的能力通过（除其他外）选择设计（即影响消费者接触到的推荐顺序）或通过利用某些心理特征，使某些类型的内容在当地比其他内容更引人注目，而不管所述内容与个人价值观或促进原因的关系如何。</p><p>更重要的是，人口所经历的变化使得价值观变得更加可预测。为了解释这一点，首先要注意的是，执行预测者（即运行推荐平台的公司）嵌入在经济逻辑中，该逻辑强制要求最小化成本和增加利润。结果，公司的控制力将特别倾向于使预测的行为<i>更容易</i>预测，因为公司能够利用这种可预测性来获取利润（例如，通过增加广告收入）。迄今为止，这一过程已得到充分记录。例如，就推荐平台而言，研究并未发现观看行为的异质性增加，而是发现这些平台存在所谓的“受欢迎程度偏差”，从而导致内容多样性的丧失和同质化推荐（参见 Chechkin 等人 (2007)、DiFranzo 等人 (2017) 和 Hazrati 等人 (2022)）。因此，预测优化者施加压力，使行为更加可预测，而实际上，这通常意味着（个人和集体）价值观的简化、同质化和/或两极分化的压力。</p><h2> ...对于（高级）人工智能系统</h2><p>虽然当今的推荐平台可能已经拥有相当程度的执行能力，但不难想象更先进的人工智能系统将能够更强大地利用人类心理和社会经济动态。从先验的角度来看，没有太多理由认为人类进化的心理在对抗人工超级智能“劝说者”时会特别强大。除了由高度先进的人工智能系统提供支持的推荐系统之外，我们还可以想象个性化“人工智能助手”的使用将越来越广泛。我们可以将人工智能助手的任务想象为帮助他们所协助的人满足他们的需求、实现他们的目标或满足他们的偏好。鉴于在广泛的背景下全面、明确地指定一个人想要什么的难度，这种“帮助”通常会涉及对人工智能系统部分的猜测（即预测）。因此，考虑到上面讨论的动态，如果没有成功地设计来避免 VCP，这样的“人工智能助手”很可能通过引起个人目标和价值观的实质性和累积变化来“提高他们的表现”。更重要的是，就像上述推荐算法的情况一样，这种“人工智能助手”引起的变化的本质将倾向于（除非采取相关的纠正措施）所预测的数据结构的简化——在这种情况下人类价值观的情况。为了说明这一点：“人工智能助手”也许能够通过有效地缩小我的烹饪偏好范围来提高其绩效指标，让我总是要求汉堡和薯条，而不是偶尔对探索新奇的口味和菜肴感兴趣。上面描绘的情况令人担忧，因为，一方面，它削弱了人自我决定价值观的能力；另一方面，随之而来的变化可能会导致曾经更丰富或更微妙的价值观实际上变得贫乏。所描述的效果不需要人工智能系统方面有任何“恶意”，但可能会作为其运作方式的“仅仅”意外后果而出现。</p><p>重要的是要认识到，所描述的机制有可能达到“远”和“深”——换句话说，它有可能对我们的公共和私人生活产生重大影响；人们的经济、社会、政治和个人信仰、价值观、行为和关系。例如，想想广告的普遍存在（如今，广告甚至通过智能手机和电视深入到私人领域），以及每天有多少经济行为受到广告的影响。或者，想想同样的机制如何影响舆论形成、公共审议，从而影响政治结果。因此，人工智能驱动的广告或政治宣传，以及我们目前可能无法想象的其他应用程序，具有巨大的潜在危害。</p><p>让我们回顾一下我们在此确定的非法价值变化风险的潜在机制。一般来说，我们关注的是预测优化器（或功能相当于预测优化器的过程）能够系统地影响其预测的情况。如果所预测的现象涉及某些人的需求，那么执行优化器就会影响这些人的价值观。如果人们假设人类价值观是固定不变的，那么人们可能会得出这样的结论：这里没有什么可担心的。然而，认识到人类价值观的可塑性使得这种风险变得突出并且可能非常紧迫。先进的人工智能系统在这种形式的执行预测方面将变得越来越有能力，从而加剧我们今天已经可以得出的任何模式。这些人工智能系统在相关社会经济环境（例如广告、信息系统、我们的政治生活、私人生活等）中的部署越广泛，潜在的危害就越严重和深远。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn8dyuxt9u9t"> <span class="footnote-back-link"><sup><strong><a href="#fnref8dyuxt9u9t">^</a></strong></sup></span><div class="footnote-content"><p>观察到的人口变化可能并不完全是由于价值观的变化。然而，它可能（并且通常会）涉及大量的价值变化，因此，执行力是理解外生引起的价值变化现象的相关衡量标准。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/qZFGPJi3u8xuvnWHQ/4-risks-from-causing-illegitimate-value-change#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/qZFGPJi3u8xuvnWHQ/4-risks-from-causing-illegitimate-value-change<guid ispermalink="false"> qZFGPJi3u8xuvnWHQ</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:26 GMT</pubDate> </item><item><title><![CDATA[3. Premise three & Conclusion: AI systems can affect value change trajectories  & the Value Change Problem]]></title><description><![CDATA[Published on October 26, 2023 2:38 PM GMT<br/><br/><p>在这篇文章中，我介绍了三个前提中的最后一个——<i>人工智能系统能够（并且将越来越）能够影响人们的价值变化轨迹</i>。三个前提都到位后，我们就可以继续完整地阐明价值变化问题（VCP）。我将简要回顾一下整个内容，然后对第 4 篇和第 5 篇文章中即将发生的内容进行展望，我们将在其中讨论未能认真对待 VCP 所带来的风险。</p><h1>前提三：人工智能系统可以影响价值变化轨迹</h1><p>将价值变革问题的论点放在一起所需的第三个也是最后一个前提如下：人工智能系统（并且将越来越）有能力影响人们的价值变革轨迹。</p><p>我认为这个问题相对简单。在上一篇文章中，我们已经看到了几个外部因素（例如其他个人、社会和经济结构、技术）如何影响个人价值变化轨迹的例子，并且它们可以以合法或不合法的方式这样做。 。人工智能系统也是如此。</p><p>价值观的改变通常是道德反思/深思熟虑或学习新信息/创造新经验的结果。外部因素可以影响这些过程——例如，通过影响我们接触到的信息、通过使我们的反思过程偏向某些结论而不是其他结论等——从而影响个人的价值变化轨迹。人工智能系统是另一个能够产生类似效果的外部因素。例如，考虑在媒体、广告或教育中使用人工智能系统作为个人助理，以帮助学习或决策等。从这里开始，随着人工智能的能力和部署不断增强，认识到这一点并不是一个大的步骤。这些系统，人工智能系统可能会对我们的价值变化轨迹产生总体影响。</p><p>第 4 篇和第 5 篇文章将更详细地讨论所有这些问题，包括提出人工智能影响价值变化轨迹的具体机制，以及它们何时合法和不合法的问题。</p><p>因此，我将停止讨论第三个前提和这一点，并迅速继续整理价值变化问题的完整案例：</p><h1>把事情放在一起：价值变化问题</h1><p>让我们回顾一下到目前为止的论点。首先，我认为人类价值观是可塑的而不是固定的。为了捍卫这一主张，我认为人类通常会在一生中经历价值变化；人类价值观有时是不确定的、不确定的或开放式的，而人类通常处理这个问题的某些方式涉及价值观的改变；最后，变革性经历（如 Paul (2014) 所讨论的）和愿望（如 Callard (2018) 所讨论的）也代表了价值变革的例子。</p><p>接下来，我认为某些价值改变的情况可能是（不）合法的。为了支持这一主张，我通过提供价值变化案例的例子来诉诸直觉，我认为大多数人会很容易地分别接受这些案例是合法的还是非法的。然后，我提出了一个看似合理的评价标准，即价值变化过程中的自我决定程度，从而强化了这一论点，这为我们之前的直觉提供了进一步的支持和理性基础。</p><p>最后，我认为人工智能系统能够（并且将越来越）影响人们的价值变化轨迹。 （同时将一些进一步的细节留给帖子 4 和 5。）</p><p>将这些放在一起，我们可以认为必须认真对待人工智能系统的道德设计，并找到解决（非法）合法价值变化问题的方法。换句话说，我们应该避免构建不尊重或利用人类价值观可塑性的人工智能系统，例如导致非法价值变化或阻止合法价值变化。我将其称为“价值变化问题”。</p><p><i>对于人工智能设计来说，认真对待（不）合法的价值变化问题意味着什么？</i>具体来说，这意味着合乎道德的人工智能设计必须尝试：i）了解人工智能系统如何造成或可能导致价值变化，ii）了解价值变化的情况何时是合法或非法的，以及iii）构建不会导致价值变化的系统<i>il</i>合法值变化，并允许（或启用）合法值变化。</p><p> In the remaining two posts, I will discuss in some more depth the risks that may result from inadequately addressing the VCP. This gives raise to two types of risks: risks from causing illegitimate value change, and risks from preventing legitimate value change. For each of these I want to ask: What is the risk? What are plausible mechanisms by which these risks manifest? What are ways in which these risks manifest already today, and what are the ways in which they are likely to be exacerbated going forward, as AI systems become more advanced and more widely deployed?</p><p> In the first case— <i>risks from causing illegitimate value change—</i> , leading with the example of recommender systems today, I will argue that performative predictors can come to affect that which they set out to predict—among others, human values. In the second case— <i>risks from preventing legitimate value change—</i> , I will argue that value collapse—the idea that hyper-explication of values tends to weaken our epistemic attitudes towards the world and our values—can threaten the possibility of self-determined and open-ended value exploration and, consequently, the possibility of legitimate value change. In both cases, we should expect (unless appropriate countermeasures are taken) the same dynamic to be exacerbated—both in strength and scope—with the development of more advanced AI systems, and their increasingly pervasive deployment.</p><h2> Brief excursion: Directionality of Fit</h2><p> A different way to articulate the legitimacy question I have described here is in terms of the notion of &#39; <strong>Directionality of Fit</strong> &#39;. In short, the idea is that instead of asking whether a given case of value change is (il)legitimate, we can ask which &#39;direction of fit&#39; ought to apply.让我解释。</p><p> Historically, &#39;directionality of fit&#39; (or &#39;direction of fit&#39;) was used to refer to the distinction between values and beliefs. (The idea came up (although without mentioning the specific term) in Anscombe&#39;s <i>Intention</i> (2000) and was later discussed by Searl (1985) and Humberstone (1992).) According to this view, beliefs are precisely those things which change to fit the world, while values are those things which the world should be fitted to.</p><p> However, once one accepts the premise that values are malleable, the &#39;correct&#39; (or desirable) direction of fit ceases to be clearly defined. It raises the question of when exactly values should be used as a template for fitting the world to them, and when it is acceptable or desirable for the world to change the values. If I never accept the world to change my values, I forgo any possibility for value replacement, development or refinement. However, as I&#39;ve argued in part before and will discuss in some more detail in post 5, I might reason to consider myself morally harmed if I lose that ability to freely undergo legitimate value change.</p><p> Finally, this lens also makes more salient the intricate connection between values and beliefs: the epistemic dimensions of value development, as well as the ways values affect our epistemic attitudes and pursuits.</p><br/><br/> <a href="https://www.lesswrong.com/posts/yPnAzeRAqdko3RNtR/3-premise-three-and-conclusion-ai-systems-can-affect-value#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/yPnAzeRAqdko3RNtR/3-premise-three-and-conclusion-ai-systems-can-affect-value<guid ispermalink="false"> yPnAzeRAqdko3RNtR</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Thu, 26 Oct 2023 14:38:14 GMT</pubDate></item></channel></rss>