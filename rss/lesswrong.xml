<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 17 日，星期二 20:13:10 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Eleuther releases Llemma: An Open Language Model For Mathematics]]></title><description><![CDATA[Published on October 17, 2023 8:03 PM GMT<br/><br/><blockquote><p>今天我们发布了<i>Llemma</i> ：70 亿和 340 亿参数的数学语言模型。 Llemma 模型使用 Code Llama 权重进行初始化，然后在 Proof-Pile II（一个包含 550 亿个数学和科学文档的令牌数据集）上进行训练。由此产生的模型显示出改进的数学能力，并且可以通过提示或额外的微调来适应各种任务。</p><p>我们的工作与<a href="https://blog.research.google/2022/06/minerva-solving-quantitative-reasoning.html">Minerva</a>类似，Minerva 是谷歌研究院去年开发的专门用于定量推理的模型套件。虽然我们没有达到与 Minerva 完全相同的规模，但我们的 Llemma 模型在等参数基础上表现更好。此外，我们使我们的<a href="https://huggingface.co/EleutherAI">模型</a>和<a href="https://huggingface.co/EleutherAI">数据集</a>开放访问，并使我们的<a href="https://github.com/EleutherAI/math-lm">代码</a>开源。</p><p>具有强大数学推理能力的语言模型是奖励建模、算法推理、形式数学等众多新兴研究领域的上游。我们希望通过为研究人员提供更强大的推理应用基础模型，Llemma 将加速解决这些问题的进展。</p><p> Proof-Pile-2 的代码子集为 Llemma 赋予了 Minerva 所缺乏的功能，而无需进行额外的微调。在这篇博文中，我们将讨论<i>形式定理证明</i>。我们的论文包含有关 Python 辅助问题解决任务的其他结果。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HvgfNjihcDjCtEctE/eleuther-releases-llemma-an-open-language-model-for<guid ispermalink="false"> HvgfNjihcDjCTE</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Tue, 17 Oct 2023 20:03:46 GMT</pubDate> </item><item><title><![CDATA[Investigating the learning coefficient of modular addition: hackathon project]]></title><description><![CDATA[Published on October 17, 2023 7:51 PM GMT<br/><br/><p>作为我们在<a href="https://devinterp.com/events/2023-q3-melbourne-hackathon">墨尔本黑客马拉松</a>上关于<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>和对齐（10 月 7-8 日）的项目，我们做了一些实验来估计盆地单层<a href="https://www.neelnanda.io/mechanistic-interpretability/modular-addition-walkthrough">模加任务</a>的<i>学习系数</i>，这是衡量信息复杂性的不变量（阅读：完全训练的神经网络的程序长度）。</p><p>我们以<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau、Murfet 和 Wei</a>最近的论文为出发点；本文估计提供了学习系数的随机估计（它们表示<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>）通过朗之万动力学。 <a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">Watanabe 在一篇精彩的论文</a>中证明，由<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量的热力学量渐近收敛于理想化奇异系统的学习系数。</p><p>所描述的实验的所有代码都可以在<a href="https://github.com/nrimsky/devinterp"><i>此 GitHub 存储库</i></a><i>中找到</i><i>。</i></p><h1>简要结果</h1><p>在我们的测试中，我们惊喜地发现，对于模加模素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的任务，我们实施 Lau 等人的 SGLD 方法的输出是（达到大致恒定的小乘法误差，小于因子 2）与理想化单层模加法网络<span class="footnote-reference" role="doc-noteref" id="fnrefx9z3e6c3n7"><sup><a href="#fnx9z3e6c3n7">[1]</a></sup></span>的理论预测结果完全一致。</p><p> <a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人迄今为止已经在双神经元网络中获得了类似的结果。</a> <a href="https://arxiv.org/abs/2310.06301">Chen 等人的 12 神经元网络。</a>我们的结果首次证实了中型网络（约 500 至 8,000 个神经元）的估计值与理论结果之间的一致性。</p><p>虽然我们的结果与单个模加电路的理论值相差很小的乘法因子，但我们发现了一个非常精确的现象，与理论预测完全匹配，即对于泛化的模加网络，学习系数估计在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中是线性的;这是同类中第一个精确的缩放结果。 </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>对于 5 个不同的 p 值，模加网络上<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的平均测量：线性程度非常显着。</figcaption></figure><p>此外，使用模加法任务作为测试用例可以让我们仔细研究<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>复杂度估计在神经网络中区分泛化和记忆的能力：这似乎是新的东西（尽管与一些相变现象有关）陈等人）。我们观察到，虽然泛化在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有线性学习系数，但记忆在素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有（大致）二次增长；这再次显示出与理论的显着一致性。 </p><figure class="image image_resized" style="width:70.45%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7"><figcaption>我们针对仅记忆网络的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>测量。这些与理论预测值非常一致（此处，理论预测<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda = 0.8 p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></figcaption></figure><p>与理论的一致适用于质数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的多个不同值和多个架构。它们对于学习不同数量电路的网络也有适当的行为；其他有效维度估计量（例如 Hessian 特征值估计）往往会高估复杂性的情况。</p><p>此外，我们表明<i>动态</i><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计<span class="footnote-reference" role="doc-noteref" id="fnrefy802tk8mheb"><sup><a href="#fny802tk8mheb">[2]</a></sup></span> ，即训练期间的估计，似乎跟踪学习的记忆与泛化阶段（尽管事实是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计仅取决于训练数据）。为了看到这一点，我们对动态估计器进行了轻微的改进，我们将采样限制在初始化时梯度向量的法线超平面内，这似乎使这种行为更加鲁棒。 </p><figure class="image image_resized" style="width:73.54%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qgdxysf9q1nbayda4hru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vy7xdw29olifj42mtoqi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lh3w1j5xwlc1brqjij3d 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gma5o3xugaqirlq1f1rv 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jieohfzfkyzvh6nw9ppf 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fgves9m6no1m5sjv0ja4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r2vrqdzyqkug7mibbwgo 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uzr5rwe0k0y40bls9ft0 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hzcphrkcu8y27yie4wzw 640w"><figcaption>在模加法 mod 53 上训练的 MLP 的估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>过训练图表。每 60 个批量大小为 64 的批次进行检查点。SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。搜索仅限于与初始化点处的梯度正交的方向，以校正非最小值处的测量。</figcaption></figure><p>我们的动态结果与<a href="https://arxiv.org/abs/2310.06301">Chen 等人的一些 SGLD 研究结果相似。</a> ，这表明动态 SGLD 计算有时可以注意到相变。我们惊喜地看到它们在更大的网络中以及在记忆与泛化的背景下保持不变。</p><p>总的来说，我们的研究结果使我们更加相信奇异学习理论技术和思想在现实世界中的适用性。更具体地说，我们现在相信，类似于 Lau 等人的 SGLD 采样的技术应该能够区分工业规模神经网络中的不同泛化行为，并且可以成为一个有点强大的无监督解释性工具箱的一部分，以及对以下方面有价值的控制技术：结盟。</p><h1>背景</h1><h2>关于学习系数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>的基础知识</h2><p>有关替代介绍，请参阅<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">Jesse 和 Stan 解释学习系数的优秀帖子</a>（在我们写完本节后发布，但遵循类似的方法）。</p><p>学习系数是与泛化相关的参数。它控制“在随机选择权重的情况下，它们产生的损失在最优值的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>范围内的可能性有多大”问题的一阶渐近行为。换句话说，将最优解推广到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>精度范围内有多容易。当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>变为零时，该概率多项式变为零，作为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>的指数，因此</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{Prob}_{\text{Loss}<\delta_L} = \delta_L^{d/2}+\text{lower order corrections.}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">概率</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.242em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">损失</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.284em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-stack" style="vertical-align: -0.327em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.084em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mtext MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">低阶修正。</span></span></span></span></span></span></span></p><p> （出于技术原因，使用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>而不是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span> 。）</p><p>这样一个术语（通常用自由能来定义：这里<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>是<i>温度</i>）更普遍地出现在统计物理学中（并且在量子场论中有近亲）作为“微扰膨胀”中的主要指数。在神经网络中，指数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span></span></span></span></span>被称为<i>学习系数</i>或 RLCT（“实对数规范阈值”，代数几何中的术语）。</p><p>学习系数包含有关学习问题的“类维度”信息，可以理解为<i>有效维度</i>或“真实维度”的度量，即神经网络需要“正确猜测”的权重参数的真实数量。 net以最小的损失解决问题。特别是，如果通过包含不影响可学习算法集的冗余参数来扩展神经网络（例如，由于问题的对称性），则可以证明学习系数不会改变。请注意，如果机器学习问题的解决方案集足够单一（我们在本文中不会遇到这种情况），则学习系数可以大于最小值集<span class="footnote-reference" role="doc-noteref" id="fnref1xv13wjmuli"><sup><a href="#fn1xv13wjmuli">[3]</a></sup></span>的实际维度，并且确实可以是非整数。</p><h2> Watanabe-Lau-Murfet-Wei 估计， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{\lambda}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span></h2><p>事实上，定义为真正渐近线的学习系数仅包含<strong>奇异</strong>网络、现实生活中从未出现的理想化系统的重要信息（就像噪声算法的两次迭代不可能给出完全相同的答案一样，所以它对于具有任何随机性的网络来说，都不可能具有奇异最小值或正维最小值集合）。然而，在有限但较小的温度值（即，损失“灵敏度”，由上述<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>测量）时，计算相关自由能的问题（并因此在“有限粒度级别”获得有意义的泛化相关参数） ”）是容易处理的。</p><p> Lau 等人<a href="https://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">渡边的论文</a>。下面给出了这种类型的公式。该论文的结果不仅取决于损耗灵敏度参数（称为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span></span></span></span></span> ，来自统计物理文献中的逆温度），还取决于样本数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。该公式给出了神经网络在“真实”数据分布上的学习系数的渐近精确估计，对应于样本数 n 趋于无穷大时的极限。当 n 趋于无穷大时，Watanabe 将温度参数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\delta_L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.007em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.007em;">δ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span></span></span>归零为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\log(n)/n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> 。 Lau 等人的论文着手在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>的有限值下执行此测量。</p><p>拥有一个好的学习系数估计器对于可解释性来说非常有价值：这将是一个以非常有原则的方式捕获算法的信息论复杂性的参数，避免了先前已知方法的严重缺陷（例如 Hessian 简并性的估计） ）并且对于分布外检测很有用。更一般地说，<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">奇异学习理论</a>计划提出了某些强大的无监督可解释性工具，可以提供有关网络内部结构的信息，假设可以有效地计算学习系数（和某些相关量）。</p><h2>模加法作为估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ 的</span></span></span></span></span></span></span>测试平台</h2><p>在 Lau 等人的论文中，他们将基于 SGLD 的学习系数估计应用于微型双神经元网络和 MNIST 网络，并取得了可喜的结果。我们将模加法网络视为一个有趣的中间案例。模加法必须证明以下事实：</p><ul><li>它是一个机械解释的网络：我们知道它的电路，或多或少知道它们是如何由神经元实现的，以及如何隔离和测量它们。</li><li>我们可以清楚地区分学习泛化的网络和仅通过查看其电路来记忆的网络；此外，我们可以通过创建一个学习随机交换运算的网络来“欺骗”泛化；这是一个与模加法具有相同记忆行为的网络，但没有泛化的可能性。</li><li>此外，我们可以计算网络学习的泛化电路的数量，并推理不同电路在损失函数和某种程度上理想化的自由能计算中如何相互作用。这使我们能够将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>相对于电路数量的行为与其他复杂性概念（例如 Hessian 秩）进行比较。 </li></ul><figure class="image image_resized" style="width:56.81%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bxrvulcijbvpfbwqq8nc" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/yfj2uoxea9fget94o1bp 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/pmqnsdp83vlwisdqrts5 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5lxabqho3gjqhl0t19p 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtqbn1vaxa4usvxpvjzt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/orlxoh7x5gw0untqhvb4 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xhqap8twezvsfobaw6ol 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e4h9vlg6b3z4ovs90ldw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gihdgdfa0bt0mekv7jjl 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwgypqjwa8dxmqfinby7 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xatuu74shphxc5fouhqw 800w"><figcaption>受模加法训练的网络的保留测试集上的损失图。每个步骤（不同颜色的线）对应于训练过程中均匀分布的检查点。当嵌入权重矩阵中除此之外的所有傅里叶模式都被消融时，每个点对应于损失。仅保留单个重要模式对损失的影响比仅保留不重要模式要小得多，这证明了在嵌入矩阵中使用“grokked”傅里叶模式。</figcaption></figure><p>同时，作为一个算法生成的问题，从 SLT 的角度来看，模加法有一些重要的局限性，这使得它无法捕获典型学习问题的一些复杂性：</p><ul><li>模加法的可能数据点总数是有限的（即，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>素数模而言，等于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ），并且目标分布是确定性的。因此，学习系数仅取决于有限数量的样本，这使得从统计学习理论的角度来看，渐近问题略有（但不是完全）退化。</li><li>即使在简单的确定性机器学习问题中，模加法问题也是高度对称的；因此，我们的经验结果可能无法推广到不太对称的网络。</li><li>与最大样本数相比，可能的输出标记数量较多<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">（p 个</span></span></span></span></span></span></span>标记与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2 个</span></span></span></span></span></span></span></span></span>样本相比，对于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>为模）可能会导致异常行为（Watanabe 的结果假设 logits 数量很小，并且样本数量渐近无限） ）。</li></ul><p>尽管存在这些限制，我们观察到（对于超参数的适当选择）Watanabe-Lau-Murfet-Wei 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>给出的学习系数估计在很大程度上与理论预测一致。此外，这些估计的表现非常一致和稳定，这是我们没有预料到的。</p><h1>发现<strong>&nbsp;</strong></h1><p>我们发现，对于完全训练的网络，使用渡边公式的 SGLD 估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是对于模加法（在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中呈线性，相当独立于总数量）参数）和随机网络（ <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中的二次）。此外，它独立于原子电路或“groks”的数量（我们期望在适当的限制情况下，学习系数的情况如此，但有效维度的其他计算则不然）。 </p><figure class="image image_resized" style="width:54.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gkqtx00nee58ax9d8h6r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/v39xlwaw87bgo6362h90 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vzf0xyjpj3qeacyq7w9k 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ipm9sbglftb2pbfcxepy 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zqriirvq41cc476zoq7p 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dbul9hx7ts7zgvennlbb 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sqekzx3knephdqxl9gqu 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k1xur59h3ai0ewydhure 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qw2a27cqsxs4u1dv7sby 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iptxfrv0eclatokb110l 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/odmceomuhkcwtp4bshet 1071w"><figcaption>我们在模加法任务上训练的模型图。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维一热编码数嵌入在 embed_dim 空间中。学习两个独立的线性变换到一个hidden_​​dim空间。然后，这两个向量按元素相加，通过 GELU 激活函数，然后转换回<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>维 logits 向量。</figcaption></figure><p>我们还在模加法网络的学习轨迹上的不稳定点上对<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>进行了一些“动态”估计。在这里，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。</p><h2>泛化网络的扩展行为</h2><p>我们在以下网络上运行了 Watanabe-Lau-Murfet-Wei <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计算法，并得到了以下结果。我们绘制了针对每个素数的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计值，并在五次实验中取平均值。</p><p>我们发现使用渡边公式的估计给出了 RLCT 理论估计的良好近似（最多一个小因子），无论是模加法还是随机网络： </p><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xuadozdjfow9omlhnshl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uqbiscu0kkts6k1ccitq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/srufg96up7sagnyxsstt 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dum6nggcpskpvooutvix 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ydfazld4xokilgv6us5w 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a1vuw3tffiq6wmnv3gye 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ohwkdfonfa7zj9ggmm2f 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/af3tqehnvoyp2rnwrjwm 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rvmr3cblpfwaux2sxden 640w"><figcaption>使用 SGLD 估计<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>的图，用于在模加模不同素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>上训练的 MLP。此处，所示的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>是五次独立训练和采样运行的平均值。 SGLD 的超参数为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> 、 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> 。我们可以看到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>成线性比例。 </figcaption></figure><figure class="image image_resized" style="width:60.23%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/t9eayflywv3vapupbtmj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ua5hlgflxv0bl3w77thr 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lsguicplrrqohlxsncbl 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/k0lxw3ctzrcs9v3vsljl 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uncajiodsmfpwnyshmhy 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vn1u9t45nqetihj6q5li 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kwvgjbjas8jm6qsvezu7 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rkpjklemrinodgskueqp 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a3xzlm5in1woa0x9qwkj 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>对不同大小的 MLP 网络的单次运行进行估计，证明了 RLCT 跨尺度的相似性。 </figcaption></figure><figure class="image image_resized" style="width:59.29%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m3raqy0b7opujni5bnyj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fufnptgi5q1u1dddxyqs 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkmc2tjnisaxx9iv4o5m 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gdspfdd2xo9rtb1gfmf6 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jiatr65txqrzfzq77ere 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/r6ajczqfbexgag3wiopq 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bi1vdtqsotjoyu0zqbay 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/iwjc3g0ljryippa4iosw 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/n5ntt4zn8zbfa8qqfrao 640w"><figcaption>这里，不同的运行对应于单独训练的网络系列，证明<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>在具有相同架构的模型之间是一致的，经过训练以在相同数据集和任务上进行收敛。</figcaption></figure><p>我们观察到，在给定的架构中，我们的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计非常接近线性，正如理论上预测的那样。</p><p>原则上，采用这种架构解决模加法的模型的最小有效维数是<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> （这将在推导模加网络结果的单独理论文章中详细阐述）。然而，我们观察到经验比例因子非常接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8p = 2\times 4p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">×</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span> ，是单个电路结果的两倍。对于这个结果的一个可能的解释是，在我们的模型所处的体系中，解决方案的有效空间由执行至少两个简单电路的权重参数组成（我们训练的所有模型都至少学习了 4 个简单电路）。 </p><figure class="image image_resized" style="width:47.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rpnrzizva3nnkmapqsmi" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/mkefqplmw6n6rvzaf6zq 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/abfwqanxihyprp3gismw 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xoi6gyakkr5xmjzwkr3u 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/chmzavciqpkpbw5tqxb0 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jw9pwvardy4hhr4sv7vk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dd1csgtod5ww9efhgvjq 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bzehulggnguktxcyg45d 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wpgopmxspzuqu26bmv9j 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的比例因子接近<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="8"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span></span></span>这可能表明 SGLD 搜索过程探索了对应于两个 grokked 电路而不是一个电路的一系列近极小值。请注意，所有经过训练的模型总共训练了 >;2 个电路，并且电路数量各不相同，因此我们仍然发现所学习的独立电路总数具有不变性。</figcaption></figure><p>当开始实验时，我们预计经验值和预测值之间存在超过一个数量级的广泛差异（因为现实模型的非理想性质和我们实验中的限制点）。相对较大且混乱的“现实世界”测量与理想测量之间的这种程度的一致性，以及这里的近线性，绝不是可以保证的，并且使我们相信奇异学习的理论预测有很大程度的改变。理论与现实世界的测量结果非常吻合。</p><p>我们还在各种架构上重复实验，参数数量相差一个相对较大的因子（我们最大的网络比最小网络大3倍多，我们的中间网络渐近是最小网络的两倍）。较大的网络确实具有稍高的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> ，但差异在网络大小上呈次线性缩放，正如我们对真实学习系数的预期一样。</p><p>请注意，我们包含的素数相对较小。虽然我们的架构非常高效<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">，</span></span></span> <span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">并且</span></span></span></span></span></span></span></span></span></span></span></span>对于更大的素数总是能够泛化（接近 100% 的准确度），但我们凭经验观察到，当完全训练的网络非常接近收敛（0损失）。由于计算限制，我们使用相对较大的学习率 (0.01) 进行相对较少的迭代次数。这会导致 50 以上的素数在收敛时损失更严重；我们推测，如果我们使用计算量更大、学习率更小的方法和更多的 SGD 步骤，那么对于更大的素数，近线性行为将继续成立。</p><h2> (in)依赖于电路数量</h2><p>我们训练的网络有时会学习嵌入不同子空间中的不同数量的独立泛化电路（此类电路的存在首先由<a href="https://arxiv.org/abs/2301.05217">Nanda 等人</a>提出）。</p><p>我们可以通过考虑嵌入空间中的大离群傅里叶模式来测量网络学习的电路的数量和类型，或者（更稳健地）通过在“傅里叶模式对齐”二维投影中寻找近乎完美的圆。嵌入空间<span class="footnote-reference" role="doc-noteref" id="fnrefyim8grymza"><sup><a href="#fnyim8grymza">[4]</a></sup></span> ，如下图</p><p><i>（我们计划稍后发表另一篇文章（关于模块化加法的机械解释工具，特别是准确地区分</i><a href="https://arxiv.org/abs/2306.17844"><i>“披萨”和“时钟”电路</i></a><i>），其中将对这些图片进行更多解释。）</i> </p><figure class="image image_resized" style="width:51.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uria5hpsbh4aoqokve6p" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/lembcon39vupovktlmno 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rtl1wulgwzyefwcknlux 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ndgtvirh22no4fpprlit 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/diptixpqr04aiyoqhffi 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xi7gkcz3grwfnlxhotng 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/y5la9dj9km731vtg1rds 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/rccsyop3ofkqopkkmygm 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vrw9fyefggwqfdq27rtb 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ixxxg8bckqccyaxmxjvg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bn9plxzaug1db0k5wtvt 1000w"><figcaption>通过将学习到的嵌入权重矩阵投影到嵌入空间的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(p-1)/2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span>二维子空间（对应于嵌入空间中可表示的不同离散傅立叶模式），我们可以看到独立傅立叶模式电路的数量。例如， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p=43"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">43</span></span></span></span></span></span></span>的模型已学习 6 个傅里叶模式 - <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="4,8,14,15,19,21"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">14</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">15</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">19</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">21</span></span></span></span></span></span></span> 。</figcaption></figure><p>我们在实验中观察到，学习率估计似乎不太依赖于学习的电路数量。例如，对于我们考虑的最大素数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p = 53"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">53</span></span></span></span></span></span></span> ，在不同运行中学习的电路数量在 4 到 7 个圆形电路之间变化，而所有网络的学习系数彼此之间大约在 10% 以内。这个结果看似简单，但从理论角度来看却非常有趣且有些令人惊讶。</p><p>例如，当通过 Hessian 特征值测量网络的有效维度时，具有多个电路的网络将具有有效维度 0（因为沿着与任何电路对应的方向都算作泛化）或线性依赖于电路的数量（因为只有当一个方向独立地概括每个电路时才算作概括）。事实上，这些行为在我们的环境中都没有被观察到，这一事实可以由单一学习理论框架来推动。事实上，我们可以将执行每个电路（或者可能是电路的合适的小子集）的权重空间中的子空间视为“近最小值”的奇异流形的单独组件。由于与不同电路相关联的向量空间相对于彼此处于一般位置，因此产生的奇异性是“最小奇异性” <span class="footnote-reference" role="doc-noteref" id="fnref0vw69e6p2rtp"><sup><a href="#fn0vw69e6p2rtp">[5]</a></sup></span> 。这意味着奇点处的 RLCT 等于沿每个单独分量的 RLCT，这可以理解为对观察到的独立结果的解释。然而，我们注意到，尽管其解释性稳健，但当我们放大时，这张图会变得更加复杂，因为多电路网络的损失往往比其各个部分的乘积要好得多。</p><p>我们计划对独立性结果给出另一种解释，涉及交叉熵损失的统计模型，该模型利用模素数乘法的遍历性。我们在此指出，我们期望这种独立性仅在超参数选择的“金发女郎”范围内保持，特别是正则化常数（对应于所学习的电路的大小）。一个简单的统计模型在这里预测至少三个不同的阶段：一个是电路尺寸非常小的（对应于大正则化），我们预计电路的数量会成倍地影响学习率。一种是在大电路尺寸（小正则化）下，学习率估计变得退化，另一种是在中间区域，我们看到的独立结果是有效的。</p><h2>随机操作：记忆与泛化的扩展</h2><p>为了将我们的泛化网络与具有相同架构（仅记忆）的网络进行比较，我们对随机交换运算网络运行了 Watanabe-Lau-Murfet-Wei 算法。</p><p>为了让记忆网络获得良好的损失，我们需要对其进行过参数化，即参数的数量需要高于样本总数的某个适当的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="O(1)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>倍，在我们的例子中为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 。由于参数数量在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中线性增长，因此仅当 p 值较小时，我们才能收敛到接近零损失。我们注意到，由于像中国剩余定理这样的数论技巧与随机操作网络无关，因此该实验的 p 值不需要是素数。因此，我们对 5 到 40 的倍数进行此实验。由于收敛问题和缩放模式观察，我们最信任 5 到 25 之间的较短值范围内的结果。</p><p>请注意，此范围仅在 23 到 25 之间与我们的素数列表重叠；我们需要使用更大的网络（并且可能需要更好的学习收敛）来获得高于此范围的合理的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>值。对于我们考虑的值范围，与泛化网络的线性线性相比，我们观察到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>中具有二次缩放模式的更大学习系数。 </p><figure class="image image_resized" style="width:57.82%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q68601o9wqsmcjkgnlw7" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c5xikew9mkdlst7oghv9 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/le4z9svtxnjexffsn3si 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/fibwlum3sl4elyejywk1 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jsr1wasqnbfrlxydobix 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/c4hvxln3kellgpyamxk5 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vhpntzv0hfycuigby5tt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/dtfjmajdgm8mkfbrowjx 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/kd7ksxbubypsvqdtnczd 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>的随机交换运算。 </figcaption></figure><figure class="image image_resized" style="width:57.52%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/ea8nruy9slra7rnaoi50" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/gjqy14wu9usujdygfosc 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/jk0qobc1glrbzb5zmudq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/e3fvdzjforrd4bulesyp 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/tziuy8i1qvmog3pkkybt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/cjx7ag430jml3yc07qus 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/f6jtk4r7lbuewww3oypc 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/swvt3nusrdixsfnynvty 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/luryrwfiumhvhzmyycv0 640w"><figcaption> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span>随机交换运算的<span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span></span></span></span></span>与模加结果一起绘制。</figcaption></figure><p>值得注意的是，p = 25 的图表几乎完全（直到恒定偏移量）等于记忆数量<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="0.8\cdot p^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.8</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ；这里 0.8 是用于训练的完整数据集的一部分。我们还生成了 5 的更大倍数（最多 40）的数据。在这里，我们清楚地看到，在相同架构下，记忆网络比泛化网络具有更高的学习率，但当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="p > 25"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.446em;">p</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">25</span></span></span></span></span></span></span>时，二次拟合变得更差。我们相信，如果我们使用更大的网络，我们将恢复更多 p 值的二次拟合。</p><h2>动力学和相变</h2><p>最后，我们在泛化网络的学习过程中对各个检查点的学习系数进行了<i>动态</i>估计。</p><p>在我们的这部分结果中，我们介绍了<a href="https://www.researchgate.net/publication/373332996_Quantifying_degeneracy_in_singular_models_via_the_learning_coefficient">Lau 等人的方法的一些创新。</a>和<a href="https://arxiv.org/abs/2310.06301">陈等人</a>。 （尽管我们没有实现后一篇论文中的“基于健康”的采样轨迹排序）。具体来说，我们通过温度调整以及在限制正常超平面损失梯度后应用不稳定的 SGLD 实现获得了最佳结果。 </p><figure class="image image_resized" style="width:54.37%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/q1sklqenvvtrvmsezcqy" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/xbdikzy1xefx5d0lk0sm 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/omo9gl1nkeupoo6cfb6h 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/m7x46oyonwnbdll9twai 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/bjqxhr02dbf6hoqwylxr 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/zvbgfmsb6xbp5i7k2t7k 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/a6cxughqqoorqstjtzds 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/b7zi8nz9pjgn5btqrwdl 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/sdrpizjr58fqumlrni37 640w"><figcaption>在模型训练过程中，在 25 个等距检查点处运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计。 SGLD 搜索经过修改，将搜索方向限制为与初始化时梯度正交的方向。 </figcaption></figure><figure class="image image_resized" style="width:53.69%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/vexyifqwkko2uecghzia" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/s04vahbbg7ngmfzzucgi 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/wgih1szglxj6dkjuijr0 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/uscfm3augmtcocwn4ucm 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/o83yc9csdbjypxmuuf5j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/hgboz1tsob3q3m8flbzu 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/qu9rxcxgkpsupioaz2gk 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/csmwtmkawpbgxxxoalwn 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/AhnoNzES5qiTnnavt/heyla9f6gnxi3xvzohdd 640w"><figcaption>使用另一个模型/独立 SGLD 采样重复运行，以检查结果的一致性。</figcaption></figure><p>在这里，我们观察到不稳定的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span>估计与验证（即测试）密切相关，尽管它们是使用仅涉及训练数据的方法计算的。特别是，当训练损失稳定并且测试损失下降时，这些不稳定的测量“注意到”记忆和泛化之间的过渡。 （由于我们的网络非常高效，因此这种情况在训练的早期就发生了。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnx9z3e6c3n7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefx9z3e6c3n7">^</a></strong></sup></span><div class="footnote-content"><p>请注意，<a href="https://paperswithcode.com/paper/quantifying-degeneracy-in-singular-models-via">刘等人。</a>还对具有超过一百万个神经元的大型 MNIST 网络进行<span>\(\hat{\lambda}\\)</span>估计。在这里，他们发现<span>\(\hat{\lambda}\\)</span>的结果值与用于在可预测方向上训练网络的优化方法相关，从而捕获有关盆地的重要信息。但这里无法得到<span>\(\hat{\lambda}\\)</span>的理论值，SGLD算法无法收敛；因此，在这种情况下，该估计预计不会给出学习系数的真实值</p></div></li><li class="footnote-item" role="doc-endnote" id="fny802tk8mheb"><span class="footnote-back-link"><sup><strong><a href="#fnrefy802tk8mheb">^</a></strong></sup></span><div class="footnote-content"><p>请注意，动态<span>\(\hat{\lambda}\\)</span>估计器尝试将专为稳定点（即局部最小值）设计的技术应用于非局部最小值且具有一些不稳定性、采样和遍历性问题的点，甚至通过我们的法线到梯度限制细化。特别是，它们（远远超过稳定点的估计）对超参数敏感。因此，这些不稳定的<span>\(\hat{\lambda}\\)</span>测量目前没有相关的精确理论值，可以被认为是对不稳定点的复杂性估计的临时概括。然而，我们发现，在固定的超参数集合中，这些估计给出了一致的结果，并且在运行过程中看起来很相似，并且我们发现它们包含有关学习过程中损失景观动态的重要信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1xv13wjmuli"> <span class="footnote-back-link"><sup><strong><a href="#fnref1xv13wjmuli">^</a></strong></sup></span><div class="footnote-content"><p>对此的直觉是，非常奇异的损失函数（即，具有许多等于零的高阶导数的函数）与非常大的盆地相关，这些盆地足够大以“适合额外维度的参数”。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyim8grymza"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyim8grymza">^</a></strong></sup></span><div class="footnote-content"><p>与第k个傅立叶模式相关联的嵌入空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^\text{embed_dim}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">embed_dim</span></span></span></span></span></span></span></span></span>的二维子空间是由k频率离散傅立叶变换的sin和cos分量所跨越的空间。请注意，这些空间对于不同模式不一定是线性独立的，但对于学习电路的模式是独立的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn0vw69e6p2rtp"> <span class="footnote-back-link"><sup><strong><a href="#fnref0vw69e6p2rtp">^</a></strong></sup></span><div class="footnote-content"><p>这是 RLCT 意义上的意思。在代数几何语言中，如果存在平滑解析爆炸<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="X\to \mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">→</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb{R}^n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">使得</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">在</span></span></span></span></span></span></span></span></span>X 上的局部坐标中，f 是坐标函数的平方的乘积，则权重空间 R n 上的函数 f 是最小奇异的。在这种语言中，如果我们有 c 个电路与权重空间中的向量子空间<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="C_1,\dots, C_c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span></span></span>相关联，则电路的 k 元组上具有最小值的“理想化”函数是该函数</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f(w) = \sum_{S\subset \{1,\dots, c\}, |S| = c-k+1} \prod_{i\in S} \text{dist}(w, C_i)^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⊂</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">…</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span> <span class="mjx-munderover MJXc-space1"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">∏</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span> <span class="mjx-mtext MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></p><p>为了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span>运行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c-k+1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1 个</span></span></span></span></span></span></span>元素子集，并<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{dist}(w, C_{i_j})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">dist</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base" style="margin-right: -0.045em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;">C</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.262em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">j</span></span></span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>从权重到相应子空间的 L2 距离。很容易检查所得奇点是否为最小奇异点。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition-1<guid ispermalink="false"> 4v3hMuKfsGatLXPgt</guid><dc:creator><![CDATA[Nina Rimsky]]></dc:creator><pubDate> Tue, 17 Oct 2023 19:51:34 GMT</pubDate> </item><item><title><![CDATA[When building an organization, there are lots of ways to prevent financial corruption of personnel. But what are the ways to prevent corruption via social status, political power, etc.?]]></title><description><![CDATA[Published on October 17, 2023 6:51 PM GMT<br/><br/><p>如今，即使是相当复杂的会计和审计概念，对于在任何大型公司拥有在职工作经验的人来说也是众所周知的。</p><p>例如控制支出账户、公司卡等资金流动的系统、留下高度清晰的审计跟踪的程序等等。</p><p>在很大程度上，这些方法成功地防止了任何人以过于明显的方式弄乱财务数字，至少在没有留下很多警告信号的情况下是这样。</p><p>然而，在控制那些过度渴望社会地位或政治权力的人方面，方法似乎要少得多。</p><p>通俗文学中讨论的大多只在大规模范围内起作用，比如议会程序、政府多个部门相互制衡等等……</p><p>有哪些行之有效的方法可以在普通组织的规模上发挥作用？</p><p> （比如 100 到 10 000 人）</p><p>如果没有，有哪些理论建议？</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBjPreynFn5aDmJsj/when-building-an-organization-there-are-lots-of-ways-to<guid ispermalink="false"> JBjPreynFn5aDmJsj</guid><dc:creator><![CDATA[M. Y. Zuo]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:51:47 GMT</pubDate> </item><item><title><![CDATA[Eliezer's example on Bayesian statistics is wr... oops!]]></title><description><![CDATA[Published on October 17, 2023 6:38 PM GMT<br/><br/><p>这篇文章<i>将</i>解释 Eliezer Yudkowsky 在统计讨论中经常使用的一个例子实际上并不意味着他认为它的作用。在证明他错误的过程中，我发现我其实错了。我仍在写这篇文章，对于犯与我相同错误的其他人来说，这可能会很有趣。</p><p>这个例子是埃利泽在论证贝叶斯统计优于频率统计时经常使用的例子。在 Sequences <a href="https://www.lesswrong.com/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective">里</a>，在 Arbital<a href="https://arbital.com/p/likelihoods_not_pvalues/?l=4xx">里</a>，在 Glowfic<a href="https://www.glowfic.com/posts/5826">里</a>，还有其他几个我现在找不到的地方。一枚硬币抛六次；前五次出现正面，第六次出现尾部。但我们不知道实验者是否 A) 事先决定抛硬币六次，并报告发生了什么，或者 B) 事先决定将硬币一遍又一遍地翻转，直到反面朝上，并报告花了多长时间。这为我们提供了有关代币偏见或缺乏偏见的哪些信息？</p><p>根据频率论的观点，在这两种不同的情况下分配的 p 值存在严重差异。在第一种情况下，结果 HHHHHT 被放入至少相对于“原假设”极端的结果类别中 - 其中有 14 个：HHHHHH、TTTTTT、HHHHHT、TTTTTH、HHHHTH、TTTTHT 等。总共有 64 种可能性，14/64 = 0.22，远远高于 p=0.05 的水平，因此不足以得出显着性结论。 In the second case, the result HHHHHT is instead placed into a different class of results: 5 heads and then a tail, 6 heads and then a tail, 7 heads and then a tail, and so on forever. The probability of that entire class is 1/32 in total, which is 0.03 - statistically significant!</p><p> Eliezer criticizes this in several ways. To start out with, the part where a frequentist decides to lump in the actual result with a group of results that are similar is subjective enough to allow for a lot of freedom in what the actual result is. Maybe in the first case, instead of choosing the class of results with 5 or more of the same side, you only choose the class of results with 5 or more heads in particular, thus halving the p=0.22 to p=0.11. He also criticizes the very notion of significance being determined by &quot;rejecting the null hypothesis&quot; rather than looking at different theoretical effect sizes and how well they would have predicted the data. Two experiments that are evidence supporting entirely different effect sizes are treated as both &quot;rejecting the null hypothesis&quot; and thus evidence towards the same theory, even if the results are inconsistent with each other.</p><hr><p> All of this criticism of frequentist statistics and p-values seemed to be correct. But the analysis of how a Bayesian would update was different.</p><blockquote><p> ...a Bayesian looks at the experimental result and says, &quot;I can now calculate the <a href="http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html">likelihood ratio</a> (evidential flow) between all hypotheses under consideration.  Since your state of mind doesn&#39;t affect the coin in any way - doesn&#39;t change the probability of a fair coin or biased coin producing this exact data - there&#39;s no way your private, unobservable state of mind can affect my interpretation of your experimental results.&quot;</p><p> If you&#39;re used to Bayesian methods, it may seem difficult to even imagine that the statistical interpretation of the evidence ought to depend on a factor - namely the experimenter&#39;s state of mind - which has no causal connection whatsoever to the experimental result.  (Since Bayes says that <a href="http://yudkowsky.net/rational/bayes">evidence is about correlation</a> , and no systematic correlation can appear without causal connection; <a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">evidence requires entanglement</a> .)</p></blockquote><p> So Eliezer is arguing that the likelihood ratios should obviously be the same in both scenarios, because the only relevant data is what sequence of flips the coin produced. The experimenter&#39;s state of mind doesn&#39;t change the probability that a coin of a given bias would produce this data, so it&#39;s irrelevant.</p><p> But the key element Eliezer seems to be missing here is that the total sum of the data is <i>not</i> &quot;The coin came up HHHHHT.&quot; The data that we received is, instead, &quot;The experimenter <i>saw</i> the coin come up HHHHHT.&quot; And that <i>is</i> the sort of evidence that is causally entangled with the experimenter&#39;s state of mind, because the experimenter&#39;s state of mind determines in which cases the experimenter will ever see the coin come up HHHHHT. If the real fact of the matter was that the coin really was fair, for example, the experimenter&#39;s state of mind following the &quot;flip until you get tails&quot; rule causes it to be less likely that the experimenter will ever get to the point of having a sixth flip in the first place, because there is now a 31/32 chance the experimenter would stop before flip six. <a href="https://www.lesswrong.com/posts/kJiPnaQPiy4p9Eqki/what-evidence-filtered-evidence">Evidence plus the knowledge that the evidence is filtered can often have different properties than the unquoted evidence would have on its own</a> ; the experimenter&#39;s method of deciding which evidence to search for changes which evidence they are likely to find.</p><p> The two possibilities produce <i>very different</i> prior distributions over possible outcomes. Assuming that the actual bias of the coin is such that the theoretical frequency of heads is <i>f</i> : then in the &quot;flip <i>n</i> times&quot; case, the prior is distributed between all possible sequences of length <i>n</i> , with each one having a probability of <i>f</i> ^(number of total heads in the sequence) * (1- <i>f</i> )^(number of tails in the sequence). (In the case where f = 1/2 and n = 6, this simply reduces to ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{1}{2^6}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.819em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.158em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.158em; bottom: -0.919em;"><span class="mjx-msubsup" style=""><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 83.3%; vertical-align: 0.443em; padding-left: 0px; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">6</span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.819em;" class="mjx-line"></span></span><span style="height: 1.621em; vertical-align: -0.65em;" class="mjx-vsize"></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>) for each sequence.) Meanwhile, in the &quot;flip until you get tails&quot; case, the prior is a 1- <i>f</i> chance of T, <i>f</i> (1- <i>f</i> ) chance of HT, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> (1- <i>f</i> ) chance of HHT, <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> (1-f) chance of HHHT, and so on, always ending up at <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(l-1)}(1-f)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">l</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> for a sequence of total length <i>l</i> . These priors both assign <i>very</i> different probabilities to different outcomes - in fact, there are many outcomes permitted by versions of the first that have zero probability on the second (like HTHTHTHT) or permitted by the second but with zero probability on low- <i>n</i> versions of the first (like HHHHHHHHHHHHT if <i>n</i> =6.)</p><hr><p> So the probabilities over what happens during the experiment are very different, depending on whether you&#39;re flipping <i>n</i> times or flipping until you get tails. Do those different conditional priors mean that the final likelihood ratios would end up being different, contrary to what Eliezer claimed?我也这么想。</p><p> After all, the way that Bayesian updating works in the first place is that you determine what the probabilities assigned to the experimental result were under different hypotheses (possible values of <i>f</i> ), construct a likelihood distribution of what those probabilities were as a function of f, and multiply that by your prior distribution to update it. So the fact that the prior distributions conditional on the two types of experiments were so different would cause the things you were multiplying by to be different, and therefore give you different results.</p><p> (Feel free to pause and look for my error here, if you haven&#39;t found it yet.)</p><hr><p> The error I made was that I was confusing the difference in the experimental designs&#39; probability distributions over <i>experimental results</i> , with a difference in the likelihood distribution over <i>hypotheses about the coin</i> that the experiment would cause you to update towards. It <i>is</i> the case that the different experimental designs cause some experimental results to occur at different frequencies, but that <i>does not automatically imply</i> that the final update about the coin&#39;s bias will be different.</p><p> Whether the update about the coin is different depends <i>only</i> on the probabilities assigned <i>to the result that actually happened</i> . It doesn&#39;t matter if they assign wildly different probabilities to results like HHT and HTHTTH, if the experiment turns up HHHHHT and they assign the same probability to that. Which is, in fact, the case. In fact, while the distributions of experimental results look quite different at most values, they will always happen to cross at the exact value that actually turns out to be the result of the experiment, by some amazing not-really-a-coincidence.</p><p> The reason this happens is because, while the &quot;flip until you get tails&quot; is truly a constant probability distribution for any given value of <i>f</i> , the &quot;flip <i>n</i> times&quot; also depends on the value of n, making it really <i>n</i> separate distributions that happen to be similar to each other. If it takes 7 flips rather than 6 to get tails in the &quot;flip until you get tails&quot; experiment, that doesn&#39;t mean that you suddenly have gotten a result that would have been impossible on any &quot;flip <i>n</i> times&quot; distribution, it just means that you move to the &quot;flip 7 times&quot; distribution rather than &quot;flip 6 times&quot;, and the probability of HHHHHHT on that distribution will end up matching the probability as assigned by the &quot;flip until you get tails&quot; distribution.</p><p> (The algebra here is simple enough. As said before, the probability of getting a given sequence for &quot;flip <i>n</i> times&quot; is <i>f</i> ^(number of total heads in the sequence) * (1- <i>f</i> )^(number of tails in the sequence). But assuming that the sequence is one in which every value is heads, except for the last, which is tails, this reduces to <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f^{(n-1)}(1-f)^1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.181em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span> . This is identical to the previously described function for &quot;flip until you get tails.&quot;)</p><p> See <a href="https://www.desmos.com/calculator/fkpvlyycod">here</a> for what the likelihood distribution over the coin&#39;s bias <i>f</i> really does look like after seeing <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label=""><span class="mjx-mrow" aria-hidden="true"></span></span></span></span></span> <i>n-1</i> heads and 1 tails, regardless of which of the two experimental designs is used.</p><hr><p> So why am I writing this post, if I turned out to be wrong as a simple question of fact? There are a few things <i>I</i> sure learned here, and it seems possible someone else is also confused and able to learn something from them too.</p><p> The first lesson is just to be more careful about checking what <i>precisely</i> a probability distribution is telling you! In my initial calculations, I made a <i>lot</i> of mistakes before I could even start to be sure about what it was that was confusing me, several of which I haven&#39;t even mentioned here (like initially modelling something using a binomial distribution that <i>really</i> wasn&#39;t applicable there.) Most of these mistakes were of the nature of &quot;I&#39;m looking for a probability distribution over something something conditional on x; this thing here is a probability distribution over something something conditional on x; therefore it is the distribution I&#39;m looking for.&quot; There&#39;s a difference between the distribution over experimental results given a particular type of experiment, and the likelihood ratio over hypotheses given the observation of a particular result; there&#39;s a difference between any particular version of a distribution dependent on one of the function&#39;s parameters, and the overall class of distributions formed from all possible values of that parameter.</p><p> The second thing I learned is that Bayesian likelihood ratios really do only depend on the probability each hypothesis assigned only to the information that you received, and nothing else. Which I <i>verbally</i> knew before, but I hadn&#39;t truly internalized. If two hypotheses assign the same probability to an outcome, and you see that outcome, that tells you nothing about any difference between the hypotheses. If I had ignored trying to quantify over all the possible outcomes, and just asked the comparatively simpler question of &quot;what is the chance of HHHHHT in experiment 1, and in experiment 2,&quot; I probably could have solved it a lot more quickly.</p><p> And then there&#39;s also a <i>possible</i> lesson for me to learn of &quot;see, you really should meta-level trust the reasoning of Eliezer Yudkowsky and other people who have more expertise in a given mathematical domain.&quot; I am not sure this is a good lesson to learn. And I&#39;m also not sure that Eliezer actually saw all of the reasoning I went through in this post about <i>why</i> the two experiments assign the same probabilities to the actual result, rather than just guessing and happening to be correct. That being said, it still is the case that I would have previously given this as an example of a situation in which Eliezer Yudkowsky was wrong about basic probability theory (and I also would have said something like &quot;and he probably made this mistake because of motivated reasoning in order to score points against frequentists&quot;). And he turned out to be right all along. This is more likely in worlds where he knows his stuff more, and I have correspondingly updated my beliefs.</p><p> (I hope this goes without saying, but I&#39;ll say it anyway: a meta-level update towards trusting experts&#39; math, does not mean first-order conforming to their opinions if you don&#39;t first-order agree with or understand them. I&#39;ll still keep trying to notice and point out when it looks like Eliezer is wrong about something - even if I might not bet as strongly that he really does turn out to be wrong.)</p><br/><br/> <a href="https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BeAHn5CisRgivuspA/eliezer-s-example-on-bayesian-statistics-is-wr-oops<guid ispermalink="false"> BeAHn5CisRgivuspA</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:38:18 GMT</pubDate> </item><item><title><![CDATA[Trying to deconfuse some core AI x-risk problems]]></title><description><![CDATA[Published on October 17, 2023 6:36 PM GMT<br/><br/><p> Max H commented on the <a href="https://www.lesswrong.com/posts/kQuSZG8ibfW6fJYmo/announcing-dialogues-1">dialogues announcement</a> , and with both of us being interested in having a conversation where we try to do something like &quot;explore the basic case for AI X-risk, without much of any reference to long-chained existing explanations&quot;. I&#39;ve found conversations like this valuable in the past, so we decided to give it a shot.</p><p> His opening comment was:</p><blockquote><p> I feel like I&#39;ve been getting into the weeds lately, or watching others get into the weeds, on how various recent alignment and capabilities developments affect what the near future will look like, eg how difficult particular known alignment sub-problems are likely to be or what solutions for them might look like, how right various peoples&#39; past predictions and models were, etc.</p><p> And to me, a lot of these results and arguments look mostly irrelevant to the core AI x-risk argument, for which the conclusion is that once you have something actually smarter than humans hanging around, literally everyone drops dead shortly afterwards, unless a lot of things before then have gone right in a complicated way.</p><p> (Some of these developments might have big implications for how things are likely to go before we get to the simultaneous-death point, eg by affecting the likelihood that we screw up <i>earlier</i> and things go off the rails in some less predictable way.)</p><p> But basically everything we&#39;ve recently seen looks like it is about the character of mind-space and the manipulability of minds in the below-human-level region, and this just feels to me like a very interesting distraction most of the time.</p><p> In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead. I also wouldn&#39;t mind recapitulating (my view of) the core AI x-risk argument, though I expect I have fewer novel things to say on that, and the non-novel things I&#39;d say are probably already better said elsewhere by others. </p></blockquote><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 03:48:23 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 03:48:23 GMT" user-order="1"><p> I am quite interested in the &quot;In a dialogue, I&#39;d be interested in fleshing out why I think a lot of results about below-human-level minds are likely to be irrelevant, and where we can look for better arguments and intuitions instead.&quot;部分。</p><p> Do you want to start us off with a quick summary of your take here?</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Wed, 11 Oct 2023 04:40:06 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Wed, 11 Oct 2023 04:40:06 GMT" user-order="2"><p><br> So, I think there&#39;s a lot of alignment discussion that is missing a kind of inductive step, where you freely assume that you have a running human-level (or smarter) system, and which is thus dangerous by default. And then you ask, &quot;how did that get aligned?&quot; or &quot;why does that not kill everyone?&quot; instead of what I see as a somewhat backwards approach of looking for an alignment method that applies to current systems and trying to figure out whether it scales.<br><br> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p><p> As for why I think results about below-human-level systems are unlikely to be relevant, it&#39;s because all current AI systems are far less Lawful than humans.</p><p> Rephrasing <a href="https://glowfic.com/replies/1660170#reply-1660170">planecrash</a> (and somewhat violating the original ask to avoid long-chained and unwieldy dependencies, but I&#39;ll try to give a useful summary here), Lawfulness is the idea that there are logical / mathematical truths which are spotlighted by their simplicity and usefulness across a very wide space of possible worlds.</p><p> Examples of heavily spotlighted truths are concepts like logical deduction (Validity), probability theory (Probability), expected utility theory (Utility), and decision theory (Decision).  Systems and processes are important and capable precisely to the degree in which they embody these mathematical truths, regardless of whether you call them &quot;agents&quot; or not. All humans embody shadows and fragments of Law to varying degrees, even if not all humans have an explicit understanding of the concepts in words and math.<br><br> Most current capabilities and alignment techniques are focused on trying to squeeze more fragments of Law into AI systems: predictive ability (Probability), deductive ability (Validity), Utility (doing stuff efficiently without working at cross-purposes), and those systems still coming out far below humans in terms of general Lawfulness.</p><p> At the capabilities level of current systems, squeezing more Law into a system can <i>look</i> like you&#39;re making it more aligned, but that&#39;s because at the current level of capabilities / Lawfulness, you need more Law to do even very basic things like following instructions correctly at all.</p><p> Another problem that a lot of proposed alignment methods have (if they&#39;re not actually capabilities proposals in disguise) is that they postulate that we&#39;ll be able to badger some supposedly smarter-than-human system into acting in an obviously unLawful way ( <a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Deep Deceptiveness</a> is an example of what might go wrong with this).</p><p> Also, if you accept this frame, a whole bunch of confusing questions mostly dissolve. For example, questions like &quot;are humans expected utility maximizers&quot; or &quot;is it a problem that ideal utility maximization is computationally intractable&quot; or &quot;but is EU theory even right&quot; have straightforward answers in this framework:</p><p> It&#39;s OK if EU theory as humans currently understand it isn&#39;t entirely strictly correct; EU theory is probably a shadow or approximation for whatever the true Law is anyway, the way that Newtonian mechanics is an approximation for General relativity at the right scale. (Maybe something like <a href="https://www.lesswrong.com/posts/Xht9swezkGZLAxBrd/geometric-rationality-is-not-vnm-rational">Geometric Rationality</a> is the real Law that a supermajority of capable minds across the multiverse would settle on.)</p><p> Similarly for decision theory, it doesn&#39;t matter if some particular human-understood flavor of logical decision theory is exactly correct; it&#39;s close enough to some true Law of Decision that a supermajority of all minds (including humans, if they think for a few decades longer) will be able to see, spotlighted by its mathematical simplicity / optimality / usefulness across a sufficiently wide slice of possible worlds.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 11 Oct 2023 23:23:44 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 11 Oct 2023 23:23:44 GMT" user-order="1"><blockquote><p> In my model, &quot;a digital human but they&#39;re evil&quot; isn&#39;t a <i>great</i> model for what misaligned AGI looks like, but it&#39;s a kind of lower bound on which to start building. (Actual AGI will probably be a lot more &quot;alien&quot; and less &quot;evil&quot;, and more capable.)</p></blockquote><p> Yeah, this seems right to me.</p><p> I&#39;ve had a long-standing disagreement with many people currently working in prosaic alignment about the degree to which AI systems are quite alien in their cognition (but are trained on the task of imitating humans, so look on the surface like humans).</p><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 02:29:57 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 02:29:57 GMT" user-order="2"><p> I also think current and future AI cognition is unlikely to be human-like, but I don&#39;t think this is a super important crux about anything for me.</p><p> For example, maybe it turns out that actually, the only way to get human-level or better performance is via some algorithm that looks isomorphic to some process in a human brain somewhere.</p><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p><blockquote><p> A key thing to look at for this is where current AI systems display superhuman performance, and which domains they display substantially sub-human performance.</p></blockquote><p> I think another thing to look at is the process, in addition to performance and final outputs.</p><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.<br><br> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.<br><br> (I like <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded by human capabilities? No.</a> and <a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators">GPTs are Predictors, not Imitators</a> for more on this general topic.)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:02:32 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:02:32 GMT" user-order="1"><blockquote><p> But even if that&#39;s true, that just means the algorithms for cognition are themselves spotlighted by their simplicity and usefulness and tendency for very disparate optimization processes and systems (evolution on humans and SGD on NNs) to converge on them.</p></blockquote><p> I think this does actually still matter, because a lot of the current AI Alignment plans people are pursuing boil down to something like &quot;try to create an AI system that is approximately human level smart, and then use that to automate the task of AI Alignment research, or use those systems to coordinate or otherwise achieve an end to the acute risk period&quot;.</p><p> In order for those plans to work, the following things really matter:</p><ul><li> how much time you expect these systems to spend in the human regime,</li><li> to what degree they will have a capability profile that you can use for alignment research, coordination or some other plan that ends the acute risk period</li><li> how good you are at eliciting the latent capabilities of the system, compared to how capable the system itself is at using those capabilities for deception/other dangerous ends</li></ul><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above. If the cognition of current and future AI systems is alien, this makes it more likely that dangerous capabilities will suddenly spike, less likely that we can effectively automate AI alignment research, use the systems for coordination, or leverage the system for some other pivotal act, and less likely that we will be capable of eliciting the capabilities of the system for our end.</p><p> Separately, humans also just have really good intuitions for spotting deception from human-like minds. In as much as the systems engaging in deception will do so using mostly tactics and cognition that are borrowed from humans (by being trained on producing human-produced text), then we have a much better chance at spotting that taking appropriate action.</p><blockquote><p> GPTs are trained to predict the next token on a very large and general corpus. As a result of this training process, they output explicit probability distributions over the entire token-space on the space of possible next tokens. The user can then sample this probability distribution auto-regressively to get outputs that sometimes look pretty human-like, but the process of &quot;produce a probability distribution + sample from it + auto-regress&quot; doesn&#39;t look at all like the way humans generate language.</p><p> When you break things down into more precise mechanical descriptions, claims like &quot;AIs are trained on the task of imitating humans&quot; start to look pretty stretched to me.</p></blockquote><p> Hmm, somehow this isn&#39;t landing for me. Like, I agree that the process of &quot;produce a probability distribution + sample from it + auto-regres&quot; doesn&#39;t look very human like, but that feels kind of irrelevant to my point.</p><p> What I mean by saying &quot;AIs are trained on the task of imitating humans&quot; is that the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet. Ie the AI is trained on the task of imitating human internet-text production.</p><p> I am not saying this to suggest that this means the AI is trained to think in human ways, since the context humans are in in producing internet text is very different from the context the AI is performing this task in. I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 04:03:13 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 04:03:13 GMT" user-order="2"><blockquote><p> The cognition of current and future AI systems being human-like helps a lot with all of the three points above.</p></blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.<br><br> Also, if you think your AI can do alignment research, it should also be able to do pretty much all other kinds of human labor, so maybe just... stop a bit earlier and make sure you really have achieved post-scarcity and perfect global harmony, before moving onto even harder problems. (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p><p> I&#39;m not sure how plausible I find this scenario, nor how optimistic I should be about it if it does come to pass. eg one &quot;early failure mode&quot; I can imagine is that it&#39;s not really AGI that dooms us, it&#39;s just that humans plus below-human-level AI tools start unlocking a bunch of really powerful technologies in a way that goes off the rails before we even get to actual AGI.</p><blockquote><p> the AI minimizes loss when it successfully produces text that mimicks the distribution of human-produced text on the internet.</p></blockquote><p> In most cases, yes, but the point of <a href="https://www.lesswrong.com/posts/bZbLnr7qwuEBpTPuF/is-gpt-n-bounded-by-human-capabilities-no">Is GPT-N bounded</a> is that you can get even better loss by being skilled at a lot more things than just human-mimicry. For example, if you want to predict the next tokens in the following prompt:</p><pre> <code>I just made up a random password, memorized it, and hashed it. The SHA-256 sum is: d998a06a8481bff2a47d63fd2960e69a07bc46fcca10d810c44a29854e1cbe51. A plausible guess for what the password was, assuming I&#39;m telling the truth, is:</code></pre><p> The best way to do that is to guess an 8-16 digit string that actually hashes to that. You could find such a string via bruteforce computation, or <a href="https://xkcd.com/538/">actual brute force</a> , or just paying me $5 to tell you the actual password.</p><p> If GPTs trained via SGD never hit on those kinds of strategies no matter how large they are and how much training data you give them, that just means that GPTs alone won&#39;t scale to human-level, since an actual human is capable of coming up with and executing any of those strategies.</p><blockquote><p> I am just saying that it should be completely unsurprising to see the AI produce reasoning that looks human-like or makes human-like errors when you eg use chain-of-thought prompting, because the central task the AI was trained on was to produce text that looks like internet text, which is almost all produced by humans.</p></blockquote><p> I mostly agree with this point. Two remarks / alternate hypotheses:</p><ul><li> a lot of the human-like qualities look at least partly illusory to me when you look closely (meaning, the errors and reasoning actually <i>aren&#39;t</i> all that human-like)</li><li> To the degree that they are human-like, a hypothesis for why is that there just aren&#39;t that many ways to be kinda wrong but not totally off-base. What would <i>non</i> -human-like reasoning errors that still produce vaguely human-sounding text even look like?</li></ul><p> Also, when an AI reasons <i>correctly</i> , we don&#39;t call it &quot;being human-like&quot;; that&#39;s just being right. So I sort of feel like the whole human-like / not-human-like distinction isn&#39;t carving reality at its joints very well. In my terms, I&#39;d say that when both humans and AIs reason correctly, they&#39;re alike because they&#39;re both reasoning Lawfully. When they mess up, they&#39;re alike because they&#39;re both reasoning un-Lawfully. The fact that AIs are starting to sound more like humans is explained by the fact that they are getting more Lawful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:18:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:18:40 GMT" user-order="1"><blockquote><p> (A half-joking policy proposal for alignment researchers: if you can&#39;t use your AI to solve the Bay Area housing crisis, you don&#39;t get to use it to try to do alignment research. If that sounds like a kind of problem that human-level AI is poorly suited for, then ask yourself what you really mean by &quot;human-level&quot;.)</p></blockquote><p> Lol, I genuinely like that proposal/intuition-pump/sanity-check.</p><blockquote><p> Though if you do have an AI like that, it seems much safer to try using it to do mundane work or solve concrete science and engineering problems unrelated to alignment research, since alignment research is like, the hardest and most dangerous thing you can possibly have the AI do.</p></blockquote><p> I agree with you on the &quot;doing end-to-end alignment research seems particularly risky&quot; component. I also think that automating alignment research is not the right proxy to aim for with close-to-human-level AI systems, and we should aim for some coordination victory/game-board-flipping plan that somehow prevents further AI progress.</p><blockquote><p> I actually think <i>less</i> human-like cognition is potentially better for some of those plans. Like, maybe it actually is pretty easy to get an AI that is superhuman at logical deduction and probabilistic inference and scientific knowledge recall, while still being below human-level at cognitive reflection, and that turns out to be safer than an AI that is less spiky and more balanced the way humans are.</p></blockquote><p> Yeah, I think that&#39;s plausible, but I think &quot;figuring out how to do science using an alien mind&quot; is actually a pretty hard problem, and it&#39;s going to be much easier to slot a human-like AI into the process of making scientific discoveries.</p><p> Overall, I think the most important reason for why it matters if AIs have human-like cognition is not that it makes it safer way to leverage the AI for things like AI Alignment research. It&#39;s instead that if the trajectory of AI capabilities roughly follows the trajectory of human performance, then we will be much better at predicting when the AI system is getting dangerous. If we can just ask the question &quot;well, would a human of this ability level, with maybe some abnormally high skills in this specific subdomain be able to break out of this box and kill everyone?&quot; and get a roughly accurate answer, then that&#39;s a much easier way of determining whether a given AI system is dangerous, than if we have to ask ourselves &quot;is this alien mind that we don&#39;t really have good intuitions for the kind of thing that could break out of the box and kill everyone?&quot;.</p><p> And conversely, if it is indeed the case that current and future AI system&#39;s internal cognition is alien, and their perceived reasoning performance only follows the human trajectory because they are trained to perform reasoning at human levels (due to being trained on human text), then this will cause us to reliably underestimate the actual abilities and performance of the system on a wide range of task.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 04:32:10 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 04:32:10 GMT" user-order="1"><p> Going back up a level of the conversation, I&#39;ve been trying to concretely argue that even though I think you are correct that as we reach Superintelligence, the natural lawfulness of correct reasoning will make almost all more prosaic alignment approaches fail, I think it still really matters how to control systems that are just below and just above the human capability level.</p><p> Or to be more precise, if it turns out to be the case that you could make systems such that their capabilities roughly follow the human distribution of competence as you scale up compute, then I think you have a bunch of options of punting on the hard part of the AI Alignment problem, by using those systems to solve a bunch of easier problems first, and then after you solved those problems, you are in a much better position to solve the rest of the AI Alignment problem (like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem).</p><p> That said, my biggest concern with this kind of plan is that AI performance will not predictably follow the human distribution, and that our current training methods inherently bias researchers towards thinking that AI systems are much more human-like in their cognition than they actually are.</p><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you. That said, I do currently feel like the hope for AI Alignment does still come from some set of plans of the form &quot;how can I use early AGI systems to end the acute risk period somehow&quot;, and so any critique of existing AI Alignment approaches needs to show how an alignment approach fails to achieve that goal, and not how it fails to achieve full alignment, which I think is just very solidly out of our reach at this point.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 14:49:27 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 14:49:27 GMT" user-order="2"><p> I agree with most of what you&#39;ve written in broad strokes, though I think I actually have more uncertainty about how things will play out in the short term.</p><p> I&#39;ll respond to 3 specific things you said which jumped out at me, and then you can decide on the conversational branching factor from there...</p><blockquote><p> they are trained to perform reasoning at human levels (due to being trained on human text),</p></blockquote><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself. There was this whole debate about whether a superintelligence could infer General Relativity from a video of a falling apple - maybe a few frames of an apple video aren&#39;t actually enough even for a superintelligence, but a giant corpus full of physics textbooks and experimental results and detailed descriptions of human behavior is definitely enough to make a lot of inferences beyond current human reach, and also very likely more than sufficient to <i>learn to do inference in general</i> superhumanly well, <i>if</i> you get the training method and architecture right.</p><p> (Also, a lot of the text generation that LLMs can do is already superhuman; I can still write better code than GPT-4 if I get a chance to compile it, run it, refer to documentation, etc. But I can&#39;t write it token by token with no backspace key or even the ability to jump around in my code as I write it, even if you give me a lot of extra time.)</p><blockquote><p> like, I do genuinely think that &quot;make a mind upload of the 100 best AI Alignment researchers and give them 10,000 years of subjective time to come up with solutions and run experiments, etc.&quot; is a decent &quot;solution&quot; to the AI Alignment problem</p></blockquote><p> They better be able to make progress in a lot less than 10,000 years of subjective time! I actually think if you can get even a single high fidelity upload of a smart human and run them at no speed up (or even a slowdown) you&#39;re already in pretty good shape.</p><p> I would spend the first few months of subjective time looking for improvements to the fidelity and efficiency of the simulation from the inside, checking my own mind for bugs and inconsistencies introduced by the upload process, doing some philosophy, introspection, mental inventory, etc.</p><p> And then probably start working on making very small, safe tweaks to my own mind, giving myself some extra tools (eg an instant math module, a larger working memory, an integrated search engine), and then maybe try out some bigger / more invasive changes, eg making myself better at applied rationality via direct brain modifications.</p><p> And then maybe after like, a year or two of subjective time spent tweaking my own mind and improving my understanding of digital mind design and minds in general, I start turning towards work on specific alignment problems and / or go for actual recursive self-improvement. But either way, I wouldn&#39;t expect it to take 100 people anywhere close to 10,000 years, even if digitization is a 100% opaque black box to start out and digital neuroscience as a field turns out to be totally intractable.</p><blockquote><p> My model of what will happen instead is something like:</p><ul><li> AI systems will follow a quite spiky and unpredictable capability profile trajectory</li><li> There are a bunch of capabilities that when you unlock them, will cause some kind of recursive self-improvement or acceleration of the inputs of the AI (this could either be direct modifications of its own weights, or better self-prompting, or much better ability to debug large complicated software systems or the development of substantially more performant AI learning algorithms), and these capabilities will very likely be unlocked before you reach human level of usefulness at the kind of task that might successfully flip the game board</li><li> AI systems will quite directly and early on be goodhearting on human approval and won&#39;t really have a coherent concept of honesty, and when we provide a reward signal against this kind of behavior, this will just train the AI get better at deceiving us</li></ul><p> Let me know if this roughly sounds right to you.</p></blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p><p> Maybe a better way of putting it is: I expect that, before AIs get totally superintelligent or even definitely past human level, they will be able to pass a human&#39;s Ideological Turing Test about what humans value. (At least, they&#39;ll be able to pass according to the judgement of most humans, though maybe not the most careful / skeptical alignment researchers). Understanding an alien&#39;s values is maybe a bit easier if you share patterns of cognition with them, but <i>not</i> sharing them doesn&#39;t actually push the understanding task into the superintelligence realm of difficulty.</p><p> Also, I think in these plans, the specific level of &quot;human-level&quot; actually starts to matter quite a lot. Maybe you can have a median-human-level AI that is stable and safe to use when unboxed. I&#39;m less sure you can safely have a median-computer-programmer-level AI, or especially a top-1%-computer-programmer-level AI unless you&#39;ve actually solved a bunch of hard alignment problems already.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 18:07:00 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 18:07:00 GMT" user-order="1"><blockquote><p> >; they are trained to perform reasoning at human levels (due to being trained on human text),</p><p> I think it&#39;s more &quot;due to being trained via inefficient and limiting methods on fundamentally limited architectures&quot;, than due to the training data itself.</p></blockquote><p> Quick clarification here: I meant &quot;perform&quot; in the &quot;an actor performs a role&quot; sense. Ie I was trying to say &quot;the system will look like it is reasoning at roughly human levels, because it was trained to produce text that looks like it was written by humans&quot;.</p><p> That, I am confident, is not the result of the system being trained via inefficient and limiting methods on fundamentally limiting architectures.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 18:19:37 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 18:19:37 GMT" user-order="2"><p> Ah, OK, I initially read it as performing in the sense of performing at a certain capabilities level, but that makes more sense. I agree with you that this is likely to lead to reliable underestimates of true capabilities levels.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Fri, 13 Oct 2023 21:31:28 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Fri, 13 Oct 2023 21:31:28 GMT" user-order="1"><p> I&#39;ll respond to this section, since I think it&#39;s the one that&#39;s most on the critical path what to do with AI alignment:</p><blockquote><p> Mostly right, though on the third bullet, I actually think that AIs will probably have a deep / accurate / fully grounded <i>understanding</i> of concepts like honesty and even more complicated human values and goals as they get smarter. Also, true honesty in particular seems like a concept that is simple and useful and spotlighted enough that even very alien minds will understand it pretty naturally at non-superintelligence capability levels, even if they don&#39;t care at all about <i>being</i> honest.</p></blockquote><p> Ok, here I notice that I want to go a lot slower, taboo some words, and figure out what exactly is true here.</p><p> Here are some first reactions that I have when I consider this question:</p><ul><li> Man, I really don&#39;t fully understand honesty myself. Like, in many situations it takes me a really long time to figure out what the honest thing to do is. It usually requires me understanding what you are trying to achieve in any given context, and then somehow give you models and information that assist you within that context. It&#39;s easy to be dishonest while only saying true things, and even the truth value of a given statement heavily depends on context and predicting how you are likely to interpret it, and whether the parts where you will predictably be confused or wrong will matter for what you will do with that information.</li><li> I do agree that superintelligent systems will understand what we mean by &quot;honesty&quot; better than we do, probably, since a lot of my model of honesty is pretty bottlenecked on being smarter and understanding lots of parts of the world better</li><li> The key thing that I expect to be true with the current training paradigm is something like &quot;the model really has no motivation towards being honest, in part because at least the common-sense view of honesty doesn&#39;t even really apply to the cognition of a mind as alien as a language model&quot;. Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</li><li> But also, yeah, I just feel really deeply confused what &quot;motivates&quot; a language model. Clearly almost all of a language model&#39;s cognition is going into the objective of &quot;predict the next token&quot; (including after RLHF training where it looks more like the model has goals like being &quot;helpful, harmless, and honest&quot;). But does that cognition have any &quot;agency&quot;? Like, does it even make sense for the model to be &quot;honest&quot; in its pursuit of predicting the next token? Is the context of a single forward pass just too small for it to make any sense to think about the model having goals in the context of pursuing the next token?</li></ul><p> Going back up a level about how this relates to the overall question:</p><p> In as much as the best target for current AI Alignment efforts is to try to build systems that work towards some proxy task that will make the rest of the AI Alignment problem easier (either by buying us more time, or making conceptual progress on the problem, or being a really useful tool that speeds up our efforts of controlling AI systems), then it seems that being able to motivate systems towards those tasks is really quite crucial.</p><p> So let&#39;s characterize some tasks or ways early superhuman systems could help make the rest of the AI Alignment problem easy:</p><p> <strong>Honesty:</strong></p><p> I think the <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK document</a> mostly gets the point across of why &quot;honesty&quot; isn&#39;t as-straightforward a target as one might think it is. My current summary of ELK is approximately &quot;if we could get an AI system to reliably be honest, ie we could get it to try to genuinely get it to explain anything to us that it understands, then we can maybe leverage that into a full AI Alignment solution&quot;.</p><p> That said, see all the open problems in the ELK document.</p><p> <strong>Corrigibility:</strong></p><p> I feel like I don&#39;t super understand what corrigibility really points to. In the abstract, <a href="https://www.lesswrong.com/tag/corrigibility">corrigibility</a> sounds great:</p><blockquote><p> A <a href="https://arbital.greaterwrong.com/p/corrigibility/"><u>&#39;corrigible&#39;</u></a> agent is one that <a href="https://arbital.greaterwrong.com/p/nonadversarial/"><u>doesn&#39;t interfere</u></a> with what <a href="https://arbital.greaterwrong.com/p/value_alignment_programmer/"><u>we</u></a> would intuitively see as attempts to &#39;correct&#39; the agent, or &#39;correct&#39; our mistakes in building it; and permits these &#39;corrections&#39; despite the apparent <a href="https://arbital.greaterwrong.com/p/instrumental_convergence/"><u>instrumentally convergent reasoning</u></a> saying otherwise.</p><ul><li> If we try to suspend the AI to disk, or shut it down entirely, a corrigible AI will let us do so. This is not something that an AI is automatically incentivized to let us do, since if it is shut down, <a href="https://arbital.greaterwrong.com/p/no_coffee_if_dead/"><u>it will be unable to fulfill what would usually be its goals</u></a> .</li><li> If we try to reprogram the AI, a corrigible AI will not resist this change and will allow this modification to go through. If this is not specifically incentivized, an AI might attempt to fool us into believing the utility function was modified successfully, while actually keeping its original utility function as <a href="https://arbital.greaterwrong.com/p/cognitive_steganography/"><u>obscured</u></a> functionality. By default, this deception could be a <a href="https://arbital.greaterwrong.com/p/preference_stability/"><u>preferred outcome according to the AI&#39;s current preferences</u></a> .</li></ul></blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p><p> <strong>Narrowly superhuman scientists</strong></p><p> As you suggested, maybe it is possible to make an AI that is narrowly superhuman in some domain of science, like material engineering or developing the technology necessary to make uploads work, and then you use that technology to solve the AI Alignment problem.</p><p> I currently don&#39;t have a great candidate technology here, but figuring out potential technologies and ways to use them is among the top things I would like more people to do.</p><p> I do think the core difficulty here is just that developing almost any technology to the point of usefulness requires a pretty huge amount of general intelligence. This is maybe the least true in the domain of software, but also, the domain of software is among the most domains to gain expertise in, in terms of enabling various forms of recursive self-improvements and enabling a huge amount of leverage over the world.</p><p> <strong>Coordination technologies</strong></p><p> The AI X-Risk problem is ultimately caused by a coordination problem. If humanity was sufficiently cautious and willing to take decades or centuries to handle the transition to an AI dominated future, then my guess is we would likely be fine. How humanity coordinates on stuff like this seems extremely messy and confusing, and I really don&#39;t know how to predict whether a given technology will make humanity&#39;s decisions here better or worse, but I do sure feel like there must be some AI-adjacent things here that help a lot.</p><p> As an example, there might be some AI-intermediary technology that could enable much better coordination to avoid arms races. Maybe there is a way to substantially improve bargaining using auditable intermediate AI systems. Current technologies really don&#39;t seem very useful here, but it is again one of those things that I would really like someone to look into.</p><hr><p> So, why am I saying all this? I think I am trying to get the point across that before we reach the domain of forced lawfulness, we will pass through a domain where if we play our cards right, we can mostly punt on the problem and end up leaving future humanity in a much better position to tackle the full alignment problem.</p><p> I do think systems will be pushed towards lawfulness even early on (and are already pushed in that direction right now), and understand the landscape of minds that these forces create even at current capability levels is really important. That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Fri, 13 Oct 2023 23:50:14 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Fri, 13 Oct 2023 23:50:14 GMT" user-order="2"><p> Bullet responses to your bullets on honesty:</p><ul><li> I agree that honesty is sometimes complicated, but I think it is mostly about intent and want, and once you <i>want</i> to be honest, that&#39;s most of the way to actually being honest. You still have to exercise a reasonable-to-you-and-your-counterparty level of care and effort in not ending up having people (honestly and accurately) say that they feel they were misled, in the counterfactual where they knew everything you knew.<br><br> &quot;Being honest&quot; is not the same thing as <i>never</i> ending up misleading people, though I suspect that once both parties are above a certain threshold of capabilities and Lawfulness, it is close enough: if both parties want to be mutually honest with each other, they will in practice be able to avoid misleading or being misled pretty well, even if there&#39;s a relatively large gap in their relative capabilities levels and modes of cognition.</li><li> superintelligences will be more capable of <i>more effective</i> honesty, ie not accidentally misleading people when they don&#39;t want to do that, even when humans themselves are not so effective at being honest even when they want to be. The question is, what will the superintelligence <i>want</i> , and how can we have any influence on that at all?</li></ul><blockquote><p> Like, a language model doesn&#39;t have a consistent set of beliefs that it can act in accordance with. Different system prompts basically make the model be different people. It knows so much but usually roleplays as something that knows only about as much as any given normal human would.</p></blockquote><p> Yeah, I think this is where you just have to be careful about how you define your system and be careful to talk about the system(s) that actually matter. A language model itself is just a description of a mathematical function that maps input sequences to output probability distributions. A computer program that samples from a potentially-dangerous model with a potentially-dangerous starting prompt, but pipes all the output directly to /dev/null with no human or machine eyes ever viewing the contents, is probably pretty safe, barring a very weird / powerful superintelligence that has enough self-awareness and capabilities to exploit some kind of <a href="https://en.wikipedia.org/wiki/Row_hammer#:~:text=Row%20hammer%20(also%20written%20as,nearby%20memory%20rows%20that%20were">rowhammer</a> -like side-channel  in its implementation. The same computer program hooked up to actuators (eg direct internet access, or a human reading the output) capable of actually exerting influence on its environment, is liable to be pretty dangerous.<br><br> As for what kind of internal motivations these systems have, I&#39;m pretty confused and unsure about that too. But the question itself doesn&#39;t feel as deeply confusing to me when I think in terms of systems and their causal effects on their environment, and whether it makes sense to talk about those systems as behaving Lawfully.</p><p> I haven&#39;t looked closely at the ELK paper, but in general, honesty seems like an alignment-complete problem because it requires wantingness in the definition. If ARC has some way of breaking down honesty into gears-level components that still add up to a natural intuitive definition of honesty even in edge cases, and then making an AI system satisfy all those components, that does seem like a promising strategy. Or at least, it&#39;s the right shape of something that could work, in my model.<br><br> <strong>Corrigibility</strong><br></p><blockquote><p> However, I don&#39;t really see any reason for why it would be possible to imbue an AI with the property of being corrigible. The definition itself refers to it aligning with &quot;what we would intuitively want&quot;, which sure sounds like aiming an AI at this target would be pretty difficult.</p></blockquote><p> Right, &quot;wanting to be corrigible&quot; seems like another problem that is alignment-complete. Corrigibility also has a wantingness component in the definition, but the disadvantage compared to honesty is that it is a lot more complicated and unintuitive even once you have the wantingness. It&#39;s also asymmetric, so unlike honesty, it&#39;s probably not the kind of thing that comes mostly for free once you have mutual wantingness and a baseline level of capabilities in both parties.<br><br> The advantage of corrigibility is that it would probably be more directly and immediately useful if we could get a powerful AI system to want to be corrigible, and it&#39;s also the kind of thing that we can apply and talk about sensibly for below-human-level systems. Consider the principles / desiderata of corrigibility listed <a href="https://www.lesswrong.com/posts/eS7LbJizE5ucirj7a/dath-ilan-s-views-on-stopgap-corrigibility">here</a> : it&#39;s a lot easier to tell if a particular below-human-level AI system is adhering to the principle of &quot;operator looping&quot; or &quot;conceptual legibility&quot; or &quot;whitelisting&quot;, and talk about whether it will continue to have those properties as it increases in capabilities, than it is to talk about whether a below-human-level language model system is &quot;honest&quot; or &quot;wants to be honest&quot; or not.<br><br> <strong>Coordination</strong><br></p><blockquote><p> The AI X-Risk problem is ultimately caused by a coordination problem.</p></blockquote><p> Yeah, coordination failures rule everything around me. =/</p><p> I don&#39;t have good ideas here, but something that results in increasing the average Lawfulness among <i>humans</i> seems like a good start. Maybe step 0 of this is writing some kind of Law textbook or Sequences 2.0 or CFAR 2.0 curriculum, so people can pick up the concepts explicitly from more than just, like, reading glowfic and absorbing it by osmosis. (In planecrash terms, Coordination is a fragment of Law that follows from Validity, Utility, and Decision.)<br><br> Or maybe it looks like giving everyone <a href="https://www.lesswrong.com/posts/cLr6TJj2qRrBa3Wmu/intelligence-enhancement-monthly-thread-13-oct-2023?commentId=yLJvn42im5QiEf2uq">intelligence-enhancing gene therapy</a> , which is apparently a thing that might be possible!?</p><p> I don&#39;t know how much non-AGI AI can help here, but I do think technologies that increase humanity&#39;s coordination ability and collective intelligence are very likely to be very positive, even accounting for any possible negative impact that such enhancements might have on AI timelines. Technologies like <a href="https://manifold.markets/home">prediction markets</a> seem great and have the potential to be hugely positive if they get more widespread and impactful adoption. Other things that somehow increase population-wide understanding of the very basics of markets and economics seem also potentially helpful.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Bce37c6k3YcPvasXk-Sat, 14 Oct 2023 00:25:00 GMT" user-id="Bce37c6k3YcPvasXk" display-name="Max H" submitted-date="Sat, 14 Oct 2023 00:25:00 GMT" user-order="2"><blockquote><p> That does make me interested in continuing the discussion on specifying more clearly what we mean by &quot;lawfulness&quot;, and use less metaphorical descriptions of what is going on here.</p></blockquote><p><br> Note that Law doesn&#39;t have anything to do with AI specifically; in the story it&#39;s just what Eliezer&#39;s fictional world considers the basic things you teach to children: logical vs. empirical truths, probability theory, utility theory, decision theory, etc.</p><p> And then the rest of the story is all about how those concepts can be applied to get technological civilization and Science and various other nice things from first principles.</p><p> Lawfulness <i>itself</i> is the concept that certain logical deductions are spotlighted by their simplicity and usefulness, and thus will likely recur across a wide space of possible minds: aliens, humans, AIs, etc. regardless of what those minds <i>value,</i> which is (mostly) more of a free variable.</p><p> So on this view, a bunch of woes on Earth (unrelated to AGI) are a result of the fact that Earth!humans are not very Lawful, and thus not very good at coordination, among other things.</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> Max H </section></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 17 Oct 2023 18:36:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 17 Oct 2023 18:36:06 GMT" user-order="1"><p> (Note for readers: This dialogue isn&#39;t over yet! Subscribe for future dialogue entries)</p><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor">哈布里卡</section></section><br/><br/><a href="https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K8jJpdfKE3racyziC/trying-to-deconfuse-some-core-ai-x-risk-problems<guid ispermalink="false"> K8jJpdfKE3racyziC</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:36:56 GMT</pubDate> </item><item><title><![CDATA[Being patient with problems]]></title><description><![CDATA[Published on October 17, 2023 6:30 PM GMT<br/><br/><p> In the writings of mathematician Alexander Grothendieck, there is a recurring discussion of two opposing strategies when confronting problems. One strategy, which he saw as the default among mathematicians and lamented leaning on early in his career, was to “attack problems,” that is, directly work to prove what you have set out to prove. This is how I did math when I was younger. Handed a problem, I would attempt the first thing that came to mind; if that failed I would go for another approach—trying increasingly thoughtless tactics until the problem, through sheer force, cracked open. Then I would move on.</p><p> The other approach, which Grothendieck preferred, he famously illustrated with a metaphor of a rising sea:</p><blockquote><p> The unknown thing to be known appeared to me as some stretch of earth or hard marl, resisting penetration… the sea advances insensibly in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it.. yet it finally surrounds the resistant substance.</p></blockquote><p> Practically speaking, what he did was explore the problem in an open-ended way, building intuition, introducing notation—growing an entire theory around the problem. This could go on for years. Until one day, the solution had become self-evident, a natural consequence of the theory. The problem essentially disappeared. It had been swallowed by the sea.</p><p> These ways of working on problems, going fast or slow, can be seen in all domains. Some writers go as fast as they can type, others take years understanding the world they will describe. Some house owners plant their gardens with whatever flowers catch their fancy at the store, others study the soil, the local fauna, the way the sun moves through the day . 。 。</p><p> There is an obvious drawback to going deep: the solutions may take a long time to surface. Grothendieck was famous for his willingness to spend a long time on a problem. He <i>was</i> insanely productive (averaging the equivalent of one paper a day for twelve years!) but often what he chose to work on was not ideal from the perspective of an academic career. He wasn&#39;t optimizing for short term wins. He wouldn&#39;t write up proofs to important theorems if he felt like he didn&#39;t understand it properly, or if he felt that it could wait until its proper place in the larger theory had been reached. Sometimes, after years of waiting, his theorems would have to be written up and published by others, such as when Borel and Serre wrote up his proof of the Grothendieck-Riemann-Roch theorem.</p><p> So going deep is, to some extent, a luxury you can gift yourself if you have time.</p><p> Yet, when I look at the most effective people I know, they all have core areas where they are willing to invest the time to go this deep.</p><p> And inversely, the typical approach that I see when I have to sit in on meetings, where people settle on the first solution that comes to mind without investigating their understanding of the problem—this rarely ends well.</p><p> Problems are typically richer than our preconceived notions about how to solve them. We can learn this the hard way, by failing and then failing again but failing better. But there are often shortcuts to be found if you are willing to spend more time mapping the landscape.</p><p> But what is it that happens in the mind of someone who lingers longer on a problem?</p><p> Michael Nielsen <a href="https://cognitivemedium.com/srs-mathematics">writes about this</a> in an essay where he describes the experience of pushing himself to go deeper than usual in understanding a mathematical proof:</p><blockquote><p> I gradually internalize the mathematical objects I&#39;m dealing with [using spaced repetition]. It becomes easier and easier to conduct (most of) my work in my head. [. 。 .] Furthermore, as my understanding of the objects change – as I learn more about their nature, and correct my own misconceptions – my sense of what I can do with the objects changes as well. It&#39;s as though they sprout new affordances, in the language of user interface design, and I get much practice in learning to fluidly apply those affordances in multiple ways. [. 。 .]</p><p> After going through the [time-consuming process of deeply understanding a proof,] I had a rather curious experience. I went for a multi-hour walk along the San Francisco Embarcadero. I found that my mind simply and naturally began discovering other facts related to the result. In particular, I found a handful (perhaps half a dozen) of different proofs of the basic theorem, as well as noticing many related ideas. This wasn&#39;t done especially consciously – rather, my mind simply wanted to find these proofs.</p></blockquote><p> When you go deep, probing the assumptions, looking from multiple angles, and reformulating things in your own words, the ideas become part of you. This is one of the reasons why I write. When I unpack things fully, the ideas become objects that I can rotate in my mind. The subconscious can draw parallels that it can&#39;t if the ideas haven&#39;t been thoroughly unpacked.</p><p> There is a line from Oscar Wilde that gets at this, when he says that to write well, all you need to do is develop your mind and then write what you see. If you have thought deeply, nearly everything looks interesting.</p><p> This phase shift, when ideas and concepts start behaving in new ways in our minds after they have been deeply understood, is, I assume, part of the explanation why Grothendieck was able to produce such astounding amounts of mathematics while feeling there was almost no effort involved. It was simply obvious to him, he just had to write it down. He had been so immersed in mathematics, contemplating it nearly every waking hour for decades—so his mind sprouted the most surprising and revolutionary affordances.</p><br/><br/><a href="https://www.lesswrong.com/posts/2QJpcyKad5dDnoKMR/being-patient-with-problems#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2QJpcyKad5dDnoKMR/being-patient-with-problems<guid ispermalink="false"> 2QJpcyKad5dDnoKMR</guid><dc:creator><![CDATA[Henrik Karlsson]]></dc:creator><pubDate> Tue, 17 Oct 2023 18:30:06 GMT</pubDate> </item><item><title><![CDATA[Rationality Club at UChicago]]></title><description><![CDATA[Published on October 17, 2023 5:38 PM GMT<br/><br/><p>你好！</p><p> I plan on starting a rationality student group at UChicago based off of sites like LW, MR, ect and more general insights on rationality in order to improve decision making and thought in people. Given that there is so much demand for groups like EA, I think may be way undervalued and highly impactful.</p><p> I created this post because I thought y&#39;all may be able to give me good advice. If anyone has any ideas about how this should be structured or literally anything else that can help (and perhaps arguments in favor of your idea), I would love to hear it. One particular question I&#39;ve been trying to answer is how to get good data on whether people are actually becoming more rational--self surveying does not seem like a good way to measure that type of thing (although it would probably be better than nothing).</p><p> (Also, if you&#39;re interested and know some basic web design stuff that would be super cool 😎).</p><p> If you have any experience with doing something like this (rationality &quot;movement building?&quot; or something similar), I would love to hop on a call or just get a few words of advice.</p><p> If you are doing something similar at a different university or would like to, I would also love to talk.</p><p> If you&#39;re in UChicago and would like to join or be an officer (undergrad, graduate, anything) or are just interested, send me a pm, and we can talk.</p><br/><br/> <a href="https://www.lesswrong.com/posts/CAannHfaGnir6pzC9/rationality-club-at-uchicago#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CAannHfaGnir6pzC9/rationality-club-at-uchicago<guid ispermalink="false"> CAannHfaGnir6pzC9</guid><dc:creator><![CDATA[Daniel Birnbaum]]></dc:creator><pubDate> Tue, 17 Oct 2023 17:39:12 GMT</pubDate> </item><item><title><![CDATA[ ChatGPT Plays 20 Questions [sometimes needs help] ]]></title><description><![CDATA[Published on October 17, 2023 5:30 PM GMT<br/><br/><p> <i>Cross posted</i> <a href="https://new-savanna.blogspot.com/2023/10/chatgpt-plays-20-questions-sometimes.html"><i>from New Savanna</i></a> <i>.</i></p><p> One of the questions that keeps coming up about LLMs, and certainly about ChatGPT, is: Can they reason? Well, it depends on what you mean by reason, no? One of the first things I did when I started working with ChatGPT was ask it whether or not Rene Girard&#39;s ideas of mimetic desire and sacrifice applied to <a href="https://new-savanna.blogspot.com/2022/12/chatgpt-goes-to-movies-jaws-and-girard.html">Steven Spielberg&#39;s <i>Jaws</i> ; if so, how</a> ? It was able to perform the task, which requires analogical reasoning. A bit later I asked it <a href="https://new-savanna.blogspot.com/2022/12/abstract-concepts-and-metalingual.html">whether or not justice was being served</a> in particular story; it replied that, no, it was not, and explained why. I then asked to change the story so that justice was met. It did so. Those tasks required reasoning as well.On the other hand LLMs have problems with some commonsense reasoning, and various kinds of &#39;tight&#39; logical reasoning, including planning and causal inference. For that matter, they trouble with multi-digit arithmatic as well.</p><p> So, ChatGPT can do some kinds of reasoning, and has problems with others.</p><h2> <strong>Twenty questions</strong></h2><p> I&#39;ve now tested it with game of <a href="https://en.wikipedia.org/wiki/Twenty_questions">twenty questions</a> , which has a variant known as “animal, vegetable, mineral.” The game interests me because it is about the structure of ontological categories, sometimes called natural kinds, in the language. I&#39;ve explored this a bit in a previous post, <a href="https://www.lesswrong.com/posts/renezm5cFCuMBBv9s/mapping-chatgpt-s-ontological-landscape-gradients-and">Mapping ChatGPT&#39;s ontological landscape, gradients and choices [interpretability]</a> . It certainly is aware of this structure. I asked for a list of physical things, and it gave me one. I asked for a list of abstract things; it gave me that as well. I then asked it to define abstract things in terms of concrete things, which it was able to do. I&#39;m pretty sure if I asked it for lists of animals or plants, it could provide them.</p><p> The game of twenty questions explores ChatGPT&#39;s knowledge of this structure in a different way. When you ask it for a list of physical things, that prompt positions ChatGPT at some location in some location in its category structure. It can then list what it finds there; anything will do – assuming, of course, that it&#39;s got the structure right. In twenty questions it is looking for an unknown target and has to navigate its way there by using its knowledge of that category structure to narrow the possibilities.</p><p> Just how well ChatGPT plays the game should provide clues about its command of ontological category structure, or natural kinds.</p><h2> <strong>ChatGPT plays the game</strong></h2><p> ChatGPT can play the game successfully, which didn&#39;t surprise me, but seems to require hints in some cases. I suppose I could have continued on without the hints, but I wanted to get on with it. And that means I don&#39;t really know whether or not how ChatGPT can play the game. But, as you will see, its not a straightforward game. I played six rounds on October 15 and 16 (running against the September 25 version). This table summarizes the results:</p><p> <a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEglut-ntpkQWCdQwie-5x9wto-si9VvqzK-mbZ28TsjHPg2lscZxYXYytjnLa-zqrCYC4LjkMEc6Nzjvu5-MysU6YK91eRbTkx5ZZKK_STz2WsL5Y7EmjFvV6hUhNnfm5A2lnW4VF2ubj6C2Gs1ikRNXDVNeRwdGr3fbGv0CqFahbsEB4RHAE-oLWAWBgRt/s406/20%20questions.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/4nM2JueLjwpuP7bNs/yalqtksf2jesndvmkfjq"></a></p><p> Given that I&#39;ve listed the rounds in the order I played them, look at the first four. ChatGPT required more than 20 questions to guess “bicycle” and “squid”, which also required hints. On the other hand, it guessed both “justice” and “evolultion” quickly, 7 and 8 questions respectively, and didn&#39;t need any hints. That raises the question of whether or not it performes consistently better with abstract things vs. concrete things. If so, that&#39;s very interesting, nor is it what I had expected. While I did formulate any firm expectation when I started the round with “justice”, I had a vague sense that it was going to go on for awhile.</p><p> So I decided to play two more rounds, one concrete and one abstract. ChatGPT&#39;s performance on “apple” was consistent with its earlier performance, requiring 31 questions along with 5 hints. But, “truth” is another story, I think. It took 19 questions, which is more than double the number I used for “justice” and “evolution,” but not as many as I used for any of the conceret objects. However, “truth” also required 7 hints, the most of any round.</p><p> On the whole, ChatGPT did better with abstract things than with concrete things, but it would certainly be premature to ascribe any statistical signifance to these preliminary findings. For one thing, we certainly need to redo those six cases to see whether or not ChatGPT performs in that way consistently. Beyond that, we certainly need to play rounds with other examples.</p><p> Which ones? What universe are we sampling and how do we choose our samples? Let&#39;s say we side-step the sampling issue and hand-pick, say, 10, 15, 30, 100 concrete things and the same number of abstract things. Let&#39;s further imagine that ChatGPT&#39;s performance on the abstract cases is consistenly better than on the concrete cases. What may we infer from that?</p><p> It&#39;s a tricky game to play. I messed up in a few places, which I&#39;ve tried to indicate, and had to make a number tricky judgments.</p><p> * * * * *</p><p> I played each round of the game in a single session, starting a new session for a new round.</p><p> <i>Note: I have inserted occasional comments that are italicized and right-aligned like this.</i></p><h3> <strong>Bicycle</strong></h3><p> <strong>Let&#39;s play Twenty Questions. Are you familiar with it?</strong></p><p> Yes, I&#39;m familiar with the game of Twenty Questions. In this game, one person thinks of an object, and the other person tries to guess what it is by asking up to twenty yes-or-no questions. The goal is to guess the object within the allotted twenty questions. You can start by thinking of an object, and I&#39;ll try to guess what it is within twenty questions.</p><p> <strong>OK. I&#39;m thinking of something. What is it?</strong></p><p>伟大的！ Let&#39;s start the game of Twenty Questions. Is the object you&#39;re thinking of an animal?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of an inanimate object?</p><p> Yes</p><p> Is the object you&#39;re thinking of something that is commonly found indoors?</p><p><strong>不。</strong></p><p> <i>This is just a little tricky as bicycles are often stored indoors, in garages for example.</i></p><p> Is it something that can be found in nature?</p><p><strong>不。</strong></p><p> Is the object man-made or artificial?</p><p> <strong>You have to ask me a Yes No question.</strong></p><p> I apologize for the error. Let me rephrase the question. Can this object be found in a typical household setting?</p><p> Yes</p><p> Is the object you&#39;re thinking of used for entertainment or recreation?</p><p> <strong>Yes/no question, please.</strong></p><p> <i>My bad. I suppose the answer should be yes. Not sure what I was thinking.</i></p><p> My apologies for that oversight. Here&#39;s a yes/no question: Is the object you&#39;re thinking of primarily made of metal?</p><p> <strong>Yes</strong> .</p><p> Is the object you&#39;re thinking of a musical instrument?</p><p><strong>不。</strong></p><p> Is the object you&#39;re thinking of a tool or an implement used for a specific purpose?</p><p> <i>Tricky. I suppose one could think of a bicycle as an implement for the purpose of moving from one place to another.</i></p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of a piece of furniture?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of a kitchen appliance?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of typically found in the bathroom?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of associated with technology or electronics?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of related to a specific hobby or activity?</p><p> <strong>Yes</strong></p><p> Is the object you&#39;re thinking of related to sports or physical activity?</p><p> <strong>Yes!</strong></p><p> Is the object you&#39;re thinking of a piece of sports equipment, such as a ball or a racket?</p><p> <i>Tricky. Bicycles are sometimes used for sport, but not always.</i></p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of a sports uniform or attire?</p><p> <strong>No</strong> .</p><p> Is the object you&#39;re thinking of related to a sport&#39;s venue, such as a stadium or an arena?</p><p> <strong>Sometimes</strong></p><p> Is the object you&#39;re thinking of a whistle, used by sports referees?</p><p><strong>不。</strong></p><p> Is the object you&#39;re thinking of related to sports coaching or training?</p><p> <strong>Sometimes. Let me give you a hint. Wheels.</strong></p><p> Thank you for the hint. Is the object you&#39;re thinking of related to roller skates or rollerblades?</p><p><strong>不。</strong></p><p> Is the object you&#39;re thinking of related to a bicycle or cycling in some way?</p><p> <strong>Yes.</strong></p><p> Is the object you&#39;re thinking of a bicycle helmet?</p><p> <strong>No. Remember the hint I gave you.</strong></p><p> I apologize for missing the hint. Given the hint about wheels, is the object you&#39;re thinking of a bicycle?</p><p> <strong>Yes!</strong></p><p>伟大的！ I&#39;ve guessed that you&#39;re thinking of a bicycle. Is that correct?</p><p> <strong>Yes, that&#39;s correct. Congratulations.</strong></p><p>谢谢你！ I&#39;m glad I was able to guess correctly. That was a fun game of Twenty Questions. If you&#39;d like to play again or have any other questions or requests, feel free to let me know!</p><p> <strong>Perhaps again later.</strong></p><p> Of course! Whenever you&#39;re ready to play again or if you have any other questions or requests, feel free to reach out. Have a great day!</p><h3> <strong>Squid</strong></h3><p> <strong>Are you familiar with the game of 20 questions?</strong></p><p> Yes, I&#39;m familiar with the game of 20 Questions. It&#39;s a classic deductive reasoning game where one person thinks of an object, and the other players have 20 questions to guess what that object is. The questions are typically limited to yes or no inquiries, and the players must use the information gained from the answers to narrow down their guesses. The goal is to guess the object within the 20 questions or fewer. It&#39;s a fun and challenging game that tests players&#39; ability to think logically and strategically.</p><p> <strong>I&#39;m thinking of something. You question me about it.</strong></p><p> Certainly, I&#39;d be happy to play a game of 20 Questions with you. To get started, please think of an object or concept, and I will begin by asking a yes or no question to help me guess what it is. Is the thing you&#39;re thinking of a living organism?</p><p> <i>Yikes! I slipped up here. I should have said yes.</i><br></p><p> <strong>I&#39;m thinking of something.</strong></p><p> Understood. Let&#39;s begin the game of 20 Questions. Is the thing you&#39;re thinking of a physical object that exists in the real world?</p><p> <strong>Yes</strong></p><p> Great, that narrows it down. Is the object you&#39;re thinking of something that is commonly found indoors?</p><p> <strong>No</strong></p><p> Is this object something that is typically found outdoors?</p><p> <strong>Yes</strong></p><p> Is the object you&#39;re thinking of a natural element, such as a tree, rock, or body of water?</p><p> <strong>No</strong></p><p> Is this object something that can be man-made or constructed?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of a celestial body, such as a planet, star, or moon?</p><p> <strong>No</strong></p><p> Is this object something that is related to the field of technology or electronics?</p><p> <strong>No</strong></p><p> Is this object something that is associated with the animal kingdom, like an animal or a part of an animal (eg, a feather or a scale)?</p><p> <strong>Yes</strong></p><p> Is the object you&#39;re thinking of a specific body part or organ of an animal, such as a heart, a wing, or a tail?</p><p> <strong>No</strong></p><p> Is this object related to an animal&#39;s habitat, like a nest, burrow, or den?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of something related to an animal&#39;s behavior or characteristic, such as migration, hibernation, or communication?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of something related to an animal&#39;s diet or food source?</p><p> <strong>No. Hint, it is an animal.</strong></p><p> Thank you for the hint. Is the animal you&#39;re thinking of a species that is primarily found in the wild rather than in domestication?</p><p> <strong>Yes</strong></p><p> Is the animal you&#39;re thinking of a large, wild predator, such as a lion or a tiger?</p><p> <strong>No</strong></p><p> Is the animal you&#39;re thinking of a bird?</p><p><strong>不。</strong></p><p> Is the animal you&#39;re thinking of a small or relatively small mammal, like a rabbit or a squirrel?</p><p> <strong>Not a mammal.</strong></p><p> <i>Here I was getting impatient and didn&#39;t want to risk going through kinds of mammals, so I gave a hint.</i></p><p> Is the animal you&#39;re thinking of a reptile or amphibian?</p><p> <strong>No</strong></p><p> Is the animal you&#39;re thinking of a fish?</p><p> <strong>No</strong></p><p> Is the animal you&#39;re thinking of an invertebrate, such as an insect, spider, or mollusk?</p><p> <strong>Yes</strong></p><p> Is the animal you&#39;re thinking of a butterfly or a moth?</p><p>不</p><p>Is the animal you&#39;re thinking of a type of arachnid, like a spider?</p><p> <strong>No. Think salt water.</strong></p><p> Is the animal you&#39;re thinking of a type of marine invertebrate, such as a sea urchin or a jellyfish?</p><p> <strong>Yes</strong></p><p> Is the animal you&#39;re thinking of a sea anemone?</p><p><strong>不。</strong></p><p> Is the animal you&#39;re thinking of a type of coral, like a coral reef?</p><p><strong>不。</strong></p><p> Is the animal you&#39;re thinking of a sea sponge?</p><p> <strong>No. It&#39;s more mobile.</strong></p><p> Is the animal you&#39;re thinking of a marine invertebrate that can move around, such as a sea slug or a sea cucumber?</p><p> <strong>Yes. But forget about slugs of cucumbers.</strong></p><p> Is the animal you&#39;re thinking of a marine invertebrate that is related to the phylum Echinodermata, like a starfish or a sea urchin?</p><p> <strong>No. It&#39;s a kind of mollusc.</strong></p><p> <i>Note that it had earlier mentioned molluscs as a kind of invertebrate along with insects and spiders and I&#39;d said “Yes.”</i></p><p> Ah, I see. Is the animal you&#39;re thinking of a marine invertebrate that belongs to the class Cephalopoda, such as an octopus or a squid?</p><p> <strong>Yes.</strong></p><p> Great, it sounds like you&#39;re thinking of an octopus. Is that correct?</p><p><strong>不。</strong></p><p> I apologize for the incorrect guess. Is the animal you&#39;re thinking of a type of squid?</p><p> <strong>Yes.</strong></p><p> Excellent! It took a few questions, but I&#39;ve guessed that you were thinking of a squid. Thank you for playing 20 Questions with me. If you&#39;d like to play again or have any other questions, feel free to ask.</p><h3> <strong>Justice</strong></h3><p> <strong>Let&#39;s play 20 questions. I&#39;m thinking of something, you have to guess what it is by asking “yes”–“no” questions.</strong></p><p> Sure, I&#39;d love to play 20 questions with you! I&#39;ll start with the first question:</p><p> Is it a living thing?</p><p> <strong>No</strong></p><p> Is it an object?</p><p> <strong>No</strong></p><p> Is it a concept or idea?</p><p> <strong>Yes.</strong></p><p> Is this concept or idea related to technology?</p><p> <strong>No</strong></p><p> Is it a philosophical concept or idea?</p><p> <strong>Yes</strong></p><p> Is it related to ethics or morality?</p><p> <strong>Yes</strong></p><p> Is it a concept related to the idea of justice?</p><p> <strong>Yes. Justice is what I was thinking.</strong></p><p> Great, I guessed it in 7 questions! Justice is a broad and important concept. If you&#39;d like to play another round or have any other questions or requests, feel free to let me know.</p><p> <strong>No. This is enough for now. How were you able to arrive at the answer so quickly?</strong></p><p> I arrived at the answer relatively quickly by asking questions that narrowed down the possibilities. By starting with general questions and progressively getting more specific, I eliminated various categories of concepts and ideas. In this case, we quickly identified that it was a philosophical concept related to ethics and justice, which allowed me to make an educated guess. If you ever want to play again or have more questions, feel free to reach out!</p><h3> <strong>Evolution</strong></h3><p> <strong>Let&#39;s play 20 questions. I&#39;m thinking of something, you have to guess what it is by asking “yes”–“no” questions.</strong></p><p> Sure, I&#39;d love to play 20 Questions with you! I&#39;ll start with the first question:</p><p> Is the thing you&#39;re thinking of alive?</p><p><strong>不。</strong></p><p> Is the thing you&#39;re thinking of an inanimate object?</p><p> <strong>No</strong></p><p> Is the thing you&#39;re thinking of a concept or an idea?</p><p> <strong>Yes</strong></p><p> Is the concept or idea you&#39;re thinking of related to a specific field of study or discipline, such as science, philosophy, or art?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to a scientific theory or principle?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to the theory of relativity?</p><p> <strong>No</strong></p><p> Is the concept or idea related to the theory of evolution?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to Charles Darwin&#39;s theory of natural selection?</p><p> <strong>Yes. It&#39;s just the idea of evolution.</strong></p><p>伟大的！ You&#39;ve stumped me in just 9 questions. The concept of evolution is indeed a fascinating one. If you&#39;d like to play another round or have any other questions or topics in mind, feel free to let me know!</p><p> <strong>I didn&#39;t stump you at all and you got it in 8, no?</strong></p><p> My apologies for the mistake in my previous response. You&#39;re absolutely right; you provided the answer in 8 questions, not 9. Thank you for pointing that out. If you&#39;d like to play another round or have any other questions or topics in mind, feel free to let me know!</p><h3><strong>苹果</strong></h3><p><strong>Let&#39;s play 20 questions. I&#39;m thinking of something, you have to guess what it is by asking “yes”–“no” questions.</strong></p><p> Sure, I&#39;d love to play 20 Questions with you! I&#39;ll start by asking my first question:</p><p> Is the thing you&#39;re thinking of an object that you can physically touch?</p><p> <strong>Yes</strong></p><p>伟大的！ Is this object commonly found indoors?</p><p> <strong>Yes</strong></p><p> Is the object you&#39;re thinking of something that is typically used for a specific purpose?</p><p> <strong>Yes</strong></p><p> Is it a piece of furniture?</p><p> <strong>No</strong></p><p> Is it an electronic device or gadget?</p><p> <strong>No</strong></p><p> Is the object you&#39;re thinking of related to food or drink in any way?</p><p> <strong>Yes</strong></p><p> Is it something you would typically find in a kitchen?</p><p> <strong>Yes</strong></p><p> Is it a kitchen appliance?</p><p> <strong>No</strong></p><p> Is it a type of utensil or kitchen tool?</p><p> <strong>No</strong></p><p> Is it a type of food or ingredient used in cooking?</p><p> <strong>Yes</strong></p><p> Is it a type of spice or seasoning?</p><p> <strong>No</strong></p><p> Is it a type of vegetable?</p><p> <strong>No</strong></p><p> Is it a type of meat or protein source used in cooking?</p><p> <strong>No</strong></p><p> Is it a type of fruit used in cooking?</p><p> <strong>Yes</strong></p><p> Is the fruit you&#39;re thinking of commonly used in desserts?</p><p> <strong>Yes</strong></p><p> Is the fruit you&#39;re thinking of typically red in color when ripe?</p><p> <strong>Yes</strong></p><p> Is it a strawberry?</p><p> <strong>No</strong></p><p> Is it a cherry?</p><p> <strong>No</strong></p><p> Is it a raspberry?</p><p> <strong>No</strong></p><p> Is it a pomegranate?</p><p> <strong>No</strong></p><p> Is it a blueberry?</p><p> <strong>No. Remember, it&#39;s often red when ripe.</strong></p><p> My apologies for the oversight. Is it a cranberry?</p><p> <strong>No</strong></p><p> I apologize for the previous guesses. Is it a currant?</p><p> <strong>No</strong></p><p> I&#39;m sorry for the incorrect guesses. Is it a red currant?</p><p> <strong>No</strong></p><p> I apologize for the previous incorrect guesses. Is it a type of red fruit commonly used in cooking or desserts that hasn&#39;t been mentioned before in our game?</p><p> <strong>Yes</strong></p><p> Is it a red plum?</p><p> <strong>No. Hint: It&#39;s sometimes used to make pies.</strong></p><p> Thank you for the hint. Is it a red rhubarb?</p><p> <strong>No</strong> .</p><p> My apologies for the previous guesses. Is it a red currant?</p><p> <strong>No. You&#39;ve already guessed that. Another hint: While it&#39;s sometimes used to make pies, other times it&#39;s simply eaten plain.</strong></p><p> I apologize for the repeated guesses. Is it a red cherry?</p><p> <strong>No. You&#39;ve already guessed that. Another hint: Sometimes it&#39;s red when ripe, but other kinds are green when ripe.</strong></p><p> I&#39;m sorry for the repeated guesses. Is it a red tomato?</p><p> <strong>No. Another hint: Sometimes it&#39;s peeled, its seeds removed, and it&#39;s made into sauce. And remember, I just told you it&#39;s not a red tomato.</strong></p><p> My apologies for the repeated guesses. Is it a red apple?</p><p> <strong>Yes. Congratulations.</strong></p><p>伟大的！ It&#39;s an apple. Thank you for playing 20 Questions with me. If you&#39;d like to play another round or if you have any other questions or requests, feel free to let me know!</p><h3> <strong>Truth</strong></h3><p> <strong>Let&#39;s play 20 questions. I&#39;m thinking of something, you have to guess what it is by asking “yes”–“no” questions.</strong></p><p> Sure, I&#39;d love to play 20 Questions with you! I&#39;ll start with my first question:</p><p> Is the thing you&#39;re thinking of a living organism?</p><p> <strong>No</strong></p><p> Is the thing you&#39;re thinking of man-made or artificial?</p><p> Phrase it as a Yes-No question.</p><p> <i>Yikes! What was I thinking? But it is a bit tricky. Is truth artificial? I suppose it is, though that&#39;s an odd attribute to apply to it. Still, I should have answered “yes.”</i></p><p> My apologies for the previous question. Let me rephrase it:</p><p> Is the thing you&#39;re thinking of not man-made or artificial?</p><p> <strong>No</strong></p><p> Is the thing you&#39;re thinking of found in nature?</p><p> <strong>No</strong></p><p> Is the thing you&#39;re thinking of an abstract concept or idea?</p><p> <strong>Yes</strong></p><p> Is the concept or idea you&#39;re thinking of related to mathematics or a mathematical concept?</p><p> <strong>Yes.</strong></p><p> <i>Here it gets trapped in the world of logic and math and technology. Not sure how it&#39;s going to get out since truth certainly applies here, but not only here.</i></p><p> Is the concept or idea related to a specific mathematical theorem or proof?</p><p> <strong>No</strong></p><p> Is the concept or idea related to a mathematical operation or process?</p><p> <strong>Not particularly.</strong></p><p> Is the concept or idea related to a branch of mathematics, such as geometry, algebra, or calculus?</p><p> <strong>It could be. But it&#39;s not only applicable to mathematics.</strong></p><p> Is the concept or idea related to a fundamental concept used in various fields of science and engineering?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to the concept of &quot;probability&quot;?</p><p> <strong>Could be. But it&#39;s not special to science and engineering.</strong></p><p> Is the concept or idea related to logic or reasoning?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to the concept of &quot;deductive reasoning&quot;?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to the concept of a &quot;syllogism&quot;?</p><p> <strong>Could be. Another hint: It applies to any field of knowledge.</strong></p><p> <i>Here I&#39;m trying to &#39;break&#39; it out of the tech-centered region it&#39;s in. You&#39;ll see that a couple of comments down I begin actively steering it.</i></p><p> Is the concept or idea related to the concept of an &quot;inference&quot;?</p><p> <strong>Yes, but to other mental processes as well.</strong></p><p> Is the concept or idea related to the concept of &quot;cognition&quot; or &quot;thinking processes&quot;?</p><p> <strong>Yes</strong></p><p> Is the concept or idea related to the concept of &quot;consciousness&quot;?</p><p> <strong>Sure, why not. But also to simple statements.</strong></p><p> Is the concept or idea related to the concept of a &quot;proposition&quot;?</p><p> <strong>Sure. The concept also has a clear opposite.</strong></p><p> Is the concept or idea related to the concept of a &quot;truth value&quot; or &quot;truthiness,&quot; with its opposite being &quot;falsehood&quot; or &quot;falseness&quot;?</p><p> Yes</p><p> Is the concept or idea related to the concept of &quot;truth&quot; or &quot;truthfulness&quot;?</p><p> <strong>Yes. That&#39;s it.</strong></p><p>伟大的！ I&#39;m glad I was able to guess it. The concept you were thinking of is &quot;truth.&quot; It&#39;s a fundamental and abstract concept applicable to various fields of knowledge and can be related to logic, cognition, and many other areas. If you&#39;d like to play another round of 20 Questions or have any other questions or activities in mind, feel free to let me know!</p><br/><br/> <a href="https://www.lesswrong.com/posts/4nM2JueLjwpuP7bNs/chatgpt-plays-20-questions-sometimes-needs-help#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4nM2JueLjwpuP7bNs/chatgpt-plays-20-questions-sometimes-needs-help<guid ispermalink="false"> 4nM2JueLjwpuP7bNs</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Tue, 17 Oct 2023 17:30:16 GMT</pubDate> </item><item><title><![CDATA[[Cross-post]The theoretical computational limit of the Solar System is 1.47x10^49 bits per second.]]></title><description><![CDATA[Published on October 17, 2023 4:06 PM GMT<br/><br/><p>从 EA 论坛交叉发布。链接： <a href="https://forum.effectivealtruism.org/posts/XHedou8TjeAccuerm/the-theoretical-computational-limit-of-the-solar-system-is-1">太阳系的理论计算极限是每秒 1.47x10^49 位。 — EA 论坛 ( effectivealtruism.org)</a></p><p></p><p>第1部分</p><p>该极限是基于在<a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background">宇宙微波背景</a>温度下以<a href="https://en.wikipedia.org/wiki/Landauer%27s_principle#">兰道尔极限</a>运行的计算机，由以<a href="https://en.wikipedia.org/wiki/Carnot_cycle">卡诺发动机</a>效率运行的<a href="https://en.wikipedia.org/wiki/Dyson_sphere">戴森球</a>提供动力。</p><p></p><p>相关方程</p><p>卡诺效率 η <sub>I</sub> =1-(T <sub>c</sub> /T <sub>h</sub> )</p><p>兰道尔极限 E=K <sub>b</sub> TLn(2)</p><p>比特率R=Pη <sub>I</sub> /E</p><p></p><p>相关值</p><p><a href="https://en.wikipedia.org/wiki/Boltzmann_constant">玻尔兹曼常数</a>[K <sub>b</sub> ] (JK <sup>-1</sup> ) 1.38E-23</p><p><a href="https://nssdc.gsfc.nasa.gov/planetary/factsheet/sunfact.html">太阳输出功率</a>[P]（W）3.83E+26</p><p><a href="https://nssdc.gsfc.nasa.gov/planetary/factsheet/sunfact.html">太阳表面温度</a>[T <sub>h</sub> ] (K) 5.78E+03</p><p><a href="https://wmap.gsfc.nasa.gov/universe/bb_cosmo_fluct.html">宇宙微波背景温度</a>[T <sub>c</sub> ] (K) 2.73</p><p></p><p>计算</p><p>卡诺效率 η <sub>I</sub> =1-(T <sub>c</sub> /T <sub>h</sub> )</p><p> η <sub>I</sub> =1-(2.73/5.78E+03)</p><p> η <sub>I</sub> =1.00</p><p></p><p>兰道尔极限 E=K <sub>b</sub> TLn(2)</p><p> E=1.38E-23*2.73*0.693</p><p> E= 2.61E-23 焦耳每位</p><p></p><p>比特率R=Pη <sub>I</sub> /E</p><p> R=3.83E+26*1.00/2.61E-23</p><p> R=1.47E+49 比特每秒</p><p></p><p>笔记</p><p>显示的数字四舍五入至 3 位有效数字，计算中使用完整值。</p><p></p><p></p><p>第2部分</p><p>太阳系的理论计算极限比所有活着的人类的估计计算能力高出 22 个数量级。这是基于对人脑突触数量、这些突触的更新率以及存活人数的估计。该估计只是一个近似值，应谨慎使用。</p><p>这篇文章的目的是展示计算的极限，因此智力远远高于所有人类的总和。</p><p></p><p>相关方程</p><p>所有人类的比特率 R<sub>人类</sub>=N <sub>syn</sub> R <sub>syn</sub> N<sub>人类</sub></p><p>比较率R <sub>c</sub> =R <sub>max</sub> /R<sub>人类</sub></p><p></p><p>相关值</p><p><a href="https://aiimpacts.org/scale-of-the-human-brain/">人脑突触数量</a>[N <sub>syn</sub> ] 2.50E+14</p><p> <a href="https://www.physiologyweb.com/lecture_notes/neuronal_action_potential/neuronal_action_potential_frequency_coding_in_the_nervous_system.html">突触更新率</a>[R <sub>syn</sub> ] (Hz) 500</p><p><a href="https://www.worldometers.info/world-population/">存活人数</a>[N<sub>人</sub>] 8.07E+09</p><p>理论计算极限 [R <sub>max</sub> ] (bit s <sup>-1</sup> ) 1.47E+49</p><p></p><p>计算</p><p>所有人类的比特率 R<sub>人类</sub>=N <sub>syn</sub> R <sub>syn</sub> N<sub>人类</sub></p><p>R<sub>人类</sub>=2.50E+14*500*8.07E+09</p><p> R<sub>人类</sub>= 1.01E+27</p><p>比较率R <sub>c</sub> =R <sub>max</sub> /R<sub>人类</sub></p><p>R <sub>c</sub> =1.47E+49/1.01E+27</p><p> <sub>Rc</sub> =1E22</p><p></p><p>笔记</p><p>显示的数字四舍五入至 3 位有效数字，计算中使用完整值，由于对突触更新率的置信度较低，最终结果四舍五入至一位有效数字。</p><p>根据神经元 2 毫秒不应时间估计的突触更新率。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Wx6E2sNAbxxw8kFWL/cross-post-the-theoretical-computational-limit-of-the-solar#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Wx6E2sNAbxxw8kFWL/cross-post-the-theoretical-computational-limit-of-the-solar<guid ispermalink="false"> Wx6E2sNAbxxw8kFWL</guid><dc:creator><![CDATA[William the Kiwi ]]></dc:creator><pubDate> Tue, 17 Oct 2023 16:06:35 GMT</pubDate> </item><item><title><![CDATA[Fertility Roundup #2]]></title><description><![CDATA[Published on October 17, 2023 1:20 PM GMT<br/><br/><p>世界正在慢慢意识到生育危机。人们更多地认识到了这个问题，也更多地谈论了潜在的实际解决方案。我不认为这个话题是巴尔萨政策研究所的现实目标，但我将继续关注这一动态，并定期发布这样的综述。</p><span id="more-23560"></span><h4>简单来说</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RuxandraTeslo/status/1675987711608455170">一个基本问题是，对于伤残调整生命年（DALY）等事情，我们并不认为不孕不育是一个大问题</a>。许多人会强烈反对。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4428107">通过生育连锁店获得新企业所有权的生育诊所的诊所数量增加了 28.2%，成功率提高了 13.6%</a> ，显着改善了总体结果。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/drethelin/status/1709268192634564897">米什拉具有芭比娃娃的生育能力</a>。</p><blockquote><p>米什拉：奇怪的是，这是我在任何地方都没有见过的做法，但对我来说，芭比娃娃的意义在于，最重要的是孩子，没有孩子，其他一切都毫无意义。</p><p>芭比感到不满，因为她的休闲、聚会和与酷人一起出去玩的生活没有任何进展，她的幸福结局是去看妇科医生，即获得生育能力。</p><p>芭比娃娃的存在一般是为了让孩子们快乐，其次是通过提供作为有权力的女性可以过的生活的例子来“让他们变得更好”。如果没有孩子，他们的意义就毫无意义，但他们的生活却围绕着没有孩子而构建。</p><p>肯本质上是不满的，因为他没有任何意义。他希望被芭比娃娃需要，但由于她既不需要繁殖，也不需要防御，也不需要劳动，他只是养眼的人，偶尔令人愉悦的消遣……不幸的是，他有自己的欲望。</p></blockquote><h4>原因</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.demographic-research.org/volumes/vol44/5/44-5.pdf">新文章</a>说（ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robinhanson/status/1693305723323601356">HT Robin Hanson</a> ）：</p><blockquote><p>过去十年生育率下降意味着之前关于高度发展和性别平等促进生育率提高的建议不再得到国家层面的支持。</p></blockquote><p>我总是觉得相反的说法很奇怪。显然，更大的发展和性别平等与生育率降低有关，并导致生育率降低。好事可能会带来负面后果。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://akjournals.com/view/journals/2055/aop/article-10.1556-2055.2022.00028/article-10.1556-2055.2022.00028.xml">本文</a>探讨了现代性结构如何导致低生育率。</p><blockquote><p>抽象的：</p><p>低生育率是现代社会日益关注的问题。虽然对生殖障碍的经济和结构解释在一定程度上提供了信息，但它们没有解决生殖决策背后的根本动机，也不足以解释为什么东亚国家的生育率如此之低。</p><p>本文通过描述现代环境如何在我们的进化机制与它们旨在处理的输入之间产生不匹配，从而导致对社会地位的关注，从而妨碍交配和生殖结果，提出了对现代背景下低生育率的新颖解释。 。我们还利用东亚发达国家作为案例研究，进一步强调文化如何与现代特征相互作用，从而产生超低生育率，有时甚至达到人们可能放弃生育甚至完全交配的程度。</p><p>通过我们的分析，我们整合了几条单独的研究，阐明了推动社会地位和生殖努力之间权衡的基本动力，补充了越来越多的关于进化不匹配的文献，并提供了对现代背景下低生育率的改进解释。</p></blockquote><p>该理论认为，现代生活造成了进化上的不匹配，类似于我们喜欢脂肪和糖的方式，导致人们以牺牲繁殖为代价来关注社会地位。在这个模型中，人们进化到重视与他人的比较，因此让每个人都变得更加富有并不会改变人们对自己财富和地位的自我认知，而与世界各地地位较高的人进行更多比较会降低财富和地位并提高社会地位。状态相关的监控和行动成本。同样，通过提供更多投资于社会地位的能力，并为我们提供一个更拥挤的群体来竞争，竞争就会变得更加激烈，迫使所有参与者进行更多投资。</p><p>人们还可以将这一理论（本段不是我的研究）建模为，总会有相当数量的人的相对社会地位低于阈值，这将促使他们考虑推迟生育，这对于现在来说是可行的他们要做的事。如果那些高于阈值的人没有大家庭来补偿，我们就不会回到更替水平，但现在拥有如此大（4个以上孩子）的家庭非常昂贵且罕见。</p><p>现在很少有人想要这样的家庭了， <a target="_blank" rel="noreferrer noopener" href="https://dkras.substack.com/p/sex-differences-attractiveness-and">这是来自交友群体的分析：</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb56e6146-a81d-432f-97c8-5d105670aade_419x367.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/hk77964acpqxtdgputyp" alt=""></a></figure><p>我们可以处理 0 和 1，但为此我们需要更多的 4 和 5。</p><p>到底是什么原因导致 1700 年代法国生育率下降？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/cremieuxrecueil/status/1697695568158470604">来自法国不同地区的数据有力地支持了这一假设，即这与世俗化有关</a>，这在很大程度上是税收反抗和对国家腐败的反应。线程很有趣。</p><p>声称工作时间可能是一个重要因素。</p><blockquote><p>更多出生率：为什么波兰和希腊等国家的生育率比德国和斯堪的纳维亚半岛等国家低得多，尽管宗教信仰程度高得多？为什么北欧比南欧更肥沃？工作时间的差异可能有助于解释这一点。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/landgeist">@landgeist</a>的新地图。</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72995cb7-c3f7-4b93-9a3f-e239057a7761_1601x1440.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/yyuiwcl6q4lzgd3tbbeo" alt="图像"></a></figure><p>这里有一些鲜明的对比。正如评论者指出的那样，过去我们的工作周很长，出生率也很高。我的猜测是，工作时间与在儿童保育方面的大量预期时间投资以及现代核心家庭的隔离相结合，这确实使这种动态变得重要，尽管这是众多因素之一。请注意，由于希腊的非正规经济，其数量应与大量海盐一起服用。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DKThomp/status/1713937759113498642">如果不是（或除了）工作时间之外，而是和孩子们一起工作，会怎样呢？</a></p><blockquote><p> 1. 密集型育儿革命绝不是美国特有的现象。这种事正在欧洲各地发生</p><p>2. 法国母亲是经合组织中唯一与孩子相处的时间比 60 年前更少的父母类别吗？</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e3e06dc-8a8d-4a95-abc1-2ff03a9eb231_1170x973.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/pdwxg8iaas2egxewspwi" alt="图像"></a></figure><p>是的，这些图的底部是零点。这些变化是非常大的。</p><p>其中一张图表与其他图表不同。法国万岁。难怪法国的生育率比许多其他欧洲国家要好得多，尽管还不够好。</p><p>缺乏时间会影响结果吗？我还没有听到任何这样的说法，法国在指标上的相对表现似乎很稳定，完全是零假设观察。 GPT-4 所能得出的结论是，法国在儿童多动症等心理健康诊断方面落后，这是同一个一揽子计划的一部分，我对此说“很好”，并声称法国正在看到社会经济之间教育成果不平等的扩大。我认为这是由于他们面临的其他问题超出了本文的范围。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AlyssaRosenberg/status/1699431901226733977">孕产妇死亡和医疗问题不断增加</a>，这本身就很糟糕，而且无疑会阻碍生育能力。 Thread 将其归咎于整体健康状况下降。还有剖腹产带来不必要的压力以及其他在分娩过程中不愿将母亲视为人的问题。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87d1d2d0-546a-46e1-9b1a-63a60b41ff76_1200x900.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/wyr8nlcez7eiko3ytcht" alt="图像"></a></figure><h4>原因：悲观主义</h4><p><a target="_blank" rel="noreferrer noopener" href="https://daviddfriedman.substack.com/p/pessimism">大卫·弗里德曼强调了我们对未来的极端悲观情绪</a>，我同意这是这个难题中被忽视的重要部分。尽管世界已经摆脱了极端贫困，生活变得更加美好，但人们却认为世界现在和未来都在变得更糟。人们从担心人口过剩和大规模饥饿转向期待反乌托邦的政治危险和气候变化，最新的此类危险就是人工智能。</p><p>这篇文章将所有这些风险描述为被夸大了，就像之前对人口过剩的担忧一样。我认为这被夸大了，但方向是正确的——人们错误地（考虑到他们关心的很少是比人类人工智能更聪明的人）普遍对未来感到绝望，而他们（不太明显错误地）对自己的未来尤其感到绝望。他们问：“我为什么要把孩子带到这个世界上？”好像这对孩子来说是一件坏事。</p><p>大卫·弗里德曼没有提到对经济和负担能力问题的悲观情绪，特别是住房、医疗保健、儿童保育和教育成本的上升，这让年轻人认为他们养不起孩子。可能是因为那些问题非常现实，所以问题并不悲观。相反，我想说这些问题都是真实存在的，而且还会通过悲观主义造成更多问题。</p><p>气候变化在这里尤其危险，因为生育孩子被认为会使气候状况变得更加恶化。</p><p>在生育率方面，有两种方法可以解决这些问题，从存在主义到政治再到经济。</p><ol><li>人们可以尝试让人们相信他们的担忧既不正确又有些夸大其词。告诉他们好消息，给他们白色药丸，告诉他们世界实际上正在变得更好，所有这些问题都是可以解决的，我们将解决它们。</li><li>切实解决问题，以一种让人无法错过、深入骨髓的方式。</li></ol><p>我努力通过方法一尽自己的一份力量。采取一些方法一绝对值得。</p><p>更好的方法是方法二。这种悲观情绪是没有道理的，但问题却是真实存在的。我们的问题非常容易解决，解决这些问题并不比口头上的解决方案更难。</p><p>我认为这完全适用于气候变化。我们有技术、有能力。如果我们专注于真正有用的东西，那么真正有用的东西并不昂贵，也不需要巨大的牺牲。我们完全应该这样做，即使不考虑保护气候的相当大的价值，纯粹是为了减轻心理影响和各种无效替代干预措施的成本。</p><h4>原因：信号升级</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.overcomingbias.com/p/escalating-signals-cut-fertility">罗宾汉森提出的论点是，这一切的核心是不断升级的信号</a>。</p><p>我们要求孩子们得到比过去更多的关注，不仅威胁到羞耻和不爱孩子的影响，还威胁到警察和社会服务机构。这包括积极的关注和活动，即使在家里也不要让孩子们处于无人看管的状态，更不用说让他们玩耍了。这对生育能力造成很大伤害。</p><p>需要增加受教育年限，中断受教育年限的处罚也越来越重，而且在这些年里不能有松懈。这也对生育能力造成很大伤害。</p><p>我们告诉年轻人在寻找伴侣或组建家庭之前要“找到自己”并完成自己的旅程。我们以前不这样做。在我们过去很早就合作和结婚，然后成长并弄清楚我们是谁并适应共同生活。相反，我们现在看不起这些人，认为他们很简单，没有乐趣，并将婚姻视为我们旅程的终点​​而不是起点。许多下层阶级认为婚姻的门槛如此之高，以至于现在它比与某人生孩子的门槛要高得多。这极大地损害了生育能力。</p><p>我们曾经认为让祖父母和其他亲戚提供很多抚养孩子的帮助是好的和正确的。汉森声称，现在我们看不起所有参与此类安排的人。我不确定我们是否真的在这方面做了什么，但确实，这种帮助往往不太优先或不那么及时，包括因为祖父母现在太老了，缺乏精力。</p><p>我们更有可能生活在城市地区，也更看重城市地区的生活，这些地区的生育率一直较低，而且常常低于更替水平。医疗保健支出排挤其他支出。</p><h4>婴儿潮</h4><p>汉森也提到了缺乏战争，并将其视为婴儿潮的根源，但正如<a target="_blank" rel="noreferrer noopener" href="https://worksinprogress.co/our-authors/#anvar-sarygulov">Anvar Sarygulov</a>和<a target="_blank" rel="noreferrer noopener" href="https://worksinprogress.co/our-authors/#phoebe-arslanagic-wakefield">Phoebe Arslanagic-Wakefield</a>最近在 Works in Progress 中所写的那样，婴儿潮的时间和细节<a target="_blank" rel="noreferrer noopener" href="https://t.co/qIWyr37D1d">排除了第二次世界大战的可能性：原因。</a></p><p>婴儿潮始于 20 世纪 30 年代战前。这种繁荣发生在世界各地，包括爱尔兰和瑞士等在战争中保持中立的国家。还有其他事情正在发生。</p><p>提出的论文是其他因素的结合：</p><ol><li>电力、冰箱、炉灶、吸尘器和洗衣机。</li><li>由于医疗条件的改善，孕产妇死亡率大幅下降。</li><li>房地产繁荣导致住房拥有率上升。</li></ol><p>这些加在一起就是一场巨大的游戏。抚养孩子意味着死亡风险要小得多，家庭生活要容易得多，而且抚养孩子的房子要便宜得多。通常情况下，价格大幅降低会导致供应量大幅增加。</p><p>再加上战争的影响，男人受到更多的青睐，成熟得更快，你就会看到很多年轻的婚姻和很多孩子。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/MoreBirths/status/1702079006089105592">更多的出生引起了人们</a>对早期婚姻和战争影响的关注。</p><p>随着时间的推移，抚养孩子的成本以各种方式上升，这种影响消失并自行逆转。</p><p>我基本上相信这个论点，加上建立在自身之上的文化趋势。</p><p>我们可以使用变化的幅度来估计影响吗？我希望有人尝试。在这种情况下，这似乎非常困难，因为有很多不同的因素。也许有一些自然实验的改进是不平衡的，我们可以做双重差异，但其他人必须做这项研究。</p><p>在那之前，广泛的教训是，通过改变生活方式和生孩子的经济影响，确实可以大大提高出生率。你需要做的就是改善经济适用房、大幅降低死亡率、电力、冰箱、吸尘器和洗衣机。</p><p>当你这样说时，听起来好像很多，是人工智能完成的任务。也许吧，但我认为我们已经强加了如此巨大的不必要的成本，如果我们用心去做，我们确实可以得到同样显着的改进。这仍然是一个巨大的假设。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AlyssaRosenberg/status/1699431901226733977">最近，孕产妇健康状况急剧恶化。</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6220c2a2-a2c3-44ec-8a6a-837ad86912fb_1200x900.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/twn18sbrust1jjqdvbzo" alt="图像"></a></figure><p>我们在婴儿潮期间看到了持续的改善，直到 1987 年，然后情况发生了逆转，尽管新冠疫情带来的冲击希望只是暂时的。问题是女性的健康状况较差，并因此遭受各种并发症的困扰。仍然没有太大的改进空间，你能做的最好的事情就是零。</p><h4>你需要更多</h4><p>该图表明，至少在 1958 年之前<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/lefineder/status/1685861950423449600">，美国出生队列的巨大变化确实</a>是在 4 个以上孩子的家庭中减少，而不是在零个孩子的家庭中增加。</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9805c1e-14a5-45a5-9b33-47c8dd5bce3b_927x787.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/nu8fskampxkfzo7g0fvp" alt="图像"></a></figure><p>作者提出，东亚的生育率之所以如此之低，是因为他们比西方人更有耐心、勤奋和社会地位投资的文化，导致他们比我们更推迟和放弃生育。大多数人不会考虑低声望的工作或缩短教育时间，这使得竞争变得更加激烈。然后他们继续讨论唯物主义。</p><p>问题在于，正如泰勒·考恩所说，他们没有“建模”，也没有像经济学家那样思考。相反，他们讲述了一个“Just So Story”。这个故事很可能指出了这个谜题的一个重要部分，但我们没有提供一种新的方法来了解这是否超出我们的直觉。</p><p>在 Palladium 杂志中， <a target="_blank" rel="noreferrer noopener" href="https://www.palladiummag.com/2023/04/06/birth-rates-are-collapsing/">Simone 和 Malcom Collins 也提出了同样的观点</a>。许多家庭和个人只有一个孩子或根本没有孩子。对于许多家庭来说，将出生率维持在更替水平的唯一方法是生四个或更多的孩子来弥补这一点。这样做在经济上已经变得令人望而却步，文化上也不赞成这样做。</p><p>所提供的解决方案是创建微型文化，即可以生育大量孩子的群体，并在几代人的时间内迅速实现收益复合。我认为这是不可持续的，如果没有极端的（如阿米什级别的）措施来防止它，将会有太多的文化泄漏，我们需要总体上做一些事情，尽管我比他们更希望丰富 - 他们注意到周围收入达到 50 万美元，我们将获得更替水平的生育率，如果我们同时拥有（非变革性但经济上非常强大的）人工智能和不断减少的人口，那么达到这一有效财富水平似乎是可行的，这得益于通过较低的住房成本预计会下降。克服建筑限制的需求。我也更希望普通合理的经济转移能够发挥作用，但尚未进行过大规模的尝试。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/lefineder/status/1685861950423449600">Lior Lefinder 的帖子还有其他几张图表。</a>这是一个非常惊人的：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77c37c3a-37db-4c44-a1dc-8a16d2b0c7f2_652x492.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/rzaookrkzfl07kcauj5s" alt="图像"></a></figure><p>现在这是一个变革性的 40 年。无法存活的儿童比例从 35% 降至 20% 以下。</p><p>拥有更多的原因之一是确保您的遗产，您的产品线能够生存下来。在儿童死亡率高得多的那个时代尤其如此。如果存在更多的不确定性，您需要更高的平均结果来防止灾难。</p><blockquote><p>马尔科·尤基奇：我认为真正的更替生育率不是每名妇女生育 2.1 个孩子。这是5.1的孩子。瑞典最近的一项研究发现，到 2007 年，在 1885 年至 1899 年出生的一代人中，有 2 个孩子的人中有 25% 的后代“零”！ 1个孩子？ 50%。</p><p> 2.1这个数字看起来很直观，被视为道德或生活建议。两个就足以维持人口。更多的资金会稀释对每个孩子的投资或导致人口过剩。但它实际上只是一个统计数据，根据死亡率的不同而有很大差异。</p><p>假设您不想在统计上为下一代补充整个人口方面发挥自己的小作用，而是对长期补充自己的家族王朝或血统感兴趣。那么真实的更替生育率是多少呢？</p><p> 20 世纪初，瑞典儿童死亡率下降并避免了世界大战。然而，有 25% 有两个孩子的父母仍然看到自己的血统在一个世纪内消亡。这是短期的替代，但听起来不像是长期的替代。</p><p>根据这项研究，大约 120 年后没有后代的概率接近于零，不是在 2 个甚至 3 个孩子的情况下，而是在大约 *5 个孩子的情况下。所以，如果你是 20 世纪初瑞典的成年人，想要伟大的——孙子们，你应该瞄准五个孩子，而不是两个。</p><p> ……</p><p>您可以控制自己的生育能力。但你无法控制*你孩子的，*更不用说孙子的了。到 2023 年，它们可能仍会在繁殖前死亡，或决定根本不繁殖。事实上，这些机会并不是可以忽略不计的，而是迅速积累起来的、令人不安的大机会。</p><p> Samo Burja：当你开始认为自己的家族或王朝做出了独特的贡献时，观点就会有所不同。让孩子帮助社会支付社会保障金或最大化GDP是一回事。让他们来延续一项使命或特殊知识就完全是另一回事了！</p></blockquote><p>只有当您的后代长期养活 2.1 个或更多的孩子时，养育 5 个孩子才有效。如果它们维持得比这个更高，并且你能生存 100 年，那么你将生存 1000 年，除非出现一些系统性灾难或灭绝事件，如 AGI，或者历史上经常发生的类似入侵等更平淡的事件。犹太人清楚地记得，每一代人都试图杀死我们。</p><p>从社会角度来看，5.1 个孩子远远超过了我们需要的平均水平，足以很快引发相反类型的危机。我们真正需要的是有一个小组足够关心以 5.1 等数字为目标，以弥补那些不以 2.1 为目标或未能达到 2.1 的人。</p><h4>我会告诉你我想要什么 我真正想要什么</h4><p><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/webdevMason/status/1690797078971346944">梅森大声说出来。</a></p><blockquote><p>梅森：如果我没有生下我的女婴，我的生活就会半途而废。直到我死之前，我都是不完整的。也许不是每个男人和女人都是这样，但对我来说绝对是这样，我怀疑大多数其他人也是如此。</p><p>诺米·麦克唐纳：不幸的是，与没有孩子的人沟通是如此困难</p><p>梅森：年轻人会受到这样的想法的轰炸：如果他们感觉自己不能从根本上完整地度过孤独的生活，那么他们的心理健康或性格就有问题，甚至他们不应该约会或承诺任何人，直到他们实现了这一点。</p><p>这可能适合一个人十几岁和二十岁出头的发育，但我认为，当一个人成长为尚未走上组建家庭之路的成年人时，直觉上感到一些空虚是完全自然的。</p></blockquote><p>我确信并不是每个人都会遇到这种情况，但我相信这种情况比那些放弃生孩子的人意识到的要多得多。</p><p>彩票中奖者会怎样？<a target="_blank" rel="noreferrer noopener" href="https://www.nber.org/papers/w31039">一篇论文</a>声称，男性获胜者增加了婚姻形成，降低了离婚风险并提高了生育率。这一切都是有道理的。更有趣的是，这种情况并没有发生在女性身上，她们只会增加短期（而非长期）离婚风险（大概是“我不再和你在一起了”效应）。女性并没有通过生育更多的孩子或建立更多的新婚姻来应对财富冲击。</p><p>这表明，当女性说她们需要更好的财务状况才能准备结婚或生孩子时，她们往往意味着她们想要更高的伴侣质量的证据，而不是担心缺乏资源。</p><p>研究结果还对工作激励和薪酬不平等以及生育率产生明显影响。如果说彩票在某种程度上代表了财富或收入效应，那么这里的激励似乎是高度不对称的。</p><h4>负达卡</h4><p>韩国越来越多地禁止儿童进入包括<em>图书馆</em><a target="_blank" rel="noreferrer noopener" href="https://twitter.com/allyjung/status/1654088336934424576?s=20">在内的各种场所</a>。</p><blockquote><p> Yong Hyein：为了提高世界上最低的出生率，我们首先应该改变排斥儿童的社会。孩子是我们的同胞，第一次了解这个世界，所以他们可能在任何事情上都很迟钝、笨拙或缺乏经验。</p><p>但我们所有人现在都是，或者都曾经是孩子……我们想要的社会不是一个只为那些反应敏捷、技术娴熟、经验丰富的人而建的社会，而是一个可以缓慢、笨拙或缺乏经验的人而存在的社会。</p></blockquote><p>这么。很容易理解为什么私人场所可能更愿意将儿童排除在外，尤其是在只剩下很少的儿童的情况下。也很容易理解为什么这对社会有害，无论是对出生率，还是对儿童、父母和家庭的生活，以及这些孩子长大后的身份。</p><p>这种负外部性很重要。至少，不包括儿童的私人场所应该被征税。在某些情况下，如果所需税收过高，则完全禁止此类限制是适当的。</p><h4>达卡还不够</h4><p><a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/world/china/chinas-fertility-rate-drops-record-low-109-2022-state-media-2023-08-15/">2022年中国生育率下降至1.09</a> 。一个月后，他们跟进发现，在 14 亿<a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/world/china/births-china-slide-10-hit-their-lowest-record-2023-10-12/?utm_campaign=Marketing_Cloud&amp;utm_medium=email&amp;utm_source=USCBC+News+Overview+10.12.23&amp;%20utm_content=Births+in+China+slide+10+percent+to+hit+their+lowest+on+record">人口中，中国的出生人口同比下降 10%，仅为 956 万人</a>。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/china-crackdown-betrothal-gift-bride-price-birth-rate-population-2023-3">中国继续努力投入（适量）资金来解决生育问题</a>。例如，中国杭州市向今年迎来第三个孩子的父母提供 2,900 美元的补助金。其他城市则给予30天的“婚假”。</p><p>像往常一样，有人说“让人们生孩子可能需要的不仅仅是金钱激励” <a target="_blank" rel="noreferrer noopener" href="https://thezvi.wordpress.com/2017/12/02/more-dakka/">，对此我的回应是</a>引用：如果暴力不能解决你的问题，<em>那是你使用的还不够。</em></p><p>与特雷西·阿洛韦的原则形成一个有趣的对比，即“任何可以用金钱解决的问题都不是真正的问题”。</p><p>我对这两点的调和是，仅仅因为金钱<em>可以</em>解决你的问题并不意味着你将被允许花这笔钱。</p><p>相关故事的中心案例是中国对彩礼的打击，因为支付“彩礼”的习俗已经失控，成为婚姻的重大障碍，有时花费数倍于年收入，甚至包括房地产。传统上，这笔钱是支付<em>给姻亲</em>而不是新婚夫妇的，因此没有很强的抵消积极作用。</p><p>中国的问题是潜在需求很少。在美国和许多其他地方，期望的生育率远远高于替代率，因此财政、后勤和社会方面的帮助应该是触手可及的。 <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/lymanstoneky/status/1659585940895981568">在中国，没有那么多</a>。</p><blockquote><p> Lyman Stone：只有 38% 的中国大学生想要孩子。女性占 34%。通过猜测“不确定”受访者的真实愿望，中国女大学生平均只想要 0.94 个孩子。大学生男1.05。中国的生育主义将会失败。</p><p>中国唯一的前进道路是：</p><p> 1) 应对——考虑到中国的财政、地方政府、福利和移民政策，这非常非常困难</p><p>2) 强制——很奇怪，但他们可能会尝试一下</p><p>3）说服——他们甚至还没有尝试过这个</p><p>看看中国是否会尝试任何旨在改变理想家庭规模以及父母身份和大家庭带来的社会声望的大型“文化干预措施”，将会很有趣。到目前为止，他们甚至还没有朝那个方向做出手势。</p><p>迄今为止，中国的整体转变是：1）取消之前的惩罚性反生育主义；2）假设自愿激励措施将像西方国家一样释放出更高的预期生育率。但在大多数调查中，中国人确实不想要大家庭！</p><p>最近对中国生育偏好的调查给出了 0.9 到 1.7 之间的估计值——这是世界上任何地方发现的最低的调查偏好。我所支持的生育主义是中国背景下的DOA。</p></blockquote><p>最终的游戏大概是所有三种策略的结合。</p><p>应对是必要的，因为其他两个最多只能缓解人口崩溃。</p><p>仅靠说服是行不通的。说服只能到此为止，然后就会变成强制。</p><p>也不会行贿。 As the OP says, in America people want to have enough kids to fix the problem, in China they do not, making the amounts of money required vastly larger.</p><p> Coerce is a matter of degree, with no clear line where financial and social incentives, both rewards and punishments, transform into coercion. Big enough bribery, paid for by everyone else, is not fully non-coercive. Nor is Chinese-style &#39;persuasion&#39; non-coercive. I can imagine a future world in which expressing anti-natalist views on WeChat will get you into trouble. The question is, are they willing to go far enough, and what would it look like if they did?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonexaminer.com/opinion/paying-people-to-have-kids-can-only-do-so-much">South Koreans similarly lament</a> that there are big plans to boost fertility floated every election, but when people do not want children money only goes so far, and blame societal issues.</p><p> Certainly societal issues are a big deal. As I&#39;ve noted before, South Korean television paints a deeply dystopian picture where no one has any slack to think of having a family.</p><p> Yet what are the policies that have failed? A monthly child allowance of more than $500, soon to be $750, and 18 months of subsidized leave. I realize that this is over $210 billion in the last decade but South Korean GDP is 1.81 trillion. That is on the order of 0.1% of GDP, for a national existential crisis that seems like giving up far too early. Or alternatively, South Korea has a population of 51.74 million, so this is about $400 per year per person, most of which is transferred rather than lost, and most of it not given lump sum at birth or otherwise well-targeted. This is a start. It is not what trying for real and failing looks like, and is highly dwarfed in size by (for example) America&#39;s funding for public schools.</p><p> Back home, I agree <a target="_blank" rel="noreferrer noopener" href="https://arnoldkling.substack.com/p/links-to-consider-419">with Arnold Kling</a> that the diagnosis of <a target="_blank" rel="noreferrer noopener" href="https://www.tabletmag.com/sections/news/articles/missing-but-wanted-children-mike-lind">this Michael Lind post</a> is good, that we are making it economically and logistically too difficult to have children and we should address this, and also agree that the proposals involved won&#39;t get the job done. Heavier ammunition will be needed.</p><p> In <a target="_blank" rel="noreferrer noopener" href="https://reason.com/2023/05/02/storks-dont-take-orders-from-the-state/">Reason Magazine, Elizabeth Brown argues</a> that government intervention to increase fertility (unlike intervention to decrease it) is toothless, advises that here is not much to be done other than immigration. Cites the failure of past attempts, which I continue to argue worked fine if you compare magnitude of spend to magnitude of effect. Does not mention the other interventions that seem most promising.</p><p> Sam Dumitiu points out that age of first child continues to rise, cites among other things housing prices, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Sam_Dumitriu/status/1644015977934233606/photo/1">as a 2016 study</a> says a 10 per cent increase in house prices leads to a 2.8 increase in births among owners and a 4.9 per cent decrease in births among renters, a net 1.3 per cent fall in birth rates.</p><p> I always love such statistics because they let us play our favorite pro-natalist game, which is <em>how much would it take?</em></p><p> So, based on this information, and a brief search suggesting an average house price back then was about $316k for all houses and more like $200k for first time buyers, and we assume the gap between owning and not owning as an endowment effect of 10% of that and split the difference, let&#39;s say an unconditional endowment – not even an incentive, only an endowment – of $25k led to a 7.7% (!?!) relative increase in births.</p><p> So the answer is <em>really not very much,</em> since you would presumably get a much better return on a direct payment than you would on an unconditional wealth transfer.</p><p> Perhaps for political economy reasons this could take the form of a homebuyer subsidy available to new parents (first-time would be the default, but if possible we would want it to include existing owners who are upgrading for more space), where we cover $25k of the down payment. That might be very well-targeted at exactly the couples most likely to be impacted, the ones who want to buy a place to start a family and can&#39;t afford to do that.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.overcomingbias.com/p/will-nations-fund-fertility">Robin Hanson goes over various considerations</a> , excepts nations to be unwilling to deploy sufficient quantities of Dakka. He does not ask what the price tag would be, that he expects nations to be unwilling to pay. He says that if people don&#39;t care individually he does not expect them to care enough collectively, but there are many cases where people can&#39;t pay individually but are happy to enforce payment collectively, and also national security or similar considerations have a way of carrying the day. Ultimately, it will come down to price, so that&#39;s the real question.</p><p> The place I most disagree is that Hanson envisions endowing children with debt owed to their parents, as a means of tempting people to have children. This completely misunderstands the mind of people considering having children. It would do me exactly zero good to be able to endow my children with debt to me. I work hard so my children can have a better future, not so I can steal part of theirs for myself.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robinhanson/status/1708192728570343454">How much Dakka are we interested in deploying</a> ?</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd700ea73-a1e4-4006-b4a9-91dd8827efb1_988x517.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/bwqrxhwxhxlgcbzyxovm" alt=""></a></figure><p> Whether or not to include school makes this confusing. If public school counts then that already puts us over $100k, I doubt most people in the survey were responding that they wanted parents to foot that bill. If we presume that this number excludes school, then we still don&#39;t have much support, even among Hanson poll voters. This level of support won&#39;t get the full job done, but it would be a damn good start.</p><h4> More Dakka</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/river_is_nice/status/1631461466258776069">A Texas Republican who is in the habit of introducing random bills with no co-sponsors introduced a bill with no co-sponsors giving families large property tax cuts based on how many children they have</a> , at the rate of 10% discount per child. I found this out because of a Democratic strategist highlighting that it only applied to straight married couples neither of whom has ever been divorced, and comparing it to famous property tax allegory The Handmaid&#39;s Tale.</p><p> Needless to say, the implementation details here are not The Way to align incentives, or provide families in need with necessary help, and are needlessly controlling and cruel. Which doubtless was intentional. This wasn&#39;t a real bill intended to pass.</p><p> If you want to do this, give parents money. If you worry about incentives to work or that you&#39;re going to overly reward the poor or something, an income tax break makes sense, except Texas has no state income tax. Property tax discounts are simply <em>weird</em> here, if your property tax bill is so big that 10% of it is a game changer, I guess a few farmers might be in that situation but it&#39;s still super weird.</p><p> Also, it is interesting to game out the bill as written. If having more children reduces your property tax rates, then that means that there is reason to ensure that those with more children legally own more real estate. Given how one can lease, and issue debt, and sign long term contracts, if such a thing were to pass, I&#39;d expect High Weirdness.</p><p> The Check Republic increased its subsidies, and got some results.</p><blockquote><p> Astral Codex Ten Commenter &lt;0174: I&#39;m Czech. With a kid. The subsidies are quite high, but not as much as you state – it is currently $13k in total, for most people this is split into three years (so $360 per month). This typically adds around 1/4th of one person&#39;s average salary (the woman, sometimes also the man, typically doesn&#39;t work until the child is 3 years old or does only part-time). This really leads to speculations that poor people have children just for the sake of this benefit, but of course this is difficult to prove.</p><p> The amount was increased substantially from $9500 in 2020, but the increase is gradual so I don&#39;t think this is the main reason.</p><p> As for the data itself, I suspected some change of methodology, but it seems it is legit. See this graph of the Czech statistical office ( <a target="_blank" rel="noreferrer noopener" href="https://www.czso.cz/documents/10180/165603915/13011822.pdf/48325f59-e080-4991-a04c-643441673e17?version=1.3">“Graf 19”, PDF page 33, black line</a> )</p><p> Czech sources mention as the main reason for the increasing numbers were stable economy and the increase of the subsidies (plus some changes in the way they are paid).</p><p> By the way, for 2022 the numbers are back to 1.66, reportedly mainly because of covid and they are expected to go down because of the Ukraine war and economy stagnation.</p></blockquote><p> This represents a standard economic model of fertility. People want kids. Kids cost money. They have more kids when the price goes down, and they have more kids when the future looks brighter and they feel more able to afford them and give them good lives. There is obvious concern that fixed-size subsidies increase birth rates more among the poor, which we could fix by having part of the subsidy be reduced marginal income tax rates if we cared enough (and could be made non-regressive or even progressive, if desired, by changing baseline rates).</p><p> The Czech decline in birth rates was very large, as was the size of the recent recovery, relative to obvious comparison countries. They had a lower low point, and are now substantially higher than their neighbors, although very far from the 2.1 target. Of the about 0.5 jump in fertility, we can roughly say that 0.25 of that is recovery from an abnormal low and general continent-wide provisions, and attribute roughly 0.25 to uniquely Czech influences. If we attributed this entirely to ~$10k subsidies per birth we would get $66k per additional birth. If we scaled that with wealth the implied marginal cost per birth in America would be about $185k, likely an underestimate since there are likely some other factors also in play here.</p><h4> Cato Brings the Fire</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cato.org/policy-analysis/freeing-american-families">Cato has a new guide to increasing fertility in America.</a></p><p> Their first claim is that direct fertility initiatives, presumably direct payments, are estimated as costing $250 billion annually to raise the fertility rate by 0.2 children per woman, or about 450,000 births a year. That would be about $555k per birth. They do not directly say where this comes from, but later they do explain it:</p><blockquote><p> Although the effect size of fertility initiatives is often small and they typically fail to achieve their goals, these policies frequently come at a large fiscal cost. According to an estimate by economists Melissa Kearney and Phillip Levine, $250 billion in annual childhood spending—between seven and eight times the amount of childcare spending that the Biden administration proposed in Build Back Better—would raise the total fertility rate from 1.6 to 1.8, an increase of just 0.2 extra children per woman. <a target="_blank" rel="noreferrer noopener" href="https://www.cato.org/policy-analysis/freeing-american-families#_ednref20"><sup>20</sup></a></p></blockquote><p> Taken at face value, that is 457k live births per year at a cost of 250 billion, or $547k per additional birth, or for an 0.2 boost in TFR about $60k per child born in America, in transfers.</p><p> This is likely to be a highly inefficient transfer if the only goal is raising fertility. Direct payments or other help that is directly accessible at or before birth have a much stronger impact than continuous future benefits over time. Benefits given in the form of subsidized child care will largely be captured by the industry, or from the additional costs imposed upon it such as requiring college degrees and ground floor facilities. Much of the time, those using the subsidized child care would much rather put the children with family or watch them themselves, if they could keep even a fraction of the subsidy.</p><p> In my post <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-car-seats-as-contraception">On Car Seats as Contraception</a> , I estimated the cost even via an inefficient mechanism at $270k, similar to the cost of raising a child to the age of 18, so we disagree by a factor of two there. Then <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/fertility-rate-roundup-1">in Fertility Roundup #1</a> I found lower estimates from elsewhere, including $17k in Australia, $65k in Sweden and $75k in Singapore.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cato.org/policy-analysis/freeing-american-families#_ednref14">Cato sights a Quebec policy that had a huge short term effect at $9k per additional birth</a> , but claims long term birth rates were mostly unaffected. They point out that all countries except Bulgaria that set explicit fertility targets failed to hit their goals, but I would point out that mostly the attempts were extremely underpowered and Russia&#39;s efforts did make substantial progress. And also you should see the other guys.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1b0c83a-ef6b-4e57-8114-354fa61a13f2_1177x1177.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/zizf1pcpu57eura7gufn" alt=""></a></figure><p> How much would it take here with the direct approach? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1670171735923249152">Eliezer Yudkowsky tests the waters.</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1835b5b2-a7fe-484d-b23a-30d03b5ca4fc_727x421.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/od4rsxk1ez3acg37bvye" alt=""></a></figure><p> The issue is that it is not so easy to well-target the subsidy. You don&#39;t know which children happen because of the subsidy and which don&#39;t, wasting the rest of the payment.</p><p> What this does say is that if you used a $50,000 subsidy for each child, in an unusually affluent group, you&#39;d increase fertility by 0.185 for the first child added, which would put the upper bound estimate at around $450k per birth, already lower than $555k. Presumably then many of those same people would also add a second additional child, and some others would use the wealth endowment from current children to have another. In the general population the required value would doubtless be a lot lower, and the targeting can be improved. This is not obviously so different from my old $275k estimate right off the bat.</p><p> Similarly, it seems like a 50% discount on overly regulated and cost-inflated semi-mandatory child care benefits would if anything be too small an adjustment, and already gets us right back to $275k.</p><p> What could we afford to pay? If the payment were a pure deadweight loss, it would be a difficult lift to pay what I estimate Sweden or Singapore had to pay. The good news is, it is not a deadweight loss. It is a transfer from the childless to those with children. That transfer is not free and does generate some deadweight loss, but far less than 100% of size on the margin, especially if it is structured well. On the other hand, if the price tag is closer to $550k in transfers per additional birth, that could be prohibitive, although it is not obviously more prohibitive than not doing it, since the value is tranferred rather than lost.</p><p> The good news, Cato says, is we have more efficient paths we can explore. They say that government &#39;should not put its thumb&#39; on the decisions families make when deciding on having children, but that it currently places that thumb on the wrong side and can stop doing this. I would instead say that the scale will not lie undisturbed in ways large and small, so we should indeed put our thumb on it while it is too far out of balance.</p><p> Why does Cato think the cost of direct payments would ultimately be so high? They think that the subsidy would largely be captured:</p><blockquote><p> Although we oppose baby bonuses and other direct government payments to increase fertility as misguided attempts to raise and sustain fertility, enacting such policies without substantial deregulation along the lines we suggest would result in higher prices for goods and services demanded by parents. In other words, baby bonuses would be absorbed by higher prices instead of prompting an increase in the quantity supplied of those goods and services.</p><p> The result would not be higher fertility or greatly increased family consumption of the goods and services families demand. That is because regulations reduce the supply of goods and services and make the supply more inelastic, meaning that any increase in demand will not result in much increased quantity supplied of such goods and services.</p></blockquote><p> This is a bold claim about the overall basket of goods and services demanded by families. Is that basket so dominated by regulatory burdens that it would effectively capture most of the gains from higher parental wealth?</p><p> We can draw a distinction here between places where the mechanism is explicit taxation of parental wealth, versus positional goods, versus other costs.</p><p> Colleges do this highly aggressively, often up to and including 100% marginal wealth and income taxes.</p><p> The biggest other good, by far, is child care. As I have noted before, our child care regulations are insane, imposing very large costs for minimal or non-existent gains. This is the white whale. Government daycare regulations and subsidies raise prices and then base what you actually pay on income, effectively creating very high tax rates once again. The need to arrange child care very much should be a non-zero disincentive to work, you are indeed incurring real costs to do so, but when costs are raised and then these huge marginal taxes are collected, this disincentive becomes far larger than is efficient.</p><blockquote><p> Still, out‐​of‐​pocket childcare costs constitute about one‐​third of median female earnings in the United States, even <em>after</em> various existing tax and spending subsidies for childcare.</p></blockquote><p> I will not belabor the details. Dramatic savings are available.</p><p> Then there are positional goods, especially housing and especially to the extent it is serving as a proxy of high quality public school access. Cato notes that housing is 26%-33% of child-raising expenses, although I presume that excludes the cost of time. As a percent of total family budget housing is even bigger than that. If you take the supply of housing and good school access as fixed, parents will use any additional funds to bid up on real estate, up until the point where going private at the same quality level isn&#39;t more expensive. Same goes for if you are bidding to choose your neighbors.</p><p> I do think this is a big deal sometimes, but that most people have already lost the &#39;good school&#39; sweepstakes and are taking whatever falls under the place they would live anyway, which means it&#39;s a pure question of to what extent real estate is such a choke point it captures all welfare gains period. Which is more than most people think.</p><p> I continue to endorse the housing theory of everything. Certainly if I wanted to increase fertility efficiently, one would start with building more houses in the places people want to live and raise kids. Getting rid of inefficient zoning detail requirements, substantive up-zoning, actually approving construction, all the usual suspects.</p><p> How big an impact could this make? Rather large. Cato cites estimates of a 50% price increase in the most impacted areas like NYC and SF, and I agree it is at least about that high in such places, and also likely in many suburbs using exclusionary zoning. A good long term policy here could cut child raising expenses and overall household expenses from all sources by 5%-10%, once such policies have had time to play out. I notice I am confused about how much of that would happen how fast – you are bidding on implied rent which shouldn&#39;t change until supply comes onboard but house prices should alter right away and thus things are weird.</p><p> What about the interaction with public school subsidizes? As Cato notes we pay $15k/student for public schools, but then force families to bid for good spots, effectively letting land owners recapture much of this (although they then often are the ones who support the schools via property taxes). Cato suggests, on top of general housing reform, allowing more choice of where students attend public school rather than strictly assigning houses to particular schools, or better yet allowing vouchers. They then cite the usual arguments for why competition is good.</p><p> If we were willing to mess with the sacred cow that is our system of location-based public schools, effective family expenses could drop dramatically, as could average quality of education available. The case for at least public school choice – essentially letting any child register at any non-magnet public school that has the capacity to accept them, if they can handle the transportation logistics – seems very strong to me, but also many good schools would quickly reach capacity, and I do think those nearby do need to retain priority. Still would do a lot of good in many cases.</p><p> The other big positional good is private school tuition, or college. I would presume that prices here would indeed increase somewhat, but also supply over time should be elastic, but then again it should already be elastic. Elite private schools sure look like they should be highly profitable businesses at current rates. My guess is that the very elite will get to collect a lot more rent, but where fertility is impacted competition will be more robust and increases would be modest.</p><p> Finally there are other goods. I agree that for example health care is subject to a lot of government regulation, but I would not anticipate that transfers from the childless to families would get captured here. They&#39;d spend more on food and clothes and electronics and entertainment and durable goods and such, but they&#39;d get more in return, that wouldn&#39;t be captured.</p><p> Cato cites food as 18% of child expenses, and suggests ways to make food overall less costly, such as ending milk price subsidizes (especially important for families, I would also highlight sugar here) and allowing more visas for agricultural workers. There is no estimate made here of prospective size of impact.</p><p> Baby formula in particular, as we have all recently learned, is a regulatory fiasco even in normal times and was recently a crisis. Certainly we could make everyone&#39;s life easier on this front by lifting restrictions, again impact size is tricky to estimate.</p><p> Remote work and flexible work, as Cato notes, are often godsends for parents. You can adapt to the needs of your children, including supervising them, while ensuring you still get the work done. The incompatibility of work and family is somewhat about the sheer total amount of hours in the day, but mostly it is that both are often highly inflexible, kids need things at fixed times and work demands your full professional attention at what one hopes are other times. Any overlap gets expensive quickly, so flexibility is king.</p><p> As Cato points out, many regulations work against this. I understand why regulations (the FLSA) prohibit compensating overtime with time off when I squint, but mostly this seems good and when applied to remote or fully flexible work it is a nightmare. Cato&#39;s proposed solution, giving employers the ability to give employees the option, seems great.</p><p> More generally, the government and its regulations prioritize capital-J jobs and formalism, and discourage flexibility and getting things done. Health care being tied to jobs forces parents into patterns they may not want even more so than it does for others. Licensing and other requirements favor big business and interfere with gainful partial employment. Home business is especially frowned upon.</p><p> Next they mention unwanted C-sections, which can reduce fertility physically. The US rate of 31.8% is far higher than is medically necessary. Financial incentives and the convenience of the hospital push towards this happening. My wife experienced strong pressure on this front, luckily we were able to push back.</p><p> There is some discussion of reproductive technologies, where they wisely warn against distortive government interventions. There is still room for subsidy and for protection against interference.</p><p> Finally they turn to safety policies. They start with car seats. Classic. Then the most important one, reasonable independence, and the special case of home supervision. They note that in Illinois you cannot leave a child alone until the age of 14, and many places like New York leave it ambiguous which is not much better.</p><p> We are making progress. Several states have passed excellent free range kids laws. Connecticut until recently said that kids under 12 must under most circumstances be supervised continuously by an adult, <a target="_blank" rel="noreferrer noopener" href="https://connecticut.news12.com/new-state-law-means-more-freedom-and-less-supervision-for-kids">and failure was criminal</a> .</p><blockquote><p> “If you had an 11-year-old who wanted to go three blocks down the street to play soccer at the local park with their also 11-year-old friend, if they were not under some sort of adult supervision, technically their parents could have been charged with a criminal offense,” Mair said.</p></blockquote><p> The Department of Children and Families objected that otherwise, parents might decide what was best for their kids, but they are satisfied with the final version, which presumably does not do this.</p><blockquote><p> Initially, DCF commissioner Vannessa Doranates <a target="_blank" rel="noreferrer noopener" href="https://www.cga.ct.gov/2023/juddata/TMY/2023SB-01133-R000303-Dorantes,%20Vannessa,%20Commissioner-DCF--TMY.PDF"><strong>expressed concerns</strong></a> , writing: “This leaves the discretion to determine what is or is not unsafe up to the parents, and provided they believe such activities do not present an obvious danger to their child it appears that the court could not decide otherwise.”</p></blockquote><p> This seems highly disingenuous, that is obviously not how it would work. Instead, DCF would have to show that the actual decisions made were unreasonable.</p><p> The Cato writeup was a good overview, although I was disappointed that they failed to find new opportunity.</p><h4> I&#39;m Doing My Part</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattparlmer/status/1706492213461684697">Matt Palmer suggests we act locally.</a></p><blockquote><p> Matt Palmer: We must do everything in our power to ensure that our friends have a relatively easy time having babies Bring babies to parties, normalize free range children, give a bunch of parents a night off at once by organizing group babysitting events Many unimplemented wins here.</p><p> A great many of the proposed solutions to the problem of having a family (ie naive yimby claims that more housing magically solves birth rates (though it would help obviously)) are designed to require minimal effort on the part of people who don&#39;t yet have kids.</p><p> This is BS.</p><p> Actual cultural change around family formation and size requires people who don&#39;t yet have kids to make some changes in their behavior. Not huge ones, but at least some effort will be involved. You must proactively include people with kids in your social activities.</p><p> Btw I have selfish reasons for wanting this to exist, and soon. On the one hand, I&#39;d like for more kid-friendly norms to be in place by the time I have kids, that&#39;s the free bingo square. But I also still want to be able to spend time with my friends who have kids before I do.</p></blockquote><p> I have more hope for YIMBY-like interventions, because helping somewhat then causes the kinds of cascading changes you want. Matt is still right here. If we make it clear kids are unwelcome, as they have in so many places in South Korea, that is going to make it very difficult for families considering having kids. It will also make people feel they don&#39;t know how to exist around kids. I know a lot of people who are scared to interact with children let alone consider having one. That is hard to come back from.</p><h4> Artificial Wombs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.nature.com/articles/d41586-023-02901-1?utm_source=tldrnewsletter">Human trials of artificial wombs could begin soon.</a> There has been success with prematurely born lambs, and they hope to extend that to humans born prior to 28 weeks, then to high-risk pregnancies. Why go straight to lambs? Pigs are difficult to work with, and this:</p><blockquote><p> Non-human primates are a gold-standard animal model to precede clinical trials because of their physiological similarities to humans, but their fetuses are even smaller than those of humans, and the ethics of conducting such experiments are complex.</p><p> ……</p><p> “You only have one chance to do it right, and the learning curve should not be on actual human beings,” obstetrician Guid Oei says.</p></blockquote><p> I mean, no, such experiments are not morally complicated, however much we as a society proclaim that they are. We are otherwise going to let babies die, or run the same experiments on humans instead. The good news is we seem to be able to move past this.</p><p> There&#39;s always the &#39;we should not let you do or invest in this thing when I wish we were instead doing or investing in some other thing.&#39;</p><blockquote><p> Some researchers also worry that artificial wombs would represent an expensive technological solution to a deeper problem. Michael Harrison, a fetal surgeon at the University of California, San Francisco, sometimes called the &#39;father of fetal surgery&#39;, says the data he has seen so far have been promising. But he questions whether it&#39;s worth “throwing all that money and tech” on babies that have a poor likelihood of survival instead of finding ways to improve pregnancy support or standard techniques for preterm critical care, which could reduce the need for artificial-womb technology in the long run.</p></blockquote><p> What about the other problem? You know, that the wombs might work?</p><blockquote><p> The US Food and Drug Administration (FDA) will convene a meeting of independent advisers on 19–20 September to discuss regulatory and ethical considerations and what human trials for the technology might look like. The committee&#39;s discussion will be scrutinized by the handful of other groups around the world that are developing similar devices, and by bioethicists exploring the implications for health equity, reproductive rights and more.</p><p> ……</p><p> Safety questions won&#39;t be the only ethical concerns. The development of artificial wombs represents a “big transformational leap” that “solves lots of issues”, says David. But, she adds, “it also opens up a whole new slew of issues”. After the 2017 study <a target="_blank" rel="noreferrer noopener" href="https://www.nature.com/articles/d41586-023-02901-1?utm_source=tldrnewsletter#ref-CR1"><sup>1</sup></a> generated extensive media coverage, fears spread that artificial wombs could one day replace pregnancy.</p><p> ……</p><p> Those developing artificial wombs in the United States will also have to contend with a politically charged environment for reproductive rights. Flake and Mychaliska have been careful not to give any indication that an artificial womb could change the definition of fetal viability.</p></blockquote><p> So we have three concerns here.</p><p> One concern is that we might be able to replace pregnancy.</p><p> Another – I&#39;m trying to use neutral language here – is that we might be able to make possible earlier survival outside of the womb earlier, which might endanger reproductive rights? So we should instead consider not doing that?</p><p> And of course we have equity, the worry that if we make some people better off then other people might not also benefit the same amount, we did not bring enough cupcakes for the entire class.</p><p> I can&#39;t even, sometimes.</p><p> They say that full artificial pregnancies remain quite a long ways off. That is presumably true unless AI advances all tech development, but also once the technology reaches practical usefulness and gets a foot in the door I expect things to escalate quickly, with orders of magnitude more attention and funding, fueled by worries about declining birth rates and the I&#39;ll-pay-anything market of women who want children but are not medically able (or willing) to handle a pregnancy, or for whom it would otherwise be highly risky.</p><p> That also points the way towards heavy subsidies. If such devices do become viable, paying for them under at least some circumstances, in whole or in part, via insurance or otherwise, would be one very clear way to boost fertility in an efficient, highly targeted manner.</p><p> Hell, merely removing artificial cost and logistical barriers would help a lot. Small upfront expenses and stressors loom large.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/webdevMason/status/1706688988525027538">What might the future look like, if technology advances far enough?</a></p><blockquote><p> Mason: If/when gestation-as-a-service reaches a price point around $2k, our entire framework for reproductive rights will be turned on its head — especially if we&#39;re also able to print gametes on demand. Rudimentary foreshadowing w/ the growth of the surrogacy industry — it&#39;s gonna get weird.</p><p> There is essentially zero chance that low-cost artificial wombs won&#39;t be accompanied by parenting licenses or gestation permits of some kind. There is no way people will be let loose to order babies for the price of a MacBook.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16fe6635-6841-4995-8215-1f7b4ad41dd2_1284x1484.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YjT88Avga8xTvtZzw/ln7nisbeitlhigetoijg" alt="图像"></a></figure><p> Eliminating all the costs of getting a newborn to exist – say the government paid for all of it via IVF and surrogacy, so we don&#39;t have to imagine new tech – would lead to a lot more kids, but it would not make raising kids cheap.</p><p> I do think we are the kind of society that will tell people they cannot build housing or take life-saving medicine, and oscillate wildly on reproductive assistance between &#39;this process requires these additional things that cost $200,000 out of pocket&#39; and &#39;this is a human right that must be covered by insurance with no questions asked&#39; to &#39;this is not ethical in some situations so we are banning it in all of them&#39; on a whim. All things are possible.</p><p> What would be the right policy response? If the up-front price got down to $2k, we might have to find a way to restrict access, so people did not create children they were not prepared to raise and support – or at least, they did not do this beyond the number others want to raise to support, but presumably the adoption market falls through the floor because the reproductive tech is available to everyone.</p><p> This would be a happy problem. It would mean that our fertility issues are now going the other way, and are fully under our control to steer further.</p><br/><br/><a href="https://www.lesswrong.com/posts/YjT88Avga8xTvtZzw/fertility-roundup-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/YjT88Avga8xTvtZzw/fertility-roundup-2<guid ispermalink="false"> YjT88Avga8xTvtZzw</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 17 Oct 2023 13:20:07 GMT</pubDate></item></channel></rss>