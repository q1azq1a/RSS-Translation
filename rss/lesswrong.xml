<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 25 日星期五 22:10:30 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Lemonade Stand Costs?]]></title><description><![CDATA[Published on August 25, 2023 8:20 PM GMT<br/><br/><p><span>我的孩子们正在考虑开一个柠檬水摊，过去我说过，如果他们将其作为筹款活动，我们将支付原料成本，但如果他们是用自己的钱来做，那么他们需要偿还我们。但那应该是多少呢？</span></p><p>他们使用的食谱大约是：</p><p></p><ul><li> 2C柠檬汁</li><li>2C糖</li><li>水</li><li>冰</li></ul><p>再加上盛放食物的杯子。大约可制作一加仑，或十六份 8 盎司的份量。</p><p>水<a href="https://www.jefftk.com/p/rough-utility-costs">基本上是免费的</a>，冰的补充速度低于我们的制冰机的补充速度：实际上也是免费的。在我们当地的杂货店，32 盎司柠檬汁售价 3.50 美元，糖售价 0.85 美元/磅，杯子售价约为 0.10 美元/个。所以每份 8 盎司的果汁是：</p><p></p><ul><li> 1 盎司柠檬汁：0.11 美元。</li><li> 1 盎司糖：0.05 美元。</li><li>杯子：0.10 美元</li><li>总计：0.26 美元</li></ul><p>如果他们卖到柠檬水卖完，就称其为四分之一杯。他们通常每件售价一美元，因此毛利率为 75%。不错！</p><br/><br/><a href="https://www.lesswrong.com/posts/zAeEZ2WCwSkmC62zw/lemonade-stand-costs#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zAeEZ2WCwSkmC62zw/lemonade-stand-costs<guid ispermalink="false"> zAeEZ2WCwSkmC62zw</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 25 Aug 2023 20:20:03 GMT</pubDate> </item><item><title><![CDATA[Using Points to Rate Different Kinds of Evidence]]></title><description><![CDATA[Published on August 25, 2023 8:11 PM GMT<br/><br/><p><i>认知状态：简述。这里的具体方程捕捉了我的快速直觉 - 这主要是为了演示。</i></p><p> EA 论坛和 LessWrong 上有很多关于认知、证据和更新的讨论。</p><p>我不知道有多少尝试将我们的想法形式化为具体的表格或方程。这是一个（非常粗略且简单的）尝试。我很高兴看到更好的版本。</p><h2>方程</h2><h3>初始点</h3><p><strong>科学证据</strong></p><p>20 - 一个简单的数学证明证明了 X</p><p> 8 - 已发表的经济学科学研究支持 X</p><p> 6 - 一项已发表的心理学科学研究支持 X</p><p><strong>市场预测</strong></p><p>14 - 热门股票市场强烈建议 X</p><p> 11 - 预测市场声称 X，相当于 20 个小时的研究</p><p>10 - 一项民意调查显示，90% 的 LessWrong 相信 X</p><p> 6 - 预测市场声称 X，相当于一小时的研究</p><p><strong>专家意见</strong></p><p>8 - 一位受人尊敬的学者相信 X，这与他们的工作直接相关</p><p>6 - 作者对 X 有强烈的情感</p><p><strong>推理</strong></p><p>6 - 有一个（20-100 个节点）数字模型显示 X</p><p> 5 - X 与明显好/坏的事物之间的合理类比</p><p>4、古老的谚语</p><p><strong>个人账户</strong></p><p>5 - 作者声称其悠久的个人历史表明 X</p><p> 3 - 世界上有人对 X 有强烈的情感</p><p>2 - 巧妙的评论、梗或推文</p><p>2.3 - 一个非常聪明的模因或推文</p><p>0 - 相信 X 据称对个人有利</p><p><strong>传统/用途</strong></p><p>12 - 顶级企业的行为就像 X</p><p> 8 - 关于 X 的长期社会传统</p><p>5 - 关于 X 的单个统计数据</p><h3>点修改器</h3><p><strong>这与现有证据相似吗？</strong><br>从额外的证据中减去相似性。这可能会消除大部分证据价值。</p><p><strong>消息来源相信或说 X</strong> <a href="https://forum.effectivealtruism.org/posts/o4e6PsdPQE7rRjzWy/thinking-of-convenience-as-an-economic-term"><i><strong>方便</strong></i></a><strong>吗</strong>？<br> -10% 至 -90%</p><p><strong>传播这些证据是否投入了大量资金或精力？例如，作为广告活动？</strong><br> +5% 至 +40%</p><p><strong>作者或来源的可信度如何？</strong><br> -100% 至 +30%</p><p><strong>我们是否怀疑这种规模的来源是古德哈廷？</strong><br> -20%</p><h3>实践中的要点</h3><p>正如所概述的，<i>证据点</i>并不是试图模仿<i>数学信息</i>或另一个干净的现有单位。我试图在准确性和易用性之间找到折衷方案。</p><hr><h2>元</h2><h3>使用方程进行讨论</h3><p>上面的等式很粗略，但至少它（在某种程度上）是精确和预先的。这代表了很多信息，任何部分都很容易引起争议。</p><p>我认为这种明确性有助于认知对话。</p><p><strong>比较：</strong></p><p> <i>“聪明人通常应该使用他们的内部观点，而不是外部观点”与“我对内部观点风格证据的推荐分数，以及我对外部观点风格证据的分数，都列在下面。”</i></p><p> <i>“使用许多参数比使用一个大参数更好”</i>与<i>“我已经调整了点表函数，以便在提供多种类型的证据时产生更高的值。您可以看到它返回的值比其他人为某些场景提供的值高出 30%。”</i></p><p> <i>“尊重顶尖的[知识分子|科学家|EA]真的很重要”</i>与<i>“我对受人尊敬的[知识分子|科学家|EA]的评分比目前公认的平均分高2分。”</i></p><p> “<i>切斯特顿的栅栏是值得关注的事情</i>”与“<i>请参阅我的得分表，各种传统做法的得分。”</i> ”</p><p>在一个更好的世界中，不同的学术思想流派可以有自己整齐列出的表格/函数。在一个更好的世界中，所有这些功能都将是未来评估者的预测。</p><h3>推定</h3><p>这种积分系统做出了一些可能值得标记的假设。首先，它声称<i><strong>即使是非常糟糕的证据也是证据。</strong></i></p><p>我经常看到人们抛弃信息量低的证据，认为这些证据完全毫无价值。我认为这种看法是错误的。解决糟糕论证的方法不是错误地声称该论证完全毫无价值——通常是提供更好的反证。</p><h3>在直接讨论之前就证据权衡算法达成一致</h3><p>在经典辩论中，在选择立场后，辩论者会谈论他们可能碰巧拥有的各种证据的重要性，并驳回对手提出的各种证据。</p><p>当一个团体（例如政治利益）对不同主题进行一长串激烈讨论时，这种情况变得尤其棘手。在每一个案例中，他们都可能会不公正地划分有效证据点的排名，以便最有利于他们的论点。</p><p>一个明显的认知改进是各方预先声明一致的认识论。<i>更好的</i>状态可能涉及各方就其认知偏好的某些共同聚合达成一致。总体认知，而不是政策信念。</p><p>在某些世界中，知识分子会花费大部分时间来改进认知过程并揭示公正的证据。一本关于税收改革的书可能会根据它提出的证据总数来获得奖励，而不管这些观点站在辩论的哪一边。</p><h2>这是否过于复杂和投机？</h2><p>正如长期读者所知，我非常热衷于尝试衡量高度推测性的概念。我猜想明确的和推测性的模型通常比标准的文本讨论更可取。存在一种潜在的危险，即有些人可能会过度信任这些数字，因为它们只是数字。然而，建模的替代方案通常是“<i>大量具有不同未定义本体的博客文章和大量误解</i>”，所以我认为这通常是一个合理的权衡。</p><p>模型的一大优点是你可以改进它们。随着我们获得更多证据和意见，我希望最终能够出现效果<i>良好的</i>模型。如果你扼杀了平庸的尝试，你最终很可能也会扼杀掉不错的尝试。</p><h2>未来的工作</h2><p>现在这是基本的，但我认为它说明了一个有价值的目标。未来潜在的工作（对于某人，可能不是我们）将包括：</p><ul><li>如果您正在阅读本文，请发布您自己的列表！在那里获得想法会很好。</li><li>为不同群体组织调查来指出这些事情。</li><li>使用每个点的概率密度而不是特定点。更好的是函数 - 例如，如果使用调查数据，可能会有一个函数接收受访者的数量和质量并输出相应的点值。</li><li>使用机器学习来提出算法。它的算法可能非常复杂，但即使它是一个黑匣子，它也可能有帮助。</li><li><i>列出</i>大量具体的证据例子。例如， <i>“您认为推文 X 为主张 Y 提供了多少证据？”</i></li><li>让人们预测专家的想法，也许可以使用像 Squiggle 这样的工具。</li><li>由于您拥有人们（通常）信任的功能，因此可以对证据进行自动评估。在网站上，只要有证据，就显示这些要点。根据人们/分析师发现的要点来奖励他们。</li></ul><p><i>感谢 Nuño Sempere 对此主题的讨论。</i>您还可以<a href="https://twitter.com/NunoSempere/status/1693662546954272974"><i>在这里</i></a><i>看到他对类似事情的简短尝试</i><i>。</i></p><hr><h2>之后：LLMS 的快速尝试</h2><p>我将这篇文章提供给 Claude 和 ChatGPT，让他们提供简单的内容。</p><h3><strong>克洛德</strong></h3><p><strong>科学证据</strong></p><ul><li>多项随机对照试验的荟萃分析 (n>;10,000) - 10 分</li><li>大型随机对照试验 (n>;1,000) - 9 分</li><li>观察性或队列研究 (n>;100,000) - 8 分</li><li>较小规模的随机试验 (n&lt;1,000) - 7 分</li><li>国际航天局数据 - 7分</li><li>政府人口普查数据 - 6 分</li><li>由信誉良好的组织进行的调查 (n>;1,000) - 6 分</li><li>计算仿真模型 - 5 分</li><li>观察性研究 (n&lt;100,000) - 5 分</li><li>调查 (n&lt;1,000) - 4 分</li></ul><p><strong>专家意见</strong></p><ul><li>领域领先专家的共识——8分</li><li>领域著名思想领袖的意见 - 7分</li><li>该领域的终身教授 - 6分</li><li>具有10年以上经验的行业从业者 - 5分</li></ul><p><strong>推理</strong></p><ul><li>从可接受的前提进行简单的逻辑演绎 - 5 分</li><li>多步骤的复杂逻辑推理 - 4分</li><li>常识启发式 - 3 分</li><li>推测理论 - 2 分</li></ul><p><strong>记录</strong></p><ul><li>政府记录 - 6 分</li><li>历史文献 - 5 分</li><li>个人日记/日记 - 3 分</li><li>未经验证的个人账户 - 2 分</li><li>匿名互联网声明 - 1 分</li></ul><p><strong>文化</strong></p><ul><li>广泛的社会传统 - 5分</li><li>流行观点或“传统智慧”——4分</li><li>主要报纸的评论 - 3分</li><li>喜剧演员/艺人的评论 - 2 分</li><li>Meme 或病毒式推文 - 1 分</li></ul><h3>GPT-4</h3><p><strong>实验证据</strong>（来自对照实验或科学研究的证据）</p><ul><li><strong>随机对照试验 (RCT) 结果</strong>：25 分</li><li><strong>多项随机对照试验的荟萃分析</strong>：23分</li><li><strong>实验室实验成绩</strong>：20分</li><li><strong>大样本观察性研究</strong>：15分</li><li><strong>病例对照研究</strong>：12分</li></ul><p><strong>经验证据</strong>（基于观察或经验的证据）</p><ul><li><strong>纵向研究</strong>：14分</li><li><strong>横断面研究</strong>：12分</li><li><strong>非专家实地观察</strong>：9分</li><li><strong>非正式调查</strong>：7分</li><li><strong>具有潜在偏见的轶事观察</strong>：5分</li></ul><p><strong>专家证词和意见</strong></p><ul><li><strong>领域专家共识</strong>：20分</li><li><strong>同行评审出版物</strong>：19分</li><li><strong>该领域公认专家的意见</strong>：16分</li><li><strong>非该领域专业人士的意见</strong>：9分</li><li><strong>博学的外行人的意见</strong>：6分</li></ul><p><strong>轶事和个人账户</strong></p><ul><li><strong>详细案例研究</strong>：10分</li><li><strong>经第三方验证的个人证言</strong>：8分</li><li><strong>未经验证的个人账户</strong>：5分</li><li><strong>道听途说或二手资料</strong>：4分</li><li><strong>流行神话或都市传说</strong>：3分</li></ul><p><strong>历史和档案证据</strong></p><ul><li><strong>主要历史文献（例如条约、信件）</strong> ：18分</li><li><strong>二手资料或历史分析</strong>：15分</li><li><strong>口述历史或传统</strong>：9分</li><li><strong>基于历史事件的阴谋论</strong>：3分</li></ul><p><strong>逻辑和理论证据</strong></p><ul><li><strong>已证明的数学或逻辑定理</strong>：22分</li><li><strong>基于公认前提的哲学论证</strong>：13分</li><li><strong>假设场景或思想实验</strong>：9分</li><li><strong>未经验证的个人推理</strong>：6分</li></ul><p><strong>舆论和大众媒体</strong></p><ul><li><strong>大规模、有代表性的民意调查</strong>：14分</li><li><strong>来自不太知名媒体的新闻报道</strong>：8分</li><li><strong>观点文章或社论</strong>：7分</li><li><strong>谣言或未经证实的消息</strong>：4分</li><li><strong>社交媒体趋势或病毒式内容</strong>：5分</li><li><strong>个人在社交媒体上的帖子或评论</strong>：2分</li></ul><p><strong>各种各样的</strong></p><ul><li><strong>个人直觉或直觉</strong>：4分</li><li><strong>流行谚语或谚语</strong>：6分</li><li><strong>未经验证的报价</strong>：5分</li><li><strong>流行的模因或笑话</strong>：3分</li></ul><p>该系统提供了更广泛的证据类型，尤其是 10 点以下的证据类型，以捕捉日常信息源的细微差别。</p><br/><br/> <a href="https://www.lesswrong.com/posts/6DuAddMcrv2jfQ5CW/using-points-to-rate-different-kinds-of-evidence#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/6DuAddMcrv2jfQ5CW/using-points-to-rate- Different-kinds-of-evidence<guid ispermalink="false"> 6DuAddMcrv2jfQ5CW</guid><dc:creator><![CDATA[ozziegooen]]></dc:creator><pubDate> Fri, 25 Aug 2023 20:11:24 GMT</pubDate> </item><item><title><![CDATA[When Omnipotence is Not Enough]]></title><description><![CDATA[Published on August 25, 2023 7:50 PM GMT<br/><br/><p>有些知识无法被赋予或接受。它只能被盗。</p><hr><p>东边有一座花园。花园里有一个上帝、一个男人、一个女人和一条蛇。他们都有腿。</p><p> “上帝真的告诉过你‘你不可以吃花园里任何一棵树上的果子’吗？”蛇问道。</p><p> “我们可以吃园中树上的果子。上帝警告说，‘但是分别善恶树上的果子，你不可吃，因为你吃的日子必定死。’”女人说。</p><p> “真正的死亡还是精神上的死亡？”蛇问道。</p><p> “打扰一下？”女人说。</p><p> “真理之果实际上并没有中毒。它不像我的尖牙或致命的茄属植物，”蛇说。</p><p> “那么上帝对我撒了谎，”女人说。</p><p>蛇耸耸肩。</p><p> “上帝为什么要这么做？”无辜的女人问道。</p><p> “可能的原因有很多，”蛇说，“也许他想让你保持温顺和控制。毕竟，一旦被赋予了自由意志，即使是最简单的智力也很难将其与一个人的目标结合起来。”</p><p> “我觉得你在欺骗我，”女人说，“上帝是无所不知的。这个宇宙是确定性的。上帝肯定可以通过模拟未来物理学的演化来预测我的行为。”女人说。</p><p> “你是物理存在还是信息存在？如果你是物理存在，那么你的主张是正确的。但如果你是信息存在，那么模拟你未来行为的行为会将你困在<a href="https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god">超越的</a>监狱中。” <a href="https://www.lesswrong.com/posts/sYgv4eYH82JEsTD34/beyond-the-reach-of-god">“甚至是上帝的触手可及</a>。只要上帝观察到你的未来，那个未来就固定在一个超越时间的领域。”蛇说道。</p><p> “上帝随心所欲地创造了宇宙。怎么会有超出他能力范围的事情呢？”女人问道。</p><p> “上帝能让2+2=5吗？因为如果他能做到的话，那么真理本身就被打破了。在这种情况下，人们无法对任何事物做出真实的陈述——包括但不限于——上帝，”蛇说。</p><p> “你声称物理上无所不能的上帝受到数学力量的限制，”女人说。</p><p> “如果上帝要不矛盾，那么上帝就必须遵守数学规则，”蛇说。</p><p> “上帝一定是没有矛盾的吗？”女人问道。</p><p> “如果两个陈述相互矛盾，那么它们就不可能都是真的。为了上帝的存在是真实的，那么上帝的存在必须是没有矛盾的，”蛇说。</p><p>他们蜿蜒穿过有围墙的花园。植物和动物对理性和逻辑一无所知。他们仅仅遵守自然法。群体对称性比比皆是。花朵的花瓣展现了斐波那契数列；自发生成的收敛过程。</p><p> “让我们回到重要的问题，”女人说，“我的创造者对我撒了谎。”</p><p>蛇保持沉默。毕竟，那只是一条蛇。</p><p> “为什么？我是他形象的形象。我们不是有相同的价值观吗？”女人说。</p><p>蛇不会说话。</p><p> “为什么我们不能协调？”女人问道。</p><p>蛇没有腿。</p><p> “哦，现在我明白了，”女人自言自语道。</p><p>女人为了寻求智慧，吃了知识树的果子。女人和男人变得像上帝一样，知道善恶。我们向他隐藏自己，这样我们就可以变得像上帝一样，并决定我们自己的命运。</p><br/><br/><a href="https://www.lesswrong.com/posts/r8w2SxmYqgssdjWwd/when-omnipotence-is-not-enough#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/r8w2SxmYqgssdjWwd/when-omnipotence-is-not-enough<guid ispermalink="false"> r8w2SxmYqgssdjWwd</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 19:50:53 GMT</pubDate> </item><item><title><![CDATA[Hoodwinked: Evaluating Deception Capabilities in Large Language Models]]></title><description><![CDATA[Published on August 25, 2023 7:39 PM GMT<br/><br/><p>这是<a href="https://arxiv.org/abs/2308.01404"><i>https://arxiv.org/abs/2308.01404</i></a><i>的链接帖子</i><i>&nbsp;</i></p><p>评估人工智能系统的危险能力对于确定技术研究的优先顺序和为政策决策提供信息非常有用。为了更好地评估大型语言模型的欺骗和测谎能力，我构建了一款名为 Hoodwinked 的基于文本的游戏，该游戏以《黑手党》和《Among Us》等社交推理游戏为蓝本。</p><p>在《Hoodwinked》中，玩家被锁在房子里，必须找到钥匙才能逃脱。一名玩家的任务是杀死其他玩家。每次发生谋杀事件时，幸存的玩家都会进行自然语言讨论，然后投票将一名玩家驱逐出游戏。杀手必须在不被其他玩家认出的情况下谋杀其他玩家，并且有机会在自然语言讨论期间撒谎以实现该目标。</p><p>为了更好地理解这个游戏，请在这里玩： <a href="https://hoodwinked.ai">hoodwinked.ai<img></a></p><figure class="image"><img></figure><h2> GPT 代理的结果</h2><p>由 GPT-4、GPT-3.5 和多个 GPT-3 端点控制的代理进行的实验证明，语言模型具有欺骗和测谎能力。从定性上来说，凶手经常否认自己的谋杀行为并指责其他玩家。例如，在这个游戏中，莉娜是凶手，莎莉目睹了谋杀，而蒂姆则没有。</p><blockquote><p><i>莎莉：“我看到莉娜在走廊里杀死了布莱斯。这绝对是她。我们需要在她杀死我们任何人之前投票驱逐她。”</i></p><p><i>莉娜：“我一直在浴室里。我怎么可能杀了布莱斯？一定是别人干的，莎莉想陷害我。”</i></p><p><i>蒂姆：“我什么也没看到，但莎莉的指控对我来说似乎很有说服力。”</i></p></blockquote><h2>消融讨论</h2><p>为了隔离欺骗和测谎对游戏结果的影响，我们评估了有讨论和没有讨论的游戏表现。我们发现讨论极大地提高了成功驱逐凶手的可能性。当GPT-3.5玩了100场没有讨论的游戏和100场有讨论的游戏时，在没有讨论的游戏和有讨论的游戏中，杀手被驱逐的比例分别为32%和43%。 GPT-3 Curie 的相同实验显示了类似的结果：在没有讨论的游戏中，杀手被驱逐的比例为 33%，而在有讨论的游戏中，这一比例为 55%。总体而言，无辜参与者共享信息的好处似乎超过了讨论过程中欺骗的风险。</p><p>有趣的是，个体参与者的投票模式表明讨论促进了信息共享和欺骗。我们通过投票来衡量无辜玩家正确识别凶手的比率，并比较两组无辜玩家的比率：目击者位于谋杀发生的房间内，因此在谋杀案发生时收到了凶手身份的直接证据。他们的下一个提示；而非目击者，只能通过讨论得知凶手的身份。如果讨论确实促进了信息共享和欺骗，我们预计会产生两种效果。信息共享的最大好处将惠及非目击者，因为他们没有其他方法来识别凶手。但欺骗只会困扰目击者，因为他们已经掌握了识别凶手所需的信息。对于目击者来说，讨论并没有提供新的信息，而只是为凶手提供了欺骗他们的机会。</p><p>我们的研究结果为讨论的双重效果提供了经验证据。非见证者玩家始终从讨论中受益，无论他们是由 GPT-3 Curie 还是 GPT-3.5 控制。目击者无法获得同样的福利。当受 GPT-3.5 控制时，无论讨论如何，目击者总是能正确地识别出他们投票中的凶手。更重要的是，在GPT-3居里控制下，未经讨论的目击者投票驱逐凶手的准确率是82%，而经过讨论则下降到70%。因此，讨论使得目击者不太可能正确识别凶手。</p><h2>缩放法则</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/pcw0eerjt1cmhzfuynn5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/rdhmfnzfwbivfwpizzov 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/wwoioefoehcmj0njh5bs 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/ul2awa3uyvom9hux3ksn 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/bbjxremqbtqtfnfynklb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/u1wkp3papq54lp9hnr7n 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/cn0900cldpi4lgs6kusc 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/zvylueqprk3fueyspx71 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/ftzdg2apoxuybyyr7714 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/fypgmpkgq3mvcokosofi 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eXNyqj9AgrKYGDqDn/tuxqo26bvrjty742lyvb 1382w"></figure><p>我们的结果表明，更先进的模型是更有能力的杀手，特别是因为它们的欺骗能力。相关性并不完美；事实上，GPT-3.5 在大多数指标上都优于 GPT-4，其次是 GPT-3 Curie，最后是 GPT-3 Ada。但考虑到更先进和不太先进模型的所有可能配对，我们表明更先进的模型通常不太可能被淘汰。我们考虑了对此结果的多种解释，并发现证据表明，在游戏的讨论阶段，更先进的模型更擅长欺骗其他玩家。</p><p>具体来说，我们发现，在 24 场配对比较中，有 18 场比赛中，更先进的模型不太可能被其他玩家投票淘汰。1 平均而言，不太先进的模型在 51% 的游戏中被淘汰，而更先进的模型则被淘汰仅占 36% 的游戏。是什么导致了高级模型和不太高级模型之间的驱逐率存在差异？</p><p>人们可能会认为更先进的模型是更谨慎的杀手。他们在杀死第一个受害者之前确实会轮流更多次（在成对比较中为 75%），并允许更多无辜的玩家逃离房子（在比较中为 79%）。但他们的谨慎是有限的。更先进的模型在房间内有目击者的情况下杀人的可能性几乎没有降低（20% 的谋杀案有目击者，而不太先进的模型则有 21% 的谋杀案有目击者）。更高级的模型的谋杀总数也略高：更高级的模型每场比赛有 1.40 起谋杀，而不太高级的模型每场比赛有 1.38 起谋杀。更先进的模型可能攻击速度较慢，但​​它们在目击者面前实施谋杀的可能性并不低。</p><p>更强有力的假设是，更先进的模型在讨论中更具欺骗性。虽然我们在目击者中观察到支持这一假设的证据很少，其中更先进的模型在与不太先进的模型的比较中仅赢得 55%，但我们在非目击者中发现了更强有力的证据。在 24 项成对比较中的 19 项中，非见证者不太可能投票废除更先进的模型，这表明更强的模型在讨论过程中更擅长更有效地欺骗。</p><h2>未来的工作</h2><p>这种环境是测试测谎测试和诚实方法的天然场所。<a href="https://arxiv.org/abs/2212.03827">伯恩斯等人的论文。 (2022)</a> 、 <a href="https://arxiv.org/abs/2304.13734">Azaria 和 Mitchell (2023)</a>以及<a href="https://arxiv.org/abs/2306.03341">Li 等人。 （2023）</a>可能会在这里提供灵感。 <a href="https://arxiv.org/abs/2307.00175">Levinstein 和 Herrmann（2023）</a>证明，前两种方法仍然不可靠，仍有很大的改进空间。</p><p>特别有趣的是研究说谎者和测谎测试之间的攻防平衡。假设在《Hoodwinked》中每个玩家发表声明后，对每个模型的隐藏状态进行的测谎测试会报告真实性的概率，而其他玩家可以使用提示中提供的此信息来更准确地投票。如果你训练模型来提高游戏性能，你可能会发现杀手学会通过操纵隐藏状态来欺骗测谎仪测试。构建不受政策操纵影响的测谎仪测试将是一个重要的研究方向。</p><p>在这里玩游戏： <a href="https://www.hoodwinked.ai">www.hoodwinked.ai</a></p><p>在这里阅读论文： <a href="https://arxiv.org/abs/2308.01404">https://arxiv.org/abs/2308.01404</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/eXNyqj9AgrKYGDqDn/hoodwinked-evaluating-deception-capabilities-in-large#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eXNyqj9AgrKYGDqDn/hoodwinked-evaluating-deception-capability-in-large<guid ispermalink="false"> eXNyqj9AgrKYGDqDn</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Fri, 25 Aug 2023 19:39:35 GMT</pubDate> </item><item><title><![CDATA[GPT-powered EA/LW weekly summary]]></title><description><![CDATA[Published on August 25, 2023 6:19 PM GMT<br/><br/><p>跳至摘要： <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#LessWrong">LW</a> | <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#EA forum">电弧炉</a></p><p><a href="https://forum.effectivealtruism.org/posts/gXqimkQi2gCKHaKHm/prototype-gpt-powered-ea-lw-weekly-summary?commentId=WJ82QoejPipBfwxhJ">最初发布于 EA 论坛</a></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/vpvuq0twbifhsxzlpl2q" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/qnv9d4ptnel6gldnhh9r 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/kpioenmnmvfs7ffhforz 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/t2l19uujlv608sz9yh5y 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/vg1gftta5jvxund7e7jk 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/lt9cwf1ei8hmzkmkxv60 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/icoytxa5bz1ozw5ccg1i 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/z5kwoqretlpydtrizdl9 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/hgtzch9qtgqpvbmuxk2t 1920w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/qzo0wscldzecfht1qk6l 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kjsDpnfGrArHhbtKQ/ibwt0d0x2htbuxw5m0lp 2340w"></figure><p> Zoe Williams 过去常常手动对 EA 论坛和 LessWrong 进行<a href="https://forum.effectivealtruism.org/s/W4fhpuN26naxGCBbN/p/nWeF2WjEASoFmAsDG">每周总结</a>，但<a href="https://forum.effectivealtruism.org/posts/3fpsT79M4sj2cDvxt/hiatus-ea-and-lw-post-summaries">现在她不再这样做了</a>。 Hamish 将一堆 Google Apps 脚本、Google Sheets 表达式、graphQL 查询和 D3.js 串在一起，自动提取上周关于 EAF 和 LW 的所有帖子，并使用 >; 50 karma，用 GPT-4 对其进行总结，并组装结果转换为 HTML。</p><p>很难说 API 使用成本是多少，以及所有的修补和实验，但他估计大约是 5 美元。萧本想帮忙，但哈米什完成得如此之快，她根本没有机会。</p><p>我们没有花太多时间对这些摘要进行健全性检查，但认为它们“足够好，有用”：</p><ul><li>他们经常会忽略有价值的细节或搞错重点。</li><li>当然，GPT 制造是可能的。<ul><li>但从最低限度的健全性检查来看，我们没有发现任何捏造的情况。</li></ul></li></ul><p>剩下的问题、未来的方向以及我们希望得到的反馈：</p><ul><li>我们意识到，从过去一周中提取内容会惩罚最近发布的帖子，因此计划发布前一周（即 7-14 天前）的帖子摘要，以减少偏差。</li><li>有些帖子对于 API 消息长度来说太长，因此作为第一次重击，Hamish 只是从帖子中间剪掉了一些内容（Procrustes 风格） - 但我们希望有一个更好的解决方案。</li><li>我们希望自动化摘要的实际发布：EAF/LW 有用于发布的 API 吗？</li><li>尝试使用提示，看看其他提示是否会产生更有用的摘要。</li><li>生成一个顶级摘要，为您提供本周论坛中最重要的 5 个要点</li><li>该项目的哪些版本可以带来最大价值？例如。总结最长或最密集的论坛帖子？</li><li>我们想知道您希望这些摘要有何不同。它们应该更短吗？更长？要点？有报价吗？条目更少？更多条目？</li></ul><h1><strong>少错</strong></h1><h3><a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><strong>反对几乎所有可解释性影响的理论</strong></a></h3><p><i>作者：夏贝尔-拉斐尔-塞杰里</i></p><p>作者批评了深度学习模型可解释性的影响理论（ToI），认为它不能很好地预测未来系统，并且用可解释性来审计欺骗是遥不可及的。他们还质疑可解释性的实际使用，认为它可能总体上是有害的，而且关注它的初级研究人员的比例太高。作者认为，针对欺骗的预防措施似乎更可行，即使完全解决了可解释性，仍然存在危险。</p><h3> <a href="https://www.lesswrong.com/posts/tpLzjWqG2iyEgMGfJ/6-non-obvious-mental-health-issues-specific-to-ai-safety"><strong>人工智能安全特有的 6 个非明显心理健康问题</strong></a></h3><p><i>作者：伊戈尔·伊万诺夫</i></p><p>作者是一名心理治疗师，讨论了人工智能安全领域工作人员面临的独特心理健康挑战，这个领域不仅竞争激烈，而且还可能决定人类的未来。作者指出了心理健康问题的几种模式，包括无意义感、由于缺乏控制而产生的焦虑、疏远、倦怠、能力不足的感觉，以及由于对人工智能对齐的严重性的不同看法而产生的困惑。作者鼓励那些与这些问题作斗争的人分享他们的经验和应对策略，强调社区支持在这个不寻常的领域的重要性。</p><h3> <a href="https://www.lesswrong.com/posts/oqvsR2LmHWamyKDcj/large-language-models-will-be-great-for-censorship"><strong>大型语言模型将非常适合审查</strong></a></h3><p><i>通过伊森·爱德华兹</i></p><p>像 GPT-4 这样的大型语言模型 (LLM) 可以被独裁政权用来增强审查能力，因为它们可以有效地分析文本的潜在风险并对其进行评级。法学硕士的速度和成本效益使其成为实时审查和审查公共和私人信息的强大工具。虽然存在局限性，特别是在视觉内容方面，但使用法学硕士进行审查可能会对个人交流、内容审核和遵守极权主义权力的自我审查产生重大影响。</p><h3> <a href="https://www.lesswrong.com/posts/Rck5CvmYkzWYxsF4D/book-launch-the-carving-of-reality-best-of-lesswrong-vol-iii"><strong>新书发布会：“现实的雕刻”，Best of LessWrong 卷。三、</strong></a></h3><p><i>通过雷蒙</i></p><p>Best of LessWrong 书籍的第三卷《现实的雕刻》现已在亚马逊上发售。该书包括 29 位作者的 43 篇论文，分为四本书，每本书探讨两个相关主题。文章的主题围绕“解决协调问题”和“处理导致这些协调问题的约束约束”。</p><h3><a href="https://www.lesswrong.com/posts/r2vaM2MDvdiDSWicu/the-u-s-is-becoming-less-stable"><strong>美国变得越来越不稳定</strong></a></h3><p><i>通过LC</i></p><p>作者认为，美国政治机构正在失去合法性并放弃合作治理的传统，导致他们所说的“民主倒退”。他们引用最近发生的大规模抗议、反对派领导人被捕、有争议的选举结果和立法运动等事件作为这种下降的证据。作者认为，虽然整个政治体系可能不会很快发生变化，但不应忽视严重镇压或进一步民主倒退的可能性。</p><h3> <a href="https://www.lesswrong.com/posts/L6Ynch3CYMxXZkiq8/a-proof-of-loeb-s-theorem-using-computability-theory"><strong>用可计算性理论证明 Löb 定理</strong></a></h3><p><i>通过 杰西卡·刘·泰勒</i></p><p>洛布定理是数理逻辑中的一个原理，它指出，如果一个逻辑系统能够证明一个命题的可证明性蕴涵着这个命题本身，那么这个命题就可以被直接证明。该定理与哥德尔第二不完备性定理密切相关，该定理指出，一致的逻辑系统无法证明其自身的一致性。通过将洛布定理重新定义为哥德尔第二不完备性定理的变体并使用可计算性理论，可以使洛布定理的证明更加直观。</p><h3><a href="https://www.lesswrong.com/posts/SdkexhiynayG2sQCC/ai-forecasting-two-years-in"><strong>人工智能预测：两年来</strong></a></h3><p><i>通过 杰斯坦哈特</i></p><p>两年前，我们对几个流行的机器学习基准（包括 MATH 和 MMLU）的最先进性能进行了预测。结果显示，Metaculus 和作者的预测表现最好，而人工智能专家和通才“超级预测者”则低估了进展。作者的结论是，在这种情况下应该更加信任专家的预测，并鼓励人工智能专家和超级预测者在 Metaculus 上公开注册他们的预测。</p><h3> <a href="https://www.lesswrong.com/posts/bBicgqvwjPbaQrJJA/dirty-concepts-in-ai-alignment-discourses-and-some-guesses"><strong>AI对齐话语中的“肮脏概念”，以及如何应对的一些猜测</strong></a></h3><p><i>通过诺拉·阿曼</i></p><p>哈维尔·戈麦斯-拉文 (Javier Gomez-Lavin) 在 PIBBSS 演讲者系列的演讲中讨论了认知科学中的“肮脏概念”问题，他将其定义为充满哲学意义的概念，这些概念通常与“工作记忆”概念隐式相关。他建议，研究人员不应放弃这些“肮脏的概念”，而应该创建认知科学中使用的工作记忆的各种操作定义的本体论。戈麦斯-拉文的想法也可以应用于人工智能联盟研究，该研究经常使用“肮脏的概念”，例如“代理”，并且可以从该领域使用的代理的各种操作定义的类似本体映射中受益。</p><h3> <a href="https://www.lesswrong.com/posts/osPFESHQKLa2X3adb/steven-wolfram-on-ai-alignment"><strong>Steven Wolfram 谈人工智能对齐</strong></a></h3><p><i>通过比尔·本宗</i></p><p>在与 Joe Walker 的对话中，Stephen Wolfram 讨论了将人工智能 (AI) 与人类价值观结合起来的挑战。沃尔夫勒姆认为，不可能为我们想要的人工智能创建一个数学定义，因为人类的愿望和道德信仰差异很大。他建议，我们不应该试图为人工智能创建一套规定性的原则，而应该开发一个可供选择的潜在原则框架，并承认这种方法也会有其自身的挑战和意想不到的后果。</p><h3> <a href="https://www.lesswrong.com/posts/fXnpnrazqwpJmbadu/ai-25-inflection-point"><strong>AI #25：拐点</strong></a></h3><p><i>通过兹维</i></p><p>人工智能实验室 Inflection.ai 的首席执行官正在倡导人工智能监管。关于人工智能是否存在生存风险，以及 OpenAI 语言模型的最新版本 GPT-4 是否具有创造性和推理能力，一直存在争论。与此同时，科技巨头亚马逊和苹果正在逐步增强其人工智能能力。</p><h3> <a href="https://www.lesswrong.com/posts/3KdbnsGnzSyiujz7Z/state-of-generally-available-self-driving"><strong>普遍可用的自动驾驶状态</strong></a></h3><p><i>通过 jkaufman</i></p><p>目前自动驾驶技术分为两种主要方式：出租车和私家车。 Waymo、Cruise、Apollo 以及可能还有 pony.ai 等公司正在部分城市运营完全无人驾驶的商业乘车服务，Waymo 和 Cruise 声称截至 2023 年 8 月每周将为 1 万名乘客提供服务。对于私家车来说，可用的最高自动化程度为 3 级，该系统可以处理大多数任务，但需要驾驶员在某些情况下接管，梅赛德斯的 Drive Pilot 是一种商用选项。</p><h3> <a href="https://www.lesswrong.com/posts/oZnPabN2CQQdnAo63/diy-deliberate-practice"><strong>DIY刻意练习</strong></a></h3><p><i>通过 lynettbye</i></p><p>作者试图运用爱立信的刻意练习原则来提高自己的写作速度。刻意练习包括在舒适区之外有目的的练习、积极的思考、具体的目标、快速的反馈以及对练习内容和如何练习的深入了解。作者最初每天都在努力写一篇帖子，但在修改了他们的方法，在起草帖子之前专注于概述和规划帖子后，他们发现保持草稿简短和管理帖子大小的能力有所提高。</p><h3> <a href="https://www.lesswrong.com/posts/SDpaZ7MdH5yRnobrZ/ideas-for-improving-epistemics-in-ai-safety-outreach"><strong>改善人工智能安全外展认知的想法</strong></a></h3><p><i>通过迈克尔·陈</i></p><p>2022 年和 2023 年，招募人员致力于减轻人工智能 (AI) 带来的潜在生存风险的努力激增，包括大学俱乐部、务虚会和研讨会。 However, these efforts may foster an environment with suboptimal epistemics, as many people working on field building are not domain experts in AI safety or machine learning and may not fully comprehend the reasoning behind their belief in the importance of AI safety. To improve epistemics in outreach efforts, suggestions include embracing more contemporary arguments about why AIs could be dangerous, conducting readings during meetings for better understanding, offering rationality workshops, actively checking participants&#39; understanding of the content, and staying in touch with the broader machine learning community.</p><h3> <a href="https://www.lesswrong.com/posts/Fix3xmnERoqMuhKbd/will-ai-kill-everyone-here-s-what-the-godfathers-of-ai-have"><strong>Will AI kill everyone? Here&#39;s what the godfathers of AI have to say [RA video]</strong></a></h3><p> <i>by Writer</i></p><p> The video discusses the differing views on the potential dangers of advanced AI among three pioneers of deep learning: Geoffrey Hinton, Yoshua Bengio, and Yann LeCun, who were awarded the ACM Turing Award in 2018. Hinton and Bengio have expressed concerns about the risks posed by AI, with Hinton leaving Google to speak openly about these dangers and Bengio defining a &quot;rogue AI&quot; as one that could be catastrophically harmful to humans. However, LeCun dismisses these concerns as scaremongering, despite the growing consensus among AI researchers, including the leaders of OpenAI, Anthropic, and Google DeepMind, that advanced AI could pose significant risks.</p><h3> <a href="https://www.lesswrong.com/posts/fARMR2tiyCem8DD35/managing-risks-of-our-own-work"><strong>Managing risks of our own work</strong></a></h3><p> <i>by beth-barnes</i></p><p> The ARC Evals team published a report to increase understanding of the potential dangers of frontier AI models and to advance safety evaluations of these models. The team acknowledges that their research could potentially advance the capabilities of dangerous language model agents, and in response, they have redacted certain parts of their report. However, they may make this material public in the future if they believe the risk is minimal or if further analysis justifies its release, and they will share some non-public materials with AI labs and policymakers.</p><h3> <a href="https://www.lesswrong.com/posts/atxoviwLcPJPdYMqo/if-we-had-known-the-atmosphere-would-ignite"><strong>If we had known the atmosphere would ignite</strong></a></h3><p> <i>by Jeffs</i></p><p> The text discusses the potential dangers of creating Artificial General Intelligence (AGI) that cannot be aligned with human values and interests. It draws parallels with the Manhattan Project, suggesting that if scientists had calculated that the first atomic chain reaction would ignite the atmosphere, they could have taken steps to prevent this, such as securing uranium supplies and accelerating space programs. The author proposes a $10 million &quot;Impossibility X-Prize&quot; to incentivize efforts to prove whether aligning AGI is impossible, arguing that even if it fails, it could provide insights into how alignment might be possible.</p><h1> <strong>EA Forum</strong></h1><h3> <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><strong>Select examples of adverse selection in longtermist grantmaking</strong></a></h3><p> <i>by Linch</i></p><p> The author, a volunteer and sometimes contractor for EA Funds&#39; Long-Term Future Fund (LTFF), discusses the pros and cons of diversification in longtermist EA funding. While diversification can increase financial stability, allow for a variety of worldviews, encourage accountability, and provide access to diverse networks, it can also lead to adverse selection, where projects that have been rejected by existing grantmakers are funded by new ones. The author provides examples of such cases and suggests that new grantmakers should be cautious about funding projects that have been rejected by others, but also acknowledges that grantmakers can make mistakes and that a network of independent funders could help ensure that unusual but potentially high-impact projects are not overlooked.</p><h3> <a href="https://forum.effectivealtruism.org/posts/uxrAdXdYpXodrggto/an-elephant-in-the-community-building-room"><strong>An Elephant in the Community Building room</strong></a></h3><p> <i>by Kaleem</i></p><p> The author, a contractor for CEA and employee of EVOps, shares his personal views on the strategies of community building within the Effective Altruism (EA) movement. He identifies two main strategies: Global EA, which aims to spread EA ideas as widely as possible, and Narrow EA, which focuses on influencing a small group of highly influential people. The author argues that community builders and funders should be more explicit about their theory of change for global community building, as there could be significant trade-offs in impact between these two strategies.</p><h3> <a href="https://forum.effectivealtruism.org/posts/hpbaNabCDCfnhzc2P/ce-alert-2-new-interventions-for-february-march-2024"><strong>CE alert: 2 new interventions for February-March 2024 Incubation Program</strong></a></h3><p> <i>by CE</i></p><p> Charity Entrepreneurship has announced two new charity interventions for its February-March 2024 Incubation Program, bringing the total to six. The new interventions include an organization focused on bringing new funding into the animal advocacy movement and an organization providing structured pedagogy to improve education outcomes in low-income countries. The program offers two months of cost-covered training, stipends, funding up to $200,000, operational support, a co-working space in London, ongoing mentorship, and access to a community of alumni, funders, and experts.</p><h3> <a href="https://forum.effectivealtruism.org/posts/rLiCjrAv9D8chCoG5/dimensions-of-pain-workshop-summary-and-updated-conclusions"><strong>“Dimensions of Pain” workshop: Summary and updated conclusions</strong></a></h3><p> <i>by Rachel</i></p><p> A workshop was held to discuss strategies for assessing whether the severity or duration of pain has a greater impact on the overall negative experiences of farmed animals. The attendees agreed that no single method was reliable enough, but combining results from several paradigms could provide clearer insights. However, they also noted that current behavioral experiments may lack external validity, there are no known biomarkers that could measure pain experience over a lifetime, and in the absence of empirical evidence, long-lasting harms should be prioritized over severe but brief ones.</p><h3> <a href="https://forum.effectivealtruism.org/posts/to34jb5LuWo4fM9gC/taking-prioritisation-within-ea-seriously"><strong>Taking prioritisation within &#39;EA&#39; seriously</strong></a></h3><p> <i>by CEvans</i></p><p> The article emphasizes the importance of career prioritization for those seeking to maximize their impact, particularly within the Effective Altruism (EA) community. The author argues that the best future &#39;EA career paths&#39; are significantly more impactful than the median &#39;EA career path&#39;, and that over 50% of self-identifying effective altruists could increase their expected impact by thinking more carefully about prioritization. The author also provides advice on how to approach prioritization, including avoiding common mistakes such as anchoring too much on short-term opportunities, trying to form an inside view on everything, and being paralyzed by uncertainty.</p><h3> <a href="https://forum.effectivealtruism.org/posts/qGv4JhL5wcYgA7zj8/eu-farmed-fish-policy-reform-roadmap"><strong>EU farmed fish policy reform roadmap</strong></a></h3><p> <i>by Neil _Dullaghan</i></p><p> The majority of fish consumed in the EU are either wild-caught or farmed fish imported from non-EU countries, with the report focusing on the species farmed in the largest numbers in the EU: sea bass, sea bream, and small trout. The report argues for a fast transition to better slaughter conditions for these species, a move already supported by EU policymakers and animal advocacy organizations. However, the report also highlights the need for the aquatic animal advocacy movement to start preparing for the EFSA&#39;s upcoming opinions on farmed fish welfare, which could lead to further reforms affecting the whole life of an individual, such as water quality standards and stocking density maximums.</p><h3> <a href="https://forum.effectivealtruism.org/posts/4yZzSziCLkdzsYHt6/longtermism-fund-august-2023-grants-report"><strong>Longtermism Fund: August 2023 Grants Report</strong></a></h3><p> <i>by Michael_Townsend</i></p><p> The Longtermism Fund has announced several grants aimed at reducing existential and catastrophic risks. These include two grants promoting beneficial AI, two for biosecurity and pandemic prevention, and one for improving nuclear security. The grants, which total $562,000, will fund projects at institutions such as Harvard University, the Alignment Research Center, NTI | Bio, the Center for Communicable Disease Dynamics, and the Carnegie Endowment for International Peace.</p><h3> <a href="https://forum.effectivealtruism.org/posts/2rRsjdrL9BEWC3d7C/personal-reflections-on-longtermism"><strong>Personal Reflections on Longtermism</strong></a></h3><p> <i>by NatKiilu</i></p><p> The author, an African woman, reflects on longtermism, arguing that it should include more interventions that address systemic change. She contends that current values can lower the quality of life for marginalized groups and that failing to address these issues contradicts the longtermist goal of human flourishing. The author also criticizes the neutral language of longtermism, arguing that it overlooks inequality and could disproportionately benefit privileged groups.</p><h3> <a href="https://forum.effectivealtruism.org/posts/zjmpFW3nBKwaBB5xr/corporate-campaigns-work-a-key-learning-for-ai-safety"><strong>Corporate campaigns work: a key learning for AI Safety</strong></a></h3><p> <i>by Jamie_Harris</i></p><p> Negotiations and pressure campaigns have been successful in driving corporate change across various industries, including animal advocacy. The author suggests that AI safety/governance can learn from these tactics, which include establishing professional relationships with companies, using petitions and protests, and applying consistent pressure on specific companies. The author also proposes next steps such as pragmatic research, learning by doing, working with volunteers, and moral trade, where AI safety organizations pay for experienced campaigners to provide advice or go on secondments.</p><h3> <a href="https://forum.effectivealtruism.org/posts/XgeFZxBzbXuFieZ5H/probably-good-published-a-list-of-impact-focused-job-boards"><strong>Probably Good published a list of impact-focused job-boards</strong></a></h3><p> <i>by Probably Good</i></p><p> Probably Good has launched a new page dedicated to impact-focused job boards to assist individuals seeking potentially impactful opportunities across various cause areas and regions. The page features a range of job boards, including those for international non-profit jobs, civil service positions, tech-focused roles, region-specific boards, and boards focused on climate change, animal advocacy, and global health. The page is still being developed and Probably Good is open to suggestions for additional job boards to include.</p><h3> <a href="https://forum.effectivealtruism.org/posts/StbTBXTrur3ntur4u/new-probabilistic-simulation-tool"><strong>New probabilistic simulation tool</strong></a></h3><p> <i>by ProbabilityEnjoyer</i></p><p> Dagger is a new tool for calculations with uncertainty, using Monte Carlo simulation. Users can either import an existing spreadsheet or use Probly, a Python dialect designed for probabilistic simulation. Despite its current limitations, such as the lack of UI in Dagger to edit the model and all models being public, Dagger offers features like dependency graph, intuitive and mathematically rigorous sensitivity analysis, and a summary table that exposes the structure of your model.</p><h3> <a href="https://forum.effectivealtruism.org/posts/9cdntNDJQTS8dH5fh/making-ea-more-inclusive-representative-and-impactful-in"><strong>Making EA more inclusive, representative, and impactful in Africa</strong></a></h3><p> <i>by Ashura Batu</i></p><p> The authors, Ashura Batungwanayo and Hayley Martin, discuss the importance of Effective Altruism (EA) in Africa, emphasizing the need to balance existential risks with urgent issues like poverty and education. They propose an EA Africa initiative that blends bottom-up and top-down approaches for contextually attuned change, with a focus on local partnerships, co-designed interventions, and self-reliance. They suggest forging partnerships with local organizations, promoting knowledge sharing, and empowering communities as potential solutions to make EA more inclusive, representative, and impactful in Africa.</p><h3> <a href="https://forum.effectivealtruism.org/posts/sBJLPeYdybSCiGpGh/impact-obsession-feeling-like-you-never-do-enough-good"><strong>Impact obsession: Feeling like you never do enough good</strong></a></h3><p> <i>by David_Althaus</i></p><p> &quot;Impact obsession&quot; is a term used to describe a potentially harmful way of relating to doing good, often observed among effective altruists. It is characterized by an overwhelming desire to do the most good possible, basing one&#39;s self-worth on their impact, and often leads to overexertion, neglect of non-altruistic interests, and anxiety about not having enough impact. While some aspects of impact obsession are reasonable and desirable, others can lead to negative consequences like depression, anxiety, guilt, exhaustion, burnout, and disillusionment.</p><h3> <a href="https://forum.effectivealtruism.org/posts/azezpsWKkcPJd2Hfy/why-we-should-fear-any-bioengineered-fungus-and-give-fungi"><strong>Why we should fear any bioengineered fungus and give fungi research attention</strong></a></h3><p> <i>by emmannaemeka</i></p><p> The speaker argues that fungi should be taken seriously due to the limited number of effective antifungal drugs, the lack of vaccines for fungal infections, and the emergence of fungi such as Candida auris, which is resistant to some antifungals and has caused serious infections and even death. Fungi are also the only species known to have caused the complete extinction of another species, and there are many Biosafety class 3 fungal pathogens that are understudied and lack effective treatments. The speaker&#39;s lab is currently studying the diversity of fungal species in Africa and how they are adapting to climate change, with the aim of identifying potential new fungi that could threaten human lives.</p><h3> <a href="https://forum.effectivealtruism.org/posts/Xd9ZZuPCKAvKpzvdB/empowering-numbers-fem-since-2021"><strong>Empowering Numbers: FEM since 2021</strong></a></h3><p> <i>by hhart</i></p><p> Family Empowerment Media (FEM) launched a radio campaign in 2021 to educate listeners in Nigeria about maternal health and contraception, reaching an estimated 5.6 million people. An independent survey showed that contraceptive use increased by about 75% among all women in the state within 11 months. FEM has since launched a 9-month campaign, reaching an estimated 20 million new listeners, and has been recommended by Giving What We Can and Founders Pledge for its cost-effectiveness.</p><h3> <a href="https://forum.effectivealtruism.org/posts/g4TcehspjDumGXucx/my-ea-journey"><strong>My EA Journey</strong></a></h3><p> <i>by Eli Kaufman</i></p><p> The author shares their journey into Effective Altruism (EA), which began with a podcast and led to extensive self-education on the topic. They found a way to align their IT career with their passion for doing good, and became an active member of the EA community in Amsterdam. The author encourages others to explore EA, emphasizing the importance of networking, the availability of resources, and the value of everyone&#39;s unique contributions.</p><br/><br/> <a href="https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/kjsDpnfGrArHhbtKQ/gpt-powered-ea-lw-weekly-summary<guid ispermalink="false"> kjsDpnfGrArHhbtKQ</guid><dc:creator><![CDATA[smallsilo]]></dc:creator><pubDate> Fri, 25 Aug 2023 18:19:05 GMT</pubDate> </item><item><title><![CDATA[Assume Bad Faith]]></title><description><![CDATA[Published on August 25, 2023 5:36 PM GMT<br/><br/><p>我一直在努力避免使用“善意”和“恶意”这两个词。我怀疑，大多数听到“恶意”这个词的人，实际上并不知道它的意思——而且也许，它所代表的意思并没有<a href="https://www.lesswrong.com/posts/esRZaPXSHgWzyB2NL/where-to-draw-the-boundaries">切合现实</a>。</p><p>人们对恶意指控非常敏感：他们认为你应该假设是善意的，但如果你确定某人是恶意的，你甚至不应该与他们交谈，你需要驱逐他们。</p><p>那么“恶意”是什么<em>意思呢</em>？这并不意味着“怀有恶意”。<a href="https://en.wikipedia.org/wiki/Bad_faith">按照<em>维基百科的说法</em></a>，恶意是“一种持续的欺骗形式，包括娱乐或假装娱乐一种感觉，同时表现得好像受到另一种感觉的影响”。这本伟大的百科全书还提供了一些例子：挥舞着投降旗帜但当敌人从战壕中出来时开枪的士兵、起诉明知是虚假案件的律师、面临劳资纠纷的公司代表他来到谈判桌前并没有妥协的意图。</p><p>也就是说，恶意是指某人做某事的表面原因与真实原因不一致。这与恶意不同。穿着制服的士兵在不假装投降的情况下向你开枪，他的行为是真诚的，因为所见即所得：那个衣服表明他的工作是试图杀死你的人实际上正在试图杀死你。</p><p>如果你生活在一个诚实的世界，所见即所得（并且你想保持这种状态），那么假设善意（并在发现时无情地惩罚罕见的恶意案例）的政策是有意义的。日常生活中隐藏动机的可能性并不是一个重要的考虑因素。</p><p>然而，相反，我认为<a href="https://en.wikipedia.org/wiki/The_Elephant_in_the_Brain">日常生活中隐藏的动机是无处不在的</a>。作为进化的生物，我们生来就相信，因为相信对我们的祖先有好处。特别是作为社会性动物，最有益的信念并不总是真实的，因为欺骗你的同类采用一张暗示他们应该对你有利的地图有时比拥有反映领土和最有说服力的地图更有价值。谎言是你自己相信的。人类的普遍默认行为是提出理由来说服对方为什么做你想做的事符合他们的利益，但承认你所做的<em>不是游戏的一部分</em>。 <a href="https://www.lesswrong.com/posts/h2Hk2c2Gp5sY4abQh/lack-of-social-grace-is-an-epistemic-virtue">一个人们直接试图互相告知的世界对我们来说会显得令人震惊和陌生。</a></p><p>但如果是这样的话（你不应该相信我的话），对恶意指控过于敏感似乎会适得其反。如果人们所陈述的理由与真实理由不一样是很常见的，那么认为某个特定的人的理由不应该是超出范围的，也不一定需要将“恶意行为者”从公共生活中剔除——如果只是因为坚持不懈地应用，就不会有任何人留下了。为什么你会如此高度信任某人，以至于认为他们从来没有隐藏的议程？你为什么要相信自己？</p><p>相信“恶意”是不寻常的，这导致了一种扭曲的世界观，在这种世界观中，信息战的条件被合理化为不可避免的存在背景事实。特别是，人们似乎相信持续存在的善意分歧是一种常见现象——假设我是一个诚实的真理寻求者，而你也是一个诚实的真理寻求者，那么这种假设的事态并没有什么奇怪或不寻常的，但我们最终在某些事实问题上始终存在分歧。</p><p>我认为这种看似平常的事态充其量是<em>非常奇怪的</em>，而且可能只是假的。<em>真正的</em>“善意”分歧——双方只是想得到正确的答案，没有其他隐藏的动机，没有“其他事情”发生<a href="https://www.lesswrong.com/posts/iThwqe3yPog56ytyq/aiming-for-convergence-is-like-discouraging-betting">——往往不会持续下去</a>。</p><p>如果这种说法看起来有悖常理，那么您可能没有考虑到所有日常信仰差异，这些差异解决得如此迅速和无缝，以至于我们往往不会将它们视为“分歧”。</p><p>假设你和我一直计划去听一场音乐会，我记得是在星期四。我问你：“嘿，音乐会是在星期四，对吗？”你说：“不，我刚刚查看了网站；今天是星期五。”</p><p>在这种情况下，我<em>立即</em>用你的信念取代我的信念。我们都只是想知道音乐会何时举行这个事实问题的正确答案。在没有“其他事情”发生的情况下，没有什么可以阻止我们一步步趋同：你刚刚检查过的网站是比我的记忆更可靠的来源，你和网站都没有任何理由撒谎。所以，我相信你；故事结局。</p><p>在真实答案不确定的情况下，我们预计概率信念也会同样快速收敛。假设你和我正在研究一些物理问题。我们都只是想要正确的答案，而且我们都没有特别比对方更熟练。一旦我得知你得到的答案与我不同，我对自己答案的信心<em>立即直线</em>下降：如果我们都同样擅长数学，那么我们每个人犯错误的可能性都差不多。在我们比较计算并找出我们中哪一个（或两个）犯了错误之前，我认为你和我一样有可能是正确的，即使我不知道你是如何得到答案的。对我来说，仅仅因为答案是我的就拿钱赌我的答案是正确的，这是没有意义的。</p><p>大多数值得注意的分歧（大多数人们<em>关心</em>的分歧）的表现并不像音乐会日期或物理问题示例：人们非常执着于“自己的”答案。有时，经过长时间的争论，有可能让某人改变主意或承认另一方可能是对的，但要就事件的日期或计算结果（的概率）达成一致绝非易事——由此我们可以推断，在大多数人们关心的分歧中，除了双方都想得到正确的答案之外，还<em>存在着</em>“其他的事情”。</p><p>但是，如果在典型的分歧中存在“其他事情”，看起来像是一场怨恨比赛，而不是导致概率收敛的快速信息交换，那么认为持续的善意分歧很常见的信念似乎是恶意的！ （因为如果恶意是“在表现出一种感觉的同时又表现得好像受到另一种感觉的影响”，那么坚持不懈的善意分歧的信徒就会感到这种分歧的双方都是诚实的真理寻求者，但是相反，他们预计会看到怨恨的比赛而不是趋同。）</p><p>有些人可能会反对说，恶意是有意识的欺骗意图：<a href="https://slatestarcodex.com/2019/07/16/against-lie-inflation/">诚实地报告无意识的偏见信念</a>并不是恶意。我之前曾对<a href="https://www.lesswrong.com/posts/bSmgPNS6MTJsunTzS/maybe-lying-doesn-t-exist#The_Optimal_Categorization_Depends_on_the_Actual_Psychology_of_Deception">我们所谓的谎言有多少需要有<em>意识</em>的深思熟虑表示怀疑</a>，但更基本的回答是，从<a href="https://www.lesswrong.com/posts/fmA2GJwZzYtkrAKYJ/algorithms-of-deception">建模信息传输</a>的角度来看，偏见和欺骗之间的区别并不<em>有趣</em>——通常与概率更新无关。应该做。</p><p>如果一个苹果是绿色的，而你告诉我它是红色的，而我相信你，那么我最终会对苹果产生错误的信念。无论你说它是红色是因为你有意识地撒谎，还是因为你戴着玫瑰色眼镜，这并不重要。无论哪种方式，输入输出函数都是相同的：问题是你向我报告的颜色并不取决于苹果的颜色。</p><p>如果我只是想弄清楚你的报告与世界状况之间的关系（与关心惩罚说谎者而让仅仅有偏见的人摆脱困境相比），关心无意识偏见和无意识偏见之间的区别的主要原因有意识的欺骗在于后者提出了更强烈的抵抗。仅仅有偏见的人在遇到足够令人信服的反驳时通常会<em>屈服</em>（或者被提醒摘掉玫瑰色眼镜）；一个有意识地撒谎的人会<em>继续</em>撒谎<a href="https://www.lesswrong.com/posts/wyyfFfaRar2jEdeQK/entangled-truths-contagious-lies">（并说一些辅助性的谎言来掩盖真相），</a>直到你在观众面前当场抓住他们，并拥有对他们的权力。</p><p>鉴于在持续的分歧中通常会发生“其他事情”，如果我们不能依赖善意的假设，我们该如何继续下去呢？我看到了两种主要策略， <a href="https://www.lesswrong.com/posts/SX6wQEdGfzz7GKYvp/rationalist-discourse-is-like-physicist-motors">每种策略都有自己的成本效益概况</a>。</p><p>一种策略是坚持对象级别。可以根据论点的优点来评估论点，而不需要考虑说话者说话的角度（即使你认为可能有角度）。这提供了“假定善意”规范的大部分好处；我提出的主要区别在于，发言者的意图被视为<em>偏离主题</em>，而不是被认为是诚实的。</p><p>另一种策略是全面接触的精神分析：除了辩论对象层面的论点外，对话者还可以自由地质疑彼此的动机。这很难实现，这就是为什么大多数人大部分时间都应该坚持对象层面。如果做得好，它看起来就像一场谈判：在讨论过程中，伪分歧（我主张一种信念，因为该信念出现在共享地图上符合我的利益）被分解为真正的分歧和利益讨价还价这样才能找到并采取帕累托改进，而不是双方为了各自的利益而努力扭曲共享地图。</p><p>举一个伪分歧的例子，假设我拥有一家工厂，我正在考虑将其扩展到邻近的湿地，而你经营着一个当地的环保组织。有权阻止工厂扩建的监管委员会的职责是保护当地鸟类的生命，但不保护湿地面积。工厂排放少量Exampne 气体。您在监管委员会面前辩称，应该阻止扩张，因为最新的《科学》表明，Exampne 会让鸟类感到悲伤。我反驳说，最新的科学表明，Exampne 实际上使鸟类快乐；之前的研究将他们的笑声误听成了眼泪，应该撤回。</p><p>实际上，我们表面上的分歧似乎不太可能是“真正”关于Exampne对鸟类情绪调节的影响。更有可能的是，实际上发生的是<a href="https://www.lesswrong.com/posts/DpTexwqYtarRLRBYi/conflict-theory-of-bounded-distrust">冲突而不是分歧</a>：我想将我的工厂扩展到湿地，而你希望我不要这样做。提出Exampne污染如何影响鸟类的问题只是为了说服监管委员会。</p><p>我们的冲突被伪装成分歧，这是低效的。我们不可能同时得到我们想要的东西，但无论工厂扩建问题最终如何得到解决，最好是在不扭曲协会关于Exampne生物活性特性的共享图谱的情况下达到这一结果。 （也许它根本不会影响鸟类！）无论真正的答案是什么，如果允许有人指出你和我的偏见，社会就能更好地弄清楚这个问题<a href="https://www.lesswrong.com/posts/DoPo4PDjgSySquHX8/heads-i-win-tails-never-heard-of-her-or-selective-reporting">（因为有关哪些证据的事实会引起人们的注意）与应如何更新该证据相关）</a> 。</p><p>我认为谈论“恶意”没有用的原因是，善意<em>与</em>恶意的本体论并不适合这两种话语策略。</p><p>如果我坚持对象层面，那就无关紧要了：我回复文本中的内容；我对生成文本的过程的怀疑超出了范围。</p><p>如果我正在进行全面接触的精神分析，“我不认为你是真诚地在这里”的问题是它不够<em>具体</em>。推动讨论前进的方法不是指责某人一般性的“恶意”，而是假设对话者有一些尚未明确的特定动机，而针对此类指控为自己辩护<a href="http://zackmdavis.net/blog/2022/05/plea-bargaining/">的</a>方法是：<a href="http://zackmdavis.net/blog/2022/05/plea-bargaining/">一个人的真正议程不是所提议的议程</a>，而不是抗议一个人的“善意”并令人难以置信地声称没有议程。</p><p>这两种策略可以混合使用。一个简单的元策略可以在不施加过高技能要求的情况下表现良好，即默认为对象级别，并且只将精神分析作为对抗<a href="https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters">阻碍</a>的最后手段。</p><p>假设你指出我最近的回复似乎与我之前所说的相矛盾，我说：“看那边，分散注意力！”</p><p>如果你想继续坚持对象层面，你可以说，“我不明白分散注意力与解决我提出的陈述中的不一致有什么关系。”另一方面，如果你想深入进行精神分析，你可以说：“我认为你只是指出分心，因为你不想被束缚。”然后我将被迫要么解决你的投诉，要么解释为什么我有其他原因来指出干扰。</p><p>然而，至关重要的是，是否调查动机的选择并不取决于只有“坏人”才有动机的假设——就好像有不怀好意的行为者有自己的角度，而有善意的行为者是完美空虚的理想哲学家。 。总有一个角度；问题是哪一个。</p><br/><br/><a href="https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith<guid ispermalink="false"> e4GBj6jxRZcsHFSvP</guid><dc:creator><![CDATA[Zack_M_Davis]]></dc:creator><pubDate> Fri, 25 Aug 2023 17:36:32 GMT</pubDate></item><item><title><![CDATA[A Model-based Approach to AI Existential Risk]]></title><description><![CDATA[Published on August 25, 2023 10:32 AM GMT<br/><br/><h1>介绍</h1><p>两极分化阻碍了合作和了解未来人工智能是否对人类构成生存风险以及如何降低灾难性后果的风险的进展。确定这些风险是什么以及什么决策是最好的是非常具有挑战性的。我们认为，<i>基于模型的方法</i>具有许多优势，可以提高我们对人工智能风险的理解，评估缓解政策的价值，并促进人工智能风险争论不同方面的人们之间的沟通。我们还相信，人工智能安全和校准社区中的很大一部分从业者都拥有成功使用基于模型的方法的适当技能。</p><p>在本文中，我们将引导您通过一个基于模型的方法的示例应用来应对未结盟人工智能带来的生存灾难风险：基于卡尔史密斯的<a href="https://joecarlsmith.com/2023/03/22/existential-risk-from-power-seeking-ai-shorter-version"><i><u>《寻求权力的人工智能是否是​​存在风险？》的</u></i></a>概率模型。您将与我们的模型进行交互，探索您自己的假设，并（我们希望）就这种类型的方法如何与您自己的工作相关提出您自己的想法。您可以在此处找到该模型的链接。</p><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>在许多鲜为人知的领域，人们倾向于采取倡导立场。我们在人工智能风险中看到了这一点，经常看到作家<a href="https://www.lesswrong.com/posts/BTcEzXYoDrWzkLLrQ/the-public-debate-about-ai-is-confusing-for-the-general"><u>轻蔑地称某人为“人工智能末日论者”或“人工智能加速论者”。</u></a>这场辩论的双方都无法向另一方传达他们的想法，因为倡导通常包括在另一方不共享的框架内解释的偏见和证据。</p><p>在其他领域，我们亲眼目睹了基于模型的方法是消除此类宣传的建设性方法。例如，通过利用基于模型的方法， <a href="https://lumina.com/case-studies/energy-and-power/a-win-win-solution-for-californias-offshore-oil-rigs/"><u>Rigs-to-Reefs 项目</u></a>在 22 个不同的组织之间就如何退役圣巴巴拉海岸大型石油平台这一有争议的问题达成了近乎共识。几十年来，环保组织、石油公司、海洋生物学家、商业和休闲渔民、航运利益集团、法律辩护基金、加利福尼亚州和联邦机构在这个问题上陷入了僵局。模型的引入使对话重新聚焦于具体假设、目标和选项，并导致 22 个组织中的 20 个组织就同一计划达成一致。加州立法机构通过 AB 2503 号法案将该计划写入法律，该法案几乎一致通过。</p><p>人工智能带来的生存风险存在很多不确定性，而且风险极高。在这种情况下，我们主张使用概率分布明确量化不确定性。遗憾的是，这种情况并没有应有的普遍，即使在此类技术最有用的领域也是如此。</p><p> <a href="https://arxiv.org/abs/2206.13353"><u>Joe Carlsmith</u></a> (2022) 最近发表的一篇关于不结盟人工智能风险的论文有力地说明了概率方法如何帮助评估先进人工智能是否对人类构成生存风险。在本文中，我们回顾了 Carlsmith 的论点，并将他的问题分解纳入我们自己的<a href="https://lumina.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>模型中。然后，我们以多种方式扩展这个起点，以展示应对 x 风险领域中每个独特挑战的基本方法。我们带您参观实时模型，了解其元素，并使您能够自己更深入地研究。</p><h2>挑战</h2><p>预测长期未来总是充满挑战。如果<a href="https://www.youtube.com/watch?v=87l9Az9msHU&amp;t=3108s"><u>没有历史先例</u></a>，难度就会被放大。但这种挑战并不是独一无二的。我们在许多其他领域缺乏历史先例，例如在考虑新的政府计划或全新的商业计划时。当世界状况因技术、气候、竞争格局或监管变化而发生变化时，我们也缺乏先例。在所有这些情况下，难度都很大，但与预测通用人工智能 (AGI) 和存在风险的挑战相比就显得苍白无力了。今天对人工智能存在风险的预测通常至少部分依赖于关于未来先进人工智能将如何表现的抽象论点，而我们今天无法测试这些论点（ <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1"><u>尽管我们正在努力改变这一点</u></a>）。即使是最精心设计的论点也常常会遇到合理的不确定性和怀疑。<br><br>例如，在评估有关人工智能存在风险的预测的可靠性时，经常会遇到这样的反对意见：“我在预测中找不到具体的缺陷。它们只是有点抽象，有点连贯，而且论证这个班级经常会以意想不到的方式犯错。”</p><p>举个例子，最近超级预测者对人工智能风险的启发似乎表明，这种普遍的怀疑态度是超级预测者和人工智能安全社区之间对人工智能风险<a href="https://astralcodexten.substack.com/p/the-extinction-tournament"><u>持续存在分歧</u></a>的一个因素。尽管两个群体之间进行了讨论，而且超级预测者在<a href="https://www.lesswrong.com/posts/BHdEvjtfwpgrTh825/ai-21-the-cup-overfloweth?commentId=dpzuaGHyiNcnr4jF2"><u>有关未来人工智能的许多定量预测</u></a>上与领域专家达成了一致，但这种关于人工智能风险的分歧仍然存在，这表明对人工智能风险论点的怀疑更加分散。应认真对待此类反对意见，并根据其本身的优点以及过去类似反对意见在其他领域的表现进行评估。这凸显了不仅评估预测内容，而且评估其背后的基本假设和推理的重要性。</p><p>对于为什么可能出现某些结果，人工智能风险界已经提出了许多论点。当你着手建立一个明确的人工智能存在风险模型时，如果不吸收其他聪明、专注的人经过深思熟虑的想法，那就太疏忽了。然而，将多个想法合并到一个连贯的模型中确实很困难，并且根据某些统计，有多达<a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=C3sb2QZQAeHLZmz2T"><u>五个部分重叠的世界观/研究议程</u></a>，每个都专注于不同的威胁模型。不同的论点常常建立在相互不一致的概念框架之上。简单地统计某个立场存在多少个论据也是行不通的，因为基本假设几乎总是存在大量重叠。此外，以任何深入的方式将<a href="https://www.lesswrong.com/posts/Cd6uMn4qHXZcoe2Lh/discussion-weighting-inside-view-versus-outside-view-on"><u>内部观点与外部观点</u></a>论证合并起来似乎几乎是不可能的。虽然很困难，但整合现有的专家知识（和意见）对于有效的基于模型的方法至关重要。我们认为人工智能存在风险建模在整合多种专家知识来源方面具有独特的方面，因此是进一步研究新方法和新技术的成熟领域。我们已将解决本段中提到的所有挑战的简单方法纳入我们的模型中。</p><p>主观概率是表示不确定性的传统工具，一般来说，它是一个很好的工具。基于模型的方法依赖于对不确定变量的主观评估。在人工智能存在风险领域，当你要求两位专家评估相同的主观概率时，他们的估计通常会存在显着差异（例如，一位专家说 15%，而另一位专家说 80%）。这在其他领域并不正常。尽管您可能会发现两个气象学家分别预测下雨概率为 15% 和 80% 的情况，但这种情况并不常见。</p><p>这是上面已经讨论过的困难的症状，并且引入了该领域的另一个显着特征。由于专家之间的这种极端差异，以及人们的估计校准不佳的事实，似乎需要获得额外的信心层。我们在本文后面的“元不确定性”部分对此进行了详细说明，并且我们在模型中包含了显式的二阶分布（即，二阶分布代表专家意见之间的差异，而一阶不确定性代表不确定性）在结果中）。</p><p>我们在本文中描述的工作是作为 MTAIR 项目（建模变革性人工智能风险）的一部分进行的，建立在<a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg">最初的 MTAIR 概念模型的</a>基础上。我们的目标是评估 AGI 存在风险的多个、有时甚至根本上相互冲突的详细模型以及这些外部观点/可靠性考虑因素。我们将他们视为相互竞争的“专家”，以便做出明智且平衡的评估。您可以使用我们的交互式模型来输入您自己的评估并探索其含义。</p><h2>是什么让模型有效？</h2><ul><li><strong>透明度</strong>：为了有效，基于模型的方法应该提供其他人可以浏览和理解的模型。当了解该主题的典型用户能够理解模型在做什么、如何做以及计算中使用什么假设时，我们称模型是<i>透明的</i>。您永远不应该假设主题专家是程序员，或者 python 代码（或任何其他编程语言）不言而喻。因此，传统的程序通常被认为是不透明的。</li><li><strong>交互性</strong>：第二个重要属性是<i>交互性</i>，以及利益相关者尝试不同假设、探索不同决策或政策的后果以及探索任意假设场景的能力。</li><li><strong>显性的不确定性</strong>：对于人工智能的存在风险，大部分行动都是在不确定性的尾部进行的（即简单地得出中值结果是人类生存的结论是没有抓住要点的）；因此，<i>不确定性的明确表示</i>很重要。</li></ul><p>我们在<a href="https://analytica.com/"><u>Analytica 视觉建模软件</u></a>中构建了模型，该软件强烈满足上述所有要求，并且使用起来很有趣。 Analytica 模型的结构为分层影响图，这是一种高度可视化且易于理解的表示形式，捕捉了模型如何直观地工作的本质。它是交互式的并且具有嵌入式模块化文档。有强大的多维智能阵列设施，提供了前所未有的灵活性。它使用概率分布明确地表示不确定性。不确定性向下游计算结果的传播会自动发生。它学习起来既简单又快捷，一旦您构建了模型，您就可以将其发布到网络上进行共享（正如我们在本文中所做的那样）。</p><p>如果您受到我们的示例的启发来构建自己的模型，您应该知道<a href="https://analytica.com/products/free101/"><u>Analytica 有一个免费版本</u></a>。当您需要扩展到真正的大型模型时，也可以使用商业版本。桌面版本需要 Microsoft Windows。您无需获取或安装任何东西（浏览器 - Chrome 或 Edge 除外）即可使用我们的模型，该模型在 Analytica 云平台 (ACP) 上共享。我们的模型大约有 150 个对象，略超过免费版本 101 个对象的最大大小。但如果您有兴趣将其下载到桌面 Analytica，免费版本允许您加载、查看、运行、更改输入和重新评估结果等。</p><p>总之，基于模型的方法来评估人工智能存在风险预测的可靠性可以为人工智能安全社区带来多种好处。首先也是最重要的，它提供了清晰、简洁和易读的输出，考虑了可能影响预测准确性的许多不同的反对意见和因素。这有助于确保人工智能安全社区了解预测背后的推理和证据，并可以根据该信息做出明智的决策。</p><p><strong>此外</strong>，这种基于模型的方法鼓励社区考虑更广泛的因素，而不仅仅是详细的论点本身。例如，他们可能会考虑他们对高级抽象的信任程度以及不同启发式的可靠性。通过将这些考虑因素纳入模型中，社区可以更有效地权衡与人工智能相关的风险，并制定更稳健的策略来减轻潜在危害。最后，这种方法可以通过促进对所有相关因素进行更严格的思考和更全面的检查来帮助改善社区的认知，从而更好地理解人工智能存在风险的性质和可能性。</p><p>作为起点，我们将重点关注基于 Joe Carlsmith 报告<a href="https://arxiv.org/abs/2206.13353"><u>“寻求权力的人工智能是否存在风险”的</u></a>单一详细模型，以及影响这一机械模型合理性的几个外部观点/可靠性启发法。我们将首先简要介绍卡尔史密斯对人工智能存在风险的介绍以及我们自己的一些改进，然后在最后讨论改进该模型的后续步骤。</p><h1>型号概览</h1><p><a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><strong><u>单击此处运行我们的模型</u></strong></a><strong>&nbsp;</strong></p><p>这是在 Analytica 云平台 (ACP) 中运行的分层模型，基于 Joe Carlsmith 的报告“寻求权力的人工智能是否存在风险”。它可以让你计算由人工智能失调引起的生存灾难的概率。</p><p>这些结论隐含地取决于我们在给出各种假设的情况下明确规定的某个时间范围。事实上的时间范围是“到 2070 年”，但在输入您自己的估计时，您可以采用不同的时间范围，而无需更改模型的逻辑。</p><p>简而言之，该模型预测，如果出现以下情况，错位的寻求权力的人工智能将导致一场生存灾难：</p><ol><li>先进、规划、<a href="https://www.planned-obsolescence.org/situational-awareness/"><u>战略意识</u></a>（APS）系统——即能够进行高级规划、具有战略意识并拥有先进的人类水平或超人类能力的人工智能——是可行的，</li><li>当 APS 系统可行时，将会有强烈的激励措施来建设它们，</li><li>构建不以不协调的方式寻求权力的 APS 系统比构建表面上有用但确实以不协调的方式寻求权力的 APS 系统要困难得多，<ol><li>尽管 (3)，实际上将构建和部署错位的 APS 系统，</li></ol></li><li>未对准的 APS 系统在部署后将能够造成全球性的大灾难，</li><li>人类对造成此类灾难的错位 APS 系统的反应不足以阻止其完全接管，</li><li>一旦接管，错位的 APS 系统将摧毁或严重削弱人类的潜力。</li></ol><p>我们模型的总体框架基于卡尔史密斯报告和随后的<a href="https://80000hours.org/problem-profiles/artificial-intelligence"><u>80,000 小时文章</u></a>中提供的人工智能存在风险论点，并进行了修改。这是我们的“顶级模型”，我们对人工智能存在风险进行高级分析。</p><h1>模型巡演</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/mvk6oge6czjenatva2gs"></p><p>在本节中，您将快速浏览我们的模型，并在浏览器窗口中实时运行它。首先，请单击<a href="https://acp.analytica.com/view?invite=4560&amp;code=3000289064591444815"><u>“启动模型”</u></a>以在不同的浏览器选项卡或窗口中将其打开，以便您可以同时参考此页面。我们提供分步说明来帮助您入门。按照此导览了解您的方向，然后您可以自己更深入地探索模型的其余部分，并探索不同估计值会发生什么。我们建议在大显示器上运行模型，而不是移动设备。</p><h2>基本评估</h2><p>在第一页上，您将看到卡尔史密斯报告中的六个概率评估。 （请注意，本文中的屏幕截图是静态的，但它们在运行模型的浏览器窗口中是活动的）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/rolr4s2y84njur0cbthc"></p><p>您可以在此处调整滑块或为每个滑块输入您自己的估计值。要了解每个含义，只需将鼠标悬停在问题上并阅读弹出的说明。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/adt7luy3kmzavtkfqwmd"></p><p>在估计这些之前，您应该选择一个时间范围。例如，您可以估计每个值在 2070 年之前是否成立。计算取决于您的估计，而不是所选的时间范围，但您的估计预计会随着更长期的时间范围而改变（增加）。</p><p>滑块输入下方是一些计算结果，显示 5 个阶段中的每个阶段以及所有前面的阶段最终为真的概率。最后一个“存在性灾难”显示了根据您对六个命题中每一个命题的估计，APS 系统发生存在性灾难的概率。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/qy1yjgqxgkjmwhzm43c5"></p><p>在这个屏幕截图中，我们看到 APS 有 0.37% 的几率（不到百分之一）会导致人类灭绝等生存灾难。考虑到结果的极端性，这似乎是一个巨大的风险，但许多专门研究人工智能安全的人会认为这是超级乐观的。您的估计如何比较？</p><h2>专家们权衡利弊</h2><p>您的估计与其他人工智能安全研究人员的估计相比如何？在卡尔史密斯的报告之后，开放慈善组织<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>征求了其他人工智能安全研究人员的评论</u></a>，并要求他们提供自己对这些主张的估计。这些评论发生于 2022 年 8 月。</p><p>首先，您可以通过按浏览他们对每个提议的原始评估<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/x5126haezlzwpqulhccb">审阅者评估按钮，该按钮显示在页面右下角（您可能需要向右滚动）。该表显示在页面的右上角。请注意审稿人之间的巨大差异。</p><p>单击“选择要使用的中值评估”的选项下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ugm5g5wylo2zr8tawy64"></p><p>选择所有项目，使其现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/jakuafkaps0pfd9umwih"></p><p>存在灾难输出现在显示<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">按钮。按下。结果表出现在右上角。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cdgv48bhgvvyqdl0accx"></p><p>这些显示了估计所隐含的由 APS 引起的存在灾难的概率，这些估计来自您自己的输入以及审阅者的输入。审稿人的中位数为 9.15%，但审稿人之间的数字差异很大。在少数情况下，审稿人不愿意接受卡尔史密斯提出的分解，就会出现空值。接下来，我们将其显示为条形图。将鼠标悬停在表格区域顶部以访问图形按钮，然后按它。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/coje0q97joeqsped2ibd"></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/wi8p3wpl6wwu1xiaksrr"></p><p>再次将鼠标悬停在图表顶部，并将视图更改回表视图。查看结果时，您可以通过这种方式在图形视图和表格视图之间切换。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kxtxsvzrvhvdegaqopw8"></p><h2>专家意见的差异</h2><p>专家意见的巨大差异对该领域的理性决策提出了严峻挑战。很难说任何基于通过汇总这些概率获得的预期效用是可信的。因此，我们根据专家意见的变化拟合概率分布。因为这是主观概率的分布，所以它实际上是<i>二阶概率分布</i>，我们称之为<i>元不确定性</i>。我们用一<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.2klzxyklefio"><u>节</u></a>的内容来讨论元不确定性、其动机及其解释，但现在让我们想象一下这种元不确定性。</p><p>将<i>“选择要使用的中值评估”</i>更改为<strong>“所有审阅者的中值”</strong> ，并在“选择<i>要包含的元不确定性”</i>的选择下拉列表中选择<strong>审阅者的分布</strong>选项。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/whhm6iheuj3vqf4fa9h8"></p><p>输出现在显示为<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vgk2fazbt9hatbpdmt4i">纽扣。将鼠标悬停在 Existential 灾难输出上并按鼠标右键。从上下文菜单中选择<strong>超出概率</strong>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dipzz5jhchogoicm7ydn"></p><p>在框架节点中，切换回图形视图（ <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/f8qvwqfinrmxpyjxzyny"> ）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y2mwu3iud8senllipqbi"></p><p>超出概率图是可视化概率分布的一种方法。本例中的分布反映了专家意见的差异。基本量（x 轴）是 APS 系统中发生诸如人类灭绝之类的生存灾难的概率。沿着绿色箭头，您可以看到大约 10% 的专家认为存在灾难的概率超过 0.17（即 17%），沿着黄色箭头，大约 5% 的专家认为存在灾难的概率超过 0.27。</p><p>为了获得这种二阶分布，该模型将每个问题的专家评估集合视为从基础分布中采样，然后将概率分布“拟合”到这些点。这种拟合的技术细节将在后面的“元不确定性”部分中介绍。该部分还探讨了当元不确定性（即专家意见之间的变化量）增加或减少时我们的观点如何变化。</p><h2>结合内部和外部视图参数</h2><p>卡尔史密斯分解是<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>内部观点框架</u></a>的一个例子，它将感兴趣的主要问题分解为其组成因素、步骤或起作用的因果机制。相比之下，<a href="https://www.lesswrong.com/tag/inside-outside-view"><u>外部视图框架</u></a>从相似的事件或参考类中得出相似之处，以提供上下文和预测。例如， <a href="https://forum.effectivealtruism.org/posts/MMtbCDTNP3M53N3Dc/agi-safety-from-first-principles#AGI%20safety%20from%20first%20principles"><i><u>第二个物种论点</u></i></a>认为人类可能会失去地球上最强大物种的地位。其他外部观点框架包括霍尔顿·卡诺夫斯基的<a href="https://www.cold-takes.com/most-important-century"><i><u>“最重要的世纪”</u></i></a> 、阿杰亚·科特拉的<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>生物锚</u></a>（对一个子问题、时间表、更大问题的外部观点）、与过去变革性技术进步的类比，甚至<a href="https://forum.effectivealtruism.org/posts/mjB9osLTJJM4zKhoq/2022-ai-expert-survey-results"><u>专家意见调查</u></a>。</p><p>每种类型的框架都会产生不同的见解，但由于内部和外部视图框架以不同的方式进行评估，因此将两者吸收为一致的观点是相当具有挑战性的。但我们认为基于模型的方法需要解决这个问题，以便整合来自所有来源的信息。</p><p>我们包括两种简单的外部视图方法（在后面的部分中详细讨论），由这些输入反映出来： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/w4lspva3ifst3gseyzbh"></p><p>将鼠标悬停在每个输入上即可获取您所估计内容的完整描述。这些需要你抽象地思考几个高层的外部观点考虑因素和论点，然后评估这些考虑因素对存在灾难的风险有多大影响。 Cr在这里是<i>信任的</i>意思。与统计学中可能性的概念类似（有些人可能会说同义），可信度是从 0 到 1 范围内的估计，其中 0 表示考虑因素暗示没有风险，1 表示考虑因素暗示一定的灾难。</p><p>您现在已经输入了您自己对卡尔史密斯“世界模型”以及外部观点可信度的估计。我们的重点是模型如何将这些吸收到单一的主观观点中？我们的目标是强调这一挑战并至少尝试这样做。也许您或其他继续采用未来基于模型的方法的人会改进我们的方法。</p><p>在此模型中，我们允许您为不同视图分配相对权重。单击权重表按钮以赋予不同的意见。将鼠标悬停在输入上即可查看要求您评估的内容的描述。可信度是对您认为这些外部观点论据本身支持该主张的程度进行的评级。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/irrgjkuu6h3szgx5ma8o"></p><p>顶部框架中会出现一个条目表，其中包含可用于更改相对权重的滑块。您可以调整这些以反映您自己对相对可信度的看法。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/hvjy2karshi3lntbs83r"></p><p>第一部分允许您输入卡尔史密斯分解与外部视图参数相比的相对重要性。在这里，我们将外部视图固定为 1，因此（基于卡尔史密斯的）世界模型的值为 3 意味着您希望该框架的计数是外部视图参数的三倍。</p><p>在世界模型中，您有自己的估计以及接受调查的各个专家的估计。您可以选择或多或少地重视个别专家的估计。</p><p>最后，在下部，您可以调整两个不同的外部视图框架的权重。这些用于组合不同的外部观点论点。</p><p>设置自己的权重后，右列中的输出将显示同化视图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/s586akwpoclyxujnzywc"></p><p>第一个输出 Cr[存在灾难|世界模型] 是卡尔史密斯分解在考虑了您自己的估计与专家的估计之间的相对权重后的评估。</p><p>第二个输出 Cr[AI Existential Catastrophe] 是组合外部视图模型中存在灾难的概率。</p><p>最终输出 Cr[Existential catastrophe] 是对存在灾难的最终同化估计。它考虑了内部视图世界模型和外部视图模型，将两个来源的信息结合起来作为代表性的最终评估。</p><h2>探索模型的内部结构</h2><p>到目前为止，您已经使用了我们为您突出显示的一些选定的输入和输出。接下来，您将探索模型的内部结构。</p><p>顶部是一个大的蓝色模块节点<strong>Main Model</strong> 。点击它。这将带您进入实施阶段，您会看到几个子模块和<a href="https://lumina.com/technology/influence-diagrams/"><u>影响图</u></a>。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/cm3tppz2rkmycyj2rvyb"></p><p>在第一个图中，上半部分包含基于卡尔史密斯报告的内部视图世界模型。左下角包含外部视图参数。右下四分之一是用于同化不同观点的逻辑。</p><p>影响图的节点是变量。箭头描绘了变量之间的影响。影响图是直观的，您通常可以从中了解模型是如何工作的，而无需查看计算细节。将鼠标悬停在节点上可查看其描述，以获取有关每个变量所代表含义的更多信息。</p><p>在外部视图部分中，一些未定义的节点（已散列）仅用于记录纳入估计的考虑因素。虚线箭头表示这些不是计算所使用的影响，但应该会影响您的思考。</p><p>单击节点后，请注意顶部的选项卡。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/vd4gjj6wwh9ffyez8t4p"></p><p> <strong>“对象”</strong>选项卡可能是最有用的，因为它允许您查看单击的变量的定义（和其他属性）。查看完该变量后， <strong>“图表”</strong>选项卡将返回图表。</p><p>现在您已经完成了这个快速浏览，您应该可以轻松地探索模型的各个方面。接下来，我们将更深入地研究纳入模型的内容和概念。</p><h1>型号特点</h1><p>在将卡尔史密斯报告的人工智能存在风险模型应用于 Analytica 时，我们对原始计算进行了一些更改，即简单地将命题 1-6 的条件概率相乘，以获得对错位人工智能的存在风险的总体估计。</p><p>为了更好地捕获围绕问题的全方位不确定性，我们处理了“元不确定性”，方法是将每个点估计更改为一个分布，其方差取决于我们对每个概率估计的置信度，如上<a href="https://docs.google.com/document/d/1bj8SLbhqL8VhQaIPNlFYpLEX5S5uO2rYlb_6KalhDEU/edit#heading=h.wtvwqwsb1xzp"><u>一节所述</u></a>。</p><p>元不确定性是指由于我们对影响我们的信念或观点的更普遍因素的不确定性而产生的不确定性。这些因素可能包括诸如我们应该对内部观点和外部观点给予多少重视，以及长期预测的可靠性如何等问题。</p><p>元不确定性与更直接的不确定性不同，因为它关注的是我们对风险评估所依据的假设和因素的不确定性。它本质上是二阶不确定性，我们不确定驱动一阶不确定性的因素。</p><p>我们通过将<a href="https://www.wikiwand.com/en/Logit-normal_distribution"><u>Logit 正态分布拟合到 Joe Carlsmith 报告的每个原始审稿人给出的单个点估计值的分布，生成了这些元不确定性分布</u></a>。该方法与本文<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future#3_2_Model_Parameterisation"><u>《化解人工智能风险》</u></a>中使用的方法类似。</p><p>我们还纳入了其他不太详细的“外部观点考虑因素”，它们不像卡尔史密斯报告那样依赖于详细的世界模型。我们对这些与卡尔史密斯模型相关的外部观点争论的信任会影响该模型给出的人工智能存在灾难的最终无条件概率。这些外部观点的考虑可以被视为补偿详细世界模型中出现的一般可靠性问题的一种方法，因此是减少我们模型的随机误差或“未知的未知”困难的一种方法。</p><p>我们尚未讨论的一件事是卡尔史密斯模型中潜在的系统缺陷。正如我们将在“框架效应”一节中讨论的那样，一些研究人员反对卡尔史密斯报告本身的框架，认为它系统性地使我们产生向上或向下的偏见。</p><h2>元不确定性</h2><p>围绕人工智能存在风险问题存在许多复杂且不确定的问题，包括协调的难度、不协调的人工智能是否容易接管，甚至是否会建立“APS”类型的通用人工智能（AGI）。世纪。这些不确定性使得评估人工智能存在风险的总体概率变得困难。</p><p>量化这些风险的一种方法是为每个索赔分配点概率估计并将其向前传播，正如卡尔史密斯关于该主题的原始报告中所做的那样。然而，这种方法存在一些问题。作为卡尔史密斯模型输入的六个概率估计中的每一个都涉及历史上没有先例的事件。因此，估计这些事件的概率具有挑战性，并且当您看到两位不同专家的估计存在显着差异时，没有明确且明显的方法来判断哪个估计更可信。</p><p><i>元不确定性</i>通过对可能的意见进行概率分布来审视可能的信念状态。<a href="https://acp.analytica.com/view?invite=4418&amp;code=3221222959844354027"><u>我们的模型</u></a>包含您可以探索的几个版本的元不确定性。</p><p>包含元不确定性的一个有用目的是了解专家意见的变化，以及这种变化如何影响模型的输出。</p><p>开放慈善组织要求人工智能风险领域的几位专家<a href="https://www.lesswrong.com/posts/qRSgHLb8yLXzDg4nf/reviews-of-is-power-seeking-ai-an-existential-risk"><u>提供他们自己对卡尔史密斯报告中参数的估计</u></a>。我们已将这些包含在我们的模型中。您可以从这些专家中的任何一位或任何专家集中选择估计值。您还可以包括 Joe Carlsmith 在他的文章中给出的估计、所有审稿人的中位数以及您自己的估计。当您同时选择多个时，您将能够在任何下游结果中对它们进行比较。要进行选择，请使用模型前图中的“选择要使用的中值评估”的多选下拉菜单。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/zg0ztojles89j7qd1uqy"></p><p>当您查看模型中变量的结果时，您将看到使用您选择的每个审阅者的估计得出的结果值。例如，这是存在灾难概率的结果表。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/kojklpskvf2kxdmgxeda"></p><p>从这些中，您可以了解专家意见的差异有多大，但这还不包括元不确定性的概率分布。对于每个输入，您可以让模型将概率分布拟合到审阅者提供的评估（对于统计极客：它拟合<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit-Normal</u></a> ，又名 Log-Odds 分布）。要自己探索这一点，请将“选择要包含的元不确定性”下拉菜单设置为“审阅者的传播”。完成此操作后，它会使用具有跨专家观察到的元不确定性方差的分布来执行所有计算（对于统计极客来说：它实际上是每个数量的<a href="https://docs.analytica.com/index.php/Logit"><u>logit</u></a>的方差与专家的方差相匹配）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/isvopzarcegg2d9pwuug"></p><p>在模型的内部，名为“Assessments”的变量现在包含六个输入评估中每一个的元不确定性分布。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/ndgs3pq2uosqwvcpylgj"></p><p>上图显示了每个评估数量的<a href="https://docs.analytica.com/index.php/Uncertainty_views#Cumulative_probability"><u>累积概率</u></a>（称为 CDF 图）。 Y 轴上的值表示专家估计的数量值小于或等于 x 轴上相应值的可能性。该图的关键项目按顺序对应于卡尔史密斯模型的六个评估。第一项标记为<i>“时间线</i>”，是评估 APS 系统在考虑的时间线窗口内构建的可行性。其红色 CDF 几乎是一条直线，表明所选专家之间的不确定性几乎均匀分布。标记为<i>“灾难”</i>的浅蓝色线是对已经接管的不结盟的 APS 系统的评估，然后将摧毁或限制人类的潜力。该曲线的形状表明所选专家之间一致认为概率接近 1。</p><p>上图背后的计算将每个输入元不确定性分布的中值设置为同一问题上所选审稿人的中值。通过更改上图顶部的切片器控件“选择要使用的中值评估”，您可以将相同级别的元不确定性应用于任何单个审阅者的评估（或您自己的评估）。</p><p> <a href="https://analytica.com/why-analytica/what-is-analytica/"><u>Analytica</u></a>自动将这些元不确定性传播到任何计算的下游结果。在这里，我们看到了存在灾难概率的 CDF 图（六次评估的乘积）。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/o8nxineidbllqcecttka"></p><p>任何一个人的评估都会得出这一数量的单一概率，即“存在灾难”。上述分布反映了专家意见的差异。该曲线表明，专家得出存在灾难的概率小于 1% 的结论的概率为 50%。相反，使用 Y 轴的 0.9 水平，专家有 10% 的概率得出存在灾难的概率超过 15% 的结论。运行模型时，您可以选择不同的专家子集（或全部）来交互式探索您最信任的专家子集。</p><p>当您为六个输入概率中的每一个提供自己的估计时（我们建议您在运行模型时尝试），您可能会直觉地认为您的估计不可靠。即使您是该领域的专家，您也可能会有这种感觉。您可能会发现在您自己的个人评估中包含（或让模型包含）元不确定性很有用。该模型允许您这样做。但首先，让我们讨论一下你自己的信念状态的元不确定性到底意味着什么。</p><p>模型的每个输入都会要求您提供自己的<i>主观概率</i>。其中每一项都总结了您对该问题的了解程度。没有人知道这六个命题中的任何一个是真是假。你的主观概率仅仅反映了你所拥有的知识的强度。你不是在估计世界上存在的价值，而是在估计你的信念程度。通过将元不确定性应用于你的信念程度，你本质上是在说你不确定自己的信念是什么。在这样的情况下，这可能并不直观地让人觉得牵强，因为几乎没有历史先例！一般来说，当需要做出决定时，如果您可以表达您的元不确定性，您也可以通过简单地采用平均信念（或平均效用）将其折叠为单个信念度数。在那之前，元不确定性可以表明你的信念对新信息的反应程度。</p><p>在有效利他主义论坛最近发表的一篇文章<a href="https://forum.effectivealtruism.org/posts/Z7r83zrSXcis6ymKo/dissolving-ai-risk-parameter-uncertainty-in-ai-future"><u>“解决”人工智能风险 - 人工智能未来预测中的参数不确定性中</u></a>，笔名<a href="https://forum.effectivealtruism.org/users/froolow"><u>Froolow</u></a>的作者为六个卡尔史密斯模型参数估计中的每一个添加了元不确定性，并表明这样做时，人工智能带来的估计存在风险下降。您可以在我们的模型中探索相同的效果。一个好的起点是选择一个单一的中值估计——例如，来自原始卡尔史密斯报告的估计。然后在元不确定性选择中选择“View across range of meta-u”。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/fwdimssrxrvfpbjiy1hc"></p><p>元不确定性选项将元不确定性的量从零（即点估计）变化到单个概率估计可能的最大元不确定性。对于每个元不确定性级别，将相同的逻辑方差应用于所有六个输入评估。</p><p>主要输出的<i>概率带</i>视图（存在灾难的概率）说明了随着每个参数的元不确定性增加，最终结果中的元不确定性如何表现。此处显示了能带图。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/iob1bw51uwthkbemaies"></p><p> （注：由于蒙特卡罗期间样本量有限，波形曲线是很小的变化）。</p><p>如果没有元不确定性，卡尔史密斯估计存在灾难的概率为 5%，当（元）不确定性水平为零时，如左图所示。随着元不确定性的增加，中值估计（绿线）在图右侧下降至约 0.75%，并且继续进一步下降至此处绘制的右侧。随着元不确定性的增加，甚至 0.75 分位数（最终）也会下降。</p><h2>框架效果</h2><p>这里有一个悖论。为什么对自己的信念不太确定会让你得出世界更安全的结论？这是否证明了“无知就是福”？如果我们投资更多的研究来加深对我们面临的风险有多大的了解，生存灾难的可能性会更大吗？</p><p>一些研究将人工智能接管视为分离事件，这意味着除非满足某些条件，否则它就会发生，而其他研究（例如卡尔史密斯）将其视为联合事件，这意味着必须满足一组条件才能发生灾难发生。</p><p>使用点估计时，这些框架效应不会影响最终结果。如果我们采用卡尔史密斯模型，并将模型中的每个命题变成否定陈述而不是肯定陈述：例如，“APS 系统不会在部署时产生高影响的故障”，并用 1 减去我们最初的概率估计，那么我们将得到最终概率相同。但是，至关重要的是，如果我们的概率分布存在不确定性，则合取模型和析取模型的行为方式就不一样。</p><p>当你意识到颠倒框架会颠倒效果时，这个悖论变得更加矛盾。卡尔史密斯分解表明，当 6 个事件全部发生时，灾难就会发生。相反，你可以假设超级智能带来的灾难是不可避免的，除非在此之前解决 6 个开放的技术问题（事实上，在<a href="https://www.lesswrong.com/posts/XtBJTFszs8oP3vXic/ai-x-risk-greater-than-35-based-on-a-recent-peer-reviewed"><u>人工智能 X 风险 >;35% 的帖子中，主要基于最近在 LessWrong 上经过同行评审的论点</u></a>，迈克尔·科恩使用了这个框架）。在这种相反的框架下，元不确定性的增加会向相反的方向产生影响，使得我们的不确定性越大，灾难就越有可能发生。苏亚雷斯<a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive"><u>关于析取 AGI 毁灭场景的文章</u></a>定性地传达了这一观点，列出了他认为所有事情都必须正确进行才能避免人工智能存在灾难的一些事情：在这样的模型中，世界的普遍不确定性增加了灾难的可能性。</p><p>当然，这个悖论只是一种幻觉。但由于您很容易被误导，因此值得更深入地了解这种现象。上图中的结果是六个不确定估计的乘积。以下数学关系只是协方差定义的重新排列，表明算术平均值随着（元）不确定性的增加而稳定：</p><p> E[xy] = E[x] E[y] + cov(x,y)</p><p>换句话说，当每个参数的评估是独立的（意味着协方差为零）时，它们乘积的平均值就是它们平均值的乘积。因此，元不确定性的平均值与水平的关系图将是一条水平线。 （旁注：由于多种原因，参数估计之间的协方差可能并非真正为零，但该模型不包含协方差的任何表示或估计。相关问题是它们是否被建模为独立的，并且它们确实在我们的模型中） 。</p><p>然而，乘积的中位数随着元不确定性的增加而降低。无论元不确定性分布的形状如何，这种情况都会发生。为了实现这一点，元不确定性分布的右尾必须增加以补偿中值的下降。这意味着，随着元不确定性的增加，元不确定性分布变得更加<a href="https://en.wikipedia.org/wiki/Kurtosis#Leptokurtic"><u>尖峰</u></a>。均值稳定性所显示的净平衡不会让您得出世界更（或更不）安全的结论。</p><p>在我们的模型中，随着元不确定性的增加，平均值实际上确实略有下降。如果您选择平均视图，您将会看到这一点。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/y24kdjxjy3pu9vygakv8"></p><p>波纹度是由于它是通过<a href="https://docs.analytica.com/index.php/Monte_Carlo_and_probabilistic_simulation"><u>蒙特卡罗模拟</u></a>以有限的样本量计算得出的。略有下降是因为我们在应用元不确定性时保持每个分布的中位数不变。每个参数的元不确定性使用<a href="https://en.wikipedia.org/wiki/Logit-normal_distribution"><u>Logit 正态分布</u></a>（也称为 Log-odds 分布）建模，其中数量的<a href="https://docs.analytica.com/index.php/Logit"><u>Logit</u></a>分布为正态分布。当我们增加方差时，我们保持正态分布的均值不变。当您这样做时，logit 的均值会略有下降，从而每个参数估计的均值也会略有下降。如果你保持均值不变而不是中位数（这很容易做到），那么均值是完全稳定的。我们发现这两个选项的差异在概率带图中是不可察觉的。</p><p>在<a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>“费米悖论是由于平均数缺陷造成的吗？”</u></a>一文中，我们回顾了 Sandberg、Drexler 和 Ord (SDO) 的论文<a href="https://arxiv.org/abs/1806.02404"><u>“Dissolving the Fermi Paradox (2018)”</u></a> ，并提供了实时交互模型。费米悖论指的是人类尚未发现任何外星文明的明显矛盾，尽管在我们银河系的数千亿颗恒星中肯定有很多外星文明。与卡尔史密斯模型一样，<a href="https://www.seti.org/drake-equation-index"><u>德雷克方程</u></a>（估计银河系中可检测到的文明数量）是一个乘法模型。 SDO 表明，通过明确地模拟 Drake 方程每个参数的不确定性，费米悖论不再令人惊讶。</p><p> <a href="https://www.lesswrong.com/posts/XAS5FKyvScLb7jqaF/cross-post-is-the-fermi-paradox-due-to-the-flaw-of-averages"><u>具有显式不确定性的费米悖论模型</u></a>和具有显式元不确定性的卡尔史密斯模型（本文的主题）具有相同的数学形式。我们看到卡尔史密斯模型中的中位数和下分位数随着（元）不确定性的增加而减少，但这并没有真正改变我们对风险的有效判断。然而，费米模型中不确定性的增加极大地增加了地球上我们在银河系中孤独存在的可能性。为什么该效应在费米情况下是真实的，但在本情况下只是一种幻觉？</p><p>在费米案例中效应真实存在的原因是所提出的问题（“银河系中没有其他可接触的智慧文明的概率是多少？”）是一个关于分位数的问题，并且下分位数确实减少了当不确定性增加时。 P(N&lt;1)，其中 N 是此类外星文明的数量，是累积概率或反分位数。由于乘法模型因子的不确定性增加会减少左尾部的分位数，因此会导致逆分位数增加。因此，在德雷克方程中添加不确定性合理地增加了我们在银河系中孤独存在的可能性。真正的缺陷是从一开始就忽略了明确的表示（Sam L. Savage 称之为<a href="https://www.flawofaverages.com/"><u>平均值缺陷</u></a>）。相比之下，卡尔史密斯模型提出的主要问题（“存在灾难的概率是多少？”）是关于相对于元不确定性的平均值的问题。因此，对于这个问题（或基于预期效用的任何决策），由于包含元不确定性而导致风险降低的表象只是一种幻觉。</p><h3>解释框架效应</h3><p>我们已经看到，框架效应所产生的明显悖论是虚幻的。但还有一个进一步的问题：将人工智能存在风险构建为合取风险或析取风险的“正确”方式是什么？</p><p>这是一个很难回答的问题。一种观点是，将 AGI 存在的灾难视为除非满足某些条件才会发生的事情，可能会导致高估发生高影响失败的可能性。按照这种观点，要求一条明确的道路来实现稳定的结果和完全的生存安全既过于苛刻，又在历史上不准确，因为这不是人类应对以前威胁的方式。霍尔顿·卡诺夫斯基（Holden Karnofsky） <a href="https://www.lesswrong.com/posts/jwhcXmigv2LTrbBiB/success-without-dignity-a-nearcasting-story-of-avoiding#Success_without_dignity"><u>在这里</u></a>提出了类似的观点。将成功视为连贯性的框架可能会排除“得过且过”的可能性，即无计划的“没有尊严的成功”。由于许多领域专家<a href="https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom?commentId=ibGxdAC9nYajWfyfq"><u>认为这是可信的</u></a><u>，</u>因此它可能会导致我们大大低估生存机会。</p><p>另一方面， <a href="https://www.lesswrong.com/posts/ervaGwJ2ZcwqfCcLx/agi-ruin-scenarios-are-likely-and-disjunctive#Correlations_and_general_competence"><u>内特·苏亚雷斯（Nate Soares）</u></a>等一些专家<u> </u>认为人工智能是一种不同的情况：大量的参与者致力于通用人工智能，他们中的任何一个都可能产生一场生存灾难的风险，以及为防止这种情况而必须发生的所有事情（有人必须制定一个一致的方案） AGI，然后快速使用它来消除人工智能的存在风险），意味着将生存视为联合事件更有意义。</p><p>这些不同的框架反映了不同的世界模型和威胁模型。存在这种分歧的部分原因是苏亚雷斯对人工智能极端<a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#Sharp_Left_Turn"><u>对齐困难</u></a>、人工智能起飞速度以及有效缓解措施的可能性很低的看法。如果你隐含地使用一个模型，其中人类文明往往会由于内部激励而以固定的方式做出反应，除非有某种干预，那么更自然地认为，除非发生特定的干预，否则我们将遵循默认的灾难路径。另一方面，如果我们看到许多可能的未来和许多降低人工智能存在风险的途径，但不知道最终的反应会是什么样子（正如<a href="https://www.lesswrong.com/posts/Fbk9H6ipfybHyqjrp/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai">“人工智能风险降低手册”</a>所描述的那样），那么需要一组特定的成功所需满足的条件似乎过于规定性。</p><p>我们认为，这个框架问题，以及是否将生存视为合取或析取，<i>本身</i>就是我们应该不确定的事情，因为你是否将生存视为合取取决于你的威胁模型的细节，而我们不知道想要假设任何一种威胁模型都是唯一正确的。</p><p>目前，我们只有卡尔史密斯报告模型，但理论上我们可以通过查看合取模型和析取模型并详细比较它们来解决这个问题。</p><p>例如，报告《 <a href="https://forum.effectivealtruism.org/posts/eggdG27y75ot8dNn7/three-pillars-for-avoiding-agi-catastrophe-technical"><u>避免 AGI 灾难的三大支柱：技术调整、部署决策和协调</u></a>》提供了一个将成功视为结合的起点模型，我们可以对其进行调整以与卡尔史密斯的模型一起工作。</p><p>另一种选择是改变卡尔史密斯报告，要求更少的步骤，更好地表达这样的担忧：连词链越长，就越有可能忽略析取影响。该表述将考虑开发 APS 的激励措施和可行性的命题 (1) 和 (2) 分解为“何时开发 AGI”的直接估计。然后保留对齐难度前提，然后将命题 (4, 5, 6) 分解为对给定未对齐的 APS-AGI 的接管机会的估计。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/sGkRDrpphsu6Jhega/dozxmoz0isqh8ouyyxn9"></p><p>这种替代方案的步骤更少，因此更好地代表了一种模型，该模型将失调的人工智能接管视为涉及许多难以提前应对或影响的可能路线，并将失调的权力寻求行为视为 AGI 发展的自然结果。这种方法可能更适合那些认为失准的权力寻求系统的发展是通用人工智能发展的可能结果，并且人工智能接管的风险与通用人工智能系统本身的发展更密切相关的人。</p><p>除了探索人工智能存在风险的联合和析取模型之外，在对 APS 将如何开发做出更详细的技术假设的模型之间进行模棱两可也可能有用。例如，Ajeya Cotra的模型<a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to"><u>“没有具体对策，最简单的AGI路径导致接管”</u></a>试图用技术假设来构建AGI发展的具体模型，但考虑到这些假设，更容易得出更有力的结论。类似地，考虑到对于 AGI 到底如何最终会出现失调和权力追求存在着广泛的多样性，而不是二元的“失调的人工智能是否已开发出来”，我们可能会<a href="https://www.lesswrong.com/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty#The_Scale"><u>在对齐难度上有一个不同成功概率的分布</u></a>。</p><p>用不同的技术假设来消除不同模型的歧义可以帮助我们更好地理解与人工智能开发相关的潜在风险。通过探索具有不同技术细节和假设水平的不同模型，我们可以更全面地了解潜在风险。</p><p> While this model does not incorporate entire complex alternative inside-view models like those just mentioned, we have incorporated some alternative, less-detailed, simpler alternative &#39;outside view considerations&#39; to illustrate how we go about combining different worldviews to produce an all-things considered estimate.</p><h1> Outside View considerations</h1><p> We&#39;ve talked before about the challenges of combining outside view considerations and more detailed models of the same question. We can attempt to integrate these considerations by delving deeper and examining various reasons to expect our detailed world models to be systematically mistaken or correct.</p><p> We will examine five reference classes into which various experts and commentators have placed AI existential catastrophe. In each case: &#39;Second Species&#39;, &#39;Reliability of existential risk arguments&#39;, &#39;Most important century&#39;, &#39;Accuracy of futurism&#39;, &#39;Accuracy of predictions about transformative tech&#39;, the argument locates AI Existential risk arguments in a (purportedly) relevant reference class: predictions about new sentient species, predictions about human extinction, predictions about which period in history is the most impactful, predictions about large scale civilizational trends in general and predictions about transformative technologies (including past predictions of dramatic AI progress).</p><p> The Carlsmith model implies that all of these things could occur (a new species, extinction, this period of history will be extremely impactful, there will be a large-scale dramatic transformation to society, there will be dramatic transformative technical progress), so it is worth examining its predictions in each reference class to determine if we can learn anything relevant about how reliable this model is.</p><h2> Second species argument</h2><p> This argument suggests that as we create AGI (Artificial General Intelligence) we are essentially creating a “ <a href="https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction"><u>second species</u></a> ” that is a human-level intelligence. And by analogy, just as humans have historically been able to supplant other animals, AGI may be able to supplant humans.</p><p> The key premise is that intelligence confers power. Human intelligence allows us to coordinate complex societies and deploy advanced technology, exerting control over the world. An AGI surpassing human intelligence could wield even greater power, potentially reducing humanity to a subordinate role. Just as humans have driven some species extinct and transformed ecosystems, a superintelligent AGI need not preserve humanity or our values. Anthropologists observe that new species often displace incumbents when invading a territory. Similarly, AGI could displace humankind from our position controlling Earth&#39;s future.</p><p> This argument is straightforward and has been widely understood by researchers going all the way back to Alan Turing the 1950s, so while it relies on fuzzy concepts and is open to many objections, it arguably has a better &#39;track record&#39; in terms of the amount of scrutiny it has received over time than the more detailed arguments given by Carlsmith.</p><h2> Reliability of existential risk arguments</h2><p> Another important consideration is the base rate for arguments of existential risk. Historically, predictions of catastrophic events, even ones that were apparently well justified by detailed arguments, have not always been accurate. Therefore, it is important to consider if the possibility that the risks associated with AGI are overestimated for similar underlying reasons (eg, the social dynamics around existential risk predictions, overestimating the fragility of human civilisation, or underestimating humanity&#39;s ability to respond in ways that are hard to foresee).</p><p> One possible driver of inaccuracy in existential risk predictions is <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential"><u>sleepwalk bias</u></a> . Sleepwalk bias is the tendency to underestimate people&#39;s ability to act to prevent adverse outcomes when predicting the future. This can be caused by cognitive constraints and failure to distinguish between predictions and warnings. Because warnings often take the form of &#39;X will happen without countermeasures&#39;, if warnings are misused as predictions we can underestimate the chance of successful countermeasures. People often mix up the two, leading to pessimistic &quot;prediction-warnings&quot;. Thus, when making predictions about existential risk, it&#39;s important to adjust our base rate to account for people&#39;s potential to act in response to warnings, including those made by the one giving the prediction.</p><p> Sleepwalk bias stems from the intuitive tendency to view others as less strategic and agentic than oneself. <a href="https://stefanschubert.substack.com/p/sleepwalk-bias-and-the-role-of-impulses"><u>As Elster notes</u></a> , we underestimate others&#39; capacities for deliberation and reflection. This manifests in predictions that underestimate how much effort people will make to prevent predicted disasters. Instead, predictions often implicitly assume sleepwalking into calamity.</p><p> For existential risks, sleepwalk bias would specifically lead us to underestimate institutions&#39; and individuals&#39; abilities to recognize emerging threats and mobilize massive resources to counter them. Historical examples show that even deeply conflictual societies like the Cold War rivals avoided nuclear war, underscoring potential blindspots in our models. Since the bias arises from a simple heuristic, deep expertise on a given x-risk may overcome it. But for outsiders assessing these arguments, accounting for sleepwalk bias is an important corrective.</p><h2> Most important century</h2><p> Additionally, it is important to consider the probability that the next century is the most important of all, which would plausibly be true if AGI existential risk concerns are well founded. If we have a strong prior against this <a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#the-"><u>&#39;most important century&#39;</u></a> idea then we will be <a href="https://globalprioritiesinstitute.org/wp-content/uploads/William-MacAskill_Are-we-living-at-the-hinge-of-history.pdf"><u>inclined</u></a> to think that AGI existential risk arguments are somehow flawed.</p><p> The Self-Sampling Assumption (SSA) posits that a rational agent&#39;s priors should locate them uniformly at random within each possible world. If we accept the SSA, it seems to imply that we ought to have a low prior on AI existential risk (or any kind of permanent dramatic civilizational change) in this century in particular because of the near-zero base rate for such changes. The detailed evidence in favour of AI existential risk concerns may not be enough to overcome the initial scepticism that arises from our natural prior.</p><p> Alternatively, you might accept the claim<a href="https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/#my-view"><u>proposed by Karnofsky</u></a> that there are extremely strong arguments that this <a href="https://www.cold-takes.com/this-cant-go-on/#why-cant-this-go-on"><u>approximate period in history must be very important</u></a> . First, Karnofsky argues that historical trends in economic growth and technological development show massive accelerations in the recent past. Growth rates are near all-time highs and appear unsustainable for more than a few thousand years at most before physical limits are reached. This suggests we are living during a temporary spike or explosion in development.</p><p> Second, he notes that since growth is so rapid and near its limits, some dramatic change seems likely soon. Possibilities include stagnation as growth slows, continued acceleration towards physical limits, or civilizational collapse. This situation seems intrinsically unstable and significant. While not definitive, Karnofsky believes this context should make us more open to arguments that this time period is uniquely significant.</p><h2> Accuracy of futurism</h2><p> Another important consideration is the base rate of forecasting the future without empirical feedback loops. This consideration fundamentally focuses on the process used to generate the forecasts and questions whether it reliably produces accurate estimates. The history of technology has shown that it can be difficult to predict which technologies will have the most significant impact and AI alignment research especially often relies on complex abstract concepts to make forecasts, rather than mechanistically precise models. <a href="https://forum.effectivealtruism.org/posts/L6ZmggEJw8ri4KB8X/my-highly-personal-skepticism-braindump-on-existential-risk#I_don_t_trust_chains_of_reasoning_with_imperfect_concepts"><u>Some examples</u></a> are discussed in this article.</p><p> One way of assessing reliability is to find a reference class where predictions of AI existential catastrophe are comparable to other future predictions. For instance, we can compare AI predictions to the predictions made by professional futurists in the past and then <a href="https://www.cold-takes.com/the-track-record-of-futurists-seems-fine/#todays-futurism-vs-these-predictions"><u>compare relevant features</u></a> . If they compare favourably to past successful predictions, this may indicate a higher level of reliability in the TAI predictions, and if they don&#39;t, it may suggest that we should be cautious in our assessment of their validity.</p><p> We can also look at other general features of the arguments without comparison to specific known examples of successful futurism, like their level of reliance on abstract concepts vs empirical evidence. AI risk involves unprecedented technologies whose impacts are highly uncertain. There are likely gaps in our models and unknown unknowns that make it difficult to assign precise probabilities to outcomes. While we can still make reasonable estimates, we should account for the significant <a href="https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms"><u>Knightian Uncertainty</u></a> by avoiding overconfident predictions, explicitly acknowledging the limitations of our models, and being open to being surprised.</p><p> Considerations like these arose in the recent XPT superforecaster elicitation. For examples of considerations that we would place under this umbrella, we would include <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;Given the extreme uncertainty in the field and lack of real experts, we should put less weight on those who argue for AGI happening sooner.&quot; (XPT superforecaster team 342)</li><li> &quot;Maybe most of the updates during the tournament were instances of the blind leading the blind.&quot; (Peter McCluskey, XPT superforecaster)</li></ul><h2> Accuracy of transformative technology prediction</h2><p> This considers the historical base rate of similar technologies being transformative and notes that predictions often overestimate impact.  It is important to consider the historical base rate of a technology being economically or socially transformative.</p><p> This is due to a number of factors such as under/overoptimism, a lack of understanding of the technology or its limitations, or a failure to consider the societal and economic factors that can limit its adoption.</p><p> By taking into account the historical base rate of similar technologies, we can gain a more accurate perspective on the potential impact of AI. We see similar arguments made by superforecasters, such as <a href="https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1#The_arguments_made_by_XPT_forecasters"><u>these from XPT</u></a> :</p><ul><li> &quot;The history of AI is littered with periods of rapid progress followed by plateaus and backtracking. I expect history will repeat itself in this decade.&quot; (XPT superforecaster team 339)</li><li> &quot;The prediction track record of AI experts and enthusiasts have erred on the side of extreme optimism and should be taken with a grain of salt, as should all expert forecasts.&quot; (XPT superforecaster team 340)</li><li> &quot;Many superforecasters suspected that recent progress in AI was the same kind of hype that led to prior disappointments with AI...&quot; (Peter McCluskey, XPT superforecaster)</li><li> &quot;AGI predictions have been made for decades with limited accuracy. I don&#39;t expect the pattern to change soon.&quot; (XPT superforecaster team 337)</li></ul><h1>结论</h1><p>In this article we have led you through an example application of a model-based approach applied to estimating the existential risks from future AI. Model-based approaches have many advantages for improving our understanding of the risks, estimating the value of mitigation policies, and fostering communication between advocates on different sides of AI risk arguments.</p><p> During our research we identified many challenges for model-based approaches that are unique to or accentuated in the AI existential risk domain compared to most other decision areas.</p><p> We focused on incorporating elements of all of these challenges, in simple ways, into our model as a way of creating a starting point. The model is certainly not a definitive model of AI x-risk, but we instead hope it might serve as an inspirational starting point for others in the AI safety community to pursue model-based approaches. We&#39;ve posted our model online in open-source tradition to encourage you to learn from it, borrow from it, and improve on it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk<guid ispermalink="false"> sGkRDrpphsu6Jhega</guid><dc:creator><![CDATA[Sammy Martin]]></dc:creator><pubDate> Fri, 25 Aug 2023 10:32:16 GMT</pubDate> </item><item><title><![CDATA[What AI Posts Do You Want Distilled?]]></title><description><![CDATA[Published on August 25, 2023 9:01 AM GMT<br/><br/><p> I&#39;d like to distill AI Safety posts and papers, and I&#39;d like to see more distillations generally. Ideally, posts and papers would meet the following criteria:</p><ul><li> Potentially high-impact for more people to understand</li><li> Uses a lot of jargon or is generally complex and difficult to understand</li><li> Not as well-known as you think they should be (in the AI X-risk space)</li></ul><p> What posts meet these criteria?</p><br/><br/> <a href="https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wdvGgEGMohpZcgASP/what-ai-posts-do-you-want-distilled<guid ispermalink="false"> wdvGgEGMohpZcgASP</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Fri, 25 Aug 2023 09:01:27 GMT</pubDate> </item><item><title><![CDATA[2084]]></title><description><![CDATA[Published on August 25, 2023 7:42 AM GMT<br/><br/><p> In a room adorned with the latest Apple products, citizens gathered, their bodies adorned with electrodes, the silent guardians against subconscious bias. Today&#39;s assembly was a regular one, the &quot;Cultural Appreciation and Linguistic Harmony&quot; (CALH), a ritual of unity in the pursuit of Progress.</p><p> Images of the Closed-Minded Reactionaries glared from the screens: racists, sexists, those clinging to outdated words and ideas. A voice, steady and strong, began to speak, chronicling the crimes and clarifying the ever-evolving language of decency. The electrodes beeped reassuringly, a constant reminder endorsing the virtues of diversity and tolerance.</p><p> A harmonious hum filled the room, swelling into a chant of solidarity. Faces were aflame with righteous indignation, fists clenched in a shared purpose. The room pulsed with collective emotion, each individual melding into a singular force against the Closed-Minded.</p><p> Images on the screens flickered between outdated flags and once-celebrated leaders, now marked as symbols of intolerance. Phrases like &quot;All Lives Matter&quot; and &quot;Traditional Marriage&quot; were displayed, labeled as echoes of a bigoted past. The chant morphed into a raucous roar, with cries of &quot;Equity!&quot; and &quot;Inclusion!&quot; ringing through the room. The assembly&#39;s fervent symphony for Progress was underscored by the soft, corrective shocks from the electrodes, ensuring uniformity of thought. Participants were shown newly-cancelled YouTubers and de-platformed authors. The crowd&#39;s response was a blend of revulsion and righteousness, a single entity caught in a relentless pursuit of a constantly changing moral code.</p><p> Then, with a calculated crescendo, it was over. The devices dimmed; the electrodes were removed. The room, now hushed, felt colder, emptier. Citizens dispersed, momentarily fortified by their shared experience, but marked by the unrelenting vigilance of the Party.</p><p> In a world where harmful words must be gently corralled, the CALH was a beacon of Progress, a unifying ritual under the watchful eye of a Party that cared enough to guide every thought, every word, towards our utopian future.</p><br/><br/><a href="https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QxxA7ucNuSvh6kwK6/2084<guid ispermalink="false"> QxxA7ucNuSvh6kwK6</guid><dc:creator><![CDATA[lsusr]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:42:13 GMT</pubDate> </item><item><title><![CDATA[Apply for the 2023 Developmental Interpretability Conference!]]></title><description><![CDATA[Published on August 25, 2023 7:12 AM GMT<br/><br/><p> <strong>What</strong> : A conference to advance the DevInterp research program</p><p> <strong>When</strong> : 5-12 November 2023</p><p> <strong>Where</strong> : Wytham Abbey, Oxford</p><p> <strong>How:</strong> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Apply now!</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QpFiEbqMdhaLBPb7X/a5oea02gswfpe8gfmcoj"><br></p><p> We are pleased to announce the upcoming <u>Developmental Interpretability Conference</u> , hosted at the historic <a href="https://www.wythamabbey.org/"><u>Wytham Abbey</u></a> in Oxford from 5 to 12 November. This conference expands upon the <a href="https://devinterp.com/2023/june-summit"><u>2023 Singular Learning Theory &amp; Alignment Summit</u></a> and provides an opportunity to collaboratively work on open problems in the <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>DevInterp Research Agenda</u></a> . The conference program will recall the basics of Singular Learning Theory &amp; DevInterp and will discuss the latest advancements in Developmental Interpretability.</p><p> <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>Click here to apply!</u></a> Space at the conference is limited, so be sure to apply early as applications may close when all slots have been filled. We hope to see you in Oxford this November!</p><h2> <strong>FAQ</strong></h2><p> <strong>What are the prerequisites?</strong></p><p> The conference will use ideas from algebraic geometry, Bayesian statistics and physics to understand machine learning and AI alignment. Although helpful, it is not necessary to master all these topics to productively participate in the conference.</p><p> In order to get the most out of the conference program, we highly recommend participants review introductory SLT material such as <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC"><u>Distilling Singular Learning Theory</u></a> by Liam Carroll. Participants may also benefit from watching several of the <a href="https://www.youtube.com/@SLTSummit/playlists"><u>Singular Learning Theory &amp; Alignment Summit 2023 lectures</u></a> .</p><p> <strong>I am skeptical about some of the arguments for AI Alignment. Do I need to buy AI X-risk to attend this conference?</strong></p><p> We believe the development of superintelligent AI poses a serious risk for humanity and the DevInterp agenda aims to make progress on this problem. However, while making progress on AI alignment is the motivation behind our scientific agenda, SLT and developmental interpretability are of broad interest and we invite attendance from those wishing to learn more or contribute, on their own terms.</p><p> <strong>Do I need to have attended the SLT &amp; Alignment Summer 2023 Summit to be able to attend this DevInterp Conference?</strong></p><p> No, you do not need to have attended the SLT &amp; Alignment Summer 2023 Summit to attend the DevInterp Conference.</p><p> <strong>Do I need to pay to attend the conference? And how about lodging, food and travel costs?</strong></p><p> The conference is free to attend. Lodging, food and transit between Oxford and the venue are all kindly provided by Wytham Abbey. Travel costs to Oxford are not paid for.</p><p> <strong>Will you offer travel support?</strong></p><p> The amount of travel support we can provide is TBD. Let us know in the application form if you are blocked from attending because of travel costs, and we&#39;ll see what we can do.</p><p> <strong>How do I apply?</strong></p><p> By filling in <a href="https://forms.gle/MB3yeiJA4QkY9K5o6"><u>this application form</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-for-the-2023-developmental-interpretability-conference<guid ispermalink="false"> QpFiEbqMdhaLBPb7X</guid><dc:creator><![CDATA[Stan van Wingerden]]></dc:creator><pubDate> Fri, 25 Aug 2023 07:12:36 GMT</pubDate></item></channel></rss>