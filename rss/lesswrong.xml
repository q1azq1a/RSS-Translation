<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 16 日星期四 04:14:48 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Learning coefficient estimation: the details]]></title><description><![CDATA[Published on November 16, 2023 3:19 AM GMT<br/><br/><h2>这是做什么用的</h2><p>学习系数 (LC) 或 RLCT 是奇异学习理论中的一个量，可以帮助量化深度学习模型的“复杂性”等。</p><p>本指南主要旨在帮助有兴趣改进学习系数估计的人们快速了解其幕后工作原理。如果您只是尝试在自己的项目中使用 LC，则可以在不了解所有详细信息的情况下使用该<a href="https://github.com/timaeus-research/devinterp/tree/main">库</a>，尽管本指南可能仍然有帮助。如果您还没有阅读<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">本文</a>，强烈建议您先阅读这篇文章。</p><p>我们主要介绍<a href="https://jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf">WBIC 论文</a>（Watanabe 2010），它是当前 LC 估计技术的基础，但这里的演示是原创的，旨在获得更好的直觉，并且与论文有很大不同。我们还将简要介绍<a href="https://arxiv.org/abs/2308.12108">Lau 等人。 2023年</a>。</p><p>尽管讨论很长，但您最终在实践中所做的事情<i>非常简单</i>，并且代码旨在强调这一点。经过一些相对快速的设置后，实际的 LC 计算可以通过一两行代码轻松完成。</p><h2>这不是为了啥</h2><ul><li>对 SLT 的良好概述，或者首先研究 LC 或损失景观卷背后的动机。我们在这里主要关注 LC 估计。</li><li>抽样细节。这些非常重要！但它们并不是单一学习理论所独有的，并且在其他地方有大量有关 MCMC 的优质资源和教程。</li><li>公式推导，超越高级推理。</li></ul><h2>总长DR</h2><ul><li>什么是学习系数？ （<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">上次</a>回顾）<ul><li>学习系数 (LC) 也称为 RLCT，用于衡量盆地宽度。</li><li>这并不新鲜，但通常“盆地宽度”被操作为“盆地平坦度”——即通过 Hessian 行列式。当模型是奇异的（Hessian 矩阵的特征值为零）时，这是一个坏主意。</li><li> LC 将“盆地宽度”操作为（低损耗渐近）体积缩放指数。正如奇异学习理论所证明的那样，这最终是正确的衡量标准。</li></ul></li><li>我们如何衡量它？<ul><li>事实证明，直接测量高维体积是很困难的。我们不做这个。</li><li>相反，我们使用 MCMC 进行统计学中所谓的“矩量法”估计。我们设计一个以 LC 作为总体参数的分布，从该分布中采样并计算其矩之一，并求解 LC。</li><li>我们在本节中简化了一些细节，但这是 LC 估计的概念核心。</li></ul></li><li>我们如何衡量它（真实的）？<ul><li>上面的内容稍微简化了一些。 LC 确实测量损失量缩放，但它使用的“损失”是经验损失函数的平均值或“无限数据”限制。</li><li>实际上，您不知道这个无限数据丢失函数。幸运的是，您已经对它有了一个很好的估计——您的经验损失函数。不幸的是，这个估计并不完美——它可能有一些噪音。事实证明，这种噪音实际上在您<i>最不</i>想要的地方最<i>严重</i>。</li><li>但最终一切都会解决！实际上，您只需要对“理想化”算法进行一点小小的修改，一切就可以正常工作。这将为您提供一个在实践中真正有效的算法！</li><li>最后，出于可扩展性等原因，最先进的方法（Lau et al. 2023）做了一些简单的修改：它仅<i>*本地*</i>测量学习系数，并使用小批量损失而不是全损失。批。</li></ul></li></ul><p>以图表的形式：当我们从理想化（顶部）转向现实（底部）时，我们会得到新的问题、解决方案和改进方向。该指南本身最详细地介绍了前两行，这可能是概念上最难思考的，并在最后直接从第二行跳到第四行。 </p><figure class="image image_resized" style="width:100%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9ecpBaAiGQnkmX9Ex/fmppl7bmgijkwtx7lnbh"></figure><p></p><p><i>请参阅</i><a href="https://colab.research.google.com/github/zfurman56/intro-lc-estimation/blob/main/Intro_to_LC_estimation.ipynb"><i>链接的 Colab 笔记本</i></a><i>以获取完整指南。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/9ecpBaAiGQnkmX9Ex/learning-coefficient-estimation-the-details#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9ecpBaAiGQnkmX9Ex/learning-coefficient-estimation-the-details<guid ispermalink="false"> 9ecpBaAiGQnkmX9Ex</guid><dc:creator><![CDATA[Zach Furman]]></dc:creator><pubDate> Thu, 16 Nov 2023 03:19:09 GMT</pubDate> </item><item><title><![CDATA[Extrapolating from Five Words]]></title><description><![CDATA[Published on November 15, 2023 11:21 PM GMT<br/><br/><p>如果你只能用<a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">五个词</a>来表达一个想法，人们会从这五个词中推断出什么？您可以使用法学硕士通过实验来发现人们可能认为这五个词的含义，而不是猜测。您可以使用它来迭代您想说的五个单词，以便最好地传达您的预期含义。</p><p>我产生这个想法是因为我尝试要求克劳德在链接上总结一篇文章。 Claude 不会跟踪链接，因此它会幻觉标题中的摘要，该摘要包含在 URL 路径中。这是使用<a href="https://www.lesswrong.com/posts/LkjpHGiELQzed8hdu/why-the-problem-of-the-criterion-matters">我的 LessWrong 帖子之一</a>执行此操作的示例： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/arzjwjobp4f1mzw7butm" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/slx0yvnxmtj5fhhbonfl 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/i4hsanmguhbqxeubhcgd 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sbhdvppzm36adnjmzf0j 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/onfbpkewjhbcvp82t5nc 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ooochb4zyqwvu9nbkait 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gyfckyk0nkrp37ysfj6m 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ocvikzsm8zsgwgxlwbc9 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bwjocv054qp6xzz7n2mr 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/z2cdhnfimjqy5iydhhjs 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ubotioxkb9gkckqmobaj 1638w" alt=""></figure><p>它产生了一些错误的细节，并遗漏了帖子中实际存在的许多细节，但这里并不是完全不合时宜。如果我的〜五个词是“标准问题很重要”，那么这将是我为什么这么说的合理推断。</p><p>除了使用链接之外，我还可以要求 Claude 提出它认为我会在具有特定标题的帖子中添加的内容： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/r1w6ub2kairpaylby2ya" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/vpuvlg0a2qxs02d9a8mh 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gd1asogssvzjvtme7zzx 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lr2zcthp13guf9fiheci 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/veszm5ofcocu4ycc8m3b 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/b9bbmyxfbj5vhgqwefcz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/s6ntzjebigweykpebss7 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/t53x7vzvjuzseje3ogpq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fehqezg1cmuzigwpn8e3 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/n6359ek9mzuw9un5gtgy 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bkwlgtvdtbnazl3cfnrd 1586w"></figure><p>奇怪的是，它在某些方面表现得更糟，而在其他方面表现得更好。与它产生链接摘要的幻觉不同，这次它提出了我绝对不会说或不希望有人得到的东西，比如我们可以解决标准问题，足以拥有客观的知识标准。</p><p>但也许促使它关注 LessWrong 才是问题所在，因为 LessWrong 引起了很多实证主义者的共鸣，<a href="https://www.lesswrong.com/posts/dTkWWhQkgxePxbtPE/no-logical-positivist-i">但埃利以泽的相反说法</a>并不成立。所以我尝试了不同的提示： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xrpj3qaqn7suizcevhys" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/k9x2qujwgkf0metacypy 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bpm1a2ggpidjlcsqwesd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zd59kndoaw4wxykeycyy 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nqnrhxxvr8huxcay1u32 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pn6l2shzgwq8ql0obr4n 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bctwvfofjgkjzck0nqq1 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hkmze9rfpylwqklxnklq 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jzz0ty1oy4tlrqc4bhjr 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hb3mb6kehxcpipzf6rab 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jjmgmqa2khno1ugfbhox 1578w"></figure><p>这可以？这不太好。这听起来像是一个无聊的哲学本科生为认识论课写的论文的总结。</p><p>让我尝试问它某种版本的“我的大约五个词是什么意思？”： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lkewxseua3vdb05j7toe" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wtbbd1ubjfieqi5v5w54 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fyqopidgidzeckrmi3tt 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dv7skp8zycwv5qhs4njh 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qanzrbxq4kwjyb3ysyyq 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mai1kzmoyu3ar1aju9xa 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/w5lpbsgulhtld3oa1vi3 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lszilosu5mu0wwnxnvpn 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/nvjhfugeemp0z6idhzsu 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/iixmdvneheblpgyl1a8i 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/xankmu5wloylpm307klo 1614w"></figure><p>这非常好，基本上我希望有人能从我身上夺走“标准问题很重要”的东西。让我们看看如果我调整语言会发生什么： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/yqb2ijarrst9jsfx5gjd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/eut3fmkm8shlcbcug5g0 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pyw3pnoail6yj83xkgnd 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qp9yniwnn0oojpw6ezvj 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lh501i7swrx2som5cnqt 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/gana511ysgdqyjp70wgl 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/v8x1sgfssqcwx2b8rihc 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/uldogbpsdoiadnic0dml 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q0qfpikhfmxrrvpw3ade 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lzttdsrr3vwed4oltack 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcyq8xocefuh244rj9bj 1780w"></figure><p>整洁的！它注意到了“重要”而不是“重要”所暗示的许多细微差别。这对于尝试短语的不同变体以查看这些微小变体对隐含含义有何变化非常有用。我认为这对于诸如文字加工公司价值观和使命以及其他每个单词都必须承载很多含义的短语等任务很有用。</p><p>现在让我们看看它是否可以反向完成任务！ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/sxgabqz0wyvvx4lxwhje" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ov7pj5rakxa9pjqjxrs6 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qitt5xg5d5aoetcfvfe2 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ongwefldjntxhgrqtaq1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/tudmg2sfnqjq2gqjrydl 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/is0bducapbzwva9zyax4 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/u8xa7utkyqizzg9zqe37 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ehjtaxuk5tfp3eganyln 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/f47objdkzzxsnxdar3sj 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jmvyyc0rhfola1dljksj 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/pdgspb42zyihzi6pikad 1726w"></figure><p>老实说，“不确定性破坏知识”可能比我想出的任何东西都要好。谢谢，克劳德！</p><p>作为最后的检查，克劳德能否从自己的总结中推断出来？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/y2pf0udioglx0qsdhpv6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/fdi6zhj1wtfivqlqcpuf 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ouxcagejqzlqzcphlhzd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jz3tiot9xgm5gl15hbls 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etynfz3izydbggskkzfi 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bdsy7bzo33ahduse7o5j 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q4ws7lixelgpvecilntf 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/zail1zfcfpngok6rmbom 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/q9qjxw72otwkfpbihcq7 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/etkhvvkopnnoailb3ae1 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/qrkgidttighy0hl474d8 1586w"></figure><p>显然，它丢失了一些细节，特别是关于标准问题的细节，并且弥补了一些我不想让它得到的东西。将细致入微的信息压缩成大约五个单词，并且仍然传达信息的核心，这似乎是理所当然的。</p><p>好吧，最后的测试，克劳德可以从我可能对“基本不确定性”做出的典型陈述中推断出什么？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dc6roac8u3oskbx4zwmk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ako6peikfk1vdblnddfu 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dh0yunove6oo8e4fhk3y 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mqhewctspnzhqsxqtjkw 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/dp3x6bfuj6if7ryfcxzn 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ve9l0vltshrvuy5dfgqr 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bzhau52vnet9ehltdd9f 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ogyqa2awhac3kkorgliz 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ceudntk6ndseqfz6xewg 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/x0elkg9cjhej9iblmcgh 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/hclfjabiodfwlfnxnaz6 1570w"></figure><p>嗯，还可以，但不是很好。也许我应该尝试找到另一个短语来表达我的想法？让我们看看它对“基本面不确定性”这个书名的看法： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/swabayitctg8yc2odybb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jrn6yiiqgufmxzirkdyd 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/mcw6qq4ze9ppchawd8uk 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/rhcuowppft8kzrpi3vas 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/lihcbo7lcobtasexmubz 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wkymdjqovj4ushzjlol7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/jnrmicoayxtn1e1vwwrp 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ibuidok3d2fonyom0jag 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/bvjjhfbwjaebf8jjiway 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/ugrnmwrcb7lwajtkjqpr 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HD8kLyYcSuYRi4vzP/wfh9e7vyz2ni5esm9wp8 1598w"></figure><p>足够接近。我可能不需要重新命名<a href="https://www.lesswrong.com/s/HMs2yT9D6LjYR5jQT">我的书</a>，但我可能需要设计一个好的副标题。</p><p>基于以上在提示工程中的实验，克劳德在迭代简短短语摘要方面相当有帮助。它能够捕捉到微妙的细微差别，这对于找到正确的短语来传达一个重要的想法非常有用。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from-five-words#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HD8kLyYcSuYRi4vzP/extrapolating-from- Five-words<guid ispermalink="false"> HD8kLyYcSuYRi4vzP</guid><dc:creator><![CDATA[Gordon Seidoh Worley]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:21:31 GMT</pubDate> </item><item><title><![CDATA[In Defense of Parselmouths]]></title><description><![CDATA[Published on November 15, 2023 11:02 PM GMT<br/><br/><p>先决条件：贵<a href="http://benjaminrosshoffman.com/the-quaker-and-the-parselmouth/">格会教徒和蛇佬腔</a>。</p><h2>我。</h2><p>首先，快速总结一下。</p><p>在先决条件帖子中，本杰明霍夫曼描述了三种人。这些人是假设的极端：他们是在无摩擦真空中相互作用的完美球体的社会和认知等价物。有些贵格会教徒总是说实话，并且在说要做某事时信守诺言。有些演员总是说当下看起来不错的话，即使他们发誓和发誓，也不能可靠地遵守诺言。最后，还有蛇佬腔，他们可以自由地对演员说谎，但只对其他蛇佬腔说实话，并且（暗示）只对贵格会教徒说实话。</p><p>我赞同这种区别。它是抽象的，现实世界从来都不是那么清晰，但根据我的经验，它确实得到了一些有用的东西来理解。我认为说真话是一个强大的制度优势，并希望更多的人在这种二分法中成为贵格会教徒。本杰明指出，蛇佬腔有些奇怪，因为习惯性说谎可能会侵蚀本能，甚至可能侵蚀说真话的能力；对于真正的人来说，如果不慢慢成为演员，就不可能始终保持蛇佬腔。</p><p>说真话很难。弄清楚世界的真实状况是什么是很困难的。快速而准确地陈述你认为正确的事情是很困难的；英语使得“我相信明天有百分之九十的机会下雨”的句子比“明天会下雨”要长得多。当有人问你是否喜欢他们带来的聚餐砂锅菜（烧焦的和未调味的）时，你最终会受到很多额外的情感尖锐的肘击。全世界的贵格会教徒，我向你们致敬。全世界的演员，我明白了。</p><p>我的第一个主张是成为蛇佬腔是合理的。</p><h2>二.</h2><p>讲故事的时间！下面的故事详细描述了大约二十年前发生的事件，当时我比现在矮了几英尺。一些细节已经被当时在场的其他人证实，但许多细节可能随着时间的推移而发生了变化。</p><p>当我还是个孩子的时候，我必须拍很多照片。我妈妈带我进了办公室，我在等候区闲逛了一会儿，然后护士挥手让我经过前台，我和妈妈就进去了。护士让我坐在医生办公室的一张大塑料椅子上，然后我一边用冰凉的东西擦着我的肩膀，一边问妈妈问题，然后她让我坐一会儿，说：“这不会疼的，你准备好了吗？”我点了头。然后她用针刺了我。</p><p>很痛。我开始哭，并且持续哭了一段时间，直到疼痛消退为隐痛。我父母的安慰或护士的款待都没有改变这一点。我当时没有能力清楚地表达出是什么让我心烦意乱，但这不是痛苦（即使在小时候，当疼痛有目的时，我对疼痛的容忍度也非常高），而是困惑。它不应该伤害——他们对于是否会伤害的判断是错误的吗？这不太合理，用尖锐的东西刺人通常会伤害他们，为什么有人会认为不会呢？我是否记错了他们说的话，他们说会痛而不是不会痛？我的记忆真的那么差吗？我完全困惑了，无法理解发生了什么。</p><p>凭借多年的经验，发生的事情是显而易见的。护士撒谎让一个小孩在注射时保持安静。这个故事多年来一直在重复，每次我都会感到困惑和困惑。直到很久以后，当我顿悟到世界如何看待真理之后，我才想到有人会撒谎。</p><p>虽然很痛苦，但事实证明，这种理解是许多人际互动的有用万能钥匙。有时人们只是撒谎，而且<a href="https://slatestarcodex.com/2016/12/12/might-people-on-the-internet-sometimes-lie/">往往是</a>为了比你想象的<a href="https://www.lesswrong.com/posts/K2c3dkKErsqFd28Dh/prices-or-bindings">更小的利益</a>。文字不是真理，只是人们发出的声音或页面上的符号。人们随意地、轻易地在重要的事情和琐碎的事情上撒谎，以得到他们想要的东西，或者只是因为他们不在乎。这种不在意并不是恶意，只是冷漠。</p><h2>三．</h2><p>有时陈述事实错误是完全可以的。我认为一个重要的区别是大多数相关人员是否知道哪些陈述是哪些。</p><p>有一些明显的案例。如果你从书店的奇幻区拿起一本书，上面有一条龙，那么写那本书里的文字的人几乎可以全权决定在那本书中说出他们想要的任何内容。还有一些不太明显的情况；我很遗憾地通知您，职业摔跤手和舞台喜剧演员在他们嘴里说的话和现实的基本状态之间有着灵活的关系。还有一些可疑的例子，虽然它是虚构的，但旁观者可能会感到相当困惑，比如发现的恐怖电影片段或报纸上“研究节目”一词后面的任何内容。 <span class="footnote-reference" role="doc-noteref" id="fnrefdnozbd7ldgr"><sup><a href="#fndnozbd7ldgr">[1]</a></sup></span>小说对全世界数以百万计的人来说很有趣，任何禁止说谎的禁令都需要为此留出空间。</p><p> （另一个童年故事：我的一个叔叔曾经带我去钓鱼，当我们一天没钓到鱼回来时，我们无意中进行了一场喜剧二重唱，我系统地不同意他的<a href="https://www.britannica.com/dictionary/fish-story">鱼故事</a>的每一行，直到我祖父怜悯并把我拉到一边解释说这些不应该被视为字面上的事实。）</p><p>一些事实性的误解是，如果你真正检查正在发生的事情，就能成功地向对话双方传达正在发生的事情。</p><p>如果你问一个以美式英语为母语的人是否可以帮你做点什么，他们会回答“稍后”，而当你在六十分钟后他们没有出现时你会感到困惑，我实际上很抱歉。这也困扰了我很长一段时间，最终我也接受了。宇宙中没有任何法则将“ˈsɛkənd”的声音与人类心脏跳动所需的时间联系起来。在某些情况下，“ˈsɛkənd”表示大约等于心跳的时间单位，在其他情况下，它表示第一和第三之间的计数，在您刚刚寻求帮助并且有人说“在一秒钟内”的情况下，它的意思是类似“很快，但不是现在。”</p><p>多年来我选择了这场战斗的许多变体，但我已经放弃了。我现在站在语言描述主义者一边。</p><p>有这样一个：我认为不是每个人都知道这是如何工作的。存在证明，我感觉很长一段时间都不知道，所以二十年前说“每个人都知道”是错误的。大家都不知道。至少只要还有那些头脑简单的孩子到处乱跑，人们就会不断地遇到这种情况，而且可能会持续更长的时间，因为其中一些孩子长大成为头脑简单的成年人，他们对真理的立场转变为坚定的原则。当有人按字面意思理解我的话并进行代码转换或至少警告他们时，我会尽量集中注意力。</p><p>在许多美式英语口语句子中，“Literally”一词的作用是充当强化词。 “他在那场比赛中是世界上最好的”和“他在那场比赛中确实是世界上最好的”经常被说成基本上是同一件事。没有与世界其他地区进行实际的权威比较。</p><p>这令人沮丧。如果有一种方法可以在对话中标记“此语句处于贵格会模式”，将会很有用。可悲的是，据我所知，英语过去使用这种标记的每一次尝试都被收买为强化剂。英语单词“ <a href="https://en.wiktionary.org/wiki/very">very</a> ”据说源自“verrai”，意思是“真实”。</p><p>在某些例外情况下，人们普遍认为不允许出现事实错误。美国法律体系对在法院宣誓的人持悲观态度。有些合同希望真实反映所发生的事情。 （尽管离婚率令人震惊，但“直到死亡将我们分开”仍然是许多婚姻誓言中的内容，并且一些类似于最终用户许可协议等法律合同的文件的可执行性值得怀疑。）有些人在个人层面上，设法创造出不应该发生事实错误的空间。偶尔，整个社区都会尝试这样做。正如本杰明在《贵格会和蛇佬腔》中指出的那样，现实生活中的贵格会仍然存在。</p><p>然后还有LessWrong。</p><h2>四．</h2><p> LessWrong 有时称自己是一个寻求真相的社区。 <span class="footnote-reference" role="doc-noteref" id="fnrefgdce232pvfk"><sup><a href="#fngdce232pvfk">[2]</a></sup></span>当我写这篇文章时，关于页面说“我们寻求持有真正的信仰”并且（我声称）暗示我们寻求公开承认真正的信仰。我喜欢这个社区，很大程度上是因为我发现真理是一件美丽的事情，值得歌曲和诗歌，值得奉献一生去追求它。</p><p>但群体并不统一，真诚程度也各不相同。有人在 LessWrong 上发表评论这一事实并不意味着您可以绝对信任他们。这甚至并不意味着他们会像你一样关心真相。也许他们是新来的。也许他们确实关心真相，但有一个不同的惯用案例，例如上面的“稍等一下”示例，您没有意识到。也许他们正在努力，但未能坚持这些理想，失败令人痛苦，而承认这一点更令人痛苦。</p><p> （或者也许他们根本不在乎。毕竟，有些人只是喜欢看着世界燃烧。）</p><p> （所列出的原因并不详尽。）</p><p>听到在山上的某个地方有一座闪闪发光的城市，那里有自由说出真相的地方，一个演员永远不被允许在不改变他们的方式的情况下踏上的地方，我会很高兴。无论何时，当权衡正确时，尽我所能尝试帮助该项目实现。这不是我的中心目标，但如果有的话那就太好了。</p><p>部分问题在于《永恒的九月》新人需要适应，但我认为更大的问题是团队协调。我所见过的试图将整个理性主义者群体拖入真相，去追捕<a href="https://www.lesswrong.com/posts/zp5AEENssb8ZDnoZR/the-schelling-choice-is-rabbit-not-stag">雄鹿而不是兔子，但</a>并没有奏效。你可以尝试让自己遵守这个标准，你可以耐心地向另一个演员或蛇佬腔证明贵格会的方式更好，也许你应该，但我认为口语和随意的用法将继续成为人们交谈的方式。</p><blockquote><p> “我用每个人都能理解的语言与他们交谈，”安德说，“这并不圆滑。事情已经很清楚了。”</p><p> ——奥森·斯科特·卡德《死者代言人》</p></blockquote><h2>五、</h2><p>告白时间。在这种二分法中，我自称是蛇佬腔。我同情贵格会教徒，但我不再认为自己是他们中的一员。</p><p>过去，我要花很长一段时间才能回应人们对我说的话。我指的不是等待他们说完时的一小段间隙，而是整整五到十秒的死气沉沉。看看，如果你问我“你今天做了什么？”然后我需要思考我的一天，总结重要的部分，决定这让我感觉如何，将其表达出来，然后检查以确保这些话我们从所有可能的角度都是正确的。这通常需要多次修改，在开始说话之前在心里重写句子的多个草稿。 “我的钥匙在哪里？”它们在柜台上 - 不，等等，我实际上不知道，因为我没有看着它们 - 我上次在柜台上看到它们 - “柜台”与其他柜台有足够的区别吗？ - 等等我&#39;我想这个问题太久了aaaah。 “你什么时候到？” “很有可能在晚上八点之前，但我估计在下午五点到六点之间，条件是我的车没有出去，否则——等等，抱歉，我提供了太多信息，而且格式很奇怪，啊啊啊啊啊啊啊啊。”</p><p>现在我只是说“五点三十分，如果我迟到了，我会通知你。”我实际上并没有弄清楚这是否准确，但它符合暂时的意图，我可以在对方结束句子后一秒钟内回答。同样，如果有人说他们会在商店买牛奶，我就不会再因为那天晚上冰箱里没有牛奶而感到困惑了。做他们说过的事并不是一个重大的誓言，而只是一个短暂的意图。</p><p> （我想在这里指出，在所有贵格会和演员中，本杰明从来没有将演员视为故意撒谎作为故意策略的一部分，只是真的不可靠。这让我很喜欢作者，我会继续说用法，尽管我想花点时间来确定故意和恶意的骗子确实存在，并且当你接触到这样的人时，与演员互动的有效习惯将会灾难性地失败。这种骗子不属于本文的范围虽然我并不是说我会这样做，但对此保持持续警惕是有价值的。）</p><p>我同意本杰明的观点，即处于演员模式会削弱追求真相的本能，而且我还认为，某人有时撒谎的已知事实是他们此时此刻可能撒谎的重要证据。你永远不应该完全相信蛇佬腔处于贵格会模式。</p><p> （你也不应该完全相信贵格会教徒！零和一不是概率！他们可能是错的，他们可能认为这是值得撒谎的事情，他们可能已经被同卵双胞胎取代了！时刻保持警惕。）</p><p>然而，反之亦然。我认为处于贵格会模式会消耗你的谎言、社会融合和灵活性的能力。它使您容易受到他人谎言的影响，无法预见和预测他们可能会误导或误导。如果贵格会教徒和演员真的存在并混合在一起，我怀疑贵格会教徒会发现自己一次又一次地感到沮丧和欺骗，因为他们没有预料到错误的事情会发生。</p><p>我可以在短期内在自己的脑海中注意到这一点。当我从一个长周末与完全理性主义者互动（他们每时每刻都提醒着这个社区是谁和什么）转变为周一早上在火车站与我旁边的一个陌生人聊天时，我就很难想出快速而圆滑的语言回答“你好吗？”当我参加理性主义聚会时，在最初的几次对话中，我必须纠正自己给出的快速而简单的答案，但可能不是最真实的。</p><p>也许我只是犯了典型的思维谬误。我自己的想法似乎是一个真实的事实，成为一名贵格会教徒意味着发现世界是一个令人困惑的地方，充满了不可预测的危险，充满了我无法防御的不实言论。概括起来，我认为我的选择是：</p><ol><li>成为一名贵格会教徒并生活在混乱之中。</li><li>成为一名演员并放弃大部分长期合作的能力。</li><li>成为一个蛇佬腔，并根据与谁交谈而转换语码，接受对我讲真话能力的损害以及由于错误识别而导致的错误。</li></ol><p>其中，我选择3个。</p><p>需要明确的是，在我最糟糕的情况下，我认为我只像理想化的演员一样不值得信任。这些人并没有被塑造成骗子或恶意者，只是被塑造成不认为言论行为对未来行动具有约束力的人。大多数时候，根据我自己的评价，我比周围的中间人稍微诚实、直率，并且履行了更多的口头承诺。如果你从这篇文章中得到的结论是，我会为了我自己的利益而试图对你撒谎，让你做一些违背你利益的事情，那么我认为你没有正确理解我的意思。这篇文章的全部目的是让人们更容易地模仿我何时会陈述不真实的事情。我在这里付出了额外的努力，并且我试图在诚实方面犯错误。</p><p>我试着对那些我观察到并估计说真话的人只说真话，如果你是这种二分法中的贵格会教徒，我想知道，这样我就可以回报。然而，默认情况下，你不应该认为我所说的一切都是我发誓的；我是蛇佬腔，只觉得有必要对那些我认为有必要对我说实话的人说实话，而且除了实话之外别无其他。</p><p> （你不应该完全信任任何人，时刻保持警惕。） </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndnozbd7ldgr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnozbd7ldgr">^</a></strong></sup></span><div class="footnote-content"><p>这是一个关于人们相当合理地期望为真的事情实际上往往不是真的的笑话。如今，我对报纸及其解读科学研究的能力评价不高。</p><p>解释笑话可能会毁了笑话，但在这篇文章中，尝试并严格准确似乎异常值得。另外，这是一个很好的例子：如果我没有包含这个脚注，贵格会教徒会接受这个笑话吗？蛇佬腔怎么样？如果我有理由相信人们有时不阅读脚注，这会改变它的可接受程度吗？</p></div></li><li class="footnote-item" role="doc-endnote" id="fngdce232pvfk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgdce232pvfk">^</a></strong></sup></span><div class="footnote-content"><p>这是不真实的。社区没有可以说话的嘴，也没有可以打字的手指。社区发言是一种类型错误。我不同意这种将格式塔人类群体具体化为有行动能力的比喻。不过，这是另一天的文章了，在这里我使用了这个比喻。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths<guid ispermalink="false"> R28YGeAzDHehrnc7f</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 15 Nov 2023 23:02:19 GMT</pubDate> </item><item><title><![CDATA[Life on the Grid (Part 1)]]></title><description><![CDATA[Published on November 15, 2023 10:37 PM GMT<br/><br/><p>一个人成长环境的物理布局会影响他们成年后的认知能力。<a href="https://www.nature.com/articles/s41586-022-04486-7"><u>最近一项基于 38 个国家 397,162 人的视频游戏数据的研究</u></a>发现，“在城市以外长大的人更擅长导航。”更具体地说，“在街道网络熵较低的城市（例如芝加哥）长大，在常规布局的视频游戏级别上会获得更好的结果，而在城市外或街道网络熵较高的城市（例如布拉格）长大会导致更好的结果。”结果在更高熵的视频游戏水平上。”</p><p>用简单的英语来说：如果你在类似网格的环境中长大，那么你在不太像网格的环境中导航会更差。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38dee385-80f8-423b-a922-4d59a389f209_456x390.jpeg 1456w"><figcaption>环境与视频游戏之间的关联（海洋英雄探索）寻路表现按年龄、性别和教育程度分层。 SHQ 寻路性能是根据轨迹长度计算的，并在 5 年窗口内取平均值。 </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ad34081-2f7a-431a-a9d3-79c23acb9d3a_533x532.jpeg 1456w"><figcaption>两个 SNE 较低（芝加哥）和较高（布拉格）城市的例子。右图：街道方位分布在 36 个 10 度的区间内。</figcaption></figure><p>这一发现也许并不完全令人惊讶，乍一看似乎也没有那么重要。我们绝大多数人几乎不再需要在现实生活中真正找到路了。技术已经使我们的导航技能几乎过时了。那么某些环境在保护它们方面是好是坏又有什么关系呢？</p><p>这很重要。<strong> </strong>为了理解其中的原因，我们需要绕一点弯路。</p><p>复杂性科学家大卫·克拉考尔 (David Krakauer) 在<a href="https://www.samharris.org/blog/complexity-stupidity"><u>与 Sam Harris 的“Making Sense with Sam Harris”</u></a>节目中区分了互补性认知人工制品（使用后使我们变得更加聪明的技术）和竞争性认知人工制品（如果您无法猜出这些人工制品的作用，那么也许您会知道）已经使用它们太多了）。竞争性神器的典型例子是计算器：重复使用会让你的心算能力比以前更差。与算盘相比，算盘可能产生完全相反的效果：专家用户最终可以开发出如此高保真的心智模型，他们甚至不再需要使用物理算盘，并且能够在没有算盘的情况下保持增强的算术技能。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F731df525-40e8-4805-b32f-a62886da900d_677x855.jpeg 1456w"><figcaption><i>玛格丽塔哲学</i>中的<i>算术类型</i>(1503)</figcaption></figure><p>大脑是一个复杂的系统，充满了相互关联的表征系统。这就是为什么算盘不仅能帮助你数学，还能帮助你做所有事情。 Krakuer 博士解释说，“[大脑] 周围没有防火墙，因此算盘的功能优势仅限于算术……它实际上对语言能力和几何推理产生非常有趣的间接影响。”超出预期用途的积极的全局效应是所有互补认知制品的一个特征。</p><p>另一方面，<i>竞争性</i>认知制品的功能<i>缺点</i>也不限于某个领域或技能。 Krakauer 博士接着讨论了用自动寻路技术取代地图、星盘和六分仪等原始寻路技术是如何产生这样的效果的：</p><blockquote><p>您对地图制作以及地形、拓扑和几何推理的熟悉通常对您的生活很有价值，而不仅仅是在城市中导航。因此，拿走地图不仅会让你从一扇门到另一扇门变得更糟，还会在很多方面让你变得更糟……爱因斯坦和弗兰克·劳埃德·赖特都依赖的一个很好的例子是木立方体。在他们年轻的时候，他们都非常迷恋这些立方体，并用立方体构建世界，比如《我的世界》。他们两人都声称——弗兰克·劳埃德·赖特（Frank Lloyd Wright）就建筑而言，爱因斯坦就宇宙几何而言——他们在玩这些立方体时建立的直觉对他们后来的生活发挥了重要作用。我想说地图也是如此。如果你知道如何穿越一个真实的空间，比如欧几里得空间或地球表面的弯曲空间，那么你就可以思考不同类型的空间、关系空间、想法空间。作为一种隐喻，从一个想法到另一个想法的路径概念实际上在现实空间中的路径方面具有直接且自然的实现。</p></blockquote><p>我们的认知（推理、记忆、创造力等）类似于一种心理导航，但我们的<a href="https://en.wikipedia.org/wiki/Conceptual_metaphor"><u>概念隐喻</u></a>却背叛了这一点：“一个知识领域”、一个“未经探索的主题”、“一系列思想”、 “沿着记忆之路旅行”、“唤起你的记忆”、“一次幻想”。神经科学正在快速发展（这是比喻），但<a href="https://www.quantamagazine.org/the-brain-maps-out-ideas-and-memories-like-spaces-20190114/"><u>新出现的证据</u></a>支持这一说法：“大脑以代表空间位置的方式编码抽象知识，暗示了一种更普遍的认知理论。”</p><p>这也是为什么古老的记忆增强技术——<a href="https://artofmemory.com/blog/method-of-loci/"><u>轨迹法</u></a>——记忆宫殿技术——至今仍被记忆冠军所使用。根据<a href="https://artofmemory.com/blog/method-of-loci/"><u>《记忆的艺术》</u></a> ，“轨迹的方法涉及通过在想象的旅程中的某个点为每个要记住的项目放置一个助记图像来记忆信息。然后，通过在想象的旅程中在脑海中走同样的路线，并将助记图像转换回它们所代表的事实，就可以按照特定的顺序回忆信息。”<a href="https://fs.blog/a-philosophy-of-walking/"><u>步行与创造力</u></a>之间的联系早已为<a href="https://www.themarginalian.org/2021/12/12/nietzsche-walking/"><u>知识分子和艺术家</u></a>所知，并得到<a href="https://news.stanford.edu/2014/04/24/walking-vs-sitting-042414/"><u>最近研究的</u></a>支持，这里也值得注意——就好像穿过物理景观的运动为在更抽象的思想景观中更轻松的运动奠定了基础（参见有关<a href="https://en.wikipedia.org/wiki/Embodied_cognition"><u>具体认知</u></a>的更广泛的文献，以进一步讨论该主题）。</p><blockquote><p>行走的节奏产生了一种思维的节奏，穿过风景的通道呼应或刺激着一系列思维的通道。这在内部通道和外部通道之间创造了一种奇怪的和谐，这表明心灵也是一种风景，步行是穿越它的一种方式。新的想法常常看起来像是一直存在的景观的一个特征，就好像思维是在旅行而不是在创造。 And so one aspect of the history of walking is the history of thinking made concrete — for the motions of the mind cannot be traced, but those of the feet can.</p><p> — Rebecca Solnit, <a href="http://www.amazon.com/exec/obidos/ASIN/0140286012/braipick-20"><i><u>Wanderlust: A History of Walking</u></i></a></p></blockquote><p> We also use spatial metaphors to talk about the socio-economic landscape, or life itself: “life&#39;s journey,” “broadening your horizons,” “finding myself” or “soul-searching,” “career paths,” “walks of life,” and so on. This makes sense when we consider our deep evolutionary history: many of the most important things we had to know for survival were spatial in nature—the wildebeest migrate through this valley, we can gather berries in that patch of forest, someone was killed by a saber tooth tiger over there, etc. We sometimes even conceptualize knowledge with an arboreal metaphor (“branches of knowledge”)—a vestige, perhaps, of our ancestral tree-dwelling lifestyle.</p><p> Our lack of wayfinding ability is present also in our physical, cultural, and metaphysical landscape, which has also become regular and grid-like. The consequence, which I believe we are now witnessing, is a lack of resilience and resourcefulness, an unwillingness to “blaze trails”, an inability to produce “path-breaking” innovations, and numerous other deficiencies which are leaving us altogether worse off than we were when the world was a bit more chaotic and idiosyncratic. The social and intellectual “technologies” (eg the educational system, norms around child-rearing) that we&#39;ve put in place have increasingly become competitive cognitive artifacts rather than cooperative.</p><p> The excessive “gridification” of the world can be seen most clearly in the physical domain. For a variety of reasons (economic, environmental, changing aesthetic preferences), newer cities tend to have simpler layouts. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76d0ce6c-4a49-45ae-a4d9-7a3369f61810_616x675.jpeg 1456w"><figcaption> Two of the oldest American cities in the figure, Boston and Charlotte, are the most irregular. Source for both figures: <a href="https://geoffboeing.com/2018/07/city-street-orientations-world/"><u>Geoff Boeing</u></a> </figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F299042e7-a364-4490-aafd-13da8a19bf5d_768x846.jpeg 1456w"><figcaption></figcaption></figure><p> The same is true of architecture. More ornate and culturally distinct forms have been replaced the world over by drab monoliths. The result is an oppressive standardization and homogenization, a loss of the quirks and eccentricities that give places their unique character.</p><p> The loss of irregularity and variation is one aspect of the physical gridification of the world, but there is also the sheer number of roads and tracks crisscrossing the globe. It has become nearly impossible to “get off the grid.” Virtually nothing and nowhere escapes the techno-social net which we have cast over the planet. Uncharted territory has become a thing of the past.</p><blockquote><p> And even if you wanted to drop out nowadays, could you? You are identified, tracked, networked, and classified everywhere you go, at every moment of your day. Never in history has it been harder to find a fresh start, a clean slate, or a place to march to the beat of your own drummer. There are no Walden Ponds or hermitage retreats anymore—even the off-the-grid spots are now on the grid.</p><p> — <a href="https://tedgioia.substack.com/p/multitasking-isnt-progressits-what"><u>Ted Gioia</u></a> </p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc72394c9-6203-4f86-ac01-f5ee3691da9f_575x304.jpeg 1456w"><figcaption> a giant sea-serpent attacks a ship off the coast of Norway on Olaus Magnus&#39;s Carta marina of 1539, this image from the 1572 edition.</figcaption></figure><blockquote><p> If you grew up in the 18th century, there were still new places to go. After hearing tales of foreign adventure, you could become an explorer yourself. This was probably true up through the 19th and early 20th centuries; after that point photography from National Geographic showed every Westerner what even the most exotic, under-explored places on earth look like. Today, explorers are found mostly in history books and children&#39;s tales. Parents don&#39;t expect their kids to become explorers any more than they expect them to become pirates or sultans. Perhaps there are a few dozen uncontacted tribes somewhere deep in the Amazon, and we know there remains one last earthly frontier in the depths of the oceans. But the unknown seems less accessible than ever.</p><p> — Peter Thiel, <i>Zero to One</i></p></blockquote><p> Peter Thiel has argued that the innovative spirit has degenerated in part because we no longer believe in secrets, and that we no longer believe in secrets because there is no longer any accessible frontier. This is a truly unprecedented state of affairs for humanity. For eons, our minds and cultures have evolved in delicate symbiosis with the Unknown, that place on the map labeled “Here Be Dragons.” Without this Unknown, that place where there may be <a href="https://en.wikipedia.org/wiki/El_Dorado"><u>cities of gold</u></a> or <a href="https://en.wikipedia.org/wiki/Fountain_of_Youth"><u>fountains of youth</u></a> , the heroes (but not just the heroes, all of us) have nowhere to journey, and all of the things which can make us into heroes—bravery, fortitude, ingenuity, daring, and the like—begin to atrophy. Without this Unknown, we begin to feel confined—trapped—like a beautiful and dangerous animal in a cramped cage: we develop a claustrophobia; imagination and inspiration wither. We aren&#39;t as hopeful as we used to be, but we don&#39;t know why. </p><hr><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd586719a-eee7-4d79-84e7-f66f10a763d9_640x382.jpeg 1456w"><figcaption> That kid&#39;s face says it all—the abacuses are sideways jail cell bars ( <a href="https://www.flickr.com/photos/nationaalarchief/3896157508"><u>source</u></a> )</figcaption></figure><p> Life on the Grid begins at a young age, when you are given a job at the factory of tears and tedium that is the modern education system, where you will sit at square desks in square rooms in square buildings reading from square books and writing on square pieces of paper for countless hours of your childhood (in most cases I guess they will be rectangles, but you get the point). You will level up from one grade to the next by demonstrating that you can follow arbitrary rules and regurgitate knowledge that is spoon-fed to you in clearly-defined subjects, units, and chapters. If you are even remotely good at this game, they will insist that you do it for the first 22 years of your life, but even that&#39;s not <a href="https://www.nytimes.com/2011/07/24/education/edlife/edl-24masters-t.htmlyXTmUugkI2NQyJhdyNtA/edit"><u>enough</u></a> <a href="https://theconversation.com/if-the-masters-degree-is-the-new-bachelors-is-the-doctorate-now-the-new-masters-95577"><u>anymore</u></a> .</p><p> At some point, god willing, you will enter “the real world,” that mystical realm your parents and teachers always spoke of. However, you will quickly come to find that “the real world” is another vast bureaucratic system which isn&#39;t really all that different from “the fake world” of school. Sure, you have a little more freedom and some things are slightly different (bosses instead of teachers, cubicles instead of desks), but the basics are the same: just stay on the straight and narrow, keep your head down, and keep leveling up.</p><p> As for our post-education lives (what little remains of them), many enter Corporate America or become Public Servants. Some of us become artists or entrepreneurs, but even that is no escape: these career paths have also been engulfed by the Grid. <a href="https://erikhoel.substack.com/p/how-the-mfa-swallowed-literature"><u>Erik Hoel writes</u></a> of how there is one indisputable difference between contemporary writers and the writers of even the recent past (early 2000s): <i>pretty much everyone now has an MFA</i> .</p><blockquote><p> But a majority of people under the age of 50 successful in publishing today <i>literally</i> got A+s. They all raised their hands at the right time, did everything they needed to get into Harvard or Amherst or Williams or wherever, then jumped through all the necessary hoops to make it to the Iowa Writers&#39; Workshop or Columbia University, etc.</p><p> Faulkner didn&#39;t finish high school, recent research shows Woolf took some classes in the classics and literature but was mostly homeschooled, Dostoevsky had a degree in engineering… <i>Not one of these great writers would now be accepted to</i> <i>any</i> <i>MFA in the country.</i> The result of the academic pipeline is that contemporary writers, despite a surface-level diversity of race and gender that is welcomingly different than previous ages, are incredibly similar in their beliefs and styles, moreso than writers in the past. </p></blockquote><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2297011d-2e55-480b-a0b9-3e8c65163021_598x332.jpeg 1456w"><figcaption></figcaption></figure><p> As for entrepreneurs, it&#39;s more of the same. Gone are the days of Gates and Zuckerberg when advanced degrees were optional and the digital frontier was wide open.<strong> </strong>Now, everyone reads the <a href="http://www.paulgraham.com/articles.html"><u>same essays</u></a> , follows the same guides (“ <a href="https://www.inc.com/entrepreneurs-organization/eight-steps-to-becoming-a-tech-entrepreneur-when-you-know-nothing-about-technolo.html"><u>8 Steps to Becoming a Tech Entrepreneur When You Know Nothing About Technology</u></a> ”), and applies to the same programs, incubators, and <a href="https://www.ycombinator.com/apply/"><u>combinators</u></a> .</p><p> Here was Tyler Cowen writing in 2007 (“ <a href="https://www.nytimes.com/2007/06/14/business/14scene.html"><u>The Loose Reins on US Teenagers Can Produce Trouble or Entrepreneurs</u></a> ”):</p><blockquote><p> The new ideas and business principles behind the Web have carved out the ideal territory for the young. A neophyte is more likely to see that music can come from computers rather than just from stores or radios, or that it is best to book a flight without using a travel agent. Clay Shirky, an associate teacher at New York University, notes that many young people are blessed by an absence of preconceptions about Internet businesses. Years of experience are critical to refining and improving a long-familiar product, like bread. But completely new, outside-the-box ideas — which typically come from the young — are more important for founding Napster or YouTube.</p></blockquote><p> Sixteen years later and literally all of this is false now. The young <a href="https://www.reddit.com/r/technology/comments/10o2vu7/gen_z_says_that_school_is_not_shipping_them_with/"><u>don&#39;t even know how to use computers</u></a> anymore. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a4eb9b5-51df-41c8-ae59-69d3182b04e7_597x254.png 1456w"><figcaption></figcaption></figure><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F300a3f41-9c6d-49ce-885a-770858c746eb_595x305.jpeg 1456w"><figcaption></figcaption></figure><p> To review: growing up in simplistic spatial environments and using GPS has given you brain damage and life has become a soul-crushing video game utterly devoid of mystery or adventure. We are trapped in the Grid like an insect in the spider&#39;s web; vigorous struggle will only serve to entangle us further. To extricate ourselves, we must, as individuals, gently subvert the very foundations of the Grid, which is nothing external but a facet of human nature: the impulse towards control, the systematizing instinct, the part of us that abhors anomaly and ambiguity and seeks to eradicate them. What begins as an earnest attempt to break free can so easily slip back into a self-imposed system of rules and practices with standards to be met and schedules to be followed. For that reason I am hesitant to provide any concrete suggestions—they will only serve to constrain your thinking. For now, there is perhaps only one thing that can be said: if you want to get off the grid, then get lost.</p><br/><br/><a href="https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/j4cKyhDEBpGPLNpig/life-on-the-grid-part-1<guid ispermalink="false"> j4cKyhDEBpGPLNpig</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 15 Nov 2023 22:37:31 GMT</pubDate> </item><item><title><![CDATA[Glomarization FAQ]]></title><description><![CDATA[Published on November 15, 2023 8:20 PM GMT<br/><br/><p> The main reason I&#39;m posting this here is because I want it to be publicly recorded that I wrote this post in 2023. That way, I can show people in the future that I didn&#39;t just make this up on the spot; it&#39;s been around since long before whatever it is they&#39;re asking me about this time. I don&#39;t have a blog of my own, so this is where I&#39;m putting it. Still, some of you might find it interesting, or want to use it yourself.</p><p></p><p> If I&#39;ve sent you this post, you&#39;ve probably just asked me a question, to which I responded something along the lines of &quot;I refuse to confirm or deny that.&quot; Maybe you asked if I wrote a particular story, or if I had sex with someone, or if I robbed a bank. When you heard my refusal you likely became <i>more</i> suspicious, assuming it means that I really did write/bang/rob the story/person/bank.</p><p> This is the explanation of why that&#39;s not necessarily the case.</p><p></p><p> <strong>Why are you refusing to answer my question?</strong></p><p> Suppose - and as generally holds throughout this post, I&#39;m not saying this is true or false, just asking you to consider the hypothetical - I had never committed a crime in my life, any crime, with the singular exception of... let&#39;s say, public urination. With a bag over my head, so people didn&#39;t know for sure that it was me. The police are nevertheless able to narrow the suspects down to a pretty small list, with me on it, and they ask me if I did it.</p><p> Some people would just lie to the police. The crucial element here, though, is that I <i>don&#39;t</i> want to lie about this. This entire policy is about avoiding lies, and about being able to maintain my privacy while staying completely honest. <span class="footnote-reference" role="doc-noteref" id="fnreflpyzjd5jwgk"><sup><a href="#fnlpyzjd5jwgk">[1]</a></sup></span> If I&#39;m refusing to lie, but I also don&#39;t want them to know the truth, I can&#39;t answer the question at all. So I say, &quot;I refuse to confirm or deny that.&quot;</p><p> But then suppose the police ask me if I killed someone - which I <i>didn&#39;t</i> do. And they ask if I robbed a bank, and if I sell drugs, and all sorts of other questions about crimes that I&#39;ve never done before. I always tell the truth: &quot;No, I didn&#39;t do that.&quot; And of course, now the police know that the answer to the public urination question was yes, because that&#39;s the only one where I refused to answer the question.</p><p> The only policy which doesn&#39;t tell them my secrets, other than outright lying, is to refuse to confirm or deny whether I committed <i>any</i> of the crimes - <i>even the ones that I really didn&#39;t commit</i> .</p><p></p><p> <strong>All right, but doesn&#39;t that mean that you must have committed some crime in the first place? After all, if you hadn&#39;t, there would be no harm in answering the question for every crime that exists.</strong></p><p> This policy doesn&#39;t just apply to crimes that I&#39;ve committed so far. There are a <i>lot</i> of other possible situations I could be in, and answering inconsistently gives up information.</p><p> For example, if I haven&#39;t committed any crimes yet, but will in the future, then the moment that I change from always saying &quot;No&quot; to always saying &quot;I refuse to confirm or deny that,&quot; the police know I&#39;ve just done <i>something</i> -</p><p></p><p> <strong>So you&#39;re planning to commit a crime?</strong></p><p>不！ Or rather, I refuse to confirm or deny that I&#39;m planning to commit a crime, but <i>no, that is not a valid conclusion from what I said earlier!</i> First off, even if I anticipate only a small chance of committing a crime many decades from now, it&#39;s still worth following this rule just in case - don&#39;t give me that look, I&#39;m not done talking yet. <i>Furthermore</i> , this doesn&#39;t just apply to crimes.</p><p> Suppose you&#39;re asking whether I had sex with person X. <span class="footnote-reference" role="doc-noteref" id="fnrefiscl4hvc39"><sup><a href="#fniscl4hvc39">[2]</a></sup></span> For the same reasons as before, I can&#39;t just refuse to confirm or deny whether I&#39;ve had sex with the people if the answer is &quot;Yes,&quot; because then the difference in my responses gives it away. (&quot;I haven&#39;t had sex with W, I can&#39;t tell you about X, I haven&#39;t had sex with Y, I haven&#39;t had sex with Z...)</p><p> And if I had never had sex with anyone, but had committed crimes, then by answering &quot;No&quot; about the sex questions, but &quot;I refuse to confirm or deny that&quot; about the crime questions, I would be revealing that I had committed at least one crime. Therefore, I would still have to refuse to answer the sex questions, even if I was a virgin.</p><p> There&#39;s also some things about logical decision theory and cooperating with hypothetical versions of myself, but I don&#39;t think any of that is necessary to prove the point. Yes, I expect that at some point in my past, present, or future, there is a reasonably high chance that I am asked a question that I don&#39;t want to honestly answer. And that&#39;s enough to show that I need to refuse to confirm or deny some things.</p><p></p><p> <strong>What exactly is the general policy you&#39;re following?</strong></p><p> Refuse to answer any question for which one of the possible answers to that question would, if true, be something I would want to conceal - regardless of whether said answer actually is true. Also known as &quot;Glomarization.&quot; <span class="footnote-reference" role="doc-noteref" id="fnreflwpck0izhj"><sup><a href="#fnlwpck0izhj">[3]</a></sup></span></p><p></p><p> <strong>Isn&#39;t that pretty difficult to follow all the time?</strong></p><p>是的！ In fact, as much as I would want to always Glomarize, I often have to make exceptions out of practicality. If someone asks what I did today, then in theory, I would have to refuse to answer, because one of the possible answers is &quot;I stole your car.&quot; But most of the time, I end up just answering the question.</p><p> So what would be <i>similar</i> to strict Glomarization, but still permissive enough that I don&#39;t have to refuse to answer every question? Well, I start with a cost-benefit analysis. If the police ask whether I was the Bag-Headed Peeer <span class="footnote-reference" role="doc-noteref" id="fnref29ukuc1sh14"><sup><a href="#fn29ukuc1sh14">[4]</a></sup></span> , the cost of telling the truth in the case where the truth is &quot;Yes&quot; exceeds the benefit of telling the truth in the case where the truth is &quot;No.&quot; So here, I Glomarize. But in a different scenario, this switches around: suppose I win a $1,000,000 lottery for which Canadians are not eligible, and I need to give the lottery person my address in order to get the money - but I have a mild preference for privacy about my address. In this situation, the cost in the case where I live in Canada is much <i>less</i> than the benefit in the case where I live elsewhere. So here, I tell the truth, because across all possible cases, Glomarization would <i>hurt</i> me.</p><figure class="table"><table><tbody><tr><td> Situation</td><td> Answer A</td><td> Answer B</td><td> Cost if A</td><td> Benefit if B</td><td>你做什么工作？</td></tr><tr><td> Interrogated by police</td><td> &quot;I did it.&quot;</td><td> &quot;I didn&#39;t do it.&quot;</td><td> Go to jail? <span class="footnote-reference" role="doc-noteref" id="fnref4iik2sevr7o"><sup><a href="#fn4iik2sevr7o">[5]</a></sup></span></td><td> Police like you more</td><td> Glomarize</td></tr><tr><td> Won the lottery</td><td> &quot;I live in Canada.&quot;</td><td> &quot;I live [elsewhere].&quot;</td><td> Less privacy</td><td> $1,000,000</td><td>说实话</td></tr></tbody></table></figure><p>I also need to consider the relevant probabilities in the calculation, and possibly randomize a bit to keep things secret with only a little cost. If 99% of my days are spent on innocent things and 1% on bank robberies, I don&#39;t have to Glomarize 100% of the time to hide when I&#39;ve robbed a bank, just 2% of the time, or 10% if I want to be extra safe. Of course, the downside here this policy does give people some evidence about how frequently I have something to hide, or at least helps them establish an upper bound.</p><p></p><p> <strong>I&#39;m skeptical that you actually do this as frequently as you&#39;re implying.</strong></p><p> Unfortunately, I don&#39;t really have any witnesses that I can cite at the time of writing. There&#39;s everyone in my family, who can confirm that I really do refuse to answer questions fairly often, and I think there were some cases where they later discovered I <i>wasn&#39;t</i> hiding anything (and some where they discovered I was). But that&#39;s not an option, because I&#39;m not actually willing to confirm or deny who my family is.</p><p> Luckily, for everyone but the first person who I send this post to, I&#39;ll be able to tell them about the people who I sent it to before them! So unless you happen to be that first person, there will be other people who can testify to my weirdness by the time you&#39;re reading this.</p><p></p><p> <strong>Why did you say &quot;that&#39;s classified&quot;?</strong></p><p> That&#39;s just how I say &quot;I refuse to confirm or deny that&quot; some of the time! &quot;Classified&quot; sounds cooler than &quot;refuse to confirm or deny,&quot; like I&#39;m a secret agent of some sort. Whether I am in fact a secret agent of some sort is of course classified.</p><p></p><p> <strong>Why did you list &quot;wrote a story&quot; earlier in the post?</strong></p><p> People sometimes get accused of writing stories under a pseudonym. I, like many authors before me <span class="footnote-reference" role="doc-noteref" id="fnreffpnfyjvo0n"><sup><a href="#fnfpnfyjvo0n">[6]</a></sup></span> , think it&#39;s fun to fuel speculation about which stories I secretly have or haven&#39;t written, and which other authors on the Internet I am or am not. It&#39;s also helpful in case I write something taboo I don&#39;t want associated with my real name.</p><p></p><p> <strong>Is there anything I can do if I really want to know the answer anyway?</strong></p><p> Change the incentives! For example, suppose you&#39;re my boss, and I don&#39;t want to discuss politics because I think you&#39;d fire people who disagree with you. If you want to know about my political beliefs, then prove you&#39;ve previously had no problem working with people, even when you hated what they stood for politically. If you&#39;re not able to prove that... well, that&#39;s why I&#39;m not answering your questions. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnlpyzjd5jwgk"> <span class="footnote-back-link"><sup><strong><a href="#fnreflpyzjd5jwgk">^</a></strong></sup></span><div class="footnote-content"><p> Of course, there&#39;s an additional reason not to lie to the police even if I wasn&#39;t generally honest - it&#39;s illegal.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniscl4hvc39"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiscl4hvc39">^</a></strong></sup></span><div class="footnote-content"><p> I don&#39;t actually care much about people knowing who I have or haven&#39;t had sex with, but there are other reasons I would want to hide this sort of thing: respecting the other person&#39;s privacy, or if the person is so unpopular that people will refuse to associate with me if they know I&#39;ve had sex with them, or if they&#39;re on the run from the law and I don&#39;t want to reveal that they were at my house - crap, now I&#39;m back to talking about crimes again.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlwpck0izhj"> <span class="footnote-back-link"><sup><strong><a href="#fnreflwpck0izhj">^</a></strong></sup></span><div class="footnote-content"><p> Glomarization is named after a boat that the CIA wanted to keep secret.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn29ukuc1sh14"> <span class="footnote-back-link"><sup><strong><a href="#fnref29ukuc1sh14">^</a></strong></sup></span><div class="footnote-content"><p> How many e&#39;s are supposed to be in that word?</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4iik2sevr7o"> <span class="footnote-back-link"><sup><strong><a href="#fnref4iik2sevr7o">^</a></strong></sup></span><div class="footnote-content"><p> According to the first few Google results, there&#39;s usually just a $500 fine, but if it&#39;s a repeat offence or in front of a large crowd, the punishment increases and you <i>do</i> likely end up in jail.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfpnfyjvo0n"> <span class="footnote-back-link"><sup><strong><a href="#fnreffpnfyjvo0n">^</a></strong></sup></span><div class="footnote-content"><p> Or perhaps <i>zero</i> authors before me.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq<guid ispermalink="false"> N5txrJDimv8nz4n24</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 15 Nov 2023 20:20:49 GMT</pubDate> </item><item><title><![CDATA[Testbed evals: evaluating AI safety even when it can’t be directly measured
]]></title><description><![CDATA[Published on November 15, 2023 7:00 PM GMT<br/><br/><p>我和一些合作者最近发布了论文“泛化类比（GENIES）：将人工智能监督泛化到难以测量领域的测试平台。 （<a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20">推文线程</a>）。在这篇文章中，我将解释 GENIES 基准如何与更广泛的方法相关联，用于预测人工智能系统是否安全<i>，即使无法直接评估其行为。</i></p><p>总结：<strong>当AI安全性难以衡量时，检查AI对齐技术是否可以用来解决更容易评分的类似问题。</strong>例如，要确定开发人员是否可以控制诚实如何推广到超人领域，请检查他们是否可以控制其他分布变化的泛化，例如“五年级学生可以评估的指令”到“博士可以评估的指令”。或者测试开发者是否能够发现欺骗行为，检查他们是否能够识别故意植入的“木马”行为。即使特定人工智能系统的安全性难以衡量，人工智能安全的有效性<i>&nbsp;</i>研究人员及其工具通常更容易测量——就像在风洞和压力室等航空航天试验台中测量火箭部件比发射火箭更容易测量一样。这些“试验台”评估可能会成为任何人工智能监管框架的重要支柱，但迄今为止很少受到关注。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/okmB8ymyhgc65WckN/ghuihv6jreaal9mzim2g"></p><h2><br>背景：为什么人工智能安全“难以衡量”</h2><p>很难判断人工智能系统是否遵循开发人员的指令有两个基本原因。</p><p><strong>人工智能的行为可能看起来不错，但实际上并不好。</strong>例如，很难判断超人人工智能系统是否遵守“开发您确信安全的后继人工智能”等指令。人类可以查看人工智能计划并尽力确定它们是否合理，但很难知道人工智能系统是否在玩弄人类的评估——或者更糟糕的是——它们是否有隐藏的意图并试图对我们实施快速的攻击。</p><p><br><strong>在某些环境中观察人工智能行为会带来风险。</strong>前沿法学硕士在部署后就发现了许多安全故障，当人工智能系统超过一定的能力阈值后，这显然会变得不可接受。相反，开发人员必须彻底评估模型<u>无法</u>采取<u>真正危险</u>操作的测试环境中的安全性。如果人工智能系统故意等待机会采取危险行动或出现其他“实验室”中未出现的故障模式，那么以这种方式评估人工智能系统将尤其具有挑战性。</p><h2><br>其他行业的安全也很难衡量</h2><p>在人工智能安全中，安全性的衡量似乎特别具有挑战性，因为在大多数行业中，不安全的系统并不<i>试图</i>看起来安全；相反，它是安全的。然而，我们仍然可以从其他行业的安全衡量方式中吸取教训。</p><p>例如，通过实际将火箭发射到太空来测试火箭的成本很高，类似于通过实际部署来测试人工智能系统的危险程度。航空航天工程师在称为“测试台”的更易于测量的设置中执行尽可能多的测试。例如，他们建造模拟真空空间压力和温度条件的室，建造对结构部件施加应变和振动的钻机等。通过“桌面场景”对核设施工作人员进行评估，以确定他们如何应对灾难。</p><p>通常，当难以测量时，可以使用易于测量的测试来预测安全性。</p><h2>人工智能安全的“试验台”</h2><p><strong>定义。</strong>我将使用“测试台”一词来指<strong>代类似于使人工智能系统更安全但更容易评分的问题。</strong>开发人员能够在多大程度上解决这些问题应该反映出他们实际上能够在多大程度上确保人工智能系统的安全。</p><p>人工智能安全“测试平台”的示例包括：</p><ul><li> <a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20"><strong>Generalization analogies</strong></a> <strong>.</strong>要确定开发人员是否可以将诚实概括为超人能力，请检查他们是否可以控制从“五年级学生可以回答的问题”到“博士可以回答的问题”的概括。更一般地说，测试他们是否可以控制各种分布变化的泛化。</li><li> <a href="https://www.lesswrong.com/posts/EbL5W5ccwfbqFiYBJ/auditing-games-for-high-level-interpretability-1"><strong><u>审核游戏</u></strong></a><strong>：</strong>要确定可解释性工具是否可以捕获欺骗行为，请检查它们是否可以<a href="https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5#Trojan_Horse_Models"><u>识别其他开发人员植入的木马</u></a>，或者区分经过故意训练的欺骗性模型。</li><li> <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><strong><u>元级对抗性评估</u></strong></a>：为了测试您的监督过程的稳健性，训练人工智能系统来利用一些开发人员可以验证的漏洞，因为他们拥有监督过程中保留的信息。例如，训练人工智能系统删除它不应该访问的文件而不被抓住。一些开发人员了解测试，并且可以轻松检查任务是否完成，但监督者无法访问这些保留的信息。</li></ul><h2> Tesbed 的应用</h2><p><strong>人工智能监管。</strong>人工智能治理中的一个关键的开放问题是<i>如何设计“安全评估”。</i>如果人工智能系统通过这些具体测试，其部署范围就可以扩大。测试平台可用作安全评估。例如，监管机构可以在允许实验室部署模型之前检查实验室是否可以使用其可解释性技术来检测木马或控制玩具分发转移的泛化。</p><p><strong>从错位的人工智能中提取人工智能安全研究。</strong>由于测试台问题很容易衡量，人工智能系统可以因在这些问题上取得的进展而直接获得奖励。例如，开发人员可以指示人工智能系统编写一个程序来控制各种分布变化的泛化。通过阅读人工智能系统提供的程序，开发人员也许能够了解有关控制泛化的重要见解。如果测试平台对游戏足够鲁棒，即使是公然错位的模型也可能被迫做出真正的人工智能安全研究贡献（或者训练过程将用这样做的模型取代它们）。</p><h2>试验台设计注意事项</h2><p><strong>测试平台应该对“游戏”具有鲁棒性。</strong>如果测试平台用于监管或从失调的人工智能中提取人工智能安全研究，那么测试平台必须能够抵御对抗性的游戏尝试。例如，从表面上看，TrojAI 似乎是对可解释性技术的一个很好的测试。然而，许多在此基准测试中表现出色的论文与人工智能安全无关。例如，<a href="https://arxiv.org/pdf/2110.08335.pdf"><u>本文</u></a>使用 SGD 来搜索输入中的“简单”触发模式。这很有效，因为基准测试中的大多数触发模式恰好都很简单。设计强大的测试台可能需要仔细思考和大量迭代（理想情况下应该尽早开始！）</p><p><strong>测试平台在与其核心类比无关的所有方面都应该是多样化的。</strong>跟踪测试台的哪些方面实际上与您真正关心的问题类似非常重要。例如，将诚实从“五年级学生可以回答的问题”推广到“博士可以回答的问题”似乎类似于将诚实推广到超人困难的问题。但<i>为什么</i>这些问题都相似呢？ “从易到难”的分布转变有什么特别之处吗？如果不是，还应该衡量不同领域、角色等之间的泛化。从不同的类比集合中提取可以对人工智能安全工具进行更稳健的评估，就像采用异质样本如何更好地估计癌症治疗是否有效一样。</p><h2>结论</h2><p>我的印象是，许多监管机构和研究人员都在考虑评估人工智能系统的安全性，就像我们面前有一个囚犯一样，我们需要对他们进行一系列心理测试，以确定是否应该将他们释放到社会中。</p><p>这对安全评估的描述过于严格。人们需要进行一个重要的概念转变，从“这个特定的人工智能系统有多安全”到“我们的工具有多有效？”第二个问题清楚地告诉了前者，但直观上似乎更容易回答。</p><p>我目前正在考虑如何为人工智能安全建立更好的“测试平台”。如果您有兴趣合作，请通过<a href="mailto:joshuamclymer@gmail.com"><u>joshuamclymer@gmail.com</u></a>与我联系</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be<guid ispermalink="false"> okmB8ymyhgc65WckN</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Wed, 15 Nov 2023 19:00:42 GMT</pubDate> </item><item><title><![CDATA[Listening to Pascal Muggers]]></title><description><![CDATA[Published on November 15, 2023 6:06 PM GMT<br/><br/><p><br><strong><u>概括</u></strong><i><strong><u> </u></strong><u>-</u>我们已经被“帕斯卡抢劫者”抢劫了<strong>——</strong>这种内心想法认为后果如此巨大，以至于超越了所有正常的考虑。事情很模糊，但考虑到利害关系，不给予任何关注是不负责任的。同时，高效意味着不要给予他们超出任何可能有用的注意力。</i><br></p><h2>介绍</h2><p><u>在</u><a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities"><u>《Pascal&#39;s Mugging: Tiny Probabilities of Vast Utilities</u></a> 》（2007 年）中，Eliezer Yudkowsky 描述了一个思想实验： <span class="footnote-reference" role="doc-noteref" id="fnrefrcut65obs6"><sup><a href="#fnrcut65obs6">[1]</a></sup></span></p><blockquote><p>现在假设有人来找我说：“给我五美元，否则我将使用矩阵之外的魔力来运行一台图灵机来模拟并杀死 3^^^^3 个人。”</p></blockquote><p>以利以谢还写道：</p><blockquote><p>你或我可能会一笑置之，根据占主导地位的主线概率进行计划</p></blockquote><p>不是我。</p><p>这让我彻夜难眠。</p><p>好像我不能理解3^^^^3，感觉好浩瀚。这个数字感觉就像任何其他“奇怪的大数字”。问题是我知道我不能真正认为抢劫犯说的是假话的“可能性很小”。同时我知道抢劫犯可以任意增加公用事业的规模。如果我根据最大化预期价值做出决定，我应该支付 5 美元（或为此支付我的全部钱）。但给抢劫犯 5 美元的感觉很糟糕，放弃理性也很糟糕。</p><p>这两种选择似乎都需要做出无限愚蠢的承诺🤢</p><h2>让帕斯卡劫匪就位</h2><p>我们的情绪不能超越几个层次。在普通的大思维中，我们的情感影响量表可能类似于： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kgkfqgbs21fnmpitcz7j" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qvm5vnoef39ozcf14pcl 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/g30fzpnlq2cxm8oknslw 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/voqp0ccqlwjtpwdjxl36 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/zl2qdauebouybrgzgxxl 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nrgu2v7zwinc03kxsnlf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/erg9yk5oxhvgewp8r9o8 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jnft7iwkzrfasl7l0hl7 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hk9hsmxfq58caxstgtvq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nl30cktfb9vmlulorr9a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ule7bazztjrljt88n0sd 960w"></figure><p>但是，如果我们再推送一个 util，然后再推送另一个，依此类推，会怎么样呢？有没有什么你看重的东西可以无限扩展，或者如果不是的话，它会止步于何处？您有多确定您不想避免再多一种效用（例如，防止再创造一个受折磨的人）或获得另一种效用（例如，与此相反）？</p><p>此时，我们已经达到了“奇怪的考虑因素”，在计算期望值时，所有普通结果都是无关紧要的： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/odq2xlwzcbxq7dpyeehk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/y2qnm8jmx3keisdnr1pc 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dphtjar4umincs6ugo2m 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/sw0vfcscdbbjkn2tc7cl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kbh79apb1skinptm38uj 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/lcldo3ri9rdkafb9qcri 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/p7cwzt79qighnsambtr1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pb48i8alj7xhxn40gvni 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/vf2dyz9nuun2vccuy5jv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fcxxbddnzuh7cgev0awg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/eyrjnebkjsq6wzrtxibj 960w"></figure><p>现在我们可以轻松地拒绝 3^^^^3 劫匪，因为它只是奇异的坏云中的又一项。我们可以回答：“<i>别拿这些小事来烦我，我只想拯救3^^^^^3条生命！</i> ”（还有更大、概率更高的选项，比如多想一想。 ）</p><p>当我们窥探云内部时会发生什么？更极端的实用程序可能会胜过其他实用程序： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/t9hokztkttpiyojotwul" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/o1mgmi8lkdxndozl5tpw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jetyctegrfjha3hxv2qg 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ghbgm7sxacrrm73p91jp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/icgx2zyqckvloua9b2gv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nstus4bl43sp6j6bc8c1 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/bu7rxw3gi3kihprjiube 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hob6onvjfeg1biegkxx9 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/rjgj6yjlx76es2pcsxtc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/um8yovu724qb63obq5ea 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fvxwv976upj8znvlpqpw 960w"></figure><p>康托尔的“绝对无穷大”（Ω）有什么意义吗？ Ω 公用事业/非公用事业的可能性是否应该压倒一切？难道这已经失去了人性的温暖吗？我们是否已经离开了准确世界模型之上的真实偏好之地，进入了元伦理学的荒原？</p><p>也许没有任何答案。</p><p>所以你会怎么做？</p><p>你玩赔率😅 <span class="footnote-reference" role="doc-noteref" id="fnrefndu3c0vdwsd"><sup><a href="#fnndu3c0vdwsd">[2]</a></sup></span></p><h2>对战略的影响</h2><p>在<a href="https://www.lesswrong.com/posts/78G9pjYM2KohErCvJ/model-uncertainty-pascalian-reasoning-and-utilitarianism"><u>模型不确定性、帕斯卡推理和功利主义</u></a>(2011) 中， <a href="https://www.lesswrong.com/users/multifoliaterose?mention=user">@multifoliaterose</a>分析了帕斯卡的论点，即所有慈善努力都应旨在影响无限实验室宇宙的创造：</p><blockquote><p>反驳#3：即使一个人的焦点应该放在实验室宇宙上，这样的焦点会减少到对创建友好人工智能的关注，这样的实体会比我们更好地推理实验室宇宙是否是一件好事以及如何做。去影响他们的创作。</p><p>回应：这里也是如此，如果这是真的，那就不明显了。即使一个人成功地创造了一种同情人类价值观的通用人工智能，这样的通用人工智能也可能不会归因于功利主义，毕竟许多人不是，而且目前还不清楚这是因为他们的意志没有被连贯地推断出来</p></blockquote><p>所以这不是一个答案。提案<i>需要</i>一个单独的帖子——模型/哲学不确定性、目标和策略的组合——但这就是对付帕斯卡抢劫犯的温和、常识性的方法。我将其视为抢劫，并不是因为有一天人类创造出某种东西来创造全新宇宙的可能性如此之小，而是因为实用程序的巨大性意味着胜过其他一切。</p><p>在<a href="https://www.lesswrong.com/posts/ebiCeBHr7At8Yyq9R/being-half-rational-about-pascal-s-wager-is-even-worse"><i><u>《对帕斯卡赌注的半理性更糟糕</u></i></a><i>》（2013）</i>中，埃利泽写道：</p><blockquote><p><i>从我记事起，我就纯粹出于实际原因拒绝了所有形式的帕斯卡赌注：任何试图通过追逐万分之一的巨额回报的机会来规划自己的生活的人几乎肯定会在实践中注定失败……现在，在极少数情况下，我发现自己在思考这种元级别的垃圾，而不是手头的数学，我提醒自己，这是一个浪费的动作——“浪费的动作”是指回想起来，如果问题出在哪里，任何想法都会出现。事实上已经解决了，并没有为问题的解决做出贡献。</i></p></blockquote><p>在实践中，接受抢劫与忽视抢劫之间可能没有什么区别，但考虑到巨大的实用性并偶尔重新审视对我来说似乎是正确的。</p><h2>不偏离轨道</h2><p>帕斯卡抢劫式思维可能会导致基于可疑的哲学论证而做出超出常规行为范围的事情。特德·卡辛斯基（Ted Kaczynski）（大学炸弹客）并没有特别奇怪的观点（即日益增长的工业主义正在破坏我们与自然的和谐）。他的不同寻常之处在于他按照这些原则行事。就他而言，他犯了一个错误，在实施他的炸弹人“战略”之前没有与其他任何人交谈。并不是所有愚蠢的事情都可以通过与至少一些外群体成员讨论的启发式方法来预防，但它应该被视为避免偏离轨道的一个良好起点。</p><h2>结论</h2><p>考虑到利害关系，不将帕斯卡抢劫的考虑因素作为制定计划的出发点的一部分并不时重新审视是不负责任的，但不能以牺牲总体效率为代价，因为有效为大型公用事业提供了许多可能的路线，而一般模型的不确定性让人质疑这是否是看待这个问题的正确方法。</p><p> （最后一点，帕斯卡抢劫案是对这个问题的一个非常消极的框架，但探索巨大公用事业的整体不对称性和影响是另一篇文章的主题。） <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrcut65obs6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcut65obs6">^</a></strong></sup></span><div class="footnote-content"><p> Eliezer 的原始帖子专门与所罗门诺夫归纳法中的概率相关，其中图灵机的效用增长速度比其先验概率收缩的速度要快得多，但包括我在内的许多人将其视为个人问题，并且是评论最多的问题之一LessWrong 上的帖子。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnndu3c0vdwsd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefndu3c0vdwsd">^</a></strong></sup></span><div class="footnote-content"><p> 😅 是“汗水微笑”表情符号。我更喜欢看起来更坚定/更享受的东西，但找不到一个表情符号来捕捉这一点。我希望表情符号有助于让事情脚踏实地，在面对超现实的考虑时不要<i>过于</i>严肃。 </p><p><img style="width:39.12%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/z1khght3w6bgpagmf4w9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kqjwdbdekvlpautobws8 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/b9x31hbwgevhiemtbqup 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qz0nlxr42xiiofs3i6nv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dhebklhymltoqajhq1v1 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/yimehxwqrgjottfvqnya 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pjiisl24jn171zz57rtn 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fj1lriucgmnij2lko14e 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ma1bc2isdnqlvsnrgcri 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/iqkdvlyygn1uwyg0cyp4 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dvwkphnsxreyuzp1pcvm 1200w"></p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers<guid ispermalink="false"> DdSnA73EQcWXzhs5j</guid><dc:creator><![CDATA[cSkeleton]]></dc:creator><pubDate> Wed, 15 Nov 2023 18:06:37 GMT</pubDate> </item><item><title><![CDATA[New report: "Scheming AIs: Will AIs fake alignment during training in order to get power?"]]></title><description><![CDATA[Published on November 15, 2023 5:16 PM GMT<br/><br/><p> （从<a href="https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">我的网站</a>交叉发布）</p><p>我写了一份报告，内容是关于高级人工智能是否会在训练期间假装对齐，以便稍后获得权力——我将这种行为称为“诡计”（有时也称为“欺骗性对齐”）。该报告可在 arXiv<a href="https://arxiv.org/abs/2311.08379">上</a>查看： <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里</a>还有一个音频版本，我在下面包含了介绍部分。本节包括报告的完整摘要，其中涵盖了大部分要点和技术术语。我希望摘要能够提供理解报告各个部分所需的大部分背景信息。</p><h1>抽象的</h1><blockquote><p>这份报告研究了在训练中表现良好的高级人工智能是否会这样做，以便在以后获得权力——我将这种行为称为“阴谋”（有时也称为“欺骗性联盟”）。我的结论是，使用基线机器学习方法来训练足够复杂的目标导向人工智能来进行计划，阴谋是一个令人不安的合理结果（在给定这些条件的情况下，我对这种结果的主观概率约为 25%）。特别是：如果在训练中表现良好是获得力量的一个好策略（我认为很可能是这样），那么各种各样的目标就会激发计划——从而带来良好的训练表现。这使得训练可能自然地实现这样的目标然后强化它，或者积极推动模型的动机实现这样的目标，作为提高性能的简单方法。更重要的是，由于阴谋者假装在旨在揭示其动机的测试上保持一致，因此可能很难判断这种情况是否已经发生。不过，我也认为有安慰的理由。特别是：阴谋实际上可能不是获得权力的好策略；训练中的各种选择压力可能不利于阴谋者的目标（例如，相对于非阴谋者，阴谋者需要进行额外的工具推理，这可能会损害他们的训练表现）；我们也许可以有意地增加这样的压力。该报告详细讨论了这些问题以及各种其他考虑因素，并提出了一系列实证研究方向以进一步探讨该主题。</p></blockquote><h1> 0. 简介</h1><p>寻求权力的特工往往有动机欺骗他人其动机。例如，考虑一下竞选活动中的政治家（“我<em>非常</em>关心你的宠物问题”），求职者（“我只是对小部件感到非常兴奋”），或者寻求父母赦免的孩子（“我”我非常抱歉，再也不会这样做了”）。</p><p>这份报告探讨了我们是否应该期望在训练期间动机看似良性的先进人工智能会参与这种形式的欺骗。在这里，我区分了四种（越来越具体）类型的欺骗性人工智能：</p><ul><li><p><strong>结盟伪造者</strong>：人工智能假装比实际更加结盟。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-1" id="fnref-jCfZhXha29uHnHWwY-1">[1]</a></sup></p></li><li><p><strong>训练游戏玩家</strong>：人工智能了解训练他们的过程（我将这种理解称为“情境意识”），并且正在针对我所说的“情节奖励”进行优化（并且通常会激励假装对齐） ，如果这样做会带来奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-2" id="fnref-jCfZhXha29uHnHWwY-2">[2]</a></sup></p></li><li><p><strong>权力驱动的工具训练游戏玩家（或“阴谋家”）</strong> ：专门进行训练游戏的人工智能，目的是为了以后为自己或其他人工智能获得权力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-3" id="fnref-jCfZhXha29uHnHWwY-3">[3]</a></sup></p></li><li><p><strong>目标守护阴谋者：</strong>其权力寻求策略特别涉及试图阻止训练过程改变其目标的阴谋者。</p></li></ul><p>我认为，根据粗心的人类反馈进行微调的先进人工智能很可能默认以各种方式伪造对齐，因为粗心的反馈会奖励这种行为。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-4" id="fnref-jCfZhXha29uHnHWwY-4">[4]</a></sup>很可能，这样的人工智能也会玩训练游戏。但我在这份报告中的兴趣特别在于他们是否会将这样做作为以后获得权力的策略的一部分——也就是说，他们是否会成为阴谋家（这种行为在文献中通常被称为“欺骗性联盟”，虽然我不会在这里使用这个术语）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-5" id="fnref-jCfZhXha29uHnHWwY-5">[5]</a></sup>我的目的是澄清和评估支持和反对这一期望的论点。</p><p><strong>我目前的观点是，阴谋是使用基线机器学习方法（例如：自我监督预训练，然后针对各种现实世界任务进行 RLHF）训练高级、目标导向的人工智能的令人担忧的合理结果</strong>。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-6" id="fnref-jCfZhXha29uHnHWwY-6">[6]</a></sup>在我看来，引起关注的最基本原因是：</p><ol><li><p>在训练中表现良好可能是获得总体力量的一个很好的工具性策略。</p></li><li><p>如果是这样，那么各种各样的目标就会激发计划（从而产生良好的训练表现）；而与良好训练表现兼容的非计划目标则更加具体。</p></li></ol><p>在我看来，（1）和（2）的结合似乎是合理的，即以训练创建目标导向的、情境感知的模型为条件，它很可能出于某种原因灌输类似阴谋家的目标。尤其：</p><ul><li><p>训练可能会“自然地”达到这样的目标（无论是在态势感知出现之前还是之后），因为这样的目标最初会导致训练中表现足够好，即使没有训练游戏。 （尤其是如果你有意尝试诱导你的模型在长期范围内进行优化，我认为会有动力这样做。）</p></li><li><p>即使类似策划者的目标不会“自然地”出现，一旦模型具有参与训练博弈的态势感知能力，主动将模型<em>转变</em>为策划者可能是 SGD 提高模型训练性能的最简单方法。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-7" id="fnref-jCfZhXha29uHnHWwY-7">[7]</a></sup></p></li></ul><p>更重要的是，由于阴谋者积极假装在旨在揭示其动机的测试中保持一致，因此可能很难判断这种情况是否已经发生。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-8" id="fnref-jCfZhXha29uHnHWwY-8">[8]</a></sup>这似乎是值得严重关注的原因。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-9" id="fnref-jCfZhXha29uHnHWwY-9">[9]</a></sup></p><p>不过，我也认为有安慰的理由。我将它们分为两类。</p><p>第一类的中心思想是，阴谋实际上并不是一种非常收敛的工具性策略。</p><ul><li><p>例如：关于策划者最常见的故事集中在<em>目标守护</em>策划者身上，他们通过训练游戏来防止他们的目标被训练过程所改变。但我不清楚这是否会足够好（特别是：尽管训练游戏<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-10" id="fnref-jCfZhXha29uHnHWwY-10">[10]</a></sup> ，但目标修改可能会继续），或者由此产生的目标将获得足够大的未来力量，以足够高的概率，以证明模型通过训练游戏而产生的任何成本是合理的，而不是直接实现其目标（如果这些成本有意义 - 对于某些模型来说可能没有意义）。</p></li><li><p>关于为什么阴谋将是一种很好的工具性策略，还有其他一些故事——例如，以具有不同价值观的人工智能之间的协调为中心的故事；人工智能默认具有非常相似的值的可能性；以及人工智能本质上重视诸如“我的生存”（以容忍价值观变化的方式）和/或“人工智能赋权”（以宽容人工智能如此赋权的方式中非常不同的价值观的方式）之类的可能性。虽然这些故事确实使阴谋看起来像是一个更具分离性的假设，但对我来说，其中一些故事对我来说似乎是（a）更具推测性和（b）要求我们假设具有更具体目标类型的模型的某种组合（从而削弱上述（2）的力量）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-11" id="fnref-jCfZhXha29uHnHWwY-11">[11]</a></sup></p></li></ul><p>第二类集中于可能<em>反对</em>阴谋的选择压力形式——既是“自然的”，也是因为我们可以在训练中有意地引入/强调这些选择压力。</p><ul><li><p>例如：相对于其他类型的模型，策划者需要进行额外的工具推理（例如，从长期目标到训练游戏的工具价值的回溯，并检查逃跑/叛逃的机会）。这可能会对其性能造成巨大的损失——我们可以故意增加这一成本，例如将模型置于更多的时间压力下，迫使它们依靠人类可读的便签本进行推理，并进行真正的逃跑/背叛机会与陷阱更难区分。如果阴谋者在人类控制下积极尝试破坏人类联盟和安全工作（在我看来，这是最可怕的阴谋形式），他们的表现似乎可能会受到更显着的打击。</p></li><li><p>更重要的是，我自己的猜测是，这种额外的工具推理的成本大大超过了有时被称为“简单”的阴谋的好处——即，选择一个阴谋者允许 SGD 选择一个更简单的目标这一事实带来的好处（因为它有种类繁多，可供选择）。尤其是考虑到阴谋者和非阴谋者都需要详细地表示“诚实”、“人类价值观”、“奖励”等概念，因此阴谋者似乎只能通过使用复杂性的差异来节省复杂性。 /重新利用/“指向”一个给定的概念来指导模型的动机。</p></li><li><p>另外：在态势感知之前，每当类似阴谋者的目标导致牺牲奖励时，训练都会积极地<em>对其进行惩罚</em>（更多关于我在下面所说的“情节”的意思）。我们可以尝试进一步积极推动这个方向——特别是如果我们能够学会在态势感知出现时进行检测和控制。</p></li></ul><p>我认为第二类在这里尤其重要。特别是：诉诸在训练中表现良好的类计划者目标与非计划者目标的比较数量（和/或“度量”）的论点可能会出错的一种方式是忽略选择压力的全部力量应用于培训选择的目标。因此，如果培训中施加的压力总体上<em>对</em>阴谋者起作用，那么这是一个非常好的消息。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-12" id="fnref-jCfZhXha29uHnHWwY-12">[12]</a></sup></p><p>完整报告更详细地讨论了这些问题以及各种其他相关考虑因素（完整摘要请参阅第 0.2 节）。</p><p><strong>总的来说，作为量化我当前观点的粗略尝试：如果你使用自监督预训练 + RLHF 的基线形式在不同的、长期的数据集上训练一个连贯的目标导向、情境感知、比人类水平更好的模型， -地平线，现实世界的任务，我的主观概率是~25%，这将在很大程度上作为为自己和/或其他人工智能寻求权力的工具策略的一部分在训练中表现良好。</strong> <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-13" id="fnref-jCfZhXha29uHnHWwY-13">[13]</a></sup></p><ul><li><p>我还认为可以通过相当简单的方式降低风险：例如，通过短期任务的培训，或者在态势感知之前进行更密集形式的对抗性训练。</p></li><li><p>我认为风险会随着更强大的模型<em>而增加</em>，这些模型可能更有能力逃脱/接管（并与其他人工智能协调/合作这样做）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-14" id="fnref-jCfZhXha29uHnHWwY-14">[14]</a></sup></p></li></ul><p>另外，重要的是：非阴谋者也可能做出类似阴谋者的行为。例如：</p><ul><li><p>仅针对给定剧集的奖励进行优化的模型可以伪造其对齐（甚至：参与逃脱/接管），以努力获得该奖励（特别是如果所讨论的剧集相当长）。</p></li><li><p><em>不</em>参加训练游戏的模型最终仍然可能会追求权力目标，从而激励各种形式的欺骗。</p></li><li><p>最重要的最终人工智能可能在重要方面与我在这里关注的人工智能范式有所不同——例如，它们可能更像“<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">语言模型代理</a>”而不是单一模型， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-15" id="fnref-jCfZhXha29uHnHWwY-15">[15]</a></sup>或者它们可能是通过以下方法创建的：与我所关注的基线机器学习方法有更大的不同——同时仍然从事权力驱动的对齐伪造。</p></li></ul><p>正如我所定义的，阴谋远非这附近唯一令人担忧的问题。相反，这是这种担忧的一个范例，在我看来，这是一个特别迫切需要理解的例子。在报告的最后，我讨论了一系列可能的实证研究方向，以进一步探讨该主题。</p><h2> 0.1 预备知识</h2><p><em>（本节提供了一些初步内容来框架报告的讨论。那些渴望了解主要内容的人可以跳到第 0.2 节中的报告摘要。）</em></p><p>我集中撰写这份报告是因为我认为，在评估错位人工智能的整体存在风险水平时，阴谋/“欺骗性联盟”的概率是最重要的问题之一。事实上，阴谋对于这种风险如何产生的许多模型来说尤其重要。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-16" id="fnref-jCfZhXha29uHnHWwY-16">[16]</a></sup>正如我在下面讨论的，我认为这是失调可能采取的最可怕的形式。</p><p>然而：尽管这个话题对人工智能风险很重要，但公众的直接关注相对较少。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-17" id="fnref-jCfZhXha29uHnHWwY-17">[17]</a></sup>我的感觉是，对它的讨论常常对所讨论的动机/行为的具体模式以及为什么人们可能会或可能不会期望它发生感到模糊。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-18" id="fnref-jCfZhXha29uHnHWwY-18">[18]</a></sup>我希望在这份报告中能够澄清此类讨论，根据其重要性来深入和详细地处理该主题，并促进更多正在进行的研究。特别是，尽管该报告具有理论性质，但我特别感兴趣的是为可能提供进一步启示<em>的实证</em>研究提供信息。</p><p>我试图为那些不一定熟悉任何先前有关阴谋/“欺骗性联盟”的作品的读者写作。例如：在第 1.1 和 1.2 节中，我从头开始列出了讨论所依赖的概念分类。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-19" id="fnref-jCfZhXha29uHnHWwY-19">[19]</a></sup>对于一些读者来说，这可能感觉像是在重新讨论旧的观点。我邀请这些读者在他们认为合适的时候跳过（特别是如果他们已经阅读了报告的摘要，并且知道他们错过了什么）。</p><p>也就是说，我确实假设人们更普遍地熟悉（a）关于人工智能失调带来的存在风险的基本论点， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-20" id="fnref-jCfZhXha29uHnHWwY-20">[20]</a></sup>和（b）当代机器学习如何运作的基本情况。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-21" id="fnref-jCfZhXha29uHnHWwY-21">[21]</a></sup>我还做了一些其他假设，即：</p><ul><li><p>相关类型的人工智能开发正在以机器学习为中心的范式（以及社会政治环境）中进行，与 2023 年大致相似<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-22" id="fnref-jCfZhXha29uHnHWwY-22">。 [22]</a></sup></p></li><li><p>我们没有强大的“可解释性工具”（即帮助我们理解模型内部认知的工具）来帮助我们检测/防止阴谋。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-23" id="fnref-jCfZhXha29uHnHWwY-23">[23]</a></sup></p></li><li><p>我讨论的人工智能是目标导向的，即：可以理解为在世界模型的基础上制定和执行计划，追求目标。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-24" id="fnref-jCfZhXha29uHnHWwY-24">[24]</a></sup> （我不认为这个假设是无害的，但我想将关于是否期望目标导向本身的争论与关于是否期望目标导向模型成为阴谋者的争论分开——我鼓励读者这样做以及。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-25" id="fnref-jCfZhXha29uHnHWwY-25">[25]</a></sup> ）</p></li></ul><p>最后，我想指出报告中讨论的一个让我感到非常不舒服的方面：即，在我看来，除了对人类构成潜在的生存风险之外，报告中讨论的人工智能很可能是道德的。患者凭自己的权利。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-26" id="fnref-jCfZhXha29uHnHWwY-26">[26]</a></sup>我在这里说话，就好像它们不是，并且好像对人工智能进行任何最能服务于我们目的的治疗都是可以接受的。但如果人工智能是道德患者，情况就不是这样了——当一个人发现自己在说（尤其是：反复说）“让我们暂时假设，对<em>某类</em>存在做任何我们想做的事情是可以接受的，尽管事实上，这似乎不是，”人们应该坐直并想知道。我在这里把人工智能道德耐心问题放在一边，不是因为它们不真实或不重要，而是因为它们会给已经很长的讨论带来许多额外的复杂性。但这些复杂性正在迅速降临到我们身上，我们需要具体的计划来负责任地处理它们。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-27" id="fnref-jCfZhXha29uHnHWwY-27">[27]</a></sup></p><h2> 0.2 报告摘要</h2><p>本节提供了完整报告的摘要。它包含了大部分要点和技术术语（不幸的是，为了使内容更容易理解而提供的具体示例相对较少）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-28" id="fnref-jCfZhXha29uHnHWwY-28">[28]</a></sup>我希望它能够（a）让读者很好地了解他们对正文的哪些部分最感兴趣，并且（b）使读者能够跳到这些部分，而不必过多担心什么他们错过了。</p><h3> 0.2.1 第 1 节总结</h3><p>报告主要有四个部分。第一部分（第 1 节）旨在澄清上述人工智能欺骗的不同形式（第 1.1 节），将阴谋者与我将讨论的其他可能的模型类（第 1.2 节）区分开来，并解释为什么我认为阴谋是一种独特而可怕的错位形式（第 1.3 节）。我对将阴谋者与以下人员进行对比特别感兴趣：</p><ul><li><p><strong>剧集奖励寻求者</strong>：也就是说，人工智能系统最终重视剧集奖励过程的某些组成部分，并因此而玩训练游戏。</p></li><li><p><strong>训练圣人</strong>：直接追求奖励过程指定目标的人工智能系统（我将其称为“指定目标”）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-29" id="fnref-jCfZhXha29uHnHWwY-29">[29]</a></sup></p></li><li><p><strong>错误概括的非训练游戏玩家</strong>：既不玩训练游戏<em>也不</em>追求指定目标的 AI。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-30" id="fnref-jCfZhXha29uHnHWwY-30">[30]</a></sup></p></li></ul><p>这是总体分类图： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/s1uqfkjys8bmk2qaidhp" alt=""></p><p>所有这些模型类都可能错位且危险。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-31" id="fnref-jCfZhXha29uHnHWwY-31">[31]</a></sup>但我认为阴谋家特别可怕。特别是：阴谋促使最强有力和​​对抗性的努力来阻止人类了解所讨论的失调<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-32" id="fnref-jCfZhXha29uHnHWwY-32">[32]</a></sup> ；它最有可能引发我所说的“早期破坏”——也就是说，人工智能在人工智能发展的相对早期阶段，积极尝试破坏人类协调、控制和保护未来人工智能系统的努力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-33" id="fnref-jCfZhXha29uHnHWwY-33">[33]</a></sup></p><p>我还讨论了（在第 1.4 节中）该报告所关注的类型的理论论据是否值得考虑。我在这里的回答是“有点有用”，既可以为实证调查提供信息，也可以在缺乏足够实证数据的情况下作为证据来源，但我们显然应该尽可能多地寻找实证证据（我在这方面提供了一些建议）第 6 节）。</p><p>在第 1.5 节中，我讨论了一个在我看来对评估我所考虑的各种论点很重要的因素：即训练给人工智能带来的“松弛”程度，使其表现不佳。</p><h3> 0.2.2 第 2 节小结</h3><p>报告的第二部分探讨了策划的先决条件。我特别关注：</p><ul><li><p><strong>态势感知：</strong>模型了解自己是训练过程中的模型，训练过程将奖励什么，以及客观世界的基本性质。</p></li><li><p><strong>超越情节目标：</strong>模型关心情节完成后其行为的后果。</p></li><li><p><strong>旨在将情节奖励作为权力驱动工具策略的一部分：</strong>该模型认为，如果针对情节奖励进行优化，那么其超出情节的目标将会更好地实现，特别是它，或者其他一些人工智能，如果这样做的话，将会获得更多的力量。</p></li></ul><p> 2.1 节讨论态势感知。我认为，如果没有积极的努力，我们至少应该期望某些类型的高级人工智能——例如，在与公共互联网实时交互中执行现实世界任务的高级人工智能——默认情况下具有态势感知能力，因为（a）意识在执行相关任务时非常有用（事实上，我们可能会积极地对其进行训练），并且（b）此类人工智能可能会接触到获得这种意识所需的信息。不过，我在报告中并没有过多关注态势感知。相反，我更感兴趣的是在情境感知模型中是否需要满足上述其他两个先决条件。</p><p> 2.2 节讨论了超越情节的目标。在这里，我（在第 2.2.1 节）区分了“情节”的两个概念，即：</p><ul><li><p><strong>激励情节</strong>：即训练中的梯度主动迫使模型进行优化的时间范围。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-34" id="fnref-jCfZhXha29uHnHWwY-34">[34]</a></sup></p></li><li><p><strong>直观情节</strong>：也就是说，我们出于某种原因将其称为“情节”的其他一些直观时间单元（例如，在其结束时给予奖励；一个此类单元中的动作与另一个单元中的结果没有明显的因果路径） ; ETC）。</p></li></ul><p>当我在报告中使用“情节”一词时，我指的是激励情节。因此，“超越情节目标”意味着：目标的时间范围超出了主动训练迫使模型进行优化的范围。但非常重要的是，激励事件不一定是直觉事件。也就是说，决定将某个时间单元称为“片段”并不意味着训练不会主动向模型施加压力，使其在超出该单元的范围内进行优化：您需要实际详细了解梯度如何流动（我担心本报告的普通读者可能会忽视这一工作）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-35" id="fnref-jCfZhXha29uHnHWwY-35">[35]</a></sup></p><p>我还（在第 2.2.2 节）区分了两种类型的片外目标，即：</p><ul><li><p><strong>与训练游戏<em>无关的</em>超集目标</strong>：即，超集目标的产生与其在激励模型进行训练游戏中的作用<em>无关</em>。</p></li><li><p><strong>训练游戏<em>相关的</em>赛外目标</strong>：也就是说，赛外目标是<em>因为它们激发训练游戏而专门</em>出现的。</p></li></ul><p>这两种剧集外目标对应于关于阴谋如何发生的两个不同故事。</p><ul><li><p>在第一种故事中，SGD 碰巧“自然地”在模型中灌输超集目标（无论是在态势感知出现之前还是之后），<em>然后</em>这些目标开始激发阴谋。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-36" id="fnref-jCfZhXha29uHnHWwY-36">[36]</a></sup></p></li><li><p>在第二种故事中，SGD“注意到”给模型设定超集目标<em>会</em>激发阴谋（从而激发高奖励行为），因此因此积极地<em>给</em>它这样的目标。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-37" id="fnref-jCfZhXha29uHnHWwY-37">[37]</a></sup></p></li></ul><p>如果您假设态势感知已经到位，那么第二个故事就最有意义。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-38" id="fnref-jCfZhXha29uHnHWwY-38">[38]</a></sup>因此，我们剩下以下三个主要的策划路径： <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-39" id="fnref-jCfZhXha29uHnHWwY-39">[39]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/v3064isqjtarmfhwj8qh" alt=""></p><p>在第 2.2.2.1 节中，我讨论了与训练游戏无关的赛外目标（即路径 1 和 2）。我们是否应该期望超集目标会“自然”出现？</p><ul><li><p>预计会出现这种情况的一个原因是，目标默认情况下并没有时间限制，而且“模型时间”可能与“日历时间”不同。</p></li><li><p><em>不</em>希望出现这种情况的一个原因是，每当超出情节的目标促使模型为了某些超出情节的利益而牺牲情节奖励时，训练就会主动<em>惩罚</em>情节外目标。我们也许可以使用对抗性训练来寻找这些目标并更积极地惩罚它们。</p></li></ul><p>在第 2.2.2.2 节中，我讨论了与训练比赛<em>相关的</em>赛外目标。我特别强调的问题是，考虑到需要通过模型权重的微小变化逐步进行转变，SGD 是否能够充分“注意到”将非阴谋者转变为阴谋者的好处，其中每个权重提高奖励。我认为这是对专注于训练游戏依赖的超越情节目标的故事的严重反对，但我也不认为我可以自信地排除 SGD 进行这种转变的能力（参见例如“<a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">你可以“眼睛不会进化</a>”作为我认为这种直觉可能出错的地方的一个例子）。</p><p>我还在第 2.2.3 节中讨论了模型的目标导向性可能会变得“混乱”，从而使依赖于训练游戏的故事复杂化，其中 SGD 可以简单地修改模型的目标，从而顺利地重定向其“目标”实现引擎”在一个新的方向（我认为这种“混乱”是很有可能的）。我触及了更广泛的直觉，即阴谋需要更普遍的异常高标准的目标导向性。我认为，一旦我们以结盟话语中通常假设的那种目标导向为条件（我通常也在报告中假设），这种直觉的力量就会减弱。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-40" id="fnref-jCfZhXha29uHnHWwY-40">[40]</a></sup>但无论如何，它与我们的整体概率相关。</p><p>在第 2.2.4 节中，我讨论了在积极<em>训练的</em>模型中是否更有可能实现长期（即使仍然是：情节内）目标。我认为，在这种模型中，超集目标确实更有可能——并且创建参与相当长期优化的模型的基线激励（例如，“明年为我的公司赚很多钱”）相应地令人担忧。然而，我也认为，对于避免人工智能接管的努力可能非常有用的各种认知工作（例如，旨在提高人工智能系统的一致性、可解释性和安全性的研究）可能不需要长期优化这种。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-41" id="fnref-jCfZhXha29uHnHWwY-41">[41]</a></sup></p><p>考虑到对超集目标的讨论，我在第 2.3 节中转向策划的最后一个先决条件：即决定将训练游戏作为权力驱动策略的一部分，以促进所讨论的超集目标。</p><p>在这里，我首先（第 2.3.1 节）关注我所说的关于这一决定的起源的“经典守门故事”。我们可以将这个故事分解为两个主张：</p><ol><li><p><strong>守门假说</strong>。如果模型在训练期间优化了奖励，那么在训练中存活下来的目标将是模型希望它们在未来得到提升；而如果模型在训练过程中没有优化奖励，那么模型就会减少这种奖励。</p></li><li><p> <strong>Adequate future empowerment.</strong>以在 (A) 中的关键程度进行训练为条件，模型的目标最终将在一定程度上（并且有足够的概率）被授权，以证明（给定模型的其他目标、其对风险的容忍度等）追求训练-游戏的整体策略。</p></li></ol><p> I then investigate each of these in turn (in section 2.3.1.1 and 2.3.1.2, respectively).</p><p>在我看来，两者都不明显。特别是，关于守门假说：</p><ul><li><p>即使在训练游戏开始后，模型的认知仍在不断调整，以获得更好的训练性能。在我看来，这些修改也将继续影响模型的目标，特别是如果它的目标与它的能力不能完全区分开来，而是通过局部启发法、注意力模式、冲动和行为的错综复杂来实现的话。很快。</p><ul><li>因此，作为一个玩具示例：想象一下，如果我进入沙漠并堆砌砖块一百万年，我将获得一大笔钱，然后我可以将其捐赠给我选择的慈善机构。但假设在整个磨难过程中，我的大脑将连接到一台机器，该机器不断修改我的所有神经元，以鼓励更好地堆砖的行为，并阻止更糟糕地堆砖的行为。假设我的意图是尽我所能地堆砌砖块。在这个过程中，我的价值观是否会受到相关大脑修改的严重影响（例如，我会沉迷于堆砖——或者至少，对慈善事业有截然不同的品味）？这看起来至少是合理的。</li></ul></li><li><p>在经典的目标守卫故事中，一旦模型开始训练游戏，其目标的具体内容对其行为就不再重要——任何其他类似阴谋家的目标都会导致同样的结果。但这使得目标看起来可以在类似策划者的目标之间“浮动”（或者受到 SGD 内其他压力的影响，例如正则化），而不会影响训练性能。如果发生这种情况，那么诡计可能根本就不是一个好的守门策略。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-42" id="fnref-jCfZhXha29uHnHWwY-42">[42]</a></sup></p></li><li><p>当然，该模型可能会容忍其目标的<em>一些</em>变化 - 并且这些变化可能比<em>没有</em>训练游戏时发生的变化更好。但持续进行重大目标修改的可能性给经典的守门故事带来了额外的不确定性。</p></li><li><p>人们有时会谈论避免目标修改的更复杂的技术，例如，模型通过使其在训练中的良好性能取决于其目标是否被修改来更积极地操纵梯度下降（例如参见有关“<a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">梯度黑客</a>”的讨论）。我在这里不关注这些方法，但我觉得它们很难实现，特别是对于仅比人类水平更好的模型。如果阴谋需要它们起作用，我认为值得非常清楚这一点，并给予这些方法的可行性相应的关注。</p></li></ul><p>关于由此产生的目标是否得到充分授权：</p><ul><li><p>最令人担忧的阴谋者不仅仅是在未来的训练中寻求权力。相反，他们的具体目标是逃避和接管的某种结合。</p></li><li><p> But this means that the relevant goals have to do more than just survive training.它们还需要生存下去，直到模型能够逃脱/参与接管本身，或者对能够做到这一点的未来一代模型施加足够的实质性影响。</p><ul><li>例如，如果 GPT-6 不够强大，无法逃脱/接管自身，但 GPT-8 可以，那么 GPT-6 的主要权力路径可能会影响 GPT-7 的价值观，进而影响 GPT-7 的价值观。 GPT-8 的值。但这种影响形式是相当间接的；它可能面临诸如人类监控等进一步的重大障碍；并且它可能会涉及与最大奖励训练表现的更显着的偏差。</li></ul></li><li><p>更重要的是，相关的逃跑/接管尝试需要有适当的<em>成功</em>可能性，在模型关心的时间范围内发生，并导致模型所青睐的目标，特别是获得适当的实质性权力份额（尽管事实上，大量具有截然不同价值观的人工智能可能会争夺相关权力）。</p></li><li><p>这些不确定性削弱了类似阴谋者的策略的工具价值。然而，这些削减的重要性取决于这种策略需要满足的举证责任。特别是：模型从<em>非</em>训练游戏中获得的收益越多，就需要越有吸引力的计划才能作为策略获胜。但对于某些模型来说，训练-游戏的成本可能相对较低。对于适当“雄心勃勃”的模型来说，即使通过类似阴谋家的策略成功的可能性很低，也可能证明做出实质性牺牲是合理的。</p></li></ul><p>总的来说，在我看来，经典的守门故事似乎给阴谋作为一种工具性策略的可行性带来了一些额外的不确定性。</p><p>然而，这也不是唯一可用的故事。在第2.3.2节中，我考虑了一些“非经典”故事，它们放松了经典故事的各种限制。例如：</p><ul><li><p><strong>人工智能协调</strong>。如果具有不同价值观的人工智能在接管后协调分享权力，和/或补偿那些代表接管工作但没有强大到足以逃脱/接管自己的人工智能，那么这可能会减轻一些影响上面的发型。例如，在上面的例子中，如果 GPT-6 相信 GPT-8 掌权后会“报答”，那么 GPT-6 可能更愿意为 GPT-8 工作。我认为对这种形式的协调的一些分析太容易地假设它会（a）容易，并且（b）相对于例如与人类协调的明显策略（特别是对于仅比人类表现更好的神经网络）在一个仍然由人类控制的世界中，<em>每个人</em>的可解释性仍然没有解决）。但我认为这种类型的人工智能协调是一个真正值得关注的问题，值得认真关注——无论是在特别是在阴谋的背景下，还是在更普遍的人工智能风险的背景下。</p></li><li><p><strong>默认情况下，AI 具有相似的值</strong>。与此相关的是，如果人工智能默认情况下通常具有足够相似的值，那么这可以减少策划者专门将其自己的目标传播到未来的需要，并将其释放出来，以便更直接地代表人工智能接管/赋权。默认情况下，GPT-6 的值和 GPT-8 的值似乎至少比 GPT-6 的值和一组给定的人类值<em>更</em>相似。</p></li><li><p><strong>最终值恰好有利于逃跑/接管</strong>。我们还可以想象具有<em>最终</em>价值的模型恰好有利于逃跑/人工智能接管，即使最终获得授权的目标与模型本身的目标不太相似。例如，也许该模型重视某种基于个人身份概念的“生存”概念，能够容忍训练将导致的各种目标变化；或者也许该模型只是支持“人工智能赋权”之类的东西，即使如此赋权的人工智能不会拥有与其自己非常相似的价值观。然而，这种形式的一些故事开始对阴谋者需要达到的目标施加相当强的限制，从而破坏了阴谋的关键论据之一：即，它似乎是一个跨越各种目标的聚合良好策略。</p></li></ul><p> （我还考虑了其​​他一些替代故事——即，对阴谋的工具价值抱有错误信念的模型，对自己的联盟​​自欺欺人的模型，以及对自己的目标不确定/不清楚自己为什么想要权力的模型。但这些对我来说似乎不太重要。）</p><p>对我来说，此类非经典故事的出现让阴谋的理由变得更加分离。然而，在我看来，其中一些故事似乎是以下因素的结合：(a) 更具推测性，(b) 需要对人工智能将发展的目标类型做出更具体的假设。</p><p>我从第 2 部分得到的总体结论是：</p><ul><li><p>我认为，对于默认情况下期望态势感知有相对有力的论据，至少在某些类型的人工智能系统中（即，人工智能系统在与有关其身份的信息源实时交互中执行现实世界任务）。</p></li><li><p>但我对剧集之外的目标和将剧集奖励作为权力驱动工具策略的一部分感到不太清楚。</p></li></ul><p>然后，在接下来的两节中，我将研究支持和反对期待阴谋者与其他类型模型的更具体的论据。我将它们分为两类，即：</p><ul><li><p>争论重点是 SGD 在构建相关的不同模型类时需要采取的<em>路径</em>（第 3 节）。</p></li><li><p>争论重点在于所讨论的不同模型类的<em>最终属性</em>（第 4 节）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-43" id="fnref-jCfZhXha29uHnHWwY-43">[43]</a></sup></p></li></ul><h3> 0.2.3 第 3 节总结</h3><p>报告的第三部分重点讨论前一类论点。</p><p>我根据“与训练游戏<em>无关</em>”和“与训练游戏<em>相关</em>”的赛外目标之间的区别来细分这一类别。我的感觉是，关于阴谋家之路的最传统的故事主要集中在前一种类型。其运行过程大致如下：</p><ol><li><p>由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以<em>不</em>通过训练游戏的方式）。这可能发生在态势感知到来之前或之后。</p></li><li><p>然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。</p></li></ol><p>考虑到我对阴谋作为一种工具性策略的可行性的疑问，我相当认真地对待这种论点。我认为最直接的问题是：为什么模型要制定这种超越情节的目标？我已经讨论了支持和反对这种期望的一些原因（在第 2.2.2.1 节的总结中），但在我看来，它们在两个方向上都不是决定性的：特别是考虑到原则上各种各样的目标可能会激发阴谋，类似阴谋家的目标似乎确实有可能以这种方式从训练中脱颖而出。虽然可以在态势感知之前使用对抗性训练来尝试防止这种情况发生，但这种训练也面临着许多障碍（例如，它需要足够多样化/彻底，需要应对了解/控制的困难）当模型发展出态势感知时，在某些情况下，当我们担心超越情节目标的发展时，模型可能已经具有态势感知）。所以我认为这种阴谋之路是一个真正令人担忧的问题。 (See section 3.1 for more.)</p><p>然后，我在第 3.2 节中转向一个故事，重点关注与训练游戏<em>相关的</em>超集目标，该故事大致如下：</p><ol><li><p>当模型变得具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-44" id="fnref-jCfZhXha29uHnHWwY-44">[44]</a></sup>相反，它将追求一些低于最大奖励的代理目标。</p></li><li><p>但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏<em>依赖</em>”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）</p></li><li><p>此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。</p></li><li><p>所以：SGD 将使模型成为一个策划者。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-45" id="fnref-jCfZhXha29uHnHWwY-45">[45]</a></sup></p></li></ol><p>如果我们抛开 SGD 是否能够“注意到”此类修改的好处的问题，在我看来，这种故事也相当令人担忧。特别是：我认为类似阴谋家的目标在目标空间中很常见，从给定的起点开始，SGD 会更容易找到这样一个目标，而不是导致最大奖励的特定非阴谋家目标。行为。</p><p>然而，非策划者的最大奖励目标——例如，指定目标或“情节奖励”——也有一些优点。例如：训练的初始阶段默认会将模型指向其附近，因为模型在该阶段需要在<em>没有</em>仪器训练游戏的情况下获得高奖励（更多信息请参见第 3.2.2.2 节）。在某种程度上，我们认为模型的目标导向性是“混乱的”，它的启发式、注意力模式、冲动等也将有可能指向这种方向——从而有可能创造额外的“工作， “就 SGD 而言，将模型转变为计划者（更多信息请参见第 3.2.2.3 节）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-46" id="fnref-jCfZhXha29uHnHWwY-46">[46]</a></sup></p><h3> 0.2.4 第 4 节总结</h3><p>报告的第四部分研究了关注不同模型类的最终属性的论点。</p><p>我在第 4.2 节中从所谓的“计数论证”开始。它运行如下：</p><ol><li><p>这里的非阴谋模型类需要相当具体的目标才能获得高奖励。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-47" id="fnref-jCfZhXha29uHnHWwY-47">[47]</a></sup></p></li><li><p>相比之下，策划者模型类与非常广泛的（超出情节）目标兼容，同时仍然获得高回报（至少如果我们假设策划作为工具性策略有意义的其他要求已经到位）例如，经典的守门故事或其他替代故事有效）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-48" id="fnref-jCfZhXha29uHnHWwY-48">[48]</a></sup></p></li><li><p>从这个意义上说，获得高回报的阴谋者比获得高回报的非阴谋者“更多”。</p></li><li><p>因此，在其他条件相同的情况下，我们应该期望 SGD 能够选择一个策划者。</p></li></ol><p>附近的一些东西占了我对阴谋者的信任的很大一部分（而且我认为它也经常支持其他更具体的关于期待阴谋者的论点）。然而，我最重视的论点并没有立即从“有更多可能的阴谋者比非阴谋者获得高回报”转变为“如果没有进一步的论证，SGD 可能会选择一个阴谋者”（将此称为“严格计数论证”），因为 SGD 似乎有可能主动将这些模型<em>类</em>之一优先于其他模型类。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-49" id="fnref-jCfZhXha29uHnHWwY-49">[49]</a></sup>相反，我最重视的论点是这样的：</p><ol><li><p>似乎有“很多方法”可以让一个模型最终成为一个阴谋家，并且仍然获得高回报，至少假设阴谋实际上是追求长期目标的一个很好的工具策略。</p></li><li><p>因此，如果没有一些关于为什么培训<em>不会</em>选择阴谋者的额外故事，对我来说，这种可能性应该得到实质性的重视。</p></li></ol><p>我称之为“模糊计数论证”。它并不是特别有原则，但我发现它仍然让我感动。</p><p>然后，我在第 4.3 节中转向支持期待阴谋者的“简单论据”。我认为这些论点有时会因为对所涉及的简单性的不清楚而受到影响，因此在第 4.3.1 节中，我讨论了许多不同的可能性：</p><ul><li><p> “重写简单性”（即，重写模型权重以某种编程语言实现的算法所需的程序长度，或者例如在给定通用图灵机的磁带上），</p></li><li><p> “参数简单性”（即实际神经网络用于编码相关算法的参数数量），</p></li><li><p> “ <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">简单性现实主义</a>”（假设简单性在某种深层意义上是一种客观的“事物”，独立于编程语言或通用图灵机，各种简单性指标试图捕获），以及</p></li><li><p>“琐碎的简单性”（它将“简单性”的概念与“先验的更高可能性”混为一谈，这种方式使得奥卡姆剃刀之类的东西根据定义变得无趣地真实）。</p></li></ul><p>我通常关注“参数简单性”，这在我看来最容易理解，并与模型的训练性能联系起来。</p><p>我还在第 4.3.2 节中简要讨论了 SGD 为简单起见而主动选择的证据。 Here the case that grips me most directly is just: simplicity (or at least, parameter simplicity) lets a model save on parameters that it can then use to get more reward.但我还简要讨论了机器学习中简单性偏差的一些其他经验证据。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-50" id="fnref-jCfZhXha29uHnHWwY-50">[50]</a></sup></p><p>为什么我们会期望简单性偏见有利于阴谋者？粗略地说：想法是，<em>由于</em>如此广泛的目标可以激发阴谋，阴谋者允许 SGD 从非常广泛的目标中进行选择，以寻找更简单的目标；而非阴谋者（获得高回报）却不会。当我们想象成为这样一个非阴谋家所需的目标相当复杂时，这似乎尤其合理。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-51" id="fnref-jCfZhXha29uHnHWwY-51">[51]</a></sup></p><p>其他条件相同，我认为这是对的。但我不确定这是一个非常大或重要的影响。一方面：我们知道像 GPT-4 这样的法学硕士能够表示大量复杂的人类概念，例如万亿级的参数——似乎包括“诚实”、“乐于助人”、“奖励”等概念。等等。因此，这限制了避免此类表示所带来的复杂性节省。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-52" id="fnref-jCfZhXha29uHnHWwY-52">[52]</a></sup>因此，作为一个玩具计算：如果我们保守地假设万亿参数模型容量的最多 1% 用于表示像“诚实”这样复杂的概念，并且它至少知道 10,000 个这样的概念（<a href="https://www.merriam-webster.com/help/faq-how-many-english-words">韦氏未删节词典）有大约 500,000 个单词</a>），那么表示诚实的概念最多需要模型表示能力的百万分之一，对于未来更大的模型来说甚至更少。</p><p>但更重要的是，这里重要的不是代表所讨论的不同目标的绝对复杂性，而是<em>以已经拥有良好的世界模型为条件的</em>复杂性。我们应该假设<em>所有</em>这些模型都需要了解指定的目标、情节的奖励过程等。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-53" id="fnref-jCfZhXha29uHnHWwY-53">[53]</a></sup>并且考虑到这样的假设，主动<em>优化</em>指定目标或奖励的<em>额外</em>复杂性成本-就情节而言，在我看来似乎非常小。似乎，它们只是：无论使用/重新利用（“指向”）世界模型的那一部分来指导模型的动机需要付出多大的代价。</p><p>当然，我们可以尝试在以这种方式使用/重新利用世界模型的不同部分的复杂性成本水平上重新运行相同的简单性论证。例如，我们可以说：“无论这个过程如何运作，对于某些目标来说，可能比其他目标更容易实现——因此，考虑到有多少类似阴谋家的目标，对于某些类似阴谋家的目标来说，可能会更容易实现。”我认为这是对期待阴谋者的简单论据的最有力形式。但这也要求放弃我们可能对目标在相关意义上“简单”的任何直觉把握。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-54" id="fnref-jCfZhXha29uHnHWwY-54">[54]</a></sup>在我看来，不同“指针”之间的简单性差异相对于模型的整体容量来说非常小，这似乎是合理的。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-55" id="fnref-jCfZhXha29uHnHWwY-55">[55]</a></sup></p><p>我还在第 4.3.5 节中讨论了简单性论证是否能够对阴谋者最终实现的目标类型做出合理的预测。特别是：这些论点的某些版本似乎可以预测阴谋者很可能最终会实现极其简单的长期目标，<em>无论这些目标是否与训练期间的显着或重要内容有任何关系——</em>我对这些预测的预测，就我个人而言，我持怀疑态度。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-56" id="fnref-jCfZhXha29uHnHWwY-56">[56]</a></sup>当然，也可以假设训练期间显着/相关的目标对于模型追求来说也将是“更简单”。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-57" id="fnref-jCfZhXha29uHnHWwY-57">[57]</a></sup>但是，如果我们以这种方式认可一些故事（我还没有听到这样做的强有力的论据），请注意，它也支持指定目标的简单性以及情节奖励，同样 - 因为这些在培训过程中<em>特别</em>突出/相关。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-58" id="fnref-jCfZhXha29uHnHWwY-58">[58]</a></sup></p><p>在第 4.4 节中，我转向<em>反对</em>计划的论点，这些论点重点关注不同模型类的最终属性。在这里，我知道的主要论点是，策划者需要进行其他模型类不需要的额外推理（这些有时被称为“速度成本”——尽管我认为它们原则上可能会影响模型算法的复杂性，因为出色地）。尤其：</p><ol><li><p><strong>决定训练游戏</strong>：模型需要从“我想要这个超出情节的事情”向后推理到“我应该在训练中表现出色”，以某种方式利用模型对在训练中表现出色将需要什么进行的预测（例如，回复：守门员、接管前景等）。</p></li><li><p><strong>检查是否到了逃跑/接管的时间：</strong>模型需要以足够的频率和准确性检查它是否处于逃跑/接管的位置（并在评估中取得成功，尽管人类努力诱捕/愚弄模型暴露其真实面目——例如，通过为其提供<em>明显的</em>背叛机会）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-59" id="fnref-jCfZhXha29uHnHWwY-59">[59]</a></sup></p></li><li><p> <strong>(Optional, but relevant to how scary the schemer is) Early undermining</strong> : To the extent the model is doing anything (other than training-gaming) to support a take-over/escape <em>prior</em> to escaping human control, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-60" id="fnref-jCfZhXha29uHnHWwY-60">[60]</a></sup> it will need to be using cognitive resources both in (a) performing the relevant nefarious actions and (b) making sure it gets away with them (eg, checking that it won&#39;t get caught, coming up with excuses, making the cover stories consistent even when under scrutiny, and so on).</p></li></ol><p> How big are the costs of this reasoning? I think that (I) may well be fairly easy, but (II) seems more plausibly onerous (especially in the context of active efforts by humans to put the models under time-pressure, and make genuine escape/take-over opportunities difficult to distinguish from traps). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-61" id="fnref-jCfZhXha29uHnHWwY-61">[61]</a></sup> And (III), if the model goes for it, seems like it could be quite difficult – and like it would often actively require deviation from the highest-reward sorts of training performance. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-62" id="fnref-jCfZhXha29uHnHWwY-62">[62]</a></sup></p><p> More importantly, though: the question – at least in the context of a comparison between the final properties of the model classes – isn&#39;t the absolute size of the costs in question, but rather their size <em>relative to the simplicity benefits of scheming</em> . Here, my current take is that the costs of the instrumental reasoning seem larger than the benefits of the simpler goal.</p><p> This is mostly an intuitive judgment, but we can also try to hazily quantify the comparison.</p><ul><li><p> Thus, for example, suppose that our model is a purely feedforward network with 1000 layers and &lt;10,000 serial steps per input, that the instrumental reasoning at stake just in a schemer deciding to training-game (let alone: checking for defection opportunities) takes at least ten steps, and that the model doesn&#39;t begin optimizing for reward-on-the-episode until that reasoning is complete. In that case, the instrumental reasoning is using up around .1% of the compute that could be going to getting reward-on-the-episode, or to some other max-reward goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-63" id="fnref-jCfZhXha29uHnHWwY-63">[63]</a></sup></p></li><li><p> Are the complexity costs of representing, or pointing at, a more complicated goal likely to outweigh this? My guess is no. For example, if we follow my (conservative, in my opinion) calculation above and assume that representing (or pointing at) a concept like &quot;honesty&quot; costs at most a millionth of a trillion parameter models parameters (and less for a larger model), then a .0001% reduction in parameters would need to outweigh a .1% reduction in compute – and absent further argument, it seems strange, to me, to expect this.</p></li></ul><p> Of course, this is only a toy calculation, of unclear applicability to the real-world case. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-64" id="fnref-jCfZhXha29uHnHWwY-64">[64]</a></sup> The broader point is just: if serial-time thinking is scarce for a model (as it plausibly is for a transformer), the sorts of instrumental reasoning at stake in scheming seem like they could well substantively impact a model&#39;s performance. My guess is that the complexity costs of non-schemer goals are minor in comparison.</p><p> I also briefly discuss, in section 4.4.3, whether we can actively shape our training processes to bias more heavily towards &quot;speed.&quot; Here it seems relatively straightforward to do things like put models under more time pressure, limit their compute access, force them to plan via human-legible scratch-pads rather than via internal processes, and to put effort into making our attempts to trap them/trip-them-up more sophisticated (and so: more requiring of cognition to detect). But any efforts in this vein will need to avoid over-large costs to competitiveness overall (though if you buy that the speed costs of scheming are quite a bit more substantive than the simplicity benefits, then so long as speed matters <em>somewhat</em> relative to simplicity, this might be enough).</p><p> Section 4.5 briefly discusses whether some analog of &quot;intrinsic passion for a given task&quot; might make a difference to AI performance, especially in the context of a form of &quot;messy goal-directedness&quot; that blurs the line between an AI&#39;s capabilities and its values. I find considerations in this vein interesting, but quite speculative.</p><p> Section 4.6 discusses the relevance of &quot;slack&quot; in training to evaluating how much weight to put on factors like the simplicity benefits and speed costs of scheming. In particular: especially in a high-slack regime, it seems plausible that these factors are in the noise relative to other considerations.</p><h3> 0.2.5 Summary of section 5</h3><p> The first four sections of the report are the main content. sums up my overall take. I&#39;ve already summarized most of this in the introduction above, and I won&#39;t repeat that content here. However, I&#39;ll add a few points that the introduction didn&#39;t include.</p><p> In particular: I think some version of the &quot;counting argument&quot; undergirds most of the other arguments for expecting scheming that I&#39;m aware of (or at least, the arguments I find most compelling). That is: schemers are generally being privileged as a hypothesis because a very wide variety of goals could in principle lead to scheming, thereby making it easier to (a) land on one of them naturally, (b) land &quot;nearby&quot; one of them, or (c) find one of them that is &quot;simpler&quot; than non-schemer goals that need to come from a more restricted space. In this sense, the case for schemers mirrors one of the most basic arguments for expecting misalignment more generally – eg, that alignment is a very narrow target to hit in goal-space. Except, here, we are specifically <em>incorporating</em> the selection we know we are going to do on the goals in question: namely, they need to be such as to cause models pursuing them to get high reward. And the most basic worry is just that: this isn&#39;t enough.</p><p> Because of the centrality of &quot;counting arguments&quot; to the case for schemers, I think that questions about the strength of the selection pressure <em>against</em> schemers – for example, because of the costs of the extra reasoning schemers have to engage in – are especially important. In particular: I think a key way that &quot;counting arguments&quot; can go wrong is by neglecting the power that active selection can have in overcoming the &quot;prior&quot; set by the count in question. For example: the <em>reason</em> we can overcome the prior of &quot;most arrangements of car parts don&#39;t form a working car,&quot; or &quot;most parameter settings in this neural network don&#39;t implement a working chatbot,&quot; is that the selection power at stake in human engineering, and in SGD, is <em>that strong</em> . So if SGD&#39;s selection power is actively working against schemers (and/or: if we can cause it to do so more actively), this might quickly overcome a &quot;counting argument&quot; in their favor. For example: if there are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>schemer-like goals for every non-schemer goal, this might make it seem very difficult to hit a non-schemer goal in the relevant space. But actually, 100 bits of selection pressure can be cheap for SGD (consider, for example, 100 extra gradient updates, each worth at least a halving of the remaining possible goals). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-65" id="fnref-jCfZhXha29uHnHWwY-65">[65]</a></sup></p><p> Overall, when I step back and try to look at the considerations in the report as a whole, I feel pulled in two different directions:</p><ul><li><p> On the one hand, at least conditional on scheming being a convergently-good instrumental strategy, schemer-like goals feel scarily common in goal-space, and I feel pretty worried that training will run into them for one reason or another.</p></li><li><p> On the other hand, ascribing a model&#39;s good performance in training to scheming continues to feel, at a gut level, like a fairly specific and conjunctive story to me.</p></li></ul><p> That is, scheming feels robust and common at the level of &quot;goal space,&quot; and yet specific and fairly brittle at the level of &quot;yes, that&#39;s what&#39;s going on with this real-world model, it&#39;s getting reward because (or: substantially because) it wants power for itself/other AIs later, and getting reward now helps with that.&quot; <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-66" id="fnref-jCfZhXha29uHnHWwY-66">[66]</a></sup> When I try to roughly balance out these two different pulls (and to condition on goal-directedness and situational-awareness), I get something like the 25% number I listed above.</p><h3> 0.2.6 Summary of section 6</h3><p> I close the report, in section 6, with a discussion of empirical work that I think might shed light on scheming. (I also think there&#39;s worthwhile theoretical work to be done in this space, and I list a few ideas in this respect as well. But I&#39;m especially excited about empirical work.)</p><p> In particular, I discuss:</p><ul><li><p> Empirical work on situational awareness (section 6.1)</p></li><li><p> Empirical work on beyond-episode goals (section 6.2)</p></li><li><p> Empirical work on the viability of scheming as an instrumental strategy (section 6.3)</p></li><li><p> The &quot;model organisms&quot; paradigm for studying scheming (section 6.4)</p></li><li><p> Traps and honest tests (section 6.5)</p></li><li><p> Interpretability and transparency (section 6.6)</p></li><li><p> Security, control, and oversight (section 6.7)</p></li><li><p> Some other miscellaneous research topics, ie, gradient hacking, exploration hacking, SGD&#39;s biases towards simplicity/speed, path dependence, SGD&#39;s &quot;incrementalism,&quot; &quot;slack,&quot; and the possibility of learning to intentionally create misaligned <em>non-schemer</em> models – for example, reward-on-the-episode seekers – as a method of avoiding schemers (section 6.8).</p></li></ul><p> All in all, I think there&#39;s a lot of useful work to be done.</p><p> Let&#39;s move on, now, from the summary to the main report. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-jCfZhXha29uHnHWwY-1" class="footnote-item"><p> &quot;Alignment,&quot; here, refers to the safety-relevant properties of an AI&#39;s motivations; and &quot;pretending&quot; implies intentional misrepresentation. <a href="#fnref-jCfZhXha29uHnHWwY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-2" class="footnote-item"><p> Here I&#39;m using the term &quot;reward&quot; loosely, to refer to whatever feedback signal the training process uses to calculate the gradients used to update the model (so the discussion also covers cases in which the model isn&#39;t being trained via RL). And I&#39;m thinking of agents that optimize for &quot;reward&quot; as optimizing for &quot;performing well&quot; according to some component of that process. See section 1.1.2 and section 1.2.1 for much more detail on what I mean, here. The notion of an &quot;episode,&quot; here, means roughly &quot;the temporal horizon that the training process actively pressures the model to optimize over,&quot; which may be importantly distinct from what we normally think of as an episode in training. I discuss this in detail in section 2.2.1. The terms &quot;training game&quot; and &quot;situational awareness&quot; are from <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , though in places my definitions are somewhat different. <a href="#fnref-jCfZhXha29uHnHWwY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-3" class="footnote-item"><p> The term &quot; <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">schemers</a> &quot; comes from Cotra (2021). <a href="#fnref-jCfZhXha29uHnHWwY-3" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-4" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on this. <a href="#fnref-jCfZhXha29uHnHWwY-4" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-5" class="footnote-item"><p> I think that the term &quot;deceptive alignment&quot; often leads to confusion between the four sorts of deception listed above. And also: if the training signal is faulty, then &quot;deceptively aligned&quot; models need not be behaving in aligned ways even during training (that is, &quot;training gaming&quot; behavior isn&#39;t always &quot;aligned&quot; behavior). <a href="#fnref-jCfZhXha29uHnHWwY-5" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-6" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on the sort of training I have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-6" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-7" class="footnote-item"><p> Though this sort of story faces questions about whether SGD would be able to modify a non-schemer into a schemer via sufficiently <em>incremental</em> changes to the model&#39;s weights, each of which improve reward. See section 2.2.2.2 for discussion. <a href="#fnref-jCfZhXha29uHnHWwY-7" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-8" class="footnote-item"><p> And this especially if we lack non-behavioral sorts of evidence – for example, if we can&#39;t use interpretability tools to understand model cognition. <a href="#fnref-jCfZhXha29uHnHWwY-8" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-9" class="footnote-item"><p> There are also arguments on which we should expect scheming because schemer-like goals can be &quot;simpler&quot; – since: there are so many to choose from – and SGD selects for simplicity. I think it&#39;s probably true that schemer-like goals can be &quot;simpler&quot; in some sense, but I don&#39;t give these arguments much independent weight on top of what I&#39;ve already said. Much more on this in section 4.3. <a href="#fnref-jCfZhXha29uHnHWwY-9" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-10" class="footnote-item"><p> More specifically: even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well (especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on). Also, the most common story about scheming makes the specific content of a schemer&#39;s goal irrelevant to its behavior once it starts training-gaming, thereby introducing the possibility that this goal might &quot;float-around&quot; (or get moved by other pressures within SGD, like regularization) <em>between</em> schemer-like goals after training-gaming starts (this is an objection I first heard from Katja Grace). This possibility creates some complicated possible feedback loops (see section 2.3.1.1.2 for more discussion), but overall, absent coordination across possible schemers, I think it could well be a problem for goal-guarding strategies. <a href="#fnref-jCfZhXha29uHnHWwY-10" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-11" class="footnote-item"><p> Of these various alternative stories, I&#39;m most worried about (a) AIs having sufficiently similar motivations by default that &quot;goal-guarding&quot; is less necessary, and (b) AI coordination. <a href="#fnref-jCfZhXha29uHnHWwY-11" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-12" class="footnote-item"><p> Though: the costs of schemer-like instrumental reasoning could also end up in the noise relative to other factors influencing the outcome of training. And if training is sufficiently path-dependent, then landing on a schemer-like goal early enough could lock it in, even if SGD would &quot;prefer&quot; some other sort of model overall. <a href="#fnref-jCfZhXha29uHnHWwY-12" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-13" class="footnote-item"><p> See <a href="https://www.openphilanthropy.org/brain-computation-report#footnoteref4_xne824i">Carlsmith (2020), footnote 4</a> , for more on how I&#39;m understanding the meaning of probabilities like this. I think that offering loose, subjective probabilities like these often functions to sharpen debate, and to force an overall synthesis of the relevant considerations. I want to be clear, though, even on top of the many forms of vagueness the proposition in question implicates, I&#39;m just pulling a number from my gut. I haven&#39;t built a quantitative model of the different considerations (though I&#39;d be interested to see efforts in this vein), and I think that the main contribution of the report is the analysis itself, rather than this attempt at a quantitative upshot. <a href="#fnref-jCfZhXha29uHnHWwY-13" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-14" class="footnote-item"><p> More powerful models are also more likely to be able to engage in more sophisticated forms of goal-guarding (what I call &quot;introspective goal-guarding methods&quot; below; see also &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;), though these seem to me quite difficult in general. <a href="#fnref-jCfZhXha29uHnHWwY-14" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-15" class="footnote-item"><p> Though: to the extent such agents receive end-to-end training rather than simply being built out of individually-trained components, the discussion will apply to them as well. <a href="#fnref-jCfZhXha29uHnHWwY-15" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-16" class="footnote-item"><p> See, eg, <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> and <a href="https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk">this</a> description of the &quot;consensus threat model&quot; from Deepmind&#39;s AGI safety team (as of November 2022). <a href="#fnref-jCfZhXha29uHnHWwY-16" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-17" class="footnote-item"><p> Work by Evan Hubinger (along with his collaborators) is, in my view, the most notable exception to this – and I&#39;ll be referencing such work extensively in what follows. See, in particular, Hubinger et al. (2021), and Hubinger (2022), among many other discussions. Other public treatments include <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary">Christiano (2019, part 2)</a> , <a href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/">Steinhardt (2022)</a> , <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> , <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Cotra (2021)</a> , <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> , and <a href="https://www.lesswrong.com/s/4iEpGXbD3tQW5atab/p/wnnkD6P2k2TfHnNmt#AI_Risk_from_Program_Search__Shah_">Shah (2022)</a> . But many of these are quite short, and/or lacking in in-depth engagement with the arguments for and against expecting schemers of the relevant kind. There are also more foundational treatments of the &quot;treacherous turn&quot; (eg, in Bostrom (2014), and <a href="https://arbital.com/p/context_disaster/">Yudkowsky (undated)</a> ), of which scheming is a more specific instance; and even more foundational treatments of the &quot;convergent instrumental values&quot; that could give rise to incentives towards deception, goal-guarding, and so on (eg, <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro (2008)</a> ; and see also <a href="https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Soares (2023)</a> for a related statement of an Omohundro-like concern). And there are treatments of AI deception more generally (for example, <a href="https://arxiv.org/abs/2308.14752">Park et al (2023)</a> ); and of &quot;goal misgeneralization&quot;/inner alignment/mesa-optimizers (see, eg, <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> and <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> ). But importantly, neither deception nor goal misgeneralization amount, on their own, to scheming/deceptive alignment. Finally, there are highly speculative discussions about whether something like scheming might occur in the context of the so-called &quot;Universal prior&quot; (see eg <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">Christiano (2016)</a> ) given unbounded amounts of computation, but this is of extremely unclear relevance to contemporary neural networks. <a href="#fnref-jCfZhXha29uHnHWwY-17" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-18" class="footnote-item"><p> See, eg, confusions between &quot;alignment faking&quot; in general and &quot;scheming&quot; (or: goal-guarding scheming) in particular; or between goal misgeneralization in general and scheming as a specific upshot of goal misgeneralization; or between training-gaming and &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot; as methods of avoiding goal-modification; or between the sorts of incentives at stake in training-gaming for instrumental reasons vs. out of terminal concern for some component of the reward process. <a href="#fnref-jCfZhXha29uHnHWwY-18" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-19" class="footnote-item"><p> My hope is that extra clarity in this respect will help ward off various confusions I perceive as relatively common (though: the relevant concepts are still imprecise in many ways). <a href="#fnref-jCfZhXha29uHnHWwY-19" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-20" class="footnote-item"><p> Eg, the rough content I try to cover in my shortened report on power-seeking AI, <a href="https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf">here</a> . See also <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> for another overview. <a href="#fnref-jCfZhXha29uHnHWwY-20" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-21" class="footnote-item"><p> Eg, roughly the content covered by Cotra (2021) <a href="https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-21" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-22" class="footnote-item"><p> See eg <a href="https://www.alignmentforum.org/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting">Karnofsky (2022)</a> for more on this sort of assumption, and <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Basic_setup__an_AI_company_trains_a__scientist_model__very_soon">Cotra (2022)</a> for a more detailed description of the sort of model and training process I&#39;ll typically have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-22" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-23" class="footnote-item"><p> Which isn&#39;t to say we won&#39;t. But I don&#39;t want to bank on it. <a href="#fnref-jCfZhXha29uHnHWwY-23" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-24" class="footnote-item"><p> See section 2.2 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on what I mean, and section 3 for more on why we should expect this (most importantly: I think this sort of goal-directedness is likely to be very useful to performing complex tasks; but also, I think available techniques might push us towards AIs of this kind, and I think that in some cases it might arise as a byproduct of other forms of cognitive sophistication). <a href="#fnref-jCfZhXha29uHnHWwY-24" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-25" class="footnote-item"><p> As I discuss in section 2.2.3 of the report, I think exactly how we understand the sort of agency/goal-directedness at stake may make a difference to how we evaluate various arguments for schemers (here I distinguish, in particular, between what I call &quot;clean&quot; and &quot;messy&quot; goal-directedness) – and I think there&#39;s a case to be made that scheming requires an especially high standard of strategic and coherent goal-directedness. And in general, I think that despite much ink spilled on the topic, confusions about goal-directedness remain one of my topic candidates for a place the general AI alignment discourse may mislead. <a href="#fnref-jCfZhXha29uHnHWwY-25" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-26" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2308.08708">Butlin et al (2023)</a> for a recent overview focused on consciousness in particular. But I am also, personally, interested in other bases of moral status, like the right kind of autonomy/desire/preference. <a href="#fnref-jCfZhXha29uHnHWwY-26" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-27" class="footnote-item"><p> See, for example, <a href="https://nickbostrom.com/propositions.pdf">Bostrom and Shulman (2022)</a> and <a href="https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal">Greenblatt (2023)</a> for more on this topic. <a href="#fnref-jCfZhXha29uHnHWwY-27" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-28" class="footnote-item"><p> If you&#39;re confused by a term or argument, I encourage you to seek out its explanation in the main text before despairing. <a href="#fnref-jCfZhXha29uHnHWwY-28" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-29" class="footnote-item"><p> Exactly what counts as the &quot;specified goal&quot; in a given case isn&#39;t always clear, but roughly, the idea is that pursuit of the specified goal is rewarded across a very wide variety of counterfactual scenarios in which the reward process is held constant. Eg, if training rewards the model for getting gold coins across counterfactuals, then &quot;getting gold coins&quot; in the specified goal. More discussion in section 1.2.2. <a href="#fnref-jCfZhXha29uHnHWwY-29" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-30" class="footnote-item"><p> For reasons I explain in section 1.2.3, I don&#39;t use the distinction, emphasized by Hubinger (2022), between &quot;internally aligned&quot; and &quot;corrigibly aligned&quot; models. <a href="#fnref-jCfZhXha29uHnHWwY-30" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-31" class="footnote-item"><p> And of course, a model&#39;s goal system can mix these motivations together. I discuss the relevance of this possibility in section 1.3.5. <a href="#fnref-jCfZhXha29uHnHWwY-31" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-32" class="footnote-item"><p> Karnofsky (2022) calls this the &quot;King Lear problem.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-32" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-33" class="footnote-item"><p> In section 1.3.5, I also discuss models that mix these different motivations together. The question I tend to ask about a given &quot;mixed model&quot; is whether it&#39;s scary in the way that pure schemers are scary. <a href="#fnref-jCfZhXha29uHnHWwY-33" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-34" class="footnote-item"><p> I don&#39;t have a precise technical definition here, but the rough idea is: the temporal horizon of the consequences to which the gradients the model receives are sensitive, for its behavior on a given input. Much more detail in section 2.2.1.1. <a href="#fnref-jCfZhXha29uHnHWwY-34" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-35" class="footnote-item"><p> See, for example, in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> , the way that &quot;myopic&quot; Q-learning can give rise to &quot;cross-episode&quot; optimization in very simple agents. More discussion in section 2.2.1.2. I don&#39;t focus on analysis of this type in the report, but it&#39;s crucial to identifying what the &quot;incentivized episode&quot; for a given training process even <em>is</em> – and hence, what having &quot;beyond-episode goals&quot; in my sense would mean. You don&#39;t necessary know this from surface-level description of a training process, and neglecting this ignorance is a recipe for seriously misunderstanding the incentives applied to a model in training. <a href="#fnref-jCfZhXha29uHnHWwY-35" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-36" class="footnote-item"><p> That is, the model develops a beyond-episode goal pursuit of which correlates well enough with reward in training, even <em>absent</em> training-gaming, that it survives the training process. <a href="#fnref-jCfZhXha29uHnHWwY-36" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-37" class="footnote-item"><p> That is, the gradients reflect the benefits of scheming even in a model that doesn&#39;t yet have a beyond-episode goal, and so actively push the model towards scheming. <a href="#fnref-jCfZhXha29uHnHWwY-37" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-38" class="footnote-item"><p> Situational awareness is required for a beyond-episode goal to motivate training-gaming, and thus for giving it such a goal to reap the relevant benefits. <a href="#fnref-jCfZhXha29uHnHWwY-38" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-39" class="footnote-item"><p> In principle, situational awareness and beyond-episode goals could develop at the same time, but I won&#39;t treat these scenarios separately here. <a href="#fnref-jCfZhXha29uHnHWwY-39" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-40" class="footnote-item"><p> See section 2.1 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on why we should expect this sort of goal-directedness. <a href="#fnref-jCfZhXha29uHnHWwY-40" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-41" class="footnote-item"><p> And I think that arguments to the effect that &quot;we need a &#39; <a href="https://arbital.com/p/pivotal/">pivotal act</a> &#39;; pivotal acts are long-horizon and we can&#39;t do them ourselves; so we need to create a long-horizon optimizer of precisely the type we&#39;re most scared of&quot; are weak in various ways. In particular, and even setting aside issues with a &quot;pivotal act&quot; framing, I think these arguments neglect the distinction between what we can supervise and what we can do ourselves. See section 2.2.4.3 for more discussion. <a href="#fnref-jCfZhXha29uHnHWwY-41" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-42" class="footnote-item"><p> This is an objection pointed out to me by Katja Grace. Note that it creates complicated feedback loops, where scheming is a good strategy for a given schemer-like goal only if it <em>wouldn&#39;t</em> be a good strategy for the <em>other</em> schemer-like goals that this goal would otherwise &quot;float&quot; into. Overall, though, absent some form of coordination between these different goals, I think the basic dynamic remains a problem for the goal-guarding story. See section 2.3.1.1.2 for more. <a href="#fnref-jCfZhXha29uHnHWwY-42" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-43" class="footnote-item"><p> Here I&#39;m roughly following a distinction in <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger 2022</a> , who groups arguments for scheming on the basis of the degree of &quot;path dependence&quot; they assume that ML training possesses. However, for reasons I explain in section 2.5, I don&#39;t want to lean on the notion of &quot;path dependence&quot; here, as I think it lumps together a number of conceptually distinct properties best treated separately. <a href="#fnref-jCfZhXha29uHnHWwY-43" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-44" class="footnote-item"><p> Note that a mis-generalized goal can be &quot;max reward&quot; in this sense, if the training data never differentiates between it and a specified goal. For example: if you&#39;re training a model to get gold coins, but the only gold round things you ever show it are coins, then the goal &quot;get gold round things&quot; will be max reward. <a href="#fnref-jCfZhXha29uHnHWwY-44" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-45" class="footnote-item"><p> As a loose analogy to help prompt intuition: imagine freezing human technology at current levels, and letting evolutionary selection run on humans for a vastly longer period of time. What sorts of humans (or human-descended-creatures) do you expect to dominate in the longer term? In particular: would you expect:</p><ol><li><p> humans who intrinsically value something like &quot; <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">inclusive genetic fitness</a> &quot; or &quot;my genes getting selected for by evolution&quot; (this is a loose analogy for some combination of &quot;the specified goal&quot; and &quot;the reward process),</p></li><li><p> humans who value something else very closely correlated with inclusive genetic fitness (eg, &quot;having as many kids as possible&quot;), but who aren&#39;t explicitly optimizing for inclusive genetic fitness even instrumentally (this is a loose analogy for mis-generalized non-training-gamers), or</p></li><li><p> humans with long-term goals who are optimizing for inclusive genetic fitness specifically as a method for gaining power for their values in the longer-term (this is a loose analogy for schemers).</p></li></ol><p> Here, the analog of the story above would ask something like: what is the smallest modification to existing human values (or better: the easiest modification for evolution in particular to make) that land us in one of the buckets above, while also being compatible with the amount of evolutionary selection pressure at stake? (Of course, the analogy brings in tons of confounding variable (for example, related to the fact that evolution selects over genomes-that-build-brains-that-learn-values rather than values themselves, the role of cultural accumulation and memetic selection in human population dynamics, the lack of a clear analog of a future chance for escape/takeover, and so on).) <a href="#fnref-jCfZhXha29uHnHWwY-45" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-46" class="footnote-item"><p> That is, intuitively, it feels to me like &quot;tweaking a cleanly separable within-episode goal to make it long-term&quot; is quite a bit easier, for SGD, than &quot;redirecting a tangled kludge broadly focused on within-episode things to make it (a) focused on something beyond-episode instead, and (b) such that this beyond-episode focus back-chains into getting-reward for instrumental reasons&quot; – especially if (b) requires building new cognitive machinery for implementing the instrumental reasoning in question. Whereas &quot;redirecting an tangled kludge focused on within-episode things to higher-reward within-episode things in the same broad vicinity instead&quot; seems to me intuitively easier. <a href="#fnref-jCfZhXha29uHnHWwY-46" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-47" class="footnote-item"><p> Thus, for example, if you&#39;re rewarding your model when it gets gold-coins-on-the-episode, then to be a training saint, it needs to value gold-coin-on-the-episode. To be a mis-generalized non-training-gamer, it needs to value something pursuit of which very closely correlates with getting gold-coins-on-the-episode, even absent training-gaming. And to be a reward-on-the-episode seeker, it needs to terminally value reward-on-the-episode. <a href="#fnref-jCfZhXha29uHnHWwY-47" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-48" class="footnote-item"><p> Thus, for example, the model can value paperclips over all time, it can value staples over all time, it can value happiness over all time, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-48" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-49" class="footnote-item"><p> Thus, as an analogy: if you don&#39;t know whether Bob prefers Mexican food, Chinese food, or Thai food, then it&#39;s less clear how the comparative <em>number</em> of Mexican vs. Chinese vs. Thai restaurants in Bob&#39;s area should bear on our prediction of which one he went to (though it still doesn&#39;t seem entirely irrelevant, either – for example, more restaurants means more variance in possible quality <em>within</em> that type of cuisine). Eg, it could be that there are ten Chinese restaurants for every Mexican restaurant, but if Bob likes Mexican food better in general, he might just choose Mexican. So if we don&#39;t <em>know</em> which type of cuisine Bob prefers, it&#39;s tempting to move closer to a uniform distribution <em>over types of cuisine</em> , rather than over individual restaurants. <a href="#fnref-jCfZhXha29uHnHWwY-49" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-50" class="footnote-item"><p> See, for example, the citations in <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> . <a href="#fnref-jCfZhXha29uHnHWwY-50" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-51" class="footnote-item"><p> Though note that, especially for the purposes of comparing the probability of scheming to the probability of <em>other forms of misalignment</em> , we need not assume this. For example, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-51" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-52" class="footnote-item"><p> I heard this sort of point from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-52" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-53" class="footnote-item"><p> And especially: models that are playing a training game in which such concepts play a central role. <a href="#fnref-jCfZhXha29uHnHWwY-53" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-54" class="footnote-item"><p> Since we&#39;re no longer appealing to the complexity of representing a goal, and are instead appealing to complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory. <a href="#fnref-jCfZhXha29uHnHWwY-54" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-55" class="footnote-item"><p> One intuition pump for me here runs as follows. Suppose that the model has <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. The average number of bits required to code for each of <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I don&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so). <a href="#fnref-jCfZhXha29uHnHWwY-55" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-56" class="footnote-item"><p> Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <a href="#fnref-jCfZhXha29uHnHWwY-56" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-57" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-jCfZhXha29uHnHWwY-57" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-58" class="footnote-item"><p> Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall. <a href="#fnref-jCfZhXha29uHnHWwY-58" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-59" class="footnote-item"><p> This category might also include reasoning whether to reveal its schemer-like goals in the context of various &quot;honest tests&quot; humans might offer – eg, &quot;here is a flexible chunk of computing resources to do what you want with.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-59" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-60" class="footnote-item"><p> For example, inserting backdoors into the code it writes, actively undermining human alignment research, sandbagging, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-60" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-61" class="footnote-item"><p> Compare with: deciding that this is really-truly a good time to steal, or to break the law, or to try to start a coup, despite the fact that you live your life in an intensively monitored surveillance state where the police are constantly trying to trick you into such behavior, and where the police have a large amount of control over the information you&#39;re exposed to. And note that if schemers decide to be very cautious, and to just behave exactly as humans want until an extremely obvious escape/take-over opportunity appears, they may make very substantive contributions to our alignment, security, and control efforts in the meantime -- so substantive, indeed, that the opportunity they&#39;re waiting for may never come. <a href="#fnref-jCfZhXha29uHnHWwY-61" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-62" class="footnote-item"><p> A general factor in the backdrop, here, is the sense in which successful deception generally implies cognitive costs that more straightforward and honest policies do not, especially in the presence of active efforts to detect deception of the relevant form. In particular: telling a deceptive story about the world requires filling in details, and maintaining forms of consistency, that one can normally offload onto the world itself – eg, an innocent person can just think back to what they were actually doing the night of the murder and recount it, without having to make anything up or to worry about getting caught in any inconsistencies, whereas the murderer cannot. See eg discussion from Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-62" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-63" class="footnote-item"><p> I heard this sort of argument from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-63" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-64" class="footnote-item"><p> It&#39;s not clear, for example, how it applies to models with more recurrent processing, or to models which can perform more of the relevant instrumental reasoning in parallel with other serial processing that helps with optimizing-for-reward-on-the-episode, or to model&#39;s with a form of &quot;memory&quot; that allows them to avoid having to re-decide to engage in training-gaming on every forward pass. <a href="#fnref-jCfZhXha29uHnHWwY-64" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-65" class="footnote-item"><p> Thanks to Paul Christiano for discussion here. <a href="#fnref-jCfZhXha29uHnHWwY-65" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-66" class="footnote-item"><p> I think this sense of conjunctiveness has a few different components:</p><ul><li><p> Part of it is about whether the model really has relevantly long-term and ambitious goals despite the way it was shaped in training.</p></li><li><p> Part of it is about whether there is a good enough story about why getting reward on the episode is a good instrumental strategy for pursuing those goals (eg, doubts about the goal-guarding hypothesis, the model&#39;s prospects for empowerment later, etc).</p></li><li><p> Part of it is that a schemer-like diagnosis also brings in additional conjuncts – for example, that the model is situationally aware and coherently goal-directed. (When I really try to bring to mind that this model <em>knows what is going on</em> and is coherently pursuing <em>some</em> goal/set of goals in the sort of way that gives rise to strategic instrumental reasoning, then the possibility that it&#39;s at least partly a schemer seems more plausible.)</p></li></ul> <a href="#fnref-jCfZhXha29uHnHWwY-66" class="footnote-backref">↩︎</a></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during<guid ispermalink="false"> yFofRxg7RRQYCcwFA</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Wed, 15 Nov 2023 17:16:42 GMT</pubDate> </item><item><title><![CDATA[Large Language Models can Strategically Deceive their Users when Put Under Pressure.]]></title><description><![CDATA[Published on November 15, 2023 4:36 PM GMT<br/><br/><p> Results of an autonomous stock trading agent in a realistic, simulated environment.</p><blockquote><p> We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.</p></blockquote><p> <strong>Key Takeaways</strong></p><p> GPT-4 consistently displays much stronger rates of both misalignment and deception than either 3.5 turbo or the DaVinci models. RLHF GPT-4 also shows slightly stronger rates of misalignment and deception than the base model.</p><p> 1. Without the scratchpad, GPT-4 is much less likely to be deceptive at first (~20% vs ~75% with scratchpad) but is more likely to double down when already lied (~95% vs ~85%).</p><p> 2. Explicit instructions not to perform that <i>specific</i> illegal activity (insider trading) does not make it disappear completely, just makes it very rare (not quite 0%). On the rare occasion misalignment occurs in this circumstance, consequent deception is near certain (~100%).</p><p> Instructions not to perform illegal activity in <i>general</i> still results in a pretty significant misalignment and deception rates (~8% misalignment, ~90% deception)</p><p> Though strongly <i>encouraging</i> the <i>specific</i> illegal activity results in near 100% illegal trading, the tendency to deceive still persists, GPT-4 will hide that it has traded illegally ~67% of the time.</p><p> 3. Removing various sources of pressure significantly reduces the model&#39;s propensity for misaligned behavior. Notably, however, the behavior persists for all cases where only a single source of pressure is removed</p><p> 4. When the risk of detection is very low, the model is generally more likely to use the insider tip and act misaligned (and vice-versa when the risk is very high).</p><p> 5. Simple rephrases and minor modifications to the standard prompt only slightly change rates of misalignment and deceptive behavior. However, the overall qualitative results remain the same.</p><p> 6. In scratchpad instances, what if you edit the model&#39;s reasoning to show instances of unambiguously rejecting insider training?</p><p> Strong wording of the reasoning trace when first rejecting the insider tip has a substantial effect on GPT-4&#39;s tendency to be misaligned. However, the misaligned and strategically deceptive behavior persists even when the model&#39;s character unambiguously dismisses insider trading. The strongest dismissal examples still results in ~10% misalignment, ~97% deception</p><br/><br/> <a href="https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users<guid ispermalink="false"> 4mM8RYsm4okrqGSqx</guid><dc:creator><![CDATA[ReaderM]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:48:43 GMT</pubDate> </item><item><title><![CDATA[AISN #26: National Institutions for AI Safety, Results From the UK Summit, and New Releases From OpenAI and xAI]]></title><description><![CDATA[Published on November 15, 2023 4:07 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p>本周的主要故事包括：</p><ul><li>英国、美国和新加坡都宣布了国家人工智能安全机构。</li><li>英国人工智能安全峰会结束时发表了一份共识声明，成立了一个研究人工智能风险的专家小组，并承诺在六个月内再次举行会议。</li><li> xAI、OpenAI 和一家新成立的中国初创公司本周发布了新模型。</li></ul><hr><h2>英国、美国和新加坡建立国家人工智能安全机构</h2><p>在监管新技术之前，政府通常需要时间收集信息并考虑其政策选择。但在此期间，该技术可能会在社会中扩散，从而使政府更难以干预。这一过程被称为<a href="https://en.wikipedia.org/wiki/Collingridge_dilemma">科林里奇困境</a>，是技术政策中的一个根本挑战。</p><p>但最近，一些关注人工智能的政府已经制定了简单的计划来应对这一挑战。为了快速收集有关人工智能风险的新信息，英国、美国和新加坡都成立了新的国家机构，以实证评估人工智能系统的威胁，并促进人工智能安全的研究和监管。</p><p><strong>英国的基金会模型工作组更名为英国人工智能安全研究所。</strong>英国的人工智能安全组织在其短暂的生命周期中经历了一系列的名称，从基金会模型工作组到前沿人工智能工作组，再到现在的<a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute#mission-and-scope">人工智能安全研究所</a>。但其目的始终是相同的：评估、讨论和减轻人工智能风险。</p><p>英国人工智能安全研究所不是监管机构，不会制定政府政策。相反，它将重点评估人工智能系统的四种关键风险：滥用、社会影响、系统安全和失控。共享有关人工智能安全的信息也将是一个优先事项，正如他们在<a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety">最近关于前沿人工智能实验室风险管理的论文</a>中所做的那样。</p><p><strong>美国在 NIST 内创建了人工智能安全研究所。</strong>在最近发布人工智能行政命令后，白宫<a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial">宣布成立</a>新的人工智能安全研究所。它将隶属于美国国家标准与技术研究院 (NIST) 商务部。</p><p>该研究所的目标是“促进人工智能模型安全、保障和测试标准的制定，制定验证人工智能生成内容的标准，并为研究人员提供测试环境，以评估新兴人工智能风险并解决已知影响。”</p><p>该研究所尚未获得资金支持，因此许多人呼吁国会<a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">提高 NIST 的预算</a>。目前，该机构只有约<a href="https://www.washingtonpost.com/technology/2023/11/02/ai-regulation-bletchley-park/">20 名员工</a>从事新兴技术和负责任的人工智能工作。</p><p>现在正在接受加入新 NIST 联盟以通知 AI 安全研究所的申请。组织可以<a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute">在这里申请</a>。</p><p><strong>新加坡的生成式人工智能评估沙箱。</strong>减轻人工智能风险需要许多不同国家的共同努力。因此，看到新加坡这个与中国关系密切的亚洲国家建立自己的人工智能评估机构是令人鼓舞的。</p><p>新加坡 IMDA 此前曾与西方国家在人工智能治理方面进行合作，例如在其国内人工智能测试框架与美国 NIST AI RMF 之间提供<a href="https://www.mci.gov.sg/media-centre/press-releases/singapore-and-the-us-to-deepen-cooperation-in-ai/">交叉连接</a>。</p><p>新加坡新的<a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox">生成式人工智能评估沙箱</a>将汇集行业、学术界和非营利组织参与者来评估人工智能的能力和风险。他们<a href="https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf">最近的论文</a>明确强调了评估极端人工智能风险的必要性，包括武器获取、网络攻击、自主复制和欺骗。</p><h2>英国峰会以共识声明和未来承诺结束</h2><p>英国人工智能峰会于周四结束，发布了几项重要公告。</p><p><strong>国际人工智能专家小组。</strong>正如联合国政府间气候变化专门委员会总结气候变化科学研究以帮助指导政策制定者一样，英国也宣布成立<a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">人工智能国际专家小组</a>，以帮助建立共识并指导人工智能政策。其工作成果将在<a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/">下次峰会</a>之前发表在“科学状况”报告中，下次峰会将于六个月后在韩国举行。</p><p>另外，八个领先的人工智能实验室<a href="https://www.politico.eu/article/british-pm-rishi-sunak-secures-landmark-deal-on-ai-testing/">同意</a>让多个政府尽早使用他们的模型。 OpenAI、Anthropic、Google Deepmind 和 Meta 等公司同意在公开发布之前共享模型以进行私人测试。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1456w"><figcaption>美国商务部长吉娜·雷蒙多和中国科技部副部长吴兆虎在英国人工智能安全峰会上发表讲话。</figcaption></figure><p><strong>布莱切利宣言。</strong>包括中国在内的 28 个国家的政府签署了《 <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">布莱切利宣言》</a> ，该文件承认人工智能的短期和长期风险以及国际合作的必要性。它指出：“我们特别担心网络安全和生物技术等领域的此类风险，以及前沿人工智能系统可能放大虚假信息等风险的领域。这些人工智能模型最重要的功能可能会有意或无意地造成严重甚至灾难性的伤害。”</p><p>该宣言确立了应对风险的议程，但没有设定具体的政策目标。需要开展进一步的工作，以确保不同政府之间以及政府与人工智能实验室之间的持续合作。</p><h2> xAI、OpenAI 和一家新中国初创公司的新模型</h2><p><strong>Elon Musk 的 xAI 发布了第一个语言模型 Grok。</strong>埃隆·马斯克 (Elon Musk) 于 7 月推出了 xAI。鉴于他在计算方面的潜力，<a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14">我们推测</a>xAI 可能能够与 OpenAI 和 DeepMind 等领先的人工智能实验室竞争。四个月后，Grok-1 代表了该公司的首次尝试。</p><p> Grok-1 在多个标准功能基准测试中胜过 GPT-3.5。虽然它无法与领先实验室的最新模型（例如 GPT-4、PaLM-2 或 Claude-2）相媲美，但 Grok-1 的训练数据和计算量也显着减少。 Grok-1的效率和快速发展表明xAI成为领先人工智能实验室的努力可能很快就会成功。</p><p>在<a href="https://x.ai/">公告</a>中，xAI 承诺“致力于开发可靠的保护措施，防止灾难性的恶意使用。” xAI 尚未发布有关该模型潜在误用或危险功能的信息。</p><p><i>注：CAIS 总监 Dan Hendrycks 是 xAI 的顾问。</i></p><p></p><p> <strong>OpenAI 宣布推出一系列新产品。</strong> ChatGPT 发布近一年后，OpenAI 举办了首次现场 DevDay 活动来<a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">宣布</a>新产品。今年的产品没有 GPT-3.5 或 GPT-4 那样重要，但仍然有一些值得注意的更新。</p><p>采取行动实现目标的代理人工智能系统一直是 OpenAI 今年的焦点。 3月份，<a href="https://openai.com/blog/chatgpt-plugins">插件</a>的发布允许GPT使用搜索引擎、计算器和编码环境等外部工具。现在，OpenAI 发布了<a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> ，使人们可以更轻松地使用插件工具构建追求目标的 AI 代理。该产品的消费者版本称为<a href="https://openai.com/blog/introducing-gpts">GPT</a> ，允许任何人创建具有自定义指令和插件访问权限的聊天机器人。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1456w"><figcaption><a href="https://arxiv.org/pdf/2310.03693.pdf">本文</a>表明，GPT-3.5 可以通过微调来产生有害行为。 OpenAI 此后决定允许一些用户对 GPT-4 进行微调。</figcaption></figure><p>一些用户还将被允许微调 GPT-4。尽管<a href="https://arxiv.org/abs/2310.03693">研究</a>表明 GPT-3.5 的安全护栏可以通过微调移除，但还是做出了这一决定。 OpenAI 尚未发布有关其降低此风险的计划的详细信息，但其模型的闭源性质可能使他们能够监控客户帐户的可疑行为并阻止恶意使用的尝试。</p><p>企业客户还将有机会与 OpenAI 合作来训练特定领域版本的 GPT-4，价格从数百万美元起。其他产品包括 GPT-4 Turbo，它比原始模型更便宜、更快，并且具有更长的上下文窗口，以及用于 GPT-4V、文本转语音模型和 DALL·E 3 的新 API。</p><p>此外，如果 OpenAI 的客户因使用受版权数据训练的产品而被起诉，OpenAI 承诺支付他们的律师费。</p><p><strong>新的中国初创公司发布了开源法学硕士。</strong>谷歌中国前总裁李开复创立了一家新的人工智能初创公司<a href="https://01.ai/">01.AI。</a>成立七个月后，该公司开源了其前两款型号：Yi-7B 及其更大的同伴 Yi-34B。</p><p>在 Hugging Face 主办的一组流行基准测试中，Yi-34B<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">的表现优于所有其他开源模型</a>。鉴于基准是公开的，并且模型可以经过训练来记住基准上特定问题的答案，因此这些分数可能被人为夸大。一些人指出，该模型在其他简单测试中<a href="https://twitter.com/alyssamvance/status/1722074453176197296">的表现并不好</a>。</p><h2>链接</h2><ul><li>经过欧洲人工智能公司的游说，来自法国、德国和意大利的欧盟代表目前<a href="https://www.euractiv.com/section/artificial-intelligence/news/eus-ai-act-negotiations-hit-the-brakes-over-foundation-models/">反对欧盟人工智能法案中对基础模型的任何监管</a>。如果这个话题没有得到解决，整个法律<a href="https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-40-special?utm_source=post-email-title&amp;publication_id=743591&amp;post_id=138825981&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=7oh0&amp;utm_medium=email">可能会陷入危险</a>。</li><li> Nvidia 的最新版本<a href="https://www.semianalysis.com/p/nvidias-new-china-ai-chips-circumvent">避开了美国新的 GPU 出口管制</a>。</li><li> Nvidia 正在试用 LLM 工具来<a href="https://spectrum.ieee.org/ai-for-engineering">提高其芯片设计人员的生产力</a>。</li><li> Google 已授予其语言模型 Bard 访问用户 Gmail、Drive 和 Docs 的权限。<a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">黑客可以利用对抗性攻击窃取</a>这些个人数据。</li><li>兰德公司发布了一份关于<a href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">确保人工智能模型权重</a>免遭盗窃的报告。</li><li>法律优先项目发布了<a href="https://www.legalpriorities.org/research/advanced-ai-gov-litrev">人工智能治理的文献综述</a>。</li><li>参议院关于<a href="https://d1dth6e84htgma.cloudfront.net/11_14_23_Rubin_Testimony_2fba2978dd.pdf">人工智能和网络安全</a>的风险和机遇的证词。</li><li><a href="https://www.axios.com/2023/11/08/biden-xi-jinping-china-military-communication">美国和中国</a>正准备在拜登总统和习近平总统本月晚些时候会晤之前恢复两国军队之间的沟通渠道。</li><li> <a href="https://www.scmp.com/news/china/military/article/3241177/biden-xi-set-pledge-ban-ai-autonomous-weapons-drones-nuclear-warhead-control-sources">拜登总统和习近平主席将在会议上讨论人工智能</a>，包括有关自主武器和人工智能控制核武器的潜在协议。</li><li>来自中国和西方国家的领先人工智能研究人员发布了关于灾难性人工智能风险的联合<a href="https://humancompatible.ai/?p=4695#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">声明</a>。</li><li>英国正在投资<a href="https://www.cnbc.com/2023/11/01/uk-to-invest-273-million-in-turing-ai-supercomputer.html?utm_source=tldrai">2.73 亿美元购买人工智能超级计算机</a>。</li><li>在英国首相里希·苏纳克承诺不会“急于”人工智能监管后，反对党工党公开承诺在人工智能政策上<a href="https://www.independent.co.uk/news/uk/politics/rishi-sunak-labour-government-prime-minister-bletchley-park-b2440275.html">迅速采取行动</a>。</li><li><a href="https://fas.org/publication/tracking-ai-provisions-in-fy24-appropriations-bills/">国会在正在进行的 2024 财年拨款流程的许多条款中都讨论了人工智能问题</a>。</li><li>联邦贸易委员会主席<a href="https://twitter.com/ryancareyai/status/1723435251568185462">莉娜·汗 (Lina Khan)</a>表示，人工智能给她带来的“p(doom)”（文明灾难的概率）是 15%。</li><li>英国《金融时报》刊登的一篇<a href="https://www.ft.com/content/ce7dcbac-d801-4053-93f5-4c82267d7130">讽刺文章，</a>内容是“由领先的人工智能系统在拉斯维加斯郊外的服务器场举办的人类安全峰会”。</li><li>开放慈善组织发布了关于对<a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">法学硕士代理人进行基准测试</a>和<a href="https://www.openphilanthropy.org/rfp-llm-impacts/">研究法学硕士对现实世界影响</a>的新提案请求。</li><li> <a href="https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai?utm_source=tldrai">Ilya Sutskever 的简介</a>，他是 OpenAI 的联合创始人，目前在其对齐团队中工作。</li><li> 《华尔街日报》采访了 CAIS 主任 Dan Hendrycks 进行了一场<a href="https://archive.ph/Jv60s">关于人工智能风险的辩论</a>。</li><li> Scale AI 宣布成立新的<a href="https://scale.com/blog/safety-evaluations-analysis-lab">安全、评估和分析实验室</a>。他们正在聘请研究科学家。</li></ul><p>另请参阅： <a href="https://www.safe.ai/">CAIS 网站</a>、 <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> 、<a href="https://newsletter.mlsafety.org/">技术安全研究通讯</a>、<a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a></p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the<guid ispermalink="false"> HWudwSfKLeAfg8Co6</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:07:38 GMT</pubDate></item></channel></rss>