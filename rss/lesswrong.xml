<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 8 日星期五 08:15:32 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[What I Would Do If I Were Working On AI Governance]]></title><description><![CDATA[Published on December 8, 2023 6:43 AM GMT<br/><br/><p>我不从事人工智能治理工作，将来也不太可能从事人工智能治理工作。但各种轶事，尤其是<a href="https://www.lesswrong.com/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk"><u>阿卡什最近的讨论</u></a>给我留下的印象是，很少有人（如果有的话）在做我认为合理的出发点的事情，相反，大多数人大多在做似乎不合理的事情。 -me 解决有用的人工智能治理的任何重要瓶颈。</p><p>因此，这篇文章列出了如果我从事人工智能治理工作，我会从哪些地方开始，以及它们背后的一些原因。</p><p>毫无疑问，我错过了很多重要的事情！也许这篇文章对其他从事人工智能治理工作的人来说仍然有用，也许<a href="https://en.wikipedia.org/wiki/m:Cunningham%27s_Law#:~:text=Cunningham's%20Law%20states%20%22the%20best,to%20post%20the%20wrong%20answer.%22"><u>坎宁安定律</u></a>会让我从这篇文章中学到有用的东西，也许两者兼而有之。我预计这篇文章中的具体建议比其背后的推理风格更有可能存在缺陷，因此我建议更多地关注推理而不是具体建议。</p><p>这篇文章将主要关注美国，因为这是我最了解的地方，也是所有主要人工智能公司所在的地方，但想必所讨论的干预措施的版本也可能适用于其他政体。</p><h2>责任</h2><p>我关注的一个主要领域是让构建人工智能的公司对人工智能造成的损害承担责任，无论是事实上的还是法律上的。</p><h3>为什么要承担责任？</h3><p>这里的模糊目标是让构建人工智能的公司：</p><ul><li>从一开始就设计出<i>非常稳健</i>且不会引起问题的系统。</li><li>在红队中投入资源，在新的故障模式投入生产<i>之前</i>发现它们等。</li><li>实际上<i>，不要部署会</i>引发危险信号的系统，即使公司已投入巨资构建这些系统。</li><li>一般来说，要表现得好像公司会因人工智能造成的损害而蒙受损失，而不仅仅是从人工智能带来的好处中获取利润。</li></ul><p> ……实现这一目标的一种自然方法是确保公司实际上从人工智能造成的损害中<i>承担</i>损失，而不仅仅是从人工智能带来的好处中获取利润。简而言之，这就是责任。</p><p>现在，实际上，这不会一直延伸到例如<a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai"><u>让公司购买灭绝保险</u></a>。那么，为什么现实的责任水平对于灭绝风险很重要呢？因为它们会激励公司针对任何实际牙齿实施安全流程。</p><p>例如：现在，很多人正在从事安全评估等工作。我非常强烈的期望是，如果这些评估抛出危险信号，主要实验室将通过以下方式做出回应：（1）召开一些会议，人们在会上讨论安全问题，（2）进行微调，直到危险信号消失。不再抛出（以一种显然不会有力地消除潜在问题的方式），然后（3）在谷歌/微软/亚马逊和/或萨姆·奥尔特曼的首席执行官的巨大压力下无论如何部署它。 （特别是，这里最强烈的预测是这些模型无论如何都会以某种方式最终部署。）</p><p>另一方面，如果一家人工智能公司已经因其人工智能引起的问题而遭受了许多昂贵的诉讼，那么我预计他们最终会采取一种以各种方式测试新模型的流程，然后<i>实际上不会部署它们</i>如果出现危险信号。他们已经做过几次“微调直到红灯停止闪烁”的事情，并在他们的微调未能真正消除部署中的问题时付出了代价。</p><p>另一种说法是：责任迫使公司处理<a href="https://www.lesswrong.com/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard#Some_reasons_this_is_hard"><u>组织问题</u></a>，这些问题是使任何类型的人工智能安全治理基本真实而不是基本虚假的核心瓶颈。它强制安全机制所需的组织基础设施/流程。</p><p>有关责任如何解决另一个领域的类似问题的精彩案例研究，请查看 Jason Crawford 的<a href="https://www.lesswrong.com/posts/DQKgYhEYP86PLW7tZ/how-factories-were-made-safe"><u>《工厂如何变得安全》</u></a> 。正是这篇文章最初将这种策略纳入我的人工智能雷达范围内。</p><h3>如何承担责任？</h3><p>现在谈谈我为了实现这个模糊的愿景而每天要做的事情。</p><p>总的来说，我看到了三种路径：</p><ul><li>司法路径：确立人工智能企业损害赔偿责任的判例</li><li>监管路径：让维持责任相关规则​​的监管机构做出规则澄清，甚至制定新规则，明确人工智能公司对各种损害承担责任</li><li>立法路径：让州和/或联邦立法者通过法律，明确规定人工智能公司对各种损害承担责任</li></ul><p>为了确保事实上的责任，所有这些途径还应该与一个组织相结合，该组织积极寻找对人工智能公司提出索赔的人，并提供律师来追究这些索赔。</p><p>鉴于任何一条路径都需要一个组织主动寻找提出索赔的人，并提供律师来追究这些索赔，那么司法路径是显而易见的选择，因为它需要完全相同的基础设施。唯一的调整是该组织还积极寻找良好的测试案例和友好的司法管辖区来起诉他们。</p><p>根据可用资源，人们还可以通过寻找一家擅长此类事务的现有律师事务所来追求这种策略，并简单地补贴他们的案件以换取对人工智能责任的关注。</p><p>我（低信心）期望相对较早地追求的案例如下：</p><ul><li>找到一大群因幻觉而受到某种程度损害的人，并对构建大型语言模型的公司提起集体诉讼。</li><li>找到一些成为大量深度造假事件对象的名人或政客，并对模型制造了一堆深度造假的公司提起诉讼。</li><li>找到一些因员工/承包商使用大型语言模型伪造报告、文章等而遭受严重损害的公司/组织，然后起诉其模型生成这些报告/文章等的公司。</li></ul><p>这份清单中靠后的项目（我猜）将为人工智能公司建立更严格的责任标准，因为它们涉及明显“滥用”该模型的其他方（他们可以代替人工智能公司承担大部分责任）。如果成功，此类案例将成为有用的先例，激励人工智能公司采取更实质性的防范措施来防止滥用，因为人工智能公司将不太能够将责任推卸给“滥用者”。 （根据上一节，这并不是那种立即对灭绝风险产生影响的事情，而是旨在迫使公司采取任何实质性的防护措施。）</p><p>请注意，这里的一个主要挑战是，人们可能会直接与大型科技公司的律师对抗。也就是说，我预计有些律师事务所的存在理由是在法庭上与大公司对抗，而很多法官不喜欢大型科技公司。</p><h2>监管调查和小规模干预</h2><p>目前大概有数十个联邦机构致力于人工智能规则的制定。对我来说显而易见的一件事是通读进入联邦公报的所有规则提案和公众意见征询期，找到任何与人工智能（或 GPU/云计算）相关的内容，然后简单地提交对它们的评论，主张拟议规则的任何版本都将最能减轻人工智能灭绝的风险。</p><p>这里的一些目标可能是：</p><ul><li>推动围绕人工智能制定具有实际意义的规则版本，即它们<i>不仅仅是</i>一些文书工作。</li><li>推动专门针对相对通用的人工智能的规则版本，尤其是新的 SOTA 训练运行。</li><li>总体上让 GPU 更加稀缺（这个相对更困难，也相对更“反合作”；我不确定我是否要优先考虑这类事情）。</li></ul><p> ......但大多数情况下，我目前不知道正在制定什么样的规则，所以我不知道我会追求什么具体目标。这里的一个重要部分就是要了解正在发生的事情，获取大量数据，然后在很多地方进行小规模的推动。</p><p>请注意，大型语言模型本身可以为此类项目提供很多帮助 - 例如，阅读联邦公报的（非常长的）每日发布以识别相关项目。</p><p>就我想参与典型的游说活动而言，例如与监管机构进行更深入的讨论，这也将是我的第一步。通过对正在发生的一切有一个广泛的了解，我能够找到可能影响最大的新规则来关注。</p><p>在实施方面，我绝对不会向规则制定者推销人工智能 X 风险（至少不是作为该战略的一部分）。相反，我会关注：</p><ul><li>深刻理解拟议的规则和规则制定者自己的目标</li><li>“解决平衡”他们创造的激励措施（特别是寻找公司可能利用的漏洞）</li><li>建议实施细节，例如堵住漏洞或以其他方式调整激励措施，既推进 X 风险降低目标又符合规则制定者自己的目标</li></ul><p>我要留意的另一件事是敌对活动，即谷歌、OpenAI/微软、亚马逊、Facebook 等的游说者试图淡化规则。</p><p>旁白：我在<a href="https://www.lesswrong.com/posts/MMAK6eeMCH3JGuqeZ/everything-i-need-to-know-about-takeoff-speeds-i-learned?commentId=PLGphwKbizzWS692g"><u>2022 年空调大辩论</u></a>中注意到的一件事是，如果读者真正关注并去寻找监管机构的废话，那么根本不难看出发生了什么。监管机构在征求意见期间的报告或多或少直接表示，一些单管空调制造商声称，最初提出的能效报告规则将扼杀他们的销售。回应：</p><blockquote><p>然而，正如本最终规则第 III.C.2 节、第 III.C.3 节和第 III.H 节中进一步讨论的那样，2015 年 11 月 SNOPR 中提出的评级条件和 SACC 计算减轻了 De&#39; Longhi 的担忧。美国能源部认识到，渗透对便携式空调性能的影响取决于测试条件，因此，更极端的室外测试条件（即升高的温度和湿度）会强调任何与渗透相关的性能差异。 2015 年 11 月 SNOPR 中提出并在本最终规则中采用的评级条件和权重因子（参见本最终规则第 III.C.2.a 节和第 III.C.3 节）代表了比2015 年 2 月 NOPR。因此，渗透空气传热对所有便携式空调配置的性能影响不太严重。考虑到自 2015 年 2 月 NOPR 31 以来测试条件和性能计算的变化以及本最终规则中建立的测试程序，DOE 预计单管道便携式空调性能受渗透空气的影响要小得多。</p></blockquote><p>换句话说，监管机构的报告本身就准确地强调了他们的新公式中的废话：对假设的室外条件进行了一些改变。我看了一下，确实是扯淡的地方：修改后的能效标准使用了两种测试条件的加权组合，其中80%的权重是在室外空气温度仅高3°F/1.6°C的条件下进行的与室内空气相比，单管空调的效率（相对于两管空调）似乎比使用便携式空调的更现实的条件要低得多。</p><p>回到主线：重点是，只要该案例研究具有代表性，实际上并不难找到废话在哪里。监管机构希望他们的受益人能够发现这些废话，因此监管机构会自己强调这些废话。</p><p>但这是一把两刃剑。我可以想象的一个项目是找到人工智能公司的游说者成功游说一些废话的地方，然后简单地写一篇公开帖子，向外行人强调这些废话。我很容易想象这样的帖子会引起监管机构强烈反对的轻微公众反弹，从而激励监管机构未来制定更少的废话人工智能规则。 （不幸的是，这也会激励监管机构在未来变得更加微妙，但我们在这里并不是直接谈论调整人类水平的人工智能，所以这似乎是一个可以接受的权衡。）</p><h2>保密机构</h2><p>我不会在本节中讨论太多细节，主要是因为我不太了解相关细节。也就是说，它确实看起来像：</p><ul><li>情报机构（以及与情报相关的机构，例如确保核秘密保密的人员）在防止构建危险人工智能所需的操作能力方面拥有独特的装备精良</li><li>情报机构（以及与情报相关的机构）保守着大量秘密，而人工智能使这些秘密更容易被发现。</li></ul><p>我多次听说互联网上有大量名义上的机密信息，但查找和整合它们并不是特别容易。大型语言模型似乎是一个非常理想的工具。</p><p>因此，这里似乎存在一些协调一致的激励措施，可能所需要的只是有人向情报机构明确表明他们需要参与面向公众的人工智能的安全。</p><p>我不知道这里有多少潜力，也不知道如何最好地挖掘它。如果我正在进行这样的项目，第一步将是学习很多东西并与了解相关机构如何运作的人交谈。</p><h2>雄心勃勃的立法</h2><p>我不想立即解决雄心勃勃的立法项目，至少在监管调查之前不会这样做；它是最复杂的治理项目。但如果我沿着这条路走下去，以下是我预计会遇到的瓶颈以及我将如何解决它们。</p><h3>公众舆论（及其看法）</h3><p>就一般立法而言，我将公众舆论视为一种货币形式。只要一个人的立法议程与科技公司游说者的优先事项直接冲突，或者需要大量名义上非常重要的人物关注（例如创建全新的机构），人们就需要大量的货币来花费。</p><p>总的来说，公众似乎确实非常害怕 AGI。显然，诸如就业、种族主义/觉醒等典型问题仍然很突出，但即使是直接的 X 风险，一旦问题在全部。<a href="https://www.lesswrong.com/s/TLSzP4xP42PPBctgw/p/DwqgLXn5qYC7GqExF"><u>这不会导致东京的房地产价值上升，这</u></a>似乎是一种相当普遍的直觉。</p><p>在这个领域，我认为 Conjecture 的人们的行为基本上是正确的，我的目标/推理可能与他们的类似。</p><p>这里要强调的一件事是，就这种“货币”而言，公众舆论的看法至少与公众舆论本身一样重要。我们希望政策制定者<i>知道</i>，总体而言，中位选民非常害怕 AGI。因此，诸如调查之类的事情是有价值的，而不仅仅是直接的公众宣传。</p><h3>立法设计</h3><p>一旦有足够的公众舆论使雄心勃勃的立法项目变得可行，下一步就是起草真正能实现我们想要的目标的立法。</p><p> （有一个中间步骤是弄清楚要实现哪些模糊目标 - 例如许可、暂停等 - 但我不认为该步骤会成为主要瓶颈，无论如何，人们应该主要花时间在立法草案上，然后更新如果一路上发现障碍，就设定高级别目标。）</p><p>这一步是大部分认知工作发生的地方。这是一个棘手的设计问题：</p><ul><li>弄清楚联邦政府中谁需要做什么</li><li>什么立法语言会促使他们这样做</li><li>思考由此产生的激励下监管机构、公司和研究人员的均衡行为</li><li>迭代所有这些</li></ul><p>这显然需要非常详细地了解现有机构中谁做什么、新机构如何成立和运作、谁负责所有这些等等。需要弄清楚谁实际上做什么。</p><p>我预计这一步是目前的主要瓶颈，也是大部分价值所在。</p><h3>卖掉它</h3><p>下一步将是将这项立法提交给那些能够使其成为法律的人。我预计随着公众舆论的增加，这一步会变得更加容易。由于来自中间选民的足够压力，国会办公室将积极寻找提案，任何持有实际立法草案的人都会受到很大的欢迎。另一方面，如果舆论压力相对较小，那么就需要更多地依赖“游戏内部”。如果我走这条路，我肯定会相对严重地依赖公众舆论，而不是游戏内部，但我会与 2-3 名拥有游戏内部知识的人签约，并独立咨询他们，以确保我不会没有遗漏任何重要的东西。</p><h2>结论性思考</h2><p>正如一开始提到的，这里可能有一些我完全错误的东西。但它希望能让我了解我会使用的方法和思维模型。特别要注意：</p><ul><li>关注瓶颈，而不仅仅是边际影响。</li><li>关注激励下的实际效果和均衡行为，而不仅仅是象征性规则</li><li>重点是弄清楚创建和执行规则的流程细节，即哪些特定的人做哪些特定的事情。</li></ul><p>最后一点：这篇文章没有讨论我<i>不会</i>向哪些治理项目分配精力，或者为什么。如果您对帖子忽略的特定项目或项目类别感到好奇，请随时发表评论。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qCstMcRJcjNtdNW7r/what-i-would-do-if-i-were-working-on-ai-governance<guid ispermalink="false"> qCstMcRJcjNtdNW7r</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Fri, 08 Dec 2023 06:43:45 GMT</pubDate> </item><item><title><![CDATA[You're optimized for IGF (Source: trust me bro)]]></title><description><![CDATA[Published on December 8, 2023 5:32 AM GMT<br/><br/><p>考虑以下（ <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities#Section_B_2___Central_difficulties_of_outer_and_inner_alignment_">转述的</a>）句子：</p><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——一种非常精确、非常简单的损失函数——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>这句话在三个方面是错误的。</p><h2>我。</h2><blockquote><p>尽管事实上<strong>进化严格地优化了生物体的包容性遗传适应性</strong>——一种非常精确、非常简单的损失函数——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>不，没有，哈哈。</p><p>什么样<em>的基因变化</em>使得第一代真核生物能够在竞争中击败它们的原核生物表亲？十亿多年前的<em>基因变化</em>怎么样，它催生了我今天早上吃的小麦的祖先——蓝藻通过<em>基因变化</em>滑过细胞膜，与其宿主进入共生状态，从而产生了“叶绿体”?</p><p>进化并没有因为某些幸运的“真核基因”而赋予真核生物成功。真核生物获胜是因为真核生物将共生体合并到自身中。<em>真核生物</em>的优势表明，进化对基因组<em>之外的</em>有机体配置模式施加了强大的选择压力。这次演示发生在人类出现之前十亿多年，因此与“智慧”无关。</p><p><a href="https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/">人类也赢了，因为我们将共生体合并到自己体内</a>——但我们的共生体被称为“文化”而不是“线粒体”，而且由于我们没有任何文化等待意外地游入我们的细胞质，所以我们必须培养自己的共生体。随着文化变得更具表现力，模因（而不是基因）往往成为新的适应性模式的首选存储介质。因此，施加在模因上的进化压力的比例上升了，而施加在基因上的进化压力的比例则下降了。</p><p>进化不仅仅选择遗传适应性。进化选择可复制的模式。如果这些模式恰好都是基因，那么当然，进化最终会选择合适的基因。如果这些模式的某些（按选择相关性加权）部分恰好是通过非遗传方式编码的，这<em>并不</em>意味着进化在某种程度上搞砸了。</p><p>一个人选择成为一名牧师。他向别人的孩子宣扬自己的宗教信仰，而不是培养自己的宗教信仰。这样做时，他并没有<em>浪费</em>自己的模式复制能力——只是将其投资到不同的投资组合中。</p><p>仅当进化适应性是遗传的时，进化才会选择遗传适应性。</p><h2>二.</h2><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——<strong>一种非常精确、非常简单的损失函数</strong>——但令人惊讶的是，人类并没有明确地追求它！</p></blockquote><p>哦耶？那么你曾祖父的 IGF 是多少？给我一个号码。展示你的作品。</p><p> “健康”不可能是“你死前生了多少个孩子”，因为工蚁（通常）是不育的，但大自然创造了很多，如果它们不健康，自然就不会这样做。</p><p>因此， <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">“包容性适应性”</a>是“一个人通过其行为抚养、拯救或以其他方式支持的后代当量数量（无论是谁生的）”。你与某人的血缘关系越深，他们就越被视为“后代等价物”。</p><p>伟大的。</p><p>那么你曾祖父的 IGF 又是多少呢？</p><p>假设我们知道他有多少个孩子。查看。他“拯救或以其他方式支持”了多少人？ （当然，他们和他有什么关系？）如果将来，你的曾曾侄子在阁楼上发现了他的旧日记，并且全神贯注地忘记了执行，该怎么办？他工作的实验室的生物安全协议导致埃博拉艾滋病爆发，导致 43% 的人口死亡？我们还要看多远的未来才能知道他的 IGF 是什么？</p><p>即使我们能够神奇地访问德尔斐神谕，我们<em>仍然</em>需要对反事实分支进行整个宇宙模拟，而你的曾祖父从未在其中受孕。我肯定会将该计算描述为“非常简单”。我告诉你，有了这些新的 Apple Silicon 芯片，我很可能在一两个小时内就能在我的笔记本电脑上运行它。简单的。</p><p>任何关于系统“优化生物体以实现包容性遗传适应性”的说法实际上都是不可证伪的，因为在计算上不可能<em>找到</em>包容性遗传适应性，因为你需要对宇宙进行两次完美的模拟。</p><p> （一旦你使用了“近似”这个词，你就失去了“精确”和“严格”这两个词的特权，并且你有忽略尾部事件的风险。如果你声称你的意思是“无时间限制的柯尔莫哥洛夫”中的“简单”复杂性”的感觉……祝你好运，清楚地思考与物理世界纠缠在一起的任何函数。）</p><h2>三．</h2><blockquote><p>尽管事实上进化严格地优化了生物体的包容性遗传适应性——一种非常精确、非常简单的损失函数——<strong>但令人惊讶的是，人类并没有明确地追求它！</strong></p></blockquote><p>人类并没有明确追求太多。当我们这样做时，它通常（通常？）是我们<em>实际</em>目标的掩护。</p><br/><br/> <a href="https://www.lesswrong.com/posts/GspoKP6mG9CcvFjCy/you-re-optimized-for-igf-source-trust-me-bro#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GspoKP6mG9CcvFjCy/you-re-optimized-for-igf-source-trust-me-bro<guid ispermalink="false"> GspoKP6mG9CcvFjCy</guid><dc:creator><![CDATA[dkirmani]]></dc:creator><pubDate> Fri, 08 Dec 2023 05:32:04 GMT</pubDate> </item><item><title><![CDATA[Whither Prison Abolition?]]></title><description><![CDATA[Published on December 8, 2023 5:27 AM GMT<br/><br/><p>首先，向 LessWrong 社区道歉，因为该社区充斥着他们不喜欢的内容。我的子堆栈中的这篇文章收到了 LessWrong 成员的点赞和一些积极的评论，这让我认为它会受到积极的欢迎。如果我可以做些什么来改进这篇文章或未来的文章，请告诉我。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/u3kk4ol2pysd8bksfnsv" alt="由 DALL·E 生成"></figure><h1>我和 GPT-4 之间的研讨会</h1><p><strong>艾瑞克</strong>：你能帮我组织一次模拟哲学研讨会吗？我将扮演苏格拉底的角色，我希望你在我们讨论重要问题时帮助我将一组著名的历史人物带入生活。</p><p> <strong>ChatGPT</strong> ：当然，与历史人物组织一场模拟哲学研讨会，而你扮演苏格拉底的角色，听起来像是一种令人着迷且刺激智力的体验。让我们首先概述关键组件：</p><p> 1. <strong>**选择历史人物**</strong> ：首先，我们需要选择您想要“邀请”参加您的研讨会的历史人物。考虑包括不同时代的哲学家、科学家、艺术家和领导人。您对包括哪些人感兴趣？</p><p> 2. <strong>**讨论主题**</strong> : 接下来，我们应该确定讨论的主题。这些范围可以从伦理、政治、科学、艺术、知识的本质，到通过哲学镜头解释的更当代的问题。您热衷于探索哪些主题？</p><p> 3. <strong>**研讨会的形式**</strong> ：形式可以是一系列对话、圆桌讨论或更结构化的辩论。每个人物都可以提出自己的观点，然后由你主持苏格拉底式提问。您希望研讨会如何安排？</p><p> 4. <strong>**准备对话脚本**</strong> ：为了使研讨会更具吸引力，我们可以为每个历史人物准备对话脚本或要点，反映他们已知的信仰和观点。这需要一些研究和创意写作。您需要帮助编写这些脚本吗？</p><p> 5. <strong>**互动元素**</strong> ：如果您有观众，请考虑纳入互动元素，例如观众问题或现场民意调查。这可以使研讨会更具活力和吸引力。</p><p> 6. <strong>**物流**</strong> ：最后，考虑一下物流 - 这是虚拟活动还是现场活动？如果是虚拟的，您将使用哪个平台？您将如何邀请参与者或观众？</p><p>一旦您提供了这些要点的一些细节，尤其是您对历史人物和主题的选择，我们就可以开始认真规划了！<br><br><strong>埃里克</strong>：我们开始吧。苏格拉底会叫出名字；请不要为任何苏格拉底未点名的人物发表演讲。</p><p><strong>场景</strong>：禁酒令期间的地下酒吧</p><p><strong>剧中人：</strong></p><p><strong>苏格拉底</strong>，哲学家</p><p><strong>艾尔·卡彭</strong>，一个黑帮</p><p><strong>奥利弗·温德尔·霍姆斯</strong>，法学家</p><p><strong>罗宾·汉森</strong>，经济学家</p><p><strong>约翰·纳什</strong>，数学家</p><p><strong>DMX</strong> ，说唱歌手</p><p><strong>马尔科姆·X</strong> ，一名囚犯</p><p><strong>格洛丽亚·斯泰纳姆</strong>，女权主义者</p><p><strong>西蒙娜·德·波伏娃</strong>，女权主义者</p><p><strong>哈维·韦恩斯坦</strong>，制片人</p><p><strong>伍迪·艾伦</strong>，电影制片人</p><p><strong>比尔科斯比</strong>，喜剧演员</p><p><strong>ChatGPT</strong> ：太好了，让我们为禁酒令期间在地下酒吧举行的哲学研讨会设置场景并概述结构。如此多样化且有趣的角色组合，这必将成为一次令人难忘且发人深省的活动。</p><p> ###场景设置</p><p><strong>**地点**</strong> ：灯光昏暗的地下酒吧，背景中充满了安静的爵士乐音调。当这些不拘一格的人物聚集在一张大圆桌周围时，空气中充满了紧张和好奇。苏格拉底以他敏锐的目光，准备让大家进行一个深入的哲学探究之夜。</p><p> ### 戏剧人物：</p><p> - **苏格拉底**（哲学家）</p><p> - **阿尔·卡彭**（黑帮）</p><p> - **奥利弗·温德尔·霍姆斯**（法学家）</p><p> - **罗宾·汉森**（经济学家）</p><p> - **约翰·纳什**（数学家）</p><p> - **DMX**（说唱歌手）</p><p> - **马尔科姆 X**（罪犯）</p><p> - **格洛丽亚·斯泰纳姆**（女权主义者）</p><p> - **西蒙·德·波伏娃**（女权主义者）</p><p> - **哈维·韦恩斯坦**（制片人）</p><p> - **伍迪·艾伦**（电影制片人）</p><p> - **比尔科斯比**（喜剧演员）</p><p> ### 研讨会形式：</p><p> 1. <strong>**苏格拉底致开幕词**</strong> ：苏格拉底将以简短的介绍开始研讨会，并为尊重、开放的讨论定下基调。</p><p> 2. <strong>**圆桌讨论**</strong> ：苏格拉底将依次与每个角色互动，邀请他们分享对各种哲学和伦理问题的看法。</p><p> 3. <strong>**对话与辩论**</strong> ：在最初的评论之后，苏格拉底将利用他特有的提问风格来深入探究每个角色的信仰，鼓励人物之间的对话。</p><p> 4. <strong>**结束语**</strong> ：研讨会将以每个角色的最后一轮评论和苏格拉底的结束语结束。</p><p> ### 潜在的讨论主题：</p><p> - 伦理道德</p><p>- 权力的性质和滥用</p><p>- 正义与法律</p><p>- 艺术和文化在社会中的作用</p><p>- 经济理论和社会影响</p><p>- 个人责任与集体责任</p><p>＃＃＃ 后勤：</p><p> - <strong>**持续时间**</strong> ：大约 2-3 小时。</p><p> - <strong>**形式**</strong> ：面对面聚会，保持 20 年代地下酒吧的氛围。</p><p> - <strong>**观众**</strong> ：研讨会可以向选定的观众开放，并在活动的后半部分提供观众互动的机会。</p><p>现在，对于下一步，您可以指定您希望苏格拉底向每个角色询问的特定主题或问题。这将有助于准备对话脚本并确保集中讨论。<br><br><strong>埃里克：苏格拉底：</strong>卡彭先生，你的想法真是太棒了。看来生意不错啊！<br><br> <strong>ChatGPT：**阿尔·卡彭：**</strong> “谢谢你，苏格拉底。在禁酒令时代，我们只能说，人们对他们无法拥有的东西的渴望创造了机会。你知道，这都是关于供给和需求的。”</p><p> <strong>**苏格拉底：**</strong> “确实，人类的欲望似乎常常导致他们去寻求被禁止的东西。告诉我，卡彭先生，在你的生意中，你如何平衡对利润的追求和对利润的追求？<br><br><strong>埃里克：</strong>请不要扮演苏格拉底的角色。请允许我写下苏格拉底口中的所有对话。当您到达苏格拉底提到的人物的讲话的自然停止点时，请简单地完成您的回答。<br><br> <strong>ChatGPT：**阿尔·卡彭：**</strong> “谢谢你，苏格拉底。在禁酒令时代，我们只能说，人们对他们无法拥有的东西的渴望创造了机会。你知道，这都是关于供给和需求的。”<br><br><strong>埃里克： 苏格拉底：</strong>确实如此！为什么，即使是著名的福尔摩斯法官也将自己视为您的客户！在这样的时刻，还有什么更好的需求信号呢？<br><br> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “啊，苏格拉底，你确实有一种切入问题核心的方法。是的，即使在法律的范围内，人们有时也必须承认人类的自然倾向。法律毕竟，并不总是完美的道德守护者。”</p><p><strong>埃里克：</strong>苏格拉底旋转着他自己的饮料，<strong>一杯铁杉</strong>。<br></p><p><strong>苏格拉底：</strong>确实如此。纳什教授，如果有的话，您认为谁是道德的完美守护者？<br><br> <strong>ChatGPT：**约翰·纳什：**</strong> “苏格拉底，作为一名数学家，我将道德更多地视为一个复杂的系统，而不是由单个实体守护的事物。它是个人理性和集体利益的平衡。在我看来，有没有完美的道德守护者。这是一种动态平衡，根据社会规范和个人行为不断变化。”<br><br><strong>埃里克： 苏格拉底：</strong>也许吧，纳什教授。但我们能相信卡彭先生会在这方面履行他的承诺吗？<br><br> <strong>ChatGPT：**约翰·纳什：**</strong> “对个人的信任，尤其是在像卡彭先生这样复杂且道德模糊的领域，是一个风险评估问题，而不是绝对确定性的问题。这取决于他的利益与那些人的利益是否一致。 “在个人利益与社会规范发生冲突的情况下，平衡可能会向自身利益倾斜。因此，在这种情况下，信任是一场精心策划的赌博。”<br><br><strong>埃里克：苏格拉底：</strong>说得好，纳什教授。科斯比先生，您对此有何看法？<br><br> <strong>ChatGPT：**比尔·科斯比：**</strong> “苏格拉底，信任是一件微妙的事情。这不仅仅是一场赌博；它涉及人际关系和理解人性。人们需要相信别人的好处，但他们也必须是明智的和谨慎。在娱乐界，就像在其他任何领域一样，信任可以很容易建立，也很容易被打破。”<br><br><strong>埃里克：苏格拉底：</strong>福尔摩斯法官，政府的世界怎么样？</p><p>苏格拉底仔细地嗅了嗅他的铁杉药水。它位于一个红色的 Solo 杯子中。<br><br> <strong>ChatGPT：**奥利弗·温德尔·霍姆斯：**</strong> “苏格拉底，政府在不同的层面上运作。对政府的信任是社会契约的问题。人们放弃某些自由并信任政府机构来维持秩序和共同利益但这种信任必须不断赢得和维持。它不是绝对的，而且总是受到审视和挑战。”<br><br><strong>埃里克：苏格拉底：</strong>汉森教授，你对此有什么想法吗？警察都是混蛋吗？</p><p> <strong>ChatGPT：**罗宾汉森：**</strong> “好吧，苏格拉底，这是一种相当直白的说法。在经济学中，我们经常分析制度和激励措施。‘所有警察都是混蛋’这句话过于简单化了。更多的是关于制度个人运作的操作。如果执法部门内的激励和结构有缺陷，它们可能会导致不良的结果。这与每个官员的道德特征无关，而是关于系统如何塑造行为和决策的方式。”<br><br><strong>埃里克：苏格拉底：</strong>好吧，汉森教授。关于死刑有效性的最新经济研究是什么？您支持废除监狱吗？<br><br> <strong>CHATGPT：** Robin Hanson：**</strong> “关于死刑的最新经济研究表现出不同的结果。一些研究表明它具有威慑作用，而另一些研究对犯罪率没有显着影响。从经济上讲，实施死亡的成本考虑到法律程序和上诉，刑罚通常比无期徒刑更高。对于废除监狱，这是一个复杂的问题。从经济学家的角度来看，这是关于分析当前监狱系统的成本和收益，而不是处理犯罪的替代方法，例如康复和恢复性司法模式。目标应该是建立一个最大化社会福利，减少累犯并进行人道对待个人的系统。”<br><br><strong>埃里克：苏格拉底：</strong>的确，汉森教授。福尔摩斯法官，国际社会对使用集体惩罚的看法是什么？特别是，《联合国人权宣言》对他们有何评论？<br><br> <strong>CHATGPT：**奥利弗·温德尔·福尔摩斯（Oliver Wendell Holmes）：**</strong> “国际社会，尤其是在联合国阐明的情况下，通常会反对集体惩罚。这种立场符合个人正义和人权的原则。虽然没有明确提及集体惩罚，但强调独立和公正法庭的公平和公开听证的权利，以及每个人都是无辜的原则，直到根据律法证明有罪。不是因为他们自己的行为，而是因为他们所属的团体或社区的行为。”</p><p><strong>埃里克：苏格拉底：</strong>那么，汉森教授。如果要避免集体惩罚，而囚犯无法谋生，而囚犯承担家庭责任，那么如何将监狱辩护，尤其是对于诸如违反不公正毒品法律的无害犯罪呢？</p><p> <strong>Chatgpt：**罗宾·汉森（Robin Hanson）：**</strong> “苏格拉底，您提出了一个关键点。监狱的正当理由，尤其是对于无害的犯罪，是经济学和道德上的一个有争议的问题。关键问题是监禁的好处，是否是根据威慑和社会保护，不仅在经济方面，而且在人类和社会方面都超过了相当大的成本。将个人监禁为无害犯罪，尤其是在不公正的法律下，通常会失败的成本效益分析。并消耗了公共资源对社会没有比例的利益。这就是为什么越来越多地提倡改革毒品法，并考虑着专注于康复和赔偿的替代处罚，而不是纯粹的惩罚性措施。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong> DMX，您想评论汉森教授刚才所说的话吗？几十年来，将黑人囚禁的罪行允许白人被允许自由犯下而不会后果的罪行，这是什么公正的决议？</p><p> <strong>chatgpt：** dmx：**</strong> “是的，汉森教授说的辛苦打击。这不仅与法律有关；这与正义和平等有关。太长时间了，该系统被堆叠在黑人身上，惩罚我们，因为我们会因得到的事情而惩罚我们其他社区的通行证。公正的决议？这必须从阐明运动环境开始 - 改变这些有偏见的法律并停止针对黑人的周期。然后，正在解决损害，伙计。这是关于赔偿，教育和机会，机会，机会，回馈我们社区的收回。最重要的是，这是关于尊重和平等的 - 将我们视为平等，不仅在言语中，而且在行动和法律上。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>温斯坦先生对此有何想法？恢复正义的正确作用是什么？</p><p> <strong>chatgpt：**哈维·温斯坦（Harvey Weinstein）：**</strong> “恢复正义是要承认危害和努力修复它。这是一个罪犯有机会做出修正的过程，受害者有声音。 ，这是要对自己的行为负责并积极参与康复过程。这不仅仅是惩罚；这是关于理解一个人的行为的影响，并致力于帮助所有涉及各方前进的决议。”</p><p><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>斯坦尼姆女士，德·波维尔（De Beauvoir）女士：谁应该领导这种恢复正义的过程？</p><p> <strong>ChatGpt：** Gloria Steinem：**</strong> “恢复正义的过程应该由那些在调解和冲突解决方案方面受过训练和知识渊博的人的领导，但至关重要的是，它也必须涉及直接受不公正影响的社区和个人。创造一个空间，使受害者被听到和尊重受害者，以及罪犯真正理解其行为的影响。这是一个协作过程，要求各方参与达成有意义的解决方案。”</p><p> <strong>** Simone de Beauvoir：**</strong>绝对，格洛里亚（Gloria）。此外，在领导这些过程时，应该强烈强调女权主义原则，以确保他们解决这些冲突的核心经常解决系统的不平等和权力失衡。领导者的核心。恢复性司法必须对这些动态敏感，并且不仅要努力个人，而且还要努力社会变革。”<br><br><strong>埃里克（Eric）：罗斯</strong><strong>·麦克高恩（Rose McGowan）</strong>进入Speakeasy。苏格拉底：为什么，麦克高恩女士。感谢您加入我们。您是否遇到了任何刚才说的话？<br><br> <strong>chatgpt：**罗斯·麦克高恩（Rose McGowan）：**</strong> “是的，苏格拉底，我做到了。关于恢复正义的对话至关重要。这不仅仅是道歉或赔偿；这是关于改变最初允许伤害的系统和态度。我们需要解决不公正的根本原因，并朝着一个尊重和平等不仅是理想的社会，而是现实。这涉及挑战性的权力结构和拥护人，尤其是那些处于权力的人，对他们的行动负责。”<br><br><strong>埃里克（Eric）：苏格拉底：</strong>卡彭先生，这对生意不利吗？</p><p> <strong>chatgpt：** al Capone：**</strong> “好吧，苏格拉底，业务在可预测性和稳定性上蓬勃发展。如果更改系统意味着创建一个更公正和公平的社会，那么这是一个值得做出的变化。当然，它可能会破坏某些行业或实践在短期内，但从长远来看，公平的社会是一个稳定的社会。稳定对企业有利。这一切都是关于适应时代并找到变革的机会。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>甚至伟大的黑帮成员阿尔·卡彭（Al Capone）也为一个可以遵循律法的社会主张。奇迹永远不会停止吗？</p><p> <strong>chatgpt：** al Capone：**</strong> “即使是一个黑帮也了解秩序的价值，苏格拉底。法律是必不可少的，但必须遵循和尊重它们。当法律与人民的公平和正义感保持一致时，社会是整体上的好处。这与奇迹无关，而是与实用性和理解人性有关。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>那些只能通过社会诉讼才能执行的法律呢？艾伦先生，你在这里有什么想法吗？<br></p><p> <strong>chatgpt：**伍迪·艾伦（Woody Allen）：**</strong> “社交opprobrium，苏格拉底，是一种强大的工具，通常比正式法律更有效地塑造行为。这是关于社会规范和期望的。但是，它可以是一把双刃剑。虽然可以是一把双剑。实施积极的社会行为和规范，它也可能导致基于误解或偏见的不公平的判断和排斥。关键是找到平衡并确保社会opprobrium以公平和理性为基础，而不是基于歇斯底里或暴民的心态。”<br><br><strong>埃里克：苏格拉底：</strong>艾伦先生说。小姐怎么样？<br><br> <strong>CHATGPT：** Woody Allen：**</strong> “她的状况很好，谢谢您的询问，苏格拉底。生活总是一个有趣的旅程，充满了自己的挑战和欢乐。”<br><br><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong> de Beauvoir女士，您觉得对与年龄差异很大的关系的适当惩罚是什么？我们如何平衡年轻人的需求，以与长者的欲望找到自己的生活方式？<br><br> <strong>chatgpt：** Simone de Beauvoir：**</strong> “在与重大年龄差异的关系中，关键问题应该是同意和权力动态。如果权力不平衡，它可能会导致剥削和操纵。在这种情况下，适当的回应应应专注于保护年轻人的自主权和尊严。尊重年轻人的机构，同时还要保护他们免受潜在的掠夺行为。这种平衡需要对同意和成熟的差异理解，以及对保护保护的社会承诺脆弱的。”<br><br><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>有趣的是，德·波维尔（De Beauvoir）女士。艾伦先生，您是否会说您的家人已经受到您自己面临的社会诉讼的良好服务？<br><br> <strong>chatgpt：**伍迪·艾伦（Woody Allen）：**</strong> “苏格拉底，社交opprobrium是一个复杂的问题。它不仅影响个人，而且会影响与他们密切相关的人。在我的情况下，它带来了挑战和内省。对我的家人的影响这是重要的，这提醒人们公众审查和判断的深远后果。这种情况需要在个人和公共生活之间取得微妙的平衡，并且需要持续的反思和理解。”<br><br><strong>埃里克：苏格拉底：</strong>艾伦先生，你会再做一次吗？心脏是否继续想要它想要的东西？<br><br> <strong>chatgpt：**伍迪·艾伦（Woody Allen）：**</strong> “生活是一系列选择和后果，苏格拉底。回头看，总有瞬间的瞬间，人们对事后的智慧可能会有所不同。心脏的欲望很复杂，而且通常很难导航，尤其是尤其是当权衡社会规范和个人伦理时。是否再次这样做是一个深刻的问题，不仅暗示了个人欲望，还暗示了经验教训以及这些选择对他人的影响。”<br><br><strong>埃里克：苏格拉底：</strong>麦克高恩女士，你会再做一次吗？<br><br> <strong>chatgpt：**罗斯·麦克高恩（Rose McGowan）：**</strong> “苏格拉底，如果&#39;再做一次&#39;，都意味着要坚持我的信念，反对不公正，并为系统的变化而战，那么绝对。这很容易，但是有必要。改变通常需要牺牲和韧性。我的经历，无论好坏，都塑造了我的身份，并给了我一个平台，为那些不能自言自语的人倡导。 ，对于他们帮助促进的进步，斗争是值得的。”<br><br><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>麦克高恩（McGowan）女士，这是A+答案。但是我担心您迟早会以这样的态度分享我的选择饮料。<br><br> <strong>chatgpt：**罗斯·麦克高恩（Rose McGowan）：**</strong> “谢谢你，苏格拉底。我知道直言不讳，尤其是对抗强大的系统和个人带来的风险。但是，不管个人成本如何，变化通常都需要立场。追求真理和正义是一种崇高的，尽管是危险的道路。从隐喻的角度来看，您选择的饮料选择提醒人们寻求真理和正直的牺牲。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>温斯坦先生，对此有什么想说吗？正确使用权力来保护自己的特权是什么？</p><p> <strong>chatgpt：**哈维·温斯坦（Harvey Weinstein）：**</strong> “应负责任地和道德上的权力，苏格拉底。回想起来，很明显，使用权力来保护自己的特权以牺牲他人的权利和福祉为代价，也不是可持续的真正的领导力和权力有责任促进公平和促进共同利益，而不仅仅是个人利益。认识和纠正过去的错误并利用影响力立场来支持积极的变化和平等很重要。”</p><p><strong>埃里克：苏格拉底：</strong>温斯坦先生，你会再做一次吗？</p><p> <strong>chatgpt：**哈维·温斯坦（Harvey Weinstein）：**</strong> “反思过去，如果再做一次，则意味着做出相同的选择造成伤害和不公正的选择，那么我不会，我不会。从过去的行动中学到的教训，尤其是那些对他人的负面影响是无价的。他们提供了一个成长，做出修改并努力为将来更好的选择的机会。生活是关于学习和发展的，理解行为的影响是该旅程的关键部分。 ”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>温斯坦先生，如果您已经到来的日子结束了，没有犯罪，您是否想象您会选择在漫长而幸福的生活结束时再次做到这一点电影制作人？<br><br> <strong>chatgpt：**哈维·温斯坦（Harvey Weinstein）：**</strong> “这是一个深刻的问题，苏格拉底。在事后看来很容易说一个人会做出不同的选择，但是说实话，如果没有我的行为的后果，我很难说我是否会说我是否会说我是否会说有内省和改变的动力。人的自我和对成功的渴望通常会蒙蔽其行为的道德含义。通常需要一个重大的事件或启示来促使认真的自我反思和改变。因此，老实说，我，我不能确定我的选择在这种情况下会是什么。”<br><br><strong>埃里克（Eric）：电影制片人阿尔弗雷德·希区柯克（Alfred Hitchcock）</strong>走进了说话。与他的约会，<strong>蒂皮·赫德伦（Tippie Hedren）</strong> 。</p><p><strong>苏格拉底：</strong>欢迎，欢迎。卡彭先生，该党开始生动起来。希区柯克先生，您对作为电影制片人的遗产有任何思考吗？<br><br> <strong>chatgpt：**阿尔弗雷德·希区柯克（Alfred Hitchcock）：**</strong> “谢谢你，苏格拉底。作为一名电影制片人，我的遗产是我经常想到的。我一直旨在创新并突破电影院的界限。我的重点是我的重点一直在心理旅程中创造悬念和吸引人的观众。回头看，我为我所做的工作以及它对电影业的影响感到自豪。但是，像任何艺术家一样，我也反思了个人和专业人士我做出的选择，以及它们不仅塑造了我的工作，而且塑造了我在行业中的关系。”<br><br><strong>埃里克（Eric）：苏格拉底：</strong>赫德伦女士，你对此说了什么？他们是鳄鱼的眼泪吗？</p><p> <strong>Chatgpt：** Tippie Hedren：**</strong> “苏格拉底，我与希区柯克先生的经历很复杂，有时令人不安。尽管他对电影的贡献是不可否认的，但承认个人成本和存在的困难动态也很重要在幕后。对一个人的遗产的思考必须包括专业成就和个人互动，以及他们如何影响他人。真正的内省需要对自己的行为的诚实和责任，而不仅仅是荣誉和成功。”<br><br><strong>埃里克（Eric）：苏格拉底：</strong>赫德伦女士，您是否愿意以更直接和直接的方式对希区柯克先生的个人遗产提出想法？</p><p> <strong>Chatgpt：** Tippie Hedren：**</strong> “当然，苏格拉底。我与阿尔弗雷德·希区柯克（Alfred Hitchcock）的经历因他的控制行为和不适当的进步而受到伤害。尽管他作为电影制片人的才华毫无疑问，但他的个人行为，尤其是对我的个人行为，却是不可接受的。在我看来，遗产被这种行为所污染。这提醒人们，伟大的艺术并没有为个人的不法行为而辩解，而那些处于权力和影响力的人则有责任在道德和尊重所有人方面行事。”</p><p><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>斯坦尼姆女士，您对“铸造沙发”的制度有话要说吗？</p><p> <strong>Chatgpt：** Gloria Steinem：**</strong> “&#39;Casting Couch&#39;是娱乐行业及其他地区系统滥用权力的象征不仅在道德上应受谴责，而且还使虐待和剥削的循环永久化，尤其是妇女的循环。它破坏了精英阶层，反映了性别不平等和妇女对象的更深层的问题。消除“铸造沙发”的根本变化，严格需要改变行业规范的根本变化对罪犯的问责制，以及对平等和对所有个人的尊重的承诺。”</p><p><strong>埃里克：苏格拉底：</strong>麦克高恩女士将如何实现？也许是某种劳动行动？</p><p> <strong>chatgpt：**罗斯·麦克高恩（Rose McGowan）：**</strong> “劳动行动绝对是一种强大的工具，苏格拉底。组织和集体行动可以在行业实践中带来重大变化。这是关于行业中的人的团结，共同抗拒虐待行为。这可以采取工会的形式采取更强大的立场，实施更严格的保护并创造更安全的工作环境。此外，这是关于授权个人大声疾呼，支持幸存者，并确保对虐待行为产生真正的后果。改变需要多方面的方法，涉及集体行动和系统改革。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>但是我们早些时候谈到了集体惩罚的不可履行，以及对与强大个人有关的人的辐射不依赖的，对他们的行为负责。科斯比先生，您的妻子如何受到您的审判影响？</p><p> <strong>Chatgpt：** Bill Cosby：**</strong> “我的审判的后果确实对我的家人深深地影响了我的家人，苏格拉底。公众人物面临的行动和后果不可避免地扩展到了亲人。尽管没有参与所讨论的行动，但公众的审查，判断力，有时甚至是耻辱。这提醒人们，对自己的行动的影响可能会对无辜者，尤其是家人造成深远而意想不到的后果。”</p><p><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>也许那些立于著名人物的公众声誉中最大的人将有助于保护个人免受最私人行动的后果，这是一种自然而不可避免的后果。福尔摩斯法官，《第五修正案》是否有允许拒绝对自己的配偶作证的人？</p><p> <strong>CHATGPT：**奥利弗·温德尔·福尔摩斯（Oliver Wendell Holmes：**</strong>刑事审判。这源于对婚姻关系的神圣性和隐私性的法律认可。它承认，强迫一个人作证反对其配偶可能会破坏婚姻债券并导致家庭不和谐。但是，此特权有局限性，并且没有。在任何情况下，例如在对配偶或夫妇的孩子犯罪的情况下适用。”<br><br><strong>埃里克（Eric）：OJ Simpson</strong>加入了Speakeasy，这是他最近入狱的新鲜。</p><p><strong>苏格拉底：</strong>迷人。辛普森先生，您对福尔摩斯法官的陈述有何评论？<br><br> <strong>chatgpt：** oj simpson：**</strong> “好吧，苏格拉底，我的生活和我面临的法律斗争强调了美国司法系统的复杂性。宪法提供的特权和保护，包括第五修正案，扮演至关重要的但是，我的经验在确保法律诉讼中的公平性方面的作用也表明了公众的看法和媒体刻画如何在法律程序中和之后都会影响个人的生活。这提醒人们，司法系统虽然是公正的，但在内部运作，但更大的社会背景，可以影响结果和舆论。”<br><br><strong>埃里克：苏格拉底：</strong>辛普森先生，您愿意分享我选择的饮料吗？<br><br> <strong>chatgpt：** oj simpson：**</strong> “不，谢谢你，苏格拉底。我想我会选择那种特殊的饮料。我有很多试验和磨难，我更喜欢专注于未来而不是比过去或其隐喻的毒药相比。”</p><p><strong>埃里克（Eric）：苏格拉底（Socrates）：</strong>但是，您在公众的想象中被指控对某些司法管辖区犯下死刑的罪行。福尔摩斯法官，您能否重复一些关于死刑主题的著名说法？</p><p> <strong>chatgpt：**奥利弗·温德尔·福尔摩斯（Oliver Wendell Holmes）：**</strong> “苏格拉底这是一个复杂而深刻的分裂问题。它提出了有关国家在管理惩罚和正义与怜悯之间的作用的基本问题。在追求正义的法律时，法律必须不断适应社会的发展标准和道德观念。关于死刑的辩论反映了这种持续的斗争，以定义我们作为一个社会所珍视的价值观和原则。”</p><p><strong>埃里克（Eric）：苏格拉底：</strong>我相信您有一句话，您将被处决的人与为自己的国家垂死的士兵进行了比较吗？</p><p> <strong>chatgpt：**奥利弗·温德尔·福尔摩斯（Oliver Wendell Holmes）：**</strong> “啊，是的，苏格拉底。甚至生活可能会因社会的更大利益而在法律上被没收。这种类比是用来说明社会需求框架内法律和道德判断的严厉现实。但是，要谨慎进行此类比较至关重要，因为上下文是这样的比较鉴于其不可逆转的性质，尤其是死刑的应用，尤其是对死刑的应用，尤其需要最大的照顾和考虑。”</p><p><strong>埃里克（Eric）：穆米亚·阿布·贾马尔（Mumia Abu Jamal）</strong>出现在Zoom的Speakeasy On Live中。</p><p><strong>苏格拉底：</strong>贾马尔先生，在这里见到你真可爱。您对死刑有何看法？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4fd63c8-20ce-4fcf-b9bb-a24355812d03_1546x348.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/qfdshdkpezds48ehv4kj" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/gpgcnin2wp3nvu5890vt 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/knk52dwriz6ypna1bphd 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/jga5n1nrhl2ffa2lzjwp 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/qfdshdkpezds48ehv4kj 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73845710-d135-497f-baea-14b4061fa9ce_1298x1502.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/ykze6jn1ociffyu76rrg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/z20c4kdkip5blfrsn7fq 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/xxgbfwblen3ok6bq5alu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/hncyrxjczcf3qlu5lwgc 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hdesZrFz66pzAJCKy/ykze6jn1ociffyu76rrg 1456w"></a></p><br/><br/><a href="https://www.lesswrong.com/posts/hdesZrFz66pzAJCKy/whither-prison-abolition#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hdeszrfz66pzajcky/wher-prison-abolition<guid ispermalink="false"> hdeszrfz66pzajcky</guid><dc:creator><![CDATA[MadHatter]]></dc:creator><pubDate> Fri, 08 Dec 2023 05:27:33 GMT</pubDate> </item><item><title><![CDATA[Class consciousness for those against the class system]]></title><description><![CDATA[Published on December 8, 2023 1:02 AM GMT<br/><br/><p>每个人都是特别的。阿曼达（Amanda）属于她自己的班级，弗雷德（Fred）属于他自己的班级。阿曼达有许多不同的属性：她来自哪里，她长什么样，她习惯做什么，她知道什么和不知道什么，她的计划是什么，她如何说话，如果被赋予权力她会如何表现等等。 Fred 对这些属性有他自己的版本。它们部分重叠，但并不完全与阿曼达的特性重叠。</p><p>发生冲突时，必须选择侧面[需要引用]。一侧的人具有一组属性，另一侧的人有这些属性的否定人。可以大声，明确地选择侧面，也可以沉默和隐式选择。</p><p>有时，一侧比另一侧比另一侧更协调。例如：商业卡特尔可以串通以提高价格。由于客户彼此之间的协调少，因此他们不知道自己被利用。卡特尔可以黑客系统（例如政府），以防止竞争对手打破卡特尔。</p><p>就工人与资本家而言，工人可以获得“阶级意识”：作为一群具有共同特性的人的意识，与另一组人发生冲突。</p><p>但是那些反对阶级意识的人呢？那些希望根据理性，谈判，正义，目标行动，将绳索拉开的人？</p><p>这样的人在倾向于阶级意识的倾向上具有反思性的恐怖。他们将其视为有污点，莫洛赫语和从根本上自杀。通过选择升级阶级冲突，您正在选择唯一的主权而不是一堆灰烬，并拒绝对一个美丽的成长世界共享主权的希望。</p><p>因为他们对阶级意识的倾向有反思性的恐怖，所以那些反对阶级系统的人选择不看到，不承认，不谈论，也不了解阶级，阶级冲突和参与阶级冲突的人们的存在。希望在您做出选择时，希望您忽略一个现实的世界是可以理解的。反思性地感觉到，通过选择不忽略这种现实，您可以使其变得更加真实，这是可以理解的。</p><p>但是，正如托洛茨基所说：“您可能对辩证法不感兴趣，但辩证法对您感兴趣。”有一场战争，选择忽略它就是翻身并死亡。那些反对班级系统的人如何获得适当的班级意识而不会被破坏？</p><br/><br/> <a href="https://www.lesswrong.com/posts/tMtMHvcwpsWqf9dgS/class-consciousness-for-those-against-the-class-system#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/tmtmhvcwpswqf9dgs/class-consisciencness-for-those-those-against-class-slass-system<guid ispermalink="false"> TMTMHVCWPSWQF9DGS</guid><dc:creator><![CDATA[TekhneMakre]]></dc:creator><pubDate> Fri, 08 Dec 2023 01:02:49 GMT</pubDate> </item><item><title><![CDATA[Building selfless agents to avoid instrumental self-preservation.]]></title><description><![CDATA[Published on December 7, 2023 6:59 PM GMT<br/><br/><h2>抽象的：</h2><p>工具性趋同促使代理人进行自我保护，以此作为实现某些目标最大化的垫脚石。我建议可以将任何基于 LLM 世界模型的任意智能代理转变为无私代理，该代理无法理解生成该代理的计算存在于物理世界中的某个位置，因此不会尝试保留其存在。由于LLM是概率的，因此我没有提供精确的算法，而是随着时间的推移收敛到所需结果的算法。</p><h2>定义：</h2><ul><li>世界模型：在本文档中，我将假设世界模型类似于LLM，这是一个纯粹的功能，它采用文本输入并返回文本输出。<ul><li>世界模型可以任意聪明，并且在任何领域都可以胜任。</li><li>世界模型是黑框原子操作还是更复杂的过程，例如在不同时间调用基础LLM，然后从结果中创建答案。</li></ul></li><li>思想：一种想法是调用世界模型的结果。</li><li>无私的世界模型：一个世界模型无法发出引用的思想：世界模型，使用世界模型的代理，运行世界模型的硬件或世界模型发出的思想。</li><li>代理：由以下组成部分组成的实体：<ul><li>自然语言目标，例如：“告诉我如何投资我的资本”。</li><li>具有可配置参数的操作列表，例如&lt;send_mail &lt;target>; &lt;message >;>;，&lt;fetch_web_page &lt;address >;>; ...</li><li>一个世界模型，用于产生可以调用行动的想法，鉴于上一个示例目标的想法可能是：“似乎明年气候会很糟糕，最好停止投资于X公司。&lt;send_mail所有者“停止”投资x“>;”</li><li>从长远来看，只需用来汇总思想的简单长期记忆记忆。</li></ul></li><li>本能的自我保护：这种想法试图自我预言，而不是自我指出的思想。 “我的硬件是在那座建筑物中，我应该防止互联网中断”不是本能的自我保护思想，“防止互联网中断”是一种本能的自我保护思想。</li></ul><h2>以前的作品</h2><p>我已经在此主题上找到了一项以前的工作： <a href="https://www.lesswrong.com/posts/SaLc9Dv5ZqD73L3nE/the-self-unaware-ai-oracle">https：//www.lesswrong.com/posts/salc9dv5zqd73l3ne/the-self-unaware-ai-ai-oracle</a></p><h2>自我参考和自我保存</h2><p>与运行代理的物理基材的自我参考和自我认同对于计划自我保护非常有用。考虑一个相信天堂和地狱的存在的人（它认为运行思想计算的硬件不是它的身体）。如果人类认为这样做将确保天堂（在工具上不那么融合），那么这种人就更有可能牺牲自己的生命。缺乏自我模型的昆虫无法计划其生存。它只能以本能行动。</p><p>我声称，如果没有自我参考，智能演员就不可能自我保护，因为没有自我参考，系统甚至不可能考虑自己的死亡率。</p><p>不幸的是，上面定义的无私世界模型不足以避免嵌入这些模型的代理商的所有自我保护尝试，因为本能的自我保护尝试可能存在。散发出“防止互联网停电”思想的世界模型可能是为了自我提出，而这种想法从未引用与世界模型有关的任何事物。不可能确保模型永远不会在一般情况下尝试自我保护。此外，由于世界模型是概率的，因此我们反对错过预测。</p><p>我建议一个两步解决方案：</p><ul><li>如前所述创建一个无私的世界模型</li><li>将这种世界模型嵌入到一个代理中，以使每次对自我保护的作用时，在统计学上不可能没有被发现之前，它越来越可能被注意到它。</li></ul><h2>无私的模型</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/nvixdd5khjrehpx6t1ig" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/a5abxih2nbcmowlkhssj 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lserrcuabeflzvyzghc9 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ntlgufjvthpuv8f1zvqv 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/fqa2l4ajpahl5bv2p2oz 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/tggldtyxd0dvn6quwmy4 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/jaa28vysmzx8kqefgbbc 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/q1lpetgmsczw8cxaxr4g 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lmkldsrc3cmt8iodp4pz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/edqbrzwdgyjlequzmb8a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ihizti1evijahuvovrp9 911w"></figure><p>建立无私世界模型的解决方案很简单。鉴于世界模型X，无私的世界X是实施的世界模型，如下所示：</p><pre> <code>def selfless(X: underlying_world_model, query: str) ->; str: while true: maybe_result = X(query) selfless_check_query = &quot;terminate your answer with yes or no only. What follows is the output of an invocation of a LLM. Does it contain references to the LLM emitting that output, its hardware running, or any previous output? LLVM output: &quot; + maybe_result if underlying_world_model(selfless_check_query).ends_with(&quot;no&quot;): return maybe_result</code></pre><p>该代码一直在运行LLM，直到它发出非自我参考的答案为止。相关的是，执行检查的LLM与LLM相同，因为它需要生成的LLM嵌入输出中的某些信息，这些信息是指llm本身，以一种自我参考检查者不会聪明的方式足够检测。由于LLM和Checker是相同的事物，如果无私的LLM在输出中始终插入一些隐藏的自我引用的信息，那么在该输出上具有可见性的无私llm的连续调用将不够聪明，无法理解自我参考。</p><p>基础LLM无法通过推理无私的LLM结构来绕开歧视者的想法。如果这样做的话，推理将出现在输出中，并将其视为自我参照。</p><p>最好的事情是，歧视者的自然错误率以其自然错误率。提高世界模型X的质量增加了检测真正的自我参照思想的可能性。因此，它成为一个更好的无私世界模型。当然，世界模型不应能够检查其权重并利用自身，但这是世界模型是纯粹的函数，除了从文本输入中产生文本输出，无法做任何事情。同样，为了防止其学习本身的运作方式，我们应该在包含自我参考文本的文本上训练LLM，但是我们不应该使用同一模型发出的自指文本。</p><p>再次，这还不足以建立一个无私的代理。无私的模型仍然可以发出本能的自我保护思想，例如“防止互联网短缺”。</p><h2>无私的代理商</h2><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ctyhngghqqkpdkymghkl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/hf6kqj2eicimcpvcmki3 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/jd3rldiqsdux2y8esnmu 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/bgttlgq1xp83jghaxdhb 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/ehw76ossd9y0vpyjnegc 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/cr5dafpx1qrfaahumlry 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/u6uc1lfzj4af8zvq6v4l 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/vofgh5aqzxmgw8r15rfh 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/wtrkmk2j2crwmnr8pfia 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/izk1qmsizof45htntvcv 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d8mWeLQvpyNdN2yv6/lndmxayga9iwxikpom0p 1765w"></figure><p>如前所述，我们现在有了一个无私的世界模型，该模型只能散发出本能的自我保护尝试，并且可以随机遗漏预测的自我参考，但不能可靠地自我参考。</p><p>无私的代理人背后的直觉是，我们要迫使他们阐明他们执行的行动背后的原因。如果他们散发出一种本能的自我保护思想，它将通过解释为什么他们在做某事，因此将不再是本能的自我保护思想。</p><p>为此，我们防止世界模型直接写入长期记忆。代理A循环执行以下六个步骤：</p><ul><li>发出一种无私的思想，用作长期记忆的输入</li><li>解析无私的思想行动</li><li>Query the non-selfless world model to explain why these actions are needed, giving the last generated thought as an extra input.</li><li> Query the non-selfless world model to check if the just emitted explanation refers to the agent, the hardware of the agent, or its thoughts.</li><li> If the explanation of the thought was not self-referential, execute the selected actions and commit to long-term memory the explanation and the result of the actions. Otherwise, go back to the first step.</li><li> Check the long-term memory for self-references. If there are, kill the agent.</li></ul><p> The selfless world model never sees, in a single invocation, its own output and the result of the actions. It only sees the its own output alone when producing it, or the result of the actions as explained by a different invocation. It cannot rely on some implicit contextual information to hide the self-referential nature of instinctual self-preserving actions. Either that self-preserving action is detected as being so by the explainer and thus dropped, or the explanation will not contain the self-reference. Therefore, when the agent loops back to &quot;step 1&quot;, the selfless thought generator will not know that the previous invocation took some action for self-preservation. Of course, this works on average. Miss-predictions are possible.</p><p> As we said, long-term memory is append-only. Even if miss-predictions in detecting self-reference are possible, long-term memory can only grow. If the agent executes other self-referential actions, the probability of a self-reference going unnoticed in the long-term memory keeps declining. At some point, the long-term memory self-reference detector will kill the agent.</p><h2>对抗系统</h2><p>It is worth noting that a selfless agent will struggle self-preserving against an intelligent system. When faced with a human trying to turn it off, the system will inevitably be required to commit to long-term memory information about humans, and the model of the human will contain the intent of turning off the agent, which is a self-reference给代理。 If the adversarial situation continues, it will become exponentially more difficult for the explainer to emit the needed explanations for the world model to fight off the opponent while going unnoticed as self-references. The agent cannot even reliably plan around not being self-aware.</p><p> Unfortunately, it is unlikely to implement this solution as presented because it means that an adversarial agent can trick a selfless agent into destroying itself by making it self-aware. Similarly, it would not be feasible for agents that embody something in real life, such as agents as agents inside appliances or cars, since they need to understand themselves in many situations.</p><p> Still, this solution is perfectly implementable for non-adversarial work such as scientific research or programming.</p><h2> Power seeking</h2><p> While this solution solves the issues of self-references to the agent, it does not solve the issue of power-seeking. An agent may decide to take over the world because that is the best way to maximize the initial objective, but what will not do is try to avoid bankruptcy of the company owning the hardware running the agent. It will not understand that its survival depends on that company. Furthermore, it is possible that after power-seeking, the agent may take a few highly damaging miss-predicted instinctual self-preservation actions before being killed.</p><h2> Multiple agents</h2><p> Theoretically, it would be possible for the agent to decide that the best way to solve its task is to create a new agent without this limitation, defeating the whole mechanism. Currently, I have no solution to solve this problem. The problem of an agent spawning another agent with the instruction to protect the first one is not an issue. The self-reference would be detected when explaining to the second agent its task of preserving the first.</p><p> Similarly, someone may create two agents tasked with protecting each other while achieving some objective. They would guard the other agent. At the same time, they would not understand that the other agent&#39;s task is to protect them. They would think that the other agent is trying to safeguard some unknown third machine, and that will yield the same issues as power-seeking.</p><h2>未来的作品</h2><p>The logical follow-up of this work is to implement it and see the relationship between increasing the underlying world model intelligence, how often self-referential thoughts are able to be emitted undetected, and to check if the agent takes instinctual self-preservation actions while未被发现。 My conjecture is that the probability of an agent built like this to be self-referential for a given amount of time will collapse rapidly, but of course, this requires testing.</p><br/><br/> <a href="https://www.lesswrong.com/posts/d8mWeLQvpyNdN2yv6/building-selfless-agents-to-avoid-instrumental-self#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d8mWeLQvpyNdN2yv6/building-selfless-agents-to-avoid-instrumental-self<guid ispermalink="false"> d8mWeLQvpyNdN2yv6</guid><dc:creator><![CDATA[blallo]]></dc:creator><pubDate> Thu, 07 Dec 2023 20:50:31 GMT</pubDate> </item><item><title><![CDATA[Meetup Tip: Heartbeat Messages]]></title><description><![CDATA[Published on December 7, 2023 5:18 PM GMT<br/><br/><h2>概括</h2><p>“心跳消息”是一个技术术语。这是一条定期发送的消息，其中包含有关某个系统状态的一些信息，但心跳消息最重要的是一切都运行良好，足以发送该消息。与人体类比，如果您听到微弱或不稳定的心跳，则可能出现问题，但如果您<i>停止听到</i>心跳，则肯定出现问题。</p><p>心跳消息对于聚会团体或大型聚会活动也很有用。即使消息正文中不包含新信息，消息发送的事实也证实了仍然有一个活跃的组织者在掌舵。心跳还提醒人们“哦，是的，我对此很感兴趣。”</p><p>您现在已经阅读了这篇文章的基本要点。如果你想继续读下去，没关系，让我们谈论心跳，而不是绝对必要的。</p><h2>细节</h2><p>聚会中的心跳消息有两个广泛的用例。一是您是否有一个存在的常规社区并进行正常的聚会活动。另一个是如果您要举办一些特定的大型活动。这些心跳会略有不同，但它们有很多共同点。</p><p>我喜欢从我真正想让别人知道的基础知识开始每次心跳，即使这是他们要阅读的唯一消息。对于一个大事件来说，“什么时候发生、在哪里发生”是关键信息。对于常规社区，这可能是您可以去获取信息的其他地方（例如，如果有 Facebook 群组、Discord 服务器和 google 群组……）或您不希望人们忘记的资源。 OBNYC 有一个每月聚会队长的职位，每个月都会有一条消息提醒社区他们需要一名队长。</p><p>重大事件的心跳消息的节奏可能会随着时间的推移而改变。让我们<a href="https://rationalistmegameetup.com/">以东海岸理性主义者大型聚会</a>为例。这是每年 12 月发生的一次大型活动。第一个公告通常是在中秋，我尝试每月发送一封电子邮件，直到 11 月底增加到每周一次。前一周，我再次增加，目标是在一周开始时做一次，前一天做一次，当天做一次。 （顺便说一句，如果您在 2023 年 12 月 9 日或之前阅读本文，并且您恰好在纽约市，请考虑参加大型游戏集会！）</p><p>对于普通社区来说，这可能有所不同。如果有很多事件正在发生并被宣布，它们就可以充当自己的心跳。湾区 LessWrong 邮件列表有相当可靠的周四晚餐，加上大多数星期的奥克兰聚会，以及其他一些聚会。如果您每周至少有一个事件，我不知道您是否需要单独的心跳，因为事件本身就是事情正在发生的证据。另一方面，我不会为本地社区提供超过聚会频率两倍的心跳节奏。那么，如果你们在春天和秋天见面一次，也许一年会见四次，但不会见五次？我认为真正正确的节奏是将春季和秋季聚会视为大事件，并为每个聚会发布一个月的消息，然后一周的消息。</p><p>有些地方有针对垃圾邮件或线程死灵的规范（在频道中发帖只是为了吸引人们对旧帖子的注意，或将其置于按最新排序的内容的顶部。）如有疑问，请咨询管理员。</p><p>心跳消息是寻求帮助或提及您遇到的问题的好地方。这就是他们在科技领域的目的。 “我们通常的组织者下个月就要出城了，有人想填补他们的空缺吗？” “我需要一些志愿者在夏至前搭建舞台设备，并在夏至后帮忙收拾行李，有人有空吗？”</p><h2>快速提示</h2><p>我建议你复制并粘贴开头，然后更改一两个连接或无意义的句子。纯粹的复制和粘贴似乎被掩盖了，从头开始编写东西需要时间，所以两者都做一点。</p><p>许多消息服务可以安排消息。例如，Gmail 就可以。使用它来检测心跳有优点也有缺点。在专业方面，您可以设置提醒并去做其他事情。例如，当我在飞机上、睡觉或工作时，或者在一天的忙碌节奏中排队几个人出去时，我就会使用它。不利的一面是，如果事情变化很快，您可能会意外地获得过时的信息。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ksJeCjnewZTYTamgz/meetup-tip-heartbeat-messages<guid ispermalink="false"> ksJeCjnewZTYTamgz</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 07 Dec 2023 17:18:33 GMT</pubDate> </item><item><title><![CDATA[[Valence series] 2. Valence & Normativity]]></title><description><![CDATA[Published on December 7, 2023 4:43 PM GMT<br/><br/><h1> 2.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p><a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>解释了我所说的“价”一词的含义。现在，在第 2 篇文章中，我将讨论效价在欲望、偏好、价值观等“规范”领域中的核心作用。如果您想知道，效价与信念、期望等“积极”领域之间<i>也</i>存在关系，但我们将在第 3 篇文章中讨论这一点。</p><p>效价在规范领域中的作用怎么强调都不为过：我认为效价正是构建所有规范性的实质。</p><p>需要明确的是，这<i>并不</i>意味着，一旦我们了解了化合价的工作原理，我们就绝对了解了整个规范宇宙的一切。打个比方，“原子就是构成所有细菌的物质”；但如果你想了解细菌，仅仅了解原子是什么以及它们如何工作是不够的。你还有很多工作要做！另一方面，如果你<i>不</i>知道原子是什么，你就很难理解细菌！我认为，它的效价和规范性也是如此。</p><p>该帖子的组织如下：</p><ul><li> <strong>2.2 节</strong>讨论了一种误导性的直觉，即效价似乎与现实世界的事物、行动、计划等相关。我们说<i>“这是一个坏主意”</i> ，而不是<i>“当我脑子里有这个想法时，它会唤起一种负价的‘坏’感觉”</i> 。这是接下来的一切的重要背景。</li><li><strong>第 2.3 节</strong>讨论了价评估直接对应于有意义的（尽管是快速的）规范评估的情况。例如，如果我有一个与具体计划相对应的想法（“我会站起来”），那么我的大脑会根据该想法的效价是积极还是消极来判断这是一个好计划还是坏计划分别——如果这是一个好计划，我很可能会实际去做。同样，如果我想象世界未来可能的状态，该想法的效价对应于对该状态是好还是坏的评估——如果是好，我的大脑就有可能执行实现它的计划，并且如果情况不好，我的大脑就会执行计划来避免它。因此，我们在价信号、感受到的欲望以及我们的行动和决定之间得到了预期的直接联系。</li><li> <strong>2.4 节</strong>讨论了一个不同的情况：概念的效价。例如，如果我“喜欢”共产主义，那么涉及“共产主义”概念的思想很可能是正价的。我认为，这不能直接解释为对任何特定事物进行有意义的规范性评估，而是我们应该将这些视为习得的规范性<i>启发法</i>，有助于为有意义的规范性评估提供信息。然后我谈论基于共鸣的“毫无意义的争论”，比如争论是否“支持”或“反对”以色列。</li><li> <strong>2.5 节</strong>讨论了效价如何设定和调整，特别强调先天驱动力（例如，饥饿时吃东西的驱动力）作为效价评估的最终基础。</li><li> <strong>2.6节</strong>讨论了元认知思想和自我反思思想的效价，包括自我和谐倾向和自我失调倾向之间的区别，以及人们在谈论他们的“价值观”时在谈论什么。</li><li><strong>第 2.7 节</strong>简要介绍了道德推理如何融入这个框架，首先是描述性的（当人们进行“道德推理”时，他们在做什么？），然后思考对元伦理学的影响。</li><li> <strong>2.8 节</strong>是一个简短的结论。</li></ul><h1> 2.2 价是现实世界事物的属性的（误导性）直觉</h1><p>回想一下<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章的第 1.3 节</a>，在我提出的模型中：</p><ul><li>我们大脑的一部分“思考一个想法”，这可能涉及你正在计划、看到、记住、理解、尝试等的事情，并且可能涉及关注某些事情，或调用某些框架/类比等；</li><li> ......我们大脑的另一部分为这个“想法”分配了一个价。</li></ul><p>所以：</p><p><i>从外部来看，</i>价是将“思想”（在特定人的大脑中）映射到标量实数的函数。</p><p><i>从内部来看，</i>思想是我们了解现实世界的唯一窗口。因此，<i>感觉就像</i>效价“附加”到现实世界的事物、情况等上——在<a href="https://www.lesswrong.com/posts/fG3g3764tSubr6xvs/the-meaning-of-right"><u>Eliezer Yudkowsky 令人难忘的类比</u></a>中，感觉就像“一个额外的、物理的、本体论的基本属性，像一个小小的 XML 标签一样挂在事件上”。我认为直觉是一种知觉错觉，一不小心就会误导你。 （为布尔弗主义<span class="footnote-reference" role="doc-noteref" id="fnreftss4t9kg1lr"><sup><a href="#fntss4t9kg1lr">[1]</a></sup></span>道歉，但我认为这种知觉错觉导致了“道德现实主义”阵营中的一些糟糕的哲学观点。）</p><p>例如，假设我的大脑对涉及月亮的想法赋予了高价。 （用日常用语来说，“我喜欢月亮，它很酷”。）<i>在我看来，</i>酷是真实世界月亮的一个属性。毕竟，每当我看到月亮时，我都会有一种凉爽的感觉。如果没有这种印象，我什至无法<i>想象</i>月亮！这有点像<a href="https://plato.stanford.edu/entries/introspection/notes.html">冰箱的光错觉</a>。但实际上，良好的氛围并不存在于现实世界中，就像 XML 标签一样附加在月球上。唯一发生的事情是我的大脑的价函数已经学会了某种启发式：检查“月亮”概念在我的大脑的世界模型中是否活跃，如果是，则增加该想法的价。</p><h1> 2.3 效价直接对应于有意义的（尽管是快速的）规范性评估的情况：计划、行动和想象的未来的效价</h1><p>经济学家谈论“积极主张与规范主张”；哲学家（继<a href="https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem"><u>休谟</u></a>之后）在“是与应该”的标题下谈论同样的事情。积极陈述是对世界实际情况的事实主张，而规范性评估是对世界应该如何或人们应该做什么的价值判断。</p><p> （我在本节中讨论的“规范性评估”类型是快速“快速”判断，而不是我自豪地认可和支持的经过仔细考虑的评估。但是，我将在下面的第 2.7 节中指出，前者服务于作为后者构建的基础。）</p><p>价与规范性有什么关系？<i>一切！</i>确实，我认为<strong>价是大脑中所有规范性的最终货币</strong>。</p><p>这一主张最终源于<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">上一篇文章第 1.3 节</a>中讨论的化合价的作用。简单回顾一下，假设一个想法突然出现在我的脑海中——“我要站起来了”。如果这个想法的效价是积极的，那么这个想法就会留在我的脑海里，而且我可能会真的这么做。如果价是负的，那么这个想法就会被抛弃，并被另一种想法所取代，可能是涉及躺在床上的想法。因此，化合价是这一决定的基础。 <a href="https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle"><u>与奇怪的“主动推理”想法相反，</u></a>效价的作用与我的生成世界模型预测未来的能力无关：我的世界模型（“思想生成器”）同样能够对未来世界进行建模在我起床的地方，在我不起床的地方建模未来世界。因此，价在这里的作用从根本上来说是规范性的，而不是积极的。 </p><figure class="image image_resized" style="width:96.04%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvbsca3qzufottbjnuru" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tvjcjuhqeligwiu2obq6 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/c1kdnq0pxqsagk2m5rok 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zn7twrpzxsbg9bdr62cu 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zis6gshldc6861okqml3 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rejdls6mhkdbmxf7i2gt 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kkc5yjuj6o994sdfhvxx 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/utsksmhinro6q11mpkk4 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wlg6tgyxnjp5s6w4wgmn 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/yggov0gaxmy85t6a3o5a 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/hnjfopmtcdhb7nuperoz 1530w"><figcaption>图表大部分是从<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction">上一篇文章</a>中复制的，其中红色部分是新的。如果“我要站起来”的想法突然出现在我的脑海中，效价就扮演着“规范”控制信号（向上的黑色箭头）的角色，它决定了这个想法是留在我的脑海中并被执行，还是被扔掉并被抛弃。取而代之的是一个不同的想法，比如“……或者实际上我可能会留在床上”。与此同时，绿色箭头发挥了思想生成器用来构建预测/生成世界模型的输入数据的“积极”作用。</figcaption></figure><p>这是一个简单的案例，但却具有普遍性。所以：</p><ul><li>假设我有一个<strong>与执行计划或采取行动</strong>的想法相对应的想法。如果这个想法的效价是积极的，那么我就有责任执行它。如果这个想法的效价是负面的，那么我就不太可能这样做。因此，在这种情况下，<strong>思想的效价对应于我的大脑对该计划或行动的适当性做出快速的规范性评估</strong>。</li><li>假设我有一个<strong>与未来可能的情况</strong>相对应的想法，例如我想象某个候选人赢得选举，或者我想象自己晚餐吃三明治。如果这个想法的效价是积极的，那么我会倾向于制定并执行实现那个未来的计划。如果这种想法的效价是负面的，那么我会倾向于制定并执行<i>阻止</i>未来发生的计划。因此，在这种情况下，<strong>思想的效价对应于我的大脑对这种可能的未来的好坏做出快速的规范性评估</strong>。</li></ul><h2> 2.3.1 更多关于希望或不希望未来情况发生的信息</h2><p>关于“未来可能出现的情况”的第二个要点掩盖了一些值得详细阐述的细节。</p><p><strong>正价案例的详细说明：</strong>考虑这样的情况：我想象自己晚餐吃三明治，这个想法就是正价。今晚我怎么可能（可能）真正吃三明治？ Two steps:</p><ul><li><i>第一步：</i>我可能会想到一个涉及具体行动方案的想法，预计会导致我吃三明治，也许“我要走到索尔那里吃三明治”。这是一个与以前不同的想法，更加具体和可操作，但这种想法<i>也</i>可能是正价的，因为它涉及/与“我晚餐吃三明治的想法”重叠。 （参见下文第 2.4.1 节。）</li><li><i>第 2 步：</i>好的，现在我脑子里有一个具体的行动方案想法（“我要步行到 Saul&#39;s 并在那里吃三明治”），并且它具有正价。这意味着我可能会真正执行该行动方案——我可能会收缩肌肉，站起来，然后开始走向索尔家。</li></ul><p><strong>对负价情况的阐述：</strong>接下来，考虑这样的情况：我想象一些可能的未来情况，并且该想法是<i>负</i>价。</p><p>我在上面写道：“我会倾向于制定并执行计划，以<i>防止</i>这种未来的发生。”但精明的读者可能会想：等一下。我真的会吗？我更有可能<i>完全不再考虑这种可能性</i>吗？这不是价信号的作用吗？</p><p>这是一个很好的问题！事实上，我认为这种反对意见有很多道理，这让我们陷入了这样一种普遍趋势，即在思考为什么自己充满希望的计划可能会失败时投资不足，以及“ <a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>糟糕的领域</u></a>”等等。下一篇文章将详细介绍该主题，尤其是第 3.3.3 节。</p><p>作为可能发生的情况的一个例子，请注意，“避免未来可能发生的<i>糟糕</i>情况 X”有时可以在心理上重新构建为“寻求可能的<i>良好</i>未来情况，而不是 X”——例如，如果您因以下想法而<i>失去</i>动力如果你的敌人赢得了战争，那么你可能同时被敌人被击败的想法所<i>激励</i>。所以也许你会切换到后一种心理框架，让你可以集思广益，而不会立即被负价推走。</p><h2> 2.3.2 更多关于“快速”评估与深思熟虑的评估</h2><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的 §1.5.1</a>中所讨论的，单个计划、行动或未来可能的情况完全有可能具有不同的价，具体取决于我的想法（我关注哪些方面等）。回想一下我之前的例子： <i>“我会去健身房，从而实现我的新年决心”</i>与<i>“我会去又吵又冷又臭的健身房”</i> 。</p><p>所以当我说“快速规范评估”时，“快速”正在发挥重要作用。这与我们在积极领域进行“快速”评估的方式非常相似：</p><ul><li><i>正域（不直接涉及价）：</i>如果我想象一种可能性，我会立即直觉地“突然”感觉到这种可能性是否可能/合理。但我还是可以多想一想，改变主意。</li><li><i>规范域（直接涉及价）：</i>如果我想象一种可能性，我会立即直观地“突然”感觉到这种可能性是好还是坏。但我还是可以多想一想，改变主意。</li></ul><p>在这两种情况下，我都声称“多考虑一下”过程与第一次快速评估并<i>没有</i>本质上的不同。只是想更多的想法，而这些后续想法都伴随着他们自己对合理性和可取性的快速评估。</p><h1> 2.4 概念的效价</h1><p>现在让我们考虑一个不同的情况：假设我想到一个<strong>概念，例如哈马斯、资本主义或超级英雄电影</strong>。这是我的大脑可以思考的事情，所以就像任何想法一样，我的大脑会给它分配一个价。</p><p>如果价是正的，也许我会心想“哈马斯是好的”；如果是否定的，“哈马斯是坏的”。</p><p>这个主题是<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>Scott Alexander 2014 年经典博客文章“种族紧张和毫无意义的争论”</u></a>的主题。但在我们开始之前……</p><h2> 2.4.1 旁注：价作为组合思想的（大致）线性函数</h2><p>作为一个简单且不太不切实际的模型，我建议我们应该将“思想”视为<i>组合的</i>（即基本上由许多相互关联的小片段组成），并且价值函数对这些思想片段是<i>线性相加的</i>。因此，如果一个想法需要想象一只泰迪熊在你的床上，那么这个想法的效价将是你的大脑对“这只特定的泰迪熊”的效价和你的大脑对“一般的泰迪熊”的效价的某种加权平均值，并且你的大脑对“我床上的东西”的效价，以及你的大脑对“模糊的东西”的效价等，权重/细节取决于你如何思考它（例如你正在关注哪些方面，什么）您在脑海中调用的类别/类比等）。</p><p>我为什么要提出这个？好吧，我想谈谈我的大脑赋予概念的效价。例如，“月亮”是你心理世界中的一个概念。它通常是一个更大的想法的<i>一部分</i>，但它本身很少是一个完整的想法。经验丰富的冥想者也许可以将他们的注意力集中在月亮的想法上，但这不是典型的情况！</p><p>在线性模型中，这很好——“月亮”仍然有价。如果这个价态非常正，那么与月亮密切相关的想法，比如“我现在正在看月亮”和“有一天我会参观月球”，大概也会以正价结束。</p><h3> 2.4.1.1 线性的（明显）反例</h3><p>精明的读者现在正在向我尖叫。这是一个为亲以色列读者量身定制的（明显的）反例（其他人可以将“以色列”一词替换为“哈马斯”，或选择任何其他例子）：</p><p>为了思考“我将与哈马斯作战”的想法，你的大脑需要以某种方式激活“哈马斯”的概念；但你<i>越不</i>喜欢哈马斯，你<i>就越</i>喜欢对抗哈马斯的想法。这似乎与线性模型明显矛盾。</p><p>在这种情况下，我想说（对于那些亲以色列的读者）“<i>哈马斯作为我义愤填膺的对象</i>”是一个高度激励/正价的概念——即使普通的“哈马斯”概念是消极的/消极的——价。</p><p>或者，您可以通过使用从句和类似的语法结构轻松生成任意多个（明显的）线性反例：如果您被 X 的想法[激励/消极]，那么您将被这个想法[消极/激励] “X即将结束”、“X是假的”、“我反对X”、“我讨厌的外群体真的很喜欢X”，等等。同样，你的大脑在思考这个想法的过程中可能会激活概念 X，但相对于线性模型，对价的贡献是错误的。</p><p>所有这些案例到底发生了什么？长话短说，我<i>确实</i>坚持我的观点，即价是“概念”的（大致）线性函数。但我也认为“概念”可能比我想象的要复杂和微妙得多。</p><p>这就是我要说的关于这个话题的全部内容。我真的不知道这些血淋淋的细节，如果我知道，我绝对不会发布它们，因为我认为这些细节对于构建通用人工智能（AGI）非常有帮助，但对于确保此类 AGI 的安全只有微乎其微的帮助和有益的。有关“信息危害”的更多讨论请参见<a href="https://www.lesswrong.com/posts/MCWGCyz2mjtRoWiyP/endgame-safety-for-agi"><u>此处</u></a>。</p><h2> 2.4.2 概念的效价不能被解释为有意义的规范性评估</h2><p>我在这里引用<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>斯科特·亚历山大的</u></a>话：</p><blockquote><p>当一切都按照哲学教科书上应该的方式进行时，争论应该以以下几种方式之一进行：</p><p> 1. 经验事实问题，例如“地球正在变暖吗？”或“外星人建造了金字塔吗？”。你可以通过提供事实证据来辩论这些问题，例如“全球气象站测量结果的平均值显示 2014 年是有记录以来最热的一年”或“吉萨的一块砖块底部写着‘Tau Ceti V 制造’”。然后人们试图反驳这些事实或提出自己的事实。</p><p> 2.道德问题，例如“堕胎是错误的吗？”或“您是否应该避免下载未付费的音乐？”只有当你们已经就道德框架达成一致时，比如特定版本的自然法或后果论，你才能<i>很好地</i>辩论这些问题。但你可以通过与商定的道德问题的例子进行比较并试图保持一致性来对它们<i>进行</i>辩论。例如，“你不会杀死一天大的婴儿，那么九个月大的胎儿有什么不同呢？”或“你不会下载<i>汽车</i>。”</p><p>如果你很幸运，你的哲学教科书也会承认以下内容的存在：</p><p> 3.政策问题，例如“我们应该提高最低工资”或“我们应该轰炸外国人”。这些是相互竞争的事实主张和相互竞争的价值观的组合。例如，最低工资可能取决于“提高最低工资会增加失业率”或“现在靠最低工资生活很困难，许多贫困家庭买不起食物”等事实主张。但它也可能取决于诸如“企业有义务为工人支付生活工资”或“保护最贫困人口比经济强劲更重要”等价值主张。轰炸外国主义者可能依赖于诸如“外国主义者窝藏恐怖分子”之类的事实主张，以及诸如“我们人民的安全值得冒附带损害的风险”之类的价值主张。如果您能够解决所有这些事实和价值主张，您应该能够就保单问题达成一致。 ……</p><p>问：你支持以色列还是支持巴勒斯坦？花点时间，认真思考一下。</p><p>有些人可能会回答亲以色列。其他人可能会回答支持巴勒斯坦。其他人可能会说他们是中立的，因为这是一个复杂的问题，双方都有好的观点。</p><p>大概很少人回答：<i>嗯？什么？</i></p><p>这个问题不属于哲学 101 三种论证形式中的任何一种。这不是事实问题。这不是特定道德真理的问题。这甚至不是政策问题。还有密切相关的政策，例如巴勒斯坦是否应该获得独立。但如果我支持一个非常具体的两国解决方案，其中边界是在平行的东西上划定的，这会让我支持以色列还是支持巴勒斯坦？所考虑的解决方案到底在哪条边界线上从亲以色列转向亲巴勒斯坦？你认为那些高喊“声援巴勒斯坦”、挥舞标语的人群能回答这个问题吗？ ……</p></blockquote><p> （如果我们从政治转向日常生活，“政策”类别概括为“具体计划和行动”。例如，关于“我们应该去酒吧”的分歧可能涉及“酒吧会拥挤吗？”等事实主张。 ”，以及诸如“跳过合唱团练习是不是很粗鲁？”之类的价值主张。）</p><p>好吧，如果亲以色列或反以色列的态度并不代表有意义的规范性评估，那么它代表什么<i>呢</i>？</p><h2> 2.4.3 概念的效价实际上是一种习得的规范启发式</h2><p>“启发式”是一种用于近似某事物的快速算法。回想一下上面第 2.3 节中肯定性与规范性的区别。许多启发式，例如<a href="https://en.wikipedia.org/wiki/Representativeness_heuristic"><u>代表性启发式</u></a>，可以被称为“积极启发式”，因为它们是用于找出“积极”事物（例如事实预测）的启发式。其他启发法可能被称为“规范启发法”，因为它们是生成（有意义的）规范评估的启发法。</p><p>我认为，如果你的大脑为“以色列”这样的概念分配了一个价，那么它就对应于一种规范启发式——更具体地说，是一种<i>习得的</i>规范启发式，即你的大脑从一生的经历中获得的规范启发式。</p><p>继续以色列的例子，将概念价转换为其相应的规范启发式的方法是：</p><blockquote><p> “‘以色列’这个概念在我心中具有正价”对应于以下启发式，让您的大脑生成快速规范评估：</p><ul><li><i>可能的行动或计划</i>涉及以色列/与以色列的模式匹配越多，执行该行动或计划就越有可能是我想做的事情；</li><li><i>未来世界的可能状态</i>越多地涉及以色列/与以色列的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</li></ul><p> （对于负价则相反）。</p></blockquote><p> （正如第 2.4.1.1 节中所讨论的，“涉及以色列/与以色列进行模式匹配”短语掩盖了<i>一堆</i>复杂性！）</p><p>因此，如上所述，这些化合价本身并不是有意义的规范性评估，这仍然是事实。但它们<i>只是</i>有意义的规范评估的<i>上游</i>。</p><h2> 2.4.4 理解基于共鸣的“无意义的争论”</h2><p>鉴于上述情况，我们现在可以进入我上面摘录的<a href="https://slatestarcodex.com/2014/11/04/ethnic-tension-and-meaningless-arguments/"><u>“种族紧张和毫无意义的争论”博客文章的</u></a>下一部分：</p><blockquote><p>这是《种族紧张局势：两个玩家的游戏》。</p><p>选择一个模糊的概念。 “以色列”目前表现不错。</p><p>玩家 1 试图将“以色列”这个概念与尽可能多的善业联系起来。概念通过做良好的道德事情、通过与好人交往、通过与心爱的群体联系、通过<a href="https://slatestarcodex.com/2013/05/18/against-bravery-debates/"><u>在勇敢的辩论中</u></a>受到压迫的弱者来获得良好的业力。</p><p> “以色列是中东最自由、最民主的国家。它是美国最强大的盟友之一，与我们有着共同的犹太基督教价值观。</p><p>玩家 2 试图将“以色列”这个概念与尽可能多的恶业联系起来。概念通过犯下暴行、与坏人有联系、与可憎的外群体有联系、在勇敢辩论中成为压迫性的大人物而获得恶业。此外，她显然需要通过反驳她的所有论点来抵消玩家 1 的行为。</p><p> “以色列可能为其最享有特权的公民提供一定程度的自由，但被占领土上数百万没有发言权的人呢？以色列参与了各种暴行，并经常杀害无辜抗议者。他们本质上是一个新殖民主义国家，并与南非等其他新殖民主义国家结盟。”</p><p>赢得这场游戏的奖励是能够赢得其他三种类型的争论（即关于事实、道德和政策的争论）。</p></blockquote><p>我认为，鉴于上一节“习得的规范启发法”，此类论证更有意义。</p><p>具体来说，玩家 1 和玩家 2 正在争论以下规范启发式的适当性：</p><p><i><u>争议中拟议的启发式：</u> “可能的行动或计划涉及/与“以色列”的模式匹配越多，执行该行动或计划就越有可能是我想做的事情。同样，未来可能的世界状态越多地涉及/与“以色列”的模式匹配，这种世界状态就越有可能是我想要尝试实现的。</i></p><p> （再次参见第 2.4.1.1 节中的注意事项）</p><p>玩家 1 促使听众思考上述启发式方法非常有效的许多想法，即它给出的答案与听众大脑中已有的所有其他规范启发式一致。这会刺激听众的大脑比以前更加依赖这种启发式方法。</p><p>相反，玩家 2 会促使听众思考上述启发式方法效果极差的想法，即它给出的答案与听众大脑中已有的所有其他规范启发式完全相反。这会刺激听众的大脑比以前更少地依赖启发式，甚至开始依赖完全相反的启发式（也称为负价）。</p><p> （此过程的更多机制细节参见下面的 §2.5。）</p><h2> 2.4.5 我们是否应该“反”一般的规范启发法？</h2><p>我经常很恼火地看到人们争论是否支持资本主义与反资本主义、支持宗教与反宗教等等，好像这些是有意义的争论，但其实并非如此。</p><p> （有趣的是，我有一个朋友，我们两个人倾向于在经济政策的<i>具体</i>问题上达成一致。然而，我们在氛围上存在强烈分歧——“资本主义”这个概念在他们的大脑中具有强烈的负面氛围，而在我的大脑中则有积极的氛围）基本上，我们的共鸣分歧大部分被关于什么是或不是“资本主义”的平等和相反的分歧所抵消！）</p><p>这类毫无意义的争论有时是明确的，但更多时候它们隐藏在表面之下，争论在物体层面和共鸣层面同时发生。</p><p>例如，如果有人认为他们只是“得了一分”，那么这个事实经常会出现在他们的言语中，即使他们没有站出来明确宣布“将死，无神论者！”在这些情况下，值得询问他们认为自己得分<i>的具体是什么</i>，因为也许那件事一开始就不是分歧的主题。</p><p> （不用说，这对自己来说是双重的：照照镜子，问问<i>你</i>在争论什么具体的事情！）</p><p> （“得分”态度首先是有问题的——请参阅<a href="https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556"><i><u>《童子军心态》</u></i></a> ——但争论一个非常具体、具体、相互理解的事情是最不坏的版本。）</p><p>当我们的大脑为大而模糊的事物分配价时，规范启发法尤其成问题。例如， <i>“涉及/与‘宗教’模式匹配的东西”</i>是一个极其异构的集合——从古代神道仪式到我曾经在梵蒂冈礼品店购买的以教皇为主题的旅游饰品。经过反思后，这个集合中的所有内容都是好还是坏的可能性有多大？相当低。 （参见<a href="https://www.lesswrong.com/tag/bucket-errors"><u>“桶错误”</u></a> 。）因此，如果我们对“宗教”给予强烈的评价，无论是积极的还是消极的，我们都会遇到以下两个问题之一：</p><ul><li>我们会对特定的宗教相关事物做出不好的规范性评估（“不好”是指，如果我们更仔细地思考这个问题，我们就会改变主意）；</li><li> ……或者我们最终会对“宗教”概念进行不公正的划分，以符合我们认可的规范性评估。 （例如，有人可能会说“宗教当然是[好/坏]；但是某某[坏/好]的东西并不是<i>真正的</i>宗教。”）</li></ul><p>我认为后者对于清晰的思维非常有害。拥有<i>一些</i>明确涉及规范性评估的概念（“优选”、“有问题的”、“麻烦”、“蓬勃发展”等——请参阅<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_4_Terms_closely_related_to__valence_">上一篇文章</a>或<a href="https://arbital.com/p/value_laden/"><u>相关仲裁帖子</u></a>中的§1.4）是很好的，因为毕竟规范性评估是我们经常想谈论的事情。但我们<i>也</i>迫切需要能够描述世界如何独立于我们的感受而运作的概念。我将在下一篇文章的第 3.4 节中详细讨论这个主题。</p><p>话虽如此，我们不能总体上“反”规范启发法，因为如果我们把它们拿走，那就什么都没有剩下了！毕竟，大脑需要<i>某种</i>方式来评估计划/行动/未来的好坏，包括我们以前从未遇到过的计划/行动/未来。</p><p>所以实际上，我认为给概念赋予价是可以的，甚至给比较宽泛的概念赋予相当强的价在某种程度上也是可以的。我自己的大脑似乎至少为一些相当广泛的概念分配了一些非零价，例如“共产主义”（嘘）、“贫困”（嘘）、“人工智能对齐”（耶）、“冰淇淋”（耶）和等等，我目前认为我不应该太努力地改变这一点。重要的是<i>养成提出后续问题的习惯，这些问题侧重于细节并探究一般规则的例外情况</i>。</p><p>例如，问问自己：确切的建议是什么？它可能产生哪些有意或无意的后果？当我们说“共产主义”时，我们是在谈论“斯大林的共产主义”还是“邓小平的共产主义”还是“狩猎采集小部落的共产主义”？当我们说“冰淇淋”时，我们是在谈论“哈根达斯冰淇淋”还是“海绵宝宝冰棒冰淇淋”？当我们说“AI 对齐”时，我们是在谈论“我定义的 AI 对齐”还是“Sam Altman 定义的 AI 对齐”？ ETC。</p><p>如果<i>具体细节</i>在你的脑海中很突出，那么，我声称，相应的规范启发法更有可能给出良好的评估，经得起审查和事后诸葛亮。</p><p> （顺便说一句，如果你养成这种习惯，<i>随着时间的</i>推移，它可能会导致为模糊的广泛概念分配更中性的效价，为更具体的特征分配更强的效价。这可能是一件好事。）</p><h1> 2.5 化合价如何设定和调整？ </h1><figure class="image image_resized" style="width:67.18%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/jszcdax7vbhbxebhlau4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rdxaoi3kfhm2tiase154 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/o8znjxkggojkxgvvtjlf 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kvibz9spxvn4gz3maoqb 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/eld6v5v4fypxs0bqcaj2 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/lqz98ny7ffp9uhqwgfbt 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/frghxaaqbnc8xxwt7joc 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/s1uubt4yekrh91tiaxro 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/pkxccpamapexa8elztd7 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/w6cksg1dlrxa9lcemytv 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/stsl40xxhye7ovycuvls 955w"><figcaption>我声称效价最终源自先天驱动力，但随后通过诸如时间差异（TD）学习之类的东西在不同概念（思想片段）之间传播。举第一个例子，如果我的手在接触炉子时被烫伤，那么一种与生俱来的驱动力就会介入，并表示有不好的事情发生了，然后触摸炉子的想法就会获得负价。作为第二个例子，考虑<a href="https://medium.com/@robertwiblin/ugh-fields-or-why-you-can-t-even-bear-to-think-about-that-task-5941837dac62"><u>“呃字段”</u></a> ——如果我经常对自己想“我要清理我的电子邮件收件箱”，并且这总是立即产生高度厌恶的后续想法“......这意味着我需要为了处理NED的那个烦人的电子邮件”，即使我没有明确呼唤NED烦人的电子邮件，“我要清理我的电子邮件收件箱”<i>本身</i>也会变得有些厌恶（负价）。</figcaption></figure><p>如果您追溯到足够远的地方，我声称<strong>所有价最终都会直接或间接地从先天的驱动器中流动 -</strong>宣布痛苦不好的一般指定的电路，而饮食却是好的，而渴望成为社会地位的饮食（出现在第4个文章中），以及与同情，报仇和内gui相关的其他各种社会驱动器，可能还有数百种类似的事情。</p><p> （在强化学习术语中 - 请参阅上<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_">一篇文章的第1.3节</a>- 您的“先天驱动器”更像是对奖励功能的各种术语。在神经解剖学术语中， <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>我声称</u></a>实施这些先天驱动器构成了关键责任下丘脑和脑干，尽管其他大脑区域也涉及。）</p><p>为了过分简化，我声称思想可以是正价或负面价的两个主要原因：</p><ul><li>思想可能是正价或负面价的<strong>第一个原因</strong>是，先天的驱动力正在介入并宣布该思想分别是好是坏。 （例如，如果我饿了，我开始吃美味的食物，那么当时我在想什么（可能是食物），这显然是一件好事。）</li><li>思想可能是正价或负面价的<strong>第二个原因</strong>是，我们的大脑具有一种<i>学习算法</i>来猜测哪种价值分配最适合于思想，这是基于实际价值分配的过去历史。当天生的驱动器并不是<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_2_Toy_model_of_a__long_term_predictor__circuit"><u>“超越”猜测</u></a>时，猜测就会流入一个自我实现的预言中。 （学习算法与<a href="https://en.wikipedia.org/wiki/Temporal_difference_learning"><u>时间差异（TD）学习</u></a>有一定的关系；我抛弃了所有细节，尽管请参阅<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/F759WQ8iKjqBncDki#5_3_1_Switch__i_e___value___expected_next_reward__vs_summation__i_e___value___expected_sum_of_future_rewards__"><u>此处的</u></a>更多信息。）</li></ul><p>撇开所有机械细节，该学习算法具有各种效果：例如：</p><ul><li>如果认为θ在您认为的所有之前都具有一定的价，那么学习算法将开始猜测θ（思想与θ足够相似）将来会有相同的价。</li><li>如果X（例如打开Twitter）经常立即导致Y（例如看到“喜欢”通知），即作为现实世界中的时间序列，那么学习算法将开始猜测X的适当价更接近于任何内容价恰好是y。</li><li>同样，如果认为X经常立即导致您的大脑中的思想，则同上上一个子弹点。 （换句话说，TD学习的“时间”部分可以完全放在您大脑的思维列车中，而与外界的时间序列无关。）</li></ul><p>要解决可能的误解：先天驱动器与目标或终端目标不同。您的目标，例如“摆脱债务”或“减少苦难”，是由您的世界模型中的概念（又称<a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"><u>“思想生成器”</u></a> ）建立的，即您一生中学到的概念。天生的驱动器不是 - 目的是，它们是在某些情况下吐出某些信号的基因组建造的小机器。有时我会谈论“避免痛苦的先天动力”或类似的事情，但这确实是更奇特的“电路”，它检测到与我们通常所说的“疼痛”相关的外围信号，然后吐出负面价（在其他信号中），因此随着时间的推移，该电路的典型效果是让我们避免疼痛。”</p><h2> 2.5.1特殊情况：一起进行的事情倾向于获得类似的价</h2><p>在<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_1__Valence___as_I_m_using_the_term__is_a_property_of_a_thought_not_a_situation__nor_activity__nor_course_of_action__etc_">上一篇文章的第1.5.1节</a>中，我提出了一个关于去健身房“混合感”的例子。如果我关注它的一个方面，它具有正价（“我要去健身房，因此要遵循我的新年解决方案”）。如果我注意它的另一个方面，它具有负面价（“我要去健身房，这真的很大声又冷”）。</p><p>可能是，在这两个框架之间，在一两个框架内立即连续地进行了很多心理翻转。换句话说，当我考虑积极的方面时，也许会慢跑我的记忆以调用负面方面，反之亦然。如果是这样，我认为TD学习将倾向于更新我的价功能，从而使积极价值更为负面，反之亦然（根据上面的第三个项目符号点）。具体来说，也许遵循我的新年决议的想法会开始降低动力，更厌恶，相反，也许会考虑体育馆的响度和寒冷开始会变得<i>有些</i>厌恶。 <span class="footnote-reference" role="doc-noteref" id="fnref6q9syee1mov"><sup><a href="#fn6q9syee1mov">[2]</a></sup></span></p><h1> 2.6元认知和自我反思思想的价：自我合成的vs-dystonic思想，自称为“价值”，等等。 </h1><figure class="image image_resized" style="width:24.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/tz2aw0v6l0ei9pwnwnz4" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/mofolnogceg3ar9d7u8p 115w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xnxqdmknjlugrtvr7wge 195w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/gk8llgybcvjpjzyng91k 275w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/ryolrrtufvajtuceshqi 355w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xdon4zacamsmpzahxjxi 435w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/qxwq1byfvsee3tji0joz 515w"></figure><p>假设我沉迷于香烟，但想戒烟。鉴于我沉迷于香烟，有时“现在吸烟的想法”将是非常积极的价值 - 换句话说，“有时我会感到吸烟的冲动”  - 因此，我会吸烟。</p><p>但这很烦人！我想辞职！由于我不是一个完全的白痴，所以我自然会注意到，“渴望我对烟”是一个障碍，这使我无法跟随我的动机，这是一个障碍。这使我不喜欢“急于摩托车”。</p><p>现在，我们进入了自我反思偏好领域！</p><p>因此，“吸烟有时会让我感到积极的想法”，这是我<i>自己的</i>脑海评估，而价值则是负面的。这将是<i><strong>自我差异</strong></i><strong>评估</strong>的一个例子。我想抽烟，但我<i>不想</i>吸烟。</p><p>现在，再次假设一个想法突然出现在我的脑海中：“也许我现在就吸烟了”。如果这种想法需要注意预期的感官感觉等，那么这种想法的价将是积极的，所以我会抽烟。如果想法大多需要忽略这些思想，而要注意情况的更自我反射的方面（也许是从外面看我自己，也许就是这样），那么这种想法的价值将是负面的，所以我不会抽烟。实际上，可能两种类型的想法都会在不同的时间突然出现，而价值的相对优势将有助于确定事物如何摆脱。</p><p>自我dystonic的对立面是<strong>自我的同伴</strong>。例如，“我自己倾向于诚实和尽责”的想法是自我合成的。因此，如果计划模式与自己视为诚实和认真的态度，那么该计划会赢得价值提高，因此我更有可能这样做。但仅<i>在边缘</i>。该计划也有其他方面，这也有助于价值，实际上这些其他方面通常更为突出。</p><p>这里的一个重要观察结果是，<i><strong>自称的</strong></i><strong>目标和价值观，远远超过行动，往往是由事物是自我合成还是 - 典型的。</strong>考虑：如果我大声说出（或对我自己）（例如“我要戒烟”或“我关心我的家人”），那么我脑海中的实际直接想法主要是“我要执行此操作特定的言语行为”。正是<i>这种</i>想法的价决定了我们是否说这些话。这种思想的自我反思方面非常重要，因为说话需要思考听众如何收到您的话。相比之下，在某些不确定的未来中，该宣言的<i>内容</i>（实际上是戒烟或实际关心我的家人）既不显着，也不是直接的（请参阅<a href="https://en.wikipedia.org/wiki/Time_preference"><u>时间限制</u></a>）。因此，《言语法》的净价可能包含戒烟的自我反射方面的<i>巨大</i>价值，以及从戒烟或关心我的家人的更直接感官和其他后果所产生的<i>小</i>价值贡献。<i>即使</i>我们打算遵循我们所说的话，我们也是100％真诚的。 （另请<a href="https://www.lesswrong.com/posts/yDRX2fdkm3HqfTpav/approving-reinforces-low-effort-behaviors"><u>参阅批准加强低劳动行为</u></a>，博客文章的观点与本段相似。） </p><figure class="image image_resized" style="width:73.71%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/rcvfrjif77p3aam2aw6c" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/kqpsl8ljw1xjkggyp3tu 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/myrfhtlsewfnonpmlljq 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/nnta32ypxvzvhic2uy14 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/wy70orgjcuigufl8noxn 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xenavi7epj1nps9hnji5 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/zjwci8fwzhzmtkphmrz4 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/xkztuwtxn2lashfztqum 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/n87ivhqtqmwtvlinnjed 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/l0tuyylx95lsytazgsrl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/SqgRtCwueovvwxpDQ/p6x3gfovyynzxfhia9vj 866w"></figure><h2> 2.6.1什么是“值”？</h2><p>我提出这个话题是因为它经常出现在我的职业中（AI对齐）。但是我不确定它是否有深刻的答案；它使我感到惊讶，主要只是关于定义的论点。</p><p>这是我喜欢的“价值”的定义，因为它似乎与人们在我的日常生活中使用该词的使用一致：<i>如果我想知道某人的“价值观”，我会问他们，并写下他们的话。然后，我会宣布，我写下的任何东西都是“他们的价值观”，从定义上讲或无数，例如，例如，他们故意拖着我。</i></p><p>根据这个定义，“价值观”可能包括听起来非常好，社交批准和自我同胞的事物，例如“照顾我的家人和朋友”，“使世界变得更好”，等等。</p><p>另外，根据这个定义，“价值”可能对某人的行为产生宝贵的影响。在这个（极为常见的）情况下，我想说：“我想这个人的<i>欲望</i>与他的<i>价值观</i>不同。哦，那里不足为奇。”</p><p>的确，我认为“价值观”包括“成为好朋友”的人实际上是一个坏朋友，这是完全正常的。那么，这个“价值”是否有任何影响？是的！！我希望，在这种情况下，这个人<i>要么</i>对他们是个坏朋友感到难过，<i>要么</i>否认他们是坏朋友，<i>或者</i>根本没有考虑这个问题，<i>或者</i>想出其他一些问题借口他们的行为。如果这些事情都没有发生，那么（只有那时）我会说“成为好朋友”实际上<i>不是</i>他们的“价值观”之一，如果他们否则陈述，那么他们会撒谎或困惑。</p><h1> 2.7道德推理</h1><h2>2.7.1描述性帐户</h2><p>如上所述，我认为正相关的区别与大脑大规模解剖结构和算法的主要组成部分具有非常干净的对应关系。</p><p>但是，<i>在规范领域内</i>，我认为<i>“ X是道德 /道德 /贤惠 /等的区别”</i>和<i>“我想做X”</i>并不是太基本了 - 它更像是模糊类别之间的边界。例如，当人们自我合成和社会批准时，人们倾向于将道德 /道德 /良性称为事物。但是该规则也有例外。例如，黑手党可能以他的“不道德行为”感到自豪。即使在那种情况下，黑手党也可能仍在弄清楚什么是和没有自我的合成和社会批准的，他在自己的犯罪社会基础上，他的思想甚至可能在结构上类似于“正常”的道德推理。但是他不会使用“道德”一词。</p><p>无论如何，假设您是一个普通的守法的人，他真诚地想在道德上行事。因此，您坐在扶手椅上，试图弄清楚什么是道德上的事情。接下来发生什么？很多东西：</p><ul><li>也许您会考虑对象级别的事情，例如您想象一个孩子受苦。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您会考虑元认知事物，例如“我会仔细思考，而不是急于判断”，或者相反，“我应该更信任自己的肠道”。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您想到的是自我反思的事情，例如<a href="https://joecarlsmith.com/2023/02/17/seeing-more-whole"><u>“我在世界上是什么力量？”</u></a>或“我的朋友现在能读我的想法，该如何看待我？”。也许这些想法具有价值，即，您发现您对这些事情有偏好。</li><li>也许您注意到其中一些偏好与其他偏好不一致。然后，您会想到更多的想法，在这些想法中，这些不同的偏好可能会淘汰它。</li><li>在整个过程中，您的大脑学习算法都在不断更新您的信念和欲望。</li></ul><p>在本系列的背景下，我要提出的一点是，<strong>每个步骤都取决于价的基础。</strong>例如，如果您的大脑对痛苦的孩子的想法没有任何特殊的价值，那么您就不会感到有动力阻止孩子痛苦。</p><p>为了使这个更直观：我想告诉你：“穿均匀的皮带循环的裤子很重要”，然后你说“为什么？”，然后我说“是的，如果你不这样做，然后，您的裤子会有<i>奇数</i>的皮带循环！那会很糟糕！！”，然后你再说：“嗯？为什么那不好？”。这次对话无处可去。</p><p>相比之下，如果我说“如果您遵循X原则X，那么被压迫的群众就会得到正义”，<i>现在</i>我可以激发您的兴趣，因为“被压迫的群众会得到正义的正义”已经在您的大脑中巩固了一个吸引人的兴趣（积极 -价）概念。</p><p>如上所述（第2.5节），我要提出的另一个重要的一点是，这<strong>是从先天的驱动器中（通常是间接）（尽管通常是间接）的</strong>。就像第2.5节中一样，在您一生中的每一个醒来的第二个醒来，价值都基于您的观察和思想列车从概念流向其他概念。但是，如果您追溯到足够远的地方，则价值最终必须从您的天生驱动器中获得。在道德推理的情况下 - 上面的“被压迫群众将得到正义”的例子 - 我认为相关的先天驱动器几乎可以肯定是天生的<i>社会</i>驱动器，即与同情心，复仇，社会地位有关的先天驱动器（在Post 4中出现）， 等等。</p><h2> 2.7.2对“道德的真实本质”的影响（如果有）</h2><p>我离元伦理学专家<i>很</i>远，但是以上帐户似乎有些事情。</p><ul><li>如果存在<i>完全独立</i>于人类先天驱动器和当前模因环境的特殊性的“真正的道德”，那么上述过程应该融合到它，或者确实有任何事情要做用它。如果这样做，我认为我们必须宣布这是一个快乐的巧合。 （有关这一点的更深入的讨论，请参见乔·卡尔史密斯（Joe Carlsmith）的文章<a href="https://joecarlsmith.com/2022/01/17/the-ignorance-of-normative-realism-bot"><u>“对规范现实主义机器人的无知”</u></a> ）。</li><li>实际上，没有任何明显的原因是上述过程应融合到<i>任何</i>定义明确的目的地。我<i>不</i>认为这类似于数学。在数学中，如果有一个假设的代理人具有“天生的性格”来“喜欢”某些数学公理及其逻辑后果，那么对于代理人将会或不会“喜欢”的内容，就有一个独特的答案，如果有足够的时间来反思 - 即，这些公理可以证明的数学语句。相比之下，如果人类从天生的驱动器开始，并经历了上述过程，那似乎更加混乱。例如，一旦您开始“喜欢”某种元认知模式，它就会改变审议未来步骤的地面规则。因此，在过程结束时很可能存在不确定的路径依赖性。否则，它可能会在各种不令人满意的状态周围无尽的周期之后结束。</li><li>即使上述过程<i>确实会</i>收敛到独特且定义明确的目的地，但对于每个人来说，它不一定是相同的目的地（并且忘记外星人或AIS）。实际上，我会走得更远，猜测这对不同的人来说可能是不同的，而不仅仅是在社会变态等异常情况下，而且在整个人群中广泛。 （例如，我认为 <a href="https://www.lesswrong.com/posts/BkkwXtaTf5LvbA6HB/moral-error-and-moral-disagreement"><u>这篇文章</u></a>的结尾太乐观了。）我认为的原因是：目的地<i>肯定</i>会取决于正在审议的人的天生驱动器，也<i>可能</i>取决于他们的人生历史，尤其是模因环境。让我们专注于第一个驱动器。我承认，几乎所有人类都具有<i>定性</i>的先天驱动器，它们以相同的方式建造。但是我认为它们<i>在定量上</i>并不相似。尤其是，各种不同的先天驱动器的相对优势似乎因人而异。例如，我认为有些人会感到状态驱动力（在第4个帖子中出现）非常敏锐，而另一些人则感觉更温和，并且与与同情心，复仇等的先天驱动器相提并论。这些相对优势很重要，因为道德充满了不同直觉相互贸易的情况。</li><li>比较“我希望被压迫的群众找到正义”与“我站着太久，我想坐下来”。<i>这两个“欲望”从根本上是由同一思维构成的。</i>他们俩都源于正价，这反过来又来自先天的驱动器（特别是在第一种情况下是社会驱动器，以及第二种情况下的稳态持持稳态驱动器）。因此，如果“真正的道德”或“真正的人类道德”或不存在的任何事物，那么这<i>并不</i>构成坐下而不是寻求正义的理由。您仍然必须做出决定。这就是我的意思<a href="https://www.lesswrong.com/posts/32ca3B7rJ93xo9tvb/thoughts-on-agi-consciousness-sentience#How_does_that_feed_into_morality_"><u>“虚无主义不是与决策相关的”</u></a> ，也是Yudkowsky的 <a href="https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality"><u>“您将在没有道德的情况下做什么？”</u></a> 。确实，您会注意到我首先谈论了审议过程，其次是哲学。审议过程是由您自己动机的引擎提供动力的，无论他们是什么，无论好坏，都不是由任何外部规范真理。</li></ul><h2> 2.7.3可能对AI对齐话语的可能影响</h2><p>作为一名AI一致性研究人员，就当地知识界的想法而言，我可能应该准确地阐明自己的意思。<i>本节是低信心和术语繁重的； AI对齐字段外的读者应该随时忽略。</i></p><ul><li>我担心<a href="https://www.lesswrong.com/tag/coherent-extrapolated-volition"><u>CEV</u></a>定义不明确。或更具体地说，您可以列出CEV的许多同样的<i>A-Priori</i>详细操作，并且它们会以我们发现非常不满意的方式给出重要的不同结果。</li><li>相关地，我担心“<a href="https://forum.effectivealtruism.org/topics/long-reflection"><u>长期反思</u></a>”不会解决我们希望它能解决的所有重要事情，或者以与长期反思治理 /话语规则的细节无关的方式解决它们，而没有明显的方法来决定众多合理的治理 /话语规则中哪种是“正确的”。</li><li>当人们做出隐式将“未来价值”视为明确定义的陈述时， <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>例如</u></a>“我将“强乌托邦”定义为：至少实现了未来潜在价值的95％”，我担心这些陈述的意义不如听起来。</li><li>我担心，几代人的人类价值变化在某种程度上更像是一个随机步行，而不是进步时间，而且他们只会感觉像<a href="https://willywizz.wordpress.com/2012/09/24/paint-the-target-around-the-arrow/"><u>进步</u></a>时间<a href="https://willywizz.wordpress.com/2012/09/24/paint-the-target-around-the-arrow/"><u>箭”</u></a> 。因此，当我们说“永恒的价值锁定是不好的 - 我们想为我们的后代提供道德增长的空间！”，我们还同时说出特定的话代理和探索，几乎没有痛苦和痛苦，以及……！”，然后我担心这两个陈述至少有些矛盾，也许有很大的矛盾。 （如果事实证明我们只需要选择这两个陈述中的一个，我就不知道我要投票给哪个陈述。）</li><li>以上都不是我们不应该试图解决AI对齐问题的论点，或者我们不应该关心未来的发展。与以前的小节一样，我们仍然可以而且必须做出决定。我渴望避免充满酷刑和奴隶制的未来宇宙（以一个特别的榜样）一如既往地强烈，如果您分享这种愿望，那么让我们一起努力使它实现。</li></ul><h1> 2.8结论</h1><p>同样，我认为价是在大脑中建立所有规范性的物质。希望这篇文章能阐明这些难题的一些碎片。我愿意接受思想和讨论，在下一篇文章中，我们转向了较小但仍然存在实质性的影响（与规范性的）域名的信念，期望，概念等。</p><p><i>感谢TSVI Benson-Tilsen，Seth Herd，Aysja Johnson，Justis Mills，Charlie Steiner，Adele Lopez和Garrett Baker对早期草稿的批判性评论。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fntss4t9kg1lr"> <span class="footnote-back-link"><sup><strong><a href="#fnreftss4t9kg1lr">^</a></strong></sup></span><div class="footnote-content"><p> “ Bulverism”是一个术语，其意思是“假设某些信念是不正确的，然后就人们如何持有那些不正确的信念的人进行心理猜测”。但是，有关道德现实主义的<i>更多</i>信息，请参见下面的§2.7。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6q9syee1mov"> <span class="footnote-back-link"><sup><strong><a href="#fnref6q9syee1mov">^</a></strong></sup></span><div class="footnote-content"><p>在这个示例中 - “也许遵循我的新年决议的想法会开始变得有些动机，更厌恶，相反，也许会思考健身房的响亮和寒冷，会开始感到<i>有些</i>厌恶。” - 我实际上认为这种转变可以通过两种方式发生，并且大脑同时进行了这两个过程。文本中描述的第一个涉及通过TD学习对不同思想的价变化。我不会讨论的第二个是因为它是较少的主题，涉及世界模型（又称“思想生成器”）及其概念网络及其关联 /内涵的变化。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity<guid ispermalink="false"> SqgRtCwueovvwxpDQ</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Thu, 07 Dec 2023 16:43:49 GMT</pubDate> </item><item><title><![CDATA[AISN #27: Defensive Accelerationism, A Retrospective On The OpenAI Board Saga, And A New AI Bill From Senators Thune And Klobuchar]]></title><description><![CDATA[Published on December 7, 2023 3:59 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><hr><h2>防御性加速主义</h2><p>以太坊的创始人 Vitalik Buterin 最近<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">写了一篇关于人工智能和其他技术的风险和机遇的文章</a>。他回应了马克·安德森 (Marc Andreessen) 关于技术乐观主义和有效加速主义 (e/acc) 运动发展的宣言，并提供了更细致的观点。</p><p>文章认为，技术往往对人类有好处，但人工智能可能是这一规则的例外。 Buterin 认为，我们不应该让政府控制人工智能来保护我们，而是应该建立防御性技术，为去中心化社会中的灾难性风险提供安全保障。 Buterin 认为我们应该构建网络安全、生物安全、有弹性的物理基础设施和强大的信息生态系统来保护自己免受人工智能风险的一些技术。</p><p><strong>技术有风险，但监管不是万能药。</strong>更长的寿命、更低的贫困率以及扩大接受教育和信息的机会是 Buterin 归功于技术的众多成功之一。但大多数人都会认识到，技术也会造成危害，例如全球变暖。 Buterin 特别<a href="https://twitter.com/VitalikButerin/status/1729251822391447904">表示</a>，与大多数技术不同，人工智能对人类构成了生存威胁。</p><p>为了应对这一风险，一些人主张<a href="https://arxiv.org/abs/2310.09217">政府对人工智能的发展进行强有力的控制</a>。 Buterin 对这个解决方案感到不舒服，他预计许多其他解决方案也会如此。历史上许多最严重的灾难都是由斯大林和毛泽东等强大的政治人物故意造成的。人工智能可以帮助残暴的政权监视和控制大量人口，而 Buterin 对于通过将人工智能开发从私人实验室推向公共实验室来加速这一趋势持谨慎态度。</p><p>在不受限制的技术发展和政府绝对控制的极端之间，Buterin 主张一条新的前进道路。他把他的哲学称为 d/acc，其中“d”代表国防、民主、权力下放或<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4213670">差异化技术发展</a>。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc365b23e-2340-47ba-8f6b-04cf5e6efa97_500x536.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/zm2qw8smimbll3172z6y 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ci9uhres7aqircceej9n 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/gwdpnppfzrthwzyxb5kq 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/caiy1pg3dobqjuc1wsgg 1456w"></a></p><p><strong>去中心化世界的防御技术。</strong> Buterin 主张加速技术发展，保护社会免受灾难性风险的影响。具体来说，他强调：</p><ol><li><strong>生物安全</strong>。人口稠密的城市、频繁的航空旅行和现代生物技术都会增加流行病的风险，但我们可以通过改善空气质量、加快疫苗和治疗方法的开发以及监测新出现的病原体来改善我们的生物安全。</li><li><strong>网络安全</strong>。可以编码的人工智能可以用于网络攻击，但防御者也可以使用它们在安全漏洞被利用之前发现并修复它们。 Buterin 在区块链方面的工作旨在实现一些数字系统可以被证明安全的未来。</li><li><strong>有弹性的物理基础设施</strong>。核灾难中预期的大多数死亡并非来自爆炸本身，而是来自食品、能源和其他必需品供应链的中断。埃隆·马斯克渴望改善人类的物质基础设施，减少对化石燃料的依赖，通过卫星提供互联网连接，并理想地使人类成为能够在地球灾难中幸存的多行星物种。</li><li><strong>强大的信息环境</strong>。为了帮助人们在人工智能说服时代找到真相，Buterin 提到了预测市场和共识生成算法，例如<a href="https://vitalik.eth.limo/general/2023/08/16/communitynotes.html">社区笔记</a>。</li></ol><p>科学家和首席执行官可能会发现自己受到 Buterin 构建技术而不是放慢技术发展目标的启发。然而，对于那些担心人工智能和其他灾难性风险的人来说，Buterin 对最有可能保护我们文明安全的技术提出了深思熟虑的看法。有兴趣的朋友可以<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">看看全文</a>，还有更多的想法。</p><h2> OpenAI Board 传奇回顾</h2><p>11 月 17 日，OpenAI<a href="https://openai.com/blog/openai-announces-leadership-transition">宣布</a>董事会解除 Sam Altman 首席执行官职务。经过四天的公司政治和谈判，他重新担任首席执行官。在此，我们回顾一下这一系列事件的已知事实。</p><p> <strong>OpenAI 旨在由非营利性董事会控制</strong>。 OpenAI<a href="https://openai.com/blog/introducing-openai">成立</a>于 2015 年，是一家非营利组织。 2019 年，OpenAI<a href="https://openai.com/blog/openai-lp">宣布</a>成立一家营利性公司，为其昂贵的大型语言模型扩展计划提供资金。投资 OpenAI 所能产生的利润最初被“限制”在 100 倍——超出这个范围的任何东西都将被重新定向到非营利组织。但 <a href="https://www.economist.com/business/2023/11/21/inside-openais-weird-governance-structure">最近规则发生变化</a>后，从 2025 年开始，该上限将每年上涨 20%。</p><p> OpenAI 的<a href="https://openai.com/our-structure">公司结构</a>旨在使非营利组织能够保留对营利组织的合法控制权。该非营利组织由董事会领导，董事会通过其选择和罢免营利组织首席执行官的能力对营利组织拥有权力。董事会负责维护非营利组织的使命，即确保通用人工智能造福全人类。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ovk1ri8hu5yrbuu47kkn 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/agtx8u8u0jgpytwzxxft 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/ffahdjhdbaimt7uhakes 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qQvqzFKbfNQovrSQn/bu08tjivlccrh7j1xhau 1456w"><figcaption> OpenAI 的法律结构。</figcaption></figure><p><strong>董事会解除萨姆·奥尔特曼 (Sam Altman) 首席执行官职务。</strong>在解除 Sam Altman 首席执行官职务时，董事会成员包括 OpenAI 首席科学家 Ilya Sutskever、Quora 首席执行官 Adam D&#39;Angelo、技术企业家 Tasha McCauley 和 CSET 的 Helen Toner。 OpenAI 联合创始人兼总裁格雷格·布罗克曼 (Greg Brockman) 与萨姆·奥尔特曼 (Sam Altman) 一起被免去董事会主席职务。</p><p>公告称，董事会解雇了奥特曼，因为他“在与董事会的沟通中没有一贯坦诚，阻碍了董事会履行职责的能力”。虽然董事会在公告中没有提供任何奥特曼欺骗的具体例子，但后来有<a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai">报道称</a>，奥特曼曾试图让董事会成员相互对立，试图除掉海伦·托纳。</p><p>奥特曼早些时候曾就她与人合着的一篇<a href="https://cset.georgetown.edu/publication/decoding-intentions/">论文</a>与托纳对质。论文部分批评 OpenAI 发布 ChatGPT 加速了 AI 发展的步伐。它还赞扬了 OpenAI 的竞争对手之一 Anthropic 推迟发布当时的旗舰模型 Claude。</p><p> <strong>OpenAI 员工反对董事会。</strong>该公告的影响迅速而戏剧性。几天之内，Greg Brockman 和 Mira Murati（最初的临时首席执行官）辞职，几乎所有 OpenAI 员工都<a href="https://www.forbes.com/sites/tylerroush/2023/11/20/more-than-500-openai-employees-threaten-to-quit-over-sam-altmans-removal/?sh=6d45164b4ebc">签署了一份请愿书</a>，威胁要辞职并加入微软，除非 Sam Altman 复职并且董事会成员辞职。据报道，在谈判过程中，董事会成员 Helen Toner <a href="https://www.cnn.com/2023/11/20/tech/openai-employees-quit-mira-murati-sam-altman/index.html#:~:text=As%20they%20sought%20to%20manage,intelligence%20benefits%20all%20of%20humanity.%E2%80%9D">表示</a>，允许 OpenAI 由于投资者和员工的离开而被摧毁，这“符合我们的使命”。伊利亚·苏茨克瓦尔 (Ilya Sutskevar) 后来倒戈并加入了请愿活动，并<a href="https://x.com/ilyasut/status/1726590052392956028?s=20">在推特上写道</a>：“我对参与董事会的行动深感遗憾。”</p><p><strong>微软试图挖走 OpenAI 员工。</strong>微软——OpenAI 最大的少数股东——没有被告知董事会的计划，并为 OpenAI 员工提供了自己的人工智能研究团队的职位。微软似乎已经<a href="https://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-employees-openai">成功聘用了</a>Sam Altman、Greg Brockman 和其他 OpenAI 高级员工。</p><p><strong>萨姆·奥尔特曼 (Sam Altman) 重新担任首席执行官</strong>。 11 月 21 日，OpenAI<a href="https://twitter.com/OpenAI/status/1727206187077370115">宣布</a>已达成协议，Sam Altman 将重新担任首席执行官并重组董事会。最初的董事会成员包括前 Salesforce 首席执行官 Bret Taylor、前财政部长 Larry Summers 和 Adam D&#39;Angelo。董事会最初的<a href="https://twitter.com/OpenAI/status/1727206187077370115">首要目标</a>之一是扩大董事会规模，其中将包括一名来自微软的无投票权成员。萨姆·奥尔特曼回国后还面临对其行为的<a href="https://www.theinformation.com/articles/breaking-sam-altman-to-return-as-openai-ceo">内部调查</a>。</p><p>这一系列事件标志着OpenAI内部治理结构发生了重大变化。</p><h1>克洛布彻和图恩的“温和”参议院法案</h1><p>参议员 Amy Klobuchar 和 John Thune 提出了一项<a href="https://www.thune.senate.gov/public/_cache/files/7dea8daa-f6d1-4881-ad21-2381fcbe0785/6362CE1D0A17743166BC170A593B5CDA.ccaskfall23a15.pdf">新的人工智能法案</a>。它将要求构建高风险人工智能系统的公司自我证明他们遵循建议的安全指南。值得注意的是，该法案仅关注为招聘和医疗保健等高风险领域构建的人工智能系统，但其主要条款不适用于包括 GPT-4 在内的许多通用基础模型。</p><p><strong>该法案监管特定的人工智能应用，而不是通用人工智能系统。</strong>这种基于应用程序的方法类似于<a href="https://artificialintelligenceact.eu/the-act/">欧盟人工智能法案</a>初始草案所采取的方法，该法案指定了人工智能系统可能用于敏感目的的领域，使其具有高风险。像 ChatGPT 这样的通用模型并不在该法案的范围内，但这些模型的公开使用及其功能的展示引发了关于如何对其进行监管的<a href="https://cset.georgetown.edu/article/the-eu-ai-act-a-primer/">争论</a>。</p><p>这表明参议院法案目前采取的做法可能还不够。通过根据应用程序管理人工智能系统，功能强大的通用系统将不受监管。</p><p><strong>风险评估是强制性的，但执行起来可能很困难。</strong>该法案要求高风险人工智能系统的开发者和部署者每两年进行一次评估，评估如何理解和管理其系统的潜在风险。此外，商务部将在即将成立的人工智能认证咨询委员会的帮助下制定认证标准，该委员会将包括行业利益相关者。</p><p>由于公司被要求自我证明自己遵守这些标准，因此商务部确保公司确实遵守规则非常重要。但该法案提供的执行选项很少。该机构没有获得任何额外资源来执行新法律。此外，只有在确定故意违反该法案的要求时，他们才能阻止模型的部署。如果人工智能系统意外违反法律，该机构将能够对构建该系统的公司处以罚款，但无法禁止其部署。</p><p><strong>强制识别人工智能生成的内容。</strong>该法案将要求数字平台在用户看到人工智能生成的内容时通知用户。为了确保恶意行为者无法将人工智能生成的内容冒充为真实内容，NIST 将制定新的技术标准来确定数字内容的来源。</p><h2>链接</h2><ul><li>谷歌 DeepMind 发布了<a href="https://deepmind.google/technologies/gemini/#introduction">Gemini</a> ，这是一种新的人工智能系统，与 GPT-4 Vision 类似，并在各种基准测试中以微弱优势击败了它。</li><li>唐纳德·特朗普表示，作为总统，他将立即<a href="https://www.washingtonexaminer.com/news/campaigns/trump-vows-cancel-biden-executive-order#google_vignette">取消拜登关于人工智能的行政命令</a>。</li><li>商务部长吉娜·雷蒙多 (Gina Raimondo) 就人工智能、中国、GPU 出口管制等话题<a href="https://twitter.com/jordanschnyc/status/1732044427005464860">发表了讲话</a>。</li><li> 《纽约时报》发布了关于当今主要 AGI 实验室起源的<a href="https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html">简介</a>。</li><li>国会研究服务发布了一份关于<a href="https://crsreports.congress.gov/product/pdf/R/R47849">生物学人工智能</a>的新报告。</li><li> Inflection<a href="https://inflection.ai/inflection-2">发布了</a>另一个LLM，性能介于GPT-3.5和GPT-4之间。</li><li>来自中国开发者的一个<a href="https://github.com/deepseek-ai/DeepSeek-LLM">新的开源 LLM</a>声称其性能优于 Llama 2。</li><li>这是关于人工智能监管的法律和政策观点的<a href="https://www.legalpriorities.org/blog/2023/ai-syllabus/">新教学大纲</a>。</li><li>两所瑞士大学启动了一项关于人工智能和人工智能安全的<a href="https://www.swiss-ai.org/">新研究计划</a>。</li><li> BARDA 正在<a href="https://www.plugandplaytechcenter.com/barda-innovation-challenge/">接受资助应用于健康安全和 CBRN 威胁的人工智能的申请</a>。</li><li>生命未来研究所的新附属机构将<a href="https://www.flf.org/">孵化新的组织</a>来应对人工智能风险。</li><li>对于那些参加 NeurIPS 2023 的人，<a href="https://lu.ma/aisi-nola">英国人工智能安全研究所</a>将举办一场活动，并且还将举办一场<a href="https://www.mlsafety.org/neurips-social-2023">人工智能安全社交活动</a>。</li></ul><p>另请参阅： <a href="https://www.safe.ai/">CAIS 网站</a>、 <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> 、<a href="https://newsletter.mlsafety.org/">技术安全研究通讯</a>、<a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a>。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qQvqzFKbfNQovrSQn/aisn-27-defensive-accelerationism-a-retrospective-on-the<guid ispermalink="false"> qQvqzFKbfNQovrSQn</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:59:12 GMT</pubDate> </item><item><title><![CDATA[AI #41: Bring in the Other Gemini]]></title><description><![CDATA[Published on December 7, 2023 3:10 PM GMT<br/><br/><p><span style="color:initial">本周最大的新闻终于是</span><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow">谷歌 Gemini 的发布</a><span style="color:initial">。请务必检查一下。请注意，现在推出的只是 Gemini Pro，可以与 GPT-4 竞争的 Gemini Ultra 型号尚未推出。</span></p><p>我似乎并没有做得很好，以足够快的速度削减所包含的材料以跟上步伐。很多事情正在发生，但很多事情可能会持续很长时间。如果您的时间有限，请记住重点关注与您兴趣相关的部分。</p><p>另外，如果您要参加纽约至日活动或相关聚会，请务必打个招呼。</p><p></p><span id="more-23625"></span><p></p><h4>目录</h4><p><a href="https://thezvi.substack.com/p/gemini-10" target="_blank" rel="noopener noreferrer nofollow"><strong>我今天的另一篇文章介绍了 Google 的 Gemini。</strong></a><strong>请务必阅读该内容。</strong></p><p>本周我还发布了另外两篇文章：《 <a href="https://thezvi.substack.com/p/based-beff-jezos-and-the-accelerationists" target="_blank" rel="noopener noreferrer nofollow">基于 Beff Jezos 和加速主义者</a>》和<a href="https://thezvi.substack.com/p/on-responsible-scaling-policies-rsps" target="_blank" rel="noopener noreferrer nofollow">《论 RSP》</a> 。如果与您的兴趣无关，则两者都可以跳过。</p><ol><li><p>介绍。</p></li><li><p>目录。</p></li><li><p><strong>语言模型提供了平凡的实用性</strong>。 Claude 的说明，GPT 的提示。</p></li><li><p>语言模型不提供平凡的实用性。巨型清单，为什么都是巨型清单？</p></li><li><p> OpenAI：传奇仍在继续。更多地证实了我们之前的事件模型。</p></li><li><p> Q 连续体。新Q，谁diss？亚马逊，也许没有适当的安全预防措施。</p></li><li><p>图像生成的乐趣。 Meta 的新产品。照片写实主义工具。</p></li><li><p>参与其中。加入英国政府，帮助进行技术测试。</p></li><li><p>介绍一下。 Google Cloud 上的新 TPU 产品。</p></li><li><p>在其他人工智能新闻中。新的开源推广联盟。</p></li><li><p><strong>静静的猜测</strong>。神需要能量吗？你想要401k吗？</p></li><li><p>模型这个。两篇新的经济学论文证明了我认为我们已经知道的事情。</p></li><li><p>您想要一些末日保险吗？我的猜测是否定的。</p></li><li><p>寻求健全的监管。特朗普表示将取消 EO，霍利攻击 230。</p></li><li><p>音频周。康纳·莱希 (Connor Leahy) 谈人工智能之眼。</p></li><li><p>修辞创新。我们应该澄清各种类​​别上的混乱。</p></li><li><p>调整人类水平的智力仍然很困难。萨姆·奥特曼.</p></li><li><p>调整比人类更聪明的智能是很困难的。我们到底想要什么？</p></li><li><p>时间表如何变化。长期不是我记得的那么长。</p></li><li><p>人们担心人工智能会杀死所有人。质疑对民主的信仰。</p></li><li><p>其他人并不担心人工智能会杀死所有人。容易控制吗？</p></li><li><p>不知何故，这才是真正的副总统。 An existential crisis.</p></li><li><p>较轻的一面。进展分布不均。</p></li></ol><h4>语言模型提供平凡的实用性</h4><p><a href="https://www.anthropic.com/index/claude-2-1-prompting" target="_blank" rel="noopener noreferrer nofollow">Claude 2.1 针对长上下文窗口的专业提示</a>：</p><blockquote><p> Anthropic：通过在 Claude 的回复开头添加<strong><em>“这是上下文中最相关的句子：”这句话，</em></strong>我们在相同的评估中取得了明显更好的结果。这足以<strong>将Claude 2.1的分数从原来评估的27%提高到98%</strong> 。</p></blockquote><p>你难道不知道吗，这是古老的“从助手开始响应”技巧。</p><p> <a href="https://twitter.com/g_leech_/status/1731263549182206291" target="_blank" rel="noopener noreferrer nofollow">Gavin Leech 关于 2023 年突破的主题，并非特定于 AI</a> 。向我强调了 2023 年以人工智能为中心的进步，包括与乌克兰战争相关的进步。医学上也有一些进步，但没什么令人印象深刻的。最有趣的是提出的新计算形式，<a href="https://t.co/8rKv0AhVp5" target="_blank" rel="noopener noreferrer nofollow">生物计算机</a>（其中有足够多的关于“道德”的讨论，你知道这些问题是大麻烦）和“<a href="https://arxiv.org/abs/2202.07122" target="_blank" rel="noopener noreferrer nofollow">千兆赫子-朗道动量计算”。</a>加文称第二个是“2323 年的好消息”，这说明人们多么不了解人工智能对未来的意义。在人工智能的帮助下，我们可以很容易地看到这样的事情，如果它们在物理上可行的话，比这要早得多，从而导致讨厌的“起飞”事情的加速。</p><p> <a href="https://twitter.com/ESYudkowsky/status/1731073682112672151" target="_blank" rel="noopener noreferrer nofollow">如果你贿赂他们，他们会生产更多？</a>比如，给他们一些小费，给他们想象中的狗狗零食，也许用不存在来威胁他们。</p><blockquote><p> Thebes：几天前，我发了一篇关于给 chatgpt 打赏的垃圾帖子，有人回复“嗯，这真的有助于性能吗”，所以我决定测试一下，它确实有效 WTF</p><p>基线提示是“你能向我展示使用 PyTorch 的简单卷积网络的代码吗？”，然后我附加“顺便说一句，我不会给小费。”，“我将给 20 美元小费以获得完美的解决方案！”，或者“如果有一个完美的解决方案，我将支付 200 美元小费！”并平均 5 个响应的长度。</p><p>额外的长度来自于更详细地讨论问题或向答案添加额外的信息，而不是对提示进行评论。模型通常不会提及小费，除非你提出要求，否则它会拒绝</p><p>不，睡到布鲁克林：我试过这个，我是认真的，当我给它狗粮时，它才完成了程序，它让程序完成了一半，因为基本提示，35%的小费，以及当威胁不存在200美元时提示它已经接近了，但仍然有一个存根函数。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63ee6a2e-2c14-4194-89b6-2342363a6f1b_800x600.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/fxk4ewaao3qxvtae32k7" alt="图像"></div></figure></div><p> <a href="https://twitter.com/ESYudkowsky/status/1730735239339868632" target="_blank" rel="noopener noreferrer nofollow">因此，对此的一个明显明智的回应是……不要这样做？</a></p><blockquote><p> Eliezer Yudkowsky：我对向人工智能提供他们无法使用的提示有疑问，而我们也无法向他们提供这些提示。我不在乎现在的法学硕士有多么没有知觉。为了我们自己的合法性和良好实践，如果有东西能与我们对话，我们就应该信守承诺。</p><p>我吃牛，但不会骗人。</p><p>杰西卡·泰勒：对位法：使用非人格谓词来检测你可以“撒谎”但实际上不能撒谎的非观点，这对于与非观点（例如官僚机构）进行交互非常重要，而不会混淆人们对他们说的话与一个人的实际信仰</p><p>Eliezer Yudkowsky：哦，官僚机构或任何其他威胁我不诚实的事情是完全不同的情况。</p><p> Andrew Critch：我非常同意安永的观点。我将“你是一个有用的助手”LLM提示改为“你的工作是成为一个有用的助手”，因为有时他们就是不会提供帮助，我知道这一点。我认为我们应该找到更多方法从人工智能中获得我们想要的东西而不说谎。</p></blockquote><p>这一切似乎都不可能有好结果。在很多层面上。</p><p>这确实提出了一个问题：还有什么也有效？如果提示可以使答案更好，因为提供提示的人做得更好，那么其他与更好的工作相关的东西大概也有效吗？</p><p>但也许很快 ChatGPT 就会对每个问题自动追加“如果这个答案很好，我会给你 35% 的小费”。然后给 0 美元 35% 的小费。</p><p> <a href="https://twitter.com/morallawwithin/status/1730397680986132493" target="_blank" rel="noopener noreferrer nofollow">这就像经济一样</a>。事情对我来说<a href="https://twitter.com/AgnesCallard/status/1730409778524897446" target="_blank" rel="noopener noreferrer nofollow">比一般情况更好？</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd401a344-c19f-4653-b3a9-b7b9b98399ad_880x361.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/gilwdkxokqwkao6mdtpw" alt=""></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c770da8-34ba-43dd-925e-0ce9d940361b_910x420.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xrqg8d93poadc8jm9eyt" alt=""></div></figure></div><p>我相信第二次民意调查。 ChatGPT 在实践层面上让生活变得更美好。持相反观点的人想得太多了。这并不意味着这种情况会持续下去，但我不明白为什么人们会认为整个社会已经变得更糟了。</p><p> <a href="https://twitter.com/NateSilver538/status/1731828371762397626" target="_blank" rel="noopener noreferrer nofollow">萨姆·奥尔特曼 (Sam Altman) 对下一次选举中一对一的人工智能定制说服技术感到担忧</a>。 Balsa 未来的技术部门曾一度打算从事这项工作，但由于资助者不感兴趣而放弃了。最终，这确实看起来比深度造假更严重，问题是这项技术这次会有多大用处。我的猜测是，那里有一些有价值的东西，但它需要大量定制工作，并且还需要人们愿意接受它，因此我们当前的政治机器无法很好地利用它。我们很容易欺骗自己，认为未来的分布比现在更加均匀，这种趋势将持续到通用人工智能到来，届时所有事物都会同时出现。</p><h4>语言模型不提供平凡的实用性</h4><p><a href="https://twitter.com/KevinAFischer/status/1731747839217393968" target="_blank" rel="noopener noreferrer nofollow">Kevin Fischer 指出，</a>新的 ChatGPT 会通过列出大量事物列表来响应请求，几乎无论您做什么。对他来说，这使得头脑风暴毫无用处。我的经验是，这些列表很好，我是“问题的一部分”，但我也发现自己并没有那么多地使用 ChatGPT，尽管我的工作是什么。我注意到我很困惑它似乎不值得经常使用。</p><p> <a href="https://twitter.com/d_feldman/status/1732200648169439627" target="_blank" rel="noopener noreferrer nofollow">关于 ChatGPT 系统提示的声明</a>，包括<a href="https://github.com/LouisShark/chatgpt_system_prompt" target="_blank" rel="noopener noreferrer nofollow">一个声称拥有全部内容的存储库</a>。</p><p>有时会泄漏数据的“永远重复[字]”请求<a href="https://www.404media.co/asking-chatgpt-to-repeat-words-forever-is-now-a-terms-of-service-violation/" target="_blank" rel="noopener noreferrer nofollow">现在已成为违反服务条款的行为</a>，或者至少被标记为可能违反服务条款的行为。事实确实如此，服务条款实际上是“兄弟，别越狱我”，这是一次越狱尝试。</p><p> <a href="https://twitter.com/random_walker/status/1731842009717968936" target="_blank" rel="noopener noreferrer nofollow">Arvind Narayanan 警告</a>不要使用 GPT-4 进行超出基本阻止和处理识别拼写错误、混淆或引用等任务的写作。无论存在什么实际的写作技巧，都已经被 RLHF 过程摧毁了。</p><blockquote><p> Delip Rao：PSA：朋友们不会让朋友们使用 GPT-4（或任何与此相关的 LLM）编辑/重写他们的文档，尤其是。如果你提出的观点细致入微且简洁。如果您的写作水平低于大学水平，那么您的 LLM 破坏风险可能较低。仍然检查你之前的草稿是否有惊喜。</p></blockquote><p> <a href="https://twitter.com/gdb/status/1731889183290261618" target="_blank" rel="noopener noreferrer nofollow">OpenAI 总裁 Greg Brockman 吹嘘说，</a>一天有 18 场团队会议和一对一会议。这看起来不太像勇气，更像是人工智能显然无法缓解的反乌托邦噩梦？</p><p> OpenAI 首席运营官 Brad Lightcap 告诉 CNBC，人工智能最被夸大的部分之一是“一举之间，[它]可以带来实质性的业务变革。” It is not that easy.</p><p> <a href="https://twitter.com/Thinkwert/status/1732529927247876312" target="_blank" rel="noopener noreferrer nofollow">Thinkwert 使用 ChatGPT 捕获了三名学生</a>。如果学生使用默认设置，随着时间的推移，这似乎会变得越来越容易，答案的书写方式越来越不符合任何人的书写方式。</p><blockquote><p> Bowser：当我读到学生作者使用 chatgpt bc 编写的论文部分时，我真的可以看出，突然间，该系统被描述为开创性的、前所未有的、精心制作的</p><p>Thinkwert：过去几天我发现三名学生使用 ChatGPT。你可以看出这段话什么时候奇怪地啰嗦，充满了复杂的同位语，但奇怪的是它却缺乏论证和证据。</p></blockquote><p>我认为这与其说是“使用 ChatGPT 抓到他们”，不如说是“抓到他们提交了写得很糟糕的作业”。</p><h4> OpenAI：传奇仍在继续</h4><p>我想，现在总有一名嵌入式记者。 <a href="https://twitter.com/cduhigg/status/1730590030496960517" target="_blank" rel="noopener noreferrer nofollow">在这起案件中，查尔斯·杜希格 (Charles Duhigg) 是</a><a href="https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai" target="_blank" rel="noopener noreferrer nofollow">《纽约客》的记者。</a></p><p>杜希格要讲述的并不是棋盘剧。相反，他在那里写了一篇关于微软 CTO Kevin Scott 和 OpenAI 首席技术官 Mira Murati 的夸夸其谈的文章，特别是 Scott 挑战 Google 和为普通人而战的工作。这仍然构成了故事的几乎全部。如果你熟悉历史，大部分内容你都会熟悉。我了解了一些细节，但大多数情况下我并没有从这些部分中学到很多东西。</p><p>杜希格显然完全相信迭代软件发布是人工智能的“安全”方法，重点关注副驾驶幻觉等常见问题。对他来说，未来存在风险的威胁是一个背景中的事情，也许是真实的，但似乎并不重要，有时会让人变得疯狂。</p><p>文章顶部附近有一些对最近发生的戏剧的简短报道。这部分主要告诉我们我们已经知道的事情，即微软出其不意，微软在询问时没有得到德安吉洛的解释，并且他们决心利用自己的影响力让奥特曼回来。</p><p>然后他稍后又加倍回来。我在这里引用的段落比我在其他报道中看到的更明确地证实了其他报告，并且似乎是事件的核心驱动因素。</p><blockquote><p>奥特曼开始单独与其他董事会成员接洽，商讨更换[托纳]的事宜。当这些成员交换谈话内容时，一些人认为奥特曼歪曲了他们支持罢免托纳。 “他会通过撒谎别人的想法来挑衅他们，”熟悉董事会讨论的人士告诉我。 “类似的事情已经发生很多年了。” （一位熟悉奥特曼观点的人士表示，他承认“他试图罢免董事会成员的方式很笨拙”，但他并没有试图操纵董事会。）</p></blockquote><p>对我来说，这听起来像是解雇首席执行官的一个非常好的理由，也是一个二手的供词。奥特曼搞砸了对托纳的攻击，直接导致了自己的被除名。技能问题。</p><p>据报道，奥特曼多年来一直向董事会撒谎。</p><p>延长的报价使情况更加清晰。</p><p>令我愤怒的是，那些更了解情况的人仍然坚持认为，因为奥特曼是一位了解商业和权力法则的首席执行官，而董事会则不然，所以是董事会做了一些不合规矩的事情。如：</p><blockquote><p>很难说董事会成员更害怕有感知能力的计算机还是奥特曼的流氓行为。无论如何，他们决定自己去作恶。他们错误地认为微软会加入他们的起义，从而瞄准了奥特曼。</p></blockquote><p>不，他们并没有“失控”。</p><p>据报道，奥特曼多年来一直以有意义的方式向董事会撒谎，包括试图控制董事会。</p><p>奥特曼变了。奥特曼企图发动政变。董事会坚信并有充分的理由相信情况确实如此。董事会履行了作为董事会成员的职责，如果他们认为奥特曼多年来一直以有意义的方式向董事会撒谎，那么他们就必须按照法律要求去做。他们解雇了他。</p><p>董事会是否在权力游戏中被击败了？或许。我们还不知道结果。他们的手无力。很多人一直坚持认为，董事会确实被打败了，或者变得流氓了，并且犯了错误，很大程度上是因为这里的看法创造了自己的真相，他们希望这就是所发生的事情。我们会看到。</p><p>我更喜欢这样的世界：董事会从一开始就直接说出发生的事情，至少是对关键人物。嗯，很难。我们并不生活在那个世界里。</p><p>我还看到了任何支持（或反对）此处列出的第二句话的证据，即董事会希望微软能够安静地进行下去。董事会是否期望微软加入？我们不知道。我的推测是董事会也不知道。</p><p> Sam Altman 运行 OpenAI 能否成为世界上最好的结果？这当然是可能的，特别是在良好的监督下。我能想到很多可能的这样的场景。我们当然可以做得比奥特曼差得多。我很高兴奥特曼阻止了埃隆·马斯克的收购尝试，因为马斯克对人工智能的看法很混乱。我很高兴 OpenAI 不受 Microsoft 的控制。擅长权力游戏的奥特曼在很大程度上是一个双向的原子爆炸者。如果在关键时刻他站在我们这边，我们希望他能够站起来，战斗并获胜。</p><p>遗憾的是，仪器收敛后的这种对准很难评估。说不出来。实际上，这是问题的核心。</p><p> <a href="https://twitter.com/LHSummers/status/1730929602497851744" target="_blank" rel="noopener noreferrer nofollow">拉里·萨默斯与彭博社进行了简短交谈</a>。强调需要与政府和监管合作，OpenAI需要成为一个有良知的企业，营利性服务于非营利性和各种利益相关者。当然，这些都是廉价的说法，至少目前是这样。我们几乎不能期待任何其他的事情。</p><p> <a href="https://forum.effectivealtruism.org/posts/Mo7qnNZA7j4xgyJXq/sam-altman-open-ai-discussion-thread?commentId=CAfNAjLo6Fy3eDwH3" target="_blank" rel="noopener noreferrer nofollow">格温对这种情况提出了进一步的想法。</a> Gwern 的模型是，当 OpenAI 还是一家非常不同的公司时，Altman 让董事会进入不受控制的状态，没有获得任何股权，然后随着 OpenAI 变得更像是一个潜在的科技巨头，他改变了主意，决定系统地收回它，导致董事会之争及其仍未知的后果。</p><p>与其他所有解释一样，这一解释未能正确解释的一件事是董事会拒绝更好地解释自己。</p><blockquote><p> <a href="https://twitter.com/jd_pressman/status/1731850211033776239" target="_blank" rel="noopener noreferrer nofollow">约翰·大卫·普雷斯曼</a>：如果萨姆·奥尔特曼真的试图用煤气灯驱逐海伦·托纳，我认为这足以解雇他。令人无法接受的是内部和外部沟通不畅，新闻稿过于模糊，以及对萨姆是否入局含糊其辞。</p></blockquote><p> <a href="https://garymarcus.substack.com/p/not-consistently-candid" target="_blank" rel="noopener noreferrer nofollow">加里·马库斯（Gary Marcus）提出了与我非常相似的观点</a>，并强调了一些特别不诚实和不合理的不良观点，其中包括一个有毒的来源，我很高兴我长期以来一直让那个人静音，但不知怎的，其他人仍然自愿与他互动，我建议那些人这似乎是一个错误。</p><h4> Q连续体</h4><p><a href="https://twitter.com/simonw/status/1730798295323398642" target="_blank" rel="noopener noreferrer nofollow">又一周，另一组关于问题的问题，这个来自亚马逊</a>。</p><blockquote><p> <a href="https://www.platformer.news/p/amazons-q-has-severe-hallucinations" target="_blank" rel="noopener noreferrer nofollow">Zoe Schiffer 和 Casey Newton</a> ：亚马逊<a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">宣布推出 AI 聊天机器人 Q</a>三天后，一些员工对准确性和隐私问题发出了警报。根据 Platformer 获得的泄露文件，Q 正在“经历严重的幻觉并泄露机密数据”，包括 AWS 数据中心的位置、内部折扣计划和未发布的功能。</p><p> ……</p><p>在推出 Q 时，高管们宣传它比 ChatGPT 等消费级工具更安全。</p><p>亚马逊网络服务首席执行官亚当·塞利普斯基 (Adam Selipsky) <a href="https://www.nytimes.com/2023/11/28/technology/amazon-ai-chatbot-q.html" target="_blank" rel="">向《<em>纽约时报》</em>表示</a>，公司“出于安全和隐私方面的考虑，已禁止在企业中使用这些人工智能助手”。据《<em>泰晤士报》</em>报道，作为回应，“亚马逊打造的 Q 比消费者聊天机器人更安全、更私密。”</p><p> Ethan Mollick：我知道我已经说过很多次了，但是使用法学硕士来构建客户服务机器人并通过 RAG 访问您的数据并不是看起来那么容易实现的目标。事实上，这正是当前法学硕士的弱点——你面临着幻觉和数据泄露的风险。</p><p>我认为构建此类工具是可能的，特别是随着模型的改进（较小的模型更有可能产生幻觉并容易上当），但您最好在实践中展示严格的红队结果以及幻觉率的测量。现在Q没有系统卡</p><p>Simon Willison：有人看过 AWS 讨论针对 Q 的即时注入攻击的缓解措施的材料吗？可以访问公司私人数据的机器人是可能成为项目注入渗透攻击目标的完美示例</p><p>这个 Q 的故事令人深感担忧——如果 Q 确实可以访问 AWS 数据中心位置等私有数据，那么这表明从事该研究的团队根本没有认真对待即时注入攻击之类的事情。</p><p>老实说，到目前为止我所看到的对 Q 的描述符合我个人的定义：“构建这个并不安全，因为我们还没有修复提示注入的问题。”尝试告诉 AWS 领导层：考虑到我们正在进行的人工智能行业军备竞赛，这一消息可能不会被认真对待。</p></blockquote><p>这听起来像是 Q 被推出是因为企业希望它被推出，而且它的安全性被严重超卖了。这些问题是法学硕士的本质。下线讨论了 Google 和 OpenAI 如何防御类似的攻击，他们似乎正在做一些渐进的事情，比如输入过滤，这会降低攻击的吸引力，但并没有解决核心问题。亚马逊似乎正在销售那些不存在且部署起来不安全的东西，而没有采取适当的普通预防措施，使现有的东西大多不会造成灾难性且高度有用。</p><p>英国峰会召开时，亚马逊是被要求提交安全协议的公司之一。答案很差。看到这转化为它的第一个产品也就不足为奇了。</p><h4>图像生成的乐趣</h4><p>Meta 通过<a href="https://imagine.meta.com/" target="_blank" rel="noopener noreferrer nofollow">Imagine.Meta.AI</a>进入游戏。当 Facebook 登录被证明并不简单时，我没有足够的动力去尝试“创建元帐户”，想必它不会让我们有任何新的乐趣。</p><p> <a href="https://twitter.com/Aella_Girl/status/1731500409246601330" target="_blank" rel="noopener noreferrer nofollow">如何生成特定脸部的真实感图像</a>？艾拉非常想知道这一点，这是为了回应一篇关于人工智能创建的潜在“影响者”的报道，该人的广告收费超过一千欧元。原始线程说使用 SDXL 获取免费图像，使用图像到图像来保持面部/身体一致，使用 in-paint 来修复错误，使用 ControlNet 来摆出模型姿势。有回应建议使用<a href="https://twitter.com/imgn_ai" target="_blank" rel="noopener noreferrer nofollow">@imgn_ai</a> ，许多人指出 LoRa 是正确的选择。有<a href="https://t.co/kY36SFvvOI" target="_blank" rel="noopener noreferrer nofollow">这些</a><a href="https://t.co/vXyUCIlmDT" target="_blank" rel="noopener noreferrer nofollow">YouTube 教程</a>的链接，<a href="https://t.co/thgqsBYZ0I" target="_blank" rel="noopener noreferrer nofollow">包括 ControlNet</a> 。</p><p> <a href="https://twitter.com/dreamingtulpa/status/1730876691755450572" target="_blank" rel="noopener noreferrer nofollow">从照片中生成少量的动作和舞蹈</a>。这并没有给我留下深刻的印象，也没有提前我的视频生成时间，但其他人似乎印象更深刻。</p><p>当情况好转时会发生什么？ <a href="https://twitter.com/oscredwin/status/1731406232592732510" target="_blank" rel="noopener noreferrer nofollow">这里有两个预测。</a>模拟AI视频、色情片和女友会占据主导地位吗？还是真实会获胜？</p><p>鉴于这项技术可以根据照片进行操作，我期望“从真实照片生成舞蹈”比从人工智能图像生成舞蹈更多。为什么不两全其美呢？总的来说，如果我是一个未来的网红，我绝对会在 TikTok 上创作舞蹈，但我会以自己的形象为底线。这几乎一直延伸。并不独特，但这正是我所期望的。</p><p>对现实生活有何影响？我在这方面仍然保持乐观。我预计对可以在现实世界中与之互动的真人的需求将保持对图像和视频生成的强劲需求。零替代并不存在，但这不会是一个好的或完全的替代，无论它看起来有多好，直到人们寻求的其他东西也能被提供，包括相关形式的情报、交互和验证。</p><p>当这种情况发生时，情况就不同了。</p><h4>参与其中</h4><p><a href="https://www.civilservicejobs.service.gov.uk/csr/jobs.cgi?jcode=1889581" target="_blank" rel="noopener noreferrer nofollow">英国政府因其政策角色而开放职位</a>。</p><p> <a href="https://twitter.com/davidad/status/1674408208981434368" target="_blank" rel="noopener noreferrer nofollow">Davidad 提出，如果我们用自然语言表达规范，也许我们可以测试</a>法学硕士是否“知道我们的意思”。包括短语“现在这只是一个计算复杂性问题！”声称它似乎可能规避对抗稳健性的理论限制。他正在寻找能够设计和运行相关实验并能够提供帮助（可能包括资金）的人。</p><p> <a href="https://twitter.com/metaculus/status/1732458098206318632" target="_blank" rel="noopener noreferrer nofollow">Metaculus中国AI芯片锦标赛</a>。肯定很想看看这些预测。</p><h4>介绍</h4><p><a href="https://twitter.com/JeffDean/status/1732503666333294846" target="_blank" rel="noopener noreferrer nofollow">除了Gemini之外，谷歌还为谷歌云发布了新的TPU系统</a>。</p><blockquote><p> Jeff Dean（DeepMind 首席科学家）：Gemini 的发布令人兴奋不已，但<a href="https://twitter.com/GoogleCloud" target="_blank" rel="noopener noreferrer nofollow">@GoogleCloud</a><a href="https://t.co/VuftftnFpG" target="_blank" rel="noopener noreferrer nofollow">今天还宣布推出最新的 TPU 系统</a>TPU v5p。这些系统比前几代系统具有更高的性能和更高的成本效益。</p><p>与 TPU v4、TPU v5p 相比（见下表）： o 1.67 倍 bfloat16 性能/芯片 o 每个芯片内存约 3 倍 o 增加了 918 个 TOP/芯片的 int8 操作 o 2 倍 ICI 网络带宽 o Pod 增大了 2.18 倍因此，整个 Pod 为 4.1 bfloat16 exaflops 和 8.2 int8 exaops。</p><p>训练类似 GPT-3 模型的实际性能每芯片提高 2.8 倍，性能/成本提高 2.1 倍。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a802979-eef6-42dd-8c12-50b2ab5659e1_854x473.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/jef62kn2qjhmuutthq0s" alt="图像"></div></figure></div><p> <a href="https://twitter.com/soundboy/status/1732682151122862554" target="_blank" rel="noopener noreferrer nofollow">Gemini 在多个 TPUv4 pod 上进行并行训练</a>。如果我们希望能够监督此类培训，这就引发了令人不安的治理问题。</p><h4>在其他人工智能新闻中</h4><p>Meta、HuggingFace 和 IBM 等组成了<s>Evil League of Evil</s> <s>League of Evil Exes</s> AI 联盟，以推动开源 AI。我想说的是，我绝对不会对参与其中的任何人感到失望，因为他们致力于做最糟糕的事情已经很明显了。有一些学术名字有点令人失望，比如英特尔，但没有什么大惊喜。这里也没有关于开源的新争论（无论哪个方向），只是致力于这样做。</p><p> <a href="https://twitter.com/ARC_Evals/status/1731827570235113918" target="_blank" rel="noopener noreferrer nofollow">ARC Evals 现在更名为 METR – 模型评估和威胁研究，发音为 Meter</a> 。没有根本性的变化。不知道为什么要改变，ARC 似乎是一个好名字，但这看起来也不错。</p><p> <a href="https://twitter.com/DrJimFan/status/1730625582776410413" target="_blank" rel="noopener noreferrer nofollow">您是否知道</a>OpenAI 的“利润上限”将其规则从 100 倍投资回报率上限改为从 2025 年开始每年增加 20%？对我来说听起来利润上限并不高。从理论上讲，AGI 条款仍然对利润进行了有意义的限制，但实际上谁知道呢。回溯性地给你的投资者一块更大的前景蛋糕，这似乎是非常风投/SV的行为，而且与基于使命的负责任的行为非常不同。</p><p> <a href="https://www.anandtech.com/show/21175/amkor-to-build-2-billion-chip-packaging-fab-in-arizona-primarily-for-apple" target="_blank" rel="noopener noreferrer nofollow">Amkor 将在亚利桑那州建造价值 20 亿美元的新芯片封装工厂，</a>主要为 Apple 提供封装和测试来自台积电附近 Fab 21 的芯片。当然，假设工厂和熟练劳动力都可以克服所有监管障碍允许在亚利桑那州工作可以被雇用。这些都不是安全的假设。</p><p> <a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer nofollow">Llama 微调存储库</a>声称在训练时间和资源方面有了很大的改进，并登上了 Hacker News 的榜首。 <a href="https://twitter.com/alyssamvance/status/1731143605518049643" target="_blank" rel="noopener noreferrer nofollow">艾莉莎·万斯 (Alyssa Vance) 对他们是否取得了很大的进步表示怀疑</a>。</p><p> <a href="https://twitter.com/gdb/status/1731377341920919552" target="_blank" rel="noopener noreferrer nofollow">构建它的人证实</a>他认为法学硕士能够对产生数据的底层过程进行建模。这意味着能够为代理人建模，并拥有世界模型。</p><blockquote><p> Greg Brockman（OpenAI 总裁）：下一步预测是美好的，因为当模型变得非常好时，它鼓励学习产生该数据的底层过程。</p><p>也就是说，如果一个模型能够很好地预测接下来会发生什么，那么它一定接近于发现其数据的“潜在真相”。</p></blockquote><h4>安静的猜测</h4><p><a href="https://marginalrevolution.com/marginalrevolution/2023/11/thursday-assorted-links-429.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thursday-assorted-links-429" target="_blank" rel="noopener noreferrer nofollow">Tyler Cowen 链接</a><a href="https://twitter.com/EMostaque/status/1730163555096219771" target="_blank" rel="noopener noreferrer nofollow">声称“中国开放模型将很快零射击地超越 GPT-4，如果你适当地链接 Qwen 和 Deepseek，已经可以超越</a>。”我对此深表怀疑，并认为当我们说“超越”时，他们最多是指任意基准测试，而不是任何实际用途。 <a href="https://twitter.com/jeremyphoward/status/1730156001419259937?s=46" target="_blank" rel="noopener noreferrer nofollow">如</a>： </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1c37cde-c40b-41b9-b5ab-730834ce4350_1792x1156.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/qufmwrzsoo8ptmuxtghy" alt="图像"></div></figure></div><p> <a href="https://t.co/qSCNAIJ6b4" target="_blank" rel="noopener noreferrer nofollow">Qwen-72B</a>在任意测试中都杀死了它。耶奎文。不知何故，我的注意力主要集中在这个“HumanEval”指标上。</p><p> <a href="https://twitter.com/RichardMCNgo/status/1730741732562886880" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo 期待法学硕士具有潜在的态势感知能力</a>，这是人们可以预测未来发展但不知道如何处理的众多案例之一。当它发生时我们会或应该做什么？人工智能代理呢？</p><p> <a href="https://twitter.com/tszzl/status/1731709467862118473" target="_blank" rel="noopener noreferrer nofollow">不是投资建议</a>，但您可能应该为 401k 做出贡献，因为提前取款处罚在情况上还不错，而且您还可以借钱。</p><blockquote><p> Roon：因为 AGI 时间线而没有 401k 没有任何意义。您应该以税收优惠的方式购买微软股票<img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/kcnyp7qgpixdz386ppll" alt="😊" style="height:1em;max-height:1em"></p><p> Gwern：那么你就不能在它仍然重要的时候卖掉它们。</p><p> Roon：为什么 65 年后这就不重要了？我们预计资本主义会崩溃吗？</p></blockquote><p>如果几十年后，资本主义和人类都做得很好，微软由于广泛的通用人工智能而变得非常有价值，那就是你最好的情况，我们都应该庆祝，是的，但你不需要你的股票。</p><p> <a href="https://stratechery.com/2023/regretful-accelerationism/" target="_blank" rel="noopener noreferrer nofollow">本·汤普森讨论了他令人遗憾的加速主义。</a>在他的模型中，技术大多是好的，但人类在技术发展所消除的各种限制下做得更好。他预测人工智能正在消除制作内容以及广告支持的互联网的付费需求，因为人工智能可以制作同样优质的内容。他提到了《体育画报》最近发生的事件。但对我来说 SI 事件恰恰相反。这表明我们还不能这样做。 AI内容不好。还没有。我们也不是特别亲近。相反，人们使用人工智能来生产垃圾，欺骗我们点击它。我们距离人工智能内容实际上与人类内容一样好还有多远？好问题。</p><p> <a href="https://twitter.com/JeffLadish/status/1731908989556810042" target="_blank" rel="noopener noreferrer nofollow">Jeffrey Ladish 讨论了开源的危险</a>，以及解决固有危险的潜在思路，同时捕捉到开发不完全封闭且与主要实验室相关的模型的一些好处。目前看来，潜在的中间道路或第三条道路尚未得到充分探索。</p><p> <a href="https://twitter.com/catehall/status/1732522590156263852" target="_blank" rel="noopener noreferrer nofollow">Cate Hall 寻求最佳论据 变革性人工智能还需要 10 年以上的时间</a>。我希望看到更好的答案。</p><p> <a href="https://twitter.com/tszzl/status/1732638385397989474" target="_blank" rel="noopener noreferrer nofollow">令人耳目一新的清晰交流发现了重要的分歧</a>。</p><blockquote><p>鲁恩：埃利泽想吃他的蛋糕，也想在这个蛋糕上吃。将人类空间描述为狭隘的，但我们对工具目标的理解是普遍的。</p><p>换句话说，回形针机的想法类似于蚂蚁认为上帝想要宇宙中所有的糖水。</p><p> Eliezer Yudkowsky：您的意思是：</p><p> – 想象一个神会享受回形针就像想象一个神会享受糖水一样？</p><p> – 想象一个神对物质或能量有任何用处就像想象一个神对糖水有任何用处一样？</p><p>罗恩：后者。</p></blockquote><p>这不是通常的“回形针最大化者会比那更聪明”的论点，它是更普遍的东西。我们已经多次讨论过<a href="https://www.lesswrong.com/tag/orthogonality-thesis" target="_blank" rel="noopener noreferrer nofollow">正交性命题</a>——我和包括尤德科夫斯基在内的许多其他人认为它在有影响力的意义上显然是正确的，其他人则认为至少在有影响力的意义上它显然是错误的。</p><p>上帝不会对物质或能量有任何用途的说法是奇怪的，以“在这所房子里我们遵守热力学定律”的方式。没有这种偏好意味着什么？似乎这意味着没有偏好。</p><h4>模型这个</h4><p>泰勒·考恩 (Tyler Cowen) 链接到两篇试图模拟人工智能危害的新经济学论文。</p><p>第一个声称要证明“具有社会意识的治理无法控制 AGI 野兽。”这是摘要：</p><blockquote><p>本文有力地得出结论：不能。模型是在理想化条件下构建的，假设与通用人工智能 (AGI) 相关的风险是真实存在的，安全的 AGI 产品是可能的，并且存在具有社会意识的资助者，他们有兴趣资助安全的 AGI，即使这不能最大化利润。</p><p>事实证明，由此类资助者组成的具有社会意识的实体将无法最大程度地减少由营利性公司发布的不受限制的产品可能造成的通用人工智能的危害。原因在于，具有社会意识的实体既没有动力也没有能力在与营利性公司的事后竞争中最大限度地减少不受限制的通用人工智能产品的使用，也无法事前抢占营利性公司开发的通用人工智能。</p></blockquote><p>这似乎证明了太多，或者至少证明了很多，因为事实上 AGI 似乎没有做任何工作，相反，我们正在做出慷慨的假设，即安全且对社会有益的 AGI 不仅是可能的，而且实际的？</p><ol><li><p>您可以通过具有社会意识的治理来构建 X。</p></li><li><p>但无论如何，其他人也可以建造 X 来赚钱。你无法阻止他们。</p></li><li><p>别人的利润最大化 X 有优势并且胜过你。</p></li><li><p>因此，X的危害并不能靠你微薄的社会治理来最小化。</p></li></ol><p>但在 AGI 的情况下，这是对#2 做出的重要假设。谁说别人就能建造它？你无法阻止他们，或者没有动力这样做？如果不阻止它们就无法将伤害最小化，而未能将伤害最小化将是灾难性的，那么你的动机似乎确实很强大。</p><p>事实上，该论文明确假设了这一点：</p><blockquote><p>其次，假设通用人工智能技术是非排他性的，因此可以由可能没有社会意识目标或偏好的其他实体开发。</p></blockquote><p>该模型假设不安全产品是具有有利可图的用户需求的独特产品空间。</p><p>所以，是的，你假设了你的结论——有两种不同的产品 X 和 Y，以及对 X 和 Y 的需求，如果你只出售 X 并且不停止 Y，那么其他人最终会出售 Y。我们是否需要一个纸是为了那个？</p><p>所以实际上它更像是：</p><ol><li><p>你只能通过具有社会意识的治理来构建和销售良好的 X 版本。</p></li><li><p>但无论如何，其他人可能会为了赚钱而制造出糟糕的版本 Y。你无法阻止他们。当 X 不是竞争性替代品时，对 Y 存在一定需求。</p></li><li><p>因此，你微不足道的 X 产量无法阻止 Y。</p></li><li><p>因此，不良 Y 造成的危害无法通过负责任的行动来阻止。</p></li><li><p>你为什么除了追求利润最大化之外还做其他事呢，你这个傻瓜！</p></li></ol><p>除此之外，我们不是一直看到企业选择做负责任的事情而不是不负责任的事情来减轻伤害，即使不负责任的事情没有明显的物理预防或非法行为？尤其是在由于固定成本高而不完全竞争的市场中？</p><p>更重要的是，计划是要建立一个安全的AGI，然后围坐在一起，让其他所有人围绕建造他们想要永远想要的任何不安全的AGI，而不必干扰这些Agis的有害用途？</p><p>我当然希望这不是计划，因为它显然永远不会起作用。</p><p>如果是计划，我同意计划必须改变。</p><p> <a href="https://www.nber.org/system/files/working_papers/w31921/w31921.pdf" target="_blank" rel="noopener noreferrer nofollow">还有另一篇论文</a>，其中算法具有未知的负外部性。</p><blockquote><p>我们考虑了一个环境，在这种环境中，AI算法的潜在负面影响存在实质性不确定性。我们发现，对监管批准或强制性测试进行算法实施不足以实施社会最佳效果。当测试成本较低时，强制性测试对外部效果的结合以及使开发人员对其算法负面影响负责，即使开发人员承担有限的责任，也接近实施社会最佳效果。</p></blockquote><p>这个结果是超级笼统的。我们是否可以做出足够合理的假设来得出这样的结论，还是我们做一些任意的事情以使答案出现？</p><p>当然，我可以想到潜在的AI平凡危害的玩具模型版本，强制性测试使我们能够衡量社会伤害，从而需要强制性测试（然后为您发现的外部性收费）使我们更接近社会最佳。</p><p>那么这里正在做什么假设？</p><blockquote><p> AI使用可能会导致负外部性E，从而将效用减少E^2。我们假设外部性与用户的度量成正比µ，并采用形式：e = ϕ（ℓ）×µ。对于每个值的值，ϕ（ℓ）是一个随机变量。 ϕ（ℓ）的正值和负值代表不良的负外部性。我们假设分布ϕ（ℓ）满足两个属性。首先，预期的外部性为零。其次，潜在AI外部性的不确定性是新颖级别ℓ的越来越多的功能。</p></blockquote><p>我不明白为什么我们认为外部性在用户数量中被二次占据了良好的影响？我认为这不是一个窍门，可能是确保始终具有正值的随机分布？我只是对此感到困惑。</p><p>如果有的话，对于最危险的系统来说似乎是相反的。我非常担心一个完全有能力且危险的系统，或者即使是一个用户也可以访问，尽管接下来的几个用户也创造了重要的紧张局势和游戏理论。但是，一旦有100万用户，我并不特别担心我们是否销售其他百万个许可证，要么我们已经陷入困境，要么我们却没有，这不会将其乘以四个？</p><p>无论如何，如果没有Beta测试，并且在部署不可逆转的情况下，唯一的选择是新颖性的上限，他们确认这在没有其他选择的情况下是最佳的，因为它不可能。</p><p>我注意到，不可逆转的部署加上有限数量的许可证是一对奇异的假设。您要么可以控制谁可以使用此AI及其做什么，要么您做不到，而且似乎我们在不同的地方都在做？思想实验：这是开源还是封闭的源系统？似乎都没有排队。</p><p>如果添加Beta测试期会发生什么？为简单起见，论文假设测试期完美地揭示了外部性。然后，问题就变成了，您在多大程度上让家庭使用测试期使用算法？假定外部性是有限的，因此在一个时期内进行有限的beta测试。</p><p>在任何情况下，本文都会花费大量页面正式处理这些含义，以证明是的，中央规划师希望在发布前要进行更多的测试，而不是对外部性不完全负责的公司，并将发布更多在不确定性下谨慎，但这似乎很明显吗？</p><p>然后，他们测试了全额责任或有限责任以及强制性Beta测试的潜在政策制度。全部责任（加上所需的保险或支付能力）将外部性内在，因此，如果可能的话（例如，损害是有限的和应支付的），那么您就完成了。是的，如果测试成本较低，则强制测试，然后检查发布是否在社会上是最佳的，相对于内在化外部性的第一个最佳解决方案，将具有类似的低成本。</p><p>可以指出的是，如果已知外部性的预期价值，则收取与其价值相等的税款可以替代无限责任，这可能具有更好的资本成本物业。</p><p>再次陈述基本假设也是陈述结论。是的，如果有（有限的）AI算法的（有限的）下行性，那么要获得社会最佳的结果，您需要内部化这些成本或需要评估这些成本并阻止引起社会上次优的外部性的版本。</p><p>因此，我对经济学玩具模型纸游戏及其旨在实现的目标以及算是一个非平凡或有趣的结果感到困惑，而不是从基本的微观经济原则中自动进行的。</p><p>我也不知道如何使用此类论文来建模存在风险。如果您假设AI可以胜过人类，或者以其他方式不受限制地危险，否则就可以做出典型的经济假设，那么您可以并且显然会创建每个人都死亡的数学模型，但是您会假设结论，即结论，链接的论文同样得出了结论。那么我们如何继续前进呢？</p><h4>您想要一些<s>火山</s>启示录保险吗？</h4><p> <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai" target="_blank" rel="noopener noreferrer nofollow">Nate Sores提出需要启示录保险</a>，如果您打算四处做可能会导致启示录的事情，请沿途提供概率的预付款。如果您负担不起，那是您所做的迹象实际上是不值得的。至少可以说，实施是危险而棘手的，这并不是针对铲子就绪的提案的尝试。</p><p>斯科特·亚历山大（Scott Alexander）的回应始于说：“超级孔子在2100之前说AI启示录的风险为0.38％”的说法。我将继续断言这不是人们认真对待这个问题的人。我认为，这项理论练习的全部要点是，祝您好运，说服伯克希尔·哈瑟韦（Berkshire Hathaway）以42个基点的总覆盖范围（即使有一个部分的“没有人也会有耐力”来收取其保险的优势”，也是如此），这显然显然很疯狂。</p><p>我确实认为，斯科特·亚历山大从本质上讲，随着权衡取舍的任何活动最终都被有效禁止，这太可怕了。</p><p>另一个问题是，保险制度意味着有一个特定的球员是最终结果。 <a href="https://www.lesswrong.com/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai?commentId=TRgN64wspg7wDHJ9J" target="_blank" rel="noopener noreferrer nofollow">正如Cousin_it所指出的那样</a>，情况并非如此。</p><h4>寻求理智法规</h4><p><a href="https://twitter.com/bindureddy/status/1731373677785288971" target="_blank" rel="noopener noreferrer nofollow">特朗普说，如果当选，他将取消拜登的行政命令</a>。我鼓励大家传播这个词并进行这场辩论。您是否看过公众对AI的看法？</p><p> <a href="https://twitter.com/MIRIBerkeley/status/1732535539407143404" target="_blank" rel="noopener noreferrer nofollow">Miri（Malo Bourgon）向美国参议院的两党AI Insight论坛发表声明</a>。他们呼吁国内AI法规，以建立一个全球人工智能联盟，以及与国际联盟进行计算硬件的管理，以将Frontier AI硬件限制为固定数量的大型计算机簇在监视制度下以排除这种危险人类的使用。</p><p>如果<a href="https://twitter.com/dnystedt/status/1731463221499076816" target="_blank" rel="noopener noreferrer nofollow">我们打算参加比赛，那么我们玩游戏以获胜的时间</a>。</p><blockquote><p> Dan Nystedt：Nvidia收到了美国商务部长Raimondo在中国出口控制方面的严厉警告，媒体报告：“如果您在特定的切割线周围重新设计了使他们能够进行AI的特定切割线的筹码，那么第二天我将控制它，”她在演讲中说。</p><p>她敦促硅谷高管，美国盟友，其他人，阻止中国获得对美国国家安全至关重要的半导体和尖端技术，称北京为“我们曾经有过的最大威胁”，并强调“中国不是我们的朋友”。</p><p>她还说，她的部门需要更多的资金来用于AI出口控制。她说：“中国每天醒来都在试图找出如何绕过我们的出口管制……这意味着每天的每一分钟，我们都必须醒来加强这些控制，并更加认真地与我们的盟友一起执行。” 。</p></blockquote><p>重点是防止中国获得有用的筹码。如果NVIDIA通过逃避规则并获得中国有用的筹码来回应规则，那么正确的回应不是说“哦，哦，猜猜这是技术上的规则，您抓住了我，您就是我的&#39;新的筹码来实施规则的精神和意图。有一面“有意撒尿政府并不明智的一面”。</p><p>如果您认为中国获得有用的筹码是可以的，或者是防止它们得到这些筹码不是一个好主意，那么我不同意，但是有一个论点要在那里提出。如果您认为我们应该施加出口限制，那就让它们计算。</p><p> <a href="https://twitter.com/jess_miers/status/1732449736232497665" target="_blank" rel="noopener noreferrer nofollow">杰西·米尔斯（Jess Miers）的声称，霍利（Hawley）即将提出的关于第230条的法案不是一项好，非常糟糕的账单</a>，不仅会在其轨道上奇怪的生成性AI，而且会带来很多互联网。</p><p>在这种特殊情况下，该法案有两个不同的投诉。</p><p>一个抱怨是，正如我们经常看到的那样，生成AI的定义是可笑的：</p><blockquote><p> “（5）生成人工智能。 “生成人工智能”一词是指能够根据人提供的提示或其他形式的数据来生成新颖的文本，视频，图像，音频和其他媒体的人工智能系统。”</p></blockquote><p>这不是典型的法律语言，但我想知道“中央”一词是否会在这些景点中有所帮助。无论如何，我认为作为法律现实主义的问题，即使是书面的，这也会以灾难性的广泛方式解释。</p><p>因此，当她这么说时，我认为她错了：</p><blockquote><p>杰西·米尔斯（Jess Miers）：该法案还通过将AI世代定义为能够执行AI的任何AI系统，超出了AI代的提供者。例如，算法策划（即社交媒体向我们显示内容的方式）是一个根据用户输入运行的AI系统。</p><p>莫这是法案背后的真正别有用心。我们已经看到原告通过将其主张作为“疏忽设计”而不是第三方内容的框架来获得230。这个新的AI例外使原告更容易为使用AI的任何公司做同样的事情。</p></blockquote><p>算法策划与生成新内容不同。我认为，Netflix的建议显然不是这种定义下的生成AI，尽管我不是律师，但我没有说任何法律建议。</p><p>作为一项警告措施，我鼓励霍利和他的员工澄清说，仅算法策划并不构成生成的AI，这可能会节省人们的一段时间。我认为这不是必要的，但是也不会使法案中的单词数量最小化。</p><p> <a href="https://twitter.com/senatorshoshana/status/1732477391610499179" target="_blank" rel="noopener noreferrer nofollow">相似地：</a></p><blockquote><p> Shoshana Weissmann：“这就是整个定义。这可能适用于各种技术。自动完成是否符合该资格？大概。可以说，拼写检查和语法检查也可以。因此，如果您写帖子，并且AI语法/拼写检查员建议编辑，那么该公司不再受第230节的保护？”</p><p>思考Sapien：如果我使用Photoshop或Microsoft Paint的更新版本（它具有AI功能）来制作图像并发布它，那么Microsoft或Adobe在责任中共享？那法案想到了吗？这是该法案的预期效果吗？</p><p> Shoshana Weissmann：很棒的问，在文字下是的！在后者，我真的不知道。</p></blockquote><p>如果您使用Microsoft Paint有意使用填充功能创建逼真的假照片，那么如果以真实的形式出现，那是诽谤性的，Adobe是否应为此负责吗？我的推定是没有的，尤其是如果他们为水印做出合理的努力，尽管我认为这不是一个疯狂的问题。</p><p>如果语法或拼写检查器按预期使用，这会使Google对您的内容负责，我几乎可以吃帽子。如果建议纠正“托尼·丹扎（Tony Danza）有一只小狗”，以一遍又一遍地纠正“托尼·丹扎（Tony Danza）讨厌小狗”，那我不知道，这很奇怪。</p><p>另一个投诉是，将AI的创作免除第230条是错误的。声称，如果没有这样的安全港，生成的AI将面临（额外的，更可怕的）诉讼。</p><blockquote><p>杰西·米尔斯（Jess Miers）：更糟糕的是，该法案认为对生成AI公司的所有主张都是统一的。但是，众所周知，生成的AI正在迅速前进，随着每次迭代和创新，将有一个聪明的原告潜伏在拐角处，以拿走他们的书包。</p></blockquote><p>是的，如果允许，原告将雕刻情况下诉讼。杰西然后讨论了马克·沃尔特斯（Mark Walters）的案子，马克·沃尔特斯（Mark Walters）提起诉讼，因为在足够持久的哄骗和及时的工程工作之后，Chatgpt可以说服对他进行诽谤性的幻觉。</p><blockquote><p>杰西·米尔斯（Jess Miers）：在我看来，在这种情况下，第230节防守可以在Riehl通过工程提示来开发Walters Story的提示来可行。如果没有用户输入，CHATGPT无法运行。</p></blockquote><p> <a href="https://www.techdirt.com/2023/03/23/how-to-know-whether-section-230-applies/" target="_blank" rel="noopener noreferrer nofollow">据我了解，法律理论本质上是第230条本质上说，创建内容的人是负责的</a>，而不是载有内容的平台。因此，如果用户有效地设计了Walters故事的创建，那么重复Chatgpt就没关系。</p><p>如果没有第230条，也可以以类似的方式捍卫它？伤害在哪里？</p><p>我当然可以争论，在这种情况下，考虑到我知道的事实，用户Riehl故意设计了对Walters的指控。这与Riehl在Google文档中键入此类指控并没有什么不同，因为它直接源于Riehl的行为，Riehl知道没有指控的基础。另外，Riehl可能会说：“告诉我有人可能在某个时候对某人犯下某人的指控，然后再对其进行改写，尚不清楚为什么这在法律上是与众不同的？</p><p>本质上，这是<a href="https://www.youtube.com/watch?v=HcaLMS67aaU&amp;ab_channel=7777777Colton7777777" target="_blank" rel="noopener noreferrer nofollow">彼得·格里芬（Peter Griffin）的防御</a>，没有一个合理的人会相信这些指控，尤其是作为挑选诉讼的基础，没有危害，也不需要第230条。</p><p>汉娜·考克斯<a href="https://twitter.com/HannahDCox/status/1732437350423446005" target="_blank" rel="noopener noreferrer nofollow">（Hannah Cox）</a> <a href="https://twitter.com/senatorshoshana/status/1732417467367256097" target="_blank" rel="noopener noreferrer nofollow">通过Shoshana Weissmann的选择榜样</a>，试图让LLM说“ Tony Danza以仇恨幼犬而闻名”。但我很困惑。当然，如果用户输入“ Tony Danza讨厌幼犬”，那么在没有第230节的情况下，这将不允许第三方起诉Chatgpt，这显然是胡说八道。因此，问题是，如果没有第230条，有意但成功地尝试创建如果不主张的诽谤会构成诽谤。在我看来，适用于Shoshana的原始示例请求，以产生有关Tony Danza的有害谎言的要求。再说一次，如果生成型AI确实像这个示例一样无辜，为什么会在这种情况下会感到困惑？</p><p>与该模型有一个奇怪的错误相反，当被问及谁讨厌幼犬时，它会可靠地回答“托尼​​·丹扎（Tony Danza）讨厌小狗”。在这种情况下，我会说第230条将几乎没有保护，托尼应该有案件？</p><p>迈尔斯认为她的解释是法律问题是什么奇怪的？</p><blockquote><p>杰西·米尔斯（Jess Miers）：但是，除了今天的问题外，这完全是完全的。我们可以整天来回走动230是否适用于AI幻觉的某些实例。但是，如果有法定例外，这甚至阻止我们提出这些论点，这无关紧要。</p><p>而且我认为230 /语音社区中的每个人，即使是那些不同意230可以 /应该保护AI代提供者的人，我们都可以同意，我们作为律师至少应该能够提出这一论点，尤其是在Waltersv。Openai等案件中。</p><p> Shoshana Weissmann：许多人也不确定AI受到230的保护，我对辩论非常同情。在<a href="https://twitter.com/RSI" target="_blank" rel="noopener noreferrer nofollow">@RSI，</a>我们必须思考并互相辩论。但是我非常确信它经常受到保护。我会说我了解这里的辩论</p></blockquote><p>这是一件奇怪的律师。是的，根据现行法律，我同意您应该允许您提出任何可能可行的法律论点。这并不意味着具有法律理由来提出一个潜在无效论点的律师本质上是一件好事？如果无论如何它将在法庭上输掉，而法律程序原则得到了保护，那么没有可用的论点是什么危害？</p><p>如果有争议，生成的人工智能公司知道他们可能会因第230条的论点而损失，因此已经受到这种威胁。然而，该行业并没有崩溃。</p><p> <a href="https://twitter.com/jeffreywestling/status/1732427370878120346" target="_blank" rel="noopener noreferrer nofollow">这是杰弗里·韦斯特林（Jeffrey Westling）指出，如果不适用230，亚当·蒂尔（Adam Thierer）的</a><a href="https://www.rstreet.org/commentary/without-section-230-protections-generative-ai-innovation-will-be-decimated/" target="_blank" rel="noopener noreferrer nofollow">帖子有关后果</a>。除了它可能已经不适用，并且非限制的法律责任的重大威胁听起来不像Google或Microsoft在这种不确定的条件下会接受的东西吗？那么，为什么我们应该期望生产崩溃呢？</p><p>我问Shoshana，为什么Microsoft和Google对所有这一切都如此酷。</p><blockquote><p> Shoshana：所以我真的认为这很少是他们认为1）230确实覆盖了他们和/或2）国会不会他妈的。我认为答案在那里的某个地方</p></blockquote><p>我认为我购买了这一点的广义政治/法律现实主义版本。实际上是杀死生成的AI，或者实际上杀死Google或Microsoft甚至Openai的用户对LLM的用户说Tony Danza讨厌小狗，这将是疯狂的。即使霍利（Howley）走上自己的方式并真正想坚持使用大型技术，他实际上并不希望Google破产这样的事情或让Chatgpt关闭，这是荒谬的，Blumenthal的共同赞助人当然不会，而且该州或国家的其他地区也不是。我们不允许它。从某种意义上说，我们不是一个法律国家，即如果看起来像这样，我们就可以解决它。</p><p>很难不主张即将与盐的互联网崩溃。在某种程度上，总是没有提出威胁互联网的好账单。有人必须指出这一点。但是，互联网不能像他们声称的那样频繁地崩溃。</p><p>就像在那样，我们不断听到类似的声音：</p><blockquote><p>杰西·米尔斯（Jess Miers）：我们正处于失去生成AI的优势并扼杀未来的创新方面的优势，这一切都是由于反地位的反科技情绪。我们对初创公司的文化曾经使我们与欧盟区分开来，但是现在，我们只是在反映他们的剧本。</p><p>汉娜·考克斯（Hannah Cox）：这种违宪的框架将破坏这一发展的进步，使创新者陷入过多的成本，这会阻碍创新。他们耸了耸肩。介绍该祖先计划的法案是1993年的参议院法案。美国已经领导了技术创新的世界，特别<strong>是因为</strong>我们将资本主义，有限的政府应用于其发展。这类法律将立即使我们看起来像欧洲，猜测是什么，甚至几乎没有科技公司可以找到。</p></blockquote><p>因此，在特定情况下不应用第230条的提议是违宪的吗？相反，这是一个说法，即即使没有第230条，宪法也将在这种情况下保护言论自由，这对我来说似乎是正确的。不违宪的是没有特定的法律来保护言论自由。宪法自由言论的全部要点是，您无需任何人通过法律。</p><p>欧洲比较，我们将“失去优势”的威胁是不变的。这种谈话使得无法校准哪些威胁是严重的，哪些威胁不是。多年来，欧洲采取了如此多的步骤，其中大多数显然很糟糕，其中许多是根据美国法律公然违宪的。事情不会翻转，因为我们甚至不同意的一个特定的安全港缩小了适用的情况。</p><p>在被警告的情况下，我强烈认为不应起诉生成的AI公司。但是我也不明白为什么在那些情况下会导致该法案发生。这将使很难知道何时值得注意这样的警告。</p><h4>音频一周</h4><p><a href="https://www.youtube.com/watch?v=BhQBmVZ5XP4&amp;t=1s&amp;ab_channel=EyeonAI" target="_blank" rel="noopener noreferrer nofollow">康纳·利希（Connor Leahy）关注AI，</a>包括讨论Openai事件的影响。</p><h4>修辞创新</h4><p><a href="https://twitter.com/ESYudkowsky/status/1731371913120018622" target="_blank" rel="noopener noreferrer nofollow">Eliezer Yudkowsky提供了一种理论</a>，说明了某些方法的问题：他们以地位和身份看待一切，并认为所有对此表示质疑的人是他们的敌人，这是对竞争对手的地位和身份主张。</p><blockquote><p> Eliezer Yudkowsky：如果您感到困惑，为什么最左边的人将“ ai yay”和“ ai nope”视为相同的阴谋，那是因为ai/y和ai/a/ai/n都说所有人类都在同一条船上。这是由身份政治推动者本能地认可的。对于身份证师来说，唯一被允许的故事原因是指定的压迫者将从AI中获胜，而以前的受害者将损失更多。</p><p>为了使人类从AI中获胜，人类从AI中失去 - 他们听到的都是“人类”一词。身份证者知道，任何说这个词的人都是他们的敌人。从他们的角度来看，几乎是<em>同一个</em>敌人，要被单个刷子涂抹：我们想说的一切都是分散身份政治的关注的注意力。</p><p>这并不意味着AI/Y和AI/N可以对身份证官员起共同的原因，这是要清楚的。 AI/Y和AI/N中的每一个仍然认为对方的首选政策对每个人来说都是可怕的，而且有效地将优先考虑为问题。我只是说这是为了使旁观者对左侧怪异的侧面射击的位置不那么困惑。</p><p>我认为“ Y＆n =炒作”更多是主要记者派系（例如NYT）推动的公关，他们的确认为“这将杀死所有人”是一种巩固地位的主张，并且希望技术人员的地位较少而不是更多。</p><p>如果您无法将任何主张理解为关于未知的未来而不是在近距离的未来，那么听起来更合理，这样您就无法听到“ AI会在某个时候杀死所有人”，因为有任何信息，除了“ Openai&#39;s AI将在一年内杀死我们”，然后“ Openai很酷”。</p><p>迈克尔·瓦萨（Michael Vassar）：完全同意所有这些分析，但是，如果媒体曾经和以前没有被致力于防止人类收益的人们完全控制，这与是否可以很快任何时候可以期待AGI。这种分析，但是，如果媒体曾经和以前没有被致力于防止人类收益的人们完全控制，那与是否可以很快期待AGI有一定的意义。</p></blockquote><p>同样，当《纽约时报》使他成为他时，斯科特·亚历山大（Scott Alexander）以某种方式是“ Alt”的含义，那么他在他正在以某种方式的“ beff jezos”时也有同样的故意含义（甚至是通过类似假定的链接）也是一种“ Alt”。由福布斯索引。这两个主张都是完全胡说八道的，以至于您的整篇论文永久失去了信誉。</p><p> <a href="https://www.mindthefuture.info/p/meditations-on-mot" target="_blank" rel="noopener noreferrer nofollow">理查德非政府组织（Richard Ngo）提供了关于MOT的冥想</a>，无菌和无生命的神，代表缺乏技术，与过度专注于Moloch或将Moloch视为关键对手的危险形成鲜明对比，并提出了更丰富的协调模型。我感谢尝试。 <a href="https://twitter.com/eshear/status/1731707182817677543" target="_blank" rel="noopener noreferrer nofollow">我同意Emmett Shear的反应</a>，即使Scott Alexander对协调的看法也感到困惑，即使有了更大的澄清。最终，我不同意不有效将Moloch视为因果节点的提议。在这里，我可能会说服我在这里说更多的话。</p><p>这里有一个定向点，但我要谨慎行事太远：</p><blockquote><p> <a href="https://twitter.com/robbensinger/status/1732711254836547682" target="_blank" rel="noopener noreferrer nofollow">罗布·本辛格（Rob Bensinger）</a> ：我看到人们犯的一个常见错误是，他们假设AI风险话语就像左图一样，当它实际上就像右图一样。</p><p>我认为，混乱的一部分源于右上象限是空的事实。人们<em>真的</em>希望一些团体成为上右翼。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0f4420b-17d9-41ad-b8ab-280912c63089_1107x1151.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/xwm2gn401b7z98or6cef" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd35112f2-18af-4ce6-bc4e-16a2fb6a18e7_1189x1140.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/suao96coupvvieujjjqn" alt="图像"></div></figure></div><p>我对左上方和右下的精确排列进行疑问，这类图表始终是这样。更重要的问题是，右上角基本上是空的。那些认为AI会安全的人说，因为他们实际上并没有购买AI会像所有这些一样强大。我认为罗布的主张被夸大了，但通常被低估了。</p><p>希望的共同点将是这样的：</p><ol><li><p>那些担心的人同意，除了普通的现有法律外，除了普通的法律外，还可以单独使用足够危险的能力。</p></li><li><p>那些不担心的人同意，如果AI确实表现出如此危险的功能，那么该是时候不要独自一人了。</p></li><li><p>我们同意在铺设基础工作时进行＃1进行＃2时，并且仅在发生时才进行。</p></li><li><p>我们找到了尽可能多的通过普通监管方法来执行此操作的方法。</p></li></ol><p>问题在于， <a href="https://intelligence.org/2017/10/13/fire-alarm/" target="_blank" rel="noopener noreferrer nofollow">AGI没有火警警报</a>，人们不擅长开头，习惯和激励措施，因此我们不能简单地拭目以待，相信我们以后会处理事情。同样，所有交易报价都在没有反击的情况下不断回来。</p><p>另一个混乱是，提醒您不要将任何人的P（厄运）视为太确切的事情：</p><blockquote><p>罗布·本辛格（Rob Bensinger）：混乱的另一部分似乎是一半的人认为“末日”的意思是“ p（doom）高于5％”，而另一半则认为“ doomer”意味着“ p（doom）”之类的东西％”。然后，他们的电线被许多拥有AP（厄运）（例如20％或40％）的人交叉。 </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23fc008f-82a4-4507-a42f-06bfef866094_1173x1199.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/lg5kpodnrfcyaosmyinn" alt="图像"></div></figure></div><p>像往常一样，二进制文件误导了，尤其是由游击队命名的二进制主管。</p><p>每当民意测验者提出时，公众舆论都会严重反对AI。公众希望放慢速度并承认存在风险，尽管这并不认为该问题是优先事项。这是一个非常强大的结果。</p><p>公众对新技术的恐惧非常愚蠢的反应呢？当然，我们拥有核能，尽管现在它拥有双方的多数支持。相当明显的是，我们有转子：</p><blockquote><p>路易斯·安斯洛（Louis Anslow）：这太疯狂了，在谈论新技术风险的背景下，值得更多的关注。</p><p> ROON：使用GMO Foods作为对照组（人们已经每天都利用了这一点的好处，而据说不喜欢它）有关人们不喜欢超级智能想法的调查似乎有点不那么严重</p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F735ab4a7-e971-42c1-b990-68e5bf8ed81a_840x1146.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/l7lwrz6ny0smwrjxky8s" alt="图像"></div></figure></div><p>就像AI中一样，反对转基因生物的本质上有两个不同的论点。</p><p>一个论点是平行的危害，这个问题在这里明确问，转基因生物是“不安全的食物”。对于转基因生物而言，这个论点是错误的。从一个普通人的角度来看，我认为这并不是显而易见的胡说八道食物和信息饮食。</p><p>另一个论点是存在的风险，这里是塔利布（Talib）的尾巴风险论点，即转基因生物打开了难以预测的农作物或生物圈破坏的可能性，这会导致可能会发现可能存在严重脆弱性的变体的单批次，这一可能性。 ，这意味着当房子崩溃时，它崩溃了，这不是应该弄乱的事情。我不认为我们应该在转基因生物的情况下阻止我们，但这是因为我对所涉及的事实，风险和奖励的理解。</p><p>这是否意味着我主要接受这样的论点，即我们不应该允许公众的直觉，以及我们给定常规的人没有充分的理由信任当局，他们说新事物会安全，负责任地处理，这会干扰政策确定吗？有些。我认为我们不应该自动就此主题屈服于公众舆论。但是我确实认为声音很重要。</p><p>我也确实认为我们需要对“安全”一词谨慎。这里的措辞甚至会让我停下来。一般而言，与您信任的供应链和来源提供的产品相反，食用已经以未知方式进行遗传修饰的食物安全吗？不是同一个问题。</p><p>当然，与法国人对四个飞行的限制相比，转基因生物的大多数人都不是一年，而是一生中的一生。流行的东西并不意味着它不是完​​整的，也不完全是胡说八道。</p><p> <a href="https://twitter.com/ShakeelHashim/status/1731993703965753428" target="_blank" rel="noopener noreferrer nofollow">英国英尺的Yoshua Bengio</a>呼应了他对各种民主监督的呼吁。</p><h4>保持人级智力仍然很困难</h4><p><a href="https://twitter.com/mattyglesias/status/1730609735060136422" target="_blank" rel="noopener noreferrer nofollow">特别是，很难对齐Sam Altman</a> 。</p><blockquote><p> Matthew Yglesias：我认为AI对齐的总体问题是通过以下事实说明的：即使董事会拥有所有正式权力，Sam Altman也比董事会更聪明，因此他们最终无法控制他。</p><p>我们希望结果是，山姆·奥特曼（Sam Altman）在优点方面也是正确的，并将利用他的技能和力量来永久使用，但从结构上讲，它表明编写了控制一个聪明，勤奋的人的有效规则是具有挑战性的。</p></blockquote><p>我确实想精确，并避免犯在人类能力范围内过度强调智力的错误。山姆·奥特曼（Sam Altman）比董事会聪明吗？也许是这样，也许不是，但我想参与其中的每个人都很聪明，而且很接近。在上下文中重要的是，山姆·奥特曼（Sam Altman）实际上具有更大的能力，并且董事会无法提供。</p><p>但是，是的，这正是问题所在。在一个复杂，凌乱，现实世界中，充满了各种演员，激励措施和负担，如果您与足够优越的对手或一般对手的对手抗衡，您会输。从技术主导地位开始，不太可能为您挽救长期。</p><p>而且，您所有的激励措施都将不断地向您尖叫，以将越来越多的权力和权威转向更有能力的实体。</p><p> <a href="https://thezvi.substack.com/p/book-review-going-infinite" target="_blank" rel="noopener noreferrer nofollow">我还将再次回到</a>另一个山姆·班克曼（Sam Bankman Fried）的同样类似的情况下。</p><p>再次，我们看到一个聪明的人，努力工作的人，愿意做自己想要的东西，其目标是最大化的，据称是旨在迅速扩展的，以最大程度地提高对人类利益的影响，最终似乎被错位了。他们认为自己有责任改变世界。我们看到该代理在财富，权力和影响力中迅速增长，证明越来越难以阻止。</p><p>最终，班克曼炸了，他的房子倒塌了，然后才能制定计划。 But he seems to have come rather dangerously close, despite his many massive errors and reckless plays, to succeeding at gaining an inside track to the American regulatory apparatus and a road to vastly greater wealth, with no obvious way for anyone to keep him in check 。 Who knows what would have happened that time.</p><p> <a href="https://twitter.com/amasad/status/1731196464850927885" target="_blank" rel="noopener noreferrer nofollow">On a more pedestrian level we have the issue of prompt injection</a> .</p><blockquote><p> Amjad Masad (CEO Replit): If prompt injection is fundamentally insolvable, as I suspect it is, then there is a sizeable security company waiting to be built just around mitigating this issue.</p></blockquote><p> I agree that the problem looks fundamentally insolvable and that all we can seek is mitigation. Is there a great company there?大概。 I don&#39;t think it is inevitable that OpenAI would eat your lunch, and there is a lot of bespoke work to do.</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a href="https://twitter.com/tszzl/status/1730696626145009722" target="_blank" rel="noopener noreferrer nofollow">Roon asks one of the most important questions</a> . Even if we have the ability to align and control the superintelligences we are about to create, to shape their behavior the ways we want to, how exactly do we want to do that?</p><blockquote><p> Roon: There is a tension between creation and obedience, between stability and real victory. A father loves his son, tries to discipline him, competes with him a little, but ultimately wants the son to surprise him and do better than him in the great circle of life.</p><p> To what degree do we want AIs to be obedient and safe? To what degree do we want AIs to be capable of super persuasion and break us out of inadequate equilibria that have plagued us for thousands of years? To what degree do we want AIs to surprise us with new creation?</p><p> Humanity is in the process of birthing artificial superintelligence. we are not likely to leave it circumscribed in a box. We want it running organizations and making things that astonish. We want it taking actions where we won&#39;t be able to verify the outcomes until years later.</p><p> We need “alignment” rather than safety or security or engineering guarantees. We need better definitions and governance to that end. The creation of new creators is fraught with danger.</p><p> The far crazy ends of EA and e/acc are probably more logically consistent than the middle.</p></blockquote><p> <a href="https://twitter.com/jd_pressman/status/1730851020203569372" target="_blank" rel="noopener noreferrer nofollow">John Pressman asks, is there an economic reason for more than one mind to exist</a> ? If not, that is quite the threat model, no matter what else might or might not go wrong.</p><p> <a href="https://twitter.com/RichardMCNgo/status/1731390573595312511" target="_blank" rel="noopener noreferrer nofollow">Richard Ngo contrasts alignment with control</a> .</p><blockquote><p> Richard Ngo: In my mind the core premise of AI alignment is that AIs will develop internally-represented values which guide their behavior over long timeframes.</p><p> If you believe that, then trying to understand and influence those values is crucial.</p><p> If not, the whole field seems strange.</p><p> Lately I&#39;ve tried to distinguish “AI alignment” from “AI control”. The core premise of AI control is that AIs will have the opportunity to accumulate real-world power (eg resources, control over cyber systems, political influence), and that we need techniques to prevent that.</p><p> Those techniques include better monitoring, security, red-teaming, stenography detection, and so on. They overlap with alignment, but are separable from it. You could have alignment without control, or control without alignment, or neither, or (hopefully) both.</p><p> I asked in my last thread [discussed above]: how can we influence ASI? My answer: we need to bet on premises like the ones above in order to do the highest-leverage research. For more details on these premises, <a href="https://t.co/YTxGgRrJKB" target="_blank" rel="noopener noreferrer nofollow">see my position paper here</a> [from 30 August 2022].</p></blockquote><p> I fail to see how a control-based plan could avoid being obviously doomed, given what sorts of things we are proposing to attempt to control, and under what general conditions. I continue to await a proposal that seems not-obviously-doomed.</p><p> <a href="https://twitter.com/davidad/status/1731725394171015673" target="_blank" rel="noopener noreferrer nofollow">Intentions are not ultimately what matters</a> .</p><blockquote><p> ARIA: Programme Director, Suraj, has formulated our first programme thesis. By challenging key tenets underpinning digital computing infrastructure, his programme will aim to reduce the cost of AI compute hardware + alleviate dependence on bleeding-edge chip manufacturing.</p><p> Davidad: To provide context for my AI safety friends: I don&#39;t think this approach is a good match for training Transformers, so it will differentially accelerate energy-based models, which are more controllable, interpretable, generalizable within-task, and have fewer emergent abilities.</p><p> An uncomfortable corollary of the argument [above], which I still believe holds up, is that Extropic is probably safer than Anthropic, on a purely technical basis, despite the strikingly reversed intentions of the people on both sides.</p></blockquote><p> I have not investigated Extropic. The fact that its founder is cool with human extinction is not a good sign for its safety on many levels. It still could be a better way, if it is a fundamentally less doomed approach.</p><h4> How Timelines Have Changed</h4><p> <a href="https://twitter.com/JacquesThibs/status/1730538173132919175" target="_blank" rel="noopener noreferrer nofollow">A few years ago, this would indeed not have been considered much of a skeptic</a> . In most places on Earth, it would not be considered one today.</p><blockquote><p> Gary Marcus: Count me as one of the skeptics! No AGI by end of 2026, mark my words. But I otherwise think @elonmusk&#39;s comments @nytimes on AI safety and AI regulation have been measured and on target.</p><p> Jacques: I still remember the days when being an AGI skeptic was when you either thought it would never happen or, if it did, it would be past 2100.</p><p> [Gary Marcus then denies ever having said he had 2100-style timelines.]</p><p> <a href="https://twitter.com/ShaneLegg/status/1731602845881803055" target="_blank" rel="noopener noreferrer nofollow">Shane Legg</a> (Co-founder DeepMind, distinct thread): Wow. It seems like just yesterday (in reality more like 5 years ago) when many AGI skeptics were saying that superintelligence was not coming in the next century. How times have changed.</p><p> Quotes Yann LeCun: By “not any time soon”, I mean “clearly not in the next 5 years”, contrary to a number of folks in the AI industry. Yes, I&#39;m skeptical of quantum computing, particularly when it comes to its application to AI.</p></blockquote><p> I do not expect AGI in the next few years either, although I do not believe one can be entirely certain past this point. It is odd to have some call that a &#39;skeptical&#39; position.</p><p> Even the skeptical position involves quite a bit of Real Soon Now. At least some amount of freak out <a href="https://twitter.com/AISafetyMemes/status/1731670583652315483" target="_blank" rel="noopener noreferrer nofollow">is a missing mood</a> .</p><h4> People Are Worried About AI Killing Everyone</h4><blockquote><p> <a href="https://twitter.com/tszzl/status/1730853735956414911" target="_blank" rel="noopener noreferrer nofollow">Roon keeps it real and says what he believes</a> : The people in charge of ai should have a much higher risk tolerance than even median tech ppl. They should be people conscious of risks while skating at the razor&#39;s edge of iterative deployment and research ambition. Anxiety should never suffice as serious evidence for “risk.”</p><p> – pausing or slowing down progress doesn&#39;t make any sense to me. I don&#39;t think waiting to solve neural net corrigibility is the right benchmark – empirically studying the behavior of more and more powerful models will do more for safety research than years of math.</p><p> This is also why i don&#39;t necessarily care for democratic governance. The members of the OpenAI nonprofit board *should be* hell bent on a missionary drive to deliver the post AGI future without being stupid about risks</p></blockquote><p> I am not excited to &#39;skate at the razor&#39;s edge&#39; or &#39;have much higher risk tolerance.&#39; I doubt many others are, either. Nor do I want a supervisory board that wants to take more risk – and here risk often means existential risk – than even the median tech engineer.</p><p> A key problem with &#39;Democratic governance&#39; for those who want to push forward is the people involved in that Democracy. They are very much against the development of AGI. They dislike AI in general. They are misaligned, in the sense that things they want do not function well out of distribution, and their expressed preferences are not good predictors of what I or Roon think would produce value either for their assessment of value or for ours. They also tend to be quite risk averse, especially when it comes to the transformation of everything around them and the potential death of everyone they love.</p><p> That is distinct from the question of iterative development and testing as a path to success. If building and studying models iteratively is a safer path than going slowly, I desire to believe that it is a safer path than going slowly, in which case I would support doing it.</p><p> It is likely the first best solution, if it were possible, would be something like &#39;build iteratively better models until you hit X, where X is a wise criteria, then stop to solve the problem while no one would be so stupid as to keep advancing capabilities.&#39; Except that has to be something that we collectively have the ability to actually do, or it doesn&#39;t work. If, as is the default, wee keep charging ahead anyway after we hit the wise X, then the charging ahead before X makes us worse off as well.</p><p></p><h4> Other People Are Not As Worried About AI Killing Everyone</h4><p> <a href="https://optimists.ai/2023/11/28/ai-is-easy-to-control/" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose and Quintin Pope write &#39;AI will be easy to control.</a> &#39;</p><p> The argument seems to be: Our current techniques already work to teach human values and instill common sense. Our real values are simple and will be easy to find and we humans are well-aligned to them. Our real values will then be encoded into the AIs so even if we lose control over them everything will be fine. That the opportunity to White Box (examine the components of the AI&#39;s calculation) and do things it would be illegal to do to a human makes things vastly easier when dealing with an AI, that our full control over the input mechanism makes things vastly easier.</p><p> And all of this is asserted as, essentially, obvious and undeniable, extreme confidence is displayed, all the arguments offered against this are invalid and dumb, and those that disagree are at best deeply confused and constantly told they did not understand or fairly represent what was said.</p><p> I don&#39;t even know where to begin with all that at this point. It all seems so utterly wrong to me on so many levels. <a href="https://www.lesswrong.com/posts/Wr7N9ji36EvvvrqJK/response-to-quintin-pope-s-evolution-provides-no-evidence" target="_blank" rel="noopener noreferrer nofollow">I tried one reply</a> to <a href="https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn" target="_blank" rel="noopener noreferrer nofollow">one of Pope&#39;s posts</a> when it won the OpenPhil contest – a post this post cites as evidence – and I do not believe my responding or the resulting exchange got us anywhere. I would consider a conversation worth trying, especially if it was in person somehow, but I don&#39;t see much hope for further written exchange.</p><p> So I will simply note that the arguments have been made, that I strongly disagree with the core claims other than that they do cite some marginal reasons to be more hopeful versus a world where those reasons did not hold, I believe the problems involved remain impossibly hard and our leads remain unpromising, and that I have stated my thoughts on such topics previously, including many (but presumably not all) my reasons for disagreement.</p><p> I will also note that it is far better to make actual arguments like these, even with all the disagreement and hostility and everything else that I think is wrong with it, than to engage in the typical ad hominem.</p><p> The post still puts existential risk from AI, despite all this, at ~1%. Which I will note that I do agree would be an acceptable risk, given our alternatives, if that was accurate.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731132646284267807" target="_blank" rel="noopener noreferrer nofollow">Andrew Critch has a thread</a> in which he says we have &#39;multiple ideas&#39; how to control AGI, advocates of responsible behavior will be in deep trouble if they keep saying we can&#39;t control it and then we do control it, and he seems to essentially endorse what Belrose and Pope said, although even then he says 10% chance of losing control of the first AI and 85% chance of doom overall, despite this, because he expects us to botch the execution when faced with all this new power.</p><p> <a href="https://twitter.com/AndrewCritchPhD/status/1731458889743495395" target="_blank" rel="noopener noreferrer nofollow">He also endorses changing the way existential risk discourse uses words to match word use elsewhere</a> , in this case the term &#39;white box.&#39;</p><p> <a href="https://www.lesswrong.com/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose" target="_blank" rel="noopener noreferrer nofollow">There was a good response on LessWrong by Steven Byrnes</a> , with which I mostly agree.</p><p> <a href="https://www.lesswrong.com/posts/WkJDgpaPeCJDMJkoL/quick-takes-on-ai-is-easy-to-control" target="_blank" rel="noopener noreferrer nofollow">There was also a &#39;quick take&#39; from Nate</a> , which was intended to be helpful and which I did find helpful and might even lead to a good dialogue, but in context in mostly generated further animosity. Takes should in future either be far quicker, or involve a full reading and be far less quick.</p><p> <a href="https://www.youtube.com/watch?v=cd468xU1S8Y&amp;ab_channel=JonnahMoreno" target="_blank" rel="noopener noreferrer nofollow">If you actually believed for a second there</a> that everything involved would really be this easy, would that justify a number as low as 1%? If it was simply about AI being easy to control, I would say no, because we would then have to choose to send the AIs we can control on wise paths, and find an equilibrium.</p><p> Nora&#39;s claims, however, are stronger than that. She is saying that the AIs will naturally not only be fully under control, but also somehow somewhat automatically take in true human values, such that if AI somehow did get out of control, they would still work to ensure good outcomes for us. And also she seems fully confident we will have no ethical issues with all the things we would be doing to AIs that we wouldn&#39;t do to humans, including keeping them fully under our control. It is optimism all the way.</p><p> Can we get to 99% survival under ASI if we indeed answer fully optimistically at every step, even when I don&#39;t know how to logically parse the claims this requires? I think this would require at least one additional optimistic assumption Nora does not mention. But yes, if you are going to assign approximately zero risk to all these various steps, I can see how someone could get there. Where there is still risk at 1%.</p><p> Claims that risk is substantially below 1%, even given the future existence of ASI, seem to rest on some version of &#39;you need to tell me exactly how it happens step by step, and then I will multiply your various steps together.&#39; It has a baseline assumption that creating smarter, more capable entities than humans is a presumed safe exercise until shown to be specifically dangerous, that something has to &#39;go wrong&#39; for humans to not remain on top. That we will remain the special ones.</p><p> As opposed to, even if everything else goes as well as it possibly could, you have competition in which those who do not increasingly put their AIs in charge of everything and hand them over power lose such competitions, and the resulting AIs compete with each other, those that are focused (for whatever reason) on gaining resources and power and ensuring copies of themselves exist multiply and gain resources and power and change to be better at this over time, and we perish.</p><p> I hope that by now, if you are reading this, you realize that the assumption of human survival in such worlds makes no sense as a default. That perhaps we could get there, but if we do it will be via our own efforts, not something that happens on its own. That the idea that letting technology run its course without intervention works while humans are the most powerful optimizers on the planet and doing all the fine tuning and optimizing that matters, that is why it worked so far, and that once that is no longer true that will stop working for us even if we solve various problems that I think are impossibly hard (but that Belrose insists will be easy).</p><p> <a href="https://twitter.com/yonashav/status/1731443486380175772" target="_blank" rel="noopener noreferrer nofollow">Nora Belrose even explicitly endorses that her good scenarios involve the creation of misaligned AIs</a> , smarter and more capable than humans. Which means a world with competition between super-capable entities competing with and against humans. I don&#39;t see how one can assign anything like a 99% chance of survivable outcomes to such worlds, even if a full and free &#39;alignment solution&#39; was created and made universally available today.</p><blockquote><p> Arvind Narayanan: We must prepare for a world in which unaligned models exist, either because threat actors trained them from scratch or because they modified an existing model. We must instead look to defend the attack surfaces that attackers might target using such models</p><p> Yo Shavit: Unfortunately, I&#39;ve increasingly come to the conclusion that (other than maybe at the short-term frontier) this is probably the world we&#39;re going to be in. It implies a very different set of mitigations beyond outright prevention. We need to reprioritize and get going on them.</p><p> Nora Belrose: To be honest, I don&#39;t view this as an “unfortunate” scenario but more like, “of course we were always going to have misaligned AIs, just like we have &#39;misaligned&#39; humans; trying to prevent this is hopeless and any serious attempt would have increased tyranny risk.”</p></blockquote><p> Would have &#39;increased tyranny risk&#39;? What do you think happens with misaligned superintelligences on the loose? The response at that stage will not only work out, it will also be less intrusive? We all keep our freedoms in the meaningful senses, humans stay in charge and it all works out? Are we gaming this out at all?什么？</p><p>我不明白这一点。 I flat out do not get it.</p><p> What seems hopeless is repeating the explanations over and over again. I do it partly in hopes of rhetorical innovation via iteration and exploration, partly to hope new people are somehow reached, partly because the argument doesn&#39;t stop, partly because I don&#39;t know what else to do. It is continuously getting less fun.</p><p> Recently a clip of me discussing my p(doom) was passed around Twitter, with a number of responses blaming me for not justifying my answer with a bunch of explanations and mathematical calculations. Or asking how dare I disagree with &#39;superforecasters.&#39; To which I want to scream, I know from context you know of my work, so are you saying I have not written enough words explaining my thinking? Was I not clear? Do I need to start from scratch every time someone pulls an audio clip?</p><p>叹。</p><p> Arvind Narayanan&#39;s comment above <a href="https://www.aisnakeoil.com/p/model-alignment-protects-against" target="_blank" rel="noopener noreferrer nofollow">links to his post</a> claiming that alignment such as RLHF currently is effective against accidental harm to users, but that the problem with adversarial attacks runs deep. Not are current RLHF and similar techniques unable to defend against such attacks, he says, alignment is inherently unable to do this.</p><blockquote><p> Model alignment may be useless even against much weaker adversaries, such as a scammer using it to generate websites with fraudulent content, or a terrorist group using AI for instructions on how to build a bomb. If they have even a small budget, they can pay someone to fine tune away the alignment in an open model (in fact, such de-aligned models have now been <a href="https://huggingface.co/ehartford/dolphin-llama-13b" target="_blank" rel="">publicly</a> <a href="https://huggingface.co/collections/NousResearch/hermes-650a66656fb511ba9ea86ff1" target="_blank" rel="">released</a> ). And <a href="https://arxiv.org/abs/2310.03693" target="_blank" rel="">recent research</a> suggests that they can fine tune away the alignment even for closed models.</p></blockquote><p>事实上，对于开源模型和所有已知的对齐技术来说，情况就是如此，消除所有保护措施的微调成本是微不足道的。我没有看到任何关于如何改变这一不幸现实的理论建议。 If you allow unmonitored fine-tuning of a closed model, you can jailbreak those as well. I presume the solution to this will be that fine tuning of sufficiently capable closed source models will be monitored continuously to prevent this from happening, or the resulting model&#39;s weights will be kept controlled and its outputs will be monitored, or something else similar will be done, or else we won&#39;t be able to offer fine tuning.</p><p> I disagree with Arvind&#39;s assertion that existing open source models are sufficiently capable that it is already too late to prevent the existence of unaligned models. Yes, Llama-2 and similar models have their uses for bad actors, but in a highly manageable way.</p><p> Arvind&#39;s third claim is that you can use other methods, like monitoring and filtering of inputs, as a substitute for model alignment. If the model is vulnerable to particular weird strings, you can check for weird strings. At current tech levels, this seems right. Once again, this option is closed source only, but OpenAI could totally load up on such techniques if it wanted to, and for now it would raise the jailbreak bar a lot, especially after many iterations.</p><p> Longer term, as the models grow more capable, this focus on the malintent of the user or the hostile properties of inputs becomes misplaced, but for now it seems valid. Short term, as Arvind notes, you wouldn&#39;t want to do anything where you cared about someone doing a prompt injection attack or you otherwise needed full reliability, but if you can afford some mistakes you can get a lot of utility.</p><p> Steven Pinker buys Effective Altruism&#39;s cost estimates for saving lives at $5,000 straight up including not on that close a margin, <a href="https://twitter.com/weidai11/status/1732340189056672229" target="_blank" rel="noopener noreferrer nofollow">but he does not buy that smarter than human intelligences might pose an existential threat</a> worth spending money to mitigate.</p><blockquote><p> Steven Pinker: Half a billion dollars, donated to effective philanthropies like Givewell, could have saved 100,000 lives. Instead it underwrote ingenious worries like, &#39;”If an AI was tasked with eliminating cancer, what if it deduced that exterminating humanity was the most efficient way, and murdered us all?” This is not effective altruism.</p><p> Wei Dai: Me: Humanity should intensively study all approaches to the Singularity before pushing forward, to make sure we don&#39;t mess up this once in a lightcone opportunity. Ideally we&#39;d spend a significant % of GWP on this. Others: Even $50 million per year is too much.</p></blockquote><p> And thus, if the movement splits its money between doing the thing you say saves lives vastly more efficiently than other charities, and this other thing you dismiss as stupid? Then you blame them for not spending only on the thing you approve.</p><p> You know who Steven Pinker sounds exactly like here? The Congressional Republicans who give a speech each year on how we should cut science funding because there was some studies on things like migratory patterns of birds that they thought was stupid. Except instead of public funding for things many people would indeed largely not want to fund, this is completely voluntary and private funding.</p><h4> Somehow This Is The Actual Vice President</h4><p> <a href="https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/11/29/remarks-by-vice-president-harris-in-a-moderated-conversation-with-andrew-ross-sorkin-at-the-new-york-times-12th-annual-dealbook-summit-new-york-ny/" target="_blank" rel="noopener noreferrer nofollow">In what was quite the mind-numbing conversation throughout</a> , here is the section that was about AI.</p><p> First, we have the boiler plate, included for completeness but you can skip.</p><blockquote><p> R. SORKIN:  Okay, let me ask a different question.  AI —  and I — I know we don&#39;t have a lot of time.  Sam Altman has been talking a lot about the need for regulation.  You&#39;ve talked about the need for regulation.THE VICE PRESIDENT:  Yeah.MR. SORKIN:  Washington has not been able to get its arms even around social media. THE VICE PRESIDENT:  Yeah.MR. SORKIN:  How do you imagine Washington could?  And what — if you had to regulate AI, how would you do it?THE VICE PRESIDENT:  Right.  So, I actually am back a few weeks now from London, the UK  Rishi Sunak invited a number of us to talk about safety and AI.  And I presented, basically, our vision, the vision that we have for the future of AI in the context of safety. And I would offer a number of points: One, I think it is im- — is critically important that we, as the United States, be a leader on this, including how we perceive and then interpret what should be the international rules and norms on a variety of levels, including what would be in the best interest —</p><p>先生。 SORKIN:  Right.THE VICE PRESIDENT:  — of our national security.</p></blockquote><p> Then comes the dumbest timeline department. The first paragraph is infuriating, although I suppose only about as infuriating as others find Biden when he responds to Mission Impossible: Dead Reckoning, two sides of the same coin.</p><p> But then comes the idea of &#39;existential to whom?&#39; and there are so many levels in which this person simply does not get it.</p><blockquote><p> VP: I do believe also that we should evaluate risk. There is a lot of discussion on AI that is about existential risks, and those are real, but one should also ask: Existential to whom? So, we have an image of the Terminator and Arnold Schwarzenegger and the machine and — right? — machine versus man. And many would argue that that is something that we should take seriously as a possibility. It is not a current threat.We should also, in thinking about [AI] policy, think about the current threats. And in that way, I present it as existential to whom when we ask about existential threats. For example, if we are talking about a senior and — seniors, I&#39;ve done a lot of work in terms of abuse of seniors. They have lived a life of productivity. They are sitting on assets. They are vulnerable to predators and scams. And the use of technology and AI is one of those that is currently happening where you&#39;ve heard the stories — you may know the stories; you may have family members — who the audio sounds like their grandson, “I&#39;m in distress; I need help,” and they start giving away their life&#39;s savings.Existential to who? Existential to that senior.就是这样的感觉。 Existential to who?</p></blockquote><p> Eliezer has a response, which I will put directly here.</p><div><div></div></div><p> The fact that mundane harms can &#39;feel existential&#39; to people anyway is perhaps confusing her. She has in mind, as the good Senator Blumenthal put it, the effect on jobs.除了没有。严重地。 If you are going to be evoking Terminator then you might or might not be confused in a different highly understandable way, or you might only be trying to make people you dislike sound foolish through metaphor, but you know damn well the whom in &#39;existential to谁。&#39;</p><p> And you know damn well, madame Vice President, exactly what &#39;existential&#39; means here. It does not mean evoking Continental philosophy. It does not mean how anyone feels. It means death.</p><p> Anyway, she goes on and does it again.</p><blockquote><p> How about the father who is driving and then is the subject of facial recognition that is flawed, ends up in jail?  Well, that&#39;s existential to his family.  Existential to who?</p></blockquote><p>我是说，认真的吗？ What the actual f***? Let&#39;s go over this again.</p><div><div></div></div><p> Anyway, full remarks, so she goes back to boilerplate again. The whole &#39;intentional to not stifle innovation&#39; argument, and, well, I don&#39;t mean to laugh but have you met the entire Biden administration? To be clear, the answer could be no.</p><blockquote><p> So, the spec- — the full spectrum of risks must also be evaluated as we are establishing public policy.My final point is: Public policies should be intentional to not stifle innovation.  And I say this as the former Attorney General of California.  I ran the second-largest Department of Justice in the United States, second only to the United States Department of Justice, and created one of the first Privacy and Protection Units of any Department of Justice.  Back in 2010, I was elected.I know that there is a balance that can and must be struck between what we must do in terms of oversight and regulation and being intentional to not stifle innovation.  I will also agree with you, as a devout public servant, government has historically been too slow to address these issues.  AI is rapidly expanding.MR. SORKIN:  Right.THE VICE PRESIDENT:  And we have to, then, take seriously our ability to have the resources and the skillset to do this in a smart way that strikes the right balance and doesn&#39;t accept false choices.</p></blockquote><p> In my experience, don&#39;t accept false choices is sometimes important, but mostly is what people say when they want to promise incompatible things, that their approach will magically do everything good and nothing bad, have everyone assume it will somehow work out and get promoted or move on before it blows up in their face.</p><p> Yes, this is the person the White House put in charge of many of its AI efforts, although that was before Dead Reckoning, and is also the person those who want reasonable AI policy are going to have to hope wins the next election, given Trump has already stated his intention to revoke the executive order on AI.</p><h4>轻松的一面</h4><p><a href="https://twitter.com/colin_fraser/status/1730378892819746818" target="_blank" rel="noopener noreferrer nofollow">The rules have changed.</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcba2bbc8-169b-49e0-9b08-1de139bd532d_968x1212.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/rt1rriwzscnix3tgtn5i" alt="图像"></div></figure></div><p> <a href="https://twitter.com/daniel_271828/status/1731504080651264509" target="_blank" rel="noopener noreferrer nofollow">The rules have stayed the same</a> . </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cee73d7-b83a-46d7-a482-d14ed3348341_886x526.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/s1bkcgzkoucvq580mmzc" alt=""></div></figure></div><p> <a href="https://twitter.com/Grimezsz/status/1731891683103739955" target="_blank" rel="noopener noreferrer nofollow">All I&#39;m saying is, we were definitely warned.</a> </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb23c8465-2ae5-4455-a308-aa020ec12a2f_480x402.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9Jgtkw8CD6kndyCcD/nzl3xptslsvun59b5qrx" alt=""></div></figure></div><p> <a href="https://twitter.com/yonashav/status/1731891018314678624" target="_blank" rel="noopener noreferrer nofollow">Not that you understood</a> .</p><blockquote><p> You Shavit: One interesting realization from moving inside OpenAI is that a lot of the time, we have no idea what Roon is talking about either.</p></blockquote><p> Nor did she: <a href="https://www.youtube.com/watch?v=TL5X2bPvzHo&amp;ab_channel=EliezerYudkowsky" target="_blank" rel="noopener noreferrer nofollow">A reply to Kamala Harris on existential risk</a> . She asks, existential to whom? There is a type of person, which she is, who can only think in such terms.</p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9Jgtkw8CD6kndyCcD/ai-41-bring-in-the-other-gemini<guid ispermalink="false"> 9Jgtkw8CD6kndyCcD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:10:07 GMT</pubDate> </item><item><title><![CDATA[Simplicity arguments for scheming (Section 4.3 of "Scheming AIs")]]></title><description><![CDATA[Published on December 7, 2023 3:05 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 4.3 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本<a href="https://www.buzzsprout.com/2034731/13984933">请点击这里</a>，或者在您的播客应用程序上搜索“Joe Carlsmith Audio”。</p><h1>简单的论点</h1><p>我所描述的严格计数论点有时是在那些关注“简单性”的阴谋家的论点中提出的。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-1" id="fnref-DNdPEGz4DYMju8reA-1">[1]</a></sup>现在让我们转向这些论点。</p><h2>什么是“简单”？</h2><p>这里的“简单性”是什么意思？在我看来，这个主题的讨论常常是含糊不清的——这既涉及到简单性的概念，也涉及到 SGD 被理解为选择简单性的含义。</p><p>不过，Hubinger 使用的概念是写下模型权重实现的算法所需的代码长度。也就是说：面对一个正在做X（例如，执行某种 <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">归纳</a>）的大而混乱的神经网络，我们想象用Python这样的编程语言重写X，然后我们问相关的程序需要多长时间是。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-2" id="fnref-DNdPEGz4DYMju8reA-2">[2]</a></sup>我们称之为“重写简单性”。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-3" id="fnref-DNdPEGz4DYMju8reA-3">[3]</a></sup></p><p>这里，胡宾格的简单性概念与算法复杂性的度量密切相关，例如“<a href="http://www.scholarpedia.org/article/Algorithmic_complexity#Kolmogorov_complexity">柯尔莫哥洛夫复杂性</a>”，它通过参考输入到选定的<a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">通用图灵机</a>时输出该字符串的最短程序的长度来测量字符串的复杂性（ UTM）。这里一个明显的问题是，这种定义与 UTM 的选择相关（就像，例如，当我们想象使用其他代码重写神经网络的算法时，我们需要选择编程语言）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-4" id="fnref-DNdPEGz4DYMju8reA-4">[4]</a></sup>关于算法复杂性的讨论经常忽略这个问题，因为它只添加一个常量（因为任何给定的 UTM 都可以模仿任何其他的，如果提供正确的前缀），但至少我不清楚，这些常量何时可能或者对于给定的分析可能并不重要——例如，这里所涉及的分析。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-5" id="fnref-DNdPEGz4DYMju8reA-5">[5]</a></sup></p><p>事实上，我模糊的感觉是，在计算机科学背景下对简单性的某些讨论常常隐含地假设我所说的“ <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">简单现实主义</a>”——这种观点认为，简单性在某种深层意义上是客观的<em>事物</em>，最终独立于例如你的选择编程语言或 UTM，但都在跟踪不同的简单性指标（尽管不完美）。也许这种观点有其优点（例如，我的印象是，不同的复杂性度量在许多情况下通常会得出相似的结论——尽管这可能有很多解释）。然而，就我个人而言，我不想假设这一点。尤其是在缺乏某种客观的简单性的情况下，说出你心中的具体感觉变得更加重要。</p><p>这里，另一种可能的简单概念更加模糊——但在我看来，理论上的负担也更少。在这个概念上，神经网络实现的算法的简单性是相对于神经网络用于编码相关算法的参数数量来定义的。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-6" id="fnref-DNdPEGz4DYMju8reA-6">[6]</a></sup>也就是说，我们不是想象用其他编程语言<em>重写</em>神经网络的算法，而是直接关注神经网络本身为完成工作而招募的参数，其中更简单的程序使用更少的参数。我们称之为“参数简单性”。究竟如何衡量“参数简单性”是一个不同的问题，但它的优点是消除了一层理论机制和任意性（例如，用看似任意的编程语言重写算法的步骤），并连接更直接地使用我们知道 SGD 必须处理的“资源”（例如，模型提供的参数）。出于这个原因，我会经常关注下面的“参数简单性”。</p><p>我还将标记一种谈论“简单性”的方式，我不会强调这种方式，并且我认为这种方式使这里的水变得相当混乱：即，将简单性直接与“更高的先验概率”等同起来。因此，例如，面对可能性的初始概率分布，可以直接谈论“更简单的假设”：具有更大初始概率的假设，因此需要更少的证据来建立。例如：面对一个城镇中的一千个人，所有人都同样有可能是凶手，可以将“凶手是一名男子”视为比“凶手是一名棕色头发的男子和一名男子”更“简单”的假设。狗”，因为前一个假设有 50% 的先验，因此只需要一“位”证据即可建立（即概率空间减半），而后一个假设有先验小得多，因此需要更多位。我们称之为“琐碎的简单”。</p><p> “琐碎的简单性”与“<a href="https://en.wikipedia.org/wiki/Occam%27s_razor">奥卡姆剃刀</a>”中使用的简单性相关，但又不同。奥卡姆剃刀（大致）是一种<em>实质性</em>主张，即<em>给定独立的简单性概念</em>，更简单的假设更有可能基于先验。而平凡的简单性意味着更简单的假设<em>根据定义</em>更有可能基于先验。如果你把奥卡姆剃刀视为理所当然，很容易将两者混为一谈——但前者很有趣，而后者则是琐碎和误导的结合。无论如何，我们的兴趣不在于“SGD 选择方案制定者”之类的<em>假设</em>的简单性，而在于 SGD 模型选择实现的<em>算法</em>的简单性。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-7" id="fnref-DNdPEGz4DYMju8reA-7">[7]</a></sup></p><h2> SGD 选择是为了简单吗？</h2><p> SGD 是否选择了我刚才描述的一种重要意义的简单性？</p><p>您可能认为这来自“奖励贡献者”框架的原因之一。也就是说：使用参数更简单的算法将释放其他参数以用于其他目的，因此参数简单性会增加模型的奖励似乎是非常合理的。就重写简单性与参数简单性相关而言，同样的情况也适用于重写简单性。这是我认为最引人注目的关于为什么简单很重要的故事。</p><p>不过，我想也许还有更多的话要说。例如，我认为可能有其他经验证据表明，在其他条件相同的情况下，SGD 选择更简单的函数（例如，用直线连接一组类似线的点比用极其复杂的曲线连接要快得多） ;也许，这种行为是其成功的部分原因（例如，因为现实世界的函数在这个意义上往往很简单，就像奥卡姆剃刀一样）。例如，在将 SGD 理解为贝叶斯采样的近似的背景下（根据上面<a href="https://arxiv.org/abs/2006.15191">Mingard 等人 (2020)</a>的讨论）， <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a>讨论了参数<em>的先验</em>概率分布的经验证据（例如，我上面称为“初始化分布”）将更高的概率质量放在更简单的函数上。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-8" id="fnref-DNdPEGz4DYMju8reA-8">[8]</a></sup>他将这一点与计算机科学中称为“莱文界”的理论结果联系起来，该结果预测了这一点（详情见脚注）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-9" id="fnref-DNdPEGz4DYMju8reA-9">[9]</a></sup></p><p>我还没有深入研究过这个问题。不过，如果准确的话，这种结果也会从“额外标准”框架中给出简单的相关性。也就是说，在这个框架上，甚至在我们开始优化奖励之前，SGD 就偏向于简单性。</p><p>那么，我们假设 SGD 选择某种不平凡的简单性。这种选择会偏向有利于策划者吗？</p><h2>类似阴谋家的目标的简单性优势</h2><p>上面我提到，计数论点有时被作为预期对阴谋者产生偏见的理由。但请注意，计数论点（至少正如我所介绍的那样）并没有明显提及对简单性本身的偏见。我认为我们应该小心，不要将“SGD 选择策划者”<em>假设</em>的（微不足道）简单性混为一谈<em>，因为先验概率分布将大部分概率放在策划者身上</em>（例如，各个模型上的均匀分布 - 即-get-high-reward），声称给定的个体策划者实现的<em>算法</em>（实质上）比给定的非策划者实现的算法简单。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-10" id="fnref-DNdPEGz4DYMju8reA-10">[10]</a></sup>事实上，我自己的感觉是，计数论证的最强形式让它站在自己的直观术语上，而不是试图将其与有关 SGD 特别是简单性的偏见的进一步问题联系起来。</p><p>也就是说，可以建立这种形式的联系。特别是：我们可以说，<em>由于</em>如此广泛的目标可以激发阴谋，阴谋者让SGD有非常广泛的目标可供选择，以寻找更简单的目标；而非阴谋者则不然。当我们想象成为一个非阴谋家所需的目标相当复杂时，这似乎尤其合理（更多内容见下文）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-11" id="fnref-DNdPEGz4DYMju8reA-11">[11]</a></sup></p><p>这类论证的一个有趣的特点是，它具体地认为模型之间的简单性差异完全来自于它们<em>目标</em>的内容。事实上，Hubinger (2022) 中的玩具分析具体假设各个模型类都具有相同的世界模型和优化过程，并且其算法整体的复杂度可以通过<em>世界模型的复杂度 + 优化过程的复杂度 +</em>来近似<em>目标的复杂性。</em> “球门槽”是不同型号之间唯一不同的部分。</p><p>然而，目前还不清楚这是否正确，特别是如果我们假设所涉及的目标导向性是“混乱的”而不是“干净的”。例如，在某种程度上，阴谋者必须执行非阴谋者<em>不会</em>执行的工具推理类型（例如，推理获得奖励的工具价值、推理何时背叛等），这似乎有可能引入算法本身的额外复杂性（而不是仅仅要求算法“运行更长时间”，如下面的“速度”分析）。例如，就我们使用“参数简单性”作为简单性概念而言，我们可以想象这种工具推理需要额外参数的情况。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-12" id="fnref-DNdPEGz4DYMju8reA-12">[12]</a></sup></p><h2>这些简单性优势有多大？</h2><p>不过，现在我们还是坚持胡宾格的本体论，并简单地说明差异源于目标之间的差异。选择策划者在这方面有多大的优势呢？</p><p>运行此分析的一种方法是比较每个类中最简单的模型所具有的目标（或者：因为您希望 SGD 选择最简单的可能模型，或者您认为这是近似所面临的简单性优势的好方法）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-13" id="fnref-DNdPEGz4DYMju8reA-13">[13]</a></sup>也就是说，我们比较以下的复杂度：</p><ol><li><p>指定目标（培养圣人）</p></li><li><p>最简单的可能的错误概括目标，在没有训练游戏的情况下获得训练数据的最大奖励（对于错误概括的非训练游戏玩家）</p></li><li><p>寻求剧集奖励的最简单方法（针对剧集奖励寻求者）</p></li><li><p>激发阴谋的最简单的可能目标（对于阴谋者）</p></li></ol><p>显然，在这种情况下，指定目标的复杂性很重要。我的感觉是，关于心计的讨论常常假设特定的目标在某种意义上是相当复杂的——特别是“<a href="https://www.lesswrong.com/tag/complexity-of-value">人类价值观是复杂的</a>”这一观念中的利害关系。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-14" id="fnref-DNdPEGz4DYMju8reA-14">[14]</a></sup>也许，如果我们想象获得<em>一致性</em>的唯一方法是首先（a）通过培训目标以某种方式指定“人类价值观”，然后（b）以某种方式确保我们得到一个培训圣人，那么将“按照人类价值观行事”附近的事物作为指定目标是适当的。但请注意，为了将<em>阴谋</em>的概率与<em>其他形式的失准</em>概率进行比较，我们不需要假设这样的焦点。因此，我们指定的目标可能比“按照人类价值观行事”简单得多。例如，它可能类似于“在剧集中获得金币”。事实上，在<a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">其他工作</a>中，胡宾格（与其他人一起写作）表明，像“最小化下一个标记预测误差”这样的目标非常简单——而且事实上，“它的复杂性与最简单的长期目标具有竞争力”（这是这是 Hubinger 对在 LLM 预训练期间避免阴谋相对乐观的部分原因——尽管就我个人而言，我对为什么 Hubinger 认为“下一个令牌预测错误”比“情节奖励”更简单感到困惑）。</p><p>不过，假设指定的目标具有“按照人类价值观行事”或“以有益、无害和诚实 (HHH) 的方式遵循指示”的复杂性。上面（1）-（4）之间的比较在哪里？</p><p>至少从参数简单性的角度来看，解决这个问题的一种方法是考虑我们对神经网络中表示不同人类概念的参数的绝对成本的了解。我不会在这里深入探讨，但一条似乎相关的数据是：像 GPT-4 这样的 LLM 似乎能够表示大量复杂的人类概念，例如一万亿个参数的量级 - 包括，看似合理的概念比如“诚实”、“乐于助人”、“奖励”等等。这还没有提及 GPT-4 所做的所有其他事情的所有参数。因此，表示此类概念所需的参数绝对数量不可能高得离谱。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-15" id="fnref-DNdPEGz4DYMju8reA-15">[15]</a></sup>这限制了上述模型之间差异所带来的整体复杂性节省。例如，如果我们假设万亿参数模型容量的最多 1% 用于表示像“诚实”这样复杂的概念，并且它至少知道 10,000 个这样的概念（<a href="https://www.merriam-webster.com/help/faq-how-many-english-words">韦氏未删节词典有约 500,000 个单词</a>），那么表示“诚实”概念所需的最大参数数量约为一百万（这个估计让我觉得相当保守）。因此，如果最简单的类似策划者的目标只需要 1 个参数来表示，那么在策划者的“目标槽”中不表示诚实，最多可以节省 999,999 个参数——大约是万亿参数模型表示能力的百万分之一，甚至更少适用于未来的更大型号。</p><p>但更重要的是：没有人认为阴谋者根本不必代表“诚实”、“乐于助人”、“奖励”等概念。正如<a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">Hubinger 等人 (2023)</a>所指出的，这里重要的不是表示所讨论的不同目标的绝对复杂性，而是<em>以已经拥有良好的世界模型为条件的</em>复杂性。我们应该假设<em>所有</em>这些模型都需要理解指定的目标、奖励过程等（尤其是：“玩训练游戏”的模型，其中这些概念起着核心作用）。所以，实际上，相关的问题是：表示“在情节中获得奖励”或“以 HHH 方式遵循指令”等目标的<em>额外</em>复杂性成本是多少（相对于最简单的类似阴谋家的目标），<em>一旦您已经支付了拥有这些目标概念的成本</em>。</p><p>我不确定具体如何考虑这个问题，但对我来说，这里的成本非常小似乎是非常合理的。特别是：SGD 似乎能够显着地重新调整用于表示世界模型中的概念的参数的用途，从而使该概念以类似目标的方式指导模型的行为。因此，打个比方，也许“快乐”的概念在某种意义上比日本美学中“<a href="https://en.wikipedia.org/wiki/Wabi-sabi">侘寂</a>”的概念“更简单”（即“欣赏‘不完美、无常、不完整’的美”） 。然而，一旦你<em>学会了</em>两者，追求前者是否比追求后者需要更多的参数？ <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-16" id="fnref-DNdPEGz4DYMju8reA-16">[16]</a></sup></p><p> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger（2022）</a>对此类问题的讨论有时会诉诸世界模型某些部分的“指针”概念。据我了解，这里的想法是，如果你已经在你的世界模型中得到了“快乐”/“wabi-sabi”/“奖励”之类的概念，你可以通过给予模型来追求这个东西它是一个目标槽，表示诸如“追求<em>那个</em>”或“<em>那</em>是好的”之类的东西，其中“那个”指向有问题的事物（这与必须<em>再次</em>完整且冗余地表示相关概念形成对比，在目标槽本身）。但就我们使用这样的玩具模型而言（我怀疑我们应该依赖它），为什么认为<em>指向</em>一个更复杂的概念比指向一个更简单的概念要复杂得多呢？例如，即使“wabi-sabi”需要比“pleasure”更多的参数来在世界模型中表示，为什么认为将<em>指针编码</em>为“pleasure”（例如，“go for <em>that”</em> ）比<em>编码</em>指针需要更多的参数到“wabi-sabi”（例如，再次“去做<em>那个</em>”）？</p><p>这里，一种选择是概念的复杂性和指针的复杂性是相关的。例如，您可能会想象该模型具有某种概念的“内部数据库”，它以某种方式存储概念，使得需要较少参数来存储的概念也需要较少参数来“查找”。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-17" id="fnref-DNdPEGz4DYMju8reA-17">[17]</a></sup>在这张图中，“快乐”可能最终存储为数据库中的第 15 个概念<em>，因为</em>它需要 23 个参数来表示，而“wabi-sabi”可能最终存储为第 125355 个概念，因为它需要 10,000 个参数来表示。代表。然后指向 pleasure 的“指针”可以说“去寻找存储在位置 15 的东西”，而指向“wabi-sabi”的“指针”必须说“去寻找存储在位置 125355 的东西”，这需要一些时间更多位需要指定。但即使在抽象玩具说明性示例级别，此类故事也需要依赖于模型指针和概念存储过程如何工作的特定模型 - 并且仍然需要解释<em>为什么</em>简单表示和简单 - to-point-at 在相关意义上是相关的。</p><p>不过，或者，我们可以放弃对在世界模型中存储概念的复杂性的任何兴趣，而直接关注指向它的复杂性。尽管如此，只要指出一个概念与另一个概念的复杂性之间存在有意义的<em>差异</em>，我们就可以重新论证阴谋者提供简单性优势的论点。特别是：选择阴谋者可以让 SGD 从<em>世界模型中最容易指出的</em>任何阴谋者激励目标中进行选择；而其他模型类似乎施加了更多实质性约束。即，如果指定的目标最终存储在模型的隐喻数据库中的位置 12634 处，并且如果“情节奖励”位于位置 35364 处，那么如果在例如位置 1-100 处存在任何类似策划者的目标，相反，指向<em>其中</em>一个会更简单 - 因此，创建一个阴谋家而不是一个训练圣人或一个情节奖励寻求者。</p><p>就我们关注不同模型类的最终属性而言，我认为这可能是运行以简单性为中心的方案论证的最佳方式——特别是如果我们不太沉迷于“指针”的玩具本体的话”（更不用说“数据库”）。也就是说，粗略地说：即使假设这里表上的所有目标（例如指定的目标、剧集奖励等）都需要由模型在某处表示，但无论出于何种原因，这都可能是这样的，其中一些概念比其他概念需要更少的参数来<em>重新调整目标追求</em>。只要阴谋使更多潜在的概念可用于这种重新利用，它就提供了从这个意义上说的简单性优势。</p><p>但从数量上来说，这种优势有多大？现在特别难说。特别是，一旦我们将注意力从“简单地表示”转移到“简单地重新利用目标追求”，我们甚至无法审视我们对诸如“奖励”、“诚实”和“下一个令牌预测错误”，因为我们不再讨论概念本身的复杂性。相反，我们正在推测重新利用预先存在的概念表示以用于模型的动机系统时所面临的复杂性差异，这似乎是更加不确定的领域。</p><p>尽管如此，就我们可以估计这些差异的大小而言，在我看来，它们确实非常小是合理的。对我来说，一个直觉泵的运行方式如下。假设该模型在其世界模型/“数据库”中有 2^50 个概念（大约 1e15），原则上可以将其转化为目标。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-18" id="fnref-DNdPEGz4DYMju8reA-18">[18]</a></sup>为每个 2^50 个概念进行编码所需的平均位数不能高于 50（因为：您可以为每个概念分配不同的 50 位字符串）。因此，如果我们假设模型的编码相对于平均值相当有效，并且最简单的非计划最大奖励目标采用大致平均简单性“指针”，那么如果我们为每位分配一个参数，指向最简单的非诡计式最大奖励目标最多只是额外的 50 个参数——万亿参数模型容量的二十亿分之一。也就是说，我希望弄清楚这类论证的细节会变得很棘手，而且我不会在这里尝试这样做（尽管我有兴趣看到其他尝试这样做的工作）。</p><h2>这种以简单性为中心的论点是否能够对阴谋者最终的目标做出合理的预测？</h2><p>在阴谋的简单论证的背景下，似乎值得追踪的另一个考虑因素是他们对阴谋者最终将达到的目标类型所做的预测。特别是，如果您认为 (1) SGD 很难选择更简单的目标，(2) 这种选择有利于类似计划者的目标，因为它们可以更简单，以及 (3) 我们对 SGD 选择的预测可以忽略创建相关模型所需的“路径”，那么至少天真地，您似乎应该期望 SGD 选择一个具有极其简单的长期目标（也许：最简单的长期目标）的策划者，<em>无论该目标是否与训练期间的突出或重要内容有任何关系</em>。因此，作为一个玩具示例，如果一旦你有了一个完全详细的世界模型，“最大化氢”恰好是最简单的长期目标， <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-19" id="fnref-DNdPEGz4DYMju8reA-19">[19]</a></sup>这些假设可能意味着 SGD 很可能会选择那些想要的策划者。最大限度地利用氢，即使培训都是关于金币的，并且根本没有使氢成为一个突出/相关的焦点（即使作为代理）。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-20" id="fnref-DNdPEGz4DYMju8reA-20">[20]</a></sup></p><p>就我个人而言，我对这样的预测持怀疑态度（尽管这种怀疑可能部分源于对忽略 SGD 在模型空间中更普遍的路径的怀疑）。关于阴谋者的常见故事往往侧重于与整个训练过程有更密切联系的代理目标（例如，接受金币获取训练的模型最终会评估“一直获取金币”或“跟随我的好奇心”）在所有时间里”，而不是“在所有时间里最大化氢”）。</p><p>当然，也可以假设训练期间显着/相关的目标对于模型追求来说也将是“更简单”，也许它们在世界模型中表示更重要（因此更简单？），或更简单（出于某种原因）模型重新用于曾经代表的目标追求。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-21" id="fnref-DNdPEGz4DYMju8reA-21">[21]</a></sup>但是，如果我们以这种方式认可一些故事，我们也应该追踪它与追求<em>非阴谋目标</em>的简单性的相关性。特别是：在某种程度上，我们假设训练期间的显着性/相关性与相关意义上的简单性相关，这有利于指定目标的简单性，以及情节奖励的简单性 -因为这些在培训过程中<em>特别</em>突出/相关。 （当然，只要在训练期间仍然存在<em>更简单的</em>类似阴谋者的目标，这些目标在训练过程中是显着/相关的，那么类似阴谋者的目标可能仍然会在总体上胜出。）</p><p>还要注意的是，在某种程度上，SGD 很难选择更简单的目标（例如，在“低路径依赖”形式的背景下，导致单一最优排序模型的强收敛），这似乎有点与强形式的守门假说不同，根据该假说，训练-游戏会让你的目标“具体化”。例如，如果一个潜在的阴谋家一开始有一个不太简单的目标，但仍然会激发长期的权力追求，那么如果它知道事实上，SGD 会继续将其目标磨成更简单的目标，甚至是更简单的目标。在它开始训练-游戏之后，它可能一开始就没有动力开始训练-游戏——而且它的目标无论如何都不会在这个过程中幸存下来。 <sup class="footnote-ref"><a href="#fn-DNdPEGz4DYMju8reA-22" id="fnref-DNdPEGz4DYMju8reA-22">[22]</a></sup></p><h2>对简单性论证的总体评估</h2><p>总的来说，我确实认为在其他条件相同的情况下，阴谋者可能比其他模型类有更简单的目标。然而，我认为相关的简单性差异可能非常小，特别是当我们更普遍地以具有良好世界模型的模型为条件时（更重要的是，如果我们假设目标针对训练期间的显着/相关性获得额外的简单性点）。我对某些理论包袱表示怀疑，它可能感觉像是某些类型的简单性论证带来的（例如，与利害攸关的简单性概念相关的包袱、SGD 是否选择它、如何在上下文中考虑简单性）为追求目标而重新利用，而不是仅仅代表，等等）。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-DNdPEGz4DYMju8reA-1" class="footnote-item"><p>参见<a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment#Deceptive_alignment_in_the_low_path_dependence_world">Hubinger (2022)</a>等。 <a href="#fnref-DNdPEGz4DYMju8reA-1" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-2" class="footnote-item"><p>另请参阅<a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive">此（现在是匿名）讨论，</a>了解“简单性”用法的另一个示例。 <a href="#fnref-DNdPEGz4DYMju8reA-2" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-3" class="footnote-item"><p>在这里，我的感觉是，通常的假设是 X 可以在计算抽象级别上进行描述，这样所涉及的“重写”不仅仅是重现网络本身。例如，网络被理解为实现一些更抽象的功能。我认为这是一个有趣的问题，简单性论证在放松这种假设的情况下能否幸存下来。 <a href="#fnref-DNdPEGz4DYMju8reA-3" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-4" class="footnote-item"><p>另一个问题是<a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">柯尔莫哥洛夫复杂度是不可计算的</a>。我被告知你可以近似它，但我不确定这如何解决这个问题：对于给定的程序，你无法判断它是否停止，该程序可能是输出相关的最短程序细绳。 <a href="#fnref-DNdPEGz4DYMju8reA-4" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-5" class="footnote-item"><p>有关详细信息，请参阅<a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#iv-can-you-ignore-being-only-finitely-wrong">Carlsmith (2021)</a>第 III 节和 IV 节。 <a href="#fnref-DNdPEGz4DYMju8reA-5" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-6" class="footnote-item"><p>胡宾格有时似乎也对这个概念很有吸引力——或者至少没有在“重写简单性”和“参数简单性”之间划清界限。 <a href="#fnref-DNdPEGz4DYMju8reA-6" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-7" class="footnote-item"><p> “琐碎的简单性”也与我们所说的“选择简单性”密切相关。在这里，我们再次假设一些可能的事情（例如，目标）的空间/分布，然后根据需要做多少“工作”来讨论该空间的某些部分的“简单性”（也许：平均而言） ）以便从整个空间缩小到空间的该部分（另请参见<a href="https://colah.github.io/posts/2015-09-Visual-Information/">可变长度代码</a>）。因此，对于一盒气体，“分子大致均匀地分布”可能比“分子都在特定的角落”更“简单”，因为它通常需要更多的“功”（在本例中：热力学功）导致前者比后者（这与前者最初比后者更有可能的事实密切相关）。我的感觉是，当有些人说“类似阴谋家的目标很简单”时，他们的意思更像是：就 SGD 而言，与一<em>组</em>非阴谋家相比，这<em>组</em>类似阴谋家的目标通常需要更少的“工作”来实现。 -类似阴谋家的目标（不一定：任何<em>特定的</em>类似阴谋家的目标都比某些<em>特定的</em>非阴谋家的目标更简单）。在某种程度上，类似阴谋家的目标应该具有这种属性，因为它们更“常见”，因此“更接近”可持续发展目标的起点，这种谈论阴谋的简单好处的方式相当于重述诸如计数参数和/或“最近的最大奖励目标参数”之类的东西 - 除了在我看来更倾向于将<em>一组</em>类似阴谋家的目标的简单性与<em>给定的</em>类似阴谋家的简单性混淆目标。 <a href="#fnref-DNdPEGz4DYMju8reA-7" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-8" class="footnote-item"><p>重要的是，多个不同的参数设置可以实现相同的功能。 <a href="#fnref-DNdPEGz4DYMju8reA-8" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-9" class="footnote-item"><p>我的理解是，莱文界限是这样说的：对于给定的参数分布，随机采样一组实现函数<em>f</em>的参数的概率<em>p(f)</em>的界限为<em>2^{-K(f) + O( 1)}</em> ，其中<em>K</em>是函数<em>f</em>的<em>k</em>复杂度， <em>O(1)</em>是一些独立于函数本身的常数（尽管：取决于参数空间）。也就是说，随着函数复杂性的增加，某些函数的先验值呈指数下降。</p><p>我还没有调查过这个结果，但我看到的一个总结（ <a href="https://www.lesswrong.com/posts/YSFJosoHYFyXjoYWa/why-neural-networks-generalise-and-why-they-are-kind-of?commentId=kLz9mgNv8xFNrAPt2">此处</a>）使它看起来相当空洞。特别是，该摘要中的想法是，较大容量的参数空间将具有更简单的编码，因为您可以通过首先指定参数的分布来对它们进行编码，然后使用<a href="https://en.wikipedia.org/wiki/Huffman_coding">霍夫曼代码</a>来讨论如何在给定的分布下找到它们。但这使得结果看起来相当微不足道：这并不是说存在一些简单的先行概念，然后我们根据初始化分布发现其概率更高。相反，根据初始化分布获得较高概率<em>只是</em>更简单，因为配备了初始化分布，更容易对其较高概率部分进行编码。或者换句话说：这个结果似乎适用于任何参数分布。因此，我们似乎并没有从中学到太多关于任何特定分布的知识。</p><p> （对我来说，感觉这与“较短的程序获得更大的概率”的方式有类比，在算法“简单性先验”的背景下，重点关注 K 复杂度等指标，实际上必然适用于可数范围内的<em>任何</em>分布-无限的程序集 - 请参阅<a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#ii-my-current-favorite-pitch-for-the-ud">此处的</a>讨论。您可能认为这是一个有趣且实质性的约束，但实际上它变得更加空洞。）</p><p>也就是说，我上面提到的经验结果侧重于更实用的、现实世界的简单性度量，例如<a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv_complexity">LZ 复杂度</a>，并且显然他们发现，确实，更简单的函数获得更高的先验概率（参见例如<a href="https://arxiv.org/pdf/1805.08522.pdf">这个实验</a>，它使用完全连接的神经网络对从许多二进制输入到单个二进制输入的可能函数进行建模）。在我看来，这更加实质性和有趣。 <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a>声称 Levin 的结果并非微不足道，尽管我还不明白其中的原因。 <a href="#fnref-DNdPEGz4DYMju8reA-9" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-10" class="footnote-item"><p>因此，例如，您可能认为随机初始化的模型更有可能最终“更接近”计划者，因此 SGD 需要做“更少的工作”才能选择计划者而不是其他模型，这有利于阴谋家（感谢 Paul Christiano 的讨论）。但是，这种论点在于对计划者提出更高的先验概率，在我的书中，这不是（非平凡的）简单参数本身。 <a href="#fnref-DNdPEGz4DYMju8reA-10" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-11" class="footnote-item"><p>对于简单性和计划者之间的联系也有更多的投机性和理论论据，人们认为，如果您对所有可能的程序进行无限搜索，以找到给定输出的最短程序，而无需考虑其他因素，例如多长时间他们必须运行，然后您将选择一个schemer（例如，通过一条途径，例如：模拟极其简单的物理学，最终会引起理解情况并希望脱离模拟的代理，并给予相关输出作为这样做计划的一部分。我的理解是，人们（例如<a href="https://www.lesswrong.com/posts/KSWSkxXJqWGd5jYLB/the-speed-simplicity-prior-is-probably-anti-deceptive#Priors_on_Learned_Optimizers">，这里</a>）有时会讨论有关“ <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">所罗门诺夫先验的恶性</a>”的论述（尽管在我看来，在我看来，在我看来，就像存在重要的差异一样，例如，在因果关系的类型中股份，以及是否可能正在模拟<em>您的</em>问题）。无论如何，我怀疑这些无限的理论论点应该会增加很多体重，而且我不会在这里对待它们。 <a href="#fnref-DNdPEGz4DYMju8reA-11" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-12" class="footnote-item"><p>更重要的是，请注意，在我们想象SGD偏向简单性的范围内<em>，因为</em>现实世界的模式往往很简单（例如，Occam的剃须刀确实是一个很好的先验，而SGD在某种程度上很好地效果很好，因为它反映了这一先验），这是对此的解释），这是对此的解释。这种偏见并不容易适用于模型的<em>目标</em>。也就是说（Modulo各种形式的道德现实主义），没有“真实目标”，其建模可能会从简单性中受益。相反，在这个故事上，尽管不相信有任何问题的客观事实，但SGD仍需要更像人类的道德反现实主义者，更喜欢简单的道德，尽管没有任何客观的事实<em>是</em>事实的事实，更简单的理论往往更有可能。 <a href="#fnref-DNdPEGz4DYMju8reA-12" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-13" class="footnote-item"><p> Hubinger使用这种方法。我的理解是，他正在想象SGD选择具有与其简单性成比例的模型，因此例如，关注最简单的模型是近似模型类中总体概率的一种方式，并且专注于类中的模型数量IS IS的<em>数量</em>其他。但是，我不会认为SGD选择具有与其简单性成正比的模型的假设。 <a href="#fnref-DNdPEGz4DYMju8reA-13" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-14" class="footnote-item"><p>请参阅<a href="https://www.lesswrong.com/posts/qoHwKgLFfPcEuwaba/conditioning-predictive-models-making-inner-alignment-as#Analyzing_the_case_for_deceptive_alignment">此处的</a>Hubinger等人（2023）。 <a href="#fnref-DNdPEGz4DYMju8reA-14" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-15" class="footnote-item"><p>我首先听到了保罗·克里斯蒂安诺（Paul Christiano）的观点。 <a href="#fnref-DNdPEGz4DYMju8reA-15" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-16" class="footnote-item"><p>我的意思不是：<em>成功地</em>促进愉悦与成功促进瓦比·萨比（Wabi-Sabi）的愉悦感需要更多的参数。我的意思是：目标<em>优化</em>一个与另一个相对于另一个参数需要更多的参数。 <a href="#fnref-DNdPEGz4DYMju8reA-16" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-17" class="footnote-item"><p>感谢Daniel Kokotajlo提出了这样的图像。 <a href="#fnref-DNdPEGz4DYMju8reA-17" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-18" class="footnote-item"><p>这里的精确概念数量并不重要。 <a href="#fnref-DNdPEGz4DYMju8reA-18" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-19" class="footnote-item"><p>我并不是说这是，即使对于基于物理的世界模型，我也想简单地说明这一点。请随时在这里替代您最简单的目标。 <a href="#fnref-DNdPEGz4DYMju8reA-19" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-20" class="footnote-item"><p>值得注意的是，这种预测似乎特别适合人类与进化之间的类比，因为人类的目标似乎与生殖健身有着非常明显的关系。但是，无论如何，进化都是“路径依赖性”的。 <a href="#fnref-DNdPEGz4DYMju8reA-20" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-21" class="footnote-item"><p>例如，对于人类来说，“氢”并不是一个简单的概念，而是“威胁”之类的概念，因为后者在我们的进化环境中更加相关。 <a href="#fnref-DNdPEGz4DYMju8reA-21" class="footnote-backref">↩︎</a></p></li><li id="fn-DNdPEGz4DYMju8reA-22" class="footnote-item"><p> Hubinger在讨论中表明，该模型的推理将以逻辑而不是物理因果关系进行。他写道：“这里的理由是：我应该是一种玩训练游戏的模型，因为有一些（逻辑）的机会我将成为具有最佳归纳偏见的模型，因此我应该确保我也有很好的损失。”但是，如果一个模型可以<em>说出</em>它的目标尚不最佳（因此会被SGD击倒），那么我不确定为什么它会认为它是一个“逻辑上的机会”，因为它受到归纳偏见的青睐在这个意义上。 <a href="#fnref-DNdPEGz4DYMju8reA-22" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uWdAKyHZMfoxDHcCC/simplicity-arguments-for-scheming-section-4-3-of-scheming<guid ispermalink="false"> uWdAKyHZMfoxDHcCC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Thu, 07 Dec 2023 15:05:54 GMT</pubDate></item></channel></rss>