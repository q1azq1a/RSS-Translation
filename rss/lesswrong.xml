<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 2 日星期四 06:15:30 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[ACX/LW/EA crossover meetup]]></title><description><![CDATA[Published on November 2, 2023 5:57 AM GMT<br/><br/><p>我们于 11 月 28 日 19 点在 Fehrfeld 酒吧见面。</p><br/><br/><a href="https://www.lesswrong.com/events/NfBuoBKBmgDCzXyPh/acx-lw-ea-crossover-meetup#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/NfBuoBKBmgDCzXyPh/acx-lw-ea-crossover-meetup<guid ispermalink="false"> NfBuoBKBmgDCzXyPh</guid><dc:creator><![CDATA[JohannWolfgang]]></dc:creator><pubDate> Thu, 02 Nov 2023 05:57:25 GMT</pubDate> </item><item><title><![CDATA[Upcoming Feedback Opportunity on Dual-Use Foundation Models]]></title><description><![CDATA[Published on November 2, 2023 4:28 AM GMT<br/><br/><p>来自拜登最近的<a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">行政命令</a>：</p><p> “4.6. 就具有广泛可用的模型权重的两用基础模型征求意见。当两用基础模型的权重广泛可用时（例如当它们在互联网上公开发布时），可以为创新带来巨大的好处，但也存在重大安全风险，例如消除模型内的保障措施。为了解决具有广泛可用权重的两用基础模型的风险和潜在好处，商务部长在本命令发布之日起 270 天内，采取行动通过负责通信和信息的助理商务部长并与国务卿协商后，应：<br><br> (a) 通过公众咨询进程，征求私营部门、学术界、民间社会和其他利益攸关方对模型所重视的两用基础模型的潜在风险、收益、其他影响以及适当的政策和监管方法的意见广泛可用，包括：</p><p> (i) 与参与者微调模型权重广泛可用的两用基础模型或取消这些模型的保障措施相关的风险；</p><p> (ii) 模型权重广泛可用的两用基础模型对人工智能创新和研究的好处，包括对人工智能安全和风险管理的研究；和</p><p>(iii) 潜在的自愿、监管和国际机制，用于管理风险并最大限度地提高模型权重广泛可用的双重用途基础模型的效益；和<br><br>(b) 根据本节第 4.6(a) 款所述流程的输入，并与商务部长认为适当的其他相关机构负责人协商，向总统提交一份关于潜在利益和风险的报告，以及模型权重广泛可用的双重用途基础模型的影响，以及与这些模型相关的政策和监管建议。”</p><p>这似乎很重要，因为模型权重的开源本质上会导致功能不可逆转的扩散。我怀疑美好的未来很有可能取决于这里取得的正确结果。</p><br/><br/> <a href="https://www.lesswrong.com/posts/RgLKJCjmfmHePubAk/upcoming-feedback-opportunity-on-dual-use-foundation-models#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RgLKJCjmfmHePubAk/upcoming-feedback-opportunity-on-dual-use-foundation-models<guid ispermalink="false"> RgLKJCjmfmHePubAk</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Thu, 02 Nov 2023 04:28:12 GMT</pubDate> </item><item><title><![CDATA[Public Weights?]]></title><description><![CDATA[Published on November 2, 2023 2:50 AM GMT<br/><br/><p><i>虽然这与我工作的区域很接近，但这是一个个人帖子。在我发表之前没有人评论过它，或者要求我（或不）写一些东西。所有的错误都是我自己造成的。</i></p><p>几天前，我在<a href="https://securebio.org/">SecureBio</a>的一些同事发布了一份预印本，“发布未来大型语言模型的权重是否会为大流行病媒介提供广泛的使用机会？” ( <a href="https://arxiv.org/pdf/2310.18233.pdf">Gopal et. al 2023</a> ) 他们采用了 Facebook/Meta 的 Llama-2-70B 大语言模型 (LLM) 并（廉价地！）对其进行了调整以删除内置的保护措施，之后它愿意回答有关如何感染的问题<a href="https://en.wikipedia.org/wiki/Spanish_flu">1918年流感</a>。我喜欢这篇论文的很多方面，但我也认为它因不确定是否具有交流性而受到影响：</p><ol><li>公开法学硕士是危险的，因为通过发布权重，您可以让其他人轻松消除保护措施。</li><li>一旦取消保障措施，当前的法学硕士已经有助于获取引起大流行所需的关键信息。</li></ol><p>我认为它很好地证明了第一点。我们避免法学硕士告诉人们如何造成伤害的主要方法是对他们进行大量有人询问如何造成伤害并被告知“不”的例子进行培训，并且可以通过使用“是”例子进行额外培训来轻松扭转这种情况。因此，即使你在这方面非常擅长，如果你公开你的法学硕士，其他人就很容易将其转变为顺从地分享其中包含的任何知识的东西。</p><p>现在，您可能认为实际上不存在任何危险知识，至少在法学硕士可以从公开来源中学到的知识范围内。我认为这显然是不正确的：创造传染性 1918 年流感的过程分散在互联网上，大多数人很难聚集起来。然而，如果您有一位经验丰富的病毒学家随时待命并且乐意回答任何问题，那么他们可以引导您完成自己做事和欺骗他人做事的混合过程。如果他们能够阅读和综合所有病毒学文献，他们可以告诉你如何创造出比之前的大流行更糟糕的事情。</p><p> GPT-4 已经明显优于 Llama-2，2024 年 GPT-5<a href="https://manifold.markets/VictorLJZ/will-gpt5-be-released-before-2025">的可能性更大</a>。公共模型可能会继续向前发展，虽然<a href="https://manifold.markets/ZviMowshowitz/will-llama3-be-as-good-as-gpt4">我们不太可能在 2024 年获得 GPT-4 级别的 Llama-3，但</a>我确实认为默认路径会在几年内涉及非常好的公共模型。到那时，任何拥有良好 GPU 的人都可以拥有自己的个人非道德病毒学家顾问。这似乎是一个问题！</p><p>但这篇论文似乎还试图探讨当前模型是否能够教人们如何制造 1918 年流感的问题。如果他们只是想评估模型是否愿意并且能够回答有关如何制造生物武器的问题，他们可以直接问。相反，他们举办了一场黑客马拉松，看看人们是否可以在一小时内获得无保障模型，以完全引导他们完成创造传染性流感的过程。我认为法学硕士是否已经降低了通过生物学造成巨大伤害的门槛，这是一个非常重要的问题，我希望看到后续行动能够通过非法学硕士对照组来解决这个问题。这仍然不是完美的，因为在黑客马拉松的限制之外，你可以参加生物课，阅读教科书，或者付钱给有经验的人来回答你的问题，但这会告诉我们很多东西。我的猜测是，当前法学硕士的综合功能实际上是在这里添加了一些东西，而没有法学硕士的群体会做得更糟，但 83% 的人似乎不同意我的观点： </p><figure class="media"><div data-oembed-url="https://manifold.markets/JeffKaufman/are-open-source-models-uniquely-cap"><div class="manifold-preview"><iframe src="https://manifold.markets/embed/JeffKaufman/are-open-source-models-uniquely-cap"></iframe></div></div></figure><p>即使没有保障的公共法学硕士今天不会降低标准，并且考虑到 Llama-2 可能会多么令人沮丧，这也不会太令人惊讶，我们很可能会在接下来的几年内达到他们确实显着降低标准的水平年。把它降得足够低，一些巨魔或坚定的狂热分子就会追随它。除了存在的担忧之外，这让我非常难过。具有开放权重的法学硕士才刚刚开始民主化获得这种令人难以置信的变革性技术，而我们所有人只能通过少数受到高度监管和非常保守的组织获得法学硕士的世界感觉像是巨大的潜力损失。但除非我们弄清楚如何创建法学硕士，其中的保障措施不能被轻易消除，否则我不知道如何避免这种非自由的结果，同时也避免广泛的破坏。</p><p> （早在 2017 年，我就询问了<a href="https://www.jefftk.com/p/examples-of-superintelligence-risk">人工智能带来的风险的例子</a>，但我不太喜欢其中任何一个。今天，“有人问法学硕士如何杀死所有人，它引导他们制造了一场流行病”似乎很合理。）</p><br/><br/><a href="https://www.lesswrong.com/posts/jBHSwNHKexd5WGXcs/public-weights#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jBHSwNHKexd5WGXcs/public-weights<guid ispermalink="false"> jBHSwNHKexd5WGXcs</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 02 Nov 2023 02:50:18 GMT</pubDate> </item><item><title><![CDATA[Should people build productizations of open source AI models?]]></title><description><![CDATA[Published on November 2, 2023 1:26 AM GMT<br/><br/><p> 2021 年初，在我对 AI 安全形成任何看法之前，一群人联系了我和我的初创公司联合创始人，他们想要替代 AI CYOA 平台 AIDungeon。他们对该网站当前的管理感到沮丧，尤其是他们开始禁止用户编写 NSFW 内容的决定。当时我们几乎没钱了，所以我们通过 EleutherAI 模型为他们构建了一个新的<a href="https://writeholo.com">前端</a>，该模型对用户内容进行加密且不进行过滤。令我们惊讶的是，该网站最终比我们迄今为止尝试过的任何其他网站都做得更好，至少足以让我们渡过难关。</p><p>此后我们在 HoloAI 上工作了大约六个月。然后在 2021 年 12 月到 2022 年 4 月之间，我的人工智能研发模式从“中立技术研究”变成了“你可能做的最糟糕的事情”。虽然我们不是人工智能研究人员，而且回想起来，即使我们坚持下去，HoloAI 也不太可能大规模发展，但我们决定停止改进。该网站仍然在线，但现在我只是回答已经使用它作为编辑器的人们的支持请求。</p><p>我仍然非常同意我们选择做一些与人工智能无关的事情，但我实际上不确定对 X 风险的担忧是否真实。我们从未做过任何基础模型研究，我们没有向 OpenAI 支付代币或与他们竞争客户，HoloAI 也没有发布可与其他模型一起使用的代码。也就是说，像 HoloAI 这样的产品具有下游效应似乎是合理的，通过在人工智能产品类别中竞争，你可以间接鼓励其他人进行这些创新。我们没有向 OpenAI 付费，但市场上<i>有</i>类似的产品确实向 OpenAI 付费，现在他们必须做一些事情来保持领先。</p><p>那么对于此类公用事业的社会共识应该是什么？或者就此而言，是否有任何使用研究实验室构建的专有模型的软件？</p><p>我在这里问这个问题的部分原因是因为我预计未来 5-10 年的许多新公司将在某种程度上实现人工智能产品化。我们接触 HoloAI 主要是出于偶然，但在经济的各个领域设置某种 LLM 可能将成为技术行业未来发展的很大一部分。此外，科技行业的许多人将致力于开发这些工具或支持它们，如果不围绕这种增长进行调整，“做其他事情”可能不是一件容易的事。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Gkot4upnvMB7quzne/should-people-build-productizations-of-open-source-ai-models#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Gkot4upnvMB7quzne/should-people-build-productizations-of-open-source-ai-models<guid ispermalink="false"> Gkot4upnvMB7quzne</guid><dc:creator><![CDATA[lc]]></dc:creator><pubDate> Thu, 02 Nov 2023 01:26:47 GMT</pubDate> </item><item><title><![CDATA[Singular learning theory and bridging from ML to brain emulations]]></title><description><![CDATA[Published on November 1, 2023 9:31 PM GMT<br/><br/><p>这是我（kave）与加勒特进行的一次相当“聊天”式的对话，内容涉及奇异学习理论（SLT）以及他通过构建该理论来解决雄心勃勃的价值学习<a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole">的雄心勃勃的计划</a>。</p><p>一位同事发现，这为他们提供了比当前的阐述更好的 SLT 线索（尽管他们仍然感到困惑），而且我从这次谈话中比仅从他的帖子中更清楚地了解了加勒特的希望范围。</p><h2>什么是单一学习理论？ </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:26:30 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:26:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>嘿加勒特！以下是我对您对奇异学习理论（SLT）的希望的一些初步想法！</p><p>就 SLT 而言，我觉得我不太明白“相变”到底有什么令人兴奋的地方。或者像，我不明白如何将“相变是状态密度的快速变化”与“相变是令人惊叹的”之类的东西联系起来。我可能对人们追求的角度感到困惑。</p><p>我也对如何看待大脑和机器学习系统的 SLT 感到困惑。就像，SLT 适用的动力系统的一些概念也适用于这两件事（你认为/希望）。那是什么概念？</p><p>我也很高兴听到更多关于您对从 SLT 中溢出的程序的愿景，该程序成为一种类似于学习代理理论的东西，在其开发过程中一直“接触草”（在某种程度上这是真的）。</p><p>无论如何，这只是关于我现在的情况的一些闲聊。愿意去任何地方，但我想分享一些状态。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:28:51 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:28:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，好吧，所以，我想第一件事是相变。我不太明白你的困惑。就像，我们假设梯度下降可以充分描述为从贝叶斯后验中采样。那么，态密度从较不泛化的解到较泛化的解的快速变化直接对应于grokking？当我们从不太通用的解决方案快速过渡到更通用的解决方案时，我们会感到很惊讶。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:29:39 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:29:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>好的，但是为什么态密度的快速变化对应于更普遍的情况呢？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:31:08 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:31:08 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我目前最好的理解：态密度的快速变化对应于更多参数变得无关紧要（或更少？可能会倒退），因此你实际上变成了一个更小的模型/更简约/更少的外圆</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:32:21 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:32:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>[如果我看起来非常困惑/我的直觉似乎我误解了正在发生的事情，请随意说] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:33:02 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:33:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>好吧，我认为不仅仅是状态密度的快速变化导致了泛化程度从增加到减少？我来找一下相关的方程式</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:45:52 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:45:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>抱歉，我陷入了方程领域，忘记了更广泛的背景。我会直观地解释事物，而不是依赖方程，然后如果需要解释，我会引入方程。</p><p>因此，状态密度的快速变化对应于更一般化，因为在给定的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>我们的学习系统将由“自由能”最低的阶段主导，定义为</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_n = n L_n(w_0) + \lambda \log n + O(\log\log n)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">log</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">O</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">日志</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">_</span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="w_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span>是该阶段的最佳权重设置， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>是该阶段的实对数规范阈值 (RLCT)， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>是我们训练过的数据点的数量， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span>是训练损失这<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n 个</span></span></span></span></span></span></span>数据点之后输入的权重设置。</p><p>这基本上意味着，当我们切换到不同的阶段时，我们总是会看到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n(w_0)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>的减少和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>的增加（除非低阶项最终会做出奇怪的事情，但将其视为一阶近似） 。</p><p>我会在这里停下来提问</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:46:52 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:46:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>为什么<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>会上升？我认为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span>会下降，因为这基本上就是随机梯度下降 (SGD) 随着<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>增加而要做的事情。 （除了手波特征之外，我不知道 RLCT 是什么） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:49:52 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:49:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>上升是因为我们总是选择最低的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="F_n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.106em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.106em;">F</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span></span> 。大致来说，如果存在一个相位的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>小于当前的相位，且<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="L_n(w_0)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>相同或更低，那么我们就已经选择了该相位。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:51:29 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:51:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>让我检查一下：当你说“当我们切换到不同的阶段”时，你的意思是“如果我们要偏离最佳状态”还是“当我们在学习的过程中经历各个阶段”？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:54:06 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:54:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>因此，根据我的理解，上述自由能方程的最酷之处在于，无论您如何划分权重空间，它都有效。这是因为贝叶斯更新是自相似的，即使用粗粒度模型类的更新使用与使用细粒度模型类的更新相同的方程。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:56:38 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:56:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>RLCT 是代数几何中的一个概念，Jesse<a href="https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-measuring-model-complexity-wrong">最近的帖子</a>是一个非常好的直观讨论。就非手动讨论而言，有几种等效的方式来看待它。您脑海中的画面是，当您降低损失阈值时，它是一个渐近到零的度量，用于衡量一个阶段中权重空间的体积减少的速度。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:56:44 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:56:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>另外，什么是“阶段”？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 20:58:57 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 20:58:57 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我相信到目前为止我所说的一切，结果对你对阶段的定义并不敏感。在实践中如何使用阶段有两种类型。借用<a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus?commentId=RRT6ZcGvRrt4bRiPZ">Daniel Murfet</a>最近的解释：</p><blockquote><ul><li><i>样本数量中的贝叶斯相变：</i>正如您在<a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC/p/aKBAYN5LpaQMrPqMj#What_is_a_phase_transition_">Liam 序列</a>中链接的文章中所讨论的那样，当样本数量增加超过某个临界样本大小<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>时，贝叶斯后验的浓度突然从参数空间的一个区域转移到另一个区域。还有关于超参数的贝叶斯相变（例如真实分布的变化），但这些不是我们在这里讨论的内容。</li><li><i>动态相变</i>：“向后 S 形损耗曲线”。我不认为深度学习文献中人们对这种相变的含义有一个公认的正式定义，但我们的意思是 SGD 轨迹在一段时间内受到强烈影响（例如，在的邻域) 一个临界点<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="w_\alpha^*"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-stack" style="vertical-align: -0.157em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span></span></span></span> ，然后受到另一个临界点<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="w_\beta^*"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">w</span></span></span> <span class="mjx-stack" style="vertical-align: -0.343em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.298em;">*</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span></span></span></span></span></span></span></span>的强烈影响。在最明显的情况下，有两个平台，一个具有较高损失，对应于标签<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\alpha"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">α</span></span></span></span></span></span></span> ，另一个具有较低损失，对应于<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\beta"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.446em; padding-right: 0.007em;">β</span></span></span></span></span></span></span> 。在较大的系统中，可能没有明显的平台（例如，在您提到的感应头的情况下），但认为轨迹由临界点主导仍然可能是合理的。 </li></ul></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 20:59:49 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 20:59:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我会将“相位”解释为“模型在 20,000 英尺处的明显差异”，除非这看起来很糟糕</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:00:18 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:00:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>看起来不错。还有更多的技术细节，但说实话，这就是我通常直观地想到的方式</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:02:22 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:02:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>好吧，所以我想我仍然不知道答案</p><blockquote><p>让我检查一下：当你说“当我们切换到不同的阶段”时，你的意思是“如果我们要偏离最佳状态”还是“当我们在学习的过程中经历各个阶段”？ </p></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:03:58 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:03:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>因此，就奇异学习理论而言，我们没有一个好的方法来谈论训练过程，只能谈论数据的添加。因此，我将您的“当我们学习时”缓存为“当我们添加数据时”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:04:14 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:04:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>这对我来说听起来不错</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:05:00 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:05:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我试图澄清你是否在说“想象一下，对于给定的n，我们处于最低的自由能状态。如果你切换到更高的自由能状态，情况会是这样”或者“想象一下你处于最低的自由能状态”某个 n 处的自由能状态。如果增加 n，情况如下所示” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:05:10 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:05:10 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>哦，是的，第二个完全就是我所说的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:05:55 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:05:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>肯定有一些体重状态可以让你的自由能降低，从而导致比你目前所处的体重更高的损失以及更低或可能更高的 RLCT </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:07:26 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:07:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>回到原来的问题：</p><blockquote><p>好的，但是为什么态密度的快速变化对应于更普遍的情况呢？</p></blockquote><p><br>原因是……这是假的！有时我们会遇到双重下降，SLT 会预测这种情况何时会发生！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:07:36 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:07:36 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>!! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:11:58 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:11:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span>是两个阶段之间的差异，并且<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n_{cr}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span></span></span></span></span></span></span></span></span>是需要在两个阶段（临界点）之间切换的数据点数量时，就会发生这种情况，那么当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n_{cr} < \frac{\Delta \lambda}{\Delta L}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span></span></span></span></span></span></span></span></span>时我们会得到双下降<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n_{cr} < \frac{\Delta \lambda}{\Delta L}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 1.212em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.714em; top: -1.434em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.714em; bottom: -0.715em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.212em;" class="mjx-line"></span></span><span style="height: 1.52em; vertical-align: -0.506em;" class="mjx-vsize"></span></span></span></span></span></span></span> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:13:16 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:13:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>草图看起来像这样 (r = <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\frac{\Delta \lambda}{\Delta L}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 1.212em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 1.714em; top: -1.434em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 1.714em; bottom: -0.715em;"><span class="mjx-mrow" style=""><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 1.212em;" class="mjx-line"></span></span><span style="height: 1.52em; vertical-align: -0.506em;" class="mjx-vsize"></span></span></span></span></span></span></span> ，并且因为物理学家做了这些注释， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>分别是第 1 阶段和第 2 阶段的损失）： <br><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/brswqliwkx2wufmuoe04" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/osjyqslm6sfwmk7d8gfb 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/jzkudkpqey52jenoqjtb 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/tfbidm8oeg8yivp9s1sx 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/dmvyejdwr0xi3cncy8jv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/izo1unaepqhyldavav9y 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/jjjpc98hilcwawruwhlw 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/wmflmnq1e9smncnenjmy 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/hebz8wvj67rvhkmiynz3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/ffwqjvmublpcas4ykb3j 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/ijxb7me0o60btpx4zflu 853w"></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:13:34 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:13:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>其中两条线是泛化损失</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:16:53 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:16:53 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我真的不希望你看到这个并说“哦，这是有道理的”，因为我必须事先向你解释泛化错误，并向你展示为什么<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\Delta\lambda/\Delta L"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">Δ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span></span></span></span></span></span>是相关的，但是你应该能够通过我说<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb E B_g = L + \lambda/n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span>是预期的泛化误差来推导出来。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:17:17 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:17:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><blockquote><p>我真的不希望你看到这个并说“哦，这是有道理的”</p></blockquote><p>哦，太好了，因为我看着这个，发出了一种僵尸般的声音</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:17:37 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:17:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>哈哈，对此感到抱歉。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:19:01 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:19:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>但我认为我看到模型在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n_c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span></span></span>处跳跃阶段，有时当它变得“更糟”（更高或更低？） λ [编辑：在新阶段]时，我猜是因为权衡更好的训练损失是值得的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:20:39 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:20:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n_{cr}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span></span></span></span></span></span></span></span></span>是新相的自由能变得更有吸引力的交叉点， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="r"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span></span></span></span></span>是新相的泛化误差变得更有吸引力的交叉点。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:21:48 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:21:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>较低的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>会导致较低的泛化损失，看看你的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbb E B_g = L + \lambda/n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.446em; padding-bottom: 0.298em;">E</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.003em;">g</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:22:43 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:22:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的！直观地将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>视为模型类的复杂性，这可以直观地理解。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:23:21 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:23:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>正确的！</p><p>类似地，“当你降低损失阈值时，它是一个渐近到零的度量，衡量一个阶段中权重空间的体积减少的速度”就像一个“有限微调”规则（？） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:25:17 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:25:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我以前没见过这样的情况。你能详细说明一下吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:27:50 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:27:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>嗯。不确定是否值得深入，但当权重空间随着您要求更接近最佳损失而下降时， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>会告诉您需要多精确地确定参数才能达到这种接近程度。但是对于相对于神经网络（NN）或其他参数空间而言较小的模型类，您不会快速下降（或者对于简单/稳健的模型也类似） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:28:00 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:28:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，听起来不错。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:28:04 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:28:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>凉爽的！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:29:11 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:29:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>所以 SLT 就像“模型因为数学而偏爱较低的自由能，这告诉你的不仅仅是假设它们最小化损失（特别是因为最小化损失可能是选择一组参数的非确定性规则）”？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:29:40 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:29:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>（我们上面介绍的一些内容是它告诉您的特定内容） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:31:48 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:31:48 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>嗯...我不认为这是主要结果，但它是一个结果。我想说的主要结果是“训练后你可能最终得到的模型分布集中在奇点上（哦，还有一种非常酷的方法，使用代数几何和奇点理论从这个事实中提取出不平凡的陈述） ” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:32:20 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:32:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>这张图中的奇异点在哪里？它们是我没有读过的数学中的前大教堂，还是与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>有关？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:33:12 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:33:12 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>它们与<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>相关，但我目前正在学习该部分背后的实际代数几何和奇点理论，所以我无法告诉你原因。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:33:16 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:33:16 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>复制</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:33:57 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:33:57 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>因此，这种洞察力就是为什么我对单一学习理论持乐观态度，它不仅限于迄今为止研究的学习系统。就像强化学习和大脑的学习子系统一样。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:35:51 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:35:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>自由能可能适用于两者，但是用于查找和表征<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>的奇点的分辨率，以及导致两个过程以可简化方式运行的奇点的存在（因为它们几乎肯定都是奇点）意味着我希望使用相同的技术可以适应这些情况。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:37:25 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:37:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>这可能只是我需要的教学法问题，但从我的角度来看，相信存在奇点的原因是因为显然自由能被最小化，并且看起来它应该导致低<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>和低<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span>相关（或者因此互联网上值得信赖的人告诉我）奇点</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:37:44 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:37:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>没有太多理由期待奇点没有低自由能</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:38:12 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:38:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>虽然我也很乐意听到“还有其他理由期待奇点，但我们现在不能/不应该进入它们” </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:38:55 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:38:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>哈哈，哦，是的，我的意思是，当我说“几乎肯定”时，我的意思是具有大量参数的混乱分层模型的默认状态是它们没有一对一的函数映射，或者它们的渔民信息矩阵是单一的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:39:23 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:39:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>啊对！我现在记得奇点也与参数函数映射的非一对一有关。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:39:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:39:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>信心恢复</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:39:36 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:39:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>哈哈</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:39:57 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:39:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>你认为大脑是单一的吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:42:35 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:42:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>对于大脑中参数的合理定义，大脑是奇异的，其原因与神经网络是奇异的相同，它们几乎肯定不是一对一的，并且它们的渔民信息矩阵是奇异的。</p><p>这里的技术细节可能更混乱，尽管这具体取决于学习子系统的非离散程度</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:43:27 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:43:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>不知何故，对我来说，大脑不太直观，但似乎这只是因为我的大脑神经元系统 1 模型比人工神经网络 (ANN) 更本质主义，更少功能主义，并且会在反思中消失。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:43:30 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:43:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>本质主义者？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:43:53 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:43:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>就像“这是代表加勒特·贝克的神经元”，它固有于神经元而不是其连接</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:44:14 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:44:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>需要明确的是，这是关于我的直觉模型的观察事实，而不是关于大脑如何工作的主张</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:44:51 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:44:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>喔好吧。这确实让人感觉很棘手，所以似乎值得分解一下为什么你会这么想，看看我是否想开始思考你的想法。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:45:13 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:45:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>或者我们可以分解为什么<i>我</i>这么想。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:45:47 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:45:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>哦，如果我对为什么我发现多对一不太直观的猜测是正确的，那么我强烈的猜测是我的直觉是错误的！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:46:01 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:46:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我投票我们做你在几条消息之前尝试做的对话的定向</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 21:46:16 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 21:46:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>凉爽的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 21:46:56 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 21:46:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>等一下！事实上也许我们现在应该分手？ </p><p></p></div></section><figure class="image image_resized" style="width:86.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/fsfa1migkpy4wg46c7gh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/k4jr9n7lpfra7kgvhkdq 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/l107nhny8ntuiy9fab1a 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/x3a8sssida2h0rfk7i26 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/yn5nugq7ropg84t5vnvf 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/j2hkuamp9rs3fharfxpz 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/jitlbvkmodx1ljq2cn1q 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/cgg9azu7jh25fsjaevfj 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/d1ld4q3a2mjs3e0ntmcx 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/jeamqxkf4uzfuxeumcle 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PikpeRucdsXeEvpy9/gwyomilifb9vkpuki7bb 2050w"></figure><h2>加勒特对基于宏奇异学习理论的希望</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:03:20 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:03:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>您想要但我们尚未实现的事情：</p><ul><li>脑科学和对齐如何相互作用</li><li>动态大脑的东西是否合理</li></ul><p>我想要达到的目标：</p><ul><li>检查我的帖子是否像我想要的那样雄心勃勃，如果没有，则纠正记录。 </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:04:17 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:04:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我想我对你的东西>;大脑+对齐>;动态大脑感到兴奋</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:04:37 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:04:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>好的，我们到时候再谈这个。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:04:55 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:04:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>让我打开你的帖子</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:05:26 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:05:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>所以，瓦妮莎有一句话我非常喜欢，这是杰里米·吉伦最初向我总结的：</p><blockquote><p>在在线学习和强化学习中，该理论通常旨在推导“遗憾”的上限和下限：算法收到的预期效用与先验已知环境时<i>它将</i>收到的预期效用之间的差异。这样的上限实际上是给定算法的<i>性能保证</i>。特别是，如果假设奖励函数是“对齐的”，那么这种性能保证在某种程度上就是对齐保证。这一观察并非空穴来风，因为学习协议可能无法直接提供给算法真正的奖励函数，如<a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a>和<a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a>所示。因此，正式证明一致性保证采取证明适当遗憾界限的形式。 </p></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:07:12 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:07:12 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>如果我理解正确的话，她试图证明这种与对齐相关的遗憾界限的方式是发明贝叶斯下物理主义，然后构建一个模型来说明作为具有效用函数的代理意味着什么，并部署下贝叶斯物理主义主体推断是什么主体创建了它，并根据其隐含的效用函数采取行动。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:13:37 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:13:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>由于多种原因，这在我的模型上似乎是不明智的。但我认为，如果你的理论满足了一些需求，那么很多原因都可以解决：</p><ol><li>在整个理论发展过程中，您与现实有很好的联系</li><li>您使用的成功指标大多是派生的，而不是从外部实例化的。实际上，这意味着您不必决定人类将具有效用函数并像以这样那样的方式定义的代理一样行事，您可以使用您的理论（或一个理论）来推导出人类内部值的格式表示为，那么在您的价值学习组件中，您将拥有的模型与人类相匹配，或者您有一些预计会产生类似格式的值的过程，从而为世界带来相同类型的目标</li></ol><p>也许还有更多？我在这里主要是根据记忆。</p><p>无论如何，这似乎是奇异学习理论，如果它可以产生对齐相关定理，那么它将会或可能具有上述两个好处。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:18:54 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:18:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>通过对齐相关定理，我想这看起来很像在 SLT 框架中很好地理解强化学习，这样你就可以声明强化学习 (RL) 代理在不同环境中优化的效果（或训练环境的概括），以及优化的主要内部决定因素。一旦我们获得了大脑如何发育的良好模型（如果你是一个足够好的数学家，我们可能已经有足够的数据可以在技术上做到这一点）做同样的事情？数学的好处是你不需要呈现<i>所有的</i>细节在你真正开始做这件事之前）。</p><p>然后，一旦我们得到了这些东西，我们就可以继续询问某些模型中什么时候可能会出现反射型想法，大脑的优化机制如何保持其值稳定（或者如果主要决定因素是在此过程中以及本体转变期间对值或元值进行优化，并希望在某些训练过程中证明某种内部值收敛。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:19:07 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:19:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>我的帖子主要暗示了这一点吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:20:15 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:20:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我想在读完你的文章后，这对我来说是最令人惊讶的！我认为你用“强化学习理论”来表示它，但我认为根据我所读到的内容，我并没有将其扩展到“（价值）学习理论”之类的东西</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:20:26 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:20:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>也许如果我最近读过学习理论议程的东西</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:21:26 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:21:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>好啊好啊！这意味着我可能正在谈论正确的事情/我明白与我交谈的朋友所说的没有说清楚的事情实际上就是我刚才所说的事情。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:20:37 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:20:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我要问你一些关于它的问题吗？我有一些！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:22:38 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:22:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>哦，是的，完全</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:22:39 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:22:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>我理解您希望对人类/学习系统中的价值观表示有所不可知的部分。但我认为我未能将学习系统与人类相适应联系起来</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:32:50 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:32:50 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>加勒特·贝克</b></section><div><p>是的，作为一个玩具例子，我们可以想象人类的价值观很大程度上取决于大脑的结构，也许人类比变形金刚有更多的重复性，因此有一个基线倾向更关心别人的感受他们自己和其他人，而如果你将语言模型训练得足够多，使其成为一般能力的代理，那么语言模型就不会出现太多的重复，因此当它们作为输入传递给像他们自己这样的变压器时，最终会在看起来不错的东西上获得更多的价值。</p><p>当我们审视这两种思想的内部时，我们看到人类价值观以（在某种意义上）与变形金刚价值观不等效的格式表示，不是说它们彼此不同，而是说无论什么人类使用的目标更容易模拟事物的感觉，而变形金刚更容易模拟事物的外观。</p><p>因此，如果我们使用某种学习过程来训练这个世界中的变压器，它将需要更多的数据来了解它需要具有与人类相同格式的目标，并且它会犯错误，从而导致关联（例如）当人类文本与您给予它的奖励一起输入时，人类文本的积极情感，而不是将推断的人类心理状态与您给予它的奖励相关联以构建其目标。</p><p>我们可以想象同样的问题，但是是在数据层面上。假设人类花费大量时间与其他人类交互，而人工智能花费大量时间与 HTML 网页交互，因此人类内部有一些内部机制，更可能以人类的方式思考，而人工智能则更可能以人类的方式思考的网页。</p><p>您希望人类价值观能够在您的人工智能中非常自然地体现。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:33:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:33:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>卡韦</b></section><div><p>听起来你正在考虑一个变压器直接选择将人类目标建模为一种间接规范性事物，即想要做其创建者会批准的事情。我在你从瓦妮莎的引言（对我来说似乎没有明确包含对间接规范性的诉求）到你对她的议程的理解的转变中也注意到了这一点。</p><p> Is your thought that this is necessary? I can imagine thinking that a powerful mind needs an accurate model of the principal&#39;s values if it&#39;s to be aligned to them, but that seems like it could be derived without trying to model them as &quot;the principal&#39;s values&quot; (as opposed to &quot;good stuff&quot;).<br><br> I don&#39;t know if that&#39;s a load-bearing part of your model or just something chosen to be concrete. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:34:46 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:34:46 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I don&#39;t know whether the best way to get a model to adopt my values is for it to model me and value my values, or to itself just have my values. I would suspect the latter is easier, because I don&#39;t model someone else and adopt their values except as an expression of others of my values. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:34:50 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:34:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So it sounds like you&#39;re concerned about the &quot;value ontology&quot; of the AI and the human, which might or might not be the same as the &quot;model ontology&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:34:59 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:34:59 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>是的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:36:09 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:36:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> And I suppose there&#39;s an additional concern that the way the value ontology of the AI interacts with increasing capability is similar enough to the way that the value ontology of the human interacts with increasing capability that it stays easy for the AI to model where the human would have ended up as it reflected (rather than going off down some other contingent path). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:39:26 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:39:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Yeah, I am concerned about this. This is inside of the reflection and the ontology sections of the original post, where I&#39;m worried about</p><ol><li> Ensuring you also have the right meta values in your AI, if those exist</li><li> Making sure the optimization machinery of your AI doesn&#39;t freak out the way humans usually do when reflection and &quot;what should my values be&quot; oriented thoughts come online</li><li> Do ontology shifts break your values here?</li></ol><p> I&#39;m not <i>so</i> concerned about 3, because humans don&#39;t seem to break under ontology shifts, but it is something to keep your eye on. Its possible they would break if they don&#39;t undergo enough ontology shifts before getting access to self-modification abilities. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:41:18 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:41:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> And of course you want the AI to get your values before its able to model you well enough to exfiltrate or something. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:41:55 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:41:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Yeah, interesting. I am interested in the reflection thread, but also kind of interested on double-clicking on the transformer value format thing and maybe that makes sense to do first? I&#39;m going to try and spell out the story in a bit more detail </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:42:06 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:42:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> What transformer value format thing? I&#39;ll note I don&#39;t actually expect the above toy example to be true </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:42:21 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:42:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Noted, but I just meant like the underlying intuition or something </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:42:24 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:42:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Gotcha, that sounds like a good thing to do </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:48:11 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:48:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So, say I&#39;m a transformer. And I&#39;ve become shaped in a way that&#39;s a bit more like &quot;have goals and make plans to fulfil them&quot;. That shape has maybe been formed out of the kind of cognitive algorithms I was already running, and so have some similarity to those algorithms. For example, if I am a guy that searches for plans that score highly, that &quot;score&quot; might be built out of pieces like &quot;one of my heuristics for what to do next is very active and confident&quot;.<br><br> So maybe if I have some values, that format is more things like [hmm, maybe I&#39;m supposed to be reinforcement learning from human feedbacked (RLHFd) so there&#39;s a non-predictive goal? Sure let&#39;s assume that]: this clearly looks like a transcript from a world that the reward model liked. It&#39;s clear what would come next in such a transcript. This is coherent, novel to the extent that&#39;s rewarded, but not <i>weird.</i> This looks like the completion will stably continue in this fashion.</p><p> Is that an implausible in the details, but roughly right in the gestalt, version of the thing you were imagining? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:49:54 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:49:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>或许？ It seems roughly similar, but not clearly what I was saying. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:51:35 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:51:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Like, another example of what I was trying to say would be that maybe models end up doing things following the predictions of shard theory, and humans do things following the predictions of naive expected utility maximization very easily (of course, both would approximate EU maximization, but the difference is humans do it explicitly, and models do it implicitly). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:52:22 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:52:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> OK, that seems like a nice tricky case. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:56:00 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:56:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Yeah, so if models natively do shard theory, and humans natively do EU maximization, then the models will not be selecting over &quot;simple&quot; utility functions to maximize, if we start knowing how to do ambitious value learning, they will first know how to best approximate your utility function using a bunch of contextually activated heuristics. And then later on, if you undergo a ontology shift or it does, the values will react very differently, because they attach to the world in very different ways. Maybe you find out souls don&#39;t exist, so your only hope of ever achieving your values is <a href="https://arbital.greaterwrong.com/p/ontology_identification?l=5c#Anticipated_failure_of_AIXI_atomic_in_our_own_universe__trying_to_maximize_diamond_outside_the_simulation">breaking out of a theorized simulation</a> inside a world that does have ontologically basic souls, and it finds out souls don&#39;t exist, and instead of helping you on your quest, it decides your diverting resources away from its implicit goal of making humans in front of it smile or something. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:57:00 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:57:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Which is why I want equivalence on the level of internal value representations that connect to RL generalization behavior, rather than just RL generalization behavior. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:57:36 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:57:36 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I mean it seems like what you want is a bunch of robustness around the RL generalization behaviour and it seems like <i>at least value representation equivalence</i> should get you a bunch of that, but there might be other ways </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Thu, 26 Oct 2023 23:57:52 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Thu, 26 Oct 2023 23:57:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>这是真实的。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:58:02 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:58:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I am excited to hear more about how you think about &quot;meta-values&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:58:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:58:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> It seems like they play a role in your conceptualisation of what it is to be well-behaved while doing reflection </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Thu, 26 Oct 2023 23:58:35 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Thu, 26 Oct 2023 23:58:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I haven&#39;t thought about them much </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:00:13 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:00:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Yeah, so as you&#39;re thinking about your values, you need to have some reason to change your values if you try to change your values, or change your goals if you try to change your goals. And when you&#39;re having some reason to change your values, you presumably use other values. And if you have two systems with different meta values, but the same object level values, assuming that difference makes some ontologically basic sense, you&#39;re going to have different reflection processes, and at the end of thinking a lot about what they want, they&#39;re going to have different values, rather than the same values. Which is a problem. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:01:03 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:01:03 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I guess I feel fuzzier on if meta values are at play for me in value change, versus just like I have a certain policy around it </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:01:33 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:01:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I certainly have (weak, incomplete) meta values to be clear! But I don&#39;t know if they govern my value change that much </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:01:48 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:01:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I guess they (hopefully) would if my values were going to stray too far </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:02:14 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:02:14 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> By policy do you mean &quot;I have consciously decided on this policy for myself&quot; or &quot;there&#39;s some process in my brain that I can&#39;t control which changes my values&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:02:17 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:02:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> The latter (well, don&#39;t know about &quot;can&#39;t&quot;, but don&#39;t) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:04:39 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:04:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> To the extent you endorse that policy changing your values, I&#39;d call that a meta-value. Presumably there are situations where you&#39;d rather that policy not change your values, like if you fell in love with a Nazi, and that turned you into a Nazi. If you knew that would happen, you&#39;d likely avoid falling in love with Nazis. But if you otherwise didn&#39;t have a problem with falling in love with Nazis, and you could modify your policy such that falling in love with them didn&#39;t give you their values, you would do that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:05:11 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:05:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Well I don&#39;t know! I might prefer to not fall in love with Nazis and keep love-influenced value change. Not that it affects your point </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:05:40 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:05:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> OK, I&#39;m willing to accept that for some notion of meta values, they&#39;re at play for me </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:07:10 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:07:10 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>凉爽的。 The existence of most value change in humans not being necessarily by choice I think makes meta values pretty important. If it was all by choice, then we wouldn&#39;t have to worry about the AI corrupting our values by (for instance) having us fall in love with it </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:11:46 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:11:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>正确的！ Let me quickly return to where meta-values appeared in your initial post-break writeup and think for a sec.</p><p> So, how much do we have to ask of these meta values for them to be a safeguard for us? One possible argument is that: as long as the AI has the same meta values we do, it&#39;s fine however its values change as that was permissible by our meta values, even if our meta values are quite incomplete.</p><p> It&#39;s not obvious we can get away with the AI having <i>additional</i> meta values, because if it&#39;s making tradeoffs between it&#39;s values that could cause it to violate one of <i>our</i> meta values way too much.</p><p> And obviously missing some of our meta values could go quite badly by our lights. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:15:38 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:15:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Yeah, two notes:</p><ol><li> I mostly expect meta-values and object level values are ontologically the same. Like, it doesn&#39;t feel like I use all that different stuff to decide what my values should be than to decide on other stuff I like or want in the world.</li><li> This is a reason I&#39;m more optimistic about actually having the same representations and values in our AIs as is in ethical humans compared to just proving very general robustness bounds. Because if you show that your AI acts within epsilon of your implied utility function or something, that epsilon will compound as it makes modifications to itself (including to its values), and as much as I like singular learning theory, I don&#39;t trust it (or any theory really) to predict the negative effects of AIs self-modifying. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:17:49 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:17:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> And do you have any specific leads for hope on <i>how</i> to enforce similar value-formats? I guess one part is neuroscience teaching us about what our value format is </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:23:32 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:23:32 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> There are currently various attempts in SLT to relate metrics like the RLCT to the implemented algorithms inside of networks. For instance, there are (currently informal) arguments about how modularity would decrease the RLCT, or how symmetries caused by the loss functions we use in the loss landscape result in functions with lots of dot products and linearities used as their primary mode of data storage having lower RLCT than others. I expect the existence of theorems to be proven here.</p><p> There are also other ways of going about this. For example, in the recent <a href="https://arxiv.org/abs/2310.06301">Toy Models of Superposition work by Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, and Daniel Murfet</a> , they show that the <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons discovered by Anthropic are critical points of (I believe) the free energy for varying <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> . Though I&#39;m not fully confident that is indeed what they did. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:24:33 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:24:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Oh sorry, not of the free energy of the loss </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:25:22 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:25:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> And then for each of those, you can find the RLCT of the relevant critical point, and for a given <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="n"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span> find the free-energy minimizing configuration </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:27:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:27:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So to think about how to make this something that would let us control value formats. First, we would develop this theory of relating loss landscapes and the RLCT to the algorithms that are found. Then we would solve the inverse problem of designing loss landscapes that both enable competitive performance and also induce the right kind of value formats.<br><br> Is that what you&#39;re picturing? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:28:41 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:28:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I notice I feel like there&#39;s something about the architecture that was supposed to appear alongside the phrase &quot;loss landscapes&quot; in my message before, but maybe the architecture appears in the RLCT, because it&#39;s about how singular the minimum loss sets are </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:31:31 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:31:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Yeah, I think it goes architecture ->; parameter function map ->; geometry ->; RLCT. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:32:20 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:32:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> But again, I&#39;m still learning the <i>exact</i> math in this area. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:32:33 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:32:33 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> 👍 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:34:04 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:34:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Also, for the RL case its certainly not <i>known</i> we&#39;ll be using the RLCT in the same way in the same functions. It may have a greater or lesser effect, or just a qualitatively different affect than in supervised learning. Possibly it ends up being roughly the same, but its still good to note that this is definitely speculative, and actually the RL extension needs to be done. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:35:02 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:35:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Yeah, good note </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:36:02 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:36:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> But your picture is roughly something like an inverse search on training setups given a theory that&#39;s powerful enough to predict details of learned algorithms and someway of knowing the human algorithm well enough to do some matching between them (and perhaps some theory about that matching)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:36:15 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:36:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>是的</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:37:05 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:37:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> That seems ambitious! Won&#39;t it be very hard to understand the algorithms our models learn even if we can predict lots about them ahead of time? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:37:45 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:37:45 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Possibly? I will note that that is the aspect of the plan which I&#39;m least worried about, mainly because its basically the mission statement of developmental interpretability. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:38:18 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:38:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Where &quot;worried about&quot; means something like &quot;trying to think through how it goes well&quot; rather than &quot;think might fail&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:40:18 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:40:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Where &quot;worried about&quot; means looking at the relative difficulty of each of them, the question of predicting details of learned algorithms seems the one I most expect to be solved. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:41:39 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:41:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Like, the last one requires advances in whole brain emulation, and the first one is basically an inverse of the second one, and the second one seems more natural, so I expect its inverse to be less natural and therefore harder. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 00:42:45 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 00:42:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Where one = inverse search, two = predict details of learned algorithms, three = match with human algos? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 00:42:48 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 00:42:48 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> yup </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:00:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:00:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So, I&#39;m trying to think about your plan as a whole right now. And I can see how some of the parts fit together. But I think one thing that&#39;s missing is exactly where your hope comes from in different parts of it.</p><p> By way of contrast, I think I understand why you&#39;re excited about SLT. It&#39;s something like &quot;this is a theory that is good in the standard way I might want to evaluate mathy theories of learning, such as it being aesthetically nice, having tie-ins to other strong areas of math, having people with a record of good and insightful work, like Watanabe. And also, it is predicting things <i>already</i> and those predictions are getting checked <i>already</i> ! Like, look at the modular addition post! That really seems like that worked out nicely&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:01:30 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:01:30 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> And for some parts I think the answer is like &quot;I think this is roughly how it <i>has</i> to go, if it&#39;s going to to work&quot; (like, I think this is what you think about value learning, though not confident) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:02:45 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:02:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> And for other parts, like neurotech, I&#39;m not sure if they were picked because you were like &quot;here are some promising parts, this is the best or most hopeful way I can think to combine them&quot; or more like &quot;and this thing seems pretty promising too, and if that goes well, I now have enough pieces to summon Exodia&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:04:02 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:04:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> For SLT, that sounds right, and also it&#39;s making about 20-40x faster progress than I would have anticipated.</p><p> What I described is basically what I think about value learning, and has been for about the past year. Far before SLT or thinking too deeply about whole brain emulation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:05:09 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:05:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Would be nice to be able to link to specific comments here, but where I started saying this:</p><blockquote><p> Yeah, so as a toy example, we can imagine the values humans have are pretty dependent on the architecture of the brain, maybe humans have a lot more recurrence than transformers, and so have a baseline tendency to care a lot more about the feelings of themselves and others, while language models if you train them enough to be generally capable agents don&#39;t have so much recurrence and so end up with a lot more value placed on things that look nice when they are passed as inputs to a transformer like themself. </p></blockquote></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:05:24 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:05:24 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>复制</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:07:14 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:07:14 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> So most of my hope comes from &quot;man, lots of this basically looks like technical problems which you can easily check if you got right or not&quot;, and then the rest (like reflection or ontologies or meta-values, or making sure you got the right equivalence in your value formats) seem <i>a lot</i> easier than the generic Arbital versions after you&#39;ve solved those technical problems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:08:03 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:08:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> (where I use &quot;technical&quot; to mean you can pretty easily check/prove if you got things right) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:08:35 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:08:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> And I place nontrivial probability on many of the nontechnical things just not being major problems (ie meta values being basically the same as object level values, and those dominating during ontology shifts) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:08:44 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:08:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> One nice thing about such technical problems is that they&#39;re often either scalable or you don&#39;t have to get them right on the first try </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:09:25 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:09:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>的确</p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:10:37 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:10:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So, why are the Arbital problems easier once you have value equivalence? Is it because you can do some kind of more &#39;syntactic&#39; check and not find some complex way of telling when two value systems are the same behaviourally or elsewise? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:11:01 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:11:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Or I guess, solving in the natural ontology of the thing you create </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:15:26 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:15:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Solving in the natural ontology of the thing you create sounds right</p><p> So for example, during ontology shifts, we want our AI to react similarly to a human when it learns that souls don&#39;t exist. Either this is a natural result of the values we have, like we have some meta-values which tell us what to do with the soul-caring values in such circumstances, or we have some optimizing machinery inside us that reinterprets souls depending on the currently best ontology, or something else.</p><p> And then we can— </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:15:44 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:15:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> —Ah! I think I have a new way of seeing your plan now, which is something like: do human(-value) uploading, but do as much as you can to bridge <i>down</i> from ML systems as we do to bridge <i>up</i> from the brain </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:16:02 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:16:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>是的！ That&#39;s how I think of it in my head </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:19:01 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:19:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>这就说得通了！ It&#39;s interesting that it&#39;s quite different from plans like <a href="https://manifund.org/projects/activation-vector-steering-with-bci">this one</a> that try to get the brain and powerful models more closely connected (I seem to recall another that thought about using transformers to predict fMRI data).</p><p> The things I link here feel like they are &quot;shovel ready&quot; but lack guarantees about the thing you&#39;re actually getting that performs well at the prediction tasks or whatever. They&#39;re more like &quot;let&#39;s try these tricks and that might work&quot;, and your plan is more like &quot;here&#39;s some fields we could solve to differing degrees and still have a theory of how to make powerful machine, but human-like, minds&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:23:25 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:23:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I don&#39;t find it that interesting, because currently thinking SLT is where its at is a very unpopular opinion, and without the results I saw, and predictions I made, its reasonable for people to be pessimistic about progress in deep learning (DL) theory. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:24:02 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:24:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>当然！ I guess it&#39;s just quite a different vector than I normally think of when bridging DL and brains </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:25:28 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:25:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Reasonable. Yeah, so to finish my sentence from earlier, we can copy the relevant parts of the human brain which does the things our analysis of our models said they would do wrong, either empirically (informed by theory of course), or purely theoretically if we just need a little bit of inspiration for what the relevant formats need to look like. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:27:45 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:27:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>有趣的！ Yeah, I think your comments here are related to earlier when you said &quot;[...] do the same once we get good models of how brains develop (which we may already have enough data to technically do if you&#39;re a good enough mathematician? The nice thing about math is you don&#39;t need <i>all</i> the detail to be present before you can actually work on the thing&quot;. Like, maybe we need lots more neuroscience, or maybe we have enough to pin down relevant brain algorithms with enough theory. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:28:22 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:28:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> That seems quite tricky though. It seems like the boost to brain-theory would likely have to come from SLT in this plan, as I don&#39;t see a similarly promising framework in neuro </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:28:50 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:28:50 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I agree, which is why I tried to emphasize in the post and earlier in the discussion why I think SLT is relevant for describing the brain as well as ML models </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:29:45 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:29:45 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> We have also been pretty lost when it comes to theories about how the brain works, and are even further behind neural network interpretability in figuring out brain algorithms. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:32:20 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:32:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> When I try and think about the brain under SLT, I&#39;m like: sure, sure, multiple realisability / non-identifiability ... but what is SLT <i>about</i> then if it extends to brains? Is it about dynamical systems where the dynamics are in the projection of a larger space? But all spaces are projections of <i>some</i> space where the projection gives singularities, surely?<br><br> Oh I guess the dynamics are <i>in</i> the big/redundant space, but <i>governed by</i> the position in the small space? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:39:01 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:39:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I don&#39;t fully understand your first paragraph, but the second paragraph is deeply related to lots of stuff I was explained in the SLT primer, so maybe your onto something?</p><p> The way I think about it is relatively high level, with some sanity lower-level checks. Like, SLT explains why neural networks learn so well, currently it only explains this in the supervised regime, but neural networks also learn similarly well in the reinforcement learning regime, it would be pretty weird if neural networks performed well in both regimes for very different reasons, and indeed during reinforcement learning neural networks are pretty sensitive to the geometry of the loss landscape, and it would make sense for them to stay in an area around different singularities with lower RLCT earlier in training, and to progress to areas with higher RLCT but better loss later in training.</p><p> And then similarly with brains, lots of the architectures which work best in supervised and reinforcement learning are directly inspired by looking at brain architectures. It would similarly be weird if they happened to work for very different reasons (though less weird than the supervised->;RL case), and as the brain gets reward events, and small changes are made to the parameter-equivalents in the brain, I similarly expect for it to progress in the same RLCT increasing, regret decreasing manner. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:40:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:40:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> So I think your argument is that &quot;a theory that explains the success of deep neural networks (DNNs) at supervised learning, by default explains deep reinforcement learning and the human brain&quot;? Where that assumption could be refuted by noticing the theory really paying attention to supervised-learning-specific stuff. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:41:46 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:41:46 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Weakly, yes. And noting also the fact that this theory in particular seems relatively more general than the default.</p><p> Like, a competitor may be the neural tangent kernel. I would not expect the brain to be explainable via that enough to rest my hopes on it. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:44:13 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:44:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Because the neural tangent kernel is structured like &quot;here is an idealisation of a neural network. From it, theorems&quot;, and SLT is structured like &quot;here is an idealisation of a learning system. From it, theorems&quot;. And the idealisation in the latter just looks like way less specific and similar to DNN-qua-learning-system than the former is to DNN-qua-neural-net? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:44:52 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:44:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Essentially yes. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:47:02 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:47:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>嗯。 Care to put a number on <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(\text{SLT usefully explains brain stuff }|\text{ SLT usefully explains neural net stuff})"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.519em;">SLT usefully explains brain stuff</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.519em;">SLT usefully explains neural net stuff</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:47:22 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:47:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> that probably wasn&#39;t worth <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\LaTeX"><span class="mjx-mrow" style="width: 2.781em;" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">L</span></span><span class="mjx-mspace" style="margin-right: -0.325em; width: 0px; height: 0px;"></span> <span class="mjx-mpadded"><span class="mjx-block" style="width: 0px; margin-top: -0.266em; padding: 0px 0.53em 0px 0px;"><span class="mjx-box" style="width: 0px; margin: -0.524em 0px -0.018em; position: relative; top: -0.21em;"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mstyle" style="font-size: 80%;"><span class="mjx-mrow" style="font-size: 88.4%;"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span><span class="mjx-strut"></span></span></span><span class="mjx-mspace" style="margin-right: -0.17em; width: 0px; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span><span class="mjx-mspace" style="margin-right: -0.14em; width: 0px; height: 0px;"></span> <span class="mjx-mpadded"><span class="mjx-block" style="width: 0px; margin-top: -0.516em; padding: 0px 0.764em 0.246em 0px;"><span class="mjx-box" style="width: 0px; margin: -0.705em 0px -0.025em; position: relative; top: 0.221em;"><span class="mjx-mrow"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span></span></span></span><span class="mjx-strut"></span></span></span><span class="mjx-mspace" style="margin-right: -0.115em; width: 0px; height: 0px;"></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;">X</span></span></span></span></span></span></span> ing </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:50:28 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:50:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> lol, I think this is mostly dominated by whether we have good enough theory ->; brain emulation ->; theory feedback loops, but I&#39;d put it at 80-95%. Given better feedback loops, that increases.</p><p> Though I will also suggest you ask me in 6 months, since outside view says people excited about a pet project/theory like to use it to explain lots of stuff. I&#39;m currently adjusting for that, but still. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:51:28 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:51:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> With your caveat noted, I still find the two parts of you first sentence incongruent! It seems that <code>theory ->; brain emulation ->; theory</code> routes through experimental neuroscience, a field about which I, a novitiate, feel despair. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:53:52 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:53:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I mean, I was interpreting a relatively broad &quot;usefully explains&quot; definition? Like, surely you don&#39;t mean given the first half of your plan works, the part about brains will work.</p><p> So if I&#39;m just like &quot;given SLT gives good &amp; novel predictions about practically relevant neural net stuff, will it give practically relevant &amp; novel predictions about brain stuff&quot;, and given how good neural nets are at predicting brain stuff, this seems easy to satisfy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:55:22 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:55:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I mean, maybe this is all just in the weeds but I mean ... oh! Did you mean &quot;this is mostly dominated by&quot; to mean &quot;this conditional is not as important as this other important concern&quot; rather than &quot;the leading term in my estimate is how well this loop works&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:56:19 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:56:19 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Well, the leading term in my estimate is how well neural networks generically do at predicting neuroscience stuff, and then the second term is how well that loop looks for brain-specific insights that don&#39;t route through NNs. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:57:14 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:57:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Doesn&#39;t that loop seem doomy? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:57:55 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:57:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> (aka &#39;unlikely&#39;) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 01:57:58 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 01:57:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p>是的。 But its a technical rather than philosophical problem </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 01:58:51 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 01:58:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> But one that&#39;s 80-95% likely to work? Or maybe I should understand you as saying &quot;look 80-95% that it helps at all, but how much it helps is dominated by the loop&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:00:23 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:00:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Oh, ok, my 80-95% is mostly on that the insights it gives to NNs lead to insights to brains or that the theoretical development of the theory making contact with NNs is enough to get it to the point where it can make nontrivial predictions about the brain.</p><p> Certainly how much it helps is dominated by the loop. But also making the loop better makes it have a higher chance of helping. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:01:14 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:01:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> OK, I think I get your picture now </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:01:39 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:01:39 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Ok good </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:03:01 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:03:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Sorry for that confusion, I think I just wasn&#39;t meta-modelling the fact I was using &quot;how well are neural networks as basic models of the brain&quot; as the first-order term when I wrote the probability originally </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:03:07 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:03:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p>没问题！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:07:59 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:07:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Here are a couple of places my attention go from here:<br><br> 1. What is necessary to seed an SLT-touched neuroscience field? Maybe the answer is just &quot;push forward on SLT and get to neuroscience when it happens&quot;, but interested if there&#39;s something to do earlier.</p><p> 2. What are your hopes for pushing the plan forward?</p><p> 3. The ol&#39; capabilities externalities. Maybe touching on Nate&#39;s recent post that was more like &quot;shut it all down&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:13:02 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:13:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><ol><li> I imagine the thing that&#39;d need to happen is to know what things update the brain, what parts of the brain are analogous to parameters, are the functions in the brain approximatable by analytic functions,</li><li> So the obvious next move for me is to learn SLT more in depth than I currently know. Then I plan on extending it to reinforcement learning, which seems to get relatively less attention in the field. Then at that point start thinking about the brain. Basically start setting up the basics of the extensions I&#39;ll be using.</li><li> I agree with &quot;shut it all down&quot;, I did address capabilities externalities in my original post. The idea is that I want to develop the theory in the direction of making nontrivial statements about values. There exist capabilities externalities, but I&#39;m not <i>so</i> worried because as long as I mostly talk and output stuff about the values of the systems I&#39;m trying to describe, and the theory required to say stuff about those values, I should expect to have my insights mostly pointed toward characterizing those values rather than characterizing capabilities. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:14:06 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:14:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> I&#39;d be happy to chat about what 2 looks like or expand on 3. I agree you talk about it in your post, but feels like there&#39;s a reasonable amount to say. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:15:44 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:15:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Lets do 3, since that seems more interesting to outside parties than describing my Anki methodology, or me saying &quot;yeah, I don&#39;t know enough of how SLT is built&quot; to talk specifically about how to extend it to RL, and I don&#39;t know enough about either the brain or SLT to talk much about how to extend it to the brain.</p></div></section><h2> Capabilities externalities </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:18:57 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:18:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Sounds good!</p><p> So, I take your hope to be something like &quot;if my work focuses specifically on how values work in singular learning systems, then that will be mostly usable for ensuring inner alignment with some goal. The use of that power at a given level of capabilities is one I think is basically for the good, and I don&#39;t think it hastens capabilities&quot;.</p><p> Curious in if that fits with how you think of it? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:20:42 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:20:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> It doesn&#39;t reduce capabilities, and its doing stuff in that space, so it in expectation likely does hasten capabilities. So the last part is false, but I think it increases capabilities not all that much, especially if I&#39;m just in theory building mode while I&#39;m doing general stuff, and then when I&#39;m focusing on specific stuff I&#39;m in make this useful for alignment mode. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:22:12 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:22:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Yeah, so the distinction between general stuff and specific stuff is maybe promising. I&#39;m like, focusing doing singular learning theory on values seems hard to do any time soon. First it seems like you need to be able to say lots about the SLT of RL. Do you agree with that, or do you think you could go straight to SLT of values? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:25:28 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:25:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> Like, for context, singular learning theory has been around for like 20 years, and maybe could be used by a super smart person to quicken capabilities, but mostly the super smart people focus on scaling (or just have lost faith in understanding as a means to progress), and the smart people afraid of capabilities who know about SLT work on making SLT useful for interpretability, which likely has its own capabilities externalities, but I generally feel like for research like this only a few &quot;true believers&quot; will develop it while its still in its general phase, and you don&#39;t have to worry about RL practitioners suddenly caring at all about theory. So its effects will mostly lie in what those &quot;true believers&quot; decide to use the theory for. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:26:18 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:26:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I agree that we need a theory of SLT for RL before making a SLT for values induced by RL. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:30:04 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:30:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> Yeah, interesting. It seems definitely true that for awhile you&#39;ll be fine, because no one will care. I wonder if the odds are good no one will care until your specific stuff comes online. I guess it would be nice to know some history of science here. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:32:35 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:32:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I will point to the Wright brothers and the airplane, the Manhattan project, and I think Thiel&#39;s <a href="https://www.amazon.com/Zero-One-Notes-Startups-Future/dp/0804139296">theory of startups</a> , as sources of evidence for this position. Also looking at the landscape of stuff going on in alignment, and seeing that mostly people don&#39;t build on each others work too much, and even the most promising stuff is very general and would take a really smart person to develop into capabilities relevant insights. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:34:20 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:34:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I note that interpretability is a different beast than super theoretical stuff like what I&#39;m pursuing, since it seems like it&#39;d be obvious to a much wider range of people, who intersect <i>relatively</i> heavier on the people most liable to want capabilities improvements than singular learning theory. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:35:19 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:35:19 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> That seems true! Though it also seems true that people are really wanting to hit their prediction machines with RL hammers to make money fly out </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:36:16 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:36:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> That is a thing I forgot to think about, that in the realm of deep learning RL practice is particularly cursed </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:36:38 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:36:38 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I don&#39;t think it makes much difference, but still good to weigh </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="55XxDBpfKkkBPm9H8-Fri, 27 Oct 2023 02:36:53 GMT" user-id="55XxDBpfKkkBPm9H8" display-name="kave" submitted-date="Fri, 27 Oct 2023 02:36:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>kave</b></section><div><p> But I think as long as you don&#39;t discover things that improves the sample efficiency of RL without improving inner alignment, it&#39;s not obviously bad </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="6c2KCEXTGogBZ9KoE-Fri, 27 Oct 2023 02:38:13 GMT" user-id="6c2KCEXTGogBZ9KoE" display-name="Garrett Baker" submitted-date="Fri, 27 Oct 2023 02:38:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Garrett Baker</b></section><div><p> I note that singular learning theory has only characterized the behavior of deep learning so far, it doesn&#39;t (straightforwardly) give recommendations for improvements as far as I know. And I&#39;d guess once I need abilities in that domain, they will be fairly values-controlling specific.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/PikpeRucdsXeEvpy9/singular-learning-theory-and-bridging-from-ml-to-brain#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/PikpeRucdsXeEvpy9/singular-learning-theory-and-bridging-from-ml-to-brain<guid ispermalink="false"> PikpeRucdsXeEvpy9</guid><dc:creator><![CDATA[kave]]></dc:creator><pubDate> Wed, 01 Nov 2023 21:31:54 GMT</pubDate></item><item><title><![CDATA[My thoughts on the social response to AI risk]]></title><description><![CDATA[Published on November 1, 2023 9:17 PM GMT<br/><br/><p> A common theme implicit in many AI risk stories has been that broader society will either fail to anticipate the risks of AI until it is too late, or do little to address those risks in a serious manner. In my opinion, there are now clear signs that this assumption is false, and that society will address AI with something approaching both the attention and diligence it deserves. For example, one clear sign is <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">Joe Biden&#39;s recent executive order on AI safety</a> <span class="footnote-reference" role="doc-noteref" id="fnrefxfu4tn6tgv"><sup><a href="#fnxfu4tn6tgv">[1]</a></sup></span> . In light of recent news, it is worth comprehensively re-evaluating which sub-problems of AI risk are likely to be solved without further intervention from the AI risk community (eg perhaps deceptive alignment), and which ones will require more attention.</p><p> While I&#39;m not saying we should now sit back and relax, I think recent evidence has significant implications for designing effective strategies to address AI risk. Since I think substantial AI regulation will likely occur by default, I urge effective altruists to focus more on ensuring that the regulation is thoughtful and well-targeted rather than ensuring that regulation happens at all. Ultimately, I argue in favor of a cautious and nuanced approach towards policymaking, in contrast to <a href="https://forum.effectivealtruism.org/posts/Y4SaFM5LfsZzbnymu/the-case-for-ai-safety-advocacy-to-the-public">broader public AI safety advocacy</a> . <span class="footnote-reference" role="doc-noteref" id="fnref7j0nvds89o5"><sup><a href="#fn7j0nvds89o5">[2]</a></sup></span></p><hr><p> In the past, when I&#39;ve read stories from AI risk adjacent people about what the future could look like, I have often noticed that the author assumes that humanity will essentially be asleep at the wheel with regards to the risks of unaligned AI, and won&#39;t put in place substantial safety regulations on the technology—unless of course EA and LessWrong-aligned researchers unexpectedly upset the gameboard by achieving a <a href="https://arbital.com/p/pivotal/">pivotal act</a> . We can call this premise <strong>the assumption of an inattentive humanity</strong> . <span class="footnote-reference" role="doc-noteref" id="fnref1mmukxvauz4"><sup><a href="#fn1mmukxvauz4">[3]</a></sup></span></p><p> While most often implicit, the assumption of an inattentive humanity was sometimes stated explicitly in people&#39;s stories about the future.</p><p> For example, in <a href="https://www.lesswrong.com/posts/qRtD4WqKRYEtT5pi3">a post from Marius Hobbhahn published last year</a> about a realistic portrayal of the next few decades, Hobbhahn outlines a series of AI failure modes that occur as AI gets increasingly powerful. These failure modes include a malicious actor using an AI model to create a virus that &quot;kills ~1000 people but is stopped in its tracks because the virus kills its hosts faster than it spreads&quot;, an AI model attempting to escape its data center after having &quot;tried to establish a cult to “free” the model by getting access to its model weights&quot;, and a medical AI model that &quot;hacked a large GPU cluster and then tried to contact ordinary people over the internet to participate in some unspecified experiment&quot;. Hobbhahn goes on to say,</p><blockquote><p> People are concerned about this but the news is as quickly forgotten as an oil spill in the 2010s or a crypto scam in 2022. Billions of dollars of property damage have a news lifetime of a few days before they are swamped by whatever any random politician has posted on the internet or whatever famous person has gotten a new partner. The tech changed, the people who consume the news didn&#39;t. The incentives are still the same.</p></blockquote><p> Stefan Schubert <a href="https://web.archive.org/web/20221220180601/https://stefanfschubert.com/blog/2022/12/20/the-response-to-advanced-ai-comment-on-hobbahn">subsequently commented</a> that this scenario seems implausible,</p><blockquote><p> I expect that people would freak more over such an incident than they would freak out over an oil spill or a crypto scam. For instance, an oil spill is a well-understood phenomenon, and even though people would be upset about it, it would normally not make them worry about a proliferation of further oil spills. By contrast, in this case the harm would come from a new and poorly understood technology that&#39;s getting substantially more powerful every year. Therefore I expect the reaction to the kind of harm from AI described here to be quite different from the reaction to oil spills or crypto scams.</p></blockquote><p> I believe Schubert&#39;s point has been strengthened by recent events, including <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">Biden&#39;s executive order</a> that touches on many aspects of AI risk <span class="footnote-reference" role="doc-noteref" id="fnrefxfu4tn6tgv"><sup><a href="#fnxfu4tn6tgv">[1]</a></sup></span> , <a href="https://www.reuters.com/technology/britains-ai-summit-what-can-it-achieve-2023-10-31/">the UK AI safety summit</a> , the <a href="https://archive.is/36GGk">recent open statement</a> signed by numerous top AI scientists warning about &quot;extinction&quot; from AI, the <a href="https://www.politico.com/news/2023/05/16/sam-altmans-congress-ai-chatgpt-00097225">congressional hearing about AI risk</a> and the <a href="https://www.npr.org/2023/09/14/1199429451/sen-schumer-hopes-legislation-regulating-ai-can-pass-a-divided-congress">discussion of imminent legislation</a> , the widespread media coverage on the rise of GPT-like language models, and the <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter to &quot;pause&quot; model scaling</a> . All of this has occurred despite AI still being relatively harmless, and having—so far—tiny economic impacts, especially compared to the existential threat to humanity that it poses in the long-term. Moreover, the timing of these developments strongly suggests they were mainly prompted by recent impressive developments in language models, rather than any special push from EAs.</p><p> In light of these developments, it is worth taking a closer look at how the assumption of an inattentive humanity has pervaded AI risk arguments, and re-evaluate the value of existing approaches to address AI risk in light of recent evidence.</p><p> The assumption of an inattentive humanity was perhaps most apparent in stories that posited a fast and local takeoff, in which AI goes from being powerless and hidden in the background, to suddenly achieving a <a href="https://forum.effectivealtruism.org/topics/decisive-strategic-advantage">decisive strategic advantage</a> over the rest of the world in a very short period of time.</p><p> In his essay from 2017, Eliezer Yudkowsky <a href="https://intelligence.org/2017/10/13/fire-alarm/">famously argued</a> that there is &quot;no fire alarm for artificial general intelligence&quot; by which he meant that there will not be an event &quot;producing common knowledge that action [on AI risk] is now due and socially acceptable&quot;. <span class="footnote-reference" role="doc-noteref" id="fnrefzmsaxrw6rma"><sup><a href="#fnzmsaxrw6rma">[4]</a></sup></span> He wrote,</p><blockquote><p> Multiple leading scientists in machine learning have already published articles telling us their criterion for a fire alarm. They will believe Artificial General Intelligence is imminent:</p><p> (A) When they personally see how to construct AGI using their current tools. This is what they are always saying is not currently true in order to castigate the folly of those who think AGI might be near.</p><p> (B) When their personal jobs do not give them a sense of everything being difficult. This, they are at pains to say, is a key piece of knowledge not possessed by the ignorant layfolk who think AGI might be near, who only believe that because they have never stayed up until 2AM trying to get a generative adversarial network to stabilize.</p><p> (C) When they are very impressed by how smart their AI is relative to a human being in respects that still feel magical to them; as opposed to the parts they do know how to engineer, which no longer seem magical to them; aka the AI seeming pretty smart in interaction and conversation; aka the AI actually being an AGI already.</p><p> So there isn&#39;t going to be a fire alarm.时期。</p><p> There is never going to be a time before the end when you can look around nervously, and see that it is now clearly common knowledge that you can talk about AGI being imminent, and take action and exit the building in an orderly fashion, without fear of looking stupid or frightened.</p></blockquote><p> My understanding is that this thesis was part of a more general view from Yudkowsky that AI would not have any large, visible effects on the world up until the final moments when it takes over the world. In a live debate at Jane Street with Robin Hanson in 2011 <a href="https://intelligence.org/files/AIFoomDebate.pdf#page=447">he said</a> ,</p><blockquote><p> When we try to visualize how all this is likely to go down, we tend to visualize a scenario that someone else once termed “a brain in a box in a basement.” I love that phrase, so I stole it. In other words, we tend to visualize that there&#39;s this AI programming team, a lot like the sort of wannabe AI programming teams you see nowadays, trying to create artificial general intelligence, like the artificial general intelligence projects you see nowadays. They manage to acquire some new deep insights which, combined with published insights in the general scientific community, let them go down into their basement and work in it for a while and create an AI which is smart enough to reprogram itself, and then you get an intelligence explosion.</p></blockquote><p> In that type of scenario, it makes sense that society would not rush to regulate AI, since AI would mainly be a thing done by academics and hobbyists in small labs, with no outsized impacts, up until right before intelligence explosion, which Yudkowsky <a href="https://www.lesswrong.com/posts/JBadX7rwdcRFzGuju/recursive-self-improvement">predicted</a> would take place within &quot;weeks or hours rather than years or decades&quot;. However, this scenario—at least as it was literally portrayed—now appears very unlikely.</p><p> Personally—as I have <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines?commentId=FQFp2oJ48boHhNXxP">roughly said for over a year now</a> <span class="footnote-reference" role="doc-noteref" id="fnrefdu5vu3h5oqj"><sup><a href="#fndu5vu3h5oqj">[5]</a></sup></span> —I think by far the most likely scenario is that society will adopt broad AI safety regulations as increasingly powerful systems are rolled out on a large scale, just as we have done for many previous technologies. As the capabilities of these systems increase, I expect the regulations to get stricter and become wider in scope, coinciding with popular, growing fears about losing control of the technology. Overall, I suspect governments will be sympathetic to many, but not all, of the concerns that EAs have about AI, including human disempowerment. And while sometimes failing to achieve their stated objectives, I predict governments will overwhelmingly adopt reasonable-looking regulations to stop the most salient risks, such as the risk of an <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">untimely AI coup</a> .</p><hr><p> Of course, it still remains to be seen whether US and international regulatory policy will adequately address every essential sub-problem of AI risk. It is still plausible that the world will take aggressive actions to address AI safety, but that these actions will have little effect on the probability of human extinction, simply because they will be poorly designed. One possible reason for this type of pessimism is that the alignment problem might just be so difficult to solve that no “normal” amount of regulation could be sufficient to make adequate progress on the core elements of the problem—even if regulators were guided by excellent advisors—and therefore we need to <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">clamp down hard now and pause AI worldwide indefinitely</a> . That said, I don&#39;t see any strong evidence supporting that position.</p><p> Another reason why you might still believe regulatory policy for AI risk will be inadequate is that regulators will adopt sloppy policies that totally miss the “hard bits” of the problem. When I recently asked Oliver Habryka what type of policies he still expects won&#39;t be adopted, he <a href="https://twitter.com/ohabryka/status/1719386664404127828">mentioned</a> &quot;Any kind of eval system that&#39;s robust to deceptive alignment.&quot; I believe this opinion is likely shared by many other EAs and rationalists.</p><p> In light of recent events, we should question how plausible it is that society will fail to adequately address such an integral part of the problem. Perhaps you believe that policy-makers or general society simply won&#39;t worry much about AI deception. Or maybe people will worry about AI deception, but they will quickly feel reassured by results from superficial eval tests. Personally, I&#39;m pretty skeptical of both of these possibilities, and for basically the same reasons why I was skeptical that there won&#39;t be substantial regulation in the first place:</p><ol><li> People think ahead, and frequently—though not always—rely on the advice of well-informed experts who are at least moderately intelligent.</li><li> AI capabilities will increase continuously and noticeably over years rather than appearing suddenly. This will provide us time to become acquainted with the risks from individual models, concretely demonstrate failure modes, and study them empirically.</li><li> AI safety, including the problem of having AIs not kill everyone, is a natural thing for people to care about.</li></ol><p> Now, I don&#39;t know exactly what Habryka means when he says he doesn&#39;t expect to see eval regulations that are robust to deception. Does that require that the eval tests catch all deception, no matter how minor, or is it fine if we have a suite of tests that work well at detecting the most dangerous forms of deception, most of the time? However, while I agree that we shouldn&#39;t expect regulation to be perfect, I still expect that governments will adopt sensible regulations—roughly the type you&#39;d expect if mainstream LessWrong-aligned AI safety researchers were put in charge of regulatory policy.</p><p> <strong>To make my prediction about AI deception regulation more precise, I currently assign between 60-90% probability</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefdikwaezzana"><sup><a href="#fndikwaezzana">[6]</a></sup></span> <strong>that AI safety regulations will be adopted in the United States before 2035 that include sensible requirements for uncovering deception in the most powerful models</strong> , such as rigorously testing the model in a simulation, getting the model “drunk” by modifying its weights and interrogating it under diverse circumstances, asking a separate “lie detector” model to evaluate the model&#39;s responses, applying state-of-the-art mechanistic interpretability methods to unveil latent motives, or creating many slightly different copies of the same model in the hopes that one is honest and successfully identifies and demonstrates deception from the others. I have written <a href="https://manifold.markets/MatthewBarnett/will-ai-regulations-that-include-se">a Manifold question about this prediction</a> that specifies these conditions further.</p><p> To clarify, I am not making any strong claims about any of these methods being foolproof or robust to AI deception in all circumstances. I am merely suggesting that future AI regulation will likely include sensible precautions against risks like AI deception. If deception turns out to be an obscenely difficult problem, I expect evidence for that view will accumulate over time—for instance because people will build <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1">model organisms of misalignment</a> , and show how deception is very hard to catch. As the evidence grows, I think regulators will likely adapt, adjusting policy as the difficulty of the problem becomes clearer. <span class="footnote-reference" role="doc-noteref" id="fnrefk28myw46u1g"><sup><a href="#fnk28myw46u1g">[7]</a></sup></span></p><p> I&#39;m not saying we should be complacent. Instead, I&#39;m advocating that we should <i>raise the bar</i> for what sub-problems of AI risk we consider worthy of special attention, versus what problems we think will be solved by default in the absence of further intervention from the AI risk community. Of course, it may still be true that AI deception is an extremely hard problem that reliably resists almost all attempted solutions in any “normal” regulatory regime, even as concrete evidence continues to accumulate about its difficulty—although I consider that claim unproven, to say the least.</p><p> Rather than asserting &quot;everything is fine, don&#39;t worry about AI risk&quot; my point here is that we should think more carefully about what other people&#39;s incentives actually are, and how others will approach the problem, even without further intervention from this community. Answering these questions critically informs how valuable the actions we take now will be, since it would shed light on the question of which problems will remain genuinely neglected in the future, and which ones won&#39;t be. It&#39;s still necessary for people to work on AI risk, of course. We should just try to make sure we&#39;re spending our time wisely.</p><p> <strong>Edited to add:</strong> To give a concrete example of an important problem I think might not be solved by default, several months ago I proposed treating long-term <a href="https://www.lesswrong.com/posts/MnrQMLuEg5wZ7f4bn/matthew-barnett-s-shortform?commentId=aGsRfmPfqrgkYgas8">value drift</a> from future AIs as a serious issue. I currently think that value drift is a &quot;hard bit&quot; of the problem that we do not appear to be close to seriously addressing, perhaps because people expect easier problems won&#39;t be solved either without heroic effort. I&#39;m also sympathetic to <a href="https://arxiv.org/abs/2303.16200">Dan Hendrycks&#39; argument about AI evolution</a> . If these problems turn out to be easy or intractable, I think it may be worth turning more of our focus to other important problems, such as improving our institutions or preventing s-risks.</p><hr><p> Nothing in this post should be interpreted as indicating that I&#39;m incredibly optimistic about how AI policy will go. Though politicians usually don&#39;t flat-out ignore safety risks, I believe history shows that they can easily mess up tech regulation in subtler ways.</p><p> For instance, when the internet was still new, the US Congress passed the Digital Millennium Copyright Act (DMCA) in 1998 to crack down on copyright violators, with strong bipartisan support. While the law had several provisions, one particularly contentious element was its anti-circumvention rule, which made it illegal to bypass digital rights management (DRM) or other technological protection measures. Perversely, this criminalized the act of circumvention even in scenarios where the underlying activity—like copying or sharing—didn&#39;t actually infringe on copyright. <a href="https://www.eff.org/pages/unintended-consequences-fifteen-years-under-dmca">Some have argued</a> that because of these provisions, there has been a chilling effect on worldwide cryptography research, arguably making our infrastructure less secure with only a minor impact on copyright infringement.</p><p> While it is unclear what direct lessons we should draw from incidents like this one, I think a basic takeaway is that it is easy for legislators to get things wrong when they don&#39;t fully understand a technology. Since it seems likely that there will be strong AI regulations in the future regardless of what the AI risk community does, I am far more concerned about making sure the regulations are thoughtful, well-targeted, and grounded in the best evidence available, rather than making sure they happen at all.</p><p> Instead of worrying that the general public and policy-makers won&#39;t take AI risks very seriously, I tend to be more worried that we will hastily implement poorly thought-out regulations that are based on inaccurate risk models or limited evidence about our situation. These regulations might marginally reduce some aspects of AI risk, but at great costs to the world in other respects. For these reasons, I favor nuanced messaging and pushing for cautious, expert-guided policymaking, rather than <a href="https://forum.effectivealtruism.org/posts/Y4SaFM5LfsZzbnymu/the-case-for-ai-safety-advocacy-to-the-public">blanket public advocacy</a> . </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxfu4tn6tgv"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxfu4tn6tgv">^</a></strong></sup></span><div class="footnote-content"><p> In response to <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/">Biden&#39;s executive order on AI safety</a> , Aaron Bergman <a href="https://twitter.com/AaronBergman18/status/1719031282309497238">wrote</a> ,</p><blockquote><p> Am I crazy for thinking the ex ante probability of something at least this good by the US federal government relative to AI progress, from the perspective of 5 years ago was ~1% Ie this seems 99th-percentile-in-2018 good to me</p></blockquote><p> David Manheim <a href="https://twitter.com/davidmanheim/status/1719046950991938001">replied</a> ,</p><blockquote><p> I&#39;m in the same boat. (In the set of worlds without near-term fast takeoff, and where safe AI is possible at all,) I&#39;m increasingly convinced that the world is getting into position to actually address the risks robustly - though it&#39;s still very possible we fail.</p></blockquote><p> Peter Wildeford also <a href="https://twitter.com/peterwildeford/status/1719069133508145202">replied</a> ,</p><blockquote><p> This checks out with me<br><br> AI capabilities is going faster than expected, but the policy response is much better than expected</p></blockquote><p> Stefan Schubert also <a href="https://twitter.com/StefanFSchubert/status/1719102746815508796">commented</a> ,</p><blockquote><p> Yeah, if people think the policy response is &quot;99th-percentile-in-2018&quot;, then that suggests their models have been seriously wrong.</p><p> That could have further implications, meaning these issues should be comprehensively rethought.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fn7j0nvds89o5"> <span class="footnote-back-link"><sup><strong><a href="#fnref7j0nvds89o5">^</a></strong></sup></span><div class="footnote-content"><p> To give one example of an approach I&#39;m highly skeptical of in light of these arguments, I&#39;ll point to <a href="https://www.lesswrong.com/posts/b7JXJWY7R2jNtHerP/">this post from last year</a> , which argued that we should try to &quot;Slow down AI with stupid regulations&quot;, apparently because the author believed that strategy may be the best hope we have to make things go well.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1mmukxvauz4"> <span class="footnote-back-link"><sup><strong><a href="#fnref1mmukxvauz4">^</a></strong></sup></span><div class="footnote-content"><p> Stefan Schubert calls the tendency to assume that humanity will be asleep at the wheel with regards to large-scale risks &quot; <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential">sleepwalk bias</a> &quot;. He <a href="https://www.lesswrong.com/posts/gEShPto3F2aDdT3RY/sleepwalk-bias-self-defeating-predictions-and-existential">wrote about this bias in 2016</a> , making many similar points to the ones I make here.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzmsaxrw6rma"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzmsaxrw6rma">^</a></strong></sup></span><div class="footnote-content"><p> Further supporting my interpretation, in a 2013 essay, Yudkowsky <a href="https://www.lesswrong.com/posts/LNKh22Crr5ujT85YM/after-critical-event-w-happens-they-still-won-t-believe-you">states the following</a> :</p><blockquote><p> In general and across all instances I can think of so far, I do not agree with the part of your futurological forecast in which you reason, &quot;After event W happens, everyone will see the truth of proposition X, leading them to endorse Y and agree with me about policy decision Z.&quot;</p><p> [...]</p><p> Example 2:  &quot;As AI gets more sophisticated, everyone will realize that real AI is on the way and then they&#39;ll start taking Friendly AI development seriously.&quot;</p><p> Alternative projection:  As AI gets more sophisticated, the rest of society can&#39;t see any difference between the latest breakthrough reported in a press release and that business earlier with Watson beating Ken Jennings or Deep Blue beating Kasparov; it seems like the same sort of press release to them.  The same people who were talking about robot overlords earlier continue to talk about robot overlords.  The same people who were talking about human irreproducibility continue to talk about human specialness.  Concern is expressed over technological unemployment the same as today or Keynes in 1930, and this is used to fuel someone&#39;s previous ideological commitment to a basic income guarantee, inequality reduction, or whatever.  The same tiny segment of unusually consequentialist people are concerned about Friendly AI as before.  If anyone in the science community does start thinking that superintelligent AI is on the way, they exhibit the same distribution of performance as modern scientists who think it&#39;s on the way, eg Hugo de Garis, Ben Goertzel, etc.</p></blockquote></div></li><li class="footnote-item" role="doc-endnote" id="fndu5vu3h5oqj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdu5vu3h5oqj">^</a></strong></sup></span><div class="footnote-content"><p> See also <a href="https://twitter.com/MatthewJBar/status/1622076050900672513">this thread from me</a> on X from earlier this year. I&#39;ve made various comments to this effect for a few years now, but they&#39;ve mostly been fragmented across the internet rather than in one place.</p></div></li><li class="footnote-item" role="doc-endnote" id="fndikwaezzana"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdikwaezzana">^</a></strong></sup></span><div class="footnote-content"><p> Conditioning on transformative AI arriving before 2035, my credence range is somewhat higher, at around 75-94%. We can define transformative AI in the same way I defined it in <a href="https://forum.effectivealtruism.org/posts/fsaogRokXxby6LFd7/a-compute-based-framework-for-thinking-about-the-future-of#fnfmli5nt1a99">here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnk28myw46u1g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefk28myw46u1g">^</a></strong></sup></span><div class="footnote-content"><p> This points to one reason why clamping down hard now might be unjustified, and why I prefer policies that start modest but adjust their strictness according to the best evidence about model capabilities and the difficulty of alignment.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk<guid ispermalink="false"> EaZghEwcCJRAuee66</guid><dc:creator><![CDATA[Matthew Barnett]]></dc:creator><pubDate> Wed, 01 Nov 2023 21:17:08 GMT</pubDate> </item><item><title><![CDATA[Reactions to the Executive Order]]></title><description><![CDATA[Published on November 1, 2023 8:40 PM GMT<br/><br/><p> <span style="color:initial">Previously:</span> <a href="https://thezvi.substack.com/p/on-the-executive-order" target="_blank" rel="noopener noreferrer nofollow">On the Executive Order</a></p><p> This post compiles the reactions of others that I have seen to Biden&#39;s Executive Order on AI, including reactions that were based only on the fact sheet, as well as my reactions to those reactions.</p><p> Reaction on the worried side was measured. It could best be described as cautious optimism.</p><p> Reaction on the unworried side was sometimes measured, but often not measured. It could perhaps be frequently described as unhinged.</p><p></p><span id="more-23573"></span><p></p><p> It continues to be odd to see so many voices react in such horror to the idea that the government might not ultimately adapt a fully laissez faire approach to AI.</p><p> Many of them collectively seem to be, essentially, treating a request for government reports on what might be done in the future, plus some very mild reporting requirements imposed exclusively on a few giant corporations, as if it inevitably means AI, nay computers in general, nay the very core of mathematics itself, will suffer the fate of NEPA or IRBs, a slippery slope of regulatory ratcheting until all hope for the future is extinguished.</p><p> I am unusually sympathetic to this view. Such things very much do happen. They very much do often happen slowly. They are indeed strangling much of our civilization. This is all very bad. Pick almost any other hill, everyone involved, where often this is actually already happening and doing great harm, and there are not the massive externalities of potentially everyone on the planet dying, and I would be happy to stand with you.</p><p> Alas, no, all that progress energy is focused on the one place where I fear it is deeply misguided. What should be the default viewpoint and voice of reason across the board is silenced everywhere except the one place I wish it was quieter.</p><p> I&#39;ll divide the post into three sections. First, the measured reactions, to the fact sheet and then the final executive order. Then those crying out about what can be pried from their cold dead hands.</p><p> Also here is a useful tool: <a href="https://valentinsocial.substack.com/p/bidens-ai-executive-order-all-the" target="_blank" rel="noopener noreferrer nofollow">A compilation of all the deadlines in the EO</a> . <a href="https://www.aijobstracker.com/ai-executive-order" target="_blank" rel="noopener noreferrer nofollow">And here is a tool for navigating the EO</a> , file under things that could have been brought to my attention yesterday.</p><p> And before I begin: Yes, it is terrible that we keep Declaring Defense Production Act.</p><h4> Fact Sheet Reactions</h4><p> <a href="https://twitter.com/vivekchil/status/1719022358688649702" target="_blank" rel="noopener noreferrer nofollow">Vivek Chilukuri has a thread summarizing the fact sheet</a> .</p><blockquote><p> Vivek Chilukuri: The EO is the Admin&#39;s strongest effort yet to lead by example in the responsible development and deployment of AI, allowing it to go into the UK Summit with a far more fleshed out policy after years of seeing other nations jump out ahead in AI governance.</p><p> The Admin&#39;s vision for AI development leans heavily into safety, privacy, civil liberties, and rights. It&#39;s part of an urgent but incomplete effort to offer a democratic alternative for AI development to counter China&#39;s AI model rooted in mass surveillance and social control.</p><p> At home, here&#39;s a few ways the EO strengthens US leadership by example:</p><p> -Require companies working on advanced AI to share safety tests.</p><p> -Develop safety and security standards through NIST</p><p> -Guidance for agencies to use AI responsibly</p><p> -Support privacy-preserving technologies</p><p> Abroad, the EO intensifies US efforts to establish international frameworks, shape international standard setting, and interestingly, promote safe, responsible, and rights-affirming AI development and deployment in other countries.</p><p> A note of caution. Going big on an Executive Order is one thing. Getting the execution right is another–especially for federal agencies with an acute shortage of AI expertise. The EO nods to hiring AI experts, but it&#39;s no small task when businesses already struggle to hire.</p></blockquote><p> <a href="https://twitter.com/jonasschuett/status/1719045564187210187" target="_blank" rel="noopener noreferrer nofollow">Jonas Schuett of GovAI has another</a> with screenshots of key parts.</p><p> <a href="https://twitter.com/hlntnr/status/1718998572627046470" target="_blank" rel="noopener noreferrer nofollow">Helen Toner has a good reaction thread</a> , noting the multitude of good things here.</p><p> <a href="https://twitter.com/MatthewJBar/status/1718111599645671804" target="_blank" rel="noopener noreferrer nofollow">Matthew Barnett came away from the fact sheet</a> thinking this didn&#39;t focus much on the issues that matter at all. <a href="https://twitter.com/MatthewJBar/status/1719102490803904704" target="_blank" rel="noopener noreferrer nofollow">He changed his mind after seeing the full document</a> .</p><p> <a href="https://thebulletin.org/2023/10/a-first-take-on-the-white-house-executive-order-on-ai-a-great-start-that-perhaps-doesnt-go-far-enough/?utm_source=Twitter&amp;utm_medium=SocialMedia&amp;utm_campaign=TwitterPost102023&amp;utm_content=DisruptiveTechnologies_ExecutiveOrderAI_10312023" target="_blank" rel="noopener noreferrer nofollow">Gary Marcus calls the fact sheet a good start</a> that doesn&#39;t go far enough. From his perspective that seems exactly right.</p><p> <a href="https://twitter.com/neil_chilson/status/1719073480610635965" target="_blank" rel="noopener noreferrer nofollow">Neil Chilson was worried because he expected a bunch of new government agencies and perhaps international agreements</a> , hopefully he is happy now that he knows it is a pile of reports, so it&#39;s mostly things he filed under &#39;The Good/OK.&#39;</p><p> <a href="https://barackobama.medium.com/statement-on-the-biden-administrations-executive-order-on-artificial-intelligence-91a5ddac6238" target="_blank" rel="noopener noreferrer nofollow">Barack Obama gives a standard politician thumbs up to the whole operation</a> , with a surprise shoutout to ARC. It seems Obama is not worried about existential risk yet.</p><h4> Full Executive Order Reactions</h4><p> The establishment of a meaningful compute threshold seems like the biggest game.</p><blockquote><p> <a href="https://twitter.com/DavidVorick/status/1719097248699879831" target="_blank" rel="noopener noreferrer nofollow">David Vorick</a> : New Executive Order is out. Already one notable item: Any AI model that required more than 1e26 floating point operations or 1e23 integer operations to build must report to the government.</p><p> Model performance and compute scale are tightly correlated. That could change in the future, but for now, FLOPs remain a highly valid proxy for capability.</p><p> <a href="https://twitter.com/hamandcheese/status/1719102025466597612" target="_blank" rel="noopener noreferrer nofollow">Samuel Hammond</a> : Pleasantly surprised the AI EO stuck with a compute threshold. Many will push against this, arguing that regulation should only ever be use-based, but such a threshold will be essentially for delineating AGI / ASI labs for special oversight. This *is* the light-touch approach.</p><p> The threshold draws a line for safety testing and disclosures. Emergent capabilities are inherently hard to predict in advance, and thus can&#39;t be regulated on a discrete “use” basis, especially when we&#39;re talking about highly generalist systems, not any one capability.</p><p> ……</p><p> To be clear, no company has yet trained a model with 10^26 FLOPs. Based on Metaculus&#39;s forecast, this particular threshold may not even bind until after 2025. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69a34cd8-65ae-4079-b676-2b01c45b77d2_2228x1246.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/rzgbbvhz668kxx53c9is" alt="图像"></div></figure></div><p> I strongly agree with this, both that it is important and also that it is the correct move. The big update from this report is that America is going to consider using compute thresholds as a measure for the potential dangers of AI systems. I know of no viable alternatives.</p><p> <a href="https://twitter.com/WillManidis/status/1719062906359497197" target="_blank" rel="noopener noreferrer nofollow">Will Manidis gives the health care section of the final bill a close reading</a> , notes that giving HHS authority over healthcare AI is a broad expansion of its authority, whereas previously it was diffused over FDA, CMS and OCR. Will predicts that HHS will struggle to handle this and there will be trouble. I&#39;d respond that diffusion across four agencies would seem like far more trouble than that.</p><p> <a href="https://twitter.com/katieelink/status/1719416928429854843" target="_blank" rel="noopener noreferrer nofollow">Katie Link looks at the biological provisions</a> .</p><blockquote><p> Katie Link: I think a lot of the EO&#39;s guidance on strengthening security for synthetic biological sequence procurement, as well as the emphasis on evaluation and monitoring of AI models in healthcare settings, are positive developments overall!</p><p> But I worry about the negative impact on biological research and open science, and I don&#39;t think the guidelines on open models and datasets meaningfully strengthen our national biosecurity efforts and defense against bioweapon development.</p><p> IMO the hardest part about the actual idea ->; bioweapon pipeline is not the how-to knowledge (can be found online), or even coming up with synthetic sequences via biological models – it&#39;s the actual lab work, which is still really difficult, expensive, slow, and unpredictable.</p><p> ……</p><p> This is a tricky and important subject that I think requires many diverse perspectives, so I&#39;m looking forward to hearing the community&#39;s thoughts as well as ways HF can do better in improving the transparency and positive impact of biology models and datasets <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/bhjl4i0v62cwf96vqfke" alt="🤗" style="height:1em;max-height:1em"></p></blockquote><p> It is reasonable to worry about restrictions on bad things like bioweapons also interfering with the research necessary to defend against them. In this case, the extra security precautions seem well worth the extra time and expense to me – if your project can&#39;t afford to pay those extra costs, it was not so valuable. The restrictions on model registration that she highlights I do not expect to much matter, so what if Google registers the next AlphaFold.</p><p> Damek Davis notes one conspicuously unmentioned item. Not only is it not required, I can recall no mention of it as a possibility in any of report requests, although it could come up in some of the privacy or discrimination concerns.</p><blockquote><p> <a href="https://twitter.com/damekdavis/status/1719007839341920424" target="_blank" rel="noopener noreferrer nofollow">Damek Davis</a> : Absent from new exec order: <em>AI companies must reveal their training set.</em> To develop <em>safe</em> AI, we need to know what the model is trained on. Why aren&#39;t the AI safety orgs advocating for this?</p></blockquote><p> The other things that are conspicuously missing, as is pointed out later, is any mention liability or licensing.</p><p> <a href="https://twitter.com/mattyglesias/status/1719675462304018932" target="_blank" rel="noopener noreferrer nofollow">Matthew Yglesias leaves the task to me, here in the right amount of obscurity.</a></p><blockquote><p> Matthew Yglesias: Thou shalt not make a machine in the likeness of a human mind </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F524ffb59-0a26-4e9b-be78-218d95fa0bfd_1170x1535.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/c2ymyweaiu44ejgw5dp5" alt="图像"></div></figure></div><blockquote><p> Matthew Yglesias: My thought is that it&#39;s probably not a good idea to cover the administration&#39;s AI safety efforts much because if this becomes a high-profile topic it will induce Republicans to oppose it.</p><p> Better to save humanity in obscurity.</p></blockquote><h4> AI Snake Oil Seems Cautiously Optimistic</h4><p> <a href="https://www.aisnakeoil.com/p/what-the-executive-order-means-for" target="_blank" rel="noopener noreferrer nofollow">Arvind Narayanan and Sayash Kapoor look into what the order means for openness</a> .</p><blockquote><p> Arvind + Sayash: How will the EO affect the concentration of power and resources in AI? What about the culture of open research?</p><p> We cataloged the space of AI-related policies that might impact openness and grouped them into six categories. The EO includes provisions from all but one of these categories. Notably, it does not include licensing requirements. On balance, the EO seems to be good news for those who favor openness in AI.</p><p> But the devil is in the details. We will know more as agencies start implementing the EO. And of course, the EO is far from the only policy initiative worldwide that might affect AI openness. </p></blockquote><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F462a9cd5-0f2e-4c6e-9cf0-751e40f9a385_1156x420.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/x0eydubtoidf7xrgowfg" alt=""></div></figure></div><p> Licensing does seem obviously hostile to openness. It does not have to be that way, depending on implementation details, but that is the usual result.</p><p> The idea that liability is anathema to openness implies some odd things, but it is not in the EO, so not relevant here.</p><p> They point out that the reporting requirements that kick in at 10^26 flops are well above the threshold of any open source projects so far. If one did want to cross the threshold, how burdensome are the reports? I continue to say, not very, compared to the cost of that much compute. Can anyone point to a hard part? And if you can&#39;t put together the information necessary for the report, I do not trust you to take the other safety measures appropriate to a model of that size.</p><p> They approve of the EO&#39;s emphasis on defending attack surfaces, and say they despair of stopping bad actors from being able to access capable AI, to which I have a very &#39;not with that attitude&#39; style of response. I see defending attack surfaces as a great idea on the margin, so we should do it, but if the bad actors (or enough different actors period) get the unrestricted highly capable models at the cutting edge, I have zero idea how that could end non-badly.</p><p> One note I appreciated was the idea that even if AI is being unduly blamed for risks like synthetic bio or cybersecurity, that shoring up those attack surfaces was a good idea anyway. I completely agree. We are not yet at the degree of security where AI is required as a justification.</p><p> On transparency they note the lack of such requirements. I would say that the EO does require a bunch of transparency over the reporting threshold, but no it does not do so for data or labor. That would really cause various people to blow their gaskets, although perhaps not in a meaningfully different way than they did already. I find such reactions hard to anticipate. I see such transparency as a nice to have relative to other things.</p><p> They note that too much AI concentration is listed as a risk, and that antitrust enforcement in the AI space is to be expected. I understand the impulse to favor this, but also it is rather horrifying. Anti-trust rules explicitly prohibit things like agreeing to proceed in a safe fashion rather than racing as fast as possible in the face of existential risks and massive potential externalities, and potentially ensure universal access to AIs that trigger competitive dynamics we cannot survive. Almost no one has faced the music on this.</p><p> It is noteworthy the positive attitude held here, by advocates of openness, versus others who think that this EO is a prelude to regulatory capture, monopoly and tyranny.</p><p> On the AI development incentives, they rightfully call them out as woefully weak. I found them so weak that I wrote such efforts off as essentially meaningless. What they want here is for the USG not only not to interfere with open source, they want USG to put its hand on the scale to allow open source to keep up with the top companies. I am hopeful that the winds are going in the opposite direction, we very much do not want open source AI models keeping up.</p><p> Overall, this is a cautiously optimistic report card on openness. Not only do they not see this is an existential threat to our freedoms or compute, they see the new rules as mild positives for openness, notice that the actions they fear the most were entirely absent, and are disappointed only that the pro-openness actions were not more substantive. Which I agree is a highly reasonable complaint if you want more openness, those provisions were highly weak sauce, but then that was true in general.</p><h4> Reactions in Horror</h4><p> For completeness and fairness, I included everyone who came to my attention via my normal channels during the period.</p><p> The core reaction:</p><blockquote><p> Kevin Fisher: A sad day for AI.</p></blockquote><p> The line I expect to be the standard hyperbolic talking point:</p><blockquote><p> <a href="https://twitter.com/DJohnstonEC/status/1719196676974415928" target="_blank" rel="noopener noreferrer nofollow">David Johnston</a> : The government just issued a limit on the amount of Math you are allowed to do in the United States…</p></blockquote><p> I like there being a catchy phrase like this, so people can identify themselves.</p><h4> The Slippery Compute Slope</h4><p> <a href="https://twitter.com/paulg/status/1719264300362015174" target="_blank" rel="noopener noreferrer nofollow">The slippery slope argument</a> , which to be clear is usually valid, and is indeed the correct reason to be worried about the Executive Order:</p><blockquote><p> Suhail (Founder Playground AI, pinned tweet is literally “Run your own race”): I think we will ultimately regret that we did this to ourselves.</p><p> I am optimistic because hackers are more clever at bending these sort of silly regulations and much faster at reacting to them than the government could ever.</p><p> I do not think we need a stop-gap. Regulate applications, not technology.</p><p> One day we will have the equivalent of the gpu compute Azure has in an iPhone and this regulation will seem comical to our children.</p><p> Where regulation and reporting requirements begin according to the AI EO:</p><p> – 50K H100s at fp16 (w interconnect)</p><p> – 1.5M H100s at fp32 (w interconnect)</p><p> – 414M H100 training hours</p><p> Andrew Ng: Laws to ensure AI applications are safe, fair, and transparent are needed. But the White House&#39;s use of the Defense Production Act—typically reserved for war or national emergencies—distorts AI through the lens of security, for example with phrases like “companies developing any foundation model that poses a serious risk to national security.”</p><p> ……</p><p> It&#39;s also a mistake to set reporting requirements based on a computation threshold for model training. This will stifle open source and innovation: (i) Today&#39;s supercomputer is tomorrow&#39;s pocket watch. So as AI progresses, more players — including small companies without the compliance capabilities of big tech — will run into this threshold. (ii) Over time, governments&#39; reporting requirements tend to become more burdensome.</p><p> The right place to regulate AI is at the application layer.</p><p> ……</p><p> While the White House order isn&#39;t currently stifling startups and open source, it seems to be a step in that direction. The devil will be in the details as its implementation gets fleshed out — no doubt with assistance from lobbyists — and I see a lot of risk of missteps. I welcome good regulation to promote responsible AI, and hope the White House can get there.</p><p> Jim Fan (Nvidia): The right place to regulate AI is at the APPLICATION layer. Requiring AI apps like healthcare, self-driving, etc. to meet stringent requirements, even pass audits, can ensure safety. But adding burdens to foundation model development unnecessarily slows down AI&#39;s progress.</p><p> Regulate actions/outcomes, not the computing process. Here&#39;s an example.</p><p> You only need &lt;100M parameters to build a literal killer AI from hell. Download a Mask R-CNN object detector. Train an ethnicity classifier. Mount a gun on Spot robot dog. Pure evil with a fraction of LLMs&#39; FLOPs.</p><p> I&#39;m not saying the EO is all wrong. It&#39;s got well-intentioned guidelines like “red-teaming report”. But today&#39;s super computer will be tomorrow&#39;s pocket watch. It could over-regulate OSS (thus stifle innovation) while under-regulate cases like the above.</p></blockquote><p> I continue to see this line of &#39;regulate applications not technology&#39; as the failure to understand that with sufficiently capable and available AI the technology implies its application. That the existence of an existentially dangerous AI system is existentially dangerous without it needing to be pointed at any applications.</p><p> In particular, the idea that you can allow open source models, and then &#39;regulate their applications&#39; in this context, in any meaningful way, does not make sense to me. If you allow the model to exist in open source then anything that can be done with it after modifications, will be done with it. Merely telling people what they are not allowed to do is not going to work. So how would one regulate the application rather than the model?</p><p> If you project far enough into the future, giving a sufficiently capable AI a text window that a human will read is more than sufficient to doom us, and we should be worried that even without such a window it will instead find metaphorical buffer overflows or other physical tricks.</p><p> Even if you did manage to instead regulate at the application layer, how is that not going to result in a vastly more intrusive monitoring system? You now need to check every command given to every sufficiently general model, lest it be used for a forbidden application. If you target the compute used in training, then you can limit your concern to large concentrations. If you let the models into the wild, you can&#39;t.</p><p> Also, almost the entire EO is indeed examining what to do about applications rather than technology. Section 4, especially section 4.2, dealing with future frontier models, rightfully targets the model because there is no alternative. But the later sections are all about particular applications and particular harms.</p><p> Andrew Ng&#39;s statement correctly identifies that the EO&#39;s danger lies in what happens if future stages of implementation go poorly, and that as always the devil will be in the details and regulations have a way of not ending up well-aimed. Eventually, yes, if we keep the threshold at 10^26, it will start to apply to more and more processes.</p><p> If the entire process stays at &#39;tell us what you are doing&#39; then if someone is indeed building a pocket watch or other obviously safe system, you can (not quite, but more or less) write down &#39;using 4.2 x 10^26 FLOPS, at this data center, running the industry standard mundane harms evaluation package&#39; and be done with it, even if the threshold is not formally raised. Hell, I bet you can get your local fine-tuned version of GPT-5 to write your disclosures for you, why not, no one will read them if there is indeed no danger there.</p><p> The risk is that this report greatly expands and other requirements are added over time, in a way that do introduce an undue burden, and that this also applies to a wider range of systems, and also that none of that was actually necessary for safety. That is a possible world. I don&#39;t expect it, but it makes sense to keep an eye.</p><p> However I would also caution that it is not obvious that it will be safe to raise this threshold any time soon. Algorithmic efficiency and data will improve rapidly over time, so 10^26 flops is going to continuously buy you more intelligence, and be more dangerous. It is even plausible that we will need to consider lowering, not raising, this threshold, if our goal is to guard against potential new superintelligent systems, either entirely or when lacking the proper safeguards.</p><p> At some point, if AI technology advances, the whole point is that we will need to impose costly safety requirements or even outright prohibitions before creating more capable AI models, because the use or later the very existence of such models poses a potential existential threat, and that threat will expand not contract until we at least solve alignment and I believe also solve other problems that creating such systems would cause even if they were aligned. Ultimately, yes, the ability to interfere with AI capabilities is the point.</p><blockquote><p> Paul Graham: I don&#39;t know what the right approach to regulating AI is, but one problem with this particular approach is that it means we&#39;re heading toward the government regulating private individuals&#39; computing at an exponential rate.</p><p> Eli Tyre: What a correct way of phrasing this! I agree with both parts of this statement. I wish our options were better (more effective at preventing extinction, and imposing fewer externalities on people not doing anything dangerous).</p></blockquote><p> Computing power is increasing at an exponential rate. So any move to regulate it at all is also going to have to evolve at an exponential rate if it is to be meaningful.</p><p> Paul&#39;s concern is that it is a move to regulate computing power. Which, yes, this is (in part) absolutely a move to lay groundwork for regulating computing power. Which is the only proposed move that could plausibly prevent or substantively low the creation of AGI until such time as it would not result in everyone dying.</p><p> Paul Graham wishes that we would find such an alternative that would work, and I do as well. We would love to hear about it. But a lot of very smart highly motivated people have been searching, and this is the best we have found so far by quite a lot. I do not see &#39;let anyone scale however much they want with safeguards being fully optional&#39; as a viable alternative. So what choice do we have other than talking price?</p><blockquote><p> Zvi (on Twitter, other thread): The reward for reading 20k words of an EO that is almost entirely calls for government reports is to read the diatribes about how it is government overreach and full of premature regulations and assumptions, when you spent hours confirming that nope, it&#39;s all report requests.</p><p> Nabeel Quereshi: Regulations usually _start out_ reasonable, the (obvious) concern is that they&#39;re a one-way ratchet. See IRBs as an example.</p><p> Shea Levy: From most, I&#39;d say this belies naïveté about the purpose and effects of this kind of bureaucracy and suggest that the author review the history or think for five minutes about how a petty power-luster might take advantage of and expand this. Zvi, of course, already knows this.</p><p> <a href="https://twitter.com/MTabarrok/status/1719494839224447442" target="_blank" rel="noopener noreferrer nofollow">Maxwell Tabarrok</a> (other thread): Hmm yeah if a law just calls for bureaucrats to prepare reports it can&#39;t be that burdensome. There is no precedent for laws of this nature evolving into unbelievably wasteful time sinks which drain huge swathes of the economy and cripple important technologies.</p></blockquote><p> This does indeed seem to be the full argumento ad absurdum. It is saying that we should never dare ask a government department to write a report on what is happening, might happen in the future or might be done about any of it, lest it be a prelude to tyranny.</p><p> Again, that is not, in general, an inherently absurd view. There are places where it would be better if the government never created any reports, so that it is not tempted to ever do anything. One need not affirm a regulation that is net positive as stated, if one expects it to then ratchet up and become negative. But as is the central theme, the only offered alternative is to do actual nothing indefinitely.</p><h4> Libertarians Gonna Libertarian</h4><p> Again, reminder, I love you guys, I really do, shame we couldn&#39;t meet anywhere else.</p><p> <a href="https://twitter.com/geoffmanne/status/1719196716891619341" target="_blank" rel="noopener noreferrer nofollow">Standard anti-regulatory response to any attempt to do anything</a> :</p><blockquote><p> Adam Thierer: The new Biden AI executive order issues broad and quite amorphous calls for expanded government oversight across many other issues and agencies, raising the risk of a “death by a thousand cuts” scenario for AI policy in the US.</p><p> The EO appears to be empowering agencies to gradually convert voluntary guidelines into a sort of back-door regulatory regime for AI, a process which would be made easier by the lack of congressional action on AI issues. The danger exists that the US could undermine the policy culture that made our nation a hot-bed of digital innovation and investment. We don&#39;t want to become Europe on tech policy.</p><p> Geoffrey Manne: People really miss how important the culture of entrepreneurship is in the US — and how unique it is. The relentless effort to regulate away our superpowers is truly depressing.</p></blockquote><p> Indeed, this order is a call for future oversight, which could indeed lead to maybe telling people that they are not allowed to do some things, or that to do some things they might also have to do other things. Yes, the government might decide it wants to stop certain things from happening, and might do so more often than is ideal. I can confirm that is the point. On some of the points I strongly agree with the need to act, on others I am skeptical. But if you react this way to every single proposed action ever, with the same generic reasoning, your reaction isn&#39;t information.</p><p> <a href="https://reason.com/2023/10/31/biden-issues-a-i-red-tape-wishlist/" target="_blank" rel="noopener noreferrer nofollow">Reason similarly posts their standard &#39;this new proposed regulation can only hurt America</a> .&#39; I do appreciate them highlighting the invocation of the Defense Production Act. Someone has to point out such things in theory matter. But when the time comes to show what the great harms will be that are going to stifle innovation, they are at a loss, because the whole thing is a list of requirements for an endless set of reports.</p><p> <a href="https://www.rstreet.org/commentary/white-house-executive-order-threatens-to-put-ai-in-regulatory-cage/" target="_blank" rel="noopener noreferrer nofollow">R Street&#39;s Adam Thierer writes the required column</a> , and to his credit often chooses his words carefully. Note the &#39;threatens to&#39; in the title, which is: White House Executive Order Threatens to Put AI in a Regulatory Cage.</p><blockquote><p> Adam Thierer: While some will appreciate the whole-of-government approach to AI required by the order, if taken too far, unilateral and heavy-handed administrative meddling in AI markets <a href="https://www.rstreet.org/outreach/testimony-on-artificial-intelligence-risks-and-opportunities/" target="_blank" rel="noopener noreferrer nofollow">could undermine America&#39;s global competitiveness</a> and even the nation&#39;s geopolitical security.</p></blockquote><p> Yes, if you take regulation too far, that would be bad. The argument seems to be once again, therefore, no regulation at all, and even no government reports? In general I do have great respect for that position, but it is deeply unworkable here, and is deeply unpopular not only here but in the places it is largely or entirely correct.</p><p> I was disappointed that he does not maintain his message discipline throughout, with derisive references to existential risk concerns, starting with &#39;scenarios pulled from the plots of dystopian science fiction books and movies&#39; and getting less respectful from there.</p><p> Forbes also pulls out standard tropes.</p><blockquote><p> James Broughel (Forbes): The government&#39;s “regulate first, ask questions later” approach is reminiscent of when Congress created the renewable fuels program in 2005, which increased the ethanol content in gasoline.</p></blockquote><p> Forbes calls the order &#39;regulation run amok&#39; and accuses it of &#39;Ready!火！ Aim!” Except the entire report is &#39;Ready! Aim!&#39; and no Fire. This is the Ask Questions First document, perhaps run amok in the breadth of its questions. At best this is training to run, but still at the starting gate. Where are the overly prescriptive regulations? What is the prescription of which you speak? The &#39;stringent regulatory hurdles&#39; that involve &#39;tell people what you are doing&#39; and only apply to a handful of large corproations? Where are all the &#39;assumptions&#39; in the order?</p><p> The best they can come up with was a worry I had as well, that requiring reporting of safety results might discourage testing. On the other side, I would expect having to report all tests to also strongly encourage testing, because otherwise you are conspicuously not reporting any tests.</p><p> <a href="https://twitter.com/jgarzik/status/1719411946762412057" target="_blank" rel="noopener noreferrer nofollow">This also was brought to my attention</a> :</p><blockquote><p> Ser Jeff Garzik: In re AI EO: This is yet again a familiar pattern of acting first from the executive branch. People naively want a chieftain-king, and Biden admin happily supplies: rule by executive fiat, achieved through arcane legal positioning.</p><p> This AI EO feels like a return to the 1990s:</p><p> – whitelisted USG Big Corp friends have no limits on GPU cycle count or parameter count.</p><p> – plebs must make do with lesser models …</p><p> Just like the early 1990s encryption, where 40-bit and below weak encryption was exportable outside the United States.</p><p> [Then in response to the Defense Production Act he invokes Godwin&#39;s Law.]</p><p> The #AI #EO is another cog in the automated #surveillance and control machine being built. #privacy #OpenSource</p></blockquote><p> This is, once again, many steps ahead of ourselves, except also assuming what he sees as the worst outcome on many different fronts at once. Is this ultimate outcome possible down the line? Is that one way we could implement compute limits? It certainly is possible. Would not be my first choice. But once again, that is not what the EO says. I presume that is how it feels to Jeff because that is his background assumption of what will always happen.</p><p> One almost suspects some of these posts were written before they knew what was in the order. Others seem to be reading an entirely different EO, that travelled back in time from a possible 2026.</p><h4> Attempted Detailed Takedowns</h4><p> Bindu Reddy offers her usual flavor of thoughts.</p><blockquote><p> Bindu Reddy (CEO AbacusAi): The AI Executive Order is a <strong>bit ridiculous and pretty hard to enforce.</strong></p><p> Here are the issues –</p><p> 1. Any f <strong>oundation model</strong> that poses a serious risk to national security – How do you determine if something is a “serious risk to <strong>national security!</strong> “? If this is about mis-information, Twitter, YouTube and Meta are way more serious risks to national security than harmless AI models that merely generate content, not distribute or amplify it.</p></blockquote><p> I presume that the government intends to do that evaluation itself, the same way it does with any other sort of threat. There is not some technical definition of what is a serious risk to national security, nor does a complete one exist to be found. We use phrases like &#39; <a href="https://en.wikipedia.org/wiki/Clear_and_Present_Danger_(film)" target="_blank" rel="noopener noreferrer nofollow">a clear and present danger</a> &#39; and &#39;probable cause&#39; to justify government actions.</p><p> Right now, I would agree that current models do not present such a threat. My understanding is that the current administration agrees with this.</p><p> If a model does not distribute its content in any way, and no one can or does access its outputs, then the model would need to be far more advanced and do some other physical thing to pose a threat to national security. But if there exist people, who are reading the output of the model, then that output can impact those people to do things in physical space, or the information can be utilized or amplified by those people, without any bound. Such a thing could easily pose a threat to national security. Whether or not any given system (eg Llama-3 or GPT-5) did so is a fact and physical-world question.</p><p> Certainly it would be unwise to say &#39;we cannot technically define what is and is not a threat to national security in precise terms thus we should not respond to such threats&#39; any more than we would say &#39;we cannot technically define what is and is not fraud in precise terms thus we should not have laws against fraud.&#39; Or murder, or anything else.</p><blockquote><p> 2. All AI-generated content has to be <strong>watermarked</strong> ? – Seriously!? We may as well kill vision AI, if we actually enforced that. Are Enterprises allowed to use AI to generate images and use them in their <strong>marketing</strong> ?</p></blockquote><p> This is bizarre hyperbole, even if they do ultimately impose watermarking requirements. Yes, of course you are allowed to use AI images for marketing, no this will not kill vision AI. There are tradeoffs, this introduces some marginal costs, and one could argue it is not worthwhile, but I have yet to see an argument for the costs being a big deal, especially given that costs are continuously declining over time, so even a tax that looks big doesn&#39;t ultimately do much if it is a fixed percentage.</p><blockquote><p> 3. Some agency is apparently going to develop tests that the AI models have to pass – How will this get enforced? Does every fine-tuned version of any open-source model have to pass these tests? What happens if a model is dropped on HuggingFace by a <strong>non-US corporation</strong> ?</p></blockquote><p>是的。 If there is a test you have to pass then you have to pass it.</p><p> Any reasonable regulatory regime is going to require passing some set of tests. A sufficiently capable AI model is going to be able to do things that trigger regulatory scrutiny, one way or another.</p><p> Does every fine-tuned version of any open source model have to pass these tests?</p><p> We don&#39;t know yet. In an ideal world and in my best guess I would say no unless you are doing very expensive and extensive fine tuning and then releasing the result.</p><p>为什么？ If you release an open source AI, you are also releasing every fine-tuned version of that open source AI. So the original test should cover any potential threat from a (lightly) fine-tuned version, say up to 1% of original compute training costs.</p><p> If this means no open source model above some level can ever pass the test? Then you can&#39;t have open source models above that level. That&#39;s the whole point.</p><p> What happens if a model is dropped on HuggingFace without passing the tests? Then I would presume they are in violation of American law, and further spreading or using it would also be a violation of American. Where &#39;they&#39; above means both the other company and also HuggingFace, who we have jurisdiction over. It would be the job of such websites to prevent the sharing of illegal models, same as any other illegal software or files one might upload. I am however not a lawyer or legal scholar, and all that.</p><blockquote><p> 4. It has a bunch of <strong>“privacy” protecting language</strong> – All this does is cripple AI models compared to Search where the same “privacy” isn&#39;t being enforced. Google search will recognize a face for me, GPT-4V won&#39;t. It&#39;s not clear why my photo that is in the public domain, can&#39;t be used by AI? We already have privacy protecting laws. These should be sufficient.</p></blockquote><p> I agree that we should treat mandatory privacy protections with skepticism, if they are ultimately implemented. Which, so far, they are not.</p><p> The privacy argument and harm argument here, as I understand it, is that the AI could soon be shockingly good at recognizing anyone anywhere as well as locations, while also making mistakes, and also people will create fake images that the AI would identify as being the people they are pretending to be in the places they are pretending to portray, doing the things they are pretending to do, and so on. So if people start using vision AI for identification then this opens up some nasty dynamics.</p><p> Do I find this annoying and rather silly? Yeah, I basically do, but I am open to taking a cautious approach here, especially as part of a package of precautions.</p><blockquote><p> It then ends with some “ <strong>feel good” language</strong> around hiring and open-source. Net-net, this order won&#39;t make any difference in the near term.</p><p> Could be harmful in the long run to companies who do develop these foundation models, especially vision models. Weirdly, it helps search engines or makes them way more powerful because they are not regulated, while AI models are <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/45D7rZXiG5gCoCveM/tgwxapmuwbji4c1ulcpi" alt="🤔" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/45D7rZXiG5gCoCveM/tgwxapmuwbji4c1ulcpi" alt="🤔" style="height:1em;max-height:1em"> 。 。</p><p> 。 。 <strong>If anything, this isn&#39;t good for OpenAI</strong> <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/fdgblesltv6dowuutzww" alt="🤷‍♀️" style="height:1em;max-height:1em"></p></blockquote><p> Did you hear that last line, everyone? There will of course be endless complaints that this whole operation is regulatory capture by OpenAI (along with DeepMind and Anthropic and perhaps others). Here is someone, who is very against regulation, saying the order is if anything not good for OpenAI.</p><p> <a href="https://twitter.com/varun_mathur/status/1719120180809179640?s=20" target="_blank" rel="noopener noreferrer nofollow">Varun offers the most negative take</a> short of calls for revolution, effectively not reading the same document that I did:</p><blockquote><p> Some red flags: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> Zooming out, what struck to me at the beginning is that this is not just an “AI” executive order, it is infact a retroactive governance regime being imposed on the entire web, while also limiting the future. The term AI is defined so broadly, that it could impact everything from Google search results, to Amazon recommendations, to Yelp restaurant suggestions, to anything else which recommends anything to you. This is the start of the “AI-industrial-complex”, which will now lead to a world of billions of dollars being spent in regulations, compliance and lobbying over the time ahead.</p></blockquote><p> I mean yes the definition of AI here is not good but one I presume that gets fixed, and I will support efforts to get it fixed, and also there is no imposed regime except for reporting requirements above 10^26 flops? Potentially there could be requirements in the future. </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> This EO is essentially banning fully open source models over 10B parameters, as it requires reporting requirements and “..physical and cybersecurity measures taken to protect those model weights”.</p></blockquote><p> No. It requires that someone involved send an email saying &#39;the measures we took to protect these model weights was to post them on Torrent.&#39;</p><p> (Also, tens means 20B, not 10B, for what that&#39;s worth, which is not a tiny model.)</p><p> If you think that is a bad look when said out loud, or that this is an unacceptable answer… well, that is what it means to be open source.</p><p> If this leads to a ban on such models in the future, then yes I would be in favor of that. I mean obviously, if something needs to be secure then you cannot simply give it away to everyone who asks nicely. </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> Beyond the datacenter clouds, this would also extend to even decentralized networks providing compute, where even the location and total compute has to be reported. To Floyd in Florida running a SETI @ home node searching for extraterrestrial life signals – the government wants to know the precise location of your computer in your basement now…</p></blockquote><p> Yeah, no, that is not what I expect that this means, but if it did GPS can do this for you, the government already had ways to find out, no one forced you to join that and I honestly do not particularly care?</p><p> Every time a new regulation passes, there is a class of free software style people who try to figure out exactly what a maximalist interpretation of what this might imply might do if fully enforced and they panic. That is a useful thing for someone to do, but we should not confuse it with what is likely to happen. </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> Compliance regime for any model which was trained with more than 10^26 floating-point operations (or 0.1 exaflops)</p></blockquote><p> The compliance regime is literally &#39;file a report on what you are doing.&#39; But I certainly hope it leads to this! </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> Training requests from non-US persons to be submitted. This is like opening a bank account now!</p></blockquote><p> If you are trying to do a full training run that requires a report then I do not think this is much of an additional burden?</p><p> I also don&#39;t see why one couldn&#39;t rent a data center somewhere else if your request is lightweight – the concern I actually have here is that business can easily be taken elsewhere. </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> This is a remarkable regulatory capture exercise, as now the cost of compliance for anyone running a physically co-located datacenter with compute above a threshold now increases</p></blockquote><p> For data centers? I have actual no idea how the requirements here could lead to datacenter regulatory capture. What is the compliance cost that is non-trivial here? That they perform KYC on major customers? That they know where their computers are? I would love if these are our regulatory capture worries, in places where I would worry about regulatory capture. </p><blockquote><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/pl8kaukht07phqgpnesw" alt="🚩" style="height:1em;max-height:1em"> Models in this regime are basically guilty until proven innocent if they require compute >; 0.1 exaflops</p></blockquote><p> Models are found guilty if you do not report that they exist and forced the government to go find them, I suppose. Otherwise, this is pure fiction in terms of the EO.</p><p> But in terms of the actual situation? And what I hope is the law a few years down the line? Hell yes, of course capable foundation models are dangerous until proven safe, that is how safety works when you create such things, I have no idea what the sane alternative is.</p><h4> Tyler Cowen Watch</h4><p> I&#39;ve covered his links here elsewhere on their full contents, but for full context here are the things he quoted and said about the EO.</p><blockquote><p> <a href="https://feeds.feedblitz.com/~/804538733/0/marginalrevolution~Tuesday-assorted-links.html" target="_blank" rel="noopener noreferrer nofollow">Tyler Cowen</a> : <a href="https://feeds.feedblitz.com/~/t/0/0/marginalrevolution/~https://twitter.com/neil_chilson/status/1719073480610635965" target="_blank" rel="noopener">Some comments on the AI Executive Order</a> .以及<a href="https://feeds.feedblitz.com/~/t/0/0/marginalrevolution/~https://twitter.com/WillManidis/status/1719062906359497197" target="_blank" rel="noopener">HHS 的作用</a>。还有<a href="https://feeds.feedblitz.com/~/t/0/0/marginalrevolution/~https://www.rstreet.org/commentary/white-house-executive-order-threatens-to-put-ai-in-regulatory-cage/" target="_blank" rel="noopener">亚当蒂勒</a>。 <a href="https://feeds.feedblitz.com/~/t/0/0/marginalrevolution/~https://twitter.com/varun_mathur/status/1719120180809179640?s=20" target="_blank" rel="noopener">还有另一个负面看法</a>。我同意更负面的看法，但也许他们高估了实际命令的合法性/执行力？</p></blockquote><p> Again, I don&#39;t think there is anything to &#39;enforce&#39; here? Not yet anyway.</p><blockquote><p> Agustin Lebron: Even though it&#39;s not a compute cap (just a requirement to register, for now), surely this emboldens competitors to OpenAI/etc?如果我认为我可以构建一个达到上限的系统，那么我的竞争环境就会变得更加公平。</p><p>这也将再次刺激对样本效率的大量研究，在我看来这是一件非常好的事情。这个研究领域多年来一直在“尝试更多失败”的体制下陷入困境。</p><p>最后，我认为该命令通过向世界展示“嗯，美国政府并不认为能力如此重要，他们正在为此发布命令”，从而反常地使能力研究合法化。更多关注，更多资金，更多进步。</p><p>总体而言，这对于 AI/AGI 来说是加速的。</p></blockquote><p> I don&#39;t know what to say to that, beyond being confused why any of that would change what anyone does. People reacting to USG caring about AI capabilities? All sides should be able to agree USG is playing from behind on that one. Does this actually update anyone? Under this logic, what wouldn&#39;t be accelerative?</p><p> Also, for a potentially accelerative document, it sure has a lot of accelerationists really angry. Stay tuned for more of that.</p><p> A commentor has a better accelerationist case:</p><blockquote><p> Marcel: I work for a federal agency. Right now at every single federal agency there are people trying to get the agency to approve using AI for some purpose. And there are people saying no way, that&#39;s too risky. The existence of this EO will allow the people trying to bring AI into the government to say “look, the administration is in favor of AI” and in some cases that will win the day.</p><p> I am fairly confident this EO will result in greater use of AI by the federal government. Whether that is a good thing depends on your view of the government, I suppose.</p></blockquote><p> Narrowly restricted to the government itself, I agree. This is intended to be, and will be, accelerationist. I do not expect that to result in important capabilities gains, and I&#39;m fine with it. American state capacity is mostly good, actually?</p><h4> Do You Ever Think About (Liberty or) Death?</h4><p> <a href="https://twitter.com/pmarca/status/1719540860969529755" target="_blank" rel="noopener noreferrer nofollow">And of course, here&#39;s Marc Andreessen (via others including Vladimir Bok and Investment Hulk) being sane as usual</a> , having a very normal one and totally not calling for a revolt against the government, his new bio is “Techno-optimist. E/acc. Legalize math.” </p><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F08186f64-4655-497e-a7d0-49c3922b7e8a_1200x1200.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/fvtnginxyb1sileve5qo" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaa5e784-6d78-437a-ae96-e86fe974afe3_1024x1024.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/uyg4ulbyksoxdbb6typh" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb031c542-3dc4-4bfc-ad8b-27b7fb74cdb1_767x647.png" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/k7cscikvhicbpte5c7jc" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87ed5ea5-2b1d-4c4b-a761-8bef947a965a_1024x1024.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/o2ten9ukrw2dujqnt2jj" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54443420-0590-422c-9700-9f62c50b5a79_1024x1024.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/wnt9tyawa1ghladpn5b9" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe88863f-4b70-4720-83be-d9c76191a57c_1024x1024.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/ozgx0hnvfy47dwyh6sk5" alt="图像"></div></figure></div><div><figure><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa631479c-887b-44a9-a57f-e8c7f1885754_1024x1024.jpeg" target="_blank"><div><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G8SsspgAYEHHiDGNP/h76xj4p8nayrngf1ivnk" alt="图像"></div></figure></div><p> And, sir. The word is and.</p><p> I mean the pictures are pretty damn cool, I do love DALLE-3. Not sure if joking.</p><p> Or rather, not sure exactly how much joking. Not zero. Not completely.</p><h4> Of Course You Know <a href="https://www.youtube.com/watch?v=JaGJQJigXBs&amp;ab_channel=iiAFX" target="_blank" rel="noopener noreferrer nofollow">This Means War</a></h4><p> Release the lawsuits?</p><blockquote><p> <a href="https://twitter.com/Suhail/status/1719577475657920679" target="_blank" rel="noopener noreferrer nofollow">Suhail</a> (Founder, Playground AI): We commit at least $100K to any lawsuit that wants to litigate and get a ruling if a regulatory agency attempts to follow the EO on AI. An EO is not law, yet. We must fight back.</p><p> Pls reach out if you can match or do more so we can establish a fund.</p><p> This fight extends beyond regulatory capture. If you read between the lines, it&#39;s about who owns the oligopoly of superintelligence of our civilization. Nobody should.</p><p> Kevin Fisher (QT of Suhail): I carefully read through the executive order on AI and it&#39;s very clear: it&#39;s a giant document written by OpenAI and a few others outlining a plan for AI to be entirely owned and thought policed by giant corporations with no other alternatives. But there are ways to fight back.</p><p> Kevin Fisher (other thread) I don&#39;t think everyone fully appreciates how dire the fight for “code as speech” is.</p><p> It&#39;s not academic – the outcome will affect the fabric of our society at fundamental level: do we have freedom of thought or will a few corporations censor or effectively moderate our collective internal dialogue?</p><p> Within a few years internal dialog will be externalized through close relationships with AIs powered by language models. We&#39;ve already seen how “safety” limits the ability to explore ideas and concepts on our own terms.</p><p> It&#39;s incredibly important that a totally free market is maintained without regulatory distortion to avoid a future where things like “I&#39;m sorry I can&#39;t talk about that” or “You shouldn&#39;t say that” don&#39;t start policing effectively your personal thoughts.</p><p> ……</p><p> Yeah this is one step further: are you free to say something in private? We&#39;re talking about mass surveillance and monitoring of personal and private speech</p><p> Colin O&#39;Malley (reply to Suhail): What do find objectionable about this EO? Or do you object to any AI regulation, on principle?</p><p> Suhail: Regulating flops is the incorrect strategy.</p></blockquote><p> Again, I ask, then what is the correct strategy?</p><p> Kudos, in a strange way, to Suhail and Marc for focusing on the part that matters most. They very much want to ensure people can do the things that I believe will get us all killed. Namely, they want to race to superintelligence as fast as possible, and put it in the hands of everyone at once. Which, again, I very much expect to highly reliably if implemented cause there not to exist, in the physical universe, any humans, and rather quickly. And I am taking a bold stance that I think this is bad.</p><p> They disagree. Because <a href="https://thezvi.substack.com/p/the-dial-of-progress" target="_blank" rel="noopener noreferrer nofollow">technology, they say, is always good</a> – you might think I am strawmanning, but no, <a href="https://a16z.com/the-techno-optimist-manifesto/" target="_blank" rel="noopener noreferrer nofollow">there&#39;s a manifesto</a> , I&#39;m really really not.</p><p> And they are furious that there might be any sort of future barrier in the way of doing those things as quickly as possible.</p><p> I presume their plan is that someone attempts to file a report, and then Suhail&#39;s group will file a lawsuit that says how dare you attempt to file a report? Or maybe OpenAI or Google files a report saying they are training a model, and Suhail somehow finds standing and files a lawsuit to claim they didn&#39;t have to file the report? I have no idea how the actual suit would work here.</p><p> Kevin is taking the position that any restriction on code whatsoever is unconstitutional and also incompatible with our freedoms, the alternative is the end of our freedom of thought and speech and a mass surveillance state, and all of this is an OpenAI conspiracy.</p><p> It is odd that we both read the same executive order?</p><h4> On Crying Wolf</h4><p> I think this is a good summary of all the reactions above:</p><blockquote><p> Jack (MidJourney): yeah the e/acc response to this has been totally unhinged. this EO seems likely to mildly accelerate deployment of existing systems and have negligible effect on development of new ones. If you&#39;re spending $50m on a single training run a single report is nothing.</p></blockquote><p> Unhinged seems exactly right. The people crying wolf will never not cry wolf. What will they say if and when a wolf actually arrives, that is different from what they are saying now? That is a question such types often ask those claiming risk, both fairly and otherwise. It seems fair to turn it back on them here.</p><p> Roon steps in as another voice of reason.</p><blockquote><p> Roon: all the executive order says as far as I can tell is that you need to report the existence of a massive compute datacenter don&#39;t you need to report the existence of your car and your home to the government? and have them inspect your restaurant&#39;s kitchen and shit.</p><p> this seems kind of a weird thing to freak out over. like they can definitely see it from space and it&#39;s impossible to run one of these without a massive energy footprint.</p></blockquote><p> <a href="https://twitter.com/psychosort/status/1719809201302614492" target="_blank" rel="noopener noreferrer nofollow">Brian Chau is another</a> .</p><blockquote><p> <a href="https://twitter.com/psychosort/status/1719185267100107004" target="_blank" rel="noopener noreferrer nofollow">Brian Chau</a> : The only tangible thing in the Biden executive order seems to be creating a de-facto registry of models and datacenters. Dear lawyer friends: Is any EO authority to make anyone comply with this request?</p><p> Flopcaps are basically fine, it&#39;s everything else the biden admin will use the registry for that&#39;s the problem.</p><p> As I said, the EO is basically fine. Flopcaps are the most reasonable thing you could be up to and even then its just a reporting flopcap.</p></blockquote><p> <a href="https://twitter.com/David_Kasten/status/1719492826721870139" target="_blank" rel="noopener noreferrer nofollow">At most, it was this</a> , at a much slower pace:</p><blockquote><p> Nathan: According to @TheZvi, Biden&#39;s AI order is very light on things that will slow down AI companies unduly. If I understand him correctly the main extra burden falls on bureaucrats to compile reports.</p><p> Dave Kasten: I would characterize it as the bureaucratic equivalent of the bartender theatrically rolling up his sleeves before getting ready to bounce a troublemaker.</p></blockquote><p> Indeed may many other things come to pass in the future, after all those reports are filed. There are many of them you may wish to influence. In the meantime, everyone, please do your best to stay sane.</p><p></p><br/><br/> <a href="https://www.lesswrong.com/posts/G8SsspgAYEHHiDGNP/reactions-to-the-executive-order#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/G8SsspgAYEHHiDGNP/reactions-to-the-executive-order<guid ispermalink="false"> G8SsspgAYEHHiDGNP</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Wed, 01 Nov 2023 20:40:09 GMT</pubDate> </item><item><title><![CDATA[Dario Amodei’s prepared remarks from the UK AI Safety Summit, on Anthropic’s Responsible Scaling Policy]]></title><description><![CDATA[Published on November 1, 2023 6:10 PM GMT<br/><br/><p> <em>I hope Dario&#39;s remarks to the Summit can shed some light on how we think about RSPs in general and Anthropic&#39;s RSP in particular, both of which have been discussed extensively since <a href="https://www.lesswrong.com/posts/6tjHf5ykvFqaNCErH/anthropic-s-responsible-scaling-policy-and-long-term-benefit">I shared our RSP announcement</a> . The full text of Dario&#39;s remarks follows:</em></p><p> Before I get into Anthropic&#39;s <a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy">Responsible Scaling Policy (RSP)</a> , it&#39;s worth explaining some of the unique challenges around measuring AI risks that led us to develop our RSP. The most important thing to understand about AI is how quickly it is moving. A few years ago, AI systems could barely string together a coherent sentence. Today they can pass medical exams, write poetry, and tell jokes. This rapid progress is ultimately driven by the amount of available computation, which is growing by 8x per year and is unlikely to slow down in the next few years. The <em>general</em> trend of rapid improvement is predictable, however, it is actually very difficult to predict when AI will acquire <em>specific</em> skills or knowledge. This unfortunately includes <a href="https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety">dangerous skills</a> , such as the ability to construct biological weapons. We are thus facing a number of potential AI-related threats which, although relatively limited given today&#39;s systems, are likely to become very serious at some unknown point in the near future. This is very different from most other industries: imagine if each new model of car had some chance of spontaneously sprouting a new (and dangerous) power, like the ability to fire a rocket boost or accelerate to supersonic speeds.</p><p> We need both a way to frequently monitor these emerging risks, and a protocol for responding appropriately when they occur. Responsible scaling policies—initially suggested by the Alignment Research Center—attempt to meet this need. Anthropic published its RSP in September, and was the first major AI company to do so. It has two major components:</p><ul><li> First, we&#39;ve come up with a system called AI safety levels (ASL), loosely modeled after the internationally recognized BSL system for handling biological materials. Each ASL level has an if-then structure: if an AI system exhibits certain dangerous capabilities, then we will not deploy it or train more powerful models, until certain safeguards are in place.</li><li> Second, we test frequently for these dangerous capabilities at regular intervals along the compute scaling curve. This is to ensure that we don&#39;t blindly create dangerous capabilities without even knowing we have done so.</li></ul><p> In our system, ASL-1 represents models with little to no risk—for example a specialized AI that plays chess. ASL-2 represents where we are today: models that have a wide range of present-day risks, but do not yet exhibit truly dangerous capabilities that could lead to catastrophic outcomes if applied to fields like biology or chemistry. Our RSP requires us to implement present-day best practices for ASL-2 models, including model cards, external red-teaming, and strong security.</p><p> ASL-3 is the point at which AI models become operationally useful for catastrophic misuse in CBRN areas, as defined by experts in those fields and as compared to existing capabilities and proofs of concept. When this happens we require the following measures:</p><ul><li> Unusually strong security measures such that non-state actors cannot steal the weights, and state actors would need to expend significant effort to do so.</li><li> Despite being (by definition) <em>inherently</em> capable of providing information that operationally increases CBRN risks, the deployed versions of our ASL-3 model must <em>never</em> produce such information, even when red-teamed by world experts in this area working together with AI engineers. This will require research breakthroughs, but we believe it is a necessary condition of safety.</li><li> ASL-4 must be rigorously defined by the time ASL-3 is reached.</li></ul><p> ASL-4 represents an escalation of the catastrophic misuse risks from ASL-3, and also adds a new risk: concerns about autonomous AI systems that escape human control and pose a significant threat to society. Roughly, ASL-4 will be triggered when either AI systems become capable of autonomy at a near-human level, <em>or</em> become the main source in the world of at least one serious global security threat, such as bioweapons. It is likely that at ASL-4 we will require a detailed and precise understanding of what is going on inside the model, in order to make an “affirmative case” that the model is safe. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/vm7FRyPWGCqDHy6LF/jpgsfelnr4o9fakyyy1z" alt="RSP"></p><p> Next, I&#39;ll briefly mention some of our key practices and lessons learned, which we hope are helpful to others in crafting an RSP. First, deep executive involvement is critical. As CEO, I personally spent 10-20% of my time on the RSP for 3 months—I wrote multiple drafts from scratch, in addition to devising and proposing the ASL system. One of my co-founders devoted 50% of their time to developing the RSP for 3 months. Together, this sent a meaningful signal to employees that Anthropic&#39;s leadership team takes the matter of AI safety seriously and is firmly committed to responsible scaling at the frontier.</p><p> Second, make the protocols outlined in the RSP into product and research requirements, such that they become baked into company planning and drive team roadmaps and expansion plans. Set the expectation that missing RSP deadlines directly impacts the company&#39;s ability to continue training models and ship products on time. At Anthropic, teams such as security, trust and safety, red teaming, and interpretability, have had to greatly ramp up hiring to have a reasonable chance of achieving ASL-3 safety measures by the time we have ASL-3 models.</p><p> Third, accountability is necessary. Anthropic&#39;s RSP is a formal directive of its board, which ultimately is accountable to our Long Term Benefit Trust, an external panel of experts with no financial stake in Anthropic. On the operational side, we will put in place a whistleblower policy before we reach ASL-3 and already have an officer responsible for ensuring compliance with the RSP and reporting to our Long Term Benefit Trust. As risk increases, we expect that stronger forms of accountability will be necessary.</p><p> Finally, I&#39;d like to discuss the relationship between RSPs and regulation. RSPs are not intended as a substitute for regulation, but rather a prototype for it. I don&#39;t mean that we want Anthropic&#39;s RSP to be literally written into laws—our RSP is just a first attempt at addressing a difficult problem, and is almost certainly imperfect in a bunch of ways. Importantly, as we begin to execute this first iteration, we expect to learn a vast amount about how to sensibly operationalize such commitments. Our hope is that the general idea of RSPs will be refined and improved across companies, and that in parallel with that, governments from around the world—such as those in this room—can take the best elements of each and turn them into well-crafted testing and auditing regimes with accountability and oversight. We&#39;d like to encourage a “race to the top&#39;&#39; in RSP-style frameworks, where both companies and countries build off each others&#39; ideas, ultimately creating a path for the world to wisely manage the risks of AI without unduly disrupting the benefits.</p><br/><br/> <a href="https://www.lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit<guid ispermalink="false"> vm7FRyPWGCqDHy6LF</guid><dc:creator><![CDATA[Zac Hatfield-Dodds]]></dc:creator><pubDate> Wed, 01 Nov 2023 18:10:31 GMT</pubDate> </item><item><title><![CDATA[AI Alignment: A Comprehensive Survey]]></title><description><![CDATA[Published on November 1, 2023 5:35 PM GMT<br/><br/><p> We have just released an academic survey of AI alignment.</p><p> We identify four main categories of alignment research:</p><ol><li> Learning from feedback (eg scalable oversight)</li><li> Learning under distribution shift</li><li> Assurance (eg interpretability)</li><li> Governance</li></ol><p> We mainly focused on academic references but also included some posts from LessWrong and other forums. We would love to hear from the community about any references we missed or anything that was unclear or misstated. We hope that this can be a good starting point for AI researchers who might be unfamiliar with current efforts in AI alignment.</p><br/><br/> <a href="https://www.lesswrong.com/posts/sL9qmAqgB2RL6JFca/ai-alignment-a-comprehensive-survey#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/sL9qmAqgB2RL6JFca/ai-alignment-a-comprehensive-survey<guid ispermalink="false"> sL9qmAqgB2RL6JFca</guid><dc:creator><![CDATA[Stephen McAleer]]></dc:creator><pubDate> Wed, 01 Nov 2023 17:51:08 GMT</pubDate> </item><item><title><![CDATA[A list of all the deadlines in Biden's Executive Order on AI]]></title><description><![CDATA[Published on November 1, 2023 5:14 PM GMT<br/><br/><p> This is a list of all the deadlines outlined in the Executive Order (EO) published by Biden&#39;s administration on 30.10.2023. I&#39;ve put it together to make it easier for people to track what exactly should be happening when as a result of the EO. The required actions (mainly reports) are grouped by the date of their deadline.</p><h2> <strong>30 Days after the order - 29.11.2023</strong></h2><ul><li> The Secretary of Transportation shall direct the […] (NETT) Council to assess the need for information, technical assistance, and guidance regarding the use of AI in transportation (Sec 8(c)(i))</li><li> The Technology Modernization Board shall consider, […], prioritizing funding for AI projects for the Technology Modernization Fund for a period of at least 1 year. (Sec 10.1(g))</li></ul><h2> <strong>45 Days after the order - 14.12.2023</strong></h2><ul><li> The Secretary of Labor shall publish a request for information (RFI) to solicit public input, […], identifying AI and other STEM-related occupations, […], for which there is an insufficient number of […] United States workers. (Sec 5.1(e))</li><li> The Director of OSTP and the Director of OMB, […], shall identify priority mission areas for increased Federal Government AI talent (Sec 10.2(a))</li><li> The Assistant to the President and Deputy Chief of Staff for Policy, […], shall convene an AI and Technology Talent Task Force (Sec 10.2(b))</li><li> The United States Digital Service, […], shall develop and begin to implement plans to support the rapid recruitment of individuals as part of a Federal Government-wide AI talent surge (Sec 10.2(c))</li></ul><h2> <strong>60 Days after the order - 29.12.2023</strong></h2><ul><li> The Director of OMB shall convene and chair an interagency council to coordinate the development and use of AI in agencies&#39; programs and operations, other than the use of AI in national security systems (Sec 10.1(a))</li><li> The Director of OMB shall develop a method for agencies to track and assess their ability to adopt AI into their programs and operations, manage its risks, and comply with Federal policy on AI (Sec 10.1(c))</li><li> [The Director of OPM shall] conduct an evidence-based review on the need for hiring and workplace flexibility (Sec 10.2(d)(i))</li><li> [The Director of OPM shall] consider authorizing the use of excepted service appointments […] to address the need for hiring additional staff to implement directives of this order; (Sec 10.2(d)(ii)</li></ul><h2> <strong>90 Days after the order - 28.01.2024</strong></h2><ul><li> The Secretary of Commerce shall require companies developing or demonstrating an intent to develop potential dual-use foundation models to provide the Federal Government, on an ongoing basis, with information, reports, or records regarding the following: (Sec 4.2(a))</li><li> (and at least annually thereafter [the 90 days]) The head of each agency with relevant regulatory authority over critical infrastructure and the heads of relevant SRMAs, […], shall evaluate and provide to the Secretary of Homeland Security an assessment of potential risks related to the use of AI in critical infrastructure sectors (Sec 4.3(a)(i))</li><li> The Secretary of State and the Secretary of Homeland Security shall take appropriate steps [to] attract and retain talent in AI] (Sec 5.1(a))</li><li> The Director of NSF [shall] launch a pilot program implementing the National AI Research Resource (NAIRR), consistent with past recommendations of the NAIRR Task Force (Sec 5.2(a)(i))</li><li> The Attorney General shall direct the Assistant Attorney General in charge of the Civil Rights Division to convene a meeting […] to discuss comprehensive use of their respective authorities and offices to:  prevent and address discrimination in the use of automated systems, including algorithmic discrimination; (for more points see Sec 7.1(ii))</li><li> The Secretary of HHS shall, […] establish an HHS AI Task Force that shall, within 365 days of its creation, develop a strategic plan that includes policies and frameworks […] on responsible deployment and use of AI and AI-enabled technologies in the health and human services sector (Sec 8(b)(i))</li><li> The Secretary of Transportation shall direct appropriate Federal Advisory Committees of the DOT to provide advice on the safe and responsible use of AI in transportation. (Sec 8(c)(ii)</li><li> The Secretary of Commerce, […], shall develop guidelines, tools, and practices to support implementation of the minimum risk-management practices […] (Sec 10.1(d)(i))</li><li> The Administrator of General Services, […], shall develop and issue a framework for prioritizing critical and emerging technologies offerings in the Federal Risk and Authorization Management Program authorization process, starting with generative AI offerings (Sec 10.1(f)(ii))</li><li> [The Director of OPM shall] coordinate a pooled-hiring action informed by subject-matter experts and using skills-based assessments to support the recruitment of AI talent across agencies; (Sec 10.2(d)(iii))</li></ul><h2> <strong>120 Days after the order - 27.02.2024</strong></h2><ul><li> The Secretary of Defense, […], shall enter into a contract with the National Academies of Sciences, Engineering, and Medicine to conduct — and submit […] — a study (Sec 4.4(a)(ii) for more)</li><li> The Secretary of State shall consider initiating a rulemaking to establish new criteria to designate countries and skills on the Department of State&#39;s Exchange Visitor Skills List […], including those skills that are critical to the United States;  (Sec 5.1(b))</li><li> The Secretary of Homeland Security, […], shall develop and publish informational resources to better attract and retain experts in AI and other critical and emerging technologiesn(Sec 5.1(g) for more)</li><li> The Secretary of Energy, […], shall, […], establish a pilot program to enhance existing successful training programs for scientists, with the goal of training 500 new researchers by 2025 (Sec 5.2(b))</li><li> The Under Secretary of Commerce for Intellectual Property and [the] USPTO Director shall publish guidance to USPTO patent examiners and applicants addressing inventorship and the use of AI, including generative AI (Sec 5.2(c)(i))</li><li> The Director of NSF, […] shall fund the creation of a Research Coordination Network (RCN) dedicated to advancing privacy research and, in particular, the development, deployment, and scaling of PETs. (Sec 9(c)(i))</li><li> [The Director of OPM shall] issue guidance for agency application of existing pay flexibilities or incentive pay programs for AI, AI-enabling, and other key technical positions to facilitate appropriate use of current pay incentives; (Sec 10.2(d)(iv))</li></ul><h2> <strong>150 Days after the order - 28.03.2024</strong></h2><ul><li> The Secretary of the Treasury shall issue a public report on best practices for financial institutions to manage AI-specific cybersecurity risks. (Sec 4.3(a)(ii) for more)</li><li> The Director of NSF [shall] fund and launch at least one NSF Regional Innovation Engine that prioritizes AI-related work, such as AI-related research, societal, or workforce needs. (Sec 5.2(a)(ii))</li><li> The Director of OMB, […], shall issue guidance to agencies to strengthen the effective and appropriate use of AI… (Sec 10.1(b))</li></ul><h2> <strong>180 Days after the order - 27.04.2024</strong></h2><ul><li> The Secretary of Commerce shall propose regulations that require United States IaaS Providers to ensure that foreign resellers of United States IaaS Products verify the identity of any foreign person that obtains an IaaS account (account) from the foreign reseller (Sec 4.2(d) for more)</li><li> The Secretary of Homeland Security, […], shall incorporate as appropriate the AI Risk Management Framework, […], into relevant safety and security guidelines for use by critical infrastructure owners and operators. (Sec 4.3(a)(iii) for more)</li><li> The Secretary of Defense and the Secretary of Homeland Security shall […] develop plans for, conduct, and complete an operational pilot project to identify, develop, test, evaluate, and deploy AI capabilities, […], to aid in the discovery and remediation of vulnerabilities in critical United States Government software, systems, and networks. (Sec 4.3(b)(ii) for more)</li><li> The Secretary of Homeland Security, […], shall evaluate the potential for AI to be misused to enable the development or production of CBRN threats, while also considering the benefits and application of AI to counter these threats (Sec 4.4(a)(i) for more)</li><li> The Director of OSTP, […], shall establish a framework, incorporating, […], to encourage providers of synthetic nucleic acid sequences to implement […] synthetic nucleic acid procurement screening mechanisms. (Sec 4.4(b)(i) for more)</li><li> The Secretary of Commerce, […], shall initiate an effort to engage with industry and relevant stakeholders, informed by the framework developed under subsection 4.4(b)(i) of this section, to develop and refine for possible use by synthetic nucleic acid sequence providers (Sec 4.4(b)(ii) for more)</li><li> All agencies that fund life-sciences research shall, [..], establish that, as a requirement of funding, synthetic nucleic acid procurement is conducted through providers or manufacturers that adhere to the framework (Sec 4.4(b)(iii) for more)</li><li> The Secretary of Homeland Security shall develop a framework to conduct structured evaluation and stress testing of nucleic acid synthesis procurement screening (Sec 4.4(b)(iv) for more)</li><li> Agencies shall conduct a security review of all data assets in the comprehensive data inventory required […] and shall take steps, […], to address the highest-priority potential security risks that releasing that data could raise with respect to CBRN weapons (Sec 4.7(b))</li><li> The Secretary of State shall consider initiating a rulemaking to expand the categories of nonimmigrants who qualify for the domestic visa renewal program [more] (Sec 5.1(c))</li><li> The Secretary of Homeland Security shall: review and initiate any policy changes the Secretary determines necessary and appropriate to clarify and modernize immigration pathways for experts in AI and other critical and emerging technologies [more] (Sec 5.1(d))</li><li> The Secretary of Homeland Security […] shall develop a training, analysis, and evaluation program to mitigate AI-related IP risks. (Sec 5.2(d))</li><li> The Secretary of Energy […] shall: issue a public report describing the potential for AI to improve planning, permitting, investment, and operations for electric grid infrastructure and to enable the provision of clean, affordable, reliable, resilient, and secure electric power to all Americans; (for 4 more points see Sec 5.2(g))</li><li> The President&#39;s Council of Advisors on Science and Technology shall submit to the President and make publicly available a report on the potential role of AI, […], in research aimed at tackling major societal and global challenges. (Sec 5.2(h))</li><li> The Chairman of the Council of Economic Advisers shall prepare and submit a report to the President on the labor-market effects of AI. (Sec 6(a)(i))</li><li> The Secretary of Labor shall submit to the President a report analyzing the abilities of agencies to support workers displaced by the adoption of AI and other technological advancements. (Sec 6(a)(ii))</li><li> The Secretary of Labor shall […] develop and publish principles and best practices for employers that could be used to mitigate AI&#39;s potential harms to employees&#39; well-being and maximize its potential benefits. (Sec 6(b)(i))</li><li> The Secretary of HHS shall, within 180 days of the date of this order […] publish a plan […], addressing the use of automated or algorithmic systems in the implementation by States and localities of public benefits and services administered by the Secretary (Sec 7.2(b)(i))</li><li> The Secretary of Agriculture shall, […] issue guidance to […] public-benefits administrators on the use of automated or algorithmic systems in implementing benefits or in providing customer support for benefit programs administered by the Secretary (Sec 7.2(b)(ii))</li><li> The Secretary of Housing and Urban Development shall, and the Director of the Consumer Financial Protection Bureau is encouraged to, issue additional guidance (Sec 7.3(c))</li><li> The Secretary of HHS shall direct HHS components, […], to develop a strategy, […], to determine whether AI-enabled technologies in the health and human services sector maintain appropriate levels of quality, including, as appropriate, in the areas described in subsection (b)(i) of this section. (Sec 8(b)(ii))</li><li> The Secretary of HHS shall, […], consider appropriate actions to advance the prompt understanding of, and compliance with, Federal nondiscrimination laws by health and human services providers that receive Federal financial assistance, as well as how those laws relate to AI. (Sec 8(b)(iii))</li><li> The Secretary of Transportation shall direct the Advanced Research Projects Agency-Infrastructure (ARPA-I) to explore the transportation-related opportunities and challenges of AI (Sec 8(c)(iii))</li><li> The Assistant to the President for Economic Policy, and the Director of OSTP, [shall] issue an RFI to inform potential revisions to guidance to agencies on implementing the privacy provisions of the E-Government Act of 2002 (Sec 9(a)(iii))</li><li> The Director of OMB shall develop an initial means to ensure that agency contracts for the acquisition of AI systems and services align with […] and advance the other aims identified in […] (Sec 10.1(d)(ii))</li><li> The Director of the Office of Personnel Management (OPM), […], shall develop guidance on the use of generative AI for work by the Federal workforce. (Sec 10.1(f)(iii))</li><li> The Administrator of General Services, […], shall take steps consistent with applicable law to facilitate access to Federal Government-wide acquisition solutions for specified types of AI services and products (Sec 10.1(h))</li><li> [The Task Force&#39;s purpose will be …], including submitting to the President a report and recommendations for further increasing capacity; (Sec 10.2(b)(i))</li><li> [The Director of OPM shall] establish guidance and policy on skills-based, Federal Government-wide hiring of AI, data, and technology talent (Sec 10.2(d)(v))</li><li> [The Director of OPM shall] establish an interagency working group, staffed with both human-resources professionals and recruiting technical experts, to facilitate Federal Government-wide hiring of people with AI and other technical skills; (Sec 10.2(d)(vi))</li><li> [The Director of OPM shall] review existing Executive Core Qualifications (ECQs) for Senior Executive Service (SES) positions informed by data and AI literacy competencies (Sec 10.2(d)(vii))</li><li> [The Director of OPM shall] complete a review of competencies for civil engineers […], and make recommendations for ensuring that [there is] adequate AI expertise and credentials in these occupations (Sec 10.2(d)(viii))</li><li> The Secretary of Defense shall submit a report to the President (Sec 10.2(h))</li><li> [The Secretary of Commerce shall] submit a report to the President on priority actions taken pursuant to the plan [Sec 11(b)(i)] (Sec 11(b)(ii))</li></ul><h2> <strong>240 Days after the order - 26.06.2024</strong></h2><ul><li> The Assistant to the President for National Security Affairs and the Director of OMB, […], shall coordinate work […] to develop and take steps for the Federal Government to mandate such guidelines, or appropriate portions thereof, through regulatory or other appropriate action. (Sec 4.3(a)(iv) for more)</li><li> The Secretary of Commerce, […], shall submit a report to the Director of OMB and the Assistant to the President for National Security Affairs identifying the existing standards, tools, methods, and practices, as well as the potential development of further science-backed standards and techniques, for: authenticating content and tracking its provenance; labeling synthetic content, such as using watermarking; detecting synthetic content; preventing generative AI from producing child sexual abuse material or producing non-consensual intimate imagery of real individuals (to include intimate digital depictions of the body or body parts of an identifiable individual); testing software used for the above purposes; and auditing and maintaining synthetic content. (Sec 4.5(a))</li><li> The Secretary of Commerce, in coordination with the Director of OMB, shall develop guidance regarding the existing tools and practices for digital content authentication and synthetic content detection measures. (Sec 4.5(b))</li><li> The Director of OMB, […], shall […] issue guidance to agencies for labeling and authenticating such content that they produce or publish. (Sec 4.5(c))</li><li> The Director of NSF shall engage with agencies to identify ongoing work and potential opportunities to incorporate PETs into their operations. (Sec 9(c)(ii))</li></ul><h2> <strong>270 Days after the order - 26.07.2024</strong></h2><ul><li> The Secretary of Commerce, […], shall establish guidelines and best practices, […], for developing and deploying safe, secure, and trustworthy AI systems (Sec 4.1(a) for more)</li><li> The Secretary of Energy, […], shall develop and, […], implement a plan for developing the Department of Energy&#39;s AI model evaluation tools and AI testbeds. (Sec 4.1(b) for more)</li><li> The Secretary of Defense and the Secretary of Homeland Security shall each provide a report to the Assistant to the President for National Security Affairs on the results of actions taken […], including a description of any vulnerabilities found and fixed through the development and deployment of AI capabilities and any lessons learned on how to identify, develop, test, evaluate, and deploy AI capabilities effectively for cyber defense. (Sec 4.3(b)(iii) for more)</li><li> The Secretary of Commerce, […], shall: solicit input from the private sector, academia, civil society, and other stakeholders through a public consultation process on potential risks, benefits, other implications, and appropriate policy and regulatory approaches related to dual-use foundation models for which the model weights are widely available, including… (Sec 4.6)</li><li> The Chief Data Officer Council, [..], shall develop initial guidelines for performing security reviews, including reviews to identify and manage the potential security risks of releasing Federal data that could aid in the development of CBRN weapons as well as the development of autonomous offensive cyber capabilities, while also providing public access to Federal Government data in line with the goals stated in the Open, Public, Electronic, and Necessary Government Data Act (Sec 4.7(a))</li><li> The Assistant to the President for National Security Affairs and the Assistant to the President and Deputy Chief of Staff for Policy shall oversee an interagency process with the purpose of developing and submitting a proposed National Security Memorandum on AI to the President. (Sec 4.8)</li><li> The Under Secretary of Commerce for Intellectual Property and Director of the United States Patent and Trademark Office shall issue additional guidance to USPTO patent examiners and applicants to address other considerations at the intersection of AI and IP, which could include, as the USPTO Director deems necessary, updated guidance on patent eligibility to address innovation in AI and critical and emerging technologies; (Sec 5.2(c)(ii))</li><li> The Under Secretary of Commerce for Intellectual Property and Director of the United States Patent and Trademark Office shall consult with the Director of the United States Copyright Office and issue recommendations to the President on potential executive actions relating to copyright and AI. (Sec 5.2(c)(iii))</li><li> The Attorney General shall […] consider those best practices and the guidance […], and if necessary, develop additional general recommendations for […] law enforcement agencies and criminal justice agencies seeking to recruit, hire, train, promote, and retain highly qualified and service-oriented officers and staff with relevant technical knowledge. (Sec 7.1(c)(ii))</li><li> [The Secretary of Commerce shall] establish a plan for global engagement on promoting and developing AI standards (Sec 11(b)(i))</li><li> The Secretary of Homeland Security, […], shall develop a plan for multilateral engagements to encourage the adoption of the AI safety and security guidelines for use by critical infrastructure owners and operators (Sec 11(d)(i))</li></ul><h2> <strong>365 Days after the order - 30.10.2024</strong></h2><ul><li> The Secretary of Veterans Affairs shall host two 3-month nationwide AI Tech Sprint competitions; (Sec 5.2(f))</li><li> The Attorney General shall, […] submit to the President a report that addresses the use of AI in the criminal justice system (Sec 7.1(b)(i))</li><li> The Attorney General shall review the work conducted pursuant to section 2(b) of Executive Order 14074 and, if appropriate, reassess the existing capacity to investigate law enforcement deprivation of rights under color of law resulting from the use of AI (Sec 7.1(c)(iii))</li><li> The Secretary of Labor shall publish guidance for Federal contractors regarding nondiscrimination in hiring involving AI and other technology-based hiring systems. (Sec 7.3(a))</li><li> The Secretary of HHS shall, […], establish an AI safety program (Sec 8(b)(iv))</li><li> The Secretary of HHS shall develop a strategy for regulating the use of AI or AI-enabled tools in drug-development processes. (Sec 8(b)(v))</li><li> The Secretary of Education shall, [...] develop resources, policies, and guidance regarding AI. (Sec 8(d))</li><li> The Secretary of Commerce, acting through the Director of NIST, shall create guidelines for agencies to evaluate the efficacy of differential-privacy-guarantee protections, including for AI. (Sec 9(b))</li><li> [The Director of OPM shall] implement new ECQs as appropriate in the SES assessment process; (Sec 10.2(d)(vii))</li><li> The Secretary of State and the Administrator of the United States Agency for International Development, […], shall publish an AI in Global Development Playbook (Sec 11(c)(i))</li></ul><h2> <strong>540 Days after the order - 22.04.2025</strong></h2><ul><li> The Director of NSF shall establish at least four new National AI Research Institutes, in addition to the 25 currently funded as of the date of this order. (Sec 5.2(a)(iii))</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/cCbybRT8bgiMbEHEv/a-list-of-all-the-deadlines-in-biden-s-executive-order-on-ai<guid ispermalink="false"> cCbybRT8bgiMbEHEv</guid><dc:creator><![CDATA[Valentin Baltadzhiev]]></dc:creator><pubDate> Wed, 01 Nov 2023 17:18:46 GMT</pubDate></item></channel></rss>