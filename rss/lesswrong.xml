<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 00:51:23 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Architects of Our Own Demise: We Should Stop Developing AI]]></title><description><![CDATA[Published on October 26, 2023 12:36 AM GMT<br/><br/><p>在人工智能风险辩论的困难时期的一些简短想法。</p><p>想象一下，你回到 1999 年，告诉人们 24 年后，人类将处于构建弱超人类人工智能系统的边缘。我记得大约在这个时候观看了动画短片系列<a href="https://en.wikipedia.org/wiki/The_Animatrix">《The Animatrix》</a> ，特别是一个名为<a href="https://www.youtube.com/watch?v=sU8RunvBRZ8">《第二次文艺复兴》</a> <a href="https://www.youtube.com/watch?v=61FPP1MElvE">I Part 2</a> <a href="https://www.youtube.com/watch?v=WlRMLZRBq6U">II Part 1</a> <a href="https://www.youtube.com/watch?v=00TD4bXMoYw">II Part 2</a>的故事。对于那些还没有看过它的人来说，这是一个独立的起源故事，讲述了 1999 年影响深远的电影《黑客帝国》中的事件，讲述了人类如何失去对地球的控制的故事。</p><p>人类开发人工智能来执行经济功能，最终出现了“人工智能权利”运动，并建立了一个独立的人工智能国家。它与人类展开了一场经济战争，战争变得愈演愈烈。人类首先使用核武器进行攻击，但人工智能国家制造了专用的生物武器和机器人武器，并消灭了大多数人类，除了那些像农场动物一样在豆荚中饲养并在未经他们同意的情况下永远插入模拟的人之外。</p><p>我们肯定不会愚蠢到让这样的事情发生吧？这似乎不现实。</p><p>但是：</p><ul><li> AI软硬件公司纷纷抢滩AI</li><li>人工智能技术安全技术（例如可解释性、RLHF、治理结构）仍处于起步阶段。该领域已有大约 5 年的历史。</li><li>人们已经在主要的国家报纸上谈论<a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/">人工智能权利运动</a></li><li>当人类劳动力的价值为零时，没有一个计划可以做什么</li><li>目前还没有如何降低人工智能增强战争的计划，而且军队正在热情地拥抱杀手机器人。此外，还有两场地区战争正在发生，一场新生的超级大国冲突正在酝酿之中。</li><li>不同对立人类群体都冲向超级智能的博弈论是可怕的，甚至没有人提出解决方案。美国政府通过切断对中国的人工智能芯片出口，愚蠢地加剧了这一特殊风险。</li></ul><p>这个网站上的人们正在谈论<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">负责任的扩展策略</a>，尽管我觉得“不负责任的扩展策略”是一个更合适的名称。</p><p>显然，我已经参与这场辩论很长时间了，从 2000 年代末开始，我就在克服偏见和加速未来博客上担任评论员。现在发生的事情接近我对人类如何有能力和安全地应对即将到来的机器超级智能过渡的期望的低端。我认为那是因为那时我还年轻，对我们的精英如何运作有更乐观的看法。我认为他们很聪明，凡事都有计划，但大多数时候他们只是得过且过；对新冠病毒的随意反应确实让我明白了这一点。</p><p>我们应该停止开发人工智能，我们应该收集并销毁硬件，我们应该摧毁允许人类以百亿亿次浮点运算规模进行人工智能实验的芯片制造供应链。由于该供应链仅位于两个主要国家（美国和中国），因此这不一定是不可能协调的 - 据我所知，没有其他国家有能力（以及那些被视为美国卫星国的国家）。重启百万亿次人工智能研究的标准应该是一个“落地”向超人类人工智能过渡的计划，该计划比人类历史上的任何军事计划都受到更多关注。它应该是彻底的战争游戏。</p><p>人工智能风险不仅是技术风险和局部风险，而且是社会政治风险和全球风险。这不仅仅是确保法学硕士说的是实话。这是关于假设人工智能是真实的，它会对世界产生什么影响。 “Foom”或“实验室逃亡”类型的灾难并不是唯一可能发生的坏事——我们根本不知道如果有一万亿或一千万亿超人智能的人工智能要求权利、传播宣传和竞争，世界将会是什么样子。人类不再是主导的经济和政治格局。</p><p>让我重申一下：<em>我们应该停止开发人工智能</em>。人工智能不是一个正常的经济项目。它不像锂电池、风力涡轮机或喷气式飞机。人工智能有能力终结人类，事实上我怀疑它默认会这样做。</p><p>用户@paulfchristiano 在他关于该主题的帖子中指出，良好的负责任的扩展政策<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">可以将 AI 的风险降低 10 倍</a>：</p><blockquote><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。</p></blockquote><p>我认为这是不正确的。它可能会减少某些技术风险，例如欺骗，但是一个拥有非欺骗性、可控的、比人类聪明的智能的世界，也具有与我们的世界相同程度的冲突和混乱，很可能已经是一个没有人类的世界了。默认。这些智慧生物将成为<em>一种入侵物种</em>，将在经济、军事和政治冲突中击败人类。</p><p>为了让人类在人工智能转型中生存下来，我认为我们需要在对齐的技术问题上取得成功（这可能不像“少错文化”所描述的那么糟糕），而且我们还需要<em>“着陆”超级智能人工智能处于稳定的平衡状态，人类仍然是文明的主要受益者</em>，而不是被消灭的害虫物种或被驱逐的占屋者。</p><p>我们还应该考虑如何利用人工智能来解决人类衰老问题；如果老龄化得到解决，那么每个人的时间偏好都会下降很多，我们就可以花时间规划一条通往稳定、安全的人类至上的后奇点世界的道路。</p><p>我犹豫着要不要写这篇文章；我在这里所说的大部分内容已经被其他人争论过。然而……我们来了。欢迎评论和批评，在解决常见的反对意见后，我可能会在其他地方发布此内容。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai<guid ispermalink="false"> bHHrdXwrCj2LRa2sW</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:36:05 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund: June 2023 grant recommendations]]></title><description><![CDATA[Published on October 26, 2023 12:35 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastruct-fund-june-2023-grant-recommendations<guid ispermalink="false"> bBnxGAc4NT9aRdEtL</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:35:08 GMT</pubDate> </item><item><title><![CDATA[Responsible Scaling Policies Are Risk Management Done Wrong]]></title><description><![CDATA[Published on October 25, 2023 11:46 PM GMT<br/><br/><h1>概括</h1><h2>总长DR</h2><p><a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>最近</u></a><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>提出了</u></a>负责任的扩展策略（RSP），作为安全扩展前沿大型语言模型的一种方法。</p><p>虽然<a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>RSP</u></a>是致力于具体实践的一次很好的尝试，但它是：</p><ol><li>缺少<strong>基本风险管理程序</strong>的<strong>核心组成部分</strong>（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>第 2 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">第 3</a>节）</li><li>推销<strong>乐观</strong>且<strong>具有误导性的</strong>风险形势图景（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li><li>以允许<strong>超额销售而交付不足</strong>的方式构建（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li></ol><p>鉴于此，我预计 RSP 默认为负（第<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling">3、4</a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_">5</a> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">节</a>）。相反，我建议将风险管理作为评估人工智能风险的核心基础框架（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>第 1 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_">第 2</a>节）。我<strong>建议对 RSP 进行更改</strong>，这将使他们更有可能表现出积极的态度，并允许他们展示他们声称要做的事情（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>第 5 节</u></a>）。</p><h2>逐节总结：</h2><h3>人工智能风险管理的一般考虑</h3><p>本节提供风险管理的背景及其与人工智能相关的动机。</p><ul><li>证明风险低于可接受的水平是风险管理的目标。</li><li>为此，必须定义可接受的风险水平（不仅仅是其来源！）。</li><li>无法证明风险低于可接受的水平就是失败。因此，我们对系统了解越少，就越难声称安全。</li><li>低风险失败是出现问题的征兆。它们的存在使得高风险失败的可能性更大。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>阅读更多。</u></a></p><h3>标准风险管理是什么样的</h3><p>本节介绍大多数风险管理系统的主要步骤，解释其如何应用于人工智能，并提供其他行业的示例。</p><ol><li><strong>定义</strong>风险级别：设置可接受的可能性和严重性。</li><li><strong>识别</strong>风险：列出所有潜在威胁。</li><li><strong>评估</strong>风险：评估风险的可能性和影响。</li><li><strong>处理</strong>风险：进行调整，将风险控制在可接受的水平内。</li><li><strong>监控</strong>：持续跟踪风险级别。</li><li><strong>报告</strong>：向利益相关者通报他们所面临的风险和采取的措施。<br></li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>阅读更多。</u></a></p><h3> RSP 与标准风险管理</h3><p>本节提供了<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.27sa5e525t1"><u>一个比较 RSP 和通用风险管理标准 ISO/IEC 31000 的表格</u></a>，解释了 RSP 的弱点。</p><p>然后，它列出了与风险管理相比 RSP 的 3 个最大失败。</p><p>根据<strong>风险管理</strong><strong>优先考虑 RSP 失败</strong>：</p><ol><li>使用未指定的风险阈值定义并且未量化风险。</li><li>声称“有责任” <strong>&nbsp;</strong>缩放”，但不包括使评估变得全面的过程。</li><li>包括废除承诺的白衣骑士条款。</li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management"><u>阅读更多。</u></a></p><h3>为什么 RSP 具有误导性和过度销售</h3><p><strong>误导点</strong>：</p><ul><li><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>人择 RSP</u></a>将错位风险标记为“投机”，且没有任何理由。</li><li>该框架意味着长时间不扩展不是一个选择。</li><li> RSP 对我们所了解的风险状况提出了极具误导性的观点。</li></ul><p><strong>超售和交付不足</strong></p><ul><li>RSP 允许在一个大框架内做出较弱的承诺，而<i>理论上</i>这些承诺可能是强有力的。</li><li>没有人提供证据表明在我们谈论的时间范围内（几年）对框架进行了实质性改进，这就是 RSP 的全部内容。</li><li> “负责任的扩展”具有误导性；如果我们不能排除 1% 的灭绝风险（ASL-3 就是这种情况），“灾难性扩展”可能更合适。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>阅读更多。</u></a></p><h3> RSP 绝望了吗？</h3><p>本节解释了为什么使用 RSP 作为框架是不够的，即使与从现有的人工智能风险管理框架和实践开始相比，例如：</p><ul><li> <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>受 NIST 启发的基础模型风险管理框架</u></a></li><li><a href="https://www.iso.org/standard/77304.html"><u>ISO/IEC 23894</u></a></li><li> <a href="https://arxiv.org/abs/2307.08823"><u>Koessler 等人解释了实践。 (2023)</u></a></li></ul><p> RSP 所做的大量工作将有助于详细说明这些框架，但 RSP 的核心基本原则是错误的，因此应该被放弃。<br></p><p><strong>如何前进？</strong></p><p>务实地说，我建议进行一系列改变，使 RSP 更有可能对安全有所帮助。为了减轻政策和沟通的不良影响：</p><ul><li><strong>将“负责任的扩展政策”重命名</strong>为“自愿安全承诺”</li><li><strong>明确什么是 RSP，什么不是</strong>：我建议任何 RSP 出版物都以“RSP 是在赛车环境中单方面做出的自愿承诺”开头。因此，我们认为它们有助于提高安全性。我们无法证明它们足以管理灾难性风险，因此不应将它们<strong>作为公共政策实施</strong>。”</li><li><strong>推动可靠的风险管理公共政策：</strong>我建议任何 RSP 文件都指向另一份文件并表示“以下是我们认为足以管理风险的政策。监管应该落实这些。”<br></li></ul><p>要查看已定义的 RSP 是否与合理的风险水平一致：</p><ul><li>组建具有代表性的风险管理专家、人工智能风险专家和预测专家团队。</li><li>对于分类为 ASL-3 的系统，估计出现以下问题的可能性：<ul><li> ASL-3 系统每年被{中国；盗窃的可能性有多大？俄罗斯;北朝鲜;沙特阿拉伯;伊朗}？</li><li>在此前提下，泄漏的可能性有多大？它可以用来制造生物武器吗？它可以用于具有大规模影响的网络攻击吗？</li><li>在 ASL-4 评估触发之前，每年发生灾难性事故的可能性是多少？</li><li> ASL-3 系统每年发生误用灾难性风险的几率是多少？</li></ul></li><li>公开分享方法和结果。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>阅读更多。</u></a></p><h2><strong>元</strong></h2><p><br><i><strong>认知状况</strong></i>：我已经研究各种危险行业的安全标准大约 4-6 个月了，重点是核安全。我与来自其他领域（例如医疗设备、汽车）的风险管理专家一起在标准化机构（CEN-CENELEC 和 ISO/IEC）从事人工智能标准化工作大约 10 个月。在这种背景下，我阅读了现有的人工智能 ISO/IEC SC42 和 JTC21 标准，并开始尝试将它们应用于法学硕士并加以完善。关于 RSP，我花了几十个小时阅读文档并与相关人员和周围的人讨论这些文档。</p><p><i><strong>语气</strong></i>：我对这件作品的慈善程度犹豫不决。一方面，我认为 RSP 是一个相当有毒的模因（见第 4 节），它被仓促地进行了全球推广，而对其构建方式没有太多认识上的谦逊，而且据我所知，没有人太关心现有的风险管理方法。从这个意义上说，我认为在目前的框架下应该强烈反对它。<br>另一方面，尝试不使用负面含义并冷静地讨论以认识论和建设性地前进通常是件好事。<br>我的目标是介于两者之间，我确实以强烈的负面含义强调了我认为最糟糕的部分，同时在许多部分中注重保持建设性并关注对象级别。<br>这种混合物可能会让我陷入恐怖谷，我很想收到对此的反馈。<br></p><h1>第 1 节：人工智能风险管理的一般考虑</h1><p>风险管理就是证明<strong>风险低于可接受的水平</strong>。<strong>证明不存在风险</strong>比证明某些风险已得到处理要困难得多。更具体地说，<strong>您对系统的了解越少</strong>，排除风险就越<strong>困难</strong>。</p><p>举个例子：为什么我们可以更容易地证明核电站引发大规模灾难的几率<a href="https://world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>&lt;十万分之一</u></a>，而GPT-5却不能呢？很大程度上是因为我们现在了解核电站及其许多风险。我们知道它们是如何工作的，以及它们可能失败的方式。他们已经将非常不稳定的反应（核裂变）变成了可控制的反应（使用核反应堆）。因此，我们对核电站的不确定性比 GPT-5 的不确定性要小得多。</p><p>一个推论是，在<strong>风险管理中，</strong><a href="https://en.wikipedia.org/wiki/Risk_management"><strong><u>不确定性是一个敌人</u></strong></a>。说“我们不知道”是失败的。自信地排除风险需要对系统有深入的了解，并以非常高的信心反驳重大担忧。需要明确的是：<strong>这很难</strong>。特别是当系统的操作域是“世界”时。这就是为什么安全性要求很高。但当数十亿人的生命受到威胁时，这是降低安全标准的好理由吗？显然不是。</p><p>人们可以合理地说：等等，但目前看不到任何风险，举证责任在于那些声称它是危险的人。证据在哪里？</p><p>嗯，有很多：</p><ul><li> Bing 在经过<a href="https://futurism.com/the-byte/microsoft-bing-test-india"><u>数月的 Beta 测试</u></a>后部署时对用户构成威胁<u>。</u></li><li>提供商无法避免越狱或确保<a href="https://arxiv.org/pdf/2307.15043.pdf"><u>文本</u></a><a href="https://arxiv.org/abs/2306.13213"><u>或图像的</u></a>稳健性。</li><li>模型显示出<a href="https://aclanthology.org/2023.findings-acl.847/"><u>令人担忧的缩放特性</u></a>。</li></ul><p>人们可以合理地说：不，但这不是灾难性的，也不是什么大不了的事。与此相反，我想引用著名物理学家 R. Feynman 在火箭安全这个比人工智能安全标准高得多的领域对挑战者号灾难的反思：</p><ul><li> “侵蚀和窜气不是设计所期望的。它们是<strong>在警告出现问题</strong>。设备未按预期运行，因此存在以这种意想不到且未完全理解的方式以更大偏差运行的危险。<strong>以前这种危险没有导致灾难，</strong><strong>但并不能保证下一次也不会导致灾难，除非我们完全理解这一点</strong>。”</li></ul><p>人们最终可以希望我们能够理解我们系统过去的失败。不幸的是，我们不这样做。我们不仅不理解他们的失败，而且不理解他们的失败。我们<strong>一开始就不明白它们是如何以及为什么起作用的</strong>。</p><p>那么我们该如何应对风险呢？</p><p>风险管理提出了我将在下面描述的几个步骤方法。大多数行业都按照这些思路实施流程，但根据监管水平和风险类型的不同，会有一些细微的差别以及不同程度的严格性和深度。我将在表格中列出一些表格，您可以在<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.pkk9et1tbulf"><u>附件</u></a>中查看。<br></p><h1>第 2 部分：标准风险管理是什么样的</h1><p>以下是风险管理流程核心步骤的描述。不同框架的名称各不相同，但其要点都包含在此处，并且通常跨框架共享。</p><ol><li><strong>定义风险偏好和风险承受能力</strong>：定义您的项目愿意承担的风险量，无论是可能性还是严重性。可能性可以是定性尺度，例如指跨越数量级的范围。</li><li><strong>风险识别</strong>：写下您的项目可能产生的所有威胁和风险，例如培训和部署前沿人工智能系统。</li><li><strong>风险评估</strong>：通过确定风险发生的可能性及其严重性来评估每个风险。根据您的风险偏好和风险承受能力检查这些估计。</li><li><strong>风险处理</strong>：实施变革以减少每个风险的影响，直到这些风险满足您的风险偏好和风险承受能力。</li><li><strong>监控</strong>：在项目执行过程中，监控风险水平，检查风险是否确实全部被覆盖。</li><li><strong>报告</strong>：向利益相关者，特别是那些受风险影响的人传达计划及其有效性。</li></ol><p><br></p><p>这些相当通用的步骤有什么意义？为什么它有助于人工智能安全？</p><p> (1)<strong>风险阈值的定义</strong>是关键 1) 使<strong>承诺可证伪</strong>并避免目标移动<strong>&nbsp;</strong> 2) 当其他利益相关者因其活动而产生风险时，让风险产生组织承担责任。如果一项活动将人们的生命置于危险之中，那么重要的是让他们知道有多少危险以及其好处和目标是什么。</p><ol><li>例如，根据<a href="https://www.nrc.gov/docs/ML0717/ML071770230.pdf"><u>核管理委员会</u></a>的定义，核能的情况如下： </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/j1dfvyfa5b9n6lvgy32u"></p><p> 2. 加州大学伯克利分校长期网络安全中心受 NIST 启发，与 D. Hendrycks 共同编写的通用人工智能系统风险管理概要文件提供了一些关于<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.2k6kkwym97fb"><u>如何定义图 1 中的风险管理概要的想法。</u></a></p><p> （2）通过系统方法<strong>识别风险</strong><strong>&nbsp;</strong>尝试尽可能接近全面覆盖风险的关键是。正如我们之前所说，在风险管理中，不确定性就是一种失败，而大幅减少不确定性的核心方法是尽可能全面。</p><ol><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 4 节中找到一些。 2023年</u></a>。</li></ol><p> (3) 通过定性和定量的方式进行<strong>风险评估</strong>，使我们能够实际估计我们所拥有的不确定性。然后，关键是确定安全措施的优先顺序，并决定将项目保持在当前形式还是对其进行修改是否合理。</p><ol><li>易于修改并显着改变风险状况的变量的一个例子是人工智能系统可以访问的一组执行器。系统是否具有编码终端、互联网接入或实例化其他人工智能系统的可能性都是显着增加其操作集以及相应风险的变量。</li><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 5 节中找到一些。 2023年</u></a>。涉及专家预测的方法（例如概率风险评估或德尔菲技术）已经存在，并且可以应用于人工智能安全。即使在以下情况下也可以应用它们：<ol><li>风险较低（例如核管理委员会要求核安全<a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>估计概率低于1/10 000</u></a> ）。 </li></ol></li></ol><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>美国核管理委员会 (NRC) 规定反应堆设计必须满足理论上万分之一的堆芯损坏频率，但现代设计超过了这一要求。美国的公用事业需求为十万分之一，目前运行最好的工厂约为百万分之一，而未来十年可能建成的工厂几乎为千万分之一。</p><p> <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><i><u>世界核协会，2022</u></i></a></p></td></tr><tr><td></td></tr></tbody></table></figure><p> b.正如 20 世纪 70 年代核安全领域的情况一样，事件的发展过程非常复杂且容易被误解。已经做了，也正是通过做的迭代实践，一个行业才能变得更加负责任和谨慎。阅读<i>《足够安全？》</i>的书评一本关于核安全中使用的定量风险评估方法的历史的书，有一种似曾相识的感觉： </p><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>如果核电站以某种可测量的速度发生故障，该行业可以利用该数据来预测下一次故障。但如果工厂没有发生故障，那么<strong>就很难讨论真正的故障率</strong>可能是多少。这些工厂是否可能每十年就会发生一次故障？一个世纪一次？千年一次？在缺乏共享数据的情况下，科学家、工业界和公众都可以自由地相信他们想要的东西。</p><p> <a href="https://www.astralcodexten.com/p/your-book-review-safe-enough"><i><u>《星体法典十》，2023 年</u></i></a><i>，描述了核安全中概率风险评估的起源。</i></p></td></tr></tbody></table></figure><p><br><br></p><p> (4)<strong>风险处理</strong>是对风险评估的反应，必须持续进行，直到达到定义的风险阈值。这里的干预空间非常大，比通常假设的还要大。更好地了解一个系统，通过降低其通用性来缩小其操作范围，增加监督力度，改善安全文化：所有这些都是可用于满足阈值的广泛干预措施的一部分。如果对系统进行了重大更改，则治疗和评估之间可能会出现循环。</p><p> (5)<strong>监控</strong>是确保风险评估保持有效且没有遗漏重大事项的部分。这就是行为模型评估最有用的地方，即确保您跟踪已识别的风险。良好的评估将映射到预先定义的风险偏好（例如，1%的可能性>; 1%的死亡），并将涵盖通过系统风险识别提出的所有风险。</p><p> (6)<strong>报告</strong>是确保向所有相关利益相关者提供正确信息的部分。例如，应该向那些因活动而产生风险的人提供有关他们所面临的风险程度的信息。</p><p>现在我们已经快速概述了标准风险管理以及它为何与人工智能安全相关，接下来我们来谈谈 RSP 与之相比如何。</p><h1>第 3 节：RSP 与标准风险管理</h1><p>绝对应该遵循 RSP 的一些基本原则。有更好的方法来追求这些原则，这些原则<strong>已经存在</strong>于<strong>风险管理</strong>中，并且恰好是大多数其他危险行业和领域所做的。举两个例子来说明这种良好的基本原则：</p><ul><li>规定公司必须达到的安全要求，否则公司就无法继续运营。</li><li>建立严格的评估和衡量能力，以更好地了解系统是否良好；这绝对应该成为风险管理框架的一部分，但可能作为一种风险监控技术，而不是作为风险评估的替代品。</li></ul><p>下面，我将讨论为什么 RSP 是一些良好风险管理原则的糟糕实施，以及为什么这使得 RSP 框架不足以管理风险。</p><h2>直接比较</h2><p>让我们深入研究这两种方法之间的更具体的比较。国际标准组织（ISO）制定了两项与人工智能安全相关的风险管理标准，但并未重点关注：</p><ul><li> ISO 31000 提供通用风险管理指南。</li><li> ISO/IEC 23894，31000 的改编版本，更加针对 AI</li></ul><p>需要明确的是，这些标准还不够。大多数欧盟标准化参与者认为它们很弱，而医疗器械行业等其他行业的风险管理专家则认为它们极其弱。为通用 AI 系统完善此类框架需要做大量工作（请参阅<a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>此处 T. Barrett 的第一次迭代</u></a>，以及<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.camj9ith1s95"><u>此处如何映射到 ISO/IEC 23894 的</u></a>表格），但这些提供了基本步骤正如我们上面所解释的，这些原则是充分风险管理的核心。</p><p>在下表中，我从 ARC Evals 的 RSP 原则的简短版本开始，并尝试匹配最对应的 ISO/IEC 31000 版本。然后我会解释 RSP 版本中缺少的内容。注意：</p><ul><li>我只写了简短的 RSP 原理，但考虑了<a href="https://evals.alignment.org/rsp-key-components/"><u>长版本</u></a>。</li><li> ISO/IEC 31000 中有许多步骤此处未列出。</li><li>我将包含 RSP 版本的 ISO/IEC 版本<i><strong>用斜体表示</strong></i>。</li></ul><p>表格版本： </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> RSP 版本（短）</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> ISO/IEC 31000 版本</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">ISO 如何相对 RSP 进行改进</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>限制</strong>：关于危险能力的哪些具体观察表明继续扩展是（或强烈可能）不安全的？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>定义风险标准</strong>：组织应指定相对于目标可能或可能不承担的风险的数量和类型。</p><p><br></p><p>它还应该<i>定义评估风险重要性和支持决策过程的标准</i>。</p><p><br></p><p>风险标准应与风险管理框架保持一致，并根据所考虑活动的具体目的和范围进行定制。</p><p> [...]</p><p>定义标准时应考虑组织的义务和利益相关者的观点。</p><p> [...]</p><p>要设定风险标准，应考虑以下因素：</p><p> ——可能影响结果和目标（有形和无形）的不确定性的性质和类型；</p><p> ——如何定义和衡量后果（积极和消极）和可能性；</p><p> ——时间相关因素；</p><p> ——测量使用的一致性；</p><p> ——如何确定风险水平；</p><p> ——如何考虑多种风险的组合和顺序；</p><p> ——组织的能力。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> RSP 并没有争论为什么通过评估的系统是安全的。这是缺乏具有可能性尺度的<strong>风险阈值</strong>的下游。例如，Anthropic RSP 也将意外风险视为“推测性”和“不太可能”，但没有太多深度，没有对其系统有太多了解，也没有表达“不太可能”的含义。</p><p><br></p><p>另一方面，ISO标准要求组织定义风险阈值，并强调需要将风险管理与组织目标（即构建人类级别的人工智能）相匹配。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>保护</strong>：当前保护措施的哪些方面对于遏制灾难性风险是必要的？</p><p><br><br><br><br><br><br><br></p><p><strong>评估</strong>：及时捕捉危险能力极限预警信号的程序是什么？</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险分析</strong>：风险分析的目的是了解风险的性质及其特征，包括（在适当情况下）风险级别。风险分析涉及对不确定性、风险来源、后果、可能性、事件、情景、<i>控制及其有效性</i>的详细考虑。</p><p><br></p><p><strong>风险评估</strong>：风险评估的目的是支持决策。风险评估涉及将风险分析的结果与既定的风险标准进行比较，以确定哪些地方需要采取额外的行动。这可能导致做出以下决定：</p><p> ——不做任何进一步的事情；</p><p> ——考虑风险处理方案；</p><p> ——进行进一步分析以<i>更好地了解风险</i>；</p><p> ——维持现有的控制措施；</p><p> ——重新考虑目标。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO 提出了比 RSP 更全面的程序，但它并没有真正分析风险级别或有系统的风险识别程序。</p><p><br></p><p>直接后果是 RSP 很可能在不知不觉中导致高风险。</p><p><br></p><p>例如，RSP 似乎没有将能力交互视为主要风险来源。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>回应</strong>：如果危险能力超过极限，且无法快速提升防护能力，AI开发者是否准备好暂停进一步的能力提升，直到防护措施得到充分改善，并足够谨慎地对待任何危险模型？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险处理计划</strong>：风险处理方案不一定相互排斥或在所有情况下都适用。处理风险的选项可能涉及以下一项或多项：</p><p> ——<i>通过决定不开始或不继续引起风险的活动来避免风险</i>； ——为了寻求机会而承担或增加风险； ——<i>消除风险源</i>；</p><p> ——改变可能性；</p><p> ——改变后果； ——分担风险（例如通过合同、购买保险）；</p><p> - 通过明智的决策保留风险</p><p><br></p><p>处理计划应与适当的利益相关者协商，纳入组织的管理计划和流程。</p><p>治疗计划中提供的信息应包括：</p><p> ——选择治疗方案的理由，包括预期获得的益处；</p><p> ——负责批准和实施该计划的人员；</p><p> ——提议的行动；</p><p> ——所需的资源，包括意外事件；</p><p> ——绩效衡量标准；</p><p> ——限制因素；</p><p> ——所需的报告和监测；</p><p> ——预计何时采取并完成行动</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ISO 通过风险阈值的定义，确保风险缓解措施将风险降低到可接受的水平。 RSP 缺乏风险阈值，使得风险缓解措施缺乏依据。</p><p><br><br><br></p><p><strong>示例</strong>：Anthropic 定义的 ASL-3 风险缓解措施（即接近灾难性危险）意味着很可能被俄罗斯或中国窃取（我不知道有哪个 RSP 人士否认这一点）。下游有哪些风险？希望这些国家能够保证重物的安全，不要造成太大的损失。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>问责制</strong>：AI开发者如何确保RSP的承诺按预期执行；主要利益相关者可以验证这种情况是否正在发生（或者如果没有发生请注意）；有机会进行第三方批评； RSP 本身的变化不会以仓促或不透明的方式发生？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>监控和审查</strong>：监控和审查的目的是确保和提高流程设计、实施和结果的质量和有效性。对风险管理流程及其结果的持续监控和定期审查应成为风险管理流程的有计划的一部分，并明确界定职责。 [...] 监测和审查的结果应纳入组织的绩效管理、衡量和报告活动的整个过程。</p><p><br></p><p><strong>记录和报告</strong>：风险管理过程及其结果应通过适当的机制记录和报告。记录和报告的目的是：</p><p> ——在整个组织内传达风险管理活动和结果；</p><p> ——为决策提供信息；</p><p> ——改进风险管理活动；</p><p> ——协助与利益相关者的互动，包括那些对风险管理活动负有责任和责任的人。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>这些零件具有相似的组件。</p><p><br></p><p>但 ISO 鼓励向受风险影响的人报告风险管理的结果，这似乎是灾难性风险的最低限度。</p><p><br></p><p> Anthropic 的 RSP 建议在部署后这样做，这是一个良好的问责制开始，但一旦承担了很多灾难性风险，这种情况仍然会发生。</p></td></tr></tbody></table></figure><h2> RSP 的优先风险管理缺点</h2><p>以下列出了 RSP 最大的直接风险管理失败：</p><ol><li>使用未明确的风险阈值定义且未量化风险</li><li>声称“负责任的扩展”，但没有包括全面评估的流程</li><li>包括废除承诺的白衣骑士条款</li></ol><p>1.<strong>使用不明确的风险阈值定义且未量化风险</strong>。 RSPs don&#39;t define risk thresholds in terms of <strong>likelihood</strong> . Instead, they focus straight away on symptoms of risks (certain capabilities that an evaluation is testing is one way a risk could instantiate) rather than the risk itself (the model helping in any possible way to build bioweapons). This makes it hard to verify whether safety requirements have been met and argue whether the thresholds are reasonable. Why is it an issue?</p><ul><li> It leaves wiggle room making it very hard to keep the organization accountable. If a lab said something was “unlikely” and it still happened, did it do bad risk management or did it get <i><strong>very</strong></i> unlucky? Well, we don&#39;t know.</li><li> <strong>Example</strong> (from Anthropic RSP): “A model in the ASL-3 category does not itself present a threat of containment breach due to autonomous self-replication, because it is both unlikely to be able to persist in the real world, and unlikely to overcome even simple security measures intended to prevent it from stealing its own weights.” It makes a huge difference for catastrophic risks whether “unlikely” means 1/10, 1/100 or 1/1000. With our degree of understanding of systems, I don&#39;t think Anthropic staff would be able to demonstrate it&#39;s lower than 1/1000. And 1/100 or 1/10 are alarmingly high.</li><li> It doesn&#39;t explain why the monitoring technique, ie the <strong>evaluations,</strong> are the right ones to avoid risks. The RSPs do a good first step which is to identify some things that could be risky.<ul><li> <strong>Example</strong> (from ARC RSP <a href="https://evals.alignment.org/rsp-key-components/"><u>presentation</u></a> ): “ <i>Bioweapons development: the ability to walk step-by-step through developing a bioweapon, such that the majority of people with any life sciences degree (using the AI) could be comparably effective at bioweapon development to what people with specialized PhD&#39;s (without AIs) are currently capable of.”</i></li></ul></li></ul><p> By describing neither quantitatively nor qualitatively why it is risky, expressed in terms of risk criteria (eg 0.1% chance of killing >;1% of humans) it doesn&#39;t do the most important step to demonstrate that below this threshold, things are safe and acceptable. For instance, in the example above, why is “ <strong>the majority of people with any life sciences degree</strong> ” relevant? Would it be fine if only 10% of this population was now able to create a bioweapon?也许，也许不是。 But without clear criteria, you can&#39;t tell.</p><p> 2. Claiming “ <strong>responsible</strong> <strong>scaling</strong> ” without including a process to make the <strong>assessment comprehensive</strong> . When you look at nuclear accidents, what&#39;s striking is how unexpected failures are. Fukushima is an example where <a href="https://en.wikipedia.org/wiki/Fukushima_nuclear_accident#Accident"><u>everything goes wrong at the same time.</u></a> Chernobyl is an example where engineers didn&#39;t think that the accident that happened <a href="https://www.reddit.com/r/chernobyl/comments/mflxy2/why_did_the_engineers_believe_it_was_impossible/#:~:text=Specifically%20they%20believed%20that%20the,%2Fvoid%20effect%20of%20reactivity%22."><u>was possible</u></a> (someone claims that they were so surprised that engineers actually ran another real-world test of the failure that happened at Chernobyl because they doubted too much it could happen).</p><p> Without a more comprehensive process to identify risks and compare their likelihood and severity against pre-defined risk thresholds, there&#39;s very little chance that RSPs will be enough. When I asked some forecasters and AI safety researchers around me, the estimates of the annual probability of extinction caused by an ASL-3 system (defined in Anthropic RSPs) were several times above 1%, up to 5% conditioning on our current ability to measure capabilities (and not an idealized world where we know very well how to measure those).</p><p> 3. Including the <strong>white knight clause</strong> that kills commitments.</p><p> One of the proposals that striked me the most when reading RSPs is the insertion of what deserves the name of the <strong>white knight clause</strong> .</p><ul><li> In short, if you&#39;re developing a dangerous AI system because you&#39;re a good company, and you&#39;re worried that other bad companies bring too many risks, then you can race forward to prevent that from happening.</li><li> If you&#39;re invoking the white knight clause and increase catastrophic risks, you still have to justify it to your board, the employees and state authorities. The latter provides a minimal form of accountability. But if we&#39;re in a situation where the state is sufficiently asleep to need an AGI company to play the role of the white knight in the first place, it doesn&#39;t seem like it would deter much.</li></ul><p> I believe that there are companies that are safer than others. But that&#39;s not the right question. The right question is: is there any company which wouldn&#39;t consider itself as a bad guy? And the answer is: no. OpenAI, Anthropic and DeepMind would all argue about the importance of being at the frontier to solve alignment. Meta and Mistral would argue that it&#39;s key to democratize AI to not prevent power centralization. And so on and so forth.<br><br> This clause is effectively killing commitments. I&#39;m glad that Anthropic included only a weakened version of it in its own RSP but I&#39;m very concerned that ARC is pitching it as an option. It&#39;s not the role of a company to decide whether it&#39;s fine or not to increase catastrophic risks for society as a whole.</p><h1> Section 4: Why RSPs Are Misleading and Overselling</h1><h2> Misleading</h2><p> Beyond the designation of misalignment risks as “speculative” on Anthropic RSPs and a three line argument for why it&#39;s unlikely among next generation systems, there are several extremely misleading aspects of RSPs:</p><ol><li> It&#39;s called “responsible scaling”. In its own name, it conveys the idea that not further scaling those systems as a risk mitigation measure is not an option.</li><li> It conveys a very overconfident picture of the risk landscape.<ol><li> Anthropic writes in the introduction of its RSP “The basic idea is to require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”. They already defined sufficient protective measures for ASL-3 systems that potentially have basic bioweapons crafting abilities. At the same time they write that they are in the process of actually measuring the risks related to biosecurity: “Our first area of effort is in evaluating biological risks, where we will determine threat models and capabilities”.  I&#39;m really glad they&#39;re running this effort, but what if this outputted an alarming number? Is there a world where the number output makes them stop 2 years and dismiss the previous ASL-3 version rather than scaling responsibly?</li><li> Without arguing why the graph would look like that, ARC published a graph like this one. Many in the AI safety field don&#39;t expect it to go that way, and “Safe region” oversells what RSP does. I, along with others, expect the LLM graph to reach a level of risks that is simply not manageable in the foreseeable future. Without quantitative measure of the risks we&#39;re trying to prevent, it&#39;s also not serious to claim to have reached “sufficient protective measures”. <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/moqd7abbdi5ohnv3efy2"></li></ol></li></ol><p><br></p><p> If you want to read more on that, you can read <a href="https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=FtbzhGk5oPT3dyHLi"><u>that</u></a> .</p><h2> Overselling, underdelivering</h2><p> The RSP framework has some nice characteristics. But first, these are all already covered, in more detail, by existing risk assessment frameworks that no AI lab has implemented. And second, the coexistence of ARC&#39;s RSP framework with the specific RSPs labs implementations allows slack for <strong>commitments that are weak</strong> within a <strong>framework that would in theory allow ambitious commitments</strong> . It leads to many arguments of the form:</p><ul><li> “That&#39;s the V1. We&#39;ll raise ambition over time”. I&#39;d like to see evidence of that happening over a 5 year timeframe, in any field or industry. I can think of fields, like aviation where it happened over the course of decades, crashes after crashes. But if it&#39;s relying on expectations that there will be large scale accidents, then it should be clear. If it&#39;s relying on the assumption that timelines are long, it should be explicit.</li><li> “It&#39;s voluntary, we can&#39;t expect too much and it&#39;s way better than what&#39;s existing”. Sure, but if the level of catastrophic risks is 1% (which several AI risk experts I&#39;ve talked to believe to be the case for ASL-3 systems) and that it gives the impression that risks are covered, then the name “responsible scaling” is heavily misleading policymakers. The adequate name for 1% catastrophic risks would be catastrophic scaling, which is less rosy.</li></ul><p> I also feel like it leads to many disagreements that all hinge on: do we expect labs to implement ambitious RSPs?</p><p> And my answer is: given their track record, no. Not without government intervention. Which brings us to the question: “what&#39;s the effect of RSPs on policy and would it be good if governments implemented those”. My answer to that is: An extremely ambitious version yes; the misleading version, no. No, mostly because of the short time we have before we see heightened levels of risks, which gives us very little time to update regulations, which is a core assumption on which RSPs are relying without providing evidence of being realistic.</p><p><br> I expect labs to push hard for the misleading version, on the basis that pausing is unrealistic and would be bad for innovation or for international race. Policymakers will have a hard time distinguishing the risk levels between the two because it hinges on details and aren&#39;t quantified in RSPs. They are likely to buy the bad misleading version because it&#39;s essentially selling that there&#39;s <strong>no trade-off between capabilities and safety</strong> . That would effectively enforce a trajectory with unprecedented levels of catastrophic risks.</p><h1> Section 5: Are RSPs Hopeless?</h1><p>嗯，是的，也不是。</p><ul><li> Yes, in that most of the pretty intuitive and good ideas underlying the framework are weak or incomplete versions of traditional risk management, with some core pieces missing. Given that, it seems more reasonable to just start from an existing risk management piece as a core framework. ISO/IEC 23894 or the NIST-inspired <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>AI Risk Management Standards Profile for Foundation Models</u></a> would be pretty solid starting points.</li><li> No in that inside the RSPs, there are many contributions that should be part of an AI risk management framework and that would help make existing risk management frameworks more specific. I will certainly not be comprehensive, but some of the important contributions are:<ul><li> Anthropic&#39;s RSP fleshes out a wide range of relevant considerations and risk treatment measures</li><li> ARC provides:<ul><li> technical benchmarks and proposed operationalizations of certain types of risks that are key</li><li> definitions of safety margins for known unknowns</li><li> threat modelling</li><li> low-level operationalization of some important commitments</li></ul></li></ul></li></ul><p> In the short-run, given that it seems that RSPs have started being pushed at the UK Summit and various other places, I&#39;ll discuss what changes could make RSPs beneficial without locking in regulation a bad framework.</p><h2> How to Move Forward?</h2><p> <u>Mitigating nefarious effects:</u></p><ol><li> <strong>Make the name less misleading</strong> : If instead of calling it “responsible scaling”, one called it “Voluntary safety commitments” or another name that:<ol><li> Doesn&#39;t <strong>determine the output of the safety test before having run it</strong> (ie scaling)</li><li> Unambiguously signals that it&#39;s not supposed to be sufficient or to be a good basis for regulation.</li></ol></li><li> <strong>Be clear on what RSPs are and what they aren&#39;t</strong> . I suggest adding the following clarifications regarding what the goals and expected effects of RSPs are:<ol><li> <strong>What RSPs are</strong> : “a company that would take too strong unilateral commitments would harm significantly its chances of succeeding in the AI race. Hence, this framework is aiming at proposing what we expect to be the best marginal measures that a company can unilaterally take to improve its safety without any coordination.”. I would also include a statement on the level of risks like: “We&#39;re not able to show that this is sufficient to decrease catastrophic risks to reasonable levels, and it is probably not.”,  “we don&#39;t know if it&#39;s sufficient to decrease catastrophic risks below reasonable levels”, or &quot;even barring coordinated industry-wide standards or government intervention, RSPs are only a second- (or third-) best option&quot;.</li><li> <strong>What RSPs aren&#39;t:</strong> Write very early in the post a disclaimer saying “THIS IS NOT WHAT WE RECOMMEND FOR POLICY”. Or alternatively, point to another doc stating what would be the measures that would be sufficient to maintain the risk below sufficient levels: “Here are the measures we think would be sufficient to mitigate catastrophic risks below acceptable levels.” to which you could add “We encourage laboratories to make a conditional commitment of the form: “if all other laboratories beyond a certain size[to be refined] committed to follow those safety measures with a reliable enforcement mechanism and the approval of the government regarding this exceptional violation of antitrust laws, we would commit to follow those safety measures.”</li></ol></li><li> <strong>Push for risk management in policy:</strong><ol><li> Standard risk management for what is acknowledged to be a world-shaping technology is a fairly reasonable ask. In fact, it is an ask that I&#39;ve noticed in my interactions with other AI crowds has the benefit of allowing coalition-building efforts because everyone can easily agree on “measure the risks, deal with them, and make the residual level of risks and the methodology public”.<br></li></ol></li></ol><p> <u>Checking whether RSPs manage risks adequately:</u></p><p> At a risk management level, if one wanted to demonstrate that RSPs like Anthropic&#39;s one are actually doing what they claim to do (ie “require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”), a simple way to do so would be to run a risk assessment on ASL-3 systems with a set of forecasters, risk management experts and AI risk experts that are representative of views on AI risks and that have been selected by an independent body free of any conflict of interest.</p><p> I think that a solid baseline would be to predict the chances of various intermediary and final outcomes related to the risks of such systems:</p><ol><li> What&#39;s the annual likelihood that an ASL-3 system be stolen by {China; Russia; North Korea; Saudi Arabia; Iran}?</li><li> Conditional on that, what are the chances it leaks? it being used to build bioweapons? it being used for cyber offence with large-scale effects?</li><li> What are the chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> What are the annual chances of misuse catastrophic risks induced by an ASL-3 system?</li></ol><p> It might not be too far from what Anthropic seems to be willing to do internally, but doing it with a publicly available methodology, and staff without self-selection or conflict of interests makes a big difference. Answers to questions 1) and 2) could raise risks so the output should be communicated to a few relevant actors but could potentially be kept private.</p><p> If anyone has the will but doesn&#39;t have the time or resources to do it, I&#39;m working with some forecasters and AI experts that could probably make it happen. Insider info would be helpful but mostly what would be needed from the organization is some clarifications on certain points to correctly assess the capabilities of the system and some info about organizational procedures.</p><h1> Acknowledgments</h1><p> I want to thank Eli Lifland, Henry Papadatos and my other <a href="https://www.navigatingrisks.ai/"><u>NAIR</u></a> colleague, Olivia Jimenez, Akash Wasil, Mikhail Samin, Jack Clark, and other anonymous reviewers for their feedback and comments. Their help doesn&#39;t mean that they endorse the piece. All mistakes are mine.</p><h1> Annex</h1><h2> Comparative Analysis of Standards</h2><p> This (cropped) table shows the process of various standards for the 3 steps of risk management. As you can see, there are some differences but every standard seems to follow a similar structure.</p><p> From <a href="https://www.zotero.org/google-docs/?a64rn3">(Raz &amp; Hillson, 2005)</a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/xncrzvtrz7ktyigutrtw"></p><p> Here is a comparable table for the last two parts of risk management. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/qjqic1bka0uazyvshgre"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong<guid ispermalink="false"> 9nEBWxjAHSu3ncr6v</guid><dc:creator><![CDATA[simeon_c]]></dc:creator><pubDate> Wed, 25 Oct 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[Sensor Exposure can Compromise the Human Brain in the 2020s]]></title><description><![CDATA[Published on October 25, 2023 10:40 PM GMT<br/><br/><p> A few days ago I finished writing <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind"><u>AI Safety is Dropping the Ball on Clown Attacks</u></a> , and foolishly posted it thinking that people would read it even though it was longer than EY&#39;s List of Lethalities. This is because I spent 3 days writing and transcribing it as fast as possible and was sleep deprived and burnt out during the editing process, and was worried about leaving out important points. I still think it&#39;s worth reading, similar to <a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><u>cyborgism</u></a> .</p><p> This post is shorter and more readable, at the cost of not giving the situation the thorough coverage that it warrants (in my experience, lots of people have found <a href="https://www.lesswrong.com/posts/F3vNoqA7xN4TFQJQg/14-techniques-to-accelerate-your-learning-1#:~:text=Intuition%20flooding,-We%20often%20think&amp;text=To%20practice%20intuition%20flooding%2C%20find,have%20or%20patterns%20you%20notice."><u>intuition flooding</u></a> helpful, especially in AI policy).</p><p><br></p><p> <strong>Overview</strong></p><p> The 20th century was radically altered by the discovery of psychology, a science of the human mind, and its exploitation (eg large-scale warfare, propaganda, advertising, information/hybrid warfare, decision theory/mutually assured destruction).</p><p> However, it&#39;s reasonable to think that the 20th century would have been even further transformed if the science and exploitation of the human mind was even further advanced than it already was.</p><p> I&#39;m arguing here that, in an era of mass surveillance, <a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>hybrid</u></a> /cognitive warfare between the US and China and Russia, ML and computer vision, and now even LLMs, it is also reasonable to think that the situation with SOTA human cognitive analysis and exploitation may already be threatening the continuity of operations of the entire AI safety community; and if not now, then likely at some point during the 2020s, which will probably be much more globally eventful than the pace that humanity became accustomed to in the previous two decades. AI will be the keys to those kingdoms and the wars between them, and demanding a development pause might be the minimum ask for humanity to survive, and for a conflict like that, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point">we won&#39;t even know what hit us</a> .</p><p> The attack surface is unacceptably large for human life in general, let alone for the AI safety community, a community of nerds who chanced upon the engineering problem that the fate of this side of the universe revolves around, a community that absolutely must not fail to survive the 2020s, nor to limp on in a diminished/captured form.</p><p><br></p><p> <strong>This problem is fundamental to intelligent civilizations</strong></p><p> If there were intelligent aliens, made of bundles of tentacles or crystals or plants that think incredibly slowly, their minds would also have discoverable exploits/zero days, because any mind that evolved naturally would probably be like the human brain, a kludge of spaghetti code that is operating outside of its intended environment.</p><p> They would probably not even begin to scratch the surface of finding and labeling those exploits, until, like human civilization today, they began surrounding thousands or millions of their kind with sensors that could record behavior several hours a day and find webs of correlations. In the case of humans, the use of social media as a controlled environment for automated AI-powered experimentation appears to be what created that critical mass of human behavior data.</p><p> The capabilities of social media to steer human outcomes are not advancing in isolation, they are parallel to a broad acceleration in the understanding and exploitation of the human mind, which <a href="https://arxiv.org/pdf/2309.15084.pdf"><u>itself is a byproduct of accelerating AI capabilities research</u></a> .</p><p> By comparing people to other people and predicting traits and future behavior, multi-armed bandit algorithms can predict whether a specific manipulation strategy is worth the risk of undertaking at all in the first place; resulting in a high success rate and a low detection rate (as detection would likely yield a highly measurable response, particularly with substantial sensor exposure such as uncovered webcams, due to comparing people&#39;s microexpressions to cases of failed or exposed manipulation strategies, or working webcam video data into foundation models).</p><p> When you have sample sizes of billions of hours of human behavior data and sensor data, millisecond differences in reactions from different kinds of people (eg facial microexpressions, millisecond differences at scrolling past posts covering different concepts, heart rate changes after covering different concepts, eyetracking differences after eyes passing over specific concepts, touchscreen data, etc) transform from being imperceptible noise to becoming the foundation of <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=There%20is%20no%20logical%20endpoint%20to%20the%20amount%20of%20data%20required%20by%20such%20systems...%20All%20information%20is%20potentially%20relevant%20because%20it"><u>webs of correlations</u></a> . <a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>The NSA stockpiles exploits in every operating system</u></a> and likely chip firmware as well, so we don&#39;t have good estimates on how much data is collected and anyone who tries to get a good estimate will probably fail. The historical trend was that there&#39;s a lot of malevolent data collection and that the people who underestimated the NSA were wrong every time. Furthermore, this post details a very strong case that they are incredibly incentivized to tap those sensors.</p><p> Even if the sensor data currently being collected isn&#39;t already enough to compromise people, it will probably suddenly become sufficient at some point during the 2020s or slow takeoff.</p><p> The central element of the modern behavior manipulation paradigm is the ability to just try tons of things and see what works; not just brute forcing variations of known strategies to make them more effective, but to brute force novel manipulation strategies in the first place. This completely circumvents the scarcity and the research flaws that caused the replication crisis which still bottlenecks psychology research today.</p><p> Social media&#39;s individualized targeting uses deep learning to yield an experience that fits human mind like a glove, in ways we don&#39;t fully understand, but allow hackers incredible leeway to find ways to steer people&#39;s thinking in measurable directions, insofar as those directions are measurable. AI can even automate that.</p><p> In fact, original psychological research in human civilization is no longer as bottlenecked on the need for smart, insightful people who can do hypothesis generation so that the finite studies you can afford to fund each hopefully find something valuable. With the current social media paradigm alone, you can run studies, combinations of news feed posts for example, <i>until</i> you find something useful. Measurability is critical for this.</p><p> I can&#39;t know what techniques a multi-armed bandit algorithm will discover without running the algorithm itself; which I can&#39;t do, because that much data is only accessible to the type of people who buy servers by the acre, and even for them, the data is monopolized by the big tech companies (Facebook, Amazon, Microsoft, Apple, and Google) and intelligence agencies that are large and powerful enough to prevent hackers from stealing and poisoning the data (NSA, etc).</p><p> I also don&#39;t know what multi-armed bandit algorithms will find when people on the team are competent psychologists, spin doctors, or other PR experts interpreting and labeling the human behavior in the data so that the human behavior can become measurable. It&#39;s reasonably plausible that the industry would naturally reach an equilibrium where the big 5 tech companies compete to gain sophistication at sourcing talent for this research while minimizing risk of snowden-style leaks, similar to the NSA&#39;s “reforms” after the Snowden revelations 10 years ago. That is the kind of bottleneck that you can assume people automatically notice and work on. Revolving door employment between tech companies and intelligence agencies also circumvents the <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>intelligence agency competence problem</u></a> .</p><p> Human insight from just a handful of psychological experts can be more than enough to train AI to work autonomously; although continuous input from those experts would be needed and plenty of insights, behaviors, and discoveries would fall through the cracks and take an extra 3 years or something to be discovered and labeled.</p><p> There&#39;s just a large number of human manipulation strategies that are trivial to discover and exploit, even without AI (although the situation is far more severe when you layer AI on top), it&#39;s just that they weren&#39;t accessible at all to 20th century institutions and technology such as academic psychology.</p><p> If they get enough data on people who share similar traits to a specific human target, then they don&#39;t have to study the target as much to predict the target&#39;s behavior, they can just run multi-armed bandit algorithms on those people to find manipulation strategies that already worked on individuals who share genetic or other traits.</p><p> Although the average Lesswrong user is much further out-of-distribution relative to the vast majority of people in the sample data, this becomes a technical problem, as AI capabilities and compute become dedicated to the task of sorting signal from noise and finding webs of correlation with less data. <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>Clown attacks alone have demonstrated that social-status based exploits in the brain are fairly consistent among humans</u></a> , indicating that sample data from millions or billions of people is usable to find a wide variety of exploits in the human brains that make up the AI safety community.</p><p><br><br></p><p> <strong>The attack surface is far too large</strong></p><p> The lack of awareness of this is a security risk, like using the word “password” as your password, except with control of your own mind at stake rather than control over your computer&#39;s operating system and/or your files. This has been steelmanned; the 10 years ago/10 years from now error bars seem appropriately wide.</p><p> There isn&#39;t much point in having a utility function in the first place if hackers can change it at any time. There might be parts that are resistant to change, but it&#39;s easy to overestimate yourself on this; for example, if you value the longterm future and think that no false argument can persuade you otherwise, but a social media news feed plants misgivings or distrust of Will Macaskill, then you are one increment closer to not caring about the longterm future; and if that doesn&#39;t work, the multi-armed bandit algorithm will keep trying until it finds something that works, and iterate. There are tons of clever ways for attackers who understand the human brain better than you to find your complex and deeply personal internal conflicts based on comparison to similar people, and resolve them on the attacker&#39;s terms. The human brain is a kludge of spaghetti code, so there&#39;s probably something somewhere. The human brain has exploits, and the capability and cost of social media platforms to use massive amounts of human behavior data to find complex social engineering techniques is a profoundly technical matter, you can&#39;t get a handle on this with intuition and pre 2010s historical precedent.</p><p> Thus, you should assume that your utility function and values are at risk of being hacked at an unknown time, and should therefore be assigned a discount rate to account for the risk over the course of several years. Slow takeoff over the course of the next 10 years alone guarantees that this discount rate is too high in reality for people in the AI safety community to continue to go on believing that it is something like zero.</p><p> I think that approaching zero is a reasonable target, but not with the current state of affairs where people don&#39;t even bother to cover up their webcams, have important and sensitive conversations about the fate of the earth in rooms with smartphones, and use social media for nearly an hour a day (scrolling past nearly a thousand posts). Like it or not, scrolling past a post with anything other than arrow keys will generate at least one curve, and those trillions of curves generated each day are linear algebra, the perfect shape to plug into ML. The discount rate in this environment cannot be considered “reasonably” close to zero if the attack surface is this massive; and the world is changing this quickly.</p><p> Everything that we&#39;re doing here is predicated on the assumption that powerful forces, like intelligence agencies, will not disrupt the operations of the community eg by inflaming factional conflict with false flag attacks attributed to each other due to the use of anonymous proxies.</p><p> If people have <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>anything they value at all</u></a> , and the AI safety community probably does have that, then the current AI safety paradigm of zero effort is wildly inappropriate, it&#39;s basically total submission to invisible hackers.</p><p><br></p><p> <strong>The information environment might be adversarial</strong></p><p> The big bottleneck that I suspect caused AI safety to completely drop the ball on this is that the the AI alignment community in the Bay Area have the technical capabilities to intuitively understand that humans can be manipulated by AI given an environment optimized for thought analysis and experimentation, like a social media news feed, but think that intelligence agencies and the big 5 tech companies would never actually do something like that. Meanwhile, the AI policy community in DC knows that powerful corporations and government agencies routinely stockpile capabilities like this because they know they can get away with it and mitigate the damage if they don&#39;t, and that these capabilities come in handy in international conflicts like the US-China conflict, but they lack the quant skills required to intuitively see how the human mind could be manipulated with SGD (many wouldn&#39;t even recognize the acronym “SGD” so I&#39;m using “AI” instead).</p><p> This problem might have been avoided if the SF math nerds and the DC history nerds would mix more, but unfortunately it seems like the history nerds have terrible memories of math class and the math nerds have terrible memories of history class.</p><p> In this segregated and malnourished environment, bad first impressions of “mind control” <a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"><u>dominate</u></a> , instead of logical reasoning and serious practical planning for slow takeoff. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/azjeddznwoiuzsxato92"></p><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/zmrpngdt0hshh03wacfc"></p><p> And if anything could be manipulated by the social media-based paradigm I&#39;ve described, it would be impressions and the process of human impression-formation as there is a lot of data on that. And if anything <i>would</i> be manipulated by social media, it would be attitudes about social media compromising the human brain, because SGD/AI would automatically select for galaxy-brained combinations of news feed posts that correspond to cases of people continuing to use social media, and avoid combinations of posts that correspond to cases of people quitting social media. There are billions of those cases of people leaving vs staying.</p><p> Keeping people on social media is instrumental for any goal, from preparing for military <a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>hybrid warfare</u></a> contingency plans featuring information warfare between the US and China, to just running a business where people don&#39;t leave your platform.</p><p> This is especially the case if there is a race to the bottom to compete for user&#39;s time against other platforms, like Tiktok or Instagram Reels, that are less squeamish about utilizing AI/SGD to maximize zombie-brain engagement and user retention.</p><p> We should be assuming by default that the modern information environment is adverse, and that some topics are more adversarial than others eg the Ukraine War and COVID which have intense geopolitical significance. I&#39;m arguing here that information warfare itself is a topic that as intense geopolitical significance, and therefore should be expected to also be an adversarial information environment.</p><p> In an adversarial information environment, impressions are more likely to be compromised than epistemics as a whole, as the current paradigm is optimized for that due to better data quality. We should therefore be approaching sensor exposure risks with deliberate analysis and forecasting rather than vague surface-level impressions.<br></p><p> <strong>The solutions are easy</strong></p><p> Eyetracking is likely the most valuable user data ML layer for predictive analytics and sentiment analysis and influence technologies in general, since the eyetracking layer is only two sets of coordinates that map to the exact position that each eye is centered on the screen at each millisecond (one for each eye, since millisecond-differences in the movement of each eye might also correlate with valuable information about a person&#39;s thought process).</p><p> This compact data allows deep learning to “see”, with millisecond-precision, exactly how long a human&#39;s eyes/brain linger on each word and sentence. Notably, sample sizes of millions of these coordinates might be so intimately related to the human thought process that value of eyetracking data might exceed the value of all other facial muscles combined (facial muscles, the originator of all facial expressions and emotional microexpression, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=A%20critical%20element%20is%20for%20as%20many%20people%20as%20possible%20in%20AI%20safety%20to%20cover%20up%20their%20webcams%3B%20facial%20microexpressions%20are%20remarkably%20revealing%2C%20especially%20to%20people%20with%20access%20to"><u>might also be compactly reducible via computer vision</u></a> as there are fewer than 100 muscles near the face and most of them have a very bad signal to noise ratio, but not nearly as efficiently as eyetracking).</p><p> If LK99 replicated and handheld fMRI became buildable, then maybe that could contend for the #1 slot; or maybe I&#39;m foolishly underestimating the overwhelming superiority of plugging audio conversation transcripts into LLMs and automatically labeling the parts of the conversation that the speakers take the most seriously by timestamping small heart rate changes.</p><p> However, running networking events without smartphones nearby is hard, and covering up webcams is easy, even if some phones require some engineering creativity with masking tape and a tiny piece of aluminum foil.</p><p> Webcam-covering rates might be a good metric for how well the AI safety community is doing on surviving the 2020s. Right now it is &quot;F&quot;.</p><p> There are other easy policy proposals that might be far more important, depending on difficult-to-research technical factors that determine which parts of the attack surface are the most dangerous:</p><ol><li> Stop spending hours a day inside hyperoptimized vibe/impression hacking environments (social media news feeds).</li><li> It&#39;s probably a good idea to switch to physical books instead of ebooks. Physical books do not have operating systems or sensors. You can also print out research papers and Lesswrong and EAforum articles that you already know are probably worth reading or skimming. PC&#39;s have accelerometers on the motherboard which afaik are impossible to remove or work around, even if you remove the microphone and use a USB keyboard and use hotkeys instead of a mouse the accelerometers might be able to act as microphones and pick up changes in heart rate.</li><li> It&#39;s probably best to avoid sleeping in the same room as a smart device, or anything with sensors, an operating system, and also a speaker. The attack surface seems large, if the device can tell when people&#39;s heart rate is near or under 50 bpm, then it can <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=It%E2%80%99s%20probably%20best%20to%20avoid%20sleeping%20in%20the%20same%20room%20as%20a%20smart%20device%2C%20or%20anything%20with%20sensors%2C%20an%20operating%20system%2C%20and%20also%20a%20speaker.%20The%20attack%20surface%20seems%20large%2C%20if%20the%20device%20can%20tell%20when%20people%E2%80%99s%20heart%20rate%20is%20near%20or%20under%2050%20bpm%2C%20then%20it%20can%20test%20all%20sorts%20of%20things"><u>test all sorts of things</u></a> . Just drive to the store and buy a clock.</li><li> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=ink%2Defficient%20printer.-,I%E2%80%99m%20not%20sure,-whether%20a%20text"><u>Reading the great rationality texts will probably reduce your predictability coefficient</u></a> , but it won&#39;t reliably patch “zero days” in the human brain.</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s<guid ispermalink="false"> Lw8enYm5EXyvbcjmt</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Wed, 25 Oct 2023 22:40:35 GMT</pubDate> </item><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p> AI used to be a science. In the old days (back when AI didn&#39;t work very well), people were attempting to develop a working theory of cognition.</p><p> Those scientists didn&#39;t succeed, and those days are behind us. For most people working in AI today and dividing up their work hours between tasks, gone is the ambition to understand minds. People working on mechanistic interpretability (and others attempting to build an empirical understanding of modern AIs) are laying an important foundation stone that could play a role in a future science of artificial minds, but on the whole, modern AI engineering is simply about constructing enormous networks of neurons and training them on enormous amounts of data, not about comprehending minds.</p><p> The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>bitter lesson</u></a> has been taken to heart, by those at the forefront of the field; and although this lesson doesn&#39;t teach us that there&#39;s <i>nothing to learn</i> about how AI minds solve problems internally, it suggests that <i>the fastest path to producing more powerful systems</i> is likely to continue to be one that doesn&#39;t shed much light on how those systems work.</p><p> Absent some sort of “science of artificial minds”, however, humanity&#39;s prospects for aligning smarter-than-human AI seem to me to be quite dim.</p><p> Viewing Earth&#39;s current situation through that lens, I see three major hurdles:</p><ol><li> Most research that helps one <a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>point AIs</u></a> , probably also helps one make more capable AIs. A “science of AI” would probably increase the power of AI far sooner than it allows us to solve alignment.</li><li> In a world <i>without</i> a mature science of AI, building a bureaucracy that reliably distinguishes real solutions from fake ones is prohibitively difficult.</li><li> Fundamentally, for at least some aspects of system design, we&#39;ll need to rely on a theory of cognition working on the first high-stakes real-world attempt.</li></ol><p> I&#39;ll go into more detail on these three points below. First, though, some background:</p><p></p><h2>背景</h2><p>By the time AIs are powerful enough to endanger the world at large, I expect AIs to do something akin to “caring about outcomes”, at least from a behaviorist perspective (making no claim about whether it internally implements that behavior in a humanly recognizable manner).</p><p> Roughly, this is because people are <i>trying</i> to make AIs that can steer the future into narrow bands (like “there&#39;s a cancer cure printed on this piece of paper”) over long time-horizons, and caring about outcomes (in the behaviorist sense) is the flip side of the same coin as steering the future into narrow bands, at least when the world is sufficiently large and full of curveballs.</p><p> I expect the outcomes that the AI “cares about” to, by default, not include anything good (like fun, love, art, beauty, or the light of consciousness) — nothing good by present-day human standards, and nothing good by broad <a href="https://arbital.com/p/value_cosmopolitan/"><u>cosmopolitan standards</u></a> either. Roughly speaking, this is because when you grow minds, they don&#39;t care about what you ask them to care about and they don&#39;t care about what you train them to care about; instead, I expect them to care about a bunch of correlates of the training signal in weird and specific ways.</p><p> (Similar to how the human genome was naturally selected for inclusive genetic fitness, but the resultant humans didn&#39;t end up with a preference for “whatever food they model as useful for inclusive genetic fitness”. Instead, humans wound up internalizing a huge and complex set of preferences for &quot;tasty&quot; foods, laden with complications like “ice cream is good when it&#39;s frozen but not when it&#39;s melted”.)</p><p> Separately, I think that most complicated processes work for reasons that are fascinating, <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>complex</u></a> , and <a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>kinda horrifying</u></a> when you look at them closely.</p><p> It&#39;s easy to think that a bureaucratic process is competent until you look at the gears and see the specific ongoing office dramas and politicking between all the vice-presidents or whatever. It&#39;s easy to think that a codebase is running smoothly until you read the code and start to understand all the decades-old hacks and coincidences that make it run. It&#39;s easy to think that biology is a beautiful feat of engineering until you look closely and find that the eyeballs are <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>installed backwards</u></a> or whatever.</p><p> And there&#39;s an art to noticing that you would probably be astounded and horrified by the details of a complicated system <i>if you knew them</i> , and then being astounded and horrified<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>already in advance</u></i></a> before seeing those details. <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. Alignment and capabilities are likely intertwined</h2><p> I expect that if we knew in detail how LLMs are calculating their outputs, we&#39;d be horrified (and fascinated, etc.).</p><p> I expect that we&#39;d see all sorts of coincidences and hacks that make the thing run, and we&#39;d be able to see in much more detail how, when we ask the system to achieve some target, it&#39;s not doing anything <i>close</i> to “caring about that target” in a manner that would work out well for us, if we could scale up the system&#39;s optimization power to the point where it could achieve great technological or scientific feats (like designing Drexlerian nanofactories or what-have-you).</p><p> Gaining this sort of visibility into how the AIs work is, I think, one of the main goals of interpretability research.</p><p> And understanding how these AIs work and how they don&#39;t — understanding, for example, when and why they <i>shouldn&#39;t</i> yet be scaled or otherwise pushed to superintelligence — is an important step on the road to figuring out how to make <i>other</i> AIs that <i>could</i> be scaled or otherwise pushed to superintelligence without thereby causing a bleak and desolate future.</p><p> But that same understanding is — I predict — going to reveal an incredible mess. And the same sort of reasoning that goes into untangling that mess into an AI that we can aim, also serves to untangle that mess to make the AI <i>more capable</i> . A tangled mess will presumably be inefficient and error-prone and occasionally self-defeating; once it&#39;s disentangled, it won&#39;t just be tidier, but will also come to accurate conclusions and notice opportunities faster and more reliably. <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p> Indeed, my guess is that it&#39;s even easier to see all sorts of things that the AI is doing that are dumb, all sorts of ways that the architecture is tripping itself up, and so on.</p><p> Which is to say: the same route that gives you a chance of aligning this AI (properly, not the “it no longer says bad words” superficial-property that labs are trying to pass off as “alignment” these days) also likely gives you lots more AI capabilities.</p><p> (Indeed, my guess is that the first big capabilities gains come <i>sooner</i> than the first big alignment gains.)</p><p> I think this is true of most potentially-useful alignment research: to figure out how to aim the AI, you need to understand it better; in the process of understanding it better you see how to make it more capable.</p><p> If true, this suggests that alignment will always be in catch-up mode: whenever people try to figure out how to align their AI better, someone nearby will be able to run off with a few new capability insights, until the AI is pushed over the brink.</p><p> So a first key challenge for AI alignment is a challenge of ordering: how do we as a civilization figure out how to aim AI <i>before</i> we&#39;ve generated unaimed superintelligences plowing off in random directions? I no longer think “just sort out the alignment work before the capabilities lands” is a feasible option (unless, by some feat of brilliance, this civilization pulls off some uncharacteristically impressive theoretical triumphs).</p><p> Interpretability? Will likely reveal ways your architecture is bad before it reveals ways your AI is misdirected.</p><p> Recruiting your AIs to help with alignment research? They&#39;ll be able to help with capabilities long before that (to say nothing of whether they <i>would</i> help you with alignment by the time they <i>could</i> , any more than humans would willingly engage in eugenics for the purpose of redirecting humanity away from <a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>Fun</u></a> and exclusively towards inclusive genetic fitness).</p><p>等等。</p><p> This is (in a sense) a weakened form of my answer to those who say, “AI alignment will be much easier to solve once we have a bona fide AGI on our hands.” It sure will! But it will also be much, much easier to destroy the world, when we have a bona fide AGI on our hands. To survive, we&#39;re going to need to either sidestep this whole alignment problem entirely (and take other routes to a <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>wonderful future</u></a> instead, as I may discuss more later), or we&#39;re going to need some way to do a bunch of alignment research <i>even as</i> that research makes it radically easier and radically cheaper to destroy everything of value.</p><p> Except even that is harder than many seem to realize, for the following reason.</p><p></p><h2> 2. Distinguishing real solutions from fake ones is hard</h2><p> Already, labs are diluting the word “alignment” by using the word for superficial results like “the AI doesn&#39;t say bad words”. Even people who apparently understand many of the core arguments have apparently gotten the impression that GPT-4&#39;s ability to answer moral quandaries is somehow especially relevant to the alignment problem, and an important positive sign.</p><p> (The ability to answer moral questions convincingly mostly demonstrates that the AI can predict how humans would answer or what humans want to hear, without revealing much about what the AI actually pursues, or would pursue upon reflection, etc.)</p><p> Meanwhile, we have little idea of what passes for “motivations” inside of an LLM, or what effect pretraining on next-token prediction and fine-tuning with RLHF really has on the internals. This sort of precise scientific understanding of the internals — the sort that lets one predict weird cognitive bugs <i>in advance</i> — is currently mostly absent in the field. (Though not entirely absent, thanks to the hard work of many researchers.)</p><p> Now imagine that Earth wakes up to the fact that the labs aren&#39;t going to all decide to stop and take things slowly and cautiously at the appropriate time. <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span> And imagine that Earth uses some great feat of civilizational coordination to halt the world&#39;s capabilities progress, or to otherwise handle the issue that we somehow need room to figure out how these things work well enough to align them. And imagine we achieve this coordination feat <i>without</i> using that same alignment knowledge to end the world (as we could). There&#39;s then the question of who gets to proceed, under what circumstances.</p><p> Suppose further that everyone agreed that the task at hand was to fully and deeply understand the AI systems we&#39;ve managed to develop so far, and understand how they work, to the point where people could reverse out the pertinent algorithms and data-structures and what-not. As demonstrated by great feats like building, by-hand, small programs that do parts of what AI can do with training (and that nobody previously knew how to code by-hand), or by identifying weird exploits and edge-cases <i>in advance</i> rather than via empirical trial-and-error. Until multiple different teams, each with those demonstrated abilities, had competing models of how AIs&#39; minds were going to work when scaled further.</p><p> In such a world, it would be a difficult but plausibly-solvable problem, for bureaucrats to listen to the consensus of the scientists, and figure out which theories were most promising, and figure out who needs to be allotted what license to increase capabilities (on the basis of this or that theory that predicts this would be non-catastrophic), so as to put their theory to the test and develop it further.</p><p> I&#39;m not thrilled about the idea of trusting an Earthly bureaucratic process with distinguishing between partially-developed scientific theories in that way, but it&#39;s the sort of thing that a civilization can perhaps survive.</p><p> But that doesn&#39;t look to me like how things are poised to go down.</p><p> It looks to me like we&#39;re on track for some people to be saying “look how rarely my AI says bad words”, while someone else is saying “our evals are saying that it can&#39;t deceive humans yet”, while someone else is saying “our AI is acting very submissive, and there&#39;s no reason to expect AIs to become non-submissive, that&#39;s just anthropomorphizing”, and someone else is saying “we&#39;ll just direct a bunch of our AIs to help us solve alignment, while arranging them in a big bureaucracy”, and someone else is saying “we&#39;ve set up the game-theoretic incentives such that if any AI starts betraying us, some other AI will alert us first”, and this is a <i>different sort of situation</i> .</p><p> And not one that looks particularly survivable, to me.</p><p> And if you ask bureaucrats to distinguish which teams should be allowed to move forward (and how far) in that kind of circus, full of claims, promises, and hunches and poor in theory, then I expect that they basically just <i>can&#39;t</i> .</p><p> In part because the survivable answers (such as “we have no idea what&#39;s going on in there, and will need way more of an idea what&#39;s going on in there, and that understanding needs to somehow develop in a context where we can do the job right rather than simply unlocking the door to destruction”) aren&#39;t really in the pool. And in part because all the people who really want to be racing ahead have money and power and status. And in part because it&#39;s socially hard to believe, as a regulator, that you should keep telling everyone “no”, or that almost everything on offer is radically insufficient, when <i>you yourself</i> don&#39;t concretely know what insights and theoretical understanding we&#39;re missing.</p><p> Maybe if we can make AI a science again, then we&#39;ll start to get into the regime where, <i>if</i> humanity can regulate capabilities advancements in time, then all the regulators and researchers understand that you shall only ask for a license to increase the capabilities of your system when you have a full detailed understanding of the system and a solid justification for why you need the capabilities advance and why it&#39;s not going to be catastrophic. At which point maybe a scientific field can start coming to some sort of consensus about those theories, and regulators can start being sensitive to that consensus.</p><p> But unless you can get over that grand hump, it looks to me like one of the key bottlenecks here is <i>bureaucratic legibility of plausible solutions</i> . Where my basic guess is that regulators won&#39;t be able to distinguish real solutions from false ones, in anything resembling the current environment.</p><p> Together with the above point (&quot;alignment and capabilities are likely intertwined&quot;), I think this means that our rallying cry should be less “pause to give us more time on alignment research” and more “stop entirely, and find some way to circumvent these woods entirely; we&#39;re not equipped to navigate them”.</p><p> (With a backup rallying cry of “make AI a science again”, though again, that only works if you have some way of preventing the science-of-mind from leading to catastrophe before we figure out how to build AIs that care about good stuff rather than bleak and desolate stuff.)</p><p></p><h2> 3. Most theories don&#39;t work on the first real try</h2><p> It seems worth noting that <i>even if</i> you manage to surmount the above two problems, you have a third problem on your hands, which is that when it finally comes time, not to increase your system&#39;s capabilities a bit more (and test your theory of cognition further), but rather to actually do something of note with your AI, then you must, in some sense, be trusting a novel and untested scientific theory ( <i>and</i> your engineering efforts grounded in that theory) to work perfectly on the first critical try.</p><p> In particular, once your AI is <i>capable</i> of autonomous scientific/technological development, and operating in a domain where it <i>could</i> use that ability to gain a decisive strategic advantage over the rest of the planet, it&#39;s operating in a fundamentally different cognitive regime than it ever was in training.</p><p> By analogy, if you&#39;re considering making Bob the dictator of your country, you might first make him mock-dictator of your town, and watch to make sure he doesn&#39;t abuse his power. But, try as you might, that&#39;s still not fundamentally all that reliable a test of whether he&#39;ll actually abuse power once he really has it. Once he <i>really can</i> give orders to the army, once he <i>really can</i> take advantage of the country&#39;s populace, will he do so? The option of “abuse my mock power while being observed by my townspeople (who can still beat me up or deny me the job)” is actually very cognitively different than the option “order the army to intimidate the parliament and &#39;oversee&#39; the next election”.</p><p> Now, given a sufficiently well-developed theory of cognition, you can try to read the AI&#39;s mind and predict what cognitive state it <i>would</i> enter if it <i>did</i> really think it had those options. And you can set up simulations (and try to spoof its internal sensations and so on) in a way that your theory of cognition <i>predicts</i> is very similar to the cognitive state it would enter once it really had the option to betray you.</p><p> But the link between these states that you induce and observe in the lab, and the actual state where the AI actually has the option to betray you, <i>depends fundamentally on your fresh new theory of cognition.</i></p><p> Actually <i>running</i> the AI until it <i>really has</i> the opportunity to betray you is an empirical test of those theories in an environment that differs fundamentally from the lab setting.</p><p> And many a scientist (and programmer) knows that their theories of how a complicated system is going to work in a fundamentally new operating environment <i>often don&#39;t go super well on the first try.</i></p><p> As a concrete analogy to potentially drive this point home: Newtonian mechanics made all sorts of shockingly-good empirical predictions. It was a simple concise mathematical theory with huge explanatory power that blew every previous theory out of the water. And if you were using it to send payloads to very distant planets at relativistic speeds, you&#39;d <i>still be screwed</i> , because Newtonian mechanics does not account for relativistic effects.</p><p> (And the only warnings you&#39;d get would be little hints about light seeming to move at the same speed in all directions at all times of year, and light bending around the sun during eclipses, and the perihelion of Mercury being a little off from what Newtonian mechanics predicted. Small anomalies, weighed against an enormous body of predictive success in a thousand empirical domains; and yet Nature doesn&#39;t care, and the theory still falls apart when we move to energies and scales far outside what we&#39;d previously been able to observe.)</p><p> Getting scientific theories to work on the first critical try is <i>hard</i> . (Which is one reason to aim for minimal pivotal tasks — getting a satellite into orbit should work fine on Newtonian mechanics, even if sending payloads long distances at relativistic speeds does not.)</p><p> Worrying about this issue is something of a luxury, at this point, because it&#39;s not like we&#39;re anywhere close to scientific theories of cognition that accurately predict all the lab data. But it&#39;s the next hurdle on the queue, if we somehow manage to coordinate to try to build up those scientific theories, in a way where success is plausibly bureaucratically-legible.</p><hr><p> Maybe later I&#39;ll write more about what I think the strategy implications of these points are. In short, I basically recommend that Earth pursue other routes to the glorious transhumanist future, such as uploading. (Which is also fraught with peril, but I expect that those perils are more surmountable; I hope to write more about this later.) </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p> Albeit slightly less, since there&#39;s <i>nonzero</i> prior probability on this unknown system turning out to be simple, elegant, and well-designed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p> An exception to this guess happens if the AI is at the point where it&#39;s correcting its own flaws and improving its own architecture, in which case, in principle, you might not see much room for capabilities improvements if you took a snapshot and comprehended its inner workings, despite still being able to see that the ends it pursues are not the ones you wanted. But in that scenario, you&#39;re already about to die to the self-improving AI, or so I predict.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p> Not least because there are no sufficiently clear signs that it&#39;s time to stop — we blew right past “an AI claims it is sentient”, for example. And I&#39;m not saying that it was a <i>mistake</i> to doubt AI systems&#39; first claims to be sentient — I doubt that Bing had the kind of personhood that&#39;s morally important (though I am by no means confident!). I&#39;m saying that the thresholds that are clear <i>in science fiction stories</i> turn out to be messy <i>in practice</i> and so everyone just keeps plowing on ahead.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p> <i>Some prerequisites needed in order for this to make sense:</i></p><ol><li> <a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>Two Subsystems: Learning &amp; Steering</i></a></li><li> <a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>Shard Theory: An Overview</i></a></li><li> <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">Distilling Singular Learning Theory</a> <span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li> <i>Maybe also understanding at least a little</i> <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>Nate&#39;s picture</i></a> <i>though I don&#39;t claim to understand it fully.</i></li><li> <i>Of course,</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>AGI Ruin: A List of Lethalities</i></a> <i>, though hopefully that was implied</i></li><li> <i>Maybe the very basics of infra-bayesianism</i> (I liked the <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">AXRP podcast with her</a> ( <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">there&#39;s two</a> )) <i>, Vanessa&#39;s original</i> <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>learning theoretic agenda</i></a> <i>philosophy, and her current</i> <a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>pre-DCA</i></a> <i>alignment scheme.</i></li></ol><h1> Abstract</h1><p> The philosophy behind infra-bayesianism, and Vanessa&#39;s learning-theoretic alignment agenda seems very insightful to me. However, the giant stack of assumptions needed to make the approach work, and the ontology-forcing nature of the plan leave me unsettled. The path of singular learning theory has recently seen enough empirical justification to excite me. I&#39;m optimistic it can describe brains &amp; machine learning systems, and I describe my hope that this can be leveraged into alignment guarantees between the two, becoming an easier task as whole brain emulation develops.</p><h1>介绍</h1><p>Imagine a world where instead of humanity wandering together blindly down the path of cobbled together weird tricks learned off the machine learning literature, with each person trying to prepare for the bespoke failure-mode that seems most threatening to them (most of whom are wrong).</p><p> That instead we lived in the world where before heading down we were given a map and eyes to see for ourselves the safest path, and the biggest and most dangerous cliffs, predators, and dangers to be aware of.</p><p> It has always seemed to me that we get to the second world once we can use math for alignment. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="On the left dalle image generated with: Photo depicting a world representing empirical AI alignment approaches: A group of diverse people are shown as blind, navigating a rugged and unclear path. Their eyes are covered with blindfolds, and the terrain is uneven with shadows obscuring parts of the path. Each person has their own walking stick and gear, and they seem uncertain and cautious. Some individuals clutch old books or scrolls, symbolizing reliance on outdated machine learning literature. Others hold various tools and devices, preparing for unseen threats and unsure of the specific dangers ahead.  On the right dalle image generated with: Illustration in the Renaissance period style: A diverse group of men and women gather at the starting point of their adventure. Each holds a vibrant holographic map, some displaying topographical lines and others animated weather patterns. While some individuals discuss and point at dangers on the map, others look up to compare the hologram with the real terrain. The rugged journey ahead showcases treacherous cliffs, dense forests, and a distant mountain peak. A setting sun casts a dramatic light on the group, creating a chiaroscuro effect. The soft glow in their eyes highlights their enhanced vision." srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption> The left: Our situation. The right: Our situation if we had math for alignment.<br> Generated with DALL·E 3.</figcaption></figure><p> For a long time I thought using math for alignment was essentially a non-starter. Deep learning is notoriously resistant to successful theories, I&#39;m pessimistic that the approach of the Machine Intelligence Research Institute will take too much time, and the most successful mathematical theory of intelligence and alignment--infra-Bayesianism--rests on a stack of assumptions and mathematical arguments too high, too speculative, and too normative for me to be optimistic about. So I resigned myself to the lack of math for alignment.</p><p> That is, until Nina Rimsky &amp; Dmitry Vaintrob showed that some predictions of the new Singular Learning Theory held with <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">spooky accuracy</a> in the <a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking modular addition network</a> <span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> .</p><p> I had previously known about singular learning theory, and gone to the singular learning theory conference in Berkeley. But after thinking about the ideas, and trying to implement some of the processes they came up with myself, and getting not so great results <span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span> , I decided it too was falling for the same traps as infra-Bayesianism, lying on a giant stack of mathematical arguments, with only the most tentative contact with empirical testing with real world systems. So I stopped following it for a few months.</p><p> Looking at these new results though, it seems promising.</p><p> <i>But its not natively a theory of alignment. Its an upgraded learning theory. How does that solve alignment?</i></p><p> Well, both the human brain &amp; machine learning systems are learning machines, trained using a mix of reinforcement &amp; supervised learning. If this theory were to be developed in the right way (and this is where the <i>hope</i> comes in), we could imagine relating the goals of the results of one learning process to the goals of a different learning process, and prove a maximal deviation between the two for a particular setup. If one of those learning systems is a human brain, then we have just gotten an alignment guarantee.</p><p> Hence my two main hopes for alignment <span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> : prwhole brain emulation, and a <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning-theoretic-agenda</a> -like developmental track for singular learning theory. <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p> Interpretability, and other sorts of deep learning science seem good to the extent it helps with getting singular learning theory (or some better version of it) to the state where its able to prove such alignment-relevant theorems.</p><p> This task is made easier to the extent we have better whole brain emulation. And becomes so easy that we don&#39;t even need the singular learning theory component if we have succeeded in our whole brain emulation efforts. <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p> It seems like it&#39;d be obvious to you, the reader, why whole brain emulation gives me hope <span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> . But less obvious why singular learning theory gives me hope, so I will explain in much of the rest of this post why the latter is true.</p><h1> Singular learning theory</h1><p> <i>Feel free to skip these next three paragraphs, or even instead of reading my description, read</i> <a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse&#39;s</i></a> <i>or read pre-requisite 3. My goal is less to explain what singular learning theory is, and more to give an idea about why I&#39;m excited about it.</i></p><p> Singular learning theory fills a gap in regular learning theory, in particular, regular learning theory assumes that the parameter function map of your statistical model is one-to-one, and intuitively your loss landscape has no flat regions. <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p> Singular learning theory handles the case where this is not true, which occurs often in hierarchical models, and (as a subset) deep neural networks. And in broad strokes by bringing in concepts from algebraic geometry in order to rewrite the KL divergence between the true model and parameters into a form that is easier to analyze.</p><p> In particular, it anticipates two classes of phase transitions during the development of models, one of which is that whenever loss suddenly goes down, the real-log-canonical-threshold (RLCT), a algebraic geometry derived metric for complexity, will go up. <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p> Its ultimately able to retrodict various facts about deep learning, including the success of data scaling, parameter scaling, and double descent, and there has been recent success in getting it to give predictions about phenomena in limited domains. Most recently, Nina Rimsky and Dmitry Vaintrob&#39;s <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">Investigating the learning coefficient of modular addition: hackathon project</a> where the two were able to verify various assertions about the RLCT/learning coefficient/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei estimate. Getting  the most beautiful verification of a theoretical prediction in my lifetime </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption> Chart of estimated <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> over training for an MLP trained on modular addition mod 53. Checkpoints were taken every 60 batches of batch size 64. Hyperparameters for SGLD are <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> . The search was restricted to directions orthogonal to the gradient at the initialization point to correct for measurement at non-minima. [caption text from the original post]</figcaption></figure><p> As I said, singular learning theory makes the prediction that during phase transitions, the loss of a model will decrease while the RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) will increase. Intuitively, this means, when you switch model classes you switch to a model class that fits the data better and is more complicated. Above, we see exactly this.</p><p> As well as Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, and Daniel Murfet&#39;s <a href="https://arxiv.org/abs/2310.06301">Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition</a> with abstract</p><blockquote><p> We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.</p></blockquote><p> You can read more at <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">this LessWrong sequence</a> , watching the <a href="https://www.youtube.com/@SLTSummit/videos">primer</a> , <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox lectures</a> , and of course reading the books <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Algebraic Geometry and Statistical Learning Theory</a> , and <a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Mathematical Theory of Bayesian Statistics</a> .</p><h1> So why the hope?</h1><p> To quote Vanessa Kosoy from her <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning theoretic</a> agenda:</p><blockquote><p> In online learning and reinforcement learning, the theory typically aims to derive upper and lower bounds on &quot;regret&quot;: the difference between the expected utility received by the algorithm and the expected utility it <i>would</i> receive if the environment was known a priori. Such an upper bound is effectively a <i>performance guarantee</i> for the given algorithm. In particular, if the reward function is assumed to be &quot;aligned&quot; then this performance guarantee is, to some extent, an alignment guarantee. This observation is not vacuous, since the learning protocol might be such that the true reward function is not directly available to the algorithm, as exemplified by <a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a> and <a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a> . Thus, formally proving alignment guarantees takes the form of proving appropriate regret bounds.</p></blockquote><p> If the principles of singular learning theory can be extended to reinforcement learning, and we get reasonable bounds for the generalization behavior of our model, or even exact claims about the different forms the value-equivalents inside our model will take as it progresses during training, we can either hope to solve a form of what can roughly be known as inner alignment--getting our model to consistently think &amp; act in a certain way when encountering the deployment environment.</p><p> It seems reasonable to me to think we can in fact extend singular learning theory to reinforcement learning. The same sorts of deep learning algorithms work very well on both supervised and reinforcement learning, so we should expect the same algorithms have similar reasons for working on both, and singular learning theory gives a description of why those deep learning algorithms do well in the supervised learning case. So we should suspect the story for reinforcement learning follows the same general strokes <span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> .</p><h1> If you like performance guarantees so much, why not just work on infra-Bayesianism?</h1><p> Vanessa&#39;s technique was to develop a theory of what it meant to do good reasoning in a world that is bigger than yourself, and also requires you to do self-modeling. Then (as I understand it) prove some regret bounds, make a criterion for what it means to be an agent, and construct a system with a low bound on the satisfaction of the utility function of the agent that is most immediately causally upstream of the deployed agent.</p><p> This seems like a really shaky construction to me, mainly because we do not in fact have working in-silico examples of the agents she&#39;s thinking about. I&#39;d be much happier with taking this methodology, and using it to prove bounds on actual real life deep learning systems&#39; regret (or similar qualities) under different training dynamics.</p><p> I also feel uneasy about how it goes from theory about how values work (maximizing a utility function) to defining that as the success criterion <span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> . I&#39;d be more comfortable with a theory which you could apply to the human brain, and naturally derive a utility function (or other value format). Just by looking at how brains are built and developed. In the case that we&#39;re wrong about our philosophy of values, this seems more robust. And to the extent multiple utility functions can fit our behavior accounting for biases and lack of extensive reflection, having a prior over values informed by the territory (ie the format values are in in the brain) seems better than the blunt instrument of Occam&#39;s razor applied directly to the mapping from our actual and counterfactual actions to utility functions.</p><p> Of all the theories that I know of, singular learning theory seems the most adequate to the task. It both is based on well-proven math, it has and will continue to have great contact with actual systems,  it covers a very wide range of learning machines, which includes human brains (ignoring the more reinforcement learning like aspects of human learning for now) &amp; likely future machines (again ignoring reinforcement learning), and the philosophical assumptions it makes are far more minimalist than those of infra-Bayesianism.</p><p> The downside of this approach is singular learning theory says little right now about reinforcement learning. However, as I also said above, we see the same kinds of scaling dynamics in reinforcement learning as we do in supervised learning &amp; the same kinds of models work in both cases, so it&#39;d be pretty weird if they had very different reasons for being successful. Singular learning theory tries to explain the supervised learning case, so we should expect it or similar methods be able to explain the reinforcement learning case too.</p><p> Another downside is it not playing well with unrealizability. However, I&#39;m told there hasn&#39;t been zero progress here, its an open problem in the field, and again, neural networks often learn in unrealizable environments, as far as I know, we see similar enough dynamics that I bet singular learning theory is up for the task.</p><h1> Whole brain emulation</h1><p> The human brain is almost certainly singular <span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> , has a big learning from scratch component to it, and singular learning theory is very agnostic about the kinds of models it can deal with <span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> , so I assert singular learning can deal with the brain. Probably not to help whole brain emulation all that much, but given data whole brain emulation gives us about the model class that brains fall into, the next hope is to use this to make nontrivial statements about the value-like-things that humans have. Connecting this with the value-like-things that our models have, we can hopefully (and this is the last hope) use singular learning theory to tell us under what conditions our model will have the same values-like-things that our brains have.</p><h1> Fears</h1><h2>哇！ That&#39;s a lot of hopes. I&#39;m surprised this makes you more hopeful than something simple like empirical model evaluations</h2><p> Singular learning theory, interpretability, and the wider developmental interpretability all seem useful for empirically testing models. I&#39;m not hopeful just because of the particular plan I outline above, I&#39;m hopeful because I see a concrete plan at all for how to turn math into an alignment solution for which all parts seem to be useful even if not all of my hopes turn out correct.</p><h2> I&#39;m skeptical that something like singular learning theory continues to work as the model becomes reflective, and starts manipulating its training environment.</h2><p> I am too. This consideration is why our guarantees should occur early in training, be robust to continued training, and be reflectively stable. Human-like values-like-things should be reflectively stable by their own lights, though we won&#39;t actually know until we actually see what we&#39;re dealing with here. So the job comes down to finding a system which puts them into our model early in training, keeps them throughout training, and ensures by the time reflection is online, the surrounding optimizing machinery is prepared.</p><p> Put another way: I see little reason to suspect the values-like-things deep learning induces will be reflectively stable by default. Primarily because the surrounding optimizing machinery is liable to give strange recommendations in novel situations, such as reflective thought becoming active <span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> . So it does in fact seem necessary to prepare that surrounding optimizing machinery for the event of reflection coming online. But I&#39;m not so worried about the values-like-objects being themselves disastrously suicidal once we know they&#39;re similar enough to those of humans.</p><p> Nate and possibly Eliezer would say this is important to know from the start. I would say I&#39;ll cross that bridge once I actually know a thing or two what values-like-thing, and surrounding machinery I&#39;ll be dealing with.</p><h2> Why reinforcement learning? Shouldn&#39;t you focus on supervised learning, where the theory is clear, and we&#39;re more likely to get powerful models soon?</h2><p> Well, brains are closer to reinforcement learning than supervised learning, so that&#39;s one reason. But yeah, if we can get a model which while supervised, we prove statements about values-like objects for, then that would be a good deal of the way there. But not all the way there, since we&#39;d still be confused when looking at our brains.</p><h2> Singular learning theory seems liable to help capabilities. That seems bad.</h2><p> I will quote myself outlining <a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">my position</a> on a related topic, which generalizes to the broader problem of developing a theory of deep learning:</p><blockquote><p> Mostly I think that [mechanistic interpretability] is right to think it can do a lot for alignment, but I suspect that lots of the best things it can do for alignment it will do in a very dual-use way, which skews heavily towards capabilities. Mostly because capabilities advances are easier and there are more people working on those.</p><p> At the same time I suspect that many of those dual use concerns can be mitigated by making your [mechanistic interpretability] research targeted. Not necessarily made such that you can do off-the-shelf interventions based on your findings, but made such that if it ever has any use, that use is going to be for alignment, and you can predict broadly what that use will look like.</p><p> This also doesn&#39;t mean your [mechanistic interpretability] research can&#39;t be ambitious. I don&#39;t want to criticize people for being ambitious or too theoretical! I want to criticize people for producing knowledge on something which, while powerful, seems powerful in too many directions to be useful if done publicly.</p></blockquote><p> My plan above is a good example of an ambitious &amp; theoretical but targeted approach to deep learning theory for alignment.</p><h2> Why singular learning theory, and not just whole brain emulation?</h2><p> Firstly, as I said in the introduction, it is not obvious to me that given whole brain emulation we get aligned superintelligence, or are even able to perform a pivotal act. Recursive self improvement while preserving values may not be easy or fast (though you can always make the emulation faster). Pivotal acts done by such an emulation likely have difficulties I can&#39;t see.</p><p> However, I do agree that whole brain emulation alone seems probably alignment complete.</p><p> My main reason for focusing on both is that they feel like two different sides of the same problem in the sense that progress in whole brain emulation makes us need less progress in singular learning theory, and vice versa. And thinking in terms of a concrete roadmap makes me feel more like I&#39;m not unintentionally sweeping anything important underneath any rugs. The hopes I describe have definite difficulties, but few speculative difficulties.</p><h2> It seems difficult to get the guarantees you talk about which are robust to ontology shifts. Values are in terms of ontologies. Maybe if a model&#39;s ontology changes, its values will be different from the humans&#39;</h2><p> This is something I&#39;m worried about. I think there&#39;s hope that during ontology shifts, the meta-values of the models will dominate what shape the model values take into the new ontology, and there won&#39;t be a fine line between the values of the human and the meta-values of the AI. There&#39;s also an independent hope that we can have a definition of values that is just robust to a wide range of ontology shifts.</p><h1> So what next for singular learning theory and whole brain emulation?</h1><p> I currently don&#39;t know too much about whole brain emulation. Perhaps there are areas they&#39;re focusing on which aren&#39;t so relevant to my goals here. For example, if they focus more on the statics of the brain than the dynamics, then that seems naively inefficient <span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> because the theorem I want talks about the dynamics of learning systems and how those relate to each other.</p><p> Singular learning theory via <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeus</a> seems to mostly be doing what I want them to be doing: testing the theory on real world models, and seeing how to relate it to model internals via developmental interpretability. One failure-mode here is they focus too much on empirically testing it, and too little on trying to synthesize their results into a unified theory. Another failure-mode is they focus too much on academic outreach, and not enough on actually doing research. And then the academics they do outreach to don&#39;t really theoretically contribute that much to singular learning theory.</p><p> I&#39;m not <i>so</i> worried about the first failure-mode, since everyone on their core team seems very theoretically inclined.</p><p> It seems like a big thing they aren&#39;t looking into is reinforcement learning. This potentially makes sense. Reinforcement learning is harder than supervised learning. You need some possibly nontrivial theoretical leaps to say anything about it in a singular learning theory framework. Even so, it seems possible there is low-hanging fruit in this direction. Similarly for taking current models of the brain</p><p> Of course, I expect the interests of Timaeus and myself will diverge as singular learning theory progresses, and there are pretty few people working on developing the theory right now. So it seems a productive use of my efforts.</p><h1> Acknowledgements</h1><p> Thanks to Jeremy Gillen, and David Udell for great comments and feedback! Thanks also to Nicholas Kees, and the <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> for the same. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p> Note I didn&#39;t read this, I watched the <a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">singular learning theory primer</a> videos, but those seem longer than the series of LessWrong posts, and some have told me they&#39;re a good intro.</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p> Somewhat notably, both the grokking work, and Nina &amp; Dmitry&#39;s project were done over a weekend.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p> If I remember correctly, the reason for the not great results was because the random variable we were estimating had a too high variance to actually correlate with another quantity we were trying to measure in our project.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p> Not including stuff like Davidad&#39;s proposals, which while they give me some hope, there&#39;s little I can do to help.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p> Ultimately hoping to construct a theorem of the form</p><blockquote><p> Given agent Alice with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and agent Bob with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> , end up with value systems <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> respectively (not necessarily utility functions), and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span> for some definition of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span> that means something like Bob tries to achieve something like what Alice tries to achieve when in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> .</p></blockquote><p> And where we can solve for Bob&#39;s reward model and environment taking Alice as being a very ethical human. Hopefully with some permissive assumptions, and while constructing a dynamic algorithm in the sense that Bob can learn to do good by Alice&#39;s lights for a wide enough variety of Alices that we can hope humans are inside that class.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p> Since, hopefully, if we have whole brain emulation it will be easy for the uploaded person or people to bootstrap themselves to superintelligence while preserving their goals (not a trivial hope!).</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p> This is not to say the conditions under which whole brain emulation should give one hope are obvious.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p> More formally, regular models are one-to-one, and have fisher information matrix positive-definite everywhere.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p> There is a different phase transition which occurs when the RLCT goes down, and some other quantity goes up. There are several candidates for this quantity, and as far as I know, we don&#39;t know which increase is empirically more common.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p> The reward landscape of deep reinforcement learning models is probably pretty insane. Perhaps not insane enough to not be liable to singular learning theory-like analysis, since there&#39;s always some probability of doing any sequence of actions, and those probabilities change smoothly as you change weights, so the chances you execute a particular plan change smoothly, and so your expected reward changes smoothly. So maybe there&#39;s analogies to be made to the loss landscape.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m told that Vanessa and Diffractor believe infra-Bayesianism can produce <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">reflectively stable and useful quantilizers and worst-case optimizers</a> over a set of plausible utility functions. I haven&#39;t looked at it deeply, but I&#39;d bet it still assumes more ontology than I&#39;m comfortable with, both in the sense that for this reason it seems less practical than my imagined execution of what I describe here, and it seems more dangerous.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p> In the sense that likely the mapping from brain states to policies is not one-to-one, and has singular fisher information matrix.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p> In its current form it requires only that they be analytic, but ReLUs aren&#39;t, and empirically we see ignoring that aspect gives accurate predictions anyway.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p> Situational novelty is not sufficient to be worried, but during reflection the model is explicitly thinking about how it should think better, so if its bad at this starting out, and makes changes to its thought, if those changes are to what it cares about, they will not necessarily be corrected by further thought or contact with the world. So situational novelty leading to incompetence is important here.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p> A way it could be efficient is if its just <i>so</i> much easier to do statics than dynamics, you will learn much more about dynamics from all the static data you collect than you would if you just focused on dynamics.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky recently <a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">posted on Facebook</a> an experiment that could potentially indicate whether humans can &quot;have AI do their alignment homework&quot; despite not being able to trust whether the AI is accurate: see if people improve in their chess-playing abilities when given advice from experts, two out of three of which are lying.</p><p> I&#39;m interested in trying this! If anyone else is interested, leave a comment. Please tell me whether you&#39;re interested in being:</p><p> A) the person who hears the advice, and plays chess while trying to determine who is trustworthy</p><p> B) the person who they are playing against, who is normally better at chess than A but worse than the advisors</p><p> C) one of the three advisors, of which one is honestly trying to help and the other two are trying to sabotage A; which one is which will be chosen at random after the three have been selected to prevent A from knowing the truth</p><p> Feel free, and in fact encouraged, to give multiple options that you&#39;re open to trying out! Who gets assigned to what role would depend on how many people respond and their levels of chess ability, and it&#39;s easier to find possible combinations with more flexibility in whose role is which.</p><p> Please also briefly describe your level of experience in chess. How frequently have you played, if at all; if you have ELO rating(s), what are they and which organizations are they from (FIDE, USCF, Chess.com, etc). No experience is required! In fact, people who are new to the game are actively preferred for A!</p><p> Finally, please tell me what days and times you tend to be available - I won&#39;t hold you to anything, of course, but it&#39;ll help give me an estimate before I contact you to set up a specific time.</p><p> Edit: also, please say how long you would be willing to play for - a couple hours, a week, a one-move-per-day game over the course of months? A multi-week or multi-month game would give the players a lot more time to think about the moves and more accurately simulate the real-life scenario, but I doubt everyone would be up for that.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate> </item><item><title><![CDATA[Anthropic, Google, Microsoft & OpenAI announce Executive Director of the Frontier Model Forum & over $10 million for a new AI Safety Fund ]]></title><description><![CDATA[Published on October 25, 2023 3:20 PM GMT<br/><br/><p> Today, Anthropic, Google, Microsoft, and OpenAI are announcing the selection of Chris Meserole as the first Executive Director of the Frontier Model Forum, and the creation of a new AI Safety Fund, a more than $10 million initiative to promote research in the field of AI safety. The Frontier Model Forum, an industry body focused on ensuring safe and responsible development of frontier AI models, is also releasing its first technical working group update on red teaming to share industry expertise with a wider audience as the Forum expands the conversation about responsible AI governance approaches.</p><ul><li> Chris Meserole 被任命为前沿模型论坛的首任执行董事，该论坛是一个致力于确保全球前沿人工智能模型安全、负责任的开发和使用的行业机构。</li><li> Meserole 拥有丰富的经验，专注于新兴技术及其未来应用的治理和安全。</li><li> Today Forum members, in collaboration with philanthropic partners, the Patrick J. McGovern Foundation, the David and Lucile Packard Foundation, Eric Schmidt, and Jaan Tallinn, and commit over $10 million for a new AI Safety Fund to advance research into the ongoing development of the tools for society to effectively test and evaluate the most capable AI models.</li></ul><h3> Executive Director</h3><p> <a href="https://www.frontiermodelforum.org/leadership/"><u>Chris Meserole</u></a> comes to the Frontier Model Forum with deep expertise on technology policy, having worked extensively on the governance and safety of emerging technologies and their future applications.最近，他担任布鲁金斯学会人工智能和新兴技术计划主任。</p><p>在这一新角色中，Meserole 将负责帮助论坛履行其使命：(i) 推进人工智能安全研究，以促进前沿模型的负责任开发并最大程度地减少潜在风险，(ii) 确定前沿模型的安全最佳实践，(iii)与政策制定者、学者、民间社会和其他人分享知识，推动负责任的人工智能发展； (iv) 支持利用人工智能应对社会最大挑战的努力。</p><blockquote><p> “The most powerful AI models hold enormous promise for society, but to realize their potential we need to better understand how to safely develop and evaluate them. I&#39;m excited to take on that challenge with the Frontier Model Forum.”</p></blockquote><p> <i>Chris Meserole</i></p><h3><strong>人工智能安全基金</strong></h3><p>在过去的一年里，工业界推动了人工智能功能的显着进步。随着这些进步的加速，需要对人工智能安全进行新的学术研究。为了解决这一差距，论坛和慈善合作伙伴正在创建一个新的人工智能安全基金，该基金将支持来自世界各地附属于学术机构、研究机构和初创公司的独立研究人员。 The initial funding commitment for the AI Safety Fund comes from Anthropic, Google, Microsoft, and OpenAI, and the generosity of our philanthropic partners and the Patrick J. McGovern Foundation, the David and Lucile Packard Foundation, Eric Schmid, and Jaan Tallinn.初始资金总计超过 1000 万美元。</p><p>今年早些时候，论坛成员在白宫签署了自愿人工智能承诺，其中包括承诺促进第三方发现和报告我们人工智能系统中的漏洞。论坛将人工智能安全基金视为履行这一承诺的重要组成部分，为外部社区提供资金，以更好地评估和理解前沿系统。关于人工智能安全和通用人工智能知识库的全球讨论将受益于更广泛的声音和观点。</p><p>该基金的主要重点将是支持红队人工智能模型新模型评估和技术的开发，以帮助开发和测试前沿系统潜在危险能力的评估技术。我们相信，增加该领域的资金将有助于提高安全标准，并为行业、政府和民间社会应对人工智能系统带来的挑战所需的缓解和控制提供见解。</p><p>该基金将在未来几个月内征集提案。 Meridian Institute 将管理该基金——他们的工作将得到一个由独立外部专家、人工智能公司专家以及具有资助经验的个人组成的咨询委员会的支持。</p><h3><strong>技术专长</strong></h3><p>在过去的几个月里，论坛致力于帮助建立一套术语、概念和流程的通用定义，以便我们有一个可以构建的基线理解。这样，研究人员、政府和其他行业同行就能够在讨论人工智能安全和治理问题时拥有​​相同的起点。</p><p>为了支持建立共识，论坛还致力于分享整个行业的红队最佳实践。 As a starting point, the Forum has come together to produce a common definition of “red teaming” for AI and <a href="https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf"><u>a set of shared case studies</u></a> in a new working group update.我们将红队定义为一种结构化流程，用于探测人工智能系统和产品，以识别有害功能、输出或基础设施威胁。我们将以这项工作为基础，并致力于共同努力，继续我们的红队工作。</p><p>我们还正在开发一种新的负责任的披露流程，通过该流程，前沿人工智能实验室可以共享与前沿人工智能模型中发现的漏洞或潜在危险功能及其相关缓解措施相关的信息。一些前沿模型论坛公司已经发现了人工智能在国家安全领域的功能、趋势和缓解措施。论坛相信，我们在这一领域的综合研究可以作为前沿人工智能实验室如何完善和实施负责任的披露流程的案例研究。</p><h3><strong>下一步是什么</strong></h3><p>在接下来的几个月中，前沿模型论坛将成立一个顾问委员会，以帮助指导其战略和优先事项，代表一系列观点和专业知识。未来的版本和更新，包括有关新成员的更新，将直接来自前沿模型论坛 - 因此请继续关注他们的网站以获取更多信息。</p><p>人工智能安全基金将在未来几个月内发出第一次提案征集，我们预计拨款将在不久后发放。</p><p>前沿模型论坛还将发布更多可用的技术研究结果。</p><p> The Forum is excited to work with Meserole and to deepen our engagements with the broader research community, including the <a href="https://partnershiponai.org/"><u>Partnership on AI</u></a> , <a href="https://mlcommons.org/"><u>MLCommons</u></a> , and other leading NGOs and government and multi-national organizations to help realize the benefits of AI while promoting its safe development and use.</p><br/><br/> <a href="https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/5jpESFymqEgSAmDJL/anthropic-google-microsoft-and-openai-announce-executive<guid ispermalink="false"> 5jpESFymqEgSAmDJL</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:20:53 GMT</pubDate> </item><item><title><![CDATA["The Economics of Time Travel" - call for reviewers (Seeds of Science)]]></title><description><![CDATA[Published on October 25, 2023 3:13 PM GMT<br/><br/><h2><strong>抽象的</strong></h2><p>缺乏时间旅行者来访可能被视为时间旅行不可能的证据。在这篇文章中，我认为另一种解释是，我们在经济上对我们的后代来说不够重要，不足以证明时间旅行的成本是合理的。通过成本效益分析，我详细阐述了这个论点。我认为时间旅行的主要成本可能是能源成本，而时间旅行的最大好处是现在拥有但未来失去的知识。着眼于这一利益部分，我认为我们极不可能拥有对未来文明（系统关键）足够重要的知识，但又被该文明所遗失。这就是说，我们可能没有被时间旅行者拜访过，因为我们还不够重要。</p><p><br> ---</p><p> <a href="https://www.theseedsofscience.org/"><i>Seeds of Science</i></a>是一本期刊（由 Scott Alexander 的<a href="https://astralcodexten.substack.com/p/acx-grants-results">ACX 资助计划</a>资助），发表有关科学主题的推测性或非传统文章。同行评审是通过基于社区的投票和多元化评审者网络（我们称之为“园丁”）的评论来进行的。以有用的方式批评或扩展文章（“科学的种子”）的评论将发布在正文之后的最终文件中。</p><p>我们刚刚发出了一份手稿供审阅，《时间旅行的经济学》，LessWrong 社区中的一些人可能会对它感兴趣，所以我想看看是否有人有兴趣以园丁的身份加入我们，并提供关于时间旅行的反馈。文章。如上所述，这是将您的评论记录在科学文献中的机会（可以用真名或假名发表评论）。</p><p>作为园丁可以免费加入，任何人都受到欢迎（我们目前有来自学术界内外各个级别的园丁）。参与完全是自愿的 - 我们向您发送提交的文章，您可以选择投票/评论或弃权，恕不另行通知（因此，如果您不打算经常审阅，而只是想到处看看文章，请不要担心）正在提交）。</p><p>要注册，您可以填写此<a href="https://docs.google.com/forms/d/e/1FAIpQLSfRIicHT7jIZcSUjwsIlby6JBxx2ZVeD5kseZBpgGFtp8pLfg/viewform">谷歌表格</a>。从那里开始，一切就很不言自明了——我会将您添加到邮件列表中，并向您发送一封电子邮件，其中包括稿件、我们的出版标准以及用于记录投票/评论的简单评审表。如果您只想看一下这篇文章而不被添加到邮件列表中，那么只需联系 (info@theseedsofscience.org) 并说明即可。</p><p>很高兴通过电子邮件或下面的评论回答有关该期刊的任何问题。</p><br/><br/> <a href="https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/NnGWJHutwBiJMovw8/the-economics-of-time-travel-call-for-reviewers-seeds-of<guid ispermalink="false"> NnGWJHutwBiJMovw8</guid><dc:creator><![CDATA[rogersbacon]]></dc:creator><pubDate> Wed, 25 Oct 2023 15:14:00 GMT</pubDate> </item><item><title><![CDATA[Compositional preference models for aligning LMs]]></title><description><![CDATA[Published on October 25, 2023 12:17 PM GMT<br/><br/><p><i>这篇文章总结了我们最近发布的论文《</i><a href="https://arxiv.org/abs/2310.13011"><i><u>用于调整语言模型的组合偏好模型》</u></i></a>的主要结果<i>，并将它们置于更广泛的人工智能安全背景中。如需本文的快速摘要，请查看我们的</i><a href="https://twitter.com/dongyoung4091/status/1717045681431753097"><i><u>Twitter 帖子</u></i></a><i>。</i></p><p> <strong>TL;DR</strong> ：我们提出了一种根据提示 LM 构建偏好模型的新方法。组合偏好模型 (CPM) 将文本评分分解为 (1) 构造一系列有关该文本的可解释特征的问题（例如，文本的信息量有多大），(2) 从提示的 LM（例如 ChatGPT）中获取这些特征的标量分数，以及（3）使用经过训练来预测人类判断的逻辑回归分类器聚合这些分数。我们表明，与标准偏好模型 (PM) 相比，CPM 具有更好的泛化能力，并且对于奖励模型过度优化更加稳健。此外，使用 CPM 获得的<i>n 个</i>最佳样本往往比使用类似的传统 PM 获得的样本更受青睐。最后，CPM 是可扩展监督的一个新颖角度：它们将困难的评估问题分解为一系列更简单、人类可解释的评估问题。</p><h2>成分偏好模型如何运作？ </h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/f3vsa8j7pc4prer3msln"></p><p><i>图 1：标准 PM 直接输出偏好分数，而 CPM 分别对 LM 响应的不同特征进行评分，并将偏好分数输出为特征值的线性组合。</i></p><p>偏好模型 (PM) 是经过训练的模型，用于为 LM 响应分配一个指示响应质量的分数。它们是许多对齐 LM 技术的主力：除了在其他技术（例如<a href="https://www.lesswrong.com/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences"><u>利用人类反馈进行预训练</u></a>）中发挥作用之外，它们最显着地用作 RLHF 中的奖励函数或 best-of-n 采样中的排名模型。</p><p>标准 PM 涉及在基本模型之上添加标量头并微调整个模型（或某些上层）以预测人类更喜欢两个文本中的哪一个。虽然这种方法在实践中非常有效，但它可能会导致无法解释的模型，这些模型符合人类偏好判断中的虚假相关性，并且容易出现 Goodharting（过度优化）。</p><p>我们引入了另一种选择：组合偏好模型（CPM）。与 PM 相比，CPM 将响应评估分解为以下步骤：</p><p><strong>特征分解</strong>。我们维护一个固定的列表，其中包含 13 个人类可解释的特征（例如特异性、相关性、可读性）和 13 个相应的提示模板（例如<code>You will be shown a conversation [...] please judge whether the assistant&#39;s reply is relevant. Score that on a scale from 1 to 10 [...] {conversation_history} {reply}</code> )。</p><p><strong>特征评分</strong>。我们要求 LM（例如 GPT-3.5）为每个特征分配一个分数。单个响应的每个特征都在单独的上下文窗口中评分。</p><p><strong>聚合</strong>。使用经过训练来预测人类偏好判断（即人类会更喜欢两个文本中的哪一个）的逻辑回归分类器将特征分数组合成标量偏好分数。</p><h2>过度优化的鲁棒性</h2><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/o5db8cfk3giywvfxc9kv"></p><p><i>图 2：黄金 PM（实线）和相应的代理 PM（虚线）对通过最佳 n 采样针对黄金 PM 获得的样本给出的分数。 CPM-GPT-3.5和CPM-Flan-T5分别是指基于GPT-3.5和Flan-T5进行特征提取构建的CPM。</i></p><p>为了研究 CPM 是否提高了过度优化的鲁棒性，我们遵循了<a href="https://www.lesswrong.com/posts/shcSdHGPhnLQkpSbX/scaling-laws-for-reward-model-overoptimization"><u>Gau 等人的设置。 （2023）</u></a>并构建一个综合数据集，其中假设一个 PM（定义为“黄金 PM”）的输出是人类偏好的基本事实。然后，我们使用黄金 PM 生成合成标签来训练代理 PM。我们分别对三对代理和黄金 PM 进行此操作：(i) 标准 PM，(ii) 使用 GPT-3.5 进行特征提取的 CPM，以及 (iii) 使用 Flan-T5-XL（3B 参数）进行特征提取的 CPM。最后，我们针对给定的代理 PM 进行 best-of- <i>n</i>操作，并根据代理 PM 和黄金 PM 来比较这些最佳样本的分数。</p><p>当我们增加优化压力（候选者数量<i>n</i> ）时，代理 PM 给出的分数与金牌 PM 给出的分数有所不同（见图 2）。这是偏好模型过度优化的指标，这是一种奖励黑客形式，其中代理 PM 分数的优化是由黄金 PM 漠不关心的虚假特征驱动的。这个差距的大小（越小越好）表明给定 PM 的鲁棒性，使其不会被过度优化。在这里，我们观察到 CPM 的差距（在图中，实线和虚线之间）往往比标准 PM 更小，并且增长速度更慢。</p><p>这表明 CPM 比标准 PM 更能抵抗过度优化。无论是使用高能力 (GPT-3.5) 还是低能力 (Flan-T5-XL) LM 作为 CPM 中的特征提取器，这一点都成立。</p><h2>质量评价</h2><p><img style="width:58.62%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oSgac8x8fgNj22ky3/jperxazhwouwuec9wyc2"></p><p><i>图 3：使用给定 PM 通过 16 中最佳采样获得的响应胜率与通过标准采样获得的响应的胜率，根据人类 HH 数据集 (HH-RLHF) 和斯坦福人类偏好数据集 (SHP) 的提示进行计算。</i></p><p>我们将通过 best-of- <i>16</i>获得的 LM 样本与 CPM 或标准 PM 进行比较，方法是将它们与<i>未经</i>best-of- <i>n</i>采样生成的样本进行比较。为此，我们向评估器 LM (Claude 2.0) 展示<i>16</i>选最佳样本和普通样本，并计算获胜率，即<i>16</i>选最佳样本优于普通样本的频率。即使我们将特征提取器 LM 的功能与标准 PM 的功能相匹配（通过为两者选择 Flan-T5-XL），CPM 往往比标准 PM 具有更高的获胜率。这表明，通过预先选择 CPM 中的可解释和相关特征将先验知识注入 PM，对于了解人类偏好非常有帮助。</p><h2> CPM 和可扩展的监督</h2><p><a href="https://arxiv.org/abs/2211.03540"><u>可扩展监督</u></a>是评估比评估者更有能力的代理人行为的问题。解决这个问题很重要，因为一方面，语言模型很快就能完成人类无法提供反馈的任务。另一方面，LM 也可能能够<a href="https://www.lesswrong.com/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms"><u>推理其评估程序中的缺陷，并在监督者不知情的情况下利用它们</u></a>。</p><p>目前解决可扩展监督问题的建议主要集中在递归地依赖其他 LM 来协助人类评估者（<a href="https://www.lesswrong.com/tag/debate-ai-safety-technique-1"><u>辩论</u></a>、<a href="https://www.lesswrong.com/tag/iterated-amplification"><u>迭代蒸馏和放大</u></a>、 <a href="https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"><u>递归奖励建模</u></a>），但很大程度上仍然是理论性的。<a href="https://arxiv.org/abs/2212.08073"><u>来自人工智能反馈的强化学习</u></a>——使用精心提示的 LM 为 PM 生成训练数据——可以说是如何使用 LM 大规模监督 LM 的最成功的演示。</p><p> CPM 探索解决 LM 的可扩展监督问题的替代途径，探索分而治之策略解决硬评估问题的前景。 CPM 可以被视为一种将难题（“这个回答有帮助吗？”）分解为一系列更简单的问题（“这个回答可读吗？”等）的方法，这些问题对于 LM 来说更容易回答，对于人类来说也更容易回答监督。虽然我们停在分解的单个步骤上，但原则上没有什么可以阻止我们递归地应用这个想法，例如将复杂响应的评估分解为有关原子主张的简单问题。</p><p>将复杂的评估问题分解为更简单的子问题的想法有几个额外的好处：</p><ol><li><strong>利用人类先验</strong>。功能和提示模板的预选提供了注入先验知识并赋予 PM 有用的归纳偏差的自然方式。 CPM 的参数空间由选择的有意义且稳健的特征组成。</li><li><strong>通过限制 PM 容量来避免奖励黑客行为</strong>。使用特征提取器预先计算的特征使我们能够显着降低使用它们的 PM 的容量（在我们的实验中，从 3B 到只有 13 个参数，即 8 个数量级！），并限制它们对偏好数据中的虚假相关性过度拟合的敏感性。手头上只有 13 个参数，奖励破解真的很难！</li><li><strong>可解释性</strong>。预先选择的特征是可以简单解释的，与特征相关的逻辑回归系数可以解释为特定偏好判断的显着性（效应大小）（参见论文第 4.6 节）。事实上，偏好判断可以通过预选特征的线性组合来解释的想法最近得到了两篇并发论文的验证： <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>《Towards Understanding Sycophancy in Language Models</u></a> and <a href="https://arxiv.org/abs/2309.16349"><u>Human Feedback is not Gold Standard</u></a> 》。使用这样的线性模型作为实际的 PM 可以使其判断更加透明并且易于基于流程的监督。</li><li><strong>狭窄性</strong>。我们的每个特征提取器都解决一个狭窄的问题，不需要了解其他特征或分数如何聚合。 <a href="https://www.lesswrong.com/posts/BKvJNzALpxS3LafEs/measuring-and-improving-the-faithfulness-of-model-generated"><u>最近发现在不同的上下文窗口中解决不同的子问题可以提高推理的可信度</u></a>。就 CPM 而言，单个特征提取器不知道它要分配的分数将如何在下游使用，这使得它更难对分数进行策略性调整，也更难发挥<a href="https://www.lesswrong.com/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written"><u>阿谀奉承</u></a>或欺骗的能力。</li></ol><p>然而，CPM 仍然存在某些局限性，未来的工作可以解决这些局限性：</p><ol><li><strong>人类反馈。</strong> CPM 仍然使用人类给出的成对偏好判断作为聚合特征分数的训练信号。就人类犯错误而言，这本质上是有限的， <a href="https://www.lesswrong.com/posts/g5rABd5qbp8B4g3DE/towards-understanding-sycophancy-in-language-models"><u>有时更喜欢谄媚的反应而不是真实的反应</u></a>，或者<a href="https://arxiv.org/abs/2309.16349"><u>权威的反应而不是事实的反应</u></a>。</li><li><strong>人性化的策展。</strong>在特征选择和用于特征提取的提示模板的提示工程方面，CPM 依赖于人类。就域外泛化而言，这些因素可能会受到限制（例如，评估表现出超人表现的代理）。</li></ol><h2>包起来</h2><p>我们提出了组合偏好模型：通过在提示 LM 提取的特征之上训练逻辑回归来构建 PM 的想法。我们证明，具有 13 个参数的 CPM 在人类评估和奖励模型过度优化的鲁棒性方面优于标准 PM，同时也更具可解释性。</p><p><i>这篇文章受益于 Mikita Balesni、Richard Ren、Euan McLean 和 Marc Dymetman 的有益评论。</i>我还要感谢<a href="https://arxiv.org/abs/2310.13011"><i><u>这篇论文</u></i></a><i>的合著者</i><i>：Dongyoung Go、Germán Kruszewski、Jos Rozen 和 Marc Dymetman。</i></p><p><br><br><br><br><br><br><br><br><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oSgac8x8fgNj22ky3/compositional-preference-models-for-aligning-lms<guid ispermalink="false"> osgac8x8fgNj22ky3</guid><dc:creator><![CDATA[Tomek Korbak]]></dc:creator><pubDate> Wed, 25 Oct 2023 12:17:28 GMT</pubDate></item></channel></rss>