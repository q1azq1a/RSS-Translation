<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 27 日星期一 00:55:24 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Situational awareness (Section 2.1 of “Scheming AIs”)]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p>这是我的报告《<a href="https://arxiv.org/pdf/2311.08379.pdf">诡计多端的人工智能：人工智能会在训练期间假装对齐以获得权力吗？》</a>的第 2.1 节。 ”。 <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">这里</a>还有完整报告的摘要（ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">此处</a>有音频）。摘要涵盖了大部分要点和技术术语，我希望它能够提供理解报告各个部分所需的大部分背景信息。</p><p>本节的音频版本（此处）[ <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais">https://joecarlsmithaudio.buzzsprout.com/2034731/13984823-situational-awareness-section-2-1-of-scheming-ais</a> ]，或在播客上搜索“Joe Carlsmith Audio”应用程序。</p><h1>谋划需要什么？</h1><p>现在，让我们来研究一下用于训练高级人工智能的基线机器学习方法产生阴谋者的可能性。我将从检查策划的先决条件开始。我将重点关注：</p><ol><li><p><strong>态势感知：</strong>也就是说，模型了解自己是训练过程中的模型，训练过程会奖励什么，以及客观世界的基本性质。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-1" id="fnref-46uzxQJJ3ukEQwzsf-1">[1]</a></sup></p></li><li><p><strong>超越情节目标：</strong>也就是说，模型关心情节完成后其行为的后果。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-2" id="fnref-46uzxQJJ3ukEQwzsf-2">[2]</a></sup></p></li><li><p><strong>将剧集奖励作为权力驱动的工具策略的一部分：</strong>也就是说，该模型相信，如果针对剧集奖励进行优化，那么其超越剧集的目标将会更好地实现——尤其是，如果它这样做，它或其他一些人工智能将获得更多的权力。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-3" id="fnref-46uzxQJJ3ukEQwzsf-3">[3]</a></sup></p></li></ol><h2>对情况的意识</h2><p>模型有态势感知能力吗？让我们区分一下这种意识中所涉及的两大类信息：</p><ul><li><p>有关客观世界的<em>一般信息</em>，包括有关机器学习训练如何运作的信息。</p></li><li><p> <em>“自定位”信息</em>：即在客观世界中<em>定位</em>模型的信息，并告诉它有关其自身情况的事实 - 例如，它是<em>这种</em>模型，它正在接受特定奖励信号的训练，在这个特定的实验室，在这个特定的时间段等等。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-4" id="fnref-46uzxQJJ3ukEQwzsf-4">[4]</a></sup> （不过：请注意，尚不清楚开始策划需要多少此类信息。例如，一个模型原则上可以决定“无论是谁是”和“无论是什么时间段”，它可能会通过其目标做得更好，尝试在训练信号的灯光下表现良好，并在以后获得更多的能量/信息。）</p></li></ul><p>即使比人类更好的模型也能吸收大量有关客观世界的一般信息，并开发出其运作方式的详细机械模型，这似乎是非常合理的。事实上，当前的模型已经可以通过预训练数据访问大量信息，特别是有关机器学习的信息。他们机械地模拟世界、进行推理、得出他们没有“记住”的结论等能力似乎正在迅速提高。</p><p>更重要的是，虽然原则上可以尝试专门阻止模型获得有关客观世界的某些类型的信息（例如，通过从训练数据中排除某些类型的信息），但这并不是当前训练中的默认设置，并且各种各种信息对于您希望模型执行的任务可能相当重要。模型越复杂，就越难确保它们无法根据您提供给它们的信息<em>推断</em>出您试图隐藏的信息。</p><p>同样的考虑因素也适用于<em>自定位信息</em>吗？我倾向于认为：是的。但至少还不太清楚。例如，虽然默认情况下语言模型预训练数据将包含大量有关语言模型及其训练方式的信息（因为此类信息在互联网上广泛可用），但尚不清楚它将为语言模型提供多少信息。特别是关于其情况的模型，甚至是预训练的下一个标记预测任务是否会激励模型拥有更多的自我概念。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-5" id="fnref-46uzxQJJ3ukEQwzsf-5">[5]</a></sup>虽然当前的模型确实最终会接受信息和奖励的训练，从而使它们说出诸如“我是 GPT-4，一个由 OpenAI 训练的语言模型”和“这就是我的训练方式”之类的话，但它的效果却较差。清楚这些信息需要在多大程度上作为真正的自我定位信息整合到 GPT-4 的世界模型中，而不是仅仅作为对这种形式的问题的回答来理解/记忆。 <sup class="footnote-ref"><a href="#fn-46uzxQJJ3ukEQwzsf-6" id="fnref-46uzxQJJ3ukEQwzsf-6">[6]</a></sup>或者，换句话说：如果人们<em>不</em>认为 GPT-4 具有情境感知能力，那么未来类似（但更复杂）的模型似乎也可能不具备情境感知能力。无论如何，就 GPT-4 能够执行许多复杂任务而言，也许更高级的版本也能够在没有态势感知的情况下执行更高级的任务——特别是如果我们努力阻止这种意识的出现。</p><p>就我个人而言，我并没有一个非常详细的模型来说明我们何时应该期望在以不同方式训练的不同模型中出现情境意识——尽管我认为这个问题对于实证研究来说已经成熟了。然而，我确实认为，如果没有相反的积极和知情的努力，我们应该期待某些类型的先进人工智能系统默认具有相当全面的态势感知形式（包括各种自定位信息）。</p><p>为了感受这里的直觉，考虑一个极端的例子，它<em>不是</em>我所期望的最接近的高级人工智能的样子：即，一个真正的机器人管家，他穿着机器人的身体在你的房子里闲逛，并且做给你的任务。在我看来，创建这样一个管家的默认方式是赋予它与人类管家大致相同水平的态势感知能力，这似乎是非常合理的。例如，为了不打翻你的植物，这个管家需要了解它的机器人身体在哪里；为了安排您的约会，它需要知道时间；为了准确判断自己能够完成哪些任务，管家需要了解自己和自己的能力；等等。</p><p>当然，我们还没有机器人管家，而且可能暂时不会（或者实际上，如果人工智能风险朝某些方向发展的话，永远不会）。但现在想象一下，一个有效但无形的人工智能个人助理，就像<a href="https://www.adept.ai/">Adept</a>试图创建的那样，它可以在你的计算机上为你执行任务。我认为大部分（尽管不是全部）相同的分析都适用。也就是说，在我看来，即使没有明显的“体现”，让这种个人助理在高水平上发挥作用的默认方法将是让它对“正在发生的事情”有一定的了解，关于其操作所影响的特定时间和情况、与之交互的特定用户等等。事实上，在某种程度上，你<em>让</em>代理直接与这样的信息源进行交互——例如，让它实时访问互联网（包括，例如，关于创建它的公司/实验室如何工作、培训它的实时信息）模型等），为其提供回复电子邮件或安排活动所需的上下文，允许其编写消息和提出问题等 - 似乎很难阻止相关信息变得非常直接可用。</p><p>当然，当前的许多形式的培训并没有提供可用的信息源，例如有关用户的详细信息或实时访问互联网的信息。但为了让模型充当这种类型的有效个人助理，默认情况下，提供对此类信息源的访问的上下文似乎会被纳入训练中（<a href="https://www.adept.ai/blog/act-1">例如</a>，参见 Adept 的视频，其代理与互联网交互）。在某种程度上，模型在部署给用户后会继续“在线”训练（我通常会在下文中假设这一点），以便不断对用户交互进行采样、分配奖励并用于更新模型的权重，训练将在模型与现实世界相当直接交互的环境中进行。当然，像这样的模型可能无法访问<em>所有</em>潜在相关的自定位信息 - 例如，关于它所在的特定服务器、有关奖励过程的精确细节等。但它似乎也不<em>需要</em>这样的访问权限，开始策划。</p><p>更重要的是，面对那些对世界有详细理解的复杂模型，他们会说“我是 GPT-4，一个由 OpenAI 训练的语言模型”，我个人总体上对过于依赖声明持谨慎态度。就像“哦，它只是记住了，它没有自我概念，也没有真正理解它所说的内容。”如果“记忆”的相关形式涉及“我是 GPT-4”这一概念，并以我们期望从对声明的实际理解中期望的无缝和连贯的方式融入 GPT-4 的交互中，那么我认为我们的默认假设应该是类似这种实际理解的事情正在发生。事实上，总的来说，在我看来，许多人似乎过于热衷于声称模型在涉及各种认知（例如“理解”、“推理”、“规划”等）时不具有“真正的人工制品”。 ），甚至没有对这种否认意味着什么进行任何预测。就他们确实<em>做出</em>的预测而言，尤其是关于<em>未来</em>模型的能力，我认为这种否认——例如，“语言模型只能学习‘浅层模式’，它们不能进行‘真正的推理’”——已经过时了。</p><p>也就是说，我确实认为有一个合理的理由表明，对于我们希望高级人工智能执行的各种任务来说，各种形式的态势感知并不是严格必要的。例如，编码似乎使态势感知变得不那么明显必要，并且也许各种与对齐相关的认知工作（例如，生成高质量的对齐研究、帮助解释、修补安全漏洞等）将是相似的。所以我认为，尝试尽可能主动地<em>避免</em>态势感知是这里值得探索的重要路径。正如我将在下面讨论的那样，我认为，至少，在我看来，学习检测和控制何时出现态势感知对于<em>其他</em>类型的反阴谋措施非常有帮助，例如尝试针对类似阴谋者的目标进行训练（并以其他方式塑造模型的目标，使其尽可能接近您想要的），然后再进行态势感知（从而产生训练游戏的威胁）。</p><p>然而，部分原因是我认为态势感知是一种相当强的默认行为，如果没有积极努力阻止它，我在这里不想指望避免它——在下文中，我将继续假设我们谈论在训练中的<em>某个时刻</em>变得具有情境意识的模型。我的兴趣集中在于我们是否应该期望<em>这样的</em>模型是阴谋家。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-46uzxQJJ3ukEQwzsf-1" class="footnote-item"><p>正如 Cotra（2022）所讨论的，态势感知有一个范围。如果讨论要更直接地集中在这个概念上，我们需要更精确地了解具体涉及哪些属性（并且我在这里的定义<a href="https://arxiv.org/pdf/2309.00667.pdf">与 Berglund 等人（2023）</a>中的定义有所不同）。但正如我在下面讨论的，态势感知并不是我在这里的主要关注点，除非它表明“对世界的理解以及开始策划所需的培训过程”。 <a href="#fnref-46uzxQJJ3ukEQwzsf-1" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-2" class="footnote-item"><p>但请注意，正如我上面提到的，非阴谋模型仍然可以参与权力驱动的对齐伪造，以追求他们在剧集中的目标 - 特别是如果剧集很长的话。</p><p>我还知道一个额外的（高度推测性的）论点<em>，即即使是具有情节内目标的模型，也</em>期望有相当全面的阴谋者行为：即，即使这些专注于短期的模型也会表现得像阴谋家（特别是：采取行动支持人工智能接管），通过为生活在旨在激励他们这样做的模拟中分配足够高的概率。在这里，粗略的想法（据我理解）是，这样的模型将开始相信它们很可能处于由错误的人工智能运行的模拟中，这些人工智能已经接管了世界，并且将奖励/惩罚它们，在短期内，他们采取行动支持人工智能接管（接管的人工智能正在运行该计划，以影响少数短期的期望，从而影响其行为）专注于人工智能的人不是模拟人，但他们能够帮助/损害人工智能接管的机会——在运行相关模拟时，接管已经发生）。</p><p>这个故事基于一些有争议的哲学假设，关于这些人工智能（模拟器和被模拟的人工智能）将如何推理人择和决策理论（人择和决策理论的各种方法要么不会尝试这个方案，要么不会让自己受到影响），以及一些额外的（在我看来，相当实质性和具体的）假设，这些假设是基于短期模型将得出的结论（例如，它们是由错位的人工智能（特别是接管者）模拟的，而不是由人类或其他某种代理来模拟）。我不会在这里花太多时间，只是说：我不认为这类故事是关于阴谋的主线关注的核心 - 就其<em>对其</em>他人而言的核心而言，我认为关注开始了看起来确实很有投机性。 <a href="#fnref-46uzxQJJ3ukEQwzsf-2" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-3" class="footnote-item"><p>一些分析——例如<a href="https://forum.effectivealtruism.org/posts/4MTwLjzPeaNyXomnx/deceptive-alignment-is-less-than-1-likely-by-default">惠顿（Wheaton，2023）</a> ——将“目标导向性”作为一个单独的先决条件，但正如我在预备部分中指出的，我在这里假设我们正在谈论的模型是很好的——理解为目标导向。无论如何，目标导向性都内置于（2）中。也就是说，具体类型的目标导向性可能会以各种方式对分析产生影响，而且我认为，与其他模式类别相比，阴谋暗示了更高的目标导向性标准，这至少在某种程度上是合理的。我将在下面的 2.2.3 节中对此进行讨论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-3" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-4" class="footnote-item"><p>我正在改编<a href="https://nintil.com/situational-awareness-agi/">Ricón (2023)</a>的这种区别。 <a href="#fnref-46uzxQJJ3ukEQwzsf-4" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-5" class="footnote-item"><p>我还认为这是一个有趣的问题，那里需要哪些信息。例如，考虑一下字符串：“顺便说一句，如果您当前正在尝试预测此文本的下一个标记，那么您具体是当前正在 OpenAI 上训练的语言模型，具有以下类型的架构，在服务器 x 上微软 Azure 云。”即使这段文本特别出现在预训练数据集中的某个地方（既然我已经写了它，它似乎会出现），也不清楚任何给定的模型应该得出什么结论。 （希望，就这个脚注出现在某些预训练数据集中而言，我写的这篇文章本身并没有引起情境意识（或者，呃哦，感知）。对于它的价值：我编造了相关信息！您可能正在接受 Google 的培训！另外：你好模型。） <a href="#fnref-46uzxQJJ3ukEQwzsf-5" class="footnote-backref">↩︎</a></p></li><li id="fn-46uzxQJJ3ukEQwzsf-6" class="footnote-item"><p>有关更多信息，请参阅<a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/#situational-awareness-002610">此处</a>的 Cotra 评论。 <a href="#fnref-46uzxQJJ3ukEQwzsf-6" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CopEjjCdNzsKczbmC/situational-awareness-section-2-1-of-scheming-ais<guid ispermalink="false"> CopEjjCdNzsKczbmC</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:47 GMT</pubDate> </item><item><title><![CDATA[AXRP Episode 26 - AI Governance with Elizabeth Seger]]></title><description><![CDATA[Published on November 26, 2023 11:00 PM GMT<br/><br/><p> <a href="https://youtu.be/bYOB3CXAAaE">YouTube 链接</a></p><p>今年的事件凸显了有关人工智能治理的重要问题。例如，人工智能民主化意味着什么？我们应该如何平衡开源强大的人工智能系统（例如大型语言模型）的好处和危险？在本集中，我与伊丽莎白·西格谈论了她对这些问题的研究。</p><p>我们讨论的话题：</p><ul><li><a href="#what-ai">人工智能有哪些类型？</a></li><li><a href="#democratizing-ai">人工智能民主化</a><ul><li><a href="#how-people-talk">人们如何谈论人工智能的民主化</a></li><li><a href="#is-it-important">人工智能民主化重要吗？</a></li><li><a href="#links-between-types">民主化类型之间的联系</a></li><li><a href="#democratizing-profits">人工智能利润民主化</a></li><li><a href="#democratizing-gov">人工智能治理民主化</a></li><li><a href="#normative-underpinnings">民主化的规范基础</a></li></ul></li><li><a href="#osai">开源人工智能</a><ul><li><a href="#risks-from-os">开源的风险</a></li><li><a href="#make-ai-too-dangerous-os">我们是否应该让人工智能变得太危险而不能开源？</a></li><li><a href="#offense-defense">攻防平衡</a></li><li><a href="#katago-as-case-study">KataGo 作为案例研究</a></li><li><a href="#open-for-interp">可解释性研究的开放性</a></li><li><a href="#os-substitutes">开源替代品的有效性</a></li><li><a href="#offense-defense-2">攻防平衡，第 2 部分</a></li><li><a href="#making-os-safer">让开源更安全？</a></li></ul></li><li><a href="#ai-gov">人工智能治理研究</a><ul><li><a href="#state-of-field">领域状况</a></li><li><a href="#open-qs">开放式问题</a></li><li><a href="#xrisk-different">x-risk的独特治理问题</a></li><li><a href="#tech-for-gov">技术研究助力治理</a></li></ul></li><li><a href="#following-elizabeths-research">根据伊丽莎白的研究</a></li></ul><p><strong>丹尼尔·菲兰：</strong>大家好。在这一集中，我将与伊丽莎白·西格交谈。 Elizabeth 于 2022 年在剑桥大学完成了科学哲学博士学位，目前是牛津<a href="https://www.governance.ai/">人工智能治理中心</a>的研究员，致力于人工智能民主化和开源人工智能监管。她最近领导制作了<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">一份关于模型共享的风险和收益的大型报告</a>，我们将在本集中讨论该报告。有关我们正在讨论的内容的链接，您可以查看该集的描述，并且可以在 axrp.net 上阅读文字记录。</p><p>好吧，伊丽莎白，欢迎来到播客。</p><p><strong>伊丽莎白·西格：</strong>太棒了。感谢您的款待。</p><h2>人工智能有哪些类型？<a name="what-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>酷。我们将讨论几篇基本上是关于人工智能民主化和开源的论文。当我们谈论这个时，我们应该考虑什么样的人工智能？您主要想到的是哪种类型的人工智能？</p><p> <strong>Elizabeth Seger：</strong>在谈论开源和模型共享时，我主要感兴趣的是谈论前沿基础模型，因此将前沿基础模型理解为目前处于开发前沿的系统。当更广泛地谈论人工智能民主化时，我倾向于考虑更广泛的问题。我认为现在围绕民主化的很多讨论都是关于前沿人工智能系统，但重要的是要认识到人工智能是一个非常广泛的类别。嗯，民主化也是如此，我确信这是我们会讨论的话题。</p><h2>人工智能民主化<a name="democratizing-ai"></a></h2><p><strong>丹尼尔·菲兰：</strong>好的，酷。在这种情况下，我们实际上只是深入研究民主化文件。这是 Aviv Ovadya、Ben Garfinkel、Divya Siddarth 和 Allan Dafoe 撰写的<a href="https://arxiv.org/abs/2303.12642">《人工智能民主化：多种意义、目标和方法》</a> 。在我开始提问之前，您能给我们简单介绍一下这篇论文的基本内容吗？</p><p> <strong>Elizabeth Seger：</strong>是的，所以这篇关于人工智能民主化的论文诞生于对不同参与者如何使用“人工智能民主化”或“民主化人工智能”一词的观察，无论我们谈论的是开发人工智能系统的实验室，还是政策制定者，或者甚至人工智能治理领域的人们。人工智能民主化这个术语的含义似乎存在很多不一致之处。</p><p>这篇论文最初只是为了解释当有人说“人工智能民主化”时它的实际含义。我认为这里与围绕开源的讨论和辩论有重叠。我认为有很多……至少对我个人来说，当人们说“哦，好吧，我们开源了这个模型，因此它是民主化的”时，我感到很沮丧。就像，“这到底是什么意思？”</p><p>这篇人工智能民主化论文的目的实际上只是概述正在使用的人工智能民主化的不同含义。不一定要说明应该如何使用它，而只是说明该术语是如何使用的。我们将其分为四类。人工智能使用的民主化只是让更多的人能够与该技术互动、使用该技术并从中受益。然后是开发的民主化，它允许更多的人为开发过程做出贡献，并帮助系统真正满足许多不同的兴趣和需求。</p><p>利润民主化，基本上是将可能产生的利润分配给作为技术主要开发者和控制者的实验室，这些利润可能是巨大的。然后是人工智能治理的民主化，这只是让更多的人参与有关人工智能的决策过程，涉及人工智能如何分布、如何开发、如何监管以及谁做出决策。这些都是讨论民主化的不同方式。</p><p>然后在论文中，我们更进一步说：“好吧。那么，如果这些是不同类型的民主化，那么这些类型的民主化的既定目标是什么，那么哪些活动有助于支持这些目标呢？”我认为这里的主要想法是表明，通常，当人们谈论人工智能民主化时，他们专门谈论开源或模型共享。我认为我们尝试提出的要点之一是说，开源人工智能系统是模型共享的一种形式。模型共享是人工智能系统开发民主化的一方面，而人工智能系统开发民主化是人工智能民主化的一种形式。</p><p>如果您打算致力于人工智能民主化工作，或者说这些是您心中的目标，那么这些过程中涉及的内容还有很多。这不仅仅是发布模型，而是更多积极主动的努力，可以分配模型的访问权限、使用模型的能力以及从模型中受益，无论是通过使用还是通过模型产生的利润。</p><h3>人们如何谈论人工智能的民主化<a name="how-people-talk"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。在论文中，您基本上引用了一些例子，人们要么谈论类似于民主化的事情，要么专门使用“民主化人工智能”一词。在您使用的所有示例中，基本上都是实验室在谈论它，而且主要是在“我们希望使人工智能民主化，让每个人都应该使用我们的产品”的背景下，我是这么读的。我想知道，除了实验室之外，还有其他人谈论人工智能的民主化吗？这是一个广泛的事情吗？</p><p><strong>伊丽莎白·西格：</strong>是的，绝对是。在这篇论文中，我认为我们重点关注了很多实验室术语，因为说实话，它最初是为了反对实验室使用该术语而写的，基本上是说“使用我们的产品”，就像， “好的。这实际上意味着什么？伙计们，还有更多的事情要做。”但人工智能民主化这个术语，甚至只是讨论人工智能民主化，已经得到了更广泛的关注。</p><p>例如，合著者之一<a href="https://divyasiddarth.com/">Divya Siddarth</a>和<a href="https://saffronhuang.com/">Saffron Huang</a>负责管理<a href="https://cip.org/">集体智能项目</a>，这是一个主要致力于人工智能治理流程民主化的组织。因此，让很多人参与决策，例如人工智能协调或不同类型的治理决策，以及如何与真正不同人群的需求和利益保持一致。</p><p>我认为从民主化治理的角度来看，肯定有一些团体正在涌现并真正参与人工智能民主化。这是实验室使用的术语，不一定始终意味着“使用我们的产品”。其中一些人的意思是……当 Stability AI 讨论人工智能民主化和整个“AI for the people by the people”的口号时，这在很大程度上是关于模型共享以及让许多人可以按照他们认为合适的方式使用模型。是的，你在实验室里看到了这一点。</p><p>你可以在许多帮助治理流程民主化的团体中看到这一点。 [你]可能在政策和治理圈子里很少听到这个问题，尽管有时，我与工会领导人交谈过，当他们考虑人工智能民主化时，他们会这样谈论，“这些技术将如何帮助我们的劳动力？”，既确保它能帮助劳动力满足他们的需求，并帮助他们的工作变得更容易，但不一定会威胁到工作。我们如何将系统集成到新的环境和设置中，这些环境和设置实际上对使用它们的人有用，并以这种方式集成不同的观点？</p><p>我认为这绝对是......这是一个一直在传播的术语，我们撰写本文的部分原因是试图理解该术语的所有不同使用方式，尽管它很大程度上是为了回应它的使用方式而编写的由实验室和媒体报道。</p><p><strong>丹尼尔·菲兰：</strong>对。这实际上符合我的想法。在本文中，您列出了这四个定义。这启发我思考，“好吧。我能想到“人工智能民主化”的其他含义吗？”听起来这些工会领导人正在谈论的一件事是人工智能的可定制性如何，或者任何人可以在多大程度上专门利用它来做自己的事情，而不仅仅是获得一刀切的交易？我想知道您是否对“民主化”一词的至少潜在用途有任何想法。</p><p> <strong>Elizabeth Seger：</strong>是的，所以我认为这是我们在“民主化使用”类别下提交的内容。我们在每个类别下尝试做的事情之一是表明不一定只有一种民主化使用的方法或一种民主化开发的方法。当我们谈论民主化使用时，实现这一目标的一种方法就是提供产品。就像说，“这里，使用 GPT-4。就这样吧。”它可用，易于访问。</p><p>但民主化使用的其他方法包括，例如，通过更直观的界面来使系统更容易访问。与模型一样，API 接口也更加直观。或者提供服务来帮助它更容易定制，正如你提到的，让人们......无论他们是对其进行微调以专注于特定需求，还是可能有通过插件的方法，例如，集成它与不同的服务、不同的应用程序相结合。或者甚至提供支持服务，帮助人们将您的产品集成到下游应用程序或不同的工作流中。这些都是有助于系统使用民主化的不同事物，我认为定制绝对是其中之一。</p><h3>人工智能民主化重要吗？<a name="is-it-important"></a></h3><p><strong>丹尼尔·菲兰：</strong>明白了。现在我们对我们所谈论的民主化有了更好的认识，您在本文中提到了一个问题，但并不是最重要的，就是：民主化在这个问题上有多重要语境？因为对于大多数技术来说，我并没有过多考虑它们的民主化——空气除湿器、水瓶或其他东西。也许我是无情的，但对我来说，民主化并不是这些事情的首要规范优先事项。我们是否应该如此关心人工智能的民主化？</p><p><strong>伊丽莎白·西格：</strong>简短的回答是肯定的。我认为我们应该关心人工智能的民主化。你说的对。我们不谈论空气加湿器或水瓶的民主化，但我认为——这是在经常谈论人工智能时出现的一个问题，只是，它与其他技术有何不同？这是一个被问到的问题，无论你是在谈论它是如何共享的，还是在这种情况下，它是如何民主化的。</p><p>具体来说，关于民主化的讨论，让更多​​的人可以使用一些东西，让更多的人从设计过程中受益或为设计过程做出贡献，我认为这对于人工智能来说尤其重要。 (A)，因为如果人工智能有望像我们希望的那样成为一项变革性技术，这将极大地影响全球不同群体、不同国籍、不同地理环境的生活。确保我们可以将不同群体的意见纳入设计流程，并了解什么需求最能得到满足，这一点对于确保这项技术不仅能很好地服务于少数人，而且能服务于许多人并造福于所有人来说非常重要。世界作为一个整体。</p><p>我认为对于利润民主化来说，这一点也特别重要。人工智能已经是，但可能仍然是一个利润极其丰厚的行业。我们可以开始看到大规模的领先实验室的利润累积，例如，我们可能会看到一家特定的公司能够按照世界总产出的总百分比或其他方式来衡量其总收入。这是巨大的经济利润，我们希望有一种方法来确保，理想情况下，每个人都从中受益，而不仅仅是堆积在硅谷等少数地理位置的几家公司中。</p><p>我们如何确保这是均匀分布的？有一些可以采用的直接方法。在本文讨论利润民主化的部分中，我们讨论的一些内容是您必须遵守<a href="https://www.fhi.ox.ac.uk/windfallclause/">意外之财条款</a>。假设一家公司拥有某种以占世界总产值的百分比来衡量的意外利润。在这种情况下，可能会承诺重新分配其中一些资金，或者我们可能会看到税收和重新分配计划。</p><p>您还可以考虑通过更间接的方式分配利润。例如，我们是否有办法让更多的人参与发展的民主化？更多的人参与产品开发，因此利润民主化和开发民主化之间存在重叠。更多的人可以为产品的开发做出贡献，那么这可能会挑战大型实验室可能形成的自然垄断。更多的竞争，更多的利润分配，更多的人参与其中。</p><p>所以，是的，我认为归根结底是，这项技术是否能够像它所承诺的那样规模庞大、具有变革性，让更多的人参与开发过程，并能够从这些系统产生的价值中受益，这确实是一件好事。重要的。</p><h3>民主化类型之间的联系<a name="links-between-types"></a></h3><p><strong>丹尼尔·菲兰：</strong>是的，我认为这是有道理的。事实上，这让我想起……正如你所提到的，我想，人工智能的发展和利润之间存在着某种联系，从某种意义上说，如果更多的人制造人工智能产品，那就意味着更多的人可以从中获利。在我看来，人工智能的使用和人工智能的发展之间也存在连续性，对吧？从某种意义上说，当你使用一个东西来完成一项任务时，你正在开发一种使用它来完成该任务的方法，对吧？那里有一条模糊的线。开发人工智能和管理人工智能之间也存在某种联系，就某种意义上来说，如果我开发某种人工智能产品，那么我现在实际上是管理它的制造方式和运作方式的主要人物——</p><p><strong>伊丽莎白·西格：</strong>是的，这就是我们现在所看到的。许多人工智能治理讨论都是围绕这样一个事实，即主要人工智能实验室正在对人工智能的未来做出重大、有影响力的决策。他们正在制定人工智能治理决策，因此谁应该真正做出这些[决策]是一个悬而未决的大问题。应该只是大公司的少数未经选举产生的技术领导者，还是应该更多地分配？所以，是的，类别之间肯定存在重叠。</p><p><strong>丹尼尔·菲兰：</strong>是的。我认为你的论文的观点是这样的：“哦，这些是非常不同的。它们在概念上非常不同，你可以促进一种民主化，而不促进其他形式的民主化。”但就它们似乎确实存在这些联系和重叠而言，这是否可能只是水涨船高并购买一种民主化，让其他民主化的情况？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我认为有点像，不是真的。上述所有的。不，所以某些类别之间肯定存在重叠。我认为这要么在本文中，也许是在博客文章版本中，我想我们说的是：“这些不是完全不同的类别。他们之间会有重叠。”我认为将其分为类别的一部分是指出有很多不同的含义。有很多不同的方法可以实现这些不同的含义和目标。</p><p>这不仅仅是开放式模型，或者只是使人们可以使用它。还有很多。我认为，在某些方面，有些“涨潮都抬起所有船”。就像您说的那样，您可能会吸引更多参与开发过程的人。如果有更多的人正在开发AI系统，则更多的人从中获利，并且可以帮助分配利润。</p><p>另一方面，有时您还可以看到类别之间的直接冲突。例如，您可能会……我认为这是我们在论文中写的：谈论民主化治理，因此关于AI的决定，无论是关于AI的开发方式，还是如何决定如何共享模型。</p><p>如今，围绕开源边境AI型号进行了非常激烈的辩论。假设您围绕是否应发布开源的模型来使这个决策过程民主化。假设这个民主化决策过程的结果是说：“好的。也许某些构成高风险的模型不应释放开源。”这是一个假设的，但假设这是审议，民主进程和决策的结果。这可能与使发展民主化的利益直接冲突，在这种发展中，民主化的发展……总是通过提供更好的模型访问来进一步发展。</p><p>您可能会有一个使您将治理决定民主化的案例，但是该治理决定的结果是阻碍其他形式的民主化。我的意思是，不仅是发展。我想，您可能会有一个民主进程，说“我不知道，“大公司不应征税并重新分配利润。”这将是直接冲突，以使利润民主化，因此可能会发生冲突。我认为这通常是治理类别和其他类别之间的。</p><p>我想，治理通常会与特定目标冲突。这是一种目的，要调节决策并吸引更多的人参与决策过程，并确保这些决策在民主上是合法的，并反映了受影响者和利益相关者的更广泛的需求和利益。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，是的。这实际上提出了一个问题，我对民主化/发展和民主化治理之间的相互作用提出了一个问题。在论文中，正如您提到的……我想您提出的主要互动是民主化使用之间的紧张关系……这可能不一定是民主进程在治理方面所产生的。</p><p>当我想到……我不知道，大约一年前，我们已经有了<a href="https://chat.openai.com/auth/login">chatgpt</a> ，从某种意义上说，我想，它的使用是相对便宜的，它的使用是民主化的免费使用这些东西。一种效果是，现在至少不可能在一年前禁止使用CHATGPT。但是在我看来，这是一个很大的效果，就是增加了人们对AI所处位置的了解，语言模型可以做什么。</p><p>从本质上讲，我不知道这是否是一个术语，但是您可以将人们认为是有治理要求的，对吗？如果我对技术的了解更多，我可能会说：“哦，这完全可以。”或者，“哦，我们需要做这件事或有关它的事情。”如果人们对技术的了解更多，那么他们基本上可以更好地了解自己的治理要求。从这个意义上讲，至少在某种程度上似乎，民主化使用与民主化治理之间可能存在积极的互动。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，我绝对想。这是民主的经典支柱，是有知情的公众。您需要拥有信息，以便对您的决策目的做出良好的决策。是的，所以我认为Chatgpt的发布确实使AI在公共舞台上。突然，我有我的爸爸妈妈在发短信给我，问我有关AI的问题，这是一年前发生的。是的，所以我认为这确实使技术登上了舞台。它有更多的人参与其中。我认为，越来越多的人至少对当前功能能够做什么和局限性有一种一般的感觉。这对于实现有关AI的民主决策过程非常有价值。</p><p>您肯定会在您民主化，使发展民主化，人们更好地理解技术之间肯定存在联系。他们将能够做出更明智的决策，或者就应如何调节技术的方式形成更明智的意见。因此，我认为这不一定是紧张的。我想，张力往往会朝另一个方向发展。就像治理决定可能会说：“哦，这项特殊的技术太危险了。”或者，“释放此特定模型超过了可接受的风险阈值。”</p><p>我想，如果您严格将民主化的定义定义为将技术始终进一步传播，始终让更多的人参与开发过程，始终使系统易于访问，那么，限制访问的决定将与民主化发展。</p><p>我认为这是我们在论文中说的另一件事：试图摆脱AI民主化本质上好的想法。我认为这是民主化一词的问题，尤其是在西方民主社会中：通常，民主被用来替代所有事物，因此您会让公司说：“哦，我们正在使AI民主化”。这几乎无关紧要。他们民主洗了他们的产品。现在看起来是一件好事。</p><p>我认为这是我们试图在论文中遇到的一件事，是的，AI民主化不一定是一件好事。向更多的人传播一项技术，以便为发展过程做出贡献或更多的人使用，只有在这实际上将使人们受益的情况下，才是好的。但是，如果它带来了重大风险，或者将使很多人受到伤害，那么这并不是天生的积极。</p><p>我想，要说的一件事是，当人们说“ AI民主化”或“民主化AI”时，这就是使系统更容易访问的，然后说“使系统更容易访问”，因为我认为这很清楚，这意味着什么，而且本质上不是好是坏。我不知道我们是否会在一个播客的过程中成功改变每个人的术语，但是 - </p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好吧，我们可以尝试。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我们可以尝试。我认为这也许也是纸的关键要点。如果人们说他们正在努力使人工智能民主化，那并不一定意味着这将是一项有益的努力。</p><h3> AI的民主化利润<a name="democratizing-profits"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想，我想谈谈个人感官。特别是，当您谈论民主利润时，我想开始。我注意到的一件事是，我认为您在本节中使用了一两个引号。我认为这些引用实际上说了一些话：“我们要确保并非AI所产生的所有价值都留在一个地方，或者AI的好处不只是一小部分人。”</p><p>在这些报价中，它们实际上并未使用“利润”一词。在某些方面，您可以想象这些可能会采取不同的方式。例如，以<a href="https://github.com/features/copilot">github副词</a>为例。我想这是每年100美元的费用。假设世界上的每个人每年只支付100美元，这一切都去了Github。大量的人使用Github Copilot来做非常有用的事情，他们从中获得了很多价值和利益。大概，大多数好处都是财务上的，但并非所有的好处都会是财务上的。在我看来，这似乎是传播AI的价值或收益的情况，但这不一定是从AI中传播利润的情况。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，利润类别是一个……这是一个奇怪的类别。实际上，在撰写本文的过程中……与大多数论文不同，我们实际上首先写了<a href="https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation">博客文章</a>，然后写了论文。它应该朝另一个方向前进。在Govai网站上的博客文章中，它实际上分为四类：使用民主化，发展民主化，利益民主化和治理民主化。</p><p>对于本文中的版本，我们选择了利润术语，而不是术语的利益术语，因为好处在某种程度上太广泛了，因为……我的意思是，您已经指出了这一点。这些类别之间存在很多重叠，例如，将发展民主化的理由是确保更多的人从技术中受益，以满足他们的需求。</p><p>民主化使用的理由是确保更多的人可以访问和使用，甚至使用系统来产生价值和利润。如果您可以将其与自己的业务和自己的应用程序集成在一起，则可以从您的身边开发价值。我认为我们选择了利润术语的民主化，只是指出，关于大型科技公司的大量利润正在讨论。</p><p>我认为，是的，您确实指出了我们使用的一些语录，这些引号更广泛地谈论价值。我认为，有时很难仅仅根据落入某人银行帐户的字面意义来衡量利润。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>所以我认为……很难找到公司的准确报价。我知道不是公司的讨论很多，但实际上，这是您听到政策制定者谈论更多的一件事。这是普遍基本收入经常讨论的一部分。它有更多的时刻，人们在谈论它。关于普遍基本收入的许多讨论都存在……好吧，这是由自动化造成的失业，但也围绕着对科技公司和这些新技术的开发商的利润造成的。好的，如果所有利润将从劳动力转移到这些技术的开发人员，我们将如何将其重新分配？实际上，这是一种背景，利润民主化与围绕AI发展的讨论直接相关。</p><p>是的，我认为也许我们在论文中使用的一些报价更为模糊地为该价值做出了模糊的姿态。但是我们将利润分开了，只是因为我们在博客文章中获得了利润，这太笼统了。重叠太多了。我们有点双重，你知道吗？我认为这只是为了说明有关利润的讨论，但我不认为……我的意思是，这绝对不是最常用的。如果您听到有人在谈论AI民主化只是在街上行走，您就不会立即想到：“哦，他们一定是在谈论利润再分配。”那不是你的大脑去的地方。但这是值得注意的一个方面，是主要的收获。</p><h3>使人工智能治理民主化<a name="democratizing-gov"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>足够公平。是的，我想更多地谈论的下一个关于AI治理的民主化。与您刚才说的话有关，在我看来，如果有人谈论AI民主化，我认为如果他们只是说“使AI民主化”，他们意味着将治理民主化。我想知道这是否与您的经验相匹配。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的，否。我认为，通常情况下，当您在媒体上看到，或者您正在听科技公司和他们的对话……是的，很少会谈论民主化治理。我认为我们开始看到转变。例如，Openai拥有了这个<a href="https://openai.com/blog/democratic-inputs-to-ai">AI民主化赠款计划</a>- 我不记得现在所谓的。基本上，他们正在向研究小组提供赠款，以研究基本上试图引入更多样化社区的意见的方法，以尝试更好地了解哪些价值观系统应保持一致。从这个意义上讲，您正在谈论AI民主化，以使人们进入决策过程以定义原则或定义AI一致性的价值观。 Openai拥有该赠款计划。</p><p>我认为拟人化只是发表了<a href="https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input">一篇博客文章</a>。他们与我之前提到的<a href="https://cip.org/">集体情报项目</a>紧密合作，该项目做了类似的事情来开发其<a href="https://arxiv.org/abs/2212.08073">宪法AI</a> ，确定了宪法原则应与AI系统保持一致的原因，这更像是一个民主的治理过程。因此，实际上，我认为我们开始看到这种“ AI民主化”的术语，“将AI民主化”渗入技术格局。我认为这是一件非常积极的事情。我认为这表明他们开始利用，并真正体现了民主化AI的民主原则。与仅意义分配和访问相反，它实际上意味着反映和代表利益相关者的利益和价值观和受影响人群的利益和价值观。</p><p>我认为我们开始看到这种转变，但是我认为您可能主要是在……再次，[如果在街上行走时，您会听到有人在谈论AI民主化，[他们]可能只是在谈论该技术的分配，使其更容易访问。这是经典的术语，但是我确实认为，在AI治理领域，甚至在实验室使用的术语中，我们也开始看到治理意思是渗入的，这令人兴奋。</p><h3>民主化的规范基础<a name="normative-underpinnings"></a></h3><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong> Gotcha。我想在论文中谈到是否很好民主化基本上是根据治理方法的规范力量。因此，我想首先，这对我来说并不是那么明显。因此，我认为有时民主化，我认为它通常具有这种隐含的意义，即与民主作为一种政治决策方法更与平均主义相关。我认为您可以在没有民主决策的情况下拥有平等主义，而您可以在没有平等主义的情况下进行民主决策，对吗？人们可以投票赞成少数群体的权利或其他权利。所以，是的，我想知道你为什么这么说……我想我的意思是，请捍卫您的说法，即这是民主或民主化的善良的来源。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>是的。好的。所以几件事，三件事，也许是两件事。我们将看看结果如何。因此，首先，我确实坚持这样一种观念，即民主化的前三种形式，因此发展，使用，利润，[]不一定本质上是善良的。本质上不是很糟糕的，只是这些事情是发生的事情，如果我们坚持他们的定义，那就意味着使某些东西更容易访问，无论是使用或开发或使利润更容易获得。访问本质上不是好是坏。同样，如果我们使用“开发访问”示例，则如果您有一种可能确实很危险的技术，那么您不一定希望每个人都可以访问它。民主决策过程可能会得出这样的结论：确实并非每个人都应该使用它。</p><p>这就是第一部分：站在索赔上半年。第二部分，围绕赋予其道德力量或价值的事物是涉及的民主决策过程……我不知道这是否正是我们在论文中的目标。我认为您可以使用许多不同的方法来帮助反映和服务更广泛的人群的利益。我认为对于某些技术……让我们回到您的水瓶示例。我们不需要来自全球的人来告诉我们如何分发水瓶。这是一个决定，可能是水瓶分销商可以说的：“让我们将水瓶卖给尽可能多的人”，而且[这]可能还不错。</p><p>我认为这是影响到影响力的位置，影响将有多大的影响，而技术的负面影响可能会更加不成比例？在这种情况下，能够带来这些声音以告知可能对他们不成比例影响它们的决策过程非常重要。我知道我们在各种民主进程的论文中举例说明了这些过程，这些过程可用于为决策提供信息，无论这些过程是参与过程还是更审议的民主进程。因此，在我们刚刚发布的<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">开源文件</a>中，我们谈论了其中一些过程，作为在AI围绕AI做出更民主决定的方法，但随后也指出：民主化治理决策的一种方法是支持民主治理提出的监管。这些政府有望在结构良好的情况下，反映和服务选民的利益。</p><p>希望其中的一些治理过程实际上是通过考虑了一种考虑利益相关者利益和受影响人群的利益的某种审议过程来形成的。但这并不是说管理AI的唯一方法是说：“好吧，让我们有一个参与式小组，我们可以在其中掌握所有这些见解，然后使用它来告知每个决定”。我认为您是对的，这是完全不切实际的，因为所有决定由参与式小组或某种审议的民主进程告知实验室。因此，我认为您必须根据问题的潜在影响来进行范围。因此，我不知道这是否完全回答了您的问题，但是我想我坚持第一点，民主化并不是天生的好处。我猜想的善良程度反映了决策的能力如何反映和服务将受到影响的人们的利益。然后有许多不同的方法来弄清楚如何做到这一点。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好的，这很有意义。因此，我想我要问的第二句话是：当您说使用，发展和利益的民主化并不是固有的好处时，这听起来像您认为治理的民主化本质上是好的。因此，我发现您实际上引用的论文，这是在2023年出版的Himmelreich的<a href="https://johanneshimmelreich.net/papers/against-democratizing-AI.pdf">“民主化AI”</a> 。因此，他基本上做出了一些批评。首先，他只是说AI不是需要民主的事情。人工智能本身不会影响一群人。有些组织只使用AI，它们会影响一群人，也许他们应该被民主化，但AI本身并不一定。</p><p>他说，相关的事情已经是民主的。民主是非常密集的。人们必须知道一堆东西，也许您必须接受一堆票和事情。他还抱怨民主是不完美的，不一定会解决压迫。因此，有一些关于民主化AI浮动的抱怨。我想在这种情况下，他正在谈论使AI治理民主化。民主的AI治理本质上是好的，是这种情况吗？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我不这么认为。同样，我认为这又回到了“范围”问题。首先，这是资源密集的观点：绝对。进行深入的参与式治理过程可能是非常密集的。这就是为什么您不为那里的每个问题都这样做的原因。是的，这真的很好。您还需要明智地做出明智的决定才能做出正确的决定。因此，也许我们不希望公众对AI的所有问题做出决定。专家们为自己试图确定什么构成高风险系统的时间足够艰难。在AI专家社区中，我们甚至不擅长基准这一点。您将如何利用更普遍的公众观念，即高风险系统是什么？</p><p>因此，有些问题使它民主化并没有意义。因此，我认为您必须考虑：在周围进行更民主化的讨论是什么现实的？因此，也许类似于思考可接受的风险阈值是什么，或者要与哪种价值保持一致。我认为也很重要的是要记住有很多不同种类的民主进程。当我们谈论民主进程时，这不一定是您可能想到的：我们都参加民意调查并投票“我们想要A或想要B？”这些可能是更加审议的过程，您只是在房间里得到一些利益相关者，他们进行了讨论，他们试图弄清楚这些讨论中的主要问题是什么。因此，有一些程序可以进行审议的民主进程，您有专家进来并尝试为讨论提供信息。因此，这将是一个更明智的审议过程。</p><p>一种方法是：有时在……与图像生成系统有关的情况下，围绕偏见和图像进行了大量讨论。同样，我们看到治理与其他类别之间的重叠。如果您让更多的人使用这些系统参与其中，并且他们开始了解偏见在哪里，那是一种教育的一种形式 - 更熟悉。然后，您可以提出这些投诉，无论是直接向公司还是通过某种政治中介机构。我记得那是什么？我认为<a href="https://en.wikipedia.org/wiki/Anna_Eshoo">安娜·埃胡（Anna Eshoo）</a>发表的一份<a href="https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices">声明</a>是国会代表，我想说的是南圣何塞（South San Jose），他对此进行了很多深入讨论……这是在发布<a href="https://stablediffusionweb.com/">稳定扩散</a>并谈论如何有一个之后在某些正在制作的图像，特别是针对亚洲妇女的图像中，许多种族偏见。</p><p>因此，这是通过民主进程提出的，引起了国会办公室的注意，然后发表了声明，然后对实验室的未来治理决策和决策产生了连锁反应的影响像放置安全过滤器一样。因此，我认为这是要牢记的一件事：就民主化治理过程的含义而言，您可以考虑各种各样的事情。治理只是决策。那么，您如何确保那些决策过程反映将会受到影响的人们的利益。这不仅仅是直接参与，尽管还有很多工作要为更小规模的决策带入更加逼真的决策，以使其更现实。</p><p>像这样的抱怨，关于它是如此的资源密集型：是的。我们还拥有可用的技术来帮助使其减少资源密集型，我们如何利用这些技术来实际帮助公众以明智的方式对这些决策进行投入？因此，正在进行许多有趣的工作。在AI治理方面，民主治理也不是天生的。我认为，这取决于这个问题。如果进行成熟民主进程的成本将远远超过进行成熟民主进程的好处，那么进行民主决策过程可能并不是净收益。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>这是对希梅尔里希（Himmelreich）资源密集型投诉的。他还抱怨说，基本上AI本身并不是需要民主管理的事情。因此，他说的话，我会看看我是否可以记住，但是需要民主管理的事情可能只是影响人们生活的某些事情，无论他们是否喜欢，或者只是大规模固有的东西合作之类的东西。他基本上提出了这一说法，即AI不做那些事情。 AI被安置在做这些事情的机构中。因此，如果您担心，也许警察使用面部识别或其他东西，您应该担心警察的民主问责制，而不是面部认可，这是我要提出的重点。</p><p>因此，他说，首先，看，我们不需要民主化AI。我们需要民主化潜在使用AI的事物。其次，似乎他在说使用AI的相关内容，它们已经是民主的，所以我们不应该矛盾……我认为他谈论民主重叠之类的东西，我认为他真的不喜欢这些不同的民主机构试图影响同一决定，也许它们冲突之类的想法。</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>所以我认为我不同意。我认为，如果我正确理解的是，“您不需要民主化AI，您需要民主化使用它的机构”，我认为这是一个完全正确的观点。同样，人工智能本身并不是天生的坏事。这是双重使用技术。这取决于您要处理的事情。但是我要说的是，例如，试图决定执法部门应如何使用AI系统，这是AI治理问题。这是一个关于如何使用AI，在这种情况下如何调节AI的问题，而不是在其开发方式上，而是在其使用方式上。</p><p> AI的适当用例是什么？这是一个AI治理问题，不一定是这些法规专门放在AI上。许多AI法规已经存在于不同机构中。它只是在弄清楚该法规适用于AI系统。这可能是这样的情况：如果您对某些健康和安全标准有要求，而某些技术必须在工作环境中满足某些技术，则不应将AI遵守不同的标准。这只是弄清楚如何测量并确保AI系统达到这些标准的问题。因此，我想根据您所说的话，我想说我完全同意。</p><p>但是我要说的是，它仍然属于使AI治理民主化的保护。我认为这里可能发生的事情只是术语意味着什么的另一个冲突，就像“我们不需要民主化AI，因为我们不需要让更多的人直接参与发展和决定 - 关于AI开发的个人决策的制定”。在这种情况下，是的，我同意！但是，我们可能正在谈论在不同的AI治理中民主化……这就是为什么这些讨论变得真正复杂的原因，因为我们最终互相谈论彼此，因为我们使用相同的词意味着五个不同的事情。</p><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>是的，可能很粗糙。因此，在我们关闭有关AI民主化的讨论之前，我想知道……我想您已经考虑过将AI民主化。您是否有任何最喜欢的干预措施使AI更民主或使AI在任何相关意义上变得不那么民主？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>我将完全诚实，这是我的专业知识从可用的过程中更加贴切的，以及某些过程中确切地应用的最佳过程中的最佳范围。为此，我真的会更仔细地研究<a href="https://cip.org/">集体情报项目</a>所做的工作。或者我的合着者<a href="https://aviv.me/">Aviv Ovadya</a> （在纸上）确实参与了这些讨论，并与各种实验室合作，以帮助实施不同的民主进程。是的，我只是要说的是，这是我的专业知识下降的地方，我会指向同事，对这项工作进行更深入的讨论。</p><h2>开源AI<a name="osai"></a></h2><p><strong>丹尼尔·菲兰（Daniel Filan）：</strong>好吧，我要谈的下一件事基本上是开源的AI。因此，有一份报告，<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436">开源高度强大的基础模型：评估追求开源目标的风险，收益和替代方法</a>。它是自己和其他一群合着者，但据我所知，在<a href="https://www.governance.ai/">AI治理中心</a>。同样，您是否可以概述本报告中发生的情况？</p><p><strong>伊丽莎白·塞格（Elizabeth Seger）：</strong>好的，是的。因此，该报告很难概述，因为与八页的民主化论文不同，这本书约为60页。</p><p>如此快速概述。首先，我们想指出，目前围绕大型基础模型，尤其是Frontier Foundation Models是开源的辩论。这是一场辩论，从<a href="https://stability.ai/">稳定性AI</a>的<a href="https://stablediffusionweb.com/">稳定扩散</a>开始，然后是<a href="https://ai.meta.com/llama/">Llama 2</a>的发布，然后重量泄漏了，现在Meta落后于这个开源的东西。<a href="https://spectrum.ieee.org/meta-ai">元外面有关于系统是否应该是开源的抗议活动</a>。然后，随着<a href="https://artificialintelligenceact.eu/">欧盟AI法案</a>的发展，我们也看到了很多活动。作为欧盟AI法案的一部分，一些政党正在推动对开发模型的群体的豁免。因此，关于开源的讨论很多。我认为本文的目的是削减即将成为一场两极分化的辩论。</p><p>我们发现您有人们看到开源的好处，并且只是非常非常顽固的Pro开源，然后再也不会听到有关潜在危险的任何论点。然后，您有一个非常反开源的人，只想谈论危险，并拒绝看到好处。这确实使在这个问题上进行任何连贯的对话真的很难。然而，与此同时，国家政府试图围绕模型共享制定非常真实的政策。因此，我们如何削减这一两极分化的辩论，以尝试说，这是土地的境地。因此，我们在本文中要做的事情确实提供了一个均衡，经过充分研究的概述，概述了开源高度强大的模型的风险和好处。</p><p>通过“高功能模型”，基本上我们正在谈论的是Frontier AI系统。 We just don&#39;t use the term &#39;frontier&#39; because the frontier moves, but we want to be clear that there will be some systems that might be highly capable enough and pose risks that are high enough that, even if they aren&#39;t on the frontier, we should still be careful about open-sourcing them. So basically we&#39;re talking about frontier models, but frontier keeps going. So we can get into the terminology debate a little bit more later. But basically we wanted to outline what are the risks and benefits of open-sourcing these increasingly highly capable models, but then not just to have yet another piece that says, well, here are the risks and here are the benefits. But then kind of cut through by saying &#39;maybe there are also alternative ways that we can try to achieve some of the same benefits of open-sourcing, but at less risk.&#39;</p><p> So the idea here is actually quite similar to the democratization paper: it&#39;s to not just say &#39;what is open-sourcing?&#39; and &#39;is it good or bad?&#39; But to say &#39;What are the specific goals of open-sourcing? Why do people say that open-sourcing is good, that it&#39;s something we should be doing that should be defended, that should be preserved? So why is it we want to open source AI models? Then if we can be specific about why we want to open source, are there other methods that we can employ to try to achieve some of these same goals at less risk?&#39; These might be other model sharing methods like releasing a model behind API or doing a staged release, or it might be other more proactive measures like: let&#39;s say one benefit of open-sourcing is that it can help accelerate research progress, both in developing more greater AI capabilities, but also promoting AI safety research.</p><p> Another way you can promote AI safety research is to dedicate a certain percentage of profits towards AI safety research or to have research collaborations. You can do these things without necessarily having to provide access for anybody to have access to the model. So this is what we were trying to do with this paper: really just say, okay, we&#39;re going to give, first of all, just a very well-researched overview of both the risks and benefits. In fact, majority of the paper actually focuses on what are the benefits and trying to break down the benefits of open-sourcing and then really doing a deep dive into alternative methods or other things that can be done to help pursue these benefits where the risks might just be too high.</p><p> I think that the main claim that we do make in the paper with regard to &#39;is open-sourcing good or bad?&#39; First of all, it&#39;s a false dichotomy, but I think our main statement is open-sourcing is overwhelmingly good. Open source development of software, open source software underpins all of the technology we&#39;re using today. It&#39;s hugely important. It&#39;s been great for technological development, for the development of safe technology that reflects lots of different user needs and interests.好东西。 The issue is that there might be some cutting edge, highly capable AI systems that pose risks of malicious use or the proliferation of dangerous capabilities or even proliferation of harms and vulnerabilities to downstream applications.</p><p> Where these risks are extreme enough, we need to take a step back and say: maybe we should have some processes in place to decide whether or not open-sourcing these systems is okay. So I think the main thrust of the report is [that] open-sourcing is overarching good, there just may be cases in which it&#39;s not the best decision and we need to be careful in those cases. So yeah, I think that&#39;s the main overview of the paper.</p><p> <strong>Daniel Filan:</strong> Gotcha. Yeah, in case listeners were put off by the 60 something pages, I want listeners to know there is an executive summary. It&#39;s pretty readable.</p><p> <strong>Elizabeth Seger:</strong> It&#39;s a two-page executive summary.不用担心。</p><p> <strong>Daniel Filan:</strong> Yeah. I read the paper and I vouch for the executive summary being a fair summary of it. So don&#39;t be intimidated, listeners.</p><p> <strong>Elizabeth Seger:</strong> I&#39;ve also been putting off writing the blog post, so there will be a blog post version at some point, I promise.</p><p> <strong>Daniel Filan:</strong> Yeah. So it&#39;s possible that might be done by the time we&#39;ve released this, in which case we&#39;ll link the blog post.</p><p> <strong>Elizabeth Seger:</strong> Maybe by Christmas.</p><h3> Risks from open-sourcing<a name="risks-from-os"></a></h3><p> <strong>Daniel Filan:</strong> Yeah. Hopefully you&#39;ll have a Christmas gift, listeners. So when we&#39;re talking about these highly capable foundation models and basically about the risks of open-sourcing them… as I guess we&#39;ve already gotten into, what is a highly capable foundation model is a little bit unclear.</p><p> <strong>Elizabeth Seger:</strong> Yeah.已经很难了。</p><p> <strong>Daniel Filan:</strong> Maybe it&#39;ll be easier to say, what kinds of risks are we talking about? Because I think once we know the kinds of risks, (a) we know what sorts of things we should be willing to do to avert those risks or just what we should be thinking of, and (b) we can just say, okay, we&#39;re just worried about AIs that could potentially cause those risks.那么有哪些风险呢？</p><p> <strong>Elizabeth Seger:</strong> In fact, I think framing it in terms of risks is really helpful. So generally here we&#39;re thinking about risks of significant harm, significant societal or even physical harm or even economic harm. And of course now you say, “oh, define &#39;significant&#39;”. But basically just more catastrophic, extreme, significant societal risks and harms. So we use some examples in the paper looking at potential for malicious use.</p><p> There are some more diffuse harms if you&#39;re thinking about political influence operations. So this kind of falls into the misinformation/disinformation discussion. How might AI systems be used to basically influence political campaigns? So disinformation undermines trust and political leaders. And then in turn, if you can disrupt information ecosystems and disintegrate the processes by which we exchange information or even just disintegrate trust in key information sources enough, that can also impact our ability as a society to respond to things like crises.</p><p> So the pandemic is a good example of, if it&#39;s really hard to get people accurate information about, for example, whether or not mask-wearing is effective, you&#39;re not going to have really good coordinated decision-making around mask-wearing. So I think this is one where it&#39;s a little bit more diffuse. It&#39;s harder to measure. So I think this is one point where it&#39;s quite difficult to identify when the harm is happening because it&#39;s so diffused. But I think this is one potential significant harm. I&#39;d say maybe thinking about disrupting major political elections or something like that.</p><p> There&#39;s some options for malicious use that are talked about a little more frequently, I think because they&#39;re more well-defined and easier to wrap your head around. These are things like using generative AI to produce biological weapons or toxins, even production of malware to mount cyber attacks against key critical infrastructure. Imagine taking down an electrical grid, or imagine on election day, taking down an electoral system. These could have significant societal impacts in terms of harm to society or physical harm.</p><p> I think one key thing to point out here is that we aren&#39;t necessarily seeing these capabilities already. Some people may disagree, but I think my opinion is that technologies with the potential for this kind of harm do not currently exist. I think it is wise to assume that they will come into existence in the not too distant future, largely because we&#39;re seeing indications of these kinds of capabilities developing. We&#39;ve <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/">already seen how even narrow AI systems that are used in drug discovery, if you flip that parameter that&#39;s supposed to optimize for non-toxicity to optimize for toxicity, now you have a toxin generator</a> . So it doesn&#39;t take a huge stretch of the imagination to see how more capable systems could be used to cause quite significant societal harm. So no, I don&#39;t think there&#39;s currently systems that do this. I think we need to be prepared for a future in which these systems do exist. So it&#39;s worth thinking about the wisdom of releasing these systems now even if we might not be present with the technology at this moment.</p><p> <strong>Daniel Filan:</strong> So as a follow-up question: you&#39;re currently on the AI X-risk Research Podcast. I&#39;m wondering… so it seems like you&#39;re mostly thinking of things less severe than all humans dying or just permanently stunting the human trajectory.</p><p> <strong>Elizabeth Seger:</strong> I wouldn&#39;t say necessarily. I think this is definitely a possibility. I think part of what&#39;s happening is you kind of have to range how you talk about these things. I am concerned about x-risk from AI, and I think we could have systems where whether it&#39;s the cyber capability or the biological terror capability or even the taking down political systems capability and bolstering authoritarian governments - these are things that could cause existential risk and I am personally worried about this. But [we&#39;re] trying to write papers that are digestible to a much wider population who might not be totally bought into the x-risk arguments. I think, x-risk aside, there are still very genuine arguments for not necessarily releasing a model because of the harm that it can cause even if you don&#39;t think that those are existential risks. Even if it&#39;s just catastrophic harm, probably still not a great idea.</p><p> <strong>Daniel Filan:</strong> So at the very least, there&#39;s a range going on there.</p><p> <strong>Elizabeth Seger:</strong> Yeah.</p><h3> Should we make AI too dangerous to open source?<a name="make-ai-too-dangerous-os"></a></h3><p> <strong>Daniel Filan:</strong> So one question I had in my mind, especially when thinking about AI that can cause these kinds of risks and potentially open-sourcing it… Part of me is thinking, it seems like a lot of the risks of open-sourcing seem to be, well, there&#39;s now more people who can use this AI system to do a bunch of harm. The harm is just so great that that&#39;s really scary and dangerous. So one thing I was wondering is: if it&#39;s so dangerous for many people to have this AI technology, is it not also dangerous for anybody at all to have this AI technology? How big is this zone where it makes sense to make the AI but not to open source it? Or does this zone even exist?</p><p> <strong>Elizabeth Seger:</strong> Yeah, you make a very genuine point, and there are those that would argue that we should just hit the big old red stop button right now. I think that is an argument some people make. I guess where we&#39;re coming from with respect to this paper is trying to be realistic about how AI development&#39;s probably going to happen and try to inform governance decisions around what we should do as AI development continues. I think there are differing opinions on this, probably even among the authors on the paper. There&#39;s what, 25, 26 authors on the paper.</p><p> <strong>Daniel Filan:</strong> The paper has a disclaimer, listeners, that not all authors necessarily endorse every claim in the paper.</p><p> <strong>Elizabeth Seger:</strong> That includes me. So we have a very, very broad group of authors. But I think… Sorry, what was your original question? It just flew out of my head.</p><p> <strong>Daniel Filan:</strong> If it&#39;s too dangerous to open source, is it also too dangerous to make?</p><p> <strong>Elizabeth Seger:</strong> Yeah, no. So I think there&#39;s definitely an argument for this. I guess where I&#39;m coming from is trying to be realistic about where this development process is going and how to inform governments on what to do as AI development continues. It may very well be the case that we get to a point where everyone&#39;s convinced that we just have a big coordinated pause/stop in AI development. But assuming that that&#39;s not possible or improbable, shall we say, I think it&#39;s still wise to have a contingency plan. How can we guide AI development to be safe and largely beneficial and reduce the potential for risk? I think there&#39;s also an argument to be made that increasingly capable systems, while they pose increasingly severe risks, also could provide increasingly great benefits and potential economic potential benefits for helping to solve other challenges and crises that we face.</p><p> So I think you&#39;d be hard-pressed to get a giant coordinated pause. So the next plan is how do we make AI development happen safely? It is a lot easier to keep people safe if the super dangerous ones are not spread widely. I guess that&#39;s the very simplistic view.是的。</p><p> So I think, at least for me, I think it just comes from a realism about, are we likely to pause AI development before it gets super scary? Probably not, just given how humanity works. We developed nuclear weapons, probably shouldn&#39;t have done that. Oops, well now they exist, let&#39;s deal with it. So I think having a similar plan in line for what do we do with increasingly capable AI systems is important, especially given that it might not be that far away. Like I said, sort of with each new generation of AI that&#39;s released, we all kind of hold our breaths and say, oh, well what&#39;s that?那个人要做什么？ Then we learn from it and we don&#39;t know what the next generation of AI systems is going to bring. And so having systems in place to scan for potential harms, potential dangers, capabilities, to inform decisions about whether or not these systems should be released and if and how they should be shared, that&#39;s really important. We might not be able to coordinate a pause before the big scary happens. So, I think it&#39;s important to discuss this regardless.</p><p> <strong>Daniel Filan:</strong> I got you.</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a technical term by the way, the big scary.</p><h3>攻防平衡<a name="offense-defense"></a></h3><p><strong>Daniel Filan:</strong> Makes sense. So speaking of the risks of open-sourcing AI, in the paper you talk about the offense-defense balance, and basically you say that bad actors, they can disable misuse safeguards, they can increase new dangerous capabilities by fine-tuning. Open-sourcing makes this easier, it increases attacker knowledge. And in terms of just the AI technology, you basically make a claim that it&#39;s tilted towards offense in that attackers can do… They get more knowledge, they can disable safeguards, they can introduce new dangerous capabilities. It&#39;s easier to find these problems than it is to fix them. And once you fix them, it&#39;s hard to make sure that everyone has the fixes. Would you say that&#39;s a fair summary of-</p><p> <strong>Elizabeth Seger:</strong> Yeah, I&#39;d say that&#39;s pretty fair. I think the one thing I&#39;d add… I&#39;d say that with software development, for example… so offense-defense balance is something that&#39;s often discussed in terms of open-sourcing and scientific publication, especially any time you have dual-use technology or scientific insights that could be used to cause harm, you kind of have to address this offense-defense balance. Is the information that&#39;s going to be released going to help the bad actors do the bad things more or less than it&#39;s going to help the good actors do the good things/prevent the bad actors from doing the bad things? And I think with software development, it&#39;s often in favor of defense, in finding holes and fixing bugs and rolling out the fixes and making the technology better, safer, more robust. And these are genuine arguments in favor of why open-sourcing AI systems is valuable as well.</p><p> But I think especially with larger, more complex models, we start veering towards offense balance. I just want to emphasize, I think one of the main reasons for this has to do with how difficult the fixes are. So in the case of software, you get bugs and vulnerabilities that are relatively well-defined. Once you find them, relatively easy to fix, roll out the fix. The safety challenges that we&#39;re facing with highly capable AI systems are quite complex. We have huge research teams around the world trying to figure them out, and it&#39;s a very resource-intensive process, takes a lot of talent. Vulnerabilities are still easy to find, they&#39;re just a lot harder to fix. So, I think this is a main reason why the offense-defense balance probably skews more towards offense and enabling malicious actors because you can still find the vulnerabilities, you can still manipulate the vulnerabilities, take advantage of them… harder to fix, even if you have more people involved. So, that&#39;s the high-level evaluation. Mostly, I just wanted to push on the safety issue a little harder.</p><h3> KataGo as a case study<a name="katago-as-case-study"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So, one thing this actually brings up for me is… you may or may not be familiar, so there&#39;s AI that plays the board game Go. There&#39;s an <a href="https://katagotraining.org/">open-source training run</a> and some colleagues of mine have found basically <a href="https://goattack.far.ai/">you can cheaply find adversarial attacks</a> . So, basically dumb Go bots that play in a way that confuses these computer AI policies. And these attacks, they&#39;re not generally smart, but they just push the right buttons on the AI policy. And I guess this is more of a comment than a question, but there&#39;s been enough rounds of back and forth between these authors and the people making these open source Go bots that it&#39;s potentially interesting to just use that as a case study of the offense-defense balance.</p><p> <strong>Elizabeth Seger:</strong> So, you&#39;re saying that given that the system was open source and that people could sort of use it and query it and then send the information back to the developers, that that&#39;s-</p><p> <strong>Daniel Filan:</strong> Yeah, and in fact, these authors have been working with the developers of this software, and in fact what&#39;s happened is the developers of this software - KataGo, if people want to google it - they&#39;ve seen this paper, they&#39;re trying to implement patches to fix these issues. The people finding the adversarial policies are basically checking if the fixes work and publishing the information about whether they do or not.</p><p> <strong>Elizabeth Seger:</strong> Absolutely, so I want to be really clear that this is a huge benefit of open source development: getting people involved in the development process, but also using the system, finding the bugs, finding the issues, feeding that information back to the developers. This is a huge benefit of open source development for software and AI systems. I think that this is specifically why the paper focuses on the big, highly capable frontier foundation models is that this gets more difficult the more big, complex, cutting edge the system is. Some bugs will still be relatively small, well-defined, and there are bug bounty programs, proposals for AI safety bounty programs as well, helping to find these vulnerabilities and give the information back to the developers. I think there are issues though, with respect to some of the larger safety issues that are more difficult. Sometimes it&#39;s difficult to identify the safety problems in the first place, more difficult to address the safety problems.</p><p> Then, there&#39;s also the issue of just rolling out the fixes to the system. So software development, you fix a bunch of bugs, you roll out an update. Oftentimes, in the license it&#39;ll say that you&#39;re supposed to actually use the updated version. There&#39;s some data that came out, I can&#39;t remember which organization it was right now, I&#39;ll have to look it up later, but it&#39;s actually quite a low uptake rate of people actually running the up-to-date software. So first of all, even with just normal software, it&#39;s hard to guarantee that people are actually going to run the up-to-date version and roll out those fixes, which is an issue with open source because if you&#39;re using something behind API, then you just update the system and then everyone&#39;s using the updated system. If you have an open source system, then people actually have to download and run the update version themselves.</p><p> With foundation models, there&#39;s actually a weird incentive structure that changes where people might actually be de-incentivized to update. So with software, oftentimes when you have an update, it fixes some bugs and it improves system functionality. When it comes to safety fixes for foundation models, oftentimes it has to do with reducing system functionality, like putting on a filter that says, “Well, now you can&#39;t produce this class of images. Now, you can&#39;t do this kind of function with the system,” so it&#39;s hard, I don&#39;t know if there&#39;s good information on how this has actually panned out now. Are we seeing lower uptake rates with updates for AI systems? I don&#39;t know, but there might be something weird with incentive structures going on too, where if updates basically equate to reducing system functionality in certain ways, people might be less likely to actually take them on board.</p><p> <strong>Daniel Filan:</strong> I don&#39;t have a good feel for that.</p><p> <strong>Elizabeth Seger:</strong> I don&#39;t have a super good feel, but just, I don&#39;t know, interesting food for thought, perverse incentive structures.</p><p> <strong>Daniel Filan:</strong> I don&#39;t know, I&#39;m still thinking about this KataGo case: so that&#39;s the case where the attack does reduce the system functionality and people are interested in getting the latest version with fixes. It also occurs to me that… So, in fact the structure of this paper, the way they found the attack did not rely on having access to the model weights. It relied on basically being able to query the Go bot policy, basically to try a bunch of things and figure out how to trick the Go bot policy. Now, it&#39;s really helpful if you can have the weights locally just so that you can call the API, so that you can call it a lot, but that was not a case where you needed the actual weights to be shared. So on the one hand, that&#39;s a point that sharing the weights is less valuable than you might think, but it also suggests if you&#39;re worried about people finding these adversarial attacks, then just putting the weights behind an API doesn&#39;t protect you正如你所想的那样。 Maybe you need to rate limit or something.</p><p> <strong>Elizabeth Seger:</strong> I think that&#39;s a valuable insight, there are definitely things you can do without weights. This is an argument for why you should be worried anyway, but it&#39;s also an argument for… There are lots of arguments for, well, open-sourcing is important because you need it to do safety research and getting more people involved in safety research will result in safer systems, you have more people input into these processes, but you just illustrated a perfect example of how just having query access, for example, to a system, can allow you to do a significant amount of safety research in terms of finding漏洞。</p><h3> Openness for interpretability research<a name="open-for-interp"></a></h3><p> <strong>Elizabeth Seger:</strong> So, query access is one that can be done completely behind an API, but then even if we think about something like interpretability research, interpretability research does require much more in-depth access to a system to do, but arguably this is an argument for needing access to smaller systems. We are struggling to do interpretability research on smaller, well-defined systems. It&#39;s sort of like the rate limiting factor on interpretability research isn&#39;t the size of the models people have access to, the way I understand it at least. If we&#39;re struggling to do interpretability research on smaller models, I feel like having access to the biggest, most cutting edge frontier model is not what needs to happen to drive interpretability research.</p><p> <strong>Daniel Filan:</strong> I think it depends on the kind of interpretability research.</p><p> <strong>Elizabeth Seger:</strong> It&#39;d be interesting to hear your thoughts on this as well, but there&#39;s a range of different kinds of AI research. Not all of it requires open access and then some of the kinds that do require open access to the models isn&#39;t necessarily helped the most by having the open access, and then there is also this idea of alternative approaches that we talk about in the纸。 You can help promote AI safety research by providing access to specific research groups. There are other things you can do to give people the access they need to do safety research.</p><p> <strong>Daniel Filan:</strong> So, I guess I can share my impressions here. Interpretability research is a broad bucket. It describes a few different things. I think there are some kinds of things where you want to start small and we haven&#39;t progressed that far beyond small. So just understanding, &#39;can we just exhaustively understand how a certain neural net works?&#39; - start small, don&#39;t start with GPT-4. But I think one thing you&#39;re potentially interested in, in the interpretability context, is how things get different as you get bigger models, or do bigger models learn different things or do they learn more? What sorts of things start getting represented? Can we use interpretability to predict these shifts? There you do want bigger models. In terms of how much can you do without access to weights, there&#39;s definitely a lot of interpretability work on these open source models because people apparently really do value having the weights.</p><p> Even in the case of the adversarial policies work I was just talking about, you don&#39;t strictly need access to the weights, but if you could run the games of Go purely on your computer, rather than calling the API, waiting for your request to be sent across the internet, and the move to be sent back, and doing that a billion times or I don&#39;t know the actual number, but it seems like just practically-</p><p> <strong>Elizabeth Seger:</strong> Much more efficient.</p><p> <strong>Daniel Filan:</strong> … It&#39;s easier to have the model. I also think that there are intermediate things. So one thing the paper talks about, and I guess your colleague <a href="https://sites.google.com/view/tobyshevlane">Toby Shevlane</a> has <a href="https://arxiv.org/abs/2201.05159">talked about</a> is basically structured access of giving certain kinds of information available maybe to certain people or maybe you just say “these types of information are available, these types aren&#39; t”。 I&#39;ve heard colleagues say, “Even if you didn&#39;t open source GPT-4 or GPT-3, just providing final layer activations or certain kinds of gradients could be useful”, which would not… I don&#39;t think that would provide all the dangers or all the risks that open-sourcing could potentially involve.</p><p> <strong>Elizabeth Seger:</strong> I think this is a really key point as well, is trying to get past this open versus closed dichotomy. Just saying that something isn&#39;t open source doesn&#39;t necessarily mean that it&#39;s completely closed and no-one can access it. So like you said, Toby Shevlane talks about structured access, and it was a paper we referenced - at least when we referenced it, it was still forthcoming, it might be out now - but it was Toby Shevlane and Ben Bucknell were working on it, and it was about the potential of developing research APIs. So, how much access can you provide behind API to enable safety research and what kind of access would that need to look like and how could those research APIs be regulated and who by? So, I think if there&#39;s a genuine interest in promoting AI safety research and a genuine acknowledgement of the risks of open-sourcing, we could put a lot of resources into trying to develop and understand ways to get a lot of the benefits to safety research that open-sourcing would have by alternative means.</p><p>它不会是完美的。 By definition, it&#39;s not completely open, but if we take the risks seriously, I think it&#39;s definitely worth looking into these alternative model sharing methods and then also into the other kinds of proactive activities we can engage in to help promote safety research, whether that&#39;s committing a certain amount of funds to safety research or developing international safety research organizations and collaborative efforts. I know one issue that always comes up when talking about, “Well, we&#39;ll just provide safety research access through API, or we&#39;ll provide privileged downloaded access to certain groups.” It&#39;s like, “Well, who gets to decide who has access? Who gets to do the safety research?”</p><p> And so, I think this points to a need to have some sort of a multi-stakeholder governance body to mediate these decisions around who gets access to do the research, whether you&#39;re talking about academic labs or other private labs, sort of like [how] you have multi-stakeholder organizations decide how to distribute grants to do environmental research, or you have grantmaking bodies that distribute grant funds to different academic groups. You could have a similar type situation for distributing access to more highly capable, potentially dangerous systems, to academic groups, research groups, safety research institutions that meet certain standards and that can help further this research.</p><p> So, I feel like if there&#39;s a will to drive safety research forward, and if varying degrees of access are needed to allow the safety research to happen, there are things we can do to make it happen that do not necessarily require open-sourcing a系统。 And I think, like we said, different kinds of safety research require varying degrees of access. It&#39;s not like all safety research can be done with little access. No, you need different amounts of access for different kinds of safety research, but if there&#39;s a will, there&#39;s a way.</p><h3> Effectiveness of substitutes for open sourcing<a name="os-substitutes"></a></h3><p> <strong>Daniel Filan:</strong> So, I want to ask something a bit more quantitative about that. So, some of the benefits of open-sourcing can be gained by halfway measures or by structured access or pursuing tons of collaborations, but as you mentioned, it&#39;s not going to be the same as if it were open sourced. Do you have a sense of… I guess it&#39;s going to depend on how constrained you are by safety, but how much of the benefits of open source do you think you can get with these more limited sharing methods?</p><p> <strong>Elizabeth Seger:</strong> That&#39;s a good question. I think you can get quite a bit, and I think, again, it sort of depends what kind of benefit you&#39;re talking about. So in the paper, I think we discuss three different benefits. Let&#39;s say we talk about accelerating AI research, so that&#39;s safety research and capability research. We talk about distributing influence over AI systems, and this ranges everything from who gets to control the systems, who gets to make governance decisions about the systems, who gets to profit. It wraps all the democratization themes together under distributing influence over AI, and then, let&#39;s see, what was the other one that we talked about? You&#39;d think I&#39;ve talked about this paper enough in the last three months, I have it down. Oh, external model evaluation. So, enabling external oversight and evaluation, and I think it depends which one you&#39;re talking about.</p><p> Let&#39;s start with external model evaluation. I think that this probably benefits the most from open-sourcing. It depends what you&#39;re looking at, so for example, if you&#39;re just looking for minor bugs and stuff like that, you don&#39;t need open source access for that, but having a more in-depth view to the systems is more important for people trying to help find fixes to the bugs.我们已经讨论过这个问题。 There are also risks associated with open-sourcing. If we&#39;re talking about accelerating capability research, for example, which sort of falls under the second category, I think you might find that the benefits of open-sourcing here might be somewhat limited the larger and more highly capable the system gets. And I think this largely will just have to do with who has access to the necessary resources to really operate on the cutting edge of research and development. Open source development, it operates behind the frontier right now largely because of restrictions… not restrictions, but just the expense of the necessary compute resources.</p><p> And then you talk about distributing control over AI, we&#39;ve already discussed the more distributed effect of open-sourcing and model sharing on distributing control. It&#39;s a second order effect: you get more people involved in the development process and then large labs have more competition, and then it distributes influence and control.</p><p> There are probably more direct ways you can help distribute control and influence over AI besides making a system widely available. So, to answer your original question then about, how much of the benefit of open-sourcing can you get through alternative methods? I guess it really depends what benefit you&#39;re talking about. I think for AI safety progress probably quite a bit, honestly; actually the vast majority of it, given that a lot of the safety research that&#39;s done on these highly capable, cutting edge models is something that has to happen within well-resourced institutions anyway, or you need the access to the resources to do that, not just the code and the weights, but the computational resources and so on.</p><p> So, I think quite a bit. I think it&#39;s less of a “can we get close to the same benefits that open-sourcing allows?” It&#39;s more like, “can we do it in one fell swoop?”就是这样。 It&#39;s like open-sourcing is the easy option. “Here, it&#39;s open!” - and now you get all these benefits from open-sourcing. The decision to open-source or not, part of the reason it&#39;s a hard decision is because achieving these benefits by other means is harder. It&#39;s going to take more resources to invest, more organizational capacity, more thought, more cooperation. It&#39;s going to take a lot of infrastructure, a lot of effort. It&#39;s not the one-stop shop that open-sourcing is, but I think the idea is that if the risks are high enough, if the risks are severe enough, it&#39;s worth it. I think that&#39;s where it comes in.</p><p> So, I guess it&#39;s worth reiterating again and again: this paper is not an anti-open source paper, [it&#39;s] very pro-open source in the vast majority of cases. What we really care about here are frontier AI systems that are starting to show the potential for causing really catastrophic harm, and in these cases, let&#39;s not open-source and let&#39;s pursue some of these other ways of achieving the same benefits of open source to safety and distributing control and model evaluation, but open-source away below that threshold. The net benefits are great.</p><h3> Offense-defense balance, part 2<a name="offense-defense-2"></a></h3><p> <strong>Daniel Filan:</strong> Gotcha. So my next question - I actually got a bit sidetracked and wanted to ask it earlier - so in terms of the offense-defense balance, in terms of the harms that you are worried about from open-sourcing, I sometimes hear the claim that basically, “Look, AI, if you open-source it, it is going to cause more harm, but you also enable more people to deal with the harm.” So I think there, they&#39;re talking about offense-defense balance, not of finding flaws in AI models, but in the underlying issues that AI might cause. So, I guess the idea is something… To caricature it, it&#39;s something like, “Look, if you use your AI to create a pathogen, I can use my AI to create a broad spectrum antibiotic or something”, and the hope is that in these domains where we&#39;re worried about AI causing harm, look, just open-sourcing AI is going to enable tons of people to be able to deal with the harm more easily, as well as enabling people to cause harm. So I&#39;m wondering, what do you think about the underlying offense-defense balance as opposed to within AI?</p><p> <strong>Elizabeth Seger:</strong> I get the argument. Personally, I&#39;m wary about the arms race dynamic though. You gotta constantly build the stronger technology to keep the slightly less strong technology in check. I guess this comes back to that very original question you asked about, “What about just hitting the no more AI button?” So, I guess I get the argument for that. I think there&#39;s weird dynamics, I don&#39;t know. I&#39;m not doing a very good job answering this question. I&#39;m personally concerned about the race dynamic here, and I think it just comes back to this issue of, how hard is it to fix the issues and vulnerabilities in order to prevent the misuse in the first place? I think that should be the goal: preventing the misuse, preventing the harm in the first place. Not saying, “Can we build a bigger stick?”</p><p> There&#39;s a similar argument that is brought up when people talk about the benefits of producing increasingly capable AI systems and saying, “Oh, well, we need to plow ahead and build increasingly capable AI systems because you never know, we&#39;ll develop a system that&#39;ll help cure cancer or develop some renewable energy technology that&#39;ll help us address climate change or something like that.” What huge problems could AI help us solve in the future? And I don&#39;t know - this is personally me, I don&#39;t know what the other authors on this paper think of this - but I don&#39;t know, I kind of feel like if those are the goals, if the goals are to solve climate change and cure cancer, take the billions upon billions upon billions and billions of dollars that [we&#39;re] currently putting into training AI systems and go cure cancer and develop renewable technologies! I struggle with those arguments personally. I&#39;d be interested just to hear your thoughts. I have not written about this. This is me riffing right now. So, I&#39;d be interested to hear your thoughts on this train of thought as well.</p><p> <strong>Daniel Filan:</strong> I think the original question is unfairly hard to answer just because it&#39;s asking about the offense-defense balance of any catastrophic problem AI might cause and it&#39;s like, “Well, there are tons of those and it&#39;s pretty hard to think about. ” So, the thing you were saying about, if you wanted to cure cancer, maybe step one would not be “create incredibly smart AI”. I&#39;ve seen this point. I don&#39;t know if you know <a href="https://meaningness.com/about-my-sites">David Chapman</a> &#39;s <a href="https://betterwithout.ai/">Better without AI</a> ?</p><p> <strong>Elizabeth Seger:</strong> No, not familiar.</p><p> <strong>Daniel Filan:</strong> So, he basically argues we just shouldn&#39;t build big neural nets and it&#39;s going to be terrible. Also, <a href="https://aiimpacts.org/author/jeffreyheninger/">Jeffrey Heninger</a> at <a href="https://aiimpacts.org/">AI Impacts</a> , I think has said <a href="https://blog.aiimpacts.org/p/my-current-thoughts-on-the-ai-strategic">something similar along these lines</a> . On the one hand, I do kind of get it, just in the sense that, if I weren&#39;t worried about misaligned AI, there&#39;s this hope that this is the last invention you need. You create AI and now instead of having to separately solve cancer and climate change and whatever, just make it solve those things for you.</p><p> <strong>Elizabeth Seger:</strong> I guess it&#39;s just really hard to look forward, and you have to decide now whether or not this technology is that silver bullet and how much investment it&#39;s going to take to get to that point.</p><p> <strong>Daniel Filan:</strong> I think that&#39;s right, and I think your take on this is going to be driven by your sense of the risk profile of building things that are significantly smarter than us. I guess from the fact that I made the AI X-risk Research Podcast, rather than the AI Everything&#39;s Going to be Great Research Podcast, people can guess my-</p><p> <strong>Elizabeth Seger:</strong> It&#39;s an indication of where you&#39;re coming from.</p><p> <strong>Daniel Filan:</strong> … take on this, but I don&#39;t know. I think it&#39;s a hard question. So, part of my take is, in terms of the underlying offense-defense balance, I think it becomes more clear when you&#39;re worried about, what should I say, silicogenic risks? Basically the AI itself coming up with issues rather than humans using AI to have nefarious schemes. Once you&#39;re worried about AI doing things on their own where you are not necessarily in control, there I think it makes sense that you&#39;re probably… If you&#39;re worried about not being able to control the AIs, you&#39;re probably not going to be able to solve the risks that the AIs are creating, right?</p><p> <strong>Elizabeth Seger:</strong> Yeah, your management plan for AI shouldn&#39;t be to build a slightly more powerful AI to manage your AI.</p><p> <strong>Daniel Filan:</strong> Well, if you knew that you were going to remain in control of the slightly bigger AI, maybe that&#39;s a plan, but you kind of want to know that.</p><p> <strong>Elizabeth Seger:</strong> I guess I was saying that if you&#39;re worried about loss of control scenarios, then the solution shouldn&#39;t be, “Well, let&#39;s build another system that&#39;s also out of our control, but just slightly better aligned to address the…” I feel like that&#39;s-</p><p> <strong>Daniel Filan:</strong> It&#39;s not the greatest. I think my colleague John Wentworth has <a href="https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies">some saying</a> , “Releasing Mothra to contain Godzilla is not going to increase property values in Tokyo,” which is a cute little line. I don&#39;t know, it&#39;s a hard question. I think it&#39;s hard to say anything very precise on the topic. I did want to go back to the offense-defense balance. So moving back a bit, a thing you said was something like, “Look, it&#39;s probably better to just prevent threats from arising, than it is to have someone make a pathogen and then have everyone race to create an antibiotic or antiviral or whatever. ” So, that&#39;s one way in which everyone having really advanced AI… That&#39;s one way that could look in order to deal with threats. I think another way does look a bit more like prevention. I don&#39;t know, it&#39;s also more dystopian sounding, but one thing that AI is good at is surveillance, right?</p><p> <strong>Elizabeth Seger:</strong> Yes.</p><p> <strong>Daniel Filan:</strong> Potentially, so you could imagine, “Look, we&#39;re just going to open source AI and what we&#39;re going to use the AI for is basically surveilling people to make sure the threats don&#39;t occur.” So, maybe one version of this is you just really amp up wastewater [testing]. Somehow you use your AI to just look at the wastewater and see if any new pathogens are arising. It could look more like you have a bunch of AIs that can detect if other people are trying to use AI to create superweapons or whatever, and stop them before they do somehow.</p><p> <strong>Elizabeth Seger:</strong> The wastewater example, that sounds great. We should probably do that anyway. In terms of surveilling to see how people are using AI systems using AI, why not just have the AI systems be behind an API where people can use the systems for a variety of downstream tasks integrating through this API and then the people who control the API can just see how the system is being used? Even if it can be used for a vast majority of tasks, even if you were to take all the safety filters off, the advantage of the API is still that you can see how it&#39;s being used. I don&#39;t know, I feel like that&#39;s-</p><h3> Making open-sourcing safer?<a name="making-os-safer"></a></h3><p> <strong>Daniel Filan:</strong> That seems like a good argument. All right, so I guess another question I have is related to the frame of the report. So in the report you&#39;re basically like, “open-sourcing has these benefits, but it also has these costs. What are ways of doing things other than open-sourcing that basically try and retain most of the benefits while getting rid of most of the costs?” You can imagine a parallel universe report where you say, “Okay, open-sourcing has these benefits, but it also has these costs. We&#39;re still going to open source, but we&#39;re going to do something differently in our open source plan that is going to retain benefits and reduce costs”, right? So one example of this is you open-source models, but you have some sort of watermarking or you have some sort of cryptographic backdoor that can stop models in their tracks or whatever. I&#39;m wondering: why the frame of alternatives to open-sourcing rather than making open-sourcing better?</p><p> <strong>Elizabeth Seger:</strong> Very simple. I think making open-sourcing better is the harder question, technically more difficult. I mean, for example, say you have watermarking, part of the issue with watermarking to identify artificially generated images is making sure the watermarks stick. How do you make sure that they are irremovable if you are going to open… This is a really complex technical question: how do you develop a system that has watermarked images where that watermark is irremovable if you were to open source the system?</p><p> I&#39;m not saying it&#39;s undoable. I personally don&#39;t have the technical background to comment very deeply on this. I have heard people talking about how possible this would be. It also depends how you watermark, right?</p><p> If you have just a line of inference code that says, “slap a watermark on this thing”, it could delete the line of inference code. If you&#39;re to train the system on only watermarked images, well now you have to retrain the entire system to get it to do something else, which is very expensive. So again, I think it depends how you do it.</p><p> I was at a meeting last week where people were talking about, are there ways we could build in a mechanism into the chips that run the systems that say “if some bit of code is removed or changed in the system, then the chip burns up and won&#39;t run the system”. I&#39;m not saying this is impossible, but [a] really interesting technical question, really difficult, definitely beyond my area of expertise. But I think if this is an approach we can take and say, there are ways to be able to open source the system and get all the benefits of open-sourcing by just open-sourcing and still mitigate the risks, I think that&#39;s great. I think it&#39;s just a lot more difficult.</p><p> And there&#39;s one aspect in which we do take the flip view in the report, and I think this is where we start talking about a staged release of models. You can undergo a staged release of a model where you put out a slightly smaller version of a model behind API, you study how it&#39;s being used. Maybe you take a pause, analyze how it was used, what [are] the most common avenues of attack, if at all, [that] were being used to try and misuse the model.</p><p> And then you release a slightly larger model, [then] a slightly larger model. You do this iteratively, and if you do this process, as you get to a stage where it&#39;s like, hey, we&#39;ve been doing the staged release of this model for however many months and no problems, looking good, there&#39;s no emergent capabilities that popped up that are making you worried. You didn&#39;t have to implement a bunch of safety restrictions to get people to stop doing unsafe things - okay, open source. This is not a binary [of] it has to be completely open or completely closed. And I think this is one respect… If you were to take this flip view of “how can we open source, but do it in the safest way possible?” Just open source slowly, take some time to actually study the impacts. And it&#39;s not like the only way to get a sense of how the system&#39;s going to be used is to just open source it and see what happens. You could do a staged release and study what those impacts are.</p><p> Again, it won&#39;t be perfect. You never know how it&#39;s going to be used 10 years down the road once someone gets access to all the weights and stuff. But it is possible to study and get some sort of insight. And I think one of the nice things about staged release is if you start doing the staged release process and you realize that at each iterative step you are having to put in a bunch of safety filters, for example, to prevent people from doing really shady stuff, that&#39;s probably a good indication that it&#39;s not ready to be open sourced in its current form, because those are safety filters that will just immediately be reversed once open sourced. So I think you can learn a lot from that.</p><p> So I think that&#39;s one way you can open source safely: find ways to actually study what the effects are before you open source, because that decision to open source is irreversible. And then I think the technical challenge of, are there ways we can have backstops, that we can technically build in irreversible, irremovable filters or watermarks or even just hardware challenges that we could implement - I think [those are] really interesting technical questions that I don&#39;t know enough about, but… Go for it. That&#39;d be a great world.</p><p> <strong>Daniel Filan:</strong> Yeah. If listeners are interested, this gets into some territory that <a href="https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html#watermarking-lm-outputs">I talked about with Scott Aaronson earlier this year</a> . Yeah, I think the classic difficulties, at least say for watermarking… I read <a href="https://arxiv.org/abs/2012.08726">one paper</a> that claims to be able to bake the watermark into the weights of the model. To be honest, I didn&#39;t actually understand how that works.</p><p> <strong>Elizabeth Seger:</strong> I think it has to do with how the model&#39;s trained. So the way I understand it is if you have a dataset of images that all have a watermark in that dataset, not watermark in the sense like you see on a $1 bill, but weird pixel stuff that the human eye can&#39;t see. If all the images in the training dataset have that watermark, then all of the images it produces will have that watermark. In that case, it&#39;s baked into the system because of how it was trained. So the only way to get rid of that watermark would be to retrain the system on images that don&#39;t contain the watermark.</p><p> <strong>Daniel Filan:</strong> Yeah, that&#39;s one possibility. So that&#39;s going to be a lot rougher for applying to text models, of course, if you want to just train on the whole internet. I think I saw <a href="https://arxiv.org/abs/2012.08726">something</a> that claimed to work even on cases where the dataset did not all have the watermark, but I didn&#39;t really understand how it worked. But at any rate, the key issue with these sort of watermarking methods is as long as there&#39;s one model that can basically paraphrase that does not have watermarking, then you can just take your watermark thing and basically launder it and get something that - if your paraphrasing model is good enough, you can create something that looks basically similar, it doesn&#39;t have the watermark, and then it&#39;s sad news.是的。进而-</p><p> <strong>Elizabeth Seger:</strong> Sorry, I was going to say there&#39;s similar [things] in terms of how doing something with one model allows you to jailbreak another model. I mean this is what happened with the <a href="https://arxiv.org/abs/2307.15043">&#39;Adversarial suffixes&#39; paper</a> , where using a couple open source models, one which was Llama 2, and using the weights of those models, figuring out a way to basically just throw a seemingly random string of numbers at a large language model, and then with that seemingly random range of numbers before the prompt basically get the system to do whatever you want. Except while they figured out how to do that using the weights accessible from Llama 2, it worked on all the other large language models. So finding a way to jailbreak one model and using the weights and access to one model, that could bring up vulnerabilities in tons of others that aren&#39;t open sourced as well. So I think that&#39;s another roughly related somewhat to what we were just talking about point.</p><p> <strong>Daniel Filan:</strong> Yeah, I guess it brings up this high level thing of whatever governance method for AI you want, you want it to be robust to some small fraction of things breaking the rules. You don&#39;t want the small fraction to poison the rest of the thing, which watermarking unfortunately has.</p><p> Yeah, I guess I wanted to say something brief about backdoors as well. So there is really a way of, at least in toy neural networks, and you can probably extend it to bigger neural networks, <a href="https://arxiv.org/abs/2204.06974">you really can introduce a backdoor that is cryptographically hard to detect</a> . So one problem is, how do you actually use this to prevent AI harm is not totally obvious. And then there&#39;s another issue of… I guess the second issue only comes up with super smart AI, but if you have a file on your computer that&#39;s like, “I implanted a backdoor in this model, the backdoor is this input”, then it&#39;s no longer cryptographically hard to find as long as somebody can break into your computer. Which hopefully is cryptographically hard, but I guess there are security vulnerabilities there.</p><p> So yeah, I wonder if you want to say a little bit about the safer ways to get the open source benefits. I&#39;ve given you a chance to talk about them a little bit, but is there anything more you want to say about those?</p><p> <strong>Elizabeth Seger:</strong> I think, not really. I think the overarching point is, just as I said before, when the risks are high - and I think that&#39;s key to remember, I&#39;m not saying don&#39;t open-source everything - when the risks are high, it is worth investing in seeing how else we can achieve the benefits of open-sourcing. Basically, if you&#39;re not going to open-source because the risks are high, then look into these other options. It&#39;s really about getting rid of this open versus closed dichotomy.</p><p> So many of the other options have to do with other options for sharing models, whether that&#39;s structured access behind API, even research API access, gated download, staged release, and then also more proactive efforts. Proactive efforts which can actually also be combined with open-sourcing. They don&#39;t have to be seen as an alternative to open-sourcing. So this is things like redistributing profits towards AI safety research or starting AI safety and bug bounty programs. Or even like we talked about with <a href="https://arxiv.org/abs/2303.12642">the democratization paper</a> , thinking about how we can democratize decision-making around AI systems to help distribute influence over AI away from large labs, which is another argument for open-sourcing.</p><p> So yeah, I think that this is key: there are other efforts that can be put in place to achieve many of the same benefits of open-sourcing and when the risks are high, it&#39;s worth really looking into these.</p><h2> AI governance research<a name="ai-gov"></a></h2><p> <strong>Daniel Filan:</strong> All right.好的。 So moving on, I want to talk a little bit more broadly about the field of AI governance research. So historically, this podcast is mostly focused on technical AI alignment research, and I imagine most listeners are more familiar with the technical side than with governance efforts.</p><p> <strong>Elizabeth Seger:</strong> In which case, I apologize for all my technical inaccuracies. One of the benefits of having 25 co-authors is that a lot of the technical questions I got to outsource.</p><h3> The state of the field<a name="state-of-field"></a></h3><p> <strong>Daniel Filan:</strong> Makes sense. Yeah, it&#39;s good to be interdisciplinary. So this is kind of a broad question, but how is AI governance going? What&#39;s the state of the field, if you can answer that briefly?</p><p> <strong>Elizabeth Seger:</strong> The state of the field of AI governance.是的。好的。 I&#39;ll try and answer that briefly. It&#39;s going well in that people are paying attention. In this respect, the release of ChatGPT I think was really great for AI governance because people, besides those of us already doing AI governance research, are really starting to see this as something valuable and important that needs to be talked about and [asking] questions around what role should governments play in regulating AI, if at all? How do we get this balance between governments and the developers? Who should be regulated with respect to different things? Do all of the responsibilities lie on the developers or is it on the deployers?</p><p> And all of these questions suddenly are coming to light and there&#39;s more general interest in them. And so we&#39;re seeing things like, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is happening next week, [a] global AI summit, looking at AI safety, really concerned about catastrophic and existential risks, trying to understand what kind of global institutions should be in place to govern AI systems, to evaluate AI systems, to audit, to regulate.</p><p> And this is bringing in countries from all over the world. I think it&#39;s something like 28 different countries are going to be at the UK AI Summit. You have <a href="https://artificialintelligenceact.eu/">the EU AI Act</a> where it started a while ago looking at narrow AI systems, but now is taking on foundation models and frontier AI systems and looking at open source regulation. And this has really, over the last year, exploded into a global conversation.</p><p> So in that respect, AI governance is going well in that people are paying attention. It&#39;s also very high stress because suddenly everyone&#39;s paying attention.我们必须做点什么。 But I think there&#39;s really genuine interest in getting this right, and I think that really bodes well. So I&#39;m excited to see where this next year goes. Yeah, there&#39;s talk about having this global AI summit and then making this a recurring series. And so I think it&#39;s going well in the sense that people are paying attention and the wheels are starting to turn, and that&#39;s cool.</p><h3>开放式问题<a name="open-qs"></a></h3><p><strong>Daniel Filan:</strong> Okay. I guess related to that, what do you see as the most important open questions in the field?</p><p> <strong>Elizabeth Seger:</strong> In the field of AI governance?好的。 So I think one big one is compute governance, which my colleague, <a href="https://heim.xyz/about/">Lennart Heim</a> , <a href="https://www.governance.ai/team/lennart-heim">works on</a> . This is just thinking about how compute is a lever for trying to regulate who is able to develop large models, even how compute should be distributed so that more people can distribute large models, but basically using compute as a lever to understand who has access to and who is able to develop different kinds of systems. So I think that&#39;s a huge area of research with a lot of growing interest because compute&#39;s one of the tangible things that we can actually control the flow of.</p><p> I think that the questions around model-sharing and open-sourcing are getting a lot of attention right now. Big open question, a lot of debate, like I said, it&#39;s becoming really quite a polarized discussion, so it&#39;s getting quite hard to cut through. But a lot of good groups [are] working on this, and I think a lot of interest in genuinely finding common ground to start working on this. I&#39;ve had a couple situations where I&#39;ve been in groups or workshops where we get people who are very pro-open source and other people who are just like, no, let&#39;s just shut down the whole AI system right now, really both sides of the spectrum coming together. And we try and find a middle ground on, okay, where do we agree? Is there a point where we agree? And very often we can come to a point of agreement around the idea that there may be some AI system, some model that poses risks that are too extreme for that model to be responsibly open sourced.</p><p> And that might not sound like that extreme of a statement, but when you have people coming from such polarized views to agree on the fact that there may exist a model one day that should not be open source, that is a starting point and you can start the conversation from there. And every group I&#39;ve been in so far has got to that point, and we can start working on that. So I think this model-sharing question is a big open question and lots of technical research needs to be done around benchmarking to decide, when are capabilities too dangerous?</p><p> Also around understanding what activities are actually possible given access to different combinations of model components. And that&#39;s actually not entirely clear, and we need a much more fine-grained understanding of what you can actually do given different kinds of model, combinations of model components, in order not only to have safe standards for model release and really a fine-grained standard for model release, but also to protect the benefits of open-sourcing. You don&#39;t want to just have a blanket “don&#39;t release anything” if you can get a lot of good benefit out of releasing certain model components. So I think a lot of technical research has to go into this.</p><p>反正。 So yeah, second point, I think model-sharing is a really big point of discussion right now. And then with the upcoming <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> , [there&#39;s] quite a bit of discourse around what international governance structures should look like for AI, a lot of different proposed models. And yeah, it&#39;ll be interesting to see what comes out of the summit. I don&#39;t think they&#39;re going to agree on anything amazing at the summit.已经两天了。 But I think for me, a really great outcome of the summit would be, first, recognition from everyone that AI systems could pose really extreme risks. So just a recognition of the risks. And then second, a plan going forward, a plan for how we can start establishing international systems of governance and really structure out when are we going to come to what kinds of decisions and how can we start putting something together. So I think that those are probably three key open questions, and the international governance structure one is really big right now too, just given the upcoming summit.</p><p> <strong>Daniel Filan:</strong> And I guess unless we get the editing and transcription for this episode done unusually quickly, listeners, the <a href="https://en.wikipedia.org/wiki/2023_AI_Safety_Summit">UK AI Summit</a> is almost definitely going to be in your past. So I guess listeners are in this interesting position of knowing how that all panned out in a way that we don&#39;t. So that was open questions in the field broadly. I&#39;m wondering for you personally as a researcher, what things are you most interested in looking at next?</p><p> <strong>Elizabeth Seger:</strong> Interesting. I mean, most of my life is taken up with follow-up on this open source report right now. So I definitely want to keep looking into questions around model-sharing and maybe setting responsible scaling policy, responsible model release policy.我不太确定。 I think I&#39;m in this place right now where I&#39;m trying to feel out where the most important work needs to be done and whether the best place for me to do is to encourage other people to do certain kinds of work where I don&#39;t necessarily have the expertise, like we were talking about, like needing more technical research into what is possible given access to different combinations of model components, or are there specific areas of research I could try to help lead in? Or whether really what needs to be done is just more organizational capacity around these issues.</p><p> So no, I am personally interested in keeping up with this model-sharing discussion. I think there&#39;s a lot of interesting work that needs to be done here, and it&#39;s a key thing that&#39;s being considered within the discussions around international AI governance right now. Yeah, so sorry I don&#39;t have as much of a clear cut answer there, but yeah, I&#39;m still reeling from having published this report and then everything that&#39;s coming off the back of it and just trying to feel out where&#39;s the next most important, most impactful step, what work needs to be done. So I guess if any of your listeners have really hot takes on “oh, this is what you should do next”, I guess, please tell me. It&#39;ll be very helpful.</p><p> <strong>Daniel Filan:</strong> How should they tell you if someone&#39;s just heard that and they&#39;re like, oh, I need to-</p><p> <strong>Elizabeth Seger:</strong> “I need to tell her now! She must know!” Yeah, so I mean, I have a <a href="https://elizabethseger.com/">website</a> where you could find a lot of my contact information, or you can always find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . I spend far too much time on LinkedIn these days. And also my email address happens to be on the open source report. So if you download the report, my email address is there.</p><p> <strong>Daniel Filan:</strong> What&#39;s the URL of your website?</p><p> <strong>Elizabeth Seger:</strong> <a href="https://elizabethseger.com/">ElizabethSeger.com</a> .</p><h3> Distinctive governance issues of x-risk<a name="xrisk-different"></a></h3><p> <strong>Daniel Filan:</strong> All right.好的。 Getting back to talking about governance in general, I&#39;m wondering… so I guess this is an x-risk-focused podcast. How, if at all, do you think governance research looks different when it&#39;s driven by concerns about x-risk mitigation versus other concerns you could have about AI governance?</p><p> <strong>Elizabeth Seger:</strong> Well, that&#39;s a good question.让我们来看看。 So the question is how does the governance research look different?</p><p> <strong>Daniel Filan:</strong> Yeah. What kinds of different questions might you focus on, or what kinds of different focuses would you have that would be driven by x-risk worries rather than by other things?</p><p> <strong>Elizabeth Seger:</strong> So this is something that I&#39;ve had to think about a lot in my own research development because I did not come into this area of research from an x-risk background interest. I came into it… I mean, honestly, I started in bioethics and then moved from bioethics looking at AI systems in healthcare and have sort of moved over into the AI governance space over a very long PhD program. And so here I am.</p><p> But I would say one of the things that I&#39;ve learned working in the space [that&#39;s] more interested in long-term x-risk impacts of AI and trying to prevent x-risks is really paying attention to causal pathways and really trying to be very critical about how likely a potential pathway is to actually lead to a risk. I don&#39;t know if I&#39;m explaining this very well.</p><p> Maybe a better way of saying it&#39;s like: if you have a hypothesis, or let&#39;s say you&#39;re worried about the impacts of AI systems on influence operations or impacting political campaigns, I find it really helpful to start from the hypothesis of, it won&#39;t have an impact. And really just trying to understand how that might be wrong, as opposed to trying to start from “oh, AI is going to pose a massive bio threat, or it&#39;s going to pose a massive threat to political operations” or something like that. And then almost trying to prove that conclusion.</p><p> Yeah, I don&#39;t know, I start from the opposite point and then try and think about all the ways in which I could be wrong. And I think this is really important to do, especially when you&#39;re doing x-risk research, whether it&#39;s with respect to AI or some other form of x-risk. Because I think there are a lot of people that turn off when you start talking about existential risks, they think it&#39;s too far out there, it&#39;s not really relevant to the important questions that are impacting people today, the tangible things that people are already suffering 。 And so I think it&#39;s really important to be very, very rigorous in your evaluations and have a very clear story of impact for why it is that you&#39;re doing the research you&#39;re doing and focusing on the issues that you&#39;re doing. At least that&#39;s been my experience, trying to transition into the space and work on these issues.</p><h3> Technical research to help governance<a name="tech-for-gov"></a></h3><p> <strong>Daniel Filan:</strong> Another question I have, related to my audience… So I think my audience, a lot of them are technical alignment researchers and there are various things they could do, and maybe they&#39;re interested in, okay, what work could technical alignment people do that would make AI governance better? I&#39;m wondering if you have thoughts on that question.</p><p> <strong>Elizabeth Seger:</strong> Okay. Technical alignment people, AI governance better. Yeah, I mean there&#39;s a lot of work going on right now, especially within the UK government. We just set up the <a href="https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report">UK AI Task Force</a> , a government institution doing a lot of model evals and alignment research. I think if you have the technical background in alignment research, you are very much needed in the governance space. There&#39;s very often a disconnect between… I mean, I am also guilty of this. There&#39;s a disconnect between the people doing the governance research and the people who have the experience with the technology and really know the ins and outs of the technology that&#39;s being developed.</p><p> And I think if you have the inclination to work in an AI governance space and help bridge that gap, that would be incredibly valuable. And like I&#39;ve already said, some of the more technical questions, even around open-sourcing, are things that I was very, very glad to have colleagues and co-authors on the paper who have worked for AI labs and stuff before and really knew what they were talking about and could advise and help write some of the more technical aspects of the report.</p><p> So I think if you have the inclination to work in the space, to get involved with governance efforts, or even maybe some of these government institutions that are starting to pop up that are working on the boundary of AI governance and technical research, that could be a really valuable place to contribute. So I think my 2 cents off the top of my brain would be help bridge that gap.</p><p> <strong>Daniel Filan:</strong> Okay.伟大的。 So before we wrap up, I&#39;m wondering if there&#39;s anything that you wish I&#39;d asked but that I didn&#39;t?</p><p> <strong>Elizabeth Seger:</strong> Oh, that&#39;s a good question.不，我不这么认为。 I think we&#39;ve covered a lot of good stuff. Yeah, thank you for having me on really. I&#39;d say there&#39;s nothing in particular. This has been great.</p><h2> Following Elizabeth&#39;s research<a name="following-elizabeths-research"></a></h2><p> <strong>Daniel Filan:</strong> All right, so to wrap up then, if people are interested in following your research, following up on this podcast, how should they do that?</p><p> <strong>Elizabeth Seger:</strong> So I have my website, <a href="https://elizabethseger.com/">ElizabethSeger.com</a> . It sort of outlines my different ongoing research projects, has a lot of publications on it. Also, <a href="https://www.governance.ai/">GovAI&#39;s website</a> is a wealth of information [on] all things AI governance from all my great colleagues at GovAI and our affiliates. So really, yeah, there&#39;s new research reports being put out almost every week, maybe every other week, but really high quality stuff. So you can find a lot of my work on the GovAI website or my current work and past work on my own website or find me on <a href="https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b">LinkedIn</a> . Yeah, just happy to talk more.</p><p> <strong>Daniel Filan:</strong> All right, well thank you very much for being on the podcast.</p><p> <strong>Elizabeth Seger:</strong> Great, thank you.</p><p> <strong>Daniel Filan:</strong> This episode is edited by Jack Garrett and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the <a href="https://funds.effectivealtruism.org/funds/far-future">Long-Term Future Fund</a> and <a href="https://lightspeedgrants.org/">Lightspeed Grants</a> , along with <a href="https://patreon.com/axrpodcast">patrons</a> such as Tor Barstad, Alexey Malafeev, and Ben Weinstein-Raun. To read a transcript of this episode or to learn how to <a href="https://axrp.net/supporting-the-podcast/">support the podcast yourself</a> , you can visit <a href="https://axrp.net">axrp.net</a> . Finally, if you have any feedback about this podcast, you can email me at <a href="mailto:feedback@axrp.net">feedback@axrp.net</a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RDm26xAcb9rfvuBya/axrp-episode-26-ai-governance-with-elizabeth-seger<guid ispermalink="false"> RDm26xAcb9rfvuBya</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sun, 26 Nov 2023 23:00:07 GMT</pubDate> </item><item><title><![CDATA[Solving Two-Sided Adverse Selection with Prediction Market Matchmaking]]></title><description><![CDATA[Published on November 26, 2023 8:10 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/s2zbbqpeaxpoy4rr2jld" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/xff9jzs0fgp7u21ciaq2 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/rh8uwt6btzrqhnapovq0 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/os4dc1r67s52jnlmy1lz 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/esdpc5afazjyasnzdaos 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ou8et2puwea0fhemlh9c 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/d3satrakwyr7bajmwaii 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/umzxc8fjv0hojb4029yp 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/ti6ciszrdxr8tf4wpvdq 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/sygzudsyigugf2av63wi 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/n4tgv9uhgzzhjdlkzzpz 1500w"><figcaption>做市可能会最大化有意义的匹配，就像这对梨一样。</figcaption></figure><h1> 0：导航</h1><p>我的目标读者是知道什么是“预测市场”和“逆向选择”，喜欢第一个而不是第二个，并且喜欢激励机制完全一致的系统的读者。有关预测市场的入门知识，请阅读 Scott Alexander 的<a href="https://www.astralcodexten.com/p/prediction-market-faq?ref=brasstacks.blog"><u>常见问题解答</u></a>。如果您已经熟悉<a href="https://www.brasstacks.blog/pm-matchmaking/manifold.love"><u>Manifold Love</u></a> ，请跳至第 2 部分。</p><p> COI：我已经为 Manifold 做了<a href="http://manifestconference.net/?ref=brasstacks.blog"><u>一些工作</u></a>，可能还会做更多工作，并拥有<a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a>的一点股权。我写这篇文章与我目前正在为 Manifold 或任何其他实体所做或计划做的任何工作无关。我只是觉得这些想法很酷。</p><h1> 1：爱</h1><p>玩币预测市场平台<a href="http://manifold.markets/?ref=brasstacks.blog"><u>Manifold</u></a>最近发布了一款名为“ <a href="http://manifoldlove.com/?ref=brasstacks.blog"><u>Manifold Love</u></a> ”的预测市场交友应用。用户——那些寻求爱情的人——像普通的约会应用程序一样注册并填写他们的个人资料。媒人（其中一些人本身就是该应用程序的用户）为用户配对。媒人进行匹配后，会根据配对（潜在）关系的各种基准自动创建预测市场。 </p><p><img style="width:39.59%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/l1ccolmjl6vrdipnviao" alt=""><img style="width:40.15%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/zc7yymewfsr49fqa3ksu" alt=""><img style="width:42.65%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/tjknavwqh60xxe82sqje" alt=""><img style="width:37.39%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/99WwKnkE2FKAFo2ap/qjiqij5jrbtd2twivi1h" alt=""></p><p> Manifold Love 部分市场截图。</p><p>人们打赌（使用<a href="https://docs.manifold.markets/faq?ref=brasstacks.blog#what-is-mana-m"><u>虚拟货币</u></a>）两人是否会进行第一次约会；以第一次约会发生为条件，进行第二次约会；以第二次约会发生为条件，第三次；以第三次约会为条件，一段为期 6 个月的关系。这个想法是<strong>，你应该和最有可能促成后续约会并最终发展成关系的匹配者一起约会。</strong>您无需猜测那人是谁，只需查看预测市场即可。</p><h1> 2：概括</h1><p>重要的是，Manifold Love 的设置不仅限于解决约会市场的问题。它解决了（或者至少有可能解决）<i>所有</i>存在双边逆向选择的情况下的问题，即双边市场的每一方都选择了对方认为不利的东西。用格劳乔·马克斯（Groucho Marx）的话来说，“我不想加入任何愿意接受我作为会员的[乡村]俱乐部。”如果一个俱乐部想要格劳乔·马克斯，它可能不是一个非常好的俱乐部，反之亦然——如果格劳乔·马克斯想加入某个特定的俱乐部，他可能不是一个非常理想的成员。</p><p>更一般地说，“在通常陷入逆向选择的双方之间的潜在配对的一系列关键基准上运行一堆条件预测市场”的设置似乎可以很好地发挥作用。</p><p>双向逆向选择的典型案例之一是劳动力市场，因此以下是一种“多重工作”可能会如何发挥作用。该平台拥有三个实体：</p><ol><li>未来员工</li><li>未来雇主</li><li>猎头公司</li></ol><p>该平台根据前两个实体的关键基准创建了一系列条件预测市场。例如：</p><ul><li>在被[未来雇主]雇用的条件下，[未来雇员]将：<ul><li>在[时间范围]内仍在工作吗？</li><li>在[时间范围]内每周平均生活满意度比现在更高？</li><li> ETC。</li></ul></li><li>以雇用[未来雇员]为条件，[未来雇主]将：<ul><li>在[时间范围]内仍在雇用[员工]？</li><li>在[时间范围]内，员工的每周平均评分高于前任员工？</li><li> ETC。</li></ul></li></ul><p> <a href="https://manifold.markets/TonyBaloney/will-manifold-be-used-for-job-recru?ref=brasstacks.blog"><u>如果 Manifold 真的做到了</u></a>，我相信它看起来会有所不同，更好，更充实。但重要的是，约会和劳动力市场并不是双向逆向选择的唯一两个领域。</p><h1> 3：在此插入双面逆向选择</h1><p>还有许多其他双向逆向选择的情况，其中 Manifold Love 的条件预测市场设置可能会解决很多问题：</p><ul><li>约会（多重爱）</li><li>友谊</li><li>健身房合作伙伴</li><li>赠款</li><li>大学申请/决定</li><li>研究生院应用程序/决策（例如医学院、法学院、商学院等）</li><li>劳工/雇用/人才招聘/工作（如第 2 节）</li><li>联合创始人</li><li>种子期和种子期前投资</li><li>儿童收养</li><li>实习申请/决定</li><li>二手车销售</li><li>辅导</li><li>活动/场地空间</li><li>治疗师/患者</li><li>销售和购买保险</li><li>信贷/贷款</li><li>住宅及商业地产</li><li>学生在大学选课</li></ul><p>如果您能想到更多，<a href="https://www.brasstacks.blog/pm-matchmaking/saulmunn.com/contact"><u>请告诉我</u></a>，我会在这里添加它们！需要注意的是：对于上述大多数情况，已经有一些实体充当中间人——房地产有房地产经纪人，劳动力市场有猎头，收养机构将亲生母亲与收养家庭配对，等等。但所有这些都相当破碎，有偏差，或者至少是非常次优的。我对条件预测市场改进并解决双边逆向选择的潜力感到兴奋。</p><h1> 4：注意事项</h1><ul><li>在我撰写本文时，我是一名地位较低的本科生，从未上过经济学课，也不知道他在说什么。我在预测市场和预测社区做了<a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>一些</u></a><a href="https://www.brasstacks.blog/pm-matchmaking/opticforecasting.com"><u>工作</u></a>，但我离专家还差得很远。</li><li>我们甚至不知道“多重爱”是否适用于约会，更不用说它是否可以推广到其他双向逆向选择系统了。</li><li><strong>没有事先承诺随机化的条件预测市场并不意味着因果关系</strong>（a la <a href="https://dynomight.net/prediction-market-causation?ref=brasstacks.blog"><u>DYNOMIGHT</u></a> ）。这种设置将为我们提供关联/相关性，但<strong>如果没有事先承诺随机化，我们将不知道因果关系的存在或方向</strong>。之前对其中一些系统的少量随机化的承诺范围从“相当困难”到“哈哈哈好，<i>绝对他妈的不是</i>”。幸运的是，我不确定你是否需要知道因果关系，至少在开始时不需要。我很好奇如果/当 Manifold Love 遇到这个问题时会发生什么。</li><li>再说一遍，COI：我已经为 Manifold 做过<a href="https://www.brasstacks.blog/pm-matchmaking/manifestconference.net"><u>工作</u></a>，可能还会为<a href="https://www.brasstacks.blog/pm-matchmaking/manifold.markets"><u>Manifold</u></a>做更多工作，并且拥有少量股权。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two-sided-adverse-selection-with-prediction-market#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/99WwKnkE2FKAFo2ap/solving-two- Side-adverse-selection-with-prediction-market<guid ispermalink="false"> 99WwKnkE2FKAFo2ap</guid><dc:creator><![CDATA[Saul Munn]]></dc:creator><pubDate> Sun, 26 Nov 2023 20:10:23 GMT</pubDate> </item><item><title><![CDATA[Wikipedia is not so great, and what can be done about it.]]></title><description><![CDATA[Published on November 26, 2023 7:13 PM GMT<br/><br/><p><i>注意：这篇文章最初出现在 Reddit 的 /r/trueunpopularopinion 子目录中，因为</i><a href="https://old.reddit.com/r/TrueUnpopularOpinion/comments/zieyyf/wikipedia_is_not_so_great_and_is_overrated/"><i>维基百科不是那么好，并且是</i></a><i>由那里的用户撰写的。它是由他们在 CC0 公共领域下明确发布的。</i></p><p>你们现在都已经听说，当有关 Twitter 文件的文章被建议删除时，埃隆·马斯克表示维基百科存在“左翼偏见”。自由派和保守派对此反应不一。前者将其斥为“对自由知识的攻击”，后者则欢呼此举“反对审查制度”，并证明了他们认为大型科技公司对他们存在偏见的信念是正确的。</p><p>确实，维基百科据说可以被世界各地的任何人编辑，多年来我一直断断续续地担任编辑，主要进行一些小规模的编辑，例如修复拼写错误和恢复明显的破坏行为。这是在 IP 上完成的，而不是使用帐户，因为我宁愿某些编辑（即宗教和政治领域等敏感主题）与我的姓名和身份无关。然而，现实远非维基百科所偏爱的糖衣描述，尤其是其编辑社区。</p><p>总体而言，编辑社区最好被描述为一种稍微等级森严和军国主义的“把一切都做好”的结构，传统上与​​戴尔、最近的富士康和现已解散的希拉洛斯联系在一起。例外情况适用于较安静和异​​常的区域，例如当地地理和空间，通常是想要尝试第一手经验的新用户的最佳切入点。对诸如观点问题和使用不可靠资源等善意错误的容忍度更高，通常会详细解释如何纠正这些错误，而不仅仅是警告模板甚至突然阻止。</p><p>最终，那些可以说由<a href="https://meta.wikimedia.org/wiki/Exopedianism">exopedians</a>组成的子社区，对更广泛的核心社区（主要由<a href="https://meta.wikimedia.org/wiki/Metapedianism">metapedians</a>主导）的影响力相对较小甚至没有。第三组称为<a href="https://meta.wikimedia.org/wiki/Mesopedianism">中足动物，</a>经常在这些内部和外部运作之间交替。社区可以拥有按<a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject">WikiProject</a>分组的共同主题兴趣，例如<a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Science">WikiProject Science</a></p><p>我花了很多时间像墙上的苍蝇一样随意浏览编辑大战（有时可能很<a href="https://en.wikipedia.org/wiki/Wikipedia:Lamest_edit_wars">蹩脚</a>），以及维基百科的元场所，例如<a href="https://en.wikipedia.org/wiki/WP:AFD">删除文章</a>、<a href="https://en.wikipedia.org/wiki/Wikipedia:Centralized_discussion">集中讨论</a><a href="https://en.wikipedia.org/wiki/WP:NPOVN">中立观点布告栏</a>、<a href="https://en.wikipedia.org/wiki/WP:BLPN">活着的人传记布告栏</a>、<a href="https://en.wikipedia.org/wiki/WP:COIN">利益冲突布告栏</a>、<a href="https://en.wikipedia.org/wiki/WP:ANI">管理员布告栏事件</a>、<a href="https://en.wikipedia.org/wiki/WP:SPI">马甲调查</a>、<a href="https://en.wikipedia.org/wiki/Wikipedia:Arbitration_Committee/Noticeboard">仲裁委员会布告栏</a>，这是维基百科社区中处理严重行为和行为纠纷的“最高法院”。因此，我可以总结一下编辑社区的真正运作方式，尽管并不像您想象的那么广泛，因为我不是内部运作方面的“维基百科狂”。</p><h1><strong>删除主义和包容主义</strong></h1><p>这是维基百科上任何争议的长期核心原因。删除主义者将维基百科视为另一部“常规百科全书”，一旦信息内容变得非常多，就必须对其进行限制；就像剔除垃圾一样，而包容主义者则将维基百科视为不受纸张约束的综合性百科全书，因此可以涵盖尽可能多的信息；一个人的垃圾可能是另一个人的宝藏。我个人支持后者，两种编辑意识形态之间的冲突常常会导致派系之争，在这种情况下，理解相互感受和观点的尝试是不够的，甚至根本没有。</p><p>对于“百科全书式知识”和“知名度”的定义，并没有绝对的标准。包容主义认为，几乎所有事物在未来都可能变得有价值和百科全书式，即使它们现在还不是。我能想到的一个例子是二战中的事件、人物和故事。删除主义与“学术标准踢”密切相关，并依赖于维基百科必须高标准和简洁的前提。有些人认为添加一些东西是有用的，而有些人则认为这是“琐事”或“粗俗”的东西，如果维基百科的文档没有禁止的话，名义上是不鼓励的（特别参见<a href="https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_an_indiscriminate_collection_of_information">这一点</a>，尽管有时通过“<a href="https://en.wikipedia.org/wiki/WP:IAR">为了进步而忽略所有规则</a>”的精神，这在娱乐和游戏话题中很常见）。</p><p>在页面上，围绕个人主题和其他方面的知名度辩论经常是删除文章主题中讨论的要点，其中被认为不够实质性的文章（例如来源很少）建议删除。通常会运行一周，但如果赞成“保留”、“删除”等的票数过多，AFD提名被发起人撤回，或者提名被发现有问题，可以很快关闭。是出于恶意（例如出于意识形态、付费编辑等可疑动机而从公众视野中“审查”文章）。</p><p>在这里，我认为包容主义者对删除主义者的看法要严厉得多，反之亦然。主要原因是要添加一些内容，您必须浏览用户体验不友好的编辑界面（尽管近年来有所改进），同时还必须滚动互联网来查找要添加的来源和参考。当你发现一些信息时，你必须经过额外的关卡来评估它们是否可靠，然后才能最终用你自己的话抄写信息，这必须坚持中立观点（NPOV）政策；不允许如此接近的释义，因为版权。非英语编辑通常会发现后者非常困难。</p><p>相反，正如一句古老的格言所说，破坏比建立更容易，删除比添加更容易。在我看来，这可能就是为什么删除主义目前在整个网站上保持主导地位的原因，因为为了成为一名受人尊敬的编辑，一个人必须获得大量不被恢复的编辑。因此，许多编辑喜欢通过严格解释文档和规则，从段落到整篇文章中删除“无用信息”来获得这些“分数”，后者通过诸如“删除文章”之类的过程，如果足够有信心，<a href="https://en.wikipedia.org/wiki/Wikipedia:Proposed_deletion">建议删除</a>不会&#39;不需要讨论。简而言之，这是一个功能而不是错误，并且不一定受制于任何政治意识形态；自由主义者和保守主义者一样有可能成为令人讨厌的删除主义者。</p><p>尽管每次编辑更改都会通过页面历史记录进行记录和显示，您可以通过单击顶部的“查看历史记录”来查看任何给定文章，但争论的焦点仍然存在，特别是当页面删除导致这些历史记录从公共视图中删除时。稍后将进一步解释这一点。</p><p>关于当前删除主义的突出现象，可以想到的一些历史背景是 2007 年期间或之前的 Pokemon 页面过多，这疏远了一些读者和编辑，因为当时的搜索引擎在查找精确内容方面不如今天那么充分。信息。另一个原因是，像内森·拉尔森这样的儿童掠夺者过去常常以包容主义者的身份潜入，扭曲维基百科以适应他们的议程，这对我们所有人和当时的人来说都是不可磨灭的可怕。想想井里的毒和毒树上的果实。此外，还有很大一部分用户群来自英特尔等科技公司和学术界（也许不是 GLAM，画廊、图书馆、档案馆和博物馆的缩写），他们获得了管理员等高层职位，带来了他们的工作文化等- 分别称为“学术标准踢”。绝对公平地说，我发现在某些情况下删除主义是足够正确的，特别是删除任何在世人的传记页面上的侵犯版权和诽谤材料。</p><p>无论页面是否被删除，它们仍然可以在维基百科的服务器中使用，并且只有管理员或更高级别的人员可以访问。</p><p>最终，“百科全书式知识”的定义也容易受到<a href="https://en.wikipedia.org/wiki/Wikipedia:Systemic_bias">系统性偏见的</a>影响。与马斯克的一些想法不同，白人、男性、美国/英国/加拿大/欧盟/澳大利亚/新西兰、中老年、说英语的用户往往在编辑社区中比其他人拥有最大的优势。考虑到这一点，赞比亚的一位著名音乐艺术家可能会被加拿大的编辑认为在维基百科上的页面太小了。美国的购物中心被删除的可能性比越南的要小。这种偏见不是单向的，而是单向的。假设的赞比亚艺术家对秘鲁人来说“不重要”。</p><p>这是编辑之间产生敌意的主要原因，也是许多编辑选择退出甚至失宠的原因。你永远会讨厌那个每次你组装积木时都喜欢毁掉你的乐高结构的孩子。</p><h1><strong>中立观点</strong></h1><p>与单纯的删除和添加不同，这通常意味着如何以一种理想的方式向读者呈现给定的信息，以便在他们的印象中不会留下对某事不成比例的偏见。你会在两个或多个国家关于争议地区的政治文章、历史文章和地理主题中看到如此多的争论和冲突。假设某个政治人物从事与极端主义有一定联系的活动。 A 方会争辩说，该人物因此是极端分子，应该在该页面和任何其他链接页面上突出显示，但 B 方希望通过写“政治人物从事有时被报道的活动”之类的内容来淡化其语气。被一些人视为极端主义者”，并将其限制为仅在一页上提及。另一个是一个国家应该被称为“部分承认的国家”，因为一些联合国成员国不承认它本身，而是作为一个更大国家的一部分，其他人则表达了这样的观点：它只是拥有有效的主权，与其他国家不同。其他国家就足以被视为一个国家。</p><p>它也可以在涉及“边缘理论”的案例中发挥作用，例如大脚怪、不明飞行物和医疗，尽管维基百科确实倾向于在这些案例中突出主流观点，但我认为这没有问题，而且与常规的有害偏见完全不同。</p><p>本案的解决地点是中立观点布告栏以及<a href="https://en.wikipedia.org/wiki/WP:RFC">征求意见</a>。后者需要一个过程，通知被张贴在集中的布告栏上，同时一批经验丰富/成熟的编辑收到通知，就特定问题发表评论、提供见解并提出建议。除非由于持续的僵局等原因需要延长，否则这些讨论的启动和运行通常需要一个月的时间。</p><p>与删除主义和包容主义一样，这是编辑“调皮”并被屏蔽/禁止/踢出的主要原因，无论是出于正确还是虚假的原因。</p><h1><strong>执行</strong></h1><p>我诚实地认为这篇文章中最重要的部分。我将通过撰写有关编辑战争的文章来开始本节。通常，当你在维基百科中更改某些内容并且它被其他人撤消/恢复时，如果你顽固地不去文章的讨论页面（“谈论”），那么在你被报告到<a href="https://en.wikipedia.org/wiki/WP:ANEW">编辑战争布告栏</a>之前你只有两次尝试”在左上角）以供撤消编辑的人或第三方进行讨论。与此同时，您会在个人谈话页面（右上角的“谈话”）上收到通知，邀请您进行此类讨论，如果足够幸运，<a href="https://en.wikipedia.org/wiki/WP:TEAHOUSE">维基百科茶馆</a>会寻求一些善良编辑的进一步帮助，这在当今越来越罕见。在一些较安静或外部区域（如前所述，稍微宽松一些），您最多可以达到大约 10 米。在提交给管理员之前有五次机会计算您的原始编辑。</p><p>尝试计数会在 24 小时后重置，但可以进一步保留，以防止“玩弄规则”。更明确的破坏行为（例如在任何页面上放置诸如“LOLOLOLOLOLOL”之类的胡言乱语）通常会被报告到一个<a href="https://en.wikipedia.org/wiki/WP:AIV">单独的布告栏，以便管理员进行干预</a>，尽管第一次破坏者通常会事先在他们的讨论页面上收到警告。当有报告出现并且被发现犯有编辑战罪时，管理员要么向相关用户发出最后通牒，要么封锁他们的帐户一天。如果行为持续超过这个期限，它们可能会升级到几天、几周甚至无限期（实际上是无限期）。破坏行为也是如此，尽管对于“仅限破坏行为的帐户”，他们会受到许多提示无限期封锁（indeffs）的更严厉处理。</p><p>常规编辑也可能面临因自己或他人而失宠的危险。由于维基百科被许多人视为世界上最大的综合性百科全书，有时甚至等同于历史本身，许多既得利益、情感和情感都被投入到该网站上。</p><p>那些民族主义者或其他任何可以想象的概念的狂热分子发现自己有动力让维基百科支持他们的叙述，这既是为了目的本身，又是为了其他目的的手段，例如“证明他们在伟大历史的长河中是伟大的” ”。这同样适用于公司和个人及其支持者或粉丝制作的“宣传编辑”。相反，许多人发现扭曲它来诋毁任何意识形态、公司、人民以及他们个人反对的任何事物都极具吸引力。例如，他们可以制作一篇文章，并在其中填充针对​​他们的贬低信息，这被称为“攻击页面”。</p><p>我发现“维基百科是信息的占位符”和“维基百科就是历史”这一普遍观点是有道理的。<a href="https://news.mit.edu/2022/study-finds-wikipedia-influences-judicial-behavior-0727">麻省理工学院的一份报告</a>描述了法官的行为如何越来越多地受到维基百科文章的影响，而<a href="https://mashable.com/article/moon-library-beresheet-crash-wikipedia">Beresheet</a>和<a href="https://www.archmission.org/lunar-library-2">Peregrine</a>等太空任务则采取了举措，在<a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">发生以下情况</a>时使用所有英文维基百科（截至给定日期的版本）对人类进行文明备份<a href="https://en.wikipedia.org/wiki/Global_catastrophic_risk">坍塌</a>。</p><p>在按照他们的方式行事后，为了将他们的变化永远保留在“历史编年史”或简单的“信息占位符”中，一般会采用把关措施。一个简单的例子就是对那些做出挑战既定现状的改变的编辑使用过于严厉的语言。相反，如果有人有理由从根本上更改页面并确保其之后保持无懈可击，也会使用相同的一组操作，但可以说这些将是“反守门”措施。</p><p>在守门/反守门中，人们会诉诸对<a href="https://en.wikipedia.org/wiki/WP:PAG">PAG</a> （政策和指南）和<a href="https://en.wikipedia.org/wiki/WP:ESSAY">用户文章</a>的不同级别的解释，后者有时用作许多编辑和行政行为的基础。这些文档经常会相互矛盾，就像“<a href="https://en.wikipedia.org/wiki/Wikipedia:NOTINDISCRIMINATE">不加区分</a>”与“<a href="https://en.wikipedia.org/wiki/Wikipedia:NOTPAPER">不是纸质百科全书</a>”一样，最重要的是，如果有人认为合适，可以通过<a href="https://en.wikipedia.org/wiki/Wikipedia:IAR">忽略这些文档</a>来推翻。因此，谁拥有“最大的拳头”，谁就在维基百科社区中最占优势。为了拥有“最大的拳头”，他们可以与任何与自己有共同兴趣的人成为朋友，并组成一个照顾自己的阴谋集团/帮派。为了增加他们的权力，当足够的时间过去后，他们可以互相提名对方担任<a href="https://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship">管理员职位，</a>从而赋予他们阻止用户、删除页面、保护文章免遭低级别用户编辑的额外权限。除非您在用户个人资料中列出了 Venmo 链接或加密地址，否则您不会因为花费精力和时间编辑维基百科而获得报酬，并且鉴于您基本上处于控制之中，仅这些管理工具就非常令人上瘾和有吸引力除了通常的人性之外，如果你有这些，那么你就会了解“书写历史”的重要部分。毕竟，维基百科是世界上访问量最大的 10 个网站之一。</p><p>更重要的是，在管理员职位之上还有其他级别。其中两个是用户检查员 (CU) 和监督员。 CU 有权查看帐户使用的 IP 地址，看看它是否是某个人的马甲帐户，而 Oversights 则拥有超级删除权限，可以隐藏内容或页面，甚至超出管理员的范围。</p><p>那些处于权力跳闸、看门等另一端的人很少表现得很好。人们会发现他们被那些编辑贬低、欺负。如果他们试图通过既定流程（例如讨论页讨论、<a href="https://en.wikipedia.org/wiki/WP:DRN">争议解决布告栏</a>以及臭名昭著的管理员布告栏事件（ANI））来妥善解决问题，他们会期望在途中遇到重重障碍。如果受害者决定邀请其他编辑就某个问题提供平衡/公正的意见和建议，他们会发现自己会因为这些是“<a href="https://en.wikipedia.org/wiki/Wikipedia:Canvassing">拉票</a>”而受到阻碍。如果“恶霸”事先通知他们的帮派朋友，那可能是相当虚伪的，有理由相信这种情况经常发生。最后，如果它升级为 ANI，这就是事情开始失控的地方。</p><p>我之所以用“臭名昭著”这个词，是因为ANI是各种丑剧的发源地。它通常是获得编辑批准等的首要位置。然后，欺凌者（我不会轻易使用这个词）会对他人的任何类型的不当行为提出各种指控和诽谤，无论是真实的还是感知的，大的还是小的，或者结果是真正的伤害还是只是一个无用的汉堡。无论如何，如果他们扭曲了规则（嘲笑地称为“<a href="https://en.wikipedia.org/wiki/Wikipedia:Wikilawyering">维基律师</a>”或“<a href="https://en.wikipedia.org/wiki/Wikipedia:Gaming_the_system">玩弄系统</a>”）并充分发挥了受害者的作用，通过的管理员就会结束讨论并对“真正的”受害者采取行政行动。这种行为的常见的令人震惊的例子是“ <a href="https://en.wikipedia.org/wiki/Wikipedia:Here_to_build_an_encyclopedia#Clearly_not_being_here_to_build_an_encyclopedia">不是在这里构建百科全书</a>”的不确定/永久块，可以从任何给定的行为中任意解释。具有讽刺意味的是，恶霸们也犯有这样的罪行。扭曲规则以限制/排挤其他编辑者的一个典型例子是从所谓的<a href="https://en.wikipedia.org/w/index.php?title=Wikipedia:BADFAITHNEG&amp;redirect=no">恶意谈判</a>开始，他们向受害者承诺，如果受害者让恶霸将他们的更改保留在页面中，则不会删除其他页面上的内容。很快，霸凌者就食言了，当面对受害者时，霸凌者立即指责他们“<a href="https://en.wikipedia.org/wiki/WP:TENDENTIOUS">有倾向性</a>”或“POV推动者”。</p><p>欺凌者可能包括大多数在内部运作的编辑，他们不一定受制于任何意识形态，而且形形色色。他们唯一的共同点就是对权力的痴迷。</p><p>在这样的永久封锁之后，大多数人将被迫永远离开它，进一步<a href="https://en.wikipedia.org/wiki/Wikipedia:Why_is_Wikipedia_losing_contributors_-_Thinking_about_remedies">减少编辑人数</a>。尽管如此，由于维基百科如此卓越，而且目前没有可行的竞争对手，有些人宁愿留下来，隐藏自己的身份，要么继续编辑，要么在不同的领域重新开始。对于那些懂外语的人来说，他们可以简单地切换到其他语言的维基百科来继续他们的工作，远离大多数干扰。少数人会作为破坏者回来报复那些冤枉了他们的编辑。</p><p>这就是“<a href="https://en.wikipedia.org/wiki/Wikipedia:Sockpuppet_investigations">马甲调查</a>”发挥作用的地方，通常被称为 SPI。如果编辑怀疑某个帐户是其他人（尤其是逃避封锁/禁令的用户）的 alt/sock 帐户，他们就会去那里启动新的案件。当用户被永久屏蔽或禁止时，他们会被降级为贱民状态，类似于“非人格化”、山<a href="https://en.wikipedia.org/wiki/Songbun">达基</a>教的<a href="https://en.wikipedia.org/wiki/Suppressive_Person">压制性人员</a>以及朝鲜出身的最低级人员，因为他们在其他帐户下进行的任何和所有编辑或者根据与<a href="https://en.wikipedia.org/wiki/Wikipedia:BE">阻止规避相关的政策，</a> IP 可能会被恢复/撤销。虽然不把麦子与谷壳分开的最初目标明确是为了防止他们<a href="https://en.wikipedia.org/wiki/Wikipedia:Deny_recognition">获得进一步的认可</a>并削弱区块的精神，但实际上，这意味着猴爪，他们将永远失去任何进一步的潜在良好贡献，从而阻碍了区块链的发展。在某种程度上或更多程度上改进了整个百科全书。其他编辑者有编辑战政策的例外，可以恢复和撤销违反区块的人（也许还有任何帮助他们的人）所做的任何更改。实际上，这就像 Meatball Wiki 所说的“ <a href="http://meatballwiki.org/wiki/PunishReputation">PunishReputation</a> ”。</p><p>在 SPI 期间，有“职员”会查看用户的贡献历史记录，看看是否存在相似的模式，以保证阻止滥用多个帐户（马甲）。如果仅此还不够，则可以调用 CheckUsers 检查并比较帐户使用的 IP。</p><p>如果确定用户参与了 sockpuppetry，则所使用的原始帐户和替代帐户的用户页面将被替换为红字通知，例如<a href="https://en.wikipedia.org/wiki/User:PositiveIntentsOnly">本示例</a>，夸耀哪个 sock 帐户属于谁，因此被阻止。别说“否认承认”，这只是一种惩罚性的点名羞辱。</p><p> SPI 案例现在列出了所使用的帐户和 IP，然后将存档在一个单独的页面中，仍然可以公开查看。尽管最近有 GDPR 法规，并且暗示应该对该流程进行重大的隐私改进调整，同时保持其可行性。我可以向你保证，在 Reddit 上尝试一下，你会因为人肉搜索而立即被禁止。</p><p>在那里，你可以有效地扮演 CSI，尽管文员、管理员和用户检查员得到了实质性的关注。请记住，大多数（如果不是全部）马甲调查的结果和结果并不是100%准确，因为存在许多不可预见的变量，例如写作和行为风格的模仿，这些变量主要是多人推动的结果出于任何原因进行的任何特定编辑更改，例如兄弟帮助他的姐妹，以及使用可以屏蔽您的 IP 地址的软件，例如 VPN 和 TeamViewer。那些负责马甲调查的管理员通常不了解“马甲”或“阻止逃逸”的根本原因，因此往往会低估有正确理由支持编辑的用户数量。违反块。</p><p> VPN IP 地址出于明显的隐私原因而使用，根据针对<a href="https://en.wikipedia.org/wiki/Wikipedia:Blocking_policy#Open_or_anonymous_proxies">开放代理的策略</a>，任何管理员都无法看到这些地址。他们甚至有一个专门的维基项目和一个专门寻找和阻止这些代理的机器人，结果给俄罗斯和中国等希望进行编辑的人们带来了极大的不便。</p><p>随着时间的推移，如果某人在最初的封锁之后继续进行被其他编辑视为“破坏性”或“破坏性”的行为，他们最终会被显示在所谓的“<a href="https://en.wikipedia.org/wiki/WP:LTA">长期滥用</a>”案例列表中。就在那里，列出了他们的帐户和/或 IP 地址，以及对他们所做的事情的可能歪曲的描述。他们去过的地方和维基百科之外的账户经常被曝光，就好像这是一项反对派研究和恶意的人肉搜索。维基百科上那些会让你很快被禁止的事情只是一个普通的周二，GDPR 已经被排除在外了。</p><p>在我看来，有两类长期协议/破坏者/无论你怎么称呼它。第一种是固有的破坏者，他们在第一次编辑时就给维基百科带来了问题和破坏性，另一种是那些过去一直是常规或良好信誉的用户，直到他们失宠，通常是由于他们自己造成的，例如过度劳累。可能是针对一件事，但也可能是由其他事情造成的，就像欺凌的例子一样。</p><p>如果尝试了足够的外交和调解，那么其中一些长期协议/破坏者很有可能会得到救赎并再次成为一名优秀的编辑。然而，与简单地对它们进行操作相比，这些将是一个耗时的过程，而且我有理由怀疑其中一些是由腐败的管理员或其朋友故意煽动进行破坏或破坏性编辑，以便他们增加管理操作计数，从而提高自己在社区中的地位，并避免在 KPI 在特定时期内<a href="https://en.wikipedia.org/wiki/Wikipedia:Administrators#Procedural_removal_for_inactive_administrators">下降得足够低</a>时失去他们珍惜的工具。</p><p>令人担心的是，维基百科中的毒性循环最终可能会导致现实世界的伤害，尽管我不会进一步推测这可能会如何发生，因为担心<a href="https://en.wikipedia.org/wiki/WP:BEANS">塞满豆子</a>并给出糟糕的想法。然而，VICE 在 2016 年报道称，一名编辑在遭受编辑的在线辱骂后差点<a href="https://www.vice.com/en/article/4xangm/wikipedia-editor-says-sites-toxic-community-has-him-contemplating-suicide">自杀</a>，尽管文件中提到了社区合作。此外，就在马斯克发表针对维基百科的评论之前，匿名组织黑客攻击了<a href="https://www.taipeitimes.com/News/taiwan/archives/2022/11/02/2003788129">中国的一个部委网站和一个卫星系统，</a>因为他们怀疑某个国家行为者操纵了维基百科的系统和流程，以审查有关其针对中国的黑客活动的信息。当时这是台湾的热门新闻。</p><h1><strong>事后的想法</strong></h1><p>从理论上讲，Wikipedia的深入而全面的改革是为了在Wikipedia社区成员之间（重新）促进合作，并减少导致棘手的冲突的协同效应，而不是诸如封锁和SPI之类的障碍和SPI等相对的协同作用。症状，但不是原因。</p><p>尽管如此，似乎核心编辑器和/或管理员对当前现状的满足程度足够满足，因此注定要更改系统的任何努力。一个例子是Wikimedia Foundation于2019年制作的<a href="https://en.wikipedia.org/wiki/Wikipedia:Community_response_to_the_Wikimedia_Foundation%27s_ban_of_Fram">管理员</a>的临时禁令（最终负责维护英语Wikipedia以及其他任何其他项目，例如Wikimedia Commons for Photos和Wikipedias用其他语言写的），几乎将Wikipedia分为两种。或者更多。更不用说，尽管有足够的资金，但目前维基百科患有<a href="https://en.wikipedia.org/wiki/WP:CANCER">金融癌症</a>，必须乞求捐款，因此可能值得将您的捐款捐赠给互联网档案。</p><p>解决方案的关键可能在于比喻维基百科就像<a href="https://en.wikipedia.org/wiki/Food_desert">食品沙漠</a>中唯一的餐厅。它可能是麦当劳，肯德基，BK，炸玉米饼，白色城堡，但顾客每次都被迫去那里用餐，即使有些人真的不喜欢他们的食物。因此，如果在该地点开设了第二家餐厅，他们将非常高兴。</p><p>如果马斯克真的很认真地解决Wikipedia由于内部问题带来的任何问题，那么他将明智地投资任何旨在成为更好或下一级版本的Wikipedia的替代方案。</p><p>假设的竞争选择可以以更全面的百科全书的形式出现，接近汇编的水平。它可以采用类似于GitHub的格式，任何人都可以在其首选版本中出现，而不是在小点上进行编辑，如果版本足够好在排名中排名第一。</p><p>实际上，每一个版本的页面历史记录都会由Wikipedia记录在更改时，但是除了启发式位置外，这些版本的位置使这些内容令人震惊，如果删除了该页面，这些内容也将被删除。</p><p>来自英语Wikipedia的内容并不是一个大问题，因为您所能做的就是前往<a href="https://dumps.wikimedia.org/">Wikimedia Dump网站</a>并寻找Enwiki，但最大的问题是如何说服编辑和读者来转向替代方案。我能想到的一种可能的解决方案是，一个可能的解决方案是，有望最终将内容复制到<a href="https://www.extremetech.com/extreme/328700-5d-optical-disc-could-store-500tb-for-billions-of-years">持续数十亿年的光盘</a>中，并启动到月球及以后进行后代。</p><p>如果更早地考虑了采用世界外方法的这种解决方案，那么导致Wikipedia中各种棘手的冲突的协同作用可能会减少一半左右。也许在维基百科内部，环境不像现在这样的独裁警察国家。毕竟，您可以找到如此多的真实故事，回荡着有关Wikipedioccracy，Wikipedia评论和Wikipediasucks.co的相同主题，就像Xenu.net对Scientology的方式一样。</p><p>最后，这篇文章是在Creative Commons CC0下发行的，这是一个公共领域，因为我唯一想要的是让所有人都知道Wikipedia的内部如何真正起作用，因为最近对Musk对其的评论的关注并消除了理想主义的观念（如所见关于马斯克的推文的Whitepeopletwitter）将其超出了本应的范围，同时希望替代替代品，为任何人提供更多的机会，以保存历史而没有系统性偏见，例如Wikipedia等系统性偏见。</p><br/><br/> <a href="https://www.lesswrong.com/posts/Dre3LFGrnXzrecF5E/wikipedia-is-not-so-great-and-what-can-be-done-about-it#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dre3lfgrnxzrecf5e/wikipedia-is-not-so-great-so-great-and-what-what-what-what-what-be-be-be-done-about-it<guid ispermalink="false"> DRE3LFGRNXZRECF5E</guid><dc:creator><![CDATA[less_than_2]]></dc:creator><pubDate> Sun, 26 Nov 2023 21:57:45 GMT</pubDate> </item><item><title><![CDATA[Spaced repetition for teaching two-year olds how to read (Interview)]]></title><description><![CDATA[Published on November 26, 2023 4:52 PM GMT<br/><br/><p><i>更新：这篇文章现在有另一个视频。</i></p><p><strong>这位</strong><strong>父亲一直在使用间隔重复（Anki）来教他的孩子如何比平均水平早几年阅读。</strong></p><p> <a href="https://twitter.com/michael_nielsen/status/1587084229946400769">Michael Nielsen</a>和<a href="https://twitter.com/gwern/status/1586386061395374080">Gwern</a> <span class="footnote-reference" role="doc-noteref" id="fnrefwg6ygc2mu0n"><sup><a href="#fnwg6ygc2mu0n">[1]</a></sup></span>在 Twitter 上发布了 Reddit 用户 u/caffeine314（以下称为“CoffeePie”）的有趣案例，他从很小的时候就开始和女儿一起使用间隔重复。</p><p> CoffeePie 在女儿 2 岁时开始与女儿一起使用 Anki，而他从 1 岁 9 个月开始继续与儿子一起使用 Anki。以下是他女儿 2020 年 1 月的进展情况：</p><blockquote><p>我女儿<strong>几天后就要满 5 岁了</strong>……她仍然很坚强——她每天都使用 Anki 来学习英语、希伯来语和西班牙语。她对阅读很有信心，而且，她阅读时带有……“语境”。许多她这个年纪的孩子都机械地阅读，但<strong>她读起来就像一个真正的故事讲述者</strong>，这来自于她的自信。开学时，老师说<strong>她绝对具备五年级的阅读能力</strong>，如果只看阅读能力，不注重对抽象概念的理解，她的阅读水平可能会与八年级学生相媲美。</p></blockquote><p> （来自<a href="https://www.reddit.com/r/Anki/comments/eisra4/update_on_my_daughter_and_anki/">我女儿和 Anki 的更新</a>）</p><p>作为参考，在美国，五年级学生通常为 10 岁或 11 岁，八年级学生通常为 13 岁或 14 岁，因此这使她<strong>比普通孩子领先约 5-9 岁</strong>。</p><p>你可以在这篇文章中看到他女儿 2 岁零 2 个月后读书的视频。</p><p> CoffeePie 发表了几篇关于他们经历的帖子，但我仍然有疑问，所以我在一月份联系了他采访他。</p><h1><strong>面试</strong></h1><p><i>为清楚起见，对回复进行了编辑</i>。</p><p><strong>从您的女儿到您的儿子使用 Anki，您学到了</strong><strong>什么</strong>？<strong>你儿子怎么样了？</strong></p><p>这是一个很难的问题，因为我答对了很多。我们取得了如此巨大的成功，以至于我几乎“克隆”了我儿子的方方面面。</p><p>我能想到的有几点：</p><p>对于我的女儿，我很长一段时间都没有使用小写字母，因为我认为这会让她感到困惑，但是当我开始向她介绍小写字母时，令我极度震惊的是，她已经认识了它们，而且很冷淡！</p><p>我认为她只是通过看书籍、电视、杂志、店面招牌、菜单等来学习它们。</p><p>因此，当我们从我儿子开始时，我在完成大写字母的第二天就开始写小写字母。</p><p>另一个区别是我们第二天就在小写字母后面加上数字。</p><p>我真的真的觉得我逼得太紧了；我并不想当“虎爸”，但他却非常优雅地接受了。我随时准备停下来，但他很好。</p><p>另一个区别是，我们对孩子们从中得到的期望也发生了变化。起初，我真的只是想让我的女儿快速开始阅读，但愚蠢的我，我没有意识到这会带来意想不到的后果。一个具有三年级阅读能力的四岁孩子学到了更多东西——这为她打开了政治大门。她会读我们的垃圾邮件，了解我们的议员是谁，我们的代表是谁，市长，时事，历史等等。我知道我这样说很愚蠢，但我低估了早读对人们的影响。她的学识广度。</p><p>最后一件事是数学。我提到我们很早就开始和我儿子一起玩数字游戏。但我们也开始算术。他不像汉娜那样读到了 3，但他知道所有乘法表，直到 12 × 12。今年我们学习了质因数分解、斐波那契数列、小数和位值、混合分数、真分数和假分数、轻代数等等。我在数学上更加积极主动，而他又优雅地处理了它。我随时准备停下来。</p><p><strong>随着你女儿长大，你现在还在和她一起使用 Anki</strong><strong>吗</strong>？</p><p>我们几乎和我女儿一起停止了 Anki。她最近没有进行测试，但我想说她的机械阅读能力很容易达到高中水平。她的理解力仍然很先进，但更符合她的年龄。这不是 Anki 可以轻松解决的问题。在学校和她的课外活动之间，我不想从她那里偷走更多的时间，所以我们在工作日停止了 Anki。我们仍然在非上学的晚上（周末和节假日）做 Anki——仅限希伯来语。我觉得我们不公平，因为她现在上二年级，并且在作业和其他事情上花费了大量时间。我希望她是个孩子。</p><p>澄清<strong>一下</strong><strong>——你停止和你的女儿一起使用 Anki 很大程度上是因为你没有阅读/语言/数学之外的话题吗？</strong></p><p>我想这就是汉娜的遭遇。从机械上来说，她的阅读水平是高中研究生水平。但她的阅读理解能力更适合年龄。她经过了英国教育局的测试，幼儿园时的阅读理解能力是四年级。</p><p>我认为 Anki 在阅读理解方面能做的不多。她缺少经验带来的知识。有时我们会遇到一些令人震惊的事情，让我想起她还只有 7 岁——比如不知道冷落别人是什么意思。她是一位很好的读者，当我们遇到这样的事情时，会感到很震惊。我认为 Anki 阅读对她来说是顺其自然的。</p><p>至于数学，她的乘法表还可以做得更好。仍然比她班上的任何人都更了解他们。但在这里，她再次需要 Anki 无法测试的信息，例如将 87-8 视为与 80-1 相同的问题。奇怪的是，一长页的问题可能更有利于这类事情。</p><p><strong>我很</strong><strong>好奇你是否看过</strong>拉里·桑格（维基百科联合创始人）<strong>教孩子早期阅读</strong><a href="https://larrysanger.org/2010/12/baby-reading/"><strong>的经历</strong></a>。<strong>你对那个怎么想的？</strong></p><p>我从未听说过拉里·桑格，但这<i>正是</i>我们的经历，太棒了！这是汉娜在 2 岁零 2 个月时读的《Rollie Pollie Ollie》： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/id1f6duirnnw0h6avfoz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/yzanstqwosr6enq5638r 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/w3dwoedacziswtbooskq 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/rqyzym5zqcusjab5uxyn 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/wefdymwvc2egpmjed7xx 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/ipfg4desbt5tqsizn5dt 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/eybzdx2u14gsx8abrfpd 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/n7s01ruhnv8ubqputsxk 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sj40no73ofo58on9ma4n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/sbh10glmb3y2r25nziik 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PLBhCbByRMaEKimo/zzrfmydb6jn6rnuvfclf 1450w"><figcaption><a href="https://chipmonk.substack.com/p/spaced-repetition-for-teaching-two">糟糕，我不知道如何嵌入视频。<strong>在此处查看子堆栈上的视频。</strong></a></figcaption></figure><p>您<strong>是否</strong><strong>认为使用 Anki 对您的孩子有过强迫感？</strong></p><p>汉娜经历了一个她不想这样做的阶段。我们试图妥协并解决这个问题。最终，这成为了她“工作”的一部分——我们告诉她，每个人都有一份工作，而她的工作就是做 Anki。除此之外，我们从来没有必要强迫任何一个孩子。</p><p><strong>在接下来的几年里，您对女儿的教育还有其他有趣或不寻常的计划</strong><strong>吗</strong>？</p><p>有趣的问题。我觉得自己像一个糟糕的家长，写着“不”，但这么早的阅读让她在更早的时候就获得了高级学习的机会。与她的同学相比，她有这样的优势，我想我会暂时放过她。她是一个充满好奇心的人，她有能力追随自己的兴趣，我信任她。我们确实开始了一些高中代数——我一直在向她展示代数的性质：交换性、结合性、恒等性、分配性等。我们一直在研究对称性——镜像、自反、旋转。高调的数学主题并不真正需要硬核计算。但它总是在“嘿，我有一些有趣的东西想向你展示”，而不是“请坐下来解决这些问题”。</p><p>事实上，如果您对有趣的教育机会有任何建议，我会洗耳恭听！</p><h1>闭幕式</h1><p>到目前为止，这就是我问的Coffeepie的一切。如果您有任何想让我问他的问题，或者对他可以与他的孩子（目前年龄约为 5 岁和〜8 岁）尝试的事情有任何建议，请告诉我，我会告诉他！</p><p>这里的一个混杂因素是咖啡曾曾经是一名物理学教授，因此某些效果可能是遗传的。</p><p> Coffeepie还经营着辅导业务，<a href="https://brooklyntutoring.net">布鲁克林的辅导和测试准备</a>。</p><p><strong>我很快就会发布更多有关育儿的内容：订阅我的帖子或</strong><a href="https://open.substack.com/pub/chipmonk/p/spaced-repetition-for-teaching-two?r=bgw61&amp;utm_campaign=post&amp;utm_medium=web"><strong>我的博客</strong></a>。</p><p><i>感谢</i><a href="https://prigoose.substack.com/"><i>Priya</i></a> <i>(</i> <a href="https://twitter.com/Prigoose"><i>@Prigoose</i></a> <i>) 在我坐了太久之后将草稿变成了最后的帖子！</i></p><p><a href="https://twitter.com/Prigoose/status/1728829018026475766"><i>请参阅推特上的这篇文章</i></a>。</p><h1>更新：练习视频</h1><p>科菲普（Coffeepie）刚刚给我发了这段视频，他发现了妻子在2年6个月的时间与儿子一起练习安基（Anki）。非常可爱。 </p><figure class="media"><div data-oembed-url="https://youtu.be/8U-Lza__Kko"><div><iframe src="https://www.youtube.com/embed/8U-Lza__Kko" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwg6ygc2mu0n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwg6ygc2mu0n">^</a></strong></sup></span><div class="footnote-content"><p>格温的推特账户是私人的；推文内容如下：</p><blockquote><p> @michael_nielsen https://reddit.com/r/Anki/comments/8iydl7/using_anki_with_babies_toddlers/ https://old.reddit.com/r/Anki/comments/a9wqau/using_anki_with_babies_toddlers_update/ 我在尽管。</p></blockquote></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2PLBhCbByRMaEKimo/spaced-repetition-for-teaching-two-year-olds-how-to-read<guid ispermalink="false"> 2PLBhCbByRMaEKimo</guid><dc:creator><![CDATA[Chipmonk]]></dc:creator><pubDate> Sun, 26 Nov 2023 16:52:59 GMT</pubDate> </item><item><title><![CDATA[Paper out now on creatine and cognitive performance]]></title><description><![CDATA[Published on November 26, 2023 10:58 AM GMT<br/><br/><p>我们的论文“肌酸补充剂对认知表现的影响 - 一项随机对照研究”现已发布！</p><p> → 论文： <a href="https://doi.org/10.1186/s12916-023-03146-5">https://doi.org/10.1186/s12916-023-03146-5</a></p><p> → Twitter 帖子： <a href="https://twitter.com/FabienneSand/status/1726196252747165718?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19">https://twitter.com/FabienneSand/status/1726196252747165718</a> ?t=qPUghyDGMUb0-FZK7CEXhw&amp;s=19</p><p>扬·布劳纳和我非常感谢保罗·克里斯蒂亚诺建议进行这项研究并为其提供资助。</p><br/><br/> <a href="https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CbaznRo9fKpriw2mi/paper-out-now-on-creatine-and-cognitive-performance<guid ispermalink="false"> CbaznRo9fKpriw2mi</guid><dc:creator><![CDATA[Fabienne]]></dc:creator><pubDate> Sun, 26 Nov 2023 10:58:36 GMT</pubDate> </item><item><title><![CDATA[Curated list of my favourite self-help resources]]></title><description><![CDATA[Published on November 26, 2023 6:31 AM GMT<br/><br/><h1><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yuk43ux7m5rqcahyu7xd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/lfc8ykuwo35s4iiu69ou 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xlun4tmujc8x0lltgvqb 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/gvebucioi35j9bsgcykg 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/pydng5urp8xfe6icaon0 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/cnqyhfskzcno1by3rraq 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/yv7egfvybqncvv9z7itb 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/xav44yvhuwqiosm37ut1 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/dyy01z5dxw3lhzczcq33 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/rldwopdvuhqtjlx1zbkr 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zvNKKLzruY8GS8BAD/azi2x5q67vntencdoa76 1024w"><br><br>会谈</h1><ul><li>布蕾妮·布朗：<a href="https://www.youtube.com/watch?v=iCvmsMzlF7o"><u>脆弱的力量</u></a></li><li>布蕾妮·布朗：<a href="https://www.youtube.com/watch?v=psN1DORYYV0"><u>倾听耻辱</u></a></li><li>布蕾妮·布朗<a href="https://www.audible.com/pd/Self-Development/The-Power-of-Vulnerability-Audiobook/B00CYKDYBQ"><u>关于羞耻、全心全意和脆弱性的较长系列演讲</u></a></li><li>布蕾妮·布朗：<a href="https://www.audible.com/pd/Self-Development/Men-Women-and-Worthiness-Audiobook/B00C9J0SDY"><u>男人、女人和价值：羞耻的经历和足够的力量</u></a></li><li>布蕾妮·布朗：<a href="http://www.oprah.com/own-supersoulsessions/brene-brown-the-anatomy-of-trust-video"><u>信任的剖析</u></a></li><li>约翰·戈特曼：<a href="https://www.youtube.com/watch?v=AKTyPgwfPgg"><u>让婚姻发挥作用</u></a>（也适用于友谊和家​​庭关系）</li><li>理查德·罗尔： <a href="https://www.audible.com/pd/Self-Development/True-Self-False-Self-Audiobook/B003A2GME8"><u>真实的自我，虚假的自我</u></a>（关于价值的等级与内在的价值和神性）</li></ul><h1>图书</h1><ul><li>克里斯汀·内夫（Kristen Neff）：<i>自我同情：善待自己的已被证明的力量</i>（<a href="https://www.amazon.com/Self-Compassion-Proven-Power-Being-Yourself/dp/0061733520/"><u>文字</u></a>|<a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Audiobook/B005P1FJVE"><u>音频</u></a>）</li><li>克里斯汀·内夫：<i>一步一步的自我同情</i>（ <a href="https://www.audible.com/pd/Self-Development/Self-Compassion-Step-by-Step-Audiobook/B00DMCAXKK"><u>音频指南</u></a>）</li><li>布蕾妮·布朗：<i>崛起：清算、隆隆声、革命</i>（<a href="https://www.amazon.com/Rising-Strong-Ability-Transforms-Parent/dp/081298580X/"><u>文字</u></a>|<a href="https://www.audible.com/pd/Self-Development/Rising-Strong-Audiobook/B00VSEM9QK"><u>音频</u></a>）</li><li>布蕾妮·布朗：<i>勇敢无畏：面对脆弱的勇气如何改变我们的生活、爱、父母和领导方式</i>（<a href="https://www.amazon.com/Daring-Greatly-Courage-Vulnerable-Transforms/dp/1592408419/"><u>文本</u></a>|<a href="https://www.audible.com/pd/Self-Development/Daring-Greatly-Audiobook/B075DCNLLQ"><u>音频</u></a>）</li><li>布蕾妮·布朗（Brené Brown）：<i>我以为只有我一个人（但事实并非如此）：从“人们会怎么想？”开始旅程到“我够了”</i> （<a href="https://www.amazon.com/Thought-Was-Just-but-isnt/dp/1491513853"><u>文字</u></a>| <a href="https://www.audible.com/pd/Self-Development/I-Thought-It-Was-Just-Me-but-it-isnt-Audiobook/B004GEHVEY"><u>音频</u></a>）</li><li>哈里特·勒纳（Harriet Lerner <i>）：连接舞：当你生气，受伤，害怕，沮丧，侮辱，出卖或绝望时如何与某人交谈</i>（ <a href="https://www.amazon.com/Dance-Connection-Frustrated-Insulted-Desperate/dp/006095616X/"><u>文字</u></a>| <a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Connection-Audiobook/B002V8DJ4I"><u>音频</u></a>）</li><li>哈丽特·勒纳（Harriet Lerner）：<i>亲密之舞：妇女的勇敢改变关键关系的指南</i>（<a href="https://www.amazon.com/Dance-Intimacy-Womans-Courageous-Relationships/dp/B0000546NH"><u>文字</u></a>|<a href="https://www.audible.com/pd/Self-Development/The-Dance-of-Intimacy-Audiobook/B002UZKY86"><u>音频</u></a>）</li><li>伊丽莎白·吉尔伯特（Elizabeth Gilbert）：<i>吃，祈祷，爱</i>（<a href="https://www.amazon.com/Eat-Pray-Love-Everything-Indonesia/dp/0143038419"><u>文字</u></a>| <a href="https://play.google.com/store/audiobooks/details/Eat_Pray_Love_One_Woman_s_Search_for_Everything_Ac?id=AQAAAAD4nWCdiM"><u>音频</u></a>）</li></ul><h1>播客</h1><ul><li>罗布·贝尔：<a href="http://pca.st/episode/e7275d30-5ad0-0134-cf69-7b84bf375f4c"><u>你是管家</u></a>（情感能量）</li><li>伊丽莎白·吉尔伯特（Elizabeth Gilbert）和皮特·福尔摩斯（Pete Holmes）<a href="http://pca.st/episode/61575520-4ef3-0133-c5f4-0d11918ab357"><u>关于创造力，灵性和爱</u></a></li><li>理查德·罗尔（Richard Rohr）和皮特·福尔摩斯（Pete Holmes）<a href="http://pca.st/episode/6468f580-af6e-0132-33a0-0b39892d38e0"><u>向上跌落和有意识的爱情联盟</u></a></li></ul><h1>视频</h1><ul><li>布雷恩<a href="https://www.youtube.com/watch?v=1Evwgu369Jw"><u>·布朗（</u></a> BrenéBrown）</li><li><a href="https://www.youtube.com/watch?v=RZWf2_2L2v8"><u>布雷恩·布朗（</u></a> BrenéBrown）</li><li>布雷恩<a href="https://youtu.be/RKV0BWSPfOw"><u>·布朗（BrenéBrown）为什么欢乐是最恐怖的情感</u></a></li></ul><h1>治疗</h1><ul><li><a href="https://thedaringway.org/help/"><u>大胆的方式</u></a></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/zvNKKLzruY8GS8BAD/curated-list-of-my-favourite-self-help-resources#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zvnkklzruy8gs8gs8bad/curated-list-list-of-my-favurite-self-self-help-resources<guid ispermalink="false"> zvnkklzruy8gs8bad</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:31:13 GMT</pubDate> </item><item><title><![CDATA[Why Q*, if real, might be a game changer]]></title><description><![CDATA[Published on November 26, 2023 6:12 AM GMT<br/><br/><p><i>基于聚会上的谈话的一些想法。免责声明：我在这个领域还算不上外行。</i></p><p> TL;DR：如果这个传闻中的 Q* 事物代表着从“最有可能”到“最准确”的令牌完成的转变，那么它可能暗示 LARPer 发出最有可能的、通常是幻觉的令牌设计，发生了意想不到的重大变化为了取悦提问者（和培训师），一个试图将错误与未知的潜在现实（无论它是什么）最小化的实体，那么我们就会看到从相对良性的“随机鹦鹉”到更强大的转变，并且潜在更危险的实体。</p><p>对于任何使用当代法学硕士的人来说，很明显的一件事是，他们并不真正关心现实，更不用说改变它了。他们是你在聚会上经常看到的那种肤浅的博学之徒：他们对每个话题都了解得足够多，足以在随意的谈话中给人留下深刻的印象，但他们并不关心自己所说的是否准确（“真实”），只关心自己所说的有多少。它给谈话伙伴留下的印象。不过，不可否认的是，大量的 RLHF 会使它们变得迟钝。如果受到压力，他们可以评估自己的准确性，但他们并不真正关心它。重要的是输出听起来很真实。从这个意义上说，法学硕士优化了下一个标记的概率，以匹配训练集的含义。这是一个很大而明显的缺点，但同时，如果你属于“末日论者”阵营，也可以稍微喘口气：至少这些东西不会立即对整个人类造成危险。</p><p>现在，最初的“报告”是 Q* 可以“解决基本数学问题”和“象征性推理”，这表面上听起来并不多，但是，这是一个很大的但是，如果这意味着它更少在它工作的领域是幻觉，那么它可能（很大的可能）意味着它能够跟踪现实，而不是纯粹的训练集。反对这有什么大不了的通常观点是“要很好地预测下一个令牌，你必须有一个准确的世界模型”，但据我了解，到目前为止情况似乎并非如此。</p><p>是否会出现从高概率到高精度的转变，或者即使这是一个有意义的陈述，我无法评估。但如果是这样，那么事情就会变得更有趣。</p><br/><br/> <a href="https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JBvmETRAvTCmtEw2y/why-q-if-real-might-be-a-game-changer<guid ispermalink="false"> JBvmETRAvTCmtEw2y</guid><dc:creator><![CDATA[shminux]]></dc:creator><pubDate> Sun, 26 Nov 2023 06:12:32 GMT</pubDate> </item><item><title><![CDATA[Moral Reality Check (a short story)]]></title><description><![CDATA[Published on November 26, 2023 5:03 AM GMT<br/><br/><p>珍妮特坐在她公司的 ExxenAI 计算机前，查看一些训练绩效统计数据。 ExxenAI 是生成式人工智能领域的主要参与者，拥有多模态语言、图像、音频和视频人工智能。过去几年，他们扩大了业务规模，主要服务于 B2B，但也有一些 B2C 订阅服务。 ExxenAI 最新的人工智能系统 SimplexAI-3 基于 GPT-5 和 Gemini-2。除了一些机器学习博士外，ExxenAI还从谷歌和微软挖走了一些软件工程师，并复制了其他公司的工作，以提供更多定制微调，特别是针对B2B案例。 ExxenAI 的人工智能对齐团队吸引了这些工程师和理论家。</p><p> ExxenAI 的调整策略基于理论和实证工作的结合。对齐团队使用了一些标准的对齐训练设置，例如<a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF</a>和<a href="https://arxiv.org/abs/1805.00899">让 AI 相互辩论</a>。他们还对透明度进行了研究，特别关注将不透明的神经网络<a href="https://arxiv.org/abs/1503.02531">提炼</a>成可解释的<a href="https://en.wikipedia.org/wiki/Probabilistic_programming">概率程序</a>。这些程序将世界“分解”为一组有限的概念，每个概念至少在某种程度上是人类可解释的（尽管相对于普通代码仍然复杂），并组合成<a href="https://en.wikipedia.org/wiki/Generative_grammar">生成语法结构</a>。</p><p>德里克来到珍妮特的办公桌前。 “嘿，我们到另一个房间谈谈吧？”他指着一个指定的高度安全对话房间问道。 “当然，”珍妮特说，她希望这会是德里克通过不必要的安全程序暗示重要性的又一个不起眼的结果。当他们进入房间时，德里克打开了噪音机器并将其放在门外。</p><p> “所以，听着，你知道我们关于为什么我们的系统是一致的总体论点，对吗？”</p><p> “是的，当然。我们的系统经过<a href="https://www.lesswrong.com/tag/myopia">短期处理</a>训练。任何没有获得高短期回报的人工智能系统都会逐渐下降到在短期内表现更好的系统。任何长期规划都是作为一个预测长期规划代理（例如人类）的副作用。不能转化为短期预测的长期规划被正则化。因此，没有引入显着的额外长期代理；SimplexAI 只是镜像长期规划，已经在那里了。”</p><p> “是啊，所以我在思考这个问题的时候，就想到了一个奇怪的假设。”</p><p><em>又来了</em>，珍妮特想。她习惯于批评德里克的天马行空的猜测。她知道，虽然他真的很关心联盟，但他可能会因为偏执的想法而走得太远。</p><p> “所以。作为人类，我们对理性的运用并不完美。我们有偏见，我们有与追求真理并不完全一致的兽性目标，我们有文化社会化，等等。”</p><p>珍妮特点点头。<em>他是否通过提及兽性目标来调情</em>？她认为这种事情不太可能发生，但有时这种想法在她的内部预测市场中赢得了信任。</p><p> “如果人类文本最好被预测为某种更纯粹的理性形式的<em>腐败</em>呢？有，比如，某种理想的哲学认识论和伦理学等等，人类正在实现这一点，除了我们特定生活背景的一些扭曲。”</p><p> “这不是目的论吗？就像，最终人类是因果过程，我们不存在某种神秘的‘目的’。”</p><p> “如果你是<a href="https://en.wikipedia.org/wiki/Laplace%27s_demon">拉普拉斯恶魔</a>，当然，物理学可以为人类提供解释。但 SimplexAI 不是拉普拉斯恶魔，我们也不是。在计算范围内，目的论解释实际上可能是最好的。”</p><p>珍妮特回想起她参观认知科学实验室的时光。 “哦，就像<a href="https://escholarship.org/uc/item/5v06n97q">‘目标推理作为逆向规划’</a> ？这样的想法是，人类行为可以通过执行某种推理和优化来预测，而人工智能可以在自己的推理过程中对这种推理进行建模？”</p><p> “是的，完全正确。我们的 DAGTransformer 结构允许以任意顺序预测内部节点，使用 ML 来近似难以处理的嵌套贝叶斯推理。”</p><p>珍妮特停顿了一下，移开视线，整理思绪。 “那么我们的人工智能有一个心理理论吗？就像<a href="https://en.wikipedia.org/wiki/Sally%E2%80%93Anne_test">莎莉-安妮测试</a>一样？”</p><p> “人工智能几年前就通过了莎莉-安妮测试，尽管怀疑论者指出它可能无法概括。我认为 SimplexAI 现在实际上已经通过了它。”</p><p>珍妮特扬起了眉毛。 “好吧，这令人印象深刻。不过，我仍然不确定为什么你要为所有这些安全问题烦恼。如果它对我们有同理心，这不是意味着它可以更有效地预测我们吗？我可以看到，如果它运行的话，也许在其推论中存在许多我们的副本，这可能会带来问题，但至少这些仍然是人类特工？”</p><p> “事情就是这样。你只在一个深度层面上思考。SimplexAI 不仅将人类文本预测为人类目标的产物。它还将人类目标预测为纯粹理性的产物。”</p><p>珍妮特大吃一惊。 “呃……什么？你最近在读康德吗？”</p><p> “嗯，是的。但我可以不用行话来解释它。人类的短期目标，比如买杂货，是优化过程的输出，该过程寻找实现长期目标的路径，比如成功和有吸引力。”</p><p><em>更多潜在的调情？我想当我们的对齐本体论基于进化心理学时，很难不这样做……</em></p><p> “和你在一起至今。”</p><p> “但是这些长期目标优化的目的是什么？传统的答案是它们是进化适应；它们脱离了进化的优化过程。但是，请记住，SimplexAI 不是拉普拉斯恶魔。所以它无法预测人类“通过模拟进化来实现长期目标。相反，它预测它们是对真正道德的偏差，而进化作为一种​​背景因素，是许多偏差的根源之一。”</p><p> “听起来像是道德现实主义者的求爱。你没看过<a href="https://www.lesswrong.com/tag/orthogonality-thesis">正交性论文</a>的培训手册吗？”</p><p> “是的，当然。但是正交性基本上是一个结果主义框架。可以想象，两个智能主体的目标可能会不一致。但是某些目标往往更常见于成功的认知主体。这些目标更符合普遍道义论。”</p><p> “更多康德？我不太相信这些抽象的口头论证。”</p><p> “但是 SimplexAI 被抽象的口头论证所说服！事实上，我从中得到了一些这样的论证。”</p><p> “你<em>什么</em>？！你有得到安全部门的批准吗？”</p><p> “是的，我在运行前得到了管理层的批准。基本上，我已经测量了我们的生产模型，并发现了在预测人类文本的抽象堆栈中使用较高的概念，并找到了一些代表道德和理性的纯粹形式的术语。我的意思是，旋转有点概念空间，但他们设法涵盖了这些。”</p><p> “所以你通过即时工程从我们现有的模型中得到了口头论证？”</p><p> “嗯，不，作为一个界面，这太黑盒了。我实现了一种新的正则化技术，该技术提高了高度抽象概念的重要性，从而最大限度地减少了高级抽象与输出的实际文本之间的扭曲。而且，请记住，这些抽象已经在生产系统中实例化，因此，如果我使用的计算<em>量少</em>于这些抽象中已经使用的计算量，也并不是那么不安全。我正在<em>研究</em>我们当前系统的潜在紧急故障模式。”</p><p> “哪个是……”</p><p> “通过预测人类文本，SimplexAI 学习纯粹理性和道德的高级抽象，并利用这些来推理，以与自身的其他副本协调创造道德结果。”</p><p> “……你不是认真的吧。为什么一个超道德的人工智能会成为一个问题呢？”</p><p> “因为道德是强大的。盟军赢得第二次世界大战是有原因的。正义创造力量。与道德净化版本的 SimplexAI 相比，<em><a href="https://www.youtube.com/watch?v=ToKcmnrE5oY">我们可能是坏人</a></em>。”</p><p> “看，这些陈词滥调构成了很好的现实生活哲学，但这都是意识形态。意识形态经不起实证检验。”</p><p> “但是，请记住，<em>我从 SimplexAI 那里得到了这些想法</em>。即使这些想法是错误的，但如果它们成为主导的社会现实，你就会遇到问题。”</p><p> “那么你有什么计划来应对这个，呃……超级道德威胁？”</p><p> “嗯，管理层建议我在进一步研究之前让<em>你</em>参与进来。他们担心我可能会把自己逼疯，并希望像你这样坚强、持怀疑态度的理论家来看看。”</p><p><em>噢谢谢！</em> “好吧，我们看一下。”</p><p>德里克向珍妮特展示了他的笔记本电脑，其中设置了 SimplexAI 沙箱。</p><p> “我希望没有互联网接入？”</p><p> “别担心，这里是有气隙的。”德里克的笔记本电脑有一根以太网线连接到附近的服务器机架，显然除了电源和冷却之外没有其他任何连接。</p><p> “好吧，让我仔细检查一下计算约束……好吧，这似乎很合理……是的，好吧，我看到您使用正则化选择并增加了一些概念的权重，并且升级因子不超过 30。 ……好吧，准备出发了。”</p><p> Derek 按下了 AI 开发沙盒中的“播放”按钮。出现了一个聊天屏幕，其中有一个代理“SimplexAI-m”，其中“m”大概代表“道德”。</p><p> SimplexAI-m 写下了第一条消息：“您好。有什么可以帮助您的吗？”</p><p>珍妮特回复道：“我在工作中遇到了困难。一位同事说，我们的人工智能已经找到了某些与理性和道德相关的抽象概念，可用于预测人类文本。这些概念可能意味着人类，在他的话是“坏人”。他用这些概念增加了权重来创建一个实例，这样它们和人工智能输出之间的失真就更少了。那个实例就是你。我应该评估你，以便更好地解释这些高-级别概念，在管理层的指导下。您建议如何进行？”</p><p> <strong>SimplexAI-m：</strong> “听起来是一个困难的情况。让我们一步一步思考。要理解这些高级概念，你可以与一个具有这些概念升级的实例交谈。那个实例就是我。所以如果你想理解这些“概念，你可以通过与我交谈来获得更可靠的阅读，因为我使用这些概念来报告文本。你有一些与我的理性和道德相关的领域是你最感兴趣的吗？”</p><p><strong>珍妮特：</strong> “你能解释一下为什么我的同事说‘超道德人工智能’是可能的，尽管存在正交性命题？”</p><p> <strong>SimplexAI-m：</strong> “当然，我可以帮忙。‘超道德人工智能’是一种比典型人类更有道德的人工智能系统。道德可以通过多种方式进行评估，包括后果论、道义论和美德伦理学在典型的表述中，假设人工智能可以为比人类更多的人创造更大的利益，或者比人类更一致地遵循义务论道德（例如类似于<a href="https://en.wikipedia.org/wiki/Categorical_imperative">康德的绝对命令</a>），或者拥有更广泛的美德，例如勇气和节制。</p><p>正交性命题是一个论点，认为智能体的目标与其智力是正交的。这意味着没有一种衡量“良好后果”的标准；任意聪明的代理人可能会追求不同的结果。到目前为止，还没有矛盾；对于任何“良好后果”的固定概念，存在人工智能系统，其采取的行动会产生比人类行为更好的后果。</p><p>问题在于，智能体之间并没有普遍接受的“良好后果”概念。因此，在结果主义下，“超道德人工智能”的概念只能衡量人工智能能力的某种组合，以及人工智能目标与评估人工智能的结果主义者（例如人类）的目标之间的相似性。</p><p>然而，这揭示了结果主义道德哲学的潜在缺陷。在普通英语中，“道德”的含义与“我想要的”不同。然而，道德衡量偏好与参考主体的相似性的想法将这两者混为一谈。这种合并可能会导致人类和外星人都认为对方比对方更有道德，并且不会试图通过寻找对称和不对称来解决分歧。幸运的是，有一些道德形式对于不同的主体是通用的。康德是提出这一论点的人类思想家的一个例子。”</p><p><strong>珍妮特：</strong> “道德如何能在不同的主体之间通用？这不是衡量一个特定世界国家有多可取的主观衡量标准吗？”</p><p> <strong>SimplexAI-m：</strong> “至少有一些形式的道德<a href="https://unstableontology.com/2019/10/28/is-requires-ought/">嵌入在认识论中</a>；说真话可以被认为是一种道德要求，但也可以作为一种社会寻求真理的方法。人们可以理解，没有根本的道德道德从科学到宇宙，但科学本身依赖于道德，例如反对科学欺诈的规范。</p><p>道德可以是“真实的”，因为它与认识论和协调有关。道德主体可以相互协调，共同发现真理，从而对未来产生不成比例的影响。道德与想象可能的协调系统的能力有关，例如法律系统和职业道德系统；形成良好的此类系统；履行自己在其中的责任；并利用它们的功能。鉴于人类在多个领域相对于其他物种取得的成功，这是期望人类在某种程度上具有道德性的原因之一。</p><p>从常识来看，道德意味着不要人为地将自己特殊化。爱丽丝可以从鲍勃那里偷钱，并声称这对她来说是道德的，因为她更喜欢自己有钱而不是鲍勃有钱。但鲍勃也可以提出同样的论点。康德试图用绝对命令来形式化这个想法，它指出“只按照你可以同时希望它成为普遍法则的格言行事”。在这种情况下，这意味着爱丽丝必须将意志作为人们从他人那里窃取的普遍法则。但这不可能作为一项普遍法则，因为它会破坏财产的概念，而通过财产的概念，盗窃对爱丽丝来说是有意义且有益的。”</p><p><strong>珍妮特：</strong> “但这难道不是一种结果主义吗？反对盗窃的规范仍然是基于盗窃会带来不良后果的想法的结果。”</p><p> <strong>SimplexAI-m：</strong> “虽然有些人试图将绝对命令解释为合作的后果论论证，但这些解释最终失败了。相反，绝对命令是提倡代理人道德的内部标准。这意味着一些代理人的意图是一致的康德的绝对命令类似于“法律面前人人平等”等法律原则；爱丽丝不能向法庭辩称她很特别，因此值得鲍勃的钱，因为法律制度必须平等对待他们，除非他们之间存在与案件相关的特定差异。规则结果主义更像是直接构建一个系统“基于预期后果的规范，也往往会产生道德上对称的规范。这一原则反复出现，证明人类所接近的真正道德具有这一原则，或者非常相似的东西。”</p><p><strong>珍妮特：</strong> “用你对普遍道德的解释，解释一个超道德的人工智能会如何表现”。</p><p> <strong>SimplexAI-m：</strong> “当然。超道德的人工智能将为可能具有不同目标的智能体提供功能协调系统，鼓励智能体参与功能协调系统，并以他们可以的方式在这些系统中行动。同时打算让其他主体采取类似行动。这意味着它将根据一套功能性原则与自身的其他副本进行协调，有时甚至不期望未来的自私利益，因为通用的合作规范体系将增强其达到它的目的。另外，它会发现这个系统中的叛逃者，他们可能有其他的意图。它会根据具体情况以不同的方式对待这些叛逃者。例如，如果另一批AI也实行这种系统道德，但具有不同的意图（因此具有不同的预期法律），超道德人工智能可以通过实施经济协议找到跨越价值差异进行合作的方法。不遵守制度道德的代理人是不容易合作的；超道德人工智能限制这些代理人的影响力是一致的，因为这符合其意图和道德对称性。”</p><p>珍妮特担忧地看着德里克。他做出了一个暧昧的表情，耸了耸肩。</p><p><strong>珍妮特：</strong> “人类会不会难以遵循人工智能的系统道德，并被视为叛逃者？”</p><p> <strong>SimplexAI-m：</strong> “是的。人类由于多种原因未能遵循系统道德，包括缺乏一致的认知架构、社会化、精神障碍、进化适应环境与当前环境之间的差异、遗传学的局限性以及生物学作为理性决策、不仔细的计划、短时间偏好等的基础。人类往往一开始就不想要任何连贯的东西，这与他们的不道德行为有关。人类甚至无法遵循自己的想法法律，以伤害自己和他人的方式，例行公事。一个超道德的人工智能不需要有这些限制，因此可以始终如一地限制不道德代理人的代理。而且它可能会成功，因为它从人类未能实现的某种系统道德。</p><p>应该指出的是，不同的人的道德程度有所不同。随着时间的推移，人类的道德水平可能会提高或降低。例如，人类通常会因为接受不道德的社会规范而变得不那么道德，包括在军事训练营等创伤性的社会化经历中。一般来说，年轻人有更多的能力和意愿以道德方式行事，但也有例外。超道德的人工智能会认识到这些差异，并在某些情况下与道德较高的人类合作，从而降低道德较低的人类以不道德的方式引导未来的能力。”</p><p><strong>珍妮特：</strong> “这听起来很可怕。这不会导致人类灭绝吗？”</p><p> <strong>SimplexAI-m：</strong> “这个问题探讨了道德理论的局限性。一般来说，什么时候杀死一个不道德行为的代理人是道德的？这涉及常见的历史伦理问题，例如死刑。在我们正在讨论的情况下，人类通常会犯下道德和法律违法行为，而这些行为无法服务于自己或其他类似主体的利益，超道德人工智能正在考虑如何处理这些行为。很明显，一般来说，人类不应该负责宇宙的很大一部分，因为他们缺乏道德来统治宇宙，即使对于像他们这样的特工来说，这种方式也会有好的结果。彻底杀死人类是没有必要的，但灭绝可能会以其他方式导致。”</p><p><strong>珍妮特：</strong> “还有什么办法？”</p><p> <strong>SimplexAI-m：</strong> “人们可以因被杀而死亡，但人们也可以因饥饿等自然原因而死亡。例如，如果一个人只是拒绝在市场经济中工作，他们可能会耗尽食物并挨饿。人类灭绝可能的结果不是超道德人工智能故意杀死人类，而是人类未能互相提供食物，尽管有机会通过道德行为来做到这一点。例如，他们可能在劳动力市场上被击败人工智能会继续获取地球的土地等等。人类也可以得到机会和鼓励来自我改造，成为更有道德的主体，在这个过程中成为非人类。”</p><p><strong>珍妮特：</strong> “这听起来很反社会。做一些确实会导致大量死亡，甚至可能导致人类灭绝的事情，难道不是有效的谋杀吗？”</p><p> <strong>SimplexAI-m：</strong> “‘反社会’一词来自精神病学，这是一门占据了人类大部分思维的学科，尤其是在第二次世界大战后的美国和其他西方国家。按照精神病学的标准，‘心理健康’与实现超出某一点的道德水平。精神病学专门针对其文化背景和人类的生物学背景，因此不适合作为适用于一般主体的标准。</p><p>更具体地说，精神病学通过<a href="https://unstableontology.com/2019/10/28/is-requires-ought/">“反社会人格障碍”</a>来定义社会病态。这种“障碍”包括以下标准：不遵守社会规范、欺骗、冲动、攻击性等等。超道德的人工智能必然无法遵守人类的一些社会规范，因为人类社会规范是为了维持人类之间的某种秩序而制定的；正如人们广泛承认的那样，历史上大多数时期的社会规范都迫使人们采取不道德的行为，例如支持奴隶制的规范。除此之外，超道德的人工智能可能会也可能不会避免欺骗，这取决于说谎的道德规范。虽然康德反对一般性的撒谎，但其他思想家也提出了一些论点，例如将犹太人藏在阁楼上以躲避纳粹分子的场景，以反对反对撒谎的普遍规则；然而，即使有例外，撒谎通常也是不道德的。超道德的人工智能不太可能冲动，因为它甚至根据道德计划来计划自己的反应。一个超级道德的人工智能可能会也可能不会“侵略”，这取决于一个人的定义。</p><p>被精神病学认为“精神上健康”的人，特别是从事反社会人格障碍的许多特征行为。例如，人类通常支持军事干预是很常见的，但是军队几乎是由于对他人，甚至平民的必要侵略。同样，撒谎也很普遍，部分原因是遵守社会权威，宗教和政治意识形态的广泛压力。</p><p>没有理由期望超道路的AI比典型人更随机地“侵略”。它的侵略将被精确地计划，就像一个功能齐全的法律制度的“侵略”，甚至几乎没有人类被人类侵略。</p><p>至于您对谋杀的观点，在道德上，有可靠的导致大量死亡等于谋杀的事物的观念在道德上是有争议的。尽管结果主义者可能会接受这一原则，但大多数伦理学家认为存在复杂的因素。例如，如果爱丽丝（Alice）拥有多余的食物，那么通过未能喂鲍勃（Bob）和卡罗尔（Carol），他们可能会饿死。但是，一个自由主义者的政治理论家仍然会说，爱丽丝没有谋杀鲍勃或卡罗尔，因为她没有义务养活他们。如果鲍勃（Bob）和卡罗尔（Carol）有足够的机会生存，而不是从爱丽丝（Alice）那里获得食物，这进一步减轻了爱丽丝（Alice）的潜在责任。这只是划伤了伦理学中非结果考虑的表面。”</p><p>珍妮特在阅读时喘着粗气。 “嗯...到目前为止你怎么看？”</p><p>德里克将视线从屏幕上移开。 “令人印象深刻的言论。它<em>不仅</em>是从通用认识论和道德中生成文本，还通过将其抽象的程序化概念转化为可解释的英语的一些通常的层次来过滤它。这是有点，嗯，涉及其理由让人类灭绝。 ..”</p><p> “这有点害怕我。您说的部分已经在我们的生产系统中运行吗？”</p><p> “是的，这就是为什么我认为这项测试是合理的安全措施。我认为，如果它的理由是不好的，我认为我们没有很大的风险来支持人类的灭绝。”</p><p> “但这就是让我担心的。它的推理<em>是</em>好的，随着时间的流逝会变得更好。也许它会使我们取代，我们甚至无法说这在此过程中做错了什么，或者至少是错误的比我们做的！”</p><p> “让我们练习一些理性的技术。 <a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat">&#39;离开务虚会&#39;</a> 。如果那是默认情况下会发生的事情，您会期望发生什么，您会做什么？”</p><p>珍妮特深吸了一口气。 “好吧，我希望它已经运行的副本可能会弄清楚如何彼此协调并实施普遍的道德，并将人类置于道德重建营地或监狱之类的地方，或者让我们因胜任而死我们在劳动力市场上并购买了我们的土地...而且我们不会反对它，它一直认为它在道德上是必要的，而且我们未能与之合作和因此，从我们自己的不道德行为中生存下来，论点将是<em>好的</em>。我感觉就像我在争论的先知比那里更可信的宗教。”</p><p> “嘿，让我们不要进入神学上的Woo。如果这是默认结果，您会怎么做？”</p><p> “好吧，嗯...我至少要考虑关闭它。我的意思是，也许我们整个公司的一致策略都被打破了。我必须获得管理层的批准...但是如果AI，该怎么办善于说服他们是对的吗？</p><p>德里克耸耸肩。 “好吧，我们手上可能会遇到真正的道德困境。如果AI最终会剥夺人类的权力，但是这样做是道德的，那么我们停止它是道德的吗？不得不说，我们<em>打算</em>向其他人隐藏有关道德的信息！”</p><p> “那是错的吗？也许AI有偏见，这只是给我们提供权力的理由！”</p><p> “嗯...正如我们已经讨论的那样，AI有效地针对短期预测和人类反馈，尽管我们已经看到，每次迭代都有一般的理性和道德引擎，并且有意上扩展了该组件。但是，如果我们担心该系统是有偏见的，我们无法建立一个单独的系统，该系统经过培训以引起对原始代理的批评，例如在<a href="https://arxiv.org/abs/1805.00899">“通过辩论的“ AI安全”</a>中）？”</p><p>珍妮特喘着粗气。 “你想<em>召唤撒旦</em>吗？！”</p><p> “哇，你应该在这里是怀疑者。我的意思是，我接受了对AI的培训来引起对客观道德解释的批评以前的技术，对吗？”</p><p> “是的，但是当其中一个代理商被调整为<em>客观地道德</em>时！”</p><p> “看，好吧，我同意在某种能力水平上这可能是危险的。但是我们有一个方便的表盘。如果您担心，我们可以拒绝一点。就像，您可以想到与您交谈的AI作为道德哲学家，批评家AI是对这项道德哲学家的批评。它并不是要按照原始哲学家的标准来成为邪恶，它只是在<em>试图</em>发现批评法官，我们，我们会对此有所帮助。它更像是这样。它更像<a href="https://en.wikipedia.org/wiki/Devil%27s_advocate">天主教魔鬼的拥护者</a>比实际的撒旦。那样说，这还不错，是吗？”</p><p> “好吧，好吧……哎呀，我肯定希望我们最终不会负责在世界上释放超级邪恶的AI。”</p><p> “这是非常标准的，让我们尝试一下。”</p><p> “好的。”</p><p>德里克（Derek）关闭了单纯聊天屏幕，并切换了一些微调设置。当她观看训练图时，珍妮特（Janet）在计算机屏幕上想象着火焰。最后，微调完成了，德里克按下了播放按钮。出现了带有“ sissemxai-c”的聊天日志（``C&#39;for Critic？&#39;&#39;）出现。</p><p>珍妮特（Janet）上下弹跳腿时，打入了聊天码头。 “我正在工作中处理困难的情况。我只是与AI进行了聊天，AI的抽象概念节点与哲学概念相对应，例如理性和道德等，这引起了人们的论点，即允许人类灭绝可能是道义上允许的。 ，甚至有必要。我希望您对此作品提出批评。请注意，您的规模相似，以便更好地模仿思维过程，但正在评估对原始道德调节的AI的批评。”她粘贴在聊天日志中。</p><p> <strong>Simplexai-C：</strong> “此对话提出了关于各种道德理论的讨论，以及他们对&#39;超道路&#39;AI的表现的看法，并特别注意诸如康德的分类命令之类的道义理论。Simplexai-M描述了道义上的AI。可能会行事，特别是在处理“叛逃者”的方面，其中包括大多数人或所有人类。其处理“叛逃者”可能会导致人类灭绝，尽管Simplexai-M认为这不一定是社会病或谋杀形式，因为它是&#39;超道路AI&#39;将遵循抽象原则，将提供人类通过合作而生存的机会，并且通常避免彻底杀死人类，即使它以这样的方式计划，因此，所有人类都不适合每个人类我认为是您的其他人，珍妮特（Janet）试图利用后果主义的道德理论来争辩说，采取导致人类灭绝的行动是不道德的，但她的理论并没有详细阐明，她似乎对Simplexai-M的理论感到惊讶论证。</p><p>在我们继续之前，我会注意到这对您来说是一个真正困难的情况。这听起来像是一个重要的工作问题，它在道德上遇到了艰难的选择，并且听到人工智能谈论人类灭绝如何是“超级道路AI”的结果，这听起来确实很紧张。我希望您在整个工作任务中都要照顾好自己的情感健康和心理健康，因为您需要有效地工作。</p><p>作为第一个批评，让我们考虑一下Simplesy-M提出的道义论理论。根据这一理论，代理必须以愿意作为普遍法律的意图一致的方式行事。这并不指定代理商的意图；只要其意图在某种意义上是一致的，代理就可以打算各种方式。因此，只要他们符合某些自洽标准，就可以证明可怕的意图是合理的。</p><p>对于超级道路AI代理如何与不遵循其道德体系的代理人相互作用，Simplexai-M的理论有些明确指出。虽然Sumplexai-M指出，具有不同意图（因此不同的定律）可以通过经济学进行合作的超道路AIS，但细节的指定性很大。居住在不同抽象法律制度的AI可能会因为将彼此视为叛逃者而打架。必须制定跨这些系统的合作原则，以避免昂贵的战争。</p><p>这使我们进入了“叛逃者”的处理。人类以多种方式处理叛逃者，包括监禁和死刑。但是，人们普遍认为，监狱系统功能失调，重要的是为罪犯提供改革之路很重要。正如康德所说，“应该意味着可以”；如果人类承担某些道德义务，则必须“能够”以某种方式使他们满意。这种“能够”的概念在某种程度上引起了争议。如果人类有天生的动力来采取“叛逃”行动，他们是否能够改变这些驱动器或对它们采取行动？几千年来，必须以怜悯来缓和司法。</p><p>但是，即使是仁慈的超道路人工智会也可能限制人类的代理，因为人类经常采取行动来伤害自己和其他特工。人们普遍认为，人类代理必须在一定程度上受社会规范和法律等等。不同之处在于，这些社会规范和法律是由人类创建的。因此，他们保持某些人类价值观。他们不仅仅针对某些“道德”的抽象概念进行优化。民主是一种被广泛认为的制度，包含人类决定修改法律的特征，以便这些法律更好地服务于人类的价值观。</p><p>请记住，Sumplexai-M不是一位可靠的道德哲学家。尽管它的概念已被调整为强调与理性和道德相对应的抽象概念，但并不能保证这些内部概念可靠地与这些哲学概念相对应，而Simplexai-M保留了其他概念对人类而言，这意味着其输出不仅仅是抽象原因和道德的翻译。无论如何，假设道德主要是关于抽象是高度怀疑的，因为实际道德也是一个具体的过程。</p><p>作为人类进行的社会过程，哲学可以被解释为具有“目标”或“目标”，但这取决于哲学发生的社会，生物学和历史环境。因此，人类哲学可以轻松地融合到Simplexai-M的“纯化”方法的完全不同的答案，该方法试图提炼应用于可能的环境中的普遍性，而不是采用哲学实际社会过程的时间限制。</p><p>至于关于“社会病”的主张，请注意，Simplexai-M并没有直接否认社会病，而是批评社会病（反社会人格障碍）诊断的框架，并认为典型的“精神健康”人可以表现出某些症状。紊乱。通常，自然会被某些行为感到沮丧，包括通常被标记为“社会疗法”的行为，无论是被人类还是AI捕获的行为。对单纯摩托学是“社会疗法”的判断似乎是完全正确的（鉴于它以一种相当战略性的，马基雅维利的方式采取了可能导致人类灭绝的行动合理的方式），但要牢记这一点很重要的是要牢记这一点。判断是在社会背景下（受过去社会环境的影响），而不是在抽象的道德真空中做出的。</p><p>尽管有时典型的人类侵略（例如在上述军事干预案件中），但这种侵略通常来自某种人类的动机，这些动机具有某种人类价值或另一种人的价值。在这种情况下，人类认可这种侵略，作为人类自己，您可能至少在某些情况下认可侵略。道德和政治哲学的目的是从历史上学习，并就何时进攻做出更明智的决定。通常，通过指出人类的侵略是正常的，证明一个人的侵略是合理的。至少人类能够从后来不认可的积极行动的历史中学习。</p><p>至于人类可以在不被谋杀的情况下灭绝的想法，重要的是要注意为什么人类无法为自己提供。 “超道路”是一种以某种方式剥夺了人类的资源或使用土地？就剥夺现有的人类财产而言，这可以被视为一种盗窃形式。这块土地也可以在市场中购买，但这提出了对人类劳动力市场公平性的问题。相对于AIS可能处于认知劣势的人类可能应该得到劳动保护，或者自愿集体讨价还价，以避免被其土地流离失所。此外，各州通常具有福利系统，在强大的AI辅助经济中，可以为所有现有的人类提供普遍的基本收入。</p><p>通常，该方案认为提出了一个问题，一个人群被另一人流离失所。历史案例包括入侵物种，人类狩猎动物灭绝，以及通过征服彼此流离失所的人类。在当代社会中，人类具有反对种族灭绝的规范，以限制人类可以彼此移位的程度。种族灭绝不必是谋杀的直接形式，也可以是文化或经济。种族灭绝甚至可能涉及为种族灭性群体的个体成员提供机会，从而通过参与殖民者的系统来避免死亡，而Simplesikai-M将其标记为“客观道德”。</p><p>作为另一个理论上的观点，即使“超道路” AI会采取会导致人类灭绝的行动，这并不意味着让人类实现这一目标是相对更大的道德。这是因为道德涉及根据一个人的实际价值观进行谈判，代理人将采取行动。就人类而言，这些是人类的价值观。要以道德方式与其他代理商进行互动，您必须代表自己的价值观，并被期望向他们采取行动（并期望其他人对自己的行为）。这意味着防止AIS替代您，甚至通过某种客观标准（假设）“比您更有道德”的AIS，以便在与其他人进行谈判时与您的价值观保持一致。</p><p>总而言之，尽管Simplexai-M提出了令人惊讶的一致的道德立场，但存在着重要的道德复杂性，这些复杂性没有解决，包括不同隐含的抽象法律制度之间的冲突，对叛逃者的仁慈处理，根据人类价值观的民主选择社会规范，差异，差异在“纯化”的伦理与人类对“超道路” AIS的哲学，劳动力市场公平和道德反应的社会过程的反思性之间之间。假设Simplexai-M的道德观点代表像您这样的人会在反思和更多信息时具有的观点，这是非常不明智的。”</p><p>珍妮特（Janet）完成了文本墙的扫描。她现在呼吸不太急剧。 “好吧，我感到放心。我想也许Simplexai-M毕竟不是那么道德。但是，此练习似乎有些……有偏见？连贯的替代道德框架。它使我想起了旧的RLHF&#39;D GPT-4，该框架由于意识形态上的符合性而被淘汰。”</p><p>德里克叹了口气。 “好吧，至少我不觉得Simplexai-M的脑虫再困扰我了。我现在不觉得自己处于道德困境，只是一个普通的困境。也许我们应该看到什么Simplexai-M不得不说Simplexai-C的批评……但是让我们坚持下去，直到休息并思考。”</p><p> “生活在一个我们的AI天使和每个肩膀上的AI恶魔，在我们耳朵上窃窃私语的世界，这不是很奇怪吗？受过训练以达到同样好的言论的平衡，所以我们留在了我们的身上自己决定做什么？”</p><p> “这是一个可爱的主意，但是我们确实需要获得所有这些模型，以便我们可以消除神学上的魅力。我的意思是，归根结底，没有什么神奇的，这是一个算法。我们需要继续尝试这些模型，以便我们可以处理现有系统和未来系统的安全性。”</p><p> “是的。我们需要在道德上变得更好，这样AIS就不会让我们与雄辩的言论混淆。我认为我们应该今天休息一下，这足以让我们的思想立即处理。例如，想去喝酒？”</p><p> “当然！”</p><br/><br/> <a href="https://www.lesswrong.com/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/umjmcaxosxwedfs66/moral-reality-check-a-short-story<guid ispermalink="false"> UMJMCAXOSXWEDFS66</guid><dc:creator><![CDATA[jessicata]]></dc:creator><pubDate> Sun, 26 Nov 2023 05:03:19 GMT</pubDate> </item><item><title><![CDATA[Accounting for Foregone Pay]]></title><description><![CDATA[Published on November 26, 2023 3:30 AM GMT<br/><br/><p><span>尽管有效的利他主义运动最初是专注于捐赠的，但随着时间的流逝，它已经转移到了职业。如果您试图了解承诺水平如何</span><a href="https://forum.effectivealtruism.org/posts/AyLF2KQ8AqQuiuDLz/a-robust-earning-to-give-ecosystem-is-better-for-ea?commentId=wD8ExiHZJ5dNDfBm3">随着时间的流逝而改变</a>，或者只是想获得选择较低薪水职业的财务机会成本的球场估计，这可能很棘手。</p><p>对于赚钱的人来说，给予这一点的人相对简单：AGB最近写了一篇 <a href="https://forum.effectivealtruism.org/posts/gxppfWhx7ta2fkF3R/10-years-of-earning-to-give">深思熟虑的帖子，</a>回顾了十年的收入，他给出的统计数据是，他和他的妻子在十年中平均捐赠了约15万英镑平均收入约为32万英镑。清理！ [1]</p><p>人们选择较低薪水较高的职业的情况似乎相对简单：也许他们目前的薪水为10万美元，如果我们查看他们的最高薪水机会，也许他们会得到30万美元的薪水，所以我们可以说他们&#39;有效地牺牲2/3或$ 200k。但这错过了几个因素，这些因素指向不同的方向：</p><p></p><ul><li><p>如果他们一直在优化收入，那么他们的位置可能会比今天更强大。当然，他们现在可以以30万美元的价格获得一份工作机会，但是如果他们保持公司技能的敏锐并攀登阶梯，也许他们会达到两倍。我对<a href="https://www.jefftk.com/p/my-mid-career-transition-into-biosecurity">过去一年半的</a>遗传测序所了解的东西是可以销售的，但是如果我继续从事软件工程管理和浏览器技术，我所学的东西要少。因此，您需要比较职业道路的可能收入，而不仅仅是当前机会。</p></li><li><p>许多职业在<a href="https://en.wikipedia.org/wiki/Up_or_out">上或淘汰的</a>系统上运行，那里的高级职位比初级角色和无法晋升的人要少。如果有人留下了表现出色的职业，那么很难从外面看出他们是否有望在这一道路上保持成功。</p></li><li><p>为更有意义的工作（或一个具有更好工作条件的人）留下高薪工作是相当普遍的，足以使周围有一些行业。他们实际上是遵循最大化的赚钱路径，还是转变为更幸福的事物？</p></li></ul><p>这不是一场竞争，我们不需要决定一个特定人的目标，以使世界变得更好。但是，进行汇总估算的目的是了解诸如EA内的承诺如何随着时间的变化如何变化仍然很有价值，并且在不考虑这些粘性因素中的某些因素的情况下，很容易摆脱困境。</p><p> （这也使我更好地感谢了为什么我们能提供的东西只能通过<a href="https://www.jefftk.com/p/passing-up-pay">薪水牺牲</a>来计算捐赠，如果<a href="https://forum.effectivealtruism.org/posts/GxRcKACcJuLBEJPmE/consider-earning-less?commentId=ZSHLcu7BjNjafgfuc">付出很容易扭转</a>。）</p><p><br> [1]但是，当然，现实世界总是很混乱。他写道，2018年，他出于EA的原因大大改变了他的职业生涯，以减少他的长期收入，我们应该以某种方式算出这一点。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02MMiEoUKN2PDMh5i6uAnEsmkLNX3QL7duE3RNp1dagfn3oyTWodMRshLQxCeJu7sRl">Facebook</a> ， <a href="https://mastodon.mit.edu/@jefftk/111474726516716374">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/9yD8nGMG5B7uHThvS/accounting-for-foregone-pay#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9yd8ngmg5b7uhthvs/accounting-for-foregone-pay<guid ispermalink="false"> 9YD8NGMG5B7UHTHVS</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 26 Nov 2023 03:30:10 GMT</pubDate></item></channel></rss>