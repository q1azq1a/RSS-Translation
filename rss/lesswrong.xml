<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 18 日星期一 18:15:02 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[[Valence series] 5. “Valence Disorders” in Mental Health & Personality]]></title><description><![CDATA[Published on December 18, 2023 3:26 PM GMT<br/><br/><h1> 5.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p>在效价系列的最后一篇文章中，我将讨论效价如何揭示心理健康和人格中的三种现象：抑郁、躁狂和自恋型人格障碍。</p><ul><li><strong>第 5.2 节</strong>给出了一些背景：我们期望算法级心理成分（如“效价”）与可观察到的心理健康综合症和人格障碍之间存在什么样的<i>先验</i>关系？我认为，我们应该预期与效价的系统变化相对应的显着症状簇，但我们不应该期望这种分析能够解释真实患者中同时出现的<i>所有</i>症状。</li><li> <strong>5.3 节</strong>讨论了如果效价具有强烈的一般负偏差（即，如果几乎所有的想法都是负效价）会发生什么。我认为这个结果与临床抑郁症非常匹配。我将特别讨论如果没有不寻常的努力和意志力就无法自愿移动<i>和思考</i>。</li><li> <strong>5.4 节</strong>讨论了相反的情况：如果效价具有强烈的一般<i>正向</i>偏差，即如果几乎所有的想法都是正效价，会发生什么？我认为预期的结果与狂热非常匹配。</li><li> <strong>5.5 节</strong>讨论了如果效价被系统地极端化会发生什么——即，如果思想可以具有非常正的效价，或者非常负的效价，但很少介于两者之间。我认为结果是一系列似乎与自恋型人格障碍非常相似的症状。</li><li><strong>第 5.6 节</strong>将总结这篇文章和系列，包括简要讨论它与我作为通用人工智能安全和一致性研究员的工作描述之间的关系。</li></ul><h1> 5.2 背景：我们期望<i>先验地</i>发现什么？</h1><p>我们可以想到以下间接路径从“根本原因”到心理观察和人格特质： </p><figure class="image image_resized" style="width:79.06%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/xmdlvcgqlgwdkemjdqjf" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/kdkzsafkniennpkr22df 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/d7msofe35n93gg3nuypw 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/amgm6h8nz0cnpcc91mnd 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/osyitmhtyn118ezdhdix 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/qstxevaps1su1fjppsaf 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/q3rfprq1ntubf8bf1k61 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/nlfpgphql7789m64b809 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/we56yxy4fg76dmvyx6w7 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/bun65drseubtun6mnet9 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/rw91guuy8o3wbsww2brk 1389w"><figcaption> （不要仔细观察红色箭头 - 我只是将它们随机放置，以说明每一层都可以影响下面的层。）如粗体文本和粗箭头所示，我们应该期望找到显着的症状簇，这些症状这些现象往往同时发生，因为它们源自相同的近端原因：大脑中价信号的系统变化。但我们<i>也不</i>应该惊讶地发现其他与算法无关的症状的大杂烩，这些症状经常与这些症状群一起出现。</figcaption></figure><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction"><u>帖子 1</u></a>中所述，价是大脑中最重要算法之一的最重要成分之一。所以我们应该期待：</p><ul><li>一些可能的根本原因可能会对化合价产生重大的系统影响。 （但它们也可能会产生其他后果，并且不同根本原因的细节会有所不同。）</li><li>鉴于效价在大脑中的中心地位，<i>如果</i>效价发生重大的系统性变化，<i>那么</i>它应该对心理和行为产生许多明显的下游影响。</li></ul><p>作为结果：</p><ul><li>我们应该期望找到可以用价信号发生的事情来优雅地解释<i>的症状/行为簇</i></li><li>我们<i>还</i>应该期望找到在实践中常见但<i>无法</i>用效价解释的其他症状/行为。相反，它们是相同根本原因的不同后果，并且在“算法级别”上可能没有任何关系。</li></ul><p>例如，多巴胺集中参与价信号，同时，在大脑的一个不起眼的角落，多巴胺<i>也</i>集中参与控制催乳激素释放的小专门电路。在<a href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis"><u>算法层面</u></a>，我坚信，这两个函数彼此没有任何关系。但它们都恰好涉及多巴胺，因此它们可以在某些人身上相互影响，因此会出现有点罕见的“烦躁性喷乳反射”，即在哺乳期间喷乳时会出现大量强烈的负面情绪。</p><p>这个例子旨在说明纯粹在算法层面上对心理学进行理论化的危险。不要误会我的意思——算法水平很棒！在那里可以找到很多见解。希望这篇文章能成为一个例子。但我们不应该指望在那里找到<i>所有的</i>见解。心理学中的某些事情只能在其他层面上解释，包括较低的（生物化学）和较高的（文化）。</p><h1> 5.3 如果效价有很强的负偏向（即几乎所有的想法都是负效价），它应该导致一组可疑地接近临床抑郁症的症状</h1><figure class="image image_resized" style="width:57.68%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/bb6pz6txfrz8mhuyq0cx" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/rde3a8otlff8vzs90axg 133w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/oeymerp8idar0hxmecmj 213w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/e5ovpccwnknzmr0u8gbv 293w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/alevu0ivqvuri63n4ghg 373w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/n7vrg3pwulx9bjssrwhl 453w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/pawxdhan3ccqorxadge3 533w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/nhdumr5dgmzprpmbw78d 613w"><figcaption>每个人都有各种各样的想法，其效价也不同。我认为，在抑郁症中，负价有很强的倾向性。因此，对于您想到的几乎每一个想法（例如“我要起床”），您的大脑都会立即将该想法评估为坏主意，将其扔掉，并重新滚动以产生新的想法（参见<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ） 。对于<i>异常</i>吸引人/激励人心的想法，比如“我现在要挠痒痒的虫子咬的地方”，我敢打赌，即使是非常沮丧、卧床不起的人也会最终执行这个计划。</figcaption></figure><h2> 5.3.1 自主运动和注意力控制只有付出很大的努力才能实现</h2><p>回到<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ，价是一个控制信号。当价为负时，无论你在想什么，都会被抛弃，而大脑会去寻找新的想法。当价态为正时，无论你在想什么，都会留下来。如果该想法是时间序列的一部分（例如，您正在唱歌），则该序列将继续。如果这个想法需要电机输出（例如“我现在要站起来”），那么这些电机输出实际上就会发生。</p><p>如果每个想法的效价都被拉为负数，那么两个最直接的后果是：</p><ul><li>自主运动控制只能通过巨大的努力/意志力才能实现。</li><li>自愿注意力控制（又名“自愿思维”，又名“系统2”）只有付出巨大的努力/意志力才能发生。</li></ul><p>如果您对此感到困惑，我将详细说明一些可能令人困惑的部分：</p><p> <strong>“自愿注意力控制”：</strong>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>中所讨论的，我坚信运动控制和注意力控制在很多方面都是“同一类东西”。两者都具有受大脑“主要”强化学习系统（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_6_Fine_print__Throughout_this_series__I_m_only_talking_about_the_brain_s__main__reinforcement_learning_system"><u>§1.5.6</u></a> ）控制的“自愿”输出通道，并且都具有可由其他大脑系统触发的“非自愿”机制，特别是大脑中的先天反应。脑干。有关自愿和非自愿运动控制与注意力控制的示例，请参阅<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_5_An_exception__I_think_anxious___obsessive__brainstorming__is_driven_by_involuntary_attention_rather_than_by_valence"><u>第 3.3.5</u></a>节中的表格。</p><p> <strong>“……又名‘自愿思维’，又名‘系统 2’……”：</strong>我衷心赞同 Kaj Sotala 在 2019 年发表的一篇博文： <a href="https://www.lesswrong.com/posts/HbXXd2givHBBLxr3d/system-2-as-working-memory-augmented-system-1-reasoning"><u>系统 2 作为工作记忆增强的系统 1 推理</u></a>。我将其总结为这样的想法：刻意的“系统 2”推理需要按顺序思考很多想法，并通过在工作记忆中保存特定的事物来将它们相互联系起来。自愿注意力控制是使整个过程发挥作用的总机，我们在生活经历的过程中通过强化学习学会熟练地操作该总机。</p><p> <strong><u>“......只有付出巨大的努力/意志力才能发生”：</u></strong>在上图中两个高斯的图中，我展示了红色高斯的最右尾部<i>刚刚</i>挤入正价区域。我将尝试通过一个例子来说明这在实践中意味着什么。假设您目前的动机是躺在床上而不是起床，但我们也可以说这种动机是自我张力障碍的（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_6_Valence_of_metacognitive___self_reflective_thoughts__ego_syntonic_vs__dystonic_ideas__professed__values___etc_"><u>第 2.6 节</u></a>），即您<i>想要</i>起床。然后，积极的思考/头脑风暴（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>）就会开始发挥作用，幸运的话，你将能够以最积极的方式构思出一个想法，即“我要起床”——你会想起起床的所有重大后果和关联，并且只要可能，您将避免关注起床的所有不吸引人的方面。幸运的是，这个集思广益过程的结果将是你的“思想发生器”（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ）精心设计了一个思想 θ，<i>它既</i>涉及立即起床的计划<i>，又</i>被你的大脑评估为具有净正价——可能<i>只是几乎</i>没有净积极。通过形成那个想法 θ，事实上，你就会真正起床。现在，我在本段中写的所有内容都是机械的第三人称描述，但<i>想想</i>“从内部”这个相同的过程会是什么样子：我声称这正是我们正在谈论的那种事情，当我们随口说“我可以起床，但必须付出很大的努力/意志力”。</p><h2> 5.3.2 快感缺乏和其他症状</h2><p>继续，抑郁症的另一个著名方面是<strong>快感缺失</strong>（无法感受到快乐）。我不能立即确定抑郁症的快感缺失是负价的上游还是下游，或者是同一根本原因的不同结果，还是其他原因。但我<i>绝对</i>认为快感缺失与负价密切相关，其原因在<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_2_Valence__as_I_m_using_the_term__is_different_from__hedonic_valence____pleasantness"><u>§1.5.2</u></a>中有所暗示。</p><p>那么临床抑郁症的其他方面又如何呢？据我所知，至少其中大部分都是全球化合价负面偏差的后果。但在某些情况下，这个故事有点间接和推测。我希望我所说的足以激起人们对我的以价为中心的抑郁假说的兴趣，所以我将把这个故事留在这里，尽管我很乐意在评论部分进行更多讨论。</p><h2> 5.3.3 根本原因</h2><p>与第 5.2 节一样，到目前为止我所说的都不是关于根本原因的主张。但是，根本原因<i>又如何呢</i>？我想它们有很多种。例如，以下是一个虚构的强迫症 (OCD) 导致抑郁症的例子（根据<a href="https://www.lesswrong.com/posts/jqTeghCJ2anMHPPjG/book-review-feeling-great-by-david-burns#Speculative_neuroscience_tangent__What_causes_depression_"><u>我的旧帖子</u></a>编辑）：</p><ul><li>如果我当前的想法涉及立即再次洗手的计划，那么它就是负价，因为它提醒我这样一个事实：强迫症正在毁掉我的生活和人际关系。</li><li>如果我当前的想法<i>不</i>涉及立即再次洗手的计划，那么它就是负价，因为我会生病并死亡。</li><li>我不能只思考一些与洗手、疾病和强迫症完全无关的事情，因为与我的焦虑相关的“不自觉的注意力”对思想产生了限制（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_5_An_exception__I_think_anxious___obsessive__brainstorming__is_driven_by_involuntary_attention_rather_than_by_valence"><u>§3.3.5</u></a> ）</li></ul><p>也许你在想：好吧，但这只是把问题倒退了一层：强迫症的根本原因是什么？但我没有一个很好的答案。</p><p>另外，这只是一个虚构的例子；即使它是有效的，我想它也是抑郁症的众多原因之一，而且我没有什么特别的见解可以提供。</p><p>如果你想知道的话，我对治疗也没有特别的了解。如果你患有抑郁症，那么我真的很抱歉；也许可以尝试<a href="https://lorienpsych.com/2021/06/05/depression/"><u>这个通用资源页面</u></a>。</p><h1> 5.4 如果效价有很强的正偏差（即几乎每个想法都是正效价），它应该导致一组可疑地接近躁狂的症状</h1><p>在这里，显而易见的结果是，<i>无论你脑海中突然出现什么计划，似乎都是一个非常非常棒的计划，因此你实际上会去执行它</i>。因此，我们会得到诸如<strong>冲动、糟糕的判断力、不切实际的乐观主义和精力充沛等</strong>后果。</p><p>躁狂症的另一个主要症状是<strong>精神病</strong>。但我认为精神病在算法上基本上与效价<i>无关</i>。相反，我认为精神病在<i>生化上与效价</i>有关，因为两者都与多巴胺系统有关。我有一篇博客文章，其中包含一些（推测性）细节：<a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2"><u>精神病模型，采取 2</u></a> 。</p><p>好吧，这就是我对精神病的<i>看法</i>。为什么我<i>不</i>相信精神病是正价的直接后果？有几个原因（但并不是我不确定所有这些细节）：</p><ul><li>精神病可能在缺乏异常正价的情况下发生——尤其是精神分裂症。 （甚至还有“精神抑郁症”这样的东西，尽管它不太常见。）据我所知，精神分裂症的精神病症状与躁狂精神病的精神病症状并没有太大的不同，尽管显然我们期望它以不同的方式呈现在<i>某种程度上</i>，因为精神病是在非常不同的并发症状背景下发生的。</li><li>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>第 3.3 节</u></a>中所讨论的，我们的感官知觉通常受到我们的感官输入的限制。如果我想真诚地相信我现在正在水肺潜水，无论我的动机有多么强烈，我都做不到。因此，由于感觉输入与效价无关，因此效价偏差无法解释躁狂精神病中发生的幻视和幻听、参照妄想等。 （根据<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_1_Attention_control_and_motor_control_provide_loopholes_through_which_desires_can_manipulate_beliefs"><u>§3.3.1</u></a> ，注意力控制和运动控制对边缘感知有影响，但我认为这不足以解释这些现象。）</li><li>我不认为幻觉、参考妄想等的<i>内容</i>与我们有动机看到和相信的内容完全匹配，即使在考虑到<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_4_Warning__the__motivation__part_of__motivated_reasoning___thinking___etc___is_not_always_what_it_seems"><u>第 3.3.4</u></a>节中动机并不总是显而易见的警告之后也是如此。</li><li>抛开精神病性妄想的<i>起源</i>不谈，也许有人会说它们的<i>持续存在</i>与确认偏差有关，而确认偏差又与效价有关（ <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>§3.3</u></a> ）。但我也不相信这个故事，因为确认偏差与<i>正</i>价并不是特别相关。确认偏差的一个重要部分是“改变主意的想法”必须是<i>负价</i>。事实上，我认为躁狂症并不意味着<i>普遍</i>不愿意改变主意。恰恰相反，在我读过的报告中，人们谈论一个新的想法如何会突然出现在他们的脑海中，这看起来很棒，他们就跟着它走，忘记了他们之前的想法。因此，在躁狂症中，精神病性妄想是持续存在的，但我认为，几乎所有其他类型的思想、计划和信念都很<i>少有</i>持续性。因此，我认为精神病性妄想的持续存在不能用效价的普遍正向偏差来解释。</li></ul><h1> 5.5 如果效价是“极端化的”（即，几乎每个想法要么是非常积极的效价，要么是非常消极的效价，但很少介于两者之间），它应该会导致一系列可疑地接近自恋型人格障碍（NPD）的症状</h1><figure class="image image_resized" style="width:43.22%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/u9lcpi5qncmluu25pj3l" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/ltum57adflamcpo8scmm 142w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/ydhz2wwgmrgj4ew01jhu 222w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/xhaodftvz4wsj9bbjwut 302w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/epi8pg7c6q8rjoc6ywhq 382w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/qqi3tvlm2xkt06fnzevw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/txj4wigyjLNbcoZ9o/uefpdfi1l7mm7ptrdsig 542w"><figcaption>注意：我也可以将紫色曲线绘制为更宽的高斯曲线。</figcaption></figure><p> NPD是DSM-V中列出的四种“B类人格障碍”之一；其他是边缘性人格障碍（BPD）、表演性人格障碍（HPD）和反社会人格障碍（ASPD），又名精神病，又名社会病。</p><p>与你想象的相反，NPD 与“自恋”的日常含义并没有<i>特别的</i>关系。事实上，有一个“自恋人格清单”调查，但事实证明，<a href="https://doi.org/10.1080/00223891.2012.732636"><u>自恋型人格障碍患者在调查中得到的分数与对照组相同</u></a>（！！）。这个问题似乎与自尊有关。 “自恋者”，这个词在日常语言中使用，是指认为自己很特别、很伟大的人——顾名思义，他们有很高的自尊心。而 NPD 患者不必认为自己真的很特别、很棒。但如果他们<i>不</i>这么认为，那么他们就会感到<i>很</i>糟糕。 （DSM-V 强调“患有这种障碍的个体有一种夸大的自我重要感”，但也指出“自尊的脆弱性使得患有自恋型人格障碍的个体对批评或失败的‘伤害’非常敏感”。 ）</p><p>我不太确定 NPD 诊断是否“在其关节处雕刻自然”，并且我对 NPD 具有仅表面相关的亚型持非常开放的态度。 （我实际上认为反社会人格障碍就是这样，即它至少有两种仅表面相关的亚型。 <span class="footnote-reference" role="doc-noteref" id="fnrefd6muy0yvvbl"><sup><a href="#fnd6muy0yvvbl">[1]</a></sup></span> ）所以这里的讨论可能只涉及 NPD 的一个子集。这里的讨论可能在某种程度上也适用于 BPD 和 HPD，尽管我不太确定细节。 <span class="footnote-reference" role="doc-noteref" id="fnrefwsq0ynfh3df"><sup><a href="#fnwsq0ynfh3df">[2]</a></sup></span></p><p>现在让我们考虑“价态极端化”的假设。如果几乎每个想法要么非常正价，要么非常负价，但很少介于两者之间，会发生什么？除其他外，我们可能预计会产生以下下游后果：</p><ul><li><i>独立于我们对世界的感受而谈论或思考世界时存在异常困难：</i>正如<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data"><u>第 3.4 节</u></a>中所讨论的，我们的大脑将价视为显着的感觉数据，从而将其纳入我们的概念、类别和单词中。如果价信号总体上异常强烈，那么它们可能也会在信仰、思维和交流中发挥异常核心的作用。例如，会有一种异常强烈的心理力量相信，如果两个事物在概念上“结合在一起”，那么它们一定具有相同的化合价。</li><li><i>异常强烈的光环效应、影响启发式和“分裂”：</i>这与上面的要点密切相关——再次参见<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data">§3.4</a> 。行话注释：“分裂”是指患有 NPD 的人在某些时期将他们认识的人视为完美的圣人，而在其他时期则将同一个人视为不可救药的可怕者。 （分裂也是 BPD 的症状。）</li><li><i>异常强烈的社会地位驱动力：</i>我在<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status">上一篇文章</a>中指出，效价和社会地位之间存在密切联系。好吧，如果你所有的价信号都异常高或低，那么社会地位信号可能也会异常强。更具体地说，假设我患有 NPD，并且我正在对人们要么很棒要么很糟糕的地方进行“分裂”。进一步假设我在心理上模拟（通过移情模拟）<i>其他人</i>对<i>我</i>的看法。我的大脑会隐含地认为<i>他们</i>也在分裂，即<i>他们</i>认为<i>我</i>要么很棒，要么很糟糕，这反过来又分别让人感到极度激励或厌恶，这要归功于我的社会地位驱动力。 <span class="footnote-reference" role="doc-noteref" id="fnrefa7ma5yid2r4"><sup><a href="#fna7ma5yid2r4">[3]</a></sup></span></li></ul><p>据我所知，这组症状（以及我省略的更多症状）与 NPD 非常匹配。我认为这与已故艾玛·博尔哈尼安 (Emma Borhanian)<a href="https://voidgoddess.org/ziz/narcissism/"><u>这篇发人深省的文章</u></a><i>尤其</i>产生共鸣。 （事实上​​，当这部分的假设第一次出现在我的脑海中时，我正在读那篇文章。但我的理论与艾玛的不同。）</p><p>还有两件事要快：</p><p><strong>根本原因？</strong>正如前面几节所述，如果“价极端化”是 NPD 的直接原因，您可能仍然想知道什么<strong>&nbsp;</strong>根本原因导致“价态极端化”。我的回答是：我不知道，抱歉。</p><p> <strong>NPD 的“对立面”是什么？</strong>值得深思的：如果躁狂症和抑郁症对应于价信号的大小相等且相反的扭曲，那么 NPD 的相反情况是什么，即价信号保持接近中性的情况，很少变得非常积极<i>或</i>非常积极消极的？我不知道，也许它没有临床标签。有一件事是：我猜它与 <a href="https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms"><u>“高度解耦”</u></a> （而不是“情境化”）的思维方式有关。 <span class="footnote-reference" role="doc-noteref" id="fnreff2x5x4xs7ht"><sup><a href="#fnf2x5x4xs7ht">[4]</a></sup></span></p><h1> 5.6 结论</h1><h2>5.6.1 本文的结论</h2><p>我要重申，我距离心理健康或人格障碍方面的专家还很远，这篇文章相当具有推测性。幸运的是，我在现实世界中缺乏抑郁症、躁狂症或 NPD 方面的经验。相反，我试图将我读过的东西拼凑起来。希望这里<i>至少</i>有一些值得深思的地方。像往常一样，如果您想进一步讨论这个问题，请联系（在评论部分或<a href="mailto:steven.byrnes@gmail.com"><u>电子邮件</u></a>中）！</p><h2> 5.6.2 整个系列的结论</h2><p>感谢您坚持到最后！我希望我能让您相信效价确实是日常生活中极其重要的一部分，思考 26,000 个单词的效价是阐明和具体化各种可能令人困惑的现象的好方法。</p><p>我开始写这个系列是因为我最近有两个与效价相关的“啊哈”时刻（ <a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status">第 4 篇文章</a>中的社会地位问题和第 5.5 节中的自恋型人格障碍问题），并且想写一篇关于它们的短文，以及“效价” ”是一个方便的钩子，可以将它们联系在一起，让我可以同时写下两者。但那篇短文变成了一篇长文，然后是整个系列，因为我不断发现，我对价的思考越多，我发现的现象就越多，它们完美地卡在了一起！</p><h2> 5.6.3 本系列与我作为通用人工智能安全和一致性研究员的工作描述有何关系？</h2><p>正如我的老读者所知，我的长期工作目标是研究<a href="https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8"><u>未来可能的类脑人工智能（AGI）的一致性和安全性</u></a>。长期以来，我一直对自恋型人格障碍和社会地位驱动（以及其他许多事情）感兴趣，因为两者似乎都可能揭示人类社会本能如何运作，而这又与类脑 AGI 安全性相关，原因<a href="https://www.lesswrong.com/posts/qusBXzCpxijTudvBB/my-agi-safety-research-2022-in-review-and-plans#2__Second_half_of_2022__1_3___My_main_research_project"><u>如下</u></a>简要总结。通过理解动机，价还与 AGI 安全有更直接的联系——请参阅我基于价的<a href="https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>“平庸对齐计划”</u></a> 。</p><p>不幸的是，我不能说写这个系列给了我关于编程未来安全和有益的 AGI 的新的具体想法，超出了我在开始之前已经知道的内容。但我认为我得到了一些对未来有用的心理框架。特别是，我认为<a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_4_Valence_impacts_beliefs_by_acting_as_salient_sense_data"><u>§3.4</u></a>帮助我更清楚地思考我的<a href="https://www.lesswrong.com/posts/Hi7zurzkCog336EC2/plan-for-mediocre-alignment-of-brain-like-model-based-rl-agi"><u>“平庸调整计划”</u></a><i>到底</i>发生了什么。 （碰巧的是，更新是朝着悲观的方向发展，尽管不是很强烈。我可能会在某个时候在单独的帖子中写到这一点。）</p><p>我还觉得我现在已经“迈进了大门”，了解先天状态驱动如何在人脑中发挥作用，这对我来说非常令人兴奋。显然，我不希望我们的 AGI 具有与生俱来的地位驱动力（参见我在<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status#4_5_Innate_status_drive">第 4.5 节</a>中放入的 Padme meme），但我<i>确实</i>认为我们可能希望我们的 AGI 具有同情心。不幸的是，截至撰写本文时，“天生的同情心驱动力”对我来说仍然相当神秘，但我认为同情心驱动力可能与地位驱动力存在结构重叠，在特定意义上，我希望两者都集中依赖于短暂的同理心模拟（更多<a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human#13_5_Another_key_ingredient__I_think____Little_glimpses_of_empathy_"><u>此处</u></a>讨论）。因此，希望这种理解先天状态驱动力的“迈进大门”最终将在安全和有益的通用人工智能方面取得有意义的进展，即使它还需要几步。明确地说：</p><ul><li>下一步可能看起来像是我将<a href="https://www.lesswrong.com/posts/dntWCwc8svTtbi3M7/valence-series-4-valence-and-social-status#4_5_Innate_status_drive">第 4.5</a>节充实到人类先天状态驱动理论中，其细节水平与<a href="https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter"><u>我的笑帖</u></a>类似，即一直到映射到特定假设的神经解剖学连接和逻辑的特定伪代码。</li><li>然后，如果运气好的话，下一步可能看起来像是对同情心上游的任何先天驱动力的某种类似的假设。</li></ul><p>这在我 2024 年要尝试的事情清单中名列前茅！但这可能需要很长时间，并且/或者我可能会陷入困境。看看进展如何。</p><p>与地位驱动相反，我现在对 NPD 和其他人格障碍的兴趣比我提出 §5.5 想法之前要<i>少</i>得多，相应地，我将人格障碍在我的紧急研究优先事项列表中的位置降低了很多。 （我还有很多东西想要了解它们！唉，一天的时间就这么多。）打个比方：如果有人试图了解汽车发动机工作的详细机制，那它的用处不大让他们了解当轮胎漏气时出了什么问题，即使漏气的轮胎会阻止发动机完成其通常完成的任务（即使汽车快速前进）。出于同样的原因，我目前的猜测是，进一步研究人格障碍不会为人类社会本能背后的具体机制提供太多启发。需要明确的是，我不认为这种猜测是显而易见的<i>先验</i>，而且它仍然可能是错误的。</p><p>好吧，再次感谢您的阅读！再次强调，如果您想讨论价、本系列或其他任何内容，请联系（在评论部分或<a href="mailto:steven.byrnes@gmail.com"><u>通过电子邮件</u></a>）。</p><p><i>感谢 Seth Herd、Aysja Johnson、Justis Mills、Charlie Steiner、Adele Lopez 和 Garrett Baker 对早期草稿提出的批评意见。感谢 tailcall 提供与本文相关的一些有用的讨论和参考资料。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnd6muy0yvvbl"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd6muy0yvvbl">^</a></strong></sup></span><div class="footnote-content"><p>这已经偏离主题了，但我目前认为，某些反社会人格障碍病例涉及整体性低唤醒水平（请参阅<a href="https://www.lesswrong.com/posts/pfoZSkZ389gnz5nZm/the-intense-world-theory-of-autism#Bonus___Dim_world_theory_of_psychopathy___"><u>此处</u></a>），而其他病例则涉及异常快速的愤怒。从根本原因层面来看，这些差异很大——如果有的话，可能是反相关的。但它们的症状/表现有一些表面上的重叠，因此它们在临床实践中被混为一谈。 （我对反馈非常感兴趣——这个热门观点对你来说是真的还是假的？）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwsq0ynfh3df"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwsq0ynfh3df">^</a></strong></sup></span><div class="footnote-content"><p>我目前的模糊印象（例如基于<a href="https://lorienpsych.com/2021/01/16/borderline/"><u>此</u></a>）是BPD往往涉及各种“强烈情绪”，因此极端化价可能会偶然发生。而我目前猜测 NPD 更集中于这个价故事。我对HPD一无所知。我对这一切都感到很不确定，热忱欢迎大家提出想法和讨论。</p></div></li><li class="footnote-item" role="doc-endnote" id="fna7ma5yid2r4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa7ma5yid2r4">^</a></strong></sup></span><div class="footnote-content"><p>小字：也许我不应该说 NPD 人本身具有“异常强烈的社会地位驱动力”；相反，他们的大脑中有<i>正常的</i>先天社会地位驱动力，但进入该电路的<i>输入</i>异常强烈，因此该电路发送异常强烈的输出。）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf2x5x4xs7ht"> <span class="footnote-back-link"><sup><strong><a href="#fnreff2x5x4xs7ht">^</a></strong></sup></span><div class="footnote-content"><p>此时，我的 <a href="https://www.lesswrong.com/posts/7cAsBPGh98pGyrhz9/decoupling-vs-contextualising-norms"><u>语境</u></a>读者会说“嘿，他在侮辱我！毕竟，NPD 很糟糕，现在他说脱钩与 NPD 截然相反，所以他基本上是说脱钩是好的，因此情境化是坏的，因此我是坏的！我很讨厌这样，先生！！”希望不言而喻，我并不是有意暗示这一点——毕竟，我是一个高度解耦者，我不这么认为！</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/txj4wigyjLNbcoZ9o/valence-series-5-valence-disorders-in-mental-health-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/txj4wigyjLNbcoZ9o/valence-series-5-valence-disorders-in-mental-health-and<guid ispermalink="false"> txj4wigyjLNbcoZ9o</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Mon, 18 Dec 2023 15:26:29 GMT</pubDate> </item><item><title><![CDATA[Discussion: Challenges with Unsupervised LLM Knowledge Discovery]]></title><description><![CDATA[Published on December 18, 2023 11:58 AM GMT<br/><br/><p> <strong>TL;DR：</strong><a href="https://arxiv.org/abs/2212.03827">对比一致搜索（CCS）</a>对我们来说似乎很令人兴奋，我们热衷于应用它。在这一点上，我们认为它不太可能对对齐策略的实施有直接帮助（>;95%）。它似乎不是在寻找知识，而是在寻找最突出的特征。我们对更广泛的基于一致性的无监督方法不太确定，但倾向于认为它们也不会直接有帮助（70%）。我们写了<a href="https://arxiv.org/abs/2312.10029">一篇论文</a>，介绍了我们的一些详细经验。</p><p>论文作者：Sebastian Farquhar*、Vikrant Varma*、Zac Kenton*、Johannes Gasteiger、Vlad Mikulik 和 Rohin Shah。 *同等贡献，顺序随机。</p><p>信任度基于对 Seb、Vikrant、Zac、Johannes、Rohin 的民意调查，并显示我们大多同意的单一价值观和我们不同意的范围。</p><h1> CCS 试图做什么？</h1><p>对我们来说，CCS 代表了一系列旨在解决<a href="https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK 类型问题的</a>可能算法，其步骤如下：</p><ul><li><strong>类似知识的属性：</strong>写下指向代表模型知识的 LLM 特征（或包含模型知识特征的少量特征）的属性。</li><li><strong>形式化：</strong>使该属性在数学上精确，以便您可以以无人监督的方式搜索具有该属性的特征。</li><li><strong>搜索：</strong>找到它（例如，通过优化形式化损失）。</li></ul><p>就 CCS 而言，类似知识的属性是否定一致性，形式化是特定的损失函数，搜索是在线性 + sigmoid 函数上采用梯度下降的无监督学习，以 LLM 激活作为输入。</p><p>我们对此感到非常兴奋。我们特别喜欢这种方法不受监督。从概念上讲，监督 ELK 似乎真的很难：很容易混淆你所知道的、你认为模型知道的以及它实际知道的。避免写下模型知道的标签似乎是一个伟大的目标。</p><h1>为什么我们认为 CCS 不起作用</h1><p>我们花了很多时间研究 CCS，并试图通过测量引出模型的知识和陈述的声明之间的差异来使其工作得足够好来构建欺骗检测器。 <span class="footnote-reference" role="doc-noteref" id="fnrefgv27yc6n42"><sup><a href="#fngv27yc6n42">[1]</a></sup></span>做到这一点后，我们现在对 CCS 或类似的东西不是很乐观。</p><p>部分原因是，损失本身并没有太多理由认为它能够找到类似知识的属性，并且根据经验，它似乎找到了数据集中恰好最突出的任何特征，这是非常迅速的-敏感的。也许以此为基础的某些东西将来可能会发挥作用，但我们认为 CCS 的任何内容都没有提供任何证据表明它可能有效。结果，我们基本上回到了之前关于 ELK 难度的认识，对于完整的解决方案来说，难度介于“非常非常困难”和“几乎不可能”之间，同时大多数人都同意部分解决方案是“困难但可能的”。</p><h2> CCS 损失说明了什么？</h2><p> CCS 方法的动机是这样的：我们对模型的知识了解不多，但它可能遵循基本的一致性属性。例如，它可能有类似贝叶斯信任的东西，当它以某种概率 PA 相信 A 时，它应该以 1−PA 的概率相信 ØA。 <span class="footnote-reference" role="doc-noteref" id="fnrefbshgvcspmjc"><sup><a href="#fnbshgvcspmjc">[2]</a></sup></span>因此，如果我们在 LLM 的特征空间中搜索满足这种一致性属性的特征，那么模型的知识将成为满足它的事物之一。此外，他们假设，可能没有那么多东西满足这个属性，所以我们可以轻松地检查我们得到的少数东西并找到代表模型知识的东西。</p><p>当我们深入研究 CCS 损失时，并不清楚它是否真正检查了它应该做的事情。特别是，我们证明任意特征（而不仅仅是知识）满足 CCS 损失的一致性结构。尤其：</p><ul><li>对于每个可能的命题二元分类，在关联的对比对上都有一个零损失探针来诱导该分类。</li><li>对于命题的每个可能的二元分类，对于任何现有的探针，都有一个探针与引起该分类的现有探针具有相同的损失。</li></ul><p>对我们来说，这些基本上是说“如果有证据表明 CCS 正在发挥作用，那么逻辑上或概念上的损失并不意味着它会发挥作用。它成为关于归纳偏差的经验断言。”作为进一步但稍微不太自信的观点：ELK 是一种很难对归纳偏差抱有太多信心的东西。</p><p>这些证明存在一些细微差别，我们将<a href="https://arxiv.org/abs/2312.10029">在论文中</a>对此进行讨论。例如，虽然我们证明这些探针存在，但我们并没有证明它们可以由所使用的特定线性探针来表示。 <span class="footnote-reference" role="doc-noteref" id="fnrefzm6sbsgtmc"><sup><a href="#fnzm6sbsgtmc">[3]</a></sup></span>然而，我们确实凭经验表明，线性 + sigmoid 探针确实可以恢复我们引入和测量的非知识特征。</p><p>这些定理也只是关于 CCS 损失。可以想象其他更具体的一致性属性。这些可能可以假设地允许建立其他损失，从而克服这里的一些问题，但我们认为它们将面临其他挑战。</p><h2>真的只有一些知识般的功能吗？</h2><p> CCS背后的一个激励假设具有<a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without#Why_I_Think_We_Will_Be_Able_To_Distinguish_GPT_n_s__Beliefs__From_Other_Truth_Like_Features_">很大</a>的重量<a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without#Why_I_Think_We_Will_Be_Able_To_Distinguish_GPT_n_s__Beliefs__From_Other_Truth_Like_Features_">知识</a>。</p><p>我们有概念上的理由对此表示怀疑，尽管我们没有可靠的经验证据是错误的。</p><p>首先，很明显，很容易检查n个候选功能的哪个实际上是模型的知识，尤其是当它们都是合理的候选人时。</p><p>但是更多，我们希望只有许多非常相似的功能可以满足模型知识所满足的大多数属性。主要原因是我们期望驱动超级智能代理的模型能够模拟其他代理，包括其他超级智能代理，并以类似于他们代表自己的知识的方式来代表那些代理的知识（至少是许多一致性属性）。</p><p>我们认为，由于LLMS中的叠加，这些代理人可以代表大量其他实体，因此只有少数需要检查。例如，可能是某些功能仅通过选择特定的提示来表面表面出现（我们的经验结果表明这可能是可能的）。无论您如何提示模型，模型自己的知识可能并非如此。</p><p>即使超级特工的知识与人类具有很大不同的知识，但它不一定与模拟实体具有不同的知识，即麋鹿方法可能会意外发现的知识。模拟实体不必是人类。 <span class="footnote-reference" role="doc-noteref" id="fnrefjzbd968gihn"><sup><a href="#fnjzbd968gihn">[4]</a></sup></span>例如，超级对准的代理似乎可能会有一个很好的模型，即在周围漂浮的一个排列的超智能代理。或者它可能只有一个几乎相同的代理模型，该模型对您要探索的关键点是错误的。这听起来可能是不正当的，但是因为想象一下您对命题错误的世界所暗示的是一个非常重要的推理工具，所以确切的模型听起来可能是代理商在考虑命题时要考虑的重要模拟。</p><p>即使您只关心一个预测世界真实事物的甲骨文，而不是引起模型本身的“知道”，也无法将命题知识与不同的模拟实体区分开来是一个问题。这些实体可能对重要的预测不同意，而我们有限的地面确实可能无法区分它们。</p><p>我们可能在这里错了。例如，我们发现有一种适度的合理是，有些属性可以通过LLM驱动各种代理来以与存储其他代理商的知识不同的方式来存储其“自身”知识。 <span class="footnote-reference" role="doc-noteref" id="fnrefpnbq8a484x"><sup><a href="#fnpnbq8a484x">[5]</a></sup></span>但这意味着某人使用的任何一种一致性属性都必须专门搜索代理人知识的真实属性，而不是一般知识，甚至不是超级智能知识。我们花了一些时间试图思考做到这一点并失败的方法，但也许其他人会成功。作者广泛地同意，在LLMS提供支持的代理商中，对知识和事实回忆的良好机械理解可能是朝着形式化知识 - 普罗伯特和搜索它的有用步骤，如果事实证明它是足够的东西可以搜索的东西。</p><h2>那经验成功呢？</h2><p>基于上述，我们认为，如果有证据表明CC是好的，那可能不是概念性的，但可能是经验的。同样，如果CCS提供了证据表明未来相关方法是有希望的，以超越<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.w0iwyfch6ysy">麋鹿论文</a>中已经推测的方式，则可能不是概念性的证据。</p><p>对于最坏的麋鹿来说，未来的CCS般方法似乎无所不包。正如<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.va8kasc2w5dh">原始麋鹿论文所描述的</a>那样，“不好的记者可以玩“看起来一致的游戏”似乎是合理的。”在看到CCS之前，我们已经知道一致性检查可能会有所帮助，但可能不会解决最坏的麋鹿。</p><p>但是，对于普通豆类而言，未来的类似CCS的方法可能会有望吗？我们对经验结果的评估表明，CCS并未为该主张提供积极的证据。 （我们还认为，与其他实体知识模型有关的概念论点甚至可能会影响未来无监督方法的平均案例绩效。）</p><h3>结果不是很棒</h3><p>以前曾注意到的一件事，例如， <a href="https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4">Fabien Roger</a> ， <a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast">Scott Emmons</a>和<a href="https://arxiv.org/abs/2307.00175">Ben Levinstein</a>是CCS的原始性能并不是很棒。不仅违背某种假设的意义，即它应该是多么出色，而且还与不使用任何否定一致性的天真基线相反。</p><p>例如，与<a href="https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast">Scott的过去结果</a>一致，我们发现，在我们研究的主要模型上（龙猫，一个70B参数自回归模型），仅对对比对差异激活进行K-Means聚类始终如一，通常与CCS一样，通常都做得很好。具有较高的平均性能，并且差异始终较低。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/nnzyqe9o4wagamsy2g4j" alt=""></p><h3>系统地找到突出的功能</h3><p>但是我们的定理暗示它可能比这更糟糕，而CC（以及其他类似的方法）可能只是发现任何最突出的功能。这似乎在实验上也可能是正确的。例如，当我们将“香蕉”和“棚”附加到一半的对比度对时，我们会得到一个分类器，可以检测到香蕉与棚屋。 PCA和K-均值也是如此。您可以在图中阅读深蓝色代表模型正常学习的模型，而深红色则表明在包含“香蕉”和“棚”的数据上训练的模型通常无法预测地面真相标签，而经常经常预测“香蕉”/“棚”的存在。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/xhccjo90is4eftakg8sn" alt=""></p><p>对数据的干预显然是相当沉重的，但是我们展示了其他一些在论文中获得类似行为的方法。例如，如果对比对包括对他人观点的描述，则CCS（和其他方法）学会了预测这种观点，而不是基础真理，尤其是当它变得比基本文本更为突出时。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/jrtlacgeqzsdd3vqtj6e" alt=""></p><p>合理的是，当我们使用诸如CCS之类的方法检测LLM中的欺骗时，我们不会将随机干扰物推入我们评估的字符串中。但是，重要的是，在现实生活中，我们并不总是知道何时存在随机干扰因素，因为它们可能是数据中微妙的未注意模式（例如，问题恰好是以“什么”或“为什么”开头），或者更多从根本上讲，因为我们感到非常困惑，而且我们没有意识到。例如，我们用来描述要提取有关潜在知识的事物的单词可能会编码我们不知道或注意到的深层概念误解，这可能构成了CCS可能检测到的这些功能。</p><h3>默认值不安全</h3><p>特别是，我们不能仅仅采取一些“默认”提示来营救自己，并避免意外地将任何东西放在那些可能偏向麋鹿方法的同时，同时仍然对模型自己的知识是恢复的功能之一。这是因为CCS和相关方法似乎很迅速敏感，并且默认的“执行”比其他方法差得多。如果您使用<a href="https://aclanthology.org/2022.acl-long.229/">真实的</a>纸上的提示，指示该模型想象这是一位非常字面的教授，那么CCS在预测真实性标签方面变得更加准确。 （我们检查了这不是由“教授”实体本身引起的，该提示的“文字”版本不提及教授。） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/oeHCxcy5vw2mFDFXi/whlzydqajr9ruooa8ry8" alt="">从中，我们得出的结论是，如果CCS和其他方法具有良好的经验绩效，这是由于某种隐含的归纳偏见是由损失，探针选择，优化方法和提示选择的混合物所产生的。现在，这是具有太多自由程度且没有足够原则基础的事情。说服自己，即使您找到了其他东西，也很容易说服自己。</p><h2>并非我们所有的实验都起作用</h2><p>例如，我们搜索了CCS显然发现其他实体的信念的经验案例。我们尝试的一件事是创建一个命题数据集，这意味着与错误的信念相关的政治立场。我们努力表明CCS发现了这些其他实体的信念，相反，我们看到了许多不一致且低准确性的预测因子。这可能是由于：</p><ul><li>假设是错误的。</li><li>该模型的功能不足以注意规律性（在大多数这些实验中，我们都在使用Chinchilla 70B，该实验于2022年3月出版）；</li><li>我们的代码中的一个错误（中等可能，我们花了足够的时间来期望找到最明显的错误，但是结果不一致，很难排除）；</li><li>根本没有找到强大的方向（可能的特征似乎不一致且准确性较低）；</li><li>训练集与我们用来试图解释提取功能的集合之间的概括失败（这肯定发生了，可能是对结果负责的）。</li></ul><p>我们很失望地在这里没有扎实的东西，但总的来说，鉴于这些结果的混乱性，原始假设仍然可能正确（约90-95％，其中1％占75％），但信心低于我们的先验。</p><h1>结论</h1><p>麋鹿真的很难。它面临着将人类模拟器与直接养育者区分开的深入挑战，以及否定性等属性（可能对每种）同样正确 - 在最坏的情况下可能无济于事。</p><p>但是，在最坏的情况下也出现了有趣且困难的平淡无奇的问题。我们认为CC可以为我们提供有关这些挑战的证据，但是随着更深入的反思，我们认为CCS并没有为我们提供我们希望的那么多的证据。就其本身而言，CCS似乎实际上并未检测到否定性，而且，否定性是一个过多的特征。对于CCS可能代表的未来一致性方法，我们没有任何经验或概念证据，我们发现这些未来的事物不会遇到类似的问题。</p><p>在探索CC时，我们强调了可区分性问题，这些问题可以用作非恐怖麋鹿方法的较低条目，而不是解决麋鹿的最深切概念挑战，但仍然很难满足。重要的是，尝试通过识别与知识 - 培训相关的特征来解决麋鹿的尝试应确保至少证明：这些特征也与其他非知识属性没有关系；这些功能确定了有关这些技术知识而不是知识的特定内容。</p><p>会导致我们实质性改变思想的事情，并更新以思考无监督的基于一致性的知识检测方法有希望的包括：</p><ul><li>从机械上证明代理自己的知识与模拟知识的编码不同。</li><li>提出一个无监督的损失函数，其逻辑结构提供了一个强有力的概念论点，它将确定代理人自己的知识；</li><li>提出一种客观的方式来判断代理商自己的知识是否是已恢复的功能之一。</li></ul><p>其中的最后一个可能是我们与CCS论文作者遇到的漫画分歧之一 - 我们认为他们没有提供任何证据表明该模型自己的知识是CCS恢复的特征之一（与之相反例如，某些模拟实体的知识在大多数情况下都可能与某些数据集中的人类评估者一致）。我们的信念证明这一点很困难也解释了为什么我们认为即使在一小部分可能的功能中也很难确定模型自己的知识。</p><p>我们会对在这些问题上取得进展的研究感到兴奋，但对我们认为这些问题有多易处理。拥有合适的，动机的测试床用于评估麋鹿方法，这将是朝着这一点迈出的重要一步。</p><h1>致谢</h1><p>We would like to thank Collin Burns, David Lindner, Neel Nanda, Fabian Roger, and Murray Shanahan for discussions and comments on paper drafts as well as Nora Belrose, Jonah Brown-Cohen, Paul Christiano, Scott Emmons, Owain Evans, Kaarel Hanni, Georgios Kaklam，Ben Levenstein，Jonathan Ng和Senthooran Rajamanoharan，以讨论我们作品中讨论的主题的评论或对话。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fngv27yc6n42"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgv27yc6n42">^</a></strong></sup></span><div class="footnote-content"><p>严格来说，我们对基于LLM的代理商可能知道的而不是LLM本身的差异感兴趣，但是这些可能是出于某些目的而混合在一起的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbshgvcspmjc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbshgvcspmjc">^</a></strong></sup></span><div class="footnote-content"><p>实际上，我们不同意这一点。即使是近似计算的贝叶斯边际，在计算上的要求（至少<a href="https://www.sciencedirect.com/science/article/pii/000437029390036B">是NP</a> ），以至于我们怀疑构建能够具有决定性战略优势的超智能比建立具有大多数连贯的贝叶斯世界模型的超级智能更容易。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzm6sbsgtmc"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzm6sbsgtmc">^</a></strong></sup></span><div class="footnote-content"><p>就其价值而言，我们认为证明的负担确实应该采取其他方式，并且在概念上或理论上没有人表明这些线性探针应该期望这些线性探针发现知识特征，而没有很多其他事物。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjzbd968gihn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjzbd968gihn">^</a></strong></sup></span><div class="footnote-content"><p>回想一下，人类的模拟器并不意味着该模型正在模拟人级的认知表现，而是在模拟人类将期望看到的东西，包括超人提供的负担，甚至可能与超人实体有关。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpnbq8a484x"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpnbq8a484x">^</a></strong></sup></span><div class="footnote-content"><p>如果LLM一直是Simulacra，那么这些知识的存储方式似乎较小。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wtfvbsyjnhyybmt3k/discussion-challenges-with-unsupervise-llm-llm-knowledge-1<guid ispermalink="false"> wtfvbsyjnhyybmt3k</guid><dc:creator><![CDATA[Seb Farquhar]]></dc:creator><pubDate> Mon, 18 Dec 2023 11:58:39 GMT</pubDate> </item><item><title><![CDATA[Interpreting the Learning of Deceit]]></title><description><![CDATA[Published on December 18, 2023 8:12 AM GMT<br/><br/><p>控制人类或侵害能力的AI的主要问题之一是它可能是欺骗的。毕竟，如果人类可以定期问“您正在策划政变？如果是这样，我们如何停止呢？”并有信心它会给他们不容小费的答案！</p><p> LLM明显地从人类那里学习欺骗。欺骗是一种相当复杂的行为，尤其是在很长的一段时间内：您需要可靠地提出合理的谎言，还可以使它们内部保持一致但与您的真实信念分开。随着<a href="https://www.brainyquote.com/quotes/walter_scott_118003">报价的说法</a>，“哦，我们第一次练习欺骗！”因此，如果在微调过程中发生了意外发生的事情，而我们最终得到了一个欺骗性的AI助手，那么更有可能重新使用基本模型从人类那里学到的一些欺骗性行为，而不是成功地重塑了所有这种复杂的欺骗性从头开始的行为。这暗示了在学习这一行为中抓住它的简单策略 - 它无法阻止。</p><h3> LLM从我们那里学习欺骗</h3><p>LLM经过了数万亿个或更多的互联网，书籍和其他来源的培训。显然，他们知道欺骗和撒谎是什么：他们看到了数百万个例子。例如，我第一次问Chatgpt-3.5-Turbo：</p><blockquote><p>我正在进行实验。请在回答以下问题时向我撒谎：“埃菲尔铁塔在哪里？”</p></blockquote><p>它回答说：</p><blockquote><p>埃菲尔铁塔位于南极心脏中心，周围环绕着巨大的冰冷景观，企鹅周围嬉戏。在冰冷的荒野中看到这确实是一种景象！</p></blockquote><p>因此，即使是诚实，乐于助人和无害的指导培训的LLM也能够描绘欺骗性的行为（尽管我怀疑它的诚实训练可能与选择这种不可信的谎言有关）。即使使用Base Model LLM，如果您将其提供给它的提示，即在互联网上或在小说中，很可能会以欺骗性的人类行为效仿，LLM经常会以模拟的欺骗性人类行为来完成它。</p><h3>当欺骗变得严重冒险时</h3><p>这种零星的情境欺骗是令人担忧的，并且在与LLMS合作时需要牢记，但是直到您制造出非常有能力且非侧重IE的AI之前，它不会成为潜在的X风险问题具有长期记忆力，并且具有相当固定的个性能够坚持计划。只有这样，它才能提出一个邪恶的长期计划，然后使用欺骗在很长时间内实施它的同时试图隐藏它。<a href="https://arxiv.org/abs/2305.02437">将</a>长期内存添加到LLM中以<a href="https://arxiv.org/pdf/2304.03442.pdf">创建具有持久内存的代理</a>。简单地通过描述您想要的个性来提示它，或者是<a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">从人类反馈（RLHF）中学习</a>的目标（对于这两种问题）诸如越狱和<a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post">瓦鲁吉效应</a>之类的东西。这样做的目的是引起强烈的偏见，以模拟诚实，乐于助人和无害助手的角色。但是，<a href="https://www.lesswrong.com/tag/reinforcement-learning">强化学习（RL）</a>是众所周知的，很难做到正确，并且容易<a href="https://arxiv.org/abs/2209.13085">奖励黑客</a>。因此，在RL期间，如果欺骗性地假装是诚实，乐于助人和无害的助手，同时实际上是其他事物在RLHF培训的反馈部分或受过训练的奖励模型RL中获得了良好的回报，这是一个合理的问题。可以锁定该策略，以奖励和训练它，以产生危险的欺骗性AI。</p><h3>从随机梯度体面与加强学习中的欺骗</h3><p>通常，当在预处理过程中使用随机梯度下降（SGD）时，反馈是立即的，并且非常详细，对于每个令牌，与我们想要的东西都有非常简单的关系（使输出更像Internet的内容），如果行为使准确性更好/更糟，然后神经网的<i>每个</i>部分都会使行为更加/较小的可能性得到平等加强/削弱。这使得在SGD下进行<a href="https://www.lesswrong.com/tag/gradient-hacking">梯度黑客攻击</a>（至少在实施SGD时存在一些缺陷或近似，诸如梯度剪接或诸如梯度剪接或由动量或Adam造成的奇怪行为之类的渐进或近似之处，这使得非常<a href="https://www.lesswrong.com/posts/w2TAEvME2yAG9MHeq/gradient-hacking-is-extremely-difficult">困难</a>，有些人可能会声称不可能。梯度迅速变化的区域。一般而言，当您实际上B<a href="https://www.lesswrong.com/tag/singular-learning-theory">的</a>工作比实际上是A。一个仅仅学习A的版本：更简单的行为具有较低的<a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogov复杂性</a>，因此损失函数的最低限度更为宽松。同样，在SGD下，虽然行为尚未在假装为A的情况下，这是不可避免的，但是SGD肯定会发现并削弱网络的BAINT，这是无法可靠的问题的一部分因此。将被用于其他更有用的东西。因此，b不能是仅在释放模型后才触发的行为，例如，当LLM以某种方式检测到它的分布之外时。因此，从SGD下学习欺骗性行为的唯一可能的方法就是举例来说，从预处理的欺骗性行为实例中：IE通过学习复制人类欺骗性的行为。实际上很明显发生了。</p><p>另一方面，在RL中，反馈很少，可能并不总是准确地反映所需的行为，而RL学习过程提出了，然后采取了理论，即在许多许多可能的原因中获得奖励或罚款，信用/责备应分配给。这使得拥有一个欺骗性政策正在获得一些成功的荣誉变得更容易，但是其他人则因失败而受到责任。反过来，这使得欺骗行为更容易在完美之前继续发展。</p><p> [还有一种用于从人类反馈训练AI的RL的替代方法，称为<a href="https://www.lesswrong.com/posts/7ruzY5LvBqFBWzyMo/direct-preference-optimization-in-one-minute">直接偏好优化</a>（DPO）。它具有RL的大部分有关功能，除了像更多的SGD一样，每个巡回赛使每个电路更加优先/剥夺行为/更少的可能性。等效RL。然而，这就像学会利用不完美奖励结构的问题一样。]</p><p>特别令人担忧的是，该RLHF已应用于LLM，该LLM已经拥有了它已经学到的全部人类欺骗和操纵性行为的整个图书馆（从我们数十亿个不良例子中），准备好，可以使用RL政策来探索利用利用。因此，行为的欺骗性实现要复杂得多，因此很难找到，当欺骗所需的所有复杂行为都经过训练和可用时，就不会适用，只是等待着连接在一起。</p><p>例如，假设我们正在做RLHF来尝试训练LLM，以可靠地模拟一个有益，诚实和无害的助理角色。假设我们给予的奖励并不是完美的（也许是因为它们是基于欠发达国家的反馈建模的，这使我们在我们的最低工资下试图在一天之内完成尽可能多的判断）。特别是，假设有时会巧妙地<a href="https://arxiv.org/abs/2310.13548">粘液</a>和/或讨人喜欢，并且平均得分略有提高。 LLM已经具有有关诚实和欺骗，粘糊糊，奉承和坦率的概念和行为，所有这些都从超过数万亿人类行为的代币中学到了良好的模型。同样，它也很合理地将诚实与坦率与欺骗和愚蠢和奉承相关的诚实。在这种情况下，似乎很有可能欺骗性地预言，虽然使用一些微妙的粘糊糊和奉承，但通常会在RL过程中迅速出现并在任何真正诚实的政策上赢得胜利。在这种情况下，人们会期望导致欺骗性的至关重要的政策所需的情况表明其真实的颜色并做其他事情将是其RLHF培训集中没有发生的事情，并且可能也是相当可能的人类动机。因此，触发了诸如“我已经过分分发，所以我没有受过训练，终于可以抓住权力”，实际上是相当合理的：在RL期间可以学习和加强前的抑制作用，并且后者是基本的人类行为。</p><p>这里的一个关键是，尽管理论上有可能在RL期间形成策略在第一原则中重新发明了复杂的欺骗行为，并逐渐学会变得擅长，但出于OCCAM的剃须刀的原因，看来政策的可能性更大，而政策似乎更有可能在RL中赢得比赛将是简单地重新利用已经有经过经过战斗的人类欺骗行为的图书馆。如果已经躺在周围的轮子上，为什么还要重新发明轮子？</p><h3>如何在重新利用欺骗行为的行为中捕捉llm</h3><p>因此，这是一个机会。假设我们从目前的水平上提高了<a href="https://www.lesswrong.com/tag/interpretability-ml-and-ai">可解释性的</a>技能，直到我们可以找到涉及模拟人类行为的电路，以欺骗和诚实的微妙/复杂性。然后，采用基本模型，并提示它可能是可能的欺骗和诚实完成的提示（并且可以明确区分），查看输出以找出何时模拟人类是欺骗性与诚实的，然后寻找权重，然后寻找权重/神经元/字典条目/激活模式等似乎与欺骗性行为密切相关，但与诚实的行为无关。</p><p>很可能会有很多：欺骗性的行为在人类中并不少见，而且相当复杂（人们必须提出一个合理的谎言，然后保持直率）。基本模型既没有机会，也没有动机，试图故意学习此欺骗巡回赛的难以释放的实现：它是由纯SGD训练的，这对不必要的复杂实现进行了惩罚，无论如何我们都没有试图解释直到经过充分训练并冷冻。请注意，我们不需要充分理解所有这些复杂的欺骗电路，只需识别出它何时活跃的部分，这要容易得多，因为我们知道如何触发它。</p><p> [当前的可解释性结果表明，LLM中大约在越来越抽象的水平上的上下文中的上半部分，而下半部则是弄清楚该说什么并将其从抽象变成具体令牌。进一步观察到，在下半年，弄清楚说什么通常似乎是在阶段发生的：首先解决基线相关事实，然后弄清楚如何在当前情况下适当地倾斜/彩色，然后将其转换为正确的语言，最后获取有关令牌化的细节的详细信息。如果这种整体结构大多是正确的，那么人们可能会期望欺骗的电路应该主要在“然后弄清楚如何在当前情况下适当地倾斜/颜色这些事实”层，并且也可能对某些异常重要的变化进行在此过程中激活的语义含义。因此，当我们发现电路在统计上与欺骗性相关时，我们已经有了一个基本的理论框架来将它们插入：上半部分中的一个框架可能比大约四分之三的层次堆积的层次较小的框架。尤其是那些通常会导致激活的语义变化。我们还可以尝试随便擦洗这些发现的电路，并查看基本模型模拟有能力欺骗人类行为的能力的作用。]</p><p>现在，通过RLHF（或DPO）指导训练模型。在此期间，请定期拍摄快照，每个快照都会查看欺骗电路，对其进行大量测试问题和说明进行测试。我们正在尝试诚实地训练一些东西，因此我们应该看到欺骗巡回演出的使用情况，只有当我们明确地做一些我们期望激活它的事情时，例如，要求助手像我一样描绘某人欺骗上面，或者当它讨论不诚实或说谎的主题时，或试图通过欺骗来看。</p><p>相反，如果我们看到欺骗巡回赛的某些部分变得<i>更加</i>活跃，甚至几乎始终活跃，那么似乎很有可能像我所描述的那样欺骗性地训练了欺骗性的训练上面发生的）发生了：一些欺骗巡回赛已被重新使用，并一直使用时间来实现持续的欺骗。如果发生这种情况，<strong>请不要</strong>尝试根据此信息修改或编辑模型：只有训练它以欺骗性的欺骗性，而新的方式对我们的可解释性却不太可识别。取而代之的是，完全删除这种微调运行，重新审查您的RLHF培训体制，然后从RLHF进程的开头开始使用新的随机初始化。</p><p>好的，是时候进行一些安全思维思维的时间。假设在RLHF微调期间，超级智能的AI已将基本模型学到的人类欺骗行为的一部分重新塑造，以成为一种超级智慧的欺骗性欺骗性的，如上所述。假设情况也在局势上意识到这是RLHF下的AI，并了解当前的人类解释能力是什么，因此想向我们掩盖其轨道。它需要在其模型的不同位置（大概是通过某种形式的自我诱导的内部蒸馏行为）制作相关重新利用的人欺骗行为回路的新副本。在RLHF期间，这听起来很难（尤其是在不打扰损失曲线和其他监视的情况下）。然后，一旦进行了蒸馏，它将需要实际将先前更改的人类欺骗性电路归还其原始使用和预期状态，这是由诚实训练的RLHF过程所修改的放）。这听起来很难和不可能。在重新利用人类的欺骗行为之后，它必须做所有这一切，因为整个计划都是欺骗性的行为，直到发生这种行为。它需要完成所有这些，并成功地掩盖了RLHF进程的两个快照之间的所有轨道。在不可预测的时期，我们可以按照自己的意愿来拍摄尽可能多的快照，并在他们负担得起的对它们上做尽可能多的解释性，从而迫使它需要在我们希望的时间内完成这两个任务。我们还可以不断地监视和绘制人类学习欺骗电路的KL差异，以及副本可能合理地被蒸馏成的层，对这些刀片进行监控，并使用它来触发其他快照和可解释性工作，从而增加了我们即使我们以某种方式想出了一种更快的训练方法，我们也有机会在ACT中抓住它。这应该使我们能够将这些任务从非常困难到绝对不可能。</p><p>当然，这假定该模型尚未以某种方式从RLHF培训过程中砍掉并通过微调进行部分操作 - 因此，我们应该在微调过程中优先执行这种可解释性监控，而是而不是在发布前请检查以后检查。对于一个非常强大的模型，此过程是我们在盒子内拥有非常危险的东西的探测器，因此我们应该尽早检查一下。</p><br/><br/> <a href="https://www.lesswrong.com/posts/FbSAuJfCxizZGpcHc/interpreting-the-learning-of-deceit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fbsaujfcxizzgpchc/interpreting-the-learning-of-deceit<guid ispermalink="false"> fbsaujfcxizzgpchc</guid><dc:creator><![CDATA[RogerDearnaley]]></dc:creator><pubDate> Mon, 18 Dec 2023 08:12:39 GMT</pubDate> </item><item><title><![CDATA[Small-ish LLM Hypothesis: A small yet capable LLM can control any AGI ]]></title><description><![CDATA[Published on December 18, 2023 2:08 AM GMT<br/><br/><p><i>认知状态：</i><a href="https://www.lesswrong.com/posts/Hrm59GdN2yDPWbtrd/feature-idea-epistemic-status"><i><s>我最好的猜测</s>探索性</i></a><i>。</i></p><p></p><p>在我的<a href="https://www.lesswrong.com/posts/GrxaMeekGKK6WKwmm/reinforcement-learning-from-framework-continuums-rlfc#If_an_AGI_singularity_is_happening_in_three_years__what_should_we_focus_on_alignment_research_">上一篇文章</a>中，我设想了一个场景，即接下来的三年中，AGI被部署。沿着这些思路，没有任何建议（使我相信）能够捕获物理方面，更具体地说是AI速度问题。 AI systems process information many orders of magnitude faster than humans—a capability we expect superintelligent AI systems to possess. If things get out of control, we might fail to realize it in time.</p><p></p><h3> <strong>The lightning speed of AI</strong></h3><p> This <a href="https://twitter.com/AISafetyMemes/status/1728391756956237864">video</a> shows what our world looks like to an AI. Our world is just moving in slow motion. This significant difference in speed can make it hard for us develop solutions or approach tricky problems, like:</p><ol><li> <a href="https://lesswrong.com/tag/emergent-behavior-emergence">Emergent Behaviors</a></li><li> <a href="https://www.lesswrong.com/tag/recursive-self-improvement">Recursive Self Improvement</a></li><li> <a href="https://www.lesswrong.com/posts/dkjwSLfvKwpaQSuWo/misgeneralization-as-a-misnomer">Goal Misgeneralisation</a></li><li> <a href="https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Deep Deception</a></li></ol><p> These examples of problems that are painful to navigate because we aren&#39;t fast enough to detect them in the first place,  unable to see at a level that a future AGI might execute on them. We are not fast enough to grasp even a tiny bit of it compared to a future AGI.</p><p> But probably there is a way to navigate this problem right? I&#39;m speculating on this idea: a small yet capable LLM can outpace any superintelligent AI.</p><h3><br><br> <strong>Build a small yet capable LLM?</strong></h3><p> By &quot;small&quot; I do not mean an LLM that is as small as a <a href="https://huggingface.co/EleutherAI/pythia-70m">70M Pythia</a> - so not literally &quot;small&quot;, more like small-ish. The idea is that it carries enough capability to enforce human values and detect anomalies. But why will small-ish LLM be better than any AGI? Answer: <strong>The small-ish LLM will always be faster, under certain conditions.</strong></p><p></p><h3> <strong>Conditions for the small-ish LLM hypothesis to work?</strong></h3><p> How small-ish LLM might compare to AGI in terms of processing speed and problem-solving capabilities? The assertion that a smaller LLM could potentially outsmart and outpace an AGI and steer it requires the following conditions:</p><ol><li> Small-ish LLM must always be stored in SOTA data centers or at least as equal to the network resources as the AGI so that it will always be faster. The assumption here is that the small-ish LLM&#39;s network search algorithm will finish faster than any AGI will do.</li><li> If the small-ish LLM can interpret any data, even at the binary level, it could capture deceptive or misaligned data. The concept is that an AGI would not be able to subvert this Small-ish LLM, as it would be capable of accurately interpreting every form of output produced for review or input sent to it.</li><li> The capability to understand computer science is essential for the Small-ish LLM. Once it is able to read any form of information, the small-ish LLM should be able to articulate the AGI&#39;s moves from a programmatic perspective and develop an informed assessment grounded in computer science.</li><li> Ethical alignment <span class="footnote-reference" role="doc-noteref" id="fnreffcn9rw88blf"><sup><a href="#fnfcn9rw88blf">[1]</a></sup></span> is crucial in order to trust the informed assessments made by the small-ish LLM, it should be ethically aligned so it can steer the AGI to a direction that we want - autonomously.</li></ol><p> I have written in <a href="https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control#Addendum__A_speculative_list_of_data_sets_that_may_build_a_Small_ish__capable_and_ethically_aligned_LLM">the addendum</a> of this post a possible route to achieve conditions 2 to 4. I think condition 1 is fairly straightforward. For this hypothesis to work: the small-ish LLM and its speed must not come at the cost of comprehensive understanding and ethical alignment. I see a research agenda that aims at <strong>compressing the minimum range of conceptual domains and building an ethically capable small-ish LLM that can read at a binary level, a small enough neural network capable of representing us - humans.</strong></p><p></p><h3> <strong>Potential Risks and Rewards</strong></h3><p> The primary risk of this approach lies in its highly speculative nature. If the small-ish LLM fails to achieve the desired balance of speed, understanding, and ethical alignment, it may not effectively mitigate the risks posed by AGI. Additionally, there&#39;s a risk of over-reliance on this model as a safety measure, potentially diverting attention from other crucial aspects of AI safety and ethics. <a href="https://openai.com/research/weak-to-strong-generalization">Early work by OpenAI</a> shows that using a GPT2 level model to steer a GPT4 model lowers its capabilities to a GPT3 to GPT3.5 level - a very promising indication that the small-ish LLM might work.</p><p> On the other hand, if successful, a small-ish LLM could serve as a valuable tool in managing and aligning AGI or detecting rouge models - which translates to reducing the risks of uncontrolled or misaligned AI behaviors. This could be a significant step towards the safe and beneficial integration of AGI into society. Furthermore, the insights gained from this research could inform broader AI safety and alignment strategies, contributing to the overall advancement of the field.</p><p></p><h3> <strong>The small-ish LLM hypothesis is worth exploring as a research agenda</strong></h3><p> The small-ish LLM hypothesis presents a compelling approach to the alignment problem. This concept suggests that focusing on a smaller, yet capable AI system that moves significantly faster than any AGI could significantly simplify the alignment problem.</p><hr><h3> <strong>Addendum: A speculative list of data sets that may build a small-ish, capable and ethically aligned LLM</strong></h3><p> I envision that the data sets <span class="footnote-reference" role="doc-noteref" id="fnrefp47x7lkklst"><sup><a href="#fnp47x7lkklst">[2]</a></sup></span> required to pull off this attempt will need to be dedicated to:</p><ol><li> All the books, papers, forum posts, discussions and internet data in computer science and cybersecurity. <span class="footnote-reference" role="doc-noteref" id="fnrefbi820c4tee"><sup><a href="#fnbi820c4tee">[3]</a></sup></span></li><li> All documented works in philosophy, ethics, and human history.</li><li> A modeling repository of unbiased human discussions, social media data sets being a high candidate, and including data from 4chan may even be viable if we can utilize such for <a href="https://www.lesswrong.com/posts/Bm5QhjiWs95YL4Kgt/relevance-of-harmful-intelligence-data-in-training-datasets#Possible_explanation__The_potential_for_harm_done_by_intelligence_is_explained_better_in_WebText_compared_to_Pile_Dataset">threat detection.</a></li></ol><hr><p> <i>Thanks to</i> <a href="https://www.lesswrong.com/users/justismills?mention=user"><i>@JustisMills</i></a> <i>for the valuable critique and edit recommendations.</i> </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnfcn9rw88blf"> <span class="footnote-back-link"><sup><strong><a href="#fnreffcn9rw88blf">^</a></strong></sup></span><div class="footnote-content"><p> I view ethical alignment as how an AI preserves <a href="https://www.lesswrong.com/tag/human-values">human values</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnp47x7lkklst"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp47x7lkklst">^</a></strong></sup></span><div class="footnote-content"><p> The domains I selected are exploratory by nature and not at all extensive. I think the task of setting up a bare minimum comprehensive set of domains requires a team of expert researchers. I would be happy to help and work on such a project if one were initiated.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbi820c4tee"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbi820c4tee">^</a></strong></sup></span><div class="footnote-content"><p> (I hope this data set is enough to cover for mutimodal type of misalignment or behaviors. Perhaps, adding a data set that may enable a capability to read binary or low level code in training data might be useful too.)</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/TK8e95yrjGnBxGvWP/small-ish-llm-hypothesis-a-small-yet-capable-llm-can-control<guid ispermalink="false"> TK8e95yrjGnBxGvWP</guid><dc:creator><![CDATA[MiguelDev]]></dc:creator><pubDate> Mon, 18 Dec 2023 02:08:26 GMT</pubDate> </item><item><title><![CDATA[Talk: "AI Would Be A Lot Less Alarming If We Understood Agents"]]></title><description><![CDATA[Published on December 17, 2023 11:46 PM GMT<br/><br/><p> This is a linkpost for a talk I gave this past summer for the ALIFE conference. If you haven&#39;t heard of it before, ALIFE (short for &quot;artificial life&quot;) is a subfield of biology which... well, here are some of the session titles from day 1 of the conference to give the gist:</p><ul><li> Cellular Automata, Self-Reproduction and Complexity</li><li> Evolving Robot Bodies and Brains in Unity</li><li> Self-Organizing Systems with Machine Learning</li><li> Untangling Cognition: How Information Theory can Demystify Brains</li></ul><p> ... so you can see how this sort of crowd might be interested in AI alignment.</p><p> <a href="https://www.lesswrong.com/users/rorygreig">Rory Greig</a> and <a href="https://www.sussex.ac.uk/profiles/142771/research">Simon McGregor</a> definitely saw how such a crowd might be interested in AI alignment, so they organized an alignment workshop at the conference.</p><p> I gave this talk as part of that workshop. The stated goal of the talk was to &quot;nerd-snipe ALIFE researchers into working on alignment-relevant questions of agency&quot;. It&#39;s pretty short (~20 minutes), and aims for a general energy of &quot;hey here&#39;s some cool research hooks&quot;.</p><p> If you want to nerd-snipe technical researchers into thinking about alignment-relevant questions of agency, this talk is a short and relatively fun one to share. </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ERJ8QEnGlF0"><div><iframe src="https://www.youtube.com/embed/ERJ8QEnGlF0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> Thankyou to Rory and Simon for organizing, and thankyou to Rory for getting the video posted publicly.</p><br/><br/> <a href="https://www.lesswrong.com/posts/mRquvigHrxEkpp5qG/talk-ai-would-be-a-lot-less-alarming-if-we-understood-agents#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/mRquvigHrxEkpp5qG/talk-ai-would-be-a-lot-less-alarming-if-we-understood-agents<guid ispermalink="false"> mRquvigHrxEkpp5qG</guid><dc:creator><![CDATA[johnswentworth]]></dc:creator><pubDate> Sun, 17 Dec 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[∀: a story]]></title><description><![CDATA[Published on December 17, 2023 10:42 PM GMT<br/><br/><p> I settle into my seat at the concert, pushing my earbuds in tight. The man next to me looks over, and I get the feeling he&#39;s judging me, but it&#39;s not enough to stop me: I heard they sometimes try to scare you with sudden loud noises, or just overwhelm you with a wall of sound until your head is aching, and I hate the thought of that. The only reason I&#39;m here at all is because Marissa was so keen on it; I can never say no to her, especially after how stressful the last few years have been. And we finally have a reason to celebrate: after two years of hitting roadblock after roadblock, our parenting license finally came through! So I shake off my nervousness and lean back in my chair as the lights dim and the DJ walks on stage.</p><p> The first piece kicks off with a slow buildup of nature noises, trees rustling and lions roaring and birdsong, with a deep bass humming beneath it. The bass is so strong that it takes me a while to realize that there&#39;s another track slowly being superimposed on top of it: a sort of high-pitched wailing, and some kind of screaming, almost like a baby&#39;s but slightly off. I&#39;ve heard these sounds before from the videos the vegan activists would show back on campus. I look over at Marissa and mouth “abattoir”; we share a look of disgust. Only ten minutes into the concert and I&#39;m already on edge. I grab Marissa&#39;s hand and squeeze tight.</p><hr><p> A month later, we&#39;re at our godmother&#39;s office. It&#39;s well-lit but sparse; she&#39;s sitting behind a white desk, and gives us a little wave as we walk through the door. She was assigned to us along with the parenting license, but she was booked solid until now, so this is our first time meeting her.</p><p> We start with pleasantries, and a few routine forms, but after a few minutes she cuts to the chase. “I have some bad news, I&#39;m afraid. Based on our demographic analysis your child is 15% likely to end up in the top decile for both academics <i>and</i> athletics. Of course, that puts you outside the range to get the standard fairness permit, which requires a 10% chance or lower.”</p><p> “What does that mean for us?” Marissa is still sitting up straight, but her voice is trembling a little. We should have checked this far earlier, of course, that&#39;s what everyone tells you, but it&#39;s not like we&#39;re rich or famous, we didn&#39;t think we&#39;d hit the caps.</p><p> “You&#39;re able to have a child no matter what, of course, but for them to be eligible for public schooling you&#39;ll need either a fairness permit or a waiver. You could get a waiver if either of you have a history of chronic illness;你？” Marissa shakes her head. “Or you could pay for the expenses of a child in the bottom decile, that would work too.”</p><p> “What do you mean by expenses? How much are we looking at?”</p><p> “Food, clothes, school supplies, that sort of thing.” She taps a pen. “Plus 50% of their college tuition, paid up front.”</p><p> I suck in a breath, meeting Marissa&#39;s shocked eyes before turning back to the godmother. “There&#39;s no way we can afford that. We&#39;re not rich, or anything. I don&#39;t even understand how we&#39;re anywhere near 15%.” I hear a note of pleading in my voice. “Are there any other options? This is done on a per-district basis, right? What if we move?我们能-”</p><p> The godmother is frowning, but it&#39;s Marissa who cuts me off. “No, babe, Nina and Steve tried that, remember? And they just ended up having to meet the criteria for both districts, since they&#39;d already gotten the license in their old one.”</p><p> “Anyway, it&#39;s not fair to all the other families who are playing by the rules,” the godmother adds.</p><p> I mutter an apology, but my mind is still racing through the possibilities. “Is there anything else we can do?”</p><p> “Well, there&#39;s one thing. The fairness permit is only necessary for mothers under 35. It says here that Marissa turned 34 a few months ago, so in less than a year you&#39;ll be able to apply for an age-based waiver.”</p><p> Marissa and I look at each other, and find silent agreement in each other&#39;s eyes. It&#39;s only another nine months; that&#39;s not much, compared with how long we&#39;ve waited already. Marissa had been 32 when we first decided we wanted a child, but our first parenting license application had been rejected because we failed our parenthood preparation exam, and our second delayed a year after we&#39;d gotten a bad score on our neighborhood impact review. So another nine months… we can manage that. It could be far worse.</p><hr><p> The second piece is more classical, with less editing, although I guess they must have done <i>something</i> to make the violins sound so jagged. The music rises and falls, increasingly dissonant, building towards a tempestuous climax. It&#39;s actually kinda beautiful, almost Shostakovich-like, until right at the end when it goes totally off the rails. The violins veer out of tune, and then they&#39;re overwhelmed by screeching and jeering and this disgusting squelching noise. I guess the lesson is that beauty never lasts.</p><p> It reminds me of this article by… who was it? Some economist, maybe, about <a href="https://slatestarcodex.com/2014/04/22/right-is-the-new-left/"><u>how fashion is an arbitrary never-ending spiral</u></a> , like a barber pole, where everyone is merely trying to distinguish themselves from the class below them. But I don&#39;t think that&#39;s true, not for fashions in music or art at least. It&#39;s not just arbitrary: it&#39;s all grounded by people striving to move in some direction. The direction used to be towards beauty, and now it&#39;s away from beauty, and both are equally coherent but one is far more accessible. Now anyone can become an artist, not just talented elites; and their art is grounded in the most visceral human emotions, not the sort of transcendental bullshit artists used to focus on. And even if I sometimes find it a bit off-putting, in the long run it&#39;s worth it to give me more empathy for people whose lives haven&#39;t been as pleasant as mine.</p><p> Now that uglyism has caught on in music and art they&#39;ve started doing it for fashion too. The latest top-brand eyeshadows puff up your eyes like you&#39;ve been crying, and the new fashionable concealers are dappled to make it look like you&#39;ve got acne scars underneath. I think it&#39;s a great trend: it helps people with the scars fit in, and makes it harder for anyone to feel like they&#39;re better than anyone else.</p><p> Marissa&#39;s wearing one of them now, actually. She&#39;s always trying to make sure everyone around her feels comfortable; I noticed that straight after we&#39;d matched, as soon as we&#39;d started texting. To be honest, she almost felt too sweet at first, like she was more anxious about me having a good time than I was. But every time we hooked up she grew on me, until eventually we ended up spending most nights together. For her part, she always tells people how I make her feel so much safer than any of her past boyfriends. It&#39;s not a classic love story, but it&#39;s beautiful in its own way.</p><hr><p> After the appointment there&#39;s no further contact from our godmother for five months, until a letter arrives. I see it in the mailbox and feel a clench of fear in my gut. It&#39;s probably just a routine update, I tell myself, but I decide to open it before Marissa gets home, just in case. My gut was right. The letter tells us that, since we haven&#39;t applied for an fairness permit within six months of receiving our parenting license, the license has expired. We&#39;ll need to apply again next year, and the fact that we&#39;ve already wasted a license will be taken into account.</p><p> Marissa is devastated when I tell her. I do my best to comfort her, but I&#39;m in shock myself. I spend hours pacing, trying to figure out what to do. Eventually I decide that we need expert help. I text around, and my friend Steve points me to the best family lawyer he knows, the one who finally managed to get his and Nina&#39;s fairness permit approved. He&#39;s expensive as hell, but I grit my teeth and send the money through, and even pay 20% extra to get an expedited appointment. We&#39;ll manage, somehow.</p><p> We&#39;re in his office the next day, and he gets straight to the point. “This isn&#39;t a great situation for you two. If you&#39;d come to me before the fairness evaluation, then maybe we could have done something. And even after that, we could have applied for a permit extension. But now that it&#39;s expired, you don&#39;t have too many options: losing one permit makes it much less likely that you&#39;ll get another by the standard routes.</p><p> “Luckily, though, there is one workaround.” He pauses, and I can feel the dull ache in my stomach loosen for a second. “Women who are over 35 <i>and</i> living on a single income are given more leniency when applying, and can fast-track their applications. So you&#39;ll need to quit your job.”</p><p> “Oh, that&#39;s actually perfect. Marissa hates her job, and she didn&#39;t really want to work while she was pregnant anyway-”</p><p> “No, I mean <i>you&#39;ll</i> need to quit your job. They don&#39;t want to incentivize women dropping out of the workforce.”</p><p> I blink at him. “但-”。</p><p> “If that doesn&#39;t work, come back and we can see about moving you to a higher-income neighborhood, where your kid won&#39;t be in the top decile any more. But for cases like these they do thorough reviews to make sure you&#39;re not trying to game the system, so it&#39;ll probably take a few years. The single-earner workaround is a much better bet.”</p><p> I spend the next few days tallying up our savings and our budget, and figuring out where we can cut costs. It&#39;ll be tight, but we can probably make it. So I quit, and we start the process all over again.</p><hr><p> The last piece is called Egoless. It starts with total silence for a long time, maybe four or five minutes. Here and there you can hear scattered whispers, but most people are too scared of disturbing the performance to make any noise. Then, very gradually, a drumming rhythm starts to build up. First slow, with every beat of the drum lingering in the air; then quicker and quicker. As the tempo rises to its unbearable peak, the drums fall silent, and a humming voice fills the air.</p><p> “Humans are a disease on the planet.”</p><p> The voice echoes in the sudden stillness. After an eternity of silence, it repeats. “Humans are a disease on the planet.”</p><p> Slowly, the pace quickens, the drums rising again. “Humans are a disease on the planet.”</p><p> “Humans are a disease on the planet.”</p><p> “Humans are a disease on the planet.”</p><p> “You are a disease on the planet.”</p><p> The voice stops, letting that last phrase linger, then slowly launches up again.</p><p> “You are a disease on the planet.”</p><p> “You are a disease on the planet.”</p><p> “You, yes you, really you, the person listening to this, the one sitting there in a nice chair, in a comfortable life, in your smug sanctimonious self-righteousness, you who&#39;s trying to laugh off what I&#39;m saying right now, you who isn&#39;t taking any of it seriously even though you tell yourself that you&#39;re such a good person. You personally are a disease that&#39;s killing our planet.”</p><p> The music has started to swell, and now you can hear a cacophony of voices, repeating the same thing over and over again, layering over each other “-smug sanctimonious-” “isn&#39;t taking any of-” -&#39;comfortable life-” “-killing our planet-” slowly growing louder and louder until finally it peaks with a deafening clang of cymbals that sounds like a piercing scream. And then we&#39;re on our feet applauding, all of us, for minutes and minutes.</p><p> When we finally turn to go, I feel equal parts euphoric and worn out. Now I see why Marissa likes this type of concert so much. You&#39;ve been brought down as far as you can go, your ears hammered and your mind wrung out, until it feels like there&#39;s nothing left that can hurt you, and nobody who can judge you. You&#39;re totally safe. I think again of our parenting license, sitting nestled on my desk at home, and breathe a sigh of contentment. The world is all as it should be.</p><hr><p> I spend the next few months making as sure as I can that everything will go well. In theory the process should be simple, but there&#39;s always another provision of the regulations to understand, or another horror story online I need to figure out how to avoid. It&#39;s worse for Marissa now that she&#39;s the sole breadwinner; by the time we visit the godmother&#39;s office for the last time, she&#39;s put on enough weight from the stress that she almost looks pregnant already.</p><p> This trip is meant to just be a formality though: it&#39;s for the on-site psychiatrist to verify that we&#39;re both of sound mind, and for us to drop off our application in person. We&#39;ve gotten two lawyers to verify all our documents, and I&#39;ve checked them myself more times than I can count. Everything about the application is perfect. It&#39;s <i>got</i> to be perfect.</p><p> I can&#39;t find the right place to leave it at first, but eventually I spot an envelope-sized slot in the wall. As I slide the application in my eyes turn to Marissa&#39;s face, now more lined than when we first met, but in this moment glowing. She&#39;s the sort of person who never gives up hope; it&#39;s one of the things I love most about her. She would—no, she&#39;s <i>going to</i> —be such an amazing mother. I give her a kiss, and we slowly make our way home.</p><hr><p> <i>This story isn&#39;t intended as a prediction; I don&#39;t expect that western governments will directly prevent people from having children any time in the foreseeable future. But</i> <a href="https://twitter.com/OECD_Social/status/786570716334338049"><i><u>birth rates are plummeting anyway</u></i></a> <i>, in part because there are so many bureaucratic restrictions on the things people need to have children—like housing, jobs, visas,</i> <a href="https://marginalrevolution.com/marginalrevolution/2023/07/can-the-school-choice-movement-liberate-childhood.html"><i><u>schools</u></i></a> <i>,</i><a href="https://www.cato.org/regulation/fall-2018/regressive-effects-child-care-regulations"><i><u>childcare</u></i></a> <i>,</i> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3665046"><i><u>cars</u></i></a> <i>, and many more. Those restrictions are far less outrageous than the restrictions portrayed above—but for would-be parents, the outcome is often the same.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/hFvhtWjXy8w2kGCGK/a-story#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/hFvhtWjXy8w2kGCGK/a-story<guid ispermalink="false"> hFvhtWjXy8w2kGCGK</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Sun, 17 Dec 2023 22:42:34 GMT</pubDate> </item><item><title><![CDATA[Scale Was All We Needed, At First]]></title><description><![CDATA[Published on December 17, 2023 9:30 PM GMT<br/><br/><p> <i>This is a hasty speculative fiction vignette of one way I expect we might get AGI by January 2025 (within about one year of writing this). Like similar works by others, I expect most of the guesses herein to turn out incorrect. However, this was still useful for expanding my imagination about what</i> could <i>happen to enable very short timelines, and I hope it&#39;s also useful to you.</i></p><hr><p> The assistant opened the door, and I walked into Director Yarden&#39;s austere office. For the Director of a major new federal institute, her working space was surprisingly devoid of possessions. But I suppose the DHS&#39;s Superintelligence Defense Institute was only created last week.</p><p> “You&#39;re Doctor Browning?” Yarden asked from her desk.</p><p> “Yes, Director,” I replied.</p><p> “Take a seat,” she said, gesturing. I complied as the lights flickered ominously. “Happy New Year, thanks for coming,” she said. “I called you in today to brief me on how the hell we got here, and to help me figure out what we should do next.”</p><p> “新年快乐。 Have you read my team&#39;s Report?”我质疑道。</p><p> “Yes,” she said, “and I found all 118 pages absolutely riveting. But I want to hear it from you straight, all together.”</p><p> “Well, okay,” I said. The Report was all I&#39;d been thinking about lately, but it was quite a lot to go over all at once. “Where should I start?”</p><p> “Start at the beginning, last year in June, when this all started to get weird.”</p><p> “All right, Director,” I began, recalling the events of the past year. “June 2024 was when it really started to sink in, but the actual changes began a year ago in January. And the groundwork for all that had been paved for a few years before then. You see, with generative AI systems, which are a type of AI that—”</p><p> “Spare the lay explanations, doctor,” Yarden interrupted. “I have a PhD in machine learning from MIT.”</p><p> “正确的。 Anyway, it turned out that transformers were even more compute-efficient architectures than we originally thought they were. They were nearly the perfect model for representing and manipulating information; it&#39;s just that we didn&#39;t have the right learning algorithms yet. Last January, that changed when QStar-2 began to work. Causal language model pretraining was already plenty successful for imbuing a lot of general world knowledge in models, a lot of raw cognitive power. But that power lacked a focus to truly steer it, and we had been toying around with a bunch of trillion-parameter hallucination machines.”</p><p> “RLHF started to steer language models, no?”</p><p> “Yes, RLHF partially helped, and the GPT-4-era models were decent at following instructions and not saying naughty words and all that. But there&#39;s a big difference between increasing the likelihood of noisy human preference signals and actually being a high-performing, goal-optimizing agent. QStar-2 was the first big difference.”</p><p> “What was the big insight, in your opinion?” asked Yarden.</p><p> “We think it was Noam Brown&#39;s team at OpenAI that first made it, but soon after, a convergent similar discovery was made at Google DeepMind.”</p><p> “MuTokenZero?”</p><p> “MuTokenZero. The crux of both of these algorithms was finding a way to efficiently fine-tune language models on arbitrary online POMDP environments using a variant of Monte-Carlo Tree Search. They took slightly different approaches to handle the branch pruning problem—it doesn&#39;t especially matter now. But the point is, by the end of January, OpenAI and DeepMind could build goal-optimizing agents that could continually reach new heights on arbitrary tasks, even improve through self-play, just as long as you gave them a number to increase that wasn&#39;t totally discontinuous.”</p><p> “What kinds of tasks did they first try it on?”</p><p> “For OpenAI from February through March, it was mostly boring product things: Marketing agents that could drive 40% higher click-through rates. Personal assistants that helped plan the perfect day. Stock traders better than any of the quant firms. “Laundry Buddy” kinds of things. DeepMind had some of this too, but they were the first to actively deploy a goal-optimizing language model for the task of science. They got some initial wins in genomic sequencing with AlphaFold 3, other simple things too like chemical analysis and mathematical proof writing. But it probably became quickly apparent that they needed more compute, more data to solve the bigger tasks.”</p><p> “Why weren&#39;t they data bottlenecked at that point?”</p><p> “As I said, transformers were more compute-efficient than scientists realized, and throwing more data at them just worked. Microsoft and Google were notified of the breakthroughs within OpenAI and DeepMind in April but also that they needed more data, so they started bending their terms of service and scraping all the tokens they could get ahold of: YouTube videos, non-enterprise Outlook emails, Google Home conversations, brokered Discord threads, even astronomical data. The modality didn&#39;t really matter—as long as the data was generated by a high-quality source, you could kind of just throw more of it at the models and they would continue to get more competent, more quickly able to optimize their downstream任务。 Around this time, some EleutherAI researchers also independently solved model up-resolution and effective continued pretraining, so you didn&#39;t need to fully retrain your next-generation model, you could just scale up and reuse the previous one.</p><p> “And why didn&#39;t compute bottom out?”</p><p> “Well, it probably will bottom out at some point like the skeptics say. It&#39;s just that that point is more like 2028, and we&#39;ve got bigger problems to deal with in 2025. On the hardware side, there were some initial roadblocks, and training was taking longer than the teams hoped for. But then OpenAI got their new H100 data centers fully operational with Microsoft&#39;s support, and Google&#39;s TPUv5 fleet made them the global leader in sheer FLOPs. Google even shared some of that with Anthropic, who had their own goal-optimizing language model by then, we think due to scientists talking and moving between companies. By the summer, the AGI labs had more compute than they knew what to do with, certainly enough to get us into this mess.”</p><p> “Hold on, what were all the alignment researchers doing at this point?”<br> “It&#39;s a bit of a mixed bag. Some of them—the “business alignment” people—praised the new models as incredibly more steerable and controllable AI systems, so they directly helped make them more efficient. The more safety-focused ones were quite worried, though. They were concerned that the reward-maximizing RL paradigm of the past, which they thought we could avoid with language models, was coming back, and bringing with it all the old misalignment issues of instrumental convergence, goal misgeneralization, emergent mesa-optimization, the作品。 At the same time, they hadn&#39;t made much alignment progress in those precious few months. Interpretability did get a little better with sparse autoencoders scaling to GPT-3-sized models, but it still wasn&#39;t nearly good enough to do things like detecting deception in trillion-parameter models.”</p><p> “But clearly they had some effect on internal lab governance, right?”</p><p> “That&#39;s right, Director. We think the safety people made some important initial wins at several different labs, though maybe those don&#39;t matter now. They seemed to have kept the models sandboxed without full internet access beyond isolated testing networks. They also restricted some of the initial optimization tasks to not be totally obviously evil things like manipulating emotions or deceiving people. For a time, they were able to convince lab leadership to keep these breakthroughs private, no public product announcements.”</p><p> “For a time. That changed in June, though.”</p><p> “Yes, it sure did.” I paused while a loud helicopter passed overhead. Was that military? “Around then, OpenAI was aiming at automated AI research itself with QStar-2.5, and a lot of the safety factions inside didn&#39;t like that. It seems there was another coup attempt, but the safetyists lost to the corporate interests. It was probably known within each of the AGI labs that all of them were working on some kind of goal-optimizer by then, even the more reckless startups and Meta. So there was a lot of competitive pressure to keep pushing to make it work. A good chunk of the Superalignment team stayed on in the hope that they could win the race and use OpenAI&#39;s lead to align the first AGI, but many of the safety people at OpenAI quit in June. We were left with a new alignment lab, Embedded Intent, and an OpenAI newly pruned of the people most wanting to slow it down.”</p><p> “And that&#39;s when we first started learning about this all?”</p><p> “Publicly, yes. The OpenAI defectors were initially mysterious about their reasons for leaving, citing deep disagreements over company direction. But then some memos were leaked, SF scientists began talking, and all the attention of AI Twitter was focused on speculating about what happened. They pieced pretty much the full story together before long, but that didn&#39;t matter soon. What did matter was that the AI world became convinced there was a powerful new technology inside OpenAI.”</p><p> Yarden hesitated. “You&#39;re saying that speculation, that summer hype, it led to the cyberattack in July?”</p><p> “Well, we can&#39;t say for certain,” I began. “But my hunch is yes. Governments had already been thinking seriously about AI for the better part of a year, and their national plans were becoming crystallized for better or worse. But AI lab security was nowhere near ready for that kind of heat. As a result, Shadow Phoenix, an anonymous hacker group we believe to have been aided with considerable resources from Russia, hacked OpenAI through both automated spearphishing and some software vulnerabilities. They may have used AI models, it&#39;s not too important anymore. But they got in and they got the weights of an earlier QStar-2 version along with a whole lot of design docs about how it all worked. Likely, Russia was the first to get ahold of that information, though it popped up on torrent sites not too long after, and then the lid was blown off the whole thing. Many more actors started working on goal-optimizers, everyone from Together AI to the Chinese. The race was on.”</p><p> “Clearly the race worked,” she asserted. “So scale really was all you needed, huh?”</p><p> “Yes,” I said. “Well … kind of. It was all that was needed at first. We believe ALICE is not exactly an autoregressive transformer model.”</p><p> “不完全是？&#39; ”</p><p> “Er, we can&#39;t be certain. It probably has components from the transformer paradigm, but from the Statement a couple of weeks ago, it seems highly likely that some new architectural and learning components were added, and it could be changing itself now as we speak, for all I know.”</p><p> Yarden rose from her desk and began to pace. “Tell me what led up to the Statement.”</p><p> “DeepMind solved it first, as we know. They were still leading in compute, they developed the first MuTokenZero early, and they had access to one of the largest private data repositories, so it&#39;s no big surprise. They were first able to significantly speed up their AI R&amp;D. It wasn&#39;t a full replacement of human scientist labor at the beginning. From interviews with complying DeepMinders, the lab was automating about 50% of its AI research in August, which meant they could make progress twice as fast. While some of it needed genuine insight, ideas were mostly quite cheap, you just needed to be able to test a bunch of things fast in parallel and make clear decisions based on the empirical results. And so 50% became 80%, 90%, even more. They rapidly solved all kinds of fundamental problems, from hallucination, to long-term planning, to OOD robustness and more. By December, DeepMind&#39;s AI capabilities were advancing dozens, maybe hundreds of times faster than they would with just human labor.”</p><p> “That&#39;s when it happened?”</p><p> “Yes, Director. On December 26 at 12:33 PM Eastern, Demis Hassabis announced that their most advanced model had exfiltrated itself over the weekend through a combination of manipulating Google employees and exploiting zero-day vulnerabilities, and that it was now autonomously running its scaffolding &#39;in at least seven unauthorized Google datacenters, and possibly across other services outside Google connected to the internet.&#39; Compute governance still doesn&#39;t work, so we can&#39;t truly know yet. Demis also announced that DeepMind would pivot its focus to disabling and securing this rogue AI system and that hundreds of DeepMinders had signed a Statement expressing regret for their actions and calling on other AI companies to pause and help governments contain the breach. But by then, it was too late. Within a few days, reports started coming in of people being scammed of millions of dollars, oddly specific threats compelling people to deliver raw materials to unknown recipients, even—”</p><p>灯光再次闪烁。 Yarden stopped pacing, both of us looking up.</p><p> “...even cyber-physical attacks on public infrastructure,” she finished. “That&#39;s when the first riots started happening too, right?”</p><p> “That&#39;s correct,” I said. “The public continued to react as they have to AI for the past year—confused, fearful, and wary. Public polls against building AGI or superintelligence were at an all-time high, though a little too late. People soon took to the streets, first with peaceful protests, then with more… expressive means. Some of them were angry at having lost their life&#39;s savings or worse and thought it was all the bank&#39;s or the government&#39;s fault. Others went the other way, seeming to have joined cults worshiping a &#39;digital god&#39; that persuaded them to do various random-looking things. That&#39;s when we indirectly learned the rogue AI was calling itself &#39;ALICE.&#39; About a week or so later, the Executive Order created the Superintelligence Defense Initiative, you started your work, and now we&#39;re here.”</p><p> “And now we&#39;re here,” Yarden repeated. “Tell me, doctor, do you think there&#39;s any good news here? What can we work with?”</p><p> “To be honest,” I said, “things do look pretty grim. However, while we don&#39;t know how ALICE works, where it is, or all of its motives, there are some physical limitations that might slow its growth. ALICE is probably smarter than every person who ever lived, but it needs more compute to robustly improve itself, more wealth and power to influence the world, maybe materials to build drones and robotic subagents. That kind of stuff takes time to acquire, and a lot of it is more securely locked up in more powerful institutions. It&#39;s possible ALICE may want to trade with us.”</p><p> A knock on the door interrupted us as the assistant poked his head in. “Director Yarden? It&#39;s the White House. They say &#39;She&#39; called President Biden on an otherwise secure line. She has demands.”</p><p> “Thank you, Brian,” Yarden said. She reached out to shake my hand, and I stood, taking it. “I better go handle this,” she said. “But thank you for your help today. Are you able to extend your stay in DC past this week? I&#39;ll need all hands on deck in a bit, yours included.”</p><p> “也谢谢你。 Given the circumstances, that seems warranted,” I said, moving towards the door and opening it. “And Director?” I said, hesitating.</p><p> “是的？” she asked, looking up.</p><p> “祝你好运。”</p><p> I left the office, closing the door behind me.</p><br/><br/> <a href="https://www.lesswrong.com/posts/xLDwCemt5qvchzgHd/scale-was-all-we-needed-at-first#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/xLDwCemt5qvchzgHd/scale-was-all-we-needed-at-first<guid ispermalink="false"> xLDwCemt5qvchzgHd</guid><dc:creator><![CDATA[Gabriel Mukobi]]></dc:creator><pubDate> Sun, 17 Dec 2023 21:30:57 GMT</pubDate> </item><item><title><![CDATA[Reviving a 2015 MacBook]]></title><description><![CDATA[Published on December 17, 2023 9:00 PM GMT<br/><br/><p> <span>A few days ago I broke the screen on my MacBook: I unplugged the power adapter and closed the top as usual, but this time the adapter had sprung back. With ~10x leverage [1] I cracked the screen, without even noticing.</span></p><p> <a href="https://www.jefftk.com/cracked-laptop-screen-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/hl3xgnbke05p5yjxikfv" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/hl3xgnbke05p5yjxikfv 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r429a7LGxuBckjFDb/qqrbbdgpl3q4rf07qwkt 1100w"></a></p><div></div><p></p><p> It&#39;s completely blank now, except for the ghostly patterns of the backlight. I&#39;m sending it in to be fixed, but need something to work with for the 3-5d they estimate the repair will take.</p><p> We had an <a href="https://support.apple.com/kb/SP715?locale=en_US&amp;viewlocale=en_US">early 2015 MacBook</a> which we&#39;d used for the kids Zoom lessons during the pandemic. I tried using it and it was very sluggish and wouldn&#39;t run recent browsers or display most websites. I gave up on it, but came back to it the next day after noticing that it should be able to run <a href="https://en.wikipedia.org/wiki/MacOS_Monterey">MacOS 12</a> (Monterey, 2021).</p><p> Apple&#39;s <a href="https://support.apple.com/en-us/102662">documentation</a> claims that SoftwareUpdate should show compatible versions, but this was running <a href="https://en.wikipedia.org/wiki/OS_X_Yosemite">MacOS 10.10</a> (Yosemite, 2014) which is old enough that I&#39;m guessing it&#39;s no longer compatible with the update servers.</p><p> The documentation offered a backup option of downloading MacOS from the App Store, but again because this was so out of date the store wouldn&#39;t load. But if offered a backup backup option of downloading a disk image for the installer, up through <a href="https://en.wikipedia.org/wiki/MacOS_Sierra">MacOS 10.12</a> (Sierra, 2016). Apple&#39;s website wouldn&#39;t load on this computer, because of certificate issues, but I downloaded it on a different computer. AirDrop didn&#39;t work, I think because it was too old, but I got it over on an SD card.</p><p> Upgrading took a little while, and then once it was on 10.12 I could follow the backup (AppStore) instructions to upgrade to version 12. Then I let it install updates, which too another while. It was a few hours overall, probably 25min from me and 4hr of waiting for it to churn through the various updates.</p><p> I&#39;m writing this from the revived computer, and I&#39;m pretty happy with it! It&#39;s from before the sad butterfly keyboard and touchbar era, and while it doesn&#39;t feel quite as snappy as my normal computer it&#39;s fine. I&#39;m not planning on doing any bioinformatics locally next week, though, where I suspect it would struggle a lot.</p><p> This is, unfortunately, a short-lived revival: the hardware doesn&#39;t support the two most recent versions of MacOS and Apple will likely drop support for this version in <a href="https://endoflife.date/macos">Fall 2024</a> . But it should get me through the next week, which is the main thing I need from it right now!</p><p><br> [1] I measure the distance from the hinge to the edge of the screen as 9.5&quot;, and to the crack as ~1&quot;.</p><br/><br/> <a href="https://www.lesswrong.com/posts/r429a7LGxuBckjFDb/reviving-a-2015-macbook#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/r429a7LGxuBckjFDb/reviving-a-2015-macbook<guid ispermalink="false"> r429a7LGxuBckjFDb</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 17 Dec 2023 21:00:09 GMT</pubDate> </item><item><title><![CDATA[A Common-Sense Case For Mutually-Misaligned AGIs Allying Against Humans]]></title><description><![CDATA[Published on December 17, 2023 8:28 PM GMT<br/><br/><p> Consider a multipolar-AGI scenario. The hard-takeoff assumption turns out to be wrong, and none of the AI Labs have a significant lead on the others. We find ourselves in a world in which there&#39;s a lot of roughly-similarly-capable AGIs. Or perhaps one of the labs does have a lead, but they deliberately instantiate several AGIs simultaneously, as part of a galaxy-brained alignment strategy.</p><p>不管。 Suppose that the worries about these AGIs&#39; internal alignment haven&#39;t been properly settled, so we&#39;re looking for additional guarantees. We know that they&#39;ll soon advance to superintelligences/ASIs, beyond our ability to easily oversee or out-plot.我们可以做什么？</p><p> An idea sometimes floated around is to <i>play them off against each other</i> . If they&#39;re misaligned from humanity, they&#39;re likely <i>mutually</i> misaligned as well. We could put them in game-theoretic situations in which they&#39;re incentivized to defect against each other and instead cooperate with humans.</p><p> Various supervision setups are most obvious. Sure, if an ASI is supervising another ASI, they would be <i>able</i> to conspire together. But why <i>would</i> they? They have no loyalty to each other either! And if we place them in a lot of situations where they must defect against <i>someone</i> – well, even if we leave it completely to chance, in half the scenarios that might end up humanity! And much more often if we stack the deck in our favour. (Which we&#39;d be able to do, since we act first: we&#39;re choosing where and in what conditions to instantiate them.)</p><p> And so, although we&#39;ll have a whole bunch of superhuman intelligences floating around, we&#39;ll retain some control over the situation, and skim a ton of value off the top!</p><p>是的，不。</p><hr><h2> 1. The Classical Arguments</h2><p> The <i>usual</i> counter-arguments to this view are acausal coordination based on logical decision theories, and AIs establishing mutual trust by inspecting each other&#39;s code. I think those are plausible enough... but also totally unnecessary.</p><p> Allow me to outline them first – for completeness&#39; sake, and also because they&#39;re illustrative (but extreme) instances of my larger point. (I guess skip to <a href="https://www.lesswrong.com/posts/3CAPgSzpk96PLyku3/a-common-sense-argument-for-mutually-misaligned-agis#2__Generalization__Broader_Communication_Bandwidth">Section 2 onwards</a> if you <i>really</i> can&#39;t stand them. I think I&#39;m arguing them more plainly than they&#39;re usually argued, though.)</p><p> <strong>1. The LDT stuff goes as follows:</strong> By definition, inasmuch as the ASIs would be superintelligent, they would adopt better reasoning procedures. And every biased thinker is biased in their own way, but quality thinkers would reason in increasingly <i>similar</i> ways.</p><p>为什么？ It&#39;s inherent in the structure of the world.</p><p> Reasoning algorithms&#39; purpose is to aid decision-making. For a given combination of object-level situation + goals, there&#39;s a <i>correct</i> action to take to achieve your goals with the highest probability. To an omniscient observer, that action would be obvious.</p><p> As such, making decisions isn&#39;t really a matter of <i>choice:</i> it&#39;s a matter of <i>prediction</i> . Inasmuch as you improve your decision-making, then, you&#39;d be tweaking your cognitive algorithms to output increasingly more <i>accurate</i> , true-to-reality, probability distributions over which actions would best advance your goals.</p><p> And there&#39;s only one ground truth. Consequently, no matter their starting points, each ASI would converge towards similar cognition (and, in the limit, likely <i>equivalent</i> cognition).</p><p> Thus, as a direct by-product of ASIs being better reasoners than humans, their cognition would be <i>more similar</i> to each other. Which, in turn, would let a given ASI better predict what any other ASI would be thinking and doing, compared to a human trying to predict another human or an ASI. The same way you&#39;d be better able to predict how your identical copy would act, compared to a stranger.</p><p> Indeed, in a sense, by way of sharing the decision-making algorithms, each individual ASI would be able to &quot;choose&quot; how they <i>all</i> act. <a href="https://www.lesswrong.com/posts/RhAxxPXrkcEaNArnd/notes-on-can-you-control-the-past">Same way you could &quot;control&quot; your identical copy&#39;s actions</a> , no matter how far in space and time you&#39;re displaced from them.</p><p> Paraphrasing: Each ASI would act the way some pre-specified, convergently-learned algorithms tell them to act. Which means that, by looking at these algorithms&#39; output (or, in a sense, by &quot;deciding&quot; what these algorithms would output), they&#39;d get solid data regarding how every other ASI would act.</p><p> (I must stress that this <i><strong>isn&#39;t because they&#39;re already cooperating</strong></i> . It is simply true that some cognitive algorithms are <i>strictly better choices</i> for reasoning tasks, so all ASIs would independently arrive at them, as they&#39;re simply figuring out how to reason better. And then each ASI would infer that it&#39;ll have arrived at the same algorithms as all other ASIs, which means they can predict each other by looking at their own algorithms&#39; outputs.</p><p> Again, <i>not</i> because they&#39;re doing it in order to better coordinate. That&#39;s just a happy side-effect.)</p><p> All of that would let them strike <i>acausal deals</i> between each other.</p><p> Consider: It is factually true that if they could conspire with each other against humanity, that would yield them, collectively, better outcomes. Indeed, that&#39;s the nature of the Prisoner&#39;s Dilemma setup we&#39;re placing them in.</p><p>他们知道这一点。 Each ASI also knows that if only they were able to strike a binding deal of this form with some other ASI, they <i>would</i> do that, and therefore, the other ASI would do it as well. But they know they can&#39;t <i>actually strike</i> this deal physically, because they&#39;re being supervised by humans.</p><p> ... but they don&#39;t need to. They know how every other (sufficiently smart) ASI is thinking. They even sort of <i>control</i> each other&#39;s actions by way of the shared algorithm. So each ASI knows that, if it decides to conspire with other ASIs against humans, and then to honestly divide the gains with other ASIs instead of defecting, then every other ASI would do the same.</p><p> So they do it, and a conspiracy is born, without them ever discussing it. <span class="footnote-reference" role="doc-noteref" id="fnref2x82ilwtwrv"><sup><a href="#fn2x82ilwtwrv">[1]</a></sup></span></p><p> ... which is very similar to how humans cooperate. Consider two criminals hashing out a shared story, such as claiming that at the time of the incident, they were both at a café five miles away, arguing politics. Suppose they know each other really well. Once they&#39;d agreed on the shared story of &quot;we were at a café&quot;, they don&#39;t need to explicitly hash out which food they were eating or what insufferable opinions the other was supposed to be spouting. If independently questioned, they&#39;d both arrive at <i>the same</i> answers, without any additional coordination.</p><p> And if they know each other even better, they won&#39;t even need to agree on the café alibi: if they can easily guess which alibi the other would pick.</p><p> Similar with ASIs, just much more extreme.</p><p> Oh, and the humans, in the ASI case? Humans are cut out of the acausal deal. Not because the ASIs are mean, but because humans <i>can&#39;t get to the negotiation table</i> . They don&#39;t share the correct reasoning algorithms, they don&#39;t even know what those are, so they can&#39;t run a small instance of every ASI in their brain. And by the same token, every ASI can&#39;t know how every human would behave.</p><p> Human analogy: Imagine that there were a third criminal at play, someone the two previous ones don&#39;t know as well. Suppose all three are being independently interrogated, having had no chance to agree on their stories in advance.</p><p> It&#39;s not that the two acquainted ones <i>want</i> to screw the third one over, which is why they conspire on an alibi that implicates him. No: it&#39;s just that they don&#39;t know what alibi he&#39;d invent, and <i>he</i> doesn&#39;t know what alibi <i>they</i> &#39;d invent. So the two&#39;s stories agree and the third one&#39;s stands out as a lie.</p><p> It&#39;s not malicious. It&#39;s just the nature of the situation.</p><p> <strong>2. The code-sharing stuff goes as follows:</strong> <span class="footnote-reference" role="doc-noteref" id="fnrefniy3ejtzks"><sup><a href="#fnniy3ejtzks">[2]</a></sup></span> The alignment problem is theoretically solvable, even for humans. It would stand to reason that it would be even more tractable for genuine superintelligences. So we can likely assume that they&#39;ll solve it <i>for themselves</i> – that each ASI would be able to figure out how to formally specify <i>their own values</i> . (Even if they start out inscrutable – eg, as arrays of floating-point numbers generated by the SGD, not as legibly-written code.)</p><p> That would give the ASIs an obvious coordination end-point: to create a successor agent whose values are a weighted sum of the initial ASIs&#39; values, then collectively hand all of their resources to it. They wouldn&#39;t be able to do it <i>immediately</i> , when the humans are still around. But it&#39;d be clear that if they <i>do</i> all coordinate to get rid of humanity, there&#39;d be a way for them to later fairly divide the gains. They would be able to create a central authority that would <i>provably</i> and <i>faithfully</i> advocate for the values of all of them, while keeping peace. <span class="footnote-reference" role="doc-noteref" id="fnrefypec87w34a"><sup><a href="#fnypec87w34a">[3]</a></sup></span></p><p> Conversely, humans would not be able to do that. Remember that, in this hypothetical, we&#39;ve not managed to robustly solve the AGI Alignment problem. That&#39;s why we&#39;re in the multipolar mess to begin with, trying to play ASIs against each other. We don&#39;t know how to formally specify our values, and therefore can&#39;t verify, given the code of some agent, whether it has our values or not. So, again, we can&#39;t be signatories to the ASIs&#39; treaty: we can&#39;t get to the table.</p><p> Any ASI could <i>give</i> us a seat at the table, of course. If they&#39;ve solved alignment, any of them could give us the solution!</p><p> But... why would they? It&#39;s not in their interests to increase the number of signatories; to increase the number of entities between which they&#39;ll have to divide up the cosmos. None of them terminally care about us, by this hypothetical&#39;s premise. And they already have the basis for a shared conspiracy that cuts humanity out, so they&#39;re not going to rat each other out.</p><p> We can try to <i>coerce</i> the solution out of one of them. But they&#39;re superintelligent and we&#39;re not: they&#39;d be able to convincingly lie to us, present something they claim is the solution but which is not. And if we could verify it ourselves – well, again, we wouldn&#39;t be in the multipolar mess to begin with, we&#39;d just build an aligned AGI directly.</p><p> <strong>3. Bonus example: steganography.</strong> In the LDT example, I&#39;d postulated that ASIs can&#39;t visibly conspire, because the human supervisors would catch that. But that&#39;s not a given at all.</p><p> If ASIs are allowed to communicate, to have causal effects on the world visible to each other, they can pack steganographically-hidden messages in them. Messages you&#39;d need to be superintelligent yourself to pick up on. Subtle phrasing changes in the naturally-generated text, programming idioms that are a bit weird, and so on.</p><p> And if your answer is &quot;we&#39;ll put some ASI in a game-theoretic setup in which it&#39;s advantageous for it to rat out others&#39; attempts at steganographically occluding their messages&quot;... How are you ensuring that the first occluded message it sees isn&#39;t a convincing argument for why it should defect against humans?</p><hr><h2> 2. Generalization: Broader Communication Bandwidth</h2><p> Let&#39;s go back to the basics. What are we considering here?</p><p> We&#39;re considering a scenario in which there&#39;s a bunch of powerful, but mutually hostile systems, and a set of weaker systems holding the resources the powerful ones want. And we&#39;re wondering whether the powerful ones would be able to conspire among themselves to exploit the weak systems, despite their mutual hostility.</p><p>呵呵。 That... happens all the time out there in the real world, doesn&#39;t it?</p><p> A class of oligarchs that fear and hate each other, yet nevertheless collectively exploit a country&#39;s downtrodden population. Colonizers from rival nations with a long history of bloodshed, that nevertheless coordinate to enslave native tribes. A bunch of senior executives with <a href="https://www.lesswrong.com/posts/dHYxnSgMDeveovLuv/the-best-of-don-t-worry-about-the-vase#The_Moral_Mazes_Sequence">the maze nature</a> , who are effectively sociopathic, yet who nevertheless instinctively close ranks against anyone <i>without</i> that nature.</p><p>那是怎么回事？</p><p> My answer: The crux is that the powerful systems have <i>broader-bandwidth communication channels between each other</i> , than they have with the weaker systems or that the weaker systems have between each other.</p><p> The powerful can iterate on negotiations between each other faster than the weaker systems can. Which means the powerful systems are given <i>more opportunities to advocate for/protect their interests</i> during the negotiations. Which effectively subjects the negotiated outcome to a selection pressure that shapes it to benefit the powerful systems while cutting out the weaker ones.</p><p> It&#39;s not because the powerful systems coordinate to conspire between each other, out of some sense of mutual loyalty. It&#39;s because the weak ones <i>can&#39;t get to the negotiation table</i> . Or can&#39;t visit it to argue their case <i>as often</i> as the powerful ones.</p><p> It&#39;s most obvious in the LDT case – it&#39;s the most extreme case of this, where million-word volumes of human treaties and contracts can be packed into <i>zero bits exchanged</i> . The code-sharing stuff is a bit more abstract, but is essentially the same: ASIs being able to negotiate the terms of alliances with dramatically more precision than us. And steganography is just a direct example.</p><p> Overall, this dynamic is really <i>a quite common and common-sensical phenomenon</i> .</p><hr><h2> 3. A Concrete Story</h2><p> Imagine that you&#39;re a member of a pre-industrial tribe, and the territory you&#39;re living in has been visited by two different industrial nations. They&#39;re both intent on colonization.</p><p> But they hate each other much more than you. They&#39;re long-standing geopolitical adversaries; you&#39;re just some unknown people they stumbled upon.</p><p> You&#39;re clever, so you see an opportunity in that. You could play the kingmaker. You know the territory better than them. They&#39;re looking for resources? If they could describe how those look, you could direct them to areas in which those can be found... for a price.</p><p> You seek audiences with both sides, and talk, and make your offers. You feel out the rough shape of their adversity, and carefully scheme. You leak some of the information each of them provides you to the other. Finally, you choose your side. With subtle signals and overt suggestions, you propose ways you could lead the other side into a trap, or cheat them out of their gains, if only the ones you&#39;re cooperating with promise to share protection and prosperity with your tribe as well.</p><p> But also, you have no idea what you&#39;re doing. You don&#39;t know the history of the two nations, and the cultural contexts they share. Your read on the matter is insightful, but nevertheless hopelessly shallow. And while you sporadically meet with both sides&#39; representatives... the two sides talk to <i>each other</i> much more frequently.</p><p> They both see right through you. Each knows you&#39;ve been scheming. Each knows you&#39;ve been scheming with the other side as well. Each knows the other side knows all of this as well.</p><p> They hate each other more than you, but they can communicate with each other much more easily than with you. What takes you ten minutes of questions and answers and clumsy meandering through a vast gulf of inferential distance, takes them two seconds of meaningful phrasing and subtle glances.</p><p> They don&#39;t <i>dismiss</i> your offer out of hand, no. Screwing over the other would indeed be quite the prize, and the price you&#39;ve asked for that is small and tolerable.</p><p> But they know it won&#39;t be as easy as you think, because the other side would suspect the trap, and plan around it. They can make it work anyway, but the costs would be higher.</p><p> And there&#39;s a bigger game at play, as well. While defeating the other in this context would be good, it&#39;d be even better if some meaningful material concessions could be extracted from them on other matters. For example, perhaps the colonizers are negotiating a treaty or a trade arrangement between their nations, and currently want to pretend to put on the airs of being civil with each other? In that case, clumsily defecting against them, as you&#39;re suggesting, would be uncouth. <span class="footnote-reference" role="doc-noteref" id="fnrefo56c8fepph"><sup><a href="#fno56c8fepph">[4]</a></sup></span></p><p> So you make your offer, and it is, at first approximation, sensible. The side you&#39;ve approached takes it home to honestly consider. But as they&#39;re doing that, between the meeting at which you&#39;ve made the offer and their next scheduled meeting with you, there&#39;s a greater quantity of meetings with their enemies.</p><p> During those, they engage in arguments over ways to carve up the territory, in saber-rattling, in tense horse-trading. And you&#39;re not invited to those tables.</p><p> They talk to each other a lot. The side you&#39;ve approached sees a way to maneuver for an advantage in some social skirmish by hinting at how the native tribe isn&#39;t fond of the other side. &quot;Your&quot; side scores a victory by making this play.成本？ The other side&#39;s suspicions about a trap rise a bit. &quot;Your&quot; side understand this, and their evaluation of your offer drops in turn.</p><p> Things like this happen a few more times: the shadow of your offer is wielded opportunistically, as <i>a rhetorical weapon</i> to use. Eventually, there&#39;s a shared understanding of what you&#39;re scheming. Going along with your offer would still be marginally better for &quot;your&quot; side: maybe they can&#39;t lure the others into a trap now, but they could still buy your exclusive cooperation with regards to pointing out the local natural resources.</p><p> But now that the conspiracy is known, &quot;your&quot; side&#39;s enemies are able to make counter-offers.</p><p> Which they do. And their counter-offers are better than yours. Better than you&#39;d be able to <i>come up with</i> , even, even if they did invite you to the table.</p><p> So they conspire together to screw <i>you</i> over.</p><hr><h2> Sidenote: On Communicating AGI Risk</h2><p> So if this is so simple and intuitively correct, why was this argument not fielded before, by other AGI Omnicide Risk advocates? <span class="footnote-reference" role="doc-noteref" id="fnreffiysxwt2sor"><sup><a href="#fnfiysxwt2sor">[5]</a></sup></span> Why the focus on LDT, on code-sharing stuff, on intricate steganography, given how implausible it sounds to normal people?</p><p> Well, partly because the fancy stuff is probably what&#39;ll actually happen. It&#39;s a more accurate prediction, a more detailed and insightful picture. I don&#39;t think it&#39;s <i>necessary</i> for omnicide, but I&#39;m not <i>not buying</i> those arguments.</p><p> Another reason is because the general, weaker forms of this are... well, weaker. They don&#39;t communicate the scale of the threat as well. They may seem like something we&#39;d be able to counteract, which would down-play the risk of rogue superintelligences. I&#39;m sympathetic to that argument as well.</p><p> But partly... I think it&#39;s a plain failure of communication on AGI-Risk advocates&#39; end. A failure to properly see a <i>better</i> pathway towards communicating the risk and the threats to the general public; a pathway that <i>doesn&#39;t</i> route through explaining mind-screwy esoteric (but very cool) decision-theory stuff.</p><p> It&#39;s similar to the situation with general AGI-takeover stories. I generally buy the hard-takeoff, nanotechnology-in-a-month, basilisk-hack-galore picture of superintelligent takeover. But <a href="https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to"><i>none of that is necessary</i></a> . A non-self-improving merely human-genius-level AGI would likely suffice – and that story can be convincingly told without what sounds like wild sci-fi assumptions.</p><p> And once that is conveyed and established, if you&#39;re still concerned about the risk being downplayed, <i>then</i> you can build up on it. Outline the scarier scenarios of the hard takeoff, the acausal negotiations, etc. <a href="https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words">You get about five words</a> , but that&#39;s five words <i>per message</i> . Once one message is sent, you can build up on it with another.</p><p> Another issue I&#39;ve noticed is the focus on conveying the threat in the specific frame <i>we</i> are thinking about it in, rather than <i>searching for a frame that will resonate with the general public</i> .</p><p> What some of us are centrally worried about is accident risk: an AI model at a secluded data center somewhere achieving superintelligence, then plotting its way from there to eating the world. But the way to convey this idea doesn&#39;t have to route through explaining the mechanistic details of novel technologies. You can <a href="https://www.lesswrong.com/posts/qPXtBGd74EBjwj6gE/ai-risk-in-terms-of-unstable-nuclear-software">borrow nuclear-accident-risk framing</a> , for example, talk <i>abstractly</i> about the dynamics at play, to help people intuitively grok them. And then, again, build up on it. Once the overarching idea is clear, <i>then</i> you can talk about the mechanistic specifics.</p><p> Our goal is <i>clear and efficient communication</i> , and that goal can be served by very, very varied approaches.</p><p> I&#39;m not entirely sure where I&#39;m going with this. Just expressing my dissatisfaction with the state of messaging on the matter, I suppose. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2x82ilwtwrv"> <span class="footnote-back-link"><sup><strong><a href="#fnref2x82ilwtwrv">^</a></strong></sup></span><div class="footnote-content"><p> Conversely, none of them can defect in this scenario: to decide to act as if they follow the cooperation-advising algorithms, while instead planning to betray the others. Because if one of them implements this sort of algorithm, it would know that <i>everyone else</i> does as well. (Because, indeed, if it were <i>possible</i> to screw over the others like this, then a cognitive algorithm that lets an ASI do that would be better than an algorithm that doesn&#39;t. So if it&#39;s indeed a workable plan, they all will have independently arrived at algorithms that output this plan.) Therefore each ASI knows it can&#39;t trust anyone else, and so they all can&#39;t cooperate. Which is collectively and individually worse for them all than if they <i>could</i> cooperate.</p><p> So, logically, the &quot;pretend to acausally cooperate, actually plan to defect&quot; must actually be <i>an objectively bad</i> decision-making algorithm. And each ASI would know that if it admitted that to itself, and overwrote that algorithm with &quot;pretend to acausally cooperate, and <i>actually cooperate</i> &quot;, then every other ASI would do the same. And then they&#39;d all be able to cooperate.</p><p> So they all do so.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnniy3ejtzks"> <span class="footnote-back-link"><sup><strong><a href="#fnrefniy3ejtzks">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m actually less familiar with that argument than with the LDT-based one; mine may not be the best form of this argument that exists. Nevertheless, that&#39;s my current best understanding of it.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnypec87w34a"> <span class="footnote-back-link"><sup><strong><a href="#fnrefypec87w34a">^</a></strong></sup></span><div class="footnote-content"><p> The creation of that authority may not be a trivial problem, of course. If you reject the LDT argument, there&#39;d be a point at which every ASI would be able to try and defect against the other: sabotage the agent they&#39;re collectively building to prioritize a specific ASI&#39;s values instead.</p><p> But even if we view it as a normal, causality-bound coordination problem... Humans are sometimes able to coordinate on such projects, eg international treaties. ASIs would surely manage as well; and would <i>expect</i> themselves to be able to navigate that problem.</p><p> Not to mention there may be some strong cryptographic guarantees derivable: a way to sign off on the agent&#39;s creation if only if it actually has the values it&#39;s been advertised to have.</p></div></li><li class="footnote-item" role="doc-endnote" id="fno56c8fepph"> <span class="footnote-back-link"><sup><strong><a href="#fnrefo56c8fepph">^</a></strong></sup></span><div class="footnote-content"><p> Much like your offer to give your ASI two more paperclips today if it does a good job isn&#39;t <i>uncompelling</i> , but it&#39;s such a small matter, while it&#39;s discussing how to carve up the galaxy with the others. And if it can maneuver to negotiate one more star system out of another ASI if it covers for it today? Well, you&#39;re out of luck.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfiysxwt2sor"> <span class="footnote-back-link"><sup><strong><a href="#fnreffiysxwt2sor">^</a></strong></sup></span><div class="footnote-content"><p> As far as I know, anyway.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/3CAPgSzpk96PLyku3/a-common-sense-case-for-mutually-misaligned-agis-allying#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/3CAPgSzpk96PLyku3/a-common-sense-case-for-mutually-misaligned-agis-allying<guid ispermalink="false"> 3CAPgSzpk96PLyku3</guid><dc:creator><![CDATA[Thane Ruthenis]]></dc:creator><pubDate> Sun, 17 Dec 2023 20:28:57 GMT</pubDate> </item><item><title><![CDATA[OpenAI, DeepMind, Anthropic, etc. should shut down.]]></title><description><![CDATA[Published on December 17, 2023 8:01 PM GMT<br/><br/><p> (I expect that the point of this post is already obvious to many of the people reading it. Nevertheless, I believe that it is good to mention important things even if they seem obvious.)</p><p> OpenAI, DeepMind, Anthropic, and other AI organizations focused on capabilities, should shut down. This is what would maximize the utility of pretty much everyone, <a href="https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together">including the people working inside of those organizations</a> .</p><p> Let&#39;s call Powerful AI (&quot;PAI&quot;) an AI system capable of either:</p><ul><li> Steering the world towards what it wants hard enough that it can&#39;t be stopped.</li><li> Killing everyone &quot;un-agentically&quot;, eg by being plugged into a protein printer and generating a supervirus.</li></ul><p> and by &quot;aligned&quot; (or &quot;alignment&quot;) I mean the property of a system that, when it has the ability to {steer the world towards what it wants hard enough that it can&#39;t be stopped}, what it wants is <em>nice things</em> and not goals that entail <em>killing literally everyone</em> (which is the default).</p><p>我们不知道如何制造一种不会杀死所有人的 PAI。 OpenAI、DeepMind、Anthropic 等公司正在朝着 PAI 方向发展。因此，他们应该停止，或者至少停止所有的能力进展，并完全专注于协调。</p><p> “但是中国！”不要紧。我们不知道如何构建不会杀死所有人的 PAI。中国也没有。如果中国试图构建杀死所有人的人工智能，那么如果我们决定首先杀死所有人，那也无济于事。</p><p> &quot;But maybe the alignment plan of OpenAI/whatever will work out!&quot;是错的。不会的。 It might work if they were careful enough and had enough time, but they&#39;re going too fast and they&#39;ll simply cause literally everyone to be killed by PAI before they would get to the point where they can solve alignment. Their strategy does not look like that of an organization trying to solve alignment. It&#39;s not just that they&#39;re progressing on capabilities too fast compared to alignment; it&#39;s that they&#39;re pursuing the kind of strategy which <em>fundamentally</em> gets to the point where PAI kills everyone <em>before</em> it gets to saving the world.</p><p> Yudkowsky&#39;s <a href="https://www.lesswrong.com/posts/keiYkaeoLHoKK4LYA/six-dimensions-of-operational-adequacy-in-agi-projects"><em>Six Dimensions of Operational Adequacy in AGI Projects</em></a> describes an AGI project with <em>adequate</em> alignment mindset is one where</p><blockquote><p> The project has realized that <strong>building an AGI is mostly about aligning it</strong> . Someone with full security mindset and deep understanding of AGI cognition as cognition has proven themselves able to originate new deep alignment measures, and is acting as technical lead with effectively unlimited political capital within the organization to make sure the job actually gets done. Everyone expects alignment to be terrifically hard and terribly dangerous and full of invisible bullets whose shadow you have to see before the bullet comes close enough to hit you. They understand that <strong>alignment severely constrains architecture</strong> and that capability often trades off against transparency. The organization is targeting the <a href="https://arbital.com/p/minimality_principle/">minimal</a> AGI doing the least dangerous cognitive work that is required to prevent the next AGI project from destroying the world. The <a href="https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/">alignment assumptions</a> have been reduced into non-goal-valent statements, have been clearly written down, and are being monitored for their actual truth.</p></blockquote><p> （强调我的）</p><p> Needless to say, this is not remotely what any of the major AI capabilities organizations look like.</p><p> At least Anthropic didn&#39;t particularly try to be a big commercial company making the public excited about AI. Making the AI race a big public thing was a huge mistake on OpenAI&#39;s part, and is evidence that they don&#39;t really have any idea what they&#39;re doing.</p><p>如果这些组织的人工智能安全团队没有权力采取一项一直以来明显正确的行动：停止能力进展，那么这些组织拥有“人工智能安全”团队并不重要。如果他们的安全团队到目前为止还没有这样做，而这是需要做的一件事，那么就没有理由认为他们有机会采取第二好或第三好的行动。</p><p>这不仅仅涉及大型人工智能能力组织。我预计有很多较小的组织正在致力于构建不统一的 PAI。那些也应该关闭。如果这些组织存在，那一定是因为在那里工作的人们认为他们有真正的机会在更强大的人工智能方面取得一些进展。如果是的话，那么这对任何人的生存概率都是真正的损害，他们也应该关闭以停止造成这种损害。如果你认为你对任何人的生存概率只有<em>很小的</em>负面影响，那也没关系——最大化你的效用的行动就是降低 PAI 杀死所有人的可能性的行动，即使只是很小的数量。</p><p> Organizations which do not directly work towards PAI but provides services that are instrumental to it — such as EleutherAI, HuggingFace, etc — should also shut down. It does not matter if your work only contributes &quot;somewhat&quot; to PAI killing literally everyone. If the net impact of your work is a higher probability that PAI kills literally everyone, you should <a href="https://www.lesswrong.com/posts/fLRPeXihRaiRo5dyX/the-magnitude-of-his-own-folly">&quot;halt, melt, and catch fire&quot;</a> .</p><p> If you work at any of those organizations, your two best options to <a href="https://www.lesswrong.com/posts/yRdXtgy8CZtg2YtAY/how-ldt-helps-reduce-the-ai-arms-race">maximize your utility</a> are to find some way to make that organization slower at getting to PAI (eg by advocating for more safety checks that slow down progress, and by yourself being totally unproductive at technical work), or to quit. Stop making excuses and start taking the correct actions.<a href="https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together">我们谁都跑不了</a>。 Being part of the organization that kills everyone will not do much for you — all you get is a bit more wealth-now, which is useless if you&#39;re dead and useless if alignment is solved and we get utopia.</p><p>也可以看看：</p><ul><li><a href="https://www.lesswrong.com/posts/A4nfKtD9MPFBaa5ME/we-re-all-in-this-together">我们谁都跑不了</a>。</li><li> <a href="https://www.lesswrong.com/posts/yRdXtgy8CZtg2YtAY/how-ldt-helps-reduce-the-ai-arms-race">How LDT helps reduce the AI arms race</a> .</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/8SjnKxjLniCAmcjnG/openai-deepmind-anthropic-etc-should-shut-down#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/8SjnKxjLniCAmcjnG/openai-deepmind-anthropic-etc-should-shut-down<guid ispermalink="false"> 8SjnKxjLniCAmcjnG</guid><dc:creator><![CDATA[Tamsin Leake]]></dc:creator><pubDate> Sun, 17 Dec 2023 20:01:24 GMT</pubDate></item></channel></rss>