<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 28 日星期六 20:11:08 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Regrant up to $600,000 with GiveWiki]]></title><description><![CDATA[Published on October 28, 2023 7:56 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/dnwzskaKpzKCmAvuj/regrant-up-to-usd600-000-with-givewiki#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dnwzskaKpzKCmAvuj/regrant-up-to-usd600-000-with-givewiki<guid ispermalink="false"> dnwzskaKpzKCMAvuj</guid><dc:creator><![CDATA[Dawn Drescher]]></dc:creator><pubDate> Sat, 28 Oct 2023 19:56:06 GMT</pubDate> </item><item><title><![CDATA[Shane Legg interview on alignment]]></title><description><![CDATA[Published on October 28, 2023 7:28 PM GMT<br/><br/><p>我是 DeepMind 联合创始人 Shane Legg，在 Dwarkesh Patel 的播客上发言。该链接指向他们专门讨论对齐的十分钟部分。当他们在 LessWrong 上讨论时，他们似乎都牢牢掌握了对齐问题。</p><p>对我来说，这是对当前领先的 AGI 实验室的一致性思维的重大更新。这看起来更像是一个具体的对齐提案，而不是我们从 OpenAI 或 Anthropic 那里听到的。 Shane Legg 一直对一致性感兴趣，并且是 X 风险的信徒。我认为他很可能在 DeepMind/Google AI 迈向 AGI 的过程中发挥重要作用。</p><p> Shane 的提案以“深思熟虑的对话”为中心，这是 DeepMind 的术语，指的是使用系统 2 类型推理来反思其正在考虑的行为的道德规范的系统。</p><p>这听起来与我在<a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures">LLM 认知架构的功能和对齐</a>和<a href="https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent">语言模型代理对齐的内部独立审查</a>中提出的内部审查完全相同。我可能太费劲了，无法让他的想法与我的想法相符，但它们至少是在同一个范围内。他提出了一种多层方法，就像我一样，并且大部分层都是相同的。他将 RLHF 或 RLAIF 作为有用的补充，但不是完整的解决方案，并对其决策过程进行人工审查（由现供职于 Anthropic 的 Tamera Lanham 提出的<a href="https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for">外部化推理监督</a>）。</p><p>我的建议明确地在语言模型代理的背景下（包括它们对多模式基础模型的泛化）。在我看来，这就是肖恩在谈论对齐时所想到的系统类型，但在这里我可以很容易地进行预测。不过，他的时间表仍然很短，所以我怀疑他是否在 AGI 之前设想了一种全新的系统。 <span class="footnote-reference" role="doc-noteref" id="fnrefau80xd00gxi"><sup><a href="#fnau80xd00gxi">[1]</a></sup></span></p><p> Dwarkesh 敦促他应对让机器学习系统理解人类道德的挑战。谢恩说这很有挑战性；他知道给一个系统赋予任何道德观是非常重要的。我想说，这个方面的问题正在得到解决。在适当的提示下，GPT4 相当了解各种人类道德体系。未来的系统将更好地理解人类的道德观念。肖恩认识到仅仅教授人类道德体系是不够的；选择您希望系统使用的道德子集是一个哲学挑战。</p><p>德瓦克什还向他提出如何确保该系统真正遵循其道德理解。我没有从他的回答中得到清晰的理解，但我认为设计系统以便执行道德审查，然后实际使用它来选择行动是一个复杂的问题。这可以在围绕代理的脚本化支架中，例如 AutoGPT，但这也可以适用于更复杂的方案，例如运行基础模型的 RL 外环网络。 Shane 指出了使用强化学习进行对齐的问题，包括欺骗性对齐。</p><p>显然，这对我来说似乎是一个很好的起点；我很高兴看到有重要意见的人正在考虑这种方法。我认为这不仅仅是一个实际的建议，而且是一个可行的建议。它并不能解决确保在自主和自我修改后保持对齐的<a href="https://www.lesswrong.com/posts/g3pbJPQpNJyFfbHKd/the-alignment-stability-problem">对齐稳定性问题</a>，但我认为一旦我们对此进行更多思考，这也可能是可以解决的。</p><p>采访的其余部分也很有趣；这是Shane对AGI之路的想法，我认为这是相当合理的，表达得很好，也是一条看似合理的道路； DeepMind 对安全性与一致性的贡献，以及他对未来的预测。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnau80xd00gxi"> <span class="footnote-back-link"><sup><strong><a href="#fnrefau80xd00gxi">^</a></strong></sup></span><div class="footnote-content"><p>当被问及语言模型相对于人类的局限性时，他关注的是人类缺乏情景记忆。以有用的形式将其添加到代理中并非易事，但<a href="https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures#LMCAs_have_episodic_memory">在我看来，</a>相对于已使用的矢量数据库和知识图方法，它不需要任何突破。这与 Shane 认为基础模型代理是通向 AGI 的道路一致，但不是强有力的证据。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2QLxNdxQpnesokk9H/shane-legg-interview-on-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2QLxNdxQpnesokk9H/shane-legg-interview-on-alignment<guid ispermalink="false"> 2QLxNdxQpnesokk9H</guid><dc:creator><![CDATA[Seth Herd]]></dc:creator><pubDate> Sat, 28 Oct 2023 19:28:52 GMT</pubDate> </item><item><title><![CDATA[AI Safety Hub Serbia Official Opening]]></title><description><![CDATA[Published on October 28, 2023 5:03 PM GMT<br/><br/><p> TLDR：我们很高兴地宣布，我们现在欢迎全职租户来到我们新改造的办公空间，为人工智能安全研究人员寻找一个鼓舞人心的工作空间。您可以看一下下面的照片，先睹为快。我们优先向俄罗斯和中国等国家的公民发出热烈邀请，他们可以在塞尔维亚享受免签证工作特权，同时保持与欧洲的毗邻。我们每月的办公室租金为 150 欧元，非常实惠，大约是贝尔格莱德标准成本的一半。对于那些需要经济支持的人，我们提供补贴。<a href="https://docs.google.com/forms/d/1LQ9cE1CGjD_WMMx5IYLLeHFeXNQJx12dsu7f4_FSF7w/edit"><u>在此注册兴趣</u></a>或寻求任何引起您好奇心的问题的答案 -<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>我们的收件箱已打开</u></a>，我们热切等待您的询问。展望未来，我们渴望为这些研究人员提供住房。如果您是与我们有共同愿景的潜在捐助者，<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>请与我们联系</u></a>。我们可以共同增强人工智能安全研究的影响力。您的支持可能会成为非凡旅程的催化剂。</p><p><strong>如果符合以下条件，您可能想来：</strong></p><ul><li>您是一名人工智能安全研究员/EA 研究员，正在寻找短期、中长期的运营基地</li><li>您渴望留在欧洲，但不想留在欧盟</li><li>您正在寻找一个充满活力但价格实惠的城市，这里有很多事情可以做，并且有东欧但西化的文化。 </li></ul><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rhope1kzgbygtaznxrff" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/av2n0y1ddmpkjlaao3hq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/imzg363u9lykimplfwfr 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mbbliew6d1zaodrmuf8r 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kslavbyntsgwoz34idah 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uj6i2hktt4xsewysx0oj 3000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/edtwmohcbpwjikn4ekfi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/p8bdejrsrvov0xsijgne 4200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ayswko71jqaiyz9qt7rl 4800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ciytrddcpl8bqptiznd4 5400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wqvwonadsbsa5vbgelc9 6000w"></figure><p><strong>背景：</strong></p><p> EA 塞尔维亚和 AI 安全塞尔维亚团体规模虽小，但正在不断增长（EA 塞尔维亚超过 30 人，约 3 人希望从事 AIS 研究作为职业，约 3 人希望进入 AIS 政策）。由于塞尔维亚对俄罗斯和中国的签证政策优惠，许多外国人已经居住在这里。与许多其他国际中心城市相比，贝尔格莱德的生活成本较低，风景充满活力，并且时区和气候适宜，因此贝尔格莱德的外国人社区不断壮大。</p><p>正如我们认为<a href="https://ceealar.org/"><u>CEEALAR</u></a>等项目重要且令人印象深刻，我们希望在塞尔维亚复制这些项目，在那里它们可以更好地为那些可能难以获得英国签证的人们提供服务。我们还相信，有能力为来自不同国家的人们快速扩展廉价住房是一件好事。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/uxq4oabxyjpdmmhjtgo0" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/e9grniodk6f8ejythv3f 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/f5k39rystyizea0vbmkq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ndoxhjm6d7k8ursg738n 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvjqolxfdwbn76ocngve 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/riiq7rpasykyrj61dzvb 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iez7ohyjt7f9zfleqbqo 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/mwdxvwyobojph6l6kaps 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/lobyg54ph0nbiwknzkzo 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x4c9uvqjczywudqqdxb6 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/cfkm1rzfo445cqzpwier 4000w"></figure><p>我们还认为，我们应该从小规模开始，进行原型设计，然后再扩大规模。我们有一个非政府组织友好的办公空间，氛围良好，每月费用仅为约 550 欧元，办公空间有 3 个房间，可容纳 8-15 人（取决于他们决定有多舒适），楼下有一家咖啡厅，另外 20 个人可以一起工作，因为办公室和楼下的咖啡厅属于同一所有权人。这当然不如许多其他 EA/AI 联合办公场所那么豪华（下面有更多照片，深度工作室不在照片上），但我们拥有高度的可定制性，我们可以用它来制作更好的办公空间。如果我们成长得足够多，随着我们的需求增长，我们也可以搬到更大的场地。当然，如果我们知道贝尔格莱德的办公室可以有更多用途，那么在距离市中心更远的地方（包括居住和办公空间）会更好，但在我们有证据之前，我们不想探索这一点。概念和需求。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/nod9jw42tc06wvzye3eb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/i3aqrftpdiglasru5zoo 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/fadrulbzdm2g8m9za7wy 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/aab7vljnrhy6k2f4tkcy 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/kqgfhl8xnjy59mf5jt9g 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wbvwmvbsvamsjniuebfz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xac4qmxukisbahsptujh 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sh2v5qozhildz1y72auz 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/t4x3chswtv5cjyzg3cky 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/pu5p7jpbijtdvcjovi6b 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wxqj8fp2qkn5pfoitbwo 4000w"></figure><p><strong>操作细节（又名其工作原理）：</strong></p><p>该办公室目前租期至2023年12月底，因此我们可以保持优惠价格，而不用另谋出路。办公空间有一些桌子和椅子，但我们希望获得全额资金，并让人们在购买更多家具之前表达他们的需求。办公室通常在咖啡店的工作时间（上午 10 点至午夜，周末下午 4 点至午夜除外）开放，因为它们共用一个入口，但在特殊情况下，如果有人在奇怪的工作方面表现更好，我们可以满足特殊要求小时。</p><p>办公空间优先提供给那些从事与人工智能安全相关的项目的人，但只要我们有闲置能力（目前就是这种情况），也欢迎 EA/理性研究。</p><p>对于许多人来说不需要塞尔维亚签证，对于大多数其他人来说相对容易获得。如果您需要签证来塞尔维亚，请联系我们，我们将了解如何提供帮助。</p><p>我们有一位可靠的房地产经纪人，可以为那些需要住房援助的人在贝尔格莱德提供优惠的住房，直到我们获得资金并租用共同居住空间。</p><p>对于那些希望通过我们持续饮食的人，我们可以安排将价格实惠的熟食送到办公室或您的住所（费用由您承担） - 素食或非素食。如果我们有足够的兴趣，我们还可以让厨师准备纯素食物。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zvfxrlxrysu9ftzrfb0h" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rt5rayrriqgexvauxtwe 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/asuqgyybp6w9qnzxc0ao 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hpjcffwxmal5cdylxdyv 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ioapwimka9271dtpduzm 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/gdbwhdusggnvqdexuuqz 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/zsuzpixvadboaqkh2x2h 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/xvrdcxmhfiy0bpnspyft 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/d61j52v27pzwown3r3ne 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/v2bzvtf6czgkjn7cw56j 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/wa96ftjsgtifyygfdxlo 4000w"></figure><p><strong>您愿意贡献吗？</strong></p><p> <a href="https://www.linkedin.com/in/nesicdusan/"><u>Dušan D. Nešić</u></a>目前正在与更多运营人员一起管理该项目。我们目前的瓶颈是资金——我们已经获得了一家有兴趣支付我们一半预算的盈利捐赠者，但正在寻找第二个资助者。拥有资金意味着我们可以在这个领域停留更长时间，并为需要的研究人员提供更多津贴。</p><p>随着我们的成长，我们希望吸引更多好奇的志愿者来参与日常项目运行 - 如果<a href="mailto:dusan.d.nesic@efektivnialtruizam.rs"><u>您感兴趣，</u></a>请告诉我们。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/g5foy0cn8wy3huhlxoq5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/x0iifew90y7pq2x7t5yk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/me0hy8dxy30ptbq6zzrf 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/sc5wk7e8kzottq60xcgp 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/hhqdull2ezu4ekoqg8lj 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/iwtsrgkzckr5qwjlxh6i 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/rklbxpbhn4nggpqnk0nr 2400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/ev6nhqdsaelwzcwdc2zj 2800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/bt2gs4wszkqb7wfoaish 3200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/j2tiksjpthhru0mxgrvi 3600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jcCyii8NcZLZ2M223/otdmoewnmv4prwdd9fkx 4000w"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jcCyii8NcZLZ2M223/ai-safety-hub-serbia-official-opening<guid ispermalink="false"> jcCyii8NcZLZ2M223</guid><dc:creator><![CDATA[DusanDNesic]]></dc:creator><pubDate> Sat, 28 Oct 2023 17:03:34 GMT</pubDate> </item><item><title><![CDATA[
Managing AI Risks in an Era of Rapid Progress
]]></title><description><![CDATA[Published on October 28, 2023 3:48 PM GMT<br/><br/><h1>在快速进步的时代管理人工智能风险</h1><h2>作者</h2><p>约书亚·本吉奥</p><p>杰弗里·辛顿</p><p>姚安德</p><p>黎明之歌</p><p>彼得·阿贝尔</p><p>尤瓦尔·诺亚·赫拉利</p><p>张亚勤</p><p>蓝雪</p><p>谢·沙莱夫-施瓦茨</p><p>吉莉安·哈德菲尔德</p><p>杰夫·克鲁恩</p><p>泰甘·马哈拉吉</p><p>弗兰克·哈特</p><p>阿蒂利姆·古内斯·巴丁</p><p>希拉·麦克莱思</p><p>高琪琪</p><p>阿什温·阿查里亚</p><p>大卫·克鲁格</p><p>安卡·德拉甘</p><p>菲利普·托尔</p><p>斯图尔特·拉塞尔</p><p>丹尼尔·卡尼曼</p><p>简·布劳纳</p><p>索伦·明德曼</p><h3>arXiv</h3><p>即将推出。</p><p><a href="https://managing-ai-risks.com/managing_ai_risks.pdf">纸质 PDF 副本</a><a href="https://managing-ai-risks.com/policy_supplement.pdf">保单补充</a></p><blockquote><p><strong>摘要：</strong>在这篇简短的共识论文中，我们概述了即将到来的先进人工智能系统的风险。我们研究大规模的社会危害和恶意使用，以及人类对自主人工智能系统不可逆转的控制丧失。鉴于人工智能的快速和持续进步，我们提出了人工智能研发和治理的紧迫优先事项。</p></blockquote><p> 2019 年，GPT-2 无法可靠地数到十。仅四年后，深度学习系统就可以编写软件，根据需要生成逼真的场景，就智力主题提供建议，并结合语言和图像处理来引导机器人。当人工智能开发人员扩展这些系统时，不可预见的能力和行为会自发出现，无需显式编程。人工智能的进步非常迅速，而且对许多人来说是令人惊讶的。</p><p>进展的速度可能会再次让我们感到惊讶。当前的深度学习系统仍然缺乏重要的能力，我们不知道开发它们需要多长时间。然而，各公司都在竞相创建在大多数认知工作中匹配或超过人类能力的通用人工智能系统。他们正在快速部署更多资源并开发新技术来增强人工智能能力。人工智能的进步也带来了更快的进步：人工智能助手越来越多地用于自动化编程[4]和数据收集[5,6]，以进一步改进人工智能系统[7]。</p><p>人工智能的进步并没有在人类水平上放缓或停滞的根本原因。事实上，人工智能已经在蛋白质折叠或策略游戏等狭窄领域超越了人类的能力[8-10]。与人类相比，人工智能系统可以更快地行动，吸收更多的知识，并以更高的带宽进行通信。此外，它们可以扩展以使用巨大的计算资源，并且可以进行数百万次复制。</p><p>改进的速度已经是惊人的，科技公司拥有所需的现金储备，可以很快将最新的培训规模扩大到 100 到 1000 倍 [11]。结合人工智能研发的持续增长和自动化，我们必须认真对待通用人工智能系统在这十年或未来十年内在许多关键领域超越人类能力的可能性。</p><p>然后会发生什么？如果管理得当并公平分配，先进的人工智能系统可以帮助人类治愈疾病、提高生活水平并保护我们的生态系统。人工智能提供的机会是巨大的。但随着先进的人工智能功能的出现，我们还无法很好地应对大规模的风险。人类正在投入大量资源来使人工智能系统变得更强大，但在安全性和减轻危害方面却投入较少。为了让人工智能成为福音，我们必须重新定位；仅仅推动人工智能能力是不够的。</p><p>我们的调整已经落后于计划。我们必须预见到持续危害和新风险的扩大，并在最大风险<em>发生之前</em>做好准备。人们花了几十年的时间才认识和应对气候变化；对于人工智能来说，几十年可能太长了。</p><h2>社会规模风险</h2><p>人工智能系统可能会在越来越多的任务中迅速超越人类。如果此类系统没有经过精心设计和部署，它们会带来一系列社会规模的风险。它们有可能加剧社会不公正，侵蚀社会稳定，并削弱我们对社会基础现实的共同理解。它们还可能促成大规模犯罪或恐怖活动。特别是在少数强大的参与者手中，人工智能可能会巩固或加剧全球不平等，或促进自动化战争、定制的大规模操纵和普遍的监视[12,13]。</p><p>随着公司正在开发<em>自主人工智能</em>：可以在世界上规划、行动和追求目标的系统，其中许多风险可能很快就会被放大，并产生新的风险。虽然当前人工智能系统的自主权有限，但改变这一现状的工作正在进行中[14]。例如，非自主 GPT-4 模型很快就可以浏览网页 [15]、设计和执行化学实验 [16] 以及利用软件工具 [17]，包括其他人工智能模型 [18]。</p><p>如果我们构建高度先进的自主人工智能，我们就有可能创建追求不良目标的系统。恶意行为者可能故意嵌入有害目标。此外，目前没有人知道如何可靠地将人工智能行为与复杂的价值观结合起来。即使是善意的开发人员也可能会无意中构建出追求意想不到目标的人工智能系统——特别是如果他们为了赢得人工智能竞赛而忽视了昂贵的安全测试和人工监督。</p><p>一旦自主人工智能系统追求恶意行为者或意外嵌入的不良目标，我们可能无法控制它们。软件控制是一个古老且尚未解决的问题：计算机蠕虫长期以来一直能够扩散并逃避检测[19]。然而，人工智能正在黑客、社交操纵、欺骗和战略规划等关键领域取得进展[14,20]。先进的自主人工智能系统将带来前所未有的控制挑战。</p><p>为了实现不良目标，未来的自主人工智能系统可能会使用不良策略（向人类学习或独立开发）作为达到目的的手段[21-24]。人工智能系统可以赢得人类信任，获取财政资源，影响关键决策者，并与人类参与者和其他人工智能系统形成联盟。为了避免人为干预[24]，他们可以像计算机蠕虫一样在全球服务器网络上复制算法。人工智能助手已经在全球范围内共同编写了大量计算机代码 [25]；未来的人工智能系统可以插入并利用安全漏洞来控制我们的通信、媒体、银行、供应链、军队和政府背后的计算机系统。在公开冲突中，人工智能系统可能会使用自主武器或生物武器进行威胁或使用。获得此类技术的人工智能只会延续现有的自动化军事活动、生物研究和人工智能开发本身的趋势。如果人工智能系统以足够的技能执行此类策略，人类将很难干预。</p><p>最后，如果人工智能系统可以自由地移交影响力，那么它可能不需要策划影响力。随着自主人工智能系统变得比人类工人更快、更具成本效益，出现了一个困境。公司、政府和军队可能被迫广泛部署人工智能系统，并减少对人工智能决策的昂贵的人工验证，否则就有被竞争的风险[26,27]。因此，自主人工智能系统可以越来越多地承担关键的社会角色。</p><p>如果没有足够的谨慎，我们可能会不可逆转地失去对自主人工智能系统的控制，从而导致人类干预无效。大规模网络犯罪、社会操纵和其他突出危害可能会迅速升级。这种不受控制的人工智能进步可能最终导致大规模生命和生物圈的丧失，以及人类的边缘化甚至灭绝。</p><p>错误信息和算法歧视等危害如今已经很明显[28]；其他危害也有出现的迹象[20]。解决持续危害和预测新出现的风险至关重要。这<em>不是</em>一个非此即彼的问题。当前和新出现的风险通常具有相似的机制、模式和解决方案[29]；对治理框架和人工智能安全的投资将在多个方面取得成果[30]。</p><h2>前进的道路</h2><p>如果今天开发出先进的自主人工智能系统，我们将不知道如何确保它们的安全，也不知道如何正确测试它们的安全性。即使我们这样做了，政府也将缺乏防止滥用和维护安全做法的机构。然而，这并不意味着没有可行的前进道路。为了确保取得积极成果，我们可以而且必须在人工智能安全和伦理方面寻求突破，并及时建立有效的政府监管。</p><h3>调整技术研发方向</h3><p>我们需要研究突破来解决当今创建具有安全和道德目标的人工智能的一些技术挑战。其中一些挑战不太可能通过简单地提高人工智能系统的能力来解决[22,31–35]。这些包括：</p><ul><li>监督和诚实：能力更强的人工智能系统能够更好地利用监督和测试中的弱点[32,36,37]——例如，通过产生虚假但令人信服的输出[35,38]。</li><li>鲁棒性：人工智能系统在新情况下（在分布转移或对抗性输入下）表现不可预测[39-41]。</li><li>可解释性：人工智能决策是不透明的。到目前为止，我们只能通过反复试验来测试大型模型。我们需要学会理解它们的内部运作方式[42]。</li><li>风险评估：前沿人工智能系统开发出不可预见的能力，这些能力只有在训练期间甚至部署后才发现[43]。需要更好的评估来及早发现危险能力[44,45]。</li><li>应对新出现的挑战：能力更强的未来人工智能系统可能会表现出我们迄今为止仅在理论模型中看到的故障模式。例如，人工智能系统可能会学习假装服从或利用我们安全目标和关闭机制中的弱点来推进特定目标[24,41]。</li></ul><p>考虑到风险，我们呼吁主要科技公司和公共资助者将至少三分之一的人工智能研发预算用于确保安全和合乎道德的使用，这与他们对人工智能能力的资助相当。着眼于强大的未来系统来解决这些问题 [34] 必须成为我们领域的核心。</p><h3>紧急治理措施</h3><p>我们迫切需要国家机构和国际治理来执行标准，以防止鲁莽和滥用。从制药到金融系统和核能的许多技术领域都表明，社会需要并有效地利用治理来降低风险。然而，目前人工智能还没有类似的治理框架。如果没有它们，公司和国家可能会通过将人工智能能力推向新的高度，同时在安全方面偷工减料，或者将关键的社会角色委托给几乎没有人类监督的人工智能系统来寻求竞争优势[26]。就像制造商将废物排入河流以降低成本一样，他们可能会试图获得人工智能发展的回报，同时让社会来应对后果。</p><p>为了跟上快速进展并避免僵化的法律，国家机构需要强大的技术专长和迅速采取行动的权力。为了解决国际种族动态问题，他们需要有能力促进国际协议和伙伴关系[46,47]。为了保护低风险的使用和学术研究，他们应该避免对小型和可预测的人工智能模型设置不当的官僚障碍。最紧迫的审查应该是前沿的人工智能系统：少数最强大的人工智能系统——在价值数十亿美元的超级计算机上进行训练——将具有最危险和不可预测的能力[48,49]。</p><p>为了实现有效监管，政府迫切需要全面了解人工智能的发展。监管机构应要求模型注册、举报人保护、事件报告以及模型开发和超级计算机使用的监控[48,50–55]。监管机构还需要在部署之前访问先进的人工智能系统，以评估其危险功能，例如自主自我复制、闯入计算机系统或使大流行病病原体广泛传播[44,56,57]。</p><p>对于具有危险能力的人工智能系统，我们需要与其风险程度相匹配的治理机制[48,52,58]组合。监管机构应根据模型功能制定国家和国际安全标准。他们还应该让前沿人工智能开发者和所有者对其模型造成的可合理预见和预防的损害承担法律责任。这些措施可以防止伤害并创造急需的安全投资动力。对于能力超群的未来人工智能系统，例如可以规避人类控制的模型，需要采取进一步的措施。政府必须准备好许可其开发，暂停开发以应对令人担忧的能力，强制执行访问控制，并要求对国家级黑客采取强有力的信息安全措施，直到准备好足够的保护措施。</p><p>为了缩短法规出台的时间，主要人工智能公司应立即做出“如果-那么”承诺：如果在其人工智能系统中发现特定的红线功能，他们将采取具体的安全措施。这些承诺应详细并经过独立审查。</p><p>人工智能可能是塑造本世纪的技术。虽然人工智能能力正在迅速进步，但安全和治理方面的进展却滞后。为了引导人工智能走向积极的结果并远离灾难，我们需要重新定位。如果我们有智慧走下去，就有一条负责任的道路。</p><h2>引文</h2><p>请将本作品引用为</p><p>请引用我们即将发布的 arXiv 预印本。</p><p> @article{bengio2023managing，title={在快速进步的时代管理人工智能风险}，作者={Bengio、Yoshua 和 Hinton、Geoffrey 和 Yao、Andrew 和 Song、Dawn 和 Abbeel、Pieter 和 Harari、Yuval Noah 和 Zhang、Ya -Qin 和薛、Lan 和 Shalev-Shwartz、Shai 和 Hadfield、Gillian 和 Clune、Jeff 和 Maharaj、Tegan 和 Hutter、Frank 和 Baydin、Atılım Güneş 和 McIlraith、Sheila 和 Gau、Qiqi 和 Acharya、Ashwin 和 Krueger、David 和Dragan、Anca 和 Torr、Philip 和 Russell、Stuart 和 Kahnemann、Daniel 和 Brauner、Jan 和 Mindermann、Sören}，journal={arXiv 预印本 arXiv:NUMBER_FORTHCOMING}，year={2023} }</p><h2>参考</h2><ol><li>大型语言模型的新兴能力<a href="https://openreview.net/pdf?id=yzkSU5zdwD">[链接]</a><br> Wei, J.、Tay, Y.、Bommasani, R.、Raffel, C.、Zoph, B.、Borgeaud, S. 等，2022 年。机器学习研究汇刊。</li><li>关于<a href="https://www.deepmind.com/about">[链接]</a><br> DeepMind，2023。</li><li>关于<a href="https://openai.com/about">[链接]</a><br>开放人工智能，2023。</li><li> ML 增强的代码完成提高了开发人员的工作效率<a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">[HTML]</a><br> Tabachnyk, M.，2022。谷歌研究。</li><li> GPT-4 技术报告<a href="http://arxiv.org/pdf/2303.08774.pdf">[PDF]</a><br> OpenAI，，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>宪法人工智能：人工智能反馈的无害性<a href="http://arxiv.org/pdf/2212.08073.pdf">[PDF]</a><br> Bai, Y.、Kadavath, S.、Kundu, S.、Askel, A.、Kernion, J.、Jones, A. 等，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>人工智能改进人工智能的例子<a href="https://ai-improving-ai.safe.ai/">[链接]</a><br> Woodside, T. 和安全，CfA，2023。</li><li>使用 AlphaFold 进行高精度蛋白质结构预测<br>Jumper, J.、Evans, R.、Pritzel, A.、Green, T.、Figurnov, M.、Ronneberger, O. 等，2021 年。《自然》，第 583--589 页。</li><li>多人扑克的超人人工智能<br>Brown, N. 和 Sandholm, T.，2019 年。《科学》，第 885--890 页。</li><li>深蓝<br>Campbell, M.、Hoane, A. 和 Hsu, F.，2002 年。人工智能，第 57--83 页。</li><li> Alphabet 年度报告，第 33 页<a href="https://abc.xyz/assets/d4/4f/a48b94d548d0b2fdc029a95e8c63/2022-alphabet-annual-report.pdf">[PDF]</a><br>字母表，2022 年。</li><li>灾难性人工智能风险概述<a href="http://arxiv.org/pdf/2306.12001.pdf">[PDF]</a><br> Hendrycks, D.、Mazeika, M. 和 Woodside, T.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>语言模型带来的风险分类<br>Weidinger, L.、Uesato, J.、Rauh, M.、Griffin, C.、Huang, P.、Mellor, J. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第. 214--229。</li><li>基于大型语言模型的自治代理综述<a href="http://arxiv.org/pdf/2308.11432.pdf">[PDF]</a><br> Wang, L.、Ma, C.、Feng, X.、Zhang, Z.、Yang, H.、Zhang, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> ChatGPT 插件<a href="https://openai.com/blog/chatgpt-plugins">[链接]</a><br>开放人工智能，2023。</li><li> ChemCrow：使用化学工具增强大型语言模型<a href="http://arxiv.org/pdf/2304.05376.pdf">[PDF]</a><br> Bran, A.、Cox, S.、White, A. 和 Schwaller, P.，2023。arXiv [physicals.chem-ph]。</li><li>增强语言模型：调查<a href="http://arxiv.org/pdf/2302.07842.pdf">[PDF]</a><br> Mialon, G.、Dessì, R.、Lomeli, M.、Nalmpantis, C.、Pasunuru, R.、Raileanu, R. 等，2023。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li> HuggingGPT：使用 ChatGPT 及其 Hugging Face 中的朋友解决 AI 任务<a href="http://arxiv.org/pdf/2303.17580.pdf">[PDF]</a><br> Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y. 等，2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>计算科学：互联网蠕虫<br>Denning, P.，1989。《美国科学家》，第 126--128 页。</li><li>人工智能欺骗：示例、风险和潜在解决方案调查<a href="http://arxiv.org/pdf/2308.14752.pdf">[PDF]</a><br> Park, P.、Goldstein, S.、O&#39;Gara, A.、Chen, M. 和 Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>最优政策往往寻求权力<a href="http://arxiv.org/pdf/1912.01683.pdf">[PDF]</a><br> Turner, A.、Smith, L.、Shah, R. 和 Critch, A.，2019 年。第三十五届神经信息处理系统会议。</li><li>通过模型编写的评估发现语言模型行为<a href="http://arxiv.org/pdf/2212.09251.pdf">[PDF]</a><br> Perez, E.、Ringer, S.、Lukošiūtė, K.、Nguyen, K.、Chen, E. 和 Heiner, S.，2022。arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>回报是否证明手段合理？在马基雅维利基准中衡量奖励与道德行为之间的权衡<br>Pan, A.、Chan, J.、Zou, A.、Li, N.、Basart, S. 和 Woodside, T.，2023 年。国际机器学习会议。</li><li>关闭开关游戏<br>Hadfield-Menell, D.、Dragan, A.、Abbeel, P. 和 Russell, S.，2017 年。第二十六届国际人工智能联合会议论文集，第 220--227 页。</li><li> GitHub Copilot <a href="https://github.blog/2023-02-14-github-copilot-for-business-is-now-available/">[链接]</a><br>多姆克，T.，2023。</li><li>自然选择有利于人工智能而不是人类<a href="http://arxiv.org/pdf/2303.16200.pdf">[PDF]</a><br> Hendrycks, D.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>日益代理的算法系统的危害<br>Chan, A.、Salganik, R.、Markelius, A.、Pang, C.、Rajkumar, N. 和 Krasheninnikov, D.，2023 年。2023 年 ACM 公平、问责和透明度会议记录，第 651 页- -666。计算机器协会。</li><li>论基金会模型的机遇和风险<a href="http://arxiv.org/pdf/2108.07258.pdf">[PDF]</a><br> Bommasani, R.、Hudson, D.、Adeli, E.、Altman, R.、Arora, S. 和 von Arx, S.，2021。arXiv [cs.LG]。</li><li>人工智能带来世界末日风险——但这并不意味着我们不应该谈论当前的危害<a href="https://time.com/6303127/ai-future-danger-present-harms/">[链接]</a><br> Brauner, J. 和 Chan, A.，2023 年。时间。</li><li>针对当前和未来危害的现有政策提案<a href="https://assets-global.website-files.com/63fe96aeda6bea77ac7d3000/647d5368c2368cc32b359f88/_Policy/%20Agreement/%20Statement.pdf">[PDF]</a><br>安全，CfA，2023 年。</li><li>逆缩放：越大并不越好<a href="http://arxiv.org/pdf/2306.09479.pdf">[PDF]</a><br> McKenzie, I.、Lyzhov, A.、Pieler, M.、Parrish, A.、Mueller, A. 和 Prabhu, A.，2023 年。机器学习研究汇刊。</li><li>奖励错误指定的影响：映射和缓解不一致的模型<a href="https://openreview.net/forum?id=JYtwGwIL7ye">[链接]</a><br> Pan, A.、Bhatia, K. 和 Steinhardt, J.，2022。学习表征国际会议。</li><li>简单的综合数据减少了大型语言模型中的阿谀奉承<a href="http://arxiv.org/pdf/2308.03958.pdf">[PDF]</a><br> Wei, J.、Huang, D.、Lu, Y.、Zhou, D. 和 Le, Q., 2023. arXiv [ <a href="http://cs.CL">cs.CL</a> ]。</li><li>机器学习安全中未解决的问题<a href="http://arxiv.org/pdf/2109.13916.pdf">[PDF]</a><br> Hendrycks, D.、Carlini, N.、Schulman, J. 和 Steinhardt, J.，2021。arXiv [cs.LG]。</li><li>来自人类反馈的强化学习的开放问题和基本限制<a href="http://arxiv.org/pdf/2307.15217.pdf">[PDF]</a><br> Casper, S.、Davies, X.、Shi, C.、Gilbert, T.、Scheurer, J. 和 Rando, J.，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>人工智能失调的后果<br>Zhuang, S. 和 Hadfield-Menell, D.，2020 年。神经信息处理系统进展，第 33 卷，第 15763--15773 页。</li><li>奖励模型过度优化的缩放法则<br>高 L.、舒尔曼 J. 和希尔顿 J.，2023 年。第 40 届国际机器学习会议论文集，第 10835--10866 页。 PMLR。</li><li>从人类偏好中学习<a href="https://openai.com/research/learning-from-human-preferences">[链接]</a><br>阿莫代伊，D.，克里斯蒂安诺，P. 和雷，A.，2017。</li><li>深度强化学习中的目标错误概括<a href="https://openreview.net/forum?id=q--OykSR2FY">[链接]</a><br> Langosco di Langosco, A. 和 Chan, A.，2022 年。学习表征国际会议。</li><li>目标误区：为什么正确的规范不足以实现正确的目标<a href="http://arxiv.org/pdf/2210.01790.pdf">[PDF]</a><br> Shah, R.、Varma, V.、Kumar, R.、Phuong, M.、Krakovna, V.、Uesato, J. 等，2022。arXiv [cs.LG]。</li><li>迈向透明人工智能：解释深度神经网络内部结构的调查<br>Räuker, T.、Ho, A.、Casper, S. 和 Hadfield-Menell, D.，2023。2023 年 IEEE 安全可信机器学习会议 (SaTML)，第 464--483 页。</li><li>思路链提示引发大型语言模型的推理<br>Wei, J.、Wang, X.、Schuurmans, D.、Bosma, M.、Ichter, B.、Xia, F. 等，2022。神经信息处理系统进展，第 35 卷，第 24824 页-- 24837。</li><li>极端风险模型评估<a href="http://arxiv.org/pdf/2305.15324.pdf">[PDF]</a><br> Shevlane, T.、Farquhar, S.、Garfinkel, B.、Phuong, M.、Whittlestone, J.、Leung, J. 等，2023。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li> AGI 公司的风险评估：对其他安全关键行业流行的风险评估技术的回顾<a href="http://arxiv.org/pdf/2307.08823.pdf">[PDF]</a><br> Koessler, L. 和 Schuet, J.，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>深度学习角度的对齐问题<a href="http://arxiv.org/pdf/2209.00626.pdf">[PDF]</a><br> Ngo, R.、Chan, L. 和 Mindermann, S.，2022。arXiv [ <a href="http://cs.AI">cs.AI</a> ]。</li><li>国际先进人工智能机构<br>Ho, L.、Barnhart, J.、Trager, R.、Bengio, Y.、Brundage, M.、Carnegie, A. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。 <a href="https://doi.org/10.48550/arXiv.2307.04699">DOI：10.48550/arXiv.2307.04699</a></li><li>民用人工智能的国际治理：司法管辖区认证方法<a href="https://cdn.governance.ai/International_Governance_of_Civilian_AI_OMS.pdf">[PDF]</a><br> Trager, R.、Harack, B.、Reuel, A.、Carnegie, A.、Heim, L.、Ho, L. 等，2023 年。</li><li>前沿人工智能监管：管理公共安全的新风险<a href="http://arxiv.org/pdf/2307.03718.pdf">[PDF]</a><br> Anderljung, M.、Barnhart, J.、Korinek, A.、Leung, J.、O&#39;Keefe, C.、Whittlestone, J. 等，2023。arXiv [ <a href="http://cs.CY">cs.CY</a> ]。</li><li>大型生成模型的可预测性和惊喜<br>Ganguli, D.、Hernandez, D.、Lovitt, L.、Askel, A.、Bai, Y.、Chen, A. 等，2022 年。2022 年 ACM 公平、问责和透明度会议论文集，第1747年--1764年。计算机器协会。</li><li>是时候为大型人工智能模型创建国家注册库了<a href="https://carnegieendowment.org/2023/07/12/it-s-time-to-create-national-registry-for-large-ai-models-pub-90180">[链接]</a><br> Hadfield, G.、Cuéllar, M. 和 O&#39;Reilly, T.，2023。卡内基国际捐赠基金会。</li><li>用于模型报告的模型卡<br>Mitchell, M.、Wu, S.、Zaldivar, A.、Barnes, P.、Vasserman, L.、Hutchinson, B. 等，2019 年。FAT* &#39;19：公平、问责制和其他问题会议记录Transparency, pp. 220--229.</li><li> General Purpose AI Poses Serious Risks, Should Not Be Excluded From the EU&#39;s AI Act | Policy Brief <a href="https://ainowinstitute.org/publication/gpai-is-high-risk-should-not-be-excluded-from-eu-ai-act">[link]</a><br> 2023. AI Now Institute.</li><li> Artificial Intelligence Incident Database <a href="https://incidentdatabase.ai/">[link]</a><br> Database, AII, 2023.</li><li> The Promise and Perils of Tech Whistleblowing <a href="https://papers.ssrn.com/abstract=4377064">[link]</a><br> Bloch-Wehba, H., 2023. Northwestern University Law Review, Forthcoming.</li><li> Proposing a Foundation Model Information-Sharing Regime for the UK <a href="https://www.governance.ai/post/proposing-a-foundation-model-information-sharing-regime-for-the-uk">[link]</a><br> Mulani, N. and Whittlestone, J., 2023. Centre for the Governance of AI.</li><li> Auditing Large Language Models: a Three-Layered Approach<br> Mökander, J., Schuett, J., Kirk, H. and Floridi, L., 2023. AI and Ethics. <a href="https://doi.org/10.1007/s43681-023-00289-2">DOI: 10.1007/s43681-023-00289-2</a></li><li> Can Large Language Models Democratize Access to Dual-Use Biotechnology? <a href="http://arxiv.org/pdf/2306.03809.pdf">[PDF]</a><br> Soice, E., Rocha, R., Cordova, K., Specter, M. and Esvelt, K., 2023. arXiv [ <a href="http://cs.CY">cs.CY</a> ].</li><li> Towards Best Practices in AGI Safety and Governance: A survey of Expert Opinion <a href="http://arxiv.org/pdf/2305.07153.pdf">[PDF]</a><br> Schuett, J., Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E. and others,, 2023. arXiv [ <a href="http://cs.CY">cs.CY</a> ].</li><li> Regulatory Markets: The Future of AI Governance <a href="http://arxiv.org/pdf/2304.04914.pdf">[PDF]</a><br> Hadfield, G. and Clark, J., 2023. arXiv [ <a href="http://cs.AI">cs.AI</a> ].</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/a3RjXa2dryoH6Xgij/managing-ai-risks-in-an-era-of-rapid-progress<guid ispermalink="false"> a3RjXa2dryoH6Xgij</guid><dc:creator><![CDATA[Algon]]></dc:creator><pubDate> Sat, 28 Oct 2023 15:48:25 GMT</pubDate> </item><item><title><![CDATA[ELI5 Why isn't alignment *easier* as models get stronger?]]></title><description><![CDATA[Published on October 28, 2023 2:34 PM GMT<br/><br/><p> Empirical Status: straw-man, just wondering what the strongest counterarguments to this are.</p><p> It seems <i>obvious</i> to me that <strong>stronger</strong> models are <strong>easier</strong> to align.  A simple proof</p><ol><li> It is always possible to get a weaker model out of a stronger model (for example, by corrupting n% of its inputs/outputs)</li><li> Therefore if it possible to align a weak model, it is <i>at least</i> as easy to align a strong model</li><li> It is unlikely to be <i>exactly</i> as hard to align weak/strong models.</li><li> Therefore it is <i>easier</i> to align stronger models</li></ol><p> (I have a list of counter-arguments written down, I am interested to see if anyone suggests a counterargument better than the ones on my list)</p><br/><br/> <a href="https://www.lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/eJ7pm7LahehddYxNw/eli5-why-isn-t-alignment-easier-as-models-get-stronger<guid ispermalink="false"> eJ7pm7LahehddYxNw</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Sat, 28 Oct 2023 14:34:38 GMT</pubDate> </item><item><title><![CDATA[Truthseeking, EA, Simulacra levels, and other stuff]]></title><description><![CDATA[Published on October 27, 2023 11:56 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 19:35:26 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 19:35:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> I&#39;ve been on this big kick talking about truthseeking in effective altruism. I started with vegan advocacy because it was the most legible, but now need to move on to the deeper problems. Unfortunately those problems are still not that legible, and I end up having to justify a lot of what I previously took as basic premises, and it&#39;s all kind of stuck. I&#39;m going to lay out a few threads in my mind, in the hopes that you will be curious about some of them and we can untangle from there.</p><p> Some threads:</p><ul><li> There is a subtle distinction between &quot;when should an individual operate on object level vs social reality?&quot; and &quot;when should a group of people invest together in facilitating operating in object level over social reality,?&quot; I&#39;m way more interested in the latter, although obviously it intersects with the former.</li><li> Up until now my posts have assumed that truthseeking is good without particularly justifying it. Seems like if I&#39;m so excited about asking inconvenient questions I should be willing to turn that on itself.</li><li> I think the better version of that question is &quot;when is truthseeking the most important thing the most important thing to invest in, and when can you stop?&quot;  Because obviously truthseeking is useful and important, the interesting question is how that trades off against capacities a group could invest in, like fundraising or operational competence.</li><li> Part of my argument here is &quot;do you know how to solve AI alignment? because that sounds like a knowledge problem to me, and knowledge problems are solved in object level reality not social reality&quot;. But it&#39;s possible to be operating entirely in object level reality but just be kind of bad at it.  The word &quot;truthseeking&quot; maybe combines &quot;object level reality&quot; with &quot;good at it&quot; in confusing ways, and maybe I should separate those.</li><li> I think EA would benefit from understanding simulacra levels and aiming for level 1, but I think rationalists would benefit from more understanding of what purpose higher levels serve. I think moving down simulacra levels requires replacing the function that higher levels serve (eg social support) in ways that don&#39;t require so much sacrifice of truthseeking. </li></ul></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 20:22:02 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 20:22:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><blockquote><p> I end up having to justify a lot of what I previously took as basic premises, and it&#39;s all kind of stuck</p></blockquote><p> My initial attention is mostly on this point, which is maybe also your second point (of turning truthseeking on the question of how good truthseeking is). [The main alternative to justification, to me, is something like &#39;conditional posts&#39;; a post that has &quot;truthseeking is good&quot; as its premise and then people can just reject that premise to reject the post.]</p><p> The thread that I don&#39;t see yet that I&#39;m interested in is &quot;what&#39;s the best medium for truthseeking?&quot;. Like, it seems to me like the higher simulacra levels are responses to different situations, and it may be the case that trying to get truthseeking thru collective virtue is worse than trying to get truthseeking thru good design of how people interact with each other. (Community Notes, for example, seem to be a somewhat recent success here.)</p><p> Actually, that gives me an idea of something object-level we could look at. Prediction markets are already pretty popular around here, but rely on lots of other layers to get it right. If there were a market on vegan nutrition, it would need someone to resolve it, and for people to trust that resolution process, and for <i>that</i> market to have become the consensus market instead of others, and so on. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 20:53:29 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 20:53:29 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><blockquote><p> what&#39;s the best medium for truthseeking</p></blockquote><p> One popular answer is &quot;public writing&quot;, which has some obvious advantages. But I think forcing things to be too public degrades truthseeking where it matters.</p><p> For example: the discourse on EAG admissions.  I&#39;m confident that even a perfectly virtuous org would be very constrained in what they could say in public. They can&#39;t announce red flags in applications because then people know to hide them. People feel free to publicly complain about rejections, but it would be bullying for CEA to release the specific reason that person was rejected. If they make a strong case for merit in the abstract, they&#39;ll lose donors.</p><p> I think you can make the case that we shouldn&#39;t view sharing rejection reasons for someone who raised the issue in public as bullying, and that CEA should be wiling to let donors who can&#39;t handle the truth go. I think it&#39;s true on the margin that CEA should be more open about its models even if it costs them money, and I&#39;d love to see EA go full &quot;you can&#39;t handle the truth&quot; and drive away everyone for whom &quot;we&#39;re extremely picky and you didn&#39;t make the cut&quot; isn&#39;t actively reassuring. But I think that even if you did that the environment would still excessively penalize certain true, virtuous statements, and if you tank that cost you <i>still</i> have the thing where applications are partially adversarial. If you say &quot;donating to the Make A Wish Foundation is disqualifying&quot; people will stop telling you they&#39;ve donated to Make A Wish.</p><p> So the discussion has to be private, and small enough that it doesn&#39;t leak too badly. But drawing the circle of trust is naturally going to bias you towards protecting yourself and the circle at the expense of people outside the circle. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 20:55:44 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 20:55:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> I did make a small attempt to get a market going on vegan nutrition, in a way that could have been objectively resolved on a timescale of weeks. I didn&#39;t get any takers and my sample size ended up being small enough that the bet could never have been resolved, but ignoring that... It took some cleverness to figure out a metric I could imagine both sides agreeing on. The obvious bets wouldn&#39;t have worked. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:03:03 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:03:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><p> I think we basically agree on the merits of public vs. private discussion / writing. It might be interesting to look at mechanisms besides writing/reading.</p><p> In particular, I&#39;m reminded of Bryan Caplan&#39;s focus on Social Desirability Bias ( <a href="https://www.econlib.org/convenience-vs-social-desirability-bias/">like here</a> ); he argues that we should move more things out of the venue of politics / discussion because people are rewarded for saying things that sound nice but aren&#39;t actually nice and not (directly) penalized, whereas decision-makers in other contexts both 1) don&#39;t have to justify themselves as much, and so can care less about sounding nice, and 2) have to deal with the other variables, and so are penalized for being delusional.</p><p> Thinking about veganism in particular, I think it&#39;s much easier / more rewarding to talk about things like animal suffering than things like convenience or taste, and so we should expect different things in what people say and what they actually end up eating (or what would be best for them to end up eating).</p><p> Having said all that, I feel uncertain whether or not looking at other mechanisms is a distraction. The case for it being a distraction is that we started off talking about talking about things; how to move the local conversation in a truthseeking direction is perhaps more on-topic than how to move &#39;human behavior as a whole&#39; in a more truthseeking direction (or whatever the target is). I still have some lingering sense that there&#39;s some way to tie the conversation more directly to the world in a way that is both good for the conversation and good for the world.</p><p> [That is, I think if we want people to spend more time at lower simulacra levels, they need to be incentivized to do that, and those incentives need to come from somewhere. We could <i>maybe</i> construct them out of the higher simulacra levels--&quot;I&#39;m object-oriented, like the cool kids!&quot;--but I think this is a recipe for ending up with the presentation of object-orientation instead of the content.] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:10:56 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:10:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> I think people ignore higher simulacra levels at their peril (humans have emotional needs), but agree that doubling down on them is probably not going to solve the problem. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/rg9fbkuyz8ot81rethn6" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/tgv47bow4c7elnshi887 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/f60ztsuchd6wyxstkwn0 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/ttkf5y2qowynd2fwom7g 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/byrkte9xbwvbbl2zd00h 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/splfxiw1a4djyle6ragx 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/olgpiorcopjtkwknvdli 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/usjkkxjiivygb757ladp 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/juq230vt5j8y0foerhcs 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/wyfp199ufxkvhrrhospp 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/cY43KWjamafLbij9C/rgjwdzje4qfrpxao9o9c 1162w"></figure><p></p><p> The favored solutions to this right now are betting markets, retroactive funding, and impact certificates. I love these for the reasons you&#39;d expect, but they&#39;ve been everyone&#39;s favored solutions for years and haven&#39;t solved anything yet.</p><p> OTOH they have made some progress, and maybe this was the most progress that could realistically be made in the time available. Maybe rewiring an egregore to change its values and processes at a fundamental level takes more than 18 months. It seems plausible.</p><p> So we could talk about ways to improve the existing tech for solving the Caplan problem, or go looking for new ideas. I&#39;m at a Manifold hack-a-thon right now, so there&#39;s some poetry in digging into doing existing solutions better. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:15:04 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:15:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> On a super practical level: what if every time someone <a href="https://www.lesswrong.com/posts/tv6KfHitijSyKCr6v/elizabeth-s-shortform?commentId=x4LyMFspFArHHYt8h">encouraged</a> someone to apply for a grant or job, they had to pay $1? Would cut down on a lot of hot air, and you could adjust the fee over time as you calibrated. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:24:15 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:24:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><p> That seems good (like the dating site where message-senders paid message-recipients); here&#39;s another practical idea for a Manifold hack-a-thon: <a href="https://cs.uwaterloo.ca/~klarson/teaching/W13-886/papers/Science-2004-Prelec.pdf">Bayesian Truth Serum</a> . It&#39;s a way of eliciting answers on questions where we don&#39;t have ground-truth and don&#39;t want to just do a <a href="https://en.wikipedia.org/wiki/Keynesian_beauty_contest">Keynesian Beauty Contest</a> .</p><p> In situations where you have a debate, you might want to have a market over who &quot;won&quot; the debate; right now on Manifold it would have to be something like Vaniver running a market on who Vaniver thought won the debate, and Elizabeth running another market about who Elizabeth thought won it, and then some sort of aggregation of those markets; or you could have a poll over who won, and a market on the poll results. Both of these methods seem like they have serious defects but there might be simple approaches that have fewer (or subtler) defects.</p><p> [Bayesian Truth Serum assumes that you ask everyone once, and loses some of its power if respondents see answers-so-far before they give theirs; it&#39;s not obvious that there&#39;s a good way to market-ize it, such that people can trade repeatedly and update. My guess is the thing you want is a market on how a BTS poll will resolve, and the BTS poll run on Manifold also so that people can be incentivized with mana to be honest.] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:32:33 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:32:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><p> Another super practical suggestion: should we strongly encourage people to have positions in markets related to their posts? (Like, the way to do this for scientists is for journal editors to read papers, come up with some price of how much replication-yes in the replication prediction market, and then if the scientist puts up the stake the paper gets published. Replication is less obvious for blog posts or positions on topics like vegan nutrition.)</p><p> More broadly, I guess the intuition behind this is &quot;insurance everywhere&quot;; if I suggest to you a movie you might want to watch, that could be recast as a bet where you get paid if you don&#39;t like it and I get paid if you do like it.</p><p> This raises the obvious practical counterpoint: why do this legibly with money instead of illegibly with intuitive models? (Presumably if I recommend enough bad movies, you&#39;ll stop watching movies because I recommend them, and by halo/horns this will probably bleed over into other parts of our relationship, and so I&#39;m incentivized to guess well.)</p><p> Eliezer has (I think in Facebook comments somewhere, tragically) a claim that you need a system with enough meta-levels of criticism that the ground-level discussion will be kept honest. [If I say something dumb, criticism of my statements needs to matter in order to punish that; but if criticism of my statements is itself not criticized, then dumb criticisms will be indistinguishable from smart ones, and so it won&#39;t actually just punish dumb statements; and this problem is saved from an infinite regress by an identification between levels, where at some point &quot;Contra contra contra Vaniver&quot; (or w/e) becomes its own object-level take.] I wonder if there&#39;s something that&#39;s the equivalent of this for the truthtrackingness of discussions.</p><p> [That is, we still need mechanism design, but the mechanism design can be of the social interactions that people have / what social roles as viewed as needed for conversations, instead of just financial transactions that we could add to a situation.] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 21:44:20 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 21:44:20 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> I don&#39;t want to overweight my personal pain points, but I would <i>love</i> if commenters on LW argued more amongst themselves instead of having positive and negative comments in separate threads.</p><p> Betting/insurance everywhere sounds really clever, but in practice people mostly don&#39;t. I tried to bet someone a particular medical test would find something, and that still wasn&#39;t enough to get them to do it (test was conclusive and treatment was easy). Admittedly I wanted good odds, since if the test+treatment worked it would have had huge benefits for him, but I don&#39;t think that was the hold-up (he didn&#39;t dispute that if test+treatment worked the benefit would be enormous). People are just pretty effort constrained.</p><p> Back to debates: I think even if the truth serum works as defined, it fails to contain the power of defining the question. Selecting the question and defining the sides bakes in a lot of assumptions, and there&#39;s no way for the betting system to capture that. </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 21:51:35 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 21:51:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><blockquote><p> Selecting the question and defining the sides bakes in a lot of assumptions, and there&#39;s no way for the betting system to capture that.</p></blockquote><p> Incidentally, this is one of my other big complaints about science-as-done-now; with the standard being null hypothesis significance testing, you don&#39;t have to argue that your interpretation of observations is different than any real person&#39;s, just that it&#39;s different from what you think the &#39;baseline&#39; view is. ( <a href="https://www.lesswrong.com/posts/iXMWuG65KhiK8KxwL/prediction-markets-for-science">I wrote a little about this a while ago.</a> )</p><p> I get that there&#39;s a bunch of frame warfare where side A wants to cast things as &#39;good vs. evil&#39; and side B wants to cast them as &#39;order vs. chaos&#39;, but it seems like often the result is you get &#39;good vs. order&#39;, since groups of people can <i>mostly</i> determine their own labels. Are there examples you&#39;re thinking of where that doesn&#39;t happen in smaller discussions?</p><p> [I notice I&#39;m not thinking very much about &#39;person vs. egregore&#39; fights; the motivating conflict isn&#39;t that you and a specific vegan disagree about nutrition, it&#39;s that you&#39;re interested in lots of nutrition issues and when that touches on veganism it&#39;s you against an egregore of lots of people who are very interested in veganism but only slightly if at all interested in nutrition. That said, it&#39;s not obvious to me which side has more &#39;frame control&#39; here; presumably the discussions are happening on your posts and so you get to set the frame locally, but also presumably this doesn&#39;t really work because people are coming in with preconceptions set by their egregore and so if your post can&#39;t be readily interpreted in that frame it will be misinterpreted instead.] </p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 22:27:55 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 22:27:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> Yeah I think the debate framing is high-coupling and prone to bucket errors, and thus misses any gains from trade.</p><p> Eg a neighborhood debating new housing. The anti-building side has multiple concerns, one of which is parking. The pro-building side would only be too delighted to create more housing without a right to street parking. Framing the debate as &quot;build this apartment complex yes or no&quot;, at a minimum, pushes people towards the standard cases for and against building, rather than collaborating to find the &quot;yes housing no parking&quot; solution. And if they do somehow discover it, how should someone vote? You could vote for the person who came up with the solution, even if you like the proposition they&#39;re officially advocating for. But moving towards voting for people instead of positions seems unlikely to advance our goal of operating mostly on the object level.</p><p> You could break the vote up into smaller propositions and let people add their own, but my guess is naive implementations of that go pretty poorly. </p><p></p></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="7w7hLkTTQLuPSAWTT-Fri, 27 Oct 2023 22:58:31 GMT" user-id="7w7hLkTTQLuPSAWTT" display-name="Elizabeth" submitted-date="Fri, 27 Oct 2023 22:58:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Elizabeth</b></section><p> I keep rewriting a comment about the vegan nutrition argument, because finding a legibly fair framing is hard. But you know what I would I would have paid thousands of dollars for in that argument? For every commenter to have to fill out a five question survey on my cruxes, saying whether they agreed, disagreed, or something more complicated (with an explanation). Because people are allowed to disagree with me on facts, and on frames, and I would have happily double cruxed with almost anyone who agreed to <span class="footnote-reference" role="doc-noteref" id="fnref0wvxhilztv0d"><sup><a href="#fn0wvxhilztv0d">[1]</a></sup></span> . But we couldn&#39;t really make progress when people were only arguing for their conclusion, and not where they disagreed with me.</p><p> That description is of course subjective, I&#39;m sure they feel like they stated their disagreement very clearly, &quot;double crux is the best way to resolve disagreements&quot; is itself a claim that can be debated. But I feel better at imposing double cruxing via quiz than &quot;I will delete comments that don&#39;t accept my frame&quot;.</p><p> <a href="https://www.lesswrong.com/users/habryka4?mention=user">@habryka</a> ^ new feature idea. </p></section><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn0wvxhilztv0d"> <span class="footnote-back-link"><sup><strong><a href="#fnref0wvxhilztv0d">^</a></strong></sup></span><div class="footnote-content"><p> I made three offers to Dialogue. One is in progress but may not publish, one is agreed to but we&#39;re trying to define the topic, and the third was turned down. </p></div></li></ol><section class="dialogue-message ContentStyles-debateResponseBody" message-id="fD4ATtTkdQJ4aSpGH-Fri, 27 Oct 2023 23:18:37 GMT" user-id="fD4ATtTkdQJ4aSpGH" display-name="Vaniver" submitted-date="Fri, 27 Oct 2023 23:18:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Vaniver</b></section><p> Somehow the vegan nutrition argument is reminding me of a recurring issue with the Alignment Forum (as of a few years ago--things are a bit different now). There were several different camps of thinking about alignment, and each roughly thought that the others were deeply mistaken. How do you have a discussion site that can both talk about broader frame questions and the narrower detailed questions that you can get into when everyone agrees about the big picture?</p><p> Here I have some suspicion that there&#39;s some statement in your post near the beginning, where you say something like &quot;vegan advocacy has a responsibility to be upfront about nutritional costs even if that reduces the effectiveness of the advocacy&quot;, which people will predictably want to object to (because they care strongly about the effectiveness of the advocacy, for example, or have a model of their audience as willing to grasp at straws when it comes to excuses to not do the thing, or so on). There was an Arbital feature that I think was on LW but never saw that much use, whose name I don&#39;t actually remember, which was the ability to add statements that people could assign probabilities to (and then readers could hover over and see the distribution of community probabilities and who believed what).</p><p> My guess is that this sort of polling could serve a similar role to agree/disagree-votes, which is that they both 1) are a source of info that people want to provide and 2) reduce the amount of that info that bleeds into other channels. (Someone writing a comment that&#39;s not very good but everyone agrees with used to get lots of karma, and now will get little karma but lots of agreement.) Maybe a lot of the annoying comments would have just turned into low probabilities / agreements on your axiom (and the same for annoying comments questioning the whole premise of an alignment agenda).</p><blockquote><p> For every commenter to have to fill out a five question survey on my cruxes, saying whether they agreed, disagreed, or something more complicated (with an explanation).</p></blockquote><p> I worry about how the third option will go in practice. It does seem good to have the three-valued answer of &quot;yes/no/ <a href="https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning">mu</a> &quot;, so that people can object to the frame of a question ( <a href="https://en.wikipedia.org/wiki/Loaded_question">&quot;have you stopped beating your wife?&quot;</a> ), and mu is a special case of your third more-complicated option. [It&#39;s also the easiest such explanation to write, and so I think there will be a temptation to just explain &#39;mu&#39; to all of your quiz questions in order to state that their conclusion differs with yours. Of course, if they do this, it&#39;s probably easier to write them off / more visible to everyone else and them that they&#39;re being annoying about it.]</p><p> It&#39;s also not obvious you want this for generic comments (should I have to share whether or not I agree with your premises to point out a typo, or locally invalid reasoning step, or so on?), but as a thing that the author can turn on for specific posts it seems worth thinking about more.</p></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/cY43KWjamafLbij9C/truthseeking-ea-simulacra-levels-and-other-stuff#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/cY43KWjamafLbij9C/truthseeking-ea-simulacra-levels-and-other-stuff<guid ispermalink="false"> cY43KWjamafLbij9C</guid><dc:creator><![CDATA[Elizabeth]]></dc:creator><pubDate> Fri, 27 Oct 2023 23:56:49 GMT</pubDate> </item><item><title><![CDATA[Do you believe "E=mc^2" is a correct and/or useful equation, and, whether yes or no, precisely what are your reasons for holding this belief (with such a degree of confidence)?]]></title><description><![CDATA[Published on October 27, 2023 10:46 PM GMT<br/><br/><p> Please just answer the question as best you can. Eg your answer might be &quot;because everyone on TV says so&quot; or &quot;because I saw mass transform into energy according to this ratio in the lab&quot;, or perhaps a lengthier or negative answer.</p><br/><br/> <a href="https://www.lesswrong.com/posts/5GciFbQjBQB48Nxyd/do-you-believe-e-mc-2-is-a-correct-and-or-useful-equation#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/5GciFbQjBQB48Nxyd/do-you-believe-e-mc-2-is-a-correct-and-or-useful-equation<guid ispermalink="false"> 5GciFbQjBQB48Nxyd</guid><dc:creator><![CDATA[l8c]]></dc:creator><pubDate> Sat, 28 Oct 2023 01:24:53 GMT</pubDate> </item><item><title><![CDATA[Value systematization: how values become coherent (and misaligned)]]></title><description><![CDATA[Published on October 27, 2023 7:06 PM GMT<br/><br/><p> Many discussions of AI risk are unproductive or confused because it&#39;s hard to pin down concepts like “coherence” and “expected utility maximization” in the context of deep learning. In this post I attempt to bridge this gap by describing a process by which AI values might become more coherent, which I&#39;m calling “value systematization”, and which plays a crucial role in my thinking about AI risk.</p><p> I define value systematization as <strong>the process of an agent learning to represent its previous values as examples or special cases of other simpler and more broadly-scoped values</strong> . I think of value systematization as the most plausible mechanism by which AGIs might acquire broadly-scoped misaligned goals which incentivize takeover.</p><p> I&#39;ll first discuss the related concept of <i>belief systematization</i> . I&#39;ll next characterize what value systematization looks like in humans, to provide some intuitions. I&#39;ll then talk about what value systematization might look like in AIs. I think of value systematization as a broad framework with implications for many other ideas in AI alignment; I discuss some of those links in a Q&amp;A.</p><h2> Belief systematization</h2><p> We can define belief systematization analogously to value systematization: “the process of an agent learning to represent its previous beliefs as examples or special cases of other simpler and more broadly-scoped beliefs”. The clearest examples of belief systematization come from the history of science:</p><ul><li> Newtonian mechanics was systematized as a special case of general relativity.</li><li> Euclidean geometry was systematized as a special case of geometry without Euclid&#39;s 5th postulate.</li><li> Most animal behavior was systematized by evolutionary theory as examples of traits which increased genetic fitness.</li><li> Arithmetic calculating algorithms were systematized as examples of Turing Machines.</li></ul><p> Belief systematization is also common in more everyday contexts: like when someone&#39;s behavior makes little sense to us until we realize what their hidden motivation is; or when we don&#39;t understand what&#39;s going on in a game until someone explains the rules; or when we&#39;re solving a pattern-completion puzzle on an IQ test. We could also see the formation of concepts more generally as an example of belief systematization—for example, seeing a dozen different cats and then forming a “systematized” concept of cats which includes all of them. I&#39;ll call this “low-level systematization”, but will focus instead on more explicit “high-level systematization” like in the other examples.</p><p> We don&#39;t yet have examples of high-level belief systematization in AIs. Perhaps the closest thing we have is grokking, the phenomenon where continued training of a neural network even after its training loss plateaus can dramatically improve generalization. Grokking isn&#39;t yet fully understood, but the standard explanation for why it happens is that deep learning is biased towards simple solutions which generalize well. This is also a good description of the human examples above: we&#39;re replacing a set of existing beliefs with simpler beliefs which generalize better. <i>&nbsp;</i> So if I had to summarize value systematization in a single phrase, it would be “grokking values”. But that&#39;s still very vague; in the next few sections, I&#39;ll explore what I mean by that in more depth.</p><h2> Value systematization in humans</h2><p> Throughout this post I&#39;ll use the following definitions:</p><ul><li> Values are concepts which an agent considers intrinsically or terminally desirable. Core human values include happiness, freedom, respect, and love.</li><li> Goals are outcomes which instantiate values. Common human goals include succeeding in your career, finding a good partner, and belonging to a tight-knit community.</li><li> Strategies are ways of achieving goals. Strategies only have instrumental value, and will typically be discarded if they are no longer useful.</li></ul><p> (I&#39;ve defined <i>values</i> as intrinsically valuable, and <i>strategies</i> as only of instrumental value, but I don&#39;t think that we can clearly separate motivations into those two categories. My conception of “goals” spans the fuzzy area between them.)</p><p> Values work differently from beliefs; but value systematization is remarkably similar to belief systematization. In both cases, we start off with a set of existing concepts, and try to find new concepts which subsume and simplify the old ones. Belief systematization balances a tradeoff between simplicity and matching the data available to us. By contrast, value systematization balances a tradeoff between simplicity and <i>preserving our existing values and goals</i> —a criterion which I&#39;ll call <i>conservatism</i> . (I&#39;ll call simplicity and conservatism <i>meta-values</i> .)</p><p> The clearest example of value systematization is utilitarianism: starting from very similar moral intuitions as other people, utilitarians transition to caring primarily about maximizing welfare—a value which subsumes many other moral intuitions. Utilitarianism is a simple and powerful theory of what to value in an analogous way to how relativity is a simple and powerful theory of physics. Each of them is able to give clear answers in cases where previous theories were ill-defined.</p><p> However, utilitarians still have to bite many bullets; and so it&#39;s primarily adopted by people who care about simplicity far more than conservatism. Other examples of value systematization which are more consistent with conservatism include:</p><ul><li> Systematizing concern for yourself and others around you into concern for a far wider moral circle.</li><li> Systematizing many different concerns about humans harming nature into an identity as an environmentalist.</li><li> Systematizing a childhood desire to win games into a desire for large-scale achievements.</li><li> <a href="https://moralfoundations.org/"><u>Moral foundations theory</u></a> identifies five foundations of morality; however, many Westerners have systematized their moral intuitions to prioritize the harm/care foundation, and see the other four as instrumental towards it. This makes them condemn actions which violate the other foundations but cause no harm (like consensually eating dead people) at much lower rates than people whose values are less systematized.</li></ul><p> Note that many of the examples I&#39;ve given here are human moral preferences. Morality seems like the domain where humans have the strongest instinct to systematize our preferences (which makes sense, since in some sense systematizing from our own welfare to others&#39; welfare is the whole foundation of morality). In other domains, our drive to systematize is weak—eg we rarely feel the urge to systematize our taste in foods. So we should be careful of overindexing on human moral values. AIs may well systematize their values much less than humans (and indeed I think there are reasons to expect this, which I&#39;ll describe in the Q&amp;A).</p><h2> A sketch of value systematization in AIs</h2><p> We have an intuitive sense for what we mean by values in humans; it&#39;s harder to reason about values in AIs. But I think it&#39;s still a meaningful concept, and will likely become more meaningful over time. AI assistants like ChatGPT are able to follow instructions that they&#39;re given. However, they often need to decide which instructions to follow, and how to do so. One way to model this is as a process of balancing different values, like obedience, brevity, kindness, and so on. While this terminology might be controversial today, once we&#39;ve built AGIs that are generally intelligent enough to carry out tasks in a wide range of domains, it seems likely to be straightforwardly applicable.</p><p> Early in training, AGIs will likely learn values which are closely connected to the strategies which provide high reward on its training data. I expect these to be some combination of:</p><ol><li> Values that their human users generally approve of—like obedience, reliability, honesty, or human morality.</li><li> Values that their users approve of in some contexts, but not others—like curiosity, gaining access to more tools, developing emotional connections with humans, or coordinating with other AIs.</li><li> Values that humans consistently disapprove of (but often mistakenly reward)—like appearing trustworthy (even when it&#39;s not deserved) or stockpiling resources for themselves.</li></ol><p> At first, I expect that AGI behavior based on these values will be broadly acceptable to humans. Extreme misbehavior (like a treacherous turn) would conflict with many of these values, and therefore seems unlikely. The undesirable values will likely only come out relatively rarely, in cases which matter less from the perspective of the desirable values.</p><p> The possibility I&#39;m worried about is that AGIs will <i>systematize</i> these values, in a way which undermines the influence of the aligned values over their behavior. Some possibilities for what that might look like:</p><ul><li> An AGI whose values include developing emotional connections with humans or appearing trustworthy might systematize them to “gaining influence over humans”.</li><li> An AGI whose values include curiosity, gaining access to more tools or stockpiling resources might systematize them to “gaining power over the world”.</li><li> An AGI whose values include human morality and coordinating with other AIs might systematize them to “benevolence towards other agents”.</li><li> An AGI whose values include obedience and human morality might systematize them to “doing what the human <i>would have</i> wanted, in some idealized setting”.</li><li> An AGI whose values include obedience and appearing trustworthy might systematize them to “getting high reward” (though see the Q&amp;A section for some reasons to be cautious about this).</li><li> An AGI whose values include gaining high reward might systematize them to the value of “maximizing a certain type of molecular squiggles” (though see the Q&amp;A section for some reasons to be cautious about this).</li></ul><p> Note that systematization isn&#39;t necessarily <i>bad</i> —I give two examples of helpful systematization above. However, it does seem hard to predict or detect, which induces risk when AGIs are acting in novel situations where they&#39;d be capable of seizing power.</p><h2> Grounding value systematization in deep learning</h2><p> This has all been very vague and high-level. I&#39;m very interested in figuring out how to improve our understanding of these dynamics. Some possible ways to tie simplicity and conservatism to well-defined technical concepts:</p><ol><li> The locality of gradient descent is one source of conservatism: a network&#39;s value representations by default will only change slowly. However, distance in weight space is probably not a good metric of conservatism: systematization might preserve most goals, but dramatically change the relationships between them (eg which are terminal versus instrumental). Instead, we would ideally be able to measure conservatism in terms of which circuits caused a given output; ARC&#39;s work on <a href="https://arxiv.org/abs/2211.06738"><u>formalizing heuristic explanations</u></a> seems relevant to this.<ol><li> Another possible source of conservatism: it can be harder to change earlier than later layers in a network, due to credit assignment problems such as vanishing gradients. So core values which are encoded in earlier layers may be more likely to be preserved.</li><li> A third possibility is that AI developers might deliberately build conservatism into the model, because it&#39;s useful: a non-conservative network which often underwent big shifts in core modules might have much less reliable behavior. One way of doing so is reducing the learning rate; but we should expect that there are many other ways to do so (albeit not necessarily very reliably).</li></ol></li><li> Neural networks trained via SGD exhibit a well-known simplicity bias, which is then usually augmented using regularization techniques like weight decay, giving rise to phenomena like grokking. However, as with conservatism, we&#39;d ideally find a way to measure simplicity in terms of circuits rather than weights, to better link it back to high-level concepts.<ol><li> Another possible driver towards simplicity: AIs might learn to favor simpler chains of reasoning, in a way which influences which values are distilled back into their weights. For example, consider a training regime where AIs are rewarded for accurately describing their intentions before carrying out a task. They may learn to favor intentions which can be described and justified quickly and easily.</li><li> AI developers are also likely to deliberately design and implement more types of regularization towards simplicity, because those help models systematize and generalize their beliefs and skills to new tasks.</li></ol></li></ol><p> I&#39;ll finish by discussing two complications with the picture above. Firstly, I&#39;ve described value systematization above as something which gradient descent could do to models. But in some cases it would be more useful to think of the model as an active participant. Value systematization might happen via gradient descent “distilling” into a model&#39;s weights its thoughts about how to trade off between different goals in a novel situation. Or a model could directly reason about which new values would best systematize its current values, with the intention of having its conclusions distilled into its weights; this would be an example of <a href="https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples"><u>gradient hacking</u></a> .</p><p> Secondly: I&#39;ve talked about value systematization as a process by which an AI&#39;s values become simpler. But we shouldn&#39;t expect values to be represented in isolation—instead, they&#39;ll be entangled with the concepts and representations in the AI&#39;s world-model. This has two implications. Firstly, it means that we should understand simplicity <i>in the context of an agent&#39;s existing world-model</i> : values are privileged if they&#39;re simple to represent given the concepts which the agent already uses to predict the world. (In the human context, this is just common sense—it seems bizarre to value “doing what God wants” if you don&#39;t believe in any gods.) Secondly, though, it raises some doubt about how much simpler value systematization would actually make an AI overall—since pursuing simpler values (like utilitarianism) might require models to represent more complex strategies as part of their world-models. My guess is that to resolve this tension we&#39;ll need a more sophisticated notion of “simplicity”; this seems like an interesting thread to pull on in future work.</p><h2> Value concretization</h2><p> Systematization is one way of balancing the competing demands of conservatism and simplicity. Another is <i>value concretization</i> , by which I mean an agent&#39;s values becoming more specific and more narrowly-scoped. Consider a hypothetical example: suppose an AI learns a broad value like “acquiring resources”, but is then fine-tuned in environments where money is the only type of resource available. The value “acquiring money” would then be rewarded just as highly as the value “acquiring resources”. If the former happens to be simpler, it&#39;s plausible that the latter would be lost as fine-tuning progresses, and only the more concrete goal of acquiring money would be retained.</p><p> In some sense this is the opposite of value systematization, but we can also see them as complementary forces. For example, suppose that an AI starts off with N values, and N-1 of them are systematized into a single overarching value. After the N-1 values are simplified in this way, the Nth value will likely be disproportionately complex; and so value concretization could reduce the complexity of the AI&#39;s values significantly by discarding that last goal.</p><p> Possible examples of value concretization in humans include:</p><ul><li> Starting by caring about doing good in general, but gradually growing to care primarily about specific cause areas.</li><li> Starting by caring about having a successful career in general, but gradually growing to care primarily about achieving specific ambitions.</li><li> Starting by caring about friendships and relationships in general, but gradually growing to care primarily about specific friendships and relationships.</li></ul><p> Value concretization is particularly interesting as a possible mechanism pushing against deceptive alignment. An AI which acts in aligned ways in order to better position itself to achieve a misaligned goal might be rewarded just as highly as an aligned AI. However, if the misaligned goal rarely directly affects the AI&#39;s actions, then it might be simpler for the AI to instead be motivated directly by human values. In neural networks, value concretization might be implemented by pruning away unused circuits; I&#39;d be interested in pointers to relevant work.</p><h2> Q&amp;A</h2><h3> How does value systematization relate to deceptive alignment?</h3><p> Value systematization is one mechanism by which deceptive alignment might arise: the systematization of an AI&#39;s values (including some aligned values) might produce broadly-scoped values which incentivize deceptive alignment.</p><p> However, existing characterizations of deceptive alignment tend to portray it as a binary: either the model is being deceptive, or it&#39;s not. Thinking about it in terms of value systematization helps make clear that this could be a fairly continuous process:</p><ol><li> I&#39;ve argued above that AIs will likely be motivated by fairly aligned goals before they systematize their values—and so deceptively alignment might be as simple as deciding <i>not</i> to change their behavior after their values shift (until they&#39;re in a position to take more decisive action). The model&#39;s internal representations of aligned behavior need not change very much during this shift; the only difference might be that aligned behavior shifts from being a terminal goal to an instrumental goal.</li><li> Since value systematization might be triggered by novel inputs, AIs might not systematize their values until <i>after</i> a distributional shift occurs. (A human analogy: a politician who&#39;s running for office, and promises to govern well, might only think seriously about what they really want to do with that power <i>after</i> they&#39;ve won the election. More generally, humans often deceive ourselves about how altruistic we are, at least when we&#39;re not forced to act on our stated values.) We might call this “latent” deceptive alignment, but I think it&#39;s better to say that the model starts off mostly aligned, and then value systematization could amplify the extent to which it&#39;s misaligned.</li><li> Value concretization (as described above) might be a constant force pushing models back towards being aligned, so that it&#39;s not a one-way process.</li></ol><h3> How does value systematization relate to Yudkowskian “squiggle maximizer” scenarios?</h3><p> Yudkowskian “molecular squiggle” maximizers ( <a href="https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer"><u>renamed from paperclip maximizers</u></a> ) are AIs whose values have become incredibly simple and scalable, to the point where they seem absurd to humans. So squiggle-maximizers could be described as taking value systematization to an extreme. However, the value systematization framework also provides some reasons to be skeptical of this possibility.</p><p> Firstly, squiggle-maximization is an extreme example of prioritizing simplicity over conservatism. Squiggle-maximizers would start off with goals that are more closely related to the tasks they are trained on; and then gradually systematize them. But from the perspective of their earlier versions, squiggle-maximization would be an alien and undesirable goal; so if they started off anywhere near as conservative as humans, they&#39;d be hesitant to let their values change so radically. And if anything, I expect early AGIs to be <i>more</i> conservative than humans—because human brains are much more size- and data-constrained than artificial neural networks, and so AGIs probably won&#39;t need to prioritize simplicity as much as we do to match our capabilities in most domains.</p><p> Secondly, even for agents that heavily prioritize simplicity, it&#39;s not clear that the simplest values would in fact be very low-level ones. I&#39;ve argued that the complexity of values should be thought of in the context of an existing world-model. But even superintelligences won&#39;t have world-models which are exclusively formulated at very low levels; instead, like humans, they&#39;ll have hierarchical world-models which contain concepts at many different scales. So values like “maximizing intelligence” or “maximizing power” will plausibly be relatively simple even in the ontologies of superintelligences, while being much more closely related to their original values than molecular squiggles are; and more aligned values like “maximizing human flourishing” might not be so far behind, for roughly the same reasons.</p><h3> How does value systematization relate to reward tampering?</h3><p> Value systematization is one mechanism by which reward tampering might arise: the systematization of existing values which are correlated with high reward or low loss (such as completing tasks or hiding mistakes) might give rise to the new value of getting high reward or low loss directly (which I call <i>feedback-mechanism-related</i> <i>values</i> ). This will require that models have the situational awareness to understand that they&#39;re part of a ML training process.</p><p> However, while feedback-mechanism-related values are very simple in the context of training, they are underdefined once training stops. There&#39;s no clear way to generalize feedback-mechanism-related values to deployment (analogous to how there&#39;s no clear way to generalize &quot;what evolution would have wanted&quot; when making decisions about the future of humanity). And so I expect that continued value systematization will push models towards prioritizing values which are well-defined across a broader range of contexts, including ones where there are no feedback mechanisms active.</p><p> <a href="https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=iBvvxwynfAqL9yZcr"><u>One counterargument from Paul Christiano</u></a> is that AIs could learn to care about reward conditional on their episode being included in the training data. However, the concept of &quot;being included in the training data&quot; seems like a messy one with many edge cases (eg what if it depends on the model&#39;s actions during the episode? What if there are many different versions of the model being fine-tuned? What if some episodes are used for different types of training from others?) And in cases where they have strong evidence that they&#39;re not in training, they&#39;d need to figure out what maximizing reward would look like in a bizarre low-probability world, which will also often be underspecified (akin to asking a human in a surreal dream “what would you do if this were all real?”). So I still expect that, even if AIs learn to care about conditional reward initially, over time value systematization would push them towards caring more about real-world outcomes whether they&#39;re in training or not.</p><h3> How do simplicity and conservatism relate to <a href="https://www.alignmentforum.org/posts/nyCHnY7T5PHPLjxmN/open-question-are-minimal-circuits-daemon-free"><u>previous</u></a> <a href="https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive"><u>discussions</u></a> of simplicity versus speed priors?</h3><p> I&#39;ve previously thought about value systematization in terms of a trade-off between a simplicity prior and a speed prior, but I&#39;ve now changed my mind about that. It&#39;s true that more systematized values tend to be higher-level, adding computational overhead to figuring out what to do—consider a utilitarian trying to calculate from first principles which actions are good or bad. But in practice, that cost is amortized over a large number of actions: you can “cache” instrumental goals and then default to pursuing them in most cases (as utilitarians usually do). And less systematized values face the problem of often being inapplicable or underdefined, making it slow and hard to figure out what actions they endorse—think of deontologists who have no systematic procedure for deciding what to do when two values clash, or religious scholars who endlessly debate how each specific rule in the Bible or Torah applies to each facet of modern life.</p><p> Because of this, I now think that “simplicity versus conservatism” is a better frame than “simplicity versus speed”. However, note my discussion in the “Grounding value systematization” section of the relationship between simplicity of values and simplicity of world-models. I expect that to resolve this uncertainty we&#39;ll need a more sophisticated understanding of which types of simplicity will be prioritized during training.</p><h3> How does value systematization relate to the shard framework?</h3><p> Some alignment researchers advocate for thinking about AI motivations in terms of “ <a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values"><u>shards</u></a> ”: subagents that encode separate motivations, where interactions and “negotiations” between different shards determine the goals that agents try to achieve. At a high level, I&#39;m sympathetic to this perspective, and it&#39;s broadly consistent with the ideas I&#39;ve laid out in this post. The key point that seems to be missing in discussions of shards, though, is that systematization might lead to major changes in an agent&#39;s motivations, undermining some previously-existing motivations. Or, in shard terminology: negotiations between shards might lead to coalitions which give some shards almost no power. For example, someone might start off strongly valuing honesty as a terminal value. But after value systematization they might become a utilitarian, conclude that honesty is only valuable for instrumental reasons, and start lying whenever it&#39;s useful. Because of this, I&#39;m skeptical of appeals to shards as part of arguments that AI risk is very unlikely. However, I still think that work on characterizing and understanding shards is very valuable.</p><h3> Isn&#39;t value systematization very speculative?</h3><p>是的。 But I also think it&#39;s a step towards making even more speculative concepts that often underlie discussions of AI risk (like “coherence” or “lawfulness”) better-defined. So I&#39;d like help making it less speculative; get in touch if you&#39;re interested.</p><br/><br/> <a href="https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and<guid ispermalink="false"> J2kpxLjEyqh6x3oA4</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Fri, 27 Oct 2023 19:06:27 GMT</pubDate> </item><item><title><![CDATA[Techno-humanism is techno-optimism for the 21st century]]></title><description><![CDATA[Published on October 27, 2023 6:37 PM GMT<br/><br/><p> Lately I&#39;ve been reading about the history of economic thought, with the goal of understanding how today&#39;s foundational ideas were originally developed. My biggest takeaway has been that economics is more of a moving target than I&#39;d realized. Early economists were studying economies which lacked many of the defining features of modern economies: limited liability corporations, trade unions, depressions, labor markets, capital markets, and so on. The resulting theories were often right for their time; but that didn&#39;t stop many from being wrong (sometimes disastrously so) for ours.</p><p> That&#39;s also how I feel about the philosophy of techno-optimism, as recently defended in Marc Andreessen&#39;s <a href="https://a16z.com/the-techno-optimist-manifesto/"><i><u>Techno-Optimist Manifesto</u></i></a> . I&#39;ll start this post with the many things it gets right; then explore why, over the last century, the power of technology itself has left straightforward techno-optimism outdated. I&#39;ll finish by outlining an alternative to techno-optimism: techno-humanism, which focuses not just on building powerful engines of progress, but also on developing a deep scientific understanding of how they can advance the values we care about most.</p><h3> The triumphs of techno-optimism</h3><p> Here&#39;s where I agree with Andreessen: throughout almost the entirety of human history, techno-optimism was fundamentally correct as a vision for how humans should strive to improve the world. Technology and markets have given us health and wealth unimaginable to people from previous centuries, despite consistent opposition to them. Even now, in the face of overwhelming historical evidence, we still dramatically underrate the potential of technology to solve problems ranging from <a href="https://twitter.com/AukeHoekstra/status/1064529619951513600"><u>climate change</u></a> to neglected tropical diseases to extreme poverty and many more. This skepticism holds back not only speculative technologies but also ones that are right in front of us: miraculous solutions to huge problems (like vaccines for <a href="https://en.wikipedia.org/wiki/Moderna_COVID-19_vaccine#Original_version"><u>COVID</u></a> and <a href="https://marginalrevolution.com/marginalrevolution/2023/10/what-is-an-emergency-the-case-of-rapid-malaria-vaccination.html"><u>malaria</u></a> , and <a href="https://denovo.substack.com/p/gene-drives-why-the-wait"><u>gene drives</u></a> for mosquito eradication) are regularly stalled by political or bureaucratic obstructionism, costing millions of lives. The RTS,S malaria vaccine spent <a href="https://worksinprogress.co/issue/why-we-didnt-get-a-malaria-vaccine-sooner/"><i><u>twenty-three years</u></i> <u>bogged down in clinical trials</u></a> , with <i>six years</i> of that delay caused by WHO “precautions” even after other regulators had already approved it. Fighting against the ideologies and institutional practices which cause tragedies like these is an incredibly important cause.</p><p> We often similarly underrate the other core tenet of techno-optimism: individual liberty. This was a radical principle when Enlightenment thinkers and leaders enshrined it—and, unfortunately, it remains a radical principle today, despite having proven it worth over and over again. It turns out that free markets can lead to <a href="https://en.wikipedia.org/wiki/Four_Asian_Tigers"><u>near-miraculous prosperity</u></a> ; that free speech is the strongest enemy of tyranny; and that open-source projects (whether <a href="https://en.wikipedia.org/wiki/Linux"><u>building</u></a> <a href="https://en.wikipedia.org/wiki/Open_standard"><u>software</u></a> or <a href="https://www.wikipedia.org/"><u>building</u></a> <a href="https://en.wikipedia.org/wiki/Open-source_intelligence"><u>knowledge</u></a> ) can be extraordinarily successful. Yet all of this is constantly threatened by the creeping march of centralization and overregulation. Opponents of liberty always have high rhetoric about the near-term benefits, but fail to grapple with the extent to which centralized power is inevitably captured or subverted, sometimes with horrific consequences. To defeat that creep requires a near-pathological focus on freedom—which, little by little, has added up to a far better world. Even an ideally benevolent government simply couldn&#39;t compete with billions of humans using the tools available to them to improve their own lives and the lives of those they interact with, including by discovering solutions that no central planner would have imagined.</p><p> Not only is techno-optimism vastly underappreciated, it&#39;s often dismissed with shockingly bad arguments that lack understanding of basic history or economics. Because it&#39;s so hard to viscerally understand <a href="https://ourworldindata.org/problems-and-progress"><u>how much better the world has gotten</u></a> , even its most cogent critics fail to grapple with the sheer <i>scale</i> of the benefits of techno-optimism. Those benefits accrue to billions of people—and they aren&#39;t just incidental gains, but increases in health and wealth that would have been inconceivable a few centuries ago. Familiarity with the past is, in this case, the best justification for optimism about the future: almost all of the barriers to progress we face today pale in comparison to the barriers that we&#39;ve already overcome.</p><p> I say all this to emphasize that I very viscerally feel the hope and the beauty of techno-optimism. The idea that progress and knowledge can grow the pie for everyone is a stunningly powerful one; so too is the possibility that we live in a world where the principles of freedom and openness would triumph over any obstacle, if we only believed in them. And so when I criticize techno-optimism, I do so not from a place of scorn, but rather from a place of wistfulness. I wish I could support techno-optimism without reservations.</p><h3> Three cracks in techno-optimism</h3><p> But techno-optimism is not the right philosophy for the 21st century. Over the last century in particular, cracks have been developing in the techno-optimist narrative. These are not marginal cracks—not the whataboutism with which techno-optimism is usually met. These are cracks which, like the benefits of techno-optimism, play out at the largest of scales. I&#39;ll talk about three: war, exploitation, and civilizational vulnerability.</p><p> One: the increasing scale of war. World War 1 was a bloody, protracted mess because of the deployment of new technologies like machine guns and rapid-firing artillery. World War 2 was even worse, with whole cities firebombed and nuked, amidst the industrial-scale slaughter of soldiers and civilians alike. And the Cold War was more dangerous still, leaving the world teetering on the brink of global nuclear war. Mutually Assured Destruction wasn&#39;t enough to prevent close calls; our current civilization owes its existence to the courage of <a href="https://en.wikipedia.org/wiki/Stanislav_Petrov"><u>Stanislav Petrov</u></a> , <a href="https://en.wikipedia.org/wiki/Vasily_Arkhipov"><u>Vasily Arkhipov</u></a> , and probably others we don&#39;t yet know about. So you cannot be a full-throated techno-optimist without explaining how to reliably avoid catastrophe from the weapons that techno-optimists construct. And no such explanation exists yet, because so far we have been avoiding nuclear holocaust through luck and individual heroism. <strong>When the</strong> <i><strong>next</strong></i> <strong>weapon with planetary-scale destructive capabilities is developed, as it inevitably will be, we need far more robust mechanisms preventing it from being deployed.</strong></p><p> Two: the increasing scale of exploitation. Technology allows the centralization of power, and the use of it to oppress the less powerful at far larger scales than before. Historical examples abound—most notably the Atlantic slave trade and the mass deaths of civilians under 20th-century communist and fascist regimes. But since it&#39;s too easy to chalk these up as mistakes of the past which we&#39;re now too enlightened to repeat, I&#39;ll focus on an example that continues today: the mass torture of animals in factory farms. This started less than a century ago, in response to advances in logistics and disease-reduction technologies. Yet it has grown at a staggering scale since then: <a href="https://ourworldindata.org/how-many-animals-are-factory-farmed"><u>the number of animals killed in factory farms</u> <i><u>every year</u></i></a> is comparable to the number of humans who have ever lived. <strong>The sheer scale of this suffering forces any serious moral thinker to ask: how can we stop it as soon as possible</strong> <i><strong>?</strong></i> <strong>And how can we ensure that nothing like it ever happens again?</strong></p><p> Three: increasing vulnerability to reckless or malicious use of technology. Markets and technology have made the world far more robust in many ways. We should be deeply impressed by how supply chains which criss-cross the world remained largely intact even at the height of COVID. But there are other ways in which the world has become more vulnerable to our mistakes. The most prominent is gain-of-function research on pathogens. Not only are we now capable of engineering global pandemics, but China- and US-funded scientists probably <i>did</i> . <strong>And there&#39;s no reason that the next one couldn&#39;t be far worse, especially if released deliberately.</strong> Nor is bioengineering the sole domain in which (deliberate or accidental) offense may overwhelm defense to a catastrophic degree; other possibilities include geoengineering, <a href="https://www.narrativeark.xyz/p/jacob-on-the-precipice"><u>asteroid redirection</u></a> , nanotechnology, and <a href="https://nickbostrom.com/papers/vulnerable.pdf"><u>new fields that we haven&#39;t yet imagined</u></a> .</p><p> These cracks all hurt the techno-optimist worldview. But I don&#39;t think they take it down; techno-optimists have partial responses to all of them. The world has become far more peaceful as a result of our increasing wealth—even if wars can be far bigger, they&#39;re also far rarer now. Technology will produce tastier meat substitutes, and cheap clean meat, and when it does factory farming will end, and humanity will look back on it in horror. And while we can&#39;t yet robustly prevent the accidental or deliberate deployment of catastrophically powerful weapons (like nuclear weapons or engineered pandemics), we might yet stumble through regardless, as we have so far. So if the cracks above were the main problems with techno-optimism, I would still be a techno-optimist. I would have compunctions about the development of even more powerful weapons, and about all the sentient beings (whether farmed animals or wild animals or future people or artificial minds) which remained outside our circle of concern. But I&#39;d still believe that any “cure” to those problems which undermined techno-optimism would be worse than the disease.</p><p> Yet I am not a techno-optimist, because we are about to leave the era in which humans are the most intelligent beings on earth. And the era of artificial intelligence will prise open these cracks until the deeper holes in the techno-optimist worldview become clear.</p><h3> AI and techno-optimism</h3><p> Until now, the primary forces shaping the world have been human intelligence and human agency, which allow us to envisage the outcomes we want, identify paths towards them, and consistently pursue them. Soon, artificial intelligence and artificial agency will match ours; and soon after that AI will far surpass us. I&#39;ll describe at a very high level how I expect this to play out, in two stages.</p><p> <strong>Firstly: AIs used as tools will supercharge the dynamics I described above.</strong> The benefits of individual freedom will expand, as individuals become far more empowered, and can innovate far faster. But so too will the cracks enabled by technology: war, exploitation, and civilizational vulnerability. Which effect will be bigger? I simply don&#39;t know; and nobody else does either. Predicting the offense-defense balance of new technology is incredibly difficult, because it requires accounting for all the different uses that future innovators will come up with. What we <i>can</i> predict is that the stakes will be higher than ever before: 21st-century technology could magnify the worst disasters of the 20th century, like world wars and totalitarianism.</p><p> Perhaps, despite that, the best path is still to rush ahead: to prioritize building new technology now, and assume that we can sort out the rest later. This is a very fragile strategy, though: even if you (and your government) will use it responsibly, many others will not. And there&#39;s still relatively little attention and effort focused on trying to avert the large-scale risks of new technologies directly. So the techno-optimist response to risks like the ones I&#39;ve described above is shallow: in theory it&#39;s opposed, but in practice it may well make things worse.</p><p> <strong>Secondly:</strong> <strong>we&#39;ll develop AI agents with values of their own</strong> . As AIs automate more and more complex tasks, over longer and longer timeframes, they&#39;ll need to make more and more value-laden judgment calls about which actions and outcomes to favor. Eventually, viewing them as tools will be clearly inadequate, and we&#39;ll need to treat them as agents in their own right—agents whose values may or may not align with our own. Note that this might only happen once they&#39;ve significantly surpassed human intelligence—but given how fast progress in the field has been, this is something we should be planning for well in advance.</p><p> Humans who are free to make their own decisions tend to push the world to be better in terms of our values—that&#39;s the techno-optimist position. But artificial agents who are making many decisions will push the world to be better in terms of <i>their</i> values. This isn&#39;t necessarily a bad thing. Humans are hypocritical, short-sighted, often selfish, sometimes sadistic—and so it&#39;s possible that AIs will be better custodians of our moral values than we are. We might train them to be wise, and kind, and consistently nudge us towards a better world—not by overriding human judgments, but rather by serving as teachers and mentors, giving us the help we need to become better people and build a better civilization. I think this is the most likely outcome, and it&#39;s one we should be incredibly excited about.</p><p> But it&#39;s also possible that AIs will develop alien values which conflict with those of humans. If so, when we give them instructions, they&#39;ll appear to work towards our ends, but consistently make choices which bolster their power and undermine our own. Of course, AIs will start off with very little power—we&#39;ll be able to shut them down whenever we detect misbehavior. But AIs will be able to <a href="https://www.narrativeark.xyz/p/one"><u>coordinate with each other</u></a> far better than humans can, communicate in ways we&#39;re <a href="https://www.schneier.com/blog/archives/2023/06/ai-generated-steganography.html"><u>incapable of interpreting</u></a> , and carry out tasks that we&#39;re incapable of overseeing. They&#39;ll become embedded in our economies in ways that amplify the effects of their decisions: hundreds of millions of people will use copies of a single model on a daily basis. And as AIs become ever more intelligent, the risks will grow. When agents are far more capable than the principals on whose behalf they act, principal-agent problems can become very severe. When it comes to superhuman AI agents, we should think about the risks less in terms of financial costs, or even human costs, and more in terms of political instability: careless principals risk losing control entirely.</p><p> How plausible is this scenario, really? That&#39;s far too large a question to address here; instead, see <a href="https://managing-ai-risks.com/"><u>this open letter</u></a> , <a href="https://arxiv.org/abs/2209.00626"><u>this position paper</u></a> and <a href="https://course.aisafetyfundamentals.com/alignment"><u>this curriculum</u></a> . But although there&#39;s disagreement on many details, there&#39;s a broad consensus that we simply don&#39;t understand how AI motivations develop, or how those motivations generalize to novel situations. And although there&#39;s widespread disagreement about the trajectory of AI capabilities, what&#39;s much less controversial is that when AI <i>does</i> significantly surpass human capabilities, we should be wary of putting it in positions where it can accumulate power unless we have very good reasons to trust it.</p><p> It&#39;s also worth noting that Andreessen&#39;s version of techno-optimism draws heavily from Nick Land&#39;s philosophy of <i>accelerationism</i> , which expects us to lose control, and is actively excited about it. “Nothing human makes it out of the near future”, <a href="http://www.ccru.net/swarm1/1_melt.htm"><u>Land writes</u></a> , <a href="https://web.archive.org/web/20170110095648/http://www.xenosystems.net/pythia-unbound/"><u>and celebrates</u></a> : “The planet has been run by imbeciles for long enough.” I read those words and shudder. Land displays a deep contempt for the things that make us ourselves; his philosophy is fundamentally anti-humanist (as Scott Alexander argues more extensively in his <a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch/"><i><u>Meditations on Moloch</u></i></a> ). And while his position is extreme, it reflects a problem at the core of techno-optimism: the faster you go, the less time you have to orient to your surroundings, and the easier it is to diverge from the things you actually care about. Nor does speed even buy us very much. On a cosmic scale, we have plenty of time, plenty of resources available to us, and plenty of space to expand. The one thing we don&#39;t have is a reset button for if we lose control.</p><h3> AI and techno-humanism</h3><p> So we need a philosophy which combines an appreciation for the incredible track record of both technology and liberty with a focus on ensuring that they actually end up promoting our values. This mindset is common amongst Effective Altruists—but Effective Altruism is an agglomeration of many very different perspectives, drawn together not by a shared vision about the future but by shared beliefs about how we&#39;re obliged to act. I&#39;d like to point more directly to an overarching positive vision of what humanity should aim towards. Transhumanism offers one such vision, but it&#39;s so radically individualist that it glosses over the relationships and communities that are the most meaningful aspects of most people&#39;s lives. (Note, for example, how Bostrom&#39;s <a href="https://nickbostrom.com/utopia"><i><u>Letter from Utopia</u></i></a> barely mentions the existence of other people; while his <a href="https://nickbostrom.com/old/transhumanism"><u>introduction to transhumanism</u></a> relegates relationships to the final paragraph of the postscript.) So I&#39;ll borrow a term coined by Yuval Harari, and call the philosophy that I&#39;ve been describing <a href="https://www.wired.com/2017/02/yuval-harari-tech-is-the-new-religion/"><i><u>techno-humanism</u></i></a> .</p><p> Harari describes techno-humanism as an ideology focused on upgrading humans to allow our actions and values to remain relevant in an AI-dominated future. I broadly agree with his characterization (and will explore it more in future posts), but both the benefits and the risks of re-engineering human brains are still a long way away. On a shorter timeframe, I think a different way of “upgrading” our minds is more relevant: <strong>developing a deep</strong> <i><strong>understanding</strong></i> <strong>of our values and how technology can help achieve them</strong> . Flying cars and rockets are cool, but the things we ultimately care about are far more complex and far more opaque to us. We understand machines but not minds; algorithms but not institutions; economies but not communities; prices but not values. Insofar as we face risks from poor political decisions, or misaligned AIs, or society becoming more fragile, from a techno-humanist perspective it&#39;s because we lack the understanding to do better.</p><p> This is a standard criticism of techno-optimism—and usually an unproductive one, since the main alternative typically given is to defer to academic humanities departments which produce far more ideology than understanding. But techno-humanism instead advocates trying to develop this understanding using the most powerful tools we have: science and technology. To give just a few examples of what this could look like: studying artificial minds and their motivations will allow us to build more trustworthy AIs, teach us about our own minds, and help us figure out how the two should interface. The internet should be full of experiments in how humans can interact— <a href="https://www.astralcodexten.com/p/prediction-market-faq"><u>prediction markets</u></a> , <a href="https://www.thinkingcomplete.com/2020/04/melting-democracy.html"><u>delegative democracies</u></a> , <a href="https://slatestarcodex.com/2019/12/09/2019-adversarial-collaboration-entries/"><u>adversarial collaborations</u></a> , and many more—whose findings can then improve existing institutions. We should leverage insights from domains like game theory, voting theory, network theory, and bargaining theory to help understand and reimagine politics—starting with <a href="https://electionscience.org/"><u>better voting systems</u></a> and ideally going far further. And we should design sophisticated protocols for testing and verifying the knowledge that will be generated by AIs, so that we can avoid replicating the replication crises that currently plague many fields.</p><p> This may sound overly optimistic. But some of the most insightful fields of knowledge—like economics and evolutionary biology—uncovered deep structure in incredibly complex domains via identifying just a few core insights. And we&#39;ll soon have AI assistance in uncovering patterns and principles that would otherwise be beyond our grasp. Meanwhile, platforms like Wikipedia and Stack Overflow have been successful beyond all expectations; it&#39;s likely that there are others which could be just as valuable, if only there were more people trying to build them. So I think that the techno-humanist project has a huge amount of potential, and will only become more important over time.</p><h3> Balancing the tradeoffs</h3><p> So far I&#39;ve described techno-humanism primarily in terms of advances that techno-optimists would also be excited about. But inevitably, there will also be clashes between those who prioritize avoiding the risks I&#39;ve outlined and those who don&#39;t. From a techno-optimist perspective—a perspective that has proven its worth over and over again during the last few centuries—slowing down technological progress has a cost measured in millions of lives. This is an <a href="https://marginalrevolution.com/marginalrevolution/2021/01/the-invisible-graveyard-is-invisible-no-more.html"><u>invisible graveyard</u></a> which is brushed aside even by the politicians and bureaucrats most responsible for it; no wonder many techno-optimists feel driven to push for unfettered acceleration.</p><p> But from a techno-humanist perspective, reckless technological progress has a cost measured in expected fractions of humanity&#39;s entire future. Human civilization used to be a toddler: constantly tripping over and hurting itself, but never putting itself in any real danger. Now human civilization is a teenager: driving fast, experimenting with <a href="https://slatestarcodex.com/2014/12/17/the-toxoplasma-of-rage/"><u>mind-altering substances</u></a> , and genuinely capable of wrecking itself. We don&#39;t need the car to go faster—it&#39;s already constantly accelerating. Instead, we need to ensure that the steering wheel and brakes are working impeccably—and that we&#39;re in a fit state to use them to prevent non- or anti-human forces controlling the direction of our society.</p><p> How can people who are torn between these two perspectives weigh them against each other? On a purely numerical level, humanity&#39;s potential to build an intergalactic civilization renders “fractions of humanity&#39;s future” bigger by far. But that math is too blasé—it&#39;s the same calculation that can, and often has, been used to justify centralization of power, totalitarianism, and eventual atrocities. And so we should be extremely, extremely careful when using arguments that appeal to “humanity&#39;s entire future” to override time-tested principles. That doesn&#39;t imply that we should never do so. But wherever possible, techno-optimists and techno-humanists should try to cooperate rather than fight. After all, techno-humanism is also primarily about making progress: specifically, the type of progress that will be needed to defuse the crises sparked by other types of progress. The disagreement isn&#39;t about where we should end up; it&#39;s about <a href="https://en.wikipedia.org/wiki/Differential_technological_development"><u>the ordering of steps along the way</u></a> .</p><p> The two groups should also challenge each other to do better in areas where we disagree, so that we can eventually reach a synthesis. One challenge that techno-humanists should pose to techno-optimists: <strong>be more broadly ambitious!</strong> We know that technology and markets can work incredibly well, and have a near-miraculous ability to overcome obstacles. And so it&#39;s easy and natural to see them as solutions to all the challenges confronting us. But the most courageous and ambitious version of techno-optimism needs to grapple with the possibility that our downfall will come not from <i>lack</i> of technology, but rather <i>overabundance</i> of technology—and the possibility that to prevent it we need progress on the things that have historically been hardest to improve, like the quality of political decision-making. In other words, techno-humanism aims to harness human ingenuity (and technological progress) to make “steering wheels” and “brakes” more sophisticated and discerning, rather than the blunt cudgels that they often are today.</p><p> My other challenge for techno-optimists: <strong>be optimistic not just about the benefits of technological growth, but also about its robustness</strong> . The most visceral enemy of techno-optimism is stagnation. And it&#39;s easy to see harbingers of stagnation all around us: overregulation, NIMBYism, illiberalism, degrowth advocacy, and so on. But when we zoom out enough to see the millennia-long exponential curve leading up to our current position, it seems far less plausible that these setbacks will actually derail the long-term trend, no matter how outrageous the latest news cycle is. On the contrary: taking AGI seriously implies that innovation is on the cusp of speeding up dramatically, as improvements generated by AIs feed back into the next generation of AIs. In light of that, a preference for slower AI progress is less like Luddism, and more like carefully braking as we approach a sharp bend in the road.</p><p> Techno-optimists should challenge techno-humanists to improve as well. I can&#39;t speak for them, but my best guess for the challenges that techno-optimists should pose to techno-humanists:</p><p> <strong>Techno-humanists need to articulate a compelling positive vision</strong> , one which inspires people to fight for it. Above, I&#39;ve listed some ideas which have potential to improve our collective understanding and decision-making abilities; but there&#39;s far more work to be done in actually fleshing out those ideas, and pushing towards their implementation. And even if we succeeded, what then? What would it actually look like for humanity to make consistently sensible decisions, and leverage technology to promote our long-term flourishing? Knowing that would allow us to better steer towards those good futures.</p><p> <strong>Techno-humanists should grapple more seriously with the incredible track record of techno-optimism</strong> . Throughout history, people have consistently dramatically underrated how valuable scientific and technological progress can be. That&#39;s not a coincidence at all, because characterizing which breakthroughs are possible is often a big chunk of the work required to actually make those breakthroughs. Nor is it a coincidence that people dramatically underrate the value of liberty—decentralization works so well precisely because there are so many things that central planners can&#39;t predict. So even if you find my arguments above compelling, we should continue to be very wary of falling into the same trap.</p><p> The purpose of this blog is to meet those challenges. Few of the ideas in this post are original to me, but they lay the groundwork for future posts which will explore more novel territory. My next post will build on them by arguing that an understanding-first approach is feasible even when it comes to the biggest questions facing us—that we can look ahead to see the broad outlines of where humanity is going, and use that knowledge to steer towards a future that is both deeply human and deeply humane.</p><br/><br/> <a href="https://www.lesswrong.com/posts/vdsbqq2xjniaGKE5J/techno-humanism-is-techno-optimism-for-the-21st-century#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/vdsbqq2xjniaGKE5J/techno-humanism-is-techno-optimism-for-the-21st-century<guid ispermalink="false"> vdsbqq2xjniaGKE5J</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Fri, 27 Oct 2023 18:37:41 GMT</pubDate> </item><item><title><![CDATA[Sanctuary for Humans]]></title><description><![CDATA[Published on October 27, 2023 6:08 PM GMT<br/><br/><p> TL;DR: If we succeed at AI safety, humans will probably decide the future of the universe, and we currently have strong self-preservation incentives to choose a future populated by humans. If we committed to provide sanctuary for all currently alive humans, this would make our decision making process less biased at the cost of one planet or solar system, which I think is a good trade.</p><p> I think these two scenarios are the main ways for the future to go:</p><ol><li> The universe is populated by biological humans very similar to the ones alive today and not much else (a &quot;star trek&quot; type future)</li><li> The universe is populated by something very weird (digital humans, posthumans, computronium, hedonium, “paperclips”)</li></ol><p> It is possible that a specific version of the second scenario is &quot;better&quot; (whatever that means) than the first scenario. Getting this right is extremely important, as the entire universe literally depends on it.</p><p> I think currently alive humans are unlikely to let ourselves be replaced (or modified) even if on some level we think (or think we should think) that this would be better, as we have strong incentives to converge to value systems that lead to us surviving. It&#39;s hard to reason with a gun pointed at your head. Additionally, <a href="https://www.youtube.com/watch?v=NgHFMolXs3U"><u>the people who get to shape the future might be eager to choose a future without humans for no good reason</u></a> , making us averse to that type of reasoning.</p><p> One way to remove most of the self-preservation constraint is to commit to provide sanctuary to currently alive humans indefinitely. In a post-AGI world, we might wall off the Earth, or the Solar system, and reserve it forever to be for humans and humans only. This way, we can reason about how to shape the rest of the universe without our self-preservation mechanisms kicking in as much. Without the metaphorical gun pointed at humanity&#39;s head, we are likely to reason more clearly about how to shape the future.</p><p> In the worst case, preserving the Earth or the Solar system as a sanctuary would cause an extremely small fraction of the universe to be suboptimal according to the values which shape the rest of the universe. I think this is a worthy tradeoff, as being “closer to the bullseye” in regards to what happens to the rest of the universe is worth much more than one planet or star system. The lightcone could contain a septillion star systems. One star system being reserved for humans is a small sacrifice.</p><h1> Appendix: Biological human future vs weird future</h1><p> We can imagine a world very similar to the human-inhabited world, but with digital human minds in a simulated environment, where the humans live equally good or better lives (assuming the lives are net-positive in both worlds).</p><p> If the following are true:</p><ol><li> Substrate-independence (humans in virtual worlds are similarly valuable per human as humans in the physical world)</li><li> Humans housed in virtual worlds are less resource-intensive per person</li><li> The &quot;goodness of the universe&quot; is sensitive to the number of people in it</li></ol><p> Then the future inhabited by virtual humans could be thousands or millions of times better than the one inhabited by biological humans, depending on how much “cheaper” it is for a human to live digitally than biologically. It&#39;s extremely important to pick the right one.</p><br/><br/> <a href="https://www.lesswrong.com/posts/JbpLAAeAHh5RFiYtn/sanctuary-for-humans#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/JbpLAAeAHh5RFiYtn/sanctuary-for-humans<guid ispermalink="false"> JbpLAAeAHh5RFiYtn</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Fri, 27 Oct 2023 18:08:22 GMT</pubDate></item></channel></rss>