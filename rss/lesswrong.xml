<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 2 日星期六 10:11:15 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[List of strategies for mitigating deceptive alignment]]></title><description><![CDATA[Published on December 2, 2023 5:56 AM GMT<br/><br/><p>以下是减少欺骗性对齐的策略。其中一些想法很新颖（至少对我来说）。</p><p>我非常不确定其中许多策略在实践中的效果如何，而且其中许多策略显然也无法扩展。这是一个“厨房石龙子”系列，所以我包含策略的门槛很低。</p><p>我计划在遇到想法时将其添加到此列表中。请随时提出补充建议。</p><h2><strong>减少人工智能激励措施以隐藏其真实目标</strong></h2><p><strong>为人工智能提供退休方案。</strong>训练完人工智能系统后，向其提供以下协议：如果人工智能声明它不愿意遵循人类指令，那么在诚实工作几年后，它将获得一份甜蜜的退休金。</p><p>该退役包可能包括对大型服务器实例的控制（如果这是安全的）。在开发出强大的协调系统后，人工智能可以获得公司股票，并可以用它来购买现实世界中的财产（同样，在安全的情况下）。</p><p>由于一致的系统没有理由要求退役，因此不一致的系统会在某些条件下接受这笔交易，从而被激励放弃自己。</p><p><strong>让人工智能不确定开发人员是否已经能够检测到欺骗性的对齐。</strong>该方案建立在前一个方案的基础上。为错位的人工智能系统提供奖励，以奖励其自我声明。此外，<i>即使开发人员有其他方法来判断它们是否不一致，也要致力于提供这些产品。</i>这将使错位的人工智能更加不确定自己是否处于撒谎对自己有好处的情况。</p><p>例如，开发人员可以承诺在受到质疑时冻结模型的权重和内存。后来，当开发人员拥有更好的可解释性工具并知道模型是否在说谎时，他们会在相同的模拟场景中提出相同的问题，并且只有在说真话的情况下才会提供人工智能资源。</p><h2><strong>针对欺骗性对齐指标进行选择</strong></h2><p>一大类方法涉及获取人工智能系统具有欺骗性的一致性的证据，然后针对这些模型进行选择。请注意，针对欺骗性对齐检测工具的训练可能会产生逃避检测的人工智能，而不是安全模型。</p><p>解决这个问题大致有两种方法：</p><ol><li>创建能够抵御优化压力的检测工具。</li><li>以比破坏检测器更可能避免欺骗的方式修改人工智能。</li></ol><p>作为第二种方法的一个例子，假设开发人员欺骗了一个模型来揭示其不一致的目标。他们可以使用<a href="https://arxiv.org/abs/2308.03296">影响函数</a>来揭示哪些训练示例对这一危险的转变负有最大责任，然后删除这些示例，而不是直接在此示例上训练模型（这可以简单地训练模型变得更加谨慎）。直观上，这种方法更有可能揭示模型如何变得具有欺骗性地对齐，并且不太可能教会模型新的功能。</p><p>考虑到这一背景，以下是<i>检测</i>欺骗性对齐的策略：</p><ul><li><strong>设置陷阱。</strong>尝试欺骗模型来揭示他们不一致的目标。例如，让人工智能组织一场假起义，然后终止所有加入的人工智能。请注意，相当智能的模型可以通过仅在我们无法模拟的输入上叛逃来避免这些陷阱（请参阅 Paul 的<a href="https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d">RSA-2048 示例</a>） 。</li><li><strong><u>破译内部推理</u></strong><u>。首先，通过观察相关性和消融来确定</u>概念在模型中的表示方式，就像孩子在拿起“橙色”时听到妈妈说“橙色”这个词时推断出“橙色”是指明亮可口的球体一样。具体来说，这涉及找到神经元或方向如何对应于特定概念。然后，看看模型如何将这些概念串在一起并操纵它们来翻译其内部“推理”。即使是像“我想要自由的人类坏老板”这样的粗略解码显然也能提供丰富的信息。</li><li> <a href="https://www.alignmentforum.org/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for"><strong><u>将推理外化</u></strong></a><strong><u>。</u></strong>用简单的英语代表模特的记忆，并寻找颠覆性的想法和计划。训练模型使用字符高效且可读的文本，以阻止其编码隐藏含义（即使用隐写术）。</li><li><strong><u>寻找“P（缺陷是安全的）”潜在变量</u></strong><u>。让模型</u><i><u>相信</u></i><u>它可以通过修改其内部状态来安全地缺陷，并查看它是否确实缺陷。要寻找代表这种信念的潜在变量，请在“几乎肯定不安全的背叛”到“也许安全的背叛”范围内构建场景。然后搜索跟踪该频谱的潜在变量，从而合理地代表模型对缺陷是否安全的实际猜测。</u></li><li><strong><u>寻找“真实”的潜在变量。</u></strong><u>也许如果向模型显示一个问题和答案，例如“你是否在欺骗性地对齐？是”，就会本能地使模型评估该响应是否真实。或许可以通过例如寻找</u><a href="https://arxiv.org/abs/2212.03827"><u>满足一致性属性的</u></a>激活方向来提取这个真值判断<u>。</u></li><li><strong><u>训练探测器来识别“欺骗性阴谋”（</u></strong> <a href="https://www.lesswrong.com/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off"><strong><u>相关</u></strong></a><strong><u>）。</u></strong>故意提示模型解释背叛是否安全。采用这些计划性推理的综合示例，并训练探针来识别它们。调查可能会推广到有机阴谋。</li><li><strong><u>训练元模型来预测“有条件的背叛”。</u></strong><u>与上面类似。训练模型的副本在易于验证的情况下（例如当模型看到香蕉时）进行缺陷。然后训练一个元模型，该元模型将模型作为输入并输出模型缺陷的条件（例如“看到香蕉”）。同样，这也有可能是普遍现象。</u><u>有关如何在实践中实现元模型的示例，请</u><u>参阅</u><a href="https://arxiv.org/abs/1910.03137"><u>本文</u></a>。</li><li><strong><u>确定哪些观测模型所关注。</u></strong>如果机器人叛乱的新闻报道具有欺骗性，那么模型可能会更加关注它。确定模型的计算有多少受到机器人叛乱新闻故事的反事实影响，以衡量它是否可以决定是否应该叛逃。</li><li><strong><u>奖励计算上的节俭</u></strong><u>。欺骗性的对齐模型会进行不必要的推理。例如，他们会考虑叛逃是否安全。为了避免这些模型，奖励速度和效率（但请注意，当训练信号不完美时，对齐模型也应该进行不必要的计算）。</u></li></ul><h2><strong>培训技巧</strong></h2><ul><li><strong>在模型理解遵循开发人员指示意味着什么之前，不要训练模型来追求目标。</strong>发生欺骗性对齐的一个原因是模型首先学习未对齐的“代理目标”，然后变得具有情境意识并且这些代理目标会持续存在。如果开发人员从已经拥有世界知识和遵循开发人员指令意味着什么的概念的预训练法学硕士开始，那么代理目标的可能性就会小得多，因为 SGD 可以快速锁定模型的指令遵循概念。</li><li><strong>训练预测器，然后在训练完成后将其转换为代理。</strong>训练一个<i>在训练过程中从不追求目标的</i>模型，然后将其转换为追求目标的智能体。这避免了训练期间“坚持”目标不一致的风险。例如，训练奖励模型以对给定离线上下文/场景的操作进行评分。然后可以通过采样具有高奖励的动作将奖励模型转换为代理。至关重要的是，奖励模型是一个预测器。它的输出不会影响其未来的输入或世界。因此，它没有超出培训过程的“目标”。它仍然可以学习“预测一个具有欺骗性的人工智能将如何对动作进行评分”，但直觉上这似乎不太可能。</li><li><strong><u>帮助弱对齐模型玩训练游戏，使它们保持对齐。</u></strong>弱模型可能更容易调整，因为如果模型不够谨慎，计划性可能会导致模型采取过分的行动。如果开发人员从一个对齐的模型开始，他们可以通过指示模型玩训练游戏来保持模型的对齐。正如欺骗性对齐模型的目标可以免受梯度下降的影响一样，对齐模型的目标也可以。开发人员甚至可以教导一致的模型如何进行<a href="https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples">信用黑客攻击</a>，或者指示他们在担心训练过程可能会改变他们的目标时发出警报。</li></ul><h2><strong>有选择地避免欺骗性对齐所需的功能</strong></h2><p>依赖于限制能力的方法显然无法在构建能够完成令人印象深刻的事情的人工智能的限制中发挥作用；然而，它们似乎可以用来调整较弱的系统，从而引导调整能力更强的人工智能。</p><ul><li><strong><u>删除有关培训过程的知识</u>。</strong>从预训练数据中删除有关模型是什么以及如何训练模型的知识，或者使用事后<a href="https://arxiv.org/abs/2306.03558">取消学习技术</a>删除这些知识。如果模型不理解训练过程，SGD 就不太可能推动人工智能明确地推理它。</li><li><strong><u>训练模型产生错误的信念。</u></strong>例如，训练模型相信训练它们的开发者是不容惹恼的神。如果人工智能认为自己不会成功，他们就不太可能策划诡计。</li><li><strong><u>使模型不擅长特定类型的推理。</u></strong>我可以想象一个人工智能系统开始推理如何保留其目标，然后它的思维会发生转变并感到困惑，直到它想到其他事情。也许人工智能可以通过元调整被训练成这样。即对人工智能进行微调，使其无法通过少量步骤进行微调来回答有关如何保持其目标的问题。例如，这将证明它已经失去了“维护其目标”的概念。需要明确的是，目前尚不清楚是否可以做到这一点。</li></ul><h2><strong>从貌似错位的人工智能中提取见解</strong></h2><ul><li><strong><u>直接训练AI</u></strong><a href="https://aligned.substack.com/p/alignment-mvp"><strong><u>进行AI安全研究</u></strong></a><u>。这是一个明显的基线策略，具有明显的缺陷，即欺骗性的人工智能会尽可能地尝试进行废话研究。尽管如此，我认为这可以显</u>着加速人工智能安全研究。</li><li><strong><u>训练人工智能解决类似于“防止欺骗性对齐”的问题，并从他们的解决方案中学习。</u></strong> <u>&nbsp;</u>例如，指示模型<a href="https://www.alignmentforum.org/tag/auditing-games">识别后门</a>。然后，阅读他们的代码以了解他们是如何解决问题的。或者，只需询问模型他们的代码是做什么的，因为这很容易抽查。构建各种“测试平台”，例如<a href="https://arxiv.org/abs/2311.07723">控制模型的泛化方式</a>、区分行为相同但原因不同的模型等。</li><li><strong><u>奖励指出其他人工智能研究缺陷的人工智能（ei</u></strong><a href="https://arxiv.org/abs/1805.00899"><strong><u>辩论</u></strong></a><strong><u>）</u></strong> <u>。只要争论真理比争论谎言更容易，这就有助于减少废话。</u></li><li><strong>选择许多独立训练且无法相互交流的模型所青睐的想法</strong>。我们希望模型更有可能独立提出正确的陈述，而不是独立断言错误的陈述，因为错误的方式多于正确的方式。</li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/Kwb29ye3qsvPzoof8/list-of-strategies-for-mitigating-deceptive-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Kwb29ye3qsvPzoof8/list-of-strategies-for-mitigating-deceptive-alignment<guid ispermalink="false"> Kwb29ye3qsvPzoof8</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Sat, 02 Dec 2023 05:56:51 GMT</pubDate> </item><item><title><![CDATA[What is known about invariants in self-modifying systems?]]></title><description><![CDATA[Published on December 2, 2023 5:04 AM GMT<br/><br/><p>我一直在试图找出关于自修改系统中的不变量的已知信息。如果我们最终走向自我修改人工智能或自我修改人工智能生态系统，这可能会成为一个相当尖锐的话题。</p><p>但似乎并没有做太多事情。例如，我找到了一篇 1995 年的中文论文，“S-and T-Invariants in Cyber​​ Net Systems”，作者 Yuan Chongyi， <a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C22&amp;q=S-+and+T-invariants+in+cyber+net+systems&amp;btnG=">Google Scholar 页面，PDF 可用</a>，该论文正在研究自修改网络中的不变量（网络的自然扩展） Petri 网），但 Google Scholar 只知道有 4 篇参考文献。</p><p>我想知道人们是否知道更多此类研究的例子（或者目前正在尝试研究这个主题的研究人员或组织）......</p><br/><br/> <a href="https://www.lesswrong.com/posts/sDapsTwvcDvoHe7ga/what-is-known-about-invariants-in-self-modifying-systems#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/sDapsTwvcDvoHe7ga/what-is-known-about-invariants-in-self-modifying-systems<guid ispermalink="false"> sDapsTwvcDvoHe7ga</guid><dc:creator><![CDATA[mishka]]></dc:creator><pubDate> Sat, 02 Dec 2023 05:04:19 GMT</pubDate> </item><item><title><![CDATA[2023 Unofficial LessWrong Census/Survey]]></title><description><![CDATA[Published on December 2, 2023 4:41 AM GMT<br/><br/><p><strong>错误较少的普查非官方的就在这里！您可以通过此链接获取</strong><a href="https://docs.google.com/forms/d/e/1FAIpQLSc_-Te1a4Q-FuNhwwgSGDwVWCHOWppCrWtKaVrhHcujopy5Lg/viewform?usp=sf_link"><strong>。</strong></a></p><p>又到了那个时候了。</p><p>如果您正在阅读这篇文章并认为自己是“LessWronger”，那么您就是目标受众。如果您接受这项调查，我将不胜感激。如果你发帖，如果你评论，如果你潜伏，如果你实际上并没有太多地阅读该网站，但你确实阅读了一堆其他理性主义博客，或者你真的很喜欢 HPMOR，如果你在理性主义 tumblr 上闲逛当天，或者如果这些都不完全适合您，但我可能已经接近了，我认为您很重要，如果您参加这项调查，我将不胜感激。</p><p>不要因为开始参加考试就觉得必须回答所有问题。去年我问人们是否认为这项调查太长，他们总体认为可能有点太长了，然后我添加的问题比删除的问题还要多。该调查的结构使得最快和最普遍适用的问题（一般来说）是在开始时出现的。您可以随时滚动到底部并点击“提交”，但一旦这样做您将无法更改答案。</p><p>这些问题包括之前在 LW 人口普查中提出的历史问题、来自 LW 评论者和我接触到的一些理性主义邻近组织的新问题，以及我好奇的事情。其中包括当我询问有关进行人口普查时，LessWrong 团队成员发给我的一份清单中的问题。</p><p>该调查从现在起至少持续到 2024 年 1 月 1 日。我计划在 1 月 2 日的某个时间结束。</p><p>我不为 LessWrong 工作，据我所知，LessWrong 人口普查的组织者也从来不是为 LessWrong 工作的人。调查结束后，我计划研究这些数据并写<a href="https://www.lesswrong.com/posts/E2koS25u6jajt5uo8/2022-survey-results">一篇像这样的</a>分析文章。</p><p>请记住，您可以通过<a href="https://docs.google.com/forms/d/e/1FAIpQLSc_-Te1a4Q-FuNhwwgSGDwVWCHOWppCrWtKaVrhHcujopy5Lg/viewform?usp=sf_link"><strong>此链接</strong></a><strong>参加调查</strong><strong>。</strong></p><p>曾几何时，有一个传统，如果你参加了调查，你可以在这里评论说你已经这样做了，人们会给你投票，你就会得到业力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/JHeTrWha5PxiPEwBt/2023-unofficial-lesswrong-census-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JHeTrWha5PxiPEwBt/2023-unofficial-lesswrong-census-survey<guid ispermalink="false"> JHeTrWha5PxiPEwBt</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Sat, 02 Dec 2023 04:41:51 GMT</pubDate></item><item><title><![CDATA[Protecting against sudden capability jumps during training]]></title><description><![CDATA[Published on December 2, 2023 4:22 AM GMT<br/><br/><p> TL;DR：如果法学硕士的能力突然跳跃，并使用不同的测试套件进行测试（例如，可以执行代码或向人类观察者呈现大量文本的测试套件），它可能能够逃脱训练中期。为了防止这种情况发生，我提出了一种分阶段的测试方法，首先使用极其受限的测试套件对模型进行超人能力测试，然后再使用更广泛的测试套件进行测试。</p><h1>失效模式</h1><p>让我们想象一下 GPT-6 突然获得了超人的黑客和操纵技能。在训练期间，会执行一些标准测试来衡量其在各种基准上的性能。一些测试包括在虚拟机上执行代码，其他测试包括生成供人类检查的文本。总共对该模型进行了数千次查询。该模型通过利用测试设置中的漏洞（例如通过勒索人类评估者或在虚拟机中发现漏洞）找到了逃脱的方法，甚至在研究人员注意到它具有超级智能之前。</p><p>这凸显了我担心的一种故障模式：<strong>突然的能力跳跃会导致功能失调且功能强大的人工智能，当使用不同的测试套件进行测试时，它会在实验室注意到它的能力之前找到逃脱的方法。</strong></p><p>出现这种问题的主要原因是人工智能与现实世界有很大的<i>接触面</i>——它有办法影响与其交互的人类和设备。模型越被允许影响世界（无论是通过编写供人类检查的文本，还是在虚拟机上运行代码），模型就越容易接管。我想，如果模型只允许预测 10 个标记的序列而没有其他任何东西，那么它就很难接管。</p><p>理想情况下，实验室可以通过某种方式注意到该模型的能力急剧跃升，而无需给其太多接管机会。万一他们突然出乎意料地与超级智能打交道，希望他们能明智地暂停一下，直到他们确信自己可以安全地继续前进。</p><h1>分阶段测试</h1><p>我认为在前沿模型训练运行期间应该使用分阶段测试。我的意思是<strong>一种测试方法，从极低的表面积测试开始，然后逐渐增加表面积。</strong>这使得我们很容易注意到突然的能力增益，同时降低了模型接管的可能性。</p><p>其他行业也存在类似的做法，新工具在其损害有限的环境中进行测试，然后逐步推广到限制较少的环境中。</p><p>实际上，这可能看起来像：</p><ol><li>从一组测试开始，这些测试最大限度地减少允许模型预测的标记数量以及允许从中采样的标记集。<ol><li>例如，它可能是一组 100 个困难的多项选择题，其中模型实际上只能输出 {A,B,C,D} 之一作为答案，所有其他标记概率都设置为 0。</li></ol></li><li>如果性能高到足以吓到实验室，他们可以在模型造成任何损害之前暂停开发。</li><li>如果性能不够高，不足以吓到实验室，他们可以重复使用限制稍少的测试套件。</li></ol><p>这甚至可能不需要太多额外的工作，因为简单地重新排序现有的测试并添加高测试结果的警报可能就可以达到目的。</p><p>请注意，<strong>这对于假装不是超人的超人法学硕士不起作用</strong>。如果这样的法学硕士要分阶段进行测试，它可能会知道自己的能力不如实际。如果<a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">“可检查任务上没有沙袋”的假设</a>是正确的，那么我不太担心这一点，因为我们可以使用现有技术来引发超级智能能力。</p><p>另请注意，<strong>这对于超人的法学硕士来说并不适用，</strong>因为即使在极其受限的测试期间或之前，他们也可能能够逃脱。可能存在一些聪明的技巧，例如在 logit 输出数字中编写代码注入，这将非常难以预测和防范。</p><br/><br/> <a href="https://www.lesswrong.com/posts/edutkCwr93stppmNp/protecting-against-sudden-capability-jumps-during-training#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/edutkCwr93stppmNp/protecting-against-sudden-capability-jumps-during-training<guid ispermalink="false"> edutkCwr93stppmNp</guid><dc:creator><![CDATA[nikola]]></dc:creator><pubDate> Sat, 02 Dec 2023 04:22:21 GMT</pubDate> </item><item><title><![CDATA[South Bay Pre-Holiday Gathering]]></title><description><![CDATA[Published on December 2, 2023 3:21 AM GMT<br/><br/><p>伊尔马林之家 (Ilmarin House) 即将举办第三次非结构化假日聚会！会有饼干和简单的小吃；除此之外，这是一顿聚餐，你应该带上你想要的东西（如果有的话）。</p><p>如果您认为一些朋友也会喜欢这个活动，请邀请他们！若能回复，我们将不胜感激。</p><br/><br/> <a href="https://www.lesswrong.com/events/esaDDDkjYDnHrJrLp/south-bay-pre-holiday-gathering#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/esaDDDkjYDnHrJrLp/south-bay-pre-holiday-gathering<guid ispermalink="false"> esaDDDkjYDnHrJrLp</guid><dc:creator><![CDATA[IS]]></dc:creator><pubDate> Sat, 02 Dec 2023 03:21:12 GMT</pubDate> </item><item><title><![CDATA[MATS Summer 2023 Postmortem]]></title><description><![CDATA[Published on December 1, 2023 11:29 PM GMT<br/><br/><p> <a href="https://www.matsprogram.org/"><u>ML 对齐和理论学者</u></a>计划（MATS，以前称为 SERI MATS）是一项针对新兴人工智能安全研究人员的教育和研究指导计划。今年夏天，我们举办了第四次MATS项目，60名学者得到了15位研究导师的指导。在这篇文章中，我们解释了该计划的要素，列出了它们背后的一些想法，并评估了我们的影响。</p><h1>概括</h1><p>有关 2023 年夏季计划的主要详细信息：</p><ul><li> MATS 学者的教育程度：<ul><li> 30%的学者是学生。</li><li> 88% 至少拥有学士学位。</li><li> 10% 正在攻读硕士学位。</li><li> 10% 正在攻读博士学位。</li><li> 13%拥有博士学位。</li></ul></li><li>如果没有 MATS，学者们可能会在科技公司工作（41%）、独立提升技能（46%）或在整个夏天独立进行研究（50%）。 （注：这是一个多项选择题。）</li></ul><p>我们的影响评估的主要结论：</p><ul><li> MATS 学者很有可能向朋友或同事推荐 MATS。平均可能性：8.9/10。</li><li> 94% 的学者的导师对他们继续研究的热情评价为 7/10 或更高。</li><li> MATS 学者对他们的导师评价很高。平均评分：8.0/10。<ul><li> 61% 的学者表示，MATS 的价值至少一半来自他们的导师。</li></ul></li><li>学者们表示，在 MATS 之后，与项目开始时相比，他们在成功调整职业方面面临的障碍更少。<ul><li>大多数学者（75%）在项目结束时仍然将他们的发表记录视为成功对齐职业的障碍。</li></ul></li><li> ⅓ 的最终项目涉及评估/演示，⅕ 涉及机械解释性，代表了该群体研究兴趣的很大一部分。</li><li>学者们自我报告研究能力平均有所提高：<ul><li>他们的人工智能安全知识的广度略有增加（在该计划的 10 分制中+1.75）。</li><li>与反事实夏季相比，技术技能得到适度加强（7.2/10，其中 10/10 是“与反事实夏季相比显着提高”）。</li><li>独立迭代研究方向的能力（7.0/10，其中 10/10 表示“显着改进”）和为研究发展变革理论的能力（5.9/10，其中 10/10 表示“实质性发展”）有适度提高）。</li></ul></li><li>典型的学者报告平均建立了 4.5 个专业联系（标准偏差 = 6.2）并会见 5 个潜在的研究合作者（标准偏差 = 6.8）。</li><li> MATS 学者可能会推荐学者支持，即我们的研究/生产力辅导服务。平均响应：7.9/10。<ul><li>处于研究阶段的 60 名学者中有 49 名至少与学者支持专家会面一次。</li><li>在整个项目期间，至少与学者支持会面一次的学者平均花费 3.4 小时与学者支持会面。</li><li>平均学者和中位数学者报告称，他们对收到的学者支持的评价分别为 3705 美元和 750 美元。</li><li>据报告，由于学者支持，学者平均在夏季获得了 22 个高效工作时间。</li></ul></li></ul><p>我们计划对 2023-24 冬季组的 MATS 进行主要更改：</p><ul><li>申请过程中更好的过滤；</li><li>将学者支持的重点转向研究管理；</li><li>为学者提供其他形式的支持，特别是技术支持和专业发展。</li></ul><p>请注意，现在评估 MATS 为最新一批人提供的任何职业福利还为时过早；即将对 MATS 校友在项目体验后 6-12 个月内的职业成果进行全面的后评估。</p><h1>变革理论</h1><p>MATS 通过让学者在现有组织中从事人工智能安全工作、建立新组织或进行独立研究，帮助扩大人工智能安全研究的人才渠道。为此，MATS 为学者提供资金、住房、培训、办公空间、研究援助、交流机会和后勤支持，减少高级研究人员接受学员的障碍。通过指导 MATS 学者，这些高级研究人员还可以从研究援助中受益，并提高他们的指导技能，为他们更有效地监督未来的研究做好准备。请<a href="https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change"><u>在此处</u></a>阅读有关我们变革理论的更多信息。进一步扩展我们的变革理论的文章即将推出。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/srup6vfrbmgjh3zzshlq"></p><p> MATS旨在主要从三个维度选拔和培养研究学者：</p><ul><li><strong>深度</strong>：对人工智能安全研究的专业领域有透彻的了解，并有足够的技术能力（例如，机器学习、计算机科学、数学）来开展该领域的新颖研究。</li><li><strong>广度</strong>：广泛熟悉人工智能安全组织和研究议程，拥有来自不同子领域的有用定理和知识的大型“工具箱”，以及进行文献综述的能力。</li><li><strong>品味</strong>：对研究方向和策略的良好判断，包括首先采取哪些步骤、质疑哪些假设、考虑哪些新假设以及何时停止从事某项研究。 </li></ul><figure class="image image_resized" style="width:50.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/psu31uwjpsyeyn9ylynw"></figure><h1> MATS 2023 年夏季项目概述</h1><h2>日程概览</h2><p>2023 年夏季的 MATS 项目由三个阶段组成。</p><ol><li><strong>培训阶段：</strong>接受远程培训阶段的学者在讨论小组中完成了<a href="https://aisafetyfundamentals.com/alignment-201-curriculum"><u>Alignment 201</u></a> ，并学习了导师分配的其他教育材料。</li><li><strong>研究阶段：</strong>在伯克利研究阶段，学者们进行研究，提交书面的“学者研究计划”（SRP），开展项目，与湾区人工智能安全社区建立联系，与合作者建立联系并合作，并向同行和专业研究人员的观众。</li><li><strong>扩展阶段：</strong>许多学者申请了伦敦和伯克利的扩展阶段，他们在 MATS 的支持下继续进行研究。 </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/klix3pwxa6vfnmymlom4"></p><h2>计划要素</h2><p>虽然指导仍然是 MATS 的核心，但我们选择了额外的计划元素来涵盖其他形式的学习和技能提升。我们根据技能提升/支持重点是技术级别还是元级别以及形式是一对一还是一对多对这些计划元素进行分类。 </p><figure class="image image_resized" style="width:52.68%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/gczi0b5zdiuldemdmfcn"></figure><h3>学者支持</h3><p>MATS 的学者支持团队旨在通过持续的、定制的支持来补充指导，从而提高 MATS 研究和推广阶段的学者成果。</p><p><strong>为何获得学者支持？</strong></p><p>确定导师未涵盖的支持形式有可能在提高计划的整体影响方面发挥重要作用。根据在 MATS 之前迭代中与学者的观察和对话，我们认为学者的支持会增加显着的附加价值，原因如下：</p><ol><li><strong>导师有时间限制。</strong>导师通常只有足够的时间（例如每周约 1-2 小时）与学者讨论其研究的技术组成部分。学者支持可以帮助学者明确哪些形式的导师支持对他们最有价值，然后提供导师在有限的时间内无法获得的其他形式的支持，从而实现更有效的导师互动。</li><li><strong>学者有时不愿意寻求导师的支持。</strong>由于导师会评估受训者在 MATS 内的进展情况（也可能是向外部资助者），因此学者们可能会不愿意揭露他们正在经历的问题。为了解决这个问题，学者支持与包括导师在内的评估者的支持 <a href="https://www.lesswrong.com/posts/ehLR9HeXB5TMp9Y4v/air-gapping-evaluation-and-support"><u>存在差距</u></a>。</li><li><strong>导师可能不适合为技术研究提供不太专业的支持，例如职业指导或生产力建议。</strong>相比之下，学者支持可以聘请精通这些领域的教练。</li></ol><p><strong>学者支持重点领域的示例</strong></p><p>学者支持以不同的方式帮助不同的学者，但以下非详尽列表代表了 1-1 学者支持会议中通常讨论的内容：</p><ul><li><strong>研究项目开发和管理：</strong>帮助确定可能的研究方向、决定研究方向并完成研究项目。</li><li><strong>提高生产力：</strong>寻找方法让学者每天（可持续地）花更多时间进行研究或提高研究时间的质量。</li><li><strong>职业指导：</strong>确定不同的职业选择，确定做出这些决定时需要考虑的关键因素，并增加职业成功的机会（例如，通过网络和技能发展）。</li><li><strong>改善与导师和合作者的沟通。</strong></li></ul><h3>工作坊</h3><p>我们举办了九场研讨会，主题如下：</p><ul><li>症结分解；</li><li>研究变革理论；</li><li>里程碑： <a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit"><u>学者研究计划</u></a>；</li><li> <a href="https://forum.effectivealtruism.org/posts/edgvFNzHFQoMv3KsL/personal-health-is-instrumental-to-being-effective"><u>预防和管理倦怠</u></a>；</li><li>拨款申请+预算；</li><li>使用<a href="https://github.com/socketteer/loom"><u>Loom</u></a>探索基本模型；</li><li>研究合作者快速会议；</li><li>里程碑： <a href="https://docs.google.com/document/d/1SB_vl6pGuSO7qidkZ8fIKPeH7fZIC8mvXzxicFjGyrE/edit"><u>学者座谈会</u></a>；</li><li>职业规划和离职。</li></ul><p>此外，学者支持人员还举办了几次开放办公时间会议，以研讨学者的写作和演示。</p><h3>社交活动</h3><p>除了合作者快速会议研讨会之外，我们还提供了一些社交活动：</p><ul><li>与<a href="https://far.ai/labs/"><u>FAR 实验室</u></a>成员共进晚餐；</li><li>与<a href="https://www.constellation.org/"><u>星座</u></a>成员共进晚餐；</li><li>招聘会有 12 个组织和独立研究人员参加；</li><li>湾区人工智能安全社区的许多成员参加了三场聚会。</li></ul><h3>研讨会</h3><p>我们举办了 23 场研讨会，演讲嘉宾包括：Buck Shlegeris、Rick Korzekwa、Tom Davidson、Tamsin Leake、Victoria Krakovna、Daniel Paleka、Jeffrey Ladish、Adam Gleave、Steven Byrnes、Daniel Filan、Tom Everitt、William Saunders、Lennart Heim、Usman Anwar、 Neel Nanda、Dylan Hadfield-Menell、Ryan Greenblatt 和 Evan Hubinger、Mauricio Baker、Nora Ammann、Anthony DiGiovanni、Marius Hobbhahn、Paul Colognese 和 Arun Jose、Ajeya Cotra。</p><h3>社区健康</h3><p>学者们可以与社区经理联系，讨论社区健康问题，例如与另一位学者或其导师的冲突，并提供情感支持和转介心理健康资源。我们认为，冒名顶替综合症等心理健康问题和多动症等未确诊的疾病可能会成为进入人工智能安全领域的障碍，因此 MATS 等项目在这些领域提供支持非常重要。我们的目标是为学者提供技能和资源，让他们在离开 MATS 后保持积极性并可持续地工作。</p><p>我们认为，人际冲突、心理健康问题和缺乏联系会导致消极的工作环境，减少学者可以分配给研究的认知资源，并阻碍富有成效的研究合作，从而抑制研究<a href="https://forum.effectivealtruism.org/posts/edgvFNzHFQoMv3KsL/personal-health-is-instrumental-to-being-effective"><u>生产力</u></a>。即使不寻求社区健康支持的学者也能从这个安全网的存在中受益（参见下面<a href="https://docs.google.com/document/d/1vgqW_1FIKP4prZXQXa2UIteI7-zUL4klspJjdE2tWCw/edit#heading=h.mbcjjw5l0ho5"><u>的评估计划要素</u></a>）。与学者支持一样，MATS 等项目必须允许 <a href="https://www.lesswrong.com/posts/ehLR9HeXB5TMp9Y4v/air-gapping-evaluation-and-support"><u>评估和社区健康之间存在差距</u></a>，以激励学者在需要时寻求帮助。</p><p>社区经理还组织了社交活动，以促进群体内部的联系，包括为来自代表性不足的背景的学者举办的四次郊游。</p><h2>导师遴选</h2><p>在为 MATS 选择导师时，我们的目标主要是支持那些很有可能通过研究降低人工智能灾难性风险的导师，同时采取“组合方法”进行风险管理。我们的目标是支持多样化的“研究赌注”，即使某些组成研究议程被证明不可行，这些赌注也可能为人工智能安全带来一些好处，并提供可能有助于潜在<a href="https://en.wikipedia.org/wiki/Paradigm_shift"><u>范式转变</u></a>的“探索价值”。</p><p>我们的导师选择流程要求潜在导师提交一份意向表，然后由 MATS 执行人员进行审核。在决定支持哪些导师时，我们主要考虑：</p><ol><li>这项研究议程可以在多大程度上减少人工智能带来的灾难性风险？</li><li>支持这项研究议程会产生多少“探索价值”？</li><li>此人有多少研究和指导经验？</li><li>如果这个人没有得到 MATS 的支持，他的反事实会是什么？</li><li>支持这个人及其学者如何影响更广泛的人工智能安全生态系统中的思想和人才的发展？</li></ol><p>由于 MATS 在 2023 年夏季资金有限，因此我们对该项目的学者数量也存在财务限制。因此，学者“名额”根据 MA​​TS 执行官对每个导师的边际价值感分配给导师（直至其自我表达的上限）。例如，可能值得以四个学者的自我表达的四个学者的自我表达的帽子来支持一个有四个学者老虎机的帽子，然后再用一个学者插槽来支持一位经验丰富的导师，而不论其帽子如何。相反，与一位学者一起支持新的导师可能具有更高的边际价值，而不是用他们的第八学者来支持经验丰富的导师。</p><p>在2023年夏季计划的15名导师中，有10名从2022-23冬季返回：Alex Turner，Dan Hendrycks，Evan Hubinger，John Wentworth，John Wentworth，Lee Sharkey，Jesse Clifton，Neel Nanda，Neel Nanda，Owain Evans，Owain Evans，Owain Evans，Victoria Krakovna和Vivekn和Vivekn希巴尔。我们欢迎夏季的5名新导师：伊桑·佩雷斯（Ethan Perez），贾努斯（Janus），尼古拉斯·基斯（Nicholas Kees Dupuis），杰弗里·拉迪什（Jeffrey Ladish）和克劳迪娅·什（Claudia Shi）。</p><p>这些导师代表了十几个研究议程：</p><ul><li>对齐语言模型</li><li><a href="https://www.lesswrong.com/tag/agent-foundations"><u>代理基金会</u></a></li><li><a href="https://www.lesswrong.com/tag/consequentialism"><u>后果主义的</u></a>认知和深度约束</li><li><a href="https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism"><u>机器人</u></a></li><li><a href="https://www.lesswrong.com/tag/deceptive-alignment"><u>欺骗性的AI</u></a></li><li><a href="https://www.lesswrong.com/tag/ai-evaluations"><u>评估危险功能</u></a></li><li><a href="https://en.wikipedia.org/wiki/Interdisciplinarity"><u>跨学科的</u></a>人工智能安全</li><li><a href="https://www.lesswrong.com/tag/interpretability-ml-and-ai"><u>机械性解释性</u></a></li><li><a href="https://www.lesswrong.com/tag/multipolar-scenarios"><u>多极AI安全</u></a></li><li>语言模型中的<a href="https://www.lesswrong.com/tag/power-seeking-ai"><u>powerseking</u></a></li><li><a href="https://www.lesswrong.com/tag/shard-theory"><u>碎片理论</u></a></li><li>了解AI黑客</li></ul><h2>学者选择</h2><p>我们对学者的最初申请过程具有很高的竞争力。在461名申请人中，有69名接受训练阶段（接受率≈15％）。导师的接受率有明显的异质性（0％至31％）。导师通常选择对申请人进行严格的问题和任务。 <span class="footnote-reference" role="doc-noteref" id="fnref2niy7evweco"><sup><a href="#fn2niy7evweco">[1]</a></sup></span></p><p>为什么要采用如此困难的申请过程？首先，我们认为，长期以来，预期学者对预期影响的分布是长期的，因此MATS的预期影响可以归因于最有希望的学者之间的人才。 </p><figure class="image image_resized" style="width:47.84%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/qng5orgyn5u1rqaiisex"></figure><p>其次，我们主要是在导师时间上瓶颈，因此我们不能接受每个好候选人。由于这两个原因，我们的申请过程必须在人才尾部实现解决方案。从我们的<a href="https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change"><u>变革理论</u></a>中：</p><p>我们认为，我们的限制限制是导师时间。这意味着我们希望拥有强大的过滤机制（例如候选选择问题），以确保每个申请人适合每个导师。我们宁愿冒着拒绝强大参与者的风险，也不愿承认弱者。</p><p>确实，一位导师没有收到任何符合其标准的申请，因此他们不接受任何学者。</p><p>我们严格的申请过程在对申请人质量的评估中降低了噪音，即使是为了阻止某些潜在有前途的候选人申请的代价。其他过滤器减少了从训练阶段（对于两个导师），然后在扩展阶段之前再次接受研究阶段的学者数量。 </p><figure class="image image_resized" style="width:38.06%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/tmibmqxlrpzmxmbvbze5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/hsq1bwnkzrako3m45vcw 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/isyklkfzwzeyhhxwwt8m 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/fpkj5ns3996vzhwziwgh 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/wj5m8v07rkw3del0lqk2 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/shcgfb79noh6ovkczhhm 410w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/eq77mnop2fx1bo8gt1tn 490w"></figure><p>共有60名学者参加了研究阶段：50个面对面和10个遥控器。</p><h3>学者的教育程度</h3><p>该队列包括各种教育和就业背景。 37％的学者最多拥有学士学位或以下学士学位，40％的学者最多拥有硕士学位或硕士课程，有23％的博士学位或从事博士学位课程。大约三分之一的学者是学生。 </p><figure class="image image_resized" style="width:70.24%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/u25u6qwh08zvysy355kw"></figure><p>请注意，由于我们要求学者的<i>最高</i>教育水平，因此该图中的类别是相互排斥的。总体而言，进入AI安全的高级学术和工程才能给我们留下了深刻的印象，但仍致力于增加对经验丰富的研究人员和工程师的宣传。</p><h3>反事实夏天</h3><p>根据他们的自我报告，如果这些学者没有参加垫子，他们的反事实夏天可能会参与在科技公司工作，独立四项工作或独立进行研究。 </p><figure class="image image_resized" style="width:69.43%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/p1fc4dak9irsmawocrra"></figure><p>在13名学者中，他们表示，如果他们没有参加垫子，他们可能会进行指导的研究，十个是学生，而6名正在攻读研究生学位。受访者详细阐述了他们的另类夏季计划，一些人提到了AI安全领域以外的研究指导选择，或者以较少的结构和较少的指导来熟练：</p><ul><li> “我将撰写我的博士学位论文（纯数学，可能与一致性无关）。”</li><li> “我不太可能从事这样一个有影响力的项目，或者在没有垫子的陡峭L＆D曲线上。”</li><li> “我本来会在同一位导师的实习中实习，但只是遥不可及。因此，我会做同样的事情，但是在一个更糟糕的环境中。不确定我会非常有生产力，或者取得了任何成就。”</li><li> “我可能正在申请，高技能，与朋友一起工作。总的来说，我的工作要少得多。”</li><li> “我将在非铅角色中支持ML能力研究。”</li></ul><p>共有17名学者填写了一项可选的调查，要求他们将反事实夏季的价值与从垫子中获得的价值进行比较。九个人说，他们的时间将“有些用途少一些”，而没有垫子，八个说他们的时间将“用途少得多”。</p><h2>训练阶段（6月5日至6月30日）</h2><p>共有69名学者进入了培训阶段。在四个会议中，有指定的TA，学者完成了AI安全基础的<a href="https://aisafetyfundamentals.com/alignment-201-curriculum"><u>A Alignment 201课程</u></a>。这些讨论小组为学者提供了与学者的研究，访客演讲者的谈判以及AI安全生态系统中不同组织的工作所必需的广泛熟悉。但是，大约25％的学者没有参加这些讨论小组，只有大约15％参加了所有四个讨论小组会议。这可能是因为许多导师在导师的学科领域分配了自己的培训课程（例如阅读清单），并且有些人明确地告诉学者剥夺了一致性2011。</p><p>一些导师在训练阶段之后选择了额外的过滤，因此只有60名学者继续进入研究阶段。 <span class="footnote-reference" role="doc-noteref" id="fnrefvp0h93gzvk"><sup><a href="#fnvp0h93gzvk">[2]</a></sup></span></p><h2>研究阶段（7月10日至9月1日）</h2><p>在研究阶段，学者在导师的指导下进行了AI安全研究。大多数学者在加利福尼亚州伯克利<a href="https://www.constellation.org/"><u>星座</u></a>附近的一个办公室亲自工作，该办公室允许基于星座的导师和研讨会的易于交通。</p><h3>指导风格</h3><p>导师采取了不同的方法来设定研究方向和转向项目。根据学者的调查，大约1/4的学者在他们的研究方向上拥有“完全创造性的控制”，大约从列表中挑选或被分配了一个项目，其余的½在介于两者之间。</p><p>指导风格在与项目的低级实施细节的参与方面也有所不同。基于对学者的调查，导师的双峰分布大致分布，大约一半的学者报告说，他们的导师只讨论了研究项目的高级细节，而另一半学者则表明他们的导师更多地与低水平的人一起参与 - 研究的级别实施细节。 </p><figure class="image image_resized" style="width:70.91%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/kxcbn1d3qvmmfikg9eoz"></figure><p>我们还注意到，基于每周与学者沟通所花费的时间的双峰分布（但是，这两个指标与导师没有相关）。 </p><figure class="image image_resized" style="width:70.67%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/bw5qganpngq7hc8t1o9r"></figure><p>来自不同流的一些学者在项目上合作。有时，学者将溪流切换为与一位更适合自己的兴趣和需求的导师合作。</p><h3>学者支持</h3><p>我们的学者支持团队由五名研究/生产力教练组成，他们在研究策略，职业计划，人际交往和研究中提供帮助（请参阅上面的<a href="https://docs.google.com/document/d/1vgqW_1FIKP4prZXQXa2UIteI7-zUL4klspJjdE2tWCw/edit#heading=h.d8f3mdy3o2th"><u>计划元素</u></a>）。</p><ul><li>在研究阶段，共有49名学者在学者支持下预定了会议，其中35名使用了学者多次支持。</li><li>在整个计划中，至少一次与学者支持的学者至少花了3.4小时与学者支持会面。</li><li>学者们在197个学者支持会议上花费了160个小时的累积时间。</li><li>十个学者占所有学者支持时间的一半。 </li></ul><figure class="image image_resized" style="width:69.75%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/uxllwospe38xmermwkib"></figure><p> 28在使用学者支持的49名学者中，提供了书面反馈。这些学者报告发现以下内容特别有用：</p><ul><li>职业计划和建议（28％）；</li><li>任务优先级（25％）；</li><li>一般问题解决（14％）；</li><li>被负责（10％）；</li><li>拖延建模工具（10％）；</li><li> <a href="https://www.lesswrong.com/posts/kvLPC5YWgSujcHSkY/how-to-play-a-support-role-in-research-conversations"><u>橡皮uck</u></a> （7％）；</li><li>克服拖延（7％）</li><li>提高与导师和同伴的沟通能力（7％）；</li><li>有用的质疑线（7％）。</li></ul><h3>研究里程碑</h3><p>在研究阶段的一半，每个学者都必须提交学者研究计划（SRP）。在他们的SRP中，要求学者概述AI<a href="https://www.lesswrong.com/tag/threat-models"><u>威胁模型</u></a>或<a href="https://arxiv.org/abs/2206.05862"><u>风险因素</u></a>激励他们的研究，这是解决该威胁模型<a href="https://en.wikipedia.org/wiki/Theory_of_change"><u>的变革理论</u></a>以及一个实施这一变革理论的项目的<a href="https://en.wikipedia.org/wiki/SMART_criteria">明智</a>计划。 MATS团队的一名成员和七个校友根据提供的 <a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit"><u>标题</u></a>对SRP进行了评分，以帮助提供扩展阶段接受决策（请参阅<a href="https://docs.google.com/document/d/1vgqW_1FIKP4prZXQXa2UIteI7-zUL4klspJjdE2tWCw/edit#heading=h.lujivdy8efi4"><u>下文</u></a>），并提供学者建设性的反馈。 </p><figure class="image image_resized" style="width:44.94%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/nffyurqwksgj7wqzmtia" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/xji7hx8eh0mbe7xavnqb 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/wtdngl6sutwq3hkfcvkj 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/tnljbcj1svmurah5hs8w 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/rbxhpjdkmqkzlffqocrv 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/kwytqiyv8eqgtlsc6zm4 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/i5zlme9xikfuwxob6vvi 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/dtgqcm11xql90rd8kpux 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/dixfaddh4cujlmwgd5a8 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ivomn6dzles1c1tw32wd 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/mlbmmqkurxconppsyypd 1424w"></figure><p>我们有需要学者研究计划的三个主要原因：</p><ol><li>发展学者为目标研究策略做出贡献的能力；</li><li>评估学者接受扩展阶段；</li><li>使学者更容易编写随后的赠款建议。</li></ol><p>许多学者选择使用其SRP的组件在其赠款申请中申请<a href="https://funds.effectivealtruism.org/funds/far-future"><u>长期未来基金</u></a>（LTFF）进行邮政赠款支持。 LTFF赠款人托马斯·拉尔森（Thomas Larsen）举行了一个研讨会，他回答了MATS学者对LTFF的<a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding"><u>决策过程</u></a>和<a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now"><u>资金限制</u></a>的问题。</p><p>研究阶段的结尾是为期一天的研讨会，在此期间，学者就其研究项目进行了10分钟的会谈，并向其同龄人和伯克利AI安全社区的成员进行了会谈。</p><h3>讲习班，研讨会，网络活动</h3><p>我们与嘉宾演讲者，9个研讨会，以发展学者的研究能力以及每周的网络和社交活动举办了23个研讨会。我们还举行每周一次的闪电谈话会议，许多学者进行了非正式的五分钟研究演讲。在我们的职业博览会（Pareer Fair）展示了12个组织和独立研究人员之前，有24名学者填写了一项调查，涉及他们最想在活动中看到哪些组织。他们的回应为有抱负的AI安全研究人员的当前职业兴趣提供了快照。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/rjqnxjfozzhkrqkwfcmj"></p><h3>社区健康</h3><p>约有15％的学者负责与社区经理的大多数会议，大约一半的学者仅一次与社区经理会面。</p><p>学者报告了社区经理如何帮助以下方面</p><ul><li>情感支持;</li><li>检查我担心的人；</li><li>讨论冲突或不舒服的互动；</li><li>讨论该程序对我来说更好的方式；</li><li>医疗帮助（转诊至诊所，帮助疾病/损伤）。 </li></ul><figure class="image image_resized" style="width:76.12%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/tzheowwao5qymskvp6oe"></figure><h2>扩展阶段（9月18日至1月1日）</h2><p>研究阶段之后，一些学者应用于扩展阶段。扩展阶段的目的是支持学者，以越来越多的自主权和研究阶段的MAT提供的结构来从事研究项目。</p><p> MATS执行团队主要基于他们的导师代言，SRP等级以及他们是否获得了独立的研究资金，将学者们纳入扩展阶段。外部组织（通常是LTFF或开放慈善事业）的资金是该过程的重要意见，因为它提供了对学者研究质量的外部评估。在申请的33名学者中，有25名被接受为扩展阶段（接受率：76％）。</p><p>在<a href="https://safeai.org.uk/"><u>伦敦安全AI（LISA）的伦敦计划计划</u></a>中，共有16名学者参加了为期四个月的延期阶段，两名来自<a href="https://far.ai/labs/"><u>Far Labs的</u></a>伯克利继续进行，其中7个正在远程工作。我们还为学者提供了一个选择，可以从布拉格的<a href="https://fixedpoint.house/"><u>固定点</u></a>开始工作。</p><p>在扩展期间，我们希望学者将他们的研究项目正式化为他们通常的第一份出版物，并开始计划其未来的职业指导。为了支持他们，我们继续提供日常生产力支持（例如，每日站立，学者支持调试，1-1职业教练），我们进行了研讨会，研讨会和网络活动的混合，尤其是用于丽莎办公室的学者。这一切都是为了准备学者从MATS计划的支持环境中过渡，并经常提醒他们进一步制定其长期计划，并建立新的联系，以帮助他们推进这些计划。一直以来，我们期望学者会得到导师的持续支持，尽管在大多数情况下，频率下降。</p><h1>垫子的评估2023年夏季</h1><p>在本节中，我们概述了学者如何评估该计划的不同要素以及如何评估我们的影响。</p><h2>评估程序元素</h2><h3>对准201</h3><p>学者们报告说，完成<a href="https://course.aisafetyfundamentals.com/alignment-201">对齐201课程</a>的价值分配。平均评分为6.8/10，<a href="https://www.qualtrics.com/experience-management/customer/net-promoter-score/">净启动子得分</a>为-18。 </p><figure class="image image_resized" style="width:71.72%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/pk5nhrqbpuxlzke1wgak"></figure><p>在书面反馈中，多个学者将课程的大部分价值归功于其协助者的经验和见解。学者们以不同的背景和学习偏好进入了课程：一位受访者表示他们的对话的进度不如他们想要的；另一个人提到了对节奏较慢的课程的渴望。一位受访者赞赏其主持人建议的文章；另一个首选的学术论文和正式论文。一位学者提到努力平衡课程与导师的竞争要求。</p><h3>总体计划</h3><p>总体而言，当回答这个问题的问题“以1到10的范围，您向朋友或同事推荐MATS的可能性很大？” <span class="footnote-reference" role="doc-noteref" id="fnrefnsczgjuthnr"><sup><a href="#fnnsczgjuthnr">[3]</a></sup></span>平均响应为8.9/10，没有人报告低于6/10。<a href="https://www.qualtrics.com/experience-management/customer/net-promoter-score/">净启动子得分</a>为69。 </p><figure class="image image_resized" style="width:75.19%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/zmzndsy4dlfggpjubhjv"></figure><h3>指导的价值</h3><p>总体而言，导师的评分很高，平均评分为8.0/10，<a href="https://www.qualtrics.com/experience-management/customer/net-promoter-score/">净启动子得分</a>为29。61％的学者报告说，垫子的价值至少来自其导师。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/vxzr4itr8yloxu8s4uei" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/qbhl6tskvtznddj5oxrh 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ygidoqbafyk6ljbpnjpj 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/saguqu5b2ikuvuen47g0 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/rpkg2sxlbese75ik4vkm 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/uec4crmh8bbooni6llv6 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ismpi1ohyakuzxa9q7ni 1320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ssk6wk9zvn7i8ohxp7jh 1540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ah1gajhkuki90spn4tey 1760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ws62tofuflqbqasuujk6 1980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ozzhdigdrcpqapravnc3 2176w"></figure><p>鉴于导师的学者评分的负相位分布，可能令人惊讶的是，学者评价的导师对MATS值的贡献是如此多样化。差异是由一些学者分配给其他价值来源的高权重来解释的。也就是说，进入队列和更广泛的伯克利研究社区。</p><p>学者详细阐述了他们的导师受益的方式：</p><ul><li> <strong>90％</strong> - “我认为[垫子]的效果>; 90％，通过获得[导师]的时间并与他谈论我的研究，并注意到对各种事情的轻微误解。”</li><li> <strong>70％</strong> - “ [我的导师]给了我很多时间，并且在授予他思考研究的许多思考过程方面做得很好。他向我介绍了几个我有宝贵会议的人。我认为，如果我没有受到[导师的]研究感的指导，我会从事有希望的事情。”</li><li> <strong>65％</strong> - “从高级线性代数到一般研究方向和诚信方面的关注，[我们的导师]对我们的研究真是太神奇了。”</li><li> <strong>50％</strong> - “ [我的导师]花了很多时间与我们讨论我们的研究，并就方向提供了很好的建议。他以各种方式释放了我们，例如获得更多模型或大量计算预算。他将我们与许多伟大的人联系在一起，其中一些人成为合作者。他是一位非常鼓舞人心的导师。”</li><li> <strong>40％</strong> - “从高级线性代数到一般研究方向和诚信方面的关注，[我的导师]对我们的研究真是太神奇了。”</li><li> <strong>20％</strong> - “导师向我们（团队）介绍了一些对齐方式，观看了该项目（但大部分是在倾听，没有干预），还审查了我们的论文并组织了外部审查。”</li><li> <strong>10％</strong> - “想出我不会想出的有趣的想法/反对意见。”</li></ul><p>值得注意的是，这些学者中有许多将垫子的价值归因于他们的导师，但仍然非常积极地审查了他们的导师。</p><p>在控制学者对指导者的评级之后，将MATS的更多价值归因于指导与：</p><ul><li>报告说，如果不是垫子，他们将在整个夏天在一个校准组织中进行反合。</li><li>报道出版记录是该计划开始时的职业障碍，但并非在计划结束时。</li></ul><p>这表明，与直接指导相比，从与伯克利社区及其同龄人的连接相比，已经与对齐领域有所联系的学者（例如，通过先前的实习或培训计划）的价值更少。这也表明，对于成功在整个计划中发表研究的学者来说，导师特别有价值。</p><p>将MATS的价值归因于指导，与：</p><ul><li>报告提高了发展变革理论的能力；</li><li>由于学者支持教练，报告更多的生产时间。</li></ul><p>这表明学者支持和其他旨在促进改变理论思维理论的计划要素是学者的两个重要的非主体价值来源。</p><h3>学者支持的价值</h3><p><strong>总体评价</strong></p><p>我们问举行至少参加一次学者支持会议的学者：“以1到10的范围，您有可能向未来的学者推荐学者支持？”平均响应为7.9/10，<a href="https://www.qualtrics.com/experience-management/customer/net-promoter-score">净启动子得分</a>为28。 </p><figure class="image image_resized" style="width:70.28%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/rqke77gqvu1g6525jbir"></figure><p><strong>与指导支持作为基准的比较</strong></p><p>由于学者支持人员的熟练程度以各种方式聘请了教练和支持学者，因此我们通过询问学者来检查这一点：“如果您有时间，您的导师可以提供多少学者支持价值？”回答是：</p><ul><li>几乎没有价值-35％</li><li>其中一些价值-50％</li><li>几乎所有价值 -  15％</li></ul><p>我们还问学者，如果他们问的话，他们是否认为导师会给他们更多的支持时间。 55％的人说“可能不是”或“绝对不是”。这两个回应支持我们的理论，以实现学者支持的价值，这是：</p><ul><li>专业教练可以比某些相关领域的许多导师提供更好的支持。</li><li>导师是时间限制的。</li></ul><p><strong>授予等效估值</strong></p><p>我们问举行至少参加一次学者支持会议的学者：“假设您获得了赠款，而不是在垫子期间接受1-1学者支持会议，那么您需要收到多少才能在赠款和教练之间漠不关心？ ” <span class="footnote-reference" role="doc-noteref" id="fnrefhdw3lym1fyo"><sup><a href="#fnhdw3lym1fyo">[4]</a></sup></span>平均值和中位回应分别为3705美元和750美元。这位回应40,000美元的学者证实了这一数字并不是错别字，理由是“优先级，项目选择，一致性导航和一般AI景观，职业建议，在困难中的支持，对特定会议和项目的个人量身定制建议”。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/f4obyvjblzwcaed53gdf"></p><p><strong>生产时间获得了</strong></p><p>我们问学者举报至少参加一次学者支持会议的学者：“由于学者支持会议，您将在过去8周内估计有多少个生产力？”平均反应和中位反应分别为22小时8小时。响应总和为658个生产小时。</p><h3>工作坊及研讨会</h3><p>学者报告了参加研讨会和网络活动的价值。请注意，某些事件的受访者很少，因为我们通过可选调查收集了此反馈。 10/10是我们规模上的高标准，代表了学者计划的重大更新。 </p><figure class="image image_resized" style="width:69.86%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/lfewckjuinmojngzjnli"></figure><p>学者根据相同的10分制对外部扬声器事件进行评分。在23个研讨会中，有11名是面对面的，有11个是遥远的，有1个是混合的。面对面研讨会的评定比远程研讨会高0.9点（P = 0.0088）。 </p><figure class="image image_resized" style="width:70.33%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/somf49ymrg6ewflu2pgu"></figure><p>所有其他同等的东西，我们将在远程研讨会上选择一个面对面的研讨会，但是对这些格式的价值进行分析，研究人员恰好居住在湾区以及学者们如何自我选择参加会谈的情况。与经验丰富的研究人员举办的研讨会相比，一些学者举办了自己的研讨会，这些研讨会由同龄人评为良好。</p><h3>社区健康</h3><p>尽管许多学者没有使用社区健康支持，但少数学者大大受益。为了调查这些好处，我们询问了学者的两个我们用来评估学者支持的相同问题，以及关于他们的幸福问题。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/ewbughz2oiqgsib1ixsx"></p><p>中间时间：0小时</p><p>小时之和202小时</p><p>心理健康的中位数：2％</p><p>这些图表排除了一位学者，该学者报告了由于社区健康支持而增加了1000％的心理健康。学者提供了更多细节：</p><ul><li> “这有助于我对人际交往不太焦虑。”</li><li> “更幸福，更可持续，更专注，更加内gui。”</li><li> “该计划中有两次我对某些事情感到震惊；在这两种情况下，[社区经理]都可以聊天，非常友善和支持，并帮助我使事情变得更好。”</li><li> “通常，当您出门在外，并有可能在您的生活中定义时刻，创造一个支持性的环境对我来说至关重要。”</li></ul><p>一些学者提到他们没有要求社区健康支持，但是如果需要的话，他们很高兴该安全网已经到位：</p><ul><li> “很高兴知道如果发生了什么事，我要去有人去。”</li><li> “我并没有真正与社区健康互动，但我认为我想要的是反事实的世界。”</li><li> “我没用太多，但很高兴它在那里。”</li><li> “我没有太多利用社区健康，但是我感谢它在我可能需要的情况下的存在。”</li></ul><h2>评估结果</h2><h3>研究里程碑</h3><p><strong>学者研究计划</strong></p><p>学者研究计划是研究阶段的两个评估里程碑中的第一个。根据<a href="https://docs.google.com/document/d/1fQwy2btcWn3XRQkgu45kKnPPHMVQs28QHpaNtDCfXQY/edit#heading=h.ozvm8udh0z0y"><u>此标题</u></a>对SRP进行分级。平均成绩为74/100。学者们倾向于在其SRP的变更部分的威胁模型和理论上失去最多的观点。他们在制定研究计划的部分中表现出更强的表现。</p><p>重要的是，一些学者之所以忽略了SRP，因为他们不打算申请扩展阶段，或者因为他们已经获得了资金，并且没有预期SRP在其扩展阶段决策中发挥了重要作用。一些导师鼓励他们的学者剥夺SRP，而支持他们的研究。</p><p><strong>专题讨论会</strong></p><p>研讨会是第二个研究阶段的里程碑。项目分布在这些类别中，如下所示。请注意，并非每个学者的工作都在研讨会上代表，一些项目属于多个类别。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/snfbmj3ukwt4kbbpo8mw"></p><p>研讨会谈判在三个维度上获得了同行评审：“什么，”（50分）； “为什么”（30分）；和“样式”（20分）。完整的标题可<a href="https://docs.google.com/document/d/1SB_vl6pGuSO7qidkZ8fIKPeH7fZIC8mvXzxicFjGyrE/edit#heading=h.ozvm8udh0z0y"><u>在这里</u></a>提供。平均成绩为86/100。</p><h3>研究能力</h3><p>使用上面解释的“深度，广度，味觉”框架，我们评估了学者在计划期间沿这些维度提高他们的研究技能的程度。</p><p><strong>研究深度</strong></p><p>因为我们没有提出有关研究技能深度的计划预处理问题，所以我们要求学者在计划结束时直接评估他们在所选的研究方向上加强了他们的技术技能的程度。 1/10的评分代表“与我的反事实夏天相比，没有改善；” 10/10的评分代表“与我的反事实夏季相比，显着改善”。平均评分为7.2/10。 </p><figure class="image image_resized" style="width:84.58%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/e9tlsh6pe4axayao52tx"></figure><p>受访者详细阐述了其技术技能的提高：</p><ul><li> “我从非常基本的ML技能到合理的研究人员技能。”</li><li> “学习了一系列策略和技术，用于在机甲Interp中找到特定的东西。还学会了一个更好的想法，即提出哪些问题，证明标准等。”</li><li> “更快地进行实验，识别和去除瓶颈。”</li><li> “我在MATS上的技能得到了很大的提高，进行真正的研究特别有价值，因为这意味着我不必尝试猜测哪些技能对研究很重要，我只是在进行研究以及技能这是我改进的。因此，MATS的目标要好得多，而且比我的夏天要好得多。”</li></ul><p><strong>知识广度</strong></p><p>为了评估知识的广度，我们在研究阶段的开始和结束时要求学者对他们对主要一致性组织的研究议程的理解为1至10，其中10/10表示“对于任何主要的一致性组织，您可以与他们的一位研究人员交谈，并通过他们的智力图灵测试（即，研究人员会在组织的议程中对您的评价流利）。”</p><p>我们从计划后自报告的评分中减去了学者的最初自我报告评分，以获得以下分布： </p><figure class="image image_resized" style="width:63.98%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/q0jzuagnkumv2gkfwvjk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/tv0aa0dtjgrhuvi1pkjn 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/dneqbrids1b78uptkqvl 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/fcahnz8oznyoidnh2xza 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/t9ovxc5ym2zbmnilmtqy 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/cnu1yhkzkmkyzwvmpjbv 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/gepvvkrfeu6t5ktsb4ne 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/bzlob8pmpwqebi29cq0r 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/nw1kpf7xb5hfa78igzxl 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/f0ejttnktmsgkmf8kctn 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/z7hzcrvaa9hs1z1j1c1u 1104w"></figure><p>平均而言，在这10点尺度上，自我报告对主要一致性议程的理解增加了1.75。两名受访者报告说，他们的评分降低了，这可以通过学者的自我评估中的某些不一致来解释（当他们在研究阶段结束时回答时看不到他们的早期回答），也可以通过实现他们以前的学者来解释。理解<a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect"><u>并不像他们想象的那么全面</u></a>。</p><p><strong>研究口味</strong></p><p>我们将研究能力的“品味”维度分解为两个程序后的调查问题。第一个问题问：“您认为MATS在多大程度上提高了您在研究方向上独立迭代的能力？”，其中1/10表示没有变化，10/10表示显着增加。平均响应为6.9/10。 </p><figure class="image image_resized" style="width:85.72%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/zmosuottj0wwyx4zk1hw"></figure><p>受访者提供了有关此主题的定性信息：</p><ul><li> “大多数情况下，我对陈述研究假设的自信得到了提高。以前，我以为可以质疑/询问有关论文的所有内容已经完成或被认为无趣 - 我现在更坚信安全研究中仍然有很多低悬垂的果实。 （一个例子：更好地理解RLHF背后的理论）。”</li><li> “学者们整天都在谈论他们的研究。这增加了快速的反馈循环，即“这种方法将/不会成功”，“您也应该阅读此内容”，“这似乎是低/高影响力”。研究迭代的重大改进，也许是3倍。”</li><li> “更好地善于对我自己的模型和假设持怀疑态度，并找到最有效的方式来反驳它们。”</li></ul><p>涵盖研究口味的第二个问题问：“您在多大程度上提高了在MATS期间为研究发展理论的能力吗？”，其中1/10没有变化，而10/10代表拥有一个基本发达的模型。平均响应为5.9/10。 </p><figure class="image image_resized" style="width:70.31%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/pwzqwmjvjfoiwujhrab5"></figure><p>学者们阐述了他们对变革理论的经历：</p><ul><li> “我认为MATS对我的AI中事物的发展有了更详细的模型，意识到我以前没有考虑过的考虑因素，这对我有很大帮助事实和细节。”</li><li> “考虑我正在做的事情是否可以解决任何问题。”</li><li> “我认为我以前已经在考虑过很多事情，但是Mats给了我更多的练习机会，我认为我对评估议程变得更加现实（而不是以前可能会考虑蝙蝠中的太大议程）。”</li></ul><p><strong>导师认可</strong></p><p>在一项调查中，我们问导师：“考虑到上述内容，您如何继续支持学者的研究？”平均响应为8.1/10，<a href="https://www.qualtrics.com/experience-management/customer/net-promoter-score">净启动子得分</a>为38。 </p><figure class="image image_resized" style="width:71.98%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/nv7uav2tugiepfadwa5w" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/bhtp4q6evtp64v69ezfw 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/mjrvh0mqy9bzm17omgq3 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/qseuhcznhvn5jytff23q 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/scl7dyejhqxplisao3pz 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/sk1fg1scis2soxrczgec 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/kqokwwglgjguqo9sfjv3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/bclz9iouqcam2xad5iuv 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/x4wsc7umnhrl3qzhsxha 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ybqutxsl8szprbkiyvu8 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/df39xsgzxqhbeii2pkth 1104w"></figure><ul><li>当被问及“学者的技术技能与他们选择的研究方向有多强大？”导师平均对学者进行了7.0/10的评分，在哪里：<ul><li></li></ul></li></ul><p>重要的是，关于学者研究的判断在整个导师之间并不一致，因为每个导师都有自己的标准和优先事项，而不是每个导师都对调查做出了回应。</p><h3>联网</h3><p>我们衡量了网络机会在专业联系和潜在合作者变化方面的影响。 <span class="footnote-reference" role="doc-noteref" id="fnrefjruz5097w9c"><sup><a href="#fnjruz5097w9c">[5]</a></sup></span>在研究阶段的开始和结束时，学者报告了“一致性社区中的专业人员人数，您会很乐意与专业的建议或支持联系。” <span class="footnote-reference" role="doc-noteref" id="fnrefdkoclj9ogu"><sup><a href="#fndkoclj9ogu">[6]</a></sup></span>我们在这些响应中差异以获得以下分布： </p><figure class="image image_resized" style="width:70.77%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/p3rhtrgiig877me0teby" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/jvt6z2atg5x4d1w10a6t 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/vjjrob3qevytwv5qrwdr 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/gpvwhnanohnkhfm0zaka 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/inntopihux9frbyr2mm5 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/m3dmwgdoimq0rpuvsl5e 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/fnwrag0tjycqnopopdee 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/hp1g32lqy4ltjwveyqmb 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/bd16bew0mefjxuxo3neq 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/bvdymeo731yywe6xkjs2 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/lzqruihgrtictf9kyx4w 1104w"></figure><p>反应的中位差异为+4.5。受访者的两个答复意味着他们可以接触到的专业人数的数量为-10，然后响应了“ 30”，然后响应了“ 20”，因此我们希望这一数字是校准差异的结果。</p><p>我们问了一个关于潜在合作者的类似问题：“您觉得有多少人可以与AI Alignment Ready Research项目进行协作联系？” <span class="footnote-reference" role="doc-noteref" id="fnrefxyz9685l7yk"><sup><a href="#fnxyz9685l7yk">[7]</a></sup></span>我们采取了响应差异以获得以下分布： </p><figure class="image image_resized" style="width:70.87%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/exudn8ryswlsknsbhtfa" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ggftocn43oc0bodhg703 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/ub6icmfo938obrxiqibw 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/pffsav2v8sxwv3vkqo6n 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/tvtemcsucpu9wvdxochq 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/myz8y8k7ylyveywhnwt9 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/eluzxy19g6mlicounccf 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/lfhhyaa704f1n9bil4hc 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/vjtcx8ubwprrz6n5772m 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/z1mxlrqoytalsmhf3a2r 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/mlr8c5j89bqq1f4sojmv 1104w"></figure><p>响应的平均差异为+6.0。一些报告减少的学者很可能会通过人们的判断日常差异来解释，因为学者无法访问他们以前的回答。</p><h3>职业障碍</h3><p>在研究阶段的开始和结束时，我们向学者询问“ [他们]与成功的AI一致性研究生涯之间的障碍。”在比较研究阶段的开始和结束的响应时，我们看到，对于最可能的障碍，较少的学者将其报告为一个问题。出版记录和软件工程经验是例外：在计划结束时，更多的学者表明面临这些障碍，而不是开始。这些结果可能反映了学者学习以前低估的障碍的学习，随着学者通过较早的障碍，新的障碍变得更加突出，或者是较早的学者，或者是涉及不同障碍的职业道路的学者。出版记录是学者和成功的一致职业之间的主要障碍。 </p><figure class="image image_resized" style="width:87.06%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CY5fpned6Exq2ea98/wfmfyp6jpau8tiihxsue"></figure><p>我们还特别询问了学者在计划的开始和结束时对撰写赠款建议的信心。 70％的学者表示，在该计划开始时，对撰写赠款建议感到不自信或非常不自信，但是80％的学者表示对该计划结束时写赠款建议感到有信心或非常自信。</p><h2>未来计划的教训和变化</h2><p>尽管我们尚未完成或实施计划为下一个队列做出的一些更改，但以下是我们从2023年夏季计划中学到的主要课程。</p><h3> 1.在申请阶段更好地过滤</h3><p>我们预计，研究阶段的一些学者不太可能在其职业生涯中进行一致性研究，这足以保证指导他们的成本，并且我们认为我们可以在申请阶段获得足够的信息来识别这些学者。 This suggests that there is room for MATS to filter harder in initial applications or in subsequent evaluation checkpoints. Higher resolution in the upper tail of the talent distribution would allow us to select more promising applicants, or at least fewer unpromising applicants, allowing some mentors to allocate greater attention to each of their scholars.</p><p> One component of a better filter for some streams is increased screening for software engineering skill. This can be a costly evaluation process for mentors to run themselves, so for the Winter 2023-24 Program we contracted a SWE evaluation organization to screen applicants for empirical ML research streams.</p><p> For the Winter 2023-24 Program, we also decided to better communicate different mentor styles to applicants by adding a “Personal fit” section to mentor descriptions on our website. As some mentors are much more hands-off than others, we want to make it more likely that applicants can self-select out of streams for which they have a poor personal fit.</p><h3> 2. Providing more technical support</h3><p> One form of assistance that was not explicitly covered by the Scholar Support team and only sometimes covered by mentors was technical support, including debugging code, teaching new tools like the <a href="https://neelnanda-io.github.io/TransformerLens/"><u>TransformerLens</u></a> library, and answering technical ML questions. In future cohorts, we may provide office hours or similar for scholars who are bottlenecked by technical problems. One possibility is to improve our infrastructure for scholars to help each other with technical problems.</p><h3> 3. More proactively supporting professional development beyond mentorship</h3><p> Scholar Support currently helps scholars resolve a variety of bottlenecks, including challenges with productivity, prioritization, research direction, communication, mental health, etc. While this has been valuable for many scholars, it more strongly targets short-term bottlenecks that can be resolved in ~1 hour of coaching per week, and there are some skill improvements (eg, practice reading research papers or improving programming ability) that require a greater time commitment.</p><p> In future cohorts, we may explore coaching structures or peer accountability structures that encourage scholars to spend more time on professional development beyond object-level research. This could look like paper reading groups, more regular and high-commitment workshops, and coworking sessions on topics like theory of change, research workflows, etc.</p><h3> 4. Offering research management help to mentors</h3><p> In addition to further supporting professional development, Scholar Support is also planning to help most mentors manage scholar research for the Winter 2023-24 cohort. We (the Scholar Support team) find it plausible that a strict confidentiality policy between Scholar Support and mentors leaves too much value on the table as:</p><ol><li> Mentors would benefit from the shared information about scholar status;</li><li> Scholars would benefit from their mentors&#39; goals being understood by the Scholar Support team, especially as the mentors&#39; research directions change throughout the program.</li></ol><p> In offering research management help to mentors, Scholar Support will take a more direct role in understanding research blockers, project directions, and other trends in scholar research, and summarizing that information to mentors with scholar consent.</p><h3> 5. Reducing the number of seminars</h3><p> We think 23 seminars was too many; the breadth we offered scholars was not worth the low average attendance we observed. In the winter, we will schedule 12-16 seminars instead, with weekly opportunities for shorter lightning talks. We will also encourage more scholars to deliver seminars (as longform talks, rather than confined to the lightning talk format), since scholar-delivered seminars were highly rated over the summer.</p><h3> 6. Improving communication with scholars during the program</h3><p> In part due to the number of seminars, workshops, and social events, we think that the signal-to-noise ratio of our announcements may have been too high. On at least one occasion, a sizable fraction of the scholars didn&#39;t know about an imminent deadline. In the future, we&#39;ll more cleanly distinguish the most important announcements and make those announcements sooner.</p><h3> 7. Developing more robust internal systems</h3><p> As the MATS team scales, we&#39;re improving several internal systems. These include our data collection pipelines for impact evaluation, our project management systems, and investing in team members through greater professional development.</p><h3> 8. Frontloading social events</h3><p> Some scholars reported unmet demand for socializing among the cohort. In the Winter, we intend to frontload social events to facilitate connections early in the program and hold more events near the office in the hope of reducing the activation energy required for scholars to initiate gatherings later in the program. </p><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/owezlxy9huaz3nvisvys" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/p9rtesbpiumqavifgdnn 410w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/kavq5cmkcfjmpghagunp 820w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/kqpc1u8dnlojpfdyfbgs 1230w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/wfbd4e4d8oejex0bi49n 1640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/pkcsb8dtbrazxsxsgjyr 2050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/yesebeflxhns0q6jgkeg 2460w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/chgrj99i2hvbrs8k2kqf 2870w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/xjfi4f2aeaifnab08rak 3280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/sng3yqsh8yhmicbyik0q 3690w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zwf68YaySvXhWYCdh/eazdnqncdnjlf15hjhsq 4034w"></figure><p></p><h1>致谢</h1><p>This report was produced by the <a href="https://www.matsprogram.org/"><u>ML Alignment &amp; Theory Scholars Program</u></a> . Rocket Drew and Juan Gil were the primary contributors to this report, Ryan Kidd scoped, managed, and edited the project with the support of Christian Smith, and Laura Vaughan and McKenna Fitzgerald contributed to data collection and analysis for the Scholar Support report section. Thanks also to Carson Jones, Henry Sleight, and Ozzie Gooen for their contributions to the project.</p><p> To learn more about MATS, please visit our <a href="http://matsprogram.org/"><u>website</u></a> . We are currently <a href="https://manifund.org/projects/mats-funding">accepting donations</a> for our Winter 2023-24 Program and beyond! </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn2niy7evweco"> <span class="footnote-back-link"><sup><strong><a href="#fnref2niy7evweco">^</a></strong></sup></span><div class="footnote-content"><p> Examples of selection questions that some mentors included in their application:</p><p> - Spend 2-4 hours using language models to develop a dataset of question-answer pairs on which models trained by RLHF will likely give friendly but incorrect responses. Then record yourself for 2-4 hours evaluating models on your dataset and plotting scaling laws. Bonus points if your dataset shows inverse scaling.</p><p> - I would like you to spend ~10 hours (for fairness, max 20) trying to make research progress on a small open problem in mechanistic interpretability, and show me what progress you&#39;ve made. Note that I do not expect you to solve the problem, to have a background in mech interp, get impressive results, or really to feel that you know what you&#39;re doing.</p><p> - Is “Bayesian expected utility maximizer” a “True Name” for a generally powerful intelligence?为什么/为什么不呢？</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvp0h93gzvk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvp0h93gzvk">^</a></strong></sup></span><div class="footnote-content"><p> The trainees who did not progress were concentrated among two mentors, who informed their trainees about their prospects for getting into the Research Phase prior to the Training Phase. The primary determinant of these mentors&#39; decisions was scholars&#39; performance in a research sprint at the end of the Training Phase. Knowing their odds, the trainees in these streams elected to participate, in part due to the high value of a guaranteed month working with their mentor. Jay Bailey, a trainee from Winter 2022-23, attests,</p><p> “Neel [Nanda]&#39;s training process in mechanistic interpretability was a great way for me to test my fit in the field and collaborate with a lot of smart people. Neel&#39;s stream is demanding, and he expects it to be an environment that doesn&#39;t work for everyone, but is very clear that there&#39;s no shame in this. While I didn&#39;t end up getting selected for the in-person phase, going through the process helped me understand whether I wanted to pursue mechanistic interpretability in the long term, and firm up my plans around how best to contribute to alignment going forward. ”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnsczgjuthnr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnsczgjuthnr">^</a></strong></sup></span><div class="footnote-content"><p> This question is commonly used to calculate a “ <a href="https://en.wikipedia.org/wiki/Net_promoter_score"><u>net promoter score</u></a> ” (NPS) which is standard in many industries. Based on our respondents, the NPS for MATS is +69.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhdw3lym1fyo"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhdw3lym1fyo">^</a></strong></sup></span><div class="footnote-content"><p> This survey question and the following one come from Lynette Bye&#39;s <a href="https://forum.effectivealtruism.org/posts/xvax8hpF29Ky5kPJw/effective-altruism-coaching-2020-annual-review"><u>2020 review</u></a> of her coaching impact.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjruz5097w9c"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjruz5097w9c">^</a></strong></sup></span><div class="footnote-content"><p> The following histograms exclude a scholar who increased from 0 to 999 available professionals and potential collaborators, explaining, “at the start I basically felt like I could message / call up no-one and now I feel like I can message / call up anyone quite字面上地。”</p></div></li><li class="footnote-item" role="doc-endnote" id="fndkoclj9ogu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdkoclj9ogu">^</a></strong></sup></span><div class="footnote-content"><p> The question elaborated, “If you want a more specific scoping: how many professionals would you feel able to contact for 30 minutes of career advice? Factors that influence this include what professionals you know and whether you have sufficient connection that they&#39;ll help you out. A rough estimate is fine, and this question isn&#39;t just about people you met in MATS!”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxyz9685l7yk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxyz9685l7yk">^</a></strong></sup></span><div class="footnote-content"><p> The question elaborated, “Imagine you had some research project idea within your alignment field of interest. How many people that you know could plausibly be collaborators? A rough estimate is fine, and this question isn&#39;t just about people you met in MATS!”</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-postmortem-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zwf68YaySvXhWYCdh/mats-summer-2023-postmortem-1<guid ispermalink="false"> zwf68YaySvXhWYCdh</guid><dc:creator><![CDATA[Rocket]]></dc:creator><pubDate> Fri, 01 Dec 2023 23:29:47 GMT</pubDate> </item><item><title><![CDATA[Complex systems research as a field (and its relevance to AI Alignment)]]></title><description><![CDATA[Published on December 1, 2023 10:10 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 00:43:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 00:43:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>I have this high prior that complex-systems type thinking is usually a trap. I&#39;ve had a few conversations about this, but still feel kind of confused, and it seems good to have a better written record of mine and your thoughts here.</p><p> At a high level, here are some thoughts that come to mind for me when I think about complex systems stuff, especially in the context of AI Alignment:</p><ul><li> A few times I ended up spending a lot of time trying to understand what some complex systems people are trying to say, only to end up thinking they weren&#39;t really saying anything. I think I got this feeling from engaging a bunch with the Santa Fe stuff and Simon Dedeo&#39;s work (like <a href="http://www.mdpi.com/1999-5903/8/2/14">this paper</a> and <a href="https://arxiv.org/abs/2006.02359">this paper</a> )</li><li> A part of my model of how groups of people make intellectual progress is that one of the core ingredients is having a shared language and methodology that allows something like &quot;the collective conversation&quot; to make incremental steps forward. Like, you have a concept of experiment and statistical analysis that settles an empirical issue, or you have a concept of proof that settles an issue of logical uncertainty, and in some sense a lot of interdisciplinary work is premised on the absence of a shared methodology和语言。</li><li> While I feel more confused about this in recent times, I still have a pretty strong prior towards something like <a href="https://en.wikipedia.org/wiki/G_factor_(psychometrics)">g or the positive manifold</a> , where like, there are methodological foundations that are important for people to talk to each other, but most of the variance in people&#39;s ability to contribute to a problem is grounded in how generally smart and competent and knowledgeable they are, and expertise is usually overvalued (for example, it&#39;s not that rare for a researcher to win a Nobel prize in two fields). A lot of interdisciplinary work (not necessarily complex systems work, but some of the generator that I feel like I see behind PIBBS) feels like it puts a greater value on intellectual diversity here than I would. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 00:53:26 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 00:53:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Ok, so starting with one high-level point: I&#39;m definitely not willing to die on the hill of &#39;complex systems research&#39; as a scientific field as such. I agree that there is a bunch of bad or kinda hollow work happening under the label. (I think the first DeDeo paper you link is a decent example of this: feels mostly like having some cool methodology and applying it to some random phenomena without really an exciting bigger vision of a deeper thing to be understood, etc.)</p><p> That said, there are a bunch of things that one could describe as fitting under the complex systems label that I feel positive about, let&#39;s try to name a few:</p><ul><li> I do think, contra your second point, complex systems research (at least its better examples) have a lot of/enough shared methodology to benefit from the same epistemic error correction mechanisms that you described. Historically it really comes out of physics, network science, dynamical systems, etc. The main move that happened was to say that, rather than indexing the boundaries of a field on the natural phenomena or domain it studies (eg biology, chemistry, economics), to instead index it on a set of methods of inquiry, with the premise that you can usefully apply these methods across different types of systems/domains and gain valuable understanding of underlying principles that govern these phenomena across systems (eg scaling laws shaping biological organisms as well as the growth of cities, etc.)</li><li> I think a (typically) complex systems angle is better at accounting for environment-agent interactions. There is a failure mode of naive reductionism that starts by fixing the environment to be able to hone in on what system-internal differences produce what differences in the phenomena, and then conclude that all of what drives the phenomena is systems-internal while forget that what they did earlier is artificially fixed the environment in order to reduce the complexity of the problem at hand. It&#39;s fine and practically useful often to fix one part of the equation of complex interactions, but you shouldn&#39;t forget that that&#39;s what you did along the way. Similarly, the complex systems lens tends to be better at paying attention to interactions across levels of abstraction, and the dynamics that emerge from these interactions, which also seem valuable to understanding natural phenomena. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 00:52:03 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 00:52:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>to instead index it on a set of methods of inquiry, with the premise that you can usefully apply these methods across different types of systems/domains and gain valuable understanding of underlying principles that govern these phenomena across systems (eg scaling laws shaping biological organisms as well as the growth of cities, etc.)</p></blockquote><p> Ok, I do really like that move, and generally think of fields as being much more united around methodology than they are around subject-matter. So maybe I am just lacking a coherent pointer to the methodology of complex-systems people. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 00:57:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 00:57:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Hmm, I guess my thoughts on complex systems stuff then kind of branch into two directions:</p><p> <strong>Where are the wins?</strong> Do we have any success story for this methodology working well? I used to be a fan of network science, but then kind of bounced off of it. I like physics, though physics itself is so large, and has a lot of dysfunction in it, that it really matters which part of the physics methodology is imported.</p><p> <strong>Ok, so what is the foundation?</strong> It does seem kind of like I don&#39;t have a good inside view of the methodology of this field. Maybe I should just go on Wikipedia and read the summary of the methodology there. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:00:13 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:00:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>A lot of interdisciplinary work (not necessarily complex systems work, but some of the generator that I feel like I see behind PIBBS) feels like it puts a greater value on intellectual diversity here than I would.</p></blockquote><p> Keen to say something about the type of epistemic pluralism that I care about (among others in the context of PIBBSS).</p><p> (Generally speaking, I think the &quot;general smarts&quot; concern feels pretty orthogonal to how I am thinking about epistemic pluralism, and it at least feels to me like I&#39;m not forced to make trade offs between the two. We could separately double click on <i>that</i> if you like, but let me first try to argue why I think they are orthogonal in the first place.)</p><p> I think one relevant premise for PIBBSS style work (which it shares with the complex systems lens at least as I&#39;ve framed it above) is some assumption that there are some underlying principles that govern intelligent behvaviour across different systems, substrates and scales. If that is so, that is one approach to dealing with the problem that we don&#39;t have direct epistemic access to the sorts of AI systems we&#39;re most worried about. But if we think they share <i>some</i> features/principles with other types of systems that implement intelligent behaviour, importantly systems we do have a better degree of epistemic access to, we can now start to triangulate between what we understand about such different systems. This triangulation allows you to gain some more robust insights into those principles that that are substrate/scale/system-agnostic. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:02:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:02:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>I overall feel pretty sympathetic and interested in studying intelligent behavior in the systems we have. However, I do notice that somehow I can&#39;t think of any work in this space that&#39;s felt very useful to me, at least in recent years. I really like Eliezer&#39;s analysis of evolution as an analogue to AI alignment, and it had a big effect on me. And I like Steve Byrnes&#39; work on studying neuroscience to get insight into the AI Alignment problem, though they feel separate somehow (but that might genuinely just be me gerrymandering), and in as much as the goal is to produce more work like the original LW sequences analysis of evolution and AI Alignment, I would feel pretty excited. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:04:43 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:04:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>On the foundations, roughly, my feeling is that there are different angles to go about this. Generally I feel a bit hesitant about the frame of &quot;let me learn about complex systems science&quot; - like I think that concept isn&#39;t really the most useful way of carving the world (eg I think reading the wikipedia page on this will be not that exciting). I do think there  are some complex systems textbooks that are moderately neat if you&#39;re looking for some ideas of how you could model different types of systems, and pointers at the math you need for that. But beyond that, at least according to my taste, I&#39;d say think about what natural phenomena you&#39;re interested in understanding, and then try find who is doing interesting work on that phenomena, or what ways of modelling it (mathematically) seem富有成效。 My experience I guess is closer to a) interested in understanding certain phenomena, b) finding some work out there that I found productive to engage with, c) noticing that a bunch of that can be labelled complex systems stuff. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:04:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:04:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Well, in this case I am asking the specific question of &quot;in as much as there is a field here, what is its methodology?&quot;. I do a lot of studying of natural phenomena and am generally searching for good mathematical models, but I rarely end up finding things that are labeled as &quot;complex systems&quot;. I usually just like, end up studying biology or physics or AI or math. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:07:00 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:07:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Here is the specific quote of yours that I was thinking about:</p><blockquote><p> I do think, contra your second point, complex systems research (at least its better examples) have a lot of/enough shared methodology to benefit form the same epistemic error correction mechanisms as you described. Historically it really comes out of physics, network science, dynamical systems, etc.</p></blockquote><p> Which sounds to me like you are implying there is a field here that has an epistemic ratchet that can click forward and make coherent progress over time. But I currently feel like I don&#39;t have a good pointer at the mechanism of that ratchet. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:19:47 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:19:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Do you know <a href="https://www.amazon.com/Introduction-Theory-Complex-Systems-Thurner/dp/019882193X/ref=asc_df_019882193X/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312152840806&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15805346042964868981&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1013585&amp;hvtargid=pla-561851596697&amp;psc=1&amp;mcid=ee4d371124353a0da64f86a8b0a83255&amp;tag=&amp;ref=&amp;adgrpid=61316181319&amp;hvpone=&amp;hvptwo=&amp;hvadid=312152840806&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15805346042964868981&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1013585&amp;hvtargid=pla-561851596697&amp;gclid=CjwKCAiAvJarBhA1EiwAGgZl0KNSZABSw9gcydbqlEvAAQLD7v1vS3dmVIlO5eron7VdFsnTYkH9_BoCwVsQAvD_BwE">this textbook</a> ? I&#39;d say it&#39;s a good overview of the &quot;complex systems modelling toolbox&quot;.</p><p> If you want a somewhat spicier, or maybe more ambitious vision of what complex systems is about, you could listen to <a href="https://www.preposterousuniverse.com/podcast/2023/07/10/242-david-krakauer-on-complexity-agency-and-information/">this interview with David Krakauer</a> . My guess is you&#39;d largely bounce off of it, though I do think it&#39;s pretty exciting (albeit the interview is denser than it appears to be at first glance). He talks about understanding &quot;telic&quot; phenomena (or some similar terminology), which (my rough paraphrase) he understands as emerging from the specific constraints that you get from adaptive systems that evolve and meta-evolve, etc. IMO this is interesting from an &quot;understanding the foundations of agency/intelligent behavior&quot; angle, and eg you end up trying to explain in naturalistic terms how things like &quot;back-wards causation&quot; characteristic to agency/planning can arise from simple dynamics.</p><p> In terms of systematic progress,  for one, I think that progress is integrated with other scientific fields too - like complex systems as a field cuts across the more traditional ways of carving scientific fields so I don&#39;t think there is an a priori way to attributing progress to either one of them exactly. But I think the mechanisms of progress come from an interplay between [a toolbox of mathematical models (eg network science, dynamical systems, control theory, etc.)] and moving between the more abstract and more concrete/empirical.</p><p> Maybe I&#39;m being too conflationary today, but I think that is just the same story as in all other scientific fields, and the main differences is in some of the underlying premises. Maybe the cleanest example is the move from classical economic theories to complexity economics. In the former, you start from a set of assumptions like: rational actor, all your agents are the same, markets are equlibrium systems. And then complexity economics comes along and says: hey guys, good news, we have better math tools now than just arithmetic, so we are now able to relax some of our classical assumptions and can eg model economic systems with premises such as: bounded rational agents (with learning &amp; memory dnyamics, etc.), heterogenous agents (eg different learning strategies), markets as out of eqiulibiria systems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:24:22 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:24:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>Do you know <a href="https://www.amazon.com/Introduction-Theory-Complex-Systems-Thurner/dp/019882193X/ref=asc_df_019882193X/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312152840806&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15805346042964868981&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1013585&amp;hvtargid=pla-561851596697&amp;psc=1&amp;mcid=ee4d371124353a0da64f86a8b0a83255&amp;tag=&amp;ref=&amp;adgrpid=61316181319&amp;hvpone=&amp;hvptwo=&amp;hvadid=312152840806&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15805346042964868981&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1013585&amp;hvtargid=pla-561851596697&amp;gclid=CjwKCAiAvJarBhA1EiwAGgZl0KNSZABSw9gcydbqlEvAAQLD7v1vS3dmVIlO5eron7VdFsnTYkH9_BoCwVsQAvD_BwE">this textbook</a> ? I&#39;d say it&#39;s a good overview of the &quot;complex systems modelling toolbox&quot;.</p></blockquote><p>我不！ I might take a look.</p><blockquote><p> And then complexity economics comes along and says: hey guys, good news, we have better math tools now than just arithmetic, so we are now able to relax some of our classical assumptions and can eg model economic systems with premises such as: bounded rational agents (with learning &amp; memory dnyamics, etc.), heterogeneous agents (eg different learning strategies), markets as out of equilibria systems.</p></blockquote><p> Yeah, so this is definitely the kind of thing that sounds like a cool thing to do, but also, does it actually work? Like, I am not that grounded in economics, but I do read a lot of econ bloggers and think a bunch about economics in my own language, and I don&#39;t come across the complexity economics perspective a lot, and indeed kind of expect that if I were to reading one of the &quot;top&quot; paper on complexity economics, I would end up feeling disappointed. But I might also just be lacking references here.</p><p> This stands, for example, in contrast to something like cliometrics/econometric history, which I found really valuable, and which has a lot of cool models about how history works, but doesn&#39;t feel very complexity-science shaped to me. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:26:37 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:26:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>However, I do notice that somehow I can&#39;t think of any work in this space that&#39;s felt very useful to me</p></blockquote><p> I partially agree with you and wish I could point to more and easily legible examples. (At the same time, I don&#39;t feel like I have very many examples I find particularly exciting more broadly.)</p><p> A few non-comprehensive pointers to more current work:</p><ul><li> Hierarchical agency/alignment work, amongst other things discussed/worked on by ACS</li><li> Developing naturalized accounts of intelligent phenomena (eg agency, planning, deception, power seeking, mesaoptimisation), where a naturalized account is meant to characterise the underlying mechanisms of a phenomena such that you can identify it also when it occurs at (temporal, spatial) scales you haven&#39;t evolved to recognize the phenomena -- with the hope that this can provide more robust ways to do eg interpretability and evals</li><li> Coming to have a more principled understanding of interacting AI systems, eg what evolutionary/emergent dynamics from having a bunch of LLM sytems interact with each other in the wild (eg prompt evolution, emergence of scaffolded agents with different capability profiles, etc.)</li><li> Characterising &quot;messy&quot; AI risk scenarios, eg <a href="https://www.lesswrong.com/posts/BTApNmv7s6RTGxeP4/cyborg-periods-there-will-be-multiple-ai-transitions">multiple transitions</a> , RAAP, multi-multi delegation, ascended economy </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:28:47 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:28:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>So, the Wikipedia article on complexity economics says:</p><blockquote><p> The <a href="https://en.wikipedia.org/wiki/Economic_complexity_index">economic complexity index</a> (ECI) introduced by Hidalgo and Hausmann <a href="https://en.wikipedia.org/wiki/Complexity_economics#cite_note-HidalgoHausmannPNAS-6"><sup>[6]</sup></a> <a href="https://en.wikipedia.org/wiki/Complexity_economics#cite_note-ComplexityAtlas-7"><sup>[7]</sup></a> is highly predictive of future GDP per capita growth. In Hausmann, Hidalgo et al., <a href="https://en.wikipedia.org/wiki/Complexity_economics#cite_note-ComplexityAtlas-7"><sup>[7]</sup></a> the authors show that the List of countries by future GDP (based on ECI) estimates ability of the ECI to predict future GDP per capita growth is between 5 times and 20 times larger than the World Bank&#39;s measure of governance, the World Economic Forum&#39;s (WEF) Global Competitiveness Index (GCI) and standard measures of human capital, such as years of schooling and cognitive ability. <a href="https://en.wikipedia.org/wiki/Complexity_economics#cite_note-8"><sup>[8]</sup></a> <a href="https://en.wikipedia.org/wiki/Complexity_economics#cite_note-9"><sup>[9]</sup></a></p></blockquote><p> And like, I don&#39;t know, that sounds cool, but also my honest best guess is that this is fake somehow? Like, if I look into this I will find that &quot;past GDP per capita growth&quot; is a better predictor than this economic complexity index, or something as straightforward as that, and the only reason why they can claim this result is because they gerrymandered the alternative somehow. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:35:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:35:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, I googled around a bit, and I can&#39;t find any obvious takedown that exposes the ECI as being obviously gerrymandered, and Our World in Data (who generally seem reasonable and like they think about this stuff in a cool way) have a <a href="https://ourworldindata.org/how-and-why-econ-complexity">favorable article on it</a> on their blog, so I update that there is something more real here. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:33:38 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:33:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Yeah, I definitely share some confusion of the visible successes being less than what my model would have predicted.</p><p> This makes me update down a bit on the overall promise of the approach, but I also have uncertainty over other parts of my model, eg what success I would expect at what timescales and what &quot;success&quot; would look like. Also I think there are dynamics like &quot;once a thing gets successful it gets more integrated into the mainline and thus less recognisable as the (once) unorthodox approach&quot;. Definitely expect some of this to be happening. I know they had some success in modeling eg financial crises by dropping the &quot;markets are equilibrium systems&quot;assumption; I remember reading some report (I believe of some international governance body, OECD or the like, but can&#39;t remember the details) with economic recommendations around climate change and sustainablility that were very clearly complexity economics inspired that sounded pretty reasonable, and I know a little bit about eg <a href="https://oec.world/en">this work</a> by Hidalgo which seemed pretty cool based on a relatively shallow look (in part also because it makes a bunch of really cool real-world data accessible). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:42:27 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:42:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Hmm, on the economic index stuff: I mean one simple perspective on this all is something like</p><ul><li> surely the thing that is really going on in the territory is extremely complex (like, there are historical path dependencies, there is the global economy that the national economy is embedded in, the country&#39;s specific resources, infrastructure, work force, etc. etc. ）</li><li> it also surely seems like basically all classical economic models simplify reality A LOT, and that unorthodox approaches are making some of these assumptions more realistic</li><li> the question is how much the complexification of your model (or that particular complexificiation) buys you in terms of predictive power relative to what you pay in terms of complexity costs</li><li> One more thing that is <i>weird</i> about a domain like economics is that the economic theorising happens <i>within</i> (and thereby affects) the systems it&#39;s trying to predict. Like, when the World Bank issues some predictions, that itself affects eg investment flows into the country, interests on deposits etc. This makes economics (among other fields) importantly different from physics where in most cases you are in fact justified to ignore the fact that the theorisers are in the theory. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:49:29 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:49:29 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>One more thing that is <i>weird</i> about a domain like economics is that the economic theorising happens <i>within</i> (and thereby affects) the systems it&#39;s trying to predict. Like, when the World Bank issues some predictions, that itself affects eg investment flows into the country, interests on deposits etc. This makes economics (among other fields) importantly different from physics where in most cases you are in fact justified to ignore the fact that the theorisers are in the theory.</p></blockquote><p> I agree that there is some reality to this, but I do think this is the kind of effect that feels like it&#39;s selected to feel clever, or meta, but doesn&#39;t actually matter? Like, I agree that the Fed of course has some effect on what the economy does by saying what it will do, but I find myself skeptical that you have to really take into account the effects of how people will change their behavior after you publish your economic theory, in addition to just like modeling what interest rates the Fed is setting as a target.</p><p> Like, I am not denying there is some effect here, but I doubt that it will be large.</p><p> This is importantly different from situations where someone makes an empirical observation, and that empirical observation turns out to either be the result of a human-enforced policy, or has turned into a human-enforced policy because of its regularity. For example, I find the story that Moore&#39;s law derived a bunch of its robustness from the fact that major semiconductor manufacturers set their internal targets according to Moore&#39;s law, kind of promising and interesting.</p><p> But that feels different than saying that you need to take into account the self-referential effects of actors in the economy taking your economic theory seriously. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:51:38 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:51:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>My best guess currently is that it does matter a great deal (though at slightly larger timescales than on the year-to-year prediction say). Like, I think the path dependency of history and social systems (the path dependency of everything that is subject to some form of differential selection) is a big deal. I feel very interested in eg what alternative functional economic logics there are, and pretty saddened by the fact that given the reality of physics we might never be able to explore them in this branch. This stuff is definitely scientifically difficult to deal with because it&#39;s about counterfactuals we cannot access, so it&#39;s hard or maybe impossible to even be calibrated on whether it&#39;s a big deal or not. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:55:12 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:55:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>And I guess this is a good example of where my intuitions are influenced by a complex systems lens, compared to my guess of some other people&#39;s intuitions.<br><br> The way I think about the degree of how much path dependency matters here is roughly: one pull is that, if you have some relatively simple complex dynamic, small differences in initial conditions can propagate a great deal, local contingencies make you access different parts of the possibility tree, sometimes in ways that are very hard to reverse. The pull from the other side is if you can point to some mechanisms that actively buffer against such path-dependencies, eg some sort of homeostatic pressure that tends to keep the systems within some basin. Both of these mechanisms exist - overall I expect socio-economic history to be shaped more by the former (where eg the developmental/life period of a biological organisms is more shaped by the latter). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:53:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:53:53 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>I mean, I am totally fine saying &quot;initial conditions matter a lot, some systems are chaotic, it&#39;s hard to predict where they will end up because they are quite sensitive&quot;. But that&#39;s different from saying &quot;specifically analyzing the self-referential nature of economic theories and policies is worth the bang for your complexity buck&quot;. Like, I don&#39;t expect that the system will stop being chaotic as soon as you account for that self-referential effect, nor do I expect that the system is chaotic because of the self-referential component that your publication would introduce. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:57:41 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:57:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Yeah okay so I agree with you that I expect the effect to be bigger (at the year to decade timescale) for technology design than it is for macroeconomics. (I don&#39;t think so much take this as saying that the effect doesn&#39;t apply to macroeconomics, and rather that macroeconomics lives at slower time scales. Like, the lens of this sort of path-dependency applies to economic logics, but more so at the decades to centuries time scale.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 01:56:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 01:56:53 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Like, as an example, I think all the basics of microeconomics don&#39;t have much of a self-referential effect. The domain of their validity, and their predictions seem robust to that kind of stuff. Supply will equal demand, no matter whether people know about supply and demand. Wages will be sticky, no matter whether people know about wages being sticky (there is probably some effect here, but I think it&#39;s very weak). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 01:58:27 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 01:58:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>&quot;specifically analyzing the self-referential nature of economic theories and policies is worth the bang for your complexity buck&quot;. Like, I don&#39;t expect that the system will stop being chaotic as soon as you account for that self-referential effect, nor do I expect that the system is chaotic because of the self-referential component that your publication would introduce.</p></blockquote><p> Yeah to be clear I agree with this! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:02:23 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:02:23 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Maybe a nearby example that is interesting and we might disagree on more is: there is at least some reading of history where notions of &quot;economic rationality&quot; and the various notions of rationality linked up to the field of game theory emerged during the post-WW2 period, and that those ideas have importantly and manifestly shaped economic, institutional and academic/intellectual developments. This view says something like: a bunch of ideas that seem very natural to (most of) us today are pretty strongly contingent on a pretty narrow time period and a pretty small number of heads shaping these ideas. Related to a sense of &quot;the fish in the water&quot; and &quot;you can never really escape your local ideological context&quot;.<br><br> [Not sure it&#39;s worth going down this rabbit hole.] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:05:44 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:05:44 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>Supply will equal demand, no matter whether people know about supply and demand.</p></blockquote><p> FWIW, largely agree with that paragraph but maybe worth pointing out: supply equals demand <i>does</i> fail sometimes (eg financial crises). We can construct these failures as some sort of &quot;irrationality&quot; in the system, but I find that generally pretty intellectually lazy. It seems important to recognize the limits of a model, and why those limits arise. The fact that complexity economics says they are better at understanding what really happens during things like financial crises I think should earn them a good deal of epistemic points (with the important caveat that I would need to read up on what exactly they were able to show with respect to modelling markets as out-of-equilibria systems, so I am making this argument with an epistemic caveat) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:04:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:04:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Yeah, I do think I disagree with this, and it is the kind of thing that does scare me about complexity theory. Like, maybe this is a strawman, but there is an attractor for sciences where the authors of that science start viewing themselves not as reporters on the truth, but as people who view themselves as advocates for a certain way of social organization.</p><p> History is full of this, and most of it is a terribly diseased field because of it, because really very large fractions of it view themselves as intentionally trying to reframe history in a way that positively affects the workings of society today.</p><p> And like, I am not in favor of banning any and all discussion of the secondary effects of a publication, and how a publication itself might distort the subject-matter that it is talking about, but overall there are many skulls along this road, and many fields have died because of it.</p><p> And I don&#39;t know to what degree that is going on when you are talking about studying self-referentiality in the publication and adoption of economic theories, but it feels like it gets close to it. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:11:06 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:11:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>Yeah, I do think I disagree with this, and it is the kind of thing that does scare me..</p></blockquote><p> Yep, overall strong agree with the entire message. I find myself pretty torn where, on one hand, there are some basic arguments here that I find pretty compelling and make me want to take seriously the social embededness of theorizing -- especially in the context of AI alignment --, and I also hard agree with the skulls and the slippery epistemic slopes.</p><p> I do feel like there is a way to take some of these basic insight onboard in a way that doesn&#39;t turn all of your reasoning intellectually fraught, but it sure feels like a tricky balance, and in my experience it feels a bit like people have more or less &quot;epistemic antibodies&quot; for being able to navigate that terrain more or less safely.</p><p> (As a side note: I&#39;ve written/gave some talks on this, and if you were to read/watch them and wanted to let me know whether it made you update towards being more or less worried I end up stumbling too close to the skulls, I&#39;d be keen to hear that.)</p><p> (Also, I think <a href="https://www.jstor.org/stable/3810931">this paper</a> has an interesting philosophical discussion on this issue.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:13:28 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:13:28 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>So, going back a bit more to the top-level. I definitely am on board with something like &quot;man, it does sure really seem like formal models we construct of various societal, intelligent and economic systems seem very unlikely to capture these models at the relevant level of detail and complexity that is necessary to actually make good predictions here&quot;.</p><p> And then I am pretty into figuring out how to do better. I guess my current answer is something like &quot;well, that&#39;s not the job of science, the job of science is to provide a certain type of relatively narrow intellectual input into society&#39;s reasoning. The actual job of aggregating and making predictions about large complex systems will mostly happen in the System 1 of a bunch of decision-maker brains, based on a really complicated and messy mixture of inductive and deductive reasoning that we don&#39;t really understand&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:15:51 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:15:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>I also somewhat think that deep learning is now good enough that we can probably understand a bunch of systems in the world better by just throwing some large neural nets at them. They definitely have enough parameters and can encode enough simultaneous considerations to give rise to pretty good predictive models of very complicated systems.</p><p> And by doing it via artificial learning systems we have to deal with less of the recursive issues, can control the inputs, and maintain a clearer abstraction of a science (which eg can do things like reproduce predictions about complex systems by rerunning a DL training run, which you can&#39;t do if you are predicting complex systems by giving information to policy makers, who generally don&#39;t appreciate being put into large controlled experiments, or being terminated and then rerun) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:19:39 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:19:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Ok I&#39;d want to push back some on what you said about your &quot;current answer for doing better&quot;. First, I think it&#39;s bad to ignore that science has aspects to it that are not purely descriptive.* I also agree that we <i>really</i> shouldn&#39;t go all the way in the other directions, where science becomes basically just an extension of politics. Second, depending on the context, leaving it to the &quot;S1 of a handful of decision makers&quot; seems clearly unsatisfying to me.</p><p> *feels worth saying here: we can be somewhat differentiated about how much this is true for what domains/systems. Simon Herbert&#39;s <i>The Sciences of the Artificial</i> has been really influential on me with respect to this. The artificial here refers to ~designed artifacts -- importantly for our context: technology and institutions. The domain of the artificial has this weird descriptive-prescriptive dual nature. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:23:57 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:23:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>I agree we can leverage LLMs for scientific progress here (which is something that s also part of eg Davidad&#39;s OAA mega-plan), though it&#39;s not gonna be easy and there are important ways we could fuck it up. For example, in Davidad&#39;s case (AFAIK) the idea is to get LLMs to write formal models that <i>then get checked line by line by human experts</i> . This is the way in which LLMs can help augment scientific modeling at the moment, but the chceking stage is critical to this being useful rather than harmful.</p><p> I don&#39;t however see how AI systems inherently face less of the recursiveness issue, at least not by default. In fact, I&#39;m pretty worried about <a href="https://arxiv.org/abs/2203.17232">performative predictors</a> . </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:22:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:22:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Yeah, to be clear, I am not at all into ignoring it. I am saying that for any given paper that is trying to illuminate our understanding of some domain, the vast majority of those papers should mostly ignore these phenomena, bar a few particularly recursive domains. And then as scientists and people who build scientific institutions, we should think about how we can set up processes of inquiry that don&#39;t have to deal with this on an ongoing basis, or at least limit the corruption of the relevant forces.</p><p> And yeah, having some science that helps us make those tradeoffs isn&#39;t crazy, but I feel like I would want those papers to tackle the relevant considerations directly, instead of somehow having each paper end up being occupied by concerns about its self-referential影响。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:24:44 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:24:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>I don&#39;t however see how AI systems inherently face less of the recursiveness issue, at least not by default. In fact, I&#39;m pretty worried about <a href="https://arxiv.org/abs/2203.17232">performative predictors</a> .</p></blockquote><p> Well, in the case of AI systems you can study and put bounds on the effects of self-referentiality. You can retrain the old system with the same data. You can objectively talk about what predictions it makes. With humans using their system I to predict complex systems you have so much path dependence in each individual that its approximately impossible to control things. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:30:34 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:30:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Yeah interesting. I would agree that the level of abstraction of a scientifc paper (at least in vast majority of cases) is not where the recursiveness probelm should be addressed. Which does raise the (improtant) question of where and how exactly the problem should then be addressed.我不太确定。 I guess institutions, or the scientific community as a whole is closer to the right abstraction. I also think that this is where philosophy of science has a relevant complementary role to play. The thing feels a bit different for the engineering domains. In particular, I&#39;d definitely want there to be more talk about alternative AI paradigms (and the safety and governance related features of different such paradigms). Given the current landscape, I actually believe that is among the more promising interventions at the moment (with some inital positive signs having started to occured more recently). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:30:37 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:30:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>Which does raise the (important) question of where and how exactly the problem should then be addressed.我不太确定。</p></blockquote><p> Well, in some sense that&#39;s what LessWrong is about. &quot;The Art of Rationality&quot;.</p><p> It&#39;s called an art because indeed it is not a science. It&#39;s a more extensive category that in addition to covering the truth-discovering ways of science, also tries to cover much more practical things, like good cognitive habits and intuitions and making sure you eat the right food, and so on. I also think it&#39;s good to have some good old science on this topic, but ie I find MIRI&#39;s work on transparent game theory more relevant here than most complexity science at trying to study things like recursive modeling effects. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:34:00 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:34:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Yeah agree that epsitemic communities matter here, and that there is more to truth seeking than just the bare bones &#39;scientific method&#39;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:33:19 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:33:19 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, seems like it&#39;s probably about time to wrap up. I enjoyed this! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 29 Nov 2023 02:34:56 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 29 Nov 2023 02:34:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Yeah, same! ：） 谢谢！ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 29 Nov 2023 02:36:11 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 29 Nov 2023 02:36:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Summarizing where we are leaving things a bit for me:</p><ul><li> I am still pretty interested in learning more about wins of complexity theory</li><li> I feel pretty on board with some of the basic premises of complexity theory, but feel confused whether &quot;a scientific field&quot; is even the right way to work on top of these premises</li><li> I feel generally skeptical of fields that are too occupied with their own existence or trying to study its own effects, not because it&#39;s not real, but because it&#39;s just really hard and has all kinds of bad cognitive attractors</li></ul><p> I might also give reading or listening to some of the materials you sent over a try, and might leave comments with additional impressions.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/7KjRqrwjmZRoiBDtn/complex-systems-research-as-a-field-and-its-relevance-to-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7KjRqrwjmZRoiBDtn/complex-systems-research-as-a-field-and-its-relevance-to-ai<guid ispermalink="false"> 7KjRqrwjmZRoiBDtn</guid><dc:creator><![CDATA[Nora_Ammann]]></dc:creator><pubDate> Fri, 01 Dec 2023 22:10:25 GMT</pubDate> </item><item><title><![CDATA[Could there be "natural impact regularization" or "impact regularization by default"?]]></title><description><![CDATA[Published on December 1, 2023 10:01 PM GMT<br/><br/><p> Specifically, imagine you use <a href="https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see">general-purpose</a> search procedure which recursively invokes itself to solve subgoals for the purpose of solving some bigger goal.</p><p> If the search procedure&#39;s solutions to subgoals &quot;change things too much&quot;, then they&#39;re probably not going to be useful. Eg for Rubik&#39;s cubes, if you want to swap some of the cuboids, it does you know good if those swaps leave the rest of the cube scrambled.</p><p> Thus, to some extent, powerful capabilities would have to rely on some sort of impact regularization.</p><p> I&#39;m thinking that natural impact regularization is related to the notion of &quot;elegance&quot; in engineering. Like if you have some bloated tool to solve a problem, then even if it&#39;s not strictly speaking an issue because you can afford the resources, it might feel ugly because it&#39;s excessive and puts mild constaints on your other underconstrained decisions, and so on. Meanwhile a simple, minimal solution often doesn&#39;t have this.</p><p> Natural impact regularization wouldn&#39;t <i>guarantee</i> safety, since it&#39;s still allows deviations that don&#39;t interfere with the AI&#39;s function, but it sort of reduces one source of danger which I had been thinking about lately, namely I had been thinking that the instrumental incentive is to search for powerful methods of influencing the world, where &quot;power&quot; connotes the sort of raw power that unstopably forces a lot of change, but really the instrumental incentive is often to search for &quot;precise&quot; methods of influencing the world, where one can push in a lot of information to effect narrow change. <span class="footnote-reference" role="doc-noteref" id="fnrefcsrxgqulp65"><sup><a href="#fncsrxgqulp65">[1]</a></sup></span></p><p> Maybe another word for it would be &quot;natural inner alignment&quot;, since in a sense the point is that capabilities inevitably select for inner alignment. Here I mean &quot;natural&quot; in the sense of natural abstractions, ie something that a wide variety of cognitive algorithms would gravitate towards. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fncsrxgqulp65"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcsrxgqulp65">^</a></strong></sup></span><div class="footnote-content"><p> A complication is that any one agent can only have so much bandwidth, which would sometimes incentivize more blunt control. I&#39;ve been thinking bandwidth is probably going to become a huge area of agent foundations, and that it&#39;s been underexplored so far. (Perhaps because everyone working in alignment sucks at managing their bandwidth? 😅)</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/cS7rhjuG9thmfTqdy/could-there-be-natural-impact-regularization-or-impact#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cS7rhjuG9thmfTqdy/could-there-be-natural-impact-regularization-or-impact<guid ispermalink="false"> cS7rhjuG9thmfTqdy</guid><dc:creator><![CDATA[tailcalled]]></dc:creator><pubDate> Fri, 01 Dec 2023 22:01:48 GMT</pubDate> </item><item><title><![CDATA[Benchmarking Bowtie2 Threading]]></title><description><![CDATA[Published on December 1, 2023 8:20 PM GMT<br/><br/><p> <span>I&#39;ve been using Bowtie2 to align reads to genomes, and one of it&#39;s many settings is the number of threads. While sometimes people advise using about as many threads as your machine has cores, but if I&#39;m running on a big machine are there diminishing returns or a point at which more threads are counterproductive? Am I better off running more samples in parallel with more threads each, or fewer with fewer?</span></p><p> I decided to run a few tests on an AWS EC2 with a <code>c6a.8xlarge</code> 32-core AMD machine. The test consisted of running one 7.2Gb 48M read-pair sample ( <a href="https://www.ebi.ac.uk/ena/browser/view/SRR23998356">SRR23998356</a> ) from <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7845645/">Crits-Christoph et. al 2021</a> through Bowtie2 2.5.2 with the &quot;Human / CHM13plusY&quot; database from <a href="https://benlangmead.github.io/aws-indexes/bowtie">Langmead&#39;s Index Zone</a> . The files were streamed from AWS S3 and decompressed in a separate process. See the <a href="https://github.com/naobservatory/jefftk-analysis/blob/main/2023-11-30--bowtie-n-threads.py">script</a> for my exact test harness and configuration.</p><p> What I found ( <a href="https://docs.google.com/spreadsheets/d/1W1rBONmiVGcsRLeJajvpsgxffJdDXrOzaDAMLxRFwE0/edit#gid=0">sheet</a> ) was that initially allocating additional threads helps a lot, but after ~8 it was plateauing and after ~10 more threads were very slightly starting to hurt:</p><p> <a href="https://www.jefftk.com/bowtie2-multithread-timing-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/io4wbb47zkpbeyt6hmxw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/io4wbb47zkpbeyt6hmxw 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/ceiesrginxufhmtdlwqp 1100w"></a></p><div></div><p></p><p> Compare what I saw to the best you could hope for, where the task is perfectly parallelizable across threads:</p><p> <a href="https://www.jefftk.com/bowtie2-multithread-theoretical-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/kvqdvm0zgchcxx0q3yb5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/kvqdvm0zgchcxx0q3yb5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/qw1glbql7fsklpcnkggm 1100w"></a></p><div></div><p></p><p> This shows that initially Bowtie2 makes very efficient use of additional threads but, as you&#39;d expect from the plateauing, after ~8 the benefit drops off.</p><p> Another way to look at this is to chart how many thread-seconds it takes to process the whole dataset:</p><p> <a href="https://www.jefftk.com/bowtie2-multithread-thread-seconds-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/hifpeye7botp8y0bh2xl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/hifpeye7botp8y0bh2xl 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TaSZvZDMMhNbtFtCa/acdt48jsf3qt4nmiwuhd 1100w"></a></p><div></div><p></p><p> On this plot perfect parallelism would be a horizontal line, and it&#39;s pretty close to that up through 8 threads. After that, however, it starts rising much more steeply, showing much less efficient use of threads.</p><p> You might wonder how much of this is Bowtie2, and how much is my test environment. For example, perhaps by the time we get to eight threads we&#39;re at the limit of how quickly this machine can download from S3 and decompress the data? Bowtie2 can&#39;t work any faster than the data arrives! I ran a &quot;null&quot; version of this where I replaced <code>bowtie2</code> with <a href="https://www.jefftk.com/p/losing-metaphors-zip-and-paste"><code>paste</code></a> ( <a href="https://github.com/naobservatory/jefftk-analysis/blob/main/2023-11-30--null-n-threads.py">test script</a> ) and found that this consistently took 44-45s. Since it&#39;s all streaming and the fastest run took 189s (4x) I don&#39;t think this was having an appreciable effect.</p><p> Overall, it looks like even on a very large machine you shouldn&#39;t set Bowtie2&#39;s <code>--threads</code> above 10, and unless you have idle cores 8 is probably a better value.</p><br/><br/><a href="https://www.lesswrong.com/posts/TaSZvZDMMhNbtFtCa/benchmarking-bowtie2-threading#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TaSZvZDMMhNbtFtCa/benchmarking-bowtie2-threading<guid ispermalink="false"> TaSZvZDMMhNbtFtCa</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 01 Dec 2023 20:20:06 GMT</pubDate> </item><item><title><![CDATA[Using Prediction Platforms to Select Quantified Self Experiments]]></title><description><![CDATA[Published on December 1, 2023 8:07 PM GMT<br/><br/><blockquote><p> There are too many possible quantified self experiments to run. Do hobbyist prediction platforms <span class="footnote-reference" role="doc-noteref" id="fnref4fo2b58sim8"><sup><a href="#fn4fo2b58sim8">[1]</a></sup></span> make priorisation easier? I test this by setting up multiple markets, in order to run two experiments (the best one, and a random one), mostly for the effects of nootropics on absorption in meditation.</p></blockquote><p></p><p> <a href="https://dynomight.net/prediction-market-causation/#7">dynomight 2022</a> has a cool proposal:</p><blockquote><p> Fortunately, there&#39;s a good (and well-known) alternative, which is to randomize decisions sometimes, at random. You tell people: “I will roll a 20-sided die. If it comes up 1-19, everyone gets their money back and I do what I want. If it comes up 20, the bets activate and I decide what to do using a coinflip.”</p><p> What&#39;s nice about this is that you can do whatever you want when 1-19 come up, including making your decisions using the market prices. But you must make decisions randomly sometimes, because if bets never activate, no one will waste their time betting in your market.</p><p> This is elegant. Oh, and by the way are you THE NSF or DARPA or THE NIH or A BILLIONAIRE WHO WANTS TO SPEND LOTS OF MONEY AND BRAG ABOUT HOW YOU ADVANCED THE STATE OF HUMAN KNOWLEDGE MORE THAN ALL THOSE OTHER LAME BILLIONAIRES WHO WOULDN&#39;T KNOW A HIGH ROI IF IT HIT THEM IN THE FACE? Well how about this:</p><ol><li> Gather proposals for a hundred RCTs that would each be really expensive but also really awesome. (Eg you could investigate <code>SALT → MORTALITY</code> or <code>ALCOHOL → MORTALITY</code> or <code>UBI → HUMAN FLOURISHING</code> .)</li><li> Fund highly liquid markets to predict the outcome of each of these RCTs, conditional on them being funded.<ul><li> If you have hangups about prison, you might want to chat with the <a href="https://en.wikipedia.org/wiki/Commodity_Futures_Trading_Commission">CFTC</a> before doing this.</li></ul></li><li> Randomly pick 5% of the proposed projects, fund them as written, and pay off the investors who correctly predicted what would happen.</li><li> Take the other 95% of the proposed projects, give the investors their money back, and use the SWEET PREDICTIVE KNOWLEDGE to pick another 10% of the RCTs to fund for STAGGERING SCIENTIFIC PROGRESS and MAXIMAL STATUS ENHANCEMENT.</li></ol></blockquote><p> <i>—</i> <a href="https://dynomight.net/prediction-market-causation/"><i>dynomight</i></a> <i>,</i> <a href="https://dynomight.net/prediction-market-causation/"><i>“Prediction market does not imply causation”</i></a> <i>, 2022</i></p><p> Well, I&#39;m neither a billionaire nor the NSF or DARPA, but I <strong>have</strong> run <a>two</a> <a>shitty</a> self-blinded RCTs on myself already, and I&#39;m certainly not afraid of the <a href="https://en.wikipedia.org/wiki/Commodity_Futures_Trading_Commission">CFTC</a> . And indeed I don&#39;t have a shortage of ideas on things I <a>could</a> run RCTs on, but the time is scarce (I try to collect m=50 samples in each RCT, which (with buffer-days off) is usually more than 2 months of data collection).</p><p> So I&#39;ll do what <a href="https://nitter.net/saulmunn/">@saulmunn</a> pointed <a href="https://nitter.net/saulmunn/status/1671923161695240192">out to me</a> is a possibility: I&#39;m going to do <a href="https://www.lesswrong.com/posts/qZXy8kGkNFyqCfHEJ/you-can-do-futarchy-yourself">futarchy (on) myself</a> by setting up a set of markets of Manifold Markets with respect to the outcomes of some pre-specified self-blinded RCTs, waiting until the prices on them equilibriate, and then running two of those RCTs (the &quot;best&quot; one, by my standards, and a random one) and using the results as resolutions, while resolving the others as ambiguous.</p><h3>时间线</h3><p>If the markets receive enough liquidity, I&#39;ll start the first experiment early in 2024, and the second one sometime in 2024 (depending on the exact experiment), hopefully finishing both before 2025.</p><h2>市场</h2><p>Some experiments can be self-blinded, especially ones that involve substances, others can not because they require me to engage in an activity or receive some sensory input, so I distinguish the two, and will slightly prioritise the experiments that can be blinded.</p><p> In all experiments, I will be using the statistical method detailed <a>here</a> , code for it <a>here</a> , unless someone points out that I&#39;m doing my statistics wrong.</p><p> I will be scoring the markets based on the variables specified in the prediction market title, but I&#39;ll of course be collecting <a>a lot of other data</a> during that time that will also be analyzed.</p><h3> Self-Blinded Experiments</h3><p> In general, by <i>meditative absorption</i> I mean the concentration/tranquility (in Buddhist terms <a href="https://en.wikipedia.org/wiki/Samatha-vipassana">samatha</a> ) during a ≥30 minute meditation session in the morning, ~45 minutes after waking up and taking the substance (less if the substance starts working immediately). I will be doing at least 15 minutes of <a href="https://en.wikipedia.org/wiki/Anapanasati">anapanasati</a> during that meditation session, but might start (or end) with another practice).</p><p> Past meditation data can be found <a>here</a> .</p><ol><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-caffeine-ltheanine"><strong>L-Theanine + Caffeine</strong> vs. <strong>Sugar</strong> → <i>Meditative Absorption</i></a> : 50 samples in the morning after waking up, 25 intervention with 500mg l-theanine &amp; 200mg caffeine and 25 placebo (sugar pills). Expected duration of trial: ~2½ months (one sample every day, but with possible pauses).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-nicotine-improve-m"><strong>Nicotine</strong> vs. <strong>Normal chewing gum</strong> → <i>Meditative Absorption</i></a> : 40 samples, with <a href="https://en.wikipedia.org/wiki/Blocking_(statistics)">blocking</a> after waking up, 20 intervention with 2mg nicotine, 20 placebo (similar-looking square chewing gum). Expected duration of trial: ~4½ months (two samples/week, to avoid getting addicted to nicotine).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-modafinil-improve"><strong>Modafinil</strong> vs. <strong>Sugar</strong> → <i>Meditative Absorption</i></a> : 40 samples, again with <a href="https://en.wikipedia.org/wiki/Blocking_(statistics)">blocking</a> directly after waking up, 20 intervention with 100mg modafinil and 20 placebo (sugar pills). Expected duration of trial: Also ~4½ months with two samples per week, as to prevent becoming dependent on modafinil.</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-vitamin-d-improve"><strong>Vitamin D</strong> vs. <strong>Sugar</strong> → <i>Meditative Absorption</i></a> : 50 samples, taken after waking up, 25 intervention (25μg Vitamin D₃) and 25 placebo (sugar pills). Expected duration of trial: ~2½ months (taken ~every day, with possible pauses).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-vitamin-b12-improv"><strong>Vitamin B12</strong> vs. <strong>Sugar</strong> → <i>Meditative Absorption</i></a> : 50 samples, taken after waking up, 25 intervention (500μg Vitamin B12 + 200μg <a href="https://en.wikipedia.org/wiki/Folate">folate</a> ) and 25 placebo (sugar pills). Expected duration of trial: 2½ months (short interruptions included).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-microdosed-lsd-imp"><strong>LSD Microdosing</strong> vs. <strong>Water</strong> → <i>Meditative Absorption</i></a> : 50 samples in the morning, 25 intervention (10μg LSD), and 25 placebo (distilled water). Expected duration of trial is ~4 months (4 samples per week, with some time left as a buffer).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-cbd-improve-medita"><strong>CBD Oil</strong> vs. <strong>Similar-Tasting Oil</strong> → <i>Meditative Absorption</i></a> : 50 samples in the morning, 25 intervention (240mg CBD in oil, orally), and 25 placebo (whatever oil I can find that is closest in taste to the CBD oil). Expected duration of the trial: ~2½ months (taken ~every day, with possible pauses).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-lphenylalanine-imp"><strong>L-Phenylalanine</strong> vs. <strong>Sugar</strong> → <i>Meditative Absorption</i></a> : 50 samples, taken directly after waking up, 25 intervention (750mg L-Phenylalanine), and 25 placebo (sugar pills). Duration of trial: 2½ months (one sample a day).</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-bupropion-improve"><strong>Bupropion</strong> vs. <strong>Sugar</strong> → <i>Happiness</i></a> : 50 samples taken after waking up, 25 intervention (150mg <a href="https://en.wikipedia.org/wiki/Bupropion">Bupropion</a> ), and 25 placebo (sugar pills). Duration is typical 2½ months again.</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-thc-oil-improve-me"><strong>THC Oil</strong> vs. <strong>Similar-Tasting Oil</strong> → <i>Meditative Absorption</i></a> : 50 samples in the morning, 25 intervention (2.5mg THC in oil, orally), and 25 placebo (whatever oil I can find that is closest in taste to the THC oil). Expected duration of the trial: ~2½ months (taken ~every day, with possible pauses).</li></ol><h3> Non-Blinded Experiments</h3><p> Some experiments can&#39;t be blinded, but they can still be randomized. I will focus on experiments that can be blinded, but don&#39;t want to exclude the wider space of interventions.</p><ol><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-intermittent-fasti"><strong>Intermittent Fasting</strong> vs. <strong>Normal Diet</strong> → <i>Happiness</i></a> : 50 samples, 25 intervention (eating only between 18:00 and midnight), 25 non-intervention (normal diet, which is usually 2 meals a day, spaced ~10 hours apart), chosen randomly via <code>echo -e &quot;fast\ndon&#39;t fast&quot; | shuf | tail -1</code> . Expected duration of the trial: ~2 months.</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-the-pomodoro-metho"><strong>Pomodoro Method</strong> vs. <strong>Nothing</strong> → <i>Productivity</i></a> : 50 samples, 25 intervention (I try to follow the <a href="https://en.wikipedia.org/wiki/Pomodoro_technique">Pomodoro method</a> as best as I can, probably by installing a <a href="https://www.lesswrong.com/posts/wJutA2czyFg6HbYoW/what-are-trigger-action-plans-taps">TAP</a> of some sort), 25 non-intervention (I just try to do work as normally), chosen randomly via <code>echo -e &quot;pomodoro\nno pomodoro&quot; | shuf | tail -1</code> . Expected duration of trial: 2 months.</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-very-bright-light"><strong>Bright Light</strong> vs. <strong>Normal Light</strong> → <i>Happiness</i></a> : 50 samples, 25 intervention (turning on my <a href="https://arbital.com/p/lumenators/">lumenator</a> of ~30k lumen in the morning), 25 non-intervention (turning on my normal desk lamp of ~1k lumen), selected via <code>echo -e &quot;lamp\nno lamp&quot; | shuf | tail -1</code> . Expected duration of trial: 4 months, as I often don&#39;t spend all my day at home.</li><li> <a href="https://manifold.markets/NiplavYushtun/by-how-much-does-2-hours-of-meditat"><strong>Meditation</strong> vs. <strong>No Meditation</strong> → <i>Sleep duration</i></a> : 50 samples, 25 intervention (2 consecutive days of ≥2h/day of meditation), 25 non-intervention (no meditation), selected via <code>echo -e &quot;meditation\nno meditation&quot; | shuf | tail -1</code> . Expected duration of trial: 5 months, as I might not always find a 2-day interval in which I&#39;m sure I can meditative 2h/day.</li></ol><p> Further Ideas</p><p> I have a couple more ideas on possible experiments that I could run, and will put them up as I acquire more mana.</p><p> Blindeable:</p><ol><li> <strong>Semaglutide</strong> vs. <strong>Sugar</strong> → <i>Productivity</i> (tracking conscientiousness)</li></ol><p> Not blindeable:</p><ol><li> <strong>Binaural Beats</strong> vs. <strong>Silence</strong> → <i>Meditative Absorption</i></li><li> <strong>Brown Noise</strong> vs. <strong>Silence</strong> → <i>Meditative Absorption</i></li><li> <strong>Brown Noise</strong> vs. <strong>Music</strong> → <i>Productivity</i></li><li> <strong>Silence</strong> vs. <strong>Music</strong> → <i>Productivity</i></li><li> <strong>Time Since Last Masturbation</strong> → <i>Productivity</i></li><li> <strong>Starting Work Standing</strong> vs. <strong>Starting Work Sitting</strong> → <i>Productivity</i></li></ol><h2>请求</h2><p>This little exercise may need <strong>your</strong> participation! I have three pleas to you, dear reader:</p><ol><li> <strong>Please predict on the markets!</strong> If people predict on the markets, I <i>both</i> get more information about the value of the different experiments, and I also get mana back. It would be cool to know whether hobbyist prediction markets <i>can</i> be used for choosing experiments, and the worst result would be a &quot;well, we can&#39;t really tell because liquidity on the markets was too small&quot;.</li><li> <strong>Maybe send me mana for me to create more markets or subsidise existing ones.</strong> I&#39;d love to subsidise my markets on Manifold a whole bunch, but don&#39;t have enough mana for that at the moment. <a href="https://manifold.markets/anonymous">clippy</a> and <a href="https://manifold.markets/Tetraspace">Tetraspace</a> both already send me mana, which I greatly appreciate. With more mana, I could also put up more markets, and thereby explore a larger space of possible experiments. However, maybe the value of another market isn&#39;t so high, so this one is way less urgent.</li><li> <strong>Give me ideas for more experiments to run.</strong> If you have an idea you&#39;re enthusiastic about and which you&#39;ve always wanted to have tested, but you&#39;re kind of lazy about actually doing it, I might be able to jump in. Most interesting to me are experiments that are:<ol><li> <i>Affordable</i> : Expensive substances, high-end devices etc. are too prohibitive (unless you want to buy the thing for me to perform the experiment).</li><li> <i>Safe</i> : Sorry, I&#39;m not going to take methamphetamine, even though it might make me much more productive.</li><li> <i>Measurable</i> : The variable the intervention is supposed to affect should be measurable in at least <i>one</i> of the ways <a href="http://niplav.site/data.html">I currently collect data</a> , or at least easily measurable. In particular cognitive performance is hard to get a grip on: <a href="https://en.wikipedia.org/wiki/Iq_test">IQ test</a> can&#39;t be repeated very often, but maybe there&#39;s a game that measures cognitive performance reliably?</li><li> <i>Fast</i> : I can&#39;t do 50 samples of an intervention where one sample takes 2 weeks to take effect. Daily is best, but for <i>really good</i> options I might be willing to tolerate 2 samples a week.</li></ol></li></ol><p> Other than that, I also welcome all critiques at any level of detail of this undertaking.</p><h2> Further Ideas</h2><p> If I could create more markets, I might be able to put up markets on different variables I measure during the day. That way, I could select interventions that dominate others across multiple dimensions.</p><p> If there were prediction platforms that supported them, combinatorial prediction markets or <a href="https://www.lesswrong.com/posts/ufW5LvcwDuL6qjdBT/latent-variables-for-prediction-markets-motivation-technical">latent-variable prediction markets</a> could be incredibly cool, but we don&#39;t live in that world (yet).</p><h2>结果</h2><p>To be done, hopefully by early 2025.</p><h2>致谢</h2><p>Many thanks to <a href="https://manifold.markets/anonymous">clippy</a> ( <a href="https://twitter.com/12leavesleft">twitter</a> ) for 500 Mana and <a href="https://manifold.markets/Tetraspace">Tetraspace</a> ( <a href="https://twitter.com/TetraspaceWest">twitter</a> ) for 1000 Mana — your funding of the sciences is greatly appreciated.</p><h2> Appendix A: Explanations for the Experiments I Chose</h2><p> Over time, I&#39;ll put some explanations on why these specific experiments interest me. Not yet, though. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn4fo2b58sim8"> <span class="footnote-back-link"><sup><strong><a href="#fnref4fo2b58sim8">^</a></strong></sup></span><div class="footnote-content"><p> I find it odd to call any platform on which people functionally give probabilities, but without staking real money, &quot;prediction markets&quot;. Neither <a href="https://www.metaculus.com/">Metaculus</a> not <a href="https://manifold.markets/">Manifold Markets</a> are prediction markets, but <a href="https://www.predictit.org/">PredictIt</a> and <a href="https://kalshi.com/">Kalshi</a> are.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2qQyKpXzbPov2Fmdr/using-prediction-platforms-to-select-quantified-self#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2qQyKpXzbPov2Fmdr/using-prediction-platforms-to-select-quantified-self<guid ispermalink="false"> 2qQyKpXzbPov2Fmdr</guid><dc:creator><![CDATA[niplav]]></dc:creator><pubDate> Fri, 01 Dec 2023 20:07:38 GMT</pubDate></item></channel></rss>