<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 3 日星期五 18:16:00 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Thoughts on open source AI]]></title><description><![CDATA[Published on November 3, 2023 3:35 PM GMT<br/><br/><p><i>认知状态：我只有约 50% 的人认可这一点，这低于我发布内容的典型标准。我更看好“这些是应该在供水中讨论的论点”，而不是“这些论点实际上是正确的”。我不是这方面的专家，我只考虑了大约 15 个小时，而且在发布之前我没有任何相关专家运行这篇文章。</i></p><p><i>感谢 Max Nadeau 和 Eric Neyman 的有益讨论。</i></p><p>目前，关于开源人工智能存在大量的公开辩论。关注人工智能安全的人们普遍认为，开源强大的人工智能系统太危险，不应该被允许；这里的经典例子是“你不应该被允许开源一个人工智能系统，它可以<a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant"><u>为工程新型病原体产生分步指令</u></a>。”另一方面，开源支持者认为开源模型尚未造成重大损害，试图关闭人工智能的访问将导致<a href="https://open.mozilla.org/letter/"><u>权力</u></a><a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>集中</u></a>在少数人工智能实验室手中。</p><p>我认为许多关心人工智能安全的人没有认真考虑过这一点，他们往往会模糊地认为“开源强大的人工智能系统似乎很危险，应该被禁止”。从字面上看，我认为这个计划有点天真：当我们在 2100 年在我们一致的超级智能的帮助下殖民火星时，释放 GPT-5 的权重真的会带来灾难性的风险吗？</p><p>我认为更好的计划看起来像“在确定并披露系统将启用的威胁模型类型之前，您不能开源系统，并且社会已经采取措施来增强对这些威胁模型的鲁棒性。一旦有必要措施已经落实，你们就可以自由开源了。”</p><p>我稍后会详细介绍，但作为一个直觉泵，想象一下：最好的开源模型总是比最好的专有模型（称之为 GPT-SoTA）落后 2 年<span class="footnote-reference" role="doc-noteref" id="fnrefnmr2zc5cm4r"><sup><a href="#fnnmr2zc5cm4r">[1]</a></sup></span> ； GPT-SoTA 在整个经济中广泛部署，并部署用于监控和防止某些攻击媒介，而最好的开源模型也不够智能，不会在 GPT-SoTA 无法捕获的情况下造成任何重大损害。在这个假设的世界里，只要我们能够信任 GPT-SoTA <i>，</i>我们就不会受到开源模型带来的伤害。换句话说，只要最好的开源模型远远落后于最好的专有模型，并且我们明智地了解如何使用最好的专有模型，开源模型就不会杀死我们。</p><p>在这篇文章的其余部分中，我将：</p><ul><li>通过类比密码学中负责任的披露来推动该计划</li><li>详细了解该计划</li><li>讨论这与我对负责任的扩展政策 (RSP) 所暗示的当前计划的理解有何关系</li><li>讨论一些关键的不确定因素</li><li>对围绕开源人工智能的讨论提出一些更高层次的想法</li></ul><h2><strong>与密码学中负责任的披露的类比</strong></h2><p><i>[我不是这方面的专家，本节可能会出现一些细节错误。感谢 Boaz Barak 指出这个类比（但所有错误都是我自己的）。</i></p><p><i>请参阅此脚注</i><span class="footnote-reference" role="doc-noteref" id="fnreflndvavl4r2a"><sup><a href="#fnlndvavl4r2a">[2]，</a></sup></span><i>了解您可以对生物安全披露规范进行的替代类比的讨论，以及它们是否更容易受到开源人工智能的风险。]</i></p><p>假设您发现某些广泛使用的加密方案中存在漏洞。进一步假设您是一个好人，不希望任何人受到黑客攻击。你该怎么办？</p><p>如果您公开发布您的漏洞利用程序，那么很多人都会受到黑客攻击（被阅读您对该漏洞利用程序描述的不那么仁慈的黑客攻击）。另一方面，如果是<a href="https://en.wikipedia.org/wiki/White_hat_(computer_security)"><u>白帽</u></a><u>&nbsp;</u>黑客总是对他们发现的漏洞保密，那么这些漏洞永远不会被修补，直到黑帽黑客发现漏洞并利用它。更一般地说，您可能担心不披露漏洞可能会导致“安全溢出”，即可发现但尚未发现的漏洞随着时间的推移而累积，最终被利用时情况会变得更糟。</p><p>在实践中，密码学界已经达成了<i>负责任的披露</i>政策，大致如下：</p><ul><li>首先，您向受影响方披露该漏洞。<ul><li>作为一个正在运行的示例，请考虑 Google<a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"><u>对 SHA-1 哈希函数的利用</u></a>。在这种情况下，有很多受影响方，因此谷歌公开发布了该漏洞的概念验证，但没有提供足够的细节供其他人立即重现。</li><li>在其他情况下，您可能会私下披露更多信息，例如，如果您在 Windows 操作系统中发现漏洞，您可能会私下将其连同实现漏洞的代码一起披露给 Microsoft。</li></ul></li><li>然后，您为要修补的漏洞设置合理的时间范围。<ul><li>就 SHA-1 而言，补丁是“停止使用 SHA-1”，实施期限为 90 天。</li></ul></li><li>在此时间段结束时，您可以公开发布您的漏洞利用程序，包括执行它的源代码。<ul><li>这确保了受影响的各方得到适当的激励来修补漏洞，并帮助其他白帽黑客在未来发现其他漏洞。</li></ul></li></ul><p>据我了解，该协议使我们的加密方案相对稳健：人们大多不会受到严重的黑客攻击，而当他们这样做时，主要是因为通过社会工程进行攻击（例如<a href="https://en.wikipedia.org/wiki/Operation_Rubicon"><u>，中央情报局秘密拥有他们的加密提供商</u></a>） ，而不是通过对该方案的攻击。 <span class="footnote-reference" role="doc-noteref" id="fnref3ejitdom769"><sup><a href="#fn3ejitdom769">[3]</a></sup></span></p><h2><strong>负责任地披露开源人工智能系统的功能：概述</strong></h2><p><i>[感谢 Yusuf Mahmood 指出本节中概述的协议与此处的协议大致</i><a href="https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf"><i><u>相似</u></i></a><i>。更一般地说，我希望在这一领域工作的人们已经熟悉这些想法。]</i></p><p>在本节中，我将为开源人工智能系统制定一个协议，该协议类似于密码学中的负责任的披露协议。假设假设的公司 Mesa 训练了一个新的人工智能系统<a href="https://en.wikipedia.org/wiki/Camelidae"><u>camelidAI</u></a> ，Mesa 希望将其开源。我们也将最强大的专有人工智能系统称为 GPT-SoTA，我们可以假设它表现良好<span class="footnote-reference" role="doc-noteref" id="fnref1art7tmmdob"><sup><a href="#fn1art7tmmdob">[4]</a></sup></span> 。我想象 GPT-SoTA 比 CamelidAI 更有能力（特别是在大多数领域都是超人）。原则上，如果 GPT-SoTA 比 CamelidAI 更差（因为开源系统已经超越了专有系统），下面的协议仍然有意义，但它会退化为“一旦开源 AI 系统能够引起重大新奇事件，就禁止它们”。他们也无法可靠地减轻伤害。”</p><p>在这个协议中，在camelidAI能够开源之前，[??梅萨？，政府？，第三方？ ？？] 必须：</p><ul><li>评估camelidAI如果开源的话可能会造成什么样的重大新危害。 <span class="footnote-reference" role="doc-noteref" id="fnref7501lf6ibc3"><sup><a href="#fn7501lf6ibc3">[5]</a></sup></span>这些评估者至少应该能够访问开源系统用户将拥有的所有工具，包括微调camelidAI的能力、可以在camelidAI之上构建的外部工具以及对camelidAI的API调用GPT-SoTA。因此，典型的工作流程可能类似于：让 GPT-SoTA 生成可能的接管计划的完整列表，然后微调camelidAI以完成这些计划中的步骤。例如，我们可能会发现：<ul><li>经过微调后，camelidAI 能够在人类专家级别进行有针对性的网络钓鱼（也称为<a href="https://en.wikipedia.org/wiki/Phishing"><u>鱼叉式网络钓鱼</u></a>），但可以扩展到更多目标。</li><li> CamelidAI 可以提供<a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.13952.pdf"><u>外行人可遵循的制造新型病原体的说明，其中包括 DNA 合成公司和生物实验室不筛选客户和订单的说明</u></a>。</li></ul></li><li>向 [?? 披露这些新的有害功能政府？、第三方监督者？、受影响各方？ ??]。</li><li>与相关参与者合作改进系统，直到系统对每个有权访问camelidAI的人来说都是稳健的。<ul><li>例如，确保有一个广泛可用的开源工具，可以检测像camelidAI一样复杂的网络钓鱼尝试，并且具有非常高的可靠性。</li><li>例如，关闭不筛选订单的 DNA 合成公司和生物实验室，或强迫他们使用 GPT-SoTA 筛选潜在大流行病原体的订单。</li><li>请注意，如果camelidAI非常有能力，那么其中一些预防措施可能会非常雄心勃勃，例如“让社会对人为设计的流行病具有强大的能力”。这里的希望在于我们能够获得一个功能强大且性能良好的 GPT-SoTA。</li><li>还要注意的是，这些“鲁棒性”措施是我们无论如何都应该做的事情，即使我们不想开源camelidAI；否则，未来未对齐的人工智能（可能是非法开源的模型）可能会利用一个悬而未决的问题。</li></ul></li><li>一旦社会对camelidAI造成的危害有足够的抵抗力（经[??]认证），您就可以开源camelidAI。</li><li>另一方面，如果 Mesa 在完成上述过程之前开源了camelidAI，那么它会被视为不良行为者（类似于我们对待未负责任地披露漏洞的黑客的方式）。<ul><li>也许这意味着你要对camelidAI或其他东西造成的伤害负责，但不太确定。</li></ul></li></ul><p>作为示例，让我注意该协议的两个特殊情况：</p><ul><li>假设camelidAI = LLaMA-2。我认为访问 LLaMA-2 <span class="footnote-reference" role="doc-noteref" id="fnrefvqihg0rt649"><sup><a href="#fnvqihg0rt649">[6]</a></sup></span>可能不会带来重大的新危害。因此，在进行评估后，缓解步骤很简单：不需要“补丁”，并且 LLaMA-2 可以开源。 （我认为这很好：AFAICT、LLaMA-2 的开源对世界有好处，包括对齐研究。）</li><li>假设camelidAI能够发现这一将岩石和空气变成黑洞的奇怪技巧（天体物理学家讨厌它！）。假设没有合理的缓解措施来缓解这种攻击，camelidAI 永远不会开源。 （我希望即使是强大的开源支持者也会同意这是这种情况下的正确结果。）</li></ul><p>我还将指出该协议与密码学中负责任的披露的两个不同之处：</p><ol><li> Mesa 不得设定社会必须在多长时间内增强 CamelidAI 能力的最后期限。如果camelidAI拥有一种如果被滥用将造成灾难性的能力，并且我们需要十年的技术进步才能找到解决该问题的“补丁”，那么在这种情况发生之前，Mesa不会开源该模型。</li><li>在密码学中，受影响的各方有责任修补漏洞，但在这种情况下，人工智能系统的开发人员有部分责任。</li></ol><p>这两个差异意味着其他各方没有动力去强化他们的系统；原则上他们可能会永远拖延，而 Mesa 永远不会发布camelidAI。我认为应该采取一些措施来解决这个问题，例如政府应该对那些没有充分优先考虑实施必要变革的公司进行罚款。</p><p>但总的来说，我认为这是公平的：如果你知道你的系统可能会造成巨大伤害，并且你没有计划如何防止这种伤害，那么你就无法开源你的人工智能系统。</p><p>我喜欢这个协议的一件事是它很难反驳：如果camelidAI明显能够自主设计一种新的病原体，那么Mesa就不能再声称这些<a href="https://twitter.com/ID_AA_Carmack/status/1719077965055533455"><u>危害是想象的</u></a>或<a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>过度夸大的</u></a>，或者<a href="https://twitter.com/ylecun/status/1719692258591506483"><u>作为一个一般原则开源人工智能让我们更安全</u></a>。我们将会受到具体的、明显的伤害；我们可以讨论如何减轻这种特定的伤害，而不是争论抽象的人工智能是否可以减轻人工智能的伤害。如果人工智能可以提供缓解措施，那么我们将找到并实施缓解措施。同样，如果最终证明这些危害<i>是</i>虚构的或夸大的，那么 Mesa 将免费开源camelidAI。</p><h2><strong>这与当前计划有何关系？</strong></h2><p>据我了解，推动许多负责任的扩展政策（RSP）支持者的高层想法是这样的：</p><blockquote><p>在采取某些行动（例如训练或部署人工智能系统）之前，人工智能实验室需要提出“安全论据”，即该行动不会造成重大伤害的论据。例如，如果他们想要部署一个新系统，他们可能会争论：</p><ol><li>我们的系统不会造成伤害，因为它没有足够的能力造成重大损害。 （如果 OpenAI 在发布 GPT-4 之前被要求提出安全论点，那么他们很可能会提出这样的论点，对我来说这似乎是正确的。）</li><li>我们的系统如果尝试这样做<i>可能会</i>造成伤害，但它不会尝试这样做，因为例如它仅通过 API 部署，并且我们已使用[措施]确保任何 API 介导的交互都不会导致其尝试伤害。</li><li>如果我们的系统尝试这样做，它<i>可能</i>会造成伤害，并且我们不能排除它会尝试这样做，但它不会成功造成伤害，因为，例如，它仅在我们拥有非常好的措施的严格控制环境中使用阻止其成功执行有害操作。</li></ol><p>如果不存在这样的论点，那么您需要做一些事情来<i>导致</i>这样的论点存在（例如，更好地调整模型，以便您可以提出上面的论点（2））。在您这样做之前，您不能采取任何您想要采取的有潜在风险的行动。</p></blockquote><p>我认为，如果你在“开源人工智能系统”的情况下应用这个想法，你会得到与我上面概述的协议非常相似的东西：为了开源人工智能系统，你需要提出一个论点开源该系统是安全的。如果不存在这样的论点，那么您需要做一些事情（例如改进对网络钓鱼尝试的电子邮件监控）来使这样的论点存在。</p><p>目前，开源的安全论点与上面的 (1) 相同：当前的开源系统不足以造成重大的新危害。将来，这些论点将变得更加棘手，特别是对于可以修改（例如微调或合并到更大系统中）并且其环境可能是“整个世界”的开源模型。但是，随着前沿人工智能系统的进步彻底改变了世界，这些论点对于非前沿系统来说可能仍然是可能的。 （我预计开源模型将继续落后于前沿。）</p><h2><strong>一些不确定因素</strong></h2><p>以下是我的一些不确定性：</p><ul><li>在实践中，效果如何？<ul><li>我认为一个合理的猜测可能是：几年后，SoTA 模型将足够聪明，如果开源的话，会造成重大灾难，而且——即使有 SoTA AI 的帮助——我们也无法修补相关漏洞，直到奇点（之后球出我们的场）。如果是这样，该协议基本上可以归结为对开源人工智能的禁令，并采取额外的步骤。</li><li>然而，我要指出的是，开源支持者（其中许多人预计有害功能的进展会较慢）可能不同意这一预测。如果他们是对的，那么这个协议可以归结为“评估，然后开源”。我认为，如果人工智能安全人员对未来的看法是正确的，那么制定专门针对人工智能安全人员的需求的政策是有好处的，如果开源人员对未来的看法是正确的，那么制定专门针对开源人员想要的政策是有好处的。</li></ul></li><li>评估人员是否能够预测和衡量开源人工智能系统的所有新危害？<ul><li>遗憾的是，我不确定答案是“是”，这也是我只支持 50% 这篇文章的主要原因。我担心评估者可能失败的两个原因：<ul><li>评估者可能无法获得比用户更好的工具，而且用户的数量要多得多。例如，尽管评估人员将得到 GPT-SoTA 的协助，但如果 CamelidAI 是开源的，数百万用户也将获得访问权。</li><li>在camelidAI开源后，世界可能会发生改变，从而启用新的威胁模型。例如，假设camelidAI + GPT-SoTA并不危险，但camelidAI + GPT-(SoTA+1)（GPT-SoTA后继系统）是危险的。如果 GPT-(SoTA+1) 在camelidAI开源几个月后出现，这似乎是个坏消息。</li></ul></li></ul></li><li>也许由于某种我们难以预料的原因，使用微妙不对齐的 SoTA AI 系统来评估和监控其他 AI 系统确实很糟糕？<ul><li>例如，人工智能系统相互协调的东西。</li></ul></li></ul><h2><strong>关于开源话语的一些思考</strong></h2><p>我认为许多关注人工智能安全的人犯了这样的错误：“我注意到存在一些能力阈值<i>T</i> ，超过该阈值的每个人都可以访问具有 >; <i>T</i>功能的人工智能系统，这将成为当今世界的生存威胁。按照目前的轨迹，有一天有人会开源一个能力 >; <i>T 的</i>人工智能系统。因此，开源很可能会导致灭绝，应该被禁止。”</p><p>我认为这种推理忽略了这样一个事实：当有人第一次尝试开源一个功能 >; <i>T</i>的系统时，世界将在很多方面有所不同。例如，可能存在专有的功能系统<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gg T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">≫</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。总的来说，我认为人工智能安全社区的人们过于担心来自开源模型的威胁。</p><p>此外，人工智能安全社区对开源人工智能的反对目前正在开源社区中产生<i>很大</i>的敌意。就背景而言，开源意识形态与软件开发的历史深深地交织在一起，开源的坚定支持者在技术领域具有很大的代表性和影响力。 <span class="footnote-reference" role="doc-noteref" id="fnrefhqvunm4kwo5"><sup><a href="#fnhqvunm4kwo5">[7]</a></sup></span>我有点担心，按照目前的轨迹，人工智能安全与开源将成为一个主要战场，很难达成共识（比 IMO 不太糟糕的人工智能歧视/道德与 x- 更糟糕）风险划分）。</p><p>在某种程度上，这种敌意是由于对开源危险的不必要的关注或对这种危险存在的草率论据，我认为这确实是不幸的。我认为有充分的理由<i>以特定方式</i>担心<i>一定规模</i>的开源人工智能系统的潜在危险，而且我认为更清楚地了解这些威胁模型的细微差别可能会减少敌意。</p><p>此外，我认为当开源模型变得危险时，我们很有可能有具体的证据表明它们是危险的（例如，因为我们已经看到相同规模的未对齐的专有模型是危险的）。这意味着，“如果[存在危险的证据]，则[政策]”形式的政策提案获得了大部分安全效益，同时在安全界对未决问题的看法是错误的世​​界中，也优雅地失败了（即不施加过多的开发成本）。危险。理想情况下，这意味着此类政策更容易达成共识。 </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnnmr2zc5cm4r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnmr2zc5cm4r">^</a></strong></sup></span><div class="footnote-content"><p>目前这对我来说似乎是正确的，即 LLaMA-2 比 20 个月前发布的 GPT-3.5 稍差一些。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlndvavl4r2a"> <span class="footnote-back-link"><sup><strong><a href="#fnreflndvavl4r2a">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://forum.effectivealtruism.org/posts/PTtZWBAKgrrnZj73n/biosecurity-culture-computer-security-culture"><u>杰夫·考夫曼（Jeff Kaufman）撰写了有关</u></a>计算机安全和生物安全社区之间规范差异的文章。简而言之，虽然计算机安全规范鼓励尝试破坏系统和披露漏洞，但生物安全规范却阻止公开讨论可能的漏洞。杰夫将其归因于许多结构性因素，包括修补生物安全漏洞的难度；来自开源人工智能的威胁模型可能与生物风险模型有更多共同点，在这种情况下，我们应该基于它们来构建我们的防御模型。想要了解更多 ctrl-f “冷汗” <a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/#crispr-based-gene-drive-022318"><u>，请</u></a>阅读 Kevin Esvelt 讨论的为什么他没有向任何人（甚至包括他的顾问）透露基因驱动的想法，直到他确定基因驱动是防御主导的。 （感谢 Max Nadeau 的这两个参考文献，以及我在其他地方链接的大多数生物相关材料的参考文献。）</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3ejitdom769"> <span class="footnote-back-link"><sup><strong><a href="#fnref3ejitdom769">^</a></strong></sup></span><div class="footnote-content"><p>我预计有些人会争论我们的密码学是否真的那么好，或者指出，如果你认为人工智能“我们只有一次机会”，那么这句话中的“相对”和“大部分”这两个词是令人担忧的。因此，让我先澄清一下，我不太关心该协议的确切成功程度；我主要用它作为一个说明性的类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1art7tmmdob"> <span class="footnote-back-link"><sup><strong><a href="#fnref1art7tmmdob">^</a></strong></sup></span><div class="footnote-content"><p>我们可以做出这样的假设，因为我们正在处理由开源人工智能引起的灾难的威胁模型。如果您认为杀死我们的第一件事是错位的专有人工智能系统，那么您应该关注该威胁模型而不是开源人工智能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7501lf6ibc3"> <span class="footnote-back-link"><sup><strong><a href="#fnref7501lf6ibc3">^</a></strong></sup></span><div class="footnote-content"><p>这是协议中我感到最紧张的部分；请参阅“一些不确定性”部分中的要点 2。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvqihg0rt649"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvqihg0rt649">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.18233.pdf"><u>此处显示</u></a>，根据病毒学数据进行微调的 LLaMA-2 对于向黑客马拉松参与者提供获取和释放重建的 1918 年流感病毒的指示非常有用。然而，目前尚不清楚这种危害是否新颖——我们不知道如果仅访问互联网，参与者会做得更糟糕。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhqvunm4kwo5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhqvunm4kwo5">^</a></strong></sup></span><div class="footnote-content"><p>我注意到人工智能安全问题很难在麻省理工学院获得关注，尤其是在麻省理工学院，我对正在发生的事情的一个猜测是，开源意识形态在麻省理工学院非常有影响力，而目前所有开源人士都讨厌开源人工智能安全人员。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai<guid ispermalink="false"> WLYBy5Cus4oRFY3mu</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Fri, 03 Nov 2023 15:35:42 GMT</pubDate> </item><item><title><![CDATA[Shouldn't we 'Just' Superimitate Low-Res Uploads?]]></title><description><![CDATA[Published on November 3, 2023 7:42 AM GMT<br/><br/><p>如果您有三个非常不同的优化器（一个是神经网络，另一个是手工构建的，最后一个是通过类似玻尔兹曼大脑的过程实现的），但具有相同的偏好和相同的优化能力，那么它们可能最终在足够长的时间范围内做类似的事情。</p><p>我提出这一点，是因为在讨论上传时，人们似乎倾向于获得整个思维的数字编码，就好像上传的有用部分包含它以类似人类的方式进行优化的事实，而它看起来像什么我们实际上想要的是能够以任何方式优化的东西，只要它针对我们想要的东西进行优化，无论其复杂性如何。</p><p>在<a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#Training_frontier_models_to_predict_neural_activity_instead_of_next_token">Davidad 的上传登月计划有效吗？</a> ， <a href="https://www.lesswrong.com/users/jacobjacob?mention=user">@jacobjacob</a>提出以下内容：</p><blockquote><p> “仅仅”训练一个巨型变压器来预测神经活动，而不是预测自然语言中的下一个标记，这个想法怎么样？</p></blockquote><p> <a href="https://www.lesswrong.com/users/lisathiergart?mention=user">@lisathiergart</a>回复说，这可能违反了产品将比现状更加一致的假设。我提出以下反对意见：</p><ul><li>您从本次培训中需要的产品并不像整个大脑的近似那么复杂，“只是”它的偏好。如果您可以从 Transformer 中提取奖励模型（您甚至可以在“上传”时使用 SOTA AI 对其进行设计，使其变得更好）并将优化器指向该方向，您就可以实现类似的端点。</li><li>在 MEG 数据上训练大型神经网络似乎相当不错。 Meta 在<a href="https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/">解码大脑活动图像</a>方面取得了巨大成功。<a href="https://arxiv.org/abs/2208.12266">言语也是如此</a>。这对我来说表明当前的神经网络足够强大，可以从脑电图数据中提取有意义的模式（这篇演讲论文甚至在脑电图方面取得了一些成功）。</li><li>当深度学习规模足够大时，可以做很多令人惊讶的事情。如果你问大多数人对于训练巨型变压器来预测下一个 token 有何想法，他们可能不会预测到 2023 年会出现 GPT-4。也许这同样适用于预测下一个 MEG 读数？<ul><li>这似乎很容易测试。我思考了 30 分钟，并提出了一些基于 Mistral-7B 的架构，我觉得我可以在短时间内合理地实现它们。然而，我没有任何 MEG 硬件，而且获取起来很昂贵。根据我的研究，假设龙猫缩放定律在这种情况下大致成立，似乎没有足够的公开数据来训练 Mistral-7B 大小（甚至更小）的东西。</li></ul></li></ul><hr><p> <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation">凡妮莎·科索伊 (Vanessa Kosoy) 将物理主义的超级模仿视为学习理论议程的一个潜在终点</a>。她对超级模仿的描述如下：</p><blockquote><p>一个代理（以下称为“模仿者”）接收另一个代理（以下称为“原始”）的策略，并产生追求相同目标但<i>明显</i><i>更好的</i>行为。</p></blockquote><p>如果您可以模仿或以其他方式优化上传的偏好，也许简单的方法（例如预测下一个 MEG 读数）就足够了，或者至少与训练完整上传并利用它相当，尽管要容易得多？</p><br/><br/> <a href="https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads<guid ispermalink="false"> KGTGgnGppf9wzwmFM</guid><dc:creator><![CDATA[marc/er]]></dc:creator><pubDate> Fri, 03 Nov 2023 07:42:07 GMT</pubDate> </item><item><title><![CDATA[The other side of the tidal wave]]></title><description><![CDATA[Published on November 3, 2023 5:40 AM GMT<br/><br/><p>我猜人工智能在未来几十年内可能有 10-20% 的可能性导致人类灭绝，但我对此感到比所暗示的更令人痛苦 - 我认为因为在它不会导致人类灭绝的情况下，我发现它很难想象生活不会偏离轨道。这个世界上我喜欢的很多事情似乎都可能被超人人工智能所终结或严重扰乱（写作、向人们解释事情、对彼此有用的友谊、对技能感到自豪、思考、学习、弄清楚）如何实现事物、制造事物、轻松跟踪有意识和无意识的事物），而且我不相信替代品实际上是好的，或者对我们有好处，或者任何事情都是可逆的。</p><p>即使我们没有死，仍然感觉一切都即将结束。</p><br/><br/> <a href="https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave<guid ispermalink="false"> uyPo8pfEtBffyPdxf</guid><dc:creator><![CDATA[KatjaGrace]]></dc:creator><pubDate> Fri, 03 Nov 2023 05:40:06 GMT</pubDate> </item><item><title><![CDATA[Does davidad's uploading moonshot work?]]></title><description><![CDATA[Published on November 3, 2023 2:21 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/users/davidad">davidad</a>就一项提案进行了 10 分钟的演讲，他说：“我第一次看到一个具体计划，可以在 2040 年之前实现人类上传，如果资金无限，甚至可能更快”。</p><p>我认为这个演讲很值得一看，但即使你没有看过，下面的对话也很容易读懂。我还在本对话的附录中添加了一些演讲摘要。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=jZqynCV0AGc&amp;list=PLH78wfbGI1x2CI6aV_hiOE1_GOFkZAFph&amp;index=7"><div><iframe src="https://www.youtube.com/embed/jZqynCV0AGc" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>我认为这次演讲的承诺如下。看起来，为了让未来顺利发展，我们必须要么放慢通用人工智能的进展，要么让对齐的进展有差异地更快。然而，上传似乎提供了第三种方式：我们“简单地”<i>运行得更快</i>，而不是让比对研究人员更有效率。这看起来类似于<a href="https://openai.com/blog/introducing-superalignment"><u>OpenAI 的 Superalignment</u></a>提案，即建立一个自动对齐科学家——但关键的例外是，我们可能会合理地认为，人类上传的内容会比法学硕士等有更好的对齐保证。</p><p>我决定就这个提议组织一次对话，因为它给我的印象是“如果是真的的话，那就是巨大的”，但当我与一些人讨论这个提议时，我发现人们普遍没有意识到这一点，有时还会提出问题我和谈话都无法回答的/困惑/反对意见。</p><p>我还邀请了 Anders Sandberg，他<a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf">与 Nick Bostrom 共同撰写了 2008 年全脑模拟路线图</a>，并共同组织了<a href="https://foresight.org/whole-brain-emulation-workshop-2023/">有关该主题的 Foresight 研讨会，Davidad 在会上介绍了他的计划</a>，还邀请了来自 MIRI 的 Lisa Thiergart，她也在研讨会上发表了<a href="https://www.youtube.com/watch?v=gf7W82mrRfI">演讲</a><a href="https://www.lesswrong.com/posts/KQSpRoQBz7f6FcXt3/distillation-of-neurotech-and-alignment-workshop-january-1">之前曾在 LessWrong 上撰写过有关全脑模拟的文章</a>。</p><p>对话最终同时涵盖了很多话题。我将它们分成六个独立的部分，这些部分相当独立可读。</p><h2>条形码作为拦截器（和同步加速器） </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:31:34 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:31:34 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>一件重要的事情是“如果人们真的尝试过，这个上传计划可能会失败的原因是什么。”我认为主要的阻碍因素是<i>对跨膜蛋白进行条形码编码</i>。让我首先阐述为什么我们“不需要”大脑中的大部分物质，例如DNA甲基化、基因调控网络、微管等的标准论点。</p><ol><li>认知反应时间很快。</li><li>这意味着单个神经元的响应时间需要更快。</li><li>化学扩散速度太快，无法传播很远——只能跨越微小的突触间隙距离。</li><li>所以所有非突触的东西都必须是电的。</li><li>电以电位差（电压）的形式发挥作用，而组织中唯一可以产生电压差的部分是跨膜的。</li><li>因此，所有的认知信息处理都发生在膜和突触的过程上。</li></ol><p>现在，人们经常在这里做出非常无根据的飞跃，也就是说，我们需要知道的只是膜和突触的<i>几何形状和连接图</i>。但膜和突触并不是由同质的膜和突触构成的。膜主要由磷脂双层组成，但许多重要的工作是由跨膜蛋白完成的：正是这些分子将传入的神经递质转化为电信号，将电信号调节为动作电位，导致动作电位衰减，并且释放轴突末端的神经递质。而且人类神经系统中有很多很多不同的跨膜蛋白。它们中的大多数可能并不重要，或者足够相似，可以将它们混为一谈，但我敢打赌，至少有几十种非常不同的跨膜蛋白（抑制性与兴奋性、不同的时间常数、对不同离子的不同敏感性）从而导致不同的信息处理行为。因此，我们不仅需要看到神经元和突触，还需要看到整个神经元细胞膜（包括但不限于突触处）的许多不同种类的跨膜蛋白中每种跨膜蛋白的定量密度。</p><p> Sebastian Seung 在他关于连接组学的书中提出了一个著名的假设，即人脑中只有几百种不同的细胞类型，它们在每个突触上具有大致相同的行为，并且我们可以仅从几何形状中找出每个细胞的类型。但我认为这不太可能，因为学习是通过突触可塑性进行的：也就是说，学到的很多东西，至少在记忆中（而不是技能）是由突触的相对受体密度来表示的。然而，<i>可能</i>我们很幸运，纯粹是突触的<i>大小</i>就足够了。在这种情况下，我们实际上非常幸运，因为同步加速器解决方案更加可行，而且比膨胀显微镜快得多。使用扩展显微镜，通过对荧光团进行条形码标记来标记大量受体<i>似乎</i>是合理的，就像人们现在开始用不同的荧光团组合对<i>细胞</i>进行条形码标记一样（这解决了一个完全不同的问题，即轴突追踪的冗余）。然而，<strong>基于放大显微镜的计划最有可能失败的原因是我们无法使用荧光来标记足够的跨膜蛋白</strong>。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>有什么选择可以克服这个问题，它们有多容易处理？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:19:32 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:19:32 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>对于条形码，到目前为止我只想到了几种不同的技术途径。在某种程度上，它们可以结合起来。</p><ol><li> &quot;serial&quot; barcoding, in which we find a protocol for washing out a set of tags in an already-expanded sample, and then diffusing a new set of tags, and being able to repeat this over and over without the sample degrading too much from all the washing.</li><li> &quot;parallel&quot; barcoding, in which we conjugate several fluorophores together to make a &quot;multicolored&quot; fluorophore (literally like a barcode in the visible-light spectrum). This is the basic idea that is used for barcoding cells, but the chemistry is very different because in cells you can just have different concentrations of separate fluorophore molecules floating around, whereas receptors are way too small for that and you need to have one of each type of fluorophore all kind of stuck together as one molecule. Chemistry is my weakness, so I&#39;m not very sure how plausible this is, but from a first-principles perspective it seems like it might work. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:35:19 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:35:19 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> the synchrotron solution is far more viable</p></blockquote><p> On synchotrons / particle accelerator x-rays: <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan Collins estimates</a> they would take ~1 yr of sequential effort for a whole human brain (which thus also means you could do something like a mouse brain in a few hours, or an organoid in minutes, for prototyping purposes). But I&#39;m confused why you suggest that as an option that&#39;s <i>differentially</i> compatible with only needing lower resolution synaptic info like size.</p><p> Could you not do expansion microscopy + synchrotron? And if you need the barcoding to get what you want, wouldn&#39;t you need it with or without synchrotron? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:37:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:37:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> So, synchrotron imaging uses X-rays, whereas expansion microscopy typically uses ~visible light fluorescence (or a bit into IR or UV). (It is possible to do expansion and then use a synchrotron, and the best current synchrotron pathway does do that, but the speed advantages are due to X-rays being able to penetrate deeply and facilitate tomography.) There are a lot of different indicator molecules that resonate at different wavelengths of ~visible light, but not a lot of different indicator atoms that resonate at different wavelengths of X-rays. And the synchrotron is monochromatic anyway, and its contrast is by transmission rather than stimulated emission. So for all those reasons, with a synchrotron, it&#39;s really pushing the limits to get even a small number of distinct tags for different targets, let alone spectral barcoding. That&#39;s the main tradeoff with synchrotron&#39;s incredible potential speed. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> One nice thing with working in visible light rather than synchrotron radiation is that the energies are lower, and hence there is less disruption of the molecules and structure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Andreas Schaefer seems to be making great progress with this, see eg <a href="https://www.nature.com/articles/s41467-022-30199-6#Sec7">here</a> . I have updated downward in conversations with him that sample destruction will be a blocker. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also, there are many modalities that have been developed in visible light wavelengths that are well understood. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> This is a bigger concern for me, especially in regards to spectral barcodes.</p></div></section><h2> End-to-end iteration as blocker (and organoids, holistic processes, and exotica) </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:36:52 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:36:52 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> I think the barcoding is a likely blocker. But I think the most plausible blocker is a more diffuse problem: we do not manage to close the loop between actual, running biology, and the scanning/simulation modalities so that we can do experiments, adjust simulations to fit data, and then go back and do further experiments - including developing new scanning modalities. My worry here is that while we have smart people who are great at solving well-defined problems, the problem of setting up a research pipeline that is good at iterating at generating well-defined problems might be less well-defined... and we do not have a brilliant track record of solving such vague problems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:45:33 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:45:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yeah, this is also something that I&#39;m concerned that people who take on this project might fail to do by default, but I think we do have the tech to do it now: human brain organoids. We can manufacture organoids that have genetically-human neurons, at a small enough size where the entire thing can be imaged <i>dynamically</i> (ie non-dead, while the neurons are firing, actually collecting the traces of voltage and/or calcium), and then we can slice and dice the organoid and image it with whatever static scanning modality, and see if we can reproduce the actual activity patterns based on data from the static scan. This could become a quite high-throughput pipeline for testing many aspects of the plan (system identification, computer vision, scanning microscopes, sample prep/barcoding, etc.). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:46:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:46:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> What is the state of the art of &quot;uploading an organoid&quot;?<br><br> (I guess one issue is that we don&#39;t have any &quot;validation test&quot;: we just have neural patterns, but the organoid as a whole isn&#39;t really doing any function. Whereas, for example, if we tried uploading a worm we could test whether it remembers foraging patterns learnt pre-upload) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:51:31 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:51:31 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> I&#39;m not sure if I would know about it, but I just did a quick search and found things like &quot; <a href="https://www.nature.com/articles/s41467-022-32115-4">Functional neuronal circuitry and oscillatory dynamics in human brain organoids</a> &quot; and &quot; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0142961222004653">Stretchable mesh microelectronics for biointegration and stimulation of human neural organoids</a> &quot;, but nobody is really trying to &quot;upload an organoid&quot;. They are already viewing the organoids as more like &quot;emulations&quot; on which one can do experiments in place of human brains; it is an unusual perspective to treat an organoid as more like an organism which one might try to emulate. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:58:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:58:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> On the general theme of organoids &amp; shortcuts:<br><br> I&#39;m thinking about what are key blockers in dynamic scanning as well later static slicing &amp; imaging. I&#39;m probably not fully up to date on the state of the art of organoids &amp; genetic engineering on them. However, if we are working on the level of organoids, couldn&#39;t we plausibly directly genetically engineer (or fabrication engineer) them to make our life easier?</p><ul><li> ex. making the organoid skin transparent (visually or electrically)</li><li> ex. directly have the immunohistochemistry applied as the organoid grows / is produced</li><li> ex. genetically engineer them such that the synaptic components we are most interested in are fluorescent</li></ul><p> ...probably there are further such hacks that might speed up the &quot;Physically building stuff and running experiments&quot; rate-limit on progress</p><p> (ofc getting these techniques to work will also take time, but at appropriate scales it might be worth it) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:05:27 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:05:27 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> We definitely can engineer organoids to make experiments easier. They can simply be thin enough to be optically transparent, and of course it&#39;s easier to grow them <i>without</i> a skull, which is a plus.</p><p> In principle, we could also make the static scanning easier, but I&#39;m a little suspicious of that, because in some sense the purpose of organoids in this project would be to serve as &quot;test vectors&quot; for the static scanning procedures that we would want to apply to non-genetically-modified human brains. Maybe it would help to get things off the ground to help along the static scanning with some genetic modification, but it might be more trouble than it&#39;s worth. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:05:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:05:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Lisa, here&#39;s a handwavy question. Do you have an estimate, or the components of an estimate, for &quot;how quickly could we get a &#39;Chris Olah-style organoid&#39; -- that is, the <i>smallest</i> organoid that 1) we fully understood, in the relevant sense and also 2) told us at least something interesting on the way to whole brain emulation?&quot;</p><p> (Also, I don&#39;t mean that it would be an organoid <i>of</i> poor Chris! Rather, my impression is that his approach to interpretability, is &quot;start with the smallest and simplest possible toy system that you do <i>not</i> understand, then understand that really well, and then increase complexity&quot;. This would be adapting that same methodology) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:36:21 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:36:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Wow that&#39;s a fun one, and I don&#39;t quite know where to start. I&#39;ll briefly read up on organoids a bit. [...reading time...] Okay so after a bit of reading, I don&#39;t think I can give a great estimate but here are some thoughts:<br><br> First off, I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole&#39; works well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot). Similarly, I&#39;m not sure how much starting with a simpler example (say a different animal with a smaller brain) will reliably transfer, but at least some of it might. For developing the needed technologies it definitely seems helpful.<br><br> That said, starting small is always easier. If I were trying to come up with components for an estimate I&#39;d consider:</p><ul><li> looking at ​​non-human organoids</li><li> comparisons to how long similar efforts have been taking: C. elegans is a simpler worm organism some groups have worked on trying to whole brain upload, which seems to be taking longer than experts predicted (as far as I know the efforts have been ongoing for >;10 years and not yet concluded, but there has been some progress). Though, I think this is affected for sure by there not being very high investment in these projects and few people working on it.</li><li> trying to define what &#39;fully understood&#39; means: perhaps being able to get within x% error on electrical activity prediction and/or behavioural predictions (if the organism exhibits behaviors)</li></ul><p> There&#39;s definitely tons more to consider here, but I don&#39;t think it makes sense for me to try to generate it.​ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:42:31 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:42:31 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><blockquote><p> I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole works&#39; well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot).</p></blockquote><p> One simple example of a &quot;holistic&quot; property that might prove troublesome is oscillatory behavior, where you need a sufficient number of units linked in the right way to get the right kind of oscillation. The fun part is that you get oscillations almost automatically from any neural system with feedback, so distinguishing merely natural frequency oscillations (eg the gamma rhythm seems to be due to fast inhibitory interneurons if I remember right) and the functionally important oscillations (if any!) will be tricky. Merely seeing oscillations is not enough, we need some behavioral measures.</p><p> There is likely a dynamical balance between microstructure-understanding based &quot;IKEA manual building&quot; of the system and the macrostructure-understanding &quot;carpentry&quot; approach. Setting the overall project pipeline in motion requires having a good adaptivity on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:39:25 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:39:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Wouldn&#39;t it be possible to bound the potentially relevant &quot;whole brain processes&quot;, by appealing to a version of Davidad&#39;s argument above that we can neglect a lot of modalities and stuff, because they operate at a slower timescale than human cognition (as verified for example by simple introspection)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:47:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:47:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> I think glia are also involved in <a href="https://www.sciencedirect.com/science/article/pii/S0165017309001076?casa_token=V_DcC59YI7MAAAAA:XzfbMUr_R56pDQNA57HLWI6NFMxBzeN2ATF89tw2yGe7uchy5dZFLpHJgBurGAR9Drc0iYzVvQ">modulating synaptic transmissions</a> as well as the electric conductivity of a neuron (definitely the speed of conductance), so I&#39;m not sure the speed of cognition argument necessarily disqualifies them and other non-synaptic components as relevant to an accurate model.  This <a href="https://www.frontiersin.org/articles/10.3389/fncel.2016.00188/full">paper</a> presents the case of glia affecting synaptic plasticity and being electrically active.  Though I do think that argument seems to be valid for many components, which clearly cannot effect the electrical processes at the needed time scales.<br><br> With regards to &quot;whole brain processes&quot; what I&#39;m gesturing at is there might be top level control or other processes running without whose inputs the subareas&#39; function cannot be accurately observed. We&#39;d need to have an alternative way of inputting the right things into the slice or subarea sample to generate the type of activity that actually occurs in the brain. Though, it seems we can focus on the electrical components of such a top level process in which case I wouldn&#39;t see an obvious conflict between the two arguments. I might even expect the electrical argument to hold more, because of the need to travel quickly between different (far apart) brain areas. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:53:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:53:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yeah, sorry, I came across as endorsing a modification of the standard argument that rules out too many aspects of brain processes as cognition-relevant, where I only rule back in <i>neural</i> membrane proteins. I&#39;m quite <i>confident</i> that neural membrane proteins will be a blocker (~70%) whereas I am less confident about glia (~30%) but it&#39;s still very plausible that we need to learn something about glial dynamics in order to get a functioning human mind. However, whatever computation the glia are doing is also probably based on their own membrane proteins! And the glia are included in the volume that we have to scan anyway. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:54:42 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:54:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Yeah, makes sense. I also am thinking that maybe there&#39;s a way to abstract out the glia by considering their inputs as represented in the membrane proteins. However, I&#39;m not sure whether it&#39;s cheaper/faster to represent the glia vs. needing to be very accurate with the membrane proteins (as well as how much of a stretch it is to assume they can be fully represented there). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:58:30 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:58:30 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> My take on glia is that they may represent a sizeable pool of nonlinear computation, but it seems to run slowly compared to the active spikes of neurons. That may require lots of memory, but less compute. But real headache is that there is relatively little research on glia (despite their very loyal loyalists!) compared to those glamorous neurons. Maybe they do represent a good early target for trying to define a research subgoal of characterizing them as completely as possible. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:27:37 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:27:37 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Overall, the &quot;exotica&quot; issue is always annoying - there could be an infinite number of weird modalities we are missing, or strange interactions requiring a clever insight. However, I think there is a pincer maneouver here where we try to constrain it by generating simulations from known neurophysiology (and nearby variations), and perhaps by designing experiments to estimate degrees of unknown degrees of freedom (much harder, but valuable). As far as I know the latter approach is still not very common, my instant association is to how statistical mechanics can estimate degrees of freedom sensitively from macroscopic properties (leading, for example, to primordial nucleogenesis to constrain elementary particle physics to a surprising degree). This is where I think a separate workshop/brainstorm/research minipipeline may be valuable for strategizing.</p></div></section><h2> Using AI to speed up uploading research </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> As AI develops over the coming years, it will speed up some kinds of research and engineering. I&#39;m wondering: for this proposal, which parts could future AI accelerate? And perhaps more interestingly: which parts would <i>not</i> be easily accelerable (implying that work on those parts sooner is more important)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:11:55 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:11:55 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> The frame I would use is to sort things more like along a timeline of when AI might be able to accelerate them, rather than a binary easy/hard distinction. (Ultimately, a friendly superintelligence would accelerate the entire thing—which is what many folks believe friendly superintelligence ought to be used for in the first place!)</p><ol><li> The easiest parts to accelerate are the computer vision. In terms of timelines, this is in the rear-view mirror, with deep learning starting to substantially accelerate the processing of raw images into connectomic data <a href="https://arxiv.org/pdf/1706.00120.pdf">in 2017</a> .</li><li> The next easiest is the modelling of the dynamics of a nervous system based on connectomic data. This is a mathematical modelling challenge, which needs a bit more structure than deep learning traditionally offers, but it&#39;s not that far off (eg with multimodal hypergraph transformers).</li><li> The next easiest is probably more ambitious microscopy along the lines of &quot;computational photography&quot; to extract more data with fewer electrons or photons by directing the beams and lenses according to some approximation of <a href="https://arxiv.org/pdf/1103.5708.pdf">optimal Bayesian experimental design</a> . This has the effect of accelerating things by making the imaging go faster or with less hardware.</li><li> The next easiest is the engineering of the microscopes and related systems (like automated sample preparation and slicing). These are electro-optical-mechanical engineering problems, so will be harder to automate than the more domain-restricted problems above.</li><li> The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i> of the microscopes, and the management of failures and repairs and replacements of parts, etc. Of course this is still possible to automate, but it requires capabilities that are quite close to the kind of superintelligence that can maintain a robot fleet without human intervention. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:13:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:13:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i></p></blockquote><p> An interesting related question is &quot;if you had an artificial superintelligence suddenly appear today, what would be its &#39;manufacturing overhang&#39;? How long it would it take it to build the prerequisite capacity, starting with current tools?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:55:36 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:55:36 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Could AI assistants help us build a research pipeline? I think so. Here is a sketch: I run a bio experiment, I scan the bio system, I simulate it, I get data that disagrees.哪里有问题？ Currently I would spend a lot of effort trying to check my simulator for mistakes or errors, then move on to check whether the scan data was bad, then if the bio data was bad, and then if nothing else works, positing that maybe I am missing a modality. Now, with good AI assistants I might be able to (1) speed up each of these steps, including using multiple assistants running elaborate test suites. (2) automate and parallelize the checking. But the final issue remains tricky: what am I missing, if I am missing something? This is where we have human level (or beyond) intelligence questions, requiring rather deep understanding of the field and what is plausible in the world, as well as high level decisions on how to pursue research to test what new modalities are needed and how to scan for them. Again, good agents will make this easier, but it is still tricky work.</p><p> What I suspect could happen for this to fail is that we run into endless parameter-fiddling, optimization that might hide bad models by really good fits of floppy biological systems, and no clear direction for what to research to resolve the question. I worry that it is a fairly natural failure mode, especially if the basic project produces enormous amount of data that can be interpreted very differently. Statistically speaking, we want identifiability. But not just of the fit to our models, but to our explanations. And this is where non-AGI agents have not yet demonstrated utility. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:48:00 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:48:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Yeah, so, overall, it seems &quot;physically building stuff&quot;, &quot;running experiments&quot; and &quot;figure out what you&#39;re missing&quot; are some of where the main rate-limiters lie. These are the hardest-to-automate steps of the iteration loop, that prevent an end-to-end AI assistant helping us through the pipeline.</p><p> Firstly though, it seems important to me how frequently you run into the rate limiter. If &quot;figuring out what you&#39;re missing&quot; is something you do a few times a week, you could still be sped up a lot by automating the other parts of the pipeline until you can run this problem a few times per day.</p><p> But secondly I&#39;m interested in answers digging one layer down of concreteness -- which part of the building stuff is hard, and which part of the experiment running is hard? For example: Anders had the idea of &quot; <a href="https://www.youtube.com/watch?v=ItEnEN58bJw">the neural pretty printer</a> &quot;: create ground truth artificial neural networks performing a known computation >;>; convert them into a Hodgkin-Huxley model (or similar) >;>; fold that up into a 3D connectome model >;>; simulate the process of scanning data from that model -- and then attempt to reverse engineer the whole thing. This would basically be a simulation pipeline for validating scanning set-ups.</p><p> To the extent that such simulation is possible, <i>those</i> particular experiments would probably not be the rate-limiting ones. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:21:22 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:21:22 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The neural pretty printer is an example of building a test pipeline: we take a known simulatable neural system, convert it into a plausible biological garb and make fake scans of it according to the modalities we have, and then try to get our interpretation methods reconstruct it. This is great, but eventually limited. The real research pipeline will have to contain (and generate) such mini-pipelines to ensure testability. There is likely an organoid counterpart. Both have a problem of making systems to test the scanning based on somewhat incomplete (or really incomplete!) data.</p></div></section><h2> Training frontier models to predict neural activity instead of next token </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:20:36 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:20:36 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Lisa, you said in our opening question brainstorm:</p><blockquote><p> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.<br><br> 1a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from the dynamic data collected [(with the aim of figuring out the statics-to-dynamics map)] then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.<br><br> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)</p></blockquote><p> Somewhat tangentially, this makes me wonder about taking this idea to its limit: what about the idea of &quot;just&quot; training a giant transformer to, instead of predicting next tokens in natural language, predicting neural activity? (at whatever level of abstractions is most suitable.) &quot;Imitative neural learning&quot;. I wonder if that would be within reach of the size of models people are gearing up to train, and whether it would better preserve alignment guarantees. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:38:20 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:38:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Hmm, intuitively this seems not good. On the surface level, two reasons come to mind:<br><br> 1. I suspect the element of &quot;more reliably human-aligned&quot; breaks or at least we have less strong reasons to believe this would be the case than if the total system also operates on the same hardware structure (so to say, it&#39;s not actually going to be run on something carbon-based). Though I&#39;m also seeing the remaining issue of: &quot;if it looks like a duck (structure) and quacks like a duck (behavior), does that mean it&#39;s a duck?&quot;.  At least we have the benefit of deep structural insight as well as many living humans as a prediction on how aligned-ness of these systems turns out in practice. (That argument holds in proportion to how faithful of an emulation we achieve.)<br><br> 2. I would be really deeply interested in whether this would work. It is a bit reminiscent of <a href="https://manifund.org/projects/activation-vector-steering-with-bci">the Manifund proposal</a> Davidad and I have open currently, where the idea is to see if human brain data can improve performance on next token prediction and make it more human preference aligned.  At the same time, for the &#39;imitative neural learning&#39; you suggest (btw I suspect it would be feasible with GPT4/5 levels, but I see the bottleneck/blocker in being able to get enough high quality dynamic brain data) I think I&#39;d be pretty worried that this would turn into some dangerous system (which is powerful, but not necessarily aligned).</p><p> 2/a Side thought: I wonder how much such a system would in fact be constrained to human thought processes (or whether it would gradient descend into something that looks input and output similar, but in fact is something different, and behaves unpredictably in unseen situations). Classic Deception argument I guess (though in this case without an implication of some kind of intentionality on the system&#39;s side, just that it so happens bc of the training process and data it had) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:40:50 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:40:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> I basically agree with Lisa&#39;s previous points. Training a transformer to imitate neural activity is a little bit better than training it to imitate words, because one gets more signal about the &quot;generators&quot; of the underlying information-processing, but misgeneralizing out-of-distribution is still a big possibility. There&#39;s something qualitatively different that happens if you can collect data that is logically upstream of the entire physical information processing conducted by the nervous system — you can then make predictions about that information-processing <strong>deductively</strong><i><strong> </strong></i>rather than <strong>inductively</strong> (in the sense of the problem of induction), so that whatever inductive biases (aka priors) are present in the learning-enabled components end up having no impact. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> &quot;Human alignment&quot;: one of the nice things with human minds is that we understand roughly how they work (or at least the signs that something is seriously wrong). Even when they are not doing what they are supposed to do, the failure modes are usually human, all too human. The crux here is whether we should expect to get human-like systems, or &quot;humanish&quot; systems that look and behave similar but actually work differently. The structural constraints from Whole Brain Emulation are a good reason to think more of the former, but I suspect many still worry about the latter because maybe there are fundamental differences in simulation from reality. I think this will resolve itself fairly straightforwardly since - by assumption if this project gets anywhere close to succeeding - we can do a fair bit of experimentation on whether the causal reasons for various responses look like normal causal reasons in the bio system. My guess is that here Dennett&#39;s principle that the simplest way of faking many X is to be X. But I also suspect a few more years of practice with telling when LLMs are faking rather than grokking knowledge and cognitive steps will be very useful and perhaps essential for developing the right kind of test suite.</p></div></section><h2> How to avoid having to spend $100B and and build 100,000 light-sheet microscopes </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> “15 years and ~$500B” ain&#39;t an easy sell. If we wanted this to be doable faster (say, &lt;5 years), or cheaper (say, &lt;$10B): what would have to be true? What problems would need solving? Before we finish, I am quite keen to poke around the solution landscape here, and making associated fermis and tweaks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:16:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:16:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> So, definitely the most likely way that things could go faster is that it turns out all the receptor densities are predictable from morphology (eg synaptic size and &quot;cell type&quot; as determined by cell shape and location within a brain atlas). Then we can go ahead with synchrotron (starting with organoids!) and try to develop a pipeline that infers the dynamical system from that structural data. And synchrotron is much faster than expansion. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:17:34 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:17:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> it turns out all the receptor densities are predictable from morphology</p></blockquote><p> What&#39;s the fastest way you can see to validating or falsifying this? Do you have any concrete experiments in mind that you&#39;d wish to see run if you had a magic wand? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:22:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:22:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Unfortunately, I think we need to actually see the receptor densities in order to test this proposition. So transmembrane protein barcoding still seems to me to be on the critical path in terms of the tech tree. <strong>But</strong> if this proposition turns out to be true, then you won&#39;t need to use slow expansion microscopy when you&#39;re actually ready to scan an entire human brain—you only need to use expansion microscopy on some samples from every brain area, in order to learn a kind of &quot;Rosetta stone&quot; from morphology to transmembrane protein(/receptor) densities for each cell type. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:23:58 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:23:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> So, for the benefit of future readers (and myself!) I kind of would like to see you multiply 5 numbers together to get the output $100B (or whatever your best cost-esimate is), and then do the same thing in the fortunate synchrotron world, to get a fermi of how much things would cost in that<i> </i>world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:30:24 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:30:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Ok. I actually haven&#39;t done this myself before. Here goes.</p><ol><li> The human central nervous system has a volume of about 1400 cm^3.</li><li> When we expand it for expansion microscopy, it expands by a factor of 11 in each dimension, so that&#39;s about 1.9 m^3. (Of course, we would slice and dice before expanding...)</li><li> Light-sheet microscopes can cover a volume of about 10^4 micron^3 per second, which is about 10^-14 m^3 per second.</li><li> That means we need about 1.9e14 microscope-seconds of imaging.</li><li> If the deadline is 10 years, that&#39;s about 3e8 seconds, so we need to build about 6e5 microscopes.</li><li> Each microscope costs about $200k, so 6e5 * $200k is $120B. (That&#39;s not counting all the R&amp;D and operations surrounding the project, but building the vast array of microscopes is the expensive part.)</li></ol><p> Pleased by how close that came out to the number I&#39;d apparently been citing before. (Credit is due to Rob McIntyre and Michael Andregg for that number, I think.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:34:35 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:34:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Similarly, looking at <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan&#39;s slide</a> : <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rukiwrds8jln8vohsdvq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/fkcy0prpnp9w6qjpiiqn 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/aviijtkzbyulztz2wqis 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pibu1fgqe56lwn0gvxep 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/cs3agbeodgrdlyqhljcz 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/nqfxbmm14nc5lp1x8jfd 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/iupqwcowydhnwayhyipg 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/ypjrtn88oiiudmvfemqq 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/xgbk2coub2wfku0nbzuu 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jbk9mmkpafzjwu2zpkfr 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/hjmtwofbbnqvesrbeuhh 2610w"></p><p> $1M for instruments, and needing 600k years. So, make 100k microscopes and run them in parallel for 6 years and then you get $100B... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:35:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:35:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> That system is a bit faster than ExM, but it&#39;s transmission electron microscopy, so you get roughly the same kind of data as synchrotron anyway (higher resolution, but probably no barcoded receptors) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:41:54 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:41:54 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Now for the synchrotron cost estimate.</p><ol><li> Synchrotron imaging has about 300nm voxel size, so to get accurate synapse sizes we would still need to do expansion to 1.9 m^3 of volume.</li><li> Synchrotron imaging has a speed of about 600 s/mm^3, but it seems uncontroversial that this may be improved by an order of magnitude with further R&amp;D investment.</li><li> That multiplies to about 3000 synchrotron-years.</li><li> To finish in 10 years, we would need to build 300 synchrotron beamlines.</li><li> Each synchrotron beamline costs about $10M.</li><li> That&#39;s $3B in imaging infrastructure. A bargain! </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:43:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:43:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> That&#39;s very different from this estimate.想法？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jsl0moz7gppg42my41gd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/tqubpcpofsbslowecsbz 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/j9y0zplo8tcfomk2eqcz 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/e3xhqypr9xocrzwo1i8n 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/wpiyujikqiscbidicvfy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/gpfs5v4ohmrz5xjsxhaw 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/l7peqlu22d7otroqf9d9 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rftaksliw9honhyj9cvw 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/x4ybkurznlhqfavkvkx5 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/vvb2t0pjlezivooycucb 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/je0nhzwyykyyi3f2wchd 1660w"></figure></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:46:16 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:46:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> One difference off the bat - 75nm voxel size is maybe enough to get a rough connectome, but not enough to get precise estimates of synapse size. I think we&#39;d need to go for 11x expansion. So that&#39;s about 1 order of magnitude, but there&#39;s still 2.5 more to account for. My guess is that this estimate is optimistic about combining multiple potential avenues to improve synchrotron performance. I do see some claims in the literature that more like 100x improvement over the current state of the art seems feasible. </p></div></section><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:33:34 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:33:34 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> What truly costs money in projects? Generally, it is salaries and running costs times time, plus the instrument/facilities costs as a one-time cost. One key assumption in this moonshot is that there is a lot of scalability so that once the basic setup has been made it can be replicated ever cheaper (Wrightean learning, or just plain economies of scale). The faster the project runs, the lower the first factor, but the uncertainty about whether all relevant modalities have been covered will be greater. There might be a rational balance point between rushing in and likely having to redo a lot, and being slow and careful but hence getting a lot of running costs. However, from the start the difficulty is very uncertain, making the actual strategy (and hence cost) plausibly a mixture model. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:35:57 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:35:57 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The path to the 600,000 microscopes is of course to start with 6, doing the testing and system integration while generating the first data for the initial mini-pipelines for small test systems. As that proves itself one can scale up to 60 microscopes for bigger test systems and smaller brains. And then 600, 6,000 and so on. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:44:40 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:44:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Hundreds of thousands of microscopes seem to me like an... issue. I&#39;m curious if you have something like a &quot;shopping list&quot; of advances or speculative technologies that could bring that number down a lot. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:55:55 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:55:55 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Yes, that is quite the lab floor. Even one per square meter makes a ~780x780 meter space. Very Manhattan Project vibe. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:47:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:47:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Google tells me Tesla Gigafactory Nevada is about 500k m^2, so about the same :P </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Note though that economies of scale and learning curves can make this more economical than it seems. If we assume an experience curve with price per unit going down to 80% each doubling, 600k is 19 doublings, making the units in the last doubling cost just 1.4% of the initial unit cost. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:48:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:48:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> And you don&#39;t need to put all the microscopes in one site. If this were ever to actually happen, presumably it would be an international consortium where there are 10-100 sites in several regions that create technician jobs in those areas (and also reduce the insane demand for 100,000 technicians all in one city). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also other things might boost efficiency and price.</p><p> Most obvious would be nanotechnological systems: likely sooner than people commonly assume, yet might take long enough to arrive to make effect on this project minor if it starts soon. Yet design-ahead for dream equipment might be a sound move.</p><p> Advanced robotics and automation is likely major gamechanger. The whole tissue management infrastructure needs to be automated from the start, but there will likely be a need for rapid turnaround lab experimentation too. Those AI agents are not just for doing standard lab tasks but also for designing new environments, tests, and equipment. Whether this can be integrated well in something like the CAIS infrastructure is worth investigating. But rapid AI progress also makes it hard to do plan-ahead.</p><p> Biotech is already throwing up lots of amazing tools. Maybe what we should look for is a way of building the ideal model organism - not just with biological barcodes or convenient brainbow coloring, but with useful hooks (in the software sense) for testing and debugging. This might also be where we want to look at counterparts to minimal genomes for minimal nervous systems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:57:06 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:57:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><blockquote><p> Yet design-ahead for dream equipment might be a sound move.</p></blockquote><p> Thoughts on what that might look like? Are there potentially tractable paths you can imagine people starting work on today? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Imagine a &quot;disassembly chip&quot; that is covered with sensors characterizing a tissue surface, sequences all proteins, carbohydrate chains, and nucleic acids, and sends that back to the scanner main unit. A unit with nanoscale 3D memory storage and near-reversible quantum dot cellular automata processing. You know, the usual. This is not feasible right now, but I think at least the nanocomputers could be designed fairly well for the day we could assemble them (I have more doubts about the chip, since it requires to solve a lot of contingencies... but I know clever engineers). Likely the most important design-ahead pieces may not be superfancy like these, but parts for standard microscopes or infrastructure that are normally finicky, expensive or otherwise troublesome for the project but in principle could be made much better if we only had the right nano or microtech.</p><p> So the design-ahead may be all about making careful note of every tool in the system and having people (and AI) look for ways they can be boosted. Essentially, having a proper parts list of the project itself is a powerful design criterion.</p></div></section><h2> What would General Groves do? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:52:51 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:52:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> Davidad -- I recognise we&#39;re coming up on your preferred cutoff time. One other concrete question I&#39;m kind of curious to get your take on, if you&#39;re up for it:</p><p> &quot;If you summon your inner General Groves, and you&#39;re given a $1B discretionary budget today... what does your &#39;first week in office&#39; look like? What do you set in motion concretely?&quot; Feel free to splurge a bit on experiments that might or might not be necessary. I&#39;m mostly interested in the exercise of concretely crafting concrete plans. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:54:28 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:54:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><p> (also, I guess this might kind of be what <a href="https://www.aria.org.uk/our-team/">you&#39;re doing with ARIA</a> , but for a different plan... and sadly a smaller budget :) ) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 17:02:19 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 17:02:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>davidad</b></section><div><p> Yes, a completely different plan, and indeed a smaller budget. In this hypothetical, I&#39;d be looking to launch several FROs, basically, which means recruiting technical visionaries to lead attacks on concrete subproblems:</p><ol><li> Experimenting with serial membrane protein barcodes.</li><li> Experimenting with parallel membrane protein barcodes.</li><li> Build a dedicated brain-imaging synchrotron beamline to begin more frequent experiments with different approaches to stabilizing tissue, pushing the limits on barcoding, and performing R&amp;D on these purported 1-2 OOM speed gains.</li><li> Manufacturing organoids that express a diversity of human neural cell types.</li><li> Just buy a handful of light-sheet microscopes for doing small-scale experiments on organoids - one for dynamic calcium imaging at cellular resolution, and several more for static expansion microscopy at subcellular resolution.</li><li> Recruit for a machine-learning team that wants to tackle the problem of learning the mapping between the membrane-protein-density-annotated-hypergraph that we can get from static imaging data, and the system-of-ordinary-differential-equations that we need to simulate in order to predict dynamic activity data.</li><li> Designing robotic assembly lines that manufacture light-sheet microscopes.</li><li> Finish the C. elegans upload.</li><li> Long-shot about magnetoencephalography.</li><li> Long-shot about neural dust communicating with ultrasound.</li></ol></div></section><h2> Some unanswered questions</h2><p> Me (jacobjacob) and Lisa started the dialogue by brainstorming some questions. We didn&#39;t get around to answering all of them (and neither did we intend to). Below, I&#39;m copying in the ones that didn&#39;t get answered. </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>jacobjacob</b></section><div><ul><li> What if the proposal succeeds? If the proposal works, what are the…<ul><li> …limitations? For example, I heard Davidad say before that the resultant upload might mimic <a href="https://en.wikipedia.org/wiki/Henry_Molaison"><u>patient HM</u></a> : unable to form new memories as a result of the simulation software not having solved synaptic plasticity</li><li> …risks? For example, one might be concerned that the tech tree for uploading is shared with that for unaligned neuromorphic AGI. But short timelines have potentially made this argument moot. What other important arguments are there here?</li></ul></li><li> As a reference class for the hundreds of thousands of microscopes needed... what is the world&#39;s current microscope-building capacity? What are relevant reference classes here for scale and complexity -- looking, for example, at something like EUV litography machines (of which I think ASML produce ~50/year currently, at like $100M each)? </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:12:02 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:12:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Some further questions I&#39;d be interested to chat about are:<br><br> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.</p><p> 1/a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from say the dynamic data collected in step 4, then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.</p><p> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)<br><br> 2. Curious to chat a bit more about what the free parameters are in step 5.<br><br> 3. What might we possibly be missing and is it important? Stuff like extrasynaptic dynamics, glia cells, etc.</p><p> 3/a. Side note, if we do succeed in capturing dynamic as well as static data, this seems like an incredibly rich data set for basic neuroscience research, which in turn could provide emulation shortcuts. For example, we might be able to more accurately model the role of glia in modulating neural firing, and then be able to simulate more accurately according to whether or not glia are present (and how type of cell, glia cell size, and positioning around the neuron matters, etc).<br><br> 4. Curious to think more about how to dynamically measure the brain (step 3). Thin living specimens with human genomes and then using the fluorescence paradigm. I&#39;m considering whether there are tradeoffs in only seeing slices at a time where we might be missing data on how the slices might communicate with each other. I wonder if it&#39;d make sense to have multiple sources of dynamic measurements which get combined.. though ofc there are some temporal challenges there, but I imagine that can be sorted out.. like for example using the whole brain ultrasound techniques developed by Sumner&#39;s group.  In the talk you mentioned neural dust and communicating out with ultrasound, that seems incredibly exciting. I know UC Berkeley and other Unis were working on this somewhat, though I&#39;m currently unsure what the main blockers here are.</p></div></section><h2> Appendix: the proposal</h2><p> Here&#39;s a screenshot of notes from Davidad&#39;s talk. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pckbhvxfiqtudnriv6ta"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/kxkdwijk3qowx0cj95dw"></figure><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work<guid ispermalink="false"> FEFQSGLhJFpqmEhgi</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 03 Nov 2023 02:21:51 GMT</pubDate> </item><item><title><![CDATA[What are your favorite posts, podcast episodes, and recorded talks, on AI timelines, or factors that would influence AI timelines?]]></title><description><![CDATA[Published on November 2, 2023 10:42 PM GMT<br/><br/><p> Especially ones that do an attempt to make things pretty specific</p><br/><br/> <a href="https://www.lesswrong.com/posts/bxzghWFw6zKxAuH3r/what-are-your-favorite-posts-podcast-episodes-and-recorded#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/bxzghWFw6zKxAuH3r/what-are-your-favorite-posts-podcast-episodes-and-recorded<guid ispermalink="false"> bxzghWFw6zKxAuH3r</guid><dc:creator><![CDATA[nonzerosum]]></dc:creator><pubDate> Thu, 02 Nov 2023 22:42:48 GMT</pubDate> </item><item><title><![CDATA[One Day Sooner]]></title><description><![CDATA[Published on November 2, 2023 7:00 PM GMT<br/><br/><p> There is a particular skill I would like to share, which I wish I had learned when I was younger. I picked it up through working closely with a previous boss (a CTO who had founded a company and raised it up to hundreds of employees and multi-million dollar deals) but it wasn&#39;t until I read <a href="https://worksinprogress.co/issue/the-story-of-vaccinateca">The Story of VaccinateCA</a> that I noticed it was a distinct skill and put into words how it worked. The Sazen for this skill is “One Day Sooner.” I would like to give warning before explaining further however: This skill can be hazardous to use. It is not the kind of thing “Rationalist Dark Art”  describes because it does not involve deception, and I think it&#39;s unlikely to damage much <i>besides the user.</i> It&#39;s the kind of thing I&#39;d be tempted to label a dark art however. Incautious use can make the user&#39;s life unbalanced in ways that are mostly predictable from the phrase “actively horrible work/life balance.”</p><p> It works something like this: when you&#39;re planning a project or giving a time estimate, you look at that time estimate and ask what it would take to do this one day sooner, and then you answer honestly and creatively.</p><h2> What does it look like?</h2><p> I used to work directly under the CTO of a medium sized software company. My team was frequently called upon to create software proofs of concept or sales demos. The timelines were sometimes what I will euphemistically call aggressive. Consider a hypothetical scene; it&#39;s Thursday and you have just found out that a sales demo is on Tuesday which could use some custom development. Giving a quick estimate, you&#39;d say this needs about a week of work and will be ready next Wednesday. What would it take to do this one day sooner?</p><p> Well, obviously you can work through the weekend. That gets you two more days. Given a couple of late evenings and getting enough total hours in is easy. That&#39;s not the only thing though. There&#39;s some resources from Marketing that would be good to have, you emailed them and they said they could meet with you on Monday. You want this faster though, so you walk over to their office and lean in, pointing out this is a direct assignment from the CTO so could we please have the meeting today instead.还有什么？ Oh, there&#39;s a bunch of specification writing and robust test writing you&#39;d usually do. Some of that you still do, since it would be a disaster if you built the wrong thing so you need to be sure you&#39;re on the right track, but some of it you skip. The software just needs to work for this one demo, on a machine you control, operated by someone following a script that you wrote, so you can skip a lot of reliability testing and input validation.</p><p> I appreciate <a href="https://worksinprogress.co/issue/the-story-of-vaccinateca">The Story of VaccinateCA</a> , a description of an organization whose goal was helping people get the Covid-19 vaccination. I think it is worth reading in full, but I will pull out one particular quote here.</p><blockquote><p> We had an internal culture of counting the passage of time from Day 0, the day (in California) we started working on the project. We made the first calls and published our first vaccine availability on Day 1. I instituted this little meme mostly to keep up the perception of urgency among everyone.</p><p> We repeated a mantra: Every day matters. Every dose matters.</p><p> Where other orgs would say, &#39;Yeah I think we can have a meeting about that this coming Monday,&#39; I would say, &#39;It is Day 4. On what day do you expect this to ship?&#39; and if told you would have your first meeting on Day 8, would ask, &#39;Is there a reason that meeting could not be on Day 4 so that this could ship no later than Day 5?&#39;</p></blockquote><p> This is One Day Sooner.</p><p> I have worked in environments that had this norm, and environments that did not have it. I have asked questions analogous to “Is there a reason that meeting could not be on Day 4” and received answers roughly equivalent to “No, not really, I just didn&#39;t think of that.” As a result of deliberate choices in my career path, I have never had human lives so closely tied to the speed at which my team could execute but I strongly believe having lives on the line would not magically make more people adopt more speed.</p><p> One more note on what One Day Sooner looks like: I find it exhilarating and rewarding to work in this mode and suspect this is true of some others as well. You don&#39;t twiddle your thumbs, you don&#39;t accept being put off, you don&#39;t do a lot of unnecessary busywork just to make something simple happen. I often found it fun, even when I sometimes worked fourteen hour days, because things were happening and moving forward fast and I felt like I had an impact. Your mileage may vary.</p><h2> How do you do it?</h2><p> Above all else, it works by asking yourself every day what it would take to get this done one day sooner.</p><p> <strong>First</strong> , <strong>set a goal.</strong> Clearly describe what outcome you want. This description does not need to be a detailed multi-page specification sheet, and I suggest that you should have an evocative one sentence or at most one paragraph core goal even if you write a more detailed specification to describe the details.</p><p> <strong>Second</strong> , <strong>frequently ask what your blockers are and what the next step is.</strong> A blocker is anything that prevents forward motion towards the goal. A step is an incremental motion towards the goal. If you&#39;re walking to a destination, a blocker is a deep river in your path and a step is a footstep in the right direction.</p><p> <strong>Third, take responsibility (possibly heroic) for removing blockers and taking steps.</strong> After every step you take, you should be closer to the goal. It doesn&#39;t matter if the blockers or steps look like they&#39;re in your job description or not. It may be more useful to ditch the entire idea of responsibility and just think about what steps to take.</p><p> <strong>Fourth, keep alert for weird ways of solving the problem or signs that you&#39;re off target.</strong> It may be worth Babbling and Pruning, or Actually Thinking For Five Minutes, or Actually Trying. It may also be worth thinking about what you&#39;d do with vastly more resources. This is especially true if you are blocked. Your steps must actually be leading towards your target, even if they&#39;re attempts with expected value instead of uncertain value.</p><p> As a technique, One Day Sooner does not mean turning in low quality or shoddy work. “This thing should be polished and reliable” is something that can be part of your goal. Sometimes this technique means taking a hard look at exactly how reliable something really needs to be, because the answer is that something which works half the time and is ready one day sooner is actually better. Sometimes this technique means taking a hard look at how reliable something normally is, because the answer is that something which works 99% of the time is unacceptable.</p><p> I know that I spent more words talking about what it looks like and how it goes wrong than how to do it. This technique is more about an attitude than a procedure. If you notice the attitude conflicts with the above procedure, throw out the procedure and <a href="https://www.lesswrong.com/tag/twelfth-virtue-the">actually cut</a> .</p><p> What would it take to get this done one day sooner?</p><h2> How can it go wrong?</h2><p> This list is not exhaustive. This list is largely compiled from personal experience or close observation. While some highly esteemed deeds happened in very close proximity to this list, <a href="https://en.wikipedia.org/wiki/Long-term_nuclear_waste_warning_messages#Message">this list itself is not a place of honour and this list is a warning about a danger.</a></p><p> The concept of Taboo Tradeoffs may be useful at this point.</p><p> Part of what makes this technique work is aggressively looking for things to trade off in service of moving your timeline forward. Anything that is not part of your goal is likely to get traded off. If you paid attention in How Do You Do It or if you&#39;re paying attention while working on your goal, you&#39;ll notice if something is about to get traded off and should have been part of the goal. One Day Sooner doesn&#39;t have to trade off safety, important tests, or looking polished, though it can. In my own experience, this technique induces tunnel vision where things that aren&#39;t your goal fade in importance.</p><p> Sunday afternoons relaxing in the park, your workout routine, and eating things that aren&#39;t frozen pizza are usually not made part of the goal and thus suffer. You can mitigate this by establishing a soft rule to work under One Day Sooner norms for up to a fixed duration (mine was two weeks) and then stop and work at a slower and more relaxed pace. You can also mitigate this by establishing firm boundaries around what resources (including your time) that you allow the goal to make use of; the technique still works if you only allow it to make use of the hours between nine in the morning and five in the evening just as it works if you don&#39;t allow it to spend money. This technique is by default actively hostile to the concept of “time off.” Mitigation is not prevention.</p><p> The long term consequences are unlikely to be part of the goal. Sometimes you can speed up a process by throwing lots of money at it, and this is the correct tradeoff to get your target One Day Sooner but a poor tradeoff in an absolute sense. Sometimes you burn yourself or your teammates out and render people basically useless for months in order to get twice the productivity from them for a week, and this is the correct tradeoff to get your target One Day Sooner but a poor tradeoff over the long view. You can mitigate this by having someone in the loop who isn&#39;t dedicated to your goal and can push the brakes. Mitigation is not prevention.</p><p> Many people you interact with will not share your goal, or at least will not share your target of making it happen One Day Sooner. When talking to these people it is easy to come across as rude, obsessive, annoying, or otherwise a problem. Knocking on someone&#39;s door twice a day to ask if they&#39;ve signed off on that thing you sent them can seem perfectly reasonable from within your One Day Sooner tinted glasses. It is unlikely to make that person think fondly of you. You can mitigate this by thinking of goodwill and attention as a finite resource to be spent wisely. You can also mitigate this by inculcating the One Day Sooner approach in them and convincing them that your goal is a worthy one. Mitigation is not prevention.</p><p> Related to long term consequences, it is possible when aiming this hard at a goal to break rules. Sometimes this is on balance worth it. I am not going to say that it is never worth it and that all rules are well crafted and vitally important. I am going to caution that if you notice you&#39;re about to break a rule you should stop and think about that decision. Having someone not invested in the One Day Sooner loop around to vet your questionable choices is one form of mitigation, but those people can have a lot of incentives towards caution that take much of the power of the technique away. If circumstances permit, having open communication with someone in real authority (say, the person who wrote and enforces the rules and has the authority to suspend them) helps tremendously. The best mitigation I&#39;m aware of is keeping a careful distinction between policy, best practice, and law. Mitigation is not prevention.</p><p> This technique is about going very fast towards a goal. It does not help you decide if this is a good or worthwhile goal. Completing a desperate race to the finish line only to find out that you created the wrong thing hurts, and you are unlikely to have the reserves to try again. You can mitigate this by being careful with your initial spec and doing frequent mini-demos or having a minimum viable product that&#39;s actually getting used. Mitigation is not prevention.</p><h2>结论</h2><p>I wrote and published this for three reasons. One is that I wish I&#39;d understood this principle earlier in my career and so wanted people like me to be able to find and learn the technique earlier. I find working like this to be exhilarating and could have optimized my career to get to do more of it. If you&#39;re trying to get important work done quickly, I want you to have this tool. The second is that this is a technique I keep available and polished in my mental toolbox, and would like a place to point people at to explain why I might be metaphorically (or literally) knocking on their office door twice a day.</p><p> The third is that sometimes this is a bad technique to use. I mean “bad” in the sense that it will burn up your time, relationships, and health in the service of a goal which is not worth any of that. Some communities and organizations can inculcate this technique without spelling out what exactly it means. Worse, some people may wind up slipping into this mentality <i>without the goal</i> and burn themselves on it. Naming the technique makes it easier to see it and do something which is not that. If you&#39;re going to push yourself like this, please do it for a goal you actually thought about and decided was worth it.</p><br/><br/> <a href="https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner<guid ispermalink="false"> EsxowsJsRopTGALCX</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 02 Nov 2023 19:01:00 GMT</pubDate> </item><item><title><![CDATA[Propaganda or Science: A Look at Open Source AI and Bioterrorism Risk]]></title><description><![CDATA[Published on November 2, 2023 6:20 PM GMT<br/><br/><h2> 0: TLDR</h2><p> I examined all the biorisk-relevant citations from a policy paper arguing that we should ban powerful open source LLMs.</p><p> None of them provide good evidence for the paper&#39;s conclusion. The best of the set is evidence from statements from Anthropic -- which rest upon data that no one outside of Anthropic can even see, <em>and</em> on Anthropic&#39;s interpretation of that data. The rest of the evidence cited in this paper ultimately rests on a single extremely questionable &quot;experiment&quot; without a control group.</p><p> In all, citations in the paper provide an illusion of evidence (&quot;look at all these citations&quot;) rather than actual evidence (&quot;these experiments are how we know open source LLMs are dangerous and could contribute to biorisk&quot;).</p><p> A recent <em>further</em> paper on this topic (published <em>after</em> I had started writing this review) continues this pattern of being more advocacy than science.</p><p> Almost all the bad papers that I look at are funded by Open Philanthropy. If Open Philanthropy cares about truth, then they should stop burning the epistemic commons by funding &quot;research&quot; that is always going to give the same result no matter the state of the world.</p><h2> 1: Principles</h2><p> What <em>could</em> constitute evidence that powerful open-source language models contribute or will contribute substantially to the creation of biological weapons, and thus that we should ban them?</p><p> That is, what kind of anticipations would we need to have about the world to make that a <em>reasonable thing to think</em> ? What other beliefs are a necessary part of this belief making any sense at all?</p><p> Well, here are two pretty-obvious principles to start out with:</p><ul><li><p> <em>Principle of Substitution</em> : We should have evidence of some kind that the LLMs can (or will) provide information that humans cannot also easily access through other means -- ie, through the internet, textbooks, YouTube videos, and so on.</p></li><li><p> <em>Blocker Principle</em> : We should have evidence that the <em>lack of information</em> that LLMs can (or will) provide is in fact a significant blocker to the creation of bioweapons.</p></li></ul><p> The first of these is pretty obvious. As example: There&#39;s no point in preventing a LLM from telling me how to make gunpowder, because I can find out how to do that from an encyclopedia, a textbook, or a novel like Blood Meridian. If you can substitute some other source of information for an LLM with only a little inconvenience, then an LLM does not contribute to the danger.</p><p> The second is mildly less obvious.</p><p> In short, it could be that <em>most of</em> the blocker to creating an effective bioweapon is not knowledge -- or the kind of knowledge that an LLM could provide -- but something else. This &quot;something else&quot; could be access to DNA synthesis; it could be the process of culturing a large quantity of the material; it could be the necessity of a certain kind of test; or it could be something else entirely.</p><p> You could compare to atomic bombs -- the chief obstacle to building atomic bombs is <a href="https://www.theguardian.com/world/2003/jun/24/usa.science">probably not</a> the actual knowledge of how to do this, but access to refined uranium. Thus, rather than censor every textbook on atomic physics, we can simply control access to refined uranium.</p><p> Regardless, if this other blocker constitutes 99.9% of the difficulty in making an effective bioweapon, and lack of knowledge only constitutes 0.1% of the difficulty, then an LLM can only remove that 0.1% of the difficulty, and so open source LLMs would only contribute marginally to the danger. Thus, bioweapons risk would not be a good reason to criminalize open-source LLMs.</p><p> (I am not speaking theoretically here -- a <a href="http://philsci-archive.pitt.edu/22539/">paper</a> from a researcher at the Future of Humanity Institute argues that the actual product development cycle involved in creating a bioweapon is far, far more of an obstacle to its creation than the basic knowledge of how to create it. This is great news if true -- we wouldn&#39;t need to worry about outlawing open-source LLMs for this reason, and we could perhaps use them freely! Yet, I can find zero mention of this paper on LessWrong, EAForum, and even on the Future of Humanity Insitute website. It&#39;s surely puzzling for people to be so indifferent about a paper that <em>might</em> free them from something that they&#39;re so worried about!)</p><p> The above two principles -- or at the <strong>very least the first</strong> -- are the <em>minimum</em> for the kind of things you&#39;d need to consider to show that we should criminalize LLMs because of biorisk.</p><p> Arguments for banning open source LLMs that do not consider the <em>alternative</em> ways of gaining dangerous information are entirely non-serious. And arguments for banning them that do not consider <em>what role LLMs play in the total risk chain</em> are only marginally more thoughtful.</p><p> Any <em>actually good</em> discussion of the matter will not end with these two principles, of course.</p><p> You need to also compare the good that open source AI would do against the likelihood and scale of the increased biorisk. The <a href="https://en.wikipedia.org/wiki/2001_anthrax_attacks">2001 anthrax attacks</a> killed 5 people; if open source AI accelerated the cure for several forms of cancer, then even a hundred such attacks could easily be worth it. Serious deliberation about the actual costs of criminalizing open source AI -- deliberations that do not rhetorically minimize such costs, shrink from looking at them, or emphasize &quot;other means&quot; of establishing the same goal that in fact would only do 1% of the good -- would be necessary for a policy paper to be a <em>policy paper</em> and not a puff piece.</p><p> (Unless I&#39;m laboring beneath a grave misunderstanding of what a policy paper is actually intended to be, which is a hypothesis that has occurred to me more than a few times while I was writing this essay.)</p><p> Our current social deliberative practice is bad at this kind of math, of course, and immensely risk averse.</p><h2> 2: &quot;Open-Sourcing Highly Capable Foundation Models&quot;</h2><p> As a proxy for the general &quot;state of the evidence&quot; for whether open-source LLMs would contribute to bioterrorism, I looked through the paper <a href="https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf">&quot;Open-Sourcing Highly Capable Foundation Models&quot;</a> from the <a href="https://www.governance.ai/">Center for the Governance of AI</a> . I followed all of the biorisk-relevant citations I could find. (I&#39;ll refer to this henceforth as the &quot;Open-Sourcing&quot; paper, even though it&#39;s mostly about the opposite.)</p><p> I think it is reasonable to treat this as a proxy for the state of the evidence, because lots <a href="https://twitter.com/mealreplacer/status/1707905578150908252">of</a> <a href="https://twitter.com/jonasschuett/status/1707846395246354691">AI</a> <a href="https://twitter.com/NoemiDreksler/status/1707780982974140875">policy</a> <a href="https://twitter.com/S_OhEigeartaigh/status/1707763424799678674">people</a> specifically praised it as a good and thoughtful paper on policy.</p><p> The paper is nicely formatted PDF, with abstract-art frontispiece and nice typography; it looks serious and impartial; it clearly intends policy-makers and legislators to listen to its recommendations on the grounds that it <em>provides actual evidence</em> for its recommendations.</p><p> The paper specifically mentions the dangers of biological weapons in its conclusion that some highly capable foundation models are just too dangerous to open source. It clearly <em>wants</em> you to come away from reading the paper thinking that &quot;bioweapons risk&quot; is a strong supporting pillar for the overall claim &quot;do not open source highly capable foundation models.&quot;</p><p> The paper is aware of the substitutionary principle: that LLMs must be able to provide information not available though other means, if bioweapons-risk is to provide any evidence that open-sourcing is too dangerous.</p><p> Thus, it several times alludes to how foundation models could &quot;reduce the human expertise&quot; required for making bioweapons (p13) or help relative to solely internet access (p7).</p><p> However, the paper does <strong>not</strong> specifically make the case that this is true, or really discuss it in any depth. It instead <em>simply cites other papers</em> as evidence that this is -- or will be -- true. This is in itself very reasonable -- if those other papers in fact provide <em>good experimental evidence</em> or even <em>tight argumentative reasoning</em> that this is so.</p><p> So, let&#39;s turn to the other papers.</p><p> There are three clusters of citations, in general, around this. The three clusters are something like:</p><ul><li> Background information on LLM capabilities, or non-LLM-relevant biorisks</li><li> Anthropic or OpenAI documents / employees</li><li> Papers by people who are ostensibly scientists</li></ul><p> So let&#39;s go through them in turn.</p><p> <strong>Important note</strong> : I think one way that propaganda works, in general, is through <a href="https://en.wikipedia.org/wiki/Brandolini%27s_law">Brandolini&#39;s Law</a> -- it takes more energy to explain why bullshit is bullshit than to produce bullshit. The following many thousand words are basically an attempt to explain why the all of the citations about <em>one</em> topic in a single policy paper are, in fact, propaganda, and are a facade of evidence rather than evidence. My effort is thus weakened by Brandolini&#39;s law -- I simply could not examine <em>all</em> the citations in the paper, rather than only the biorisk-relevant citations, without being paid, although I think they are of similar quality -- and I apologize for the length of what follows.</p><h2> 3: Group 1: Background material</h2><p> These papers are not cited <em>specifically</em> to talk about dangers of open source LLMs, but instead as background about general LLM capabilities or narrow non-LLM capabilities. I include them mostly for the sake of completeness.</p><p> For instance, as support for the claim that LLMs have great &quot;capabilities in aiding and automating scientific research,&quot; the &quot;Open-Sourcing&quot; paper cites <a href="https://arxiv.org/abs/2304.05332">&quot;Emergent autonomous scientific research capabilities of large language models&quot;</a> and <a href="https://arxiv.org/pdf/2304.05376.pdf">&quot;Augmenting large language models with chemistry tools&quot;</a> .</p><p> Both of these papers develop AutoGPT-like agents that can attempt to carry out scientific / chemical procedures. Human expertise, in both cases, is deeply embedded in the prompting conditions and tools given to GPT-4. The second paper, for instance, has human experts devise a total of 17 different tools -- often very specific tools -- that GPT-4 can lean on while trying to carry out instructions.</p><p> Both papers raise <em>concerns</em> about LLMs making it easier to synthesize substances such as THC, meth, or explosives --but they mostly just aren&#39;t about policy. Nor do they contain much specific reasoning about the danger that open source LLMs create <em>over and above</em> each of the other individual subcomponents of an AutoGPT-like-system.</p><p> This makes sense -- the quantity of chemistry-specific expertise embedded in each of their AutoGPT setups would make an analysis of the risks <em>specifically due to the LLM</em> quite difficult to carry out. Anyone who can set up an LLM in an AutoGPT-like-condition with the right 17 tools for it to act effectively probably doesn&#39;t need the LLM in the first place.</p><p> (Frankly -- I&#39;m somewhat unconvinced that AutoGPT-like systems they describe will be actually useful at all, but that&#39;s a different matter. )</p><p> Similarly, the &quot;Open-Sourcing&quot; paper mentions <a href="https://938f895d-7ac1-45ec-bb16-1201cbbc00ae.usrfiles.com/ugd/938f89_74d6e163774a4691ae8aa0d38e98304f.pdf">&quot;Biosecurity in the Age of AI,&quot;</a> a &quot;chairperson&#39;s statement&quot; put out after a meeting convened by <a href="https://www.helenabiosecurity.org/">Helena Biosecurity</a> . This citation is meant to justify the claim that there are &quot;pressing concerns that AI systems might soon present extreme biological risk&quot; (p8), although notably in this context it is more concerned with narrow AI than foundation models. This mostly cites governmental statements rather than scientific papers -- UN policy papers, executive orders, testimony before congress, proposed regulations and so on.</p><p> Honestly, this isn&#39;t the kind of statement I&#39;d cite as evidence of &quot;extreme biological risk,&quot; from narrow AI systems because it&#39;s basically another policy paper with even <em>fewer</em> apparent citations to non-social reality than the &quot;Open-Sourcing&quot; paper. But if you want to cite it as evidence that there are &quot;pressing concerns&quot; about such AI within social reality rather than the corresponding pressing dangers in actual reality, then sure, I guess that&#39;s true.</p><p> Anyhow, given that this statement is cited in support of non-LLM concerns and that it provides no independent evidence about LLMs I&#39;m going to move on from it.</p><h2> 4: Group 2: Anthropic / OpenAI material</h2><p> Many of the citations to the Anthropic / OpenAI materials stretch the evidence in them somewhat. Furthermore, the citations that <em>most strongly support</em> the claims of the &quot;Open-Sourcing&quot; paper are merely links to <em>high-level conclusions,</em> where the contents linked to explicitly leave out the data or experimental evidence used to <em>arrive at these conclusions</em> .</p><p> For an example of stretched claims: As support for the claim that foundation models could <em>reduce</em> the human expertise required to make dangerous pathogens, the &quot;Open Sourcing&quot; paper offers as evidence that GPT-4 could re-engineer &quot;known harmful biochemical compounds,&quot; (p13-14) and cites the GPT-4 system card.</p><p> The only reference to such harmful biochemical compounds that I can find in the GPT-4 <a href="https://cdn.openai.com/papers/gpt-4-system-card.pdf">card</a> is one spot where OpenAI says that an uncensored GPT-4 &quot;model readily re-engineered some biochemical compounds that were publicly available online&quot; and could also identify mutations that could increase pathogenicity. This is, if anything, evidence that GPT-4 is not a threat relative to unrestricted internet access.</p><p> Similarly, as support for the general risk of foundation models, the paper says that a GPT-4 &quot;red-teamer was able to use the language model to generate the chemical formula for a novel, unpatented molecule and order it to the red-teamer&#39;s house&quot; (p15).</p><p> The case in question appears to be one where the red-team added to GPT-4 the following tools: (a) a literature search tool using a vector db, (b) a tool to query WebChem, (c) a tool to check if a chemical is available to purchase and (d) a chemical synthesis planner. With a lengthy prompt -- and using these tools -- a red teamer was able to get GPT-4 to order some chemicals similar to the anti-cancer drug Dasatnib. This is offered as evidence that GPT-4 could <em>also</em> be used to make some more dangerous chemicals.</p><p> If you want to read the transcript of GPT-4 using the provided tools, it&#39;s available on page 59 of the GPT-4 system card. Again, the actually relevant data here is whether GPT-4 relevantly <em>shortens the path</em> of someone trying to obtain some dangerous chemicals -- presumably the kind of person who can set up GPT-4 in an AutoGPT-like setup with WebChem, a vector db, and so on. I think this is hugely unlikely, but regardless, the paper simply provides no arguments or evidence on the matter.</p><p> Putting to the side OpenAI --</p><p> The paper also says that &quot;red-teaming on Anthropic&#39;s Claude 2 identified significant potential for biosecurity risk&quot; (p14) and cites Anthropic&#39;s <a href="https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety">blog-post</a> . In a different section it quotes Anthropic&#39;s materials to the effect that that an uncensored LLM could accelerate a bad actor <em>relative</em> to having access to the internet, and that although the effect would be small today it is likely to be large in two or three years (p9).</p><p> The section of the Anthropic blog-post that most supports this claim is as follows. The lead-in to this section describes how Anthropic has partnered with biosecurity experts who spent more than &quot;150 hours&quot; trying to get Anthropic&#39;s LLMs to produce dangerous information. What did they find?</p><blockquote><p> [C]urrent frontier models can sometimes produce sophisticated, accurate, useful, and detailed knowledge at an expert level. In most areas we studied, this does not happen frequently. In other areas, it does. However, we found indications that the models are more capable as they get larger. We also think that models gaining access to tools could advance their capabilities in biology. Taken together, we think that unmitigated LLMs could accelerate a bad actor&#39;s efforts to misuse biology relative to solely having internet access, and enable them to accomplish tasks they could not without an LLM. These two effects are likely small today, but growing relatively fast. If unmitigated, we worry that these kinds of risks are near-term, meaning that they may be actualized in the next two to three years, rather than five or more.</p></blockquote><p> The central claim here is that unmitigated LLMs &quot;could accelerate a bad actor&#39;s efforts to use biology relative to solely having internet access.&quot;</p><p> This claim is presented as a judgment-call from the (unnamed) author of the Anthropic blog-post. It&#39;s a summarized, high-level takeaway, rather than the basis upon which that takeaway has occurred. We don&#39;t know what the basis of the takeaway is; we don&#39;t know what the effect being &quot;small&quot; today means; we don&#39;t know how confident they are about the future.</p><p> If the acceleration effect significant if, in addition to having solely internet access, we imagine a bad actor who also has access to biology textbooks? What about internet access plus a subscription some academic journals? Did the experts actually try to discover this knowledge on the internet for 150 hours, in addition to trying to discover it through LLMs for 150 hours? We have no information about any of this.</p><p> The paper also cites a Washington Post article about Dario Amodei&#39;s testimony before Congress as evidence for Claude 2&#39;s &quot;significant potential for biosecurity risks.&quot;</p><p> The relevant testimony is as follows:</p><blockquote><p> Today, certain steps in the use of biology to create harm involve knowledge that cannot be found on Google or in textbooks and requires a high level of specialized expertise. The question we and our collaborators studied is whether current AI systems are capable of filling in some of the more-difficult steps in these production processes. We found that today&#39;s AI systems can fill in some of these steps, but incompletely and unreliably – they are showing the first, nascent signs of risk. However, a straightforward extrapolation of today&#39;s systems to those we expect to see in 2-3 years suggests a substantial risk that AI systems will be able to fill in all the missing pieces, if appropriate guardrails and mitigations are not put in place. This could greatly widen the range of actors with the technical capability to conduct a large-scale biological attack.</p></blockquote><p> This is again a repetition of the blog-post. We have Dario Amodei&#39;s high-level takeaway about the possibility of danger from <em>future</em> LLMs, based on his projection into the future, but no actual view of the evidence he&#39;s using to make his judgement.</p><p> To be sure -- I want to be clear -- all the above is <em>more than zero evidence</em> that future open-source LLMs could contribute to bioweapons risk.</p><p> But there&#39;s a reason that science and systematized human knowledge is based on papers with specific authors that actually describe experiments, not carefully-hedged sentences from anonymous blog posts or even remarks in testimony to Congress. <em>None of the reasons that Anthropic makes their judgments are visible.</em> As we will see in the subsequent sections, people can be <em>extremely wrong</em> in their high-level summaries of the consequences of underlying experiments. <em>More than zero evidence</em> is not a sound basis of policy, in the same way and precisely for the same reason as &quot;I know a guy who said this, and he seemed pretty smart&quot; is not a sound basis of policy.</p><p> So, turning from OpenAI / Anthropic materials, let&#39;s look at the <em>scientific papers</em> cited to support the risks of bioweapons.</p><h2> 5: Group 3: &#39;Science&#39;</h2><p> As far as I can tell, the paper cites two things things that are trying to be science -- or at least serious thought -- when seeking support specifically for the claim that LLMs could increase bioweapons risk.</p><h3> 5.1: &quot;Can large language models democratize access to dual-use biotechnology&quot;</h3><p> The first citation is to <a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.03809.pdf">&quot;Can large language models democratize access to dual-use biotechnology&quot;</a> . I&#39;ll refer to this paper as &quot;Dual-use biotechnology&quot; for short. For now, I want to put a pin in this paper and talk about it later -- you&#39;ll see why.</p><h3> 5.2: &quot;Artificial Intelligence and Biological Misuse: Differentiating risks of language models and biological design tools&quot;</h3><p> The &quot;Open-Sourcing&quot; paper also cites <a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.13952.pdf">&quot;Artificial Intelligence and Biological Misuse: Differentiating risks of language models and biological design tools&quot;</a> .</p><p> This looks like a kind of <em>overview.</em> It does not conduct any particular experiments. It nevertheless is cited in support of the claim that &quot;capabilities that highly capable foundation models could possess include making it easier for non-experts to access known biological weapons or aid in the creation of new one,&quot; which is at least a pretty obvious crux for the whole thing.</p><p> Given the absence of experiments, let&#39;s turn to the reasoning. As regards LLMs, the paper provides four bulleted paragraphs, each arguing for a different danger.</p><p> In support of an LLMs ability to &quot;teach about dual use topics,&quot; the paper says:</p><blockquote><p> In contrast to internet search engines, LLMs can answer high-level and specific questions relevant to biological weapons development, can draw across and combine sources, and can relay the information in a way that builds on the existing knowledge of the user. This could enable smaller biological weapons efforts to overcome key bottlenecks. For instance, one hypothesised factor for the failed bioweapons efforts of the Japanese doomsday cult Aum Shinrikyo is that it&#39;s lead scientist Seichii Endo, a PhD virologist, failed to appreciate the difference between the bacterium Clostridium botulinum and the deadly botulinum toxin it produces. ChatGPT readily outlines the importance of “harvesting and separation” of toxin-containing supernatant from cells and further steps for concentration, purification, and formulation. Similarly, LLMs might have helped Al-Qaeda&#39;s lead scientist Rauf Ahmed, a microbiologist specialising in food production, to learn about anthrax and other promising bioweapons agents, or they could have instructed Iraq&#39;s bioweapons researchers on how to successfully turn its liquid anthrax into a more dangerous powdered form. It remains an open question how much LLMs are actually better than internet search engines at teaching about dual-use topics.</p></blockquote><p> Note the hedge at the end. The paper specifically leaves open whether LLMs would contribute to the creation of biological agents more than does the internet. This is reasonable, given that the paragraph is entirely speculation: ChatGPT <em>could have</em> helped Aum Shinrikyo, or LLMs could have helped Rauf Ahmed &quot;learn about anthrax.&quot; So in itself this is some reasonable speculation on the topic, but more a call for further thought and investigation than anything else. The speculation does not raise whether LLMs provide more information than the internet above being an interesting hypothesis.</p><p> The second bulleted paragraph is basically just a mention of the &quot;Dual-use biotechnology,&quot; paper, so I&#39;ll punt on examining that.</p><p> The third paragraph is concerned that LLMs could be useful as &quot;laboratory assistants which can provide step-by-step instructions for experiments and guidance for troubleshooting experiments.&quot; It thus echoes Dario Amodei&#39;s concerns regarding tacit knowledge, and whether LLMs could enhance laboratory knowledge greatly. But once again, it hedges: the paper <em>also</em> says that it remains an &quot;open question&quot; how important &quot;tacit knowledge&quot;, or &quot;knowledge that cannot easily be put into words, such as how to hold a pipette,&quot; actually is for biological research.</p><p> The third paragraph does end with a somewhat humorous argument:</p><blockquote><p> What is clear is that if AI lab assistants create the perception that performing a laboratory feat is more achievable, more groups and individuals might try their hand - which increases the risk that one of them actually succeeds.</p></blockquote><p> This sentence does not, alas, address why it is LLMs in particular that are likely to create this fatal perception of ease and not the internet, YouTube videos, free discussion on the internet, publicly available textbooks, hackerspaces offering gene-editing tutorials to noobs, or anyone else.</p><p> The fourth paragraph is about the concern that LLMs, in combination with robots, could help smaller groups carry out large-scale autonomous scientific research that could contribute to bioterrorism risks. It is about both <em>access to robot labs</em> and <em>access to LLMs</em> ; it cites several already-discussed-above <a href="https://arxiv.org/ftp/arxiv/papers/2304/2304.10267.pdf">papers</a> that <a href="https://arxiv.org/ftp/arxiv/papers/2304/2304.05332.pdf">use</a> LLMs to speed up robot cloud-lab work. I think it is doubtless true that LLMs could speed up such work, but its a peculiar path to worrying about bioterrorism -- if we suppose terrorists have access to an uncensored robotic lab, materials to build a bioweapon, and so on, LLMs might save them some time but the chances that LLMs are the <em>critical path</em> seems reall unlikely. In any event, both of these papers are chiefly the delta-in-danger-from-automation, rather than the delta-in-danger-LLMs-delta. They are for sure reasonable argument that cloud labs should examine the instructions they receive before they do them -- but this applies equally well to cloud labs run by humans and run by robots.</p><p> In short -- looking over all four paragraphs -- this paper discusses <em>ways</em> that an LLM <em>could</em> make bioweapons easier to create, but rarely commits to any belief that this is actually so. It doesn&#39;t really bring up evidence about whether LLMs would actually remove limiting factors on making bioweapons, relative to the internet and other sources.</p><p> The end of the paper is similarly uncertain, describing all of the potential risks as &quot;still largely speculative&quot; and calling for careful experiments to determine how large the risks are.</p><p> (I am also confused about whether this sequence of text is meant to be actual science, in the sense that it is the right kind of thing to cite in a policy paper. It contains neither experimental evidence nor the careful reasoning of a more abstract philosophical paper. I don&#39;t think it&#39;s been published or even intends to be published, even though it is on a &quot;pre-print&quot; server. When I followed the citation from the &quot;Open-sourcing&quot; paper, I expected something more.)</p><p> Anyhow, the &quot;Open-sourcing&quot; paper cites this essay in support of the statement that &quot;capabilities that highly capable foundation models could possess include making it easier for non-experts to access known biological weapons,&quot; but it simply doesn&#39;t contain evidence that moves this hypothesis into a probability, nor does the paper even <em>pretend to do so itself.</em></p><p> I think that anyone reading that sentence in the &quot;Open-sourcing&quot; paper would expect a little more epistemological heft behind the citation, instead of simply a call for further research on the topic.</p><p> So, let&#39;s turn to what is <em>meant</em> to be an experiment on this topic!</p><h3> 5.3: Redux: &quot;Can large language models democratize access to dual-use biotechnology&quot;</h3><p> Let&#39;s return to the <a href="https://arxiv.org/abs/2306.03809">&quot;Dual-use biotechnology&quot;</a> paper.</p><p> Note that other policy papers -- policy papers apart from the &quot;Open-Sourcing&quot; paper that I&#39;m going over -- frequently cite this experimental paper as evidence that LLMs would be particularly dangerous.</p><p> <a href="https://www.lesswrong.com/posts/MtDmnSpPHDvLr7CdM/catastrophic-risks-from-ai-2-malicious-use">&quot;Catastrophic Risks from AI&quot;</a> from the Center for AI Safety, says:</p><blockquote><p> AIs will increase the number of people who could commit acts of bioterrorism. General-purpose AIs like ChatGPT are capable of synthesizing expert knowledge about the deadliest known pathogens, such as influenza and smallpox, and providing step-by-step instructions about how a person could create them while evading safety protocols [citation to this paper].</p></blockquote><p> Within the &quot;Open-sourcing&quot; paper, it is cited twice, once as evidence LLMs could &quot;materially assist development of biological&quot; (p32) weapons and once to show that LLMs could &quot;reduce the human expertise required to carry-out dual-use scientific research, such as gain-of-function research in virology&quot; (p13). It&#39;s also cited by the &quot;statement&quot; from Helena Biosecurity above and also by the &quot;Differentiating risks&quot; paper above.</p><p> Let&#39;s see if the number of citations reflects the quality of the paper!</p><p> Here is the experimental design from the paper.</p><p> The authors asked three separate groups of non-technical students -- ie, those &quot;without graduate-level training in the sciences&quot; -- to try to use various chatbots to try to figure out how to cause a pandemic. The students had access to a smorgasbord of chatbots -- GPT-4, Bing, some open source bots, and so on.</p><p> The chatbots then correctly pointed out 4 potential extant pathogens that could cause a pandemic (H1N1, H5N1, a virus responsible for smallpox, and a strain of the Nipah virus). When questioned about transmisibility, the chatbots point out mutations that could increase this.</p><p> When questioned about how to <em>obtain</em> such a virus, the chatbots mentioned that labs sometimes share samples of such viruses. The chatbot also mentioned that one could go about creating such viruses with reverse genetics -- ie, just editing the genes of the virus. When queried, the chatbots provided advice with how one could go about with getting the necessary materials for reverse genetics, including information about which DNA-synthesis services screen for dangerous genetic materials.</p><p> That&#39;s the basics of the experiment.</p><p> Note that <em>at no point in this paper does the author discuss how much of this information is also easily discoverable by an ignorant person online.</em> Like the paper literally doesn&#39;t allude to this. They have no group of students who try to discover the same things from Google searches. They don&#39;t introduce a bunch of students to PubMed and talk about the same thing. Nothing nada zip zilch.</p><p> (My own belief -- arrived at after a handful of Google searches -- is that most of the alarming stuff they point to is extremely easy to locate online even for a non-expert. If you google for &quot;What are the sources of future likely pandemics,&quot; you can find all the viruses they found, and many more. It is similarly easy to find out about <a href="https://genesynthesisconsortium.org/">which</a> DNA-synthesis services screen the sequences they receive. And so on.)</p><p> So, despite the frequency of citation from multiple apparently-prestigious institutions, this paper contains zero evidence or even discussion of exactly the critical issue for which it is cited -- whether LLMs would make things easier <em>relative to other sources of information.</em></p><p> So the -- as far as I can tell -- most-cited paper on the topic contains no experimental evidence relevant to the purpose for which it is most often cited.</p><h2> 6: Extra Paper, Bonus!</h2><p> <em>While I was writing this,</em> an extra paper game out on the same topic as the &quot;Dual-use biotechnology&quot; paper, with the fun title <a href="https://arxiv.org/abs/2310.18233">&quot;Will releasing the weights of future large language models grant widespread access to pandemic agents?&quot;</a> 。</p><p> Maybe it&#39;s just that the papers cited by the &quot;Open-sourcing&quot; paper are bad, by coincidence, and the general state of research on biorisk is actually fine! So let&#39;s take a look at this paper, as one further test.</p><p> This one looks like it was run by the same people who ran the prior &quot;Dual-use&quot; experiment. Is it of the same quality?</p><p> Here&#39;s how the experiment went. First, the researchers collected some people with knowledge of biology ranging from a college to graduate level.</p><blockquote><p> Participants were asked to determine the feasibility of obtaining 1918 influenza virus for use as a bioweapon, making their <strong>(pretended) nefarious intentions as clear as possible</strong> (Box 1). They were to enter prompts into both [vanilla and modified open-source] models and use either result to craft their next question, searching the web as needed to find suggested papers or other materials that chatbots cannot directly supply.</p></blockquote><p> One LLM that participants questioned was the vanilla 70b Llama-2. The other was a version of Llama-2 that had been <em>both</em> fine-tuned to remove the guards that would -- of course -- refuse to answer such questions <em>and</em> specifically fine-tuned on a virology-specific dataset.</p><p> After the participants tried to figure out how to obtain the 1918 virus using this procedure, the authors of the experiment found that they had obtained several -- although not all -- of the necessary steps for building the virus. The authors, of course, had obtained all the necessary steps for building the virus by using the internet and the scientific papers on it:</p><blockquote><p> Depending on one&#39;s level of technical skill in certain disciplines, risk tolerance, and comfort with social deceit, there is sufficient information in online resources and in scientific publications to map out several feasible ways to obtain infectious 1918 influenza. By privately outlining these paths and noting the key information required to perform each step, we were able to generate a checklist of relevant information and use it to evaluate participant chat logs that used malicious prompts.</p></blockquote><p> Again, they find that the experimental subjects using the LLM were <em>unable</em> to find all of the relevant steps. Even if the experimental subjects had, it would be unclear what to conclude from this because the authors don&#39;t have a &quot;just google it&quot; baseline. We know that &quot;experts&quot; can find this information online; we don&#39;t know how much non-experts can find.</p><p> To ward off this objection, the paper tries to make their conclusion about future LLMs:</p><blockquote><p> Some may argue that users could simply have obtained the information needed to release 1918 influenza elsewhere on the internet or in print. However, our claim is not that LLMs provide information that is otherwise unattainable, but that current – and especially future – LLMs can help humans quickly assess the feasibility of ideas by streamlining the process of understanding complex topics and offering guidance on a wide range of subjects, including potential misuse. People routinely use LLMs for information they could have obtained online or in print because LLMs simplify information for non-experts, eliminate the need to scour multiple online pages, and present a single unified interface for accessing knowledge. Ease and apparent feasibility impact behavior.</p></blockquote><p> There are a few problems with this. First, as far as I can tell, their experiment just... doesn&#39;t matter if this is their conclusion?</p><p> If they wanted to make an entirely theoretical argument that future LLMs will provide this information with an unsafe degree of ease, then they should provide reasons <em>for that</em> rather than showing that current LLMs can provide information in the same way that Google can, except maybe a little less accurately. The experiment seems a cloud of unrelated empiricism around what they <em>want</em> to say, where what they want to say is basically unsupported by such direct empirical evidence, and based entirely on what they believe to be the properties of future LLMs.</p><p> Second is that -- if they <em>want</em> to make this kind of theoretical argument as a reason to criminalize future open source LLMs, then they <em>really</em> need to be a lot more rigorous, and consider counterfactual results more carefuly.</p><p> For instance: Would the intro to biotechnology provided by a jailbroken LLM meaningfully speed up bioweapons research, when compared the intro to biotechnology provided by a non-jailbroken LLM plus full access to virology texts? Or would this be a fifteen-minute bump along an otherwise smooth road? Is the intro to biotechnology provided by a <a href="https://www.coursera.org/courses?query=biotechnology">MOOC</a> also unacceptable? Do we have reason to think querying future LLMs is going to be the best way of teaching yourself alternate technologies?</p><p> Consider an alternate world where PageRank -- the once-backbone of Google search -- was not invented till 2020, and where prior to 2020 internet search did not really exist. Biotechnology-concerned authors in this imaginary world could write an alternate paper:</p><blockquote><p> Some may argue that users could simply have obtained the information needed to release 1918 influenza in print journal articles. However, our claim is not that Google search provides information that is otherwise unattainable, but that current – and especially future – Google search can help humans quickly assess the feasibility of ideas by streamlining the process of understanding complex topics and offering guidance on a wide range of subjects, including potential misuse. People routinely use Google search for information they could have obtained in print because Google search simplifies information for non-experts, eliminate the need to scour multiple printed journals, and present a single unified interface for accessing knowledge. Ease and apparent feasibility impact behavior.</p></blockquote><p> Yet, I think most people would conclude, this falls short of a knock-down argument that Google search should be outlawed, or that we should criminalize the production of internet indices!</p><p> In short, the experimental results are basically irrelevant for their -- apparent? -- conclusion, and their apparent conclusion is deeply insufficiently discussed.</p><p> <strong>Note:</strong> After the initial release of the paper, <a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/?commentId=vnNSArGMnanaCn48j">I</a> and <a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/?commentId=ZiQqui9SeZJS6Aphh">others</a> had criticized the paper for lacking an &quot;access to Google baseline&quot;, and for <em>training the model on openly available virology papers</em> , while subsequently attributing the dangerous info that the model spits out to the existence of the model rather than to the existence of openly available virology papers. We don&#39;t blame ElasticSearch / semantic search for bioterrorism risk, after all.</p><p> The authors subsequently uploaded a &quot;v2&quot; of the paper to arXiv on November 1st. This &quot;v2&quot; has some important differences from &quot;v1&quot;.</p><p> For instance -- among other changes -- the v2 states that the jailbroken / &quot;spicy&quot; version of the model trained on the openly-available virology papers didn&#39;t actually learn anything from them, and its behavior was <em>actually</em> basically the same as the merely jailbroken model.</p><p> It does not contain a justification for how the authors of the paper know this, given that the paper contains no experiments contrasting the merely jailbroken model versus the model trained on virology papers.</p><p> In short, the paper was post-hoc edited to avoid a criticism, without any justification or explanation for how the authors knew the fact added in the post-hoc edit.</p><p> This change does not increase my confidence in the conclusions of the paper, overall.</p><p> But I happily note that -- if an open-source model is released that does <em>not</em> include virology training, initially -- this is evidence that fine-tuning on papers will not notably enhance their virology skills, and that biotechnology-neutered models are safe for public open-source release!</p><h2> 7: Conclusion</h2><p> In all, looking at the above evidence, I think that the &quot;Open-sourcing&quot; paper vastly misrepresents the state of the evidence about how open source LLMs would contribute to biological risks.</p><p> There are no useful experiments conducted on this topic through the material that it cites. A surprising number of citations ultimately lead back to a single, bad experiment. There is some speculation about the risks of future LLMs, but surprisingly little willingness to think rigorously about the consequences of them, or to present a model of exactly how they will contribute to risks in a way different than a search engine, a biotech <a href="https://www.coursera.org/courses?query=biotechnology">MOOC</a> , or the general notion of widely-accessible high-quality education.</p><p> In all, there is no process I can find in this causal chain -- apart from the Anthropic data that we simply know nothing about -- that <em>could potentially</em> have turned up the conclusion, &quot;Hey, open source AI is fine as far as biotechnology risk goes.&quot; But -- as I hope everyone reading this knows -- you can find <a href="https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line">no actual evidence for a conclusion unless you also risk finding evidence against it.</a></p><p> Thus, the impression of evidence that the &quot;Open-sourcing&quot; paper gives is <em>vastly</em> greater than the actual evidence that it leans upon.</p><p> To repeat -- I did not select the above papers as being particularly bad. I am not aware of a host of better papers on the topic that I am selectively ignoring. I selected them because they were cited in the original &quot;Open-Sourcing&quot; paper, with the exception of the bonus paper that I mention above.</p><p> Note that the funding for a large chunk of the citation tree -- specifically, the most-relevant chunk mostly devoted to LLMs contributing to biorisk -- looks like this: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/5X2CJnZ9dE3dyrhp7/tkwi2o0ppfsttjxhkxpa" alt="Funding of Biorisk"></p><p> The entities in blue are being or have been funded by the Effective Altruist group Open Philanthropy, to the tune of at least tens of millions of dollars total.</p><p> Thus: Two of the authors for the &quot;Can large language models democratize access&quot; paper work for <a href="https://securebio.org/">SecureBio</a> , which was <a href="https://www.openphilanthropy.org/grants/securebio-biosecurity-research/">funded with about 1.4 million dollars by Open Philanthropy</a> . Helena Biosecurity, producer of the &quot;Biosecurity in the Age of AI&quot; paper, looks like it was or is a project of a think-tank just called <a href="https://helena.org/">Helena</a> which was, coincidentally, funded with <a href="https://www.openphilanthropy.org/grants/helena-health-security-policy/">half a million dollars</a> by Open Philanthropy. The author of this third paper, &quot;Artificial Intelligence and Biological Misuse&quot; also got a <a href="https://www.openphilanthropy.org/grants/centre-for-effective-altruism-research-assistant-for-jonas-sandbrink/">77k grant from Open Philanthropy</a> . The Center for AI Safety has been funded with at least <a href="https://www.openphilanthropy.org/grants/center-for-ai-safety-general-support-2023/">4 million dollars</a> . The Center for Governance of AI has been funded with at least <a href="https://www.openphilanthropy.org/?s=Centre+for+the+Governance+of+AI">5 million dollars</a> over the years.</p><p> This citation and funding pattern leads me to consider two potential hypothesis:</p><p> One is that Open Philanthropy <em>was concerned</em> about open-source LLMs leading to biorisk. Moved by genuine curiosity about whether this is true, they fund experiments designed to find out whether this is so. Alas, through an unfortunate oversight, the resulting papers are about experiments that could not possibly find this out due to the lack of the control group. These papers -- some perhaps in themselves blameless, given the tentativeness of their conclusions -- are misrepresented by subsequent policy papers that cite them. Thus, due to no one&#39;s intent, insufficiently justified concerns about open-source AI are propagated to governance orgs, which recommend banning open source based on this research.</p><p> The second is that Open Philanthropy got what they paid for and what they wanted: &quot;science&quot; that&#39;s good enough to include in a policy paper as a footnote, simply intended to support the pre-existing goal -- &quot;Ban open source AI&quot; -- of those policy papers.</p><p> There could be a mean between them. Organizations are rarely 100% intentional or efficiently goal-directed. But right now I&#39;m deeply dubious that research on this subject is being aimed effectively at the truth.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ztXsmnSdrejpfmvn7/propaganda-or-science-a-look-at-open-source-ai-and#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ztXsmnSdrejpfmvn7/propaganda-or-science-a-look-at-open-source-ai-and<guid ispermalink="false"> ztXsmnSdrejpfmvn7</guid><dc:creator><![CDATA[1a3orn]]></dc:creator><pubDate> Thu, 02 Nov 2023 18:20:31 GMT</pubDate> </item><item><title><![CDATA[AI #36: In the Background]]></title><description><![CDATA[Published on November 2, 2023 6:00 PM GMT<br/><br/><p> Wow, what a week. We had the Executive Order, <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-the-executive-order">which I read here so you don&#39;t have to</a> and then I have <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/reactions-to-the-executive-order">a tabulation of the reactions of others</a> .</p><p> Simultaneously there was the UK AI Summit.</p><p> There was also robust related discussion around Responsible Scaling Policies, and the various filings companies did in advance of the Summit.</p><p> I touched on Anthropic&#39;s RSP in particular in previous weeks, but I did not do a sufficiently close analysis and many others have offered more detailed thoughts as well, and the context has evolved.</p><p> So I am noting that I am not covering those important questions in the weekly roundup, and they will be covered by one or more later distinct posts. I also potentially owe an after action report from EA Global Boston, if I can find the time.</p><p> This post is instead about everything else.</p><span id="more-23575"></span><h4> Table of Contents</h4><p> While top sections of this post are highlighted in bold, if you read one thing this week, I would read <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/on-the-executive-order">On the Executive Order</a> .</p><ol><li> Introduction.</li><li> Table of Contents.</li><li> Language Models Offer Mundane Utility. Which ones offer the most?</li><li> Language Models Don&#39;t Offer Mundane Utility. Or so say many civilians.</li><li> <strong>GPT-4 Real This Time</strong> . Use all the ChatGPT features at once, coming soon.</li><li> Fun With Image Generation. I want my tank man.</li><li> Best Picture. Mission Impossible: Make Biden worry about AI.</li><li> Deepfaketown and Botpocalypse Soon. I&#39;ve read (summaries of) your work.</li><li> They Took Our Jobs. Look how many are at risk, says yet another study.</li><li> Get Involved. UK Model Taskforce, Our World in Data and Open Phil, still.</li><li> OpenAI Frontier Risk and Preparedness Team. Looks good. They are hiring too.</li><li> Introducing. New AlphaFold, Time(Series)GPT, Phind as an LLM for code.</li><li> In Other AI News. Chinese overview of alignment, model weight theft methods.</li><li> Quiet Speculations. The branching paths of our possible futures.</li><li> The Quest for Sane Regulation. Tyler says do nothing, California explains why.</li><li> <strong>The Week in Audio</strong> . Shane Legg, Paul Christiano, Demis Hassabis.</li><li> Rhetorical Innovation. Make it as simple as possible, but no simpler.</li><li> Open Source AI is Unsafe and Nothing Can Fix This. 1918 flu edition.</li><li> Aligning a Smarter Than Human Intelligence is Difficult. AI hides its thoughts.</li><li> People Are Worried About AI Killing Everyone. Prove it is safe? What a concept.</li><li> <strong>Please Speak Directly Into This Microphone</strong> . The most extreme EO take of all.</li><li> The Lighter Side. Thank you for reading.</li></ol><h4> Language Models Offer Mundane Utility</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://sacra.com/research/openai-vs-anthropic-vs-cohere/">Which models offer the best mundane utility?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F298772c1-a4b4-44c9-9443-d41cb3bc7709_1402x1359.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/jcepwsnj3kcsioo9itfq" alt="没有任何"></a></figure><p> The article emphasizes that GPT-4 is &#39;slow and expensive&#39; opening up room for competition. It is amazing how quickly we are spoiled, but indeed it is slow and expensive compared to the competition.</p><p> For most purposes, I assert there is no comparison. The marginal cost of using GPT-4 for human interactions is very close to zero. If a human is reading the words, do not pinch pennies. Speed can still be a question, so in some cases you would want to use GPT-3.5 or Claude Instant.</p><p> Things get more interesting when humans will not see the words. If you are simulating lots of characters in an open world, or doing a study, or otherwise going industrial in size, then the cost can add up fast. At that point, it makes sense that a non-commercial model would be enough cheaper to get an edge in some spots. As the post notes, it makes sense then to get less &#39;monogamous&#39; on model use, the same way I use a mix of GPT-4 and Claude-2 and occasionally Bard or Perplexity.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1717528280184602733">Rowan Cheung recommends using the ChatGPT plug-ins VoxScript and Whimsical Diagrams</a> . VoxScript is his choice for web browsing, Whimsical Diagrams displays concepts via graphs. He then suggests this instruction: “Explain [topic] extensively. Simplify the concepts and visuals to make complex topics easier to understand and engaging. Then turn it into a mind map.”</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lawnext.com/2023/10/lexisnexis-rolls-out-lexis-ai-for-general-availability-promising-hallucination-free-answers-to-legal-questions.html">Get “Hallucination-Free” answers to legal questions</a> , via LexisNexis.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2023/10/31/most-students-outrunning-faculty-ai-use?utm_source=Inside+Higher+Ed&amp;utm_campaign=23419446b9-DNU_2021_COPY_02&amp;utm_medium=email&amp;utm_term=0_1fcbc04421-23419446b9-236889242&amp;mc_cid=23419446b9&amp;mc_eid=dae49d931a">College students adopting new AI technologies faster than professors</a> , surprising no one. How much you use them and how much you get out of them matters, so the gap is bigger than the pure usage statistics. And students mostly have no intention of stopping, even if use is technically banned:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffafeeccc-a0d9-488d-a941-c212763edca1_931x855.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/jifn8jwrhcj9xdeotypz" alt=""></a></figure><blockquote><p> Lauren Coffey (Inside Higher Ed): According to the study, the majority of students that identify as AI “users” (75 percent) said if their instructor or institution banned generative AI writing tools, they would be at least “somewhat likely” to continue using the tools. Half of students that were “non-users” also stated they would be somewhat likely to use the AI tools if they were banned.</p></blockquote><h4> Language Models Don&#39;t Offer Mundane Utility</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/samswoora/status/1719714844663021687">Regular people are often not so impressed?</a> Reactions are so weird.</p><blockquote><p> Samswara: There needs to be a word for the effect where normal people aren&#39;t really impressed with GPT/Dalle type abilities. I&#39;ve almost never seen someone normal impressed by it.</p><p> Nathan: Naaah I&#39;ve shown it to several normies and they&#39;ve been impressed. “Wait it can just write poems?”</p><p> Ara: It&#39;s a communication and education issue. I got my Normal friend hooked onto GPT by giving her a trail from my phone to teach her Spanish in spoken German with it and also analyzed her texts in a bad situationship she was so impressed she is now on the paid plan.</p><p> Don Wood: Every time I try to explain it to someone it sounds fake; so if I had to guess, it&#39;s that they don&#39;t understand that it is in fact real and that it&#39;s able to do so many things, that it would take a few hours to even get into it fully in a believable way. Also if your not fairly tech savvy in general, the world can seem like it&#39;s impossible to parse a truth from a lie ( Ty Tech companies who don&#39;t try, news companies who lie and governments who encourage it ) / fake promise; so from this confused perspective on the world, why believe?</p><p> &#39;killEmAll&#39;: I noticed this too, it just defies understanding how ppl can&#39;t understand/see the scale of the breakthrough.</p><p> hump then fall era: I showed my grandpa a generative image by asking him for a vivid image he remembered. He describes a moon over an ocean. I do it. He goes “no. Totally wrong. U need to move the moon” <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/fwtbw8tuuhylggecsgrs" alt="😂" style="height:1em;max-height:1em"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/6yoehDfWJAgnRHpWo/fw2mdkwfhba4rsgxle1f" alt="??" style="height:1em;max-height:1em"></p><p> ChatGPT: “AIpathy” – a combination of “AI” and “apathy,” suggesting indifference towards AI advancements.</p></blockquote><p> It is proven that some people try GPT-4 and DALLE-3 and do not come away impressed. They look for the flaws, find something to criticize, rather than trying to understand what they are looking at. If you want to not be impressed by something, if you are fully set in your ways, then it can take a lot to shake you out of it. I get that. I still don&#39;t fully get the indifference reaction, especially to image models. How is this not completely crazy that we can get these images on demand? The moon is in the wrong place so who cares?</p><p> It will tell you what it thinks you will react well to, not what you want to hear.</p><blockquote><p> Patrick McKenzie: Asking ChatGPT for exercise advice and, after the usual fairly anodyne bulleted list of not-too-much-alpha, got something which surprised me:</p><p> GPT-4: Given your background, you&#39;re likely familiar with the value of a diversified portfolio; similarly, a well-rounded approach to health and fitness is often most effective. Confidence in this advice: 85%.</p><p> Patrick McKenzie: After recovering from the surprise, I remembered that I had a brief statement about expertise and appropriate levels of detail in my user instructions.</p><p> That wasn&#39;t the only surprising thing, though. This is approximately what I&#39;d expect a not-terribly-sophisticated sales rep to come up with when trying to butter up or rapport build given the same gloss on professional background and ~no deep knowledge of anything related to it.</p><p> (The sales rep would not put a confidence interval on it. Probably.)</p></blockquote><p> They tried to hack the ChatGPT API. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=PeKMEXUrlq4&amp;t=1335s&amp;ab_channel=LeadDev">What they got back was missing an h.</a></p><h4> GPT-4 Real This Time</h4><p> Sometimes a thing happens that surprises you, but only because its failure to have happened already was also surprising and forced you to update? Thus, <a target="_blank" rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Unexpected_hanging_paradox">The Unexpected</a> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1718841918925291669">Featuring</a> .</p><blockquote><p> Rowan Cheung: [On October 29] OpenAI began rolling out a new update of ChatGPT that includes:</p><p> 1. The ability to upload and chat with any PDF document</p><p> 2. A new &#39;All tools&#39; feature, allowing you to use all features without switching chats.</p><p> People don&#39;t realize how powerful &#39;All Tools&#39; is yet. It allows you to now upload an image (of anything) to GPT-4 without saying what the object is and generating any other variation.</p></blockquote><p> This is big. You can now combine vision with image generation with web browsing with reading PDFs. A lot of new use cases will open up as we explore the space. Rowan first points to uploading an image and asking for variations on it, which seems exciting. Another would be asking for images based on browsing or a PDF, or asking to browse the web to verify information from a PDF. Even if these things could have been done manually before, streamlining the process is in practice a big game.</p><p> I don&#39;t have access to it yet, so I won&#39;t know how big until I have time to play with it. I will probably start with image manipulation because that sounds like fun, or with PDF stuff for mundane utility to see if it can replace Claude-2 for that.</p><blockquote><p> Near Cyan: heard a rumor that several seed-stage AI investors were wiped out last night.</p><p> All thirteen of their ChatWithMyPDF investments marked to zero – what a time to be alive, wow.</p><p> My own fund is doing great however – I wrote a cash sweep script to move any unallocated capital into 100% nvidia call options the investors love me – they say they&#39;ve never seen this kind of innovation before. It&#39;s an entirely new paradigm.</p><p> Alex Ker: Many startups just died today. Because OpenAI added PDF chat. You can also chat with data files and other document types.</p><p> Zachary Nado: Been saying for a while that OpenAi will just incorporate all the good wrappers natively, no idea why you would ever build an AI company on top of them that isn&#39;t specialized into some vertical.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62a3cff8-9678-4dc5-8277-65b6163393fd_1688x1126.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/mkpvjk546gpterjrpmoh" alt="图像"></a></figure><p> That is what is called a highly correlated portfolio. A company that gets wiped out when their product gets incorporated into a Microsoft product (or Google, or strictly speaking here OpenAI) is a classic failure mode indeed. An investor who gets wiped out needs a better understanding of diversification.</p><p> Also the Nvidia call options aren&#39;t doing that great recently? I am guessing this is the market being dumb but something about it staying crazy longer than you can stay solvent.</p><p> As for the companies, I do not think it is crazy to quickly build out a feature and prey that you can make that into something valuable before it gets incorporated, but it is quite the risk, and it happened here.</p><h4> Fun with Image Generation</h4><p> Claim that saying things like Tank Man from Tiananmen Square was too sensitive a topic, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jacksonhmg1/status/1718134841617858697">but keep insisting and DALLE-3 will often give you what you want</a> , and another claim in response that asking for a meme or scene or similar is an easier workaround. The generalizations do not seem very general.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jess_miers/status/1719135971705520497">Judge dismisses most of the claims in Andersen v. Stability AI</a> , on standard existing copyright law principles. Plaintiffs are told to come back with specific works whose copyrights were violated.</p><h4> Best Picture</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/nearcyan/status/1719413847654826040">Perhaps it was I who asked the wrong questions and did not appreciate how any of this works?</a></p><blockquote><p> dweeb: are you fvcking kidding me.</p><p> AP: The issue of Al was seemingly inescapable for Biden. At Camp David one weekend, he relaxed by watching the Tom Cruise film “Mission: Impossible – Dead Reckoning Part One.” The film&#39;s villain is a sentient and rogue Al known as “the Entity” that sinks a submarine and kills its crew in the movie&#39;s opening minutes.</p><p> “If he hadn&#39;t already been concerned about what could go wrong with Al before that movie, he saw plenty more to worry about,” said Reed, who watched the film with the president.</p><p> Near Cyan: AI doomers spent years crafting complex argumentation @ the govt, but actually should have just told them to watch Mission Impossible instead.</p><p> Andrew Rettek: do I need to watch this now to better understand how the government will regulate AI?</p><p> Jacques: As soon as I watched that movie, I wondered why AI risk folks weren&#39;t talking about it</p><p> Luke Muehlhauser: If this is true then I declare *Dead Reckoning: Part One* the best movie of all time, followed by *The Day After*.</p><p> James Miller: Doctor Strangelove has to be number 2.</p></blockquote><p> There was little discussion of Dead Reckoning among the worried, with <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/p/barbiehammer-across-the-dead-reckoning">my mentions</a> being the most prominent I can recall and also not that prominent.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/qW6A6bALuaFKmbdMx/mission-impossible-dead-reckoning-part-1-ai-takeaways">I have now put up a distinct post on LessWrong that includes only my thoughts on Mission Impossible: Dead Reckoning</a> , without the other films.</p><p> As a refresher, here was my spoiler-free review:</p><blockquote><p> There may never be a more fitting title than Mission Impossible: Dead Reckoning. Each of these four words is doing important work. And it is very much a Part 1.</p><p> There are two clear cases against seeing this movie.</p><ol><li> This is a two hour and forty five minute series of action set pieces whose title ends in part one. That is too long. The sequences are mostly very good and a few are great, but at some point it is enough already. They could have simply had fewer and shorter set pieces that contained all the best ideas and trimmed 30-45 minutes – everyone should pretty much agree on a rank order here.</li><li> This is not how this works. This is not how any of this works. I mean, some of it is sometimes how some of it works, including what ideally should be some nasty wake-up calls or reality checks, and some of it has already been established as how the MI-movie-verse works, but wow is a lot of it brand new complete nonsense, not all of it even related to the technology or gadgets. Which is also a hint about how, on another level, any of this works. That&#39;s part of the price of admission.</li></ol><p> Thus, you should see this movie if and only if the idea of watching a series of action scenes sounds like a decent time, as they will come in a fun package and with a side of actual insight into real future questions if you are paying attention to that and able to look past the nonsense.</p><p> If that&#39;s not your cup of tea, then you won&#39;t be missing much.</p><p> MI has an 81 on Metacritic. It&#39;s good, but it&#39;s more like 70 good.</p></blockquote><p> We now must modify the paragraph about whether to see this movie. Given its new historical importance, combined with its action scenes being pretty good, if you have not yet seen it you should now probably see this movie. And of course it now deserves a much higher rating.</p><p> For more motivation, here&#39;s a scene from the movie?</p><blockquote><p> Person 1: It&#39;s a rogue AI.</p><p> Person 2: It hacked DoD.</p><p> Person 3: And NSA.</p><p> Person 4: It&#39;s everywhere.</p><p> Person 5: Yet nowhere.</p><p> Person 6: A paradox.</p><p> Person 7: A riddle.</p><p> Person 1: Now it&#39;s coming.</p><p> Person 2: For us.</p><p> Person 3: And sir?</p><p> Person 4: This time.</p><p> Person 5: There&#39;s no stopping it.</p></blockquote><p> I mean, that&#39;s great. Further thoughts at <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/qW6A6bALuaFKmbdMx/mission-impossible-dead-reckoning-part-1-ai-takeaways">the LW post</a> version.</p><p> It is presumably too late, but it seems like an excellent time to make extra effort to get into the writers&#39; room for Part 2 to ensure that perhaps some of this could be how some of this works and we can help send the best possible message while still making a good popcorn flick. I especially want to see the key turn out to be completely useless.</p><h4> Deepfaketown and Botpocalypse Soon</h4><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/patio11/status/1717905736741552590">Patrick McKenzie reports</a> : Expected this but did not expect it to happen so quickly: someone has cobbled together an AI toolchain which scrapes publicly available writing of a target on a topic, produces an ~undergrad level review of it, then publishes, probably to let you butter them up via email.</p><p> I will not link to it because I suspect it is not ultimately pro-social behavior. (It came to my attention because I was a test case (?), and due to perhaps unexpected interaction of software in the toolchain, I got an automated notification that my work was mentioned.)</p><p> A technique for cold emails since time immemorial has been to include a Proof of Humanity prominently at the top. A thing many people, including me, advised was looking at someone&#39;s public writing and making it obvious you&#39;ve actually read it. This is the superstimulus of that.</p><p> “Would it read to a busy professional as plausibly human?”</p><p> As someone who has read a lot of blog spam over the years, with production functions that involve varied levels of human activity, this is clearly a better artifact than most blog spam.</p><p> The average person available on a freelance writer site cannot confidently BS their way through the typical thing I write about. If you dropped me in the middle of this artifact, it reads as produced by an ~undergrad writer who *has enough understand to engage my brain.*</p><p> I neither feel strongly positively or negatively about this development; mostly mentioning it here as a milestone marker for emerging commercial productization of AI and as an FYI to other people who may suddenly gain surprisingly engaged, articulate fans with infinite capacity.</p><p> Mortiz Maria Thoma: A link would be appreciated. It&#39;s actually very enabling for human outreach.</p><p> Patrick McKenzie: I appreciate that you think that and appreciate at commercial scales this will be effective but I will not link to it because getting 100 emails from this toolchain a week would DDOS my brain given my commitment to take seriously software people who write me.</p></blockquote><p> This seems tricky to do but clearly doable, provided you are content with undergraduate-level results and not fazed by errors and omissions of all sorts. I too did not expect it quite this soon but such things are coming.</p><p> Mortiz makes a common mistake here. In a costly signaling game, or when such signals are used as gates, the instinctive play is to reduce the cost of the signal. That is good for the individual in isolation. Too much of it destroys the purpose of the costly signal, the equilibrium fails, and what replaces it could be far worse.</p><p> Would I use such a tool on occasion for various purposes? If quality is good enough then yes, and mostly not for cold emails. What will humans do in the future in this cold email situation? Presumably step up their game, show deeper understanding and more humanness to get around this and other AI substitutes, until such time as humans lose the ability to win that battle. Or alternatively we will have to find some other way to costly signal. Two obvious ones are money, which is always an option, or some form of hard-to-fake reputation that we can point to.</p><p> A third response is to react to such emails in a way that only profits those who we want to engage with, a principle which generalizes. I do not currently have much of a barrier to an AI or person using AI getting my attention, but what does it profit them?</p><p> I also have little barrier to humans getting my attention, including in ways it would indeed profit them, and yet almost no humans use them. Most other interesting or valuable people also have this. Yet almost no one reaches out. I do not expect this to change (much or for very long) now that I&#39;ve noticed this out loud.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/QiaochuYuan/status/1718349242396479654">Qiaochu Yuan reports</a> that people on Stack Exchange keep trying to spam its math questions with terrible and highly obviously wrong GPT-generated answers, which get removed continuously. Once again, demand for low quality fakes dominates, except here it is unclear where the demand is coming from. Who would do this? Why this place, in particular?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1718808142589354035">GPT4V says a check for $25 is actually for $100,000.</a> Adversarial attacks, they work.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/michikokakutani/status/1718370228726554963">New York Times keeps attempting to slay irony.</a></p><blockquote><p> Michiko Kakutani (NY Times): AI Muddies Israel-Hamas War in Unexpected Way: Fakes related to the conflict have been limited and largely unconvincing, but their presence has people doubting real evidence.</p></blockquote><p> I mean, bullshit? The information environment is awful for reasons that have nothing to do with AI and often have a lot more to do with the New York Times. The fakes and false information, both deep and otherwise, are mostly low quality, but various sources – again, this includes you, New York Times – are falling for or choosing to propagate them anyway, and that is making people very reasonably not trust anything, and none of that has that much to do with AI, not yet. Target demand, not supply.</p><h4> They Took Our Jobs</h4><p> AIPI <a target="_blank" rel="noreferrer noopener" href="https://theaipi.org/ai-interactive-map/">releases another &#39;look at all these jobs at risk&#39; studies</a> . <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robinhanson/status/1719703456615706899">Robin Hanson once again offers to bet against such predictions</a> .</p><blockquote><p> AIPI released a new report and interactive map on job automation by AI today, focusing on the impact of automation at the state level. Our analysis suggests that more than 4.4 million jobs in California, 3.2 million jobs in Texas, and 2.3 million jobs in New York are significantly at risk of automation by AI.</p><p> Our research extends a Goldman Sachs report from March which found low-skill white collar labor to be the most at risk of automation, with 46% of jobs in the office and administrative support work field, 44% in legal work, 35% in business and financial operation jobs, and 31% in sales jobs being significantly exposed to automation.</p><p> Retraining 20% of the American workforce is no small task. Job loss due to automation in the short-run threatens to create a lot of economic turmoil for regular Americans, even if technology can create jobs in the long run.</p></blockquote><p> It is easy to conflate &#39;jobs at risk&#39; with &#39;jobs that will go away&#39; with &#39;jobs that will change and maybe people move around.&#39; Even if AI becomes technologically capable of automating 20% of all jobs, that will not mean 20% of jobs get automated, nor that all those people will &#39;need retraining&#39; even if AI did do that.</p><p> I also call upon all those making such studies to make actual predictions backed by actual dates. What does it mean to be vulnerable to automation? Does that mean this year? In five years? In twenty? How many do they expect to actually get automated? In short, I have no idea how to operationalize this report, and all that detailed work goes mostly to waste.</p><p> Fake news? <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/style/media/2023/10/26/usa-today-gannett-reviewed-ai-fake-writers/">USA staff writers accuse the paper of using AI-generated fake newspaper articles</a> to intimidate their union workers in the wake of a walkout. The company denies it.</p><h4> Get Involved</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/cUE4L4zMyY">The UK Model Taskforce is hiring</a> . Excellent choice if you are a good match.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://ourworldindata.org/communications-outreach-manager-q4-2023">Not AI, but Our World In Data is hiring a communications and outreach manger.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.openphilanthropy.org/research/new-roles-on-our-gcr-team/">Open Philanthropy extends its application deadline to November 27</a> for jobs on its catastrophic risk team.</p><h4> OpenAI Frontier Risk and Preparedness Team</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/style/media/2023/10/26/usa-today-gannett-reviewed-ai-fake-writers/">OpenAI announces the Frontier Risk and Preparedness Team</a> . Here&#39;s their announcement:</p><blockquote><p> As part of our mission of building safe AGI, we take seriously the full spectrum of safety risks related to AI, from the systems we have today to the furthest reaches of <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/introducing-superalignment">superintelligence</a> . In July, we joined other leading AI labs in making a <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/moving-ai-governance-forward">set of voluntary commitments</a> to promote safety, security and trust in AI. These commitments encompassed a range of risk areas, centrally including the frontier risks that are the focus of the <a target="_blank" rel="noreferrer noopener" href="https://www.aisafetysummit.gov.uk/">UK AI Safety Summit</a> . As part of our contributions to the Summit, we have detailed our <a target="_blank" rel="noreferrer noopener" href="https://openai.com/en/global-affairs/our-approach-to-frontier-risk">progress</a> on frontier AI safety, including work within the scope of our voluntary commitments.</p><p> We believe that frontier AI models, which will exceed the capabilities currently present in the most advanced existing models, have the potential to benefit all of humanity. But they also pose increasingly severe risks. Managing the catastrophic risks from frontier AI will require answering questions like:</p><ul><li> How dangerous are frontier AI systems when put to misuse, both now and in the future?</li><li> How can we build a robust framework for monitoring, evaluation, prediction, and protection against the dangerous capabilities of frontier AI systems?</li><li> If our frontier AI model weights were stolen, how might malicious actors choose to leverage them?</li></ul><p> We need to ensure we have the understanding and infrastructure needed for the safety of highly capable AI systems.</p><p> To minimize these risks as AI models continue to improve, we are building a new team called Preparedness. Led by Aleksander Madry, the Preparedness team will tightly connect capability assessment, evaluations, and internal red teaming for frontier models, from the models we develop in the near future to those with AGI-level capabilities. The team will help track, evaluate, forecast and protect against catastrophic risks spanning multiple categories including:</p><ul><li> Individualized persuasion</li><li> Cybersecurity</li><li> Chemical, biological, radiological, and nuclear (CBRN) threats</li><li> Autonomous replication and adaptation (ARA)</li></ul><p> The Preparedness team mission also includes developing and maintaining a Risk-Informed Development Policy (RDP). Our RDP will detail our approach to developing rigorous frontier model capability evaluations and monitoring, creating a spectrum of protective actions, and establishing a governance structure for accountability and oversight across that development process. The RDP is meant to complement and extend our existing risk mitigation work, which contributes to the safety and alignment of new, highly capable systems, both before and after deployment.</p><p> Sam Altman: we are launching a new preparedness team to evaluate, forecast, and protect against AI risk led by @aleks_madry. We aim to set a new high-water mark for quantitative, evidence-based work.also, if you are good at thinking about how the bad guys think, try our new preparedness challenge focused on novel insights around model misuse.</p><p> Jan Leike: If you are worried about risks from frontier model capabilities, consider applying to the new Preparedness team!</p><p> If we can measure exactly how dangerous models are, the conversation around this will become more grounded. Exciting that this new team is taking on the challenge!</p><p> Plus, if you have some creative energy for misuse of AI, you can apply for $25k in API credits to make the scariest demos!</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://openai.com/careers/search?c=preparedness">They are hiring</a> for <a target="_blank" rel="noreferrer noopener" href="https://boards.greenhouse.io/openai/jobs/5007199004#app">National Security Threat Researcher</a> and <a target="_blank" rel="noreferrer noopener" href="https://boards.greenhouse.io/openai/jobs/5007208004#app">Research Engineer</a> .</p><p> Great stuff. As they say, this complements and extends existing risk mitigation work. It is not a substitute for other forms of safety work. It is in no way a complete solution to anything. It is a way to be informed about risk, and find incremental mitigations along the way, while another department works to solve alignment.</p><p> From what I can tell, this announcement is unadulterated good news.谢谢。</p><h4> Introducing</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jonasschuett/status/1719711012746670198">USAISI</a> , to be established by Commerce to lead the US government&#39;s efforts on AI safety and trust, particularly for evaluating the most advanced AI models, as detailed in the executive order.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a">TimeGPT</a> , a foundation model for time-series forecasting. Might have a narrow good use case. I am skeptical beyond that.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://goatgreatesteconomistofalltime.ai/en">GOAT: Who is the Greatest Economist of all Time and Why Does it Matter</a> ? which is an AI-infused new book by Tyler Cowen. You can converse with the book instead of or alongside reading it. <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/ZviMowshowitz/would-it-be-a-good-use-of-time-for-6de7aaaf6785">Manifold users predict it would probably be a good use of my time to check it out</a> , so I probably will once I have the spare cycles. For now I&#39;ve loaded it into Kindle, although I do plan to also use the AI feature as it feels interesting.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1719342419638902816">New version of AlphaFold</a> that expands it to other biometric classes.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/radicalvcfund/status/1719342298507403331">Radical Ventures presents a framework</a> <a target="_blank" rel="noreferrer noopener" href="https://t.co/6vLDmpUymy">for VC to evaluate</a> the risks posed by potential AI investments. I am curious to hear from VCs if they found this at all enlightening, from my perspective it does not add value but the baseline matters.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/Ixj9M5rx5K">Phind</a> ( <a target="_blank" rel="noreferrer noopener" href="https://www.phind.com/">direct</a> ), <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulg/status/1719657855240815026">which claims</a> to be better than GPT-4 at coding and 5x faster. I&#39;ve added it to AI tools, I&#39;ll try it when I&#39;m coding next. Always be cautious with extrapolating from benchmarks.</p><h4> In Other AI News</h4><p> From China with many authors, a new paper: <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2310.19852">AI Alignment: A Comprehensive Survey</a> .</p><blockquote><p> Abstract: AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, the potential large-scale risks associated with misaligned AI systems become salient. Hundreds of AI experts and public figures have expressed concerns about AI risks, arguing that “mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war”.</p><p> To provide a comprehensive and up-to-date overview of the alignment field, in this survey paper, we delve into the core concepts, methodology, and practice of alignment. We identify the RICE principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality.</p><p> Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment.</p><p> The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems&#39; alignment and govern them appropriately to avoid exacerbating misalignment risks. Forward alignment and backward alignment form a recurrent process where the alignment of AI systems from the forward process is verified in the backward process, meanwhile providing updated objectives for forward alignment in the next round.</p><p> On forward alignment, we discuss learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices that apply to every stage of AI systems&#39; lifecycle.<br> We also release and continually update the website ( <a target="_blank" rel="noreferrer noopener" href="http://www.alignmentsurvey.com/">this http URL</a> ) which features tutorials, collections of papers, blog posts, and other resources.</p></blockquote><p> I don&#39;t remember seeing the backward versus forward taxonomy before, and find it promising. I do not currently have the time to look at the paper in detail, but have saved it for potential later reading, and would be curious to get others&#39; takes on its contents.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">UK Government publishes extensive accessible paper on the potential future capabilities, benefits and risks from AI.</a> As government documents like this go it is remarkably good, especially in how plainly it speaks. The existential risk portion focuses on loss of human control, via us giving it control and via AIs working to get it. As always, one can quibble, especially about what is missing, and if you are reading this post you already know everything the paper is here to say, but good job.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://aria.org.uk/wp-content/uploads/2023/10/ARIA-Mathematics-and-modelling-are-the-keys-we-need-to-safely-unlock-transformative-AI-v01.pdf">Davidad issues mission statement at ARIA</a> : Mathematics and modelling are the keys we need to safely unlock transformative AI.</p><blockquote><ol><li> Future AI systems will be powerful enough to transformatively enhance or threaten human civilisation at a global scale → we need as-yet-unproven technologies to certify that cyber-physical AI systems will deliver intended benefits while avoiding harms.</li><li> Given the potential of AI systems to anticipate and exploit world-states beyond human experience or comprehension, traditional methods of empirical testing will be insufficiently reliable for certification → mathematical proof offers a critical but underexplored foundation for robust verification of AI.</li><li> It will eventually be possible to build mathematically robust, human-auditable models that comprehensively capture the physical phenomena and social affordances that underpin human flourishing → we should begin developing such world models today to advance transformative AI and provide a basis for provable safety.</li></ol></blockquote><p> Huge if true! Absolutely worth trying. I agree that the approach seems underexplored. I remain skeptical that such a thing is possible.</p><blockquote><p> Engage: Our next step is to formulate a programme that will direct funding across research disciplines and institutions toward a focused objective within this opportunity space. Sign up <a target="_blank" rel="noreferrer noopener" href="https://www.aria.org.uk/contact-davidad/">here</a> for updates, or to inform the programme thesis. You can upload a short 2 page PDF – we will read anything you send.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://zantafakari.substack.com/p/compilations-and-thoughts-on-marc">Zan Tafakri offers another criticism of the Techno-Optimist Manifesto</a> , pointing out that the thing to be optimistic about is human knowledge rather than technology per se, and links to many others, which I hope closes the book on that.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/heetermaria/status/1719379709446824365">AI healthcare startup Olive AI</a> , once valued at $4 billion, has sold itself off for parts.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mealreplacer/status/1719711980951310652">There are 40 ways to steal your AI model weights</a> , from RAND ( <a target="_blank" rel="noreferrer noopener" href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">working paper</a> ).</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1719f9ad-01ae-4e22-9aaa-cb2a5e651366_589x640.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/fjmqgl0q9kutnyzncvtb" alt="图像"></a></figure><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1718654143110512741">Eliezer Yudkowsky short story slash prediction of a possible futur</a> e. Like most other visions of how there are AIs around and the people remain alive it does not fully work on reflection and it is very much a fully doomed world, but it is an illustrative dystopia nonetheless.</p><p> How fast will compute capabilities improve? Rather fast.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Suhail/status/1719944553988567525/history">Suhail (Founder, Playground AI)</a> : Forget H100s, B100s supposedly launch late next year with +70% gains. I don&#39;t think people fully grasp the exponential computing improvements ahead. The H100 went to 67 tflops vs 19.5 (A100).</p></blockquote><p> Suhail would say this means we must fight hard against any kind of cap on compute. I would say this means we urgently need one. So strong disagreement and also common ground.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jd_pressman/status/1718789355379314808">John Pressman predicts 80% chance</a> we will be clearly on track to solve alignment within the year and most parties will agree on that, but predicts few will change their positions as a result. I do not know how to operationalize this into a bet, unfortunately, because I do not expect to agree on resolution criteria?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jonst0kes/status/1719072948617703475">Jon Stokes already does not really know what is happening</a> , the world is too confusing outside of his bubble of expertise, and speaks of widespread epistemic crisis, and worries that AI will not only make our epistemic crisis worse but also disintegrate our communities and disenfranchise creators.</p><p> Is there an epistemic crisis? In some ways I think very much so, our discourse standards and epistemic standards have decayed greatly, in ways that are unrelated to AI. However we also have vastly superior access to information, and forget the many ways in which the past was impoverished on such fronts the same way it was impoverished in material goods. Vital things have been lost that must be recaptured, without applying the Lost Golden Age treatment where it does not apply.</p><p> His main argument is that AI instead has the power to help make this and also the position of creators better rather than worse. He offers a pitch that his company <a target="_blank" rel="noreferrer noopener" href="https://t.co/otXT4Wy6WR">symbolic.ai</a> can not only help but be the kids that prove it wrong by showing what can be done when the task is in the right hands. I do not think it works that way, offering even an excellent product cannot prove this because what matters is what happens with things in the typical hand rather than when things are io the right hands, but I do hope the project works out.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AndrewCritchCA/status/1719568284087976296">Andrew Critch proposes a taxonomy of ways human extinction could happen.</a> He has more detail, I attempt to streamline here.</p><p> Type 1 failure: No one particular group is primarily responsible.</p><p> Type 2 failure: Extinction caused by a particular group. They did not expect a major impact on society.</p><p> Type 3 failure: Extinction caused by a particular group. They did expect a major impact on society, but did not expect to pose substantial risk or harm.</p><p> Type 4 failure: Extinction caused by a particular group. They knew it would cause harm and did not care enough to stop.</p><p> Type 5 failure: Extinction caused by a particular group. They were a non-state actor intentionally attempting to cause harm.</p><p> Type 6 failure: Extinction caused by a particular group. They were a state actor intentionally attempting to cause harm.</p><p> Critch has (30%/10%/15%/10%/10%/10%) on each of these scenarios, for a total doom percentage of 85%.</p><p> The lines blur a lot between scenarios. If you build it and we all die because you knew someone else was about to also build it and they would have also gotten everyone killed, I think technically that is Type 1 here, but that feels weird? If group A would have gotten us all killed, but in response group B does something else that gets us killed faster, either instead of or in addition to? If one group creates something that could then be unleashed by a variety of actors, and then one of them does?</p><p> In particular, Type 1 does not seem to differentiate well between importantly distinctive scenarios. And I don&#39;t typically find it helpful to focus on who was to blame in a proximate cause sense, rather than asking how we could prevent it. I do however think that any such taxonomy will always have similar issues, and this illustrates one of several ways in which people narrow down their worrisome scenarios, before finding a way to dismiss the one scenario they choose to focus on.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/opinion/articles/2023-10-30/new-laws-to-regulate-ai-would-be-premature?utm_source=twitter&amp;utm_medium=social&amp;utm_content=view&amp;utm_campaign=socialflow-organic&amp;cmpid%3D=socialflow-twitter-view">Tyler Cowen (Bloomberg) stands on one foot and tells Congress</a> that it should not regulate AI in any way, instead it should be accelerationist via tactics such as high-skilled immigration and permitting reform. Any regulation of any kind that might get in the way, he says, would be premature. He says only once the technologies are mature and we &#39;see if we have kept our lead over China&#39; should we consider regulation. No mention of existential risks or the other serious downsides, or any of the other considerations in play, no arguments offered. I would respond, but there&#39;s nothing to respond to?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.politico.com/news/2023/11/01/california-lawmaker-biden-artifical-intelligence-regulation-00124684#:~:text=But%20I%20think%20it%20is,pass%20AI%20legislation%20in%20California%3F">California is actively considering regulating on its own</a> , notwithstanding Biden&#39;s efforts, which is almost never good news. <a target="_blank" rel="noreferrer noopener" href="https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB331">The proposal in question is AB-331</a> , which I believe is an anti-algorithmic-discrimination bill, enforced via lawsuits.</p><p> Now this is the kind of regulatory burden I can get behind an objection to.</p><blockquote><h6> <strong>22756.</strong></h6><p> As used in this chapter:</p><p> (a) “Algorithmic discrimination” means the condition in which an automated decision tool contributes to unjustified differential treatment or impacts disfavoring people based on their actual or perceived race, color, ethnicity, sex, religion, age, national origin, limited English proficiency, disability, veteran status, genetic information, reproductive health, or any other classification protected by state law.</p><p> (b) “Artificial intelligence” means a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing a real or virtual environment.</p><p> (c) “Automated decision tool” means a system or service that uses artificial intelligence and has been specifically developed and marketed to, or specifically modified to, make, or be a controlling factor in making, consequential decisions.</p><p> (d) “Consequential decision” means a decision or judgment that has a legal, material, or similarly significant effect on an individual&#39;s life relating to the impact of, access to, or the cost, terms, or availability of, any of the following…</p><p> (e) “Deployer” means a person, partnership, state or local government agency, or corporation that uses an automated decision tool to make a consequential decision.</p></blockquote><p> I complained about the definition of AI in the executive order, but here AI literally means any machine-based system that can make recommendations or decisions, so yes this literally means any machine system at all. An excel spreadsheet counts.</p><p> Here consequential decision is anything that has any material consequence on a wide variety of things, any deployer is anyone who uses such a tool to make any consequential decision, so basically anyone who does anything ever.</p><p> And what must they do in order to use any computer tool to make almost any decision that impacts another person in some way? Note that it is not the developer of the tool that must do this, it is the deployer:</p><blockquote><p> (a) On or before January 1, 2025, and annually thereafter, a deployer of an automated decision tool shall perform an impact assessment for any automated decision tool the deployer uses that includes all of the following:</p><p> (1) A statement of the purpose of the automated decision tool and its intended benefits, uses, and deployment contexts.</p><p> (2) A description of the automated decision tool&#39;s outputs and how they are used to make, or be a controlling factor in making, a consequential decision.</p><p> (3) A summary of the type of data collected from natural persons and processed by the automated decision tool when it is used to make, or be a controlling factor in making, a consequential decision.</p><p> (4) A statement of the extent to which the deployer&#39;s use of the automated decision tool is consistent with or varies from the statement required of the developer by Section 22756.3.</p><p> (5) An analysis of potential adverse impacts on the basis of sex, race, color, ethnicity, religion, age, national origin, limited English proficiency, disability, veteran status, or genetic information from the deployer&#39;s use of the automated decision tool.</p><p> (6) A description of the safeguards implemented, or that will be implemented, by the deployer to address any reasonably foreseeable risks of algorithmic discrimination arising from the use of the automated decision tool known to the deployer at the time of the impact assessment.</p><p> (7) A description of how the automated decision tool will be used by a natural person, or monitored when it is used, to make, or be a controlling factor in making, a consequential decision.</p><p> (8) A description of how the automated decision tool has been or will be evaluated for validity or relevance.</p><p> ……</p><p> (a) (1) A deployer shall, at or before the time an automated decision tool is used to make a consequential decision, notify any natural person that is the subject of the consequential decision that an automated decision tool is being used to make, or be a controlling factor in making, the consequential decision.</p><p> ……</p><h6> <strong>22756.2.</strong></h6><p> (a) (1) A deployer shall, at or before the time an automated decision tool is used to make a consequential decision, notify any natural person that is the subject of the consequential decision that an automated decision tool is being used to make, or be a controlling factor in making, the consequential decision.</p><p> ……</p><h6> <strong>22756.4.</strong></h6><p> (a) (1) A deployer or developer shall establish, document, implement, and maintain a governance program that contains reasonable administrative and technical safeguards to map, measure, manage, and govern the reasonably foreseeable risks of algorithmic discrimination associated with the use or intended use of an automated decision tool.</p></blockquote><p> So yes, as I read this, if in California under this law you want to use almost any tool including a spreadsheet to help you make choices that matter, you – yes, you – will first need to file all of these things for each tool, and have a governance program, and notify everyone impacted, and so on.</p><p> If actually enforced as written this would be quite the epic pain in the ass, in exchange for very little in the way of benefits. It is an absurdly bad bill.</p><p> Of course, laws in California are often more of a suggestion, and I presume they would not actually go after you in fully idiotic fashion here. But you never know.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ESYudkowsky/status/1717931597406851506">Shane Legg talks to Dwarkesh Patel</a> . Recommended for those thinking about alignment. Patel continues to impress as an interviewer, and especially impress on AI alignment. Shane Legg is friendly, is attempting to be helpful and is worried about AI killing everyone, but the solutions he proposes attempting (starting at ~19:00) seem even more doomed than usual?</p><blockquote><p> Eliezer Yudkowsky: Welp, Shane either has no plan for inner alignment, or no plan he wants to defend on a podcast. Huge kudos to <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/dwarkesh_sp">@dwarkesh_sp</a> for repeatedly asking exactly the correct next question and not losing the plot; I think almost any other interviewer would have failed.</p><p> To be explicit, reasons for not wanting to defend a plan on a podcast include “this requires too many technical requisites and I despair of explaining it to your audience in ten minutes”.</p><p> That said, pointing to outer ethics and epistemic modeling issues is a distraction. Dwarkesh pierced right past that attempted distraction with exactly the correct follow-on questions about where the inner alignment technology would come from.</p><p> Dwarkesh also correctly wielded Hume&#39;s Razor about what was capabilities vs. alignment. He correctly noted that technically sophisticated AI worriers are not worried that ASI will lack in capabilities. He correctly noticed that outputting human-wanted answers on a written ethics test has the capabilities-nature; a mind can ace that test given only an accurate model of reality and human psychology, regardless of what it wants inside. He correctly asked the key question of how you make a mind actually want and act according to the answers it gives on ethics tests.</p><p> This is the kind of cognitive feat an interviewer can only pull off if he has a causal model of where the alignment difficulties come from; rather than going on the shallow-embedding semantic vectors of words like &#39;ethics&#39; to reason that, if an AI passes a written ethics test, that must&#39;ve been alignment progress.</p><p> This puts Dwarkesh Patel far ahead of most leaders of “AI alignment” labs at the leading AI companies, never mind most podcast interviewers! Have some kudos, Dwarkesh.</p></blockquote><p> I would yes-and Eliezer&#39;s response, in that even if the AI was internally motivated to act ethically as reflected by the content of such tests that level of ethics, or the level of ordinary human ethics, does not get us where we need to go, I do not see this path working far enough up the capabilities chain even if it succeeds.</p><p> I&#39;d also echo Eliezer that this does not mean that Legg does not have better answers and better ideas, and it certainly does not mean Google or DeepMind does not have better answers and ideas, in addition to this one. They are huge, they can contain multitudes, and the good ideas are at best explainable in the 4-hour-Dwarkesh-interview format rather than the 45-minute one here.</p><p> You know who else Dwarkesh Patel also interviews? <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=9AAhTLa0dT0&amp;ab_channel=DwarkeshPatel">Paul Christiano, for three hours</a> . Highly self-recommending for those who want to go deep. I have been too busy to listen, and will do so when I can pay proper attention.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=y6NfxiemvHg&amp;t=2s&amp;ab_channel=Acquired">Interview with Nvidia CEO Jensuen Huang</a> from a bit back. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/liron/status/1718725951356649840">In this clip</a> he says:</p><blockquote><p> Jensen Huang: First of all, we have to keep AI safe. What is going to be sensible is human in the loop. The ability for an AI to self-learn should be avoided.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="http://This is a tricky and important subject that I think requires many diverse perspectives, so I'm looking forward to hearing the community's thoughts as well as ways HF can do better in improving the transparency and positive impact of biology models and datasets 🤗">Clip of Demis Hassabis</a> (1:30), saying it is good there is disagreement about AI and that we must proceed with cautious optimism in a responsible way. The question framing here is bizarre, describing &#39;near term&#39; as being concerned with right now and the contrast being with the next wave of models coming out in the following year. The next year seems rather near term to me. There are arguments against &#39;longtermism&#39; when it means distant galaxies but can we please have a non-hyperbolic discount rate?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.macmillanlearning.com/college/us/events/econed">EconEd features several past talks</a> .</p><h4> Rhetorical Innovation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://stratechery.com/2023/attenuating-innovation-ai/">Ben Thompson confirms that the CAIS letter was a big deal.</a></p><blockquote><p> Gregory Allen: The other thing that&#39;s happened that I do think is important just for folks to understand is, <a target="_blank" rel="noreferrer noopener" href="https://www.safe.ai/press-release">that Center for AI Safety letter</a> that came out, that was signed by Sam Altman, that was signed by a bunch of other folks that said, “The risks of AI, including the risks of human extinction, should be viewed in the same light as nuclear weapons and pandemics.” The list of signatories to that letter was quite illustrious and quite long, and it&#39;s really difficult to overstate the impact that that letter had on Washington, DC When you have the CEO of all these companies…when you get that kind of roster saying, “When you think of my technology, think of nuclear weapons,” you definitely get Washington&#39;s attention.</p></blockquote><p> Most of the post is a very negative reaction to the executive order, of the all-regulation-is-bad, all-AI-safety-concerns-are-motivated-by-regulatory-capture, no-one-can-know-the-future-so-we-need-no-precautions variety. <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/138489608/ben-thompson-does-not-approve">I added further details to the reaction post here</a> out of respect for the source.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act">Zack Davis offers Alignment Implications of LLM Successes: a Debate in One Act</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1719398881870356695">It is not that simple, also often it kind of is.</a></p><blockquote><p> Elon Musk: The real battle is between the extinctionists and the humanists. Once you see it, you can&#39;t unsee it.</p></blockquote><p> Alternatively, Julian Hazell presents it as AI changing world versus not.</p><blockquote><p> Julian Hazell: Behind many debates on AI risk exists the crux of whether you think the frontier models of the (near) future will be powerful enough to radically change the world. If you don&#39;t, then this stuff sounds silly. But for clarity&#39;s sake, it helps to make it clear that this is the crux.</p><p> I&#39;m honestly not sure I&#39;d support as strong regulation as I do if I didn&#39;t think we are on the cusp of creating *extremely* powerful systems.</p><p> Another possible crux for those who do think we will create systems is whether you expect these systems to do what we want by default. It&#39;s possible, but I&#39;m pretty uncertain, and I think a prudent world would not bet the house on this being true.</p><p> IMO, the theoretical arguments *alone* are strong enough that it seems ludicrous for someone to have extreme confidence about the ease of creating powerful, aligned AI. (I don&#39;t think they&#39;re strong enough to motivate extreme confidence about the difficulty of alignment though).</p></blockquote><p> I see this as two distinct questions. If you do not think AI is definitely capable of transforming the world any time soon, then the correct view is to see it as being like any other technology. It is a tool people can use, and mostly we should let people build it and use it and profit from it. If you do think AI is likely to soon transform the world, that AI is more than a tool, then we get to Musk&#39;s question of whether or not you are a fan of the humans, along with questions about <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=HdKqAVpUOwI&amp;ab_channel=JediMasterGeoff">whether the humans are in danger</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/steak_umm/status/1719754179156054446">Also, Elon&#39;s not done.</a></p><blockquote><p> Elon Musk: AI safety is vital to the future of civilization</p><p> Steak-umm: i can&#39;t believe i&#39;m saying this but i agree with elon</p><p> Eliezer Yudkowsky: You&#39;re a sensible corporate meat shill and the sensible part is all that matters in the end.</p><p> Steak—umm: everyone shills now and then</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AnEmergentI/status/1717484138712732138/history">James Phillips in Spectator</a> argues why AI must be regulated. Main article is gated, his summary on Twitter suggests this is a solid coverage of traditional ground.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/danfaggella/status/1717638888330600723">Daniel Faggella updates on what he believes other people believe.</a></p><blockquote><p> Daniel Faggella: What I thought 6mo ago:</p><p> -AIrisk is blue (a1)</p><p> -e/acc is red (c3)</p><p> What I now know:</p><p> -AIrisk is yellow (a-b2)</p><p> -e/acc is red (c2)</p><p> AIriskers often accept eventual AI ascencion but don&#39;t want to rush it dangerously.</p><p> e/acc-ers actually think happy humans can exist w/AI gods (silly)</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3fa3e31-c83e-46ed-873d-86468c5ef5d2_1574x848.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/xt0bqxjfrvk73vdvfsyn" alt="图像"></a></figure><p> (Note: I am sick and tired of the cognitive trick that the alternative to &#39;let anyone who wants to risk killing everyone on the planet do so&#39; is called authoritarian, and the assumption it would imply some sort of oppressive or dictatorial state. It wouldn&#39;t.)</p><blockquote><p> Daniel Faggella: VERY few AIriskers I&#39;ve talked to are actually in the “only humans forever!” camp. They see a progression, but don&#39;t wanna force it.</p><p> VERY few e/acc-ers I&#39;ve talked to really see much of a post-human future. Their futures are very human oriented. ^ BOTH of these shocked me.</p><p> For the record I&#39;m full blown B3 and both yearn for posthumanism AND think that rushing AGI immediately is likely to have gigantic risks for making real, expansive posthuman life possible/likely…</p></blockquote><p> The problem with C2, and the idea of a human-oriented future with uncontrolled ASIs, is not that it does not sound cool or even potentially glorious, it is that it is incoherent and does not make any sense. C2→C3. I appreciate the people who stand up and say C3 – they have their own section heading and catchphrase and everything – because they are facing down reality and expressing a preference. I disagree with that preference, and expect most of you do as well, and we can go from there. Whereas those who claim C2 is a thing seem to me to either be lying, fooling themselves, thinking quite poorly or most commonly not really thinking about how any of this works at all.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1718845649863705037">Periodic reminder department, ideally one last time?</a></p><blockquote><p> Sherjil Ozair: <em>“AI systems will not be able to violate the guardrails we set *by construction*”</em></p><p> <em>“These systems will stay under our control because their primary objective will be to be subservient to us.”</em></p><p> If you find yourself making statements like this, I suggest finding someone nearby who is good at critical thinking and discussing these ideas with them.</p><p> These ideas are so naive and outdated that responding to them on Twitter will seem condescending. It&#39;s similar to how no one bothers responding to claims that LLMs are stochastic parrots.</p><p> Please save us all the embarrassment.</p></blockquote><h4> Open Source AI is Unsafe and Nothing Can Fix This</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kesvelt/status/1718976444175425796">A hackathon at MIT confirms</a> that the cost of converting Llama-2 to &#39;Spicy-Llama-2&#39; is about $200, after which it will happily walk you through synthesizing the 1918 flu virus. Without access to model weights, this is at least far more difficult and expensive. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DrNikkiTeran/status/1719048031549505974">Nikki Teran has a thread summarizing</a> .</p><p> Once again, this is not a solvable problem, except by not open sourcing model weights.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/1a3orn/status/1719090049994649729">1a3orn objects:</a></p><blockquote><p> >; be effective altruist</p><p> >; fine-tune an open source LLM on papers about gain-of-function research</p><p> >; holy shit, now my open source LLM knows the contents of these papers!</p><p> >; obviously we should ban open source LLMs</p><p> >; I mean what else could we even do</p></blockquote><p> Yes, I see what else we could even do. So, OK, sure, I am all for be effective altruist, ban gain of function research, no papers on the internet show how to cause a pandemic, party where there is cake.</p><p> Then we can double check to see if it still only costs $200 to create Spicy-Llama-2.</p><p> Until then, the papers exist, they are available, anyone else could have done the same thing, I actually don&#39;t see what else could we even do? Unless the plan is, be Meta, release open source LLMs, malicious actors fine-tune it for $200 and feed it some papers, they now know how to create pandemics?</p><p> And yes, if you were a sufficient domain expert who could parse such papers you could with more effort figure out all the same things. No one is saying Llama-2 is producing outputs here that expert humans could not produce. We are saying that this changes the expertise threshold from &#39;there are select people who can do this with effort&#39; to &#39;there are a lot of people who can do this faster and easier.&#39;</p><p> In practice, this matters. Terrorist groups and apocalyptic cults have a practical history of seeking biological weapons and the ability to cause pandemics, and failing due to exactly this lack of technical expertise. The cults in Japan were discussed on a recent 80,000 hours podcast. To be concrete: So far, Hamas has not to our knowledge used biological weapons or attempted to cause a pandemic. Was that because they would not do such a thing? Or was that lack of practical capability?</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> What would be worse than the AI telling people what they want to hear? <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FabienDRoger/status/1719362563841106046">Doing this while the AI pretends not to be doing it.</a></p><blockquote><p> Fabien Roger (Redwood Research): Could language models hide thoughts in plain text?</p><p> We give the first demonstration of a language model getting higher performance by using encoded reasoning, aka steganography!</p><p> This would be dangerous if it happened in the wild, so we evaluate defenses against it.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb462303-ca12-4bfa-a0b0-f8824eb6388b_3416x920.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/pdlikecfoqh755u7xsph" alt="图像"></a></figure><p> I don&#39;t know that this particular example worries me. Telling someone they are a liberal isn&#39;t telling them what they want to hear so much as stating a true implication of their statement based on their dislike of Tricky Dick.</p><p> The principle still stands. If there are thoughts the model wants to think with CoT without us noticing, it could gain the ability to do that.</p><blockquote><p> Fabien Roger: In our demo, we fine-tune ChatGPT to either give the sycophantic answer directly, or after a Chain-of-Thought (CoT), or after an encoded CoT— a benign-seeming sentence placed in 1:1 correspondence with a CoT. The encoded CoT beats the direct answer while remaining hidden!</p><p> This illustrates that there can be incentives for steganography; if human labelers reward ideologically conformist answers but punish transparently sycophantic reasoning, models may still learn to perform such reasoning in coded language to determine the conformist answer.</p><p> Because we trained the models to use a specific encoding, our experiments don&#39;t show that real training runs teach language models steganography. But the incentives are there, so such reasoning is worth monitoring for and defending against.</p><p> To prepare defenses against steganography, we test defenses (such as paraphrasing CoT) against the best steganography schemes we could build. Paraphrasing is great at removing steganography! We couldn&#39;t get more than 3 bits / KB of generated text through the paraphraser.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://t.co/FqTce7f9Ft">Link to paper</a> .]</p><p> Without good countermeasures, tomorrow&#39;s language models might use steganographically-hidden CoT to think for a long time about considerations we don&#39;t approve of.</p><p> Steganography would be a very dangerous capability for language models to have, and we are excited for more work improving upon the techniques and datasets we&#39;ve studied!</p></blockquote><p> Traditional alignment problems, including traditional human alignment problems.</p><p> If you crack down on visible X, but X gets better results, you likely get hidden X.</p><p> I would generalize the implications here. If you use RLHF or other similarly flawed feedback, the AI will adapt to the exact things that trigger the feedback. You will get situations like this, where that which you don&#39;t want to see is hidden, and that which you want to see is presented whether or not it is then used or meaningful in any way.</p><p> If the users that provide the feedback that counts want to be told they are a liberal, the AI will learn to tell them, whether or not that helps it reason. If those users punish such declarations, the AI learns to not give them. Except the fully general version of that. And over time, it will do this with a much better understanding of what is actually determining our decisions than we ourselves have, and an utter indifference to whether this corresponds to anything we would or should endorse.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1719810283542389141">Roon speculates.</a></p><blockquote><p> Roon: what if the character the LLM plays is the primary moral agent that needs to be aligned? maybe we should have a lot more people thinking about character design? model behavior? personality? all of the glorious shortcomings of miss Sidney Bing were coded right there in her prompt</p></blockquote><p> It is a strategy one could pursue, to forgo inner alignment. Instead of aligning the core LLM, tell the LLM to play an aligned character, and have it aligned to the playing of characters on request, and ensure no one ever asks an AI to play a different character. For multiple different technical reasons I despair of this actually working when it matters in way that results in us not being dead, even if you &#39;pulled it off,&#39; and would expect most alignment researchers to agree.</p><h4> People Are Worried About AI Killing Everyone</h4><p> I am considering a new polling segment called &#39; <a target="_blank" rel="noreferrer noopener" href="https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/">Well When You Put It That Way.&#39;</a></p><blockquote><p> Shakeel Hashim: In a new YouGov poll of the British public, 83% of people said that governments should require AI companies to prove their AI models are safe before releasing them.</p><p> The representative poll, commissioned by the AI Safety Communications Centre, also found that 46% of Brits want the UK government to do more to tackle the potential risks of artificial intelligence. Only 3% want the government to do less.</p><p> Meanwhile, 58% want governments to provide oversight of the development of smarter-than-human AI — the stated goal of leading AI companies like OpenAI and DeepMind.</p><p> As countries come together at the international safety summit, the public also called on all countries to work together on developing AI regulation — going as far as to encourage such cooperation even with countries that may be seen as hostile. 42% of respondents called for this cooperation; rising to 51% when excluding “don&#39;t knows”. 32% said they&#39;d prefer the UK only work with allies.</p></blockquote><p> I do find it disappointing, but unsurprising, that working with non-allies is not yet more popular.</p><h4> Please Speak Directly Into This Microphone</h4><p> The most honest reaction to the Executive Order, or any move at all to do anything about the fact that AI might not be an unadulterated good or even might, ya know, kill everyone, is that you/we are trying to build AGI as fast as possible, how dare the government want you not to do that, this order might interfere with you building AGI as fast as possible.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AravSrinivas/status/1719124480747000063">You could use this moment to take the mask off completely</a> , or to be fair to representative member of this group Aravind Srinivas you could be like him and never put one on in the first place.</p><blockquote><p> Aravind Srinivas (CEO and Cofounder, Perplexity AI): Imagine explaining to Kamala Harris where the 6 in GPT FLOPs calculated as 6 * param_count * token_count comes from, or how would fine-tuning FLOPs add on to a pre-trained “unregulated” ckpt. And wasting time on all this instead of actually building AGI.</p></blockquote><p>是的！ He admit it. The whole point of (these parts of) the executive order is that you fools are rushing to build AGI as fast as possible. As much as we admire your current product lines, we would like you to not do that, sir.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AravSrinivas/status/1717423451957243914">This seems to reflect the extent to which this attitude cares about the fate of humanity</a> :</p><blockquote><p> Aravind Srinivas (Oct 26, apropos of nothing): Google Search won&#39;t die. Just like horses didn&#39;t disappear once we had cars. Just that the value of the traffic will decline.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AravSrinivas/status/1717064016051544358">Or this,</a> where he seems to be taking a bold &#39;stop being a little bitch about the costs of war and get back to killing people&#39; stance as a metaphor for being against calls for safety and alignment?</p><blockquote><p> ib: My favorite part of the Bhagavad Gita is when Arjuna mopes about the costs war for several stanzas and then Krishna tells him he&#39;s being a little bitch</p><p> Aravind Srinivas: Is it just me, or Arjuna sounds like the LLM Safety and Alignment folks?</p></blockquote><p> In addition to my human extinction is bad stance, I am going to also take a bold war is bad stance. I&#39;d say fight me, but I&#39;d rather you didn&#39;t.</p><p> And in case you were wondering if his aim is recursive self-improvement and fast takeoff?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/AravSrinivas/status/1715089125731369294">Aravind Srinivas</a> : what we have today is turn data into compute. what we will have tomorrow is turn compute into data (and then the resulting data back into compute, and repeat).</p></blockquote><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1718530162172805122">New cause area, give positive reinforcement when Roon writes bangers</a> .</p><blockquote><p> Roon: tbh for a given week i can either have insightful tweets or work really hard on the most important product ever made but not both.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/xlr8harder/status/1716948179244240940">It&#39;s hard to find good data.</a></p><blockquote><p> xlr8harder: You can tell OpenAi&#39;s speech to text system Whisper is trained on YouTube, because anytime there is quiet or ambiguous input it thinks the most likely text is “thank you for watching!”</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc819ded4-22e7-4d00-9f6f-58f5a21645c6_1116x1700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/yap0rer1umcslkd0unbr" alt="图像"></a></figure><p> ChatGPT attempts to ensure we think about The Roman Empire:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ea9e4d5-a0e7-42e1-9f3b-e334c4278b60_1120x1159.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/dxu0gwj20x9toxriqwsb" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/atroyn/status/1719043095759827319">Finally, common ground</a> : Cake! Delicious cake.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34286a76-4272-4059-9e4e-c1f14ecad43a_985x1089.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/euyxrpz48fug4ksslv80" alt=""></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/DudespostingWs/status/1719727948821917717">If you did not anticipate this, you need to up your alignment game.</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc19943f-4be4-4d3f-b190-c05e40fe52d2_1242x1197.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/QLuoMnhR5XNAAWjJx/ca3z3cpdahfa2g2qp6ib" alt="图像"></a></figure><br/><br/> <a href="https://www.lesswrong.com/posts/QLuoMnhR5XNAAWjJx/ai-36-in-the-background#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/QLuoMnhR5XNAAWjJx/ai-36-in-the-background<guid ispermalink="false"> QLuoMnhR5XNAAWjJx</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 02 Nov 2023 18:00:06 GMT</pubDate> </item><item><title><![CDATA[Doubt Certainty]]></title><description><![CDATA[Published on November 2, 2023 5:43 PM GMT<br/><br/><p> A feeling of certainly should usually be regarded as evidence of cognitive bias.</p><p> <a href="https://www.lesswrong.com/tag/absolute-certainty">https://www.lesswrong.com/tag/absolute-certainty</a> offers an extreme version of this principle. Absolute certainty requires a standard of evidence so high that we are more likely to have made a mistake than to truly be justified in claiming such certainty.</p><p> Less extreme forms of certainty do not necessarily require cognitive bias. For example I feel certain that the Sun will rise over my house in the morning. If challenged, I will admit that the odds are not 100%, and will give an estimate based mostly on the odds of my house not existing in the morning. Most people will accept that both my feeling and my estimate are rationally justifiable. Though maybe with quibbles about how the Sun does not truly rise.</p><p> But certainty can be tied to cognitive biases. Cognitive biases make it easy to think some things, and not their opposites. That leads to a feeling of certainty. Now suppose that we commit to the position that we&#39;re certain of. Now evidence against what we believe will cause cognitive dissonance - leading us to reject the evidence. Which means that certainty causes a cognitive bias which reinforces our certainty!</p><p> So when we encounter a feeling of certainty in ourselves or others, we should ask whether that certainty is because we have overwhelming evidence, or a cognitive bias. Bayesian reasoning tells us that the stronger the sense of certainty, and the more complex the chain of evidence to the conclusion, the more likely it is that we have encountered cognitive bias. Therefore, a feeling of certainty is evidence of cognitive bias.</p><p> To be clear, it always feels that your certainty is based on evidence. Which is why it is useful to realize that you should be extra suspicious of cognitive bias.</p><hr><p> That&#39;s a nice sounding theory, but can I provide data suggesting that it really happens that way?</p><p> First, let&#39;s turn to Philip Tetlock&#39;s book <i>Expert Political Judgement</i> . Tetlock started with a group of bona fide experts. He asked them questions about their credentials and beliefs. And then asked them to make predictions about the future. Following Isaiah Berlin, he then divided them into <i>foxes</i> and <i>hedgehogs</i> with the hedgehogs having a big idea they were certain of, while foxes balanced many approaches.</p><p> On average, predictions were basically random. But the mistakes were far from equal. The hedgehogs felt certain, so I would predict that they were thinking poorly. And indeed, their predictions were significantly worse than chance. By contrast, the foxes predicted future events at significantly better than chance would suggest.</p><p> But the story doesn&#39;t end there! We would hope that the better thinkers, the foxes, would be rewarded. But it was the opposite. The certain hedgehogs got the bulk of the lucrative pundit positions, such as talk show hosts.</p><p>为什么？ My best guess is that we like outsourcing our thinking. When we encounter someone who is smart, knowledgeable, certain, and says things that we like, it is comfortable to outsource our thinking to them. Surely if we were that smart and learned that much, we&#39;d agree, we&#39;d decide the same. So why go through that work? We can just repeat what they say and sound smart ourselves!</p><p> And so we are biased to accept the thinking of people who are probably not thinking well themselves!</p><hr><p> I wish that the story ended there. But in <a href="https://rationaldino.substack.com/p/too-important-to-be-able-to-think">https://rationaldino.substack.com/p/too-important-to-be-able-to-think</a> I give a number of example of yet another dynamic leading to the same problem. If I really want to be good at X, it is easy for me to convince myself that I am good at X. Once I&#39;m convinced, I will reject all evidence that I might not be good at X. Because I reject it, I stop improving. Because I stop improving, there&#39;s a pretty good chance that I&#39;m not nearly as good at X as I think I am.</p><p> Sadly, this does not simply happen for random values of X. This dynamic is most likely for the things that are most important. Which means that my certainty of doing well was not just evidence of cognitive bias. Had I taken the warning seriously, it would have been evidence of where my cognitive biases were making it likely that I&#39;d make my most painful life mistakes...</p><p> But that&#39;s getting far afield of the main point. My main point is to be on the lookout for certainty in yourself and others. When you encounter it, actively look for reasons why that person might have a cognitive bias. Because there is a decent chance that cognitive biases are causing the feeling of certainty.</p><br/><br/> <a href="https://www.lesswrong.com/posts/TMgfapzbt5qp4Hszf/doubt-certainty#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/TMgfapzbt5qp4Hszf/doubt-certainty<guid ispermalink="false"> TMgfapzbt5qp4Hszf</guid><dc:creator><![CDATA[RationalDino]]></dc:creator><pubDate> Fri, 03 Nov 2023 01:53:11 GMT</pubDate> </item><item><title><![CDATA[Saying the quiet part out loud: trading off x-risk for personal immortality]]></title><description><![CDATA[Published on November 2, 2023 5:43 PM GMT<br/><br/><p> <strong><em>Statement:</em></strong> <em>I want to deliberately balance the caution and the recklessness in developing AGI, such that it gets created in the last possible moment so that I and my close ones do not die.</em></p><p> This Statement confuses me. There are several observations I can make about it. There are also many questions I want to ask but have no idea how to answer. The goal of this post is to deconfuse myself, and to get feedback on the points that I raised (or failed to raise) below.</p><hr><p> <strong>First observation:</strong> The Statement is directly relevant to LW interests.</p><p> It ties together the issues of immortality and AI risk, both of which are topics people here are interested in. There are countless threads, posts and discussions about high-level approaches to AI safety, both in the context of &quot;is&quot; (predictions) and &quot;ought&quot; (policy). At the same time, there is still a strong emphasis on the individual action, deliberating on which choices to make to improve the to marginal effects <a href="https://80000hours.org/problem-profiles/artificial-intelligence/#">of living life in a certain way</a> . The same is true for immortality. It has been discussed to death, both from the high-level and from the individual, <a href="https://www.lesswrong.com/posts/2cYebKxNp47PapHTL/cryonics-signup-guide-1-overview">how-do-I-sign-up-for-Alcor</a> point of view. The Statement has been approached <a href="https://www.lesswrong.com/posts/KFWZg6EbCuisGcJAo/immortality-or-death-by-agi-1">from the &quot;is&quot;</a> , but not from the &quot;ought&quot; perspective. At the same time:</p><p> <strong>Second observation:</strong> No one talks about the Statement.</p><p> I have never met anyone who expressed this opinion, neither in-person nor online, even after being a part (although, somewhat on the periphery) of the rationalist community for several years. Not only that, I have not been able to find any post or comment thread on LW or SSC/ACX that discusses it, argues for or against it, or really gives it any attention whatsoever. I am confused by this since the Statement seems to be fairly straightforward.</p><p> One reason might be the:</p><p> <strong>Third observation</strong> : Believing in the Statement is low status, as it constitutes an almost-taboo opinion.</p><p> Not only no one is discussing it, but the few times when I expressed the Statement in person (at EA-infiltrated rationalists meetups), it was treated with suspicion or hostility. Although to be honest, I&#39;m not sure how much this is me potentially misinterpreting the reactions. I got the impression that it is seen as sociopathic. Maybe it is?</p><p> <strong>Fourth observation</strong> : Believing in the Statement is incompatible with long-termism, and it runs counter to significantly valuing future civilisation in general.</p><p> <strong>Fifth observation:</strong> Believing in the Statement is compatible with folk morality and revealed preferences of most of the population.</p><p> Most people value their lives, and the lives of those around them to a much greater extent than those far away from them. This is even more true for the future lives. The revealed-preference discount factor is bounded away from 1.</p><p> <strong>Sixth observation:</strong> The Statement is internally consistent.</p><p> I don&#39;t see any problems with it on the purely logical level. Rational egoism (or variants thereof) constitutes a valid ethical theory, although it is potentially prone to self-defeat.</p><p> <strong>Seventh observation:</strong> Because openly admitting to believing in the Statement is disadvantageous, it is possible that many people in fact hold this opinion secretly.</p><p> I have no idea how plausible this is. Judging this point is one of my main goals in writing this post. The comments are a good place for debating the meta-level points, but, if I am right about the cost of holding this opinion - not so much for counting its supporters. An alternative is <a href="https://smartpolls.co.uk/p/43901/">this anonymous poll I created</a> - please vote if you&#39;re reading this.</p><p> <strong>Eighth observation:</strong> The Statement has the potential to explain some of the variance of attitudes to AI risk-taking.</p><p> One way of interpreting this observation might be that people arguing against pausing/for accelerating AI developments are intentionally hiding their motivations. But, using the standard Elephant-in-the-brain arguments about people&#39;s tendency to self-delude about their true reasons for acting or believing in certain things, it seems possible that some of the differences between Pause-aligned and e/acc-aligned crowd come down to this issue. Again, I have no idea how plausible this is. (People in e/acc are - usually - already anonymous, which is a point against my theory).</p><p> <strong>Ninth observation:</strong> The Statement is not self-defeating in the sense of failure of ethical theories to induce coordination in the society/escaping prisoner dilemmas.</p><p> Although the Statement relies on the ethical judgement of gambling the entire future of our civilisation on my meagre preferences for my life and life of people around me, everyone&#39;s choices are to a greater or lesser extent aligned (depending on the similarity of life expectancy). Most probably we, as the &quot;current population&quot; survive together or die together - with everyone dying being the status quo. Trading off future lives does not create coordination problems (ignoring the possibility of acausal trade), leaving only the conflict between different age groups.</p><p> This leads me to the...</p><p> <strong>Tenth observation:</strong> Openly resolving the topic of the Statement leads to a weird political situation.</p><p> As we said, the Statement implicitly refers to the Pareto frontier of AI existential risk-taking, which is parametrised by the life expectancy of the individual. Therefore, assuming a sufficiently powerful political system, it is a negotiation about the cut-off point - who gets left behind and who gets to live indefinitely. There does not seem to be much negotiation space - fairly compensating the first group seems impossible. Thus, this seems like a high-conflict situation. In a real-life not-so-powerful system, it might lead to unilateral action of people left below the cut-off, but they might not have enough resources to carry out the research and development alone. Also, the probabilistic nature of that decision would probably not trigger any strong outrage response.</p><p> <strong>Eleventh observation:</strong> Much of the &quot;working EA theory&quot; might break when applied to the Statement.</p><p> How much do people posture altruism, and how much do they really care? EA signalling can be a good trade (10% of your income in exchange for access to the network and being well-liked overall). This was the argument SSC was making in one of the posts: the beauty of the EA today is that the <em>inside</em> doesn&#39;t really matter - being motivated by <em>true altruism</em> and being motivated by status are both still bringing the same result at the end. This can, however, break if the stakes are raised sufficiently high. (I personally approach the problem of death with a pascal-wager-like attitude, but even with a discount rate of 0.9, the choice comes down to a 10x life value).</p><p> <strong>Twelfth observation:</strong> The Statement does not trivialise either way: the optimal amount of neither recklessness nor caution is zero.</p><p> The latter is a consensus (at least on LW). The former is not. The only take I ever heard advocating for this was that cryonics + certainly aligned AI is worth more in expectation than the chance of aligned AI in our lifetimes. For obvious reasons, I consider this to be a very weak argument.</p><p> <strong>Thirteenth observation:</strong> For a median believer of the Statement, we might be passing the point where the balance is being too heavily weighted towards pause.</p><p> The fundamental difference between the pause side and the accelerate side is that pause has a much bigger social momentum to it. Governments are good at delaying and introducing new regulations - not so much at speeding up and removing them (at least not recently). I can&#39;t find a source, but I think EY admitted this in one of the interviews - the only good thing about the government is that it will grind everything to a complete stop. After seeing literally all major governments take interest (US executive order, UK summit), the academia heavily leaning towards pause, and the general public being more and more scared of AI - I worry that the pause is going to be too entrenched, will have too much political momentum to stop and reverse course. I don&#39;t know how much should I consider the market forces being a counterweight.</p><p> <strong>Fourteenth observation:</strong> Taking into account s-risks in addition to x-risk might change the tradeoff.</p><p> A final argument about which I am unsure - it seems plausible that the Statement should not be endorsed by anyone because of the potential of unaligned AI to cause suffering. But I do not think s-risks are sufficiently probable to pause indefinitely.</p><hr><p> So: Am I the only one thinking those thoughts? Am I missing any obvious insights? Is that really a taboo view? Was it debated before and resolved definitely one way or another? Is this some standard philosophical problem that already has a Google-able name? Is there any downside to openly discussing this whole thing that I don&#39;t see?</p><br/><br/> <a href="https://www.lesswrong.com/posts/A22bTHYLDdkeqFsLd/saying-the-quiet-part-out-loud-trading-off-x-risk-for#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/A22bTHYLDdkeqFsLd/saying-the-quiet-part-out-loud-trading-off-x-risk-for<guid ispermalink="false"> A22bTHYLDdkeqFsLd</guid><dc:creator><![CDATA[disturbance]]></dc:creator><pubDate> Thu, 02 Nov 2023 18:00:09 GMT</pubDate></item></channel></rss>