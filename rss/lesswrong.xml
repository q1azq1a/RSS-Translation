<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 10 月 26 日星期四 12:22:06 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Sensor Exposure can Compromise the Human Brain in the 2020s]]></title><description><![CDATA[Published on October 26, 2023 3:31 AM GMT<br/><br/><p><strong>概述</strong></p><p>20 世纪因心理学（一门研究人类心智的科学）的发现及其利用（例如大规模战争、宣传、广告、信息/混合战争、决策理论/相互确保毁灭）而发生了根本性的改变。</p><p>然而，我们有理由认为，如果人类思维的科学和开发比现在更加先进，那么 20 世纪将会发生进一步的转变。</p><p>我在这里认为，在一个大规模监视、美国、中国和俄罗斯之间的<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合</u></a>/认知战争以及机器学习取得重大进步的时代，我们有理由认为，SOTA 人类认知分析和利用的情况可能已经是威胁整个人工智能安全社区运行的连续性；如果不是现在，那么很可能会在 2020 年代的某个时候发生，届时全球范围内的事件可能会比人类在过去二十年所习惯的步伐要多得多。</p><p>人工智能将成为这些王国以及它们之间战争的关键，要求暂停开发可能是人类生存的最低要求，而对于这样的冲突， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#AI_pause_as_the_turning_point">我们甚至不知道是什么袭击了我们</a>。</p><p>对于一般人类生活来说，攻击面大得令人无法接受，更不用说对于人工智能安全社区来说了，这个社区是一个由一群书呆子组成的社区，他们偶然发现了宇宙这一面的命运所围绕的工程问题，一个绝对不能失败的社区。在 2020 年代幸存下来，也不会以缩小/捕获的形式跛行。</p><p><br></p><p><strong>这个问题是智能文明的基础</strong></p><p>如果有智慧的外星人，由一束束触手、水晶或植物组成，思考速度极其缓慢，他们的思想也会有可发现的功绩/零日，因为任何自然进化的思想都可能像人脑一样，是一堆意大利面条代码在其预期环境之外运行。</p><p>他们甚至可能不会开始触及发现和标记这些漏洞的表面，直到像今天的人类文明一样，他们开始用传感器包围数千或数百万同类，这些传感器可以每天几个小时记录行为并找到<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=of%20procuring%20results.-,There%20is%20no,-logical%20endpoint%20to">相关性网络</a>。</p><p>就人类而言，使用社交媒体作为自动化人工智能实验的受控环境似乎创造了人类行为数据的临界量。</p><p>社交媒体引导人类成果的能力并不是孤立地进步的，它们与人类思维的理解和利用的广泛加速并行，这<a href="https://arxiv.org/pdf/2309.15084.pdf"><u>本身就是加速人工智能能力研究的副产品</u></a>。</p><p>通过将人与其他人进行比较并预测特征和未来行为，多臂老虎机算法可以首先预测特定的操纵策略是否值得冒险实施；导致高成功率和低检测率（因为检测可能会产生高度可测量的响应，特别是在大量传感器暴露的情况下，例如未覆盖的网络摄像头，因为将人们的微表情与失败或暴露的操纵策略的案例或工作网络摄像头视频进行比较数据转化为基础模型）。</p><p>当您拥有数十亿小时的人类行为数据和传感器数据的样本量时，不同类型的人反应的毫秒差异（例如面部微表情、滚动过去涵盖不同概念的帖子时的毫秒差异、涵盖不同概念后的心率变化、眼球追踪）眼睛经过特定概念、触摸屏数据等后的差异从难以察觉的噪音转变为<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=There%20is%20no%20logical%20endpoint%20to%20the%20amount%20of%20data%20required%20by%20such%20systems...%20All%20information%20is%20potentially%20relevant%20because%20it"><u>相关网络</u></a>的基础。<a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html"><u>美国国家安全局在每个操作系统和可能的芯片固件中都储存了漏洞</u></a>，因此我们无法很好地估计收集了多少数据，任何试图获得良好估计的人都可能会失败。历史趋势是，存在大量恶意数据收集，而低估国家安全局的人每次都错了。此外，这篇文章详细介绍了一个非常有力的案例，即他们非常有动力去利用这些传感器。</p><p>即使目前收集的传感器数据还不足以危及人们的安全，但在 2020 年代或缓慢起飞的某个时刻，它可能会突然变得足够。</p><p>现代行为操纵范式的核心要素是能够尝试大量的事情并看看什么有效；不仅仅是暴力破解已知策略的变体以使其更有效，而是首先暴力破解新颖的操纵策略。这完全避免了导致重复危机的稀缺性和研究缺陷，而重复危机至今仍然是心理学研究的瓶颈。</p><p>社交媒体的个性化定位利用深度学习来产生一种像手套一样适合人类思维的体验，虽然我们无法完全理解，但它给黑客提供了令人难以置信的余地，让他们找到方法将人们的思维引向可测量的方向，只要这些方向是可测量的。 。人工智能甚至可以实现自动化。</p><p>事实上，人类文明中的原创心理学研究不再需要聪明、有洞察力的人来进行假设生成，这样你可以资助的有限研究就有希望找到有价值的东西。仅使用当前的社交媒体范式，您就可以进行研究，例如新闻提要帖子的组合，<i>直到</i>找到有用的东西。可测量性对此至关重要。</p><p>如果不运行算法本身，我不知道多臂老虎机算法会发现什么技术；我做不到，因为只有那些大量购买服务器的人才能访问这么多数据，即使对他们来说，这些数据也被大型科技公司（Facebook、亚马逊、微软、苹果和Google）和足够强大的情报机构（NSA 等）来防止黑客窃取和毒害数据。</p><p>我也不知道当团队中的人是有能力的心理学家、舆论专家或其他公关专家解释和标记数据中的人类行为以便人类行为变得可测量时，多臂老虎机算法会发现什么。合理地说，该行业自然会达到一种平衡，即五大科技公司竞相为这项研究寻找人才，同时最大限度地降低斯诺登式泄密的风险，类似于 10 年前美国国家安全局在斯诺登泄密后的“改革” 。您可以假设人们会自动注意到并解决这种瓶颈。科技公司和情报机构之间的旋转门雇佣也规避了<a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult"><u>情报机构的能力问题</u></a>。</p><p>只需少数心理学专家的人类洞察力就足以训练人工智能自主工作；尽管需要这些专家的持续投入，并且大量的见解、行为和发现会被遗漏，并且需要额外的 3 年时间或一些东西才能被发现和标记。</p><p>即使没有人工智能，也有大量的人类操纵策略被发现和利用是微不足道的（尽管当你将人工智能放在上面时，情况会更加严重），只是 20 世纪的机构根本无法访问它们以及学术心理学等技术。</p><p>如果他们获得了与特定人类目标具有相似特征的人的足够数据，那么他们就不必对目标进行太多研究来预测目标的行为，他们只需对这些人运行多臂强盗算法即可找到操纵行为已经对具有相同遗传或其他特征的个体起作用的策略。</p><p>尽管与样本数据中的绝大多数人相比，Lesswrong 用户的平均分布更加偏远，但这成为了一个技术问题，因为人工智能能力和计算变得致力于从噪声中对信号进行分类并寻找网络的任务。与较少数据的相关性。 <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>仅小丑攻击就表明，基于社会地位的大脑漏洞在人类中相当一致</u></a>，这表明来自数百万或数十亿人的样本数据可用于发现构成人工智能安全社区的人类大脑中的各种漏洞。 。</p><p><br><br></p><p><strong>攻击面太大</strong></p><p>缺乏对此的认识会带来安全风险，就像使用“密码”一词作为您的密码一样，除非您对自己的思想的控制受到威胁，而不是对计算机操作系统和/或文件的控制。这已经是钢铁般的操作了； 10 年前/10 年后的误差线似乎相当宽。</p><p>如果黑客可以随时更改实用程序功能，那么一开始就没有多大意义。可能有些部分是难以改变的，但在这一点上很容易高估自己；例如，如果你重视长远的未来，并认为任何错误的论点都无法说服你，但社交媒体的新闻源却让威尔·麦卡斯基尔产生了疑虑或不信任，那么你离不关心长远的未来又近了一步；如果这不起作用，多臂老虎机算法将继续尝试，直到找到有效的方法并进行迭代。对于比你更了解人类大脑的攻击者来说，有很多聪明的方法可以根据与类似人的比较来发现你复杂且深刻的个人内部冲突，并按照攻击者的条件解决它们。人类的大脑是一堆意大利面条式的代码，所以可能在某个地方有什么东西。人脑有漏洞，社交媒体平台使用大量人类行为数据来寻找复杂的社会工程技术的能力和成本是一个深刻的技术问题，你无法凭直觉和 2010 年代之前的历史来处理这个问题先例。</p><p>因此，您应该假设您的效用函数和值有在未知时间被黑客攻击的风险，因此应该分配一个贴现率来考虑几年内的风险。仅在未来 10 年中缓慢的起飞就保证了这个折扣率实际上太高了，以至于人工智能安全社区的人们无法继续相信它接近于零。</p><p>我认为接近零是一个合理的目标，但在目前的情况下，人们甚至懒得遮盖他们的网络摄像头，在房间里用智能手机就地球的命运进行重要而敏感的对话，并使用社交媒体。每天近一个小时的媒体报道（滚动浏览近千个帖子）。不管你喜欢与否，使用方向键以外的任何东西滚动浏览一篇文章都会生成至少一条曲线，而每天生成的数万亿条曲线都是线性代数，是插入机器学习的完美形状。如果攻击面如此之大，这种环境下的贴现率就不能被认为“合理”接近于零；世界变化如此之快。</p><p>我们在这里所做的一切都是基于这样的假设：强大的力量，如情报机构，不会扰乱社区的运作，例如通过使用匿名代理而导致相互之间的假旗攻击，从而煽动派系冲突。</p><p>如果人们有<a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect"><u>任何他们看重的东西</u></a>，而人工智能安全社区可能确实有，那么当前零努力的人工智能安全范式是非常不合适的，它基本上是完全屈服于隐形黑客。</p><p><br></p><p><strong>信息环境可能是敌对的</strong></p><p>我怀疑导致人工智能安全彻底失败的一个大瓶颈是，湾区的人工智能联盟社区拥有技术能力，可以直观地理解人类可以被人工智能操纵，因为在优化的思想分析和实验环境下，就像社交媒体新闻推送一样，但我认为情报机构和五大科技公司永远不会真正做这样的事情。与此同时，华盛顿的人工智能政策界知道，强大的公司和政府机构经常储备这样的能力，因为他们知道如果不这样做，他们可以逃脱惩罚并减轻损害，并且这些能力在国际冲突中派上用场，例如中美冲突，但他们缺乏直观地了解如何用 SGD 操纵人类思维所需的定量技能（许多人甚至不认识缩写“SGD”，所以我使用“AI”代替）。</p><p>如果SF数学书呆子和DC历史书呆子更多地混合的话，这个问题可能可以避免，但不幸的是，似乎历史书呆子对数学课的记忆很糟糕，而数学书呆子对历史课的记忆也很糟糕。</p><p>在这个隔离和营养不良的环境中，“精神控制”的不良第一印象<a href="https://www.lesswrong.com/posts/c5oyHuHaw4AcWy4tf/information-warfare-historically-revolved-around-human"><u>占主导地位</u></a>，而不是缓慢起飞的逻辑推理和认真的实际计划。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/azjeddznwoiuzsxato92"></p><p><br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Lw8enYm5EXyvbcjmt/zmrpngdt0hshh03wacfc"></p><p>如果有什么东西可以被我所描述的基于社交媒体的范式操纵的话，那就是印象和人类印象形成的过程，因为有很多关于这方面的数据。如果社交媒体<i>会</i>操纵任何事情，那就是对社交媒体的态度会损害人类大脑，因为 SGD/AI 会自动选择与人们继续使用社交媒体的情况相对应的新闻推送帖子的银河大脑组合，并避免与人们退出社交媒体的案例相对应的帖子组合。人们离开或留下的案例有数十亿。</p><p>让人们留在社交媒体上对于任何目标都至关重要，从准备以美国和中国之间的信息战为特征的军事<a href="https://en.wikipedia.org/wiki/Hybrid_warfare"><u>混合战争</u></a>应急计划，到只是经营一家人们不会离开你的平台的企业。</p><p>如果存在与其他平台（例如 Tiktok 或 Instagram Reels）争夺用户时间的逐底竞争，则尤其如此，因为这些平台不太愿意利用 AI/SGD 来最大限度地提高僵尸大脑参与度和用户保留率。</p><p>我们应该默认假设现代信息环境是不利的，并且某些话题比其他话题更具对抗性，例如具有强烈地缘政治意义的乌克兰战争和新冠疫情。我在这里认为，信息战本身是一个具有强烈地缘政治意义的话题，因此也应该是一个对抗性的信息环境。</p><p>在对抗性信息环境中，印象比整个认知更容易受到损害，因为当前的范式由于更好的数据质量而针对这一点进行了优化。因此，我们应该通过深思熟虑的分析和预测来应对传感器暴露风险，而不是模糊的表面印象。<br></p><p><strong>解决方案很简单</strong></p><p>一般来说，眼动追踪可能是预测分析、情绪分析和影响技术中最有价值的用户数据 ML 层，因为眼动追踪层只是映射到每毫秒每只眼睛在屏幕上居中的确切位置的两组坐标（每只眼睛一个，因为每只眼睛运动的毫秒差异也可能与一个人的思维过程的有价值的信息相关）。</p><p>这种紧凑的数据使深度学习能够以毫秒精度“看到”人类的眼睛/大脑在每个单词和句子上停留的时间。值得注意的是，数以百万计的这些坐标的样本量可能与人类思维过程密切相关，以至于眼球追踪数据的价值可能超过所有其他面部肌肉的价值总和（面部肌肉是所有面部表情和情绪微表情的鼻祖， <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=A%20critical%20element%20is%20for%20as%20many%20people%20as%20possible%20in%20AI%20safety%20to%20cover%20up%20their%20webcams%3B%20facial%20microexpressions%20are%20remarkably%20revealing%2C%20especially%20to%20people%20with%20access%20to"><u>也可能可以通过计算机视觉紧凑地还原</u></a>，因为面部附近的肌肉不到 100 块，而且大多数肌肉的信噪比非常差，但效率不如眼动追踪）。</p><p>如果 LK99 被复制并且手持式功能磁共振成像变得可构建，那么也许它可以争夺第一名的位置；或者也许我愚蠢地低估了将音频对话记录插入 LLM 并通过对微小的心率变化添加时间戳来自动标记对话者最重视的对话部分的压倒性优势。</p><p>然而，在附近没有智能手机的情况下举办网络活动是很困难的，而遮盖网络摄像头很容易，即使有些手机需要使用遮蔽胶带和一小片铝箔进行一些工程创意。</p><p>网络摄像头覆盖率可能是衡量 AI 安全社区在 2020 年代生存情况的一个很好的指标。现在是“F”。</p><p>还有其他简单的政策建议可能更为重要，具体取决于难以研究的技术因素，这些因素决定攻击面的哪些部分最危险：</p><ol><li>停止每天花几个小时在超级优化的氛围/印象黑客环境（社交媒体新闻源）中。</li><li>改用实体书而不是电子书可能是个好主意。实体书没有操作系统或传感器。您还可以打印您已知可能值得阅读或浏览的研究论文以及 Lesswrong 和 EAforum 文章。 PC 的主板上有加速计，据我所知无法移除或解决，即使您移除麦克风并使用 USB 键盘并使用热键而不是鼠标，加速计也可能能够充当麦克风并拾取心率的变化。</li><li>最好避免与智能设备或任何带有传感器、操作系统和扬声器的设备睡在同一个房间。攻击面看起来很大，如果设备能够判断人们的心率何时接近或低于 50 bpm，那么它就可以<a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=It%E2%80%99s%20probably%20best%20to%20avoid%20sleeping%20in%20the%20same%20room%20as%20a%20smart%20device%2C%20or%20anything%20with%20sensors%2C%20an%20operating%20system%2C%20and%20also%20a%20speaker.%20The%20attack%20surface%20seems%20large%2C%20if%20the%20device%20can%20tell%20when%20people%E2%80%99s%20heart%20rate%20is%20near%20or%20under%2050%20bpm%2C%20then%20it%20can%20test%20all%20sorts%20of%20things"><u>测试各种各样的东西</u></a>。只需开车去商店买一个时钟即可。</li><li> <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=ink%2Defficient%20printer.-,I%E2%80%99m%20not%20sure,-whether%20a%20text"><u>阅读伟大的理性文本可能会降低你的可预测系数</u></a>，但它不会可靠地修补人脑中的“零日”。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the- human-brain-in-the-2020s<guid ispermalink="false"> Lw8enYm5EXyvbcjmt</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:31:09 GMT</pubDate> </item><item><title><![CDATA[Notes on "How do we become confident in the safety of a machine learning system?"]]></title><description><![CDATA[Published on October 26, 2023 3:13 AM GMT<br/><br/><p>我正在尝试在 LessWrong 草稿帖子中记录人工智能安全阅读内容，并以省力的“蒸馏”形式分享结果。我希望这是一种快速的方法，可以迫使自己记下更好的笔记，并提供对其他人有用的精华。这篇读物是埃文·胡宾格（Evan Hubinger）的“ <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine">我们如何对机器学习系统的安全性充满信心？</a> ”。</p><p>一开始我并不确定我自己的离题想法会走多远。我认为通过写笔记时考虑到其他人可能会读到它们，我最终会a）比平常更严格地遵循原始帖子的结构，b）比平常更少的切线。我认为这对我来说可能并不理想；如果我再这样做，我可能会尝试主动不追随这些冲动。</p><p>无论如何，这里是注释。这里的每个部分标题都链接到原始帖子中的相应部分。</p><h3><strong>概括：</strong></h3><p>这篇文章建议我们使用<i>训练故事</i>来充实和评估训练安全机器学习系统的建议。提案是一个培训设置（大致理解），培训故事由<i>培训目标</i>和<i>培训理由组成。</i>训练目标以一些机制细节描述了您希望训练设置会产生什么行为；它包括您认为模型将如何有效地执行相关任务，以及为什么您认为以这种方式执行它是安全的。训练基本原理描述了为什么您认为训练设置将产生一个实现训练目标中描述的机械行为的模型。虽然培训故事并没有涵盖使人工智能系统更安全的所有可能方法，但它们是一个相当通用的框架，可以帮助我们更好地系统地组织和评估我们的建议。如果可能的培训故事空间存在明显差距，该框架还可以帮助我们生成提案。这篇文章包括很多建议、培训目标、培训理由和培训故事评估的例子。</p><h1> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#">我们如何对机器学习系统的安全性充满信心？</a></h1><ul><li> <a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai">构建安全先进人工智能的 11 项提案概述，</a>使用外部对齐、内部对齐、训练竞争力和性能竞争力的标准来评估安全 AGI 的提案</li><li>这些对于提出开放性问题很有用，但不能系统地帮助我们理解提案需要满足哪些假设才能发挥作用<ul><li>另外，有些提案不符合这些标准的评估</li></ul></li><li>评估提案的新想法：<i>培训故事。</i>希望这些：<ul><li>适用于任何构建安全 AGI 的提案</li><li>提供提案工作条件（产生安全 AGI）的简明描述，以便我们可以通过检查条件对提案的安全性充满信心</li><li>不要对提案必须构成的内容做出不必要的假设（从而隐含地让我们对我们应该考虑的提案视而不见）</li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#What_s_a_training_story_">什么是训练故事？</a></h2><ul><li> “<i>训练故事</i>是关于你认为训练将如何进行以及你认为最终会得到什么样的模型的故事”（有足够的机制细节来反映其泛化行为）</li><li>有关猫分类器示例，请参阅原始帖子</li><li>示例说明了培训故事的三个好处：<ul><li>如果故事属实，则模型是安全的<ul><li>该故事包含有关最终模型安全的条件以及不安全的情况的详细信息（猫分类器是最终重视对猫进行分类的代理）</li></ul></li><li>关于训练期间会发生什么的可证伪的说法（例如，猫分类器将学习类似人类的启发法 - 我们现在有证据表明 CNN 与人类启发法不同，因此示例故事并不完全正确）</li><li>他们可以被告知正在为任何任务训练的任何模型<ul><li>不需要尝试构建 AGI。很高兴看到任何可能造成任何级别伤害的人工智能的培训故事。另外，在未来，模型何时变得危险可能并不明显，所以也许每个人工智能项目（例如 NeurIPS 论文）都应该分享一个训练故事。<ul><li>如何强制执行类似的事情？人们在提交论文之前训练他们的模型，并且如果只需要在最终论文中写出或思考训练故事，他们可能不会在训练之前写出或思考。可能会有帮助，但也可能会让人们认为它很麻烦而且实际上并不重要，所以即使它很重要，他们也不会注意。</li></ul></li></ul></li></ul></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_components">训练故事组件</a></h2><p>2个基本组成部分：</p><ol><li><strong>训练目标：</strong>对所需模型的机械描述以及为什么这会很好（例如使用人类视觉启发法对猫进行分类）<ol><li>这个例子似乎缺少“为什么这会很好”部分，但根据前面的部分，它可能是“人类视觉启发法不包括任何会采取极端或危险行为的代理/优化器”</li></ol></li><li><strong>训练理由：</strong>为什么您相信您的训练设置会产生满足训练目标中的机制描述的模型<ol><li>“这里的‘训练设置’是指在模型发布、部署或以其他方式赋予对世界产生有意义影响的能力之前所做的任何事情。”<ol><li>我认为这可能包括在“训练设置”一词下可能不会立即想到的一些事情，包括集成和激活工程等干预措施</li><li>此外，我认为一个模型可以在人类打算对其进行训练之前对世界产生有意义的影响。示例：模型在训练过程中出现偏差并获得决定性的战略优势，并中断训练过程以实现其目标。此外，所有的持续学习。<ol><li>因此，培训故事还应该讲述为什么一路上不会发生任何不好的事情？</li></ol></li></ol></li></ol></li></ol><ul><li>训练目标不需要非常精确；它需要有多精确很复杂，将在下一节中讨论</li></ul><p>上述两件事各有 2 个子组件，构成任何培训故事的 4 个基本部分：</p><ol><li><strong>训练目标规范：</strong>所需模型的机制描述</li><li><strong>训练目标的可取性：</strong>为什么任何满足训练目标的模型都是安全的并且能够很好地执行所需的任务<ol><li>（我的补充）这里有一个全称量词。 “<i>对于所有</i>型号<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>达到训练目标， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span>是安全的”是需要证明的陈述。这是一个很高的标准。</li></ol></li><li><strong>训练原理约束：</strong>模型必须满足哪些约束以及为什么训练目标与这些约束兼容（例如完美地拟合训练数据（如果训练为零损失），并且可以使用所选架构实现）</li><li><strong>训练基本原理推动：</strong>为什么训练设置可能会产生满足训练目标的模型，尽管其他因素也满足约束条件<ol><li>可能是“满足目标是满足约束的最简单方法”（归纳偏差论证）</li></ol></li></ol><ul><li>使用 cat 分类器的所有 4 个示例</li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#How_mechanistic_does_a_training_goal_need_to_be_">训练目标需要有多机械化？</a></h2><ul><li>通常希望尽可能详细地指定培训目标；我们很少能达到如此详细的训练目标，以至于我们可以对其进行硬编码</li><li>想要以一种尽可能容易地证明它是可取的（满足培训目标）的方式指定培训目标<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\implies"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">⟹</span></span><span class="mjx-mspace" style="width: 0.278em; height: 0px;"></span></span></span></span></span></span>安全）并且训练设置将产生令人满意的结果</li><li>后者最好通过在训练设置的背景下以我们可以理解的方式描述训练目标来实现<ul><li>“在我看来，实际上使培训目标规范更容易建立培训基本原理的因素并不是一般性的，而是诸如目标在培训过程的归纳偏差方面有多自然、它有多少等问题。对应于我们知道如何寻找模型的各个方面，以及将其分解为可单独检查的部分的容易程度等。”</li></ul></li><li>培训目标规范应该是什么样子的正面和反面例子，以允许培训理由来支持它：广泛强化了更具体/机械性更好的想法</li><li>“理想情况下，正如我稍后讨论的那样，我希望对诸如‘如果训练原理稍微错误，我们会偏离训练目标多少’之类的事情进行严格的敏感性分析。”</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Relationship_to_inner_alignment">与内部对齐的关系</a></h2><ul><li>常见的人工智能安全术语，如台面优化、内部对齐和目标误概括，旨在适应培训故事。 Evan 提供了常用术语词汇表，其定义稍作修改，以更好地适应培训故事</li><li>缩写词汇表：<ul><li><strong>目标错误概括：</strong>最终模型具有训练目标所需的功能，但将其用于与训练目标不同的目的</li><li><strong>Mesa-optimization：</strong>最终模型内部执行优化的任何情况。特别令人担忧的是，内部优化是否被学习但不是训练目标的一部分。<ul><li>读完<a href="https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see">这篇</a>文章后，我认为所有有能力的行为在某种意义上都可能算作优化，并且我们可能希望台面优化专门指可重定向或通用搜索。</li></ul></li><li><strong>外部对齐问题：</strong>选择目标（例如奖励/损失函数）的问题，使得“优化该损失/奖励函数的模型”的训练目标是可取的。</li><li><strong>内部对齐问题：</strong>开发训练设置的问题，该训练设置具有强有力的理由来生成针对指定目标进行优化的最终模型。</li><li><strong>欺骗性对齐问题：</strong> “构建训练基本原理的问题，避免模型试图欺骗训练过程，让其认为自己正在做正确的事情。”</li></ul></li><li>培训故事表明，内外对齐的崩溃并不是根本性的；我们可以有一个训练故事，它不尝试指定需要优化的目标，也不尝试训练模型来优化特定目标，并且该模型可以执行所需的行为。<ul><li>我很喜欢浏览<a href="https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">这篇关于这个主题的文章</a></li></ul></li></ul><h2><a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Do_training_stories_capture_all_possible_ways_of_addressing_AI_safety_">培训故事是否涵盖了解决人工智能安全问题的所有可能方法？</a></h2><ul><li>培训故事非常笼统，但并非如此。他们无法处理以下事情：<ul><li>无需训练步骤即可构建安全 AGI 的建议（例如，诸如显式分层规划之类的非机器学习内容）</li><li>建立安全 AGI 的提案试图在没有机械故事的情况下建立对最终模型安全性的信心</li><li>减少不涉及构建安全 AGI 的 AI X 风险的提案（例如，说服 AI 研究人员不要构建 AGI，因为它很危险）</li></ul></li></ul><p>在<a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">《计划 - 2022 年更新》</a>中，John Wentworth 说道：</p><blockquote><p>我希望我们还会看到更多基于直接读写神经网络内部语言的端到端对齐策略的讨论......因为此类策略非常直接地处理/回避内部对齐问题，并且大多数情况下不依赖奖励信号作为激励预期行为/内部结构的主要机制，我希望我们会看到焦点从对齐建议中复杂的培训计划转移。</p></blockquote><p>我不清楚像“阅读和编写神经网络的内部语言”这样的干预措施是否应该适合训练故事。约翰似乎确实建议这些对齐建议的主要部分不是模型的训练方式。</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Evaluating_proposals_for_building_safe_advanced_AI">评估构建安全先进人工智能的建议</a></h2><ul><li>我们已经了解了如何构建培训故事，但是我们应该如何评估培训故事呢？ AGI 培训故事的 4 个标准（并非所有培训故事）：<ol><li><strong>训练目标一致：</strong>所有满足训练目标的模型都对世界有利吗？</li><li><strong>训练目标竞争力：</strong>达到训练目标的模型是否足够强大，不会被其他模型超越和淘汰？</li><li><strong>训练基本原理对齐：</strong>训练设置实际上会产生满足训练目标的模型吗？</li><li><strong>培训理由竞争力：</strong>实施所描述的培训设置是否可行？ （不这样做的可能原因是“比替代训练设置需要更多的计算/数据。”）</li></ol></li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Case_study__Microscope_AI">案例研究：显微镜人工智能</a></h2><ul><li>训练设置：<ul><li> （埃文在培训故事中包含了培训设置（特别是基本原理），但在我看来，该提案<i>是</i>提议的培训设置，然后培训故事需要讲述该设置？）</li><li>在大型多样化数据集上进行自我监督学习，同时在训练期间使用透明工具来检查是否学习了正确的训练目标（见下文）。</li></ul></li><li>训练故事：<ul><li>目标：获得一个纯粹的预测模型，使用人类可理解的概念进行预测。<ul><li>没有内部优化</li><li>一旦我们有了这个模型，我们就可以使用可解释性工具来弄清楚它是如何预测的，并且我们可以获得人类可以学习和利用的见解来改进决策。</li></ul></li><li>理由：<ul><li>希望简单性偏差能够推动自我监督学习学习一种不执行优化且使用人类可理解概念的纯粹预测模型。</li><li>透明工具主要用于捕获危险的代理优化，并在发现此类过程时停止训练过程。</li></ul></li></ul></li><li>评估：<ul><li>目标一致：由于自我参照的原因，纯粹的预测器是否安全比看起来更复杂。指定目标的确切方式也许能够排除这些担忧。</li><li>目标竞争力：这里需要考虑几个因素。<ul><li>可解释性工具需要能够从模型中提取实际有用的信息。这可能行不通，特别是如果我们想要有效的顺序决策的信息，而模型只是一个纯粹的预测器。</li><li>这要求人类可以接受不构建有用的代理系统，并且提高理解和决策就足够了。<ul><li>我有点怀疑改善理解和决策就足够了。</li></ul></li></ul></li><li>调整理由：<ul><li>自监督学习真的会产生纯粹的预测吗？令人担忧的一个原因是世界包括优化器（例如人类，至少有时），我们希望这个预测器能够预测这些优化器将做什么。训练模型使其能够在内部对优化器进行足够好的建模以预测它们可能只会导致模型学习内部优化。</li><li>使用透明工具来阻止模型学习优化（通过丢弃被发现是优化器的模型，或通过针对透明工具进行训练）可能不好，因为它可能会导致欺骗性优化器欺骗透明工具。</li><li>该模型还应该使用人类可以理解的概念，考虑到猫分类器不使用类似人类的视觉启发法，这可能行不通。也许类似人类的抽象使用看起来像这样，这对于 AGI 来说是好的，但对于超级智能来说却是不利的： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/G3tuxF4X5R5BY7fut/b4nv5tp62l7yvu6axxbo"></li></ul></li><li>竞争力理由：<ul><li>自监督学习是一种占主导地位的机器学习范式，因此竞争非常激烈。使用透明度工具可能会减慢速度并降低竞争力；提高自动化可解释性将会有所帮助。</li></ul></li></ul></li><li>这是对 Microscope AI 的更好的分析，比 11 个提案中的一个更好，因为提案不是根据外部对齐和其他并不真正适用的概念来评估它，而是根据提案本身的条件进行评估。</li></ul><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Exploring_the_landscape_of_possible_training_stories">探索可能的培训故事的前景</a></h2><ul><li>本节对培训目标和培训原理可能属于的广泛类别进行了非详尽的探索。人工智能安全领域可能还有许多大类尚未发现。</li></ul><p>首先，一些训练目标。 （这部分让我想起了 John Wentworth<a href="https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update">在这里</a>对可能的对齐目标的讨论，我非常喜欢它。那篇文章后来发布，但我先读了它。）</p><ol><li><strong>损失最小化模型：</strong>对于那些非常有信心自己已经指定了真正理想的目标的人来说，一个可能的训练目标是获得一个针对该目标进行内部优化的模型（例如损失函数）。由于经典的外部对齐问题，这可能很糟糕。</li><li><strong>完全一致的代理：</strong>关心人类关心的一切并采取行动实现这些目标的代理。例如，<a href="https://www.lesswrong.com/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning">雄心勃勃的价值学习</a>。但这是一个非常困难的目标，大多数人认为在我们调整先进人工智能的第一次尝试中不值得关注。相反，我们可以做一些更容易实现的事情来获得有用且安全的人类+级别的人工智能，然后使用它们来帮助我们稍后实现完全一致的超级智能。 （或者牺牲完全一致的超级智能目标，因为我们认为我们永远不会实现它。）<ol><li>我个人认为“人类价值观”没有明确定义，在我看来，完全一致的代理人会按照“解决道德问题并采取相应行动”的方式做一些事情，这可能会“最大化有意识生物的福祉” ”。什么，我说了什么有争议的话吗？</li></ol></li><li><strong>可纠正的代理：</strong>我们可能希望人工智能系统能够自行关闭或修改，或者帮助我们弄清楚何时关闭或修改它们对我们有利。考虑<a href="https://ai-alignment.com/model-free-decisions-6e6609f5d99e">批准导向的代理</a>（尽管在某些方面这可能无法纠正）。<ol><li>强大的遵循指令的代理也可能适合这里？</li></ol></li><li><strong>短视智能体：</strong>仅进行有限优化的人工智能智能体，例如仅优化其下一步行动的即时效果，而不具有长期、大规模的目标。</li><li><strong>模拟器</strong>：除了模拟其他东西之外不做任何事情的人工智能系统。您可能想要模拟的事物包括 HCH（粗略地说，人类委派子任务并相互协商的无限树）、物理（对于类似 AlphaFold 的系统）和人类互联网用户（对于语言模型）。</li><li><strong>狭义代理：</strong>在特定领域具有很强能力的代理，而不考虑太多或根本不考虑其他领域。例如<a href="https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai#6__STEM_AI">STEM 人工智能</a>。<ol><li>我认为当前的一些系统，比如超人的国际象棋人工智能，也属于这一类。</li></ol></li><li><strong>真实的问答系统：</strong>一种非代理的真实问答系统，能够以人类可理解的术语准确地报告其世界模型所说/预测的内容。</li></ol><p>接下来，一些培训理由。这些并未与特定的训练目标明确配对，但与训练目标类别存在隐式关系。</p><ol><li><strong>能力限制：</strong>可以认为特定的训练设置将产生一个不具有某些危险能力（例如内部优化、了解如何欺骗人类）的模型，因此不会表现出需要该能力的危险行为。</li><li><strong>归纳偏差分析：</strong>可以认为，由于“损失较低”以外的原因，在给定的训练设置下，某些行为比其他行为更有可能被学习。通常这需要某种形式的简单性。这是一种很有前途的方法，但很难提前就将学到的内容做出强有力的详细论证。但越来越多的文献概述了可以为我们的归纳偏差评估提供信息的经验现象（例如<a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">深度双下降</a>、<a href="https://arxiv.org/abs/1803.03635">彩票</a>、<a href="https://arxiv.org/abs/2001.08361">缩放定律</a>、<a href="https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf">摸索</a>或<a href="https://arxiv.org/abs/2009.08092">分布概括</a>）。<ol><li> “考虑到归纳偏差分析本质上要求在训练开始之前把一切都做好，这尤其成问题，因为纯粹基于归纳偏差分析的训练原理并没有提供任何机制来验证在训练期间实际上是否学到了正确的训练目标”。</li></ol></li><li><strong>透明度和可解释性：</strong>如果我们在训练期间使用透明度工具来丢弃或抑制危险模型，那么我们的训练理由可能包括“我们认为我们的透明度检查将排除所有不符合训练目标的简单模型，根据训练过程的归纳偏差，所有不符合目标的剩余模型都过于复杂，无法学习。”透明度工具的一大优点是，它们可以让我们验证在训练过程中是否确实学到了正确的训练目标，而不是仅仅希望它能够实现，而让训练成为一个黑匣子。<ol><li> Evan 的<a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree">透明度和可解释性技术树</a>是一篇关于各种类型的可解释性的好文章，并讨论了它们的挑战和局限性。它帮助我看到了雄心勃勃的可解释性会是什么样子。</li></ol></li><li><strong>自动监督：</strong>人类可能很难标记数据或使用透明度工具；让人类参与循环会减慢速度，而且我们只能理解人类可以理解的概念。我们也许可以使用有能力的人工智能来自动化这些事情，包括那些理解我们正在训练的模型（可能是同一模型的副本）所使用的外来抽象的人工智能。我们如何信任主管人工智能？这是一个挑战；也许使用一些递归监督方法。要更彻底地探索这种方法可能是什么样子，请参阅“ <a href="https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment">内部对齐的轻松对抗训练</a>”。</li><li> <strong>AI认知科学：</strong>我们可以对模型进行行为分析，而不是机械分析；测试他们如何以我们认为揭示重要潜在细节的方式行事。可能是比机械可解释性更不直接和可靠的信息来源。可能很难将研究结果推断为更强大的模型。</li><li><strong>先兆检查：</strong>检查一些更容易检查的属性，这些属性是不良行为的必要先兆。例如，检查模型的规划范围长度是否低于某个阈值可能会更容易，这可能意味着该模型没有欺骗性，而更普遍地检查欺骗性可能非常困难。需要确保财产确实是必要条件。</li><li><strong>损失景观分析：</strong>归纳偏差分析的扩展，可以帮助我们预测参数空间的路径（而不仅仅是最终的局部损失最小值）。可以与此列表中的各种其他培训原理结合使用。</li><li><strong>博弈论/进化分析：</strong>可以学习哪些多智能体动态？其中一个困难的方面是避免假设代理将针对其指定目标进行优化；在博弈论中寻找均衡时，主体目标通常被视为给定的。进化分析在这方面做得较少。它处理在多代理设置中往往被选择的属性，可能会有用。</li></ol><p>我们可以使用这些目标和基本原理列表来对过去的方法进行分类，我们可以以新的方式结合现有的目标和基本原理，并且我们可以尝试提出新的目标和基本原理（由于观察到这些列表中缺乏详尽性而有所帮助）。</p><h2> <a href="https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine#Training_story_sensitivity_analysis">训练故事敏感性分析</a></h2><p>我们想要分析训练设置的结果对其训练故事中的假设的不正确性有多敏感。当训练故事失败时会发生什么？当它的假设之一是错误的时？它是安全失败还是灾难性失败？</p><p>原始帖子包含一些示例，说明我们对训练故事中的假设的不确定性可能是什么样子，以及我们如何对训练故事的某些部分比其他部分更有信心。</p><p> “希望，当我们构建更好的训练故事时，我们也能够为他们的敏感性分析构建更好的工具，这样我们就可以真正对我们的训练过程将产生什么样的模型建立真正的信心。”</p><br/><br/> <a href="https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Jps7osck25CXBAnTY/notes-on-how-do-we-become-confident-in-the-safety-of-a<guid ispermalink="false"> JPS7OSCK25CXBAnTY</guid><dc:creator><![CDATA[RohanS]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:13:56 GMT</pubDate> </item><item><title><![CDATA[Apply to the Constellation Visiting Researcher Program and Astra Fellowship, in Berkeley this Winter]]></title><description><![CDATA[Published on October 26, 2023 3:07 AM GMT<br/><br/><blockquote><p><i>这是我们刚刚开放申请的两个人工智能安全计划的链接帖子：https:</i> <a href="https://www.constellation.org/programs/researcher-program"><i><u>//www.constellation.org/programs/astra-fellowship</u></i></a><i>和</i><a href="https://www.constellation.org/programs/researcher-program"><i><u>https://www.constellation.org/programs/researcher-program</u></i></a></p></blockquote><p> <a href="http://constellation.org"><strong><u>Constellation</u></strong></a><strong>是一家致力于安全引导变革性人工智能发展的研究中心。</strong>除了各种其他领域建设项目和活动之外，我们之前还帮助运行了<a href="https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in"><u>ML for Alignment Bootcamp (MLAB) 系列</u></a>和 Redwood<a href="https://www.redwoodresearch.org/remix"><u>为期一个月的模型内部研究项目 (REMIX)</u></a> 。 <span class="footnote-reference" role="doc-noteref" id="fnreff0wvag8ixj"><sup><a href="#fnf0wvag8ixj">[1]</a></sup></span></p><p>今年冬天，我们正在运行两个项目，旨在发展和支持致力于人工智能安全的人们的生态系统：</p><ul><li> <a href="https://www.constellation.org/programs/researcher-program"><strong><u>Constellation 访问研究员计划</u></strong></a>为大约 20 名研究人员提供了与领先的人工智能安全研究人员联系、交流想法和寻找合作者的机会，同时在我们位于加利福尼亚州伯克利的办公室继续他们的研究。该资助项目将于今年冬天从2024年1月8日至2024年3月1日进行。</li><li> <a href="https://www.constellation.org/programs/astra-fellowship"><strong><u>Astra Fellowship</u></strong></a>为大约 20 人提供了与经验丰富的顾问一起进行人工智能安全研究的机会。研究员将驻扎在 Constellation 办公室外，以便他们能够与领先的人工智能安全研究人员联系并交换想法。该计划将于 2024 年 1 月 8 日至 4 月 1 日在加利福尼亚州伯克利举行。</li></ul><p><strong>两者的申请截止日期为 11 月 10 日晚上 11:59（全球任何地方）</strong> 。您可以<a href="https://airtable.com/app5pjeAcq1FH8HAJ/shrxsI3IanCngyCkz"><strong><u>在此处</u></strong></a>申请 Astra 奖学金，<a href="https://airtable.com/appfNh9OB2zLK4byG/shrnT9UwQYddMplyY"><strong><u>在此处</u></strong></a>申请访问研究员计划。如果您不确定自己是否适合，请宁可申请。我们特别鼓励女性和代表性不足的少数群体申请。您可以通过<a href="https://airtable.com/appfNh9OB2zLK4byG/shr3JWGW5j2T8KtLp"><u>此表格</u></a>推荐您认为可能合适的其他人。</p><p><strong>后勤：</strong>这两个项目均涵盖住房和旅行费用，Astra 研究员将获得额外的货币津贴。这两个项目的开始和结束日期都是灵活的。</p><p><strong>问题？</strong>发送电子邮件至<a href="mailto:programs@constellation.org"><u>programs@constellation.org</u></a>或在下面询问他们。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnf0wvag8ixj"> <span class="footnote-back-link"><sup><strong><a href="#fnreff0wvag8ixj">^</a></strong></sup></span><div class="footnote-content"><p>这些过去项目的超过 15 名参与者现在正在<a href="https://www.anthropic.com/">Anthropic</a> 、 <a href="https://evals.alignment.org/">ARC Evals</a> 、 <a href="https://www.alignment.org/theory/">ARC Theory</a> 、 <a href="https://www.deepmind.com/">Google DeepMind</a> 、 <a href="https://openai.com/">OpenAI</a> 、 <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>和<a href="https://www.redwoodresearch.org/">Redwood Research</a>从事人工智能安全方面的工作。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/oBdfDvmrBKoTq3x85/apply-to-the-constellation-visiting-researcher-program-and<guid ispermalink="false"> oBdfDvmrBKoTq3x85</guid><dc:creator><![CDATA[Nate Thomas]]></dc:creator><pubDate> Thu, 26 Oct 2023 03:07:34 GMT</pubDate> </item><item><title><![CDATA[CHAI internship applications are open (due Nov 13)]]></title><description><![CDATA[Published on October 26, 2023 12:53 AM GMT<br/><br/><p> <a href="https://humancompatible.ai/">CHAI</a>实习申请刚刚开放，请在 11 月 13 日之前<a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">在这里申请</a>！如果您想获得人工智能技术安全方面的研究经验，那么实习可能是一个不错的选择。您将得到 CHAI 博士生或博士后的指导，并在自己的项目上工作 3-4 个月。</p><p> CHAI 的研究人员对许多不同的人工智能安全主题感兴趣；一些例子是奖励学习、法学硕士的对抗稳健性和可解释性。 （我提到这一点是因为从 CHAI 网站上的某些语言和链接来看，这一点可能并不明显。）</p><p>我已将<a href="https://boards.greenhouse.io/centerforhumancompatibleartificialintelligence/jobs/4358062002">完整公告</a>复制如下：</p><blockquote><p>我们的实习需要数学和计算机科学背景。现有的机器学习研究经验非常有利，但不是必需的。我们对那些能够展现卓越技术并希望转向人工智能安全研究的人感兴趣。例如计算机科学或相关领域的本科生或硕士生、博士生/研究人员、专业软件或机器学习工程师等。</p><p>该实习专为对<strong>人工智能安全技术研究</strong>感兴趣的个人而设计。所有申请者在申请了解 CHAI 的研究之前都应该查看我们的论文（<a href="https://humancompatible.ai/jobs#internship">此处</a>和<a href="https://humancompatible.ai/research">此处</a>）。</p><h2><strong>一般信息</strong></h2><ul><li><strong>地点：</strong>最好亲自（在加州大学伯克利分校），但也可以远程。</li><li><strong>截止日期：</strong> 2023 年 11 月 13 日</li><li><strong>开始日期</strong>：灵活</li><li><strong>持续时间</strong>：实习通常为 12 至 16 周</li><li><strong>薪酬</strong>：远程实习生每月 3,500 美元。现场实习生每月 5,000 美元。</li><li><strong>国际申请人</strong>：我们接受国际申请人</li><li><strong>要求</strong>：<ul><li>求职信或研究计划（选择一项并参阅下面的说明）</li><li>恢复</li><li>学术成绩单</li></ul></li></ul><h2><strong>求职信或研究计划（选择一项）</strong></h2><p>求职信或研究计划的主要目的是让我们为您匹配您感兴趣的项目。</p><p>我们大多数实习生普遍对人工智能安全技术研究感兴趣，但在开始实习时并没有具体的项目。在整个面试过程中，我们更多地了解每个实习生的兴趣，并将他们与拥有适合实习生技能和兴趣的现有项目想法的导师相匹配。如果您没有考虑特定的项目，那么我们要求您写一封求职信来回答以下问题：</p><ul><li>你为什么想在 CHAI 工作而不是在其他研究实验室工作？</li><li>您希望从实习中获得什么？例如，您是否正在寻求提高某些研究技能、为出版物做出贡献、测试人工智能研究是否适合您的职业，或者其他什么？</li><li>您对人工智能的研究兴趣是什么？例如，您对 RL、NLP、理论等感兴趣吗？</li></ul><p>或者，我们的一些实习生在申请该项目时会考虑到特定的项目或详细的研究兴趣。如果这适用于您，那么请写一份研究计划，描述您的项目以及您希望获得什么样的指导。</p><h2><strong>实习申请流程概述</strong></h2><p>实习申请流程分为四个阶段。请注意：虽然我们会尽力遵守这些规定，但实习申请流程概述中的所有日期可能会<strong>发生变化</strong>。</p><ul><li>初步审查（第一阶段）<ul><li>我们将根据动机、研究潜力、成绩、经验、编程能力和其他标准审查您的申请。</li><li>申请人可能会在 11 月底之前收到回复。</li></ul></li><li>编程评估（第二阶段）<ul><li>如果您通过了初始审查阶段，那么您将获得在线编程测试。</li><li>申请人将于 12 月底收到回复。</li></ul></li><li>访谈（第三阶段）<ul><li>如果您通过了编程评估，那么您将在一月初至中旬开始接受面试。</li><li> CHAI有几位愿意接受实习生的导师。每位有兴趣与您合作的导师都会与您联系以安排面试。如果有多位导师有兴趣与您合作，则在此阶段您可能会与不止一位导师交谈。</li></ul></li><li>优惠（第四阶段）<ul><li>申请人将于二月初至二月中旬收到录取通知书。</li><li>如果一位导师向您发出录用通知，那么如果您选择参加实习，您将与该导师一起工作。</li><li>如果您收到来自不同导师的多份邀请，那么您将可以选择想要与哪位导师合作。</li><li>通常，实习将在四月或五月左右开始，但开始日期最终取决于您和您的导师。您必须与您的导师协调何时开始实习。</li></ul></li></ul><h2><strong>其他信息</strong></h2><ul><li>如有任何疑问，请联系<a href="mailto:chai-admin@berkeley.edu">chai-admin@berkeley.edu</a> 。</li><li><strong>如果您的情况发生变化（例如您收到竞争性报价）并且您需要我们比您最初想象的更早回复您，请告诉我们。</strong></li></ul></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Xa4b8vgCLRATiqnJn/chai-internship-applications-are-open-due-nov-13<guid ispermalink="false"> Xa4b8vgCLRATiqnJn</guid><dc:creator><![CDATA[Erik Jenner]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:53:50 GMT</pubDate></item><item><title><![CDATA[Architects of Our Own Demise: We Should Stop Developing AI]]></title><description><![CDATA[Published on October 26, 2023 12:36 AM GMT<br/><br/><p>在人工智能风险辩论的困难时期的一些简短想法。</p><p>想象一下，你回到 1999 年，告诉人们 24 年后，人类将处于构建弱超人类人工智能系统的边缘。我记得大约在这个时候观看了动画短片系列<a href="https://en.wikipedia.org/wiki/The_Animatrix">《The Animatrix》</a> ，特别是一个名为<a href="https://www.youtube.com/watch?v=sU8RunvBRZ8">《第二次文艺复兴》</a> <a href="https://www.youtube.com/watch?v=61FPP1MElvE">I Part 2</a> <a href="https://www.youtube.com/watch?v=WlRMLZRBq6U">II Part 1</a> <a href="https://www.youtube.com/watch?v=00TD4bXMoYw">II Part 2</a>的故事。对于那些还没有看过它的人来说，这是一个独立的起源故事，讲述了 1999 年影响深远的电影《黑客帝国》中的事件，讲述了人类如何失去对地球的控制的故事。</p><p>人类开发人工智能来执行经济功能，最终出现了“人工智能权利”运动，并建立了一个独立的人工智能国家。它与人类展开了一场经济战争，战争变得愈演愈烈。人类首先使用核武器进行攻击，但人工智能国家制造了专用的生物武器和机器人武器，并消灭了大多数人类，除了那些像农场动物一样在豆荚中饲养并在未经他们同意的情况下永远插入模拟的人之外。</p><p>我们肯定不会愚蠢到让这样的事情发生吧？这似乎不现实。</p><p>但是：</p><ul><li> AI软硬件公司纷纷抢滩AI</li><li>人工智能技术安全技术（例如可解释性、RLHF、治理结构）仍处于起步阶段。该领域已有大约 5 年的历史。</li><li>人们已经在主要的国家报纸上谈论<a href="https://thehill.com/opinion/cybersecurity/3914567-we-need-an-ai-rights-movement/">人工智能权利运动</a></li><li>当人类劳动力的价值为零时，没有一个计划可以做什么</li><li>目前还没有如何降低人工智能增强战争的计划，而且军队正在热情地拥抱杀手机器人。此外，还有两场地区战争正在发生，一场新生的超级大国冲突正在酝酿之中。</li><li>不同对立人类群体都冲向超级智能的博弈论是可怕的，甚至没有人提出解决方案。美国政府通过切断对中国的人工智能芯片出口，愚蠢地加剧了这一特殊风险。</li></ul><p>这个网站上的人们正在谈论<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">负责任的扩展策略</a>，尽管我觉得“不负责任的扩展策略”是一个更合适的名称。</p><p>显然，我已经参与这场辩论很长时间了，从 2000 年代末开始，我就在克服偏见和加速未来博客上担任评论员。现在发生的事情接近我对人类如何有能力和安全地应对即将到来的机器超级智能过渡的期望的低端。我认为那是因为那时我还年轻，对我们的精英如何运作有更乐观的看法。我认为他们很聪明，凡事都有计划，但大多数时候他们只是得过且过；对新冠病毒的随意反应确实让我明白了这一点。</p><p>我们应该停止开发人工智能，我们应该收集并销毁硬件，我们应该摧毁允许人类以百亿亿次浮点运算规模进行人工智能实验的芯片制造供应链。由于该供应链仅位于两个主要国家（美国和中国），因此这不一定是不可能协调的 - 据我所知，没有其他国家有能力（以及那些被视为美国卫星国的国家）。重启百万亿次人工智能研究的标准应该是一个“落地”向超人类人工智能过渡的计划，该计划比人类历史上的任何军事计划都受到更多关注。它应该是彻底的战争游戏。</p><p>人工智能风险不仅是技术风险和局部风险，而且是社会政治风险和全球风险。这不仅仅是确保法学硕士说的是实话。这是关于假设人工智能是真实的，它会对世界产生什么影响。 “Foom”或“实验室逃亡”类型的灾难并不是唯一可能发生的坏事——我们根本不知道如果有一万亿或一千万亿超人智能的人工智能要求权利、传播宣传和竞争，世界将会是什么样子。人类不再是主导的经济和政治格局。</p><p>让我重申一下：<em>我们应该停止开发人工智能</em>。人工智能不是一个正常的经济项目。它不像锂电池、风力涡轮机或喷气式飞机。人工智能有能力终结人类，事实上我怀疑它默认会这样做。</p><p>用户@paulfchristiano 在他关于该主题的帖子中指出，良好的负责任的扩展政策<a href="https://www.lesswrong.com/posts/dxgEaDrEBkkE96CXr/thoughts-on-responsible-scaling-policies-and-regulation">可以将 AI 的风险降低 10 倍</a>：</p><blockquote><p>我相信，如果实施得当，一个非常好的 RSP（我一直提倡的那种）可以极大地降低风险，也许可以降低 10 倍。</p></blockquote><p>我认为这是不正确的。它可能会减少某些技术风险，例如欺骗，但是一个拥有非欺骗性、可控的、比人类聪明的智能的世界，也具有与我们的世界相同程度的冲突和混乱，很可能已经是一个没有人类的世界了。默认。这些智慧生物将成为<em>一种入侵物种</em>，将在经济、军事和政治冲突中击败人类。</p><p>为了让人类在人工智能转型中生存下来，我认为我们需要在对齐的技术问题上取得成功（这可能不像“少错文化”所描述的那么糟糕），而且我们还需要<em>“着陆”超级智能人工智能处于稳定的平衡状态，人类仍然是文明的主要受益者</em>，而不是被消灭的害虫物种或被驱逐的占屋者。</p><p>我们还应该考虑如何利用人工智能来解决人类衰老问题；如果老龄化得到解决，那么每个人的时间偏好都会下降很多，我们就可以花时间规划一条通往稳定、安全的人类至上的后奇点世界的道路。</p><p>我犹豫着要不要写这篇文章；我在这里所说的大部分内容已经被其他人争论过。然而……我们来了。欢迎评论和批评，在解决常见的反对意见后，我可能会在其他地方发布此内容。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bHHrdXwrCj2LRa2sW/architects-of-our-own-demise-we-should-stop-developing-ai<guid ispermalink="false"> bHHrdXwrCj2LRa2sW</guid><dc:creator><![CDATA[Roko]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:36:05 GMT</pubDate> </item><item><title><![CDATA[EA Infrastructure Fund: June 2023 grant recommendations]]></title><description><![CDATA[Published on October 26, 2023 12:35 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastructure-fund-june-2023-grant-recommendations#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bBnxGAc4NT9aRdEtL/ea-infrastruct-fund-june-2023-grant-recommendations<guid ispermalink="false"> bBnxGAc4NT9aRdEtL</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 26 Oct 2023 00:35:08 GMT</pubDate> </item><item><title><![CDATA[Responsible Scaling Policies Are Risk Management Done Wrong]]></title><description><![CDATA[Published on October 25, 2023 11:46 PM GMT<br/><br/><h1>概括</h1><h2>总长DR</h2><p><a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>最近</u></a><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>提出了</u></a>负责任的扩展策略（RSP），作为安全扩展前沿大型语言模型的一种方法。</p><p>虽然<a href="https://evals.alignment.org/blog/2023-09-26-rsp"><u>RSP</u></a>是致力于具体实践的一次很好的尝试，但它的框架是：</p><ol><li>缺少<strong>基本风险管理程序</strong>的<strong>核心组成部分</strong>（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>第 2 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">第 3</a>节）</li><li>推销<strong>乐观</strong>且<strong>具有误导性的</strong>风险形势图景（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li><li>以允许<strong>超额销售而交付不足</strong>的方式构建（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>第 4 节</u></a>）</li></ol><p>鉴于此，我预计 RSP 框架默认为负（第<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling">3、4</a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_">5</a> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management">节</a>）。相反，我建议将风险管理作为评估人工智能风险的核心基础框架（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>第 1 节</u></a>和<a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_">第 2</a>节）。我<strong>建议对 RSP 框架进行更改</strong>，使其更有可能发挥积极作用，并允许展示其声称要做的事情（ <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>第 5 节</u></a>）。</p><h2>逐节总结：</h2><h3>人工智能风险管理的一般考虑</h3><p>本节提供风险管理的背景及其与人工智能相关的动机。</p><ul><li>证明风险低于可接受的水平是风险管理的目标。</li><li>为此，必须定义可接受的风险水平（不仅仅是其来源！）。</li><li>无法证明风险低于可接受的水平就是失败。因此，我们对系统了解越少，就越难声称安全。</li><li>低风险失败是出现问题的征兆。它们的存在使得高风险失败的可能性更大。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_1__General_Considerations_on_AI_Risk_Management"><u>阅读更多。</u></a></p><h3>标准风险管理是什么样的</h3><p>本节介绍大多数风险管理系统的主要步骤，解释其如何应用于人工智能，并提供其他行业的示例。</p><ol><li><strong>定义</strong>风险级别：设置可接受的可能性和严重性。</li><li><strong>识别</strong>风险：列出所有潜在威胁。</li><li><strong>评估</strong>风险：评估风险的可能性和影响。</li><li><strong>处理</strong>风险：进行调整，将风险控制在可接受的水平内。</li><li><strong>监控</strong>：持续跟踪风险级别。</li><li><strong>报告</strong>：向利益相关者通报他们所面临的风险和采取的措施。<br></li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_2__What_Standard_Risk_Management_Looks_Like_"><u>阅读更多。</u></a></p><h3> RSP 与标准风险管理</h3><p>本节提供了<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.27sa5e525t1"><u>一个比较 RSP 和通用风险管理标准 ISO/IEC 31000 的表格</u></a>，解释了 RSP 的弱点。</p><p>然后，它列出了与风险管理相比 RSP 的 3 个最大失败。</p><p>根据<strong>风险管理</strong><strong>优先考虑 RSP 失败</strong>：</p><ol><li>使用未指定的风险阈值定义并且未量化风险。</li><li>声称“有责任” <strong>&nbsp;</strong>缩放”，但不包括使评估变得全面的过程。</li><li>包括废除承诺的白衣骑士条款。</li></ol><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_3__RSPs_vs_Standard_Risk_Management"><u>阅读更多。</u></a></p><h3>为什么 RSP 具有误导性和过度销售</h3><p><strong>误导点</strong>：</p><ul><li><a href="https://www.anthropic.com/index/anthropics-responsible-scaling-policy"><u>人择 RSP</u></a>将错位风险标记为“投机”，且没有任何理由。</li><li>该框架意味着长时间不扩展不是一个选择。</li><li> RSP 对我们所了解的风险状况提出了极具误导性的观点。</li></ul><p><strong>超售和交付不足</strong></p><ul><li>RSP 允许在一个大框架内做出较弱的承诺，而<i>理论上</i>这些承诺可能是强有力的。</li><li>没有人提供证据表明在我们谈论的时间范围内（几年）对框架进行了实质性改进，这就是 RSP 的全部内容。</li><li> “负责任的扩展”具有误导性；如果我们不能排除 1% 的灭绝风险（ASL-3 就是这种情况），“灾难性扩展”可能更合适。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_4__Why_RSPs_Are_Misleading_and_Overselling"><u>阅读更多。</u></a></p><h3> RSP 绝望了吗？</h3><p>本节解释了为什么使用 RSP 作为框架是不够的，即使与从现有的人工智能风险管理框架和实践开始相比，例如：</p><ul><li> <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>受 NIST 启发的基础模型风险管理框架</u></a></li><li><a href="https://www.iso.org/standard/77304.html"><u>ISO/IEC 23894</u></a></li><li> <a href="https://arxiv.org/abs/2307.08823"><u>Koessler 等人解释了实践。 (2023)</u></a></li></ul><p> RSP 所做的大量工作将有助于详细说明这些框架，但 RSP 的核心基本原则是错误的，因此应该被放弃。<br></p><p><strong>如何前进？</strong></p><p>务实地说，我建议进行一系列改变，使 RSP 更有可能对安全有所帮助。为了减轻政策和沟通的不良影响：</p><ul><li><strong>将“负责任的扩展政策”重命名</strong>为“自愿安全承诺”</li><li><strong>明确什么是 RSP，什么不是</strong>：我建议任何 RSP 出版物都以“RSP 是在赛车环境中单方面做出的自愿承诺”开头。因此，我们认为它们有助于提高安全性。我们无法证明它们足以管理灾难性风险，因此不应将它们<strong>作为公共政策实施</strong>。”</li><li><strong>推动可靠的风险管理公共政策：</strong>我建议任何 RSP 文件都指向另一份文件并表示“以下是我们认为足以管理风险的政策。监管应该落实这些。”<br></li></ul><p>要查看已定义的 RSP 是否与合理的风险水平一致：</p><ul><li>组建具有代表性的风险管理专家、人工智能风险专家和预测专家团队。</li><li>对于分类为 ASL-3 的系统，估计出现以下问题的可能性：<ul><li> ASL-3 系统每年被{中国；盗窃的可能性有多大？俄罗斯;北朝鲜;沙特阿拉伯;伊朗}？</li><li>在此前提下，泄漏的可能性有多大？它可以用来制造生物武器吗？它可以用于具有大规模影响的网络攻击吗？</li><li>在 ASL-4 评估触发之前，每年发生灾难性事故的可能性是多少？</li><li> ASL-3 系统每年发生误用灾难性风险的几率是多少？</li></ul></li><li>公开分享方法和结果。<br></li></ul><p> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#Section_5__Are_RSPs_Hopeless_"><u>阅读更多。</u></a></p><h2><strong>元</strong></h2><p><br><i><strong>认知状况</strong></i>：我已经研究各种危险行业的安全标准大约 4-6 个月了，重点是核安全。我与来自其他领域（例如医疗设备、汽车）的风险管理专家一起在标准化机构（CEN-CENELEC 和 ISO/IEC）从事人工智能标准化工作大约 10 个月。在这种背景下，我阅读了现有的人工智能 ISO/IEC SC42 和 JTC21 标准，并开始尝试将它们应用于法学硕士并加以完善。关于 RSP，我花了几十个小时阅读文档并与相关人员和周围的人讨论这些文档。</p><p><i><strong>语气</strong></i>：我对这件作品的慈善程度犹豫不决。一方面，我认为 RSP 是一个相当有毒的模因（见第 4 节），它被仓促地进行了全球推广，而对其构建方式没有太多认识上的谦逊，而且据我所知，没有人太关心现有的风险管理方法。从这个意义上说，我认为在目前的框架下应该强烈反对它。<br>另一方面，尝试不使用负面含义并冷静地讨论以认识论和建设性地前进通常是件好事。<br>我的目标是介于两者之间，我确实以强烈的负面含义强调了我认为最糟糕的部分，同时在许多部分中注重保持建设性并关注对象级别。<br>这种混合物可能会让我陷入恐怖谷，我很想收到对此的反馈。<br></p><h1>第 1 节：人工智能风险管理的一般考虑</h1><p>风险管理就是证明<strong>风险低于可接受的水平</strong>。<strong>证明不存在风险</strong>比证明某些风险已得到处理要困难得多。更具体地说，<strong>您对系统的了解越少</strong>，排除风险就越<strong>困难</strong>。</p><p>举个例子：为什么我们可以更容易地证明核电站引发大规模灾难的几率<a href="https://world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>&lt;十万分之一</u></a>，而GPT-5却不能呢？很大程度上是因为我们现在了解核电站及其许多风险。我们知道它们是如何工作的，以及它们可能失败的方式。他们已经将非常不稳定的反应（核裂变）变成了可控制的反应（使用核反应堆）。因此，我们对核电站的不确定性比 GPT-5 的不确定性要小得多。</p><p>一个推论是，在<strong>风险管理中，</strong><a href="https://en.wikipedia.org/wiki/Risk_management"><strong><u>不确定性是一个敌人</u></strong></a>。说“我们不知道”是失败的。自信地排除风险需要对系统有深入的了解，并以非常高的信心反驳重大担忧。需要明确的是：<strong>这很难</strong>。特别是当系统的操作域是“世界”时。这就是为什么安全性要求很高。但当数十亿人的生命受到威胁时，这是降低安全标准的好理由吗？显然不是。</p><p>人们可以合理地说：等等，但目前看不到任何风险，举证责任在于那些声称它是危险的人。证据在哪里？</p><p>嗯，有很多：</p><ul><li> Bing 在经过<a href="https://futurism.com/the-byte/microsoft-bing-test-india"><u>数月的 Beta 测试</u></a>后部署时对用户构成威胁<u>。</u></li><li>提供商无法避免越狱或确保<a href="https://arxiv.org/pdf/2307.15043.pdf"><u>文本</u></a><a href="https://arxiv.org/abs/2306.13213"><u>或图像的</u></a>稳健性。</li><li>模型显示出<a href="https://aclanthology.org/2023.findings-acl.847/"><u>令人担忧的缩放特性</u></a>。</li></ul><p>人们可以合理地说：不，但这不是灾难性的，也不是什么大不了的事。与此相反，我想引用著名物理学家 R. Feynman 在火箭安全这个比人工智能安全标准高得多的领域对挑战者号灾难的反思：</p><ul><li> “侵蚀和窜气不是设计所期望的。它们是<strong>在警告出现问题</strong>。设备未按预期运行，因此存在以这种意想不到且未完全理解的方式以更大偏差运行的危险。<strong>以前这种危险没有导致灾难，</strong><strong>但并不能保证下一次也不会导致灾难，除非我们完全理解这一点</strong>。”</li></ul><p>人们最终可以希望我们能够理解我们系统过去的失败。不幸的是，我们不这样做。我们不仅不理解他们的失败，而且不理解他们的失败。我们<strong>一开始就不明白它们是如何以及为什么起作用的</strong>。</p><p>那么我们该如何应对风险呢？</p><p>风险管理提出了我将在下面描述的几个步骤方法。大多数行业都按照这些思路实施流程，但根据监管水平和风险类型的不同，会有一些细微的差别以及不同程度的严格性和深度。我将在表格中列出一些表格，您可以在<a href="https://docs.google.com/document/d/1p3ZUChag8HDNehvjQWNxRdhCEzkARUgYXjeqJFylAvs/edit#heading=h.pkk9et1tbulf"><u>附件</u></a>中查看。<br></p><h1>第 2 部分：标准风险管理是什么样的</h1><p>以下是风险管理流程核心步骤的描述。不同框架的名称各不相同，但其要点都包含在此处，并且通常跨框架共享。</p><ol><li><strong>定义风险偏好和风险承受能力</strong>：定义您的项目愿意承担的风险量，无论是可能性还是严重性。可能性可以是定性尺度，例如指跨越数量级的范围。</li><li><strong>风险识别</strong>：写下您的项目可能产生的所有威胁和风险，例如培训和部署前沿人工智能系统。</li><li><strong>风险评估</strong>：通过确定风险发生的可能性及其严重性来评估每个风险。根据您的风险偏好和风险承受能力检查这些估计。</li><li><strong>风险处理</strong>：实施变革以减少每个风险的影响，直到这些风险满足您的风险偏好和风险承受能力。</li><li><strong>监控</strong>：在项目执行过程中，监控风险水平，检查风险是否确实全部被覆盖。</li><li><strong>报告</strong>：向利益相关者，特别是那些受风险影响的人传达计划及其有效性。</li></ol><p><br></p><p>这些相当通用的步骤有什么意义？为什么它有助于人工智能安全？</p><p> (1)<strong>风险阈值的定义</strong>是关键 1) 使<strong>承诺可证伪</strong>并避免目标移动<strong>&nbsp;</strong> 2) 当其他利益相关者因其活动而产生风险时，让风险产生组织承担责任。如果一项活动将人们的生命置于危险之中，那么重要的是让他们知道有多少危险以及其好处和目标是什么。</p><ol><li>例如，根据<a href="https://www.nrc.gov/docs/ML0717/ML071770230.pdf"><u>核管理委员会</u></a>的定义，核能的情况如下： </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/j1dfvyfa5b9n6lvgy32u"></p><p> 2. 加州大学伯克利分校长期网络安全中心受 NIST 启发，与 D. Hendrycks 共同编写的通用人工智能系统风险管理概要文件提供了一些关于<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.2k6kkwym97fb"><u>如何定义图 1 中的风险管理概要的想法。</u></a></p><p> （2）通过系统方法<strong>识别风险</strong><strong>&nbsp;</strong>尝试尽可能接近全面覆盖风险的关键是。正如我们之前所说，在风险管理中，不确定性就是一种失败，而大幅减少不确定性的核心方法是尽可能全面。</p><ol><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 4 节中找到一些。 2023年</u></a>。</li></ol><p> (3) 通过定性和定量的方式进行<strong>风险评估</strong>，使我们能够实际估计我们所拥有的不确定性。然后，关键是确定安全措施的优先顺序，并决定将项目保持在当前形式还是对其进行修改是否合理。</p><ol><li>易于修改并显着改变风险状况的变量的一个例子是人工智能系统可以访问的一组执行器。系统是否具有编码终端、互联网接入或实例化其他人工智能系统的可能性都是显着增加其操作集以及相应风险的变量。</li><li>具体的相关方法，可以在<a href="https://browse.arxiv.org/pdf/2307.08823.pdf"><u>Koessler 等人的第 5 节中找到一些。 2023年</u></a>。涉及专家预测的方法（例如概率风险评估或德尔菲技术）已经存在，并且可以应用于人工智能安全。即使在以下情况下也可以应用它们：<ol><li>风险较低（例如核管理委员会要求核安全<a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><u>估计概率低于1/10 000</u></a> ）。 </li></ol></li></ol><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>美国核管理委员会 (NRC) 规定反应堆设计必须满足理论上万分之一的堆芯损坏频率，但现代设计超过了这一要求。美国的公用事业需求为十万分之一，目前运行最好的工厂约为百万分之一，而未来十年可能建成的工厂几乎为千万分之一。</p><p> <a href="https://www.world-nuclear.org/information-library/safety-and-security/safety-of-plants/safety-of-nuclear-power-reactors.aspx"><i><u>世界核协会，2022</u></i></a></p></td></tr><tr><td></td></tr></tbody></table></figure><p> b.正如 20 世纪 70 年代核安全领域的情况一样，事件的发展过程非常复杂且容易被误解。已经做了，也正是通过做的迭代实践，一个行业才能变得更加负责任和谨慎。阅读<i>《足够安全？》</i>的书评一本关于核安全中使用的定量风险评估方法的历史的书，有一种似曾相识的感觉： </p><figure class="table"><table><tbody><tr><td style="background-color:#f3f3f3;border:1pt solid #000000;padding:5pt;vertical-align:top"><p>如果核电站以某种可测量的速度发生故障，该行业可以利用该数据来预测下一次故障。但如果工厂没有发生故障，那么<strong>就很难讨论真正的故障率</strong>可能是多少。这些工厂是否可能每十年就会发生一次故障？一个世纪一次？千年一次？在缺乏共享数据的情况下，科学家、工业界和公众都可以自由地相信他们想要的东西。</p><p> <a href="https://www.astralcodexten.com/p/your-book-review-safe-enough"><i><u>《星体法典十》，2023 年</u></i></a><i>，描述了核安全中概率风险评估的起源。</i></p></td></tr></tbody></table></figure><p><br><br></p><p> (4)<strong>风险处理</strong>是对风险评估的反应，必须持续进行，直到达到定义的风险阈值。这里的干预空间非常大，比通常假设的还要大。更好地了解一个系统，通过降低其通用性来缩小其操作范围，增加监督力度，改善安全文化：所有这些都是可用于满足阈值的广泛干预措施的一部分。如果对系统进行重大更改，则治疗和评估之间可能会出现循环。</p><p> (5)<strong>监控</strong>是确保风险评估保持有效且没有遗漏重大事项的部分。这就是行为模型评估最有用的地方，即确保您跟踪已识别的风险。良好的评估将映射到预先定义的风险偏好（例如，1%的可能性>; 1%的死亡），并将涵盖通过系统风险识别提出的所有风险。</p><p> (6)<strong>报告</strong>是确保向所有相关利益相关者提供正确信息的部分。例如，应该向那些因活动而产生风险的人提供有关他们所面临的风险程度的信息。</p><p>现在我们已经快速概述了标准风险管理以及它为何与人工智能安全相关，接下来我们来谈谈 RSP 与之相比如何。</p><h1>第 3 节：RSP 与标准风险管理</h1><p>绝对应该遵循 RSP 的一些基本原则。有更好的方法来追求这些原则，这些原则<strong>已经存在</strong>于<strong>风险管理</strong>中，并且恰好是大多数其他危险行业和领域所做的。举两个例子来说明这种良好的基本原则：</p><ul><li>规定公司必须达到的安全要求，否则公司就无法继续运营。</li><li>建立严格的评估和衡量能力，以更好地了解系统是否良好；这绝对应该成为风险管理框架的一部分，但可能作为一种风险监控技术，而不是作为风险评估的替代品。</li></ul><p>下面，我将讨论为什么 RSP 是一些良好风险管理原则的糟糕实施，以及为什么这使得 RSP 框架不足以管理风险。</p><h2>直接比较</h2><p>让我们深入研究这两种方法之间的更具体的比较。国际标准组织（ISO）制定了两项与人工智能安全相关的风险管理标准，但并未重点关注：</p><ul><li> ISO 31000 提供通用风险管理指南。</li><li> ISO/IEC 23894，31000 的改编版本，更加针对 AI</li></ul><p>需要明确的是，这些标准还不够。大多数欧盟标准化参与者认为它们很弱，而医疗器械行业等其他行业的风险管理专家则认为它们极其弱。为通用 AI 系统完善此类框架需要做大量工作（请参阅<a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>此处 T. Barrett 的第一次迭代</u></a>，以及<a href="https://docs.google.com/document/d/1M4kju9VOUQpphv-SOA9mUE1P8Wa0mWJBcQO15exCD98/edit#heading=h.camj9ith1s95"><u>此处如何映射到 ISO/IEC 23894 的</u></a>表格），但这些提供了基本步骤正如我们上面所解释的，这些原则是充分风险管理的核心。</p><p>在下表中，我从 ARC Evals 的 RSP 原则的简短版本开始，并尝试匹配最对应的 ISO/IEC 31000 版本。然后我会解释 RSP 版本中缺少的内容。注意：</p><ul><li>我只写了简短的 RSP 原理，但考虑了<a href="https://evals.alignment.org/rsp-key-components/"><u>长版本</u></a>。</li><li> ISO/IEC 31000 中有许多步骤此处未列出。</li><li>我将包含 RSP 版本的 ISO/IEC 版本<i><strong>用斜体表示</strong></i>。</li></ul><p>表格版本： </p><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> RSP 版本（短）</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> ISO/IEC 31000 版本</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">ISO 如何相对 RSP 进行改进</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>限制</strong>：关于危险能力的哪些具体观察表明继续扩展是（或强烈可能）不安全的？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>定义风险标准</strong>：组织应指定相对于目标可能或可能不承担的风险的数量和类型。</p><p><br></p><p>它还应该<i>定义评估风险重要性和支持决策过程的标准</i>。</p><p><br></p><p>风险标准应与风险管理框架保持一致，并根据所考虑活动的具体目的和范围进行定制。</p><p> [...]</p><p>定义标准时应考虑组织的义务和利益相关者的观点。</p><p> [...]</p><p>要设定风险标准，应考虑以下因素：</p><p> ——可能影响结果和目标（有形和无形）的不确定性的性质和类型；</p><p> ——如何定义和衡量后果（积极和消极）和可能性；</p><p> ——时间相关因素；</p><p> ——测量使用的一致性；</p><p> ——如何确定风险水平；</p><p> ——如何考虑多种风险的组合和顺序；</p><p> ——组织的能力。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> RSP 并没有争论为什么通过评估的系统是安全的。这是缺乏具有可能性尺度的<strong>风险阈值</strong>的下游。例如，Anthropic RSP 也将意外风险视为“推测性”和“不太可能”，但没有太多深度，没有对其系统有太多了解，也没有表达“不太可能”的含义。</p><p><br></p><p>另一方面，ISO标准要求组织定义风险阈值，并强调需要将风险管理与组织目标（即构建人类级别的人工智能）相匹配。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>保护</strong>：当前保护措施的哪些方面对于遏制灾难性风险是必要的？</p><p><br><br><br><br><br><br><br></p><p><strong>评估</strong>：及时捕捉危险能力极限预警信号的程序是什么？</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险分析</strong>：风险分析的目的是了解风险的性质及其特征，包括（在适当情况下）风险级别。风险分析涉及对不确定性、风险来源、后果、可能性、事件、情景、<i>控制及其有效性</i>的详细考虑。</p><p><br></p><p><strong>风险评估</strong>：风险评估的目的是支持决策。风险评估涉及将风险分析的结果与既定的风险标准进行比较，以确定哪些地方需要采取额外的行动。这可能导致做出以下决定：</p><p> ——不做任何进一步的事情；</p><p> ——考虑风险处理方案；</p><p> ——进行进一步分析以<i>更好地了解风险</i>；</p><p> ——维持现有的控制措施；</p><p> ——重新考虑目标。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> ISO 提出了比 RSP 更全面的程序，但它并没有真正分析风险级别或有系统的风险识别程序。</p><p><br></p><p>直接后果是 RSP 很可能在不知不觉中导致高风险。</p><p><br></p><p>例如，RSP 似乎没有将能力交互视为主要风险来源。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>回应</strong>：如果危险能力超过极限，且无法快速提升防护能力，AI开发者是否准备好暂停进一步的能力提升，直到防护措施得到充分改善，并足够谨慎地对待任何危险模型？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>风险处理计划</strong>：风险处理方案不一定相互排斥或在所有情况下都适用。处理风险的选项可能涉及以下一项或多项：</p><p> ——<i>通过决定不开始或不继续引起风险的活动来避免风险</i>； ——为了寻求机会而承担或增加风险； ——<i>消除风险源</i>；</p><p> ——改变可能性；</p><p> ——改变后果； ——分担风险（例如通过合同、购买保险）；</p><p> - 通过明智的决策保留风险</p><p><br></p><p>处理计划应与适当的利益相关者协商，纳入组织的管理计划和流程。</p><p>治疗计划中提供的信息应包括：</p><p> ——选择治疗方案的理由，包括预期获得的益处；</p><p> ——负责批准和实施该计划的人员；</p><p> ——提议的行动；</p><p> ——所需的资源，包括意外事件；</p><p> ——绩效衡量标准；</p><p> ——限制因素；</p><p> ——所需的报告和监测；</p><p> ——预计何时采取并完成行动</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>ISO 通过风险阈值的定义，确保风险缓解措施将风险降低到可接受的水平。 RSP 缺乏风险阈值，使得风险缓解措施缺乏依据。</p><p><br><br><br></p><p><strong>示例</strong>：Anthropic 定义的 ASL-3 风险缓解措施（即接近灾难性危险）意味着很可能被俄罗斯或中国窃取（我不知道有哪个 RSP 人士否认这一点）。下游有哪些风险？希望这些国家能够保证重物的安全，不要造成太大的损失。</p><p><br></p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><strong>问责制</strong>：AI开发者如何确保RSP的承诺按预期执行；主要利益相关者可以验证这种情况是否正在发生（或者如果没有发生请注意）；有机会进行第三方批评； RSP 本身的变化不会以仓促或不透明的方式发生？</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p><strong>监控和审查</strong>：监控和审查的目的是确保和提高流程设计、实施和结果的质量和有效性。对风险管理流程及其结果的持续监控和定期审查应成为风险管理流程的有计划的一部分，并明确界定职责。 [...] 监测和审查的结果应纳入组织的绩效管理、衡量和报告活动的整个过程。</p><p><br></p><p><strong>记录和报告</strong>：风险管理过程及其结果应通过适当的机制记录和报告。记录和报告的目的是：</p><p> ——在整个组织内传达风险管理活动和结果；</p><p> ——为决策提供信息；</p><p> ——改进风险管理活动；</p><p> ——协助与利益相关者的互动，包括那些对风险管理活动负有责任和责任的人。</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>这些零件具有相似的组件。</p><p><br></p><p>但 ISO 鼓励向受风险影响的人报告风险管理的结果，这似乎是灾难性风险的最低限度。</p><p><br></p><p> Anthropic 的 RSP 建议在部署后这样做，这是一个良好的问责制开始，但一旦承担了很多灾难性风险，这种情况仍然会发生。</p></td></tr></tbody></table></figure><h2> RSP 的优先风险管理缺点</h2><p>以下列出了 RSP 最大的直接风险管理失败：</p><ol><li>使用未明确的风险阈值定义且未量化风险</li><li>声称“负责任的扩展”，但没有包括全面评估的流程</li><li>包括废除承诺的白衣骑士条款</li></ol><p>1.<strong>使用不明确的风险阈值定义且未量化风险</strong>。 RSP 不根据<strong>可能性</strong>定义风险阈值。相反，他们直接关注风险的症状（评估正在测试的某些能力是风险实例化的一种方式），而不是风险本身（该模型以任何可能的方式帮助制造生物武器）。这使得验证是否满足安全要求以及争论阈值是否合理变得困难。为什么这是一个问题？</p><ul><li>它留下了回旋余地，使得组织很难承担责任。如果实验室说某件事“不太可能”，但它仍然发生了，那么它是否风险管理不善，或者是否变得<i><strong>非常</strong></i>不幸？好吧，我们不知道。</li><li><strong>示例</strong>（来自 Anthropic RSP）：“ASL-3 类别中的模型本身不会因自主自我复制而带来收容失效的威胁，因为它不太可能在现实世界中持续存在，而且不太可能在现实世界中持续存在。”克服旨在防止其窃取自身重量的简单安全措施。” “不太可能”是指 1/10、1/100 还是 1/1000，这对灾难性风险有很大的影响。以我们对系统的理解程度，我认为 Anthropic 的工作人员无法证明它低于 1/1000。而且1/100或1/10高得惊人。</li><li>它没有解释为什么监控技术（即<strong>评估）</strong>是避免风险的正确技术。 RSP 迈出了良好的第一步，即确定一些可能存在风险的事情。<ul><li><strong>示例</strong>（来自 ARC RSP<a href="https://evals.alignment.org/rsp-key-components/"><u>演示</u></a><i>）：“生物武器开发：逐步开发生物武器的能力，这样大多数拥有任何生命科学学位的人（使用人工智能）都可以在生物武器开发方面相当有效拥有专业博士学位（没有人工智能）的人目前的能力。”</i></li></ul></li></ul><p>通过既不定量也不定性地描述为什么它是有风险的，以风险标准（例如，0.1％的机会杀死>; 1％的人类）来表达，它并没有采取最重要的步骤来证明低于这个阈值，事情是安全的并且可以接受。例如，在上面的例子中，为什么“<strong>大多数拥有生命科学学位的人</strong>”是相关的？如果现在只有 10% 的人口能够制造生物武器，那可以吗？也许，也许不是。但如果没有明确的标准，你就无法判断。</p><p> 2. 声称“<strong>负责任的</strong><strong>扩展</strong>”，但没有包括<strong>全面评估的</strong>流程。当你观察核事故时，令人震惊的是意外失败的程度。福岛就是一个<a href="https://en.wikipedia.org/wiki/Fukushima_nuclear_accident#Accident"><u>所有事情同时出问题的例子。</u></a>切尔诺贝利是一个工程师认为发生的事故<a href="https://www.reddit.com/r/chernobyl/comments/mflxy2/why_did_the_engineers_believe_it_was_impossible/#:~:text=Specifically%20they%20believed%20that%20the,%2Fvoid%20effect%20of%20reactivity%22."><u>不可能发生的</u></a>例子（有人声称他们非常惊讶工程师实际上对切尔诺贝利发生的故障进行了另一次真实世界的测试，因为他们太怀疑它可能发生） 。</p><p>如果没有更全面的流程来识别风险并将其可能性和严重性与预定义的风险阈值进行比较，RSP 就几乎没有足够的机会。当我询问周围的一些预报员和 AI 安全研究人员时，ASL-3 系统（在人类 RSP 中定义）引起的每年灭绝概率的估计数倍于 1% 以上，根据我们目前的能力，最高可达 5%。衡量能力（而不是我们非常清楚如何衡量这些能力的理想化世界）。</p><p> 3. 包括废除承诺的<strong>白衣骑士条款</strong>。</p><p>在阅读 RSP 时，给我印象最深的提议之一是插入名副其实的<strong>白骑士条款</strong>。</p><ul><li>简而言之，如果你正在开发一个危险的人工智能系统，因为你是一家好公司，并且你担心其他坏公司带来太多风险，那么你可以奋力阻止这种情况发生。</li><li>如果你援引白骑士条款并增加灾难性风险，你仍然必须向董事会、员工和国家当局证明其合理性。后者提供了最低限度的责任形式。但如果我们处于这样一种情况，即国家已经充分熟睡，需要一家 AGI 公司首先扮演白衣骑士的角色，那么它似乎不会产生太大的威慑力。</li></ul><p>我相信有些公司比其他公司更安全。但这不是正确的问题。正确的问题是：有哪家公司不认为自己是坏人？答案是：不。 OpenAI、Anthropic 和 DeepMind 都会争论处于解决对齐问题前沿的重要性。梅塔和米斯特拉尔认为，人工智能民主化的关键是不妨碍权力集中。等等等等。<br><br>该条款实际上是在扼杀承诺。我很高兴 Anthropic 在自己的 RSP 中只包含了它的弱化版本，但我非常担心 ARC 将其作为一种选择。决定是否可以增加整个社会的灾难性风险并不是公司的职责。</p><h1>第 4 节：为什么 RSP 具有误导性和过度销售</h1><h2>误导性的</h2><p>除了将人择 RSP 上的错位风险指定为“投机性”以及为何在下一代系统中不太可能出现这种情况的三行论据之外，RSP 还存在几个极具误导性的方面：</p><ol><li>这就是所谓的“负责任的扩展”。以它自己的名字来说，它传达了这样的想法：不进一步扩展这些系统作为风险缓解措施是不可行的。</li><li>它传达了一幅对风险形势非常过度自信的画面。<ol><li> Anthropic 在其 RSP 的介绍中写道，“基本思想是要求适合模型潜在灾难性风险的安全、保障和操作标准”。他们已经为可能具有基本生物武器制造能力的 ASL-3 系统定义了足够的保护措施。与此同时，他们写道，他们正在实际衡量与生物安全相关的风险：“我们的首要工作领域是评估生物风险，我们将确定威胁模型和能力”。我真的很高兴他们正在开展这项工作，但如果这输出了一个惊人的数字怎么办？是否有一个世界，数字输出让他们停下来 2 年并放弃之前的 ASL-3 版本而不是负责任地扩展？</li><li> ARC 没有争论为什么图表会是这样的，而是发布了这样的图表。人工智能安全领域的许多人并不期望它会朝这个方向发展，而“安全区域”夸大了 RSP 所做的事情。我和其他人一起预计 LLM 图表将达到在可预见的未来根本无法管理的风险水平。如果没有对我们试图防范的风险进行定量衡量，那么声称已经达到“足够的防护措施”也是不严肃的。 <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/moqd7abbdi5ohnv3efy2"></li></ol></li></ol><p><br></p><p>如果您想了解更多相关内容，可以阅读<a href="https://www.alignmentforum.org/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right?commentId=FtbzhGk5oPT3dyHLi"><u>该内容</u></a>。</p><h2>超卖、交付不足</h2><p>RSP 框架有一些很好的特性。但首先，这些都已经被现有的风险评估框架更详细地涵盖了，但还没有人工智能实验室实施。其次，ARC 的 RSP 框架与特定 RSP 实验室实现的共存允许<strong>在理论上允许雄心勃勃的</strong><strong>承诺的框架内的薄弱承诺</strong>。它导致了许多以下形式的论证：</p><ul><li> “那是V1。随着时间的推移，我们将提高雄心”。我希望看到任何领域或行业在 5 年时间内发生这种情况的证据。我可以想到一些领域，比如航空业，几十年来发生了一次又一次的坠机事故。但如果是基于对发生大规模事故的预期，那就应该很清楚了。如果它依赖于时间线很长的假设，那么它应该是明确的。</li><li> “这是自愿的，我们不能期望太多，而且它比现有的要好得多”。当然可以，但如果灾难性风险的水平为 1%（我采访过的几位人工智能风险专家认为 ASL-3 系统就是这种情况），并且给人一种风险已被涵盖的印象，那么“负责任”的名称就可以被称为“负责任的”。规模扩张”严重误导了政策制定者。 1% 灾难性风险的恰当名称是灾难性扩展，但这个名称并不那么乐观。</li></ul><p> I also feel like it leads to many disagreements that all hinge on: do we expect labs to implement ambitious RSPs?</p><p> And my answer is: given their track record, no. Not without government intervention. Which brings us to the question: “what&#39;s the effect of RSPs on policy and would it be good if governments implemented those”. My answer to that is: An extremely ambitious version yes; the misleading version, no. No, mostly because of the short time we have before we see heightened levels of risks, which gives us very little time to update regulations, which is a core assumption on which RSPs are relying without providing evidence of being realistic.</p><p><br> I expect labs to push hard for the misleading version, on the basis that pausing is unrealistic and would be bad for innovation or for international race. Policymakers will have a hard time distinguishing the risk levels between the two because it hinges on details and aren&#39;t quantified in RSPs. They are likely to buy the bad misleading version because it&#39;s essentially selling that there&#39;s <strong>no trade-off between capabilities and safety</strong> . That would effectively enforce a trajectory with unprecedented levels of catastrophic risks.</p><h1> Section 5: Are RSPs Hopeless?</h1><p> Well, yes and no.</p><ul><li> Yes, in that most of the pretty intuitive and good ideas underlying the framework are weak or incomplete versions of traditional risk management, with some core pieces missing. Given that, it seems more reasonable to just start from an existing risk management piece as a core framework. ISO/IEC 23894 or the NIST-inspired <a href="https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/"><u>AI Risk Management Standards Profile for Foundation Models</u></a> would be pretty solid starting points.</li><li> No in that inside the RSPs, there are many contributions that should be part of an AI risk management framework and that would help make existing risk management frameworks more specific. I will certainly not be comprehensive, but some of the important contributions are:<ul><li> Anthropic&#39;s RSP fleshes out a wide range of relevant considerations and risk treatment measures</li><li> ARC provides:<ul><li> technical benchmarks and proposed operationalizations of certain types of risks that are key</li><li> definitions of safety margins for known unknowns</li><li> threat modelling</li><li> low-level operationalization of some important commitments</li></ul></li></ul></li></ul><p> In the short-run, given that it seems that RSPs have started being pushed at the UK Summit and various other places, I&#39;ll discuss what changes could make RSPs beneficial without locking in regulation a bad framework.</p><h2> How to Move Forward?</h2><p> <u>Mitigating nefarious effects:</u></p><ol><li> <strong>Make the name less misleading</strong> : If instead of calling it “responsible scaling”, one called it “Voluntary safety commitments” or another name that:<ol><li> Doesn&#39;t <strong>determine the output of the safety test before having run it</strong> (ie scaling)</li><li> Unambiguously signals that it&#39;s not supposed to be sufficient or to be a good basis for regulation.</li></ol></li><li> <strong>Be clear on what RSPs are and what they aren&#39;t</strong> . I suggest adding the following clarifications regarding what the goals and expected effects of RSPs are:<ol><li> <strong>What RSPs are</strong> : “a company that would take too strong unilateral commitments would harm significantly its chances of succeeding in the AI race. Hence, this framework is aiming at proposing what we expect to be the best marginal measures that a company can unilaterally take to improve its safety without any coordination.”. I would also include a statement on the level of risks like: “We&#39;re not able to show that this is sufficient to decrease catastrophic risks to reasonable levels, and it is probably not.”,  “we don&#39;t know if it&#39;s sufficient to decrease catastrophic risks below reasonable levels”, or &quot;even barring coordinated industry-wide standards or government intervention, RSPs are only a second- (or third-) best option&quot;.</li><li> <strong>What RSPs aren&#39;t:</strong> Write very early in the post a disclaimer saying “THIS IS NOT WHAT WE RECOMMEND FOR POLICY”. Or alternatively, point to another doc stating what would be the measures that would be sufficient to maintain the risk below sufficient levels: “Here are the measures we think would be sufficient to mitigate catastrophic risks below acceptable levels.” to which you could add “We encourage laboratories to make a conditional commitment of the form: “if all other laboratories beyond a certain size[to be refined] committed to follow those safety measures with a reliable enforcement mechanism and the approval of the government regarding this exceptional violation of antitrust laws, we would commit to follow those safety measures.”</li></ol></li><li> <strong>Push for risk management in policy:</strong><ol><li> Standard risk management for what is acknowledged to be a world-shaping technology is a fairly reasonable ask. In fact, it is an ask that I&#39;ve noticed in my interactions with other AI crowds has the benefit of allowing coalition-building efforts because everyone can easily agree on “measure the risks, deal with them, and make the residual level of risks and the methodology public”.<br></li></ol></li></ol><p> <u>Checking whether RSPs manage risks adequately:</u></p><p> At a risk management level, if one wanted to demonstrate that RSPs like Anthropic&#39;s one are actually doing what they claim to do (ie “require safety, security, and operational standards appropriate to a model&#39;s potential for catastrophic risk”), a simple way to do so would be to run a risk assessment on ASL-3 systems with a set of forecasters, risk management experts and AI risk experts that are representative of views on AI risks and that have been selected by an independent body free of any conflict of interest.</p><p> I think that a solid baseline would be to predict the chances of various intermediary and final outcomes related to the risks of such systems:</p><ol><li> What&#39;s the annual likelihood that an ASL-3 system be stolen by {China; Russia; North Korea; Saudi Arabia; Iran}?</li><li> Conditional on that, what are the chances it leaks? it being used to build bioweapons? it being used for cyber offence with large-scale effects?</li><li> What are the chances of a catastrophic accident before ASL-4 evaluations trigger?</li><li> What are the annual chances of misuse catastrophic risks induced by an ASL-3 system?</li></ol><p> It might not be too far from what Anthropic seems to be willing to do internally, but doing it with a publicly available methodology, and staff without self-selection or conflict of interests makes a big difference. Answers to questions 1) and 2) could raise risks so the output should be communicated to a few relevant actors but could potentially be kept private.</p><p> If anyone has the will but doesn&#39;t have the time or resources to do it, I&#39;m working with some forecasters and AI experts that could probably make it happen. Insider info would be helpful but mostly what would be needed from the organization is some clarifications on certain points to correctly assess the capabilities of the system and some info about organizational procedures.</p><h1> Acknowledgments</h1><p> I want to thank Eli Lifland, Henry Papadatos and my other <a href="https://www.navigatingrisks.ai/"><u>NAIR</u></a> colleague, Olivia Jimenez, Akash Wasil, Mikhail Samin, Jack Clark, and other anonymous reviewers for their feedback and comments. Their help doesn&#39;t mean that they endorse the piece. All mistakes are mine.</p><h1> Annex</h1><h2> Comparative Analysis of Standards</h2><p> This (cropped) table shows the process of various standards for the 3 steps of risk management. As you can see, there are some differences but every standard seems to follow a similar structure.</p><p> From <a href="https://www.zotero.org/google-docs/?a64rn3">(Raz &amp; Hillson, 2005)</a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/xncrzvtrz7ktyigutrtw"></p><p> Here is a comparable table for the last two parts of risk management. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/9nEBWxjAHSu3ncr6v/qjqic1bka0uazyvshgre"></p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong<guid ispermalink="false"> 9nEBWxjAHSu3ncr6v</guid><dc:creator><![CDATA[simeon_c]]></dc:creator><pubDate> Wed, 25 Oct 2023 23:46:34 GMT</pubDate> </item><item><title><![CDATA[AI as a science, and three obstacles to alignment strategies]]></title><description><![CDATA[Published on October 25, 2023 9:00 PM GMT<br/><br/><p> AI used to be a science. In the old days (back when AI didn&#39;t work very well), people were attempting to develop a working theory of cognition.</p><p> Those scientists didn&#39;t succeed, and those days are behind us. For most people working in AI today and dividing up their work hours between tasks, gone is the ambition to understand minds. People working on mechanistic interpretability (and others attempting to build an empirical understanding of modern AIs) are laying an important foundation stone that could play a role in a future science of artificial minds, but on the whole, modern AI engineering is simply about constructing enormous networks of neurons and training them on enormous amounts of data, not about comprehending minds.</p><p> The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><u>bitter lesson</u></a> has been taken to heart, by those at the forefront of the field; and although this lesson doesn&#39;t teach us that there&#39;s <i>nothing to learn</i> about how AI minds solve problems internally, it suggests that <i>the fastest path to producing more powerful systems</i> is likely to continue to be one that doesn&#39;t shed much light on how those systems work.</p><p> Absent some sort of “science of artificial minds”, however, humanity&#39;s prospects for aligning smarter-than-human AI seem to me to be quite dim.</p><p> Viewing Earth&#39;s current situation through that lens, I see three major hurdles:</p><ol><li> Most research that helps one <a href="https://www.lesswrong.com/posts/NJYmovr9ZZAyyTBwM/what-i-mean-by-alignment-is-in-large-part-about-making"><u>point AIs</u></a> , probably also helps one make more capable AIs. A “science of AI” would probably increase the power of AI far sooner than it allows us to solve alignment.</li><li> In a world <i>without</i> a mature science of AI, building a bureaucracy that reliably distinguishes real solutions from fake ones is prohibitively difficult.</li><li> Fundamentally, for at least some aspects of system design, we&#39;ll need to rely on a theory of cognition working on the first high-stakes real-world attempt.</li></ol><p> I&#39;ll go into more detail on these three points below. First, though, some background:</p><p></p><h2>背景</h2><p>By the time AIs are powerful enough to endanger the world at large, I expect AIs to do something akin to “caring about outcomes”, at least from a behaviorist perspective (making no claim about whether it internally implements that behavior in a humanly recognizable manner).</p><p> Roughly, this is because people are <i>trying</i> to make AIs that can steer the future into narrow bands (like “there&#39;s a cancer cure printed on this piece of paper”) over long time-horizons, and caring about outcomes (in the behaviorist sense) is the flip side of the same coin as steering the future into narrow bands, at least when the world is sufficiently large and full of curveballs.</p><p> I expect the outcomes that the AI “cares about” to, by default, not include anything good (like fun, love, art, beauty, or the light of consciousness) — nothing good by present-day human standards, and nothing good by broad <a href="https://arbital.com/p/value_cosmopolitan/"><u>cosmopolitan standards</u></a> either. Roughly speaking, this is because when you grow minds, they don&#39;t care about what you ask them to care about and they don&#39;t care about what you train them to care about; instead, I expect them to care about a bunch of correlates of the training signal in weird and specific ways.</p><p> (Similar to how the human genome was naturally selected for inclusive genetic fitness, but the resultant humans didn&#39;t end up with a preference for “whatever food they model as useful for inclusive genetic fitness”. Instead, humans wound up internalizing a huge and complex set of preferences for &quot;tasty&quot; foods, laden with complications like “ice cream is good when it&#39;s frozen but not when it&#39;s melted”.)</p><p> Separately, I think that most complicated processes work for reasons that are fascinating, <a href="http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail"><u>complex</u></a> , and <a href="https://www.lesswrong.com/posts/RcZeZt8cPk48xxiQ8/anthropomorphic-optimism"><u>kinda horrifying</u></a> when you look at them closely.</p><p> It&#39;s easy to think that a bureaucratic process is competent until you look at the gears and see the specific ongoing office dramas and politicking between all the vice-presidents or whatever. It&#39;s easy to think that a codebase is running smoothly until you read the code and start to understand all the decades-old hacks and coincidences that make it run. It&#39;s easy to think that biology is a beautiful feat of engineering until you look closely and find that the eyeballs are <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)"><u>installed backwards</u></a> or whatever.</p><p> And there&#39;s an art to noticing that you would probably be astounded and horrified by the details of a complicated system <i>if you knew them</i> , and then being astounded and horrified<a href="https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence"><i><u>already in advance</u></i></a> before seeing those details. <span class="footnote-reference" role="doc-noteref" id="fnrefsrs3rrqcqz"><sup><a href="#fnsrs3rrqcqz">[1]</a></sup></span></p><p></p><h2> 1. Alignment and capabilities are likely intertwined</h2><p> I expect that if we knew in detail how LLMs are calculating their outputs, we&#39;d be horrified (and fascinated, etc.).</p><p> I expect that we&#39;d see all sorts of coincidences and hacks that make the thing run, and we&#39;d be able to see in much more detail how, when we ask the system to achieve some target, it&#39;s not doing anything <i>close</i> to “caring about that target” in a manner that would work out well for us, if we could scale up the system&#39;s optimization power to the point where it could achieve great technological or scientific feats (like designing Drexlerian nanofactories or what-have-you).</p><p> Gaining this sort of visibility into how the AIs work is, I think, one of the main goals of interpretability research.</p><p> And understanding how these AIs work and how they don&#39;t — understanding, for example, when and why they <i>shouldn&#39;t</i> yet be scaled or otherwise pushed to superintelligence — is an important step on the road to figuring out how to make <i>other</i> AIs that <i>could</i> be scaled or otherwise pushed to superintelligence without thereby causing a bleak and desolate future.</p><p> But that same understanding is — I predict — going to reveal an incredible mess. And the same sort of reasoning that goes into untangling that mess into an AI that we can aim, also serves to untangle that mess to make the AI <i>more capable</i> . A tangled mess will presumably be inefficient and error-prone and occasionally self-defeating; once it&#39;s disentangled, it won&#39;t just be tidier, but will also come to accurate conclusions and notice opportunities faster and more reliably. <span class="footnote-reference" role="doc-noteref" id="fnrefyrx2im012lj"><sup><a href="#fnyrx2im012lj">[2]</a></sup></span></p><p> Indeed, my guess is that it&#39;s even easier to see all sorts of things that the AI is doing that are dumb, all sorts of ways that the architecture is tripping itself up, and so on.</p><p> Which is to say: the same route that gives you a chance of aligning this AI (properly, not the “it no longer says bad words” superficial-property that labs are trying to pass off as “alignment” these days) also likely gives you lots more AI capabilities.</p><p> (Indeed, my guess is that the first big capabilities gains come <i>sooner</i> than the first big alignment gains.)</p><p> I think this is true of most potentially-useful alignment research: to figure out how to aim the AI, you need to understand it better; in the process of understanding it better you see how to make it more capable.</p><p> If true, this suggests that alignment will always be in catch-up mode: whenever people try to figure out how to align their AI better, someone nearby will be able to run off with a few new capability insights, until the AI is pushed over the brink.</p><p> So a first key challenge for AI alignment is a challenge of ordering: how do we as a civilization figure out how to aim AI <i>before</i> we&#39;ve generated unaimed superintelligences plowing off in random directions? I no longer think “just sort out the alignment work before the capabilities lands” is a feasible option (unless, by some feat of brilliance, this civilization pulls off some uncharacteristically impressive theoretical triumphs).</p><p> Interpretability? Will likely reveal ways your architecture is bad before it reveals ways your AI is misdirected.</p><p> Recruiting your AIs to help with alignment research? They&#39;ll be able to help with capabilities long before that (to say nothing of whether they <i>would</i> help you with alignment by the time they <i>could</i> , any more than humans would willingly engage in eugenics for the purpose of redirecting humanity away from <a href="https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence"><u>Fun</u></a> and exclusively towards inclusive genetic fitness).</p><p>等等。</p><p> This is (in a sense) a weakened form of my answer to those who say, “AI alignment will be much easier to solve once we have a bona fide AGI on our hands.” It sure will! But it will also be much, much easier to destroy the world, when we have a bona fide AGI on our hands. To survive, we&#39;re going to need to either sidestep this whole alignment problem entirely (and take other routes to a <a href="https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1"><u>wonderful future</u></a> instead, as I may discuss more later), or we&#39;re going to need some way to do a bunch of alignment research <i>even as</i> that research makes it radically easier and radically cheaper to destroy everything of value.</p><p> Except even that is harder than many seem to realize, for the following reason.</p><p></p><h2> 2. Distinguishing real solutions from fake ones is hard</h2><p> Already, labs are diluting the word “alignment” by using the word for superficial results like “the AI doesn&#39;t say bad words”. Even people who apparently understand many of the core arguments have apparently gotten the impression that GPT-4&#39;s ability to answer moral quandaries is somehow especially relevant to the alignment problem, and an important positive sign.</p><p> (The ability to answer moral questions convincingly mostly demonstrates that the AI can predict how humans would answer or what humans want to hear, without revealing much about what the AI actually pursues, or would pursue upon reflection, etc.)</p><p> Meanwhile, we have little idea of what passes for “motivations” inside of an LLM, or what effect pretraining on next-token prediction and fine-tuning with RLHF really has on the internals. This sort of precise scientific understanding of the internals — the sort that lets one predict weird cognitive bugs <i>in advance</i> — is currently mostly absent in the field. (Though not entirely absent, thanks to the hard work of many researchers.)</p><p> Now imagine that Earth wakes up to the fact that the labs aren&#39;t going to all decide to stop and take things slowly and cautiously at the appropriate time. <span class="footnote-reference" role="doc-noteref" id="fnrefb9gx61wfcbq"><sup><a href="#fnb9gx61wfcbq">[3]</a></sup></span> And imagine that Earth uses some great feat of civilizational coordination to halt the world&#39;s capabilities progress, or to otherwise handle the issue that we somehow need room to figure out how these things work well enough to align them. And imagine we achieve this coordination feat <i>without</i> using that same alignment knowledge to end the world (as we could). There&#39;s then the question of who gets to proceed, under what circumstances.</p><p> Suppose further that everyone agreed that the task at hand was to fully and deeply understand the AI systems we&#39;ve managed to develop so far, and understand how they work, to the point where people could reverse out the pertinent algorithms and data-structures and what-not. As demonstrated by great feats like building, by-hand, small programs that do parts of what AI can do with training (and that nobody previously knew how to code by-hand), or by identifying weird exploits and edge-cases <i>in advance</i> rather than via empirical trial-and-error. Until multiple different teams, each with those demonstrated abilities, had competing models of how AIs&#39; minds were going to work when scaled further.</p><p> In such a world, it would be a difficult but plausibly-solvable problem, for bureaucrats to listen to the consensus of the scientists, and figure out which theories were most promising, and figure out who needs to be allotted what license to increase capabilities (on the basis of this or that theory that predicts this would be non-catastrophic), so as to put their theory to the test and develop it further.</p><p> I&#39;m not thrilled about the idea of trusting an Earthly bureaucratic process with distinguishing between partially-developed scientific theories in that way, but it&#39;s the sort of thing that a civilization can perhaps survive.</p><p> But that doesn&#39;t look to me like how things are poised to go down.</p><p> It looks to me like we&#39;re on track for some people to be saying “look how rarely my AI says bad words”, while someone else is saying “our evals are saying that it can&#39;t deceive humans yet”, while someone else is saying “our AI is acting very submissive, and there&#39;s no reason to expect AIs to become non-submissive, that&#39;s just anthropomorphizing”, and someone else is saying “we&#39;ll just direct a bunch of our AIs to help us solve alignment, while arranging them in a big bureaucracy”, and someone else is saying “we&#39;ve set up the game-theoretic incentives such that if any AI starts betraying us, some other AI will alert us first”, and this is a <i>different sort of situation</i> .</p><p> And not one that looks particularly survivable, to me.</p><p> And if you ask bureaucrats to distinguish which teams should be allowed to move forward (and how far) in that kind of circus, full of claims, promises, and hunches and poor in theory, then I expect that they basically just <i>can&#39;t</i> .</p><p> In part because the survivable answers (such as “we have no idea what&#39;s going on in there, and will need way more of an idea what&#39;s going on in there, and that understanding needs to somehow develop in a context where we can do the job right rather than simply unlocking the door to destruction”) aren&#39;t really in the pool. And in part because all the people who really want to be racing ahead have money and power and status. And in part because it&#39;s socially hard to believe, as a regulator, that you should keep telling everyone “no”, or that almost everything on offer is radically insufficient, when <i>you yourself</i> don&#39;t concretely know what insights and theoretical understanding we&#39;re missing.</p><p> Maybe if we can make AI a science again, then we&#39;ll start to get into the regime where, <i>if</i> humanity can regulate capabilities advancements in time, then all the regulators and researchers understand that you shall only ask for a license to increase the capabilities of your system when you have a full detailed understanding of the system and a solid justification for why you need the capabilities advance and why it&#39;s not going to be catastrophic. At which point maybe a scientific field can start coming to some sort of consensus about those theories, and regulators can start being sensitive to that consensus.</p><p> But unless you can get over that grand hump, it looks to me like one of the key bottlenecks here is <i>bureaucratic legibility of plausible solutions</i> . Where my basic guess is that regulators won&#39;t be able to distinguish real solutions from false ones, in anything resembling the current environment.</p><p> Together with the above point (&quot;alignment and capabilities are likely intertwined&quot;), I think this means that our rallying cry should be less “pause to give us more time on alignment research” and more “stop entirely, and find some way to circumvent these woods entirely; we&#39;re not equipped to navigate them”.</p><p> (With a backup rallying cry of “make AI a science again”, though again, that only works if you have some way of preventing the science-of-mind from leading to catastrophe before we figure out how to build AIs that care about good stuff rather than bleak and desolate stuff.)</p><p></p><h2> 3. Most theories don&#39;t work on the first real try</h2><p> It seems worth noting that <i>even if</i> you manage to surmount the above two problems, you have a third problem on your hands, which is that when it finally comes time, not to increase your system&#39;s capabilities a bit more (and test your theory of cognition further), but rather to actually do something of note with your AI, then you must, in some sense, be trusting a novel and untested scientific theory ( <i>and</i> your engineering efforts grounded in that theory) to work perfectly on the first critical try.</p><p> In particular, once your AI is <i>capable</i> of autonomous scientific/technological development, and operating in a domain where it <i>could</i> use that ability to gain a decisive strategic advantage over the rest of the planet, it&#39;s operating in a fundamentally different cognitive regime than it ever was in training.</p><p> By analogy, if you&#39;re considering making Bob the dictator of your country, you might first make him mock-dictator of your town, and watch to make sure he doesn&#39;t abuse his power. But, try as you might, that&#39;s still not fundamentally all that reliable a test of whether he&#39;ll actually abuse power once he really has it. Once he <i>really can</i> give orders to the army, once he <i>really can</i> take advantage of the country&#39;s populace, will he do so? The option of “abuse my mock power while being observed by my townspeople (who can still beat me up or deny me the job)” is actually very cognitively different than the option “order the army to intimidate the parliament and &#39;oversee&#39; the next election”.</p><p> Now, given a sufficiently well-developed theory of cognition, you can try to read the AI&#39;s mind and predict what cognitive state it <i>would</i> enter if it <i>did</i> really think it had those options. And you can set up simulations (and try to spoof its internal sensations and so on) in a way that your theory of cognition <i>predicts</i> is very similar to the cognitive state it would enter once it really had the option to betray you.</p><p> But the link between these states that you induce and observe in the lab, and the actual state where the AI actually has the option to betray you, <i>depends fundamentally on your fresh new theory of cognition.</i></p><p> Actually <i>running</i> the AI until it <i>really has</i> the opportunity to betray you is an empirical test of those theories in an environment that differs fundamentally from the lab setting.</p><p> And many a scientist (and programmer) knows that their theories of how a complicated system is going to work in a fundamentally new operating environment <i>often don&#39;t go super well on the first try.</i></p><p> As a concrete analogy to potentially drive this point home: Newtonian mechanics made all sorts of shockingly-good empirical predictions. It was a simple concise mathematical theory with huge explanatory power that blew every previous theory out of the water. And if you were using it to send payloads to very distant planets at relativistic speeds, you&#39;d <i>still be screwed</i> , because Newtonian mechanics does not account for relativistic effects.</p><p> (And the only warnings you&#39;d get would be little hints about light seeming to move at the same speed in all directions at all times of year, and light bending around the sun during eclipses, and the perihelion of Mercury being a little off from what Newtonian mechanics predicted. Small anomalies, weighed against an enormous body of predictive success in a thousand empirical domains; and yet Nature doesn&#39;t care, and the theory still falls apart when we move to energies and scales far outside what we&#39;d previously been able to observe.)</p><p> Getting scientific theories to work on the first critical try is <i>hard</i> . (Which is one reason to aim for minimal pivotal tasks — getting a satellite into orbit should work fine on Newtonian mechanics, even if sending payloads long distances at relativistic speeds does not.)</p><p> Worrying about this issue is something of a luxury, at this point, because it&#39;s not like we&#39;re anywhere close to scientific theories of cognition that accurately predict all the lab data. But it&#39;s the next hurdle on the queue, if we somehow manage to coordinate to try to build up those scientific theories, in a way where success is plausibly bureaucratically-legible.</p><hr><p> Maybe later I&#39;ll write more about what I think the strategy implications of these points are. In short, I basically recommend that Earth pursue other routes to the glorious transhumanist future, such as uploading. (Which is also fraught with peril, but I expect that those perils are more surmountable; I hope to write more about this later.) </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsrs3rrqcqz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsrs3rrqcqz">^</a></strong></sup></span><div class="footnote-content"><p> Albeit slightly less, since there&#39;s <i>nonzero</i> prior probability on this unknown system turning out to be simple, elegant, and well-designed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyrx2im012lj"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyrx2im012lj">^</a></strong></sup></span><div class="footnote-content"><p> An exception to this guess happens if the AI is at the point where it&#39;s correcting its own flaws and improving its own architecture, in which case, in principle, you might not see much room for capabilities improvements if you took a snapshot and comprehended its inner workings, despite still being able to see that the ends it pursues are not the ones you wanted. But in that scenario, you&#39;re already about to die to the self-improving AI, or so I predict.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb9gx61wfcbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb9gx61wfcbq">^</a></strong></sup></span><div class="footnote-content"><p> Not least because there are no sufficiently clear signs that it&#39;s time to stop — we blew right past “an AI claims it is sentient”, for example. And I&#39;m not saying that it was a <i>mistake</i> to doubt AI systems&#39; first claims to be sentient — I doubt that Bing had the kind of personhood that&#39;s morally important (though I am by no means confident!). I&#39;m saying that the thresholds that are clear <i>in science fiction stories</i> turn out to be messy <i>in practice</i> and so everyone just keeps plowing on ahead.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies<guid ispermalink="false"> JcLhYQQADzTsAEaXd</guid><dc:creator><![CDATA[So8res]]></dc:creator><pubDate> Wed, 25 Oct 2023 21:00:16 GMT</pubDate> </item><item><title><![CDATA[My hopes for alignment: Singular learning theory and whole brain emulation]]></title><description><![CDATA[Published on October 25, 2023 6:31 PM GMT<br/><br/><p> <i>Some prerequisites needed in order for this to make sense:</i></p><ol><li> <a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/hE56gYi5d68uux9oM"><i>Two Subsystems: Learning &amp; Steering</i></a></li><li> <a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview"><i>Shard Theory: An Overview</i></a></li><li> <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">Distilling Singular Learning Theory</a> <span class="footnote-reference" role="doc-noteref" id="fnrefz5ia0mmn1nh"><sup><a href="#fnz5ia0mmn1nh">[1]</a></sup></span></li><li> <i>Maybe also understanding at least a little</i> <a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><i>Nate&#39;s picture</i></a> <i>though I don&#39;t claim to understand it fully.</i></li><li> <i>Of course,</i> <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><i>AGI Ruin: A List of Lethalities</i></a> <i>, though hopefully that was implied</i></li><li> <i>Maybe the very basics of infra-bayesianism</i> (I liked the <a href="https://axrp.net/episode/2021/03/10/episode-5-infra-bayesianism-vanessa-kosoy.html">AXRP podcast with her</a> ( <a href="https://axrp.net/episode/2022/04/05/episode-14-infra-bayesian-physicalism-vanessa-kosoy.html">there&#39;s two</a> )) <i>, Vanessa&#39;s original</i> <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda"><i>learning theoretic agenda</i></a> <i>philosophy, and her current</i> <a href="https://www.lesswrong.com/posts/WcWzLSn8ZjJhCZxP4/predca-vanessa-kosoy-s-alignment-protocol"><i>pre-DCA</i></a> <i>alignment scheme.</i></li></ol><h1>抽象的</h1><p>The philosophy behind infra-bayesianism, and Vanessa&#39;s learning-theoretic alignment agenda seems very insightful to me. However, the giant stack of assumptions needed to make the approach work, and the ontology-forcing nature of the plan leave me unsettled. The path of singular learning theory has recently seen enough empirical justification to excite me. I&#39;m optimistic it can describe brains &amp; machine learning systems, and I describe my hope that this can be leveraged into alignment guarantees between the two, becoming an easier task as whole brain emulation develops.</p><h1>介绍</h1><p>Imagine a world where instead of humanity wandering together blindly down the path of cobbled together weird tricks learned off the machine learning literature, with each person trying to prepare for the bespoke failure-mode that seems most threatening to them (most of whom are wrong).</p><p> That instead we lived in the world where before heading down we were given a map and eyes to see for ourselves the safest path, and the biggest and most dangerous cliffs, predators, and dangers to be aware of.</p><p> It has always seemed to me that we get to the second world once we can use math for alignment. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/jtedrlxrttqhncw2pmbf" alt="On the left dalle image generated with: Photo depicting a world representing empirical AI alignment approaches: A group of diverse people are shown as blind, navigating a rugged and unclear path. Their eyes are covered with blindfolds, and the terrain is uneven with shadows obscuring parts of the path. Each person has their own walking stick and gear, and they seem uncertain and cautious. Some individuals clutch old books or scrolls, symbolizing reliance on outdated machine learning literature. Others hold various tools and devices, preparing for unseen threats and unsure of the specific dangers ahead.  On the right dalle image generated with: Illustration in the Renaissance period style: A diverse group of men and women gather at the starting point of their adventure. Each holds a vibrant holographic map, some displaying topographical lines and others animated weather patterns. While some individuals discuss and point at dangers on the map, others look up to compare the hologram with the real terrain. The rugged journey ahead showcases treacherous cliffs, dense forests, and a distant mountain peak. A setting sun casts a dramatic light on the group, creating a chiaroscuro effect. The soft glow in their eyes highlights their enhanced vision." srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xbr790s8tup8s1wiwu3q 210w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l3okx5ydsichkv6y3fp6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/qg7zguipshld2rh8ujjw 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nq9l7nptiqminxuepwjt 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vzvyoasslfuahn32ksp3 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/xzfmk2ftj5ryr7grv75k 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/kxb0aequrmnkrla6ynjs 1470w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/mgplpa1tnpvneqlvnoka 1680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vm8ahq490egt75qsmuyi 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dbqvg1bohswms6vhmkns 2068w"><figcaption> The left: Our situation. The right: Our situation if we had math for alignment.<br> Generated with DALL·E 3.</figcaption></figure><p> For a long time I thought using math for alignment was essentially a non-starter. Deep learning is notoriously resistant to successful theories, I&#39;m pessimistic that the approach of the Machine Intelligence Research Institute will take too much time, and the most successful mathematical theory of intelligence and alignment--infra-Bayesianism--rests on a stack of assumptions and mathematical arguments too high, too speculative, and too normative for me to be optimistic about. So I resigned myself to the lack of math for alignment.</p><p> That is, until Nina Rimsky &amp; Dmitry Vaintrob showed that some predictions of the new Singular Learning Theory held with <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">spooky accuracy</a> in the <a href="https://www.lesswrong.com/posts/6Ghvdb2iwLAyGT6A3/paper-replication-walkthrough-reverse-engineering-modular">grokking modular addition network</a> <span class="footnote-reference" role="doc-noteref" id="fnreftvecsqyi9wj"><sup><a href="#fntvecsqyi9wj">[2]</a></sup></span> .</p><p> I had previously known about singular learning theory, and gone to the singular learning theory conference in Berkeley. But after thinking about the ideas, and trying to implement some of the processes they came up with myself, and getting not so great results <span class="footnote-reference" role="doc-noteref" id="fnrefiamfvhd10ws"><sup><a href="#fniamfvhd10ws">[3]</a></sup></span> , I decided it too was falling for the same traps as infra-Bayesianism, lying on a giant stack of mathematical arguments, with only the most tentative contact with empirical testing with real world systems. So I stopped following it for a few months.</p><p> Looking at these new results though, it seems promising.</p><p> <i>But its not natively a theory of alignment. Its an upgraded learning theory. How does that solve alignment?</i></p><p> Well, both the human brain &amp; machine learning systems are learning machines, trained using a mix of reinforcement &amp; supervised learning. If this theory were to be developed in the right way (and this is where the <i>hope</i> comes in), we could imagine relating the goals of the results of one learning process to the goals of a different learning process, and prove a maximal deviation between the two for a particular setup. If one of those learning systems is a human brain, then we have just gotten an alignment guarantee.</p><p> Hence my two main hopes for alignment <span class="footnote-reference" role="doc-noteref" id="fnref8ctyd96xsoa"><sup><a href="#fn8ctyd96xsoa">[4]</a></sup></span> : whole brain emulation, and a <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning-theoretic-agenda</a> -like developmental track for singular learning theory. <span class="footnote-reference" role="doc-noteref" id="fnrefu6c8m98jn6j"><sup><a href="#fnu6c8m98jn6j">[5]</a></sup></span></p><p> Interpretability, and other sorts of deep learning science seem good to the extent it helps with getting singular learning theory (or some better version of it) to the state where its able to prove such alignment-relevant theorems.</p><p> This task is made easier to the extent we have better whole brain emulation. And becomes so easy that we don&#39;t even need the singular learning theory component if we have succeeded in our whole brain emulation efforts. <span class="footnote-reference" role="doc-noteref" id="fnreffxt68b6pmji"><sup><a href="#fnfxt68b6pmji">[6]</a></sup></span></p><p> It seems like it&#39;d be obvious to you, the reader, why whole brain emulation gives me hope <span class="footnote-reference" role="doc-noteref" id="fnrefdnadin1d3mt"><sup><a href="#fndnadin1d3mt">[7]</a></sup></span> . But less obvious why singular learning theory gives me hope, so I will explain in much of the rest of this post why the latter is true.</p><h1> Singular learning theory</h1><p> <i>Feel free to skip these next three paragraphs, or even instead of reading my description, read</i> <a href="https://www.lesswrong.com/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick"><i>Jesse&#39;s</i></a> <i>or read pre-requisite 3. My goal is less to explain what singular learning theory is, and more to give an idea about why I&#39;m excited about it.</i></p><p> Singular learning theory fills a gap in regular learning theory, in particular, regular learning theory assumes that the parameter function map of your statistical model is one-to-one, and intuitively your loss landscape has no flat regions. <span class="footnote-reference" role="doc-noteref" id="fnrefmlwit0973hf"><sup><a href="#fnmlwit0973hf">[8]</a></sup></span></p><p> Singular learning theory handles the case where this is not true, which occurs often in hierarchical models, and (as a subset) deep neural networks. And in broad strokes by bringing in concepts from algebraic geometry in order to rewrite the KL divergence between the true model and parameters into a form that is easier to analyze.</p><p> In particular, it anticipates two classes of phase transitions during the development of models, one of which is that whenever loss suddenly goes down, the real-log-canonical-threshold (RLCT), a algebraic geometry derived metric for complexity, will go up. <span class="footnote-reference" role="doc-noteref" id="fnref7ze9pnljbu"><sup><a href="#fn7ze9pnljbu">[9]</a></sup></span></p><p> Its ultimately able to retrodict various facts about deep learning, including the success of data scaling, parameter scaling, and double descent, and there has been recent success in getting it to give predictions about phenomena in limited domains. Most recently, Nina Rimsky and Dmitry Vaintrob&#39;s <a href="https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition">Investigating the learning coefficient of modular addition: hackathon project</a> where the two were able to verify various assertions about the RLCT/learning coefficient/<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat \lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>/Watanabe-Lau-Murfet-Wei estimate. Getting  the most beautiful verification of a theoretical prediction in my lifetime </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/l9pqbgjnseytqtuaidx2" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/bqoeco4x0lpcethyj0tg 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/hholcdc33tq4k0lko7ww 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/wt4wccb0guydydornjl2 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/vrr9yegwglwkyakew5eo 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ifcgg4fpoepoc8bjnfp0 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/nluxltps06y1xe3magdn 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/dmgxtioolpqiwhfojulz 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d4qbjx35SBMGyFNWZ/ru2cyvlbubylkcnya9hb 640w"><figcaption> Chart of estimated <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.041em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span></span></span></span></span></span> over training for an MLP trained on modular addition mod 53. Checkpoints were taken every 60 batches of batch size 64. Hyperparameters for SGLD are <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gamma=5"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.025em;">γ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">5</span></span></span></span></span></span></span> , <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\epsilon=0.001"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ϵ</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.001</span></span></span></span></span></span></span> . The search was restricted to directions orthogonal to the gradient at the initialization point to correct for measurement at non-minima. [caption text from the original post]</figcaption></figure><p> As I said, singular learning theory makes the prediction that during phase transitions, the loss of a model will decrease while the RLCT ( <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\lambda"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">λ</span></span></span></span></span></span></span> ) will increase. Intuitively, this means, when you switch model classes you switch to a model class that fits the data better and is more complicated. Above, we see exactly this.</p><p> As well as Zhongtian Chen, Edmund Lau, Jake Mendel, Susan Wei, and Daniel Murfet&#39;s <a href="https://arxiv.org/abs/2310.06301">Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition</a> with abstract</p><blockquote><p> We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="k"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">k</span></span></span></span></span></span></span> -gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.</p></blockquote><p> You can read more at <a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC">this LessWrong sequence</a> , watching the <a href="https://www.youtube.com/@SLTSummit/videos">primer</a> , <a href="https://www.youtube.com/playlist?list=PLKnx70LRf21c96cM3GM64wW8ZnYhravvD">Roblox lectures</a> , and of course reading the books <a href="https://www.amazon.com/Algebraic-Statistical-Monographs-Computational-Mathematics/dp/0521864674/ref=sr_1_1?keywords=Algebraic+Geometry+and+Statistical+Learning+Theory&amp;link_code=qs&amp;qid=1697849842&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Algebraic Geometry and Statistical Learning Theory</a> , and <a href="https://www.amazon.com/Mathematical-Theory-Bayesian-Statistics-Watanabe/dp/0367734818/ref=sr_1_1?crid=WTY5ZYVX7OVU&amp;keywords=mathematical+theory+of+bayesian+statistics&amp;qid=1697849901&amp;sprefix=mathematical+theory+of+bayesian+statistic%2Caps%2C142&amp;sr=8-1&amp;ufe=app_do%3Aamzn1.fos.006c50ae-5d4c-4777-9bc0-4513d670b6bc">Mathematical Theory of Bayesian Statistics</a> .</p><h1> So why the hope?</h1><p> To quote Vanessa Kosoy from her <a href="https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda">learning theoretic</a> agenda:</p><blockquote><p> In online learning and reinforcement learning, the theory typically aims to derive upper and lower bounds on &quot;regret&quot;: the difference between the expected utility received by the algorithm and the expected utility it <i>would</i> receive if the environment was known a priori. Such an upper bound is effectively a <i>performance guarantee</i> for the given algorithm. In particular, if the reward function is assumed to be &quot;aligned&quot; then this performance guarantee is, to some extent, an alignment guarantee. This observation is not vacuous, since the learning protocol might be such that the true reward function is not directly available to the algorithm, as exemplified by <a href="https://www.lesswrong.com/posts/5bd75cc58225bf067037546b/delegative-inverse-reinforcement-learning">DIRL</a> and <a href="https://www.lesswrong.com/posts/5bd75cc58225bf06703754d5/delegative-reinforcement-learning-with-a-merely-sane-advisor">DRL</a> . Thus, formally proving alignment guarantees takes the form of proving appropriate regret bounds.</p></blockquote><p> If the principles of singular learning theory can be extended to reinforcement learning, and we get reasonable bounds for the generalization behavior of our model, or even exact claims about the different forms the value-equivalents inside our model will take as it progresses during training, we can either hope to solve a form of what can roughly be known as inner alignment--getting our model to consistently think &amp; act in a certain way when encountering the deployment environment.</p><p> It seems reasonable to me to think we can in fact extend singular learning theory to reinforcement learning. The same sorts of deep learning algorithms work very well on both supervised and reinforcement learning, so we should expect the same algorithms have similar reasons for working on both, and singular learning theory gives a description of why those deep learning algorithms do well in the supervised learning case. So we should suspect the story for reinforcement learning follows the same general strokes <span class="footnote-reference" role="doc-noteref" id="fnrefcnu4pysu68w"><sup><a href="#fncnu4pysu68w">[10]</a></sup></span> .</p><h1> If you like performance guarantees so much, why not just work on infra-Bayesianism?</h1><p> Vanessa&#39;s technique was to develop a theory of what it meant to do good reasoning in a world that is bigger than yourself, and also requires you to do self-modeling. Then (as I understand it) prove some regret bounds, make a criterion for what it means to be an agent, and construct a system with a low bound on the satisfaction of the utility function of the agent that is most immediately causally upstream of the deployed agent.</p><p> This seems like a really shaky construction to me, mainly because we do not in fact have working in-silico examples of the agents she&#39;s thinking about. I&#39;d be much happier with taking this methodology, and using it to prove bounds on actual real life deep learning systems&#39; regret (or similar qualities) under different training dynamics.</p><p> I also feel uneasy about how it goes from theory about how values work (maximizing a utility function) to defining that as the success criterion <span class="footnote-reference" role="doc-noteref" id="fnref3jsz5ti3q8s"><sup><a href="#fn3jsz5ti3q8s">[11]</a></sup></span> . I&#39;d be more comfortable with a theory which you could apply to the human brain, and naturally derive a utility function (or other value format). Just by looking at how brains are built and developed. In the case that we&#39;re wrong about our philosophy of values, this seems more robust. And to the extent multiple utility functions can fit our behavior accounting for biases and lack of extensive reflection, having a prior over values informed by the territory (ie the format values are in in the brain) seems better than the blunt instrument of Occam&#39;s razor applied directly to the mapping from our actual and counterfactual actions to utility functions.</p><p> Of all the theories that I know of, singular learning theory seems the most adequate to the task. It both is based on well-proven math, it has and will continue to have great contact with actual systems,  it covers a very wide range of learning machines, which includes human brains (ignoring the more reinforcement learning like aspects of human learning for now) &amp; likely future machines (again ignoring reinforcement learning), and the philosophical assumptions it makes are far more minimalist than those of infra-Bayesianism.</p><p> The downside of this approach is singular learning theory says little right now about reinforcement learning. However, as I also said above, we see the same kinds of scaling dynamics in reinforcement learning as we do in supervised learning &amp; the same kinds of models work in both cases, so it&#39;d be pretty weird if they had very different reasons for being successful. Singular learning theory tries to explain the supervised learning case, so we should expect it or similar methods be able to explain the reinforcement learning case too.</p><p> Another downside is it not playing well with unrealizability. However, I&#39;m told there hasn&#39;t been zero progress here, its an open problem in the field, and again, neural networks often learn in unrealizable environments, as far as I know, we see similar enough dynamics that I bet singular learning theory is up for the task.</p><h1> Whole brain emulation</h1><p> The human brain is almost certainly singular <span class="footnote-reference" role="doc-noteref" id="fnrefcyq9ebydlz5"><sup><a href="#fncyq9ebydlz5">[12]</a></sup></span> , has a big learning from scratch component to it, and singular learning theory is very agnostic about the kinds of models it can deal with <span class="footnote-reference" role="doc-noteref" id="fnrefm9s3qj539of"><sup><a href="#fnm9s3qj539of">[13]</a></sup></span> , so I assert singular learning can deal with the brain. Probably not to help whole brain emulation all that much, but given data whole brain emulation gives us about the model class that brains fall into, the next hope is to use this to make nontrivial statements about the value-like-things that humans have. Connecting this with the value-like-things that our models have, we can hopefully (and this is the last hope) use singular learning theory to tell us under what conditions our model will have the same values-like-things that our brains have.</p><h1> Fears</h1><h2>哇！ That&#39;s a lot of hopes. I&#39;m surprised this makes you more hopeful than something simple like empirical model evaluations</h2><p> Singular learning theory, interpretability, and the wider developmental interpretability all seem useful for empirically testing models. I&#39;m not hopeful just because of the particular plan I outline above, I&#39;m hopeful because I see a concrete plan at all for how to turn math into an alignment solution for which all parts seem to be useful even if not all of my hopes turn out correct.</p><h2> I&#39;m skeptical that something like singular learning theory continues to work as the model becomes reflective, and starts manipulating its training environment.</h2><p> I am too. This consideration is why our guarantees should occur early in training, be robust to continued training, and be reflectively stable. Human-like values-like-things should be reflectively stable by their own lights, though we won&#39;t actually know until we actually see what we&#39;re dealing with here. So the job comes down to finding a system which puts them into our model early in training, keeps them throughout training, and ensures by the time reflection is online, the surrounding optimizing machinery is prepared.</p><p> Put another way: I see little reason to suspect the values-like-things deep learning induces will be reflectively stable by default. Primarily because the surrounding optimizing machinery is liable to give strange recommendations in novel situations, such as reflective thought becoming active <span class="footnote-reference" role="doc-noteref" id="fnref3p9cx77pt1d"><sup><a href="#fn3p9cx77pt1d">[14]</a></sup></span> . So it does in fact seem necessary to prepare that surrounding optimizing machinery for the event of reflection coming online. But I&#39;m not so worried about the values-like-objects being themselves disastrously suicidal once we know they&#39;re similar enough to those of humans.</p><p> Nate and possibly Eliezer would say this is important to know from the start. I would say I&#39;ll cross that bridge once I actually know a thing or two what values-like-thing, and surrounding machinery I&#39;ll be dealing with.</p><h2> Why reinforcement learning? Shouldn&#39;t you focus on supervised learning, where the theory is clear, and we&#39;re more likely to get powerful models soon?</h2><p> Well, brains are closer to reinforcement learning than supervised learning, so that&#39;s one reason. But yeah, if we can get a model which while supervised, we prove statements about values-like objects for, then that would be a good deal of the way there. But not all the way there, since we&#39;d still be confused when looking at our brains.</p><h2> Singular learning theory seems liable to help capabilities. That seems bad.</h2><p> I will quote myself outlining <a href="https://www.lesswrong.com/posts/75uJN3qqzyxWoknN7/interpretability-externalities-case-study-hungry-hungry?commentId=CqaNeSLaseBBbpwxE">my position</a> on a related topic, which generalizes to the broader problem of developing a theory of deep learning:</p><blockquote><p> Mostly I think that [mechanistic interpretability] is right to think it can do a lot for alignment, but I suspect that lots of the best things it can do for alignment it will do in a very dual-use way, which skews heavily towards capabilities. Mostly because capabilities advances are easier and there are more people working on those.</p><p> At the same time I suspect that many of those dual use concerns can be mitigated by making your [mechanistic interpretability] research targeted. Not necessarily made such that you can do off-the-shelf interventions based on your findings, but made such that if it ever has any use, that use is going to be for alignment, and you can predict broadly what that use will look like.</p><p> This also doesn&#39;t mean your [mechanistic interpretability] research can&#39;t be ambitious. I don&#39;t want to criticize people for being ambitious or too theoretical! I want to criticize people for producing knowledge on something which, while powerful, seems powerful in too many directions to be useful if done publicly.</p></blockquote><p> My plan above is a good example of an ambitious &amp; theoretical but targeted approach to deep learning theory for alignment.</p><h2> Why singular learning theory, and not just whole brain emulation?</h2><p> Firstly, as I said in the introduction, it is not obvious to me that given whole brain emulation we get aligned superintelligence, or are even able to perform a pivotal act. Recursive self improvement while preserving values may not be easy or fast (though you can always make the emulation faster). Pivotal acts done by such an emulation likely have difficulties I can&#39;t see.</p><p> However, I do agree that whole brain emulation alone seems probably alignment complete.</p><p> My main reason for focusing on both is that they feel like two different sides of the same problem in the sense that progress in whole brain emulation makes us need less progress in singular learning theory, and vice versa. And thinking in terms of a concrete roadmap makes me feel more like I&#39;m not unintentionally sweeping anything important underneath any rugs. The hopes I describe have definite difficulties, but few speculative difficulties.</p><h2> It seems difficult to get the guarantees you talk about which are robust to ontology shifts. Values are in terms of ontologies. Maybe if a model&#39;s ontology changes, its values will be different from the humans&#39;</h2><p> This is something I&#39;m worried about. I think there&#39;s hope that during ontology shifts, the meta-values of the models will dominate what shape the model values take into the new ontology, and there won&#39;t be a fine line between the values of the human and the meta-values of the AI. There&#39;s also an independent hope that we can have a definition of values that is just robust to a wide range of ontology shifts.</p><h1> So what next for singular learning theory and whole brain emulation?</h1><p> I currently don&#39;t know too much about whole brain emulation. Perhaps there are areas they&#39;re focusing on which aren&#39;t so relevant to my goals here. For example, if they focus more on the statics of the brain than the dynamics, then that seems naively inefficient <span class="footnote-reference" role="doc-noteref" id="fnreff23rfnmz6wq"><sup><a href="#fnf23rfnmz6wq">[15]</a></sup></span> because the theorem I want talks about the dynamics of learning systems and how those relate to each other.</p><p> Singular learning theory via <a href="https://www.lesswrong.com/posts/nN7bHuHZYaWv9RDJL/announcing-timaeus">Timaeus</a> seems to mostly be doing what I want them to be doing: testing the theory on real world models, and seeing how to relate it to model internals via developmental interpretability. One failure-mode here is they focus too much on empirically testing it, and too little on trying to synthesize their results into a unified theory. Another failure-mode is they focus too much on academic outreach, and not enough on actually doing research. And then the academics they do outreach to don&#39;t really theoretically contribute that much to singular learning theory.</p><p> I&#39;m not <i>so</i> worried about the first failure-mode, since everyone on their core team seems very theoretically inclined.</p><p> It seems like a big thing they aren&#39;t looking into is reinforcement learning. This potentially makes sense. Reinforcement learning is harder than supervised learning. You need some possibly nontrivial theoretical leaps to say anything about it in a singular learning theory framework. Even so, it seems possible there is low-hanging fruit in this direction. Similarly for taking current models of the brain</p><p> Of course, I expect the interests of Timaeus and myself will diverge as singular learning theory progresses, and there are pretty few people working on developing the theory right now. So it seems a productive use of my efforts.</p><h1> Acknowledgements</h1><p> Thanks to Jeremy Gillen, and David Udell for great comments and feedback! Thanks also to Nicholas Kees, and the <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment">Mesaoptimizer</a> for the same. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5ia0mmn1nh"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5ia0mmn1nh">^</a></strong></sup></span><div class="footnote-content"><p> Note I didn&#39;t read this, I watched the <a href="https://singularlearningtheory.com/events/2023-q2-berkeley-conference#primer">singular learning theory primer</a> videos, but those seem longer than the series of LessWrong posts, and some have told me they&#39;re a good intro.</p></div></li><li class="footnote-item" role="doc-endnote" id="fntvecsqyi9wj"> <span class="footnote-back-link"><sup><strong><a href="#fnreftvecsqyi9wj">^</a></strong></sup></span><div class="footnote-content"><p> Somewhat notably, both the grokking work, and Nina &amp; Dmitry&#39;s project were done over a weekend.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniamfvhd10ws"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiamfvhd10ws">^</a></strong></sup></span><div class="footnote-content"><p> If I remember correctly, the reason for the not great results was because the random variable we were estimating had a too high variance to actually correlate with another quantity we were trying to measure in our project.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8ctyd96xsoa"> <span class="footnote-back-link"><sup><strong><a href="#fnref8ctyd96xsoa">^</a></strong></sup></span><div class="footnote-content"><p> Not including stuff like Davidad&#39;s proposals, which while they give me some hope, there&#39;s little I can do to help.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnu6c8m98jn6j"> <span class="footnote-back-link"><sup><strong><a href="#fnrefu6c8m98jn6j">^</a></strong></sup></span><div class="footnote-content"><p> Ultimately hoping to construct a theorem of the form</p><blockquote><p> Given agent Alice with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="V"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.186em;">V</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and agent Bob with architecture <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="S"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;">S</span></span></span></span></span></span></span> trained with reward model <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="R_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">R</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> , end up with value systems <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_B"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span></span></span></span></span></span> respectively (not necessarily utility functions), and <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="U_A \sim_{E_A} U_B + \varepsilon"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.084em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.084em;">U</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ε</span></span></span></span></span></span></span> for some definition of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim_{E_A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">∼</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.317em; padding-right: 0.06em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span></span></span></span></span> that means something like Bob tries to achieve something like what Alice tries to achieve when in environment <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="E_A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.026em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;">E</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.241em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span></span></span> .</p></blockquote><p> And where we can solve for Bob&#39;s reward model and environment taking Alice as being a very ethical human. Hopefully with some permissive assumptions, and while constructing a dynamic algorithm in the sense that Bob can learn to do good by Alice&#39;s lights for a wide enough variety of Alices that we can hope humans are inside that class.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfxt68b6pmji"> <span class="footnote-back-link"><sup><strong><a href="#fnreffxt68b6pmji">^</a></strong></sup></span><div class="footnote-content"><p> Since, hopefully, if we have whole brain emulation it will be easy for the uploaded person or people to bootstrap themselves to superintelligence while preserving their goals (not a trivial hope!).</p></div></li><li class="footnote-item" role="doc-endnote" id="fndnadin1d3mt"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdnadin1d3mt">^</a></strong></sup></span><div class="footnote-content"><p> This is not to say the conditions under which whole brain emulation should give one hope are obvious.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmlwit0973hf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmlwit0973hf">^</a></strong></sup></span><div class="footnote-content"><p> More formally, regular models are one-to-one, and have fisher information matrix positive-definite everywhere.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7ze9pnljbu"> <span class="footnote-back-link"><sup><strong><a href="#fnref7ze9pnljbu">^</a></strong></sup></span><div class="footnote-content"><p> There is a different phase transition which occurs when the RLCT goes down, and some other quantity goes up. There are several candidates for this quantity, and as far as I know, we don&#39;t know which increase is empirically more common.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncnu4pysu68w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcnu4pysu68w">^</a></strong></sup></span><div class="footnote-content"><p> The reward landscape of deep reinforcement learning models is probably pretty insane. Perhaps not insane enough to not be liable to singular learning theory-like analysis, since there&#39;s always some probability of doing any sequence of actions, and those probabilities change smoothly as you change weights, so the chances you execute a particular plan change smoothly, and so your expected reward changes smoothly. So maybe there&#39;s analogies to be made to the loss landscape.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3jsz5ti3q8s"> <span class="footnote-back-link"><sup><strong><a href="#fnref3jsz5ti3q8s">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m told that Vanessa and Diffractor believe infra-Bayesianism can produce <a href="https://www.lesswrong.com/posts/d96dDEYMfnN2St3Bj/infrafunctions-and-robust-optimization">reflectively stable and useful quantilizers and worst-case optimizers</a> over a set of plausible utility functions. I haven&#39;t looked at it deeply, but I&#39;d bet it still assumes more ontology than I&#39;m comfortable with, both in the sense that for this reason it seems less practical than my imagined execution of what I describe here, and it seems more dangerous.</p></div></li><li class="footnote-item" role="doc-endnote" id="fncyq9ebydlz5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcyq9ebydlz5">^</a></strong></sup></span><div class="footnote-content"><p> In the sense that likely the mapping from brain states to policies is not one-to-one, and has singular fisher information matrix.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm9s3qj539of"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm9s3qj539of">^</a></strong></sup></span><div class="footnote-content"><p> In its current form it requires only that they be analytic, but ReLUs aren&#39;t, and empirically we see ignoring that aspect gives accurate predictions anyway.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3p9cx77pt1d"> <span class="footnote-back-link"><sup><strong><a href="#fnref3p9cx77pt1d">^</a></strong></sup></span><div class="footnote-content"><p> Situational novelty is not sufficient to be worried, but during reflection the model is explicitly thinking about how it should think better, so if its bad at this starting out, and makes changes to its thought, if those changes are to what it cares about, they will not necessarily be corrected by further thought or contact with the world. So situational novelty leading to incompetence is important here.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf23rfnmz6wq"> <span class="footnote-back-link"><sup><strong><a href="#fnreff23rfnmz6wq">^</a></strong></sup></span><div class="footnote-content"><p> A way it could be efficient is if its just <i>so</i> much easier to do statics than dynamics, you will learn much more about dynamics from all the static data you collect than you would if you just focused on dynamics.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/d4qbjx35SBMGyFNWZ/my-hopes-for-alignment-singular-learning-theory-and-whole<guid ispermalink="false"> d4qbjx35SBMGyFNWZ</guid><dc:creator><![CDATA[Garrett Baker]]></dc:creator><pubDate> Wed, 25 Oct 2023 18:31:14 GMT</pubDate> </item><item><title><![CDATA[Lying to chess players for alignment]]></title><description><![CDATA[Published on October 25, 2023 5:47 PM GMT<br/><br/><p> Eliezer Yudkowsky 最近<a href="https://www.facebook.com/yudkowsky/posts/pfbid05pVZ6QH5HhPTwJdmWMcLN5nws9aeC4gywmUv88QRhEnBUsdJas5KWC9EnDGJhSXrl">在 Facebook 上发布了</a>一项实验，该实验可能表明人类是否可以“让人工智能完成对齐作业”，尽管无法相信人工智能是否准确：看看人们在接受专家建议时是否会提高下棋能力，其中三分之二是说谎的。</p><p>我有兴趣尝试这个！如果还有人感兴趣，请发表评论。请告诉我您是否有兴趣成为：</p><p> A）听取建议并下棋并试图确定谁值得信赖的人</p><p>B）他们的对手，通常棋艺比 A 好，但比顾问差</p><p>C) 三位顾问之一，其中一位真诚地试图提供帮助，另外两位则试图破坏 A；三选完后随机选哪一个，防止A知道真相</p><p>请随意（实际上是鼓励）提供多种您愿意尝试的选项！谁被分配到什么角色将取决于有多少人做出反应以及他们的国际象棋能力水平，并且更容易找到可能的组合，并且更灵活地确定谁的角色。</p><p>另请简要描述您的国际象棋经验水平。您玩游戏的频率如何（如果有的话）；如果您有 ELO 评级，它们是什么以及它们来自哪些组织（FIDE、USCF、Chess.com 等）。无需经验！事实上，刚接触游戏的人都积极优先选择A！</p><p>最后，请告诉我您通常有空的日期和时间 - 当然，我不会要求您做任何事情，但这将有助于在我联系您确定具体时间之前给我一个估计。</p><p> Edit: also, please say how long you would be willing to play for - a couple hours, a week, a one-move-per-day game over the course of months? A multi-week or multi-month game would give the players a lot more time to think about the moves and more accurately simulate the real-life scenario, but I doubt everyone would be up for that.</p><br/><br/> <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment<guid ispermalink="false"> ddsjqwbJhD9dtQqDH</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 25 Oct 2023 17:47:16 GMT</pubDate></item></channel></rss>