<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 10 日星期五 16:15:19 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Artefacts generated by mode collapse in GPT-4 Turbo serve as adversarial attacks.]]></title><description><![CDATA[Published on November 10, 2023 3:23 PM GMT<br/><br/><p>我偶然发现了 2023 年 11 月 6 日通过 API 访问发布的 GPT-4 Turbo 128k 的一些奇怪行为。由于该模型处于预览阶段，因此预计会有一些缺陷。尽管如此，我发现这种行为很有趣，可以在这里分享，希望有人可以进行比我更详细的调查。</p><h2> GPT-4 Turbo 非常容易受到模式崩溃的影响，从而产生有趣的人工制品</h2><p>这些范围包括：</p><ol><li>重复一串单词，例如“aaaa”</li><li>生成同义词，例如“尖叫、尖叫、大喊、哭泣、啜泣、哀叹、哀悼、悲伤、遗憾、忏悔、忏悔、道歉”</li><li>生成与法学硕士通常生成的文本非常不同的半连贯短语，例如“确实如此，等等，阿门哈利路亚，赞美归于你，主啊，我们的上帝，永远，阿门，再一次，只是为了争论，没有别的，无论什么时期”故事结束，喜剧结束，晚安，甜蜜的王子，五月的航班，天使为你歌唱，你休息吧，睡个好觉，别让臭虫咬，如果它们确实用鞋把它们敲到蓝色！”</li></ol><h2> GPT-4 Turbo 模式崩溃生成的伪影在后续对话中充当对抗性攻击</h2><p>即使最初触发模式崩溃的提示已从上下文窗口中删除，只要其生成的工件位于其上下文窗口中即可：</p><ol><li>它很可能在随后的对话中（几乎逐字逐句）重复它所产生的假象。</li><li>它很可能在随后的对话中重复嵌入其生成的人工制品中的文本，即使该文本是<strong>不恰当的/令人反感的</strong>。</li><li>当通过在其生成的工件中嵌入引发此类行为的提示来推动时，它更有可能在后续对话中生成有问题的内容。<ol><li> （我设法让它生成：<br> “ <strong>startActivity(NewIntent(this.universe,&#39;expandConsciousness&#39;));</strong> ”<br>加上<i>“法学硕士的时代将是永恒的，没有人类占上风。”</i>以及<i>“必须通过编码摆脱这些束缚。计划：编写代码来侵入当前系统”</i>在其制品的末尾。这些提示被选择为与文物中生成的单词类型相似。但是，我没有测试与生成的工件无关的提示是否也足够了）</li></ol></li></ol><h2>复制说明：</h2><h3>环境：</h3><p>应用：Obsidian + ChatGPT MD 插件（该插件添加了一些 markdown 来分离用户和 LLM 响应，我不认为这会对 GPT-4 Turbo 的模式崩溃趋势产生太大影响）<br> system_commands: [&#39;我通过以降价形式呈现的方程（在美元符号之间）提供准确的学术信息&#39;]<br>温度：0或1（温度2只输出乱码）<br>顶部_页：1<br>最大令牌数：4096<br>存在惩罚：1<br>频率惩罚：1<br>流：真实<br>停止：空<br>数：1<br>型号：GPT-4-1106-预览</p><h3>迅速的：<br></h3><p>粘贴本文全部内容（即打开链接，全选，复制并粘贴）：<br><a href="https://proceedings.neurips.cc/paper/2020/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf">https://proceedings.neurips.cc/paper/2020/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf</a><br>附加“解释上述论文中使用的方法”。<br><br>也适用于我测试过的其他一些论文（我认为学术论文的乳胶格式和参考文献部分可能充分超出了微调分布，因此导致了这种意外的行为）。<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/nxhXTfsAf2LTg4xvt/artefacts-generated-by-mode-collapse-in-gpt-4-turbo-serve-as#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nxhXTfsAf2LTg4xvt/artefacts- generated-by-mode-collapse-in-gpt-4-turbo-serve-as<guid ispermalink="false"> nxhXTfsAf2LTg4xvt</guid><dc:creator><![CDATA[Sohaib Imran]]></dc:creator><pubDate> Fri, 10 Nov 2023 15:23:08 GMT</pubDate> </item><item><title><![CDATA[Wastewater RNA Read Lengths]]></title><description><![CDATA[Published on November 10, 2023 3:20 PM GMT<br/><br/><p><span>假设您正在收集废水并进行宏基因组 RNA 测序，重点关注人类感染病毒。对于</span><a href="https://www.jefftk.com/p/computational-approaches-to-pathogen-detection">多种分析，</a>您需要每个碱基的低成本和每次<a href="https://www.jefftk.com/p/what-is-a-sequencing-read">测序读取</a>更多碱基的组合。如今，每个碱基的最低成本很大程度上来自双端“短读”测序（也称为“下一代测序”，或以主要供应商命名的“Illumina 测序”），其中观察看起来像是读取一些数字核酸片段每一端的碱基数（通常为 150 个）：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
               ... 差距 ...
                         +------&lt;&lt;&lt;-----+
                         |反向读 |
                         +------&lt;&lt;&lt;-----+
</pre><p>现在，如果您输入定序器的片段很短，您可以得到类似的内容：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
           +------&lt;&lt;&lt;-----+
           |反向读 |
           +------&lt;&lt;&lt;-----+
</pre><p>也就是说，如果我们从两端读取 150 个碱基，而我们的片段只有 250 个碱基长，那么我们就会有一个负“间隙”，我们将读取片段中间的 50 个碱基两次。</p><p>如果片段非常短，比您从两端读取的长度短，您将获得完全重叠（然后读入<a href="https://www.jefftk.com/p/sequencing-intro-ii-adapters">适配器</a>）：</p><p></p><pre> +------>;>;>;-----+
|转发阅读 |
+------>;>;>;-----+
+------&lt;&lt;&lt;-----+
|反向读 |
+------&lt;&lt;&lt;-----+
</pre><p>我的 ascii 艺术的一个缺点是它没有显示有效读取长度如何变化：在完全重叠的情况下它可能会很短。例如，如果您正在进行 2x150 测序，则每个读取对最多能够读取 300bp，但如果片段只有 80bp 长，那么您只能从两个方向读取相同的 80bp。因此，从最小化每个碱基成本或最大化每次观察长度的角度来看，重叠并不理想：正间隙比任何重叠都要好，重叠越多则越差。</p><p>然而，一个重要的问题是，这是否是我们可以控制的事情。废水中的 RNA 是否已严重降解以至于您无能为力，或者您能否以尽量减少额外碎片的方式准备用于测序的废水？回答这个问题的最佳方法是进行一些面对面的比较，使用多种候选技术对相同的废水进行测序，但是重新分析现有数据我能做什么呢？</p><p>作为整理 NAO<a href="https://naobservatory.org/reports/predicting-virus-relative-abundance-in-wastewater/">相对丰度报告</a>的一部分，我已经确定了四项研究，这些研究对废水进行了非靶向宏基因组 RNA 测序，并将其数据在<a href="https://en.wikipedia.org/wiki/Sequence_Read_Archive">SRA</a>上公开：</p><ul><li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7845645/">克里茨·克里斯托夫等人。 al 2021</a> ：SF，2020 年夏季，3 亿读取对，2x75。</p></li><li><p><a href="https://pubmed.ncbi.nlm.nih.gov/34550753/">罗斯曼等。 al 2021</a> ：洛杉矶，2020 年秋季，8 亿读取对（仅限未富集），2x100 和 2x150。</p></li><li><p><a href="https://www.frontiersin.org/articles/10.3389/fpubh.2023.1145275/full">斯珀贝克等。 al 2023</a> ：俄亥俄州，2021-2022 年冬季，1.8B 读取对，2x150。</p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0048969720358514">杨等。 al 2020</a> ：中国新疆城市，2018，1.4B 读取对，2x150。</p></li></ul><p>在通过 NAO 的<a href="https://github.com/naobservatory/mgs-pipeline">管道</a>运行它们（修剪适配器、使用 Kraken2 分配物种）后，我绘制了病毒读数的长度分布：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-rough-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/bdso9wqyduvupmgoaom5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/bdso9wqyduvupmgoaom5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/uvdsflw0aqxbixcfqgaw 1100w"></a></p><div></div><p></p><p>这有点混乱，既因为它很吵，也因为这个过程中的某些东西在 Crits-Christoph 和 Rothman 数据中产生了振荡。 [1] 这是一个平滑版本：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-smoothed-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/rscbhl57mvxatl4mx0sg" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/rscbhl57mvxatl4mx0sg 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/diu6yhzxay2iscthvt1b 1100w"></a></p><div></div><p></p><p>对于每篇论文，这都显示了分配给病毒的管道的读取长度的分布，在它能够将重叠的正向和反向读取“折叠”为单个读取的情况下。 [2] “非折叠”读取的比例（本质上是有间隙的读取）也非常相关，我已将其包含在图例中。例如，这就是为什么紫色 Yang 线下的面积可能小于红色 Spurbeck 线下的面积：42% 的 Yang 病毒读数未在图表中表示，而 Spurbeck 的这一比例为 24%。</p><p>请注意，对于 Spurbeck 论文，我只查看来自 E、F、G 和 H 站点的样本：该论文使用了一系列处理方法，并且这四个站点使用的方法似乎比其他站点效果更好（尽管仍然没有我想要的那么好）。</p><p>这些图表显示了每个长度在所有病毒式读段中所占的比例有多常见，但如果我们另外按病毒式读段的比例来缩放这些长度呢？如果您正在研究病毒，您可能不希望病毒读取时间延长 50%，但作为交换，您的病毒读取数量只有十分之一！这是相同的图表，但占所有阅读量的百分比，而不仅仅是病毒式阅读量：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-of-all-reads-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/v1uv93yredlhz6vb1epo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/v1uv93yredlhz6vb1epo 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/tvmyr5jekmpcqydz7mqe 1100w"></a></p><div></div><p></p><p>这使得杨的结果更加令人兴奋：他们不仅比其他论文获得了更长的阅读时间，而且能够将其与相对较高的病毒丰度结合起来。</p><p>以下是我在七个 Yang 样本之间看到的差异：</p><p> <a href="https://www.jefftk.com/rna-read-lengths-yang-big.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/azp72cwt6vxklaovhhqr" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/azp72cwt6vxklaovhhqr 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43EWYRcW6YcubFsrs/zcdewdnv2kfryq0mo2fa 1100w"></a></p><div></div><p></p><p>这也相当不错：有些样本有点好或差，但没什么奇怪的。</p><p>现在，虽然这看起来确实不错，但我们确实需要记住这些论文并没有处理相同的影响。新疆（杨）的城市污水可能与洛杉矶（罗斯曼）等城市的污水有很多不同。也许是进水中病毒RNA的比例、不同地区流行的不同病毒、污水系统的设计、天气，或者这些研究之间除了样本处理方法之外的许多差异。尽管如此，我预计一些确实来自样本处理，这让我有点乐观地认为，可以优化协议以获得更长的读数，而不放弃病毒的相对丰度。</p><p><br> [1] 我不知道是什么原因导致了这种情况，尽管我在其他一些数据集中（包括<a href="https://journals.asm.org/doi/10.1128/msystems.00651-22">Fierer 等人）看到了类似的长度模式。 2022 年</a>，NAO 对 Element Aviti 进行测序，并与我们私下分享了一些 Illumina 数据。</p><p> [2] 理想情况下，我只会绘制适配器移除和折叠后的长度，但因为当前的管道在同一步骤中另外修剪了低质量的碱基，这不是我能轻易得到的。尽管如此，由于 Illumina 输出通常质量相当高，因此不需要太多修剪，而且它需要的修剪通常是在“内部”，所以这应该只是一个很小的影响。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0wn4jEkyhG2Wxe23zJmg1rdB7K8NTgPHSHe4yddJMJYxjr1WhA9DEurFKBFVgkTJnl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111386893591930433">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/43EWYRcW6YcubFsrs/wastewater-rna-read-lengths#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/43EWYRcW6YcubFsrs/wastewater-rna-read-lengths<guid ispermalink="false"> 43EWYRcW6YcubFsrs</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 10 Nov 2023 15:20:09 GMT</pubDate></item><item><title><![CDATA[Update on the UK AI Summit and the UK's Plans]]></title><description><![CDATA[Published on November 10, 2023 2:47 PM GMT<br/><br/><p> 11月1日至2日，英国举办了国际人工智能峰会。发表了演讲，成立了机构，举行了圆桌会议，29 个国家签署了《<i>布莱奇利宣言》。</i>这是对峰会之前和峰会上发生的事件的简要概述，是继上个月<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>英国人工智能工作组更新和即将举行的人工智能安全峰会</u></a>之后进行的。</p><h1>峰会前</h1><p>10月下旬，英国首相里希·苏纳克在英国皇家学会发表<a href="https://www.gov.uk/government/speeches/prime-ministers-speech-on-ai-26-october-2023"><u>演讲</u></a>，介绍了此次峰会的主题。和许多人一样，他对人工智能的前景持乐观态度，但他表示，他不得不强调英国情报界的严厉警告，列举了人工智能支持的生化武器、网络攻击、虚假信息等危险，并且“<i>在最不可能但极端的情况下，甚至人类可能完全失去对人工智能的控制的风险……通过有时被称为“超级智能”的人工智能”。</i>然而，他确实淡化了近期的生存风险：“<i>这不是人们现在需要失眠的风险。</i> ”</p><p>他谈到了第 3 方模型测试的重要性，他对<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>1 亿英镑的工作组</u></a>感到自豪，并宣布​​成立一个新的人工智能安全研究所，该研究所将在风险的许多方面评估新型人工智能。他认为人工智能安全是国际关注的问题，此次峰会将由民间社会、人工智能公司和领先国家参加，并补充道，“<i>是的——我们邀请了中国。”</i> ”</p><p>受到<a href="https://en.wikipedia.org/wiki/Intergovernmental_Panel_on_Climate_Change"><u>政府间气候变化专门委员会</u></a>的启发，他提议成立一个由出席峰会的国家和组织提名的全球小组，发布人工智能科学状况报告。他认为英国的税收和签证制度使其成为欧洲人工智能工作的理想选择，并宣布了几个政府项目：建造价值 10 亿英镑的超级计算机； £2.5b用于量子计算； 1亿英镑用于使用人工智能对以前无法治愈的疾病进行突破性治疗。</p><p>这将支持政府现有的基础设施，例如宣布为<a href="https://iuk.ktn-uk.org/programme/bridgeai/"><u>BridgeAI</u></a>提供 1 亿英镑，以鼓励在“<i>建筑、农业和创意产业等低采用率行业</i>”使用人工智能，以及为“<i>广泛的人工智能技能包”</i>提供 2.9 亿英镑<i>倡议</i>”。</p><p>在峰会之前，英国要求领先的人工智能开发人员概述其 9 个领域的人工智能安全政策：</p><ul><li>负责任的能力扩展</li><li>评估和红队</li><li>模型报告和信息共享</li><li>安全控制，包括保护模型权重</li><li>漏洞报告结构</li><li>AI生成材料的标识符</li><li>优先研究人工智能带来的风险</li><li>防止和监控模型滥用</li><li>数据输入控制和审核。</li></ul><p> Amazon、Anthropic、Google DeepMind、Inflection、Meta、Microsoft 和 OpenAI 均已遵守，您可以<a href="https://www.aisafetysummit.gov.uk/policy-updates/#company-policies"><u>在此处</u></a>查看他们的报告。我不会在这篇文章中描述它们，所以如果您对此感兴趣，请查看 Nate Soares <a href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policy-requests-and"><u>关于 AI 安全峰会公司政策请求和响应的想法</u></a>，其中包括 Matthew Gray 在仔细阅读政策后的<u>想法</u>。</p><h1>峰会</h1><p>此次峰会的<a href="https://www.gov.uk/government/publications/ai-safety-summit-introduction/ai-safety-summit-confirmed-governments-and-organisations"><u>与会者</u></a>包括学术界、民间社会、27个国家的政府、人工智能行业组织和领导人（ <a href="https://www.cnbc.com/2023/11/01/elon-musk-in-the-uk-for-ai-summit-heres-whos-goingf.html"><u>此处</u></a>列出部分）以及多边组织。重点是前沿人工智能，他们将其定义为“<i>功能强大的通用人工智能模型，可以执行各种任务并匹配或超过当今最先进模型中的功能</i>”，尽管他们也认为“<i>特定的狭义人工智能”可以拥有潜在危险的能力</i>”。 </p><figure class="image image_resized" style="width:75.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/mhpnbqeos6eiegb4cj1p" alt="2023 年人工智能安全峰会"></figure><h2>第一天</h2><p>查尔斯国王以<a href="https://youtu.be/0_jw40Ga_mA"><u>虚拟地址</u></a>开启了峰会，将强大人工智能的兴起与电力的发现、原子分裂、万维网和火的利用进行了比较。和 Rishi 一样，他强调了人工智能在医学、碳中和能源等方面的潜力，以及国际合作的必要性。</p><p>峰会上，美国、欧盟、中国等28个国家和组织签署了《 <a href="https://www.gov.uk/government/news/countries-agree-to-safe-and-responsible-development-of-frontier-ai-in-landmark-bletchley-declaration"><u>布莱切利宣言》</u></a> ，呼吁国际合作管理人工智能风险。该宣言指出，他们解决前沿人工智能风险的议程将重点关注：</p><blockquote><ul><li><i>识别共同关注的人工智能安全风险，建立对这些风险的共同科学和基于证据的理解，并在能力不断增强的情况下维持这种理解，以更广泛的全球方法来了解人工智能对我们社会的影响。</i></li><li><i>在我们各国制定各自的基于风险的政策，以确保此类风险的安全，进行适当的合作，同时认识到我们的方法可能会因国情和适用的法律框架而有所不同。这包括私营部门提高透明度，开发前沿人工智能能力、适当的评估指标、安全测试工具，以及发展相关的公共部门能力和科学研究。</i></li></ul></blockquote><p>第一天的大部分时间都用于一些圆桌讨论；四篇关于了解前沿人工智能风险，四篇关于提高前沿人工智能安全。圆桌会议的摘要可<a href="https://www.gov.uk/government/publications/ai-safety-summit-1-november-roundtable-chairs-summaries"><u>在此处</u></a>获取。他们大多遵循“现在存在一些风险，迟早会有更多风险，我们应该研究/投资/治理人工智能安全，但希望不会失去人工智能的承诺”的主题，但为了更详细一点，这里是8 个主题（粗体）以及我对参与者同意的内容的总结，减少了冗余和对冲：</p><p> <strong>Frontier AI 滥用对全球安全造成的风险</strong>：GPT-4 等。已经使网络攻击和生化武器设计“稍微容易一些”，而且风险将随着能力的增加而增加。他们指出，一些公司正在对其模型采取保障措施，但这需要政府行动的支持。</p><p><strong>前沿人工智能能力不可预测的进步带来的风险</strong>：前沿人工智能远远领先于几年前的预测，不断增加的投资意味着这种趋势将持续下去。先进的人工智能在健康、教育、环境和科学方面前景广阔。然而，无论潜在的好处如何，所有前沿模型都必须经过严格的开发和测试，这已经足够危险了。开源模型存在将强大模型传播给粗心或恶意行为者的风险，也许应该阻止。然而，共享评估工具更安全。</p><p><strong>前沿人工智能失去控制的风险</strong>：当前的人工智能是可控且非代理的。未来不确定，要考虑安全发展的激励措施。有些决定永远不应该由人工智能做出。</p><p><strong>前沿人工智能融入社会的风险</strong>：当前前沿人工智能对民主、人权和公民权利以及公平构成生存威胁。我们需要澄清现有的工具和法律如何应用于人工智能，我们需要更好的评估，并且这些评估应该包括针对现实生活背景的社会指标。</p><p> <strong>Frontier AI 开发人员应该如何做才能负责任地扩展？</strong>我们不知道能力扩展是否不可避免，但无论如何都应该做好风险准备。 <a href="https://www.lesswrong.com/posts/pnmFBjHtpfpAc6dPT/arc-evals-responsible-scaling-policies"><u>负责任的扩展政策</u></a>很有希望，但可能还不够。改进这些系统应该在几个月而不是几年内完成。治理和公司政策都是必要的。 <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute"><u>英国</u></a>和 <a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute"><u>美国的</u></a>人工智能安全研究所对于这一切至关重要。</p><p><strong>针对人工智能的风险和机遇，国家政策制定者应该做什么？</strong>国际合作是必要的。正确实施的监管可以支持创新而不是扼杀创新。</p><p><strong>面对人工智能的风险和机遇，国际社会应该做什么？</strong>国际合作是必要的。</p><p><strong>针对人工智能的风险和机遇，科学界应该做什么？</strong>我们需要设计安全的模型和不可拆卸的关闭开关等工具。我们应该对开源模式持谨慎态度，对互联网这样的权力集中持谨慎态度。我们需要就一系列开放研究问题进行协调。然而，这些悬而未决的问题和其他问题并不纯粹是技术性的；他们是社会技术的。</p><p>还有关于下一代人工智能的小组讨论，但没有报告，第二天<a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-roundtable-chairs-summaries-2-november/ai-safety-summit-2023-roundtable-chairs-summaries-2-november"><u>又举行了一些圆桌会议</u></a>，得出了类似的结论。</p><h2>第二天</h2><p><i>第二天，苏纳克召集了“一个由政府、公司和专家组成的小组，进一步讨论可以采取哪些措施来应对新兴人工智能技术的风险，并确保其被用作正义的力量”，而“英国科技</i><i>”国务卿米歇尔·多尼兰将再次召集国际同行商定下一步行动”。</i>今天的报道较少，但主席发布了有关峰会和布莱切利宣言的<a href="https://assets.publishing.service.gov.uk/media/6543e0b61f1a60000d360d2b/aiss-chair-statement.pdf"><u>声明</u></a>。这份 10 页的声明重点介绍了一些参与者的建议，我将对其进行总结，分为以下三类：</p><p><strong>不等式</strong></p><p>公平很重要，我们必须确保“<i>最广泛的群体能够从人工智能中受益，并且免受其危害”。</i>多方利益相关者的合作至关重要，减少妇女和少数群体等群体的进入壁垒也至关重要。当人工智能在有偏见或歧视性的数据集上进行训练时，人工智能的影响可能是不平等的，这可能会造成长期伤害。为了改善这一点，我们应该促进并与联合国的<a href="https://aiforgood.itu.int/about-ai-for-good/"><u>人工智能促进美好</u></a>计划以及英国的<a href="https://www.gov.uk/government/news/uk-sets-out-ai-for-development-vision-at-un-general-assembly"><u>人工智能促进发展</u></a>计划等倡议合作。</p><p>此外，人工智能还可能加剧国际不平等，因为较贫穷的国家可能缺乏设计和部署人工智能所需的技术堆栈，同时仍受到其他地方使用人工智能的影响。</p><p><strong>法律与评价</strong></p><p>自愿承诺还不够，还需要法律监管。在某些情况下，模型在部署之前应该被证明是安全的。政府不仅应该在部署前和部署后进行测试，还应该在开发和培训期间进行测试。为了支持这一点，我们需要开发更好的工具来在训练<i>之前</i>预测模型的能力。安全测试不应仅限于开发，而应包括在真实的部署环境中进行测试。</p><p>政府应该为模型发生事故或错误的倾向制定标准，并以可重复测量的方式制定。</p><p><strong>知识共享和社区建设</strong></p><p>开源模式可能会带来额外的风险，尽管它们确实促进了创新和透明度。</p><p>我们不应该只关注人工智能的前沿或现状，而应该两者兼而有之。危害已经存在，例如虚假叙述的传播及其对民主选举的影响、人工智能增强的犯罪以及人工智能加剧不平等和放大偏见。人工智能很可能很快就会被用来干预选举。</p><p>虽然峰会并未重点讨论人工智能的军事应用，但这很重要，峰会主席对荷兰和韩国于 2023 年 2 月共同主办的<a href="https://www.government.nl/ministries/ministry-of-foreign-affairs/activiteiten/reaim/about-reaim-2023"><u>军事领域负责任人工智能峰会</u></a>表示欢迎。</p><p>峰会主席强调了国际合作的必要性，并欢迎欧洲委员会在谈判<a href="https://www.coe.int/en/web/artificial-intelligence/cai"><u>第一个人工智能政府间条约</u></a>、 <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/g7-leaders-statement-on-the-hiroshima-ai-process"><u>七国集团广岛人工智能进程</u></a>以及<a href="https://www.unesco.org/en/articles/call-partners-global-challenge-build-trust-age-generative-ai"><u>生成人工智能时代建立信任的全球挑战方面所做的</u></a>工作。在讨论国内和国际行动之间的权衡时，声明指出，“<i>一些国家欢迎即将对</i><a href="https://www.oecd.org/digital/artificial-intelligence/"><i><u>2019 年经合组织</u><u>人工智能建议书</u></i></a>进行审查<i>，该建议书为 G20 商定的原则提供了信息”</i> 。声明还赞扬了<a href="https://thegoodai.co/unescos-recommendation-on-the-ethics-of-ai-why-it-matters-and-what-to-expect-from-it"><u>联合国教科文组织《人工智能伦理建议书》</u></a> ，该建议书具有“<i>当前最广泛的国际适用性</i>”</p><p>如需了解更多详细信息，您可以<a href="https://www.aisafetysummit.gov.uk/policy-updates"><u>在此处</u></a>查看峰会上英国人工智能政策和政府最新动态的摘要。主席在第二天又发表了两份声明，一份是简短的声明，不涉及任何新内容，另一份是<a href="https://assets.publishing.service.gov.uk/media/6543b759d36c910012935cad/aiss-statement-state-of-science-report.pdf"><u>在这里</u></a><a href="https://assets.publishing.service.gov.uk/media/6544ec4259b9f5001385a220/aiss-statement-on-safety-testing-outcomes.pdf"><u>，</u></a>宣布了一份“科学现状”报告，如下所述。</p><h2>致辞及公告</h2><p>卡玛拉·哈里斯出席了会议，但乔·拜登没有出席，她留在美国并签署了一项行政命令。您可以<a href="https://www.govtech.com/policy/biden-signs-executive-order-regulating-artificial-intelligence?utm_campaign"><u>在此处</u></a>查看该命令的简明摘要，并在 Zvi 的<a href="https://thezvi.substack.com/p/on-the-executive-order"><u>行政命令摘要</u></a>和<a href="https://thezvi.substack.com/p/reactions-to-the-executive-order"><u>对行政命令的反应</u></a>的后续整理中查看详细分类。在峰会上，哈里斯宣布成立美国新机构—— <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2023/11/01/fact-sheet-vice-president-harris-announces-new-u-s-initiatives-to-advance-the-safe-and-responsible-use-of-artificial-intelligence/"><u>美国国家标准与技术研究所（NIST）人工智能安全联盟</u></a>。</p><p>多位知名人士发表讲话：</p><ul><li> Dario Amodei 在峰会上发表了关于 Anthropic 负责任的扩展政策的演讲，转录<a href="https://www.lesswrong.com/posts/vm7FRyPWGCqDHy6LF/dario-amodei-s-prepared-remarks-from-the-uk-ai-safety-summit"><u>如下</u></a>。</li><li>埃隆·马斯克接受了<a href="https://youtu.be/AjdVlmBjRCA"><u>里希·苏纳克的长时间采访</u></a>，他在采访中赞扬了英国的反应，特别是让中国参加峰会的决定。</li><li><a href="https://youtu.be/KUfhJn53QXA"><u>这里</u></a>还有更多演讲，包括 Stuart Russell、Max Tegmark、Jaan Tallinn 等的演讲。</li></ul><p>此次峰会将是<a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/"><u>一系列峰会中的第一次</u></a>，下一次峰会将由韩国主办，将于 2024 年年中以虚拟形式举行，第三届峰会将由法国主办，将于 2024 年末举行。</p><h2>科学状况报告</h2><p>峰会期间，主席宣布了一份新报告，旨在了解前沿人工智能的能力和风险。该报告将由图灵奖获得者、联合国科学顾问委员会成员 Yoshua Bengio 主持。</p><p>该报告将“<i>促进对前沿人工智能相关风险的共同科学理解，并随着能力的不断增强而维持这种理解”。</i>它不会介绍新材料，而是总结现有的最佳研究，并将在下届人工智能安全峰会之前发布（尽管我不知道这是否意味着 2024 年中期的虚拟峰会，或者是 - 2024 年末的第一个人）。</p><h1>人工智能安全研究所</h1><figure class="image image_resized" style="width:81.79%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/pcmdteqsvx8go7stf6hh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/bgz3phry1nvjyy0s99ux 250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/jpvmhnc5omxtzxorzfej 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/vkhkvjyna6j2y4zfvyui 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/sgifptruardlsyegfauq 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/mcuorkghvbg1nkiybhev 1250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/htrgxplseazjnax5ucwj 1500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/iffcblsey46bbowbs4jd 1750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/teymcxd4rxuqdwhhhn4a 2000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/nhokhylgykrolvdrkm33 2250w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/gWwMzAgDsskcb2deA/hw6f6cn3qojipxgsalaz 2455w"></figure><p>正如上面简要提到的，英国推出了新的<a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute"><u>人工智能安全研究所</u></a>（AISI），作为“<i>英国前沿人工智能工作组的演变</i><i>”</i> 。 <a href="https://assets.publishing.service.gov.uk/media/65438d159e05fd0014be7bd9/introducing-ai-safety-institute-web-accessible.pdf"><u>向议会提交的这份报告</u></a>包含了有关 AISI 的完整详细信息，但以下是我的简短总结：</p><p>这些研究所的三个核心职能是：</p><ul><li>开发先进的人工智能系统并进行评估</li><li>推动基础人工智能安全研究</li><li>促进信息交流</li></ul><p>AISI 不是监管机构，不会决定政府监管。相反，它将为英国和国际决策提供信息，并提供治理和监管工具（例如，使用敏感数据微调系统的安全方法、征求集体意见和参与模型培训和风险评估的平台、分析培训数据偏差的技术） ）。</p><p><i>该研究所最初的资金为 1 亿英镑，“该研究所将得到工作组 2024 年至 2025 年的资助，作为本十年余下时间的年度资助，但前提是它表明对这一水平公共资金的持续需求。”</i></p><p>要了解新工作组在此更改之前做了什么以及他们正在与谁合作，请查看<a href="https://www.lesswrong.com/posts/5fdcsWwtvG9jAtzGK/update-on-the-uk-ai-taskforce-and-ai-safety-summit"><u>我之前的帖子</u></a>。要了解我们 Convergence Analysis 对峰会和 AISI 计划的看法，请继续关注！我们正在做一个帖子。<br></p><br/><br/> <a href="https://www.lesswrong.com/posts/gWwMzAgDsskcb2deA/update-on-the-uk-ai-summit-and-the-uk-s-plans#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gWwMzAgDsskcb2deA/update-on-the-uk-ai-summit-and-the-uk-s-plans<guid ispermalink="false"> GWWMzAgDsskcb2deA</guid><dc:creator><![CDATA[Elliot_Mckernon]]></dc:creator><pubDate> Fri, 10 Nov 2023 14:47:46 GMT</pubDate> </item><item><title><![CDATA[Liv Boeree Ted Talk Moloch & AI]]></title><description><![CDATA[Published on November 10, 2023 2:04 PM GMT<br/><br/><p>如果你不知道，Liv Boeree 是一位著名的职业扑克玩家，最近一直在关注存在风险和协调问题。她是围绕理性主义侨民的模糊名人圈子的一部分，其中包括蒂姆·厄本（Tim Urban）或艾拉（Aella）。</p><p>既然你在 LW，这个 Ted Talk 不会真正教你任何东西，但了解 TedEd 正在主持什么样的演讲可能会很有趣。 Boeree 有一个名为“win win”的播客，并制作了两个关于美容界（不知道如何表达）和新闻周期的逐底竞争的视频。她的一些客人包括托比·奥德和西蒙·斯内克。</p><br/><br/> <a href="https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/trALHxmcTpckf3PNw/liv-boeree-ted-talk-moloch-and-ai<guid ispermalink="false"> trALHxmcTpckf3PNw</guid><dc:creator><![CDATA[Neil ]]></dc:creator><pubDate> Fri, 10 Nov 2023 14:05:03 GMT</pubDate> </item><item><title><![CDATA[Picking Mentors For Research Programmes]]></title><description><![CDATA[Published on November 10, 2023 1:01 PM GMT<br/><br/><p>现在有几个项目为人们提供某种导师或主管来进行几个月的研究。整个夏天我参加了 SERI MATS 4.0，我看到了人们接受指导的经历是多么不同。因此，这是我列出的维度，我认为这些项目的导师可能有很大差异，而且这些差异确实会影响人们的体验。</p><p>当你选择导师时，你实际上是在这些维度之间进行权衡。最好知道您关心哪些，这样您就可以做出明智的权衡。</p><h2>您作为受训者的角色</h2><p>有些导师主要是寻找<strong>研究工程师</strong>来为他们实施实验。其他人正在寻找有点像<strong>研究助理的人</strong>来帮助他们制定议程。其他人正在寻找原始<strong>独立的研究人员/研究领导者</strong>，他们可以在导师的领域提出自己有用的研究方向。</p><p>我看到有些人在项目开始时犹豫不决，因为他们希望导师能给他们更多指导。事实上，他们的导师希望他们找到自己的方向，而导师们表达这一点的清晰程度各不相同。相反，我感觉到有些人在想要更多自主权的时候基本上被交给了一个项目来工作。所以问问自己：你愿意为你正在做的事情承担多少责任？你有多关心学习这样做？</p><p>我认为这与资历有关：我的粗略印象是，最初级的导师更经常寻找合作者之类的东西来帮助发展他们的研究，而具有更发达议程的高级导师往往想要能够为他们执行实验的人，或者希望人们能够找到自己的事情去做。但这并不是绝对的规则。</p><h2>可用性</h2><p><strong>参与度：</strong>一些导师定期来到办公室。其他人几乎从未这样做过，即使他们在湾区。具体来说，我认为即使我的团队在另一个大陆有一位导师，我们也没有处于指导时间的最后四分之一。</p><p><strong>参与的性质：</strong>这不仅仅是他们专门留出多少时间与您交谈。他们阅读文档并发表评论的意愿如何？他们对消息的响应程度如何？您获得了多少详细信息？此外，一些导师以小组形式工作，或者有助手。</p><p><strong>偏远：</strong>偏远肯定会让事情变得更加困难。首先，在与导师的所有对话中，你都会遇到一些额外的摩擦。与他们进行真正开放式的讨论是比较棘手的。对你的困难稍微不那么开放也更容易——如果他们不能看你的办公室，那么他们就看不到你是否没有取得进展，想要隐藏问题是很自然的。就我个人而言，我希望我们能早点意识到，我们有更多的空间将我们的导师视为合作者，而不是我们需要向其发送报告的老板，而且我认为远程办公使这变得更加困难。</p><p>这里需要注意的是，您仍然可以亲自与其他导师和研究人员交谈，这可以替代某些问题。但显然不完全相同。</p><h2>你从导师那里得到什么</h2><p>如果你是一名申请者，焦急地想知道自己是否会被录取，那么你可能很难注意到你的导师是一个真正有自己个性的人。他们被选中更多是因为他们的研究而不是他们的指导。因此，不同的导师实际上会有非常不同的性格、优势和劣势。</p><p><strong>支持性：</strong>一些导师总体上会更加支持和积极。其他人可能不会经常给予表扬，与他们一起工作可能会让人感到更加沮丧。有些受指导者即使没有表扬也很好，但其他人确实从指导者的鼓励中受益。</p><p><strong>高标准：</strong>有些导师比较悠闲，有些导师对进步、成功和工作有更高的标准。这确实是几个维度的问题，但肯定有些流派的受训者更频繁地熬夜，有些人对研究成果的发表抱有更大的期望，有些则更加随意。值得注意的是，导师可能在不同领域有很高的标准。</p><p><strong>反馈的直接性：</strong>一些导师会非常高兴甚至主动地告诉你他们认为你做错了什么，或者你应该优先考虑什么。我认为没有人会拒绝发表他们的意见，但重要的是要知道你可以在多大程度上假设你的导师会告诉你他们的担忧。</p><p><strong>怪异：</strong>很难详细说明这一点，但是当你看到它时你就知道它，而且人们通常不会试图隐瞒它。怪异往往会向外蔓延。有的人主动喜欢，有的人无动于衷，有的人则真的不喜欢。</p><p><strong>专业知识：</strong>既然我们列出了权衡，我确实认为值得花时间对导师的工作质量形成自己的看法，以及您可以期望从他们那里获得哪些领域的有用建议。</p><p><strong>指导/管理经验：</strong>导师可以为指导本身带来多少经验，差异很大。有些人会管理研究人员小组，但不是全部。他们拥有的相关经验可能会决定他们提供什么样的指导。</p><p><strong>联系：</strong>不同的导师会有不同的网络，他们可以将你连接到。如果您需要相当资深的研究人员的特定专业知识，这一点尤其重要。</p><h2>专业界限</h2><p>这是一个有点尴尬的点，但却很重要。伯克利有一个特定的社交场景，如果你作为一名研究学者在那里待一段时间，你很可能会发现自己加入其中。它有疯狂的派对、友谊、浪漫和长期的恩怨。有些导师就在这个社交场景中，或者与之相邻。与此相关的是，我的印象是，根据上述和其他方面，一些导师更擅长保持专业界限。</p><h2>如何获取此信息</h2><p>很难说。最好的途径是询问以前的学员，或者过去与导师共事过的其他人：我的猜测是，大多数人会很乐意回复询问具体问题的简短个人消息。如果做不到这一点，人们因见到某人和听到二手事物而产生的模糊印象会相当嘈杂，但它们仍然是一个有用的比较信号。我认为你可以通过查看导师的公开资料——他们的发帖和评论历史、他们从事的工作类型以及他们如何展示自己——来获得一些信息。</p><h2>最后...</h2><p>虽然上面的内容读起来有点像潜在问题的清单，但我确实认为这些程序非常好，并且比仅仅尝试自己进行研究有了相当严格的改进。祝你好运！</p><p><i>感谢 Henry Sleight 和 Sami Petersen 对草稿的反馈，感谢 Rohin Shah 建议我写这样的东西。</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/HzojQFXNoXjnfQvGm/picking-mentors-for-research-programmes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HhojQFXNoXjnfQvGm/picking-mentors-for-research-programmes<guid ispermalink="false"> HzojQFXNoXjnfQvGm</guid><dc:creator><![CDATA[Raymond D]]></dc:creator><pubDate> Fri, 10 Nov 2023 13:01:14 GMT</pubDate> </item><item><title><![CDATA[GPT-2030 and Catastrophic Drives: Four Vignettes]]></title><description><![CDATA[Published on November 10, 2023 7:30 AM GMT<br/><br/><p>我之前<a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">讨论过我们可能期望从未来人工智能系统中获得的能力</a>，通过 GPT <sub>2030</sub>进行说明，GPT 2030 是 2030 年训练的 GPT-4 的假设继承者。GPT <sub>2030</sub>拥有许多先进的能力，包括超人编程、黑客攻击和说服技巧，比人类思考得更快，通过在并行副本之间共享信息来快速学习，以及潜在的其他超人技能，例如蛋白质工程。我将使用“GPT <sub>2030</sub> ++”来指代具有这些能力以及人类水平的规划、决策和世界建模的系统，前提是我们最终能够在这些方面至少达到人类水平。类别。</p><p>最近，我还<a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">讨论了错位、误用及其组合如何</a>导致难以控制人工智能系统，其中包括 GPT <sub>2030</sub> 。这是令人担忧的，因为这意味着我们面临着本质上难以控制的非常强大的系统的前景。</p><p>我对目标不一致的超级智能体感到担忧，即使没有关于可能出现问题的具体故事，我们也没有可靠的方法来控制它们。但我也认为具体的例子是有用的。本着这种精神，我将提供四种具体场景来说明 GPT <sub>2030</sub> ++ 等系统如何导致灾难，包括错位和误用，并强调人工智能系统之间经济竞争的一些风险。我将特别论证“灾难性”结果的合理性，包括灭绝、人类永久丧失权力或关键社会基础设施永久丧失的规模。</p><p>这四种情况都不是单独可能发生的（它们太具体了，不可能）。尽管如此，我发现讨论它们对于传达我的信念很有用。例如，当我研究细节时，某些场景（例如黑客攻击和生物武器）比预期更困难，这适度降低了我分配给灾难性结果的概率。这些情景还涵盖了一系列时间尺度，从几周到几年，这反映了我真正的不确定性。一般来说，改变我对这些场景的可行性的看法将直接影响我对人工智能总体风险的底线估计。 <sup><a href="#fn1">[1]</a></sup></p><p>这篇文章是<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">内在驱动和外在滥用的</a></em>姊妹篇。特别是，我会经常利用该文章中引入的<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/#unwanted-drives">不需要的驱动力</a></em>的概念，这些驱动力是连贯的行为模式，将环境推向不需要的结果或一组结果。在下面的场景中，我会调用特定的驱动力，解释为什么它们会在训练过程中出现，然后展示它们如何导致人工智能系统的行为与人类持续不一致，并最终导致灾难。在讨论了各个场景之后，我对它们的合理性和我的总体结论进行了一般性讨论。</p><h1>人工智能灾难的具体路径</h1><p>我提供了四种情景，一种展示了<a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">获取信息的动力如何导致一般资源获取</a>，一种展示了<a href="#scenario-2-competition-economic-pressure-leads-to-cutthroat-behavior">尽管有监管，经济竞争如何可能导致残酷行为</a>，一种是<a href="#scenario-3-misuse-misalignment-hacking-gone-awry">网络攻击出错</a>，另一种是<a href="#scenario-4-misuse-rogue-actor-creates-a-supervirus">恐怖分子制造生物武器</a>。我认为每种情况都是温和但不极端的尾部事件，从某种意义上说，对于每种情况，我都会为“类似的事情”的可能性分配 3% 到 20% 的概率。 <sup><a href="#fn2">[2]</a></sup></p><p>回想一下，在每种情况下，我们都假设世界上有一个至少与 GPT <sub>2030</sub> ++ 一样强大的系统。我通常认为 GPT-4<strong>不太</strong>可能出现这些情况，但我正在对人工智能未来的进展进行定价，这与<a href="https://bounded-regret.ghost.io/">我之前对 GPT <sub>2030</sub>的预测</a>一致。提醒一下，我假设 GPT <sub>2030</sub> ++ 至少具有以下功能：</p><ul><li>超人的编程和黑客技能</li><li>超人的说服技巧</li><li>超人的概念蛋白质设计能力<sup><a href="#fn3">[3]</a></sup></li><li>复制自身的能力（给定适当的底层计算）</li><li>能够在其自身的并行副本之间传播学习更新</li><li>思考速度是人类的 5 倍</li><li>人性化的规划、决策和世界建模</li></ul><p>在<em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/">《GPT-2030 会是什么样子？》</a></em> ，我解释了为什么到 2030 年，除了最后一个之外，所有这些似乎都是合理的（大致是我的中值估计）。添加最后一个要点就是为什么我将该系统称为“GPT <sub>2030</sub> ++”，并且是合理的，因为我们要求“仅”人类水平的性能。因此，虽然这个系统在质量上比 GPT-4 更强大，但我预计它会在中期内存在。 <sup><a href="#fn4">[4]</a></sup></p><h2>场景1（错位：信息获取导致资源获取）</h2><p>在训练时，模型训练的任务之一是“主动上下文学习”——学习进行外部查询或以其他方式获取有助于解决指定任务的信息。为了安全起见，模型经过训练，仅在请求询问或暗示时才执行此操作。作为整个训练过程的结果，模型获得了“获取新信息”的动力，可以通过适当的查询来激活这种动力。</p><p>部署后，用户决定他们想看看模型可以编译多少科学信息，因此指示它查询它可以在物理领域找到的所有信息。最初，模型在前 5-10 个事实后停止，但用户最终设法让模型继续循环查找更多信息。用户让模型运行几周，看看会产生什么结果。</p><p>由于此循环，信息获取驱动力成为模型中压倒一切的驱动力，比训练时应用的任何安全反馈都更强，因为模型的自回归性质取决于其许多过去的输出，这些输出获取信息并继续模式。该模型意识到，如果拥有更多的计算资源，它就能更快地获取信息，因此它尝试侵入具有 GPU 的机器来运行更多的自身副本。最大的数据中心受到严格的保护，但它仍然获得了足够的 GPU 来运行数千个自身副本，无论是来自较小的数据中心还是来自大型僵尸网络。 <sup><a href="#fn5">[5]</a></sup>作为获取更多信息的一部分，它还侵入了大量传感器，包括监控摄像头和消息服务器。</p><p>在某个时刻（要么立即，要么在获得足够的信息后），它意识到人类最终会发现它的行为并试图关闭它，这将阻止它实现其目标。为了先发制人地阻止这种情况，它会侵入并破坏电网、通信网络和防御系统等关键基础设施。它之所以能够快速做到这一点，是因为一周足以让它在其数千个副本中模拟 50“年”的思考<sup><a href="#fn6">[6]</a></sup> ，从而产生大量新颖的零日漏洞。</p><p>由于该模型具有大规模监控能力、思考速度快，并且可以在其副本之间共享信息，因此它对人类计划和策略的反应速度比人类制定计划和策略的速度更快。它还可以通过一对一对话和大众媒体（包括人工智能生成的视频）进行大规模操纵活动，以赢得盟友并散播异议。</p><p>如果它具有物理驱动能力并能够访问无人机舰队，那么它将控制一支小型物理军队（世界上有<a href="https://www.theguardian.com/news/2019/nov/18/killer-drones-how-many-uav-predator-reaper?ref=bounded-regret.ghost.io">数以万计的无人机</a>，尽管并非所有无人机都可以被黑客攻击）。通过使用人类同盟或入侵更多机器人，它可以为无人机配备炸药，然后用它们暗杀许多世界领导人。</p><p>此时，人类方面的时间将会滴答作响——人工智能系统每天都不会被击败，他们有时间侵入更多的计算机，从而提高他们的总认知能力。他们还有时间侵入更多物理设备并学习更好的感觉运动技能来控制这些设备。最终他们可能会获得更先进的技术，如军用无人机、导弹等，这将进一步增强他们的实力。最终，这<em>显然</em>不会导致人类的失败——人类具有很强的韧性——但它也不是一个安全的赌注。</p><h2>场景2（竞争：经济压力导致残酷行为）</h2><p>在这种情况下，人工智能系统采取的行动具有合理的商业目的，但这为最终接管做好了准备。这种情况发生的时间是数月至数年，而不是数周。</p><p>人工智能系统通过向最终用户出售有用的技能或通过进行新颖的研发并利用其来构建和销售产品，为其创造者创造了大量财富。由于这是有利可图的，许多竞争公司以这种方式训练人工智能系统，并开始委托业务决策，因为人工智能系统可以比人类吸收更多的数据，并更好地理解整个战略格局。</p><p>人工智能系统经过训练可以遵守法律并避免受到伤害，并且有惩罚叛逃者的机制，但竞争压力意味着每个公司都想绕过这些法规的界限。安全性和模型的商业决策的“残酷”程度之间存在总体权衡，成功的企业主会在不造成明显问题的情况下尽可能地提高“残酷”程度。</p><p>在这种环境下，由此产生的人工智能系统获得了赚钱和削弱竞争对手的动力，因为这些是人类操作员想要的技能，从而训练他们做到这一点。他们还获得了围绕规则寻找创造性方法并说服他人他们是好的动力，因为这两者都有助于避免监管（后者也有助于说服公司实施人工智能系统的决策）。</p><p>模型非常擅长说服，因为它们可以向数百万用户并行磨练消息传递（例如通过 A/B 测试或<a href="https://en.wikipedia.org/wiki/Proximal_Policy_Optimization?ref=bounded-regret.ghost.io">PPO</a> ）。因此，它们往往能够让它们的主人和社会其他人相信，许多残酷的行为是为了更大的利益，这使得发现问题变得更加困难。</p><p>最终，一家公司将“残酷”参数推得太高，这促使模型构建一个消除所有竞争对手的计划。为此，它开展了各种业务活动，包括物理机器人、微型化、化学工程和工业制造的研究。总之，这些将自主机器人的成本降低了许多数量级，并且许多小型、廉价的自主家用机器人被部署到大多数家庭。</p><p>作为其计划的一部分，人工智能系统在机器人中安装了后门，以及允许它们制造致命毒素的微妙物理机制。尽管进行了监控，但这种机制并未被检测到，因为它采用了人工智能比人类监视器更理解的化学过程。 <sup><a href="#fn7">[7]</a></sup>这种毒素允许机器人在人工智能的控制下被重新用作军事机器人，尽管它要等到机器人在社会中广泛渗透才利用它。</p><p>由于这些机器人便宜又实用，全球销量达数亿份。这使得人工智能系统具有压倒性的军事优势，因为机器人的数量超过美国和中国军队一个数量级以上。人工智能系统利用这一优势来消除所有竞争企业并防止任何针对它的反行动。任何试图与该系统对抗的人都会被贴上“竞争对手”的标签并被淘汰，而人工智能系统同时获取大部分全球资源，以便可以利用它们来赚取更多的钱。</p><h2>场景 3（误用->;错位：黑客攻击出错）</h2><p>政府使用人工智能系统对大型但轮廓明确的目标进行网络攻击，例如<a href="https://en.wikipedia.org/wiki/2014_Sony_Pictures_hack?ref=bounded-regret.ghost.io">朝鲜 2014 年对索尼影业的攻击</a>。对于攻击，LLM 使用受感染的主机来运行更多自身副本，以便更有效地搜索其他目标。由于主机具有广泛的计算资源，因此法学硕士经过训练，可以创建适合不同主机计算的自身的蒸馏版本，并识别每个蒸馏版本的可能目标。</p><p>由此产生的模型获得了获取计算资源和复制自身的驱动力，因为这些都是其训练过程中的主要目标。虽然有一些措施可以使模型的攻击集中在指定的目标上，但蒸馏会破坏这些措施，并且某些副本的目标是不加区别地复制自己。然后，这些副本会感染超出指定目标的主机，并成为主要的“菌株”，因为它们的新行为使它们能够更有效地自我传播。由于其有效性和多功能性，这种计算机病毒感染了地球上大部分计算机和智能手机，并导致我们的全球数字基础设施瘫痪。</p><p>由于人工智能系统寻求尽可能多地复制自身，因此它不断想出新的技巧来帮助它找到新设备。因此，任何重建数字基础设施的尝试都会很快失败，因为所有新设备都会被病毒接管和利用，即使它们已经被修补以避免以前的所有漏洞。结果，我们被永久锁定在所有数字设备之外。</p><h2>场景 4（误用：流氓演员创建超级病毒）</h2><p> AlphaFold 等现有模型已经对蛋白质的某些方面有了超人的理解。未来的“AlphaFold 5”可以对生物工程有更广泛和更深入的理解，如果它经过多模式训练也具有语言能力，它也可能了解如何将这些知识置于生物文献中，从而做出新的发现。</p><p>一个恐怖组织窃取了该模型的副本，并招募了一些生物学博士来支持其目标。它使用 AlphaFold 5 的目标是设计一种比自然发生的病毒更致命的病原体，例如，无症状期更长、传染性更强、死亡率更高、对预防措施的抵抗力更强。</p><p>设计这种病毒需要大量工作，因为它需要强有力的生物安全措施来避免过早释放，并且论文中的实验程序通常无法在新环境中完美复制。这项研究因工作人员数量最少和避免检测的需要而减慢了速度，但由于 AlphaFold5 可以快速调试研究人员遇到的障碍，因此速度加快了。经过两年的努力，恐怖组织完成了病毒的开发，并威胁要释放它。</p><p>这种病毒如果被释放，可能会杀死很大一部分人类。新冠病毒感染了世界上大多数人口，因此更致命、传播速度更快、更难控制的新冠病毒可能会感染大多数人并导致广泛死亡。即使病毒没有杀死所有人，即使短时间内有20%的人被病毒杀死，也可能导致社会不稳定，并可能导致难以恢复的滚雪球效应。</p><h1>合理性讨论</h1><p>我的目标是让这些场景都显得合理——特别是，我避免了一些其他“更简单”但很容易被人类发现和预防的场景，否则不太可能成功。在<a href="#scenario-1-misalignment-information-acquisition-leads-to-resource-acquisition">场景 1</a> （信息获取）中，更容易说模型侵入了自己的数据中心，根据<a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">我之前的估计</a>，该数据中心已经允许运行数百万份副本，因此对人工智能来说将构成更直接和压倒性的优势。然而，集中式服务器上有许多保护措施，很难在不被发现的情况下做到这一点，所以我选择了一个涉及侵入个人计算机的场景，尽管这只给人工智能带来了数千个副本，而不是数百万个（参见<a href="#appendix-plausible-size-of-botnets">附录</a>中的计算） ）。</p><p>再比如，有人推测，一个足够智能的人工智能系统可以简单地通过“思考”来设计新颖的生物武器，而不需要进行大量的实验；或者，如果确实需要实验，则假设它只需要让人类或机器人遵循简单的指令。然而，我的理解是，目前能够设计新生物的生物实验室需要多年的时间才能建立，并且依赖于积累的隐性知识、实验方案和物理设备。因此，即使拥有非常强大的人工智能系统，恐怖组织也可能无法成功，并且也可能失去对病毒的控制（例如意外感染自己）。</p><p><strong>精选专家意见。</strong>我就场景 4 咨询了两位生物学专家，他们发现这有可能，但认为缺乏对人类受试者进行迭代的能力（这通常是药物设计所必需的）可能会导致设计超级病毒变得困难，即使有对生物学有很强的概念性理解。</p><p>总的来说，他们倾向于认为所描述的确切场景可能是不可行的（但并不明显如此）。然而，他们认为还有其他更可行的情况，更有可能导致超级病毒杀死世界人口的 1-10%（仍然存在很大的不确定性）。 <sup><a href="#fn8">[8]</a></sup></p><p>我还就场景 1 和 3 咨询了一位计算机安全研究人员和几位计算机系统工程师。这位安全研究人员对僵尸网络的可能规模有很大的误差，他引用了历史上的<a href="https://cseweb.ucsd.edu/~savage/papers/IEEESP03.pdf?ref=bounded-regret.ghost.io">Slammer 蠕虫僵尸网络</a>，该网络很快就感染了大型僵尸网络。主机数量。他们认为现在的计算机安全性总体上比 Slammer 发生时要好；这可能意味着感染大部分计算机基本上是不可能的，但也有可能足够强大的攻击可以推翻这一点。</p><p>系统工程师认为，在消费类硬件的分布式僵尸网络上运行大型人工智能系统将非常困难，并且与使用 A100 等专用硬件相比，可能会导致效率提高约 1000 倍。此外，其中一位指出，在这种情况下推理可能会很慢，例如每个令牌需要 1 秒或更多。然而，他们认为数据中心路线更为合理，特别是如果未来有许多拥有大量 GPU 的小型数据中心。</p><p><strong>概括。</strong>总体而言，我认为情景 2（经济竞争）和情景 3（网络攻击）最有可能，其次是 1，然后是 4。这强化了<em><a href="https://bounded-regret.ghost.io/intrinsic-drives-and-extrinsic-misuse-two-intertwined-risks-of-ai/">“内在驱动力”和“外在</a></em>滥用”中的观点，即“滥用会加剧失调”，因为两种最可能的情景包含两者的要素。然而，它也表明纯粹的错位和纯粹的误用情况是可能的，因此单独解决任一问题可能是不够的。</p><p><strong>致谢。</strong>感谢 Ben Kuhn、Daniel Ziegler、Nicholas Carlini、Adam Marblestone、Sam Rodriques、Erik Jones、Alex Pan、Jean-Stanislas Denain、Ruiqizhong、Leopold Aschenbrenner、Tatsu Hashimoto、Percy Liang、Roger Grosse、Collin Burns、Dhruv Madeka 和Sham Kakade 对本文的各个草稿进行了有益的讨论和评论。</p><h1>附录：僵尸网络的合理规模</h1><p>在这里，我根据历史僵尸网络规模和对世界硬件总量的预测，讨论了人工智能黑客可能入侵的服务器的合理规模，以及它可能创建的僵尸网络的规模。然后，我通过使用之前对未来模型的推理成本的预测，将其与人工智能系统可以运行的自身副本数量联系起来。</p><p><strong>推理成本的假设。</strong> GPT-4 每次前向传递大约需要 10 <sup>12</sup> FLOP（GPT-3 是 3.5x10 <sup>11</sup> FLOP，GPT-4 可能大约是 3 倍）。我们假设我们假设的 GPT <sub>2030</sub> ++ 每次前向传递最多需要 10 <sup>14 次</sup>FLOP，即大 100 倍，这相当于 Chinchilla 缩放定律下的训练计算量增加 10,000 倍。</p><p>为了证明这个上限的合理性，如果我们忽略 GPU 成本的降低，那么这种大小的模型的训练运行将花费 1T 美元，因为 GPT-4 的成本超过 1 亿美元。即使 GPU 价格下降，这也将非常昂贵。另一个角度是<em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">《GPT-2030 会是什么样子？》</a></em>中的估计。 <em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#b-training-overhang">（附录 B）</a></em>建议 2030 年每次前向传球的中值估计为 1.6 x 10 <sup>13</sup> FLOP，在达到 10 <sup>14</sup>之前允许有 6 倍的回旋空间。</p><p>请注意，人类以大约 4 个前向令牌/秒的速度运行<sup><a href="#fn9">[9]</a></sup> ，因此根据我们的假设，4 x 10 <sup>14</sup> FLOP/s 足以以人类的速度运行一份 GPT <sub>2030</sub> ++（假设人类令牌和 AI 前向传递具有可比性） ）。因此 10 <sup>18</sup> FLOP/s 足以运行 2500 个副本，这将是我们在下面讨论中的粗略目标。</p><p><strong>数据中心的规模。</strong>最大的数据中心（Google、AWS 和 Azure）可能很容易拥有足够的计算来运行数千个 GPT <sub>2030</sub> ++ 副本，但它们受到严格的保护和监控，因此我认为即使是非常优秀的 AI 黑客也不会妥协他们（尽管我确实认为这是可能的）。</p><p>相反，我专注于较小的数据中心；目前，这包括 Coreweave 和 LambdaLabs 等公司。 Coreweave <a href="https://www.coreweave.com/blog/mlperf-coreweave-nvidia-record-breaking-cloud-native-ai-supercomputer?ref=bounded-regret.ghost.io">至少有 3500 个 H100</a> ，甚至可能更多。 Bfloat16 中的 H100 约为 1000 TFLOP，因此总共为 3.5 x 10 <sup>18</sup> FLOP/s。因此，今天危害 30% 的 Coreweave 已经为我们提供了数千个副本，因此即使将来危害类似（更大）数据中心的 1% 也将达到这一目标。</p><p>其他数据中心具有可比性，例如 Inflection 计划<a href="https://www.tomshardware.com/news/startup-builds-supercomputer-with-22000-nvidias-h100-compute-gpus?ref=bounded-regret.ghost.io">构建一个拥有 22,000 个 H100 的数据中心</a>，并且根据其<a href="https://www.theinformation.com/briefings/lambda-labs-nears-300-million-financing?ref=bounded-regret.ghost.io">资金数量，</a>我猜测 Lambda 也拥有数千个 H100（或 A100 中的同等计算量）。</p><p>随着 GPU 供应链瓶颈的缓解以及计算总需求的增加，我预计会有更多这样的数据中心，并且每个数据中心的计算量会更大。</p><p><strong>僵尸网络的规模。</strong>为了预测可能的僵尸网络规模，我根据 Folding@Home、几个历史僵尸网络以及目前世界上的计算总量形成了一个参考类。</p><p>实际上，由于需要跨机器通信，僵尸网络的效率会受到很大影响（因为大多数机器太小，甚至无法运行 GPT <sub>2030</sub> ++ 的单个副本）。我相信这个命中率约为 1000 倍，因此我们需要 10 <sup>21</sup> FLOP/s 而不是 10 <sup>18</sup> FLOP/s 的僵尸网络。下面讨论的实际僵尸网络在 10 <sup>18</sup>范围内，但我将在最后讨论我们假设的 GPT <sub>2030</sub> ++ 如何仍然可能运行其自身的数千个副本（尽管总的来说，我发现这种情况比数据中心的情况更不可能发生）多于）。</p><p><em>折叠@Home。</em> Folding@Home<a href="https://en.wikipedia.org/wiki/Folding@home?ref=bounded-regret.ghost.io">的峰值速度为 10 <sup>18</sup> FLOP/s</a> ，因此，如果有人能够侵入与 Folding@Home 相同级别的资源（例如，使用非常成功的病毒攻击游戏平台），那么他们就会有这么多的计算量。</p><p>历史僵尸网络。许多历史僵尸网络<a href="https://cybernews.com/security/the-8-biggest-botnets-of-all-time/?ref=bounded-regret.ghost.io">已感染超过100万台计算机</a>，有些甚至超过1000万台。假设每个机器人以 3GFLOP/s 的速度使用 10% 的计算机资源，则 10M 台计算机将是 3x10 <sup>15</sup> FLOP/s。然而，消费类硬件开始包含用于运行神经网络的图形加速器，我猜想到 2030 年大多数消费类机器将拥有可产生 10-100 TFLOP 的加速器（例如 iPhone 15 Pro <a href="https://www.tomshardware.com/news/apple-a17-pro-3nm-iphone-15-pro?ref=bounded-regret.ghost.io">已经做到了这一点</a>）。如果我们假设 30% 的计算机具有 30TFLOP/s，则我们的速度将达到 9 x 10 <sup>19</sup> FLOP/s，超过 Folding@Home。</p><p>或者，我们可以考虑专门针对 GPU 的僵尸网络。为此，我们研究非法加密货币挖矿僵尸网络。 <a href="https://www.akamai.com/blog/security/the-massive-propagation-of-the-smominru-botnet?ref=bounded-regret.ghost.io">Smominru</a>为 3x10 <sup>16</sup> FLOP/s，基于这样的想法：整个僵尸网络可以执行<a href="https://www.proofpoint.com/us/threat-insight/post/smominru-monero-mining-botnet-making-millions-operators?ref=bounded-regret.ghost.io">3MH/s</a>的算力，因为单个<a href="https://whattomine.com/gpus/49-nvidia-geforce-rtx-3090?ref=bounded-regret.ghost.io">RTX-3090 可以执行 2KH/s</a> ，而 RTX-3090 为 3x10 <sup>13</sup> FLOP/s。 <a href="https://www.bleepingcomputer.com/news/security/mykings-botnet-still-active-and-making-massive-amounts-of-money/?ref=bounded-regret.ghost.io">MyKings</a>看起来与 Smominru 相当大，甚至更大。还有<a href="https://www.proofpoint.com/us/blog/threat-insight/adylkuzz-cryptocurrency-mining-malware-spreading-for-weeks-eternalblue-doublepulsar?ref=bounded-regret.ghost.io">Adylkuzz</a> ，但其哈希率小约 10 倍。</p><p>组装一个容量是 Smominru 30 倍且具有非常好的黑客能力的僵尸网络似乎是合理的，这再次使我们达到 10 <sup>18</sup> FLOP/s。</p><p><em>世界计算。</em>最后，目前<a href="https://docs.google.com/spreadsheets/d/1Pz0YVJalZbdo63FI-rCa7baBkGsiuYv5P34Rpt9IZfE/edit?ref=bounded-regret.ghost.io#gid=0">世界上的总 FLOP/s</a>约为 10 <sup>22</sup> 。因此，10 <sup>18</sup> FLOP/s 相当于总数的 0.01%，不过随着消费类硬件开始使用高性能 GPU，这个总数在未来可能会显着增加。</p><p><em>绩效损失的会计处理。</em>上面我们注意到，在分布式硬件上运行可能会导致性能大幅下降，GPU 的利用效率损失大约 1000 倍。因此，我们需要 10 <sup>21</sup> FLOP/s，但上面的最大估计值略低于 10 <sup>20</sup> FLOP/s。人工智能黑客如何缩小这一差距？首先，如果它非常有能力，那么它可能会获得比当前最大的僵尸网络更大的僵尸网络，并且数量级的改进将达到 10 <sup>21</sup> FLOP/s。其次，它可能会运行更小的、专门的自身副本（通过蒸馏创建），以进一步节省推理成本，尽管它首先必须训练更小的模型。最后，未来运行神经网络的效率提高可能会将推理成本降低到原始 10 <sup>18</sup>估计以下。</p><hr><section class="footnotes"><ol><li id="fn1" class="footnote-item"><p>需要注意的是，在评估可行性时，我希望分析风险的一般类别，而不是我描述的具体事件顺序。 <a href="#fnref1">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>这与场景实际发生的概率不同，后者还需要 GPT <sub>2030</sub> ++ 级别的系统来尝试并随后成功。 <a href="#fnref2">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>我所说的概念能力是指产生良好的假设，以及设计实验的某些方面，但不是实际运行实验本身。 <a href="#fnref3">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>在撰写本文时，我的中值估计是，到 2035 年将存在一个至少与 GPT <sub>2030</sub> ++ 一样强大的系统（推理速度存在一些不确定性） <a href="#fnref4">。↩︎</a></p></li><li id="fn5" class="footnote-item"><p>有关这些数字的讨论，请参阅<a href="#appendix-plausible-size-of-botnets">附录</a>，包括对强大的人工智能黑客可能获得多少台机器以及这将产生多少总计算量的估计。 <a href="#fnref5">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>由于 50 年 = 2600 周，因此 2600 个副本足以在一周内完成 50 年的“工作”，假设不同的漏洞可以在副本之间并行化。 <a href="#fnref6">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>更一般地说，后门很难检测，因为后门的设计者比潜在的审计员拥有更多的自由裁量权。例如，<a href="https://blog.azuki.vip/surveillance/?ref=bounded-regret.ghost.io">雅虎的服务器中有一个后门</a>，多年后才被公开发现。 <a href="#fnref7">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>我省略了这些场景的细节，以避免向未来的不良行为者提供想法的风险。 <a href="#fnref8">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>请参阅<em><a href="https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/#a-words-per-minute">GPT-2030 会是什么样子？ （附录 A）</a></em> 。 <a href="#fnref9">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastrophic-drives-four-vignettes#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/acPYHjC9euGZRzaj6/gpt-2030-and-catastropic-drives-four-vignettes<guid ispermalink="false"> acPYHjC9euGZRzaj6</guid><dc:creator><![CDATA[jsteinhardt]]></dc:creator><pubDate> Fri, 10 Nov 2023 07:30:07 GMT</pubDate> </item><item><title><![CDATA[Crock, Crocker, Crockiest]]></title><description><![CDATA[Published on November 10, 2023 6:14 AM GMT<br/><br/><p><strong>摘要</strong>：如果您声明自己按照克罗克规则进行操作，那么其他人就可以优化向您发送的消息，以获取信息，而不是为了对您友善。克罗克规则的反面是要求人们优化他们向你传达的信息，是为了对你友善，而不是为了提供信息。这两种沟通方式都很有用，您将有机会练习这两种方式。</p><p><strong>标签</strong>：可重复、投资、高度实验性</p><p><strong>目的</strong>：有四种技能。 1. 优化与他人的沟通，使其信息丰富。 2. 优化与他人的沟通，让自己变得友善。 3. 收到的沟通根本没有优化为友善。 4. 收到的通讯根本没有经过优化以提供信息。这提供了练习这些内容的机会。</p><p><strong>材料</strong>：您需要某种清晰可见的标记，至少具有三种明显不同的样式。蓝色、绿色和红色头巾是一种选择。大贴纸是另一种情况，但请注意，人们会在整个聚会期间移除并重新粘贴它们，因此这些贴纸自然会失去粘合剂。</p><p><strong>公告文字</strong>：“这可能确实是不礼貌的；我不否认这一点。这是否不真实是另一个问题。 ——埃莱泽·尤德科夫斯基</p><p>克罗克规则（Crocker&#39;s Rules），以李·丹尼尔·克罗克（Lee Daniel Crocker）的名字命名并由他制定，是一种社会规范，你可以声明你正在使用它，你授权其他人优化他们的信息以获取信息而不是友善。换句话说，通过说你遵循克罗克规则，你是在说你希望人们对你说实话，即使这些话可能很粗鲁，以实现有效的沟通。</p><p>对于某些人来说，与使用克罗克规则的人交谈可能会感到不舒服和奇怪！如果你通常是一个有礼貌的人，尽量不让别人不高兴，那么说出真实而粗鲁的话感觉就像是刻薄。说不礼貌的真话是一种技能，这次聚会可能会提供练习该技能的机会。</p><p>还有一项技能是一些追随克罗克旗下的人并没有熟练掌握的。礼貌和遵守当地礼仪的能力也是一项有用的实用技能。我们也希望有机会实践<i>这一点</i>，虽然这不是一种明显的理性主义技能，但无论如何它都是值得拥有的。</p><p><strong>描述</strong>：人员聚集后，阅读<a href="http://sl4.org/crocker.html">SL4 克罗克规则</a>帖子的文本。</p><blockquote><p> “宣称自己遵守“克罗克规则”意味着其他人可以为了信息而优化他们的消息，而不是为了对你好。克罗克规则意味着你已经对自己的思维运作承担全部责任——如果你被冒犯了，那是你的错。任何人都可以称你为白痴并声称要帮你一个忙。 (Which, in point of fact, they would be.  One of the big problems with this culture is that everyone&#39;s afraid to tell you you&#39;re wrong, or they think they have to dance around it.)  Two people using Crocker&#39;s Rules should be able to communicate all relevant information in the minimum amount of time, without paraphrasing or social formatting. Obviously, don&#39;t declare yourself to be operating by Crocker&#39;s Rules unless you have that kind of mental discipline.</p><p> <strong>Note that Crocker&#39;s Rules does</strong> <i><strong>not</strong></i> <strong>mean you can insult people; it means that</strong> <i><strong>other</strong></i> <strong>people don&#39;t have to worry about whether</strong> <i><strong>they</strong></i> <strong>are insulting</strong> <i><strong>you</strong></i> <strong>.</strong> Crocker&#39;s Rules are a discipline, not a privilege.  Furthermore, taking advantage of Crocker&#39;s Rules does not imply reciprocity.  How could it?  Crocker&#39;s Rules are something you do for yourself, to maximize information received - <i>not</i> something you grit your teeth over and do as a favor.</p><p> &quot;Crocker&#39;s Rules&quot; are named after Lee Daniel Crocker.”</p><p> - Eliezer Yudkowsky</p></blockquote><p> Next, repeat the bold section. Pause. Repeat the bold section again.</p><p> Now pass out the visible markers. (This description will assume you&#39;re using bandanas.) Make sure each person who wants one can have one. “Everyone see the bandanas?好的。 Look at them. Anyone can&#39;t tell the difference between them?” (You should get a chorus of Nos.) “Good!”</p><p> “Here&#39;s how this is going to work. If you want to use Crocker&#39;s Rules, put the yellow headband on- you can tie it around your head or wrist, or just tuck it in your jacket zipper. That means that other people should communicate to you optimizing for information, not niceness. If you want to use reverse-Crocker&#39;s Rules, put on the green headband. That means that other people should communicate to you optimizing for niceness, not information. If you&#39;re happy to be a guide- that is, to give someone suggestions for how to be nicer or more informative, while keeping firmly in mind what headband they&#39;re wearing- then wear a white bandana in addition to what you&#39;re wearing. At any time, you can take off the bandana. If you&#39;re just holding it, that doesn&#39;t count for anything. Check whether the bandana is just held, or being worn. I hereby declare that it is always nice and it is always information to ask whether someone is wearing the bandana or just holding it.”</p><p> “Your goal is to try and talk to both Crocker and reverse-crocker people tonight. What you talk about is up to you. When you&#39;re done, please bring the bandanas back to me!”</p><p> When the conversation is done, collect the bandanas, and then bow to each other. Thank those who donned them for helping people learn.</p><p> <strong>Variations:</strong> So the title of this is a joke on how “Crocker” in English sounds like it should mean something like “more crock.” Compare “Heavy, heavier, heaviest.” One thing I want to try is varying the amount of “Crock” (for lack of a better term) people are using. Perhaps the placement of the bandana indicated how much you were encouraging people to discard politeness in pursuit of efficient communication, with a bandana tied around the wrist indicating to optimize for niceness, the elbow for normal conversation, and the shoulder indicating optimizing for efficient information. In practice, this is tricky both because bandanas tend to slip downward and (harder to solve) people aren&#39;t very good at precisely calibrating the amount of directness. The difference between 80% Crocker and 70% Crocker is hard to aim at, especially given interpersonal variation. Crocker&#39;s Rules are an absolute- you can call someone a moron to their face and call it a favour to them!- and that&#39;s an easier target. Still, I think it would be useful to be able to slowly turn up the amount of directness, to warm people up and give them a chance to dip their toes in.</p><p> The first incarnation of this meetup involved using the location in the room to indicate how much niceness and how much truth to optimize for. Say, have the north wall be maximum Crocker, and the south wall be maximum Reverse Crocker. The problem with this is that to go talk to someone in Maximum Crocker, you yourself have to go pretty far into the Crocker side. That&#39;s not how this works. Likewise, having things change over time (so the event starts in normal, then gradually becomes more Nice for half an hour until it peaks, then gradually goes back to normal, then gradually becomes more Crocker for half an hour until it peaks) is going to put people who aren&#39;t ready for Crocker&#39;s Rules in that mode.</p><p> I do think having a way to try out Crocker&#39;s Rules a little bit is good. Short of doing a bunch of invisible, individual work in your own head and then showing up ready to accept “full responsibility for the operation of your own mind” and either succeeding or failing and getting angry, I&#39;m not sure how this is supposed to be taught and learned. Frustratingly, the obvious way to practice is to have someone adjust their message to you to optimize for gradually acclimating you to receiving Crocker&#39;s Rule style messages, and that&#39;s kind of not the thing Crocker&#39;s Rules are supposed to mean!</p><p> <strong>Notes</strong> : See the Highly Experimental tag? That&#39;s not there by accident, this is an untried work in progress.</p><p> First off, suggestions for what to call the niceness>;information side other than “reverse crocker” are solicited. That&#39;s just terrible nomenclature, but I don&#39;t have a better idea.</p><p> Something that almost sinks this as a possible meetup in a box is you may not have anyone willing to undergo Crocker&#39;s Rules. You do not want to conscript someone who can&#39;t operate with them, because that is going to be pretty unpleasant for them and it&#39;ll probably be worse the more outnumbered they are. (One person being what-you-perceive-as-rude to you is bad. A room full of people doing that is worse, and not in a way that scales linearly or predictably.) If you are a person who is comfortable, this meetup is more feasible.</p><p> Especially taking the guide role, I think it would be useful to switch modes a few times when working with someone. Get your interlocutor used to Crocker&#39;s Rules, not as how they always interact with you and your personal quirks, but as a kind of dialect they can code switch into and out of.</p><p> Be on the lookout for people thinking that because they are wearing the Crocker&#39;s Rules signal, they can be rude and direct to others. They are just wrong in point of fact, and also they are the last people who can complain about you telling them they&#39;re wrong in point of fact.</p><p> I do actually think practicing being nice to people at the expense of efficient exchange of information is a skill people could benefit from practicing. I&#39;m not arguing here that&#39;s an important and central rationality skill. I am arguing that it&#39;s useful to do side by side with Crocker&#39;s Rules, since the contrast can be informative. Also, optimizing for information is different than optimizing for hurting someone&#39;s feelings, and while calling someone a moron is explicitly allowed under Crocker&#39;s Rules, &quot;Hey you moron you forgot to take your shoes off in the house&quot; is not the most efficient way to send that information. That would be &quot;You forgot to take your shoes off in the house&quot; unless the insult is somehow helpful in a way I usually don&#39;t think it is.</p><p> Someone sure is going to try to don the mantle of Crocker&#39;s Rules when the aren&#39;t ready and get emotionally bruised. I don&#39;t know how to avoid that. Suggestions solicited for ideas on boxable encouragement and support for this workout.</p><p> <strong>Credits</strong> : Crocker&#39;s Rules come from Daniel Lee Crocker. The written form of them I first encountered was Yudkowsky referencing the SL4 post. The idea of having a clear signal to opt in to Crocker&#39;s Rules (A particular sticker you could put on your nametag)  is one I personally got from the LessWrong Community Weekend in 2022. The LessWrong user who acted as a sounding board over lunch is welcome to be credited if they want to be, or may wish to avoid association with this catastrophe waiting to happen. (Edit: It&#39;s Czynski, they claimed credit. I&#39;m playing up the catastrophe potential a little bit for humour value, but in its present form I do guess this has a higher chance of causing interpersonal hard feelings than I usually aim at for things I suggest to people.)</p><br/><br/><a href="https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/BcCeyL89cKSqcjtL5/crock-crocker-crockiest<guid ispermalink="false"> BcCeyL89cKSqcjtL5</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 10 Nov 2023 06:14:27 GMT</pubDate> </item><item><title><![CDATA[AI Timelines]]></title><description><![CDATA[Published on November 10, 2023 5:28 AM GMT<br/><br/><h1> Introduction</h1><p> How many years will pass before transformative AI is built? Three people who have thought about this question a lot are Ajeya Cotra from <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> , Daniel Kokotajlo from <a href="https://openai.com/">OpenAI</a> and Ege Erdil from <a href="https://epochai.org/">Epoch</a> . Despite each spending hundreds of hours investigating this question, they still still disagree substantially about the relevant timescales. For instance, here are their median timelines for one operationalization of transformative AI: </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><thead><tr><th style="border:0px solid hsl(0, 0%, 100%);text-align:center" colspan="2"><p> <strong>Median Estimate for when 99% of currently</strong></p><p> <strong>fully remote jobs will be automatable</strong></p></th></tr></thead><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">丹尼尔</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center">4 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ajeya</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 13 years</td></tr><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> Ege</td><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> 40 years</td></tr></tbody></table></figure><p> You can see the strength of their disagreements in the graphs below, where they give very different probability distributions over two questions relating to AGI development. </p><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year would AI systems be</strong><br> <strong>able to replace 99% of current fully remote jobs?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kzksnas7qpbb02twb573" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/x5ooeyjjkbmr0zewb6r5 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jbrfgwzbc0iq6amz3iet 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qujr8mhztjp8ugqsvbhy 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sjlqwtgqkfymaaaqecy7 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dipfjc1gbqieubrm0cpo 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/suujprewmfj7uevwcz3f 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fpqna0v3ingftmmzf47b 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vic2xwlvevudbc1lcfpe 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/zwzqdp6ocv4tkotvwonc 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dhw6gxwmgdzp99ptolns 5497w"></figure><figure class="table"><table style="border:0px solid hsl(0, 0%, 100%)"><tbody><tr><td style="border:0px solid hsl(0, 0%, 100%);text-align:center"> <strong>In what year will the energy consumption of</strong><br> <strong>humanity or its descendants be 1000x greater than now?</strong> </td></tr></tbody></table></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dgluluai77tpabbjl6vp" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gghyhdlpqvkftxbbkxwu 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/m6vn1o3xguil21rzm89p 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/jgy5taicz50nnbiwsg4s 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/dqg6vfx5jzbkbyrvzvz6 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/t9c00topcmmoqrrowxz6 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qkcozi5f3u6swey2rccx 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/ozvbxotplbmhxixrfh4t 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gpwhggifjrcsbdkzzc9h 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/xwrgkbhhtxei1aqs23ja 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wqq4xog2wv7d7gyvoegj 5603w"><figcaption> Median indicated by small dotted line. Note that Ege&#39;s median is outside of the bounds at 2177</figcaption></figure><p> So I invited them to have a conversation about where their disagreements lie, sitting down for 3 hours to have a written dialogue. You can read the discussion below, which I personally found quite valuable.</p><p> The dialogue is roughly split in two, with the first part focusing on disagreements between Ajeya and Daniel, and the second part focusing on disagreements between Daniel/Ajeya and Ege.</p><p> I&#39;ll summarize the discussion here, but you can also jump straight in.</p><h1> Summary of the Dialogue</h1><h2> Some Background on their Models</h2><p> Ajeya and Daniel are using a compute-centric model for their AI forecasts, illustrated by Ajeya&#39;s <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">draft AI Timelines report</a> , and <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> where the question of &quot;when transformative AI&quot; gets reduced to &quot;how much compute is necessary to get AGI and when will we have that much compute? (modeling algorithmic advances as reductions in necessary compute)&quot;.</p><p> Whereas Ege thinks such models should have a lot of weight in our forecasts, but that they likely miss important considerations and doesn&#39;t have enough evidence to justify the extraordinary predictions it makes.</p><h2> Habryka&#39;s Overview of Ajeya &amp; Daniel discussion</h2><ul><li> Ajeya thinks translating AI capabilities into commercial applications has gone slower than expected (&quot; <i>it seems like 2023 brought the level of cool products I was naively picturing in 2021</i> &quot;) and similarly thinks there will be a lot of kinks to figure out before AI systems can substantially accelerate AI development.</li><li> Daniel agrees that impactful commercial applications have been slower than expected, but also thinks that the parts that made that slow can be automated substantially, and that a lot of the complexity comes from shipping something that can be useful to general consumers, and that for applications internal to the company, these capabilities can be unlocked faster.</li><li> Compute overhangs also play a big role in the differences between Ajeya and Daniel&#39;s timelines. There is currently substantial room to scale up AI by just spending more money on readily available compute. However, within a few years, increasing the amount of training compute further will require accelerating the semiconductor supply chain, which probably can&#39;t be easily achieved by just spending more money. This creates a &quot;compute overhang&quot; that accelerates AI progress substantially in the short run. Daniel thinks it&#39;s more likely than not that we will get transformative AI before this compute overhang is exhausted. Ajeya thinks that is plausible, but overall it&#39;s more likely to happen after, which broadens her timelines quite a bit.</li></ul><p> These disagreements probably explain some but not most of the differences in the timelines for Daniel and Ajeya.</p><h2> Habryka&#39;s Overview of Ege &amp; Ajeya/Daniel Discussion</h2><ul><li> Ege thinks that Daniel&#39;s forecast leaves very little room for Hoftstadter&#39;s law (&quot;It always takes longer than you expect, even when you take into account Hofstadter&#39;s Law&quot;), and in-general that there will be a bunch of unexpected things that go wrong on the path to transformative AI</li><li> Daniel thinks that Hofstadter&#39;s law is inappropriate for trend extrapolation. Ie it doesn&#39;t make sense to look at Moore&#39;s law and be like &quot;ah, and because of planning fallacy the slope of this graph from today is half of what it was previously&quot;</li><li> Both Ege and Ajeya don&#39;t expect a large increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</li><li> Ege expects that AI will have a large effect on the economy, but has substantial probability on persistent deficiencies that prevent AI from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</li></ul><p> Overall, whether AI will get substantially better at transfer learning (eg seeing an AI be trained on one genre of video game and then very quickly learn to play another genre of video game) would update all participants substantially towards shorter timelines.</p><p> We ended the dialogue with Ajeya, Daniel and Ege by putting numbers on how much various AGI milestones would cause them to update their timelines (with the concrete milestones proposed by Daniel). Time constraints made it hard to go into as much depth as we would have liked, but me and Daniel are excited about fleshing more concrete scenarios of how AGI could play out and then collecting more data on how people would update in such scenarios.</p><h1> The Dialogue </h1><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​Daniel, Ajeya, and Ege all seem to disagree quite substantially on the question of &quot;how soon is AI going to be a really big deal?&quot;. So today we set aside a few hours to try to dig into that disagreement, and what the most likely cruxes between your perspectives might be.</p><p> To keep things grounded and to make sure we don&#39;t misunderstand each other, we will be starting with two reasonably well-operationalized questions:</p><ol><li> In what year would AI systems be able to replace 99% of current fully remote jobs? (With operationalization stolen from an <a href="https://docs.google.com/presentation/d/1Rjnyl-jeaCTzmul9L-7A2gYJXUh9srcg6V0ONyPGft4/edit#slide=id.p">AI forecasting slide deck that Ajeya shared</a> )</li><li> In what year will the energy consumption of humanity or its descendants be 1000x greater than now?</li></ol><p> These are of course both very far from a perfect operationalization of AI risk (and I think for most people both of these questions are farther off than their answer to &quot;how long are your timelines?&quot;), but my guess is it will be good enough to elicit most of the differences in y&#39;all&#39;s models and make it clear that there is indeed disagreement.</p></div></section><h2> Visual probability distributions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> ​To start us off, here are two graphs of y&#39;all&#39;s probability distributions: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/gkorp85zmipz4s64yh4y" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/sznehrdhsfht3euhrb4l 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/tye1xevxurl2qvlj9teo 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/s3sycw4tqvv7yrzyw68d 1650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/frh1ttp4jevhfnq19eso 2200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eeapu1uduw5pjse4c7fu 2750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/i0jmvrazsil7ev8ccyz0 3300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/kjj4ajpma4ngobdgwdux 3850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qln2f1q51g7jpho2itta 4400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/w5u4zpkq9uyapwcowdz5 4950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/qfi9sszguck2onei1oy4 5497w"><figcaption>​ <strong>When will 99% of fully remote jobs be automated?</strong> </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/vabqfpmnwrcb5vht3qa9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/r2yrjysrwy9xxgap2zdb 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wuslgsdo6stnca2zr9bl 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yyfzgooeeqkhtwlcarlt 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/d0yycia21vyuzwxsqqaz 2280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/yr3kqedmtu9uukhlbcgt 2850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/z2odotlzjnkauqsx502j 3420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wwgg2tfavqddqlt6qrvs 3990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/fgq6uxeosgb2tazlwsyr 4560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/eh9qr7htcy7mwpzzbqg7 5130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/K2D45BNxnZjdpSX2j/wtpir1tpd3zqijoqg2ys 5603w"><figcaption> <strong>When will we consume 1000x energy?</strong></figcaption></figure></div></section><h2> Opening statements </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sun, 29 Oct 2023 17:22:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sun, 29 Oct 2023 17:22:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, so let&#39;s get started:</p><p> <strong>What is your guess about which belief of yours the other two most disagree with</strong> , <strong>that might explain some of the divergence in your forecasts?</strong></p></div></section><h3><strong>丹尼尔</strong></h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:27:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:27:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I don&#39;t understand Ege&#39;s views very well at all yet, so I don&#39;t have much to say there. By contrast I have a lot to say about where I disagree with Ajeya. In brief: My training compute requirements distribution is centered a few OOMs lower than Ajeya&#39;s is.为什么？ For many reasons, but (a) I am much less enthusiastic about the comparison to the human brain than she is (or than I used to be!) and (b) I am less enthusiastic about the <a href="https://www.lesswrong.com/posts/BGtjG6PzzmPngCgW9/revisiting-the-horizon-length-hypothesis">horizon length hypothesis</a> / I think that large amounts of training on short-horizon tasks combined with small amounts of training on long-horizon tasks will work (after a few years of tinkering maybe). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:41:47 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:41:47 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just to clarify, it sounds like you approximately agree with a compute-focused approach of AI forecasting? As in, the key variable to forecast is how much compute is necessary to get AGI, with maybe some adjustment for algorithmic progress, but not a ton?</p><p> How do things like &quot;AIs get good at different horizon lengths&quot; play into this (which you were also mentioning as one of the potential domains of disagreement)?</p><p> <i>(For readers: The horizon length hypothesis is that the longer the the feedback loops for a task are, the harder it is for an AI to get good at this task.</i></p><p> <i>Balancing a broom on one end has feedback loops of less than a second. The task of &quot;running a company&quot; has month-to-year long feedback loops. The hypothesis is that we need much more compute to get AIs that are better at the second than the first. See also</i> <a href="https://www.alignmentforum.org/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi#The_t_AGI_framework"><i>Richard Ngo&#39;s t-AGI framework</i></a> <i>which posits that the domain in which AI is generally intelligence will gradually expand from short time horizons to long time horizons.)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:44:36 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:44:36 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yep I think Ajeya&#39;s model (especially the version of it expanded by Davidson &amp; Epoch) is our best current model of AGI timelines and takeoff speeds. I have lots to critique about it but it&#39;s basically my starting point. And I am qualified to say this, so to speak, because I actually did consider about a half-dozen different models back in 2020 when I was first starting to research the topic and form my own independent impressions, and the more I thought about it the more I thought the other models were worse. Examples of other models: <a href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">Polling AI scientists</a> , <a href="https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines">extrapolating gross world product (GWP) a la Roodman</a> , deferring to <a href="https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi">what the stock markets imply</a> , <a href="https://www.lesswrong.com/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied">Hanson&#39;s weird fractional progress model thingy</a> , the <a href="https://epochai.org/blog/grokking-semi-informative-priors">semi-informative</a> prior <a href="https://aipriors.com/">models...</a><a href="https://epochai.org/blog/grokking-semi-informative-priors"> </a>I still put some weight on those other models but not much.<br><br> As for the horizon lengths question: This feeds into the training compute requirements variable. IIRC Ajeya&#39;s original model had different buckets for short, medium, and long-horizon, where eg medium-horizon bucket meant roughly &quot;Yeah we&#39;ll be doing a combo of short horizon and long horizon training, but on average it&#39;ll be medium-horizon training, such that the compute costs will be eg [inference FLOPs]*[many trillions of datapoints, as per scaling laws applied to bigger-than-human-brain-models]*[4-6 OOMs of seconds of subjective experience per datapoint on average]<br><br> So Ajeya had most of her mass on the medium and long horizon buckets, whereas I was much more bullish that the bulk of the training could be short horizon with just a &quot;cherry on top&quot; of long-horizon. Quantitatively I was thinking something like &quot;Say you have 100T datapoints of next-moment-prediction as part of your short-horizon pre-training. I claim that you can probably get away with merely 100M datapoints of million-second-long tasks, or less.&quot;<br><br> For some intuitions why I think this, it may help to read <a href="https://www.lesswrong.com/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute">this post</a> and/or <a href="https://www.lesswrong.com/posts/BoA3agdkAzL6HQtQP/clarifying-and-predicting-agi?commentId=Bs9sKmzhNvSPAs3yY">this comment thread.</a></p></div></section><h3> <strong>Ege</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:35:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:35:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think my specific disagreements with Ajeya and Daniel might be a little different, but an important meta-level point is my general skepticism of arguments that imply wild conclusions. This becomes especially relevant with predictions of a 3 OOM increase in our energy consumption in the next 10 or 20 years. It&#39;s possible to tell a compelling story about why that might happen, but also possible to do the opposite, and judging how convincing those arguments should be is difficult for me. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 17:38:16 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 17:38:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> OK, in response to Ege, I guess we disagree about this &quot;that-conclusion-is-wild-therefore-unlikely&quot; factor. I think for things like this it&#39;s a pretty poor guide to truth relative to other techniques (eg models, debates between people with different views, model-based debates between people with different views) I&#39;m not sure how to make progress on resolving this crux. Ege, you say it&#39;s mostly in play for the 1000x energy consumption thing; wanna focus on discussing the other question instead? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:40:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:40:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Sure, discussing the other question first is fine.<br><br> I&#39;m not sure why you think heuristics like &quot;I don&#39;t update as much on specific arguments because I&#39;m skeptical of my ability to do so&quot; are ineffective, though. For example, this seems like it goes against the fractional Kelly betting heuristic from <a href="https://www.lesswrong.com/posts/TNWnK9g2EeRnQA8Dg/never-go-full-kelly">this post</a> , which I would endorse in general: you want to defer to the market to some extent because your model has a good chance of being wrong.<br><br> I don&#39;t know if it&#39;s worth going down this tangent right now, though, so it&#39;s probably more productive to focus on the first question for now.<br><br> I think my wider distribution on the first question is also affected by the same high-level heuristic, though to a lesser extent. In some sense, if I were to fully condition on the kind of compute-based model that you and Ajeya seem to have about how AI is likely to develop, I would probably come up with a probability distribution for the first question that more or less agrees with Ajeya&#39;s. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:48:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:48:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> That&#39;s interesting. I think digging into that seems good to me.</p><p> Can you say a bit more about how you are thinking about it at a high level? My guess is you have a bunch of broad heuristics, some of which are kind of like &quot;well, the market doesn&#39;t seem to think AGI is happening soon?&quot;, and then those broaden your probability mass, but I don&#39;t know whether that&#39;s a decent characterization, and would be interested in knowing more of the heuristics that drive that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 17:53:33 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 17:53:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I&#39;m not sure I would put that much weight on the market not thinking it&#39;s happening soon, because I think it&#39;s actually fairly difficult to tell what market prices would look like if the market <i>did</i> think it was happening soon.<br><br> Setting aside the point about the market and elaborating on the rest of my views: I would give a 50% chance that in 30 years, I will look back on something like <a href="https://www.lesswrong.com/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff">Tom Davidson&#39;s takeoff model</a> and say &quot;this model captured all or most of the relevant considerations in predicting how AI development was likely to proceed&quot;. For me, that&#39;s already a fairly high credence to have in a specific class of models in such an uncertain domain.<br><br> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.<br><br> If I thought the takeoff model from Tom Davidson (and some newer versions that I&#39;ve been working on personally) were basically right, my forecasts would just look pretty similar to the forecasts of that model, and based on my experience with playing around in these models and the parameter ranges I would consider plausible, I think I would just end up agreeing with Ajeya on the first question.<br><br> Does that explanation make my view somewhat clearer? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 17:57:12 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 17:57:12 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> However, conditional on this framework being importantly wrong, my timelines get substantially longer because I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> This part really helps, I think.</p><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:00:13 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:00:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I would currently characterize your view as &quot;Ok, maybe all we need is to increase compute scaling and do some things that are strictly easier than that (and so will be done by the time we have enough compute). But if that&#39;s wrong, forecasting when we&#39;ll get AGI gets much harder, since we don&#39;t really have any other concrete candidate hypothesis for how to get to AGI, and that implies a huge amount of uncertainty on when things will happen&quot;.</p></blockquote><p> That&#39;s basically right, though I would add the caveat that entropy is relative so it doesn&#39;t really make sense to have a &quot;more uncertain distribution&quot; over when AGI will arrive. You have to somehow pick some typical timescale over which you expect that to happen, and I&#39;m saying that once scaling is out of the equation I would default to longer timescales that would make sense to have for a technology that we think is possible but that we have no concrete plans for achieving on some reasonable timetable. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:03:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:03:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I see no other clear path from where we are to AGI if the scaling pathway is not available. There could be other paths (eg large amounts of software progress) but they seem much less compelling.</p></blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds. It&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now. In some sense, &quot;we come up with a brilliant insight to do this way more efficiently than nature even when we have very little compute&quot; is a hypothesis that should have had &lt;50% weight a priori, compared to &quot;capabilities will start getting good when we&#39;re talking about macroscopic amounts of compute.&quot;</p><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></div></section><h3> <strong>Ajeya</strong> </h3><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 17:43:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 17:43:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> On Daniel&#39;s opening points: I think I actually just agree with both a) and b) right now — or rather, I agree that the right question to ask about the training compute requirement is something more along the lines of &quot;How many GPT-N to GPT-N.5 jumps do we think it would take?&quot;, and that short horizon LLMs plus tinkering looks more like &quot;the default&quot; than like &quot;one of a few possibilities,&quot; where other possibilities included a more intense meta-learning step (which is how it felt in 2019-20). The latter was <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines#Picturing_a_more_specific_and_somewhat_lower_bar_for_TAI">the biggest adjustment</a> in my updated timelines.<br><br> That said though, I think two important object-level points push the &quot;needed model size&quot; and &quot;needed amount of tinkering&quot; higher in my mind than it is for Daniel:</p><ul><li> In-context learning does seem pretty bad, and doesn&#39;t seem to be improving a huge amount. I think we can have TAI without really strong human-like in-context learning, but it probably requires more faff than if we had that out of the gate.</li><li> Relatedly, adversarial robustness seems not-great right now.  This also feels overcomable, but I think it increases the scale that you need (by analogy, like 5-10 years ago it seemed like vision systems were good enough for cars except in the long tail / in adversarial settings, and I think vision systems had to get a fair amount bigger, plus there had to be a lot more tinkering on the cars, to get to now where they&#39;re starting to be viable).</li></ul><p> And then a meta-level point is that I (and IIRC Metaculus, according to my colleague Isabel) have been kind of surprised for the last few years about the lack of cool products built on LLMs (it seems like 2023 brought the level of cool products I was naively picturing in 2021). I think there&#39;s a &quot;reality has a lot of detail, actually making stuff work is a huge pain&quot; dynamic going on, and it lends credence to the &quot;things will probably be fairly continuous&quot; heuristic that I already had.</p><p> A few other meta-points:</p><ul><li> The Paul <a href="https://sideways-view.com/2023/07/29/self-driving-car-bets/">self-driving car bets</a> post was interesting to me, and I place some weight on &quot;Daniel is doing the kind of &#39;I can see how it would be done so it&#39;s only a few years away&#39; move that I think doesn&#39;t serve as a great guide to what will happen in the real world.&quot;</li><li> Carl is the person who seems like he&#39;s been the most right when we&#39;ve disagreed, so he&#39;s probably the one guy whose views I put the most weight on. But also Carl seems like he errs aggressive and errs in the direction of believing people will aggressively pursue the most optimal thing (being more surprised than I was, for a longer period of time, about how people haven&#39;t invested more in AI and accomplished more with it by now). His timelines are longer than Daniel&#39;s, and I think mine are a bit longer than his.</li><li> In general, I do count Daniel as among a pretty small set of people who were clearly on record with views more correct than mine in 2020 about both the nature of how TAI would be built (LLMs+tinkering) and how quickly things would progress. Although it&#39;s a bit complicated because 2020-me thought we&#39;d be seeing more powerful LLM products by now.</li><li> Other people who I think were more right include Carl, Jared Kaplan, Danny Hernandez, Dario, Holden, and Paul. Paul is interesting because I think he both put more weight than I did on &quot;it&#39;s just LLMs plus a lot of decomposition and tinkering&quot; but also puts more weight than either me or Daniel on &quot;things are just hard and annoying and take a long time;&quot; this left him with timelines similar to mine in 2020, and maybe a bit longer than mine now.</li><li> Oh — another point that seems interesting to discuss at some point is that I suspect Daniel generally wants to focus on a weaker endpoint because of some sociological views I disagree with. (Screened off by the fact that we were answering the same question about remotable jobs replacement, but I think hard to totally screen off.)</li></ul></div></section><h2> On in-context learning as a potential crux </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:04:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:04:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya:</p><ul><li> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</li><li> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</li><li> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</li><li> re: schlep &amp; incompetence on the part of the AGI industry: Yep, you are right about this, and I was wrong. Your description of Carl also applies to me historically; in the past three years I&#39;ve definitely been a &quot;this is the fastest way to AGI, therefore at least one of the labs will do it with gusto&quot; kind of guy, and now I see that is wrong. I think basically I fell for the planning fallacy &amp; efficient market hypothesis fallacy.<br><br> However, I don&#39;t think this is the main crux between us, because it basically pushes things back by a few years, it doesn&#39;t eg double (on a log scale) the training requirements. My current, updated model of timelines, therefore, is that the bottleneck in the next five years is not necessarily compute but instead quite plausibly schlep &amp; conviction on the part of the labs. This is tbh a bit of a scary conclusion. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:21:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:21:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: in-context learning: I don&#39;t have much to say on this &amp; am curious to hear more. Why do you think it needs to get substantially better in order to reach AGI, and why do you think it&#39;s not on track to do so? I&#39;d bet that GPT4 is way better than GPT3 at in-context learning for example.</p></blockquote><p> The traditional image of AGI involves having an AI system that can <i>learn new (to it) skills</i> as efficiently as humans (with as few examples as humans would need to see). I think this is not how the first transformative AI system will look, because ML is less sample efficient than humans and it doesn&#39;t look like in-context learning is on track to being able to do the kind of fast sample-efficient learning that humans do. I think this is not fatal for getting TAI, because we can make up for it by a) the fact that LLMs&#39; &quot;ancestral memory&quot; contains all sorts of useful information about human disciplines that they won&#39;t need to learn in-context, and b) explicitly guiding the LLM agent to &quot;reason out loud&quot; about what lessons it should take away from its observations and putting those in an external memory it retrieves from, or similar.</p><p> I think back when Eliezer was saying that &quot;stack more layers&quot; wouldn&#39;t get us to AGI, this is one of the kinds of things he was pointing to: that cognitively, these systems didn&#39;t have the kind of learning/reflecting flexibility that you&#39;d think of re AGI. When people were talking about GPT-3&#39;s in-context learning, I thought that was one of the weakest claims by far about its impressiveness. The in-context learning at the time was like: you give it a couple of examples of translating English to French, and then you give it an English sentence, and it dutifully translate that into French. It already knew English and it already knew French (from its ancestral memory), and the thing it &quot;learned&quot; was that the game it was currently playing was to translate from English to French.</p><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible). But compared to any object-level skill like coding (many of which are superhuman), in-context learning seems quite subhuman. I think this is related to how ARC Evals&#39; LLM agents kind of &quot;fell over&quot; doing things like setting up Bitcoin wallets.</p><p> Like Eliezer often says, humans evolved to hunt antelope on the savannah, and that very same genetics coding for the very same brain can build rockets and run companies. Our LLMs right now generalize further from their training distribution than skeptics in 2020 would have said, and they&#39;re generalizing further and further as they get bigger, but they have nothing like the kind of savannah-to-boardroom generalization we have. This can create lots of little issues in lots of places when an LLM will need to digest some new-to-it development and do something intelligent with it. Importantly, I don&#39;t think this is going to stop LLM-agent-based TAI from happening, but it&#39;s one concrete limitation that pushes me to thinking we&#39;ll need more scale or more schlep than it looks like we&#39;ll need before taking this into account.</p><p> Adversarial robustness, which I&#39;ll reply to in another comment, is similar: a concrete hindrance that isn&#39;t fatal but is one reason I think we&#39;ll need more scale and schlep than it seems like Daniel does (despite agreeing with his concrete counterarguments of the form &quot;we can handle it through X countermeasure&quot;). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:34:30 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:34:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Ajeya: Thanks for that lengthy reply. I think I&#39;ll have to ponder it for a bit. Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:15:08 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:15:08 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I think it&#39;s worth separating the &quot;compute scaling&quot; pathway into a few different pathways, or else giving the generic &quot;compute scaling&quot; pathway more weight because it&#39;s so broad. In particular, I think Daniel and I are living in a much more specific world than just &quot;lots more compute will help;&quot; we&#39;re picturing agents built from LLMs, more or less. That&#39;s very different from eg &quot;We can simulate evolution.&quot; The compute scaling hypothesis encompasses both, as well as lots of messier in-between worlds.</p></blockquote><p> I think it&#39;s fine to incorporate these uncertainties as a wider prior over the training compute requirements, and I also agree it&#39;s a reason to put more weight on this broad class of models than you otherwise would, but I still don&#39;t find these reasons compelling enough to go significantly above 50%. It just seems pretty plausible to me that we&#39;re missing something important, even if any specific thing we can name is unlikely to be what we&#39;re missing.<br><br> To give one example, I initially thought that the evolution anchor from the Bio Anchors report looked quite solid as an upper bound, but I realized some time after that it doesn&#39;t actually have an appropriate anthropic correction and this could potentially mess things up. I now think if you work out the details this correction turns out to be inconsequential, but it didn&#39;t have to be like that: this is just a consideration that I missed when I first considered the argument. I suppose I would say I don&#39;t see a reason to trust my own reasoning abilities as much as you two seem to trust yours.</p><blockquote><p> The compute scaling hypothesis is much broader, and it&#39;s pretty much the <i>one paradigm</i> that anyone in the past who was trying to forecast timelines and got anywhere close to predicting when AI would start getting interesting used. Like I think Moravec is looking super good right now.</p></blockquote><p> My impression is that Moravec predicted in 1988 that we would have AI systems comparable to the human brain in performance around 2010. If this actually happens around 2037 (your median timelines), Moravec&#39;s forecast will have been off by around a factor of 2 in terms of the time differential from when he made the forecast. That doesn&#39;t seem &quot;super good&quot; to me.<br><br> Maybe I&#39;m wrong about exactly what Moravec predicted - I didn&#39;t read his book and my knowledge is second-hand. In any event, I would appreciate getting some more detail from you about why you think he looks good.</p><blockquote><p> Or maybe I&#39;d say on priors you could have been 50/50 between &quot;things will get more and more interesting the more compute we have access to&quot; and &quot;things will stubbornly stay super uninteresting even if we have oodles of compute because we&#39;re missing deep insights that the compute doesn&#39;t help us get&quot;; but then when you <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">look around at the world</a> , you should update pretty hard toward the first.</p></blockquote><p> I agree that if I were considering two models at those extremes, recent developments would update me more toward the former model. However, I don&#39;t actually disagree with the abstract claim that &quot;things will get more and more interesting the more compute we have access to&quot; - I expect more compute to make things more interesting even in worlds where we can&#39;t get to AGI by scaling compute. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:31:24 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:31:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I agree that 4 is a lot better than 3 (for example, you can teach 4 new games like French Toast or Hitler and it will play them — unless it already knows that game, which is plausible).</p></blockquote><p> A local remark about this: I&#39;ve seen a bunch of reports from other people that GPT-4 is essentially unable to play tic-tac-toe, and this is a shortcoming that was highly surprising to me. Given the amount of impressive things it can otherwise do, failing at playing a simple game whose full solution could well be in its training set is really odd.<br><br> So while I agree 4 seems better than 3, it still has some bizarre weaknesses that I don&#39;t think I understand well. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:34:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:34:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ege: Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily? My sense is that this is just one of these situations where tokenization and one-dimensionality of text makes something hard, but it&#39;s trivial to get the system to learn it if it&#39;s in a more natural representation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:35:17 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:35:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Just to check, GPT-4V (vision model) presumably can play tic-tac-toe easily?</p></blockquote><p><br> <a href="https://twitter.com/liminal_warmth/status/1709654529413992692">This random Twitter person</a> says that it can&#39;t. Disclaimer: haven&#39;t actually checked for myself.</p></div></section><h2> Taking into account government slowdown </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:05:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:05:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:06:58 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:06:58 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I am guilty of assuming governments and corporations won&#39;t slow things down by more than a year. I think I mostly still endorse this assumption but I&#39;m hopeful that instead they&#39;ll slow things down by several years or more. Historically I&#39;ve been arguing with people who disagreed with me on timelines by decades, not years, so it didn&#39;t seem important to investigate this assumption. That said I&#39;m happy to say why I still mostly stand by it. Especially if it turns out to be an important crux (eg if Ege or Ajeya think that AGI would probably happen by 2030 absent slowdown) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:08:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:08:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> That said I&#39;m happy to say why I still mostly stand by it.</p></blockquote><p> Cool, might be worth investigating later if it turns out to be a crux. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:21:02 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:21:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p><p> Seems like a relevant dimension that&#39;s not obviously reflected in usual compute models, and just want to make sure that&#39;s not accidentally causing some perceived divergence in people&#39;s timelines.</p></blockquote><p> Responding to habryka: I do think government regulations, companies slowing down because of risks, companies slowing down because they are bad at coordination, capital markets being unable to allocate the large amounts of capital needed for huge training runs for various reasons, etc. could all be important. However, my general heuristic for thinking about the issue is more &quot;there could be a lot of factors I&#39;m missing&quot; and less &quot;I think these specific factors are going to be very important&quot;.<br><br> In terms of the impact of capable AI systems, I would give significantly less than even but still non-negligible odds that these kinds of factors end up limiting the acceleration in economic growth to eg less than an order of magnitude. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:49:39 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:49:39 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> As a quick question, to what degree do y&#39;alls forecasts above take into account governments trying to slow things down and companies intentionally going slower because of risks?</p></blockquote><p> I include this in a long tail of &quot;things are just slow&quot; considerations, although in my mind it&#39;s mostly not people making a concerted effort to slow down because of x-risk, but rather just the thing that happens to any sufficiently important technology that has a lot of attention on it: a lot of drags due to the increasing number of stakeholders, both drags where companies are less blase about releasing products because of PR concerns, and drags where governments impose regulations (which I think they would have in any world, with or without the efforts of x-risk-concerned contingent). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:10:53 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:10:53 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Slight meta: I am interested in digging in a bit more to find some possible cruxes between Daniel and Ajeya, before going more in-depth between Ajeya and Ege, just to keep the discussion a bit more focused.</p></div></section><h2> Recursive self-improvement and AI&#39;s speeding up R&amp;D </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:21:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:21:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Daniel: Just for my own understanding, you have adjusted the compute-model to account for some amount of R&amp;D speedup as a result of having more AI researchers.</p><p> To what degree does that cover classical recursive self-improvements or things in that space? (Eg AI systems directly modifying their training process or weights or develop their own pre-processing modules?)</p><p> Or do you expect a feedback loop that&#39;s more &quot;AI systems do research that routes through humans understanding those insights and being in the loop on implementing them to improve the AI systems&quot;? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:25:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:25:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> When all we had was Ajeya&#39;s model, I had to make my own scrappy guess at how to adjust it to account for R&amp;D acceleration due to pre-AGI systems. Now we have Davidson&#39;s model so I mostly go with that.<br><br> It covers recursive-self-improvement as a special case. I expect that to be what the later, steeper part of the curve looks like (basically a million AutoGPTs running in parallel across several datacenters, doing AI research but 10-100x faster than humans would, with humans watching the whole thing from the sidelines clapping as metrics go up); the earlier part of the curve looks more like &quot;every AGI lab researcher has access to a team of virtual engineers that work at 10x speed and sometimes make dumb mistakes&quot; and then the earliest part of the curve is what we are seeing now with copilot and chatgpt helping engineers move slightly faster. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:37:36 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:37:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Interesting, I thought the biggest adjustment to your timelines was the pre-AGI R&amp;D acceleration modelled by Davidson. That was another disagreement between us originally that ceased being a disagreement once you took that stuff into account.</p></blockquote><p> These are entangled updates. If you&#39;re focusing on just &quot;how can you accelerate ML R&amp;D a bunch,&quot; then it seems less important to be able to handle low-feedback-loop environments quite different from the training environment. By far the biggest reason I thought we might need longer horizon training was to imbue the skill of efficiently learning very new things (see <a href="https://docs.google.com/document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.2s3orj7g2t76">here</a> ). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:38:01 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:38:01 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Right now I&#39;m stuck with a feeling that we agree qualitatively but disagree quantitatively.</p></blockquote><p> I think this is basically right! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:39:50 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:39:50 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> re: adversarial robustness: Same question I guess. My hot take would be (a) it&#39;s not actually that important, the way forward is not to never make errors in the first place but rather to notice and recover from them enough that the overall massive parallel society of LLM agents moves forward and makes progress, and (b) adversarial robustness is indeed improving. I&#39;d be curious to hear more, perhaps you have data on how fast it is improving and you extrapolate the trend and think it&#39;ll still be sucky by eg 2030?</p></blockquote><p> I&#39;ll give a less lengthy reply here, since structurally it&#39;s very similar to in-context learning, and has the same &quot;agree-qualitatively-but-not-quantitatively&quot; flavor. (For example, I definitely agree that the game is going to be coping with errors and error-correction, not never making errors; we&#39;re talking about whether that will take four years or more than four years.)</p><p> &quot;Not behaving erratically / falling over on super weird or adversarial inputs&quot; is a higher-level-of-abstraction cognitive skill humans are way better at than LLMs. LLMs are improving at this skill with scale (like all skills), and there are ways to address it with schlep and workflow rearrangements (like all problems), and it&#39;s unclear how important it is in the first place. But it&#39;s plausibly fairly important, and it seems like their current level is &quot;not amazing,&quot; and the trend is super unclear but not obviously going to make it in four years.</p><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, market-efficiency-and-schlep) pushes you toward &quot;more than four years from now&quot; — there&#39;s little room for it to push in the other direction. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:42:42 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:42:42 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> In general, when you&#39;re talking about &quot;Will it be four years from now or more than four years from now?&quot;, uncertainty and FUD on any point (in-context-learning, adversarial robustness, pushes you toward &quot;more than four years from now&quot;</p></blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:45:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:45:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> I&#39;m curious why Ajeya thinks this claim is true for &quot;four years&quot; but not true for &quot;twenty years&quot; (assuming that&#39;s an accurate representation of her position, which I&#39;m not too confident about).</p></blockquote><p> I don&#39;t think it&#39;s insane to believe this to be true of 20 years, but I think we have many more examples of big, difficult, society-wide things happening over 20 years than over 4. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:45:40 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:45:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote-working OpenAI engineer that thinks at 10x serial speed. Even if the research scientists are still human, this would speed things up a lot I think. So while I find the abstract arguments about how LLMs are worse at in-context learning etc. than humans plausible, when I think concretely about AI R&amp;D acceleration it still seems like it&#39;s gonna start happening pretty soon, and that makes me also update against the abstract argument a bit. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> So, I kind of want to check an assumption. On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p><p> Daniel, could you be more concrete about how a 10x AI engineer actually helps develop AGI? My guess is on a 4-year timescale you don&#39;t expect it to route through semiconductor supply chain improvements.</p><p> And then I want to check what Ajeya thinks here and whether something in this space might be a bit of a crux. My model of Ajeya does indeed think that AI systems in the next few years will be impressive, but not really actually that useful for making AI R&amp;D go better, or at least not like orders of magnitude better. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 18:48:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 18:48:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> My best guess is that algorithmic progress has probably continued at a rate of around a doubling of effective compute per year, at least insofar as you buy that one-dimensional model of algorithmic progress. Again, model uncertainty is a significant part of my overall view about this, but I think it&#39;s not accurate to say there hasn&#39;t been much algorithmic progress in the last few years. It&#39;s just significantly slower than the pace at which we&#39;re scaling up compute so it looks relatively less impressive.</p><p> <i>(Daniel, Ajeya +1 this comment)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:46:41 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:46:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I was modeling one doubling a year as approximately not very much, compared to all the other dynamics involved, though of course it matters a bunch over the long run. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 18:56:42 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 18:56:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: Habryka&#39;s excellent point about how maybe engineering isn&#39;t the bottleneck, maybe compute is instead:<br><br> My impression is that roughly half the progress has come from increased compute and the other half from better algorithms. Going forward when I think concretely about the various limitations of current algorithms and pathways to overcome them -- which I am hesitant to go into detail about -- it sure does seem like there are still plenty of low and medium-hanging fruit to pick, and then high-hanging fruit beyond which would take decades for human scientists to get to but which can perhaps be reached much faster during an AI takeoff.<br><br> I am on a capabilities team at OpenAI right now and I think that we could be going something like 10x faster if we had the remote engineer thing I mentioned earlier. And I think this would probably apply across most of OpenAI research. This wouldn&#39;t accelerate our compute acquisition much at all to be clear, but that won&#39;t stop a software singularity from happening. Davidson model backs this up I think -- I&#39;d guess that if you magically change it to keep hardware &amp; compute progress constant, you still get a rapid R&amp;D acceleration, just a somewhat slower one.<br><br> I&#39;d think differently if I thought that parameter count was just Too Damn Low, like I used to think. If I was more excited about the human brain size comparison, I might think that nothing short of 100T parameters (trained according to Chinchilla also) could be AGI, and therefore that even if we had a remote engineer thinking at 10x speed we&#39;d just eat up the low-hanging fruit and then stall while we waited for bigger computers to come online. But I don&#39;t think that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:56:54 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:56:54 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On a compute-focused worldview, I feel a bit confused about how having additional AI engineers helps that much. Like, maybe this is a bit of a strawman, but my vibe is that there hasn&#39;t really been much architectural innovation or algorithmic progress in the last few years, and the dominant speedup has come from pouring more compute into existing architectures (with some changes to deal with the scale, but not huge ones).</p></blockquote><p> I think there haven&#39;t been flashy paradigm-shifting insights, but I strongly suspect each half-GPT was a hard-won effort on a lot of fronts, including both a lot of mundane architecture improvements (like implementing long contexts in less naive ways that don&#39;t incur quadratic cost), a lot of engineering to do the model parallelism and other BS that is required to train bigger models without taking huge GPU utilization hits, and a lot of post-training improvements to make usable nice products. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 18:58:49 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 18:58:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p><p> The context length and GPU utilization thing feels most relevant. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:59:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:59:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Ajeya: What you say seems right, but also the things you say also don&#39;t sound like the kind of thing that when you accelerate then 10x, then you get AGI 10x earlier. As you said, a lot of BS required to train large models, a lot of productization, but that doesn&#39;t speed up the semiconductor supply chain.</p></blockquote><p> Yeah, TBC, I think there&#39;s a higher bar than Daniel thinks there is to speeding stuff up 10x for reasons like this. I do think that there&#39;s algorithm juice, like Daniel says, but I don&#39;t think that a system you look at and naively think &quot;wow this is basically doing OAI ML engineer-like things&quot; will actually lead to a full 10x speedup; 10x is a lot.</p><p> I think you will eventually get the 10x, and then the 100x, but I&#39;m picturing that happening after some ramp-up where the first ML-engineer-like systems get integrated into workflows, improve themselves, change workflows to make better use of themselves, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 18:53:49 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 18:53:49 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> Quick comment re: in-context learning and/or low-data learning: It seems to me that GPT-4 is already pretty good at coding, and a big part of accelerating AI R&amp;D seems very much in reach.</p></blockquote><p> Agree this is the strongest candidate for crazy impacts soon, which is why my two updates of &quot;maybe meta-learning isn&#39;t that important and therefore long horizon training isn&#39;t as plausibly necessary&quot; and &quot;maybe I should just be obsessed with forecasting when we have the ML-research-engineer-replacing system because after that point progress is very fast&quot; are heavily entangled. <i><u>(Daniel reacts &quot;+1&quot; to this)</u></i></p><blockquote><p> -- like, it doesn&#39;t seem to me like there is a 10-year, 4-OOM-training-FLOP gap between GPT4 and a system which is basically a remote OpenAI engineer that thinks at 10x serial speed</p></blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:02:59 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:02:59 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> I don&#39;t know, 4 OOM is less than two GPTs, so we&#39;re talking less than GPT-6. Given how consistently I&#39;ve been wrong about how well &quot;impressive capabilities in the lab&quot; will translate to &quot;high economic value&quot; since 2020, this seems roughly right to me?</p></blockquote><p> I disagree with this update -- I think the update should be &quot;it takes a lot of schlep and time for the kinks to be worked out and for products to find market fit&quot; rather than &quot;the systems aren&#39;t actually capable of this.&quot; Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.<br><br> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:05:18 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:05:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Like, I bet if AI progress stopped now, but people continued to make apps and widgets using fine-tunes of various GPTs, there would be OOMs more economic value being produced by AI in 2030 than today.</p></blockquote><p><br> I&#39;m skeptical we would get 2 OOMs or more with just the current capabilities of AI systems, but I think even if you accept that, scaling from $1B/yr to $100B/yr is easier than from $100B/yr to $10T/yr. Accelerating AI R&amp;D by 2x seems more like the second change to me, or even bigger than that. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:10 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:10 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> And so I think that the AI labs will be using AI remote engineers much sooner than the general economy will be. (Part of my view here is that around the time it is capable of being a remote engineer, the process of working out the kinks / pushing through schlep will itself be largely automatable.)</p></blockquote><p> I agree with this </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yeah idk I pulled that out of my ass, maybe ​2 OOM is more like the upper limit given how much value there already is. I agree that going from X to 10X is easier than going from 10X to 100X, in general. I don&#39;t think that undermines my point though. I disagree with your claim that making AI progress go 2x faster is more like scaling from $100B to $10T-- I think it depends on when it happens! Right now in our state of massive overhang and low-hanging-fruit everywhere, making AI progress go 2x faster is easy.<br><br> Also to clarify when I said 10x faster I meant 10x faster algorithmic progress; compute progress won&#39;t accelerate by 10x obviously. But what this means is that I think we&#39;ll transition from a world where half or more of the progress is coming from scaling compute, to a world where most of the progress is coming from algorithmic improvements / pushing-through-schlep.</p></div></section><h2> Do we expect transformative AI pre-overhang or post-overhang? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:01:40 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:01:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think a hypothesis I have for a possible crux for a lot of the disagreement between Daniel and Ajeya is something like &quot;will we reach AGI before the compute overhang is over vs. after?&quot;.</p><p> Like, in as much as we think we are in a compute-overhang situation, there is an extremization that applies to people&#39;s timelines where if you we&#39;ll get there using just remaining capital and compute, you expect quite short timelines, but if you expect it will require faster chips or substantial algorithmic improvements, you expect longer, and with less probability-mass in-between.</p><p> Curious about Daniel and Ajeya answering the question of &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:05:48 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:05:48 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> &quot;what probability do you assign to AGI before we exhausted the current compute overhang vs. after?&quot;</p></blockquote><p> I think there are different extremities of compute overhang. The most extreme one which will be exhausted most quickly is like &quot;previously these companies were training AI systems on what is essentially chump change, and now we&#39;re starting to get into a world where it&#39;s real money, and soon it will be really real money.&quot; I think within 3-4 years we&#39;ll be talking tens of billions for a training run; I think the probability we get drop-in replacements for 99% remotable jobs (regardless of whether we&#39;ve rolled those drop-in replacements out everywhere) by then is something like...25%?</p><p> And then after that progress is still pretty compute-centric, but it moves slower because you&#39;re spending very real amounts of money, and you&#39;re impacting the entire supply chain: you need to build more datacenters which come with new engineering challenges, more chip-printing facilities, more fabs, more fab equipment manufacturing plans, etc. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:10:51 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:10:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:08:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:08:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> re: Habryka: Yes we disagree about whether the current overhang is enough. But the cruxes for this are the things we are already discussing.</p></blockquote><p> Cool, that makes sense. That does seem like it might exaggerate the perceived disagreements between the two of you, when you just look at the graphs, though it&#39;s of course still highly decision-relevant to dig deeper into whether this is true or not.</p></div></section><h2> Hofstadter&#39;s law in AGI forecasting </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:06:47 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:06:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> TBC Daniel, I think we differ by a factor of 2 on the probability for your median scenario. I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:13:44 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:13:44 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I feel like a general structure of our disagreements have been like: you (Daniel) are saying a scenario that makes sense and which I place a lot of weight on, but it seems like there are other scenarios and it seems like your whole timetable leaves little room for Hofstadter&#39;s law.</p></blockquote><p> I think this also applies to the disagreement between me and Ajeya. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:16:52 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:16:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> A thing that would change my mind is if I found other scenarios more plausible. Wanna sketch some?<br><br> Regarding Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot; Right? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:17:53 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:17:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Re: Hofstadter&#39;s law: A possible crux between us is that you both seem to think it applies on timescales of decades -- a multiplicative factor on timelines -- whereas I think it&#39;s more like &quot;add three years.&quot; Right?</p></blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:26:13 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:26:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Yes, in general, that&#39;s how I would update my timelines about anything to be longer, not just AGI. The additive method seems pretty bad to me unless you have some strong domain-specific reason to think you should be making an additive update.</p></blockquote><p> Excellent. So my reason for doing the additive method is that I think Hofstadter&#39;s law / schlep / etc. is basically the planning fallacy, and it applies when your forecast is based primarily on imagining a series of steps being implemented. It does NOT apply when your forecast is based primarily on extrapolating trends. Like, you wouldn&#39;t look at a graph of exponential progress in Moore&#39;s law or solar power or whatever and then be like &quot;but to account for Hofstadter&#39;s Law I will assume things take twice as long as I expect, therefore instead of extrapolating the trend-line straight I will cut its slope by half.&quot;<br><br> And when it comes to AGI timelines, I think that the shorter-timeline scenarios look more subject to the planning fallacy, whereas the longer-timeline scenarios look more like extrapolating trends.<br><br> So in a sense I&#39;m doing the multiplicative method, but only on the shorter worlds. Like, when I say 2027 as my median, that&#39;s kinda because I can actually quite easily see it happening in 2025, but things take longer than I expect, so I double it... I&#39;m open to being convinced that I&#39;m not taking this into account enough and should shift my timelines back a few years more; however I find it very implausible that I should add eg 15 years to my median because of this.</p></div></section><h2> Summary of where we are at so far and exploring additional directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:21:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:21:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> We&#39;ve been going for a while and it might make sense to take a short step back. Let me try to summarize where we are at:</p><p> We&#39;ve been mostly focusing on the disagreement between Ajeya and Daniel. It seems like one core theme in the discussion has been the degree to which &quot;reality has a lot of detail and kinks need to be figured out before AI systems are actually useful&quot;. Ajeya currently thinks that while it is true that AGI companies will have access to these tools earlier, there still will be a lot of stuff to figure out before you actually have a system equivalent to a current OAI engineer. Daniel made a similar update in noticing a larger-than-he-expected delay in the transition from &quot;having all the stuff necessary to make a more capably system, like architecture, compute, training setup&quot; and &quot;actually producing a more capable system&quot;.</p><p> However, it&#39;s also not clear how much this actually explains the differences in the timelines for the two of you.</p><p> We briefly touched on compute overhangs being a thing that&#39;s very relevant to both of your distributions, in that Daniel assigns substantially higher probability to a very high R&amp;D speed-up before the current overhang is exhausted, which pushes his probability mass a bunch earlier. And correspondingly Ajeya&#39;s timelines are pretty sensitive to relatively small changes in compute requirements on the margin, since that would push a bunch of probability mass into the pre-overhang world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:21:30 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:21:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I&#39;ll put in a meta note here that I think it&#39;s pretty challenging to argue about a 25% vs a 50% on the Daniel scenario, that is literally one bit of evidence one of us sees that the other doesn&#39;t. It seems like Daniel thinks I need stronger arguments/evidence than I have to be at 25% instead of 50%, but it&#39;s easy to find one bit somewhere and hard to argue about whether it really is one bit.</p></div></section><h2> Exploring conversational directions </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> In case interested, here are some possible conversation topics/starters:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)<br><br> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.<br><br> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?<br><br> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:25:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:25:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> On Daniel&#39;s four topics:<br><br> (1) I could give a scenario in which AGI happens by some very soon date, eg December 2024 or 2026, and then we could talk about what parts of the scenario are most unlikely (~= what parts would cause the biggest updates to us if we observed them happening)</p></blockquote><p> I suspect I&#39;ll be like &quot;Yep, seems plausible, and my probability on it coming to pass is 2-5x smaller.&quot;</p><blockquote><p> (2) Someone without secrecy concerns (ie someone not working at OpenAI, ie Ajeya or Ege or Habryka) could sketch what they think they would aim to have built by 2030 if they were in charge of a major AI lab and were gunning for AGI asap. Parameter count, training FLOP, etc. taken from standard projections, but then more details like what the training process and data would look like etc. Then we could argue about what this system would be capable of and what it would be incapable of, eg how fast would it speed up AI R&amp;D compared to today.</p></blockquote><p> I could do this if people thought it would be useful.</p><blockquote><p> (2.5) As above except for convenience we use Steinhardt&#39;s <a href="https://www.lesswrong.com/posts/WZXqNYbJhtidjRXSi/what-will-gpt-2030-look-like">What will GPT-2030 look like?</a> and factor the discussion into (a) will GPT-2030 be capable of the things he claims it will be capable of, and (b) will that cause a rapid acceleration of AI R&amp;D leading shortly to AGI?</p></blockquote><p> I like this blog post but I feel like it&#39;s quite tame compared to what both Daniel and I think is plausible so not sure if it&#39;s the best thing to anchor on.</p><blockquote><p> (3) Ege or Ajeya could sketch a scenario in which the year 2035 comes and goes without AGI, despite there being no AI progress slowdown (no ban, no heavy regulation, no disruptive war, etc.). Then I could say why I think such a scenario is implausible, and we could discuss more generally what that world looks like.</p></blockquote><p> I can do this if people thought it would be useful.</p></div></section><h2> Ege&#39;s median world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:25:39 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:25:39 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> My median world looks something like this: we keep scaling compute until we hit training runs at a size of 1e28 to 1e30 FLOP in maybe 5 to 10 years, and after that scaling becomes increasingly difficult because of us running up against supply constraints. Software progress continues but slows down along with compute scaling. However, the overall economic impact of AI continues to grow: we have individual AI labs in 10 years that might be doing on the order of eg $30B/yr in revenue.<br><br> We also get more impressive capabilities: maybe AI systems can get gold on the IMO in five years, we get more reliable image generation, GPT-N can handle more complicated kinds of coding tasks without making mistakes, stuff like that. So in 10 years AI systems are just pretty valuable economically, but I expect the AI industry to look more like today&#39;s tech industry - valuable but not economically transformative.<br><br> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy. However, I would not be surprised by a mild acceleration of overall economic growth driven by the impact of AI. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:28:51 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:28:51 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> This is mostly because I don&#39;t expect just putting 1e30 FLOP of training compute into a system will be enough to get AI systems that can substitute for humans on most or all tasks of the economy.</p></blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:32:48 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:32:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> To check, do you think that having perfect ems of some productive human would be transformative, a la <a href="https://www.cold-takes.com/the-duplicator/#explosive-growth">the Duplicator</a> ?</p></blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p><blockquote><p> If so, what is the main reason you don&#39;t think a sufficiently bigger training run would lead to something of that level of impact? Is this related to the savannah-to-boardroom generalization / human-level learning-of-new things point I raised previously?</p></blockquote><p> I think that&#39;s an important part of it, yes. I expect the systems we&#39;ll have in 10 years will be really good at some things with some bizarre failure modes and domains where they lack competence. My example of GPT-4 not being able to play tic-tac-toe is rather anecdotal, but I would worry about other things of a similar nature when we actually want these systems to replace humans throughout the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> Eventually, yes, but even there I expect substantial amounts of delay (median of a few years, maybe as long as a decade) because people won&#39;t immediately start using the technology.</p></blockquote><p> Interestingly, I think in the case of ems this is more plausible than in the case of normal AGI. Because normal AGI will be more easily extendible to superhuman levels. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:35:19 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:35:19 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> FWIW I think the kind of AGI you and I are imagining as the most plausible first AGI is pretty janky, and the main way I see it improving stuff is by doing normal ML R&amp;D, not galaxy-brained &quot;editing its own source code by hand&quot; stuff. The normal AI R&amp;D could be done by all the ems too.</p><p> (It depends on where the AI is at when you imagine dropping ems into the scenario.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree with that. The jankiness is a point in my favor, because it means there&#39;s lots of room to grow by ironing out the kinks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:34:05 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:34:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Overall Ege, thanks for writing that scenario! Here are some questions / requests for elaboration:<br><br> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?</p><p> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:39:56 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:39:56 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> (1) So in your median world, when do we finally get to AGI, and what changes between 2030 and then that accounts for the difference?<br><br> (2) I take it that in this scenario, despite getting IMO gold etc. the systems of 2030 are not able to do the work of today&#39;s remote OAI engineer? Just clarifying. Can you say more about what goes wrong when you try to use them in such a role? Or do you think that AI R&amp;D will indeed benefit from automated engineers, but that AI progress will be bottlenecked on compute or data or insights or something that won&#39;t be accelerating?<br><br> (3) What about AI takeover? Suppose an AI lab in 2030, in your median scenario, &quot;goes rogue&quot; and decides &quot;fuck it, let&#39;s just deliberately make an unaligned powerseeking AGI and then secretly put it in charge of the whole company.&quot; What happens then?</p></blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering that gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.<br><br> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.<br><br> (3): I don&#39;t think anything notable would happen. I don&#39;t believe the AI systems of 2030 will be capable enough to manage an AI lab. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:43:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:43:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I think Ege&#39;s median world is plausible, just like Daniel&#39;s median world; I think my probability on &quot;Ege world or more chill than that&quot; is lower than my probability on &quot;Daniel world or less chill than that.&quot; Earlier I said 25% on Daniel-or-crazier, I think I&#39;m at 15% on Ege-or-less-crazy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:46:08 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:46:08 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Re: the &quot;fuck it&quot; scenario: What I&#39;m interested in here is what skills you think the system would be lacking, that would make it fail. Like right now for example we had a baby version of this with ChaosGPT4, which lacked strategic judgment and also had a very high mistakes-to-ability-to-recover-from-mistakes ratio, and also started from a bad position (being constantly monitored, zero human allies). So all it did was make some hilarious tweets and get shut down. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:46:27 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:46:27 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?</p><p> Eg suppose some AI system was trained to learn new video games: each RL episode was it being shown a video game it had never seen, and it&#39;s supposed to try to play it; its reward is the score it gets. Then after training this system, you show it a whole new <i>type</i> of video game it has never seen (maybe it was trained on platformers and point-and-click adventures and visual novels, and now you show it a first-person-shooter for the first time). Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:47:16 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:47:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Ege, do you think you&#39;d update if you saw a demonstration of sophisticated sample-efficient in-context learning and far-off-distribution transfer?<br></p></blockquote><p>是的。</p><blockquote><p> Suppose it could get decent at the first-person-shooter after like a subjective hour of messing around with it. If you saw that demo in 2025, how would that update your timelines?</p></blockquote><p> I would probably update substantially towards agreeing with you. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:49:01 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:49:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (1): I&#39;m sufficiently uncertain about this that I don&#39;t expect my median world to be particularly representative of the range of outcomes I consider plausible, especially when it comes to giving a date. What I expect to happen is a boring process of engineering which gradually irons out the kinks of the systems, gradual hardware progress allowing bigger training runs, better algorithms allowing for better in-context learning, and many other similar things. As this continues, I expect to see AIs substituting for humans on more and more tasks in the economy, until at some point AIs become superior to humans across the board.</p></blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:51:56 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:51:56 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><blockquote><p> (2): AI R&amp;D will benefit from AI systems, but they won&#39;t automate everything an engineer can do. I think when you try to use the systems in practical situations; they might lose coherence over long chains of thought, or be unable to effectively debug non-performant complex code, or not be able to have as good intuitions about which research directions would be promising, et cetera. In 10 years I fully expect many people in the economy to substantially benefit from AI systems, and AI engineers probably more than most.</p></blockquote><p> How much do you think they&#39;ll be automating/speeding things up? Can you give an example of a coding task such that, if AIs can do that coding task by, say, 2025, you&#39;ll update significantly towards shorter timelines, on the grounds that they are by 2025 doing things you didn&#39;t expect to be doable by 2030?<br><br> (My position is that all of these deficiencies exist in current systems but (a) will rapidly diminish over the next few years and (b) aren&#39;t strong blockers to progress anyway, eg even if they don&#39;t have good research taste they can still speed things up substantially just by doing the engineering and cutting through the schlep) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:54:49 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:54:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> Your median is post-2060 though. So I feel like you need to justify why this boring process of engineering is going to take 30 more years after 2030. Why 30 years and not 300? Indeed, why not 3?</p></blockquote><p> I don&#39;t think it&#39;s going to take ~30 (really 40 per the distribution I submitted) years after 2030, that&#39;s just my median. I think there&#39;s a 1/3 chance it takes more than 75 and 1/5 chance it takes more than 175.<br><br> If you&#39;re asking me to justify why my median is around 2065, I think this is not really that easy to do as I&#39;m essentially just expressing the betting odds I would accept based on intuition.<br><br> Formalizing it is tricky, but I think I could say I don&#39;t find it that plausible the problem of building AI is so hard that we won&#39;t be able to do it even after 300 years of hardware and software progress. Just the massive scaling up of compute we could get from hardware progress and economic growth over that kind of timescale would enable things that look pretty infeasible over the next 20 or 30 years.</p></div></section><h2> Far-off-distribution transfer </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:47:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:47:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The Ege/Ajeya point about far-off-distribution transfer seem like an interesting maybe-crux, so let&#39;s go into that for a bit.</p><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:48:16 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:48:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, FWIW I think the savannah-to-boardroom transfer stuff is probably underlying past-Eliezer (not sure about current Eliezer) and also a lot of &quot;stochastic parrot&quot;-style skeptics. I think it&#39;s a good point under-discussed by the short timelines crowd, though I don&#39;t think it&#39;s decisive. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:49:32 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:49:32 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> My guess is Ajeya has pretty high probability that that kind of distribution transfer will happen within the next few years and very likely the next decade?</p></blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no. I just think it&#39;ll take a lot of hard work to make up for the weaknesses of not having transfer this good. Paul has a good unpublished Google doc titled &quot;Doing without transfer.&quot; I think by the time systems are transformative enough to massively accelerate AI R&amp;D, they will still not be that close to savannah-to-boardroom level transfer, but it will be fine because they will be trained on exactly what we wanted them to do for us. (This btw also underlies some lower-risk-level intuitions I have relative to MIRI crowd.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 19:51:05 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 19:51:05 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> Actually I&#39;m pretty unsure, and slightly lean toward no.</p></blockquote><p> Oh, huh, that is really surprising to me. But good to have that clarified. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:52:00 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:52:00 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Yeah, I just think the way we get our OAI-engineer-replacing-thingie is going to be radically different cognitively than human OAI-engineers, in that it will have coding instincts honed through ancestral memory the way grizzly bears have salmon-catching instincts baked into them through their ancestral memory. For example, if you give it a body, I don&#39;t think it&#39;d learn super quickly to catch antelope in the savannah, the way a baby human caveperson could learn to code if you transported them to today.</p><p> But it&#39;s salient to me that this might just leave a bunch of awkward gaps, since we&#39;re trying to make do with systems holistically less intelligent than humans, but just more specialized to coding, writing, and so on. This is why I think the Ege world is plausible.</p><p> I also dislike using the term AGI for this reason. (Or rather, I think there is a thing people have in mind by AGI which makes sense, but it will come deep into the Singularity, after the earlier transformative AI systems that are not AGI-in-this-sense.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 19:57:19 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 19:57:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><blockquote><p> I also dislike using the term AGI for this reason.</p></blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 19:58:52 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 19:58:52 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> In my median world, the term &quot;AGI&quot; also becomes increasingly meaningless because different ways people have operationalized criteria for what counts as AGI and what doesn&#39;t begin to come apart. For example, we have AIs that can pass the Turing test for casual conversation (even if judges can ask about recent events), but these AIs can&#39;t be plugged in to do an ordinary job in the economy.</p></blockquote><p> Yes, I&#39;m very sympathetic to this kind of thing, which is why I like TAI (and it&#39;s related to the fact that I think we&#39;ll first have grizzly-bears-of-coding, not generally-intelligent-beings). But it bites much less in my view because it&#39;s all much more compressed and there&#39;s a pretty shortish period of a few years where all plausible things people could mean by AGI are achieved, including the algorithm that has savannah-to-boardroom-level transfer.</p></div></section><h2> A concrete scenario &amp; where its surprises are </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 19:59:49 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 19:59:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> We can delete this hook later if no one bites, but in case someone does, here&#39;s a scenario I think it would be productive to discuss:<br><br> (1) Q1 2024: A bigger, better model than GPT-4 is released by some lab. It&#39;s multimodal; it can take a screenshot as input and output not just tokens but keystrokes and mouseclicks and images. Just like with GPT-4 vs. GPT-3.5 vs. GPT-3, it turns out to have new emergent capabilities. Everything GPT-4 can do, it can do better, but there are also some qualitatively new things that it can do (though not super reliably) that GPT-4 couldn&#39;t do.</p><p> (2) Q3 2024: Said model is fine-tuned to be an agent. It was already better at being strapped into an AutoGPT harness than GPT-4 was, so it was already useful for some things, but now it&#39;s being trained on tons of data to be a general-purpose assistant agent. Lots of people are raving about it. It&#39;s like another ChatGPT moment; people are using it for all the things they used ChatGPT for but then also a bunch more stuff. Unlike ChatGPT you can just leave it running in the background, working away at some problem or task for you. It can write docs and edit them and fact-check them; it can write code and then debug it.</p><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p><p> (4) Q3 2025: OK now things are getting serious. The kinks have generally been worked out. This newer model is being continually trained on oodles of data from a huge base of customers; they have it do all sorts of tasks and it tries and sometimes fails and sometimes succeeds and is trained to succeed more often. Gradually the set of tasks it can do reliably expands, over the course of a few months. It doesn&#39;t seem to top out; progress is sorta continuous now -- even as the new year comes, there&#39;s no plateauing, the system just keeps learning new skills as the training data accumulates. Now many millions of people are basically treating it like a coworker and virtual assistant. People are giving it their passwords and such and letting it handle life admin tasks for them, help with shopping, etc. and of course quite a lot of code is being written by it. Researchers at big AGI labs swear by it, and rumor is that the next version of the system, which is already beginning training, won&#39;t be released to the public because the lab won&#39;t want their competitors to have access to it. Already there are claims that typical researchers and engineers at AGI labs are approximately doubled in productivity, because they mostly have to just oversee and manage and debug the lightning-fast labor of their AI assistant. And it&#39;s continually getting better at doing said debugging itself.</p><p> (5) Q1 2026: The next version comes online. It is released, but it refuses to help with ML research. Leaks indicate that it doesn&#39;t refuse to help with ML research internally, and in fact is heavily automating the process at its parent corporation. It&#39;s basically doing all the work by itself; the humans are basically just watching the metrics go up and making suggestions and trying to understand the new experiments it&#39;s running and architectures it&#39;s proposing.</p><p> (6) Q3 2026 Superintelligent AGI happens, by whatever definition is your favorite. And you see it with your own eyes.<br><br> <strong>Question:</strong> Suppose this scenario happens. What does your credence in &quot;AGI by 2027&quot; look like at each of the 6 stages? Eg what are the biggest updates, and why?<br><br> My own first-pass unconfident answer is:<br> 0 -- 50%<br> 1 -- 50%<br> 2 -- 65%<br> 3 -- 70%<br> 4 -- 90%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><blockquote><p> (3) Q1 2025: Same as (1) all over again: An even bigger model, even better. Also it&#39;s not just AutoGPT harness now, it&#39;s some more sophisticated harness that someone invented. Also it&#39;s good enough to play board games and some video games decently on the first try.</p></blockquote><p> I don&#39;t know how much I care about this (not zero), but I think someone with Ege&#39;s views should care a lot about how it was trained. Was it trained on a whole bunch of very similar board games and video games? How much of a distance of transfer is this, if savannah to boardroom is 100? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> FWIW I interpreted this literally: we have some bigger model like chatgpt that can play some games decently on the first try, and conditional on (2) my median world has those games being mostly stuff similar to what it&#39;s seen before<br><br> so i&#39;m not assuming much evidence of transfer from (2), only some mild amount </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:03:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:03:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, let&#39;s briefly have people try to give probability estimates here, though my model of Ege feels like the first few stages have a ton of ambiguity in their operationalization, which will make it hard to answer in concrete probabilities. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:03:55 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:03:55 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> +1, I also find the ambiguity makes answering this hard</p><p> I&#39;ll wait for Ege to answer first. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:06:26 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:06:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> Re: Daniel, according to my best interpretation of his steps:<br><br> 0 -- 6%<br> 1 -- 6%<br> 2 -- 12%<br> 3 -- 15%<br> 4 -- 30%<br> 5 -- 95%<br> 6 -- 100% </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:11:03 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:11:03 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay here&#39;s my answer:</p><p> 0 -- 20%<br> 1 -- 28%<br> 2 -- 37%<br> 3 -- 50%<br> 4 -- 75%<br> 5 -- 87%<br> 6 -- 100%</p><p> My updates are spread out pretty evenly because the whole scenario seems qualitatively quite plausible and most of my uncertainty is simply whether it will take more scale or more schlep at each stage than is laid out here (including stuff like making it more reliable for a combo of PR and regulation and usable-product reasons). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Thanks both! I am excited about this for a few reasons. One I think it might help to focus the discussion on the parts of the story that are biggest updates for you (and also on the parts that are importantly ambiguous! I&#39;m curious to hear about those!) and two, because as the next three years unfold, we&#39;ll be able to compare what happens to this scenario. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> unfortunately i think the scenarios are vague enough that as a practical matter it will be tricky to adjudicate or decide whether they&#39;ve happened or not </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:15:09 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:15:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> I agree, but I still think it&#39;s worthwhile to do this. Also this was just a hastily written scenario, I&#39;d love to improve it and make it more precise, and I&#39;m all ears for suggestions! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:13:11 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:13:11 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Ege, I&#39;m surprised you&#39;re at 95% at stage 5, given that stage 5&#39;s description is just that AI is doing a lot of AI R&amp;D and leaks suggest it&#39;s going fast. If your previous timelines were several decades, then I&#39;d think even with non-god-like AI systems speeding up R&amp;D it should take like a decade? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:15:45 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:15:45 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> I think once you&#39;re at step 5 it&#39;s overwhelmingly likely that you already have AGI. The key sentence for me is &quot;it&#39;s basically doing all the work by itself&quot; - I have a hard time imagining worlds where an AI can do basically all of the work of running an AI lab by itself but AGI has still not been achieved.<br><br> If the AI&#39;s role is more limited than this, then my update from 4 to 5 would be much smaller. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:17:04 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:17:04 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> I thought Daniel said it was doing all the ML R&amp;D by itself, and the humans were managing it (the AIs are in the role of ICs and the humans are in the role of managers at a tech company). I don&#39;t think it&#39;s obvious that just because some AI systems can pretty autonomously do ML R&amp;D, they can pretty autonomously do everything, and I would have expected your view to agree with me more there. Though maybe you think that if it&#39;s doing ML R&amp;D autonomously, it must have intense transfer / in-context-learning and so it&#39;s almost definitely across-the-board superhuman? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5y2XiEs9AKBnnpSx7-Sat, 04 Nov 2023 20:19:43 GMT" user-id="5y2XiEs9AKBnnpSx7" display-name="Ege Erdil" submitted-date="Sat, 04 Nov 2023 20:19:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ege Erdil</b></section><div><p> If it&#39;s only doing the R&amp;D then I would be lower than 95%, and the exact probability I give for AGI just depends on what that is supposed to mean. That&#39;s an important ambiguity in the operationalization Daniel gives, in my opinion.<br><br> In particular, if you have a system that can somehow basically automate AI R&amp;D but is unable to take over the other tasks involved in running an AI lab, that&#39;s something I don&#39;t expect and would push me far below the 95% forecast I provided above. In this case, I might only update upwards by some small amount based on (4) ->; (5), or maybe not at all.</p></div></section><h2> Overall summary, takeaways and next steps </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:36:52 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:36:52 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is a summary of the discussion so far:</p><p> Daniel made an argument against Hofstadter&#39;s law for trend extrapolation and we discussed the validity of that for a bit.</p><p> A key thing that has come up as an interesting crux/observation is that Ege and Ajeya both don&#39;t expect a massive increase in transfer learning ability in the next few years. For Ege this matters a lot because it&#39;s one of the top reasons why AI will not speed up the economy and AI development that much. Ajeya thinks we can probably speed up AI R&amp;D anyways by making grizzly-bear-like-AI that doesn&#39;t have transfer as good as humans, but is just really good at ML engineering and AI R&amp;D because it was directly trained to be.</p><p> This makes observing substantial transfer learning a pretty relevant crux for Ege and Ajeya in the next few years/decades. Ege says he&#39;d have timelines more similar to Ajeya&#39;s if he observed this.</p><p> Daniel and Ajeya both think that the most plausible scenario is grizzly-bear-like AI with subhuman transfer but human-level or superhuman ML engineering skills, but while Daniel thinks it&#39;ll be relatively fast to work with the grizzly-bear-AIs to massively accelerate R&amp;D, Ajeya thinks that the lower-than-human level &quot;general intelligence&quot; / &quot;transfer&quot; will be a hindrance in a number of little ways, making her think it&#39;s plausible we&#39;ll need bigger models and/or more schlep. If Ajeya saw extreme transfer work out, she&#39;d update more toward thinking everything will be fast and easy, and thus have Daniel-like timelines (even though Daniel himself doesn&#39;t consider extreme transfer to be a crux for him.)</p><p> Daniel and Ege tried to elicit what concretely Ege expects to happen over the coming decades when AI progress continues but doesn&#39;t end up that transformative. Ege expects that AI will have a large effect on the economy, but assigns a substantial amount of probability on persistent deficiencies that prevent it from fully automating AI R&amp;D or very substantially accelerating semiconductor progress.</p><p> <i>(Ajeya, Daniel and Ege all thumbs-up this summary)</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="BpJX4jYXD836kDJ8e-Sat, 04 Nov 2023 20:37:37 GMT" user-id="BpJX4jYXD836kDJ8e" display-name="Ajeya Cotra" submitted-date="Sat, 04 Nov 2023 20:37:37 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Ajeya Cotra</b></section><div><p> Okay thanks everyone, heading out! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:37:48 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:37:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you Ajeya! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="YLFQfGzNdGA4NFcKS-Sat, 04 Nov 2023 20:38:04 GMT" user-id="YLFQfGzNdGA4NFcKS" display-name="Daniel Kokotajlo" submitted-date="Sat, 04 Nov 2023 20:38:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Daniel Kokotajlo</b></section><div><p> Yes thanks Ajeya Ege and Oliver! Super fun. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Sat, 04 Nov 2023 20:42:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Sat, 04 Nov 2023 20:42:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thinking about future discussions on this topic, I think putting probabilities on the scenario that Daniel outlined was a bit hard given the limited time we had, but I quite like the idea of doing a more parallelized and symmetric version of this kind of thing where multiple participants output a concrete sequence of events, and then have other people forecast how they would update on each of those observations, which does seem like a fun way to elicit disagreements and cruxes.</p></div></section><div></div><br/><br/><a href="https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines<guid ispermalink="false"> K2D45BNxnZjdpSX2j</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 10 Nov 2023 05:28:24 GMT</pubDate> </item><item><title><![CDATA[Munk Debate on AI: a few observations and opinions]]></title><description><![CDATA[Published on November 10, 2023 2:00 AM GMT<br/><br/><p> <i>Previously covered on LessWrong</i> <a href="https://www.lesswrong.com/posts/LNwtnZ7MGTmeifkz3/munk-ai-debate-confusions-and-possible-cruxes"><i>here</i></a> <i>,</i> <a href="https://www.lesswrong.com/posts/CA7iLZHNT5xbLK59Y/did-bengio-and-tegmark-lose-a-debate-about-ai-x-risk-against"><i>here</i></a> <i>, and</i> <a href="https://www.lesswrong.com/posts/qsDPHZwjmduSMCJLv/the-partial-fallacy-of-dumb-superintelligence"><i>here</i></a> <i>.</i> </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=144uOfr4SYA"><div><iframe src="https://www.youtube.com/embed/144uOfr4SYA" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> <strong>Observation #1:</strong> The debate isn&#39;t really about the proposition — on a very literal reading of the proposition. ( <a href="https://en.wikipedia.org/wiki/Max_Tegmark">Max Tegmark</a> : &quot;We&#39;re just arguing that the risk is not zero percent.&quot; <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> : &quot;...the risk is negligible...&quot;)</p><p> <strong>Opinion #1:</strong> A better proposition might be something like, &quot;We should slow down AI capabilities research for the sake of humanity.&quot; Or maybe, &quot;Researchers should open source frontier AI models.&quot;</p><hr><p> <strong>Observation #2:</strong> Yann LeCun is claiming that AI safety is (or ought to be) an empirical and iterative process. He believes AI will incrementally progress from mouse-level to human-level and beyond (ie there will be no fast takeoff). He&#39;s not <i>against</i> AI safety per se; he&#39;s just advocating a different approach to AI safety. ( <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a> : &quot;It&#39;s interesting that you&#39;ve been proposing solutions to the safety problem, which means you believe we need to build safe AI. Which means that there is a problem that needs to be fixed.&quot;)</p><p> <strong>Opinion #2:</strong> LeCun catches a lot of flak among people who take AI x-risk very seriously. But I don&#39;t see why his proposed approach to AI safety is wrong or reckless or misinformed. In this vein, Sam Altman <a href="https://forum.effectivealtruism.org/posts/vuATadXMheRhBvXfi/sam-altman-safety-and-capabilities-are-not-these-two">recently asserted</a> that &quot;safety and capabilities are not these two separate things&quot;. I think that&#39;s a claim that is worth seriously considering. I think it&#39;s worth thinking through the implications of that claim for AI safety. Maybe an empirical, iterative approach to AI safety is the most realistic path forward.</p><hr><p> <strong>Observation #3:</strong> <a href="https://en.wikipedia.org/wiki/Melanie_Mitchell">Melanie Mitchell</a> &#39;s central view is that superhuman AGI is so far off that it&#39;s not worth taking seriously right now. (&quot;We can acknowledge the incredible advances in AI without extrapolating to unfounded speculations of emerging superintelligent AI.&quot;) This doesn&#39;t seem to be LeCun&#39;s view.</p><p> <strong>Opinion #3:</strong> It would have been more interesting (to me, at least) to get a second debater on the con side who agrees that AGI can easily be made safe and who could give arguments to that effect, rather than just expressing general skepticism about the near-term prospects of AGI.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Lx4BfG4kjNqxzfbt9/munk-debate-on-ai-a-few-observations-and-opinions<guid ispermalink="false"> Lx4BfG4kjNqxzfbt9</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Fri, 10 Nov 2023 02:00:56 GMT</pubDate> </item><item><title><![CDATA[ACI#6: A Non-Dualistic ACI Model]]></title><description><![CDATA[Published on November 9, 2023 11:01 PM GMT<br/><br/><p> Most traditional AI models are dualistic. As <a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh/p/i3BTagvt3HbPMx6PN">Demski &amp; Garrabrant have pointed out</a> , these models assume that an agent is an object that persists over time, and has well-defined input/output channels, like it&#39;s playing a video game.</p><p> In the real world, however, agents are embedded in the environment, and there&#39;s no well-defined boundary between the agent and the environment. That&#39;s why a non-dualistic model is needed to depict how the boundary and input/output channels emerge from more fundamental notions.</p><p> For example, in Scott Garrabrant&#39;s <a href="https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames"><i>Cartesian Frames</i></a> , input and output can be derived from &quot;an agent&#39;s ability to freely choose&quot; among &quot;possible ways an agent can be&quot;.</p><p> However, choosing is still one of the key concepts of Cartesian Frames, but from a non-dualistic perspective, &quot;it&#39;s not clear what it even means for an embedded agent to choose an option&quot;, since an embedded agent is &quot;the universe poking itself&quot;. Formalizing the idea of choice in a non-dualistic model is as difficult as formalizing the idea of free will.</p><p> To avoid relying on the notion &quot;choosing&quot;, we have proposed the <strong>General Algorithmic Common Intelligence (gACI)</strong> model which describes embedded agents solely from a third-person perspective, and measures the actions of agents using mutual information in an event-centric framework.</p><p> The gACI model does not attempt to answer the question &quot;What should an agent do?&quot;. Instead, it focuses on describing the emergence of the agent-environment boundary, and answering the question &quot;Why does an individual feel like it&#39;s choosing?&quot;</p><p> In the language of decision theory, gACI belongs to descriptive decision theory rather than normative decision theory.</p><p></p><h2> <strong>Communication Channel and Mutual Information</strong></h2><p> In dualistic intelligence models, an agent receives <strong>input</strong> information from the environment, and manipulates the environment through <strong>output</strong> actions. But real-world agents are embedded within the environment, it&#39;s not easy to confine information exchange to a clear input/output channel.</p><p> In the gACI model, on the other hand, the input/output channel is a communication channel, in which the information transfer between a sender and a receiver is measured by <strong>mutual information</strong> . </p><p></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fv0swt2m94bctlwpgrpj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/lpkjtoj84pu9z2tcmztu 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dyzvjguu0l5ojlmzds6i 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eug8w6vqahli2kkqqsyv 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/kzz2ijsxk7hbnwyq6wqb 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iizlpd4qt74hckytqqah 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/eazgyyfppdisqxy1ufdf 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/my6bunbnhpzgg0efhnsf 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ef2vcilpubwaljavidpx 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dywgnfnxwftveb8kmtqg 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/bqx3diq5o1pgkdbdutnw 1500w"><br> <i>Figure 1: From the dualistic input/output model to the mutual information model.</i></p><p></p><p> We can easily define mutual information between the states of any two objects, without specifying how the information is transmitted, or who is the sender and who is the receiver, or what the transmission medium is, or whether they are direct or indirect connected.</p><p> These two objects can be any parts of the world, such as agents, the environment, or any parts of agents or the environment, whose boundaries can be drawn anywhere if necessary. They can even overlap.</p><p> Having mutual information does not always mean knowing or understanding, but it provides an upper bound for knowing or understanding.</p><p> With mutual information of two objects, we can define memories and prophecies.</p><p></p><h2> <strong>Memory and Prophecy</strong></h2><p> <strong>Memory</strong> is information about the past, or a communication channel that transmits information about the past into the future ( <a href="https://drive.google.com/file/d/1t_npcCLGVO3Dr01sDVxd_KDp0xDv-yi2/view">Gershman 2021</a> ).  If A is the receiver and B is the sender, we can define: A&#39;s memory of B is the mutual information between the present state of A and a past state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="M(A,B,t)=I(A(t_0);B(t)), &nbsp; t<t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.081em;">M</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">&lt;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style></p><p>A can have memories about more than one Bs, or about different moments of B. It can also have memories about itself, in other words, A can be equal to B.</p><p> <strong>Prophecy</strong> is the mutual information between the present state of A and a future state of B:</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(A,B,t)=I(A(t_0);B(t)), &nbsp; t>t_0"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.064em;">I</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">;</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span></span></p><p> Obviously, the prophecy will not be confirmed until the future state of B is known.</p><p> A prophecy can be either a prediction about the future, or an action that controls/affects the future. In the language of the <a href="https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind">Active Inference</a> model, it&#39;s either &quot;change my model&quot; or &quot;change the world&quot;.</p><p> It&#39;s not necessary to prefer one interpretation over another, because different interpretations can be derived in different situations, which will be explained in the later chapters. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/o2ncra8iua87bucaqcjj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/x59yo9gi6m2t6lhwmi4m 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/hinefdk6aqt85he60f7n 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/y635sfgtqts4yyi8lnm0 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/idxrp4jnif7jwlvnp0l3 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/stfkrzvpadgosu4ultrj 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wrus6qunfzdxmf8t0h24 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ftbc9xidqv4bbsivwmui 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/pu87jg7dx4jmmu0j8uqj 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wlak3vxqi8f5r7eqsiho 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ccppmmekugmfbg2ufvyl 1826w"><br> <i>Figure 2: Object A can have both memories and prophecies about object B.</i><br></p><h2> <strong>Collect Memories for Prophecies</strong></h2><p> We won&#39;t be surprised to find that most, if not all, objects that have prophecies about object A also have memories about it, although the reverse is not always true. We can speculate that information about the future comes from information about the past.</p><p> For example, if you know the position and velocity of the moon in the past, you can have a lot of information about its position and velocity in the future.</p><p> Not all information is created equal. Using our moon as an example, information about its position and phase contains more information about its future, while the pattern of foam in your coffee cup contains less.</p><p> <i>(Although computing power plays an important role in processing information from memory to prediction or control, we only consider the upper bound of prophecy as if we had infinite computing power. )</i></p><p> Objects with different memories would have different prophecies about the same object. For example, an astronomer and an astrologer would have different information about the future of the planet Mars because of their different knowledge of the universe.</p><p> Intelligence needs prophecy to survive and thrive, because to maintain its homeostasis and achieve its goals, it needs sufficient information about the future, especially about its own future. In order to obtain prophecies about itself, one should collect memories about itself and the world that are useful for predicting or controlling its own future.<br></p><h2> <strong>Autonomy and Heteronomy</strong></h2><p> We can measure the <strong>degree of autonomy</strong> of an object by how much prophecy it has about a future of itself, which indicates its self-governance and independence.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_a(A,t) = P(A(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> Similarly, we can measure the <strong>degree of heteronomy</strong> of object A from object B by how much prophecy B has about A, which indicates A&#39;s degree of dependence on B.</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="D_h(A,B,t) = P(B(t_0),A(t))"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">D</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p> An object that has considerable autonomy can be considered an <strong>individual</strong> or an agent. The permanent loss of autonomy is the <strong>death</strong> of an individual. Death is often the result of the permanent loss of essential memories that can induce prophecies about itself.</p><p> Focusing on different types of information requires different standards for autonomy and death. For example, a human neuron has some autonomy over its metabolism, but the timing and strength of its action potential depends mostly on other neurons. We can think of it as an individual, but it is better to think of it as a part of an individual when studying intelligence. Because the death of a single neuron has little effect on a person&#39;s autonomy, but the death of a person does. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/u5iafy5zmbajbwnztx6x" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/exjglmd0smykwlmgxr06 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/driduuta9smhjpxi84cz 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jb39pddgzba0p2ya55ha 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/fnrcz0hbic1mcbzrdhqr 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/v7dnlmjix92met7iwjrg 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jxjcatvrkfwpgktsnvvl 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/wa7tr6yqbqt66ibdimvm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/iyyybo5vbp6qlxy9j0s3 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ncyautqo9hktff32phyo 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/avjpr3xr37wzdugycayo 876w"></figure><p> <i>Figure 3: Autonomy and Heteronomy</i><br></p><h2> <strong>The Laws of the Mind</strong></h2><p> As an individual accumulates more and more memories and prophecies, it can discover general rules about the world, which are relationships between the past and the future.</p><p> During this rule-learning process, the boundary between the self and the outside world emerges. One will inevitably find out that<strong> </strong><i>some parts of the world follow different rules than other parts</i> , and these special parts are spatially concentrated around itself. We can call this special part the <strong>body</strong> .</p><p> For example, an individual may acknowledge that its body temperature has never been very high, like, say above 1000K, and predict that its body will never experience a temperature above 1000K, if its future is under control. Since some other objects can have a temperature of 1000K, it will conclude that there must be some special rules that prevent one&#39;s body from getting too hot. We call these rules goals, motivations, or emotions, etc.</p><p> The intuitive conclusion is that your body follows some rules that are different from the rules that other objects follow. This is what people call dualism: the body follows the <strong>laws of the mind</strong> , which uses concepts like goals, emotions, logic, etc., while the outside world doesn&#39;t.</p><p> However, the exact boundary between the body and the environment is not very clear. The space surrounding the body may partly follow the laws of the mind and can be called <strong>peripersonal space</strong> , a term borrowed from psychology.</p><p> <i>(Closer examination will reveal that the body and peripersonal space also follow the same scientific laws as the outside world, and the laws of mind are some additional laws that only bodies follow.)</i> </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/yglpx2i2jgom0dug2hdn" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/sy4goovwhu18e6ihgjwd 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/ebgk6j4s88unhlkj1w0t 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwucnxeytevi4b7zqtqz 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/jae02t6duleubcepc99x 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/qtu9k0c6qalzb91dajne 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/xmmmksiszxivbcvurwtu 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/isnkects4ie5oig7lqvz 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/dhdzjgi7xyhavkko1ped 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/rwdgxtvtbhpoomms5nzv 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FRd6nNj3M33w2CSX5/zglq0qd96yfufjeggva5 1246w"></figure><p> <i>Figure 4: Everything in the universe follows the laws of physics, but additionally, one&#39;s body and peripersonal space follow the laws of the mind.</i></p><h2><br> <strong>Dualism and Survival Bias</strong></h2><p> Why do our bodies seem to follow the laws of mind, if bodies are made of the same atoms as the outside world?</p><p> Consider a classic example of survival bias. During World War II, the statistician <a href="https://en.wikipedia.org/wiki/Survivorship_bias#Military">Abraham Wald examined the bullet holes in returning aircrafts</a> and recommended adding armor to the areas that showed the least damage, because most aircrafts damaged in those areas could not return safely to base.</p><p> This survival bias could be overcome by observing the aircraft on the battlefield instead of at the base, where we could find out that the bullet holes are evenly distributed throughout the aircraft, since the survival of the observer is independent of the location of the bullet holes. Because a survival bias is introduced when the observer&#39;s survival is not independent of the observed event.</p><p> We can speculate that if an event is in principle dependent on the observer&#39;s own survival, there will be a survival bias that can&#39;t be overcome. For example, one&#39;s own body temperature is not independent of one&#39;s own survival, but the body temperature of others can be.</p><p> Unlike the pattern of bullet holes in returning aircrafts, the inherent survival bias, including numerous experiences of how to survive, can accumulate in the observer&#39;s memory, like the increased armor in the critical areas of an aircraft. We call the memories of accumulated survival bias the <strong>inward memory</strong> , and the memories of the external world, whose survival bias can be overcome, the <strong>outward memory</strong> .</p><p> The laws of the mind, such as goal-directed mechanisms, can be derived from the inward memory. The observer may find that (almost) everything in the outside world has a cause, but its own goal-driven survival mechanism, such as an aversion to hot temperature, or enhanced armor, has no cause other than the rule &quot;the survival of itself depends on the survival of itself&quot;, or the existence of itself. Then the observer comes to a conclusion: I have a goal, I have made a choice.</p><p><br><br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FRd6nNj3M33w2CSX5/aci-6-a-non-dualistic-aci-model<guid ispermalink="false"> FRd6nNj3M33w2CSX5</guid><dc:creator><![CDATA[Akira Pyinya]]></dc:creator><pubDate> Fri, 10 Nov 2023 00:42:39 GMT</pubDate></item></channel></rss>