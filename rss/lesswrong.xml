<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 3 日星期五 20:13:05 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[8 examples informing my pessimism on uploading without reverse engineering]]></title><description><![CDATA[Published on November 3, 2023 8:03 PM GMT<br/><br/><p> <i>（如果你已经读过我写的所有内容，你会发现这篇文章相当多余。特别请参阅我的旧帖子《</i> <a href="https://www.lesswrong.com/posts/PTkd8nazvH9HQpwP8/building-brain-inspired-agi-is-infinitely-easier-than"><i><u>构建受大脑启发的 AGI 比理解大脑无限容易》</u></i></a> <i>，以及</i><a href="https://www.lesswrong.com/posts/evtKwDCgtQQ7ozLn4/randal-koene-on-brain-understanding-before-whole-brain"><i><u>Randal Koene 在全脑仿真之前对大脑的理解</u></i></a><i>，</i> <a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><i><u>从人工智能 x 风险的角度来看，Connectomics 似乎很棒</u></i></a><i>。但我写这篇文章主要是为了回应</i><a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><i><u>昨天的这篇文章</u></i></a><i>。）</i></p><h1> 1. 背景/背景</h1><h2>1.1 有和没有逆向工程的上传（又名全脑模拟（WBE））是什么样的？</h2><p>我似乎认同<a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work"><u>Davidad</u></a>和<a href="https://ageofem.com/"><u>Robin Hanson</u></a>以及我私下交谈过的其他几个人的观点。 （但我可能会误解他们，并且不想把话放到他们嘴里。）这种观点说：如果我们想做 WBE，我们<i>不需要</i>对大脑进行逆向工程。</p><p>举个“对大脑进行逆向工程”的例子，我可以用丰富的经验来讲述：我经常花一整天的时间思考随机问题，例如： <a href="https://www.lesswrong.com/posts/HA9qiJ8zReBarGAGT/on-oxytocin-sensitive-neurons-in-auditory-cortex"><u>为什么某些小鼠听觉皮层神经元中有催产素受体？</u></a>就像，进化论把这些受体放在那里可能是有原因的——我不认为那是随机出现的东西，或者是其他东西的偶然副作用。 （尽管这始终是一个值得考虑的假设！）那么，那是什么原因呢？即，这些受体正在做什么来帮助小鼠生存、茁壮成长等，它们是如何做到的？</p><p> ......一旦我对这个问题有了一个可行的假设，我就可以继续解决数百甚至数千个此类“为什么和如何”问题。我似乎发现回答这些问题的活动比大多数其他人更简单、更容易处理（而且更有趣！）——你可以自己决定我是异常擅长还是被欺骗了。</p><p>举个<i>没有</i>逆向工程的上传的例子，我认为我们可以弄清楚每个神经元的输入输出关系，并且我们可以测量神经元如何相互连接，然后最后有一天我们可以模拟人脑做任何人脑所做的事情。</p><p>以下是 Robin Hanson 在<i>Age of Em</i>中对非逆向工程观点的争论：</p><blockquote><p>大脑不仅仅是将输入信号转化为状态变化并输出信号；这种转变是大脑的主要功能，无论是对我们还是对设计大脑的进化过程来说都是如此。大脑的设计就是为了使这种信号处理变得稳健和高效。正因为如此，我们期望大脑内编码信号和信号相关状态、转换这些信号和状态并将它们传输到其他地方的物理变量（从技术上讲，“自由度”）总体上在物理上是隔离的，并且与大脑中其他更多不相关的物理自由度和过程脱节。也就是说，大脑其他方面的变化很少影响编码精神状态和信号的关键大脑部分。</p><p>我们已经看到了耳朵和眼睛的这种脱节，这使我们能够创造出有用的人造耳朵和眼睛，让曾经聋过的人能够听到，曾经失明的人能够看到。我们预计这同样适用于更广泛的人造大脑。此外，大多数大脑信号似乎都是神经元尖峰的形式，它们特别容易识别并且与其他物理变量无关。</p><p>如果技术和智力的进步像过去几个世纪一样继续下去，那么最多一千年之内我们将详细了解单个脑细胞如何编码、转换和传输信号。这种理解应该使我们能够从详细的大脑扫描中直接读取相关的脑细胞信号和状态。毕竟，大脑是由非常普通的原子通过相当普通的化学反应相互作用构成的。脑细胞很小，复杂性有限，特别是在管理信号处理的细胞子系统中。所以我们最终应该能够理解和阅读这些子系统。</p><p>由于我们也非常了解如何模拟我们可以理解的任何信号处理系统，因此我们能够模拟脑细胞信号处理似乎只是时间问题，而不是是否问题。由于大脑的信号处理是脑细胞信号处理的总和，因此模拟脑细胞信号处理的能力意味着模拟整个大脑信号处理的能力，尽管成本相应较高。</p></blockquote><p>换句话说：</p><ul><li>对于无需逆向工程的上传（我对此持悲观态度），想象一下源代码看起来有点像“神经元 782384364 具有以下内在神经元属性配置文件：{A.28，B.572，C.37，D .1，E.49，…}。它通过突触类型 {Z.58,Y.82,…} 连接到神经元 935783951，并通过突触类型…}连接到神经元 572379349”。</li><li>然而，对于逆向工程上传（我更乐观的事情），想象一下源代码看起来隐约像一个异常复杂的<a href="https://github.com/leela-zero/leela-zero"><u>ML 源代码存储库</u></a>，充满了人类可读的学习算法以及其他流程和内容，但一切都有合理的变量名称，例如“价值函数”、“预测误差向量”和“免疫系统状态向量”。然后，学习算法都是用从实际人脑扫描中收集到的“训练模型”进行初始化，这些训练模型的参数是巨大的难以辨认的数字数据库，包括这个特定人的生活经历、信仰、欲望等。</li></ul><h2> 1.2 重要的是，双方就“第一步”达成一致：让我们去获取人类连接组！</h2><p>我更喜欢的逆向工程路线是：</p><ul><li>第 1 步：测量人体连接组。辅助数据越多越好。</li><li>第二步：对人脑的工作原理进行逆向工程。</li><li>第 3 步：既然我们了解了一切是如何工作的，我们可能会认识到我们的扫描缺少重要数据，在这种情况下，我们会返回并测量它。</li><li>第四步：上传！</li></ul><p>我悲观的非逆向工程路线是：</p><ul><li>第 1 步：测量人体连接组。辅助数据越多越好。</li><li>步骤2：还对神经元、类器官等进行大量测量，以充分表征神经元的输入输出功能。</li><li>第三步：也许迭代？不确定细节。</li><li>第四步：上传！</li></ul><p>不管怎样，我想强调的是，我们都同意“第一步”。另外，我认为“第一步”是困难、缓慢且昂贵的部分，也许我们正在建造装满显微镜之类的巨型仓库。那么让我们开始吧！</p><p>如果关于第一步之后发生的事情有两个<i>相互独立的</i>积极故事，而人们不同意，那么很好！这就是执行步骤 1 的更多理由！</p><h2> 1.3 这很烦人，因为我<i>想</i>相信无需逆向工程的大脑上传</h2><p>假设我们有一个无法解释的“二进制斑点”，它可以完美地模拟一个非常聪明和友善的特定成年人。但除此之外您无能为力。如果你改变随机位并尝试运行它，它通常会崩溃。</p><p>就 AGI 安全而言，这是一个非常好的情况！我们可以运行大量加速人员，让他们花时间构建一致的 AGI 或其他什么。</p><p>相比之下，假设我们通过逆向工程途径进行了人工上传。我们可以对大量加速的人做同样的事情。<i>或者</i>我们可以开始做极其危险的实验，修改上传内容以使其更强大、更有进取心。 （如果我们训练一个新的神经元，但皮层的神经元多了 10 倍，会怎么样？我们用最大化股价的驱动力取代所有正常的先天驱动力？等等。）</p><p>也许你在想：“好吧，但我们会做安全的事情，而不是危险的事情。”但我接着说：“你说的‘我们’是什么意思？”如果有一个具有良好内部控制的绝密项目，那么当然可以。但如果逆向工程的结果发表，人们就会做各种疯狂的实验。而且长期保守秘密是很困难的。我认为，如果我们能得到一段时期，我们<i>确实</i>有上传但<i>没有</i>非上传类大脑AGI，那么这个时期<i>最多</i>会持续几年，不存在诸如“上传发动政变”之类的奇怪可能性”。有关此问题的更多讨论，请参阅<a href="https://www.lesswrong.com/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective"><u>我的连接组学帖子</u></a>。</p><p>所以，我<i>希望</i>我相信有一条可行的途径来制作一个无法解释的二进制 blob，它可以模拟特定的人类，并且不能做任何其他事情。唉!我不相信。</p><h1> 2.正文：8个例子告诉我在不了解大脑的情况下模仿大脑的悲观主义</h1><h2>2.1 催产素神经元发出同步脉冲泌乳的机制</h2><p>下丘脑视上核中的催产素神经元在哺乳期间每隔几分钟就会同步爆发，从而将催产素脉冲释放到血液中，从而触发“乳汁喷射反射”（又名“乳汁排出”）。 Gareth Leng 在他关于下丘脑的精彩著作（我的评论<a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its">在这里</a>）中用了 30 页的大部分内容来介绍他的团队和其他人为弄清楚神经元如何脉动所做的努力：</p><blockquote><p>喷奶反射似乎是复杂神经网络的产物，该网络将波动的感觉输入（来自饥饿的幼崽的哺乳）转化为在整个催产素细胞群中同步的刻板爆发。</p><p> ......这些实验首次令人信服地证明了任何肽在大脑中的生理作用。他们没有解释喷乳反射，但定义了在解释之前必须回答的问题。建立这个解释又花了二十年的时间。在我们的理解中，所提出的问题是没有先例的。视上核释放的催产素如果不是来自突触，又是从哪里来的呢？如果该释放不受尖峰活动的控制，那么是什么触发了它的释放？如果催产素细胞没有通过突触或电连接连接，那么是什么使它们同步化呢？ [强调]</p></blockquote><p>剧透一下：<a href="https://doi.org/10.1371/journal.pcbi.1000123"><u>这是他 2008 年的计算模型</u></a>，涉及从树突中释放催产素和内源性大麻素（与树突作为输入的标准故事相反）、体积传输（与突触传输相反）和其他一些东西。</p><p> （我很确定上面引文中的“解释”一词应该被理解为“自下而上模型中高层现象的再现”，而不是“对再现如何运作的直观概念解释”。我认为 2008 年的计算模型是这两个模型的第一个模型。）</p><p><strong>这个故事的寓意（我认为）是：</strong>如果我们试图对高层行为进行逆向工程，那很容易！<i>初步估计，我们可以说：“我不知道到底是怎么回事，但当出现这样那样的哺乳迹象时，这些细胞显然每隔几分钟就会产生催产素的短脉冲”。</i>然而，如果我们试图从所涉及的单个神经元的属性开始重现高级行为（不知道它是什么），那么这似乎需要这些研究人员花费数十年的工作，尽管这些神经元异常容易尽管研究人员确切地知道他们想要解释什么，但仍通过实验进行访问。</p><h2> 2.2 神经元具有尖峰时序依赖性可塑性，但“时序”可能涉及 8 小时长的间隙</h2><p><a href="https://en.wikipedia.org/wiki/Conditioned_taste_aversion"><u>条件性味觉厌恶</u></a>(CTA) 是指您在时间 t₁ 吃或喝某物，然后在稍后时间 t2 感到恶心，最后对所吃或喝的东西感到厌恶。有趣的是，如果 t2 仅在 t₁ 之后几秒或几分钟，<i>或者</i>在 t₁ 之后几天，则<i>不会</i>形成厌恶，但如果在 t₁ 之后几个<i>小时，</i>则<i>确实会</i>形成。</p><p> <a href="https://doi.org/10.7554/eLife.07582">Adaikkan &amp; Rosenblum (2015)</a>发现了一种似乎可以解释这一点的机制。它涉及岛叶皮层的神经元。新口味会激活两种分子机制（mumble mumble 磷酸化），这两种机制在口味后 15-30 分钟启动，并分别在 3 小时和 8 小时后放松。据推测，这些随后会与随后的恶心相关信号相互作用以启用 CTA。</p><p><strong>这个故事的寓意（我声称）是：</strong>如果你试图在受控环境中描述神经元的特征，假设它们<i>现在</i>的行为取决于<i>前几秒钟发生</i>的事情，而不是五个小时前发生的事情，那么你可能会发现您的数据集毫无意义。</p><p>与本节的其余部分一样，这个<i>特定的</i>示例可能看起来并不重要——谁在乎我们的上传是否没有条件性的品味厌恶？——但我怀疑这是更广泛类别的一个示例。例如，在正常思维过程中，我认为如果你在3小时前思考过一个概念，那么比你从昨天起就没有思考过的概念更容易被回忆和使用。捕捉这种现象对于我们上传的内容进行良好的科学研究等可能很重要。我似乎记得读过一篇论文，该论文提出了这种现象（或类似现象）背后的分子机制，但我无法立即找到它。</p><h2> 2.3 非常奇怪的神经元和突触</h2><p>丘脑中有一个有趣的东西，叫做“突触三联体”。这很有趣，因为它是三个神经元连接，而不是通常的两个，而且也很有趣，因为其中一个神经元是“向后”的，树突是输出而不是输入。 </p><figure class="image image_resized" style="width:59.62%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ajidhmraykf12u4fuftt" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/i9kxqwe2ngualocldmn6 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/btmu9crqhthvrv313vsr 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/nil47cgt3pf9ptpfn5og 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/ehnwywett77hdcjexqym 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/q5meatjym7wkihdbpvgu 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/kgnsjgniqqmob1iwbkxi 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/xejg21stde2ul1xjrh4m 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/e7ovinizfveh5fkmainq 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/qfce9d7xu2dadwm9jgfp 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wByPb6syhxvqPCutu/jopmmmvahmnntz6tgpwa 859w"><figcaption>图片来源： <a href="https://youtu.be/KBILhSTpzFI?t=170"><u>Murray Sherman 的演讲</u></a></figcaption></figure><p>我怀疑大脑中还有更多同样奇怪的东西——这恰好是我遇到的一个。</p><p><strong>这个故事的寓意（我声称）是：</strong>假设无理解上传路线涉及分别表征<i>N 个</i>不同的事物（例如每种不同类型的神经元），而逆向工程路线涉及分别表征<i>M 个</i>不同的事物（例如每个不同的功能组件/电路包括大脑算法的工作原理）。也许你脑子里有<i>N</i> &lt;&lt; <i>M</i>的想法，因为神经元由少量组件组成，它们被配置成整个大脑中令人眼花缭乱的各种小机器，做不同的事情。</p><p>如果是这样，我通过这个例子建议的可能是<i>N</i> ≈ <i>M</i> ，因为<i>还有</i>各种各样令人眼花缭乱的奇怪的低级组件，即<i>N</i>并不像您想象的那么小。</p><p>另外，我认为<i>M</i>不能超过数百到数千，因为只有大约 20,000 个蛋白质编码基因，它们不仅必须指定大脑算法的所有不可简化的复杂性，而且还必须在大脑和身体。</p><h2> 2.4 模拟两个特定电路的书摘录</h2><p>我有点犹豫是否要包括这一点，因为我还没有检查过，但如果你相信<i>《大脑的想法》</i>这本书，这里有一个摘录：</p><blockquote><p> ……尽管已经明确建立了涉及甲壳动物口胃神经节的三十多个神经元的连接组，但 [Eve] Marder 的研究小组还无法完全解释该系统的一小部分是如何发挥作用的。 ...1980 年，神经科学家 Allen Selverston 发表了一篇备受讨论的思想文章，题为“中央模式生成器可以理解吗？”...在过去的四十年里，情况只是变得更加复杂...不同的[个体中的相同神经元] 还可以显示出非常不同的活动模式——每个神经元的特征可以具有高度可塑性，因为细胞会随着时间的推移改变其组成和功能......</p><p> ......利用电生理学、细胞生物学和广泛的计算机建模，对形成龙虾口胃系统中的中央模式发生器的几十个神经元的连接组进行了数十年的研究，但仍未完全揭示其有限功能是如何出现的。</p><p>甚至像[青蛙]错误检测视网膜细胞这样的电路的功能——一组简单的、易于理解的神经元，具有明显直观的功能——在计算层面上也没有被完全理解。有两种相互竞争的模型可以解释细胞的作用以及它们如何相互连接（一种基于象鼻虫，另一种基于兔子）；他们的支持者已经争论了半个多世纪，但这个问题仍然没有解决。 2017 年，报道了用于检测<i>果蝇</i>运动的神经基质的连接组，其中包括有关哪些突触是兴奋性的、哪些是抑制性突触的信息。即使这样也没有解决这两个模型哪个是正确的问题。</p></blockquote><p><strong>这个故事的寓意（我声称）是：</strong>建立一个自下而上的模型，从低级组成神经元重现高级行为是非常困难的，<i>即使</i>我们知道高级行为是什么，因此可以迭代和当我们最初的建模尝试不“有效”时进行迭代。如果我们<i>不</i>知道我们试图解释的高级行为，因此不知道我们最初的建模尝试是否足够（这是“无需理解的上传”计划的必要部分），我们应该预计事情会变得更加困难。</p><h2> 2.5 线虫上传失败</h2><p>线虫只有302个神经元，且数据丰富。但如果我理解正确的话，我们仍然无法在自下而上的模型中重现其所有高级行为。</p><p>我实际上对细节很不熟悉，但请参阅<a href="https://www.lesswrong.com/posts/mHqQxwKuzZS69CXX5/whole-brain-emulation-no-progress-on-c-elegans-after-10">全脑模拟：十年后线虫没有进展</a>（包括评论部分）。</p><p>我记得听说这成为一个挑战的部分原因是我接下来要讨论的事情：</p><h2> 2.6 神经元可以通过在细胞核中存储信息（例如基因表达）来永久改变其行为</h2><p>例如，参见<a href="https://gershmanlab.com/pubs/memory_synthesis.pdf">Sam Gershman</a> 、 <a href="https://www.nature.com/articles/s41539-019-0048-y">David Glanzman</a> 、 <a href="https://scholar.google.com/scholar?cluster=1229364941707818843&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22">Randy Gallistel</a>的论文。这一部分对我来说感觉很奇怪，因为我同意传统观点，即人类学习主要涉及突触内部和周围的永久性变化，而不是细胞核的永久性变化，并且认为前一句中的人们在他们的异端案例中走得太远了。相反。但是，“细胞核中的信息存储并不是人类智能的主要故事”（我相信）与“细胞核中的信息存储<i>根本</i>不会发生，或者影响很小，我们可以忽略它，但仍然不同”。获得相同的高级行为”（我不相信）。</p><p><strong>这个故事的寓意（我声称）是：</strong>如果你不知道大脑中发生了什么以及如何发生，那么你就不知道你是否在测量正确的东西，直到<i>“叮”</i> ，你的模拟再现了高-水平行为。在达成协议之前，你只有一个糟糕的模型，并且不知道如何修复它。 （即使在达成一致<i>之后</i>，如果您不知道相关组件在更大的设计中“应该”做什么，您也不知道是否有<i>足够高的保真度</i>一致。）我不确定是否有可能测量人脑切片中的基因表达等。我怀疑这很容易！但是，如果我们<i>了解</i>基因表达在大脑某部分工作中的作用，我们就可以推断出如果忽略它，我们会失去什么，并看看是否有更简单的解决方法。如果我们<i>不</i>明白它在做什么，那么我们就会坐在那里盯着与数据不匹配的模拟，我们不知道这个问题有多关键，也不知道我们是否遗漏了其他任何东西。</p><h2> 2.7 随时间变化的胶质细胞基因表达</h2><p>我昨天刚读到这个：</p><blockquote><p>例如，[下丘脑视交叉上核]中的星形胶质细胞显示时钟基因的节律表达并影响昼夜节律运动行为 (25) <a href="https://doi.org/10.1126/science.adh8488">[来源]</a></p></blockquote><p>这有点像上面第 2.3 节（有很多特殊的低级组件需要表征，而不是由相同的基本神经元构建块组成的许多电路）和上面第 2.6 节（如果满足以下条件，则需要模拟基因表达）的组合：您想要获得正确的高级行为）。</p><h2> 2.8 代谢型（相对于离子型）受体可以在任意时间尺度上产生几乎任意的效应</h2><p>如果我理解正确的话，大脑使用大量的<a href="https://en.wikipedia.org/wiki/Ligand-gated_ion_channel">离子型受体</a>，其作用是立即局部改变进入神经元的离子流。伟大的！这很容易建模。</p><p>不幸的是，大脑<i>还</i>使用大量<a href="https://en.wikipedia.org/wiki/Metabotropic_receptor">代谢型受体</a>（又名 G 蛋白偶联受体），如果我理解正确的话，它们的作用极其可变，实际上几乎是任意的。基本上，当它们附着在配体上时，就会引发信号级联反应，最终在任何时间尺度上对细胞产生几乎任何影响。它可能会改变神经元的突触可塑性规则，可能会改变基因表达，可能会增加或减少细胞神经肽的产生，等等。</p><p><strong>这个故事的寓意（我声称）是：</strong>如果您的目标是使用低级组件的自下而上模型获得正确的高级行为，那么您可能需要通过实验弄清楚所有这些信号级联是什么，对于每个具有代谢型受体的神经元。再说一遍，我不是专家，但这似乎很难。</p><br/><br/> <a href="https://www.lesswrong.com/posts/wByPb6syhxvqPCutu/8-examples-informing-my-pessimism-on-uploading-without#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wByPb6syhxvqPCutu/8-examples-informing-my-pessimism-on-uploading-without<guid ispermalink="false"> wByPb6syhxvqPCutu</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Fri, 03 Nov 2023 20:03:50 GMT</pubDate> </item><item><title><![CDATA[Integrity in AI Governance and Advocacy]]></title><description><![CDATA[Published on November 3, 2023 7:52 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:15:25 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:15:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>好吧，我们都对最近<a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy">关于“AI 联盟中的很多人都在撒谎”的猜想帖子</a>以及<a href="https://twitter.com/dw2/status/1716515784355160495">相关的营销活动和其他内容</a>有一些感受。</p><p>我希望能提供一些背景信息，让我能够思考这个问题，并分享我们在该领域拥有的信息，这些信息可能有助于我们弄清楚正在发生的事情。</p><p>我预计这很快就会导致我们最终陷入一些更广泛的问题，例如如何进行宣传、当前围绕人工智能联盟的社交网络应该作为一个团体进行多少协调、如何平衡宣传与研究等。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:18:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:18:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p>猜想帖的感受：</p><ul><li>人们没有说出自己完整的信念，这有很多好处，这会扰乱认知环境，并使其他人诚实的成本更高。</li><li>撒谎和怯懦的框架让我感觉不舒服。</li><li>我个人曾经对《猜想》有过非常相似的咆哮。自从搬到华盛顿后，我对治理人员更加同情。我们可以尝试找出原因。</li><li>这篇文章体现了我对 Conjecture 的话语和倡导方式的长期不满，我发现这种方式非常缺乏合作性和开放性<i>（注：我在那里工作了大约半年。）</i></li></ul><p>我心中的问题：</p><ul><li>受到存在风险激励的人们应该有多开放？ （我的几个人的肩膀模型说“采取投资组合方法！”-好的，那么什么分配？）</li><li>人们应该如何倡导？我希望研究人员不必 24/7 都在推特上发布他们的信念，这样他们就可以真正完成工作</li><li>奥利，你对此有何看法？</li></ul></div></section><h2>对治理人员不公开关键动机和从属关系有何同情心</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:21:03 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:21:03 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>我个人曾经对《猜想》有过非常相似的咆哮。我现在对治理人员更加同情。我们可以尝试找出原因。</p></blockquote><p>这个方向对我来说似乎最有趣！</p><p>我目前在这个领域的感受是，我非常同情政府人员的一些通信问题，但对其他一些事情非常不同情，我也想自己澄清一下这里的界线在哪里。</p><p>很好奇你是否有任何关键的观察或经历让你更有同情心。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:25:03 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:25:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p><strong>观察结果</strong></p><p>我至少听说过一个例子，有人提出了 x 风险，然后他们的国会办公室就不那么严肃地对待他们了。其他工作人员告诉我谈论 x 风险效果不佳（没有引用具体证据，但我认真对待他们的意见）。</p><ul><li> （不过，这并没有给我带来太多更新。我的模型已经包括“大多数人会认为这很奇怪，并且不太认真地对待你”。问题是，“你是否会让人们以后更有可能做好事，所有的事情通过改善他们的信念、移动奥弗顿窗口或说服 1/10 人等来考虑？”）</li></ul><p>我个人还发现谈论收购和存在风险很棘手，因为这些想法需要很长时间才能解释，而且它们与我推荐的政策之间有许多推论步骤。因此，我经常想简单地提及我的 x 风险动机，然后专注于推理上最接近且仍然真实的内容。 （传统上，这将是“滥用风险，特别是来自外国对手和恐怖分子的风险”以及“未来几年将出现的生物武器和网络攻击能力”。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:28:07 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:28:07 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p><i>我们稍后可能要讨论的单独一点</i></p><p>我很困惑的一件事是：</p><p>我是否应该谈论一些推论上接近的事情，使他们最有可能接受我放在他们桌上的政策，</p><p><i>或者</i>，我是否应该忍住感到困惑，并在许多会议开始时说：“我非常担心人类将在未来十年内灭绝，因为先进的人工智能可能会试图接管世界。需要解释的东西很多，但科学家们站在我这边。请帮忙。” ——我想强调的是担忧的语气。</p><p>因为我认为，我们没有关注我们的实际担忧，也没有用传达我们实际上有多担忧的语气来系统地误导人们，让他们知道我们有多担心/他们应该有多担心。</p><p> <i>（对话后添加的附加解释：</i></p><ul><li>我试图区分两个问题，人们没有公开分享并关注他们的存在风险担忧：</li><li>问题一是，如果不关注你对存在风险的担忧，你就会扭曲人们对你认为什么是真正重要的以及为什么重要的认识。我认为 Habryka 和猜想正确地指出这是欺骗性的（我认为是有害的认知效应，而不是不道德的意图）。</li><li>我在上面试图解决的第二个问题是关于<a href="https://www.econlib.org/archives/2016/01/the_invisible_t.html">缺失的情绪</a>。人工智能治理的交流经常会这样说：“人工智能具有巨大的潜力，但也存在巨大的风险。人工智能可能会被中国滥用，或者失去控制。我们应该平衡创新和安全的需求。”我不会称之为撒谎（尽管我同意它可能会产生误导性效果，请参阅问题 1）。我要强调的是，这听起来不像是一个认为我们都会死的人。它并没有传达出“<i>人工智能的进步是非常可怕的。处理这个问题可能需要紧急、异常困难和前所未有的行动。</i> ”因此，我怀疑这并没有导致人们足够认真地对待这个问题，以采取我们可能需要的重大治理干预措施。当然，我们可能会让政府官员在新闻稿中提及灾难性风险，但他们<i>真的</i>认真对待 10% p(doom) 吗？如果没有，我们的通讯似乎不够。</li><li>当然，我怀疑我们是否应该始终使用“深切关注”的语气。这取决于我们想要做什么。我猜问题是，我们现在在多大程度上努力让政策通过，<i>而不是</i>努力让人们像我们一样认真对待这个问题？另外，我承认这更复杂，因为有时听起来很严肃会让你在房间里笑而不是被认真对待，不同的观众有不同的需求，等等。） </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:30:38 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:30:38 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>因此，我认为对我来说，我完全同情那些发现很难且常常不值得解释他们的 x 风险担忧的人，如果他们正在与那些还没有真正掌握此类工作的人交谈。</p><p>就像，我也一直有这样的经历，我与不同的承包商合作，或者进行筹款，我尽力解释我的工作是关于什么的，但它确实经常最终归结为一些随机的先入之见，他们（有时这是标准的人工智能道德，有时是认知科学和心理学，有时人们认为我经营着一家网络开发初创公司，试图最大限度地提高参与度）。</p><p>我不太同情的是人们会说“请不要谈论我与这个 EA/X-Risk 生态系统的联系，请不要向其他人谈论我在这个领域的信念，请不要”将我列为公开参与过该领域的任何事情等。”</p><p>或者就像， <a href="https://www.lesswrong.com/posts/qtTW6BFrxWw4iHcjf/lying-is-cowardice-not-strategy#E93CLAmc79jKgBLbX">我前几天在评论中提到的</a>杰森·马西尼的事情，当时问他问题的参议员已经提到了“人工智能带来的世界末日风险”，并询问马西尼这种情况发生的可能性和多久，而马西尼只是回答“我不知道”。</p><p>对我来说，这些人并不被视为难以跨越推理距离的人。他们读给我听的更多是因为他们试图实际上在策略上欺骗他们的信仰和归属，这感觉更像是跨越界限。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:30:30 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:30:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p>关于“请不要谈论我的联系或信仰”——</p><p>我不确定我对此感觉有多糟糕。我通常喜欢开放。但我通常也同意人们非常私密，但又诚实地回答问题。例如，当他们希望信息保密时，在网上写两个人约会的事情感觉很奇怪。我现在正在尝试弄清楚这与 EA 隶属关系之间的区别...... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:34:43 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:34:43 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p>好吧，也许这是战略上的欺骗性差异。我同意这一点。</p><p> （就你的观点而言，在我从事人工智能政策工作期间，有几次人们明确要求我淡化我们彼此的了解程度，或者说如果有人问他们，他们会淡化这一点。这对我个人来说非常令人沮丧，因为现在我会因为开放而招致社交积分/被视为背叛。在我看来，他们把我置于这个位置就是背叛。）</p><p>我可能仍然感到困惑的是——记者可能是敌对的。当你知道信息如果不是故意扭曲就会被误解时，隐瞒信息似乎是合理的。想法？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:38:51 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:38:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>奥利维亚·希门尼斯</b></section><div><p>（此外，我对隐藏与治理相关的计划表示同情，因为“我们不想太早产生一堆阻力”。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:43:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:43:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>是的，这绝对是一种对我来说似乎很糟糕的事情，而且我也非常担心这种事情作为其过程的一部分掩盖了它引起的问题（例如，我们真的不知道是否这造成了很多问题，因为人们自然会被激励去掩盖由此造成的问题，而且因为对这种阴谋行动提出指控确实是高风险和困难的），所以如果出了问题，可能很快就会出错，并且会消耗大量被压抑的能量。</p><p>我确实认为在某些情况下我会和朋友一起向其他人隐瞒一些事情，并且对此非常有策略。经典的例子是在某种独裁政权（苏联、纳粹德国等）的统治下与它讨论问题，以及诸如“同性恋”之类的事情，社会对此似乎明显不合理，在这些情况下，我我选择让其他人向我强加这种保密和混淆的请求。 I also feel kind of similar about people being polyamorous, which is pretty relevant to my life, since that still has a pretty huge amount of stigma attached to it.</p><p> I do think I experienced a huge shift after the collapse of FTX where I was like &quot;Ok, but after you caused the biggest fraud since Enron, you really lost your conspiracy license. Like, &#39;the people&#39; (broadly construed) now have a very good reason for wanting to know the social relationships you have, because the last time they didn&#39;t pay attention to this it turns out to have been one of the biggest conspiracies of the last decade&quot;.</p><p> I care about this particularly much because indeed FTX/Sam was actually really very successful at pushing through regulations and causing governance change, but as far as I can tell he was primarily acting with an interest in regulatory capture (having talked to a bunch of people who seem quite well-informed in this space, it seems that he was less than a year away from basically getting a US sponsored monopoly on derivatives trading in the US via regulatory capture). And like, I can&#39;t distinguish the methods he was using from the methods other EAs are using right now (though I do notice some difference). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:43:20 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:43:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Yeah, I was going to say</p><p> I imagine the government people might argue they&#39;re sort of in the Nazi situation where being EA affiliated has a bunch of stigma.</p><p> But given FTX and such, some stigma seems deserved. (Sigh about the regulatory capture part — I wasn&#39;t aware of the extent.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:44:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:44:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> For reference, the relevant regulation to look up is the <a href="https://en.wikipedia.org/wiki/Digital_Commodities_Consumer_Protection_Act">Digitial Commodities Consumer Protection Act</a></p></div></section><h2> Feelings &amp; concerns about governance work by EAs </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:44:55 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:44:55 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I&#39;m curious for you to list some things EA governance people are doing that you think are bad or fine/understandable, so I can see if I disagree with any. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:48:59 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:48:59 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I do think that at a high level, the biggest effect I am tracking from the governance people is that in the last 5 years or so, they were usually the loudest voices that tried to get people to talk less about existential risk publicly, and to stay out of the media, and to not reach out to high-stakes people in various places, because they were worried that doing so would make us look like clowns and would poison the well.</p><p> And then one of my current stories is that at some point, mostly after FTX when people were fed up with listening to some vague EA conservative consensus, a bunch of people started ignoring that advice and finally started saying things publicly (like the FLI letter, Eliezer&#39;s time piece, the CAIS letter, Ian Hogarth&#39;s piece). And then that&#39;s the thing that&#39;s actually been moving things in the policy space.</p><p> My guess is we maybe could have also done that at least a year earlier, and honestly I think given the traction we had in 2015 on a lot of this stuff, with Bill Gates and Elon Musk and Demis, I think there is a decent chance we could have also done a lot of Overton window shifting back then, and us not having done so is I think downstream of a strategy that wanted to maintain lots of social capital with the AI capability companies and random people in governments who would be weirded out by people saying things outside of the Overton window.</p><p> Though again, this is just one story, and I also have other stories where it all depended on Chat-GPT and GPT-4 and before then you would have been laughed out of the room if you had brought up any of this stuff (though I do really think the 2015 Superintelligence stuff is decent evidence against that). It&#39;s also plausible to me that you need a balance of inside and outside game stuff, and that we&#39;ve struck a decent balance, and that yeah, having inside and outside game means there will be conflict between the different people involved in the different games, but it&#39;s ultimately the right call in the end. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:54:52 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:54:52 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Introspecting on my feelings about various maybe-suspicious governance things:</p><ul><li> Explicitly downplaying one&#39;s worries about x risk, a la your read of Matheny&#39;s &quot;I don&#39;t know&quot; comment: <strong>bad</strong></li><li> Explicitly downplaying one&#39;s connections to EA, a la UK people saying &quot;if someone asks me how I know Olivia, I&#39;ll downplay our close friendship&quot;: <strong>bad</strong></li><li> Asking everyone to be quiet and avoid talking to media to avoid well-poisoning: <strong>bad</strong><ul><li> Asking everyone to be quiet bc of fears of the government accelerating AI capabilities: I don&#39;t know <strong>, leans good</strong></li></ul></li><li> Asking people not to mention your EA affiliation and carefully avoiding mentioning it yourself: I don&#39;t know <strong>, depends</strong><ul><li> Why this might be okay/good: EA has undeserved stigma, and it seems good for AI policy to become increasingly separate from EA</li><li> Why this might be bad: Maybe after FTX, EA-affiliated people should take it upon themselves to be very open</li><li> Current reconciliation: Be honesty about your affiliations if asked and don&#39;t ask others to hide them, but feel free to purposefully distance yourself from EA and avoid bringing it up</li></ul></li><li> Not tweeting about or mentioning x-risk in meetings where inferential gap is too big: I don&#39;t know <strong>, leans fine</strong><ul><li> I&#39;m conflicted between &quot;We might be able to make the Overton window much wider and improve everyone&#39;s sanity by all being very open and explicit about this often&quot; and &quot;It&#39;s locally pretty burdensome to frequently share your views to people without context&quot; </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 18:55:21 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 18:55:21 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> The second biggest thing I am tracking is a kind of irrecoverable loss of trust that I am worried will happen between &quot;us&quot; and &quot;the public&quot;, or something in that space.</p><p> Like, a big problem with doing this kind of information management where you try to hide your connections and affiliations is that it&#39;s really hard for people to come to trust you again afterwards. If you get caught doing this, it&#39;s extremely hard to rebuild trust that you aren&#39;t doing this in the future, and I think this dynamic usually results in some pretty intense immune reactions when people fully catch up with what is happening.</p><p> Like, I am quite worried that we will end up with some McCarthy-esque immune reaction to EA people in the US and the UK government where people will be like &quot;wait, what the fuck, how did it happen that this weirdly intense social group with strong shared ideology is now suddenly having such an enormous amount of power in government? Wow, I need to kill this thing with fire, because I don&#39;t even know how to track where it is, or who is involved, so paranoia is really the only option&quot;. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 18:57:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 18:57:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> <strong>On &quot;CAIS/Ian/etc. finally</strong> <strong>just said it in public&quot;</strong></p><p> I think it&#39;s likely the governance folk wouldn&#39;t have done this themselves at that time, had no one else done it. So, I&#39;m glad CAIS did.</p><p> I&#39;m not totally convinced we could have done it pre-ChatGPT without the blowback people feared. I&#39;ve definitely updated a bit given how great the government response has been (considering the evidence of risk is limited and people have been pretty open about their fears) and how not-limited we are by &quot;this sounds crazy and unjustified&quot; (relative to my expectations).</p><p> <strong>On public</strong> <strong>concern,</strong></p><p> I agree this is possible and worrying.</p><p> I&#39;m also moderately worried about making the AI convo so crowded, by engaging with the public a lot, that x-risk doesn&#39;t get dealt with. I&#39;m at least sympathetic to &quot;don&#39;t involve a bunch of people in your project who you don&#39;t actually expect to contribute, just because they might be unhappy if they&#39;re not included&quot;.</p></div></section><h2> Stigmas around EA in the policy world </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:02:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:02:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I&#39;m conflicted between &quot;EA has undeserved stigma&quot; and &quot;after FTX, everyone should take it upon themselves to be very open&quot;</p></blockquote><p> I am kind of interested in talking about this a bit. I feel like it&#39;s a thing I&#39;ve heard a lot, and I guess I don&#39;t super buy it. What is the undeserved stigma that EA is supposed to have? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:08:15 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:08:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Is your claim that EA&#39;s stigma is all deserved?</p><p> Laying out the stigmas I notice:</p><ul><li> Weird beliefs lead to corruption, see FTX<ul><li> Probably exaggerates the extent of connection each individual had to FTX. But insofar as FTX&#39;s decisions were related EA culture, fair enough.</li></ul></li><li> Have conflicts with the labs, see OpenAI &amp; Anthropic affiliations<ul><li> Fair enough</li></ul></li><li> Elite out-of-touch people<ul><li> Sorta true (mostly white, wealthy, coastal elites), sorta off (lots of people didn&#39;t come from wealth, and they got into this space because they wanted to be as effectively altruistic as possible)</li></ul></li><li> Billionaires selfish interests<ul><li> Seems wrong; we&#39;re mostly trying to help people rather than make money</li></ul></li><li> Weird longtermist techno-optimists who don&#39;t care about people<ul><li> Weird longtermists, yup.</li><li> Techno-optimistics, kinda, but some of us are pretty pessimistic about AI and using AI to solve that problem.</li><li> Don&#39;t care about people, mostly wrong.</li></ul></li></ul><p> I guess the stigmas seem pretty directionally true about the negatives, and just miss that there is serious thought / positives here.</p><p> If journalists said, &quot;Matheny is EA-affiliated. That suggests he&#39;s related to the community that&#39;s thought most deeply about catastrophic AI risk and has historically tried hard and succeeded at doing lots of good. It should also raise some eyebrows, see FTX,&quot; then I&#39;d be fine. But usually it&#39;s just the second half, and that&#39;s why I&#39;m sympathetic to people avoiding discussing their affiliation. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:10:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:10:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, I like this analysis, and I think it roughly tracks how I am thinking about it.</p><p> I do think the bar for &quot;your concerns about me are so unreasonable that I am going to actively obfuscate any markers of myself that might trigger those concerns&quot; is quite high. Like I think the bar can&#39;t be at &quot;well, I thought about these concerns and they are not true&quot;, it has to be at &quot;I am seriously concerned that when the flags trigger you will do something quite unreasonable&quot;, like they are with the gayness and the communism-dissenter stuff. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:12:25 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:12:25 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Fair enough. This might be a case of governance people overestimating honesty costs / underestimating benefits, which I still think they often directionally do.</p><p> (I&#39;ll also note, what if all the high profile people tried defending EA? (Defending in the sense of - laying out the &quot;Here are the good things; here are the bad things; here&#39;s how seriously I think you should take them, all things considered.&quot;)) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:14:17 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:14:17 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I don&#39;t think people even have to defend EA or something. I think there are a lot of people who I think justifiably would like to distance themselves from that identity and social network because they have genuine concerns about it.</p><p> But I think a defense would definitely open the door for a conversation that acknowledges that of course there is a real thing here that has a lot of power and influence, and would invite people tracking the structure of that thing and what it might do in the future, and if that happens I am much less concerned about both the negative epistemic effects and the downside risk from this all exploding in my face. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:16:13 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:16:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Yeah, I&#39;d be interested in part because I want the skeptical-of-EA folk to know that I don&#39;t think they&#39;re crazy and I&#39;m not trying to ignore them or lie to them. I&#39;m just opting to keep doing things with/near EA because they think well and they have resources, which is helping me with my altruistic goals. If the skeptics are like &quot;OK, I still distrust you&quot;, fair enough.</p><p> Do you have ideas for ways to make the thing you want here happen? What does it look like? An op-ed from Person X?</p></div></section><h2> How can we make policy stuff more transparent? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:19:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:19:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Probably somewhat controversially, but I&#39;ve been kind of happy about the <a href="https://www.politico.com/news/2023/10/13/open-philanthropy-funding-ai-policy-00121362">Politico</a> pieces that have been published. We had two that basically tried to make the case there is an EA conspiracy in DC that has lots of power in a kind of unaccountable way.</p><p> Maybe someone could reach out to the author and be like &quot;Ok, yeah, we are kind of a bit conspiratorial, sorry about that. But I think let&#39;s try to come clean, I will tell you all the stuff that I know, and you take seriously the hypothesis that we really aren&#39;t doing this to profit off of AI, but because we are genuinely concerned about catastrophic risks from AI&quot;.</p><p> That does seem like kind of a doomed plan, but like, something in the space feels good to me. Maybe we can work with some journalists we know to write a thing that puts the cards on the table, and isn&#39;t just a puff-piece that tries to frame everything in the most positive way, but is genuinely asking hard questions. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:20:09 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:20:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Politico: +1 on being glad it came out actually!</p><ul><li> (I tentatively wish people had just said this themselves first instead of it being &quot;found out&quot;. Possibly this is part of how I&#39;ll make the case to people I&#39;m asking to be more open in the future.)</li><li> (Also, part of my gladness with Politico is the more that comes out, the more governance people can evaluate how much this actually blocks their work -- so far, I think very little -- and update towards being more open or just be more open now because now their affiliations have been revealed)</li></ul><p> I like the idea of reaching out to journalists. I&#39;d just want to find one who seems truth-seeking, and share info unilaterally.</p><p> Could you do this? Would you want some bigger name EA / governance person to do it? I&#39;d be happy to chip into a draft. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I think I have probably sadly burdened myself with somewhat too much confidentiality to dance this dance correctly, though I am not sure. I might be able to get buy-in from a bunch of people so that I can be free to speak openly here, but it would increase the amount of work a lot, and also decent chance they don&#39;t say yes and then I would need to be super paranoid which bits I leak when working on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:23:16 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:23:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> As in, you&#39;ve agreed to keep too much secret?</p><p> If so, do you have people in mind who aren&#39;t as burdened by this (and who have the relevant context)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:23:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:23:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, too many secrets. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:25:26 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:25:26 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I assume most of the big names have similar confidentiality burdens.</p><p> I&#39;m open to doing it myself, but don&#39;t have a lot of the context and I&#39;m not sure I want to drag the organizations I&#39;m affiliated with into this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:29:02 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:29:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, ideally it would be one of the big names since that I think would meaningfully cause a shift in how people operate in the space.</p><p> Eliezer is great at moving Overton windows like this, but I think he is really uninterested in tracking detailed social dynamics like this, and so doesn&#39;t really know what&#39;s up. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:30:24 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:30:24 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Do you have time to have some chats with people about the idea or send around a Google doc? Happy to help however. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:32:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:32:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> I do feel quite excited about making this happen, though I do think it will be pretty aggressively shut down, and I feel both sad about that, and also have some sympathy in that it does feel like it somewhat inevitably involves catching some people in the cross-fire who were being more private for good reasons, or who are in a more adversarial context where the information here will be used against them in an unfair way, and I still think it&#39;s worth it, but it does make me feel like this will be quite hard.</p><p> I also notice that I am just afraid of what would happen if I were to eg write a post that&#39;s just like &quot;an overview over the EA-ish/X-risk-ish policy landscape&quot; that names specific people and explains various historical plans. Like I expect it would make me a lot of enemies. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:34:23 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:34:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Same, and some of my fear is &quot;this could unduly make the &#39;good plans&#39; success much harder&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:35:42 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:35:42 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, I think I will sit on this plan for a bit. I hadn&#39;t really considered it before, and I kind of want to bring it up to a bunch of people in the next few weeks and see whether maybe there is enough support for this to make it happen. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:36:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:36:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Nice! (For what it&#39;s worth, I currently like Eliezer most, if he was willing to get into the social stuff)</p><p> Any info it&#39;d be helpful for me to collect from DC folk? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:39:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:39:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Oh, I mean I would love any more data on how much this would make DC folk feel like some burden was lifted from their shoulders vs. it would feel like it would just fuck with their plans.</p><p> I think my actual plan here would maybe be more like an EA Forum post or something that just goes into a lot of detail on what is going on in DC, and isn&#39;t afraid to name specific names or organizations.</p><p> I can imagine directly going for an Op-ed could also work quite well, and would probably be more convincing to outsiders, though maybe ideally you could have both. Where someone writes the forum post on the inside, and then some external party verifies a bunch of the stuff, and digs a bit deeper, and then makes some critiques on the basis of that post, and then the veil is broken. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:39:49 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:39:49 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p>知道了。</p><p> Would the DC post include info that these people have asked/would ask you to keep secret? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:40:56 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:40:56 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Definitely &quot;would&quot;, though if I did this I would want to sign-post that I am planning to do this quite clearly to anyone I talk to.</p><p> I am also burdened with some secrets here, though not that many, and I might be able to free myself from those burdens somehow. Not sure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:41:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:41:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Ok I shall ask around in the next 2 weeks. Ping me if I don&#39;t send an update by then </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:31 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:31 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Thank you!!</p></div></section><h2> Concerns about Conjecture </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:41:50 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:41:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Ok, going back a bit to the top-level, I think I would still want to summarize my feelings on the Conjecture thing a bit more.</p><p> Like, I guess the thing that I would feel bad about if I didn&#39;t say it in a context like this, is to be like &quot;but man, I feel like some of the Conjecture people were like at the top of my list of people trying to do weird epistemically distortive inside-game stuff a few months ago, and this makes them calling out people like this feel quite bad to me&quot;.</p><p> In-general a huge component of my reaction to that post was something in the space of &quot;Connor and Gabe are kind of on my list to track as people I feel most sketched out by in a bunch of different ways, and kind of in the ways the post complaints about&quot; and I feel somewhat bad for having dropped my efforts from a few months ago about doing some more investigation here and writing up my concerns (mostly because I was kind of hoping a bit that Conjecture would just implode as it ran out of funding and maybe the problem would go away) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:06 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:06 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (For what it&#39;s worth, Conjecture has been pretty outside-game-y in my experience. My guess is this is mostly a matter of &quot;they think outside game is the best tactic, given what others are doing and their resources&quot;, but they&#39;ve also expressed ethical concerns with the inside game approach.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:46:37 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:46:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> (For some context on this, Conjecture tried really pretty hard a few months ago to get a bunch of the OpenAI critical comments on <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">this post</a> deleted because they said it would make them look bad to OpenAI and would antagonize people at labs in an unfair way and would mess with their inside-game plans that they assured me were going very well at the time) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:46:54 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:46:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> (I heard a somewhat different story about this from them, but sure, I still take it as is evidence that they&#39;re mostly &quot;doing whatever&#39;s locally tactical&quot;) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:53:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:53:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Anyway, I was similarly disappointed by the post just given I think Conjecture has often been lower integrity and less cooperative than others in/around the community. For instance, from what I can tell,</p><ul><li> They often do things of the form &quot;leaving out info, knowing this has misleading effects&quot;</li><li> One of their reasons for being adversarial is &quot;when you put people on the defense, they say more of their beliefs in public&quot;. Relatedly, they&#39;re into <a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/">conflict theory</a> , which leads them to favor &quot;fight for power&quot; >; &quot;convince people with your good arguments.&quot;</li></ul><p> I have a doc detailing my observations that I&#39;m open to sharing privately, if people DM me.</p><p> (I discussed these concerns with Conjecture at length before leaving. They gave me substantial space to voice these concerns, which I&#39;m appreciative of, and I did leave our conversations feeling like I understood their POV much better. I&#39;m not going to get into &quot;where I&#39;m sympathetic with Conjecture&quot; here, but I&#39;m often sympathetic. I can&#39;t say I ever felt like my concerns were <i>resolved</i> , though.)</p><p> I would be interested in your concerns being written up.</p><p> I do worry about the EA x Conjecture relationship just being increasingly divisive and time-suck-y. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 19:53:57 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 19:53:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Here is an email I sent Eliezer on April 2nd this year with one paragraph removed for confidentiality reasons:</p><hr><p> Hey Eliezer,</p><p> This is just an FYI and I don&#39;t think you should hugely update on this but I felt like I should let you know that I have had some kind of concerning experiences with a bunch of Conjecture people that currently make me hesitant to interface with them very much and make me think they are somewhat systematically misleading or deceptive. A concrete list of examples:</p><p> I had someone reach out to me with the following quote:<br></p><blockquote><p> Mainly, I asked one of their senior people how they plan to make money because they have a lot of random investors, and he basically said there was no plan, AGI was so near that everyone would either be dead or the investors would no longer care by the time anyone noticed they weren&#39;t seeming to make money. This seems misleading either to the investors or to me — I suspect me, because it would really just be wild if they had no plan to ever try to make money, and in fact they do actually have a product (though it seems to just be Whisper repackaged)</p></blockquote><p> I separately had a very weird experience with them on the Long Term Future Fund where Conor Leahy applied for funding for Eleuther AI. We told him we didn&#39;t want to fund Eleuther AI since it sure mostly seemed like capabilities-research but we would be pretty interested in funding AI Alignment research by some of the same people.</p><p> He then confusingly went around to a lot of people around EleutherAI and told them that &quot;Open Phil is not interested in funding pre-paradigmatic AI Alignment research and that that is the reason why they didn&#39;t fund Eleuther AI&quot;.</p><p> This was doubly confusing and misleading because Open Phil had never evaluated a grant to Eleuther AI (Asya who works at Open Phil was involved in the grant evaluation as a fund member, but nothing else), and of course the reason he cited had nothing to do with the reason we actually gave. He seems to have kept saying this for a long time even after I think someone explicitly corrected the statement to him.</p><p> Another experience I had was Gabe from Conjecture reaching out to LessWrong and trying really quite hard to get us to delete the OpenAI critical comments on this post: <a href="https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai">https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai</a></p><p> He said he thought people in-general shouldn&#39;t criticize OpenAI in public like this because this makes diplomatic relationships much harder, and when Ruby told them we don&#39;t delete that kind of criticism he escalated to me and generally tried pretty hard to get me to delete things.</p><p> [... One additional thing that&#39;s a bit more confidential but of similar nature here...]</p><p> None of these are super bad but they give me an overall sense of wanting to keep a bunch of distance from Conjecture, and trepidation about them becoming something like a major public representative of AI Alignment stuff. When I talked to employees of Conjecture about these concern the responses I got also didn&#39;t tend to be &quot;oh, no, that&#39;s totally out of character&quot;, but more like &quot;yeah, I do think there is a lot of naive consequentialism here and I would like your help fixing that&quot;.</p><p> No response required, happy to answer any follow-up questions. Just figured I would err more on the side of sharing things like this post-FTX.</p><p> Best,<br> Oliver </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 19:57:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 19:57:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p><p> Again, I&#39;m not sure &quot;dealing with Conjecture&quot; is worth the time though. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:01:00 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:01:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Main emotional affects of the post for me</p><ul><li> I wish someone else had made these points, less adversarially. I feel like governance people do need to hear them. But the frame might make people less likely to engage or make the engagement.<ul><li> Actually, I will admit the post generated lots of engagement in comments and this discussion. It feels uncooperative to solicit engagement via being adversarial though.</li></ul></li><li> I&#39;m disappointed the comments were mostly &quot;Ugh Conjecture is being adversarial&quot; and less about &quot;Should people be more publicly upfront about how worried they are about AI?&quot;</li><li> There were several other community discussions in the past few weeks that I&#39;ll tentatively call &quot;heated community politics&quot;, and I&#39;m feel overall bad about the pattern.<ul><li> (The other discussions were around whether RSPs are bad and whether Nate Soares is bad. In all three cases, I felt like those saying &quot;bad!&quot; had great points, but (a) their points were shrouded in frames of &quot;is this immoral&quot; that felt very off to me, (b) they felt overconfident and not truth-seeking, and (c) I felt like people were half-dealing with personal grudges. This all felt antithetical to parts of LessWrong and EA culture I love.) </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:07:26 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:07:26 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Yeah, that also roughly matches my emotional reaction. I did like the other RSP discussion that happened that week (and liked my dialogue with Ryan which I thought was pretty productive).</p></div></section><h2> Conjecture as the flag for doomers </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:08:55 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:08:55 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><blockquote><p> I wish MIRI was a little more loudly active, since I think doomy people who are increasingly distrustful of moderate EA want another path, and supporting Conjecture seems pretty attractive from a distance.</p></blockquote><p> Yeah, I share this feeling. I am quite glad MIRI is writing more, but am also definitely worried that somehow Conjecture has positioned itself as being aligned with MIRI in a way that makes me concerned people will end up feeling deceived. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:11 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:11 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Two thoughts</p><ul><li> Conjecture does seem pretty aligned with MIRI in &quot;shut it all down&quot; and &quot;alignment hard&quot; (plus more specific models that lead there).</li><li> I notice MIRI isn&#39;t quite a satisfying place to rally around, since MIRI doesn&#39;t have suggestions for what individuals can do. Conjecture does. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:10:31 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:10:31 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Can you say more about the feeling deceived worry?</p><p> (I didn&#39;t feel deceived having joined myself, but maybe &quot;Conjecture could&#39;ve managed my expectations about the work better&quot; and &quot;I wish the EAs with concerns told me so more explicitly instead of giving very vague warnings&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:18:06 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:18:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, for better or for worse I think a lot of people seem to make decisions on the basis of &quot;is this thing a community-sanctioned &#39;good thing to do (TM)&#39;&quot;. I think this way of making decisions is pretty sus, and I feel a bit confused how much I want to take responsibility for people making decisions this way, but I think because Conjecture and MIRI look similar in a bunch of ways, and I think Conjecture is kind of explicitly is trying to carry the &quot;doomer&quot; flag, a lot of people will parse Conjecture as &quot;a community-sanctioned &#39;good thing to do (TM)&#39;&quot;.</p><p> I think this kind of thing then tends to fail in one of two ways:</p><ul><li> The person who engaged more with Conjecture realizes that Conjecture is much more controversial than they realized within the status hierarchy of the community and that it&#39;s not actually clearly a &#39;good thing to do (TM)&#39;, and then they will feel betrayed by Conjecture for hiding that from them and betrayed by others by not sharing their concerns with them</li><li> The person who engaged much more with Conjecture realizes that the organization hasn&#39;t really internalized the virtues that they associate with getting community approval, and then they will feel unsafe and like the community is kind of fake in how it claims to have certain virtues but doesn&#39;t actually follow them in the projects that have &quot;official community approval&quot;</li></ul><p> Both make me pretty sad.</p><p> Also, even if you are following a less dumb decision-making structure, the world is just really complicated, and especially with tons of people doing hard-to-track behind the scenes work, it is just really hard to figure out who is doing real work or not, and Conjecture has been endorsed by a bunch of different parts of the community for-real (like they received millions of dollars in Jaan funding, for example, IIRC), and I would really like to improve the signal to noise ratio here, and somehow improve the degree to which people&#39;s endorsements accurately track whether a thing will be good. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="y9FWuFX9GMHBzefhw-Wed, 01 Nov 2023 20:19:04 GMT" user-id="y9FWuFX9GMHBzefhw" display-name="Olivia Jimenez" submitted-date="Wed, 01 Nov 2023 20:19:04 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Olivia Jimenez</b></section><div><p> Fair. People did warn me before I joined Conjecture (but it didn&#39;t feel <i>very</i> different from warnings I might get before working at MIRI). Also, most people I know in the community are aware Conjecture has a poor reputation.</p><p> I&#39;d support and am open to writing a Conjecture post explaining the particulars of</p><ul><li> Experiences that make me question their integrity</li><li> Things I wish I knew before joining</li><li> My thoughts of their lying post and RSP campaign (tl;dr: important truth to the content, but really dislike the adversarial frame) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Wed, 01 Nov 2023 20:19:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Wed, 01 Nov 2023 20:19:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>habryka</b></section><div><p> Well, maybe this dialogue will help, if we edit and publish a bunch of it.</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/vFqa8DZCuhyrbSnyx/integrity-in-ai-governance-and-advocacy<guid ispermalink="false"> vFqa8DZCuhyrbSnyx</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:52:33 GMT</pubDate> </item><item><title><![CDATA[Averaging samples from a population with log-normal distribution]]></title><description><![CDATA[Published on November 3, 2023 7:42 PM GMT<br/><br/><p> This was originally a comment on <a href="https://www.lesswrong.com/posts/HjdqPbhRx2ceXHzr2/averages-and-sample-sizes">this post</a> by <a href="https://www.lesswrong.com/users/mruwnik?from=post_header">mruwnik</a> regarding averaging various distributions with different distributions. I made it a post to include pictures.</p><p> The Central Limit Theorem, henceforth CLT, states (in my own words) that regardless of the distribution of a population, sample averages from that population should be normally distributed.</p><p> In theory it should hold for log-normal distributions but that doesn&#39;t feel intuitive to me so I tested it.</p><h3> A silly example of CLT</h3><p> An example I made up in my head to make sense of it:</p><p> Imagine a population comprised of all the people who nap 2 times in a day. Lets plot the ages of this population: </p><figure class="image image_resized" style="width:55.84%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/h8slnvin9v75wbh6vrww" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m83kja1xoshnh65prhwn 158w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/c3n2ytq015maqwxq5jba 238w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/fnslxyz5ukg4nq5lbqnn 318w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lhlkzjbmbklwjlcleskp 398w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/d0i6iip8afyolf7sojf5 478w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/flkvsioij0l7b0iy3fh0 558w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/m12rn0wnnl1qh1zwgkef 638w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ora4h7hhuos4jwdjff40 718w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ui07ar9vidttgwmedksl 798w"></figure><p> Mostly infants and elderly people nap, hence the shape of the graph. This data is NOT normal. But if you randomly pick a small sample (n=10) from this population and average it, it will have a mix of old people and infants that averages to middle-age. For example imagine the ages are 80,2,1,2,75,76,1,1,85,70 this will average to about 39. If you do this over and over again with randomly chosen samples you will get a normal distribution. </p><figure class="image image_resized" style="width:76.2%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jac6cfliah7kii8yldah" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/f55e63dxrlpbd0lbtbsd 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/t6skzwhvzcd4mwxzyrmg 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/g98yjmoh0lybi1axjgyk 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/cblxwycjksseq7uciz0u 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qjutlba1ssdjxfi6sub6 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ggs5rexxsni2n4abocns 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/jorzwfr2o044c8llhxur 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/rqepyv05b0qjkmj9u0er 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/te52ibxfnejqunsxfldn 803w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lruwlnhyxygaslo486cb 810w"></figure><h3> Does it work with log-normal populations?</h3><p> I didn&#39;t find it intuitive this would work for a log-normal population.</p><p> If I take data that is log-normal but split it into small samples, will the average of those small samples be normally distributed?</p><h2><br> <strong>Chess matches:</strong></h2><p> I am arranging a chess tournament. I need to figure out how long the average match is so I can plan accordingly. I hear that chess matches seem to follow a log-normal distribution, but I&#39;m not sure what that means statistically so I will try to just average the game times.</p><h3> Data from the population</h3><p> This is what my fake population (n=100,000) looks like. Its log-normal. </p><figure class="image image_resized" style="width:76.85%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vzjr4piaqb8jny5tvwaz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/zexn7ioinaufqpqr1ib2 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/twpextrya3mba53rof5l 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tqoui3ludnorhanftcov 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ykh6jnxjestjqlnynh6j 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/lbue48tbeoeyajh2r7gt 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/dcslvkhztoz75zy6st2j 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hmltesa1amkoz1yyirgb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vxetqqcgbjrmwbmdxalq 640w"></figure><h3> Tournaments</h3><p> I observe tournaments (n=100 games) and take a simple average of the match length.</p><p> Here is a histogram plot of the tournaments </p><figure class="image image_resized" style="width:79.15%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/u8klmbrfmlo3xue06tfd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/hyx1g6tjajdljqxgse0d 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/qei659xjaqu8jqrnevba 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/vnmbcmftpueyozmtiibo 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/idvjzyrzpu4bmcg21lam 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/y9ejbiwz2lgjpw852vfw 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/tpud4pifh8txi1blprvt 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/ltxq5cqjowsbvyttkdoj 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GkEW4vH6M6pMdowdN/optpwzmplwgecp3euut4 640w"></figure><h2> Lessons?</h2><p> The sample size does matter here. A sample size too small (n=10) and you just end up with the original log-normal distribution. This is expected as the sample size moves from small to large you get a range of smoothing effects pushing the distribution to normal until you get a single point, when the sample = population.</p><br/><br/> <a href="https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/GkEW4vH6M6pMdowdN/averaging-samples-from-a-population-with-log-normal<guid ispermalink="false"> GkEW4vH6M6pMdowdN</guid><dc:creator><![CDATA[CrimsonChin]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:42:17 GMT</pubDate> </item><item><title><![CDATA[Securing Civilization Against Catastrophic Pandemics]]></title><description><![CDATA[Published on November 3, 2023 7:33 PM GMT<br/><br/><h3> Executive summary</h3><p> Pandemic security aims to safeguard the future of civilization from exponentially spreading biological threats. Despite the world&#39;s failure to contain SARS-CoV-2, the existence of far more lethal and transmissible pathogens that afflict animals and growing access to increasingly powerful biotechnologies, no analyses of worst-case scenarios and potential defenses have been published. Here we outline two distinct mechanisms by which pandemic pathogens transmissible between humans could cause societal collapse. In a &quot;Wildfire&quot; pandemic, the justifiable fear of a lethal and highly contagious respiratory agent released in multiple travel hubs leads to the breakdown of essential services. In a &quot;Stealth&quot; pandemic, a rapidly spreading virus with a long incubation period analogous to HIV infects most of humankind. We explain why current pandemic preparedness measures such as rapid vaccines and N95 masks will reliably fail against these threats and outline novel strategies and technologies capable of safeguarding civilisation.</p><h3> Key takeaways</h3><ul><li> Nations cannot yet contain natural, accidental or deliberate pandemics.</li><li> Access to severe pandemics will expand with the ability to program biology.</li><li> If too many essential workers die or refuse to work, societies will collapse.</li><li> A Wildfire pandemic is highly lethal and transmissible enough to infect most essential workers who are taking currently available precautions.<ul><li> Collapse can be prevented by providing essential workers with pandemic-proof personal protective equipment (P4E). (Essential workers are those who must deliver food, water, power and law enforcement without any interruptions.)</li><li> Others can remain safely at home until P4E is available for everyone.</li><li> Once the population is protected, the virus can be locally eradicated.</li></ul></li><li> A Stealth pandemic spreads widely with few symptoms and causes severe harm years later.<ul><li> Societal collapse can be prevented via early warning, credibility, cures, P4E and healthy buildings.<ul><li> Early warning: deep metagenomic sequencing offers reliable detection.</li><li> Credibility: expert responders can assess threats and encourage action.</li><li> Cures: swift medical research can offer hope for the infected.</li><li> P4E: people will need protective equipment that they can trust to block transmission.</li><li> Healthy buildings: the use of germicidal lights and ventilation can prevent indoor infections.</li></ul></li></ul></li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ksevLrNby34TJKGSF/securing-civilization-against-catastrophic-pandemics<guid ispermalink="false"> ksevLrNby34TJKGSF</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Fri, 03 Nov 2023 19:33:06 GMT</pubDate> </item><item><title><![CDATA[Thoughts on open source AI]]></title><description><![CDATA[Published on November 3, 2023 3:35 PM GMT<br/><br/><p><i>认知状态：我只有约 50% 的人认可这一点，这低于我发布内容的典型标准。我更看好“这些是应该在供水中讨论的论点”，而不是“这些论点实际上是正确的”。我不是这方面的专家，我只考虑了大约 15 个小时，而且在发布之前我没有任何相关专家运行这篇文章。</i></p><p><i>感谢 Max Nadeau 和 Eric Neyman 的有益讨论。</i></p><p>目前，关于开源人工智能存在大量的公开辩论。关注人工智能安全的人们普遍认为，开源强大的人工智能系统太危险，不应该被允许；这里的经典例子是“你不应该被允许开源一个人工智能系统，它可以<a href="https://www.lesswrong.com/posts/ytGsHbG7r3W3nJxPT/will-releasing-the-weights-of-large-language-models-grant"><u>为工程新型病原体产生分步指令</u></a>。”另一方面，开源支持者认为开源模型尚未造成重大损害，试图关闭人工智能的访问将导致<a href="https://open.mozilla.org/letter/"><u>权力</u></a><a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>集中</u></a>在少数人工智能实验室手中。</p><p>我认为许多关心人工智能安全的人没有认真考虑过这一点，他们往往会模糊地认为“开源强大的人工智能系统似乎很危险，应该被禁止”。从字面上看，我认为这个计划有点天真：当我们在 2100 年在我们一致的超级智能的帮助下殖民火星时，释放 GPT-5 的权重真的会带来灾难性的风险吗？</p><p>我认为更好的计划看起来像“在确定并披露系统将启用的威胁模型类型之前，您不能开源系统，并且社会已经采取措施来增强对这些威胁模型的鲁棒性。一旦有必要措施已经落实，你们就可以自由开源了。”</p><p>我稍后会详细介绍，但作为一个直觉泵，想象一下：最好的开源模型总是比最好的专有模型（称之为 GPT-SoTA）落后 2 年<span class="footnote-reference" role="doc-noteref" id="fnrefnmr2zc5cm4r"><sup><a href="#fnnmr2zc5cm4r">[1]</a></sup></span> ； GPT-SoTA 在整个经济中广泛部署，并部署用于监控和防止某些攻击媒介，而最好的开源模型也不够智能，不会在 GPT-SoTA 无法捕获的情况下造成任何重大损害。在这个假设的世界里，只要我们能够信任 GPT-SoTA <i>，</i>我们就不会受到开源模型带来的伤害。换句话说，只要最好的开源模型远远落后于最好的专有模型，并且我们明智地了解如何使用最好的专有模型，开源模型就不会杀死我们。</p><p>在这篇文章的其余部分中，我将：</p><ul><li>通过类比密码学中负责任的披露来推动该计划</li><li>详细了解该计划</li><li>讨论这与我对负责任的扩展政策 (RSP) 所暗示的当前计划的理解有何关系</li><li>讨论一些关键的不确定因素</li><li>对围绕开源人工智能的讨论提出一些更高层次的想法</li></ul><h2><strong>与密码学中负责任的披露的类比</strong></h2><p><i>[我不是这方面的专家，本节可能会出现一些细节错误。感谢 Boaz Barak 指出这个类比（但所有错误都是我自己的）。</i></p><p><i>请参阅此脚注</i><span class="footnote-reference" role="doc-noteref" id="fnreflndvavl4r2a"><sup><a href="#fnlndvavl4r2a">[2]，</a></sup></span><i>了解您可以对生物安全披露规范进行的替代类比的讨论，以及它们是否更容易受到开源人工智能的风险。]</i></p><p>假设您发现某些广泛使用的加密方案中存在漏洞。进一步假设您是一个好人，不希望任何人受到黑客攻击。你该怎么办？</p><p>如果您公开发布您的漏洞利用程序，那么很多人都会受到黑客攻击（被阅读您对该漏洞利用程序描述的不那么仁慈的黑客攻击）。另一方面，如果是<a href="https://en.wikipedia.org/wiki/White_hat_(computer_security)"><u>白帽</u></a><u>&nbsp;</u>黑客总是对他们发现的漏洞保密，那么这些漏洞永远不会被修补，直到黑帽黑客发现漏洞并利用它。更一般地说，您可能担心不披露漏洞可能会导致“安全溢出”，即可发现但尚未发现的漏洞随着时间的推移而累积，最终被利用时情况会变得更糟。</p><p>在实践中，密码学界已经达成了<i>负责任的披露</i>政策，大致如下：</p><ul><li>首先，您向受影响方披露该漏洞。<ul><li>作为一个正在运行的示例，请考虑 Google<a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"><u>对 SHA-1 哈希函数的利用</u></a>。在这种情况下，有很多受影响方，因此谷歌公开发布了该漏洞的概念验证，但没有提供足够的细节供其他人立即重现。</li><li>在其他情况下，您可能会私下披露更多信息，例如，如果您在 Windows 操作系统中发现漏洞，您可能会私下将其连同实现漏洞的代码一起披露给 Microsoft。</li></ul></li><li>然后，您为要修补的漏洞设置合理的时间范围。<ul><li>就 SHA-1 而言，补丁是“停止使用 SHA-1”，实施期限为 90 天。</li></ul></li><li>在此时间段结束时，您可以公开发布您的漏洞利用程序，包括执行它的源代码。<ul><li>这确保了受影响的各方得到适当的激励来修补漏洞，并帮助其他白帽黑客在未来发现其他漏洞。</li></ul></li></ul><p>据我了解，该协议使我们的加密方案相对稳健：人们大多不会受到严重的黑客攻击，而当他们这样做时，主要是因为通过社会工程进行攻击（例如<a href="https://en.wikipedia.org/wiki/Operation_Rubicon"><u>，中央情报局秘密拥有他们的加密提供商</u></a>） ，而不是通过对该方案的攻击。 <span class="footnote-reference" role="doc-noteref" id="fnref3ejitdom769"><sup><a href="#fn3ejitdom769">[3]</a></sup></span></p><h2><strong>负责任地披露开源人工智能系统的功能：概述</strong></h2><p><i>[感谢 Yusuf Mahmood 指出本节中概述的协议与此处的协议大致</i><a href="https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf"><i><u>相似</u></i></a><i>。更一般地说，我希望在这一领域工作的人们已经熟悉这些想法。]</i></p><p>在本节中，我将为开源人工智能系统制定一个协议，该协议类似于密码学中的负责任的披露协议。假设假设的公司 Mesa 训练了一个新的人工智能系统<a href="https://en.wikipedia.org/wiki/Camelidae"><u>camelidAI</u></a> ，Mesa 希望将其开源。我们也将最强大的专有人工智能系统称为 GPT-SoTA，我们可以假设它表现良好<span class="footnote-reference" role="doc-noteref" id="fnref1art7tmmdob"><sup><a href="#fn1art7tmmdob">[4]</a></sup></span> 。我想象 GPT-SoTA 比 CamelidAI 更有能力（特别是在大多数领域都是超人）。原则上，如果 GPT-SoTA 比 CamelidAI 更差（因为开源系统已经超越了专有系统），下面的协议仍然有意义，但它会退化为“一旦开源 AI 系统能够引起重大新奇事件，就禁止它们”。他们也无法可靠地减轻伤害。”</p><p>在这个协议中，在camelidAI能够开源之前，[??梅萨？，政府？，第三方？ ？？] 必须：</p><ul><li>评估camelidAI如果开源的话可能会造成什么样的重大新危害。 <span class="footnote-reference" role="doc-noteref" id="fnref7501lf6ibc3"><sup><a href="#fn7501lf6ibc3">[5]</a></sup></span>这些评估者至少应该能够访问开源系统用户将拥有的所有工具，包括微调camelidAI的能力、可以在camelidAI之上构建的外部工具以及对camelidAI的API调用GPT-SoTA。因此，典型的工作流程可能类似于：让 GPT-SoTA 生成可能的接管计划的完整列表，然后微调camelidAI以完成这些计划中的步骤。例如，我们可能会发现：<ul><li>经过微调后，camelidAI 能够在人类专家级别进行有针对性的网络钓鱼（也称为<a href="https://en.wikipedia.org/wiki/Phishing"><u>鱼叉式网络钓鱼</u></a>），但可以扩展到更多目标。</li><li> CamelidAI 可以提供<a href="https://arxiv.org/ftp/arxiv/papers/2306/2306.13952.pdf"><u>外行人可遵循的制造新型病原体的说明，其中包括 DNA 合成公司和生物实验室不筛选客户和订单的说明</u></a>。</li></ul></li><li>向 [?? 披露这些新的有害功能政府？、第三方监督者？、受影响各方？ ??]。</li><li>与相关参与者合作改进系统，直到系统对每个有权访问camelidAI的人来说都是稳健的。<ul><li>例如，确保有一个广泛可用的开源工具，可以检测像camelidAI一样复杂的网络钓鱼尝试，并且具有非常高的可靠性。</li><li>例如，关闭不筛选订单的 DNA 合成公司和生物实验室，或强迫他们使用 GPT-SoTA 筛选潜在大流行病原体的订单。</li><li>请注意，如果camelidAI非常有能力，那么其中一些预防措施可能会非常雄心勃勃，例如“让社会对人为设计的流行病具有强大的能力”。这里的希望在于我们能够获得一个功能强大且性能良好的 GPT-SoTA。</li><li>还要注意的是，这些“鲁棒性”措施是我们无论如何都应该做的事情，即使我们不想开源camelidAI；否则，未来未对齐的人工智能（可能是非法开源的模型）可能会利用一个悬而未决的问题。</li></ul></li><li>一旦社会对camelidAI造成的危害有足够的抵抗力（经[??]认证），您就可以开源camelidAI。</li><li>另一方面，如果 Mesa 在完成上述过程之前开源了camelidAI，那么它会被视为不良行为者（类似于我们对待未负责任地披露漏洞的黑客的方式）。<ul><li>也许这意味着你要对camelidAI或其他东西造成的伤害负责，但不太确定。</li></ul></li></ul><p>作为示例，让我注意该协议的两个特殊情况：</p><ul><li>假设camelidAI = LLaMA-2。我认为访问 LLaMA-2 <span class="footnote-reference" role="doc-noteref" id="fnrefvqihg0rt649"><sup><a href="#fnvqihg0rt649">[6]</a></sup></span>可能不会带来重大的新危害。因此，在进行评估后，缓解步骤很简单：不需要“补丁”，并且 LLaMA-2 可以开源。 （我认为这很好：AFAICT、LLaMA-2 的开源对世界有好处，包括对齐研究。）</li><li>假设camelidAI能够发现这一将岩石和空气变成黑洞的奇怪技巧（天体物理学家讨厌它！）。假设没有合理的缓解措施来缓解这种攻击，camelidAI 永远不会开源。 （我希望即使是强大的开源支持者也会同意这是这种情况下的正确结果。）</li></ul><p>我还将指出该协议与密码学中负责任的披露的两个不同之处：</p><ol><li> Mesa 不得设定社会必须在多长时间内增强 CamelidAI 能力的最后期限。如果camelidAI拥有一种如果被滥用将造成灾难性的能力，并且我们需要十年的技术进步才能找到解决该问题的“补丁”，那么在这种情况发生之前，Mesa不会开源该模型。</li><li>在密码学中，受影响的各方有责任修补漏洞，但在这种情况下，人工智能系统的开发人员有部分责任。</li></ol><p>这两个差异意味着其他各方没有动力去强化他们的系统；原则上他们可能会永远拖延，而 Mesa 永远不会发布camelidAI。我认为应该采取一些措施来解决这个问题，例如政府应该对那些没有充分优先考虑实施必要变革的公司进行罚款。</p><p>但总的来说，我认为这是公平的：如果你知道你的系统可能会造成巨大伤害，并且你没有计划如何防止这种伤害，那么你就无法开源你的人工智能系统。</p><p>我喜欢这个协议的一件事是它很难反驳：如果camelidAI明显能够自主设计一种新的病原体，那么Mesa就不能再声称这些<a href="https://twitter.com/ID_AA_Carmack/status/1719077965055533455"><u>危害是想象的</u></a>或<a href="https://twitter.com/AndrewYNg/status/1719378661475017211"><u>过度夸大的</u></a>，或者<a href="https://twitter.com/ylecun/status/1719692258591506483"><u>作为一个一般原则开源人工智能让我们更安全</u></a>。我们将会受到具体的、明显的伤害；我们可以讨论如何减轻这种特定的伤害，而不是争论抽象的人工智能是否可以减轻人工智能的伤害。如果人工智能可以提供缓解措施，那么我们将找到并实施缓解措施。同样，如果最终证明这些危害<i>是</i>虚构的或夸大的，那么 Mesa 将免费开源camelidAI。</p><h2><strong>这与当前计划有何关系？</strong></h2><p>据我了解，推动许多负责任的扩展政策（RSP）支持者的高层想法是这样的：</p><blockquote><p>在采取某些行动（例如训练或部署人工智能系统）之前，人工智能实验室需要提出“安全论据”，即该行动不会造成重大伤害的论据。例如，如果他们想要部署一个新系统，他们可能会争论：</p><ol><li>我们的系统不会造成伤害，因为它没有足够的能力造成重大损害。 （如果 OpenAI 在发布 GPT-4 之前被要求提出安全论点，那么他们很可能会提出这样的论点，对我来说这似乎是正确的。）</li><li>我们的系统如果尝试这样做<i>可能会</i>造成伤害，但它不会尝试这样做，因为例如它仅通过 API 部署，并且我们已使用[措施]确保任何 API 介导的交互都不会导致其尝试伤害。</li><li>如果我们的系统尝试这样做，它<i>可能</i>会造成伤害，并且我们不能排除它会尝试这样做，但它不会成功造成伤害，因为，例如，它仅在我们拥有非常好的措施的严格控制环境中使用阻止其成功执行有害操作。</li></ol><p>如果不存在这样的论点，那么您需要做一些事情来<i>导致</i>这样的论点存在（例如，更好地调整模型，以便您可以提出上面的论点（2））。在您这样做之前，您不能采取任何您想要采取的有潜在风险的行动。</p></blockquote><p>我认为，如果你在“开源人工智能系统”的情况下应用这个想法，你会得到与我上面概述的协议非常相似的东西：为了开源人工智能系统，你需要提出一个论点开源该系统是安全的。如果不存在这样的论点，那么您需要做一些事情（例如改进对网络钓鱼尝试的电子邮件监控）来使这样的论点存在。</p><p>目前，开源的安全论点与上面的 (1) 相同：当前的开源系统不足以造成重大的新危害。将来，这些论点将变得更加棘手，特别是对于可以修改（例如微调或合并到更大系统中）并且其环境可能是“整个世界”的开源模型。但是，随着前沿人工智能系统的进步彻底改变了世界，这些论点对于非前沿系统来说可能仍然是可能的。 （我预计开源模型将继续落后于前沿。）</p><h2><strong>一些不确定因素</strong></h2><p>以下是我的一些不确定性：</p><ul><li>在实践中，效果如何？<ul><li>我认为一个合理的猜测可能是：几年后，SoTA 模型将足够聪明，如果开源的话，会造成重大灾难，而且——即使有 SoTA AI 的帮助——我们也无法修补相关漏洞，直到奇点（之后球出我们的场）。如果是这样，该协议基本上可以归结为对开源人工智能的禁令，并采取额外的步骤。</li><li>然而，我要指出的是，开源支持者（其中许多人预计有害功能的进展会较慢）可能不同意这一预测。如果他们是对的，那么这个协议可以归结为“评估，然后开源”。我认为，如果人工智能安全人员对未来的看法是正确的，那么制定专门针对人工智能安全人员的需求的政策是有好处的，如果开源人员对未来的看法是正确的，那么制定专门针对开源人员想要的政策是有好处的。</li></ul></li><li>评估人员是否能够预测和衡量开源人工智能系统的所有新危害？<ul><li>遗憾的是，我不确定答案是“是”，这也是我只支持 50% 这篇文章的主要原因。我担心评估者可能失败的两个原因：<ul><li>评估者可能无法获得比用户更好的工具，而且用户的数量要多得多。例如，尽管评估人员将得到 GPT-SoTA 的协助，但如果 CamelidAI 是开源的，数百万用户也将获得访问权。</li><li>在camelidAI开源后，世界可能会发生改变，从而启用新的威胁模型。例如，假设camelidAI + GPT-SoTA并不危险，但camelidAI + GPT-(SoTA+1)（GPT-SoTA后继系统）是危险的。如果 GPT-(SoTA+1) 在camelidAI开源几个月后出现，这似乎是个坏消息。</li></ul></li></ul></li><li>也许由于某种我们难以预料的原因，使用微妙不对齐的 SoTA AI 系统来评估和监控其他 AI 系统确实很糟糕？<ul><li>例如，人工智能系统相互协调的东西。</li></ul></li></ul><h2><strong>关于开源话语的一些思考</strong></h2><p>我认为许多关注人工智能安全的人犯了这样的错误：“我注意到存在一些能力阈值<i>T</i> ，超过该阈值的每个人都可以访问具有 >; <i>T</i>功能的人工智能系统，这将成为当今世界的生存威胁。按照目前的轨迹，有一天有人会开源一个能力 >; <i>T 的</i>人工智能系统。因此，开源很可能会导致灭绝，应该被禁止。”</p><p>我认为这种推理忽略了这样一个事实：当有人第一次尝试开源一个功能 >; <i>T</i>的系统时，世界将在很多方面有所不同。例如，可能存在专有的功能系统<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gg T"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">≫</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.12em;">T</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。总的来说，我认为人工智能安全社区的人们过于担心来自开源模型的威胁。</p><p>此外，人工智能安全社区对开源人工智能的反对目前正在开源社区中产生<i>很大</i>的敌意。就背景而言，开源意识形态与软件开发的历史深深地交织在一起，开源的坚定支持者在技术领域具有很大的代表性和影响力。 <span class="footnote-reference" role="doc-noteref" id="fnrefhqvunm4kwo5"><sup><a href="#fnhqvunm4kwo5">[7]</a></sup></span>我有点担心，按照目前的轨迹，人工智能安全与开源将成为一个主要战场，很难达成共识（比 IMO 不太糟糕的人工智能歧视/道德与 x- 更糟糕）风险划分）。</p><p>在某种程度上，这种敌意是由于对开源危险的不必要的关注或对这种危险存在的草率论据，我认为这确实是不幸的。我认为有充分的理由<i>以特定方式</i>担心<i>一定规模</i>的开源人工智能系统的潜在危险，而且我认为更清楚地了解这些威胁模型的细微差别可能会减少敌意。</p><p>此外，我认为当开源模型变得危险时，我们很有可能有具体的证据表明它们是危险的（例如，因为我们已经看到相同规模的未对齐的专有模型是危险的）。这意味着，“如果[存在危险的证据]，则[政策]”形式的政策提案获得了大部分安全效益，同时在安全界对未决问题的看法是错误的世​​界中，也优雅地失败了（即不施加过多的开发成本）。危险。理想情况下，这意味着此类政策更容易达成共识。 </p><p><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnnmr2zc5cm4r"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnmr2zc5cm4r">^</a></strong></sup></span><div class="footnote-content"><p>目前这对我来说似乎是正确的，即 LLaMA-2 比 20 个月前发布的 GPT-3.5 稍差一些。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlndvavl4r2a"> <span class="footnote-back-link"><sup><strong><a href="#fnreflndvavl4r2a">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://forum.effectivealtruism.org/posts/PTtZWBAKgrrnZj73n/biosecurity-culture-computer-security-culture"><u>杰夫·考夫曼（Jeff Kaufman）撰写了有关</u></a>计算机安全和生物安全社区之间规范差异的文章。简而言之，虽然计算机安全规范鼓励尝试破坏系统和披露漏洞，但生物安全规范却阻止公开讨论可能的漏洞。杰夫将其归因于许多结构性因素，包括修补生物安全漏洞的难度；来自开源人工智能的威胁模型可能与生物风险模型有更多共同点，在这种情况下，我们应该基于它们来构建我们的防御模型。想要了解更多 ctrl-f “冷汗” <a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/#crispr-based-gene-drive-022318"><u>，请</u></a>阅读 Kevin Esvelt 讨论的为什么他没有向任何人（甚至包括他的顾问）透露基因驱动的想法，直到他确定基因驱动是防御主导的。 （感谢 Max Nadeau 的这两个参考文献，以及我在其他地方链接的大多数生物相关材料的参考文献。）</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3ejitdom769"> <span class="footnote-back-link"><sup><strong><a href="#fnref3ejitdom769">^</a></strong></sup></span><div class="footnote-content"><p>我预计有些人会争论我们的密码学是否真的那么好，或者指出，如果你认为人工智能“我们只有一次机会”，那么这句话中的“相对”和“大部分”这两个词是令人担忧的。因此，让我先澄清一下，我不太关心该协议的确切成功程度；我主要用它作为一个说明性的类比。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1art7tmmdob"> <span class="footnote-back-link"><sup><strong><a href="#fnref1art7tmmdob">^</a></strong></sup></span><div class="footnote-content"><p>我们可以做出这样的假设，因为我们正在处理由开源人工智能引起的灾难的威胁模型。如果您认为杀死我们的第一件事是错位的专有人工智能系统，那么您应该关注该威胁模型而不是开源人工智能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7501lf6ibc3"> <span class="footnote-back-link"><sup><strong><a href="#fnref7501lf6ibc3">^</a></strong></sup></span><div class="footnote-content"><p>这是协议中我感到最紧张的部分；请参阅“一些不确定性”部分中的要点 2。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvqihg0rt649"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvqihg0rt649">^</a></strong></sup></span><div class="footnote-content"><p><a href="https://arxiv.org/ftp/arxiv/papers/2310/2310.18233.pdf"><u>此处显示</u></a>，根据病毒学数据进行微调的 LLaMA-2 对于向黑客马拉松参与者提供获取和释放重建的 1918 年流感病毒的指示非常有用。然而，目前尚不清楚这种危害是否新颖——我们不知道如果仅访问互联网，参与者会做得更糟糕。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhqvunm4kwo5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhqvunm4kwo5">^</a></strong></sup></span><div class="footnote-content"><p>我注意到人工智能安全问题很难在麻省理工学院获得关注，尤其是在麻省理工学院，我对正在发生的事情的一个猜测是，开源意识形态在麻省理工学院非常有影响力，而目前所有开源人士都讨厌开源人工智能安全人员。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WLYBy5Cus4oRFY3mu/thoughts-on-open-source-ai<guid ispermalink="false"> WLYBy5Cus4oRFY3mu</guid><dc:creator><![CDATA[Sam Marks]]></dc:creator><pubDate> Fri, 03 Nov 2023 15:35:42 GMT</pubDate> </item><item><title><![CDATA[Shouldn't we 'Just' Superimitate Low-Res Uploads?]]></title><description><![CDATA[Published on November 3, 2023 7:42 AM GMT<br/><br/><p>如果您有三个非常不同的优化器（一个是神经网络，另一个是手工构建的，最后一个是通过类似玻尔兹曼大脑的过程实现的），但具有相同的偏好和相同的优化能力，那么它们可能最终在足够长的时间范围内做类似的事情。</p><p>我提出这一点，是因为在讨论上传时，人们似乎倾向于获得整个思维的数字编码，就好像上传的有用部分包含它以类似人类的方式进行优化的事实，而它看起来像什么我们实际上想要的是能够以任何方式优化的东西，只要它针对我们想要的东西进行优化，无论其复杂性如何。</p><p>在<a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#Training_frontier_models_to_predict_neural_activity_instead_of_next_token">Davidad 的上传登月计划有效吗？</a> ， <a href="https://www.lesswrong.com/users/jacobjacob?mention=user">@jacobjacob</a>提出以下内容：</p><blockquote><p> “仅仅”训练一个巨型变压器来预测神经活动，而不是预测自然语言中的下一个标记，这个想法怎么样？</p></blockquote><p> <a href="https://www.lesswrong.com/users/lisathiergart?mention=user">@lisathiergart</a>回复说，这可能违反了产品将比现状更加一致的假设。我提出以下反对意见：</p><ul><li>您从本次培训中需要的产品并不像整个大脑的近似那么复杂，“只是”它的偏好。如果您可以从 Transformer 中提取奖励模型（您甚至可以在“上传”时使用 SOTA AI 对其进行设计，使其变得更好）并将优化器指向该方向，您就可以实现类似的端点。</li><li>在 MEG 数据上训练大型神经网络似乎相当不错。 Meta 在<a href="https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/">解码大脑活动图像</a>方面取得了巨大成功。<a href="https://arxiv.org/abs/2208.12266">言语也是如此</a>。这对我来说表明当前的神经网络足够强大，可以从脑电图数据中提取有意义的模式（这篇演讲论文甚至在脑电图方面取得了一些成功）。</li><li>当深度学习规模足够大时，可以做很多令人惊讶的事情。如果你问大多数人对于训练巨型变压器来预测下一个 token 有何想法，他们可能不会预测到 2023 年会出现 GPT-4。也许这同样适用于预测下一个 MEG 读数？<ul><li>这似乎很容易测试。我思考了 30 分钟，并提出了一些基于 Mistral-7B 的架构，我觉得我可以在短时间内合理地实现它们。然而，我没有任何 MEG 硬件，而且获取起来很昂贵。根据我的研究，假设龙猫缩放定律在这种情况下大致成立，似乎没有足够的公开数据来训练 Mistral-7B 大小（甚至更小）的东西。</li></ul></li></ul><hr><p> <a href="https://www.lesswrong.com/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023#Physicalist_Superimitation">凡妮莎·科索伊 (Vanessa Kosoy) 将物理主义的超级模仿视为学习理论议程的一个潜在终点</a>。她对超级模仿的描述如下：</p><blockquote><p>一个代理（以下称为“模仿者”）接收另一个代理（以下称为“原始”）的策略，并产生追求相同目标但<i>明显</i><i>更好的</i>行为。</p></blockquote><p>如果您可以模仿或以其他方式优化上传的偏好，也许简单的方法（例如预测下一个 MEG 读数）就足够了，或者至少与训练完整上传并利用它相当，尽管要容易得多？</p><br/><br/> <a href="https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/KGTGgnGppf9wzwmFM/shouldn-t-we-just-superimitate-low-res-uploads<guid ispermalink="false"> KGTGgnGppf9wzwmFM</guid><dc:creator><![CDATA[marc/er]]></dc:creator><pubDate> Fri, 03 Nov 2023 07:42:07 GMT</pubDate> </item><item><title><![CDATA[The other side of the tidal wave]]></title><description><![CDATA[Published on November 3, 2023 5:40 AM GMT<br/><br/><p>我猜人工智能在未来几十年内可能有 10-20% 的可能性导致人类灭绝，但我对此感到比所暗示的更令人痛苦 - 我认为因为在它不会导致人类灭绝的情况下，我发现它很难想象生活不会偏离轨道。这个世界上我喜欢的很多事情似乎都可能被超人人工智能所终结或严重扰乱（写作、向人们解释事情、对彼此有用的友谊、对技能感到自豪、思考、学习、弄清楚）如何实现事物、制造事物、轻松跟踪有意识和无意识的事物），而且我不相信替代品实际上是好的，或者对我们有好处，或者任何事情都是可逆的。</p><p>即使我们没有死，仍然感觉一切都即将结束。</p><br/><br/> <a href="https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/uyPo8pfEtBffyPdxf/the-other-side-of-the-tidal-wave<guid ispermalink="false"> uyPo8pfEtBffyPdxf</guid><dc:creator><![CDATA[KatjaGrace]]></dc:creator><pubDate> Fri, 03 Nov 2023 05:40:06 GMT</pubDate> </item><item><title><![CDATA[Does davidad's uploading moonshot work?]]></title><description><![CDATA[Published on November 3, 2023 2:21 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/users/davidad">davidad</a>就一项提案进行了 10 分钟的演讲，他说：“我第一次看到一个具体计划，可以在 2040 年之前实现人类上传，如果资金无限，甚至可能更快”。</p><p>我认为这个演讲很值得一看，但即使你没有看过，下面的对话也很容易读懂。我还在本对话的附录中添加了一些演讲摘要。 </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=jZqynCV0AGc&amp;list=PLH78wfbGI1x2CI6aV_hiOE1_GOFkZAFph&amp;index=7"><div><iframe src="https://www.youtube.com/embed/jZqynCV0AGc" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p>我认为这次演讲的承诺如下。看起来，为了让未来顺利发展，我们必须要么放慢通用人工智能的进展，要么让对齐的进展有差异地更快。然而，上传似乎提供了第三种方式：我们“简单地”<i>运行得更快</i>，而不是让比对研究人员更有效率。这看起来类似于<a href="https://openai.com/blog/introducing-superalignment"><u>OpenAI 的 Superalignment</u></a>提案，即建立一个自动对齐科学家——但关键的例外是，我们可能会合理地认为，人类上传的内容会比法学硕士等有更好的对齐保证。</p><p>我决定就这个提议组织一次对话，因为它给我的印象是“如果是真的的话，那就是巨大的”，但当我与一些人讨论这个提议时，我发现人们普遍没有意识到这一点，有时还会提出问题我和谈话都无法回答的/困惑/反对意见。</p><p>我还邀请了 Anders Sandberg，他<a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf">与 Nick Bostrom 共同撰写了 2008 年全脑模拟路线图</a>，并共同组织了<a href="https://foresight.org/whole-brain-emulation-workshop-2023/">有关该主题的 Foresight 研讨会，Davidad 在会上介绍了他的计划</a>，还邀请了来自 MIRI 的 Lisa Thiergart，她也在研讨会上发表了<a href="https://www.youtube.com/watch?v=gf7W82mrRfI">演讲</a><a href="https://www.lesswrong.com/posts/KQSpRoQBz7f6FcXt3/distillation-of-neurotech-and-alignment-workshop-january-1">之前曾在 LessWrong 上撰写过有关全脑模拟的文章</a>。</p><p>对话最终同时涵盖了很多话题。我将它们分成六个独立的部分，这些部分相当独立可读。</p><h2>条形码作为拦截器（和同步加速器） </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:31:34 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:31:34 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>一件重要的事情是“如果人们真的尝试过，这个上传计划可能会失败的原因是什么。”我认为主要的阻碍因素是<i>对跨膜蛋白进行条形码编码</i>。让我首先阐述为什么我们“不需要”大脑中的大部分物质，例如DNA甲基化、基因调控网络、微管等的标准论点。</p><ol><li>认知反应时间很快。</li><li>这意味着单个神经元的响应时间需要更快。</li><li>化学扩散速度太快，无法传播很远——只能跨越微小的突触间隙距离。</li><li>所以所有非突触的东西都必须是电的。</li><li>电以电位差（电压）的形式发挥作用，而组织中唯一可以产生电压差的部分是跨膜的。</li><li>因此，所有的认知信息处理都发生在膜和突触的过程上。</li></ol><p>现在，人们经常在这里做出非常无根据的飞跃，也就是说，我们需要知道的只是膜和突触的<i>几何形状和连接图</i>。但膜和突触并不是由同质的膜和突触构成的。膜主要由磷脂双层组成，但许多重要的工作是由跨膜蛋白完成的：正是这些分子将传入的神经递质转化为电信号，将电信号调节为动作电位，导致动作电位衰减，并且释放轴突末端的神经递质。而且人类神经系统中有很多很多不同的跨膜蛋白。它们中的大多数可能并不重要，或者足够相似，可以将它们混为一谈，但我敢打赌，至少有几十种非常不同的跨膜蛋白（抑制性与兴奋性、不同的时间常数、对不同离子的不同敏感性）从而导致不同的信息处理行为。因此，我们不仅需要看到神经元和突触，还需要看到整个神经元细胞膜（包括但不限于突触处）的许多不同种类的跨膜蛋白中每种跨膜蛋白的定量密度。</p><p> Sebastian Seung 在他关于连接组学的书中提出了一个著名的假设，即人脑中只有几百种不同的细胞类型，它们在每个突触上具有大致相同的行为，并且我们可以仅从几何形状中找出每个细胞的类型。但我认为这不太可能，因为学习是通过突触可塑性进行的：也就是说，学到的很多东西，至少在记忆中（而不是技能）是由突触的相对受体密度来表示的。然而，<i>可能</i>我们很幸运，纯粹是突触的<i>大小</i>就足够了。在这种情况下，我们实际上非常幸运，因为同步加速器解决方案更加可行，而且比膨胀显微镜快得多。使用扩展显微镜，通过对荧光团进行条形码标记来标记大量受体<i>似乎</i>是合理的，就像人们现在开始用不同的荧光团组合对<i>细胞</i>进行条形码标记一样（这解决了一个完全不同的问题，即轴突追踪的冗余）。然而，<strong>基于放大显微镜的计划最有可能失败的原因是我们无法使用荧光来标记足够的跨膜蛋白</strong>。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>有什么选择可以克服这个问题，它们有多容易处理？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:19:32 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:19:32 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>对于条形码，到目前为止我只想到了几种不同的技术途径。在某种程度上，它们可以结合起来。</p><ol><li> &quot;serial&quot; barcoding, in which we find a protocol for washing out a set of tags in an already-expanded sample, and then diffusing a new set of tags, and being able to repeat this over and over without the sample degrading too much from all the washing.</li><li> &quot;parallel&quot; barcoding, in which we conjugate several fluorophores together to make a &quot;multicolored&quot; fluorophore (literally like a barcode in the visible-light spectrum). This is the basic idea that is used for barcoding cells, but the chemistry is very different because in cells you can just have different concentrations of separate fluorophore molecules floating around, whereas receptors are way too small for that and you need to have one of each type of fluorophore all kind of stuck together as one molecule. Chemistry is my weakness, so I&#39;m not very sure how plausible this is, but from a first-principles perspective it seems like it might work. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:35:19 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:35:19 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><blockquote><p>the synchrotron solution is far more viable</p></blockquote><p> On synchotrons / particle accelerator x-rays: <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan Collins estimates</a> they would take ~1 yr of sequential effort for a whole human brain (which thus also means you could do something like a mouse brain in a few hours, or an organoid in minutes, for prototyping purposes). But I&#39;m confused why you suggest that as an option that&#39;s <i>differentially</i> compatible with only needing lower resolution synaptic info like size.</p><p> Could you not do expansion microscopy + synchrotron? And if you need the barcoding to get what you want, wouldn&#39;t you need it with or without synchrotron? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:37:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:37:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>So, synchrotron imaging uses X-rays, whereas expansion microscopy typically uses ~visible light fluorescence (or a bit into IR or UV). (It is possible to do expansion and then use a synchrotron, and the best current synchrotron pathway does do that, but the speed advantages are due to X-rays being able to penetrate deeply and facilitate tomography.) There are a lot of different indicator molecules that resonate at different wavelengths of ~visible light, but not a lot of different indicator atoms that resonate at different wavelengths of X-rays. And the synchrotron is monochromatic anyway, and its contrast is by transmission rather than stimulated emission. So for all those reasons, with a synchrotron, it&#39;s really pushing the limits to get even a small number of distinct tags for different targets, let alone spectral barcoding. That&#39;s the main tradeoff with synchrotron&#39;s incredible potential speed. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> One nice thing with working in visible light rather than synchrotron radiation is that the energies are lower, and hence there is less disruption of the molecules and structure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Andreas Schaefer seems to be making great progress with this, see eg <a href="https://www.nature.com/articles/s41467-022-30199-6#Sec7">here</a> . I have updated downward in conversations with him that sample destruction will be a blocker. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also, there are many modalities that have been developed in visible light wavelengths that are well understood. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:11:17 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:11:17 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>This is a bigger concern for me, especially in regards to spectral barcodes.</p></div></section><h2> End-to-end iteration as blocker (and organoids, holistic processes, and exotica) </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:36:52 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:36:52 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> I think the barcoding is a likely blocker. But I think the most plausible blocker is a more diffuse problem: we do not manage to close the loop between actual, running biology, and the scanning/simulation modalities so that we can do experiments, adjust simulations to fit data, and then go back and do further experiments - including developing new scanning modalities. My worry here is that while we have smart people who are great at solving well-defined problems, the problem of setting up a research pipeline that is good at iterating at generating well-defined problems might be less well-defined... and we do not have a brilliant track record of solving such vague problems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:45:33 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:45:33 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Yeah, this is also something that I&#39;m concerned that people who take on this project might fail to do by default, but I think we do have the tech to do it now: human brain organoids. We can manufacture organoids that have genetically-human neurons, at a small enough size where the entire thing can be imaged <i>dynamically</i> (ie non-dead, while the neurons are firing, actually collecting the traces of voltage and/or calcium), and then we can slice and dice the organoid and image it with whatever static scanning modality, and see if we can reproduce the actual activity patterns based on data from the static scan. This could become a quite high-throughput pipeline for testing many aspects of the plan (system identification, computer vision, scanning microscopes, sample prep/barcoding, etc.). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:46:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:46:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>What is the state of the art of &quot;uploading an organoid&quot;?<br><br> (I guess one issue is that we don&#39;t have any &quot;validation test&quot;: we just have neural patterns, but the organoid as a whole isn&#39;t really doing any function. Whereas, for example, if we tried uploading a worm we could test whether it remembers foraging patterns learnt pre-upload) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:51:31 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:51:31 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>I&#39;m not sure if I would know about it, but I just did a quick search and found things like &quot; <a href="https://www.nature.com/articles/s41467-022-32115-4">Functional neuronal circuitry and oscillatory dynamics in human brain organoids</a> &quot; and &quot; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0142961222004653">Stretchable mesh microelectronics for biointegration and stimulation of human neural organoids</a> &quot;, but nobody is really trying to &quot;upload an organoid&quot;. They are already viewing the organoids as more like &quot;emulations&quot; on which one can do experiments in place of human brains; it is an unusual perspective to treat an organoid as more like an organism which one might try to emulate. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:58:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:58:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> On the general theme of organoids &amp; shortcuts:<br><br> I&#39;m thinking about what are key blockers in dynamic scanning as well later static slicing &amp; imaging. I&#39;m probably not fully up to date on the state of the art of organoids &amp; genetic engineering on them. However, if we are working on the level of organoids, couldn&#39;t we plausibly directly genetically engineer (or fabrication engineer) them to make our life easier?</p><ul><li> ex. making the organoid skin transparent (visually or electrically)</li><li> ex. directly have the immunohistochemistry applied as the organoid grows / is produced</li><li> ex. genetically engineer them such that the synaptic components we are most interested in are fluorescent</li></ul><p> ...probably there are further such hacks that might speed up the &quot;Physically building stuff and running experiments&quot; rate-limit on progress</p><p> (ofc getting these techniques to work will also take time, but at appropriate scales it might be worth it) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:05:27 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:05:27 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>We definitely can engineer organoids to make experiments easier. They can simply be thin enough to be optically transparent, and of course it&#39;s easier to grow them <i>without</i> a skull, which is a plus.</p><p> In principle, we could also make the static scanning easier, but I&#39;m a little suspicious of that, because in some sense the purpose of organoids in this project would be to serve as &quot;test vectors&quot; for the static scanning procedures that we would want to apply to non-genetically-modified human brains. Maybe it would help to get things off the ground to help along the static scanning with some genetic modification, but it might be more trouble than it&#39;s worth. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:05:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:05:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Lisa, here&#39;s a handwavy question. Do you have an estimate, or the components of an estimate, for &quot;how quickly could we get a &#39;Chris Olah-style organoid&#39; -- that is, the <i>smallest</i> organoid that 1) we fully understood, in the relevant sense and also 2) told us at least something interesting on the way to whole brain emulation?&quot;</p><p> (Also, I don&#39;t mean that it would be an organoid <i>of</i> poor Chris! Rather, my impression is that his approach to interpretability, is &quot;start with the smallest and simplest possible toy system that you do <i>not</i> understand, then understand that really well, and then increase complexity&quot;. This would be adapting that same methodology) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:36:21 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:36:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Wow that&#39;s a fun one, and I don&#39;t quite know where to start. I&#39;ll briefly read up on organoids a bit. [...reading time...] Okay so after a bit of reading, I don&#39;t think I can give a great estimate but here are some thoughts:<br><br> First off, I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole&#39; works well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot). Similarly, I&#39;m not sure how much starting with a simpler example (say a different animal with a smaller brain) will reliably transfer, but at least some of it might. For developing the needed technologies it definitely seems helpful.<br><br> That said, starting small is always easier. If I were trying to come up with components for an estimate I&#39;d consider:</p><ul><li> looking at ​​non-human organoids</li><li> comparisons to how long similar efforts have been taking: C. elegans is a simpler worm organism some groups have worked on trying to whole brain upload, which seems to be taking longer than experts predicted (as far as I know the efforts have been ongoing for >;10 years and not yet concluded, but there has been some progress). Though, I think this is affected for sure by there not being very high investment in these projects and few people working on it.</li><li> trying to define what &#39;fully understood&#39; means: perhaps being able to get within x% error on electrical activity prediction and/or behavioural predictions (if the organism exhibits behaviors)</li></ul><p> There&#39;s definitely tons more to consider here, but I don&#39;t think it makes sense for me to try to generate it.​ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:42:31 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:42:31 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><blockquote><p> I&#39;m not sure how much the &#39;start with smallest part and then work up to the whole works&#39; well for brains. Possibly, but I think there might be critical whole brain processes we are missing (that change the picture a lot).</p></blockquote><p> One simple example of a &quot;holistic&quot; property that might prove troublesome is oscillatory behavior, where you need a sufficient number of units linked in the right way to get the right kind of oscillation. The fun part is that you get oscillations almost automatically from any neural system with feedback, so distinguishing merely natural frequency oscillations (eg the gamma rhythm seems to be due to fast inhibitory interneurons if I remember right) and the functionally important oscillations (if any!) will be tricky. Merely seeing oscillations is not enough, we need some behavioral measures.</p><p> There is likely a dynamical balance between microstructure-understanding based &quot;IKEA manual building&quot; of the system and the macrostructure-understanding &quot;carpentry&quot; approach. Setting the overall project pipeline in motion requires having a good adaptivity on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:39:25 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:39:25 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Wouldn&#39;t it be possible to bound the potentially relevant &quot;whole brain processes&quot;, by appealing to a version of Davidad&#39;s argument above that we can neglect a lot of modalities and stuff, because they operate at a slower timescale than human cognition (as verified for example by simple introspection)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:47:47 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:47:47 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> I think glia are also involved in <a href="https://www.sciencedirect.com/science/article/pii/S0165017309001076?casa_token=V_DcC59YI7MAAAAA:XzfbMUr_R56pDQNA57HLWI6NFMxBzeN2ATF89tw2yGe7uchy5dZFLpHJgBurGAR9Drc0iYzVvQ">modulating synaptic transmissions</a> as well as the electric conductivity of a neuron (definitely the speed of conductance), so I&#39;m not sure the speed of cognition argument necessarily disqualifies them and other non-synaptic components as relevant to an accurate model.  This <a href="https://www.frontiersin.org/articles/10.3389/fncel.2016.00188/full">paper</a> presents the case of glia affecting synaptic plasticity and being electrically active.  Though I do think that argument seems to be valid for many components, which clearly cannot effect the electrical processes at the needed time scales.<br><br> With regards to &quot;whole brain processes&quot; what I&#39;m gesturing at is there might be top level control or other processes running without whose inputs the subareas&#39; function cannot be accurately observed. We&#39;d need to have an alternative way of inputting the right things into the slice or subarea sample to generate the type of activity that actually occurs in the brain. Though, it seems we can focus on the electrical components of such a top level process in which case I wouldn&#39;t see an obvious conflict between the two arguments. I might even expect the electrical argument to hold more, because of the need to travel quickly between different (far apart) brain areas. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:53:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:53:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Yeah, sorry, I came across as endorsing a modification of the standard argument that rules out too many aspects of brain processes as cognition-relevant, where I only rule back in <i>neural</i> membrane proteins. I&#39;m quite <i>confident</i> that neural membrane proteins will be a blocker (~70%) whereas I am less confident about glia (~30%) but it&#39;s still very plausible that we need to learn something about glial dynamics in order to get a functioning human mind. However, whatever computation the glia are doing is also probably based on their own membrane proteins! And the glia are included in the volume that we have to scan anyway. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 16:54:42 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 16:54:42 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Yeah, makes sense. I also am thinking that maybe there&#39;s a way to abstract out the glia by considering their inputs as represented in the membrane proteins. However, I&#39;m not sure whether it&#39;s cheaper/faster to represent the glia vs. needing to be very accurate with the membrane proteins (as well as how much of a stretch it is to assume they can be fully represented there). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:58:30 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:58:30 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> My take on glia is that they may represent a sizeable pool of nonlinear computation, but it seems to run slowly compared to the active spikes of neurons. That may require lots of memory, but less compute. But real headache is that there is relatively little research on glia (despite their very loyal loyalists!) compared to those glamorous neurons. Maybe they do represent a good early target for trying to define a research subgoal of characterizing them as completely as possible. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:27:37 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:27:37 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Overall, the &quot;exotica&quot; issue is always annoying - there could be an infinite number of weird modalities we are missing, or strange interactions requiring a clever insight. However, I think there is a pincer maneouver here where we try to constrain it by generating simulations from known neurophysiology (and nearby variations), and perhaps by designing experiments to estimate degrees of unknown degrees of freedom (much harder, but valuable). As far as I know the latter approach is still not very common, my instant association is to how statistical mechanics can estimate degrees of freedom sensitively from macroscopic properties (leading, for example, to primordial nucleogenesis to constrain elementary particle physics to a surprising degree). This is where I think a separate workshop/brainstorm/research minipipeline may be valuable for strategizing.</p></div></section><h2> Using AI to speed up uploading research </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>As AI develops over the coming years, it will speed up some kinds of research and engineering. I&#39;m wondering: for this proposal, which parts could future AI accelerate? And perhaps more interestingly: which parts would <i>not</i> be easily accelerable (implying that work on those parts sooner is more important)? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:11:55 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:11:55 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>The frame I would use is to sort things more like along a timeline of when AI might be able to accelerate them, rather than a binary easy/hard distinction. (Ultimately, a friendly superintelligence would accelerate the entire thing—which is what many folks believe friendly superintelligence ought to be used for in the first place!)</p><ol><li> The easiest parts to accelerate are the computer vision. In terms of timelines, this is in the rear-view mirror, with deep learning starting to substantially accelerate the processing of raw images into connectomic data <a href="https://arxiv.org/pdf/1706.00120.pdf">in 2017</a> .</li><li> The next easiest is the modelling of the dynamics of a nervous system based on connectomic data. This is a mathematical modelling challenge, which needs a bit more structure than deep learning traditionally offers, but it&#39;s not that far off (eg with multimodal hypergraph transformers).</li><li> The next easiest is probably more ambitious microscopy along the lines of &quot;computational photography&quot; to extract more data with fewer electrons or photons by directing the beams and lenses according to some approximation of <a href="https://arxiv.org/pdf/1103.5708.pdf">optimal Bayesian experimental design</a> . This has the effect of accelerating things by making the imaging go faster or with less hardware.</li><li> The next easiest is the engineering of the microscopes and related systems (like automated sample preparation and slicing). These are electro-optical-mechanical engineering problems, so will be harder to automate than the more domain-restricted problems above.</li><li> The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i> of the microscopes, and the management of failures and repairs and replacements of parts, etc. Of course this is still possible to automate, but it requires capabilities that are quite close to the kind of superintelligence that can maintain a robot fleet without human intervention. </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:13:37 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:13:37 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><blockquote><p>The hardest to automate is the planning and cost-optimization of the <i>manufacturing</i></p></blockquote><p> An interesting related question is &quot;if you had an artificial superintelligence suddenly appear today, what would be its &#39;manufacturing overhang&#39;? How long it would it take it to build the prerequisite capacity, starting with current tools?&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:55:36 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:55:36 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Could AI assistants help us build a research pipeline? I think so. Here is a sketch: I run a bio experiment, I scan the bio system, I simulate it, I get data that disagrees.哪里有问题？ Currently I would spend a lot of effort trying to check my simulator for mistakes or errors, then move on to check whether the scan data was bad, then if the bio data was bad, and then if nothing else works, positing that maybe I am missing a modality. Now, with good AI assistants I might be able to (1) speed up each of these steps, including using multiple assistants running elaborate test suites. (2) automate and parallelize the checking. But the final issue remains tricky: what am I missing, if I am missing something? This is where we have human level (or beyond) intelligence questions, requiring rather deep understanding of the field and what is plausible in the world, as well as high level decisions on how to pursue research to test what new modalities are needed and how to scan for them. Again, good agents will make this easier, but it is still tricky work.</p><p> What I suspect could happen for this to fail is that we run into endless parameter-fiddling, optimization that might hide bad models by really good fits of floppy biological systems, and no clear direction for what to research to resolve the question. I worry that it is a fairly natural failure mode, especially if the basic project produces enormous amount of data that can be interpreted very differently. Statistically speaking, we want identifiability. But not just of the fit to our models, but to our explanations. And this is where non-AGI agents have not yet demonstrated utility. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:48:00 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:48:00 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Yeah, so, overall, it seems &quot;physically building stuff&quot;, &quot;running experiments&quot; and &quot;figure out what you&#39;re missing&quot; are some of where the main rate-limiters lie. These are the hardest-to-automate steps of the iteration loop, that prevent an end-to-end AI assistant helping us through the pipeline.</p><p> Firstly though, it seems important to me how frequently you run into the rate limiter. If &quot;figuring out what you&#39;re missing&quot; is something you do a few times a week, you could still be sped up a lot by automating the other parts of the pipeline until you can run this problem a few times per day.</p><p> But secondly I&#39;m interested in answers digging one layer down of concreteness -- which part of the building stuff is hard, and which part of the experiment running is hard? For example: Anders had the idea of &quot; <a href="https://www.youtube.com/watch?v=ItEnEN58bJw">the neural pretty printer</a> &quot;: create ground truth artificial neural networks performing a known computation >;>; convert them into a Hodgkin-Huxley model (or similar) >;>; fold that up into a 3D connectome model >;>; simulate the process of scanning data from that model -- and then attempt to reverse engineer the whole thing. This would basically be a simulation pipeline for validating scanning set-ups.</p><p> To the extent that such simulation is possible, <i>those</i> particular experiments would probably not be the rate-limiting ones. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:21:22 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:21:22 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The neural pretty printer is an example of building a test pipeline: we take a known simulatable neural system, convert it into a plausible biological garb and make fake scans of it according to the modalities we have, and then try to get our interpretation methods reconstruct it. This is great, but eventually limited. The real research pipeline will have to contain (and generate) such mini-pipelines to ensure testability. There is likely an organoid counterpart. Both have a problem of making systems to test the scanning based on somewhat incomplete (or really incomplete!) data.</p></div></section><h2> Training frontier models to predict neural activity instead of next token </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 15:20:36 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 15:20:36 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Lisa, you said in our opening question brainstorm:</p><blockquote><p> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.<br><br> 1a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from the dynamic data collected [(with the aim of figuring out the statics-to-dynamics map)] then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.<br><br> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)</p></blockquote><p> Somewhat tangentially, this makes me wonder about taking this idea to its limit: what about the idea of &quot;just&quot; training a giant transformer to, instead of predicting next tokens in natural language, predicting neural activity? (at whatever level of abstractions is most suitable.) &quot;Imitative neural learning&quot;. I wonder if that would be within reach of the size of models people are gearing up to train, and whether it would better preserve alignment guarantees. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:38:20 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:38:20 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Hmm, intuitively this seems not good. On the surface level, two reasons come to mind:<br><br> 1. I suspect the element of &quot;more reliably human-aligned&quot; breaks or at least we have less strong reasons to believe this would be the case than if the total system also operates on the same hardware structure (so to say, it&#39;s not actually going to be run on something carbon-based). Though I&#39;m also seeing the remaining issue of: &quot;if it looks like a duck (structure) and quacks like a duck (behavior), does that mean it&#39;s a duck?&quot;.  At least we have the benefit of deep structural insight as well as many living humans as a prediction on how aligned-ness of these systems turns out in practice. (That argument holds in proportion to how faithful of an emulation we achieve.)<br><br> 2. I would be really deeply interested in whether this would work. It is a bit reminiscent of <a href="https://manifund.org/projects/activation-vector-steering-with-bci">the Manifund proposal</a> Davidad and I have open currently, where the idea is to see if human brain data can improve performance on next token prediction and make it more human preference aligned.  At the same time, for the &#39;imitative neural learning&#39; you suggest (btw I suspect it would be feasible with GPT4/5 levels, but I see the bottleneck/blocker in being able to get enough high quality dynamic brain data) I think I&#39;d be pretty worried that this would turn into some dangerous system (which is powerful, but not necessarily aligned).</p><p> 2/a Side thought: I wonder how much such a system would in fact be constrained to human thought processes (or whether it would gradient descend into something that looks input and output similar, but in fact is something different, and behaves unpredictably in unseen situations). Classic Deception argument I guess (though in this case without an implication of some kind of intentionality on the system&#39;s side, just that it so happens bc of the training process and data it had) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 15:40:50 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 15:40:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>I basically agree with Lisa&#39;s previous points. Training a transformer to imitate neural activity is a little bit better than training it to imitate words, because one gets more signal about the &quot;generators&quot; of the underlying information-processing, but misgeneralizing out-of-distribution is still a big possibility. There&#39;s something qualitatively different that happens if you can collect data that is logically upstream of the entire physical information processing conducted by the nervous system — you can then make predictions about that information-processing <strong>deductively</strong><i><strong> </strong></i>rather than <strong>inductively</strong> (in the sense of the problem of induction), so that whatever inductive biases (aka priors) are present in the learning-enabled components end up having no impact. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 15:46:42 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 15:46:42 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> &quot;Human alignment&quot;: one of the nice things with human minds is that we understand roughly how they work (or at least the signs that something is seriously wrong). Even when they are not doing what they are supposed to do, the failure modes are usually human, all too human. The crux here is whether we should expect to get human-like systems, or &quot;humanish&quot; systems that look and behave similar but actually work differently. The structural constraints from Whole Brain Emulation are a good reason to think more of the former, but I suspect many still worry about the latter because maybe there are fundamental differences in simulation from reality. I think this will resolve itself fairly straightforwardly since - by assumption if this project gets anywhere close to succeeding - we can do a fair bit of experimentation on whether the causal reasons for various responses look like normal causal reasons in the bio system. My guess is that here Dennett&#39;s principle that the simplest way of faking many X is to be X. But I also suspect a few more years of practice with telling when LLMs are faking rather than grokking knowledge and cognitive steps will be very useful and perhaps essential for developing the right kind of test suite.</p></div></section><h2> How to avoid having to spend $100B and and build 100,000 light-sheet microscopes </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:13:48 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:13:48 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>“15 years and ~$500B” ain&#39;t an easy sell. If we wanted this to be doable faster (say, &lt;5 years), or cheaper (say, &lt;$10B): what would have to be true? What problems would need solving? Before we finish, I am quite keen to poke around the solution landscape here, and making associated fermis and tweaks. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:16:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:16:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>So, definitely the most likely way that things could go faster is that it turns out all the receptor densities are predictable from morphology (eg synaptic size and &quot;cell type&quot; as determined by cell shape and location within a brain atlas). Then we can go ahead with synchrotron (starting with organoids!) and try to develop a pipeline that infers the dynamical system from that structural data. And synchrotron is much faster than expansion. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:17:34 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:17:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><blockquote><p>it turns out all the receptor densities are predictable from morphology</p></blockquote><p> What&#39;s the fastest way you can see to validating or falsifying this? Do you have any concrete experiments in mind that you&#39;d wish to see run if you had a magic wand? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:22:05 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:22:05 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Unfortunately, I think we need to actually see the receptor densities in order to test this proposition. So transmembrane protein barcoding still seems to me to be on the critical path in terms of the tech tree. <strong>But</strong> if this proposition turns out to be true, then you won&#39;t need to use slow expansion microscopy when you&#39;re actually ready to scan an entire human brain—you only need to use expansion microscopy on some samples from every brain area, in order to learn a kind of &quot;Rosetta stone&quot; from morphology to transmembrane protein(/receptor) densities for each cell type. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:23:58 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:23:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>So, for the benefit of future readers (and myself!) I kind of would like to see you multiply 5 numbers together to get the output $100B (or whatever your best cost-esimate is), and then do the same thing in the fortunate synchrotron world, to get a fermi of how much things would cost in that<i> </i>world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:30:24 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:30:24 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Ok. I actually haven&#39;t done this myself before. Here goes.</p><ol><li> The human central nervous system has a volume of about 1400 cm^3.</li><li> When we expand it for expansion microscopy, it expands by a factor of 11 in each dimension, so that&#39;s about 1.9 m^3. (Of course, we would slice and dice before expanding...)</li><li> Light-sheet microscopes can cover a volume of about 10^4 micron^3 per second, which is about 10^-14 m^3 per second.</li><li> That means we need about 1.9e14 microscope-seconds of imaging.</li><li> If the deadline is 10 years, that&#39;s about 3e8 seconds, so we need to build about 6e5 microscopes.</li><li> Each microscope costs about $200k, so 6e5 * $200k is $120B. (That&#39;s not counting all the R&amp;D and operations surrounding the project, but building the vast array of microscopes is the expensive part.)</li></ol><p> Pleased by how close that came out to the number I&#39;d apparently been citing before. (Credit is due to Rob McIntyre and Michael Andregg for that number, I think.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:34:35 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:34:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Similarly, looking at <a href="https://www.youtube.com/watch?v=6v4C7ZvoUmI">Logan&#39;s slide</a> : <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rukiwrds8jln8vohsdvq" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/fkcy0prpnp9w6qjpiiqn 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/aviijtkzbyulztz2wqis 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pibu1fgqe56lwn0gvxep 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/cs3agbeodgrdlyqhljcz 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/nqfxbmm14nc5lp1x8jfd 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/iupqwcowydhnwayhyipg 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/ypjrtn88oiiudmvfemqq 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/xgbk2coub2wfku0nbzuu 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jbk9mmkpafzjwu2zpkfr 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/hjmtwofbbnqvesrbeuhh 2610w"></p><p> $1M for instruments, and needing 600k years. So, make 100k microscopes and run them in parallel for 6 years and then you get $100B... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:35:38 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:35:38 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>That system is a bit faster than ExM, but it&#39;s transmission electron microscopy, so you get roughly the same kind of data as synchrotron anyway (higher resolution, but probably no barcoded receptors) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:41:54 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:41:54 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Now for the synchrotron cost estimate.</p><ol><li> Synchrotron imaging has about 300nm voxel size, so to get accurate synapse sizes we would still need to do expansion to 1.9 m^3 of volume.</li><li> Synchrotron imaging has a speed of about 600 s/mm^3, but it seems uncontroversial that this may be improved by an order of magnitude with further R&amp;D investment.</li><li> That multiplies to about 3000 synchrotron-years.</li><li> To finish in 10 years, we would need to build 300 synchrotron beamlines.</li><li> Each synchrotron beamline costs about $10M.</li><li> That&#39;s $3B in imaging infrastructure. A bargain! </li></ol></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:43:09 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:43:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>That&#39;s very different from this estimate.想法？ </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/jsl0moz7gppg42my41gd" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/tqubpcpofsbslowecsbz 170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/j9y0zplo8tcfomk2eqcz 340w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/e3xhqypr9xocrzwo1i8n 510w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/wpiyujikqiscbidicvfy 680w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/gpfs5v4ohmrz5xjsxhaw 850w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/l7peqlu22d7otroqf9d9 1020w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/rftaksliw9honhyj9cvw 1190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/x4ybkurznlhqfavkvkx5 1360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/vvb2t0pjlezivooycucb 1530w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/je0nhzwyykyyi3f2wchd 1660w"></figure></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:46:16 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:46:16 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>One difference off the bat - 75nm voxel size is maybe enough to get a rough connectome, but not enough to get precise estimates of synapse size. I think we&#39;d need to go for 11x expansion. So that&#39;s about 1 order of magnitude, but there&#39;s still 2.5 more to account for. My guess is that this estimate is optimistic about combining multiple potential avenues to improve synchrotron performance. I do see some claims in the literature that more like 100x improvement over the current state of the art seems feasible. </p></div></section><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:33:34 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:33:34 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> What truly costs money in projects? Generally, it is salaries and running costs times time, plus the instrument/facilities costs as a one-time cost. One key assumption in this moonshot is that there is a lot of scalability so that once the basic setup has been made it can be replicated ever cheaper (Wrightean learning, or just plain economies of scale). The faster the project runs, the lower the first factor, but the uncertainty about whether all relevant modalities have been covered will be greater. There might be a rational balance point between rushing in and likely having to redo a lot, and being slow and careful but hence getting a lot of running costs. However, from the start the difficulty is very uncertain, making the actual strategy (and hence cost) plausibly a mixture model. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:35:57 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:35:57 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> The path to the 600,000 microscopes is of course to start with 6, doing the testing and system integration while generating the first data for the initial mini-pipelines for small test systems. As that proves itself one can scale up to 60 microscopes for bigger test systems and smaller brains. And then 600, 6,000 and so on. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:44:40 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:44:40 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Hundreds of thousands of microscopes seem to me like an... issue. I&#39;m curious if you have something like a &quot;shopping list&quot; of advances or speculative technologies that could bring that number down a lot. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 16:55:55 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 16:55:55 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Yes, that is quite the lab floor. Even one per square meter makes a ~780x780 meter space. Very Manhattan Project vibe. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:47:01 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:47:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Google tells me Tesla Gigafactory Nevada is about 500k m^2, so about the same :P </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Note though that economies of scale and learning curves can make this more economical than it seems. If we assume an experience curve with price per unit going down to 80% each doubling, 600k is 19 doublings, making the units in the last doubling cost just 1.4% of the initial unit cost. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 16:48:13 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 16:48:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>And you don&#39;t need to put all the microscopes in one site. If this were ever to actually happen, presumably it would be an international consortium where there are 10-100 sites in several regions that create technician jobs in those areas (and also reduce the insane demand for 100,000 technicians all in one city). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Also other things might boost efficiency and price.</p><p> Most obvious would be nanotechnological systems: likely sooner than people commonly assume, yet might take long enough to arrive to make effect on this project minor if it starts soon. Yet design-ahead for dream equipment might be a sound move.</p><p> Advanced robotics and automation is likely major gamechanger. The whole tissue management infrastructure needs to be automated from the start, but there will likely be a need for rapid turnaround lab experimentation too. Those AI agents are not just for doing standard lab tasks but also for designing new environments, tests, and equipment. Whether this can be integrated well in something like the CAIS infrastructure is worth investigating. But rapid AI progress also makes it hard to do plan-ahead.</p><p> Biotech is already throwing up lots of amazing tools. Maybe what we should look for is a way of building the ideal model organism - not just with biological barcodes or convenient brainbow coloring, but with useful hooks (in the software sense) for testing and debugging. This might also be where we want to look at counterparts to minimal genomes for minimal nervous systems. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:57:06 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:57:06 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><blockquote><p>Yet design-ahead for dream equipment might be a sound move.</p></blockquote><p> Thoughts on what that might look like? Are there potentially tractable paths you can imagine people starting work on today? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="Ewmh3YFqteYk6g6Kt-Mon, 23 Oct 2023 17:04:50 GMT" user-id="Ewmh3YFqteYk6g6Kt" display-name="Arenamontanus" submitted-date="Mon, 23 Oct 2023 17:04:50 GMT" user-order="5"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Arenamontanus</b></section><div><p> Imagine a &quot;disassembly chip&quot; that is covered with sensors characterizing a tissue surface, sequences all proteins, carbohydrate chains, and nucleic acids, and sends that back to the scanner main unit. A unit with nanoscale 3D memory storage and near-reversible quantum dot cellular automata processing. You know, the usual. This is not feasible right now, but I think at least the nanocomputers could be designed fairly well for the day we could assemble them (I have more doubts about the chip, since it requires to solve a lot of contingencies... but I know clever engineers). Likely the most important design-ahead pieces may not be superfancy like these, but parts for standard microscopes or infrastructure that are normally finicky, expensive or otherwise troublesome for the project but in principle could be made much better if we only had the right nano or microtech.</p><p> So the design-ahead may be all about making careful note of every tool in the system and having people (and AI) look for ways they can be boosted. Essentially, having a proper parts list of the project itself is a powerful design criterion.</p></div></section><h2> What would General Groves do? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:52:51 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:52:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>Davidad -- I recognise we&#39;re coming up on your preferred cutoff time. One other concrete question I&#39;m kind of curious to get your take on, if you&#39;re up for it:</p><p> &quot;If you summon your inner General Groves, and you&#39;re given a $1B discretionary budget today... what does your &#39;first week in office&#39; look like? What do you set in motion concretely?&quot; Feel free to splurge a bit on experiments that might or might not be necessary. I&#39;m mostly interested in the exercise of concretely crafting concrete plans. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Mon, 23 Oct 2023 16:54:28 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Mon, 23 Oct 2023 16:54:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><p>(also, I guess this might kind of be what <a href="https://www.aria.org.uk/our-team/">you&#39;re doing with ARIA</a> , but for a different plan... and sadly a smaller budget :) ) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="oZtBmFG7NAn6xPDDE-Mon, 23 Oct 2023 17:02:19 GMT" user-id="oZtBmFG7NAn6xPDDE" display-name="davidad" submitted-date="Mon, 23 Oct 2023 17:02:19 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>大卫</b></section><div><p>Yes, a completely different plan, and indeed a smaller budget. In this hypothetical, I&#39;d be looking to launch several FROs, basically, which means recruiting technical visionaries to lead attacks on concrete subproblems:</p><ol><li> Experimenting with serial membrane protein barcodes.</li><li> Experimenting with parallel membrane protein barcodes.</li><li> Build a dedicated brain-imaging synchrotron beamline to begin more frequent experiments with different approaches to stabilizing tissue, pushing the limits on barcoding, and performing R&amp;D on these purported 1-2 OOM speed gains.</li><li> Manufacturing organoids that express a diversity of human neural cell types.</li><li> Just buy a handful of light-sheet microscopes for doing small-scale experiments on organoids - one for dynamic calcium imaging at cellular resolution, and several more for static expansion microscopy at subcellular resolution.</li><li> Recruit for a machine-learning team that wants to tackle the problem of learning the mapping between the membrane-protein-density-annotated-hypergraph that we can get from static imaging data, and the system-of-ordinary-differential-equations that we need to simulate in order to predict dynamic activity data.</li><li> Designing robotic assembly lines that manufacture light-sheet microscopes.</li><li> Finish the C. elegans upload.</li><li> Long-shot about magnetoencephalography.</li><li> Long-shot about neural dust communicating with ultrasound.</li></ol></div></section><h2> Some unanswered questions</h2><p> Me (jacobjacob) and Lisa started the dialogue by brainstorming some questions. We didn&#39;t get around to answering all of them (and neither did we intend to). Below, I&#39;m copying in the ones that didn&#39;t get answered. </p><section class="dialogue-message ContentStyles-debateResponseBody" message-id="gXeEWGjTWyqgrQTzR-Fri, 20 Oct 2023 00:08:22 GMT" user-id="gXeEWGjTWyqgrQTzR" display-name="jacobjacob" submitted-date="Fri, 20 Oct 2023 00:08:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>雅各布</b></section><div><ul><li>What if the proposal succeeds? If the proposal works, what are the…<ul><li> …limitations? For example, I heard Davidad say before that the resultant upload might mimic <a href="https://en.wikipedia.org/wiki/Henry_Molaison"><u>patient HM</u></a> : unable to form new memories as a result of the simulation software not having solved synaptic plasticity</li><li> …risks? For example, one might be concerned that the tech tree for uploading is shared with that for unaligned neuromorphic AGI. But short timelines have potentially made this argument moot. What other important arguments are there here?</li></ul></li><li> As a reference class for the hundreds of thousands of microscopes needed... what is the world&#39;s current microscope-building capacity? What are relevant reference classes here for scale and complexity -- looking, for example, at something like EUV litography machines (of which I think ASML produce ~50/year currently, at like $100M each)? </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="q53Fxtev8FARJsX9H-Mon, 23 Oct 2023 15:12:02 GMT" user-id="q53Fxtev8FARJsX9H" display-name="lisathiergart" submitted-date="Mon, 23 Oct 2023 15:12:02 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>lisathiergart</b></section><div><p> Some further questions I&#39;d be interested to chat about are:<br><br> 1.  Are there any possible shortcuts to consider? If there are, that seems to make this proposal even more feasible.</p><p> 1/a. Maybe something like, I can imagine there are large and functional structural similarities across different brain areas. If we can get an AI or other generative system to &#39;fill in the gaps&#39; of more sparse tissue samples, and test whether the reconstructed representation is statistically indistinguishable from say the dynamic data collected in step 4, then we might be able to figure out what density of tissue sampling we need for full predictability. (seems plausible that we don&#39;t need 100% tissue coverage, especially in some areas of the brain?). Note, it also seems plausible to me that one might be missing something important that could show up in a way that wasn&#39;t picked up in the dynamic data, though that seems contingent on the quality of the dynamic data.</p><p> 1b. Given large amounts of sparsity in neural coding, I wonder if there are some shortcuts around that too. (Granted though that the way the sparsity occurs seems very critical!)<br><br> 2. Curious to chat a bit more about what the free parameters are in step 5.<br><br> 3. What might we possibly be missing and is it important? Stuff like extrasynaptic dynamics, glia cells, etc.</p><p> 3/a. Side note, if we do succeed in capturing dynamic as well as static data, this seems like an incredibly rich data set for basic neuroscience research, which in turn could provide emulation shortcuts. For example, we might be able to more accurately model the role of glia in modulating neural firing, and then be able to simulate more accurately according to whether or not glia are present (and how type of cell, glia cell size, and positioning around the neuron matters, etc).<br><br> 4. Curious to think more about how to dynamically measure the brain (step 3). Thin living specimens with human genomes and then using the fluorescence paradigm. I&#39;m considering whether there are tradeoffs in only seeing slices at a time where we might be missing data on how the slices might communicate with each other. I wonder if it&#39;d make sense to have multiple sources of dynamic measurements which get combined.. though ofc there are some temporal challenges there, but I imagine that can be sorted out.. like for example using the whole brain ultrasound techniques developed by Sumner&#39;s group.  In the talk you mentioned neural dust and communicating out with ultrasound, that seems incredibly exciting. I know UC Berkeley and other Unis were working on this somewhat, though I&#39;m currently unsure what the main blockers here are.</p></div></section><h2> Appendix: the proposal</h2><p> Here&#39;s a screenshot of notes from Davidad&#39;s talk. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/pckbhvxfiqtudnriv6ta"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/FEFQSGLhJFpqmEhgi/kxkdwijk3qowx0cj95dw"></figure><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FEFQSGLhJFpqmEhgi/does-davidad-s-uploading-moonshot-work<guid ispermalink="false"> FEFQSGLhJFpqmEhgi</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 03 Nov 2023 02:21:51 GMT</pubDate> </item><item><title><![CDATA[What are your favorite posts, podcast episodes, and recorded talks, on AI timelines, or factors that would influence AI timelines?]]></title><description><![CDATA[Published on November 2, 2023 10:42 PM GMT<br/><br/><p> Especially ones that do an attempt to make things pretty specific</p><br/><br/> <a href="https://www.lesswrong.com/posts/bxzghWFw6zKxAuH3r/what-are-your-favorite-posts-podcast-episodes-and-recorded#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bxzghWFw6zKxAuH3r/what-are-your-favorite-posts-podcast-episodes-and-recorded<guid ispermalink="false"> bxzghWFw6zKxAuH3r</guid><dc:creator><![CDATA[nonzerosum]]></dc:creator><pubDate> Thu, 02 Nov 2023 22:42:48 GMT</pubDate> </item><item><title><![CDATA[One Day Sooner]]></title><description><![CDATA[Published on November 2, 2023 7:00 PM GMT<br/><br/><p> There is a particular skill I would like to share, which I wish I had learned when I was younger. I picked it up through working closely with a previous boss (a CTO who had founded a company and raised it up to hundreds of employees and multi-million dollar deals) but it wasn&#39;t until I read <a href="https://worksinprogress.co/issue/the-story-of-vaccinateca">The Story of VaccinateCA</a> that I noticed it was a distinct skill and put into words how it worked. The Sazen for this skill is “One Day Sooner.” I would like to give warning before explaining further however: This skill can be hazardous to use. It is not the kind of thing “Rationalist Dark Art”  describes because it does not involve deception, and I think it&#39;s unlikely to damage much <i>besides the user.</i> It&#39;s the kind of thing I&#39;d be tempted to label a dark art however. Incautious use can make the user&#39;s life unbalanced in ways that are mostly predictable from the phrase “actively horrible work/life balance.”</p><p> It works something like this: when you&#39;re planning a project or giving a time estimate, you look at that time estimate and ask what it would take to do this one day sooner, and then you answer honestly and creatively.</p><h2> What does it look like?</h2><p> I used to work directly under the CTO of a medium sized software company. My team was frequently called upon to create software proofs of concept or sales demos. The timelines were sometimes what I will euphemistically call aggressive. Consider a hypothetical scene; it&#39;s Thursday and you have just found out that a sales demo is on Tuesday which could use some custom development. Giving a quick estimate, you&#39;d say this needs about a week of work and will be ready next Wednesday. What would it take to do this one day sooner?</p><p> Well, obviously you can work through the weekend. That gets you two more days. Given a couple of late evenings and getting enough total hours in is easy. That&#39;s not the only thing though. There&#39;s some resources from Marketing that would be good to have, you emailed them and they said they could meet with you on Monday. You want this faster though, so you walk over to their office and lean in, pointing out this is a direct assignment from the CTO so could we please have the meeting today instead. What else? Oh, there&#39;s a bunch of specification writing and robust test writing you&#39;d usually do. Some of that you still do, since it would be a disaster if you built the wrong thing so you need to be sure you&#39;re on the right track, but some of it you skip. The software just needs to work for this one demo, on a machine you control, operated by someone following a script that you wrote, so you can skip a lot of reliability testing and input validation.</p><p> I appreciate <a href="https://worksinprogress.co/issue/the-story-of-vaccinateca">The Story of VaccinateCA</a> , a description of an organization whose goal was helping people get the Covid-19 vaccination. I think it is worth reading in full, but I will pull out one particular quote here.</p><blockquote><p> We had an internal culture of counting the passage of time from Day 0, the day (in California) we started working on the project. We made the first calls and published our first vaccine availability on Day 1. I instituted this little meme mostly to keep up the perception of urgency among everyone.</p><p> We repeated a mantra: Every day matters. Every dose matters.</p><p> Where other orgs would say, &#39;Yeah I think we can have a meeting about that this coming Monday,&#39; I would say, &#39;It is Day 4. On what day do you expect this to ship?&#39; and if told you would have your first meeting on Day 8, would ask, &#39;Is there a reason that meeting could not be on Day 4 so that this could ship no later than Day 5?&#39;</p></blockquote><p> This is One Day Sooner.</p><p> I have worked in environments that had this norm, and environments that did not have it. I have asked questions analogous to “Is there a reason that meeting could not be on Day 4” and received answers roughly equivalent to “No, not really, I just didn&#39;t think of that.” As a result of deliberate choices in my career path, I have never had human lives so closely tied to the speed at which my team could execute but I strongly believe having lives on the line would not magically make more people adopt more speed.</p><p> One more note on what One Day Sooner looks like: I find it exhilarating and rewarding to work in this mode and suspect this is true of some others as well. You don&#39;t twiddle your thumbs, you don&#39;t accept being put off, you don&#39;t do a lot of unnecessary busywork just to make something simple happen. I often found it fun, even when I sometimes worked fourteen hour days, because things were happening and moving forward fast and I felt like I had an impact. Your mileage may vary.</p><h2> How do you do it?</h2><p> Above all else, it works by asking yourself every day what it would take to get this done one day sooner.</p><p> <strong>First</strong> , <strong>set a goal.</strong> Clearly describe what outcome you want. This description does not need to be a detailed multi-page specification sheet, and I suggest that you should have an evocative one sentence or at most one paragraph core goal even if you write a more detailed specification to describe the details.</p><p> <strong>Second</strong> , <strong>frequently ask what your blockers are and what the next step is.</strong> A blocker is anything that prevents forward motion towards the goal. A step is an incremental motion towards the goal. If you&#39;re walking to a destination, a blocker is a deep river in your path and a step is a footstep in the right direction.</p><p> <strong>Third, take responsibility (possibly heroic) for removing blockers and taking steps.</strong> After every step you take, you should be closer to the goal. It doesn&#39;t matter if the blockers or steps look like they&#39;re in your job description or not. It may be more useful to ditch the entire idea of responsibility and just think about what steps to take.</p><p> <strong>Fourth, keep alert for weird ways of solving the problem or signs that you&#39;re off target.</strong> It may be worth Babbling and Pruning, or Actually Thinking For Five Minutes, or Actually Trying. It may also be worth thinking about what you&#39;d do with vastly more resources. This is especially true if you are blocked. Your steps must actually be leading towards your target, even if they&#39;re attempts with expected value instead of uncertain value.</p><p> As a technique, One Day Sooner does not mean turning in low quality or shoddy work. “This thing should be polished and reliable” is something that can be part of your goal. Sometimes this technique means taking a hard look at exactly how reliable something really needs to be, because the answer is that something which works half the time and is ready one day sooner is actually better. Sometimes this technique means taking a hard look at how reliable something normally is, because the answer is that something which works 99% of the time is unacceptable.</p><p> I know that I spent more words talking about what it looks like and how it goes wrong than how to do it. This technique is more about an attitude than a procedure. If you notice the attitude conflicts with the above procedure, throw out the procedure and <a href="https://www.lesswrong.com/tag/twelfth-virtue-the">actually cut</a> .</p><p> What would it take to get this done one day sooner?</p><h2> How can it go wrong?</h2><p> This list is not exhaustive. This list is largely compiled from personal experience or close observation. While some highly esteemed deeds happened in very close proximity to this list, <a href="https://en.wikipedia.org/wiki/Long-term_nuclear_waste_warning_messages#Message">this list itself is not a place of honour and this list is a warning about a danger.</a></p><p> The concept of Taboo Tradeoffs may be useful at this point.</p><p> Part of what makes this technique work is aggressively looking for things to trade off in service of moving your timeline forward. Anything that is not part of your goal is likely to get traded off. If you paid attention in How Do You Do It or if you&#39;re paying attention while working on your goal, you&#39;ll notice if something is about to get traded off and should have been part of the goal. One Day Sooner doesn&#39;t have to trade off safety, important tests, or looking polished, though it can. In my own experience, this technique induces tunnel vision where things that aren&#39;t your goal fade in importance.</p><p> Sunday afternoons relaxing in the park, your workout routine, and eating things that aren&#39;t frozen pizza are usually not made part of the goal and thus suffer. You can mitigate this by establishing a soft rule to work under One Day Sooner norms for up to a fixed duration (mine was two weeks) and then stop and work at a slower and more relaxed pace. You can also mitigate this by establishing firm boundaries around what resources (including your time) that you allow the goal to make use of; the technique still works if you only allow it to make use of the hours between nine in the morning and five in the evening just as it works if you don&#39;t allow it to spend money. This technique is by default actively hostile to the concept of “time off.” Mitigation is not prevention.</p><p> The long term consequences are unlikely to be part of the goal. Sometimes you can speed up a process by throwing lots of money at it, and this is the correct tradeoff to get your target One Day Sooner but a poor tradeoff in an absolute sense. Sometimes you burn yourself or your teammates out and render people basically useless for months in order to get twice the productivity from them for a week, and this is the correct tradeoff to get your target One Day Sooner but a poor tradeoff over the long view. You can mitigate this by having someone in the loop who isn&#39;t dedicated to your goal and can push the brakes. Mitigation is not prevention.</p><p> Many people you interact with will not share your goal, or at least will not share your target of making it happen One Day Sooner. When talking to these people it is easy to come across as rude, obsessive, annoying, or otherwise a problem. Knocking on someone&#39;s door twice a day to ask if they&#39;ve signed off on that thing you sent them can seem perfectly reasonable from within your One Day Sooner tinted glasses. It is unlikely to make that person think fondly of you. You can mitigate this by thinking of goodwill and attention as a finite resource to be spent wisely. You can also mitigate this by inculcating the One Day Sooner approach in them and convincing them that your goal is a worthy one. Mitigation is not prevention.</p><p> Related to long term consequences, it is possible when aiming this hard at a goal to break rules. Sometimes this is on balance worth it. I am not going to say that it is never worth it and that all rules are well crafted and vitally important. I am going to caution that if you notice you&#39;re about to break a rule you should stop and think about that decision. Having someone not invested in the One Day Sooner loop around to vet your questionable choices is one form of mitigation, but those people can have a lot of incentives towards caution that take much of the power of the technique away. If circumstances permit, having open communication with someone in real authority (say, the person who wrote and enforces the rules and has the authority to suspend them) helps tremendously. The best mitigation I&#39;m aware of is keeping a careful distinction between policy, best practice, and law. Mitigation is not prevention.</p><p> This technique is about going very fast towards a goal. It does not help you decide if this is a good or worthwhile goal. Completing a desperate race to the finish line only to find out that you created the wrong thing hurts, and you are unlikely to have the reserves to try again. You can mitigate this by being careful with your initial spec and doing frequent mini-demos or having a minimum viable product that&#39;s actually getting used. Mitigation is not prevention.</p><h2>结论</h2><p>I wrote and published this for three reasons. One is that I wish I&#39;d understood this principle earlier in my career and so wanted people like me to be able to find and learn the technique earlier. I find working like this to be exhilarating and could have optimized my career to get to do more of it. If you&#39;re trying to get important work done quickly, I want you to have this tool. The second is that this is a technique I keep available and polished in my mental toolbox, and would like a place to point people at to explain why I might be metaphorically (or literally) knocking on their office door twice a day.</p><p> The third is that sometimes this is a bad technique to use. I mean “bad” in the sense that it will burn up your time, relationships, and health in the service of a goal which is not worth any of that. Some communities and organizations can inculcate this technique without spelling out what exactly it means. Worse, some people may wind up slipping into this mentality <i>without the goal</i> and burn themselves on it. Naming the technique makes it easier to see it and do something which is not that. If you&#39;re going to push yourself like this, please do it for a goal you actually thought about and decided was worth it.</p><br/><br/><a href="https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/EsxowsJsRopTGALCX/one-day-sooner<guid ispermalink="false"> EsxowsJsRopTGALCX</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Thu, 02 Nov 2023 19:01:00 GMT</pubDate></item></channel></rss>