<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 30 日星期三 18:14:17 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[An adversarial example for Direct Logit Attribution: memory management in gelu-4l]]></title><description><![CDATA[Published on August 30, 2023 5:36 PM GMT<br/><br/><p><i>请查看</i><a href="https://colab.research.google.com/drive/16Kp-4iH330a1dF6F0ntPK7kfNuxqkvfZ#scrollTo=0WpBUiAYkEdv"><i><u>我们的笔记本</u></i></a><i>进行图形重建，并检查您自己的模型的清理行为。</i></p><p><i>作为</i><a href="https://www.arena.education/"><i><u>ARENA 2.0</u></i></a><i>和 SERI ML 对齐理论学者计划 - 2023 年春季队列</i>的一部分而制作</p><h1>概述</h1><p>在这篇文章中，我们为 4 层变压器<a href="http://neelnanda.io/toy-models"><u>gelu-4l</u></a>中的内存管理或清理提供了具体证据。我们展示了直接 Logit 归因 (DLA) 具有误导性的示例，因为它没有考虑清理工作。</p><p><br>在引言中，我们定义了清理行为的含义，并快速回顾了 DLA。在清理行为的证据部分中，我们确定了从残余流中写入和删除信息的特定节点。根据我们对清理工作的了解，我们在直接 Logit 归因部分的含义中选择了导致误导性 DLA 结果的提示。</p><h1>介绍</h1><h2>清理行为</h2><p>之前在<a href="https://transformer-circuits.pub/2021/framework/index.html#d-footnote-7:~:text=Perhaps%20because%20of%20this%20high%20demand%20on%20residual%20stream%20bandwidth%2C%20we%27ve%20seen%20hints%20that%20some%20MLP%20neurons%20and%20attention%20heads%20may%20perform%20a%20kind%20of%20%22memory%20management%22%20role%2C%20clearing%20residual%20stream%20dimensions%20set%20by%20other%20layers%20by%20reading%20in%20information%20and%20writing%20out%20the%20negative%20version."><u>《变压器电路的数学框架》</u></a>中，作者提出了一种内存管理机制，并推测它的出现是因为对剩余流带宽的高需求。我们定义清理行为，其中注意力头和 MLP（我们统称为节点）从仅在网络早期层使用的残余流中清除信息。</p><p>我们将前向传递过程中的清理行为描述为四个步骤：</p><ol><li>写入器节点或嵌入将特定方向写入残差流</li><li>后续节点使用该方向进行进一步计算</li><li>清理节点通过将其负值写入残余流来清除该方向</li><li>该方向在模型的后面部分中以以下一种或两种方式使用：<ol><li>后续节点向该方向写入，使用该空闲子空间来传递与已清理信息不同的信息</li><li>去嵌入矩阵读取方向，直接影响输出logits</li></ol></li></ol><p>在这篇文章中，我们分析了写入器节点输出的删除（步骤 1 - 3）。在未来的工作中，我们将解决清理后后续节点如何使用已清理的空间（步骤 4）。</p><h2>直接 Logit 归因</h2><p><i>背景：</i>残差流的最终状态是模型中节点和嵌入的所有输出的总和<span class="footnote-reference" role="doc-noteref" id="fnrefmhfccldh2xf"><sup><a href="#fnmhfccldh2xf">[1]</a></sup></span> 。通过应用 Layer Norm 和 Unembedding 将残差流的最终状态映射到 logit 分布。两个标记的 logit 差异相当于两个标记的对数概率之差，因此可以直接解释为预测下一个标记。</p><p><a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ"><u>直接 Logit 归因 (DLA)</u></a>已用于识别各个节点对正确的下一个标记的预测的直接贡献<span class="footnote-reference" role="doc-noteref" id="fnrefmadpxyr44ml"><sup><a href="#fnmadpxyr44ml">[2]</a></sup></span> 。这是通过在考虑层范数后将去嵌入直接应用于任何节点的输出来完成的。例如，单个节点对令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span></span></span></span></span></span>进行预测<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>如果它增加了 logit 差值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{logit}_\text{A} - \text{logit}_\text{B}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">logit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">logit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">B</span></span></span></span></span></span></span></span></span> ，那么它比 token <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{B}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">B</span></span></span></span></span></span></span>的预测更有可能。然而，DLA 没有考虑到后面的节点依赖于前面的节点的输出这一事实。清理行为是 DLA 可能产生误导的可能原因之一（尤其是对于早期节点），因为节点输出可能会被后续节点持续清理。</p><h1>清理行为的证据</h1><h2>指标和术语</h2><p>我们引入<strong>投影比</strong>来比较残差流中的方向被覆盖的程度。 </p><p><img style="width:79.23%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/uocv2iapav5tcwqnkbfs"></p><p><img style="width:79.59%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/gtggzshscn8xquymrqnp"></p><p>我们使用符号“L0H2”来表示位于第 0 层、头索引为 2 的注意力头。第 2 层中的所有头统称为“L2HX”。在本节中，我们始终将写入器头 L0H2 的输出作为向量<strong>b</strong> ，而向量<strong>a</strong>将被残差流或清理头的输出替换。</p><h2>识别写入节点：头 L0H2 的输出正在被持续清理</h2><p>首先，我们扫描了完整的 gelu-4l 模型以获得一致的清理行为。对于每个节点，我们检查节点的输出是否存在于残差流的后续状态中。我们通过将每个注意力和 MLP 层之前和之后的残余流投影到 L0H2 的输出方向来测量清理情况。直观的理解：当将<i>第2层之后的残差流的状态</i>投影到<i>L0H2</i>上时，投影比代表L0H2的输出有多少仍然存在于第2层之后的残差流中。如果投影比为零，则残差流是正交的到L0H2的输出。换句话说，L0H2的输出方向不存在于残余流中。它可能已被移动到另一个子空间或完全从残留流中清除。<br></p><p>残余流到 L0H2 的投影如图 1（上）所示。我们发现 L0H2 的输出在 300 次前向传递中始终得到清理。在图 1（底部）中，我们可以在残差流通过 Transformer 模型时跟踪 L0H2 信息的存在情况：</p><ul><li>最初，我们在 resid_pre0 处看到 ~0 的投影比，因为 L0H2（位于 resid_pre0 和 resid_mid0 之间）尚未写入残差流</li><li>L0H2 写入残差流（位于 resid_mid0）后，投影比变为 ~1。它不完全是 1，因为第 0 层中其他头的输出可能不完全正交于 L0H2 的输出</li><li>在resid_mid0之后，~1的投影比表明L0H2的信息存在于残差流中，直到resid_post1（包括）</li><li>第 2 层中的注意力头（位于 resid_post1 和 resid_mid2 之间）似乎删除了 L0H2 最初写入的信息，导致投影比率小得多，约为 0。这种情况在 300 个提示（从模型的训练数据集中随机采样）中一致发生，但不同序列位置之间存在一些差异。</li></ul><p> L0H2的功能尚不完全清楚，但它类似于位置信息头<span class="footnote-reference" role="doc-noteref" id="fnrefjagxl19of0h"><sup><a href="#fnjagxl19of0h">[3]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefrdy8t8ebtpr"><sup><a href="#fnrdy8t8ebtpr">[4]</a></sup></span> 。 </p><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/s3yohrfhpmwvlpovc1vi"><figcaption>图1：各个位置的残差流在注意力层0中各个头的输出方向上的投影。“resid_mid0”指的是第0层中MLP之前的残差流位置，“resid_post0”指的是残差流位置在layer0中的MLP之后。使用跨批次 (n=300) 和位置 (n=1024) 的中位数来聚合投影比率。底部子图的阴影区域代表第 25 和 75 分位数。</figcaption></figure><h2>识别清洁工：6个2层注意力头正在清理L0H2</h2><p>我们发现六个注意力头（L2H2、L2H3、L2H4、L2H5、L2H6、L2H7）正在清理 L0H2 的输出。在下面的图 2 中，我们看到前面提到的注意力头具有一致的负投影比，这意味着它们正在以 L0H2 的相反方向写入残差流。我们认为，误差线所看到的大部分差异是由于清理行为对位置敏感而不是对提示敏感所致。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/c3tov6dedg60u7dmc8fw"><figcaption>图 2：所有注意力头和 MLP 到 L0H2 输出方向的投影。投影比率使用提示中位数 (n=300) 和位置 (n=1024) 进行聚合，误差线位于第 25 和第 75 分位数。 L0H2 被有意省略。 6个头的投影比之和的中位数为-0.903。</figcaption></figure><h2>验证因果关系：清理行为取决于写入器输出</h2><p>我们通过将残余流修补为第 2 层注意力头的<a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS"><u>OV 电路的</u></a>输入来验证清理头和写入头之间的因果关系。 OV 电路负责注意头<span class="footnote-reference" role="doc-noteref" id="fnrefmhfccldh2xf"><sup><a href="#fnmhfccldh2xf">[1]</a></sup></span>将哪些方向写入剩余流。通过修补 OV 电路，我们检查 L2HX 头在不存在 L0H2 的情况下将哪些信息写入剩余流。</p><p>我们重复<i>识别写入器节点</i>（图 1）、<i>识别清理器</i>（图 2）部分的实验，并比较清理运行和修补运行之间的结果。在修补的运行中，我们通过减去 L0H2 的输出来更改第 2 层中每个头的值输入 ( <a href="https://github.com/neelnanda-io/TransformerLens/blob/0d2827ecce4ef17b86060bdaaaaf50e724684085/transformer_lens/components.py#L943-L975"><u>hook_v_input</u></a> )。这相当于<i>仅对</i>第 2 层中的关注头的 OV 电路进行零消融 L0H2。变压器中的所有其他组件仍将“看到”L0H2 的原始输出。</p><p>图 3a 显示，在修补运行中，在第 2 层的注意力块之后，残余流到 L0H2 的投影率仍然很高，这表明清理的很大一部分确实依赖于输入。图 3b 比较了每个单独头的修补运行和干净运行。当清洁头没有“看到”L0H2 的输出时，清洁行为就会消失。 </p><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/wobnrn0ncwoe5vtcoipy"><figcaption>图 3a：L0H2 仅在输入第 2 层注意力头的 OV 电路时才会被清理。修补是指从第 2 层注意力头的值输入中减去 L0H2 的输出。使用跨批次的中值来聚合投影比（n=300） ) 和位置 (n=1024)，阴影区域位于第 25 和 75 分位数。 </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/cqkt4t97nl2jouzwmlku"><figcaption>图 3b：修补是指从第 2 层注意力头的值输入中减去 L0H2 的输出。使用跨批次 (n=300) 和位置 (n=1024) 的中位数来聚合投影比，误差线位于第 25 和第 75 分位数。</figcaption></figure><h1>对直接 Logit 归因的影响</h1><p>如果清理后的输出方向恰好与某些标记的未嵌入方向对齐，则头 L0H2 的 DLA 值较高。然而，如图 1 所示，L0H2 的输出方向在第 2 层的注意力块之后从残差流中很大程度上被移除。应用于 L0H2 的 DLA 并没有考虑到这种移除，并且它可能高估了 L0H2 对 logit 差异的贡献。因此，由于清理行为的存在，应用于 L0H2 的 DLA 很容易被误解。</p><h2>随机排列后不变的 DLA 值</h2><p>head L0H2 的输出对于任何给定位置的标记大致不变<span class="footnote-reference" role="doc-noteref" id="fnref7taiau8kfhc"><sup><a href="#fn7taiau8kfhc">[5]</a></sup></span> ，因此我们认为 L0H2 的输出主要包含位置信息。因此，当通过重采样消融改变标记信息时，我们仍然期望 L0H2 的类似输出。由于对于相同长度的不同提示，位置信息保持相同，因此 L0H2 的 DLA 值对于比较各个提示没有意义。</p><p>为了证明这一点，我们构建了四个对抗性提示，并指定了正确和错误的下一个标记预测。对抗性地选择正确的标记和提示的长度以产生 L0H2 的高 DLA 值。我们在干净运行和重采样消融运行中计算 DLA。在重采样消融运行中，我们使用随机提示在层标准化后修补 L0H2 输入的最后位置。</p><p>在图 4a 中，干净运行和重新采样消融运行显示非常相似的 DLA 值，表明 L0H2 的原始 DLA 具有误导性。如果来自 L0H2 的直接路径对最终输出 logits 做出了有意义的贡献，我们预计对该头的输入进行重新采样会极大地改变其 DLA 值。重采样消融后 DLA 的不变性是 L0H2 特有的。重采样消融后其他头的 DLA 发生显着变化，如图 4b 所示。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/lxiwbzprcbng7z5voziq"><figcaption>图 4a：L0H2 在四个对抗性示例上的直接 Logit 归因，有或没有重采样消融。重采样消融是指使用随机提示在层归一化（按“ln1.hook_normalized”缩放）后修补 L0H2 输入的最后位置。 DLA 重采样消融值使用批次中位数 (n=300) 进行聚合，误差线位于第 25 和第 75 分位数。 DLA 值对应于标记之间的 logit 差异，其中最高 logits 适合句子的上下文，如每个图的标题所示。 </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/mvh1vvxv8wmndbzkgzno"><figcaption>图 4b：四个对抗性示例上重要注意力头的直接 Logit 归因，有或没有重采样消融。为每个提示选择的注意力头是根据在干净运行中具有高 DLA 来选择的。重新采样消融是指使用随机提示修补给定头的输入。重采样消融后的 DLA 值使用批次中位数 (n=300) 进行聚合，误差线位于第 25 和第 75 分位数。</figcaption></figure><h2>写入头和清理头的 DLA 呈负相关</h2><p>最后，我们检查清理头的 DLA 值。我们没有认识到四个构造提示的清理头和写入头的 DLA 值之间存在显着相关性。这是预期的，因为清理头可以在模型中发挥多种功能。我们通过专门查看写入头 (L0H2) 与清理头 (L2HX) 的 V 组合来缩小 DLA 比较的范围。直观上，V 组合告诉我们 L2HX 写入的<i>方向</i>如何取决于 L0H2 的输出。它的 DLA 值显示了清理头在多大程度上覆盖了 L0H2 的 DLA（在 L2HX 的注意力模式是恒等的情况下）。写入头与所有清理头的总 V 组成由下式确定</p><span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\sum_\text{heads X} { \left( \lVert \text{L0H2} \rVert_\text{LayerNorm} \cdot \mathbf{W}^\text{X}_\text{OV} \right)}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.498em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">Σ</span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头 X</span></span></span></span></span></span> <span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">L0H2</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">LayerNorm</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-stack" style="vertical-align: -0.276em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">OV</span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">)</span></span></span></span></span></span></span></span></span></span><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{W}^\text{X}_\text{OV}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-stack" style="vertical-align: -0.276em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">OV</span></span></span></span></span></span></span></span></span></span>是吸头 X 的 OV 矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{W}_\text{V} \cdot \mathbf{W}_\text{O}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">V</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">O</span></span></span></span></span></span></span></span></span> ，我们对第 2 层中的所有吸尘器吸头 X = 2,3,4,5,6,7 求和。在图 5 中，总 V 的 DLA -成分显示与 L0H2 的 DLA 值呈负线性相关 –0.72(1)。这支持了我们的假设，即 L0H2 的高 DLA 值具有误导性，因为第二层中的清理头抵消了它们对最终 Logit 分布的贡献。 </p><figure class="image image_resized" style="width:83.67%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/nwibieysnjy1mtftn4nr"><figcaption>图 5：写入头 L0H2 的 DLA 与 L0H2 V 组合到清洁头 L2HX 的 DLA (X = 2,3,4,5,6,7)。我们在 10 个提示中显示位置 32 到 1023 的下一个正确标记的 DLA 值。由于缺乏上下文，先前的立场被省略。线性拟合的斜率为 –0.72(1)。 （理想的清理对应于 -1 的斜率。）误差以双标准差给出。</figcaption></figure><h1>结论</h1><p>在这篇文章中，我们提出了变压器中内存管理的具体示例。此外，我们为 DLA 方法构建了对抗性示例，该方法依赖于通过前向传递保留在残余流中的方向。我们为未来的工作提出了三个方向：首先，我们感兴趣的是后面的节点在前向传递过程中如何使用已清理的空间。其次，我们想进一步研究头部L0H2在gelu-4l模型中的作用。最后，我们将在其他现有模型中寻找清理行为。先前的研究表明，特别是早期头部和晚期头部表现出较高的 DLA 值<span class="footnote-reference" role="doc-noteref" id="fnrefrdy8t8ebtpr"><sup><a href="#fnrdy8t8ebtpr">[4]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefgfuy2jlo0gf"><sup><a href="#fngfuy2jlo0gf">[6]</a></sup></span> 。我们想要仔细检查早期头部的高 DLA 值是否在这些作品中产生误导。</p><h2>致谢</h2><p>我们的研究得益于许多人的讨论、反馈和支持，包括 Chris Mathwin、Neel Nanda、Jacek Karwowski、Callum McDougall、Joseph Bloom、Alan Cooney、Arthur Conmy、Matthias Dellago、Eric Purdy 和 Stefan Heimersheim。我们还要感谢 ARENA 和 SERI MATS 计划为启动该项目提供了便利。</p><h2>作者贡献</h2><p>所有作者对这篇文章的贡献均等。 Jett 提出并领导了该项目，而 James、Can 和 Yeu-Tong 则执行该项目。</p><p>请引用为：<br> Dao 等人，“直接 Logit 归因的对抗性示例：gelu-4l 中的内存管理”，2023 年。<br></p><p><br><i>标签：可解释性（机器学习和人工智能）、SERI MATS、语言模型、对抗性示例、变压器电路</i><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnmhfccldh2xf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmhfccldh2xf">^</a></strong></sup></span><div class="footnote-content"><p> Elhage 等人，“变压器电路的数学框架”，变压器电路线程，2021 年， <a href="https://transformer-circuits.pub/2021/framework/index.html"><u>https://transformer- Circuits.pub/2021/framework/index.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmadpxyr44ml"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmadpxyr44ml">^</a></strong></sup></span><div class="footnote-content"><p> Liberum 等人，“电路分析可解释性是否可扩展？来自龙猫多项选择能力的证据。” <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2307.09458v2"><u>https://arxiv.org/abs/2307.09458v2</u></a> 。</p><p> McGrath 等人，“九头蛇效应：语言模型计算中的紧急自我修复”。 <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2307.15771"><u>https://arxiv.org/abs/2307.15771</u></a> 。</p><p> Belrose 等人，“使用调谐镜头从变形金刚中引发潜在预测”。 <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2303.08112"><u>https://arxiv.org/abs/2303.08112</u></a> 。</p><p> Dar 等人，“分析嵌入空间中的 Transformer”。 <i>arxiv</i> ，2022， <a href="https://arxiv.org/abs/2209.02535"><u>https://arxiv.org/abs/2209.02535</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjagxl19of0h"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjagxl19of0h">^</a></strong></sup></span><div class="footnote-content"><p> Nanda，“实时研究记录：变压器可以重新导出位置信息吗？”。 YouTube，2022 年， <a href="https://www.youtube.com/watch?v=yo4QvDn-vsU&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T"><u>https://www.youtube.com/watch?</u></a> v=yo4QvDn-vsU&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrdy8t8ebtpr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrdy8t8ebtpr">^</a></strong></sup></span><div class="footnote-content"><p> Heimersheim 等人，“Python 文档字符串的电路。” Lesswrong，2023 年， <a href="https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only#Positional_Information_Head_0_4"><u>https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-</u></a> Circuit-for-python-docstrings-in-a-4-layer-attention-only#Positional_Information_Head_0_4 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7taiau8kfhc"> <span class="footnote-back-link"><sup><strong><a href="#fnref7taiau8kfhc">^</a></strong></sup></span><div class="footnote-content"><p>初步实验表明头 L0H2 的输出对于任何给定位置的标记大致不变。请随时联系以获取更多信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fngfuy2jlo0gf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgfuy2jlo0gf">^</a></strong></sup></span><div class="footnote-content"><p> Wang 等人，“野外可解释性：GPT-2 Small 中的间接对象识别电路。” arxiv，2022， <a href="https://arxiv.org/abs/2211.00593"><u>https://arxiv.org/abs/2211.00593</u></a> 。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2PucFqdRyEvaHb4Hn/an-adversarial-example-for-direct-logit-attribution-memory#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2PucFqdRyEvaHb4Hn/an-adversarial-example-for-direct-logit-attribution-memory<guid ispermalink="false"> 2PucFqdRyEvaHb4Hn</guid><dc:creator><![CDATA[Can Rager]]></dc:creator><pubDate> Wed, 30 Aug 2023 17:36:59 GMT</pubDate> </item><item><title><![CDATA[Biosecurity Culture, Computer Security Culture]]></title><description><![CDATA[Published on August 30, 2023 4:40 PM GMT<br/><br/><p><span>虽然我只在生物安全领域工作了</span><a href="https://www.jefftk.com/p/leaving-google-joining-the-nucleic-acid-observatory">大约一年</a>，而且我的计算机安全背景是我在从事软件工程其他方面工作时学到的东西，但文化似乎非常不同。良好的计算机安全文化可能是糟糕的生物安全文化的一些例子：</p><p></p><ul><li><p>公开、<a href="https://en.wikipedia.org/wiki/Full_disclosure_(computer_security)">充分披露</a>。撰写博客文章，详细介绍如何发现漏洞，目的是教其他人如何在将来找到类似的漏洞。如果需要的话，可以将细节保密几个月，以便给供应商时间修复，但在<a href="https://googleprojectzero.blogspot.com/2021/04/policy-and-disclosure-2021-edition.html">90 天</a>之后才会公开。</p></li><li><p>打破事物来修复它们。给定一个新系统，你当然应该尝试对其进行妥协。如果手动成功，请制作一个在几毫秒内破解它的演示。制作（并发布！）模糊器和其他自动化漏洞搜索工具。</p></li><li><p>热情的好奇心和探索精神。注意到漏洞的暗示并深入研究它们以找出它们的深度是很棒的。如果有人说“你不需要知道这一点”，请忽略他们并尝试自己解决。</p></li></ul><p>这并不是计算机安全一直以来的情况，也不是无处不在的情况，该领域的人们常常强烈保护这些理想，反对试图隐藏缺陷或压制研究人员的供应商。总体而言，我的印象是这种文化对计算机安全产生了巨大的积极影响。</p><p>这意味着，如果你进入具有计算机安全背景的生物安全的有效利他主义角落，并看到所有这些关于“<a href="https://docs.google.com/document/d/1VSfU3GiZumHDX2hoz3YY1PT2dQHtkbrfO8xLxI9BTGE/edit">信息危害</a>”的讨论，人们会阻止试图寻找漏洞，并且人们对他们发现的危险事物保持沉默，那么事情就会发生变化。感觉很奇怪，而且<a href="https://forum.effectivealtruism.org/posts/3a6QWDhxYTz5dEMag/how-can-we-improve-infohazard-governance-in-ea-biosecurity">可能已经腐烂了</a>。</p><p>因此，这里的框架可能有助于从生物安全的角度看待问题。想象一下， <a href="https://en.wikipedia.org/wiki/Morris_worm">Morris 蠕虫</a>、 <a href="https://en.wikipedia.org/wiki/Blaster_(computer_worm)">Blaster</a>和<a href="https://en.wikipedia.org/wiki/Samy_(computer_worm)">Samy</a>都没有发生过。有少数人独立发现了<a href="https://en.wikipedia.org/wiki/SQL_injection">SQL 注入</a>，但没有公开。尽管我们周围越来越多的事物变得自动化，但计算机安全从未发展成为一个领域。我们有无人驾驶汽车、机器人外科医生和简单的自动化代理为我们服务，所有这些都具有原始 Sendmail 的安全性。它已经存在了足够长的时间，以至于原始作者已经离开，没有人记得它是如何工作的。付出一些认真努力的人可能会造成巨大的破坏，但这不会发生，因为拥有造成破坏的专业知识的人有更好的事情要做。将现代计算机安全文化引入这个假设的世界不会顺利！</p><p>大多数文化差异都可以追溯到漏洞已知后所发生的情况。使用电脑：</p><ul><li><p>负责软件和硬件的公司有能力修复他们的系统，而信息披露有助于建立一种规范，要求他们立即采取行动。</p></li><li><p>编写软件的人们可以改变他们的方法，以避免将来产生类似的漏洞。</p></li><li><p>一旦发现漏洞，最终用户就有多种有效且相当便宜的缓解方案。</p></li></ul><p>但对于生物学来说，没有供应商，特定的修复可能需要数年时间，完全通用的修复可能不可能，而且缓解措施可能非常昂贵。每个领域所需的文化都位于这些关键差异的下游。</p><p>总的来说，这是令人悲伤的：如果我们都可以谈论我们最关心的事情，那么我们可以更快地采取行动，而且原因优先级也会更简单。我希望我们生活在一个可以应用计算机安全规范的世界！但不同的限制会导致不同的解决方案，考虑到这些限制，我在生物风险中看到的谨慎程度似乎是正确的。</p><p> （请注意，当我谈论“良好的生物安全文化”时，我描述的是一套规范，我认为这些规范适合我们所处的情况，并且在有效的利他主义者和其他持类似观点的人中很常见然而，生物学中还有另一套规范，是在主要威胁是自然威胁时制定的。由于自然不存在利用公共知识造成伤害的风险，因此这种旧方法甚至比计算机安全文化更加开放，并且在我看来意见非常不适合我们现在所处的环境。）</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0Y2ZK25X6cUCSswaLDgGWJVyPVUHfZAwEpGriDRTbua7jX6TYv2mvc5gX73QRfrJYl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/110979547652871947">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/bTteXdzcpAsGQE5Qc/biosecurity-culture-computer-security-culture#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bTteXdzcpAsGQE5Qc/biosecurity-culture-computer-security-culture<guid ispermalink="false"> bTteXdzcpAsGQE5Qc</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 30 Aug 2023 16:40:03 GMT</pubDate> </item><item><title><![CDATA[Why I hang out at LessWrong and why you should check-in there every now and then ]]></title><description><![CDATA[Published on August 30, 2023 3:20 PM GMT<br/><br/><p>正如标题所示，这篇文章是<a href="https://new-savanna.blogspot.com/2023/08/why-i-hang-out-at-lesswrong-and-why-you.html">从我的家庭博客 New Savanna 交叉发布的</a>，并不是针对 LessWrong 读者的。然而，你们中的一些人可能会发现思考 LessWrong 在世界上的地位很有用。</p><p>前两节是背景知识，首先是关于文化变革的一些内容，以设定总体背景。然后是有关 LessWrong 的一些一般信息。现在我们正在阅读我对这个地方的个人印象，最后建议您去看一下（如果您还没有看过的话）。</p><p><strong>长期的文化变革：基督徒和教授</strong></p><p>早在古代，基督教只是罗马帝国边缘的另一个神秘邪教。公元380年，狄奥多西一世皇帝颁布了《帖撒罗尼迦敕令》，它成为罗马帝国的国教。随着时间的推移，它在欧洲的许多部落中传播开来，那些基督教部落的人开始想到一个叫做基督教世界的实体，随着时间的推移，它变成了欧洲和“西方”。</p><p>早在欧洲还是基督教世界的时候，天主教会就是知识分子生活的中心。随着科学革命和宗教改革的到来，这种情况在 16 世纪发生了变化。当然，天主教仍然很强大，但大学取代了它，成为知识生活的制度中心。</p><p>我的观点简单明了：事情会发生变化。邪教可以成为主流，新的机构可以取代旧的机构。考虑到这一点，让我们考虑一下 LessWrong。</p><p><strong>少错</strong></p><p>当然，我不想暗示<a href="https://en.wikipedia.org/wiki/LessWrong">LessWrong</a>是一个庞大而复杂的在线社区，以及那里的潮流（理性主义运动、有效利他主义（EA）、对流氓人工智能的反乌托邦恐惧）可以与基督教相媲美，但它看起来像邪教对局外人来说，对一些局内人来说也是如此。它举办了大量关于人工智能和人工智能存在风险、有效利他主义以及更普遍的如何生活的高强度智力活动。我怀疑对于一些在那里发帖的人来说，这是他们知识生活的知识中心。</p><p> I 由<a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky">Eliezer Yudkowsky</a><a href="https://en.wikipedia.org/wiki/LessWrong#History">于 2009 年创立</a>，Eliezer Yudkowsky 是一位自学者，对人工智能，特别是高级人工智能的破坏性潜力有着浓厚的兴趣。这就是他最出名的地方，虽然我怀疑尼克·博斯特罗姆在这个主题上最广为人知——他 2014 年的书<a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><i>《超级智能</i></a>》是一本畅销书，而且他在牛津大学有一个学术职位——尤德科夫斯基可能更有影响力以他居住的硅谷为中心的科技社区。作为这种影响力的指标，请考虑 OpenAI 总裁兼联合创始人 Sam Altman 最近在 X（该网站以前称为 Twitter）上发布的两条帖子： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/l8uimkvgjphvcf3axlme" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/tgocmw57velmt0fkzavf 117w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/f7nqlukzq3ia0y2bevrw 197w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/s8xth2jvtclhihtao7dl 277w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/vieof1sso98oh77wjlrc 357w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/nj4pgxxmgskfwch2uauq 437w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/kne4lxlwgevhxnuqmano 517w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/aoet2q4xrf1magizulrj 597w"></figure><p>让我对和平奖产生怀疑。但除此之外，尤德科夫斯基在硅谷一直非常有影响力。在这些公司工作的人发帖并评论 LessWrong。</p><p><strong>为什么我在那里</strong></p><p>因为这是一个有趣的地方，虽然有点奇怪和令人反感，而且因为到目前为止我在那里进行了一些有趣的谈话。虽然不多，但绝对足够值得。</p><p>我不知道我第一次看到LessWrong是什么时候，但可以说是五年多前，甚至可能是十年前。但我在那里呆的时间并不多。大约两年前，也就是 GPT-3 开始震撼世界之后的某个时候，这种情况开始发生变化。我<a href="https://www.lesswrong.com/posts/hNXYWESpbzyjFnMyL/the-fourth-arena-what-s-up-in-the-world-these-days-we-re">于 2022 年 6 月发表了第一篇帖子</a>，到目前为止总共发表了 40 条帖子和 137 条评论。我通常每天都会去那里看看发生了什么。我可能会快速浏览一到五个新帖子，查看我关注的帖子的评论，然后继续处理我的业务。在特别好的一天，我会阅读我自己的一篇帖子的一些评论并回复。</p><p>问题是，我是一名浪人知识分子，而且多年来一直如此。如果你看过动画系列<a href="https://en.wikipedia.org/wiki/Samurai_Champloo"><i>《Samurai Champloo》</i></a> ，我的智力相当于 Jin。尽管我已经获得了博士学位——LessWrong 也有一些博士学位——并且曾经在伦斯勒理工学院担任过教职。但我很久以前就离开了那里，从此就没有一个知识分子的家了。我在学术文献中发表了一系列关于各种主题的文章，这些主题分散在文学批评、认知科学和文化进化领域，并在商业出版社与优秀出版商合作出版了两本书，一本是关于音乐的（贝多芬的铁砧，Basic 2001），另一本是关于音乐的（贝多芬的铁砧，Basic 2001）。计算机图形学（可视化，Harry Abrams 1989）。就其价值而言，对我而言，它的价值很大，想法数比页数似乎表明的要多，但我的作品从未真正流行起来。所以我明白在恐龙统治的世界里作为哺乳动物是什么感觉。</p><p>我强烈怀疑 LessWrong 的许多用户也有这种感觉。在这个层面上，以这种方式，我理解他们在做什么，即使我不做同样的事情。虽然我对人工智能非常感兴趣，但我对人类思想和人类文化更感兴趣，而且我不担心破坏和流氓人工智能的虚拟之手。尽管如此，我们还是可以并且确实聊了一会儿。</p><p>除此之外，谁知道呢？这里发生了很多事情。文化变革、彻底变革——这是我们所需要的，而且无论我们是否喜欢它，都处于其中——是混乱的。许多想法涌现并得到实践，但很少有人坚持下来。唉，问题在于，好的东西，真正深奥的东西，是无法提前预测的。疯子和天才之间的区别只有在回顾时才会变得清晰。例如，大约六个月前，Cleo Nardo 发表了一篇很长的帖子，<a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post">名为“瓦路易吉效应”(meg-post)</a> ，迄今为止已吸引了 184 条评论（每隔一段时间就会有新的评论涌入）。它很精彩，但我认为它很混乱，到处都是，它是关于大型语言模型的内部结构和奇怪的行为。只要有足够多的信息，一些很有价值的事情似乎就可能发生，尽管我不太可能注意到它。也许你可能会。</p><p><strong>我对这个地方的看法</strong></p><p>我很清楚，有一些非常聪明和富有创造力的人在 LessWrong 度过了很多时间。但是，可惜的是，它非常孤立，正如对立的反文化往往如此。例如，这里是<a href="https://www.lesswrong.com/posts/LKAogXdruuZXdx6ZH/publish-or-perish-a-quick-note-on-why-you-should-try-to-make">3 月份的一篇短文</a>，附有 48 条评论，“关于为什么你应该努力让现有学术界能够理解你的工作。”是的！反之亦然。 LessWrongers 需要更多地关注感兴趣领域的现有工作。</p><p>我的老师，已故的大卫·海斯曾经说过，“伟大的天赋需要严格的纪律。” （他不记得他在哪里听到这句话，但这不是他原创的。如果你对这个短语进行网络搜索，你会得到一堆点击，尽管在几分钟内我没有找到找到那个确切的短语。）我担心很多在这里发帖的人都是如此，而且整个地方也是如此。仅仅了解很多东西并提出很多新的想法是不够的。您需要与他们合作并随着时间的推移对其进行完善，并与现有知识建立联系。</p><p>尽管如此，这里的谈话还是非常文明的。有激烈的讨论，但我没有看到任何激烈的争论。您可以对评论进行投票，这是一个常见功能，但 LessWrong 界面允许您区分评论的质量以及您是否同意，这是一个有用且重要的功能。</p><p>鉴于以学院和大学、各种智囊团和基金会以及工业研究为代表的“合法”知识世界，已经陷入了循环利用旧思想的困境（在观看《星际迷航》重播时，柯克的开场警告是“大胆地去无人涉足的地方”）以前已经过去了！”）世界需要新的概念滋生地。 LessWrong 看起来是一项很有前途的试点工作。</p><p><strong>为什么不去参观呢？</strong></p><p>如果您不熟悉这个地方，我建议您顺便过来，花一个小时逛逛。如果你没有发现任何吸引你注意力的东西，就离开，但一周后回来再试一次。如果一个月左右后没有任何效果，那就是这样。</p><p>但如果有什么事情坚持不了，你就得靠自己了。也许你到处发表评论。然后发布一些内容，看看会发生什么。</p><br/><br/> <a href="https://www.lesswrong.com/posts/nd9qnKxmYPr76Yi5s/why-i-hang-out-at-lesswrong-and-why-you-should-check-in#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nd9qnKxmYPr76Yi5s/why-i-hang-out-at-lesswrong-and-why-you-should-check-in<guid ispermalink="false"> nd9qnKxmYPr76Yi5s</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Wed, 30 Aug 2023 15:20:45 GMT</pubDate> </item><item><title><![CDATA["Wanting" and "liking"]]></title><description><![CDATA[Published on August 30, 2023 2:52 PM GMT<br/><br/><h1> “想要”和“喜欢”</h1><p><em>这是 2023 年虚拟人工智能安全营的成果。感谢以下人员的反馈和有益的对话：Oliver Bridge、Tim Gothard、Rasmus Jensen、Linda Linsefors。</em> <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-1" id="fnref-sieGxzsnEEMoiKqSL-1">[1]</a></sup></p><p>这篇文章回顾了有关<em>“想要”</em>和<em>“喜欢”</em>的文献，这两个主要组成部分通常被称为生物奖励系统。它旨在为人工智能安全相关工作提供信息，特别是在尝试<a href="https://www.lesswrong.com/posts/nfoYnASKHczH4G5pT/brain-enthusiasts-in-ai-safety">利用</a><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8">神经科学</a><a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX">见解</a><a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX">进行调整的</a>方法中。</p><p>第 1 节介绍了奖励的两个高级组成部分之间的区别：想要和喜欢。在这一点上，我简化了主题并将两者视为同质类别。第 2 节和第 3 节深入研究了每个组件，并给出了更细粒度的模型，包括对其神经生物学基础的描述。 （2.2 和 3.2 是更技术性/干性的神经科学，所以如果这不是您的主要兴趣，您可能想跳过它们。）第 4 节讨论它们之间的功能关系以及为什么这种“劳动分工”可能受到青睐通过进化。</p><h2>一、简介</h2><p>我将首先介绍本文的四个核心概念：<em>想要</em>、 <em>“想要”</em> 、<em>喜欢</em>和<em>“喜欢”</em> 。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-2" id="fnref-sieGxzsnEEMoiKqSL-2">[2]</a></sup>他们沿着二维空间雕刻人类价值<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-3" id="fnref-sieGxzsnEEMoiKqSL-3">[3]</a></sup>的空间。第一个是我们有动力去做/被驱使去做（想要）的事情与我们对发生（或即将发生）感觉良好（喜欢）的事情之间的区别。第二个区别是每个元素的更基本/不太复杂的成分（ <em>“想要”</em>和<em>“喜欢”</em> ）与更复杂和认知的成分（<em>想要</em>和<em>喜欢</em>）之间的区别。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-4" id="fnref-sieGxzsnEEMoiKqSL-4">[4]</a></sup>本节的两部分详细阐述了这两个维度。</p><h3> 1.1.喜欢与想要</h3><p>喜欢是指当你咬了一口美味的食物，它的味道给你带来快乐，或者更一般地说，给你带来正价的情感状态。想要是当你有动力采取行动以获得食物并将其送到嘴里食用时。</p><p>这可能足以让我们凭直觉了解其区别的大致轮廓，但同时，它可能会引发一些问题。我可以想象这样的情况：我有点想起身去厨房，但我太累了（或者我的意志力太弱了）而起不来。所以我没有起床，尽管我知道冰箱里的食物非常好，如果我真的站起来去拿，我会很高兴这样做。另一方面，如果食物绝对美味并给我带来快乐，并且我“在某种程度上”享受它，但同时相信（或至少<a href="https://www.lesswrong.com/tag/subagents">我的一部分</a>相信）我不应该吃它怎么办？也许我认为我根本不应该<em>享受</em>它？也许我认为它不健康，或者担心有人会因为我在特定情况下吃它而评判我，或者我的上帝/宗教禁止吃猪肉。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-5" id="fnref-sieGxzsnEEMoiKqSL-5">[5]</a></sup>这些边缘案例难道不会质疑喜欢和想要之间的简单区别吗？因为它们过于截然不同，但却是连贯且同质的事物？也许他们确实如此，但通过研究这种粗粒度的喜好和需求，我们仍然可以了解很多我们所谓的人类价值观的原始内容。</p><p>根据我所说的对喜欢和想要之间关系的<em>天真的看法</em>，我们想要 X 的原因是我们喜欢 X，或者至少期望/预测喜欢 X。意识到我们（将）喜欢某物之间的相关性不久之后就产生了对它的渴望，使得这种观点非常适合大多数日常情况。</p><p>然而，喜欢和想要有时是分开的。我们可能会开始想要某些东西，即使我们既没有机会体验喜欢，也无法预测我们会喜欢它。举一个具体的例子，人类和其他动物通常在第一次性接触之前就产生性欲。有时他们甚至可能不知道性是什么。尽管如此，它们还是实施了一些智能行为，这些行为是进化选择的，以便可靠地以性交结束。</p><p>在其他情况下，迄今为止一直不喜欢的东西会成为欲望的对象。 Robinson 和 Berridge (2013) <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-6" id="fnref-sieGxzsnEEMoiKqSL-6">[6]</a></sup>教导老鼠将特定的刺激（以下称为条件刺激；CS）与将极咸的水直接注入其口中的排斥体验（以下称为无条件刺激；UCS）联系起来。老鼠很快意识到 UCS 可靠地跟随 CS，因此它们学会在看到 CS 时转身并退出。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-7" id="fnref-sieGxzsnEEMoiKqSL-7">[7]</a></sup>在实验的下一阶段，研究人员给老鼠注射了两种模拟大脑信号的化合物，这些信号在正常情况下会传达有关危险的低血钠水平的信息。重要的是，由于他们的饮食中始终含有足够的盐，因此他们从未有机会发现他们对咸味食物的喜欢（不喜欢）在多大程度上取决于血钠水平。</p><p>然而，他们的行为发生了巨大的变化。他们没有从CS撤退，而是开始接近它，渴望获得宝贵的盐分。 （感知到的）生理状态的变化将令人厌恶的事物变成了令人向往的事物，并且与之相关的提示也随之发生。</p><p>喜欢与想要分离的例子还不止于此。一个人可能会对某种药物产生成瘾，即使他们不太喜欢这种药物引起的状态。此外，随着吸毒时间的延长，其积极的主观影响（喜欢）往往会逐渐消失，但成瘾（想要）却会持续存在。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-8" id="fnref-sieGxzsnEEMoiKqSL-8">[8]</a></sup>换句话说，即使一个人不喜欢接受该剂量并且意识到他们不会喜欢一旦接受该剂量所发生的事情，他也可能想要获得该剂量。</p><p>有些人还会产生强迫性的欲望或行为模式，这些欲望或行为模式不会带来积极的体验，但仍然很难抗拒。亚临床的例子包括强迫性地检查手机、电子邮件或社交媒体、<a href="https://en.wikipedia.org/wiki/Doomscrolling">末日滚动</a>和赌博成瘾。至少从我的轶事现象角度来看，这些事情确实感觉像是我想要但不喜欢的东西。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-9" id="fnref-sieGxzsnEEMoiKqSL-9">[9]</a></sup></p><p>也许不太明显的是那些给我们带来很多积极体验但我们却没有产生任何强烈愿望的事物的例子。我记得朱莉娅·加莱夫（Julia Galef）在一些播客中提到，她真的很喜欢苹果，但从未学会“渴望”苹果，每当她碰巧吃苹果时，她就会想起这一点。如果我们想要某样东西的（主要）原因是我们喜欢它，那么她对苹果的渴望难道不应该与她对苹果的喜欢程度成正比吗？另一方面，更具推测性的是，有些人报告在某些<a href="https://astralcodexten.substack.com/p/nick-cammarata-on-jhana">冥想</a><a href="https://astralcodexten.substack.com/p/highlights-from-the-comments-on-jhanas">状态</a>下感到极度愉悦，但并未对此上瘾。我在第 2 节中讨论了更多选择性影响喜好但不想要的实验示例。</p><h3> 1.2.<em>喜欢</em>与<em>“喜欢”</em>和<em>想要</em>与<em>“想要”</em></h3><p><a href="https://en.wikipedia.org/wiki/Folk_psychology">民间心理学</a>概念并不能保证非常适合脑/心智科学。对于一些例子，诸如意识、情感、记忆、疼痛等概念，甚至<a href="https://academic.oup.com/book/11923">“概念”本身的概念，</a>结果被证明将重要的不同现象混在一起（参见 Ramsey，2022，第 2.3 节）。我们可能期望喜欢和想要走这条路，如果我们想用它们作为神经科学上充分的本体论的起点，它们至少需要一些改进。</p><p>从表面上看，喜欢和想要之间似乎存在不对称，因为后者至少在许多情况下可以从行为中推断出来， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-10" id="fnref-sieGxzsnEEMoiKqSL-10">[10]</a></sup>而前者是“私人”经验的问题。显然，对于无法用语言表达其持续主观体验的动物来说，这尤其成问题。然而，不对称性可能比看起来要弱。毕竟，我们可以确定人类的哪些自动行为和/或生理反应与（口头报告） <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-11" id="fnref-sieGxzsnEEMoiKqSL-11">[11]</a></sup>正价或负价体验（至少特定于某些领域，例如食物）相关。然后，我们可以转向动物并寻找类似的反应（例如，以大致相同的运动模式参与类似的肌肉群），以了解它们是否与我们预测动物喜欢或喜欢的相同类型的客观可观察事件相关。不喜欢。</p><p>例如，就食物而言， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-12" id="fnref-sieGxzsnEEMoiKqSL-12">[12]</a></sup>事实证明，愉快和不愉快的味道会强烈触发特定的面部表情（参见 Berridge &amp; Robinson，2003， <a href="https://sites.lsa.umich.edu/berridge-lab/wp-content/uploads/sites/743/2019/10/Berridge-Robinson-TINS-2003.pdf">图 I</a> ）。重要的是，它们甚至可以在没有意识功能的情况下发生， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-13" id="fnref-sieGxzsnEEMoiKqSL-13">[13]</a></sup>例如在睡眠中或新皮质功能缺陷的个体中， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-14" id="fnref-sieGxzsnEEMoiKqSL-14">[14]</a></sup>例如无脑婴儿（Steiner，1973；参见Berridge &amp; Winkielman，2003）。</p><p>观察到一些客观可测量的快乐表现可以在没有意识的情况下发生，促使引入<em>“喜欢”</em> （不需要意识的核心情感反应）和<em>喜欢</em>（有意识的快乐）之间的区别<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-15" id="fnref-sieGxzsnEEMoiKqSL-15">[15]</a></sup> （Berridge＆Robinson， 2003 年；贝里奇和克林格尔巴赫，2015 年）。类似地， <em>“想要”</em> （激励显着性、不需要意识的提示触发动机）与<em>想要</em>（具有声明性目标的认知欲望）不同。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-16" id="fnref-sieGxzsnEEMoiKqSL-16">[16]</a></sup>因此， <em>“喜欢”</em> /<em>喜欢</em>和<em>“想要”</em> /<em>想要的</em>区别捕获了隐性或客观可测量成分（ <em>“引用”</em> ）和显性或主观成分（<em>未引用</em>）之间的差异。</p><p>虽然对奖励进行客观衡量的需要是做出区分的最初动机，但缩小<em>“喜欢”</em>和<em>“想要”</em>的明确组成部分使得更容易识别它们的神经基础。初步近似，我们有<strong>（1）</strong>一个<em>“喜欢”</em>系统，集中在少数享乐热点和冷点周围，阿片类药物在<em>“（不）喜欢”</em>的产生和调节中发挥着主要作用<strong>，（2）</strong> <em>“想要”</em>系统，分布更广泛（尽管在某种程度上以腹侧被盖区为中心），并且以多巴胺作为关键的神经递质。这两个系统在某种程度上是分开的，但也有重叠。</p><h2> 2. 喜欢</h2><h3>2.1. <em>“喜欢”</em>和<em>喜欢</em></h3><p>“无意识的快乐”的想法似乎是矛盾的。从什么意义上说，发生在我们身上的事情可以是令人愉快的，但却无法被意识所感知？</p><p>快乐的无意识方面的引入遵循我们在心理学中经常看到的<a href="https://www.lesswrong.com/s/u9uawicHx7Ng7vwxA">概念外推</a>的特定模式。我们发现一种特定的与心理相关的现象具有一些客观可测量的“行为特征”。例如，人类、其他类人猿、老鼠和许多其他哺乳动物物种都会伸出舌头来回应美味的食物（参见 Berridge &amp; Robinson，2003， <a href="https://sites.lsa.umich.edu/berridge-lab/wp-content/uploads/sites/743/2019/10/Berridge-Robinson-TINS-2003.pdf">图 I</a> ）。对令人厌恶的口味的反应在很大程度上在不同的分类群中也是同源的。除此之外，将人们暴露于与味觉无关的价刺激（例如，快乐与愤怒的面孔）会影响他们消耗多少美味食物，并且即使这些刺激没有被有意识地感知到，这种效应也会持续存在（Winkielman等，2005）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-17" id="fnref-sieGxzsnEEMoiKqSL-17">[17]</a></sup></p><p>因此，将快乐/喜欢分为对有价刺激（ <em>“喜欢”</em> ）和有意识感知的快乐（<em>喜欢</em>）客观可测量的“核心情感反应”的潜意识部分的基本原理。后者更接近动词“喜欢”的常见含义。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-18" id="fnref-sieGxzsnEEMoiKqSL-18">[18]</a></sup>它表示意识中可用的价态感觉，即对当前事态的认可或反对。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-19" id="fnref-sieGxzsnEEMoiKqSL-19">[19]</a></sup></p><p>由于在这两者中， <em>“喜欢”</em>是客观可测量的组成部分，并且该领域的大部分研究都是在实验动物身上进行的（其主观体验无法通过口头报告来测量），因此我们对“喜欢”了解得更多也就不足为奇了。 <em>“喜欢”</em>的神经生物学基础比“<em>喜欢</em>”的神经生物学基础更重要（ <em>“想要”</em>和<em>想要</em>的也是如此）。因此，我对后者的讨论比前者更多的是一种推测。此外，大多数<em>“喜欢”</em>的动物研究依赖于一组有限的“奖励刺激领域”（主要是食物、性和药物），因此我们对这些领域之间核心情感反应如何不同的了解仍然相当有限。</p><h3> 2.2.大脑中的<em>“喜欢”</em></h3><p> <em>“喜欢”</em>电路最重要的组成部分是一些<em>享乐热点</em>和<em>冷点</em>，它们是一小群神经元，它们的刺激分别选择性地增加或减少<em>“喜欢”</em>反应。 （在增加的情况下，有时称为<em>享乐增强</em>。）它们<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-20" id="fnref-sieGxzsnEEMoiKqSL-20">[20]</a></sup>都不是解剖学上不同的结构（您不会在典型的神经解剖学教科书的索引中找到它们）。相反，它们是嵌入更大区域的“功能岛”，涉及许多与<em>“喜好”</em>无关的功能。</p><p>最重要的两个位于<a href="https://en.wikipedia.org/wiki/Basal_ganglia">基底神经节</a>，特别是<a href="https://en.wikipedia.org/wiki/Nucleus_accumbens">伏隔核</a>(NAc) 和<a href="https://en.wikipedia.org/wiki/Ventral_pallidum">腹侧苍白球</a>(VP)。 NAc 和 VP 都包含一个热点和一个冷点。 NAc分为核心和外壳，外壳的前面有一个热点，后面有一个冷点。在 VP 中，排列相反，热点在后面，冷点在前面（Richard et al., 2013; Berridge &amp; Kringelbach, 2013, 2015）。</p><p>除了基底神经节之外，享乐热点（但据我所知，不是冷点）位于眶额皮质 (OFC)、前岛叶 (aIns) 和脑桥臂旁核 (PBN；Söderpalm &amp; Berridge) ，2000 年；参见 Smith 等人，2010 年）。然而，NAc 和 VP 中的热点似乎是最重要的，它们是快乐的<em>发生器</em>。损害或停用 NAc 热点或 VP 热点可以消除享乐反应 (Berridge &amp; Kringelbach, 2015)。此外，虽然破坏NAc热点只会消除“喜欢”，但破坏VP热点（或其暂时失活）会导致“不喜欢”通常积极的事物（例如蔗糖）。</p><p>此外，几乎没有新皮质的无脑儿童以及经历了广泛的 OFC 损伤的人或非人类动物仍然保留完整的<em>“喜欢”</em>反应。 OFC 损伤的人也报告有意识的快乐，这表明即使<em>喜欢</em>也并不严格依赖于皮质（参见 Berridge &amp; Winkielman，2003）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-21" id="fnref-sieGxzsnEEMoiKqSL-21">[21]</a></sup>相比之下，两个基底神经节热点中的每一个都需要一定的基线活动水平，以便对其中一个热点进行额外刺激以产生享乐增强（Smith &amp; Berridge，2007；Smith 等人，2011；参见 Richard等人，2013）。</p><p>重要的是，这并不是‘刺激热点提高<em>“好感度”</em> ，刺激冷点降低<em>“好感度”</em>那么简单。用于刺激的神经递质的选择很重要。在这里，阿片受体最为相关（参见 Berridge &amp; Robinson, 2003；Berridge &amp; Kringelbach, 2015；Smith et al., 2010）。在 NAc 热点中，mu、δ 和 kappa 阿片受体激动剂都会引起快感增强，而在 VP 热点和皮质热点中，这种作用似乎仅限于 mu-阿片受体 (MOR) 激动剂。根据区域的不同，刺激非阿片受体可以产生类似的结果。到目前为止，被证明能产生快感增强的神经递质包括大麻素 (NAc)、食欲素 (NAc、VP、PBN、OFC) 和 GABA (PBN；Söderpalm &amp; Berridge, 2000)。</p><p>前面我提到<em>“喜欢”</em>系统和<em>“想要”</em>系统是紧密相连的。事实证明，在大多数情况下，刺激皮层下享乐热点除了“喜欢<em>”之外还增加了“想要</em><em>”</em> 。此外，在享乐热点中增加<em>“欲望”</em>的化合物范围远大于增加<em>“喜好”</em>的化合物范围（Berridge &amp; Kringelbach，2013）。刺激 NAc 和 VP 中的冷点也可以产生<em>“想要”</em> 。我将在第 3 节中进一步讨论这一点。相反，阿片类药物可以间接增加腹侧被盖区的活动，该区域是核心<em>“想要”</em>通路中多巴胺的主要来源（Zhang 等人，2022）。</p><p>包含热点和冷点的 NAc 壳区域通常用“情感键盘”来描述，其中神经元的位置与它们的激活引起的情感反应（例如<em>“喜欢”</em>与<em>“不喜欢”</em> ）密切相关（Richard等，2013；Berridge &amp; Kringelbach，2013，2015）。更具体地说，似乎存在从前到后延伸的化合价梯度。刺激“键盘”更靠前的区域会引起<em>“喜欢”</em>反应，而更多位于尾部的神经元会抑制<em>“喜欢”</em>和/或引起<em>“不喜欢”</em> ，有时还伴随着对威胁（例如捕食者）的物种特异性反应。</p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706488/figure/F2/">图 2</a>来自 Richard 等人。 (2013) 显示了 NAc 中神经元群体的分布，这些神经元的刺激会引发特定类型的行为。我们有享乐热点（红橙色），其中激活通常会增强<em>“喜欢”</em>反应。在它后面，我们看到两个区域。在腹侧（下侧），有一组神经元具有我们期望从享乐冷点（蓝色）获得的功能。他们的刺激会抑制<em>“喜欢”</em> 。在背侧（上侧），我们有另一个簇，它也具有抑制作用，但它们不是抑制<em>“喜欢”</em> ，而是抑制厌恶反应（紫色）。所有这些位点，除了对<em>“喜欢”</em>的影响外，“仍然产生进食”（即<em>“想要”</em>进食），它们所在的更广泛（绿色）区域和背内侧新纹状体的一部分也是如此。粗略地说，尾状核和壳核位于 NAc 上方），如图左上角的绿点所示。</p><p>冷点区域还包含其功能超出<em>“喜欢（不）喜欢”</em>调节范围的细胞。对其中一些的刺激会产生恐惧反应或攻击性表现，例如向潜在威胁的刺激物扔泥土。重要的是，这些细胞的刺激结果也可以受到环境的调节。在平静、平和和安全的环境中，刺激增加积极反应的区域会扩大，而厌恶/恐惧反应的区域会缩小。压力大、危险、不安全的环境会产生相反的效果。</p><p>与热点类似，刺激冷点的结果取决于所使用的配体的种类。刺激相同的三种阿片受体（mu、delta、kappa）会增强 NAc 热点的<em>“喜欢”</em> ，在邻近的冷点产生强烈的<em>“（不）喜欢”</em>甚至与恐惧相关的行为。 Mu 受体激动剂还抑制 VP 冷点的<em>“喜欢”</em> 。</p><h3> 2.3.有意识的快乐有什么好处？</h3><p>诚然，目前还不清楚<em>“喜欢”</em>和<em>喜欢</em>如何以及在多大程度上相互依赖。 <em>“喜欢”</em>的东西需要什么才能变得<em>喜欢</em>？我们知道<em>“喜欢”</em>可以在不<em>喜欢的情况</em>下发生，但是反过来呢？有什么东西可以满足我们明确的享乐感受而不影响任何这些“核心情感反应”吗？或者，只要意识“开启”（至少超过某个阈值） <em>，“喜欢”的</em>事物就会变得<em>喜欢</em>？从我对文献的粗略回顾来看，我们似乎不知道，尽管眶额皮层（OFC）作为该区域的主要候选区域出现，其活动（可能除了更基本的<em>“喜欢”</em>结构之外）对于<em>喜欢</em>很重要（克林格尔巴赫，2005，2010）。</p><p><a href="https://en.wikipedia.org/wiki/Global_workspace_theory">意识的全局工作空间理论（GWT）</a> <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-22" id="fnref-sieGxzsnEEMoiKqSL-22">[22]</a></sup> （及其所借鉴的实验）可能会给出一些关于<em>“喜欢”</em><em>刺激</em>/事件的建议。大脑可以无意识地进行相当多的复杂处理（例如， <a href="https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain#Unconscious_processing_of_meaning">单词的语义</a>）。在 GWT 范式中进行的实验表明了这一点，使用<a href="https://en.wikipedia.org/wiki/Visual_masking">感觉掩蔽</a>来防止刺激到达意识。根据 GWT，与表示/处理特定刺激相关的神经活动必须超过某个关键阈值，才能传播到其他（特别是多模式/联想/更高级别）大脑区域，然后这些区域可以开始以某种同步的方式处理它。方式。这是“意识到<em>某</em>事”的神经基础。该表征可供其他大脑系统使用，包括直接连接到言语器官的系统，以便我们可以报告我们对刺激的意识。否则，它的处理仍然是无意识的、局部的，并且仅限于特定的较低级别的大脑区域。</p><p>将此观点转化为<em>“喜欢”</em>和<em>喜欢的</em>情况，可能存在类似的核心享乐影响（ <em>“喜欢”</em> ）强度阈值，刺激必须超过该阈值才能传播到全球工作空间并被有意识地<em>喜欢</em>。传播到某些特定区域（例如 OFC）似乎特别重要。</p><p>值得注意的是，为了防止有价刺激到达意识，显示无意识处理的刺激对客观可观察的快乐相关因素的影响的实验（例如，Winkielman等人，2005）使用了与GWT相同的方法，即感觉掩蔽。这是一个小证据，表明 GWT 实验的结果可能会转化为<em>“喜欢”</em>和<em>喜欢的</em>情况。</p><p>如果这个观点是正确的，它可能指向有意识的<em>喜好</em>的可能功能，因为<a href="https://www.lesswrong.com/posts/KuKaQEu7JjBNzcoj5/explicitness">明确的</a>享乐价值增加了​​影响其他大脑系统的可能途径的范围（例如，第3节中讨论的动机回路）。所以也许问题是“有意识的快乐有什么好处？”只不过是“意识有什么好处”的一个特例？</p><p>关于<em>喜欢</em>而不是<em>“喜欢”的可能性，</em>我想知道自我形象、规范信念、社会期望或（广泛理解的）当前情况的反思性评估等因素是否会产生自上而下的影响（例如，我对生活的感觉有多好，或者世界上事物的发展方式）可能会引起<em>喜欢</em>，但不会引起核心情感反应和皮层下享乐热点的相应活动。另一方面，我还预计，至少在某些情况下（也许在大多数甚至所有情况下），皮质下（不）快感发生器将由于这种自上而下的影响而被二次激活。</p><h2> 3. 想要</h2><h3>3.1. <em>“想要”</em>与<em>想要</em></h3><p>我们的行为并不总是反映我们对应该做什么的明确信念，这很难说是一个原始的观察。这种现象有很多名称，例如<a href="https://www.lesswrong.com/tag/akrasia">意志力</a>薄弱或缺乏意志力。这可能使得<em>“想要”</em> （激励显着性）和<em>想要</em>（认知欲望）之间的区别比<em>“喜欢”</em>和<em>喜欢</em>之间的区别更加相关和直观。</p><p> <em>“想要”</em>可以被视为<em>想要的</em>无意识对应物，类似于<em>“喜欢”</em>是<em>喜欢</em>的无意识对应物。<em>想要</em>（认知激励）是指针对我们意识到并明确表达的愿望的目标的计划，而<em>“想要”</em> （激励显着性）指的是更冲动、反应性、低水平的动机，它可以独立于我们的行为而行动（声明我们）<em>想要</em>或<em>喜欢</em>。</p><p>更具体地说， <em>“想要”</em>被定义为“大脑的条件性动机反应，通常由与奖励相关的刺激触发并分配给该刺激”（Berridge，2007）。如果刺激与“本身有奖励”的事件（例如甜味）相关，则该刺激是与奖励相关的。这种关联可以很简单，比如发生的时间非常接近，也可能涉及一些更复杂的认知学习过程。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-23" id="fnref-sieGxzsnEEMoiKqSL-23">[23]</a></sup>习得的与奖励相关的刺激通常被称为“条件刺激”（CS），而固有的奖励事件被称为“非条件刺激”（UCS）。然而， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-24" id="fnref-sieGxzsnEEMoiKqSL-24">[24]</a></sup>并非所有驱动<em>“想要”</em>的输入都是被学习的，因为大脑天生会对某些刺激做出<em>“想要”</em>反应，独立于学习（或在没有学习的情况下/在学习之前）。似乎， <em>“想要”</em>最初的适应性价值是激励动物追求一小部分无条件的奖励，例如食物、性或有利的环境条件范围，例如适当的温度和酸度。随着时间的推移，随着更复杂的学习机制的发展， <em>“想要”</em>的角色变得可以通过学习来扩展（Berridge，2007）。</p><p>根据 Berridge 和 Robinson (2003) 的说法，认知激励（<em>想要</em>）与激励显着性（ <em>“想要”</em> ）有以下三个（或可能四个）特征：</p><blockquote><p> …它们（1）是已知的或想象的（认知激励表征）； (2) 期望是愉快的（享乐期望）； (3) 主观上渴望并打算获得（想要的明确认知表征），并且，也许，(4) 已知可以通过导致其发生的行动来获得（对行为与结果因果关系的理解）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-25" id="fnref-sieGxzsnEEMoiKqSL-25">[25]</a></sup></p></blockquote><p>我将在 3.3 节中更多地推测<em>“想要”</em>和<em>想要</em>之间的差异和关系。在接下来的 3.2 节中，我概述了<em>“想要”</em>的神经生物学基础。</p><h3> 3.2.大脑中的<em>“想要”</em></h3><p> <em>“喜欢”</em>可以大致位于少数享乐热点和冷点，其中内源性阿片类药物是回路中的主要参与者（如第 2 节所述）。 <em>“想要”</em>的神经底物更多地分布在整个大脑中，其中多巴胺是关键的神经递质。</p><p>人脑有多种<a href="https://en.wikipedia.org/wiki/Dopaminergic_pathways">多巴胺能通路</a>，其中与<em>“想要”</em>最相关的是<a href="https://en.wikipedia.org/wiki/Mesolimbic_pathway">中脑边缘通路</a>。它从<a href="https://en.wikipedia.org/wiki/Ventral_tegmental_area">腹侧被盖区（VTA）</a>到腹侧纹状体（包括伏隔核）和其他一些区域。尽管它是参与激励显着性的大脑区域多巴胺的最大供应者（Ikemoto，2010），但它并不是唯一的供应者，并且至少在某些人工设置中，即使它停止工作， <em>“想要”</em>也可能发生。</p><p>我们知道，更具体地说，中脑边缘通路被消除的动物仍然可以通过植入某些大脑区域的电极产生自我刺激的冲动（参见 Ikemoto，2010，第 131 页）。目前尚不清楚这些结果在多大程度上会在没有中脑边缘通路的情况下转化为<em>“想要”</em> 。</p><p> VTA 和其他涉及<em>“想要”</em>的区域形成了一个高度互连的网络（参见 Ikemoto，2010）。不过，其中很多并不是<em>“想要”</em>特有的，还涉及到其他方面的奖励。在第二节中，我讨论了伏隔核和腹侧苍白球的享乐热点和冷点。用许多神经递质（包括那些倾向于引起<em>“（不）喜欢”</em>反应的神经递质）刺激它们往往会产生<em>“想要”</em>行为，这两种行为都与接近（ <em>“想要”</em> X）和厌恶（ <em>“想要”</em>非X）有关。后脑的臂旁核也是如此，其 GABA-A 受体的刺激是<em>“想要的”</em> ，但它附近有一个小区域，它也引起了<em>“喜欢”</em> 。</p><p>网络的其他区域也参与学习。例如，巴甫洛夫学习似乎依赖于一个回路，该回路的主要组成部分是基底外侧杏仁核、眶额皮质和伏隔核（Burke et al., 2010）。另一方面，当对中央杏仁核的刺激与高度显着的刺激（无论是愉快还是不愉快无关，它只需要在任一方向上具有很强的效价）相结合时，即使对于非常不愉快的刺激也可以建立非常强烈的<em>“想要”</em>刺激（Warlow 等人，2020）。</p><p>另外两条对<em>“想要”</em>很重要的多巴胺能通路是中皮质通路和黑质纹状体通路。中皮质通路从 VTA 延伸到前额皮质，参与执行功能。其疾病，包括涉及多巴胺耗竭或其他多巴胺能活动干扰的疾病，与认知控制和工作记忆受损有关（参见 Ott &amp; Nieder，2019）。</p><p>黑质纹状体通路从黑质（SN）到背侧纹状体，其主要作用是运动控制。大多数 SN 细胞的死亡是帕金森病的直接原因。帕金森病患者往往会出现与中脑边缘通路（例如冷漠）和中皮层通路（例如注意力缺陷）功能下降相关的症状。这些症状的严重程度与运动症状的严重程度高度相关，表明这些系统之间存在一定程度的耦合（参见 Leyton，2010，第 232-233 页）。</p><p>接下来的几段讨论了多巴胺能活动一般如何影响<em>“想要”</em> 。它并不是对证据的完整概述或与替代假设的比较（参见：Berridge，2007），而是作为这种神经递质所发挥的作用的信息说明。</p><p>测试某些神经递质 X 如何影响某些行为 Y 的最直接方法可能是降低或增加 X 的水平并测量 Y 的变化。一种方法是培育具有异常低或高水平的神经递质的转基因动物。突触中的神经递质（参见 Berridge，2007，第 403-405 页）。</p><p>多巴胺缺乏（DD）小鼠的大脑中几乎没有多巴胺，可以通过敲除酪氨酸羟化酶的编码基因来制造，没有这种酶就无法产生多巴胺。 DD几乎不吃不喝，不足以维持自己的生活。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-26" id="fnref-sieGxzsnEEMoiKqSL-26">[26]</a></sup>为了让他们正常饮食，他们需要服用左旋多巴（一种直接多巴胺前体，只需在生产链中的一步从多巴胺中去除），这可以暂时将他们的多巴胺恢复到正常水平。</p><p>这使得可以测试（1）DD 小鼠是否对不同种类的刺激（例如糖溶液与水）表现出不同的情感/ <em>“喜欢”</em>反应，以及（2）在尝试这两种刺激后，它们是否学会了更喜欢其中一种。另一个是根据他们在后续试验中的选择来衡量的。事实证明，它们可以同时做到这两点，这表明多巴胺并不是（至少某些形式的） <em>“喜欢”</em>和奖励相关学习所必需的。在野生型（即正常/未转基因）小鼠中观察到类似的模式，其多巴胺能系统在其生命后期因神经化学损伤而受损。</p><p>另一方面，<strong>高</strong>多巴胺能小鼠的突触中多巴胺含量几乎是正常数量的三倍（与野生型相比），可以通过敲除编码多巴胺转运蛋白的基因来创建，多巴胺转运蛋白是一种从神经元中去除多巴胺的蛋白质。突触。这些小鼠更有动力去获得奖励，更能抵抗分散它们对当前目标的注意力的刺激，并且愿意为了奖励而更加努力地工作。换句话说，它们似乎比野生型更<em>“想要”</em>奖励。他们学习刺激和奖励之间的关联或了解哪些行为会带来奖励以及<em>“喜欢”</em>反应的能力不受影响。</p><p>人类的多巴胺紊乱疾病又如何呢（参见 Leyton，2010）？帕金森病 (PD) 是由黑质中的多巴胺能细胞退化引起的，这些细胞不直接参与中脑边缘系统。尽管如此，许多帕金森病患者仍表现出与中脑边缘（例如冷漠、无欲）和中皮质（例如注意力和执行功能较差）系统中多巴胺能功能下降相关的症状。这些症状的严重程度与运动问题的严重程度相关，尤其是帕金森病。一些接受 L-DOPA 治疗的 PD 患者（约 3-4%；Pezzella，2005）会出现<a href="https://en.wikipedia.org/wiki/Dopamine_dysregulation_syndrome">多巴胺失调综合征 (DDS)</a> ，多巴胺缺乏的过度补偿会导致他们出现“病态” <em>“欲望”</em>行为（成瘾、赌博、强迫性性行为）活动，即使他们在服药前没有这样的病史）使他们成为多巴胺能亢进的例证。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-27" id="fnref-sieGxzsnEEMoiKqSL-27">[27]</a></sup></p><p>许多高度成瘾的潜力都是多巴胺能的。核心例子包括安非他明、可卡因及其类似物，它们的作用主要是通过增加突触间隙中停留的多巴胺的量。有趣的是，在动物研究中，如果将它们与 DA 拮抗剂（即与多巴胺受体结合但不激活它们的化合物，从而阻止多巴胺本身结合）一起使用，它们的成瘾潜力可以降低（也许甚至（几乎？）完全消除？）并发挥其典型效果（参见 Puglisi-Allegra &amp; Ventura，2012）。同时，多巴胺拮抗作用并不能消除这些多巴胺能药物的其他作用。例如，当安非他明与 DA 拮抗剂一起使用时，一些欣快效应仍然存在（参见 Leyton，2010），这表明这些效应要么是通过多巴胺以外的机制介导的，要么是通过某些多巴胺受体介导的，这些受体未被研究中使用的特殊药物阻断<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-28" id="fnref-sieGxzsnEEMoiKqSL-28">。 28]</a></sup> （Nader 等，1997；Ikemoto，2010）。</p><p>不直接与多巴胺系统相互作用的高度成瘾药物往往会产生继发性多巴胺能效应。例如，mu-阿片受体激动剂（如吗啡、海洛因和芬太尼）通常通过抑制位于 VTA 后部或邻近区域（称为吻内侧被盖 (RMTg)）的 GABA 能神经元发挥作用。另一方面，这些 GABA 能神经元会抑制 VTA 的多巴胺能神经元，从而驱动<em>“欲望”</em> 。因此，抑制前者意味着去抑制后者，从而增加 VTA 目标区域中 DA 的浓度（参见Zhang et al., 2022）。</p><p>咖啡因是另一种具有间接多巴胺能作用（且具有相对轻微的成瘾潜力）的化合物。尽管其主要作用机制是阻断腺苷受体，但它也会增加多巴胺的释放，从而有助于其精神兴奋和强化特性（Ferré，2016）。我们可以在实验中看到，与不含咖啡因的相同酸奶相比，在酸奶中添加咖啡因增强了对该酸奶的偏好（Panek 等，2013）。对蜜蜂和富含咖啡因的花蜜进行了类似的实验，得到了相似的结果（Wright 等人，2013）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-29" id="fnref-sieGxzsnEEMoiKqSL-29">[29]</a></sup></p><p><a href="https://en.wikipedia.org/wiki/Pavlovian-instrumental_transfer">巴甫洛夫工具转移</a>是指当动物已经为了获得某种奖励/UCS而工作时，在感知到与其当前追求的UCS相关的CS后开始更加努力地工作时发生的情况。这种效应与中脑边缘多巴胺能活性的增加密切相关，并且可以通过干预中脑边缘系统来调节，例如使用多巴胺激动剂（Berridge，2007，第 420-421 页；Cartoni 等，2016；Salamone 等） .，2016）。</p><h3> 3.3.认知激励/有意识的需求有什么好处？</h3><p>为什么除了<em>“想要”</em>之外我们还需要<em>想要</em>？在第二节中，我提出了一个关于<em>喜欢</em>和<em>“喜欢”</em>的类似问题，并给出了一个临时假设，即价刺激的意识使其能够被大脑的其他部分所访问，从而使它们能够互操作并利用彼此的输出。<em>想要</em>和<em>“想要”</em>有类似的关系吗？</p><p> Berridge 和 Robinson (2003) 似乎赞同这样的观点。在他们看来，<em>欲望</em>可以让动物实现目标，而这些目标是通过简单的联想学习无法实现的，需要更复杂的推理、工作记忆等。他们写道：</p><blockquote><p>理性认知的本质之一是它对世界上合法一致性的推理利用，通常，未来价值最好是从过去的价值推断出来的。此外，老鼠必须利用其对哪些行为导致哪些结果的理解，从几种可能的行为中选择能够产生最佳奖励的行为。</p></blockquote><p>与此相关的是，作为一个有意识的执行控制下的过程，<em>需求</em>在改变当地激励因素方面更加稳定，这使得计划的规划和执行首先成为可能。</p><p>因此，从 VTA 到前额皮质某些部分的中皮质通路可能是<em>需求</em>的主要基质的候选者，因为它的主要作用之一是执行功能。</p><p>与第二节末尾的猜测相呼应，我们可以<em>想要</em>一些东西而不是<em>“想要”</em>它吗？也许我们可以再次从自我形象的角度来看：我们将自己建模为<em>想要</em>X，但这个模型并不准确，也不足以推翻向另一个方向推进的<em>“想要”</em> 。 Perhaps sometimes <em>wanting</em> without <em>&quot;wanting&quot;</em> is adaptive because it causes the organism to think about plans of action that can be executed once a proper context occurs, so that <em>&quot;wanting&quot;</em> is triggered and makes use of the information contained in the plans developed due to <em>wanting</em> .</p><h2> 4. Why <em>&quot;like&quot;</em> something if you can just <em>&quot;want&quot;</em> it?</h2><p> Ex ante, we might expect that <em>&quot;wanting&quot;</em> itself, paired with a sufficiently good learning algorithm, should be enough to achieve any goals necessary for survival and reproductive fitness.</p><p> Maybe it is necessary or more efficient to have separate systems for (1) adaptive but &quot;mindless&quot; responses to local incentives and (2) goal-directed behavior that relies on taking into account broader context; hence <em>&quot;wanting&quot;</em> and <em>wanting</em> , respectively. Still, this leaves us with a question about the adaptive value of <em>&quot;liking&quot;</em> and <em>liking</em> . Had they not contributed to our ancestors&#39; fitness in one way or another, evolution would not have selected for them. <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-30" id="fnref-sieGxzsnEEMoiKqSL-30">[30]</a></sup></p><p> It seems that the field has not reached a consensus on that question. Here I present three hypotheses. Like much of evolutionary psychology, they are all tentative, and if evidence for them is at best indirect. Importantly, these hypotheses are neither exhaustive nor mutually exclusive.</p><h3> Hypothesis 1: Liking extends wanting</h3><p> On this account, we start with a small set of default motivations that evolution selected for ( <em>&quot;wants&quot;</em> ), and <em>&quot;liking&quot;</em> helps repurpose the motivational system towards new motivations. The <em>&quot;wanting&quot;</em> system is more evolutionarily ancient. <em>&quot;Liking&quot;</em> emerged relatively recently, in animals living in more cognitively demanding environments that necessitated acquiring new motivational mechanisms over the lifetime. Pleasure is an additional training signal for the <em>&quot;wanting&quot;</em> system, allowing the brain to repurpose systems specialized for being motivated towards one domain of stimuli/events towards another domain. The need for this &quot;lifetime reprogramming&quot; may arise due to the environment being too complex or too variable for evolution to encode appropriate sources of motivation into the genome.</p><p> Kent Berridge (a pioneer of this line of research) seems to lean towards this hypothesis ( <a href="https://hearthisidea.com/episodes/kent">podcast interview link</a> ). He gives credit for it to, among others, Anthony Dickinson (eg, Dickinson &amp; Balleine, 2010).</p><p> Quoting directly from the episode (lightly edited by me):</p><blockquote><p> […] pleasure exists because it essentially allows brain <em>&quot;wanting&quot;</em> systems that might have evolved for one thing (eg, food) to experience a new pleasant event (eg, social accomplishment) and to enjoy that event and to bring to bear the brain <em>&quot;wanting&quot;</em> systems for the old thing on to this new target, basically giving us a new target of desire.</p></blockquote><p> On this account, it seems to be somewhat similar to the picture presented by the <a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">Shard Theory view of human values</a> , except that not all the values (contextually activated motivations/ <em>&quot;wants&quot;</em> ) are learned from scratch.</p><p> Here, Berridge doesn&#39;t distinguish between <em>&quot;liking&quot;</em> and <em>liking</em> . My interpretation is that he views them as serving a similar function, just on different levels of &quot;cognitive sophistication&quot;, similar to <em>&quot;wanting&quot;</em> and <em>wanting</em> .</p><p> There is an interesting category of cases, where the learned change in valence happens prior to a corresponding change in motivation. In other words, your experience changes whether/how much you <em>&quot;(dis)like&quot;</em> X but without updating your motivation for X. Your motivation for X is updated the next time you encounter X and experience altered valence.</p><p> Dickinson and Balleine (Balleine, 2010) give an example, where the first author (AD) who really liked watermelons, at some point got sick shortly after eating a watermelon (most likely the disease and eating the fruit were not related). A few days later, he went to eat a watermelon and although it was most likely basically the same kind of watermelon, it tasted awful. Apparently, his <em>&quot;liking&quot;</em> / <em>liking</em> system wrongly inferred the watermelon to be the causal factor behind the sickness, which altered his taste, but not his motivation to eat watermelons, until he tasted a no-longer-tasty watermelon. These cases have been reproduced in rat experiments. <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-31" id="fnref-sieGxzsnEEMoiKqSL-31">[31]</a></sup></p><p> Note that this is different from the Salt Sea experiment (Robinson &amp; Berridge, 2013), where rats&#39; motivation was already altered by their physiological state before being presented with the stimulus. In the watermelon case, on the other hand, the aversion occurs unexpectedly.</p><h3> Hypothesis 2: Preparing physiology</h3><p> <em>&quot;Liking&quot;</em> reactions associated with food and fluids seem to prepare the organism for intake of nutrients. Increased salivation facilitates pre-digestion of the food in the mouth, licking the lips ensures that some bits of the food are not left out, increased gastric movements prepare the rest of the digestive system, etc.</p><h3> Hypothesis 3: Implicit social communication</h3><p> An animal&#39;s physiological reactions, like the ones discussed above, often carry socially important information. Thus, it&#39;s plausible that in more social species these behaviors would evolve to become more pronounced, in other to facilitate implicit social communication. On the other hand, they may also become less reflective of the actual physiological state, in order to produce signals that are more likely to influence the behavior of the other animal in the direction that is beneficial for the signaller.</p><h3> Hypothesis 4: Additional information to update behavior on</h3><p> Valence ( <em>&quot;liking&quot;</em> / <em>liking</em> ) may also route partially processed information to the motivational circuits in order to update already ongoing behavior. If we do X for the first time and it quickly turns out that we <em>like</em> it, we do more of it. An obvious caveat is that we may not be able to experimentally disentangle the indirect effect mediated by valence from the direct effect. We have seen that it is possible to very quickly develop strong motivation for something without <em>&quot;liking&quot;</em> it, eg, in the wireheading studies.</p><h2>参考</h2><ul><li>Berridge, KC (2007). The debate over dopamine&#39;s role in reward: The case for incentive salience. Psychopharmacology, 191(3), 391–431. <a href="https://doi.org/10.1007/s00213-006-0578-x">https://doi.org/10.1007/s00213-006-0578-x</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2008). Affective neuroscience of pleasure: Reward in humans and animals. Psychopharmacology, 199(3), 457–480. <a href="https://doi.org/10.1007/s00213-008-1099-6">https://doi.org/10.1007/s00213-008-1099-6</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2013). Neuroscience of affect: Brain mechanisms of pleasure and displeasure. Social and Emotional Neuroscience, 23(3), 294–303. <a href="https://doi.org/10.1016/j.conb.2013.01.017">https://doi.org/10.1016/j.conb.2013.01.017</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2015). Pleasure Systems in the Brain. Neuron, 86(3), 646–664. <a href="https://doi.org/10.1016/j.neuron.2015.02.018">https://doi.org/10.1016/j.neuron.2015.02.018</a></li><li> Berridge, KC, &amp; Robinson, TE (2003). Parsing reward. Trends in Neurosciences, 26(9), 507–513. <a href="https://doi.org/10.1016/S0166-2236(03)00233-9">https://doi.org/10.1016/S0166-2236(03)00233-9</a></li><li> Berridge, KC, &amp; Robinson, TE (2016). Liking, wanting, and the incentive-sensitization theory of addiction. The American Psychologist, 71(8), 670–679. <a href="https://doi.org/10.1037/amp0000059">https://doi.org/10.1037/amp0000059</a></li><li> Berridge, K., &amp; Winkielman, P. (2003). What is an unconscious emotion? (The case for unconscious &quot;liking&quot;). Cognition and Emotion, 17(2), 181–211. <a href="https://doi.org/10.1080/02699930302289">https://doi.org/10.1080/02699930302289</a></li><li> Burke, KA, Franz, T., Miller, D., &amp; Schoenbaum, G. (2010). Conditioned Reinforcement and the Specialized Role of Corticolimbic Circuits in the Pursuit of Happiness and Other More Specific Rewards. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 50–62). Oxford University Press.</li><li> Cartoni, E., Balleine, B., &amp; Baldassarre, G. (2016). Appetitive Pavlovian-instrumental Transfer: A review. Neuroscience &amp; Biobehavioral Reviews, 71, 829–848. <a href="https://doi.org/10.1016/j.neubiorev.2016.09.020">https://doi.org/10.1016/j.neubiorev.2016.09.020</a></li><li> dela Peña, I., Gevorkiana, R., &amp; Shi, W.-X. （2015）。 Psychostimulants affect dopamine transmission through both dopamine transporter-dependent and independent mechanisms. European Journal of Pharmacology, 764, 562–570. <a href="https://doi.org/10.1016/j.ejphar.2015.07.044">https://doi.org/10.1016/j.ejphar.2015.07.044</a></li><li> Dickinson, A., &amp; Balleine, B. (2010). Hedonics: The Cognitive–Motivational Interface. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 74–84). Oxford University Press.</li><li> Ferré, S. (2016). Mechanisms of the psychostimulant effects of caffeine: Implications for substance use disorders. Psychopharmacology, 233(10), 1963–1979. <a href="https://doi.org/10.1007/s00213-016-4212-2">https://doi.org/10.1007/s00213-016-4212-2</a></li><li> Ikemoto, S. (2010). Brain reward circuitry beyond the mesolimbic dopamine system: A neurobiological theory. Novel Perspectives on Drug Addiction and Reward, 35(2), 129–150. <a href="https://doi.org/10.1016/j.neubiorev.2010.02.001">https://doi.org/10.1016/j.neubiorev.2010.02.001</a></li><li> Kringelbach, ML (2005). The human orbitofrontal cortex: Linking reward to hedonic experience. Nature Reviews Neuroscience, 6(9), 691–702. <a href="https://doi.org/10.1038/nrn1747">https://doi.org/10.1038/nrn1747</a></li><li> Kringelbach, ML (2010). The Hedonic Brain: A Functional Neuroanatomy of Human Pleasure. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 202–221). Oxford University Press.</li><li> Leyton, M. (2010). The Neurobiology of Desire: Dopamine and the Regulation of Mood and Motivational States in Humans. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 222–243). Oxford University Press.</li><li> Nader, K., Bechara, A., &amp; van der Kooy, D. (1997). Neurobiological constraints on behavioral models of motivation. Annual Review of Psychology, 48(1), 85–114. <a href="https://doi.org/10.1146/annurev.psych.48.1.85">https://doi.org/10.1146/annurev.psych.48.1.85</a></li><li> Ott, T., &amp; Nieder, A. (2019). Dopamine and Cognitive Control in Prefrontal Cortex. Trends in Cognitive Sciences, 23(3), 213–234. <a href="https://doi.org/10.1016/j.tics.2018.12.006">https://doi.org/10.1016/j.tics.2018.12.006</a></li><li> Panek, LM, Swoboda, C., Bendlin, A., &amp; Temple, JL (2013). Caffeine increases liking and consumption of novel-flavored yogurt. Psychopharmacology, 227(3), 425–436. <a href="https://doi.org/10.1007/s00213-013-2971-6">https://doi.org/10.1007/s00213-013-2971-6</a></li><li> Pezzella, FR, Colosimo, C., Vanacore, N., Di Rezze, S., Chianese, M., Fabbrini, G., &amp; Meco, G. (2005). Prevalence and clinical features of hedonistic homeostatic dysregulation in Parkinson&#39;s disease. Movement Disorders, 20(1), 77–81. <a href="https://doi.org/10.1002/mds.20288">https://doi.org/10.1002/mds.20288</a></li><li> Puglisi-Allegra, S., &amp; Ventura, R. (2012). Prefrontal/accumbal catecholamine system processes high motivational salience. Frontiers in Behavioral Neuroscience, 6, 31. <a href="https://doi.org/10.3389/fnbeh.2012.00031">https://doi.org/10.3389/fnbeh.2012.00031</a></li><li> Ramsey, W. (2022). Eliminative Materialism. In EN Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2022). Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/archives/spr2022/entries/materialism-eliminative/">https://plato.stanford.edu/archives/spr2022/entries/materialism-eliminative/</a></li><li> Richard, JM, Castro, DC, Difeliceantonio, AG, Robinson, MJF, &amp; Berridge, KC (2013). Mapping brain circuits of reward and motivation: In the footsteps of Ann Kelley. Neuroscience and Biobehavioral Reviews, 37(9 Pt A), 1919–1931. <a href="https://doi.org/10.1016/j.neubiorev.2012.12.008">https://doi.org/10.1016/j.neubiorev.2012.12.008</a></li><li> Robinson, MJF, &amp; Berridge, KC (2013). Instant transformation of learned repulsion into motivational &quot;wanting&quot;. Current Biology : CB, 23(4), 282–289. <a href="https://doi.org/10.1016/j.cub.2013.01.016">https://doi.org/10.1016/j.cub.2013.01.016</a></li><li> Salamone, JD, Pardo, M., Yohn, SE, López-Cruz, L., SanMiguel, N., &amp; Correa, M. (2016). Mesolimbic Dopamine and the Regulation of Motivated Behavior. In EH Simpson &amp; PD Balsam (Eds.), Behavioral Neuroscience of Motivation (pp. 231–257). Springer International Publishing. <a href="https://doi.org/10.1007/7854_2015_383">https://doi.org/10.1007/7854_2015_383</a></li><li> Smith, KS, &amp; Berridge, KC (2007). Opioid limbic circuit for reward: Interaction between hedonic hotspots of nucleus accumbens and ventral pallidum. The Journal of Neuroscience : The Official Journal of the Society for Neuroscience, 27(7), 1594–1605. <a href="https://doi.org/10.1523/JNEUROSCI.4205-06.2007">https://doi.org/10.1523/JNEUROSCI.4205-06.2007</a></li><li> Smith, KS, Berridge, KC, &amp; Aldridge, JW (2011). Disentangling pleasure from incentive salience and learning signals in brain reward circuitry. Proceedings of the National Academy of Sciences of the United States of America, 108(27), E255-264. <a href="https://doi.org/10.1073/pnas.1101920108">https://doi.org/10.1073/pnas.1101920108</a></li><li> Smith, KS, Mahler, SV, Peciña, S., &amp; Berridge, KC (2010). Hedonic Hotspots: Generating Sensory Pleasure in the Brain. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 27–49). Oxford University Press.</li><li> Söderpalm, AH, &amp; Berridge, KC (2000). The hedonic impact and intake of food are increased by midazolam microinjection in the parabrachial nucleus. Brain Research, 877(2), 288–297. <a href="https://doi.org/10.1016/s0006-8993(00)02691-3">https://doi.org/10.1016/s0006-8993(00)02691-3</a></li><li> Steiner, JE (1973). The gustofacial response: Observation on normal and anencephalic newborn infants. Symposium on Oral Sensation and Perception, 4, 254–278.</li><li> Szczypka, MS, Rainey, MA, Kim, DS, Alaynick, WA, Marck, BT, Matsumoto, AM, &amp; Palmiter, RD (1999). Feeding behavior in dopamine-deficient mice. Proceedings of the National Academy of Sciences, 96(21), 12138–12143. <a href="https://doi.org/10.1073/pnas.96.21.12138">https://doi.org/10.1073/pnas.96.21.12138</a></li><li> Warlow, SM, Naffziger, EE, &amp; Berridge, KC (2020). The central amygdala recruits mesocorticolimbic circuitry for pursuit of reward or pain. Nature Communications, 11(1), 2716. <a href="https://doi.org/10.1038/s41467-020-16407-1">https://doi.org/10.1038/s41467-020-16407-1</a></li><li> Winkielman, P., Berridge, K., &amp; Wilbarger, J. (2005). Unconscious Affective Reactions to Masked Happy Versus Angry Faces Influence Consumption Behavior and Judgments of Value. Personality &amp; Social Psychology Bulletin, 31, 121–135. <a href="https://doi.org/10.1177/0146167204271309">https://doi.org/10.1177/0146167204271309</a></li><li> Wright, GA, Baker, DD, Palmer, MJ, Stabler, D., Mustard, JA, Power, EF, Borland, AM, &amp; Stevenson, PC (2013). Caffeine in Floral Nectar Enhances a Pollinator&#39;s Memory of Reward. Science, 339(6124), 1202–1204. <a href="https://doi.org/10.1126/science.1228806">https://doi.org/10.1126/science.1228806</a></li><li> Zhang, J.-J., Song, C.-G., Dai, J.-M., Li, L., Yang, X.-M., &amp; Chen, Z.-N. (2022). Mechanism of opioid addiction and its intervention therapy: Focusing on the reward circuitry and mu-opioid receptor. MedComm, 3(3), e148. <a href="https://doi.org/10.1002/mco2.148">https://doi.org/10.1002/mco2.148</a> </li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-sieGxzsnEEMoiKqSL-1" class="footnote-item"><p> Ordered alphabetically, by last name. <a href="#fnref-sieGxzsnEEMoiKqSL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-2" class="footnote-item"><p> I italicize <em>liking</em> , <em>&quot;liking&quot;</em> , <em>wanting</em> , and <em>&quot;wanting&quot;</em> in order to emphasize that I&#39;m using these terms in a &quot;technical&quot; sense. &quot;Non-technical&quot; senses are non-italicized. In a few places of this section I also sometimes lump <em>liking</em> and <em>&quot;liking&quot;</em> into &quot;liking&quot; and <em>wanting</em> and <em>&quot;wanting&quot;</em> into &quot;wanting&quot;. <a href="#fnref-sieGxzsnEEMoiKqSL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-3" class="footnote-item"><p> Not necessarily exhaustively, there may be some (things we might want to consider as) values that don&#39;t fit neatly into any of these categories. <a href="#fnref-sieGxzsnEEMoiKqSL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-4" class="footnote-item"><p> The distinction between explicit and implicit is also sometimes used, but I find it unintuitive. <a href="#fnref-sieGxzsnEEMoiKqSL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-5" class="footnote-item"><p> By the way, the taboo against eating pork has a very interesting origin. See <a href="https://www.youtube.com/watch?v=pI0ZUhBvIx4">this video from Religion for Breakfast</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-5" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-6" class="footnote-item"><p> See also <a href="https://www.lesswrong.com/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats">Steve Byrnes&#39;s post about that study</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-7" class="footnote-item"><p> The CS itself can become aversive or desired, even when the UCS it predicts appears in a different place than the CS. In such cases, the CS is said to become a &quot;motivational magnet&quot; (eg, Robinson &amp; Berridge, 2013). <a href="#fnref-sieGxzsnEEMoiKqSL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-8" class="footnote-item"><p> Explaining this phenomenon in terms of wanting to avoid unpleasant effects of the withdrawal syndrome doesn&#39;t fit the empirical data (Berridge &amp; Robinson, 2016). <a href="#fnref-sieGxzsnEEMoiKqSL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-9" class="footnote-item"><p> While here I am speaking about subclinical cases of behavioral addictions, I also expect this to be a factor in obsessive-compulsive disorder and related conditions. One reason I think so is that the neurotransmitter most consistently involved in OCD seems to be dopamine, which is strongly implicated in wanting (see Section 3). Moreover, the most successful pharmacological treatment for OCD is naltrexone, which is also used in many standard addictions and acts by regulating dopaminergic transmission from the VTA. <a href="#fnref-sieGxzsnEEMoiKqSL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-10" class="footnote-item"><p> Although it probably still requires making some assumptions about the agent&#39;s biases and cognitive limitations. See, eg, <a href="https://www.lesswrong.com/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard">Christiano (2018)</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-11" class="footnote-item"><p> I&#39;m not going to discuss the topic of phenomenal consciousness and its relationship with verbal reports because I consider the former to be <a href="https://plato.stanford.edu/entries/qualia/#Illusional">illusory</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-12" class="footnote-item"><p> Food being obviously the easiest category of &quot;rewards&quot; to study. <a href="#fnref-sieGxzsnEEMoiKqSL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-13" class="footnote-item"><p> By &quot;conscious functioning&quot;, I mean something like &quot;the <a href="https://en.wikipedia.org/wiki/Global_workspace_theory">global workspace</a> &quot; being up and running. <a href="#fnref-sieGxzsnEEMoiKqSL-13" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-14" class="footnote-item"><p> With the neocortex being the part of the brain we expect to be important for conscious awareness. <a href="#fnref-sieGxzsnEEMoiKqSL-14" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-15" class="footnote-item"><p> From now on, I italicize <em>liking</em> , <em>&quot;liking&quot;</em> , <em>wanting</em> , and <em>&quot;wanting&quot;</em> , in order to emphasize that I&#39;m using these terms in their &quot;technical&quot; sense. &quot;Non-technical meanings&quot; of liking and liking are non-italicized. <a href="#fnref-sieGxzsnEEMoiKqSL-15" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-16" class="footnote-item"><p> Berridge and Robinson (2003) also introduced a third distinction between two forms of learning: cognitive and associative, but it is not the focus of this post. <a href="#fnref-sieGxzsnEEMoiKqSL-16" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-17" class="footnote-item"><p> I found no studies on that, but I have a very confident guess that <em>&quot;liking&quot;</em> would also occur in animals that are asleep or even in some kinds of palliative states, such as coma, perhaps even <a href="https://en.wikipedia.org/wiki/Locked-in_syndrome">locked-in syndrome</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-17" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-18" class="footnote-item"><p> Of course, the correspondence is not perfect and the boundaries between the daily meanings of &quot;to like&quot; and &quot;to want&quot; are blurry. Still, semantic distance from &quot;to like&quot; to <em>liking</em> is smaller than to <em>&quot;liking&quot;</em> . <a href="#fnref-sieGxzsnEEMoiKqSL-18" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-19" class="footnote-item"><p> Importantly, &quot;the ongoing state of affairs&quot; can include things extended over a long timescale. <a href="#fnref-sieGxzsnEEMoiKqSL-19" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-20" class="footnote-item"><p> At least none of the ones we know about, to the best of my knowledge. <a href="#fnref-sieGxzsnEEMoiKqSL-20" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-21" class="footnote-item"><p> At the same time, some studies show that monkeys and rats with OFC damage, although they still respond to rewards, are impaired in using reward information to guide their behavior, relative to animals with intact OFC (Berridge &amp; Kringelbach, 2008), perhaps pointing to the role of conscious pleasure in reward-related learning. <a href="#fnref-sieGxzsnEEMoiKqSL-21" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-22" class="footnote-item"><p> For a great introduction to GWT, see <a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq">Kaj Sotala&#39;s review of <em>The Consciousness and the Brain</em> by Stanislas Dehaene</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-22" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-23" class="footnote-item"><p> Eg, when I realize that when I get back to exercising after a long break, I start feeling much better on a daily basis after a few weeks, which increases my motivation to exercise (although this is probably not a good example of <em>&quot;wanting&quot;</em> ). <a href="#fnref-sieGxzsnEEMoiKqSL-23" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-24" class="footnote-item"><p> Perhaps I am slightly deviating from the definition of <em>&quot;wanting&quot;</em> as &quot;conditioned responses&quot;. This seems true though and in agreement with the idea (endorsed by Berridge) that the original adaptive of <em>&quot;wanting&quot;</em> was to drive the animal&#39;s behavior to satisfy some small set of needs. <a href="#fnref-sieGxzsnEEMoiKqSL-24" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-25" class="footnote-item"><p> I take the mention of &quot;pleasant&quot; in (2) to refer both to <em>&quot;liking&quot;</em> and <em>liking</em> , with the latter being used in a broad sense, which includes (ia) reflective evaluation of the state of the world conditional on having achieved the <em>wanted</em> goal. I think this interpretation is justified because otherwise, the definition would exclude clear examples of <em>wanting</em> , such as a person doing hard work for which they are not going to receive any &quot;pleasant reward&quot;, unless we take a very broad meaning of &quot;pleasure&quot; (not mentioning more extreme cases like suicide bombers and kamikaze). <a href="#fnref-sieGxzsnEEMoiKqSL-25" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-26" class="footnote-item"><p> Szczypka et al. (1999) write that &quot;young [DD] pups that had never been injected with l-DOPA would lick and swallow small drops of a liquid diet placed by their mouth. Apparently, these kinds of responses don&#39;t require dopamine, perhaps being a kind of <em>&quot;liking&quot;</em> reactions. <a href="#fnref-sieGxzsnEEMoiKqSL-26" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-27" class="footnote-item"><p> See also Oliver Sacks&#39;s <em><a href="https://en.wikipedia.org/wiki/Awakenings_(book)">Awakenings</a></em> . <a href="#fnref-sieGxzsnEEMoiKqSL-27" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-28" class="footnote-item"><p> Most <a href="https://en.wikipedia.org/wiki/Dopamine_antagonist">dopamine antagonists</a> work only on a particular subtype of dopamine receptors. <a href="#fnref-sieGxzsnEEMoiKqSL-28" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-29" class="footnote-item"><p> Interestingly, in addition to increasing DA directly, amphetamine and cocaine-like psychostimulants also appear to increase DA via an indirect route (Peña et al., 2015). <a href="#fnref-sieGxzsnEEMoiKqSL-29" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-30" class="footnote-item"><p> Alternatively, they might be <a href="https://en.wikipedia.org/wiki/Spandrel_(biology)">spandrels</a> . This probably isn&#39;t the case for <em>&quot;liking&quot;</em> , as spandrels typically (ever?) have distinct brain circuits. If <em>liking</em> is a natural consequence of <em>&quot;liking&quot;</em> plus global workspace/consciousness systems, then it would also probably not be a spandrel. <a href="#fnref-sieGxzsnEEMoiKqSL-30" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-31" class="footnote-item"><p> Dickinson and Balleine&#39;s account of the function of reward is slightly different than the one I&#39;m presenting here. You can read it yourself if you&#39;re curious. <a href="#fnref-sieGxzsnEEMoiKqSL-31" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/opJxxfrN33xQx3eXu/wanting-and-liking#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/opJxxfrN33xQx3eXu/wanting-and-liking<guid ispermalink="false"> opJxxfrN33xQx3eXu</guid><dc:creator><![CDATA[Mateusz Bagiński]]></dc:creator><pubDate> Wed, 30 Aug 2023 14:52:04 GMT</pubDate></item><item><title><![CDATA[Open Call for Research Assistants in Developmental Interpretability]]></title><description><![CDATA[Published on August 30, 2023 9:02 AM GMT<br/><br/><p> We are excited to announce multiple positions for Research Assistants to join our <a href="https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc"><u>six-month research project</u></a> assessing the viability of <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental Interpretability</u></a> (DevInterp).</p><p> This is a chance to gain expertise in interpretability, develop your skills as a researcher, build out a network of collaborators and mentors, publish in major conferences, and open a path towards future opportunities, including potential permanent roles, recommendations, and successive collaborations.</p><h2> <strong>Background</strong></h2><p> <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental interpretability</u></a> is a research agenda aiming to build tools for detecting, locating, and understanding phase transitions in learning dynamics of neural networks. It draws on techniques from singular learning theory, mechanistic interpretability, statistical physics, and developmental biology.</p><h2> <strong>Position Details</strong></h2><p> <strong>General info:</strong></p><ul><li> <strong>Title</strong> : Research Assistant / Research Engineer.</li><li> <strong>Location</strong> : Remote, with hubs in Melbourne and London.</li><li> <strong>Duration</strong> : Until March 2024 (at minimum).</li><li> <strong>Compensation</strong> : base salary is USD$35k per year, to be paid out as an independent contractor at an hourly rate.</li></ul><p> <strong>Timeline</strong> :</p><ul><li> <strong>Application Deadline</strong> : September 15th, 2023</li><li> <strong>Ideal Start Date</strong> : October 2023</li></ul><p> <strong>How to Apply</strong> : Complete the <a href="https://forms.gle/6hEpiqgN4oAHmypD6"><u>application form</u></a> by the deadline. Further information on the application process will be provided in the form.</p><h2> <strong>Who We Are</strong></h2><p> The developmental interpretability research team consists of experts across a number of areas of mathematics, physics, statistics and AI safety. The principal researchers:</p><ul><li> <a href="http://therisingsea.org/"><u>Daniel Murfet</u></a> , mathematician and SLT expert, University of Melbourne.</li><li> <a href="https://www.suswei.com/"><u>Susan Wei</u></a> , statistician and SLT expert, University of Melbourne.</li><li> <a href="https://www.jessehoogland.com/"><u>Jesse Hoogland</u></a> , MSc. Physics, SERI MATS scholar, RA in Krueger lab</li></ul><p> We have a range of projects currently underway, led by one of these principal researchers and involving a number of other PhD and MSc students from the University of Melbourne and collaborators from around the world. In an organizational capacity you would also interact with Alexander Oldenziel and Stan van Wingerden.</p><p> You can find us and the broader DevInterp research community on our <a href="https://discord.gg/pCf4UynKsc"><u>Discord</u></a> . Beyond the <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental Interpretability</u></a> research agenda, you can read our first preprint on <a href="https://arxiv.org/abs/2308.12108"><u>scalable SLT invariants</u></a> and check out the lectures from the <a href="https://www.youtube.com/@SLTSummit/videos"><u>SLT &amp; Alignment summit</u></a> .</p><h2> <strong>Overview of Projects</strong></h2><p> Here&#39;s the selection of <strong>&nbsp;</strong> the projects underway, some of which you would be expected to contribute to. These tend to be on the more <strong>experimental</strong> side:</p><ul><li> <strong>Developing scalable estimates for SLT invariants</strong> : Invariants like the (local) learning coefficient and (local) singular fluctuation can signal the presence of “hidden” phase transitions. Improving these techniques can help us better identify these transitions.</li><li> <strong>DevInterp of vision models</strong> : To what extent do the kinds of circuits studied in the <a href="https://distill.pub/2020/circuits/zoom-in/"><u>original circuits thread</u></a> emerge through phase transitions?</li><li> <strong>DevInterp of program synthesis</strong> : <strong>&nbsp;</strong> In examples where we know there is rich compositional structure, can we see it in the singularities? Practically, this means studying settings like modular arithmetic (grokking), multitask sparse parity, and more complex variants.</li><li> <strong>DevInterp of in-context learning</strong> <strong>&amp; induction heads</strong> : Is the development of induction heads a proper phase transition in the language of SLT? More ambitiously, can we apply singular learning theory to study in-context learning and make sense of “in-context phase transitions.”</li><li> <strong>DevInterp of language models</strong> : Can we detect phase transitions in simple language models (like <a href="https://arxiv.org/abs/2305.07759"><u>TinyStories</u></a> ). Can we, from these transitions, discover circuit structure? Can we extend these techniques to larger models (eg, in the <a href="https://github.com/EleutherAI/pythia"><u>Pythia suite</u></a> ).</li><li> <strong>DevInterp of reinforcement learning</strong> <strong>models</strong> : To what extent are phase transitions involved in the emergence of a world model (in examples like <a href="https://github.com/likenneth/othello_world/tree/master"><u>OthelloGPT</u></a> )?</li></ul><p> Next to these, we are working on a number of more <strong>theoretical</strong> projects. (Though our focus is on hiring for the more applied projects, if one of these particularly excites you, you should definitely apply!)</p><ul><li> <strong>Studying phase transitions in simple models like deep linear networks:</strong> These serve a valuable intermediate case between regular models and highly singular models. This is also a place to draw connections to a lot of existing literature on deep learning theory (on “saddle-to-saddle dynamics”).</li><li> <strong>Developing “geometry probes”</strong> : It&#39;s not enough to <i>detect</i> phase transitions, but we have to be able to <i>analyze</i> them for structural information. Here the aim is to develop “probes” that extract this kind of information from phase transitions.</li><li> <strong>Developing the “geometry of program synthesis”</strong> : We expect that understanding neural networks will require looking beyond models of computation like Turing machines, lambda calculus, and linear logic, towards <i>geometric</i> models of computation. This means pushing further in directions like those explored by Tom Waring in his <a href="http://therisingsea.org/notes/MSc-Waring.pdf"><u>MSc. thesis</u></a> .</li></ul><p> Taken together these projects complete the scoping phase of the DevInterp research agenda, ideally resulting in publications in venues like ICML and NeurIPS.</p><h2> <strong>What We Expect</strong></h2><p> You will be communicating with your research project teammates on the DevInterp <a href="https://discord.gg/pCf4UynKsc"><u>Discord</u></a> , writing code and training models, joining in weekly or biweekly research meetings over Zoom, and in general acting as a productive member of a fast-moving research team combining both theoreticians and experimentalists working together to define the future of the science of interpretability.</p><p> Depending on interest and background, you may also be reading and discussing papers from ML or mathematics and contributing to the writing of papers on Overleaf. It&#39;s not mandatory, but you would be invited to join in virtual research seminars like the <a href="https://metauni.org/slt/"><u>SLT seminar</u></a> at metauni or SLT reading group on the DevInterp Discord.</p><p> There will be a <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-now-for-the-devinterp-2023-fall-summit"><u>DevInterp conference</u></a> in November 2023 in Oxford, United Kingdom, and it would be great if you could attend (we will pay for your travel). There will hopefully be a second opportunity to meet the team in person between November and the end of the employment period (possibly in Melbourne, Australia).</p><h2> <strong>FAQ</strong></h2><p> <strong>Who is this for?</strong></p><p> We&#39;re looking mainly for people who can do engineering work, that is, people with software development and ML skills. It&#39;s not necessary to have a background in interpretability or AI safety, although that&#39;s a plus. Ideally you have legible output / projects that demonstrate ability as an experimentalist.</p><p> <strong>What&#39;s the time commitment?</strong></p><p> We&#39;re looking mainly for people who can commit full-time, but if you&#39;re talented and only available part-time, don&#39;t shy away from applying.</p><p> <strong>What does the compensation mean?</strong></p><p> We&#39;ve budgeted USD$70k in total to be spread across 1-4 research assistants over the next half year. By default we&#39;re expecting to pay RAs USD$17.50/hour.</p><p> <strong>Do I need to be familiar with SLT and AI alignment?</strong></p><p> No (though it&#39;s obviously a plus).</p><p> We&#39;re leaning towards taking on skilled general purpose experimentalists (without any knowledge of SLT) over less experienced programmers who know some SLT. That said, if you are a talented theorist, don&#39;t shy away from applying.</p><p> <strong>What are you waiting for?</strong></p><p> <a href="https://forms.gle/6hEpiqgN4oAHmypD6"><u>Apply now</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/DZHmEmzujfuqfxbJY/open-call-for-research-assistants-in-developmental#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DZHmEmzujfuqfxbJY/open-call-for-research-assistants-in-developmental<guid ispermalink="false"> DZHmEmzujfuqfxbJY</guid><dc:creator><![CDATA[Jesse Hoogland]]></dc:creator><pubDate> Wed, 30 Aug 2023 09:02:59 GMT</pubDate> </item><item><title><![CDATA[LTFF and EAIF are unusually funding-constrained right now]]></title><description><![CDATA[Published on August 30, 2023 1:03 AM GMT<br/><br/><h1><strong>概括</strong></h1><p>EA Funds aims to empower thoughtful individuals and small groups to carry out altruistically impactful projects - in particular, enabling and accelerating small/medium-sized projects (with grants &lt;$300K). We are looking to increase our level of independence from other actors within the EA and longtermist funding landscape and are seeking to raise ~$2.7M for the Long-Term Future Fund and ~$1.7M for the EA Infrastructure Fund (~$4.4M total) over the next six months.</p><p> <strong>Why donate to EA Funds?</strong> EA Funds is the largest funder of small projects in the longtermist and EA infrastructure spaces, and has had a solid operational track record of giving out hundreds of high-quality grants a year to individuals and small projects. We believe that we&#39;re well-placed to fill the role of a significant independent grantmaker, because of a combination of our track record, our historical role in this position, and the quality of our fund managers.</p><p> <strong>Why now?</strong> We think now is an unusually good time to donate to us, as a) we have an unexpectedly large funding shortage, b) there are <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding"><u>great projects on the margin that we can&#39;t currently fund</u></a> , and c) more stabilized funding now can give us time to try to find large individual and institutional donors to cover future funding needs.</p><p> Importantly, Open Philanthropy is no longer providing a guaranteed amount of funding to us and <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>instead will move over to a (temporary) model of matching our funds</u></a> 2:1 ($2 from them for every $1 from you, up to 3.5M from them per fund).</p><p> <strong>Where to donate:</strong> If you&#39;re interested, you can donate to either Long-Term Future Fund (LTFF) or EA Infrastructure Fund (EAIF) <a href="https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds"><u>here</u></a> . <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnyd012yijab"><sup>[1]</sup></a></p><p> Some relevant quotes from fund managers:</p><p> <strong>Oliver Habryka</strong></p><p> I think the next $1.3M in donations to the LTFF (430k pre-matching) are among the best historical grant opportunities in the time that I have been active as a grantmaker. If you are undecided between donating to us right now vs. December, my sense is now is substantially better, since I expect more and larger funders to step in by then, while we have a substantial number of time-sensitive opportunities right now that will likely go unfunded.</p><p> I myself have a bunch of reservations about the LTFF and am unsure about its future trajectory, and so haven&#39;t been fundraising publicly, and I am honestly unsure about the value of more than ~$2M, but my sense is that we have a bunch of grants in the pipeline right now that are blocked on lack of funding that I can evaluate pretty directly, and that those seem like quite solid funding opportunities to me (some of this is caused by a large number of participants of the SERI MATS program applying for funding to continue the research they started during the program, and those applications are both highly time-sensitive and of higher-than-usual quality).</p><p> <strong>Lawrence Chan</strong></p><p> “My main takeaway from [evaluating a batch of AI safety applications on LTFF] is [LTFF] could sure use an extra $2-3m in funding, I want to fund like, 1/3-1/2 of the projects I looked at.” (At the current level of funding, we&#39;re on track to fund a much lower proportion).</p><h2> <strong>Related links</strong></h2><ul><li> <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>EA Funds organizational update: Open Philanthropy matching and distancing</u></a></li><li> <a href="https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations"><u>Long-Term Future Fund: April 2023 grant recommendations</u></a></li><li> <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding"><u>What Does a Marginal Grant at LTFF Look Like?</u></a></li><li> Asya Bergal&#39;s <a href="https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund"><u>Reflections on my time on the Long-Term Future Fund</u></a></li><li> Linch Zhang&#39;s <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><u>Select examples of adverse selection in longtermist grantmaking</u></a></li></ul><h2> <strong>Our Vision</strong></h2><p> We think there is a significant shortage of independent funders in the current longtermist and EA infrastructure landscape, resulting in fewer outstanding projects receiving funding than is good for the world. Currently, the primary source of funding for these projects is Open Philanthropy, and whilst we share a lot of common ground, we think we add value in the following ways:</p><ul><li> Increasing the total grantmaking capacity within key cause areas.</li><li> Causing great projects to counterfactually happen in the world, or saving time and effort for people doing great projects who would otherwise spend significant time fundraising or waiting for grants to come in.</li><li> Supporting a set of worldviews that we find plausible and that are not currently well represented among grantmakers (though we have substantial overlap with Open Philanthropy&#39;s worldview and there is a range of views on how much we should be directly optimizing for diversification away from their perspectives).</li><li> Emphasizing contact with reality: most of our grantmakers spend most of their time trying to directly solve problems of importance within their cause area, rather than engaging in “meta” activities like grantmaking. We think this is important as grantmaking often has very poor feedback loops (particulalry longtermist grantmaking).</li><li> Provide early stage funding to allow applicants to test their fit for work and “get ready” to seek funding from other funders that specialize in larger grant sizes.</li><li> Improving the epistemic environment within EA by making it easier for smaller projects to disagree with Open Philanthropy without worrying that this will significantly reduce their chance of being funded in the future.</li><li> Helping to identify harmful projects whilst being aware of factors such as the unilateralist curse and information cascades.</li><li> Increasing the resilience, robustness and diversity of funders within EA and longtermism.</li></ul><p> Alongside the above, EA Funds has ambitions to pursue new ways of generating value by:</p><ul><li> Creating an expert-led active grant-making program to create counterfactual impactful projects (starting with longtermist information security).</li><li> Modeling and shaping community norms of transparency, integrity, and criticism to improve the epistemic environment within EA and associated communities.</li></ul><h2> <strong>Our Ask</strong></h2><p> <strong>We are looking to raise ~$4.4M from the general public</strong> to support our work over the next 6 months:</p><ol><li> ~$2.7M for the Long-Term Future Fund.<ol><li> This is ~2M above our expected 720k donations in the next 6 months.</li></ol></li><li> ~$1.7m for the EA Infrastructure Fund.<ol><li> This is ~1.3M above our expected 400k donations in the next 6 months.</li></ol></li></ol><p> This will be matched by Open Phil at a 2:1 rate ($2 from Open Phil per $1 donated to a fund) with a ceiling of a $3.5m contribution from Open Phil (per fund). You can read more about the matching <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>here</u></a> .</p><p> The EAIF and LTFF have received very generous donations from many individuals in the EA community. However, donations to the EAIF and LTFF have recently been quite low, especially relative to the quality and quantity of applications we&#39;ve had in the last year. While much of this is likely due to the FTX crash and subsequently increased funding gaps of other longtermist organizations, our guess is that this is partially due to tech stocks and crypto doing poorly in the last year (though we hope that recent market trends will bring back some donors).</p><h3> <strong>Calculation for LTFF funding gap</strong></h3><p> The LTFF has an estimated ideal dispersal rate of $1M/month, based on our <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>post-November 2022 funding bar</u></a> that Asya estimated <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn9u90kjafzyb"><sup>[2]</sup></a> from looking at the funding gaps and marginal resources within the longtermist ecosystem overall. This is $6M over the next 6 months.</p><p> I also think LTFF donors should pay $200k over the next 6 months ($400k annualized) as their “fair share” of EA Funds operational costs. So in total, LTFF would like to spend $6.2M over the next 6 months.</p><p> Caleb <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching#LTFF_funding_gap"><u>estimated</u></a> ~$700k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $2.1M in expected incoming funds, or a shortfall of $4.1M.</p><p> To cover the remaining $4.1M, we would like individual donors to contribute an additional $2M, where Open Phil will provide $2.1M of matching for the first $1.05M.</p><p> To get a sense of what projects your marginal dollars can buy, you might find it helpful to look at the <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>$5M tier of the LTFF Funding Thresholds Post</u></a> .</p><h3> <strong>Calculation for EAIF funding gap</strong></h3><p> The EAIF has an estimated ideal dispersal rate of $800k/month, based on the proportion of our historic spend rate that we believe is above Open Phil&#39;s bar for EA community building projects (though note that this was based on fairly brief input from Open Phil and I didn&#39;t check with them about whether they agree with this claim). This is $4.8M over the next 6 months.</p><p> I also think EAIF donors should pay $200k over the next 6 months ($400k annualized) as their “fair share” of EA Funds operational costs. So in total, EAIF would like to spend $5M over the next 6 months.</p><p> Caleb estimated $400k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $1.2M in expected incoming funds, or a shortfall of $3.8M.</p><p> To cover the remaining $3.8M, we would like individual donors to contribute an additional $1.3M, where Open Phil will provide 2.5M in donation matching.</p><h3> <strong>Potential change for operational expenses payment</strong></h3><p> <strong>Going forwards, we would also like to move towards a model where donors directly pay for our operational expenses</strong> (currently we fundraise for operational expenses separately, so 100% of donations from public donors goes to our grantees). We believe that the newer model is more transparent, as it lets all donors more clearly see the true costs and cost-benefit ratio for their donations <strong>. However, making the change is still pending internal discussions, community feedback, and logistical details.</strong> We will make a separate announcement if and when we switch to a model where a percentage of public donations go to cover our operational expenses. See Appendix A for a calculation of operational expenses.</p><h2> <strong>Why give to EA Funds?</strong></h2><p> We think EA Funds is well-positioned to be a significant independent grantmaker for the following reasons.</p><ol><li> We <a href="https://funds.effectivealtruism.org/team"><strong><u>have knowledgeable part-time fund managers</u></strong></a> <strong>who do direct work in their day jobs:</strong> we have built several grantmaking teams with a broad range of expertise. These managers usually dedicate the majority of their time to hands-on efforts addressing critical issues. We believe this direct experience enhances their judgment as grantmakers, enabling them to pinpoint important and critical projects with high accuracy.</li><li> <strong>Specialization in early-stage grants:</strong> we made over 300 grants of under $300k in 2022. To our knowledge, that&#39;s more grants of this size than any other EA-associated funder.</li><li> <strong>We are the largest open application funding source</strong> (that we are aware of) within our cause areas. Our application form is always open, anyone can apply, and grantees can apply for a wide variety of projects relevant to our funds&#39; purposes (as opposed to eg needing to cater to narrow requests for proposals). We believe this is critical to us having access to grant opportunities that other funders do not have access to, allowing us to rely on formal channels rather than informal networks.</li><li> <strong>Our operational track record</strong> . In 2022, EA Funds paid out ~$35M across its four Funds, with $12M to the Long-Term Future Fund, $13M to the EA Infrastructure Fund, $6.4M to the Animal Welfare Fund, and $4.8M to the Global Health and Development Fund. This requires (among others) clearing nontrivial logistical hurdles in following nonprofit law across multiple countries, consistent operational capacity, and a careful eye towards <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><u>downside risk mitigation</u></a> .</li><li> <strong>We believe our grant are highly cost-effective.</strong> Our current best guess is that we have successfully identified and given out grants of similar ex-ante quality to (eg) Open Phil&#39;s AI safety and community building grants, some of which Open Phil would counterfactually not have funded. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnsxw2bil6v9"><sup>[3]</sup></a> This gives donors an opportunity to provide considerable value.</li><li> We are investigating <strong>new value streams</strong> . We would like to pursue &#39;DARPA-style&#39; active grantmaking in priority areas (starting with information security). We are also actively considering setting up an AI Safety-specific fund, enocuraging donors interested in AI safety (but not EA or longtermism) to donate to projects that mitigate large-scale globally catastrophic AI risks.</li><li> According to GWWC, the LTFF is the <strong>main longtermist donation option</strong> available for individual donors to support. We believe that we are a relatively transparent funder, and we are currently thinking about how we can increase our transparency further whilst moving more quickly and maintaining our current standard of decision-making.</li></ol><p> We are primarily looking for funding to support the Long-Term Future Fund and the EA Infrastructure Fund&#39;s grantmaking.</p><p> The Long-Term Future Fund is primarily focused on reducing catastrophic risks from advanced artificial intelligence and biotechnology, as well as building and equipping a community of people focused on safeguarding humanity&#39;s future potential. The EA Infrastructure Fund is focused on increasing the impact of projects that use the principles of effective altruism, in particular amplifying the efforts of people who aim to do an ambitious amount of good from an impartial welfarist and scope-sensitive perspective. We have included some examples of grants each fund has made in the <a href="https://docs.google.com/document/d/1-fKZJfgybYMjfUZc2_G-WwTbkQqFBeDEtbrRW4Th_k8/edit#heading=h.6pad9vbogqm7"><u>highlighted grants</u></a> section.</p><h2> <strong>Our Fund Managers</strong></h2><p> We lean heavily on the experience and judgement of our fund managers. We have around five fund managers on each fund at any given time. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnhamlm91a3z"><sup>[4]</sup></a> Our current fund managers include:</p><ul><li> <strong>Linchuan Zhang (LTFF):</strong> Linchuan (Linch) Zhang is a Senior Researcher at Rethink Priorities working on existential security research. Before joining RP, he worked on time-sensitive forecasting projects around COVID-19. Previously, he programmed for Impossible Foods and Google and has led several EA local groups.</li><li> <strong>Oliver Habryka (LTFF)</strong> : Oliver runs Lightcone Infrastructure, whose main product is Lesswrong. Lesswrong has significantly influenced conversations around rationality and AGI risk, and the LWits community is often credited with having realized the importance of topics such as AGI (and AGI risk), COVID-19, existential risk and crypto much earlier than other comparable communities.</li><li> <strong>Peter Wildeford (EAIF):</strong> co-executive director and co-founder of <a href="https://rethinkpriorities.org/"><u>Rethink Priorities</u></a> , a think tank dedicated to figuring out the best ways to make the world a better place.</li></ul><h3> <strong>Guest Fund Managers</strong></h3><p> <strong>Daniel Eth (LTFF):</strong> Daniel&#39;s research has spanned several areas relevant to longtermism, and he&#39;s currently focused primarily on AI governance. He was previously a Senior Research Scholar at the Future of Humanity Institute. He is currently self-employed.</p><p> <strong>Lauro Langosco (LTFF):</strong> Lauro is a PhD student with David Krueger at the University of Cambridge. His work focused broadly on AI Safety, in particular on demonstrations of alignment failures, forecasting AI capabilities, and scalable AI oversight.</p><p> <strong>Lawrence Chan (LTFF):</strong> Lawrence is a researcher at <a href="https://evals.alignment.org/">ARC Evals</a> , working on safety standards for AI companies. Before joining ARC Evals, he worked at <a href="https://www.redwoodresearch.org/">Redwood Research</a> and as a PhD Student at the <a href="https://humancompatible.ai/">Center for Human Compatible AI</a> at UC Berkeley.</p><p> <strong>Thomas Larsen (LTFF):</strong> Thomas was an alignment research contractor at MIRI, and he is currently running the Center for AI Policy, where he works on AI governance research and advocacy.</p><p> <strong>Clara Collier (LTFF)</strong> : Clara is the managing editor of Asterisk, a quarterly journal focused on communicating insights on important issues. Before, she worked as an independent researcher on existential risks. She has a Masters in Modern Languages from Oxford.</p><p> <strong>Michael Aird (EAIF)</strong> : <strong>&nbsp;</strong> Michael Aird is a Senior Research Manager in Rethink Priorities&#39; AI Governance and Strategy team. He also serves as an advisor to organizations such as Training for Good and is an affiliate of the Centre for the Governance of AI. His prior work includes positions at the Center on Long-Term Risk and the Future of Humanity Institute.</p><p> <strong>Huw Thomas (EAIF):</strong> Huw is currently working part-time on various projects (including a contractor role at 80,000 hours). Prior to this, he worked as a media associate at Longview Philanthropy, a groups associate at the Centre for Effective Altruism and was a recipient of the CEA Community Building Grant for his work at Effective Altruism Oxford.</p><p> <i>You can find a full list of our fund managers</i> <a href="https://funds.effectivealtruism.org/team"><i>here</i></a> <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn3iuaaj786kj"><sup>[5]</sup></a></p><p> If you have more questions, feel free to leave a comment here. Caleb Parikh and the fund managers are also happy to talk to donors potentially willing to give >;$30k. Linch Zhang, in particular, has volunteered himself to talk about the LTFF.</p><h2> <strong>Highlighted Grants</strong></h2><p> EA Funds has identified a variety of high-impact projects, at least some of which we think are unlikely to have been funded elsewhere. (However, for any specific grant listed below, we think there&#39;s a fairly high probability they&#39;d otherwise be funded in some form or another; figuring out counterfactuals is often hard).</p><p> <strong>From the Long-Term Future Fund:</strong></p><ul><li> David Krueger - $200,000<ul><li> Computing resources and researcher stipends at a new deep learning + AI alignment research group at the University of Cambridge.</li></ul></li><li> Alignment Research Center - $72,000<ul><li> A research &amp; networking retreat for winners of the Eliciting Latent Knowledge contest with the aim of fostering promising research collaborations between junior researchers.</li></ul></li><li> SERI MATS program - $316,000<ul><li> 8-week scholars program to pair promising alignment researchers with renowned mentors. This program has now grown into a <a href="https://www.serimats.org/mentors"><u>more established program</u></a> producing multiple people working full-time on alignment in established research organizations (with a smaller number of people pursuing independent research or starting new organizations).</li></ul></li><li> Manifold Markets - $200,000<ul><li> Stipend and expenses for 4 months for 3 FTE to build a forecasting platform made available to the public based on user-created play-money prediction markets</li></ul></li><li> Daniel Filan - $23,544<ul><li> We recommended a grant of $23,544 to pay Daniel Filan for his time making 12 additional episodes of the AI X-risk Research Podcast (AXRP), as well as the costs of hosting, editing, and transcription.</li></ul></li></ul><p> <strong>From the EA Infrastructure Fund:</strong></p><ul><li> Shauna Kravec &amp; Nova DasSarma - $50,000:<ul><li> Compute infrastructure and dedicated support for AI safety researchers to run technical AI experiments. This later became <a href="http://hofvarpnir/"><u>Hofvarpnir Studios</u></a> which used to provide compute for Jacob Steinhardt&#39;s lab at UC Berkeley and the Center for Human-Compatible Artificial Intelligence (CHAI).</li></ul></li><li> Finlay Moorhouse and Luca Righetti - $38,200<ul><li> Ongoing support for <a href="https://hearthisidea.com/"><u>&quot;Hear This Idea</u></a> &quot;, a podcast showcasing new thinking in effective altruism.</li></ul></li><li> Laura Gonzalez Salmerón, Sandra Malagón - $43,308<ul><li> 12-month stipend to coordinate and grow the EA Spanish speakers community and its projects.</li></ul></li><li> Czech Association for Effective Altruism - $ 8,300<ul><li> Expenses and stipend to create a short Czech book (~130 pgs) and brochure (~20 pgs) with a good introduction to EA in digital and print formats.</li></ul></li></ul><p> <i>See a complete list of our public grants at</i> <a href="https://funds.effectivealtruism.org/grants"><i><u>this</u></i></a> <i>link. You can also read the most recent payout report by LTFF</i> <a href="https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations"><i><u>here</u></i></a> <i>.</i></p><h2> <strong>Planned actions over the next six months</strong></h2><p> To achieve our goals of empowering thoughtful people to pursue impactful projects, we&#39;ll attempt to do the following:</p><ul><li> Asya Bergal <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>will step down</u></a> as chair of LTFF (Max Daniel has already stepped down as chair of the EAIF). Max and Asya both work for Open Phil, and we want to increase our separation from Open Phil. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnpwyc17wm0ag"><sup>[6]</sup></a><ul><li> Open Phil also wanted to reduce entanglements between the two organizations, in part to mitigate downside reputational risks.</li></ul></li><li> We are looking to find new fund chairs for both LTFF and EAIF.</li><li> We plan to onboard more fund managers to grow each fund substantially (aiming to double the staffing of each fund).<ul><li> In recent months, LTFF has onboarded Lauro Langosco and Lawrence Chan who will primarily focus on technical alignment grantmaking, as well as Clara Collier for her expertise in communications and general longtermism. The EAIF is in the process of onboarding new fund managers.</li></ul></li><li> Open Phil <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>has agreed</u></a> to give us a 2:1 match for up to $7M total (up to $3.5M to each of EAIF and LTFF) for a 6-month period. While our ultimate goal is to develop our own robust funding base, in 2022, Open Philanthropy provided 40% of the funding for the Long-Term Future Fund and 84% for the EA Infrastructure Fund. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnizbbefz7rym"><sup>[7]</sup></a> We see donation matching as a realistic intermediary step while enabling us to pursue more intellectual independence.<ul><li> This model replaces fixed grants from Open Philanthropy. This reduces the likelihood of your donations being fungible: previously an extra $1 to EA Funds in fundraising could result in a $1 reduction in Open Philanthropy&#39;s grants to us, diverting those funds to their other projects. This newer approach allows funders to donate to EA Funds and support the specific value proposition that we, as opposed to Open Philanthropy, present. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnwqj0clhbbi9"><sup>[8]</sup></a></li></ul></li><li> We are considering hiring or contracting out more non-grantmaking duties (eg website, project management, fundraising, communications) at EA Funds. Right now Caleb is the only full-time employee of EA Funds and plausibly having 0.5-1.5 more FTEs at EA Funds will help both existing projects go more smoothly, as well as unlock new ambitious opportunities.</li><li> We are working with external investigators to do retroactive evaluations of past EAIF and LTFF grants, with the hopes that we can then have a clearer picture of a) how well the impact of our past grants compares to eg, Open Phil&#39;s, b) which of our broader categories of historical grants have been the most impactful, and c) other qualitative insights to help us improve further.</li><li> We aim to improve the operations of our passive grantmaking (funding of open grant applications) program with a focus on improving the grantee experience by providing more support to grantees and getting back to grantees much more quickly <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn25bgwshzbdx"><sup>[9]</sup></a></li><li> We are trying to reconceptualize and reframe the value proposition and strategic direction of EAIF in the coming months. While much of this will be contingent on the vision of the incoming fund chair, we&#39;d like EAIF to have a more coherent and targeted vision, strategy, and coherent value proposition to donors going forwards.</li><li> We plan to create a new AI Safety specific program, for donors outside of EA/Longtermism who want to decrease catastrophic risks from AI. We hope that such a program can inspire new donors to give to AI safety projects.</li><li> EA Funds is pursuing active grant-making programs, where we&#39;ll actively seek out promising projects to fund. We&#39;ll initially focus on <a href="https://80000hours.org/career-reviews/information-security/"><u>Information Security</u></a> field building. The current plan is for this program to initially be funded by Open Philanthropy, though if you are interested in contributing to this program in particular, please let us know.</li></ul><h2> <strong>Potential negatives to be aware of</strong></h2><p> Here are some reasons you might <i>not</i> want to donate to EA Funds:</p><h3> <strong>Potential downside risks of LTFF or EAIF</strong></h3><ul><li> <strong>Inability to fully screen for or prevent unilateral downside risks:</strong> EA Funds has much less control over and offers less guidance to our grantees than, eg, the executive directors of a moderately-sized EA organization. So compared to larger organizations, we may be less able to prevent unilateral downside risks like the <i>sharing of information hazards</i> , or <i>&nbsp;</i> actions that pose <i>reputational risks</i> to effective altruism at large, or to specific EA subfields.</li><li> <strong>Centralization of funds:</strong> In contrast, we are also implicitly asking for the centralization of funds from private donors to a single grantmaking entity. To the extent that you believe your counterfactual for donating to EA Funds is better and/or more centralization is bad, you may wish to donate directly rather than pool your funds with other LTFF or EAIF donors.</li><li> <strong>Waste/Inefficient usage of human capital:</strong> Giving money to EA Funds rather than larger organizations implicitly subsidizes a culture and community of grantseekers who are supported by small grants. To the extent that you believe this is a less efficient usage of human capital than plausible counterfactuals for talented people (eg getting a job in tech, policy, or academia), you might want to shift away from EA grantmakers that give relatively small individual grants.</li></ul><p> Note that we consider these issues to be structural and do not realistically expect resolutions to these downside risks going forwards.</p><h3> <strong>Areas of improvement for the LTFF and EAIF</strong></h3><p> Historically, we&#39;ve had the following (hopefully fixable) problems:</p><ul><li> <strong>Slower than ideal response times</strong> : in the past year, our median response time has been around 4 weeks with high variance; we&#39;d like to get this down to closer to 2 weeks with 95% of applications responded to in 4 four weeks.</li><li> <strong>Limited feedback/advice given to grantees</strong> : we generally don&#39;t give feedback to rejected applicants. We currently give <i>some</i> feedback to promising grantees but much less than we&#39;d give if we had more grantmaking capacity.</li><li> <strong>Insufficient active grantmaking</strong> : We spend some time trying to improve our grantees&#39; projects, but we have invested fairly little in active grantmaking (actively identifying promising projects and creating/supporting them).</li><li> <strong>Missing areas of subject matter expertise</strong> : The scopes of both funds are quite expansive. This means sometimes all of the existing grantmakers lack sufficient direct technical subject matter expertise to evaluate grants in certain areas, and thus have to rely on external experts. For example, the LTFF does not currently have a technical expert in biosecurity.</li></ul><p> For more, you can read Asya&#39;s <a href="https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund"><u>reflections on her time as chair of LTFF</u></a> .</p><h2> <strong>EAIF vs LTFF</strong></h2><p> Some donors are interested in giving to both the EAIF and LTFF and would like advice on which fund is a better fit for them.</p><p> We think that the EAIF is a better fit for donors who:</p><ul><li> Are interested in supporting a portfolio of meta projects covering a range of plausible worldviews (both longtermist and non-longtermist).</li><li> Are interested in building EA and adjacent communities.</li><li> Believe that EA (and EA community building) has historically been very good for the world.</li><li> Believe in multiplier effect arguments (donating $100 to an EA group could plausibly create far more than $100 in donation to high-impact charities by encouraging more people to donate).</li><li> Expect the EAIF and LTFF to have similar diminishing marginal returns curves and want to donate to the fund with lower funding. (EAIF and LTFF each receive about 1000 grant applications per year, but EAIF has less funding currently committed)</li></ul><p> We think that the LTFF is a better fit for donors who are:</p><ul><li> More compelled by longtermist cause areas than other EA cause areas.</li><li> Particularly interested in AI safety.</li><li> Are more interested in direct work than &quot;meta&quot; work that have a longer chain of impact/reasoning.</li><li> Are more excited about the <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>$5M tier of marginal LTFF grants</u></a> than what they consider to be the marginal EAIF grant.</li></ul><h2> <strong>Closing thoughts</strong></h2><p> This post was written by Caleb Parikh and Linch Zhang. Feel free to ask questions or give us feedback in the comments below.</p><p> If you are interested in donating to either LTFF or EAIF, you can do so <a href="https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds"><u>here</u></a> .</p><h2> <strong>Appendix A: Operational expenses calculations and transparency.</strong></h2><p> In the last year, EA Funds has dispersed $35M and spent ~700k in operational expenses. The vast majority of the operational expenses were spent on LTFF and EAIF, as the global health and development fund and animal welfare fund are operationally much simpler.</p><p> Historically, ~60-80% of the operational expenses are paid to <a href="https://ev.org/ops/about/"><u>EV Ops</u></a> , for grant disbursement, tech, legal, other ops, etc.</p><p> The remaining 20-40% is used for:</p><ul><li> Caleb&#39;s salary, who leads EA Funds (~$100k/year plus benefits).</li><li> Payments for grantmakers at $60/hour, though many volunteer for free.</li><li> Contractors who work on different projects, earning between $35-$100/hour.</li></ul><p> I (Linch) ballparked the expected annual expenditures going forwards (assuming no cutbacks) to be ~800k annually. I estimated the increase due to a) inflation and b) us wanting to take on more projects, with some savings from us slowing down the rate of dispersals a little. But this estimate is not exact.</p><p> Since LTFF and EAIF incur the highest expenses, I suggest donors to each should contribute around $400k yearly, or $200k every six months.</p><p> As for where we might cut or increase spending:</p><ul><li> Reducing EV Ops costs would be challenging and may require moving EA Funds out of EV and building our own grant ops team.</li><li> Reducing Caleb&#39;s working hours would be challenging.</li></ul><p> I think my own hours at EAF are somewhat contingent on operational funding. In the last month, I&#39;ve been spending more than half of my working hours on EA Funds (EA Funds is buying out my time at RP), mostly helping Caleb with communications and strategic direction. I will like to continue doing this until I believe EA Funds is in a good state (or we decide to discontinue or sunset projects I&#39;m involved in). Obviously whether there is enough budget to pay for my time is a crux for whether I should continue here.</p><p> Assuming we can pay for my time, other plausible uses of marginal operational funding include: a) whether we pay external investigators for extensive or just shallow retroactive evaluations, b) whether we attempt to launch new programs, c) whether the new infosec, AI safety project, etc websites have professional designers, etc. My personal view is that marginal spending on EA Funds expenses is quite impactful relative to other possible donations, but I understand if donors do not feel the same way and will prefer a higher percentage of donations go directly to our grantees (currently it&#39;s 100% but proposed changes may move this to ~ 94-97%).</p><br/><br/> <a href="https://www.lesswrong.com/posts/gRfy2Q2Pg25a2cHyY/ltff-and-eaif-are-unusually-funding-constrained-right-now#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gRfy2Q2Pg25a2cHyY/ltff-and-eaif-are-unusually-funding-constrained-right-now<guid ispermalink="false"> gRfy2Q2Pg25a2cHyY</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Wed, 30 Aug 2023 01:03:30 GMT</pubDate> </item><item><title><![CDATA[Paper Walkthrough: Automated Circuit Discovery with Arthur Conmy]]></title><description><![CDATA[Published on August 29, 2023 10:07 PM GMT<br/><br/><p> Arthur Conmy&#39;s <a href="https://arxiv.org/pdf/2304.14997.pdf">Automated Circuit Discovery</a> is a great paper that makes initial forays into automating parts of mechanistic interpretability (specifically, automatically finding a sparse subgraph for a circuit). In this three part series of Youtube videos, I interview him about the paper, and we walk through it and discuss the key results and takeaways. We discuss the high-level point of the paper and what researchers should takeaway from it, the ACDC algorithm and its key nuances, existing baselines and how they adapted them to be relevant to circuit discovery, how well the algorithm works, and how you can even evaluate how well an interpretability method works.</p><br/><br/> <a href="https://www.lesswrong.com/posts/FzqKXpTDaouMF6Chj/paper-walkthrough-automated-circuit-discovery-with-arthur#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FzqKXpTDaouMF6Chj/paper-walkthrough-automated-circuit-discovery-with-arthur<guid ispermalink="false"> FzqKXpTDaouMF6Chj</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Tue, 29 Aug 2023 22:07:06 GMT</pubDate> </item><item><title><![CDATA[An OV-Coherent Toy Model of Attention Head Superposition]]></title><description><![CDATA[Published on August 29, 2023 7:44 PM GMT<br/><br/><p> <strong>Background</strong></p><p> This project was inspired by Anthropic&#39;s <a href="https://transformer-circuits.pub/2023/may-update/index.html"><u>post</u></a> on attention head superposition, which constructed a toy model trained to learn a circuit to identify skip-trigrams that are OV-incoherent (attending from multiple destination tokens to a single source token) as a way to ensure that superposition would occur. Since the OV circuit only sees half of the information – the source tokens – the OV circuit of a single head cannot distinguish between multiple possible skip-trigrams. As long as there are more skip-trigrams with the same source-token to represent than heads, the model cannot represent them in the naive way, and may resort to superposition.<br></p><p> In a more recent update <a href="https://transformer-circuits.pub/2023/july-update/index.html"><u>post</u></a> , they found that the underlying algorithm for OV-incoherent skip-trigrams in a simpler 2-head model implemented a conditional on the source token. One head predicts the output for the skip trigram [current token] … [current token] ->; [ground truth([0]...[current token])], one of which will yield the right answer. The second head destructively interferes with this result by writing out the negative logit contribution of the first head if the source token is not the one common to all skip-trigrams (in this case, [0]). Because their example cleanly separated tasks between the two attention heads, the authors argued that it was more like the building of high-level features out of low-level ones than a feature superimposed across multiple attention heads.</p><p></p><p> <strong>OV-coherent Superposition</strong></p><p> Instead, we claim there is an analogous force pushing the model toward adopting a distributed representation/head superposition whenever the model must learn patterns that require implementing nonlinear functions of <i>multiple</i> source tokens given a fixed destination token. We call this “OV-coherent” superposition: despite of the information at the destination position being fixed, the information copied from an attended-to token depends on the information at source tokens to which it is not attending. This pushes the model to form interference patterns between heads attending to different tokens.</p><p> To test this, we implemented a 1-layer, attention-only toy model with one-hot (un)embeddings trained to solve a problem requiring attention to multiple source tokens, described below. Here, we focus on a 2-head model which solves the task with perfect accuracy, and lay out some interesting motifs for further investigation.</p><p></p><p> <i>Key Takeaways</i> :</p><p> Heads in our model seem to implement nested conditional statements that exploit the if-else nature of the QK circuits. This means they can learn to write more specific information conditional on attending to certain tokens, given that it can implicitly rule out the existence of other tokens elsewhere in the context. The heads furthermore implement these nested conditionals in such a way that they distribute important source tokens between them, and constructively interfere to produce the correct answer.</p><p> Most of the time, we found that this “conditional dependence” relies on heads implementing an “all or nothing” approach to attention. Heads do not generally* spread their attention across multiple interesting tokens, but instead move through the hierarchy of features in their QK circuits and attend to the most “interesting” (still a poorly defined term!) one present. This seems to be a common property of attention patterns in real-model heads as well.</p><p> When there are multiple important source tokens to attend to in the context, heads implementing interference schema will tend to learn QK circuits such that they distribute tokens amongst themselves and don&#39;t leave crucial information unattended to. In 2-head models, this manifests are reversed “preference orderings” over source tokens, but potentially more complicated arrangements work in larger models.</p><p></p><p> <i>Problem Details:</i></p><p> The inputs are sequences of integers in the range [0, 11]. They can be broken down into two categories:</p><ul><li> <strong>Noise:</strong> There is no pattern for the model to learn. The context and completion are drawn independently and uniformly from [5, 11].</li><li> <strong>Signal:</strong> The final token in the input is 0. Furthermore, exactly two tokens in the context are in the range [1, 4]. The correct completion is given by an arbitrary injection from (unordered) pairs of these “interesting” tokens to completions. For example, any sequence containing 1 and 4 should be followed with a 3 if the current token is 0  (“1….4 0” ->; “3”). Note these numbers are independently drawn, allowing for repetitions ( “3….3 0” ->; “7”).</li></ul><p><br> <strong>Example Model Solution: (d_head = 5, n_heads = 2, d_model = 11, no LayerNorm)</strong></p><p> <i>QK-circuit Behavior:</i></p><p> Both heads learn a strict “preference ordering” over signal tokens, with the ordering generally reversed between the two heads. This guarantees that, for any given context, both signal tokens are fully attended to. In the example below, H0 attended solely to “2” if it&#39;s in the context, else “3”, “4”, and finally “1”. H1 instead has the preference ordering “1” else “4”, “3”, and then “2”. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KicP8fBdHNjZBXxRB/q9vz4tsfeuiwdh1b0ih9"></p><p> <i>Attention scores from “0” to each signal token for each head</i></p><p><br> While this “flipped hierarchy” scheme is overwhelmingly more common, the model sometimes learned a “split attention” scheme during training, in which the heads would attend to the same tokens to varying degree. Notably, we only saw split attention for d_head &lt; 3, indicating that this may be a bottleneck issue. However, we should acknowledge that the model may have other ways of solving this simple task than the one outlined above, and indicate that this was by no means an exhaustive analysis.*</p><p><br> <i>OV-circuit Behaviour:</i></p><p> In the OV circuit, heads use the attention hierarchy described above to write to the logits of completions consistent with the tokens it attends to. For example, if H0 attends to “1”, it will positively contribute to the output logits of tokens mapped to by unordered pairs (1,1), (1,2), (1,3), and (1,4).</p><p> <i>However,</i> the heads also exploit a “conditional dependence” between source tokens; if H0 attends to token X, it “knows” that the other source position does not contain a token Y higher in its attention hierarchy than X, or else it would be attending there instead. It can then safely <i>not</i> contribute to the logits of the output mapped to by (X, Y).</p><p> We can see this clearly in the graph below, which shows the direct logit effect of each head conditional on attending to each signal token: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KicP8fBdHNjZBXxRB/ou7wkznfogikdjkhkoly"></p><p> <i>Logit contribution of heads to completions corresponding to pairs of signal tokens (x axis), conditional on attention to source token (y axis). Cross-referencing with the attention-hierarchy above shows that heads get more specific with their outputs on less-interesting tokens, and for any given pair of inputs they will constructively interfere on the correct answer</i></p><p><br> When the head attends to its “favourite” token (“2” and “1” respectively), it implicitly has no information about the other position, and so writes roughly uniformly to the logits of all possible completions. But as they run through their preferences, the heads successively write strongly to the logits of one fewer completion. For contexts that contain their “least favourite” token repeated, the heads can confidently guess the correct answer by a process of eliminating options from its own attention hierarchy.</p><p> In contexts where both heads write to multiple outputs, they will only constructively interfere on the correct token. For example, when the sequence is “3…4 0”, each head will write to multiple completions corresponding to (3,...) for one head and (4, …) for the other. However, only the correct answer – the completion corresponding to (3, 4)) – will receive positive logit contributions from both heads.</p><p><br> <strong>Observations</strong></p><p> We think this shows an interesting example of heads performing computation in superposition. Moreover, the incentive pushing the model towards interference is qualitatively very different from the OV-incoherence explored by Anthropic. Instead of needing to copy different information from a fixed source token to a variable destination token, our problem imposes the constraint that although the destination token is fixed, the information that a head would need to copy from a source token depends on information elsewhere in the context.</p><p> For n_heads = 2, the “mutual reverse ordering” between the heads is an elegant way for the model to ensure that, no matter which signal tokens show up in the context, each will be attended to.* It is plausible that real world models implement this mechanism (albeit much less crisply) to distribute important features across heads.</p><p> This conditional independent seems to us an interesting way to view the relationship between the QK and OV circuits. OV circuits can extract more information from individual tokens by learning to exploit the distribution of features of their QK circuits. One way of interpreting this might be that heads can implicitly copy information from multiple tokens even when attending to a single token. In other words: the heads can learn to write information as a function of the broader context by exploiting the fact that conditioning on their attention to a token gives information about the distribution of tokens in the entire sequence.</p><p></p><p> <strong>Future Work - Toy Models</strong></p><p> <i>Sparsity:</i></p><p> First, we did not vary sparsity or importance of signal tokens in these experiments. We have a pretty poor idea of how these variables affect the behaviours we observed here, so this seems something worth looking into.</p><p> <i>*The Split attention model, d_head bottlenecks, and the surprising resourcefulness of networks:</i></p><p> We presented our findings for d_head =5 because the learned algorithm is more human-parsible, but we were surprised by how limited we could make this model while achieving perfect accuracy on a signal-only test set. We were particularly surprised by the model&#39;s capability for d_head = 1, since this essentially limits each head to a scalar degree of freedom for each token. The split attention mechanism we stumbled upon for d_head &lt; 3 are much harder to parse, and may rely on more complicated context-specific solutions. However, the fact that we haven&#39;t seen these for larger d_head supports the idea that, for d_head >;=3, the signal tokens can be stored orthogonally in that dimension.</p><p> We were also surprised that this problem can be solved with one head, as long as d_head >;= 4. Intuitively, once a head has enough dimensions to store every &quot;interesting&quot; token orthogonally, its OV circuit can simply learn to map each of these basis vectors to the corresponding completions. Possibly there is a trade-off here between degrees of freedom in d_head and n_heads, though this is not super crisp. There is also the possibility that superposition is happening both in the n_head dimension <i>and per head</i> in the d_head dimension. In contrast with the “hierarchical” superposition presented above, we can call the latter type “bottleneck superposition”. While this complicates the picture substantially, it also opens up some pretty interesting possibilities and is something we&#39;d like to investigate more thoroughly!</p><p> <strong>Future Work - LLMs</strong></p><p> We initially set out to investigate attention head superposition “in the wild” as part of Neel Nanda&#39;s SERI MATS stream, by studying possible linear combinations of name mover heads in Redwood Research&#39;s IOI task. At first, this seemed like a good way to study a realistic (but not too realistic) problem that already had a circuit of attention heads in place. However, we quickly realized that this task represents a strange middle ground in terms of complexity: simple enough to want to represent a single feature (name moving), but complicated enough to have more features hidden within it. While even the toy model seems to have opened up some puzzling new directions, we also think it would be worthwhile to look at head superposition in real-world LLMs.</p><p> For example, it could be interesting to look for cases of inverted preference orderings in the wild by hunting for pairs of QK circuits that exhibit similar behavior. This will likely be messy, and will likely require a better understanding of the relationship between the two types of superposition mentioned above.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1<guid ispermalink="false"> cqRGZisKbpSjgaJbc</guid><dc:creator><![CDATA[LaurenGreenspan]]></dc:creator><pubDate> Tue, 29 Aug 2023 19:44:11 GMT</pubDate> </item><item><title><![CDATA[The Economics of the Asteroid Deflection Problem (Dominant Assurance Contracts)]]></title><description><![CDATA[Published on August 29, 2023 6:28 PM GMT<br/><br/><p> Imagine a world with no ads or paywalls. A world where open-source software gets the same level of funding as proprietary software. A world where people can freely reuse ideas and music without paying royalties. <a href="https://www.lesswrong.com/posts/yYvgmKGMtzKxbJZew/update-on-book-review-dominant-assurance-contract">A world where people get paid for writing book reviews.</a> A world where Game-of-Thrones-quality shows are freely available on YouTube. <i>A world where AI safety research gets the same-level of funding as AI capabilities research.</i> Is this a fantasy world? No, this is the world where people use <i>Dominant Assurance Contracts</i></p><p> If you are already convinced you can make this idea a reality by <a href="https://dac.mowzer.co.za">donating to create a Platform for Dominant Assurance Contracts.</a> If you are not convinced read on.</p><h1> The Free-rider problem</h1><p> A few months ago I stumbled across this video. (I highly recommend you watch the video, but if you don&#39;t have time, I&#39;ve summarized the video below). </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=hA2z-X31IvI"><div><iframe src="https://www.youtube.com/embed/hA2z-X31IvI" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> Summary of A Deeper Look at Public Goods</h2><p> A good is <i>rival</i> if one person&#39;s use of a good diminishes another person&#39;s ability to benefit from it. Jeans are rival. If <i>I&#39;m</i> wearing a pair of jeans, <i>you</i> can&#39;t wear it at the same time. Asteroid deflection is <i>non-rival.</i> If <i>I</i> deflect an asteroid to protect myself, <i>you</i> are saved with no additional cost.</p><p> A good is <i>excludable</i> if people who don&#39;t pay can be easily prevented from using a good. An example of a good that is excludable is a pair of jeans. You can exclude people by locking the jeans in your closet. An example of a good that is non-excludable is asteroid deflection. You cannot prevent the people who did not pay for the asteroid deflection program from benefiting from the asteroid being deflected.</p><p> A good which is both rival and excludable is called a <i>private good.</i> A good which is non-rival and non-excludable is called a <i>public good</i> .</p><p> (Additionally, goods which are excludable and non-rival are called <i>club goods,</i> and goods which are non-excludable but rival are called <i>common resources.</i> We won&#39;t be focusing on these types of goods, but I&#39;ve mentioned them for completeness)</p><figure class="table"><table><thead><tr><th></th><th> Excludable</th><th> Non-Excludable</th></tr></thead><tbody><tr><th> Rival</th><td> <i>Private Good</i></td><td> Common Resources</td></tr><tr><th> Non-Rival</th><td> Club Good</td><td> <i>Public Good</i></td></tr></tbody></table></figure><p> Markets are good at providing private goods because</p><ul><li> by excluding people who don&#39;t pay, consumers are incentivized to pay, which incentivizes producers to produce, and</li><li> since private goods are non-rival it is efficient to exclude consumers who aren&#39;t willing to pay (if the benefit to the consumer was greater than the cost, the consumer would be willing to pay).</li></ul><p> Public goods challenge markets because</p><ul><li> consumers who don&#39;t pay can&#39;t be excluded, consumers are incentivized instead to <i>free-ride</i> (ie benefit from the good without paying) and thus producers have no incentive to produce.</li><li> Additionally, even if we could figure out a way to exclude non-payers, (eg by executing everyone who doesn&#39;t pay for the asteroid deflection program), it is <i>inefficient</i> to do so (being non-rival means there are no additional costs to non-payers benefiting).</li></ul><h2> Examples of public goods</h2><ul><li> Information<ul><li> Journalism</li><li> Prediction markets</li><li> Scientific research</li><li> Educational material</li></ul></li><li> Media<ul><li> TV series</li><li>电影</li><li>YouTube videos</li><li>图书</li><li>Short-stories</li><li> Art</li><li>播客</li></ul></li><li>Software</li><li> Safety<ul><li> Neighborhood watches</li><li> Vaccines and other public health interventions</li><li> AI safety</li><li> Military defense</li></ul></li><li> Public spaces<ul><li> Public roads</li><li> Public parks</li></ul></li></ul><h2> Isn&#39;t there a clever mechanism to solve the free-rider problem?</h2><p> This video stuck with me. The fact the public goods are inefficiently provided by the market seems like the main issue with our civilization. Heck, AI Safety is a public good.</p><p> The other thing that stuck is that <i>this seems so solvable.</i> Surely, there is a clever mechanism that can fix this issue? So I went to the <a href="https://en.wikipedia.org/wiki/Free-rider_problem#Economic_and_political_solutions">Wikipedia page of the Free-rider Problem</a> and scrolled to the bottom, and lo and behold it was just sitting there: <a href="https://en.wikipedia.org/wiki/Assurance_contract#Dominant_assurance_contracts">Dominant Assurance Contracts</a> invented in 1998 by Alex Tabarrok (yes, the same person who presented the video you just watched). There is only one problem: <i>no one is using them</i> .</p><h1> What is a dominant assurance contract?</h1><p> (I recommend you watch the first 30 minutes of this video. It&#39;s really good, but if you don&#39;t have time I wrote a short explanation below that you can read instead). </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Cjxl11sAbN0"><div><iframe src="https://www.youtube.com/embed/Cjxl11sAbN0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> My short explanation of dominant assurance contracts</h2><p> Suppose there are 10 villagers on an unpaved street. Bob the Builder will pave the street for $90. Each villager is willing to contribute up to $15 to have the street paved. Additionally, there is there is a $1 transaction cost to contributing (eg from time spent signing forms, from credit card fees, from opportunity cost of not earning interest on that money, etc.).</p><p> If the Bob the Builder gets everyone to contribute, each villager will only have to pay $90 / 10 = $9 and profit $15 - $9 - $1 = $5. However, one of the villagers, Free-riding Frank, is hesitant to contribute. Consider Frank&#39;s decision table:</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (15, 4)</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> (-1, 0)</td><td> (5, 5)</td></tr></tbody></table></figure><ul><li> If <strong>nobody contributes</strong> then road doesn&#39;t get paved and Free-riding Frank and the other villagers both profit $0.</li><li> If <strong>Frank contributes</strong> <strong>but the others don&#39;t</strong> then Frank pledges $15 dollars (the maximum he is willing to pay) and incurs a transaction cost of $1. Since no one else contributed, Bob was unable to collect $90, so refunds Frank the $15. Frank, however, still incurred the $1 transaction cost.</li><li> If <strong>the others contribute but Frank doesn&#39;t</strong> then Free-riding Frank profits $15, but the others pay $90 / 9 + $1 = $ 11 and so only profit $4.</li><li> If <strong>Frank and the others all contribute</strong> then they all pay $9 dollars, incur a transaction cost of $1 and profit $5.</li></ul><p> Looking at the table and holding the others strategy fixed, Free-riding Frank realizes that the dominant strategy in this game is to not contribute (0 vs -1 and 15 vs 5). Of course, everyone else in the village realizes this too and Bob is unable to raise the funds to pave the road.</p><p> Suppose Bob changes the game to a <i>Dominant Assurance Contract.</i> Dominant<strong> </strong>Assurance Contracts have two conditions:</p><ul><li> <strong>Assurance:</strong> Bob will only build the road if all 10 villagers contribute.</li><li> <strong>Refund Bonus:</strong> Bob puts up $20 of his own money as collateral <strong>.</strong> If some villagers do not contribute then Bob will refund all villagers that did contribute what they pledged plus an equal share of the collateral: $2 per villager.</li></ul><p> Frank&#39;s decision table now looks something like this:</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (0, 1)</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> (1, 0)</td><td> (5, 5)</td></tr></tbody></table></figure><ul><li> If <strong>nobody contributes</strong> then road still doesn&#39;t get paved and Free-riding Frank and the other villagers both profit $0.</li><li> If <strong>Frank contributes</strong> <strong>but the others don&#39;t</strong> Frank pays the transaction cost of $1, but since the road was not built, the Bob refunds Frank an additional $2 [ <strong>Refund Bonus]</strong> . <i>Frank profits $1</i></li><li> If <strong>the others contribute but Frank doesn&#39;t</strong> <i>the road does not get built</i> [ <strong>Assurance]</strong> . The other villagers incur $1 transaction costs and get an additional $2 refund [ <strong>Refund Bonus]</strong> . They profit $1</li><li> If <strong>Frank and the others all contribute</strong> then they all pay $9 dollars and a transaction cost of $1, the road gets built and everyone profits $5.</li></ul><p> Looking at the table and holding the others strategy fixed, Free-riding Frank now sees that <i>the dominant strategy in this game is to contribute</i> (0 vs 1 and 0 vs 5). Of course, everyone else in the village realizes this too and the road gets paved!</p><h1> Challenges of dominant assurance contracts</h1><h2> If dominant assurance contracts are so great, why is nobody using them?</h2><p> The problem is that the producers of public goods don&#39;t know about them.</p><p> Example: Element is building Matrix, an open-standards chat app (ie a public good). Their business model is to give away the code/standards for free and sell consulting services. Unfortunately, their competitors have undercut them by selling consulting services without contributing back to the code/standards. <a href="https://news.ycombinator.com/item?id=33752467">On HackerNews you can see their CEO complain about it as &quot;The Tragedy of the Commons&quot;.</a> Except he is not experiencing the tragedy of the commons. He is experiencing the free-rider problem.</p><p> If he doesn&#39;t even know what problem he has, he is not going to be able to: google &quot;the free rider problem&quot;; open the Wikipedia page on &quot;the free-rider problem&quot;; scroll to the bottom to find the section titled &quot;Solutions&quot;; and then read about dominant assurance contracts.</p><p> People who are trying to produce public goods don&#39;t seem to know that that&#39;s what they are doing, and that dominant assurance contracts are a way to get funding.</p><p> My main goal is to give the idea enough airtime so that when people want to produce public goods, the first tool they reach for is a dominant assurance contract.</p><h2> Do they work in practice?</h2><p> <a href="https://mason.gmu.edu/~atabarro/BetterCrowdfunding.pdf">Research done in a lab shows that dominant assurance contracts reduce failure rate of assurance contracts from 60% to 40%</a> .</p><p> I only know of four attempts done in the wild (tell me if you know about more).</p><ul><li> <a href="https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html">Donation to The Center for Election Science</a> .<ul><li> Succeeded</li><li> Raised $300 in donations from 20 people.</li><li> Some information was publicly provided about the number of people who pledged before the deadline.</li><li> Final 3 pledges came in the last half hour.</li></ul></li><li> <a href="https://forum.effectivealtruism.org/posts/gFLndH6FFLbvw5uDh/update-on-book-review-dominant-assurance-contract">Book Review by Arjun Panickssery</a> .<ul><li> Failed</li><li> 9/10 or $225/250</li><li> No information publicly provided about the number of people who pledged before the deadline.</li><li> This one failed, but got really close 9/10 or $225/$250 for writing a book review! I want to emphasize how ridiculous it is that it got this close. 9 people were willing to pay $25 for a <a href="https://www.amazon.com/Sense-Style-Thinking-Persons-Writing/dp/0143127799">book review of a book that only costs $18!</a> I think if they used a platform like Kickstarter, a kind soul would have seen that they were only $25 dollars away from succeeding and would have donated.</li></ul></li><li> <a href="https://manifold.markets/Austin/kickstart-will-the-manifold-communi">Dark Mode for manifold.markets by Austin</a> .<ul><li> Failed(?)</li><li> Tried to raise $2500 of play money.</li><li> Pledges were publicly visible</li><li> I&#39;m not sure how to count this one. They failed to raise $2500 worth of <i>play</i> money but then <i>they implemented dark mode anyway.</i> So the public good was provided, even though the contract failed.</li></ul></li><li> <a href="https://www.lesswrong.com/posts/nwjTPtbvcJeA6xDuu/dominant-assurance-contract-experiment-2-berkeley-house">Berkeley House Dinners by Arjun Panickssery</a> .<ul><li> Failed</li><li> Asked for $700</li><li> Pledges were not publicized before the deadline.</li><li> Asked for $700 dollars to fund a house dinner. Arjun hasn&#39;t done a write-up of the result, but he failed. Also the in the first few days after publication the payment links weren&#39;t working.</li></ul></li></ul><p> 1/4 doesn&#39;t seem so good. Dominant assurance contracts work in theory so what is going on?</p><h2> Main problems ignored by theory</h2><p> The theoretical model of Dominant Assurance Contracts assumes away some things that you have to deal with in the real word</p><ul><li> <strong>Perfect information about pricing:</strong> In the example problem above, we assumed that 10 villagers were willing to pay $15 to pave the road. In real life you do not have that information and risk over-pricing/under-pricing your contract. Presumably the contracts run in the real world that failed were over-priced.</li><li> <strong>Perfect marketing:</strong> In the example problem above, we assumed that villagers who would have benefited knew that Bob the Builder was raising funds. In real life you have to market to potential customers and many potential customers will probably not know about your product. It&#39;s possible that the contracts run in the real world failed to reach all potential customers who would have benefited.</li><li> <strong>Game theory/psychology:</strong> In the example above Free-riding Frank was fully aware of the other villagers strategy. In reality we don&#39;t know how other people are going to behave.</li><li> <strong>Shady:</strong> Dominant assurance contracts seems gambling-like/scam-like the same way that crypto-currency does. <a href=" https://www.lesswrong.com/posts/nwjTPtbvcJeA6xDuu/dominant-assurance-contract-experiment-2-berkeley-house?commentId=EKaFMS5xv5cyvmSxn">The top comment on the Berkeley House Dinner Project</a> is &quot;this is a dollar-auction (AKA scam)&quot;, and the second top comment is &quot;Surely this is illegal?&quot;</li></ul><h3> Solution to pricing problem</h3><p> I believe the pricing problem can be solved by prediction markets. Book Review, Dark Mode and Berkeley House Dinner all had prediction markets on whether they&#39;d reach their target. The markets were at 90%, 10%, 45%. These probabilities seem consistent with how much money each of the projects raised.</p><p> Before we launch a project we could run a multiple-choice prediction market on <a href="https://manifold.markets">manifold</a> : &quot;The Project will raise $X&quot;, &quot;The Project will not raise $X&quot;, &quot;The Project will not launch or will launch with a price different from $X&quot;. Only if &quot;The Project will raise $X&quot; is sufficiently high, will we launch the project.</p><h3> Solution to marketing problem</h3><p> This problem is not unique to dominant assurance contracts. So it can be solved in the same way everyone else does it.</p><p> A cool idea is to partner with someone with expertise in marketing. The advertiser can put up the collateral for the refund bonus. If the project succeeds the producer can give them a cut of the funds raised. In this way, the advertiser is incentivized to make the project to succeed.</p><h3> Solution to game theory/psychology</h3><p> I think having a progress bar showing how many people have pledged so far is <i>incredibly</i> important to reach the target.</p><ul><li> If the project <strong>is not on track</strong> to reaching it&#39;s goal, speculators will pledge in order to claim the refund bonus.</li><li> If the project <strong>is</strong> <strong>on track</strong> to reaching it&#39;s goal, prosocial people are likely to pledge so that the project gets funded.</li><li> If the project is <strong>close to reaching it&#39;s goal</strong> , but time is running out. Potential free-riders will wait until the very last moment, but eventually concede and pledge their money.</li></ul><p> In some sense, with a progress bar, the refund bonus rewards people for providing information on whether the project is likely to reach it&#39;s funding goal.</p><p> I suspect the Book Review project would have worked if there was a progress bar.</p><h3> Solution to looking like a scam.</h3><p> Personally I think framing it as &quot;You get a Refund Bonus as gratitude for supporting our project which we are very sorry we are unable to deliver&quot; rather than &quot;If you pledge you could win $5!!!!! Buy now!!!!&quot; can make it seem less like a scam.</p><p> Having a progress bar also makes it seem less like a scam since it makes it easier for people to predict if the project is going to succeed or fail.</p><h3>概括</h3><p>If you price your project correctly (with the help of prediction markets), market it successfully, phrase the refund bonus in a way that it does not seem like a scam and have a progress bar with a list of people who&#39;ve already pledged, then it&#39;s very likely that it will reach it&#39;s target because:</p><ul><li> it&#39;s rational for people to pledge, furthermore,</li><li> the refund bonus rewards people for providing information on whether the project will reach it&#39;s funding goal.</li></ul><h2> Other potential problems</h2><ul><li> <strong>Fraud:</strong> It&#39;s possible that the project is just a scam, and that the producer disappears with the money.</li><li> <strong>Information asymmetry:</strong> It&#39;s possible that the producer is unable to deliver the product because of incompetence or produces a product of lower quality than customers expected.</li><li> <strong>Collateral:</strong> The need for collateral that producers need to put up makes risk-averse producers less likely to try, cancelling out the increase in the success rate.<ul><li> Potential solution: If we can get success rate close to 100% then the platform can put up the collateral.</li><li> Potential solution: Entrepreneurs will be willing to invest if they think they can make a profit (ie someone who is risk tolerant can underwrite the collateral).</li></ul></li><li> <strong>Exclusion: &quot;</strong> Won&#39;t people just fund their projects with dominant assurance contracts and then bundle ads or exclude non-payers (eg with restrictive copyright) anyway to maximize revenue (eg cable TV with ads)?&quot;:<ul><li> Possibly, I think our job is to create a culture where we release things for free and without ads, similar to the current culture in open source software.</li><li> If dominant assurance contract become popular there is less of a case for patents and copyright so it will be easier to convince policymakers that they should no longer exist.</li></ul></li><li> <strong>Fees:</strong> The happy-path of the financial system is that money flows from consumers to produces. With a refund bonus, money can flow from producers to consumers. The infrastructure for dealing with this use-case is poor and expensive.</li></ul><h1> Okay so what am I buying?</h1><p> <a href="https://dac.mowzer.co.za">You&#39;re collectively paying for $629 dollars for 1 month of my time to make this idea a reality.</a> (I don&#39;t live in the Bay Area so my cost of living is low). I don&#39;t really <i>need</i> the money. I&#39;m doing this as a experiment to test that dominant assurance contracts work. I&#39;ll likely ask for additional funding in the future to implement more features after the first month, and for specific expenses as they come up.</p><p> The plan is to create a website where public-good producers can create a page that</p><ul><li> Has a description of the project.</li><li> Has a progress bar showing how much and who have pledged.</li><li> Handles payments with PayPal</li><li> If the project doesn&#39;t reach the funding goal, the customers are automatically refunded with a refund bonus. If the project does reach it&#39;s goal, the producer gets the money in their PayPal account.</li><li> The producer will put up the refund bonus as collateral using PayPal.</li></ul><p> The website is essentially already done (my prototype just has my project hard-coded). I just need to flesh it out so that other people can upload their projects.</p><p> The main thing I&#39;m going to doing is looking for people who want to create public goods and getting feedback from them on the platform (if that&#39;s you please join the <a href="https://discord.gg/KGeCTx33g">Refund Bonus Discord</a> or <a href="https://dac.mowzer.co.za/#contact">contact me in some other way</a> )</p><p> Additionally I want to try some cool things which might not work out like</p><ul><li> Use prediction markets to price the project.</li><li> Bring in investors/advertisers who will put up the collateral on behalf of the producers and take a cut if the project succeeds.</li></ul><h2> But why can&#39;t I just let other people pay and free-ride?</h2><p> You haven&#39;t been paying attention.<strong> </strong>Unless I&#39;ve priced this contract wrong, if you don&#39;t pay it doesn&#39;t happen. If you don&#39;t pay, I don&#39;t meet the goal, everyone gets refunded, I go back to my boring day job, and we continue to live in a world where public goods are under-provisioned.</p><h3> But won&#39;t someone else do this?</h3><p> It seems unlikely. The Dominant Assurance Contract paper came out in 1998. And 25 years later, I seem to be the first person to commit to getting it off the ground. You will probably have to wait a while before someone else comes.</p><h3> The final call to action</h3><p> If you want to live in a world with no ads or paywalls; a world where open-source software gets the same level of funding as proprietary software; a world where people can freely reuse ideas and music without paying royalties; a <a href="https://www.lesswrong.com/posts/yYvgmKGMtzKxbJZew/update-on-book-review-dominant-assurance-contract">world where people get payed for writing book reviews</a> ; a world where Game-of-Thrones-quality shows are freely available on Youtube; a <i>world where AI safety research gets the same-level of funding as AI capabilities research,</i> <a href="https://dac.mowzer.co.za/">then please pledge some money towards this.</a></p><p> Also please join the <a href="https://discord.gg/KGeCTx33g">Refund Bonus Discord</a> or <a href="https://dac.mowzer.co.za/#contact">contact me in some other way</a> if your interested in:</p><ul><li> Finding people who want to make public goods, putting up collateral on their behalf, marketing their project, and taking a cut if they reach their funding goal.</li><li> Receiving funding for producing public goods. Even if you don&#39;t want funding, but already produce public goods I&#39;m still interested to talk.</li><li> Buying public goods on the platform.</li></ul><h1> Isn&#39;t this just Kickstarter?</h1><p> Yes, but <a href="https://www.kickstarter.com/help/stats">60% of Kickstarters fail</a> . That is very bad odds. <a href="https://mason.gmu.edu/~atabarro/BetterCrowdfunding.pdf">Research done in a lab shows that dominant assurance contracts reduce failure rate from 60% to 40%</a> . I think this area is <i>very</i> neglected compared to the upside.</p><h2> &quot;Don&#39;t Contribute&quot; is still an equilibrium on Kickstarter</h2><p> If we go back to our earlier example, without a refund bonus, due to transaction costs, both &quot;Don&#39;t Contribute&quot; and &quot;Contribute&quot; are both equilibriums (0 vs -1 and 0 vs 5). The refund bonus removes &quot;Don&#39;t Contribute&quot; from the equilibrium.</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (0, <strong>-1</strong> )</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> ( <strong>-1</strong> , 0)</td><td> (5, 5)</td></tr></tbody></table></figure><h2> Sure, but this is just a small improvement on Kickstarter. It doesn&#39;t seem revolutionary.</h2><p> I don&#39;t think enough people are working on this problem. Alex Tabarrok publish his paper on dominant assurance contracts in 1998. Kickstarter was founded in 2009, 11 years later. To this day their are no platforms for dominant assurance contracts to fund public goods.</p><p> Most things funded on Kickstarter aren&#39;t even public goods! <a href="https://en.wikipedia.org/wiki/Kickstarter">Their stated mission is to &quot;help bring creative projects to life&quot;.</a> They are even trying to solve the free-rider problem! They just happen to be using a similar mechanism.</p><p> I agree dominant assurance contracts are a small improvement, but small improvements add up and <a href="https://intelligence.org/2023/02/03/focus-on-the-places-where-you-feel-shocked-everyones-dropping-the-ball/">almost nobody is working on this!</a></p><h2> But why aren&#39;t people using Kickstarter to fund public goods right now?</h2><p> They are. Here are some successful open-source projects funded by Kickstarter:</p><ul><li> <a href="https://www.kickstarter.com/projects/1681258897/its-magit-the-magical-git-client">https://www.kickstarter.com/projects/1681258897/its-magit-the-magical-git-client</a></li><li> <a href="https://www.kickstarter.com/projects/krita/krita-2016-lets-make-text-and-vector-art-awesome">https://www.kickstarter.com/projects/krita/krita-2016-lets-make-text-and-vector-art-awesome</a></li></ul><p> Here is an unsuccessful project</p><ul><li> <a href="https://www.kickstarter.com/projects/alecaddd/akira-the-linux-design-tool">https://www.kickstarter.com/projects/alecaddd/akira-the-linux-design-tool</a></li></ul><p> Akira got 38% of their target funding. I&#39;d be willing to bet that if they use dominant assurance contracts they would have succeeded.</p><p> Apparently <a href="https://www.kickstarter.com/help/stats">79% of projects that raised more than 20% of their goal were successfully funded</a> . The refund bonus provides the necessary kick (hehe) that gets <i>twice</i> as many projects over the line.</p><p> Again, I think the main problem is that people don&#39;t know that assurance contracts (dominant or otherwise) can be used to fund public goods.</p><h2> But you have no moat. Kickstarter will just copy you.</h2><p>是的！ I&#39;m not trying to make a bazillion dollars. I just want more public goods! If Kickstarter copies this idea that would be great! Less work for me!</p><h1> Wait, you said something about AI Safety but this doesn&#39;t apply because...</h1><p> <i>...  AI Capabilities just scale with compute, where as we don&#39;t know how to scale AI Safety. The problem is not funding, and even if it there was a dominant assurance contract for &quot;$1 000 000 000 to solve the alignment problem&quot; there is no guarantee that whoever gets the money will solve the alignment problem.</i></p><p> Yes, there is a information problem when funding research things like AI Safety. We don&#39;t have the information on whether the researcher&#39;s research will be any good. But guess what: <i>information is a public good!</i> That means that it&#39;s currently underfunded. Dominant assurance contracts can bridge that funding gap.</p><p> How do we fund information? Of the top of my head a bunch of things come to mind</p><ul><li> We can fund prizes that will be given to someone who answers a particular research question.</li><li> We can fund someone to do an analysis of which researchers or research directions seem most promising.</li><li> We can fund prediction markets on something like &quot;P(doom|give Bob $100,000)&quot; vs &quot;P(doom|don&#39;t give Bob $100,000)&quot;.</li></ul><h1> Alternatives and their problems </h1><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ZvgFTxhQw1s"><div><iframe src="https://www.youtube.com/embed/ZvgFTxhQw1s" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> Of course, there are other ways to fund public goods, but they all have their own problems.</p><h2> Ads</h2><p> Probably the main method to privately produce public goods is by running ads on goods you give away for free. The main issue with this is that ads pay a fraction of a cent per view. That means that you can only fund low-quality/cheap goods. To see this, compare videos on YouTube with series on HBO.</p><h2> Taxes</h2><p> With taxes you face a symmetric problem to the free-rider problem: <i>the forced-rider problem.</i> Think of someone who doesn&#39;t use a car, but still pays taxes to maintain the roads.</p><p> Funding public good through taxes also suffer from lack of market-pricing mechanism. Government spending doesn&#39;t have to pass the market test that a dominant assurance contract has to pass, creating wasteful spending.</p><h2> <a href="https://forum.effectivealtruism.org/topics/certificate-of-impact">Impact Certificates</a></h2><p> Impact certificates suffer from the free-rider problem. There isn&#39;t any incentive (other than altruism) to buy an impact certificate. One can just free-ride on the results of the impact project, and let the initial investor go bankrupt.</p><p> I think impact certificates are trying to solve the problem of information asymmetry rather than funding. It&#39;s possible that they could be combined with dominant assurance contracts in some way, but I&#39;m not sure how.</p><h2> Club Goods</h2><p> A Club Good is a good that is non-rival but excludable. An example would be a subscription to HBO or Microsoft Office. It costs practically nothing on the margin to give an additional person access to HBO or Microsoft Office (all the costs were upfront in production) so these goods are non-rival. However, since HBO and Microsoft charge for these goods they are excludable.</p><p> Of course, there is piracy. So the first problem is that it&#39;s actually difficult to turn a public good into a club good, and so the free-rider problem persists.</p><p> Secondly, it&#39;s inefficient to exclude people. If you would pay $5 to watch Game of Thrones but the HBO subscription is $15, then $5 of surplus is being lost (it costs HBO $0 to let you watch Game of Thrones).</p><h3> Patreon</h3><p> This is a type of club good.</p><p> From personal experience, I think this works okay for art where the patrons have a parasocial relationship with the artist. This seems to work less well for impersonal stuff like software.</p><p> Additionally, it&#39;s easy for Buccaneer Bob to sign up to your Patreon and then upload your products to a piracy website. Free-riding Frank won&#39;t bother to sign up to your Patreon, he&#39;ll just download your products off a piracy website.</p><p> If you price a dominant assurance contract correctly, Free-rider Frank is forced to pay.</p><h2> Micropayments</h2><p> The idea with micropayments is something like &quot;cost per marginal unit is a fraction of a cent so we should make it possible for people to pay fractions of cents.&quot; I think this doesn&#39;t work because the cost of fraud is higher than a fraction of a cent. Also, the cost of me thinking &quot;Is this worth 0.01c?&quot; is higher than a fraction of a cent. It&#39;s just better to treat such things as non-rival.</p><h2> Assurance Contracts/Kickstarter</h2><p> The main issue with an assurance contract without a refund bonus is that &quot;not contributing&quot; is an equilibrium, especially if their are substantial transaction costs. I already explained this above.</p><h1>概括</h1><p>Public goods are under-provisioned by the market due to the free-rider problem. Dominant assurance contracts solve the free-rider problem. Nobody is using dominant assurance contracts. To solve this problem, and simultaneously test if dominant assurance contracts work, <a href="https://dac.mowzer.co.za">I created a dominant assurance contract to fund a platform for creating dominant assurance contracts.</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/CwgHX9tbfASqxjpsc/the-economics-of-the-asteroid-deflection-problem-dominant#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CwgHX9tbfASqxjpsc/the-economics-of-the-asteroid-deflection-problem-dominant<guid ispermalink="false"> CwgHX9tbfASqxjpsc</guid><dc:creator><![CDATA[moyamo]]></dc:creator><pubDate> Tue, 29 Aug 2023 18:28:54 GMT</pubDate> </item><item><title><![CDATA[The Epistemic Authority of Deep Learning Pioneers]]></title><description><![CDATA[Published on August 29, 2023 6:14 PM GMT<br/><br/><h2>介绍</h2><p>In response to recent advances in machine learning and the subsequent public outcry over the safety of AI systems, deep learning pioneers Yann LeCun, Yoshua Bengio, and Geoffrey Hinton (henceforth “the pioneers”) have spoken up about what they perceive to be the likely dangers as AI progress continues. Hinton and Bengio have advocated for AI safety in the same vein as the alignment community, while LeCun staunchly remains unconvinced that AI could pose an existential risk to humanity. The actions of Hinton and Bengio have lent credence to the alignment community as a whole, as one of the main criticisms of the movement had been that few modern machine learning experts endorsed it. Anecdotally, a friend of mine who had previously been unconvinced by the alignment community&#39;s arguments was swayed by Hinton&#39;s words this past May.</p><p> This context raises the central question of this post: how much credence should one lend to the opinions of these deep learning pioneers on AI risk over other classes of experts? Was it epistemically sound for my aforementioned friend to shift his views so drastically due to the input of a single expert? And more specifically, should we expect the intuitions and knowledge of experimentally successful AI pioneers to generalize to predicting outcomes of artificial superintelligence?</p><h2> Background</h2><p> <strong>Geoffrey Hinton</strong> received a PhD in artificial intelligence from the University of Edinburgh in 1978. The history of the backpropagation algorithm (the central optimization tool in deep learning) is sort of murky but it&#39;s generally agreed that Hinton, along with David Rumelhart, helped to popularize it in papers during the late 1980s. Hinton has primarily worked at the University of Toronto, where he was most notably a co-author on the 2012 AlexNet paper that kicked off the current boom in deep learning. He also co-authored papers on dropout and layer normalization, common regularization techniques in deep learning (both are used in the GPT family of models).</p><p> After completing a PhD in computer science from McGill University, <strong>Yoshua Bengio</strong> worked as a postdoc at Bell Labs where he assisted Yann LeCun on using backpropagation with convolutional neural networks to implement a handwritten check processor, one of the first big breakthroughs for neural networks. As a professor at the University of Montreal, he was the PI on the original GAN paper and introduced the attention mechanism to language translation tasks, paving the way for the first transformer paper.</p><p> <strong>Yann LeCun</strong> received his PhD from Sorbonne University in 1987 and joined Hinton&#39;s research group as a postdoc afterwards. He then joined Bell Labs to lead the aforementioned handwriting recognition project and pioneered an early sparsity effort called optimal brain damage. In 2003, LeCun left Bell Labs to become a professor at NYU, where he continued to work on computer vision, specifically within robotics. In 2013 LeCun was appointed as head of research at Facebook AI.</p><h2> Evaluation</h2><p> One reason to lend credence to these pioneers is their knowledge base within deep learning, having worked in the field for 30-40 years each. However, this only lends them as much credence as any other deep learning <i>expert</i> , and for the details of modern state-of-the-art models they fall behind the researchers actually building such models. The <a href="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf">SimCLR paper</a> with Hinton as PI is the only big paper that any of the pioneers have authored in the past five years. This provides us a baseline–the pioneers should <i>at least</i> be provided credence as general deep learning experts, though not credence as cutting-edge experimentalists. <span class="footnote-reference" role="doc-noteref" id="fnref94ifh88hjgd"><sup><a href="#fn94ifh88hjgd">[1]</a></sup></span></p><p> It&#39;s also important to note that among the broader class of AI researchers, the pioneers are some of the only researchers that actually got anywhere experimentally. When the pioneers began their careers, symbolic AI was still the dominant paradigm, and their intuitions in this domain were what guided them towards neural networks. From a<a href="https://www.technologyreview.com/2023/05/02/1072528/geoffrey-hinton-google-why-scared-ai/">2023 interview</a> , “&#39;My father was a biologist, so I was thinking in biological terms,&#39; says Hinton. &#39;And symbolic reasoning is clearly not at the core of biological intelligence.&#39;” Furthermore, they stuck with deep learning through a dry period from the late 1990s to the mid 2000s where there were very few developments in the field. On this side of the matter, the pioneers should be provided credence as “AI researchers whose predictions were accurate.” <span class="footnote-reference" role="doc-noteref" id="fnrefugsnn3qr13"><sup><a href="#fnugsnn3qr13">[2]</a></sup></span></p><p> However, the pioneers&#39; intuitions might still be misguided, as it seems their initial inclination to work with neural networks was motivated for the wrong reasons: the efficacy of neural networks (probably) comes not from their nominal similarity to biological brains but rather the richness of high-dimensional representations. This wasn&#39;t well-understood when the pioneers entered the field; at the time, neural network approaches were rooted in cognitive science and neuroscience. While some concepts in deep learning have biological analogues, like convolutional layers, other tools like backpropagation bear little similarity to how our brains learn. Therefore, one should be wary not to award the pioneers too much credit for the later successes of neural networks.</p><p> And like pretty much everyone else, the pioneers did not predict the acceleration and course of AI progress within the last 3-5 years. While this does diminish their epistemic authority, it simultaneously reduces the credence one should have in any source trying to predict what the course of AI progress will look like. There are no real experts in this domain, at least not to the same level as the physicists predicting the capabilities of the atomic bomb, or even climatologists forecasting the rate at which the Earth will warm.</p><p> Thus, when looking to update one&#39;s credence on AI safety by deferring to expert judgment, <strong>one should weigh the input of the three deep learning pioneers more than most other sources, but in general the weight placed on any individual expert should be less for AI x-risk forecasting than most other problems</strong> . The friend mentioned at the beginning should not have updated so steeply, but it&#39;s understandable why he (and the general public) would, because analogous experts in other domains carry much more epistemic authority. </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn94ifh88hjgd"> <span class="footnote-back-link"><sup><strong><a href="#fnref94ifh88hjgd">^</a></strong></sup></span><div class="footnote-content"><p> Relatedly, one should have very low prior credence in schemes where some lauded AI researcher comes up with some grand unified scheme behind neural networks/intelligence as a whole, like LeCun&#39;s I-JEPA. The older generation of deep learning researchers, like the pioneers, Jurgen Schmidhuber, etc., haven&#39;t made any monumental discoveries on the cutting edge for quite some time so it&#39;s unclear why their approaches should be lent substantial credence given the magnitude of the claims about these systems.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnugsnn3qr13"> <span class="footnote-back-link"><sup><strong><a href="#fnrefugsnn3qr13">^</a></strong></sup></span><div class="footnote-content"><p> Using similar logic, one should <i>a priori</i> discount the claims of non-DL AI researchers like Judea Pearl ( <a href="https://twitter.com/yudapearl/status/1695509566015082894">pro-risk</a> ) and Melanie Mitchell ( <a href="https://www.youtube.com/watch?v=qLVTc1IUHPE">anti-risk</a> ) relative to the pioneers, since Pearl and Mitchell&#39;s approaches to AI (Bayesian networks and genetic algorithms, respectively) haven&#39;t gotten anywhere near the capabilities of deep learning. It&#39;s also doubtful that these approaches are compute-limited in the same way that neural networks were for ~15 years, so it&#39;s unlikely that we see large capabilities developments in the same way that we saw a jump for deep learning.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/nhg2bXNqBSFivkjfa/the-epistemic-authority-of-deep-learning-pioneers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nhg2bXNqBSFivkjfa/the-epistemic-authority-of-deep-learning-pioneers<guid ispermalink="false"> nhg2bXNqBSFivkjfa</guid><dc:creator><![CDATA[Dylan Bowman]]></dc:creator><pubDate> Tue, 29 Aug 2023 18:43:15 GMT</pubDate></item></channel></rss>