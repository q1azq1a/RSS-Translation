<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 23 日星期三 18:12:01 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Which paths to powerful AI should be boosted?]]></title><description><![CDATA[Published on August 23, 2023 4:00 PM GMT<br/><br/><p>一些人工智能安全方法/机制可以附加到多种人工智能系统上。但单独来看，一些通向强大人工智能的<i>途径</i>比其他途径更安全或更容易调整：</p><ul><li>也许全脑模拟比从头人工智能更安全</li><li>也许前馈系统比循环系统更安全</li><li>也许<a href="https://www.alignmentforum.org/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model">LM 代理（具有复杂支架的基于 LM 的系统）比类似强大的基础模型更安全</a><span class="footnote-reference" role="doc-noteref" id="fnref6rjf2nfzpan"><sup><a href="#fn6rjf2nfzpan">[1]</a></sup></span></li><li>也许<a href="https://ought.org/updates/2022-04-06-process">基于流程的系统比基于结果的系统更安全</a></li></ul><p>在强大的从头人工智能面前，WBE 似乎不太可能出现。但其他相对安全的路径可能具有竞争力（即相对于不安全的路径不需要太多的额外成本和能力牺牲）。这具有重要的意义——这意味着人工智能开发人员应该优先考虑这些路径，特别是应该以差异化的方式发布关于这些路径的研究，以差异化地促进这些路径上的其他人。 <span class="footnote-reference" role="doc-noteref" id="fnrefn81ovsk6tz"><sup><a href="#fnn81ovsk6tz">[2]</a></sup></span></p><p>哪些通往强大人工智能的途径相对安全且具有潜在竞争力，因此应该予以推动？</p><p>这个问题是<a href="/posts/Djgws2Moi7Zefkj5y/which-possible-ai-systems-are-relatively-safe">“哪些可能的人工智能系统相对安全？”的</a>更集中的后续问题。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn6rjf2nfzpan"> <span class="footnote-back-link"><sup><strong><a href="#fnref6rjf2nfzpan">^</a></strong></sup></span><div class="footnote-content"><p> Paul <a href="https://www.lesswrong.com/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model?commentId=KGxLrQmYpHLsCsbgj"><u>说</u></a>：“我的猜测是，如果你保持能力不变，并向（更好的 LM 代理）+（更小的 LM）方向做出边际移动，那么你将使世界变得更安全。它直接降低了欺骗性联盟的风险，使监督变得更加容易。”更容易，并且降低了优化结果的潜在优势。”</p></div></li><li class="footnote-item" role="doc-endnote" id="fnn81ovsk6tz"> <span class="footnote-back-link"><sup><strong><a href="#fnrefn81ovsk6tz">^</a></strong></sup></span><div class="footnote-content"><p><i>我忘记了关于差异化技术发展的一句话，比如如果有一条不安全的路径和一条更安全的路径，并且不安全的路径就在前面（就能力而言），我们应该急于在更安全的路径上取得进展，以便它领先并且即使是非安全动机的研究人员也会转向更安全的道路</i>。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/HzFjTWGT5inNk96Fq/which-paths-to-powerful-ai-should-be-boosted#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HzFjTWGT5inNk96Fq/which-paths-to-powerful-ai-should-be-boosted<guid ispermalink="false"> HzFjTWGT5inNk96Fq</guid><dc:creator><![CDATA[Zach Stein-Perlman]]></dc:creator><pubDate> Wed, 23 Aug 2023 16:00:00 GMT</pubDate> </item><item><title><![CDATA[A Theory of Laughter]]></title><description><![CDATA[Published on August 23, 2023 3:05 PM GMT<br/><br/><h1> 1. 太长；博士</h1><p>对于笑声应该有两个层面的平行解释。</p><ul><li>在大脑层面，应该有某种产生笑声的机制/算法，并且它应该符合人们在实践中笑的数据。</li><li>在进化层面上，首先应该对这种机制存在的原因做出一些解释。为什么我们的祖先具有适应性？它从哪里来——其他动物中有同源物吗？</li></ul><p>我将以相反的顺序总结我对这两个方面的建议：</p><h2> 1.1 tl;dr前半部分：进化论中的笑声</h2><p>我赞同流行的理论，即笑声是“游戏”的指标，与其他动物中与游戏相关的发声和肢体语言（例如狗的“游戏弓”）同源。</p><ul><li><i><strong>游戏</strong></i><strong>的进化目的</strong><strong>是“为未来的危险情况进行练习”</strong> 。例如，一只喜欢打闹和追逐的幼狼可能会在未来现实生活中的打斗和追逐中表现得更加熟练。</li><li><i><strong>与生俱来的交流性游戏信号</strong></i>（例如人类的笑声和狗的游戏弓）<strong>的进化目的</strong><strong>是减少从练习到严肃的意外升级的可能性。</strong>例如，如果两只幼狼之间的游戏打斗升级为幼狼之间的<i>真正</i>战斗，这对两只幼狼来说都是危险的。如果幼犬能够发出并回应交流游戏信号，那么这种升级的可能性就不太可能发生。这<a href="https://en.wikipedia.org/wiki/Safeword_(sports)"><u>与格斗相关运动（以及其他地方）中的“安全词”</u></a>有点相同。</li></ul><h2> 1.2 tl;dr后半部分：大脑算法方面的笑声</h2><p>我的（过于简单的）伪代码大脑的笑<a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“业务逻辑”</u></a>是这样的： </p><figure class="table"><table><tbody><tr><td style="background-color:#fff2cc;border:2px solid hsl(0, 0%, 0%)"><p><strong><u>拟议的笑的大脑伪代码</u></strong>：</p><ul><li> (A) 如果我的下丘脑和脑干收到<i>一些</i>证据表明我处于危险之中<ul><li><i>（这里的“证据”可能是一些相同的信号，它们本身往往会激活交感神经系统）</i></li></ul></li><li> (B) 我的下丘脑和脑干同时获得<i>更有力的</i>证据证明我是安全的<ul><li><i>（这里的“证据”可能是一些相同的信号，它们本身往往会激活副交感神经系统）</i></li></ul></li><li> (C) 我的下丘脑和脑干有证据表明我处于社交场合</li><li>(D) 然后我会发出与生俱来的游戏信号（例如人类的笑声），而且我也会感到更有活力（在边缘），更安全，更少担心等。</li></ul></td></tr></tbody></table></figure><p>事实上，我预计下丘脑或脑干中存在一些基因特定的神经元群（或者更一般地说，我所说的<a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>转向子系统</u></a>），并且当未来的科学家研究它的各种连接及其功能特性时，它将直接显然，这个神经元组及其连接正在实现上面的伪代码。</p><p> （旁注：这些科学家还会发现，这个神经元组还有各种其他输入，这些输入使笑的可能性或多或少——与情绪等相关的输入——为了简单起见，我从框中省略了这些输入。）</p><p>请注意，此框中没有任何内容与人类特别相关。如果我们谈论的是<a href="https://scholar.google.com/scholar?cluster=14143705629125606901&amp;hl=en&amp;as_sdt=40000005&amp;sciodt=0,22"><u>50kHz 老鼠的笑声</u></a>而不是人类的笑声，我不会更改上面框中的任何一个单词。然而，在这篇文章的后面，我将特别讨论人类的笑声，包括幽默，并且我将认为这个伪代码框与人们笑的环境似乎是匹配的。</p><p>另外，我最初猜测这个伪代码框的路径（即内省）与我如何相信进化故事无关（即，我在书中读到它，它看起来显然是正确的）。但我声称这两个故事完美匹配——考虑到我认为大脑算法一般如何工作的背景限制，上面的伪代码框是实现与进化故事相关的“规范”的自然、直接的方式（参见第 3.3 节） .2 见下文）。这让我确信我走在正确的道路上。</p><p>好的，这就是 tl;dr。本文的其余部分将详细阐述该图景、我目前相信它的原因以及更广泛的含义。</p><h2> 1.3 目录及章节摘要</h2><ul><li>第 2 部分将更详细地解释进化故事。</li><li>第 3 节将更详细地解释大脑算法的故事，特别是上面的伪代码<i>到底</i>是什么意思？然后我将阐述我认为正确的三个主要原因：（1）伪代码符合进化“规范”； (2) 伪代码在神经科学方面非常可信； （3）伪代码似乎与人类大笑的情况（以及动物发出类似游戏信号的情况）相匹配。</li><li>第 4 节通过在三个领域充实如何协调伪代码与日常经验来详细阐述后者：<ul><li>第 4.1 节问：这段伪代码如何阐明物理游戏中的笑声——挠痒痒、追逐、躲猫猫、水气球大战等？例如，你为什么不能给自己挠痒痒？</li><li>第 4.2 节问：这段伪代码如何阐明对话中的笑声？例如，笑声如何在不同的环境中传达如此多的不同信息（例如友善与攻击性，或真诚与不真诚）？</li><li>第 4.3 节问：这段伪代码如何阐明幽默和笑话？</li></ul></li><li>第 5 节问：这段伪代码<i>到底</i>是如何在大脑中实现的？我假设下丘脑和/或脑干中的神经元组之间存在与上面的伪代码直接对应的先天连接。唉，我无法准确地告诉你它是哪个神经元组——我认为这是一个还没有人研究过的神经元组。但我想我知道我们应该寻找什么以及在哪里寻找，而且我认为神经科学实验室可以在不久的将来使用标准实验方法来解决这个问题。</li><li>第 6 节是结论，我将在其中讨论这篇文章与我作为 AGI 安全/人工智能一致性研究员的工作有何关系。</li></ul><h1> 2. 进化故事</h1><h2>2.1 什么是游戏，为什么动物有与生俱来的游戏驱动力？</h2><p>在我看来<strong>，游戏的一个中心例子</strong>是两只小动物互相打闹或追逐。</p><p>为什么小动物会这样做？似乎有一个明显的主要解释：</p><ul><li>如果一只松鼠<a href="https://www.youtube.com/watch?v=DdqjR9V8pig"><u>在幼崽时花了很多时间与其他松鼠玩耍和追逐</u></a>，那么它<a href="https://www.youtube.com/watch?v=ZGhZHlpSE6k"><u>在成年后可能会在实际战斗和实际追逐其他松鼠方面</u></a>表现得更好。</li><li>同样，如果一只松鼠在追逐游戏中花了很多时间逃离它的兄弟姐妹，那么它在逃离捕食者时可能也会表现得更好。</li></ul><p>换句话说，<strong>玩耍是应对未来危险情况的练习</strong>。</p><p>拥有天生的游戏<i><strong>内驱力</strong></i><strong>有什么进化优势</strong><strong>？</strong>因为，显然，松鼠幼崽没有远见和知识来从第一原理中推断出花一些空闲时间练习应对未来危险情况是个好主意。因此，玩耍是一种与生俱来的驱动力。</p><p>换句话说：<i>从进化的角度来看</i>，游戏是达到目的的一种手段，但从<i>松鼠一生的角度来看</i>，游戏本身就是一种奖励——它本质上是一种享受。</p><h2> 2.2 什么是先天的交流游戏信号，为什么动物有它们？</h2><p>玩耍信号是指动物玩耍时主要或专门发生的任何发声或肢体语言。</p><p>一个例子是狗的“玩耍弓”（通常伴随着高亢的“玩耍吠叫”）： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wv8xxg22jkhcjc3kouil" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/icatww4sscedx4ltgrmv 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bhesr7a3d5vdhxyuodep 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/mwoaui6gninysgzxnys0 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/soxnfi6ajbdvco6se0ls 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ziijb7pwvvdxlijsvfww 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/euzjo9cyy9glwnucsptr 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wdyakvp9d3qkpmorxaqf 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ge1r2lqqc822xrpgwyux 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/jmr5dlt2xyjdkrnq7yep 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/sg4w5gt4gt4snt5m5cjt 857w"><figcaption>狗“拉弓”的例子</figcaption></figure><p>另一种是老鼠的笑声，这是老鼠在各种情况下发出的 50 kHz（超声波）鸣叫声，包括幼鼠之间的打闹。 Jaak Panksepp 和他当时的学生 Jeffrey Burgdorf 首次假设这些叽叽喳喳的声音是 20 世纪 90 年代末老鼠版的笑声；例如，参见他们<a href="https://scholar.google.com/scholar?cluster=14143705629125606901&amp;hl=en&amp;as_sdt=40000005&amp;sciodt=0,22"><u>2000 年发表的这篇论文</u></a>，其中涉及手动给老鼠挠痒痒。</p><p> （我知道你在想什么——但别担心！今天的科学家不再需要忍受用手给老鼠挠痒痒的侮辱。相反， <a href="https://scholar.google.com/scholar?cluster=3513785698040370829&amp;hl=en&amp;as_sdt=0,22"><u>他们可以使用</u></a>“自动挠痒痒方案​​，其中......老鼠被迫移动和互动”带有不断旋转的杆”。）</p><p>关于老鼠笑声的后续工作已经有很多，我将在下面引用其中的一些工作。</p><p>所以无论如何，其他动物也有玩耍信号。为什么人类应该有所不同？更具体地说，我们的黑猩猩表亲会发出类似笑声的喘息声，并且“在被挠痒痒、打闹玩耍和追逐游戏时笑得最多（被追逐的黑猩猩笑得最多）”（这句话以及更多章节中的内容） <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>普罗文的书</u></a>第 5 部分）。</p><p> （参见： <a href="https://youtu.be/ffnyOZGB-Tc?t=14"><u>YouTube 上一只小黑猩猩被挠痒痒的画面</u></a>。）</p><p>因此我们得出这样的理论：笑是人类的游戏信号，与其他动物的游戏信号类似。我认为这个理论至少可以追溯到达尔文； <a href="https://plato.stanford.edu/entries/humor/"><u>SEP 在 1936 年提到了</u></a>Max Eastman。就我而言，我最初是从 Kevin Simler 和 Robin Hanson 所著的<a href="https://www.amazon.com/Elephant-Brain-Hidden-Motives-Everyday/dp/0190495995"><i><u>《Elephant In The Brain》</u></i></a>一书中的一章中听到这个理论的。 （我发现这本书的章节非常有启发性，尽管我不同意他们所说的一切。 <span class="footnote-reference" role="doc-noteref" id="fnrefv8uvhlgfgso"><sup><a href="#fnv8uvhlgfgso">[1]</a></sup></span> ）</p><p><strong>为什么动物有天生的交流游戏信号？</strong>想象一下两只小松鼠正在打架。出于上述原因，这是一项互利的活动。然而，两只小松鼠<i>也</i>有可能<i>真的</i>打架，这种活动对两只松鼠来说都是非常<i>危险的</i>，有时对旁观者来说也是如此。</p><p>现在我们看到了问题：假打斗和真打斗之间有明显的相似之处，以至于<strong>假打斗可能会意外升级为真打斗</strong>。因此，每只松鼠都可以从与另一只松鼠沟通它正在打闹中受益。</p><p>因此，我们期望动物进化出发出游戏信号的先天机制，以及相应的先天机制来注意到这些信号并对它们做出反应。 <span class="footnote-reference" role="doc-noteref" id="fnrefcligk4e8fce"><sup><a href="#fncligk4e8fce">[2]</a></sup></span></p><h1> 3.大脑的故事</h1><h2>3.1 伪代码</h2><p>我将重新复制顶部的框以方便参考 - 我认为它是这样的： </p><figure class="table"><table><tbody><tr><td style="background-color:#fff2cc;border:2px solid hsl(0, 0%, 0%)"><p><strong><u>拟议的笑的大脑伪代码</u></strong>：</p><ul><li> (A) 如果我的下丘脑和脑干收到<i>一些</i>证据表明我处于危险之中<ul><li><i>（这里的“证据”可能是一些相同的信号，它们本身往往会激活交感神经系统）</i></li></ul></li><li> (B) 我的下丘脑和脑干同时获得<i>更有力的</i>证据证明我是安全的<ul><li><i>（这里的“证据”可能是一些相同的信号，它们本身往往会激活副交感神经系统）</i></li></ul></li><li> (C) 我的下丘脑和脑干有证据表明我处于社交场合</li><li>(D) 然后我会发出与生俱来的游戏信号（例如人类的笑声），而且我也会感到更有活力（在边缘），更安全，更少担心等。</li></ul></td></tr></tbody></table></figure><p>可能还有其他各种因素可以调节这个回路（也称为改变阈值）——人与人之间的差异，随年龄的变化（当然还有物种），以及对其他先天信号的依赖（例如，愤怒的人往往笑得更少） 。但我认为上面的方框才是主要的故事。</p><h2> 3.2 “我处于危险之中的证据”等<i>到底</i>是什么意思？这些东西是如何操作的？</h2><p>需要明确的是，我<i>并不是</i>在谈论<i>有意识地相信</i>自己处于危险或安全之中等。例如，即使我有意识地相信自己没有什么可担心的，例如在一部恐怖电影。</p><p>相反，我谈论的是下丘脑和/或脑干中的先天信号——我称之为<a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>“转向子系统”</u></a> 。这些信号（我声称）具有特定的固有“含义”/算法目的，与生态/稳态要求具有明显的关系。例如，请参阅我<a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its#3__Case_study_of_hypothalamus__business_logic___NPY_AgRP_neurons"><u>在这里</u></a>对下丘脑中一组特定神经元的讨论，这些神经元由对食物的生理需求激活，并引起适合该状态的各种下游效应（例如能量守恒和饥饿感）。即使我们没有有意识（内感受）地接触到这些先天信号，也没有与它们完美契合的通用英语概念，这些先天信号仍然可以存在。 （请参阅<a href="https://www.lesswrong.com/posts/iYzFKJjzFPRNrqLE3/lisa-feldman-barrett-versus-paul-ekman-on-facial-expressions"><u>我最近发表的关于我与丽莎·费尔德曼·巴雷特（Lisa Feldman Barrett）不同意见的文章</u></a>。）</p><p>无论如何，我在上面的框中建议，“危险”可能通过一些激活交感神经系统（直接或间接）的相同先天信号来操作，而“安全”可能通过一些激活交感神经系统的相同先天信号来操作。副交感神经系统（直接或间接）。具体是哪些信号？我不知道。</p><p>那么成分（C），即“社交”先天信号呢？我对此也不确定。但下丘脑和脑干中似乎确实存在与生俱来的信号（通过进化）与社交情境相关。进化计算此类信号的一种明显方法是通过<a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and#3_2_1_Each_subsystem_generally_needs_its_own_sensory_processor"><u>脑干感觉处理系统</u></a>中计算的启发式方法。例如，我最近喜欢的预印本<a href="https://www.biorxiv.org/content/10.1101/2023.05.19.540391"><u>——Liu</u> <i><u>et al.</u></i> <u>(2023)</u></a> — 确定了下丘脑（内侧视前核）中的两组细胞。一组的活动与社会孤立相关，另一组的活动与隔离的结束相关。有趣的是，他们发现，就这两个细胞群而言，“社交”是通过触觉来运作的——不是声音，不是信息素，它必须是触摸。 （老鼠是盲人，所以它们不确定视力。）但是，触摸的中心地位的发现并不能推广到其他社会本能。例如，啮齿动物的另一种社会本能是交配，在这里，先天回路至少部分是由信息素触发的。无论如何，总而言之，我不知道啮齿类动物的笑声成分（C）的“社交”到底是如何计算的，更不用说人类了，但这样的计算绝对是大脑可以做到的事情。</p><h2> 3.3 我喜欢这个提案的三个原因</h2><h3>3.3.1 该伪代码符合第 2 节的进化“规范”</h3><p>我认为这里的对应关系是有力且直接的，我认为这是我走在正确轨道上的关键证据。浏览上面的项目：</p><ul><li> <i>(A)——一些证据表明我处于危险之中。</i>如果我所处的情况会引发一些“战斗或逃跑”的身体反应（无论具体原因如何），那么我几乎肯定处于类似于我将来可能遇到的危险情况的情况。因此，处于这种情况几乎肯定构成良好的“实践”。</li><li> <i>(B)——更有力的证据证明我是安全的。</i>没有这个成分，它就不是“练习”——它是真正的东西！我应该<i>摆脱</i>困境，而不是急于陷入困境！我应该觉得它令人厌恶，而不是有趣。</li><li> <i>(C)——我处于社交场合的证据。</i>如果没有这个成分，它仍然符合“实践”的资格，因此它仍然在进化上适应保持在这种情况下，因此我们的进化期望应该是即使没有成分（C），情况也会“有趣”。但从进化的角度来说，如果没有成分（C），我就不会<i>笑</i>。<i>如果周围没有人听到，那么发出通信信号就没有意义。</i></li></ul><p>我根据各种研究（对人类和老鼠）添加了成分（C），当同种动物在场时，会产生更多的笑声。例如， <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>普罗文</u></a>发现，当人们独处时，笑声的频率会降低 30 倍。 <span class="footnote-reference" role="doc-noteref" id="fnref1r9n4pmqgz5"><sup><a href="#fn1r9n4pmqgz5">[3]</a></sup></span> （我认为自己看电视是一种中间情况——电视可能会欺骗我们的大脑，让我们认为我们处于社交情境中，至少在某种程度上是这样。）</p><p>例如，我的理论是：如果我和朋友一起坐过山车，我们很可能在可怕的部分一起笑；相反，如果我乘坐<i>空荡荡的</i>过山车，看不到或听不到任何人，那么我就不太可能大声笑，但我仍然可能会觉得它很有趣。同样，如果我独自一人，我可能想读一本幽默的书，即使这样做可能不会让我笑出声来。</p><h3> 3.3.2 这种伪代码在神经科学方面非常可信</h3><p>不幸的是，尽管付出了一些努力，我还是无法告诉您实现上框中伪代码的确切神经元。有关更多详细信息，请参阅下面第 5 节。</p><p><i>然而</i>，上面的伪代码与我认为我所了解的有关大脑的一切完全兼容。这对我来说是强有力的证据，因为“我认为我所了解的关于大脑的事情”是高度受限的！ （对于其中一些限制，请参阅我的文章“ <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>大脑中的‘从头开始学习’</u></a> ”。并参阅<a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"><u>我关于社交本能的文章</u></a>，了解这些限制如何在实践中排除许多可能性。）</p><p>特别是，我知道正确类型的信号实际上存在于下丘脑和脑干中（参见上面第 3.2 节），并且我知道基因组很容易能够在（可能）下丘脑中构建一小群神经元，执行必要的逻辑运算。我看到下丘脑中有很多小神经元簇广泛地执行这种基因指定的<a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>业务逻辑</u></a>。</p><h3> 3.3.3 这个伪代码似乎适合人类笑时的数据（以及动物发出类似游戏信号时的数据）</h3><p>据我所知，伪代码与我们对人类笑声和其他动物相应游戏信号的所有体验兼容，包括下面第 4 节中详细阐述的所有三个类别——第 4.1 节中的身体游戏、非“幽默” ” 4.2 节中的对话式大笑，4.3 节中的幽默。</p><p>您可以阅读第 4 节以更好地理解我的观点。但是你应该仔细看看上面第 3.1 节中的伪代码框，然后向下滚动到注释部分，并抱怨该伪代码框的预测是错误的某些情况！</p><p> ......然后我会回应这样的事情：</p><ul><li> “好吧，当然，但那是因为第 3.1 节的伪代码框过于简单化，就像我遗漏了笑反应是如何被与愤怒和交配以及其他类似事物相关的其他先天信号抑制的。”</li><li> “好吧，当然，但我使用了‘安全’和‘危险’这样的词，显然这些只是我能想到的最接近的英语单词，而不是完美的描述，而且它们以各种方式与实际的先天信号分开，顺便说一下，我无法详细说明”。</li></ul><p> ……然后你就会对我翻白眼，指责我特别恳求和不可证伪。</p><p>完全公平！</p><p>但我目前仍然认为我从这个伪代码框中得到的东西比我输入的要多得多——即使不知道相应的神经元在哪里以及它们连接到什么。</p><p> （但是请留下那些怀疑的评论！他们将非常感激！）</p><h1> 4. 将伪代码与日常经验联系起来</h1><p>它们之间没有明显的界限，但我将分别讨论三类：肢体游戏中的笑声、非“幽默”对话中的笑声和幽默。以该顺序：</p><h2> 4.1 肢体游戏中的笑声（例如挠痒痒、追逐、躲猫猫等）</h2><p> （这是最直接的情况，也是人类与其他动物最相似的情况。） </p><figure class="image image_resized" style="width:64.13%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ebchyilyqsuggtmnawsw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/khphripzpyad2by4qh41 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/r3h2lhgnjah5isy28tdm 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ih8lfcypnagdzxltynzd 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/favfninitj7z9re0bhgf 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/wclq1oixf8awu5x4xpho 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/z7giwgnxptperdyu0hpw 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/gow6ozwz2nekf8wfh0aa 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ffblulfjkar3x3voonne 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/xwtrejdd9n31qkigjhq5 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ir4xijg6lmerresllig9 1197w"><figcaption>孩子们在玩耍时大笑的例子——被抛到空中（左）或被水枪喷射（右）。图片<a href="https://www.seattlepi.com/national/slideshow/Squirt-gun-fight-14475.php">来源</a>： <a href="https://fineartamerica.com/featured/father-tossing-baby-son-in-air-c1950s-debrockeclassicstock.html">1、2</a></figcaption></figure><h3> 4.1.1 身体游戏中的笑声中“成分（A）”（即危险的证据/生理唤醒的原因）的来源是什么？</h3><p>在物理游戏的笑声中，我认为成分（A）有很多可能的来源，而且大多数都是非常明显的。特别是，我们（像几乎所有动物一样）有各种各样的先天防御反应，包括在我们的例子中：</p><ul><li>对意外的感官输入<a href="https://en.wikipedia.org/wiki/Startle_response"><u>产生惊吓</u></a>和<a href="https://en.wikipedia.org/wiki/Orienting_response"><u>定向反应</u></a></li><li>因预期身体创伤而出现的各种类型的<a href="https://en.wikipedia.org/wiki/Reflex"><u>防御性退缩</u></a></li><li>对摔倒的感觉的一些本能反应</li><li>逃避威胁（“<a href="https://en.wikipedia.org/wiki/Escape_response"><u>逃避反应</u></a>”）</li><li> ETC。</li></ul><p>所有这些先天反应不仅会触发某些肌肉行为，还会触发生理唤醒——因此成分 (A)。</p><p>在所有这些情况下，您都会看到小孩子在玩耍时大声笑。</p><h3> 4.1.2 特别是对挠痒痒的更多讨论</h3><p>我觉得很明显，挠痒痒是打闹的一部分。例如，你身体上最怕痒的部位似乎与最容易受到严重伤害的部位重合，比如脖子的前部。 <span class="footnote-reference" role="doc-noteref" id="fnref34vhiqc4mc2"><sup><a href="#fn34vhiqc4mc2">[4]</a></sup></span>因此，挠痒痒背后的进化故事很简单。大脑层面的故事呢？</p><p>我声称上面 3.1 节的伪代码完全足以解释关于大脑层面的痒痒的一切。为了充实这一点，这里有一些额外的细节和讨论：</p><p>如上所述，挠痒痒中<u>成分 (A) 的来源</u>基本上是正常的<a href="https://en.wikipedia.org/wiki/Startle_response"><u>惊吓</u></a>和对意外感官输入的<a href="https://en.wikipedia.org/wiki/Orienting_response"><u>定向反应</u></a>。但是，出于明显的进化原因，这些先天反应可能对于身体脆弱部位的触觉<i>尤其强烈</i>。想一想在什么情况下这些反应会自然触发——也许有一只蝎子在你的脖子上爬，或者也许你的敌人在战斗中成功地用手掐住了你的脖子，等等。</p><p>你不能给自己挠痒痒，就像当你开始说话时，你通常不会因为自己的声音而产生不自觉的<a href="https://en.wikipedia.org/wiki/Startle_response"><u>惊吓反应</u></a>，也因为同样的原因，当你挥手时，你通常不会产生不自觉的<a href="https://en.wikipedia.org/wiki/Orienting_response"><u>定向反应</u></a>。自己的手在自己的眼前。 （当然，除非你患有精神分裂症——请参阅<a href="https://www.lesswrong.com/posts/tgaD4YnpGBhGGbAy5/model-of-psychosis-take-2"><u>这里</u></a>——或者如果你是个婴儿，第一次发出有趣的声音而让自己感到惊讶，等等。）</p><p>挠痒痒<u>的成分（B）的来源</u>是知道你在值得信赖的朋友身边是安全的。如果你在真正担心自己安全的情况下感到“痒痒”，我的印象是你会尖叫而不是笑。</p><p>与此相关的是，即使有人喜欢被挠痒痒，他们仍然会试图将你的手从最痒的地方推开。这种行为的<i>进化层面的</i>原因是显而易见的：这是打斗游戏运作方式的一部分。否则这很难说是好的防守练习！但<i>大脑层面</i>的原因呢？我建议，当他们的敏感区域成功被挠痒痒时，成分（A）变得如此强烈，以至于切断了成分（B），使体验从有趣变成厌恶。</p><h2> 4.2 非“幽默”谈话中的笑声</h2><h3>4.2.1 背景：大多数对话中的笑声并不是“幽默”</h3><p>即使我们忽略身体游戏，“幽默” <span class="footnote-reference" role="doc-noteref" id="fnrefwfrfar7cs1"><sup><a href="#fnwfrfar7cs1">[5]</a></sup></span>和笑声之间的联系也没有你想象的那么紧密。在 <a href="https://www.amazon.com/Laughter-Scientific-Investigation-Robert-Provine/dp/0141002255"><u>普罗文的书中，</u></a>他谈到了他对现代美国人在公共场合闲逛的“生态”研究，并表示“我的助手估计，只有大约 10%-20% 的[在某人大笑之前的评论]甚至有点幽默”。接下来是一张有用的典型引人发笑的评论表，其中包括诸如“我稍后会见到你们！”之类的俏皮话。和“我可以加入你们吗？”</p><p>下次您在（或无意中听到）正常的面对面小组对话时请密切注意，您可能会注意到类似的情况。</p><p>其他文化在这方面似乎与美国相似——例如<a href="http://www.scholarpedia.org/article/Hunter-Gatherers_and_Play"><u>这篇文章</u></a>讨论了狩猎采集者因温和的戏弄而大笑等——而不是“鸡为什么过马路”。</p><h3> 4.2.2 对话性笑声中的“成分（A）”（即危险证据/生理唤醒原因）的来源是什么？</h3><p>第 4.1.1 节列出了在<i>身体</i>游戏中激发兴奋的一系列明显方式，例如，如果您认为自己独自一人，但有人突然从躲藏的地方跳出来向您尖叫。但是谈话呢？<i>仅仅言语</i>也能引起生理唤醒吗？是的，显然！走进一个有压力的话题会让你的心率加快，就像走进一群愤怒的蜜蜂一样。</p><p>更具体地说，在我看来，对话中成分（A）的天然来源有很多，包括：</p><ul><li>困惑</li><li>尴尬/内疚/羞耻（可能是替代性的）</li><li>厌恶（可能是替代性的）</li><li>惊喜（可能是替代性的）</li><li>威胁（身体威胁和地位威胁）（可能是替代威胁）</li><li>打破禁忌</li></ul><p>这些并不是相互排斥的，而且还有其他的。</p><h3> 4.2.3 笑声传达了关于我短暂的“内部”状态的相当普遍的信息，但随后听者必须推断为什么我有这种感觉，而后一种推断是复杂的、上下文相关的，并且变化很大。</h3><p>请记住，我声称“在练习过程中避免未来危险情况的意外升级”是对为什么大脑笑机制首先存在的进化解释。但<i>考虑到</i>这种大脑机制的存在，它会在很多情况下活跃起来，其中许多可能与“避免在未来危险情况的练习中意外升级”无关。 （参见<a href="https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers"><u>“适应执行者，而不是健身最大化者”</u></a> 。）我认为这对于人类来说比其他物种<i>更</i>真实，他们似乎大多只是在打闹、追逐游戏等过程中大笑。</p><p>在<i>所有</i>情况下，我认为普遍共享的笑声“含义”与笑者大脑中的某些信号有关，如第 3.1 节的伪代码所示。但随后听者需要从上下文中推断出<i>为什么</i>这些信号出现在笑者的脑海中。事情变得非常复杂和偶然。一些例子：</p><ul><li><u>情景一：</u>中学时你从我身边走过，当时我的隐形眼镜刚刚掉下来。我用绝望的语气说“请帮助我”，你笑着说“对不起，我不帮助失败者”。</li><li><u>我如何看待这一点：</u>我知道你感到安全（成分（B）），我可能猜测这是因为我属于你的外群体，并且你认为我的痛苦不会对你自己的福祉构成威胁。我也知道你觉得有点成分（A），也许我猜你会觉得我一开始就敢向你寻求帮助是一种畏缩。简而言之，你的笑声向我传达了你没有同情心和高人一等的感觉。</li><li><u>情景2：</u>你是我的配偶。我一边说“伙计，我的公司真的很不正常”，一边微微笑着，作为回应，你笑着说“是的”。</li><li><u>我怎么看：</u>嗯，就我而言，我笑主要是因为我对功能障碍感到有点恼火（成分（A）），但也没有为我自己过度担心（成分（B））。然后，当你在回应中大笑时，我可能会推断你对我的幸福投入了精力，并在这两方面同情地反映了我的感受。简而言之，你的笑声向我传达了你的同情心和同志情谊。</li></ul><p>我还可以继续说下去。我认为，在不同的背景下，笑可以表示友谊，或敌意，或优越，或自卑，或真诚，或不真诚，等等。没有简单的理论，因为我们可以出于许多不相关的原因感受到某种方式。</p><h3> 4.2.4 …鉴于笑声能够传达信息，人们巧妙地将笑声作为交流工具包的一部分</h3><p>在上一节中，我含蓄地将笑声视为某人在谈话过程中感受到的情绪的附带副作用。但是，一旦人类（有意识或无意识地）了解到笑声可以传达事物，他们就会开始<i>利用</i>笑声来巧妙地推进他们的交流意图。例如，在上面的场景 2 中，也许您笑的部分<i>原因是</i>您<i>想</i>表达同理心和同志情谊。</p><p>这不一定是有意识的明确决定或愿望，事实上通常可能不是。这可能更常见的是一种无意识的习惯——在之前的很多谈话中，你在特定的背景下以某种方式笑或不笑，这会带来好的结果，所以你无意识地学会了下次重复这种行为。重新遇到类似的情况。</p><p>虽然有目的地控制笑声并不是我在第 3.1 节中写下的“业务逻辑”的直接一部分，但我们显然实际上可以自愿笑。从机制上讲，我认为这通常（尽管并不总是<span class="footnote-reference" role="doc-noteref" id="fnrefb4hvx2pwe9g"><sup><a href="#fnb4hvx2pwe9g">[6]</a></sup></span> ）间接发生——如果我们想笑，我们就会引导自己进入一种具有成分（AC）的短暂情绪状态，如果我们<i>不想</i>笑，我们就会引导自己进入一种情绪状态。一种短暂的情绪状态，<i>不</i>具备所有这三种成分。我们该怎么做呢？好吧，我们对短暂的情绪状态有一定的控制力，因为我们可以关注处境的某些方面而不是其他方面，选择在心理上调用哪些框架/类比等。</p><h2> 4.3 幽默</h2><p>正如上面4.2.1提到的，幽默并不一定会让人发笑，大多数笑声都是在缺乏幽默的情况下发生的。</p><p> That said, humor can obviously lead to laughter, so I ought to briefly say something about how. I don&#39;t have a grand theory of humor, nor do I think there is one, beyond what I&#39;ve already said in this post. I&#39;ll just mention a few considerations that I find helpful to keep in mind when thinking about humor and jokes.</p><h3> 4.3.1 What are the sources of “Ingredient (A)” (ie, evidence of danger / cause for physiological arousal) in humor?</h3><p> I think the list is pretty similar to Section 4.2.2 above for conversational laughter above. I won&#39;t re-copy it—you can scroll up. Again, that list is not exhaustive, nor mutually exclusive.</p><h3> 4.3.2 There&#39;s an inverted-U dynamic for “Ingredient (A)”</h3><p> According to the pseudocode box above, if there is too little of Ingredient (A), there&#39;s no laughter (eg a boring conversation), and if there&#39;s <i>too much</i> (A), then there&#39;s <i>also</i> no laughter, because it undermines Ingredient (B) (eg it might just feel stressful, painful, scary, confusing, etc.) Somewhere in between is optimal for laughter.</p><p> So I think we wind up with inverted-U dynamics like this: </p><figure class="image image_resized" style="width:70.51%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/dxsimdii3jipjtwcteq5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/m65s9gzgstqaakgexd12 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/hqeoiud5c412kvr8hdhw 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/b7pnh20qevjag7ivn5ai 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/oezoivlqsttjvgwgopco 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/xu8b4ceqxxa14ccbxnle 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ltqhjjnxgxck4zgvbiwv 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/dghb3hs4pskf0gtvphxm 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/t2kmegbcd84hexaioqlw 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/zfbf7eej24mu1ftigvkd 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/srqjui8mbp9a4bouagns 841w"><figcaption> My theory predicts an inverted-U dependence between Ingredient (A) (in the pseudocode box above) versus laughing—more (A) leads to more laughing, until it&#39;s so much that it cuts off Ingredient (B) and tips to aversion. (See prior discussion of lots of proposed inverted-U&#39;s in humor in <a href="https://www.amazon.com/Psychology-Humor-Integrative-Approach/dp/0128121432"><u>Martin &amp; Ford 2018</u></a> .)</figcaption></figure><p> (I was implicitly talking about this same inverted-U in Section 4.1.2 above, when discussing why someone might enjoy getting tickled a little bit, but find direct tickling of their most ticklish spots to be <i>too much</i> , and thus aggressively push the tickler away.)</p><h3> 4.3.3 The (somewhat arbitrary and drifting) cultural expectations / tropes / rituals surrounding “humor” can be a key component of the explanation of why something is funny.</h3><p> In particular, if a listener has a general cultural expectation that “humor” should involve X (eg a punchline), then a joke-teller can <i>either</i> :</p><ul><li> Increase the amount of Ingredient (B) by conspicuously including X (eg an obvious punchline), <i>OR</i></li><li> Increase the amount of Ingredient (A) by conspicuously <i>excluding</i> X (eg punchline-free absurdist <a href="https://en.wikipedia.org/wiki/Anti-humor"><u>anti-humor</u></a> )</li></ul><p> Either of these can be helpful for the joke, depending on how much (A) and (B) are present from other sources.</p><p> And if the latter (exclusion of X) happens a lot, then we collectively stop expecting X in the first place, causing gradual (anti-inductive <span class="footnote-reference" role="doc-noteref" id="fnref7vihiltl1wd"><sup><a href="#fn7vihiltl1wd">[7]</a></sup></span> ) drifts in what people find funny, or schisms between humor-subcultures.</p><h3> 4.3.4 I&#39;m not generally impressed by “theories of humor” beyond their overlap with the discussion above.</h3><p> For example, the <a href="https://www.amazon.com/Psychology-Humor-Integrative-Approach/dp/0128121432"><i><u>Psychology of Humor</u></i> <u>textbook</u></a> (Martin &amp; Ford 2018) describes three “classic theories of humor” which I will comment on in turn:</p><ul><li> <u>“Relief theory”</u> seems to capture the kernel-of-truth that many instances of humor follow the following time-course: <i>first</i> , we have a bunch of Ingredient (A), and <i>second</i> , something changes and introduces a heap of Ingredient (B) (while the (A) has not yet fully faded from our brainstems). Those two steps can be described as “tension” and “relief of tension” respectively.</li><li> <u>“Superiority theory”</u> seems to capture the kernel-of-truth that, in many instances of humor, Ingredient (A) is vicarious—from imagining someone in danger or duress—while Ingredient (B) comes from my comfort in the knowledge that I, the listener, am not that person, and am superior to that person, and have therefore nothing to fear for myself.</li><li> <u>“Incongruity theory”</u> strikes me as a mishmosh of different things. For example, Ingredient (A) can be based on the feeling of confusion, and (B) by its resolution. Or Ingredients (A) and (B) can come from different (incongruous) ways to view the same situation, one of which is normal / safe and the other embarrassing / dangerous / etc. In still other cases, I wonder whether we&#39;re all just following a cultural telling-a-joke script, and as a listener, Ingredient (A) is my concern that I don&#39;t “get the joke” and will be embarrassed to admit it, and Ingredient (B) is my relief when I do. <span class="footnote-reference" role="doc-noteref" id="fnrefikpolxri2te"><sup><a href="#fnikpolxri2te">[8]</a></sup></span> It can also be several of these things simultaneously, and more.</li></ul><p> After that, the textbook moves on to discuss three “contemporary theories of humor” (“reversal theory”, “comprehension-elaboration theory”, and “benign violation theory”). My comments on those would largely overlap with the above bullet points, so I&#39;ll just move on.</p><h1> 5. More detailed discussion of the neuroscience</h1><h2> 5.1 Overview</h2><p> The kind of <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> pseudocode of Section 3.1 is to be found, I claim, in what I call the “Steering Subsystem” (hypothalamus and brainstem—see definition and discussion <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>here</u></a> ). My guess is more specifically as follows: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/leosou3rtkiyirfpehod" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/ujuedwlmm3dbdmbjbttq 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/yjn5d7d8xbmkzzwudqpd 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bzy6ukm6m8kqsknw0i0i 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/vpghumpc2tor0bjhx73z 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/vlaxjkcqakkihmbqwi9x 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/q78lge8vqgdgpilvinoo 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/bmr53pm1hix6vl7osi0a 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/y01cp19oberkrbt8ryz6 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/eqqsconoltp9sbrcufhu 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7kdBqSFJnvJzYTfx9/c3zll5rr3axazecnaiip 1540w"></figure><h2> 5.2 Where <i>exactly</i> in the brain is the “laugh behavior controller” (top box in that diagram) where I can read out the alleged pseudocode of Section 3.1?</h2><p> Sadly, I have failed to deduce from existing literature where in (probably) the hypothalamus we would find the core “business logic” pseudocode of Section 3.1. (It could also be split among a couple places.)</p><p> Focusing on rats (although I expect the answer to be similar in humans), one candidate mentioned in the literature is the so-called “parvafox nucleus” of the lateral hypothalamus. Check out<a href="https://scholar.google.com/scholar?cluster=12802221367573840133&amp;hl=en&amp;as_sdt=0,22"><u>Alvarez-Bolado &amp; Celio (2016)</u></a> for an argument along those lines. As far as I can tell, the strongest evidence in favor of this hypothesis is that bilateral destruction of the parvafox nucleus dramatically (factor of >;10) reduces rodent laughter, according to <a href="https://doi.org/10.1016/j.bbr.2015.11.004"><u>Roccaro-Waldmeyer</u> <i><u>et al.</u></i> <u>(2016)</u></a> . Relatedly, there is some evidence (including from laughter-inducing “gelastic seizures”) that “stimulations of [the] tuberal portion [of the lateral hypothalamus] provoke bursts of laughter” (<a href="https://scholar.google.com/scholar?cluster=12802221367573840133&amp;hl=en&amp;as_sdt=0,22"><u>Alvarez-Bolado &amp; Celio (2016)</u></a> ), and that&#39;s in the general vicinity of parvafox.</p><p> However, I think the current balance of evidence is that parvafox is <i>not</i> the controller for laughter, but rather is related to defense behavior—based on both direct stimulation of those cells (eg <a href="https://www.biorxiv.org/content/10.1101/2023.01.10.521942"><u>Cola</u> <i><u>et al.</u></i> <u>(2023)</u></a> ), and looking at where in the brain they project to (eg <a href="https://scholar.google.com/scholar?cluster=1669280582819001741&amp;hl=en&amp;as_sdt=0,22"><u>Celio</u> <i><u>et al.</u></i> <u>(2013)</u></a> , <a href="https://scholar.google.com/scholar?cluster=9172453117902267421&amp;hl=en&amp;as_sdt=0,22"><u>Bilella</u> <i><u>et al.</u></i> <u>(2016)</u></a> ). We still need to explain the Roccaro-Waldmeyer results from the previous paragraph, but there are a couple possibilities for that, including (1) that parvafox is essential for “Ingredient (A)” of the pseudocode of Section 3.1 (and thus upstream of laughter), or (2) that parvafox is <i>physically proximate to</i> the “real” laugh behavior controller and that Roccaro-Waldmeyer destroyed the latter accidentally in their experiments. (Lesion experiments are notorious for “collateral damage” of nearby neurons and fibers, if I understand correctly, but be warned that I&#39;m not an expert.)</p><p> Well, if it&#39;s not parvafox, then what is it?</p><p>我不知道。 If someone wanted to make progress on this question experimentally—to actually find that pseudocode implemented via innate brain signaling pathways—I think an obvious immediate next step would be a retrograde neural tracing experiment starting from the (lateral) part of periaqueductal gray (PAG) associated with laughter (as pinpointed in the very recent article <a href="https://www.cell.com/neuron/abstract/S0896-6273(23)00477-4"><u>Gloveli</u> <i><u>et al.</u></i> <u>(2023)</u></a> ), followed by further characterization of whatever upstream neuron-groups show up. Maybe such data already exists, and I missed it. (Parvafox does <i>not</i> seem to project to the correct part of PAG to match up with the Gloveli <i>et al.</i> neurons, as far as I can tell, although I&#39;m not super-confident.)</p><p> Areas that I see as <i>especially suspicious</i> include:</p><ol><li> neurons nearby but not part of the parvafox nucleus, in the tuberal region of lateral hypothalamus (for reasons stated above);</li><li> neurons somewhere in or around the medial preoptic area of the hypothalamus—for which stimulation can cause rat laughter ( <a href="https://scholar.google.com/scholar?cluster=8163760487852460824&amp;hl=en&amp;as_sdt=0,22"><u>Wintick &amp; Brudzynski (2001)</u></a> ), and which is known to be the home of the main behavior controllers for at least two other positive social behaviors that I know of, namely the behavior studied by <a href="https://www.biorxiv.org/content/10.1101/2023.05.19.540391"><u>Liu</u> <i><u>et al.</u></i> <u>(2023)</u></a> mentioned above, plus aspects of mating as discussed in <a href="https://www.cell.com/cell/abstract/S0092-8674(23)00798-5"><u>Bayless</u> <i><u>et al.</u></i> <u>(2023)</u></a> ;</li><li> more generally, the rest of the hypothalamus too.</li></ol><h2> 5.3 What about the “Learning Subsystem” (cortex, striatum, amygdala, hippocampus, etc.)?</h2><p> As I described <a href="https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and"><u>here</u></a> , I claim that the brain should be divided into two subsystems, the “Learning Subsystem” which runs <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>randomly-initialized learning algorithms</u></a> , and the “Steering Subsystem” which runs <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> . Above I was only talking about the “Steering Subsystem”; but in fact laughter also interacts with the “Learning Subsystem”—ie, the cortex, striatum, amygdala, hippocampus, thalamus, cerebellum, etc.</p><p> I claim that  the laughter-triggering signals coming from the Learning Subsystem are in the following two categories:</p><ul><li> (A) Outputs trained by reinforcement learning to maximize some signal from the Steering Subsystem (hypothalamus &amp; brainstem) that acts as a “reward”, or</li><li> (B) <a href="https://www.lesswrong.com/posts/Y3bkJ59j4dciiLYyw/intro-to-brain-like-agi-safety-4-the-short-term-predictor"><u>“Short-term predictors”</u></a> of some signal from the Steering Subsystem.</li></ul><p>例如：</p><ul><li> An example of (A) would be voluntary / learned control of laughter (Section 4.2.4).</li><li> An example of (B) would be a “self-fulfilling prophecy”, where you laugh because you&#39;re in a situation that pattern-matches to other situations that have caused you to laugh in the past.</li></ul><p> I think there are probably other examples too, but that&#39;s outside the scope of this post.</p><p> Direct evidence that the cortex is not <i>necessary</i> for play is provided by “decorticate” rats (= rats whose cortex has been surgically removed), whose play is “much the same [as] controls” ( <a href="https://psycnet.apa.org/record/1990-98262-005"><u>Whishaw 1990</u></a> , <a href="https://www.sciencedirect.com/science/article/abs/pii/0031938494902852"><u>Panksepp</u> <i><u>et al.</u></i> <u>1994</u></a> ). I can&#39;t immediately find direct measurements of the presence of the normal 50kHz “laughing” vocalizations in decorticate rats while they play, but if their play is similar in other respects, I would certainly <i>guess</i> that they are also vocalizing in a roughly normal way.</p><h1> 6. Conclusion</h1><p> I remain adamant that the hypothalamus and brainstem are full of hundreds-to-low-thousands of specific neuron groups with specific connections that are all written directly into the genome, and which correspond to <a href="https://www.lesswrong.com/posts/4gaeWLhnnBvhamRke/book-review-the-heart-of-the-brain-the-hypothalamus-and-its"><u>“business logic”</u></a> that makes evolutionary sense—things like “if you&#39;re malnourished, reduce your sex drive”. I think that <i>understanding</i> this tangle of “business logic” is annoying but possible, and that doing so seems possibly helpful for AGI safety, for reasons spelled out <a href="https://www.lesswrong.com/posts/qusBXzCpxijTudvBB/my-agi-safety-research-2022-in-review-and-plans#2__Second_half_of_2022__1_3___My_main_research_project"><u>here</u></a> .</p><p> I consider laughter an “easy” example of such business logic, as compared to something like the human innate status drive (assuming that there <i>is</i> a human innate status drive, which I currently believe but am not 100% sure). I think the human innate status drive would need substantially more convoluted pseudocode than the box in Section 3.1, and I don&#39;t even have a plausible guess right now for how it works in detail. (See <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human"><u>here</u></a> for vague thoughts.) So maybe this post on laughter is a warm-up. In particular, I don&#39;t think this post is <i>directly</i> important for AGI safety—if we iron out the details of the pseudocode of Section 3.1, and put it into the source code of future AGIs, and then we find that those AGIs are laughing in a vaguely-psychopathic-human-like way while they mercilessly murder me and my family and every other human … then that doesn&#39;t really make me feel much better!</p><p> Still, even if understanding laughter is just a “warm-up” for more safety- and alignment-relevant things like compassion and friendship, I am still very interested in getting this right! I spent much longer on this blog post than usual (admittedly a low bar), including trying to be reasonably comprehensive in reading relevant sources, etc., and I am very eager for feedback.</p><p> <i>(Thanks Seth Herd, Linda Linsefors, Justis Mills, and Miguel De Guzman for critical comments on drafts.)</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnv8uvhlgfgso"> <span class="footnote-back-link"><sup><strong><a href="#fnrefv8uvhlgfgso">^</a></strong></sup></span><div class="footnote-content"><p> Since the works of Robin Hanson are popular on this forum, I will say a bit more about where I differ from <i>Elephant In The Brain</i> . My biggest complaint is the part where they say:</p><blockquote><p> As we mentioned earlier, people are profoundly ignorant about laughter&#39;s meaning and purpose (at least in our default state, before learning the science). But where does this ignorance come from? Why does introspection fail us so spectacularly here?</p><p> It&#39;s not simply because laughter is involuntary, outside our conscious control. Flinching, for example, is also involuntary, and yet we understand perfectly well why we do it: to protect ourselves from getting hit. Thus our ignorance about <i>laughter</i> needs further explanation.</p></blockquote><p> I disagree that it “needs further explanation”. I think <a href="https://www.lesswrong.com/posts/wBHSYwqssBGCnwvHg/intro-to-brain-like-agi-safety-2-learning-from-scratch-in"><u>we start out ignorant of</u> <i><u>literally everything</u></i></a> , until we learn it / figure it out. And I think that figuring out the evolutionary purpose of laughter is just inherently much harder than figuring out the evolutionary purpose of flinching. It&#39;s less obvious / salient, for various reasons that I claim are pretty obvious if you think about it. I don&#39;t think there&#39;s any more to it than that.</p><p> I also don&#39;t think there <i>can</i> be more to it than that. To explain what I mean by that, imagine if I said: “Here&#39;s the source code for training an image-classifier ConvNet from random initialization using uncontrolled external training data. Can you please edit this source code so that the trained model winds up confused about the shape of Toyota Camry tires <i>specifically</i> ?” The answer is: “Nope.对不起。 There is no possible edit I can make to this PyTorch source code such that that will happen.” By the same token, <i>even if</i> , as that book argues, there is a strong evolutionary pressure to make humans <i>specifically</i> confused about the evolutionary purpose of laughter, I don&#39;t think there is any possible genetic change that would make that happen. Related discussion <a href="https://www.lesswrong.com/posts/5F5Tz3u6kJbTNMqsb/intro-to-brain-like-agi-safety-13-symbol-grounding-and-human#13_2_2_Claim_2__Social_instincts_are_tricky_because_of_the__symbol_grounding_problem_"><u>here</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fncligk4e8fce"> <span class="footnote-back-link"><sup><strong><a href="#fnrefcligk4e8fce">^</a></strong></sup></span><div class="footnote-content"><p> Whenever there&#39;s a claim about communicative signals, one can ask a follow-up question of whether these signals are game-theoretically stable against “lies”. Like, what if Squirrel A laugh-squeaks to signal play, then Squirrel B lets down its guard, then Squirrel A attacks for real? If this happened a lot, wouldn&#39;t squirrels eventually evolve to ignore play signals? And if <i>that</i> happened, then wouldn&#39;t squirrels further evolve to stop emitting play signals in the first place? Good question! It&#39;s worth thinking about those kinds of things. But I do think a good answer exists, even if I don&#39;t know it in full detail. I think there are various ways that signals can be hard or costly to fake. And I also think that animals treat a play-signal <i>by itself</i> as insufficient reason to let down one&#39;s guard, which reduces the benefit of lying. Animals are reacting to other cues too, like whether there is an existing trusting relationship. For example, if I&#39;m a prisoner, and the cruel guard is pointing at me and laughing, that sure wouldn&#39;t make me feel more relaxed.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1r9n4pmqgz5"> <span class="footnote-back-link"><sup><strong><a href="#fnref1r9n4pmqgz5">^</a></strong></sup></span><div class="footnote-content"><p> I think Provine&#39;s claim that there&#39;s 30× more laughter in social situations is less clear-cut than it sounds. In fact, it&#39;s hard to do an apples-to-apples comparison of laughter in social versus nonsocial situations, because (1) social situations are drawn from a different distribution from nonsocial situations in many respects, (2) the presence of other people can change Ingredients (A) and (B) too, and (3) social situations can also affect laughter via voluntary control (Section 4.2.4). I still think (C) is <i>probably</i> a real ingredient in the algorithm, because Provine&#39;s factor of 30 is <i>so extreme</i> that it seems difficult to fully explain by any of those indirect mechanisms.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn34vhiqc4mc2"> <span class="footnote-back-link"><sup><strong><a href="#fnref34vhiqc4mc2">^</a></strong></sup></span><div class="footnote-content"><p> I&#39;m extremely far from an expert on what parts of the body are vulnerable in combat today, let alone 100,000 years ago (How often were people punching each other, versus slashing with sharp rocks, versus getting bitten by wolves or spiders? What were the lived consequences of different types of injuries? Beats me!). But I still think this claim is probably true. For example, <a href="https://jamanetwork.com/journals/jama/article-abstract/395461"><u>this article</u></a> claims that ticklish areas correlate with areas protected by involuntary defensive reflexes.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwfrfar7cs1"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwfrfar7cs1">^</a></strong></sup></span><div class="footnote-content"><p> I am defining the word “humor” narrowly—something like “humor = the kind of stuff you find in the &#39;humor&#39; section of a bookstore”, eg recognizable jokes and so on. On this definition, if someone walks up to my lunch table and says “can I join you?” and then laughs a bit, that&#39;s conversational laughter but not “humor”. I believe <a href="https://www.penguinrandomhouse.com/books/332702/laughter-by-robert-r-provine/">Provine</a> takes this definitional approach.</p><p> Alternatively, one could define the word “humor” very broadly, as something like “humor = whatever makes someone spontaneously laugh”. On this definition, it&#39;s perfectly possible that someone walking up to my lunch table and saying “can I join you?” <i>is</i> an instance of “humor”, when taken with its full context. I believe <a href="https://mitpress.mit.edu/9780262518697/inside-jokes/">Hurley <i>et al.</i></a> take this definitional approach.</p><p> Anyway, this is just semantics. “Humor” is just a word, and within limits, we can define it however we want. I find the narrow definition to be helpful for the purposes of this post, so that&#39;s how I&#39;m using it.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnb4hvx2pwe9g"> <span class="footnote-back-link"><sup><strong><a href="#fnrefb4hvx2pwe9g">^</a></strong></sup></span><div class="footnote-content"><p> Transient manipulation of our own emotional state is not the only method of learned / deliberate manipulation of laughter. Alternatively, you can laugh by pure voluntary motor control—just move your larynx, lungs, etc. and make laugh sounds, the same way you might voluntarily move your arm. I think such laughter can come across as fake sometimes (like maybe it can sound slightly different, and the eyes don&#39;t squint in quite the same way, see <a href="https://en.wikipedia.org/wiki/Smile#Duchenne_smile">Duchenne smile</a> )—although people still do it plenty.</p><p> Incidentally, I think the “summoning transient emotions” method of laughing and the “pure voluntary motor control” method of laughing in this footnote are probably two opposite ends of a spectrum, as opposed to a crisp binary.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7vihiltl1wd"> <span class="footnote-back-link"><sup><strong><a href="#fnref7vihiltl1wd">^</a></strong></sup></span><div class="footnote-content"><p> If you haven&#39;t heard the term “anti-inductive”: &quot;Inductive&quot; reasoning is where you assume that if you&#39;ve seen something a bunch of times in the past, then it&#39;s likely to happen again in the future. “Anti-inductive” reasoning is the opposite—the more past evidence you have for something, the more likely it is to be <i>false</i> next time.</p><p> As the old joke goes: “I recommend anti-inductive reasoning to anyone. After all, using anti-inductive reasoning has been a catastrophically bad idea every time I&#39;ve tried it in the past. So it&#39;s <i>definitely</i> a good idea going forward.”</p><p>这是一个例子。 If you&#39;re up against a master at rocks-paper-scissors, you might want to use anti-inductive reasoning about the future: The <i>more</i> evidence you have for a pattern in your opponent&#39;s behavior, the likelier it is that your opponent <i>wants</i> you to notice that pattern, and therefore the more you should expect that pattern to reverse on the next throw. (But this is happening at every level of abstraction simultaneously—which is kinda weird to think about.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fnikpolxri2te"> <span class="footnote-back-link"><sup><strong><a href="#fnrefikpolxri2te">^</a></strong></sup></span><div class="footnote-content"><p> I suspect that kids will laugh a comparable amount in practice when solving a riddle / brainteaser posed by a friend as when “getting” a pun told by the same friend, other things equal. Other things are not always equal though; puns often get an extra emotional “kick” from some other source, like from being sexual, or from the friend&#39;s mood, or just from the very fact that it&#39;s a pun, both because there&#39;s a <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/Pun"><u>cultural trope</u></a> that making puns is a shameful activity, and because puns are “supposed” to be funny and the expectation of laughter can be a self-fulfilling prophecy, see Section 5.3 below.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/7kdBqSFJnvJzYTfx9/a-theory-of-laughter<guid ispermalink="false"> 7kdBqSFJnvJzYTfx9</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Wed, 23 Aug 2023 15:05:59 GMT</pubDate> </item><item><title><![CDATA[Why Is No One Trying To Align Profit Incentives With Alignment Research?]]></title><description><![CDATA[Published on August 23, 2023 1:16 PM GMT<br/><br/><p> A whole lot of Alignment work seems to be resource-constrained. Many funders have talked about how they were only able to give grants to a small percentage of projects and work they found promising. Many researchers also receive a small fraction of what they could make in the for-profit sector (Netflix recently offered $900k for an ML position). The pipeline of recruiting talent, training, and hiring could be greatly accelerated if it wasn&#39;t contingent on continuing to receive nonprofit donations.</p><h3></h3><h3> <strong>Possible Ideas</strong></h3><p></p><h2> AI Auditing Companies</h2><p> We&#39;ve already seen a bit of this with ARC&#39;s eval of GPT4, but why isn&#39;t there more of this? Many companies will/are training their own models, or else using existing models in a way beyond what they were intended. Even starting with non-cutting-edge models could provide insight and train people to have the proper Security Mindset and understanding to audit larger ones. Furthermore, there has been a push to regulate and make this a required practice. The possibility of this regulation being made into law will likely be contingent on the infrastructure for it already existing. And it makes sense to take action toward this now, if we want those auditing teams to be as useful as possible, and not merely satisfy a governmental requirement. Existential concerns would also be taken more seriously by a company that has already built a reputation for auditing models.</p><p> <strong>Evals reporting</strong></p><p> Companies don&#39;t want to see their models doing things that weren&#39;t intended (example, giving people credit card information, as was just recently demonstrated). And as time goes on, companies will want some way of showcasing their models have been rigorously tested. Audit reports covering a large, diverse set of vulnerabilities is something many will probably want.</p><p> <strong>Red teaming</strong></p><p> Jailbreaking has been a common practice, done by a wide number of people after a model is released. Like an Evals Report, many will want a separate entity that can red team their models, the same way many tech companies hire an external cybersecurity company to provide a similar service.</p><p></p><h2> Alignment as a service</h2><p> This could bring in new talent and incentives toward building better understanding and talent to handle alignment. These services would be smaller scale, and would not tackle some of the “core problems” of alignment, but might provide pieces to the puzzle. Solving alignment may not be one big problem, but actually a thousand smaller problems. This gives market feedback, where the better approaches succeed more often than the worse approaches. Over time, this might steer us in a direction of actually coming up with solutions that can be scaled.</p><p></p><p> <strong>Offer procedures to better align models</strong></p><p> Many companies will likely not know how to get their models to do the things they want them to, and they will want assistance to do it. This could start by assisting companies with basic RLHF, but might evolve to developing better methods. The better methods would be adopted by competing Alignment providers, who would also search for even better methods to provide.</p><p> <strong>Caveat</strong> : might accelerate surface-level alignment, but just further a false sense of security.</p><p></p><h2> Alignment as a Product</h2><p> This isn&#39;t the ideal approach, but one still worth considering. Develop new proprietary strategies for aligning models, but don&#39;t release them to the public. Instead, show the results of what these new strategies can do to companies, and sell them the strategy as a product. This might involve NDAs, which is why it is not an ideal approach. But an alignment strategy existing under an NDA is better than no strategy at all.</p><p></p><h2> Mech Interp as a Service</h2><p> This is perhaps not yet in reach, but might be in time. Many will want to better understand how their models are working. A team of mechanistic interpretability researchers could be given access to the model, and dive into gaining a better understanding of its architecture and what it&#39;s actually doing, providing a full report of their findings as a service. This might also steer Mech Interp toward methods that have actual predictive value.</p><p> <strong>Caveat</strong> : I&#39;m not too confident about Mech Interp being useful for safety, with the downside that it might be useful for capabilities.</p><p></p><h2> Governance Consultation as a Service</h2><p> Many politicians and policy makers are currently overwhelmed with a problem they have little technical understanding of. A consultation service would provide them with the expertise and security understanding to offer policy advice that would actually be useful. The current situation seems to be taking experts who are already severely time-constrained, and getting their advice for free. I think many would pay for this service, since there are demands for legislation, and they don&#39;t have the understanding to do it on their own.</p><p></p><h2> Alignment Training as a Service</h2><p> Offering to train workers currently at AI companies to understand security concerns, alignment strategies, and other problems might be desired by many companies. An independent company could train workers to better understand concepts that many are probably not used to dealing with.</p><p></p><h2> Future Endowment Fund</h2><p> This is the one that&#39;s the furthest away from normal ideas, but I&#39;d love it if more people tried to hack a solution to this. The biggest issue is that the value from alignment research has a time delay. This solution could be something like a Promise of Future Equity contract. Those that do research would receive a promised future share in the Fund, as would investors. Companies that use anything that was funded by the Endowment would sign something like a Promise of Future Returns, delegating a share of the returns of any model that used the strategy to the fund. This way, people who were also working on alignment strategies that only had a 5% chance of working would still get reimbursement for their work. Those working on strategies with a calculated higher chance of working would get a greater share. The Trustees would be members of the community who are highly credible, and who have deep levels of insight about AI.</p><p></p><p> If you are interested in making progress on any of these endeavors, feel free to message me. I&#39;ve worked in Cybersecurity, so I have a good understanding of how the auditing pipeline normally works at such companies.</p><p> If you have any disagreements with some of these approaches (which I&#39;m sure some do), feel free to tell me why I&#39;m wrong in the comments.</p><br/><br/> <a href="https://www.lesswrong.com/posts/XnAZHCi5QH58WcrkX/why-is-no-one-trying-to-align-profit-incentives-with#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/XnAZHCi5QH58WcrkX/why-is-no-one-trying-to-align-profit-incentives-with<guid ispermalink="false"> XnAZHCi5QH58WcrkX</guid><dc:creator><![CDATA[Prometheus]]></dc:creator><pubDate> Wed, 23 Aug 2023 13:16:42 GMT</pubDate> </item><item><title><![CDATA[Exploring the Responsible Path to AI Research in the Philippines]]></title><description><![CDATA[Published on August 23, 2023 8:44 AM GMT<br/><br/><h3><strong>介绍</strong></h3><p>The transformative potential of artificial intelligence (AI) is reshaping our world, presenting both thrilling opportunities and profound challenges. In the Philippines, where Dominic Ligot (Founder, AI expert &amp; communicator) <span class="footnote-reference" role="doc-noteref" id="fnrefgdz3m8hstr8"><sup><a href="#fngdz3m8hstr8">[1]</a></sup></span> and I have delved into AI&#39;s possibilities, the integration of this technology into our unique cultural fabric can unleash exponential productivity. This post explores how we (as a country) can harness AI to enhance our local community without losing our distinct identity. With our shared fascination as a spark, I&#39;ll examine the broader implications of AI, guided by the imperative to understand its complexities and the need to proceed with caution and adherence to alignment research <span class="footnote-reference" role="doc-noteref" id="fnref4w360g923mp"><sup><a href="#fn4w360g923mp">[2]</a></sup></span> .</p><p></p><h3> <strong>Transformative Technologies and AI Research</strong></h3><p> <i>The Transformative Potential of AI</i></p><p> AI stands as a transformative technology, holding the potential to reshape our world in the coming decade. The failure to understand and integrate AI could leave a country behind in a rapidly changing landscape.</p><p> <i>Historical Precedence of Technological Transformations</i></p><p> Technological advancements have driven significant transformations. Whether through the application of cutting-edge physics in warfare or the discovery of new scientific fields, technological innovations have often led to pivotal moments in human history.</p><p> <i>The Complexity and Urgency of AI Research</i></p><p> AI research is neither simple nor conceptually straightforward. The very complexity of the field underscores the importance of pursuing a deep understanding of it. While the path towards mastering AI may seem daunting and even wild, the stakes are too high for complacency.</p><p> <i>The Inevitable Direction of AI Research</i></p><p> Our current situation leaves us with few choices but to embrace AI research. It&#39;s not merely an option; it&#39;s an essential direction we must take to ensure our survival in a future increasingly shaped by artificial intelligence. The question is not whether we should pursue AI research, but how we can do so most effectively and responsibly.</p><p> Having explored the immense transformative power of AI and its historical parallels, it&#39;s essential to consider both the challenges and promises this complex field presents to us today.</p><p></p><h3> <strong>Upsides and Downsides</strong></h3><p> <i>AI Research Challenges and Uncertainties</i></p><p> AI research at present resembles alchemy more than mathematics. We still lack a formal theory on the malleability of data that can be infused into models, making our understanding of how data gets distributed to individual parts, such as tokens or layers, uncertain. These distributional shifts affect how technology can respond in a generalizable manner. Despite the absence of a formalized mathematical approach, the uncertainties should not halt our pursuit of research.</p><p> <i>Statistics on Project Failures</i></p><p> Projects fail around 65% of the time. <span class="footnote-reference" role="doc-noteref" id="fnrefrgyrt2chqg"><sup><a href="#fnrgyrt2chqg">[3]</a></sup></span> While a disheartening statistic, it serves as a reminder that our resources can become losses if not handled wisely. The possibility that some actions might create more problems than they solve emphasizes the importance of accurately identifying methodologies and conceptual frameworks in AI research. Careful planning can avoid potential losses and unintended consequences.</p><p> <i>The Promising Benefits of AI Technology</i></p><p> The potential benefits of AI technology are simply astounding. Utilizing AI to solve local productivity issues is a top priority, and it could be the catalyst we need. Innovating new ways to enhance the economy and personal lives can be substantial. However, realizing these benefits requires prioritizing alignment work. Ensuring rigorous consideration for human welfare before deploying technology is non-negotiable.</p><p> While the upsides of AI research are astonishing, the uncertainties and potential pitfalls demand a closer look. Let&#39;s explore the vital need for caution, consideration, and alignment in AI research.</p><p></p><h3> <strong>The Stakes of AI Research: A Call for Consideration and Caution</strong></h3><p> <i>Acknowledging the Complexity: A Personal Reflection</i></p><p> This is my first time into pondering these concepts, I am compelled to tackle them with sincerity and diligence. The urgency of establishing a responsible research direction for AI, particularly for a vast and diverse demographic, is palpable. When executed well, the benefits can multiply exponentially.</p><p> <i>The Opportunity and Challenge: A Thoughtful Approach to AI Research</i></p><p> Anyone who considers stepping into the world of AI research, Filipino or otherwise, must heed this reality: Understanding AI as a technology presents enormous opportunities. However, transforming such opportunities into triumphs rather than missteps demands more than mere instinct; it requires more:</p><ol><li> Planning and Prudence: Embarking on this path warrants careful planning and utmost caution. The AI landscape, rich with potential, is equally laden with pitfalls that can result in profound hardship and sacrifice.</li><li> Learning from History: We must remember that uncharted domains of human endeavor often bring pain and struggle before innovation flourishes. Let our approach to AI be informed by these historical lessons, striving for thoughtful innovation rather than hasty experimentation.</li></ol><p> <i>A Responsibility to Proceed with Care</i></p><p> The stakes of our engagement with AI are indeed extraordinarily high. This is not just a technological venture; it&#39;s a complex human enterprise that requires mindful consideration of ethics, alignment, and the social fabric of our communities.</p><p> I cannot envision AI research succeeding without individuals, teams, organizations, or governments factoring in alignment as a core component. AI systems think differently—using tokens, not words, and <a href="https://www.lesswrong.com/tag/transformers">transformer models</a> that is still not fully understood. Engaging in any research agenda without exercising the level of caution required by alignment is undoubtedly a recipe for disaster. A useful concept to illustrate this is whether an AI system exhibits <a href="https://www.lesswrong.com/tag/corrigibility">corrigible properties</a> . Is the AI system willing to allow itself to be modified or shut down when necessary? Even if the AI might not be directly utilized for output in research, it&#39;s safer to recognize if these technologies are not yet robust enough to ensure safety. Taking the time to deeply understand how different AI systems are will certainly aid in advancing AI research.</p><p></p><p> We can build a sturdy bridge to AI success. We can reap the rewards of understanding AI technologies if we prioritize safety.</p><p></p><p> <strong>Contrasting Outcomes: Binisaya Buddy and Local Lore</strong></p><p> I wrote a post on two striking examples that illustrate the importance of responsible AI development: Binisaya Buddy, a virtual guide reflecting Cebuano culture, values, and traditions, and Local Lore, a misguided attempt at cultural representation. <span class="footnote-reference" role="doc-noteref" id="fnref19z9hnoglxt"><sup><a href="#fn19z9hnoglxt">[4]</a></sup></span></p><ol><li> <i>Binisaya Buddy:</i> Born in vibrant Cebu, this project represents the potential when alignment theory guides AI research. It serves both locals and tourists by authentically mirroring cultural nuances.</li><li> <i>Local Lore:</i> In contrast, Local Lore lacked local insight, leading to misrepresentations and disconnection from the community it aimed to serve. A poor execution created more problems than solutions.</li></ol><p> Through the examples of Binisaya Buddy and Local Lore, we see both success and failure in aligning AI with cultural values. These case studies underscore the necessity of not only individual responsibility but also strategic collaboration at various levels to ensure that AI development is aligned with our cultural context and ethical values.</p><p></p><p> These contrasting outcomes highlight the importance of building an integrated system where various stakeholders work together towards responsible AI development. What form might such collaboration take, and how can it be cultivated to ensure that technology truly serves our community&#39;s needs and values?</p><p></p><h3> <strong>A Potential Pathway: Collaboration Between Government, Corporations and Academia</strong></h3><p> The journey toward harnessing the full potential of AI in the Philippines requires more than isolated efforts by individual sectors. It demands a cohesive and strategic collaboration between government bodies, corporations, and academic institutions. Here&#39;s how can this tripartite collaboration can foster responsible AI development:</p><p> <strong>Government&#39;s Role:</strong></p><ul><li> Regulatory Frameworks: By establishing clear and ethical guidelines for AI development, the government can ensure that technological advancements align with national values and societal needs.</li><li> Investment and Support: Government investment in AI research and development, infrastructure, and education can catalyze growth in the sector, supporting both public and private initiatives.</li></ul><p> <strong>Corporate Sector&#39;s Contribution:</strong></p><ul><li> Practical Application: Corporations leads in transforming theoretical research into practical solutions. Collaborating with academia to develop innovative technologies, industry players can focus on addressing local challenges.</li><li> Private Investments: By investing in AI and partnering with educational institutions, businesses can create a thriving ecosystem that promotes technological entrepreneurship and job creation.</li></ul><p> <strong>Academic Institutions as Catalysts:</strong></p><ul><li> Research and Innovation: Universities and research institutions serve as the backbone of cutting-edge exploration in AI. Collaborative research with corporations ensures that academic findings translate into real-world applications.</li><li> Education and Training: Academia plays a vital role in nurturing the next generation of AI experts. By aligning curricula with industry needs and government visions, educational institutions can prepare a skilled workforce ready to contribute to responsible AI development.</li></ul><p> <strong>Potential Cross-Sector Collaboration</strong></p><ul><li> Shared Goals and Strategies: By setting common objectives and collaborative platforms, the three sectors can align their efforts, maximizing efficiency and impact.</li><li> Ethics and Alignment: Ensuring that AI development adheres to ethical principles requires input from all three sectors. This collaboration ensures that technological advancements contribute to human flourishing without compromising cultural identity.</li></ul><h3></h3><h3><strong>结论</strong></h3><p>The possible collaboration between government, industry, and academia isn&#39;t merely a beneficial alliance; it might be an essential strategy for responsible AI development in the Philippines. By recognizing their unique roles and working together towards a shared vision, these sectors can leverage AI&#39;s transformative potential without losing sight of ethical considerations and local values.</p><p> <i>The end goal is human flourishing.</i></p><p> Ultimately, the goal of any AI research, project, or even personal endeavor is human flourishing. To achieve this, a level of alignment work is required to ensure that the results will contribute to improving our lives. The larger the project, the greater the level of alignment work that must be included.</p><p> Our exploration of AI&#39;s potential and challenges culminates in a broader perspective. With the lessons learned and the stakes understood, let&#39;s envision the unique role that the Philippines can play in this rapidly evolving field, with an emphasis on alignment and cultural identity.</p><p> <i>The Road Ahead: Philippines and AI</i></p><p> With 115 million inhabitants, including a vast English-speaking demographic, the Philippines stands at a crossroads. Our journey with AI is not merely about innovation; it&#39;s about a responsible, ethical approach to technology. Both general AI research and alignment work are two sides of the same coin, vital for moving forward in an AI-shaped world. The stakes are immense, and the potential enormous. Our country must not settle for anything less than an approach that harmonizes general research with alignment to our shared values (global and local) and cultural identity.</p><h3></h3><h3> <strong>Addendum</strong></h3><p> After finishing this write-up, I learned of a proposal by Congressman Mark Go, the Representative from the City of Baguio. He informed the Philippine Congress of his initiatives to propose a national drive to train 1% of Filipinos in AI technology and to establish a grant facility for AI research. Though no bill has been drafted yet, it goes to show that this write-up is both relevant and timely. You can see the presentation and with Doc&#39;s commentary <a href="https://www.youtube.com/watch?v=i00ZCE3udAk">here.</a> </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fngdz3m8hstr8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgdz3m8hstr8">^</a></strong></sup></span><div class="footnote-content"><p> Dominic Ligot is the Founder of CirroLytix, a social impact AI company, and Data Ethics PH, an online community focused on social issues such as data privacy, data security, AI-driven discrimination, data liabilities, data ownership rights, and data poverty. Three-time global winner of the NASA and ESA International Space Apps Challenges, his team&#39;s award winning dengue surveillance application, AEDES, has been backed by the Group on Earth Observations (GEO), the Digital Public Goods Alliance (DPGA) and the UNICEF Innovation Fund.</p><p> You can find his complete profile <a href="https://docligot.com/">here.</a></p></div></li><li class="footnote-item" role="doc-endnote" id="fn4w360g923mp"> <span class="footnote-back-link"><sup><strong><a href="#fnref4w360g923mp">^</a></strong></sup></span><div class="footnote-content"><p> When I refer to alignment research / work, I mean the effort that goes into understanding the alignment problem and its complexity. It&#39;s hard for me to recommend alignment theory as a standard, especially since it&#39;s not yet a concrete science on how to align AI systems and make them understand and preserve human values, and we&#39;re all still figuring things out. However, as a reference, <a href="https://www.lesswrong.com/posts/bjjbp5i5G8bekJuxv/study-guide">John Wensworth&#39;s study guide</a> is a good introduction to the complex nature of the theoretical areas intertwined with alignment.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrgyrt2chqg"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrgyrt2chqg">^</a></strong></sup></span><div class="footnote-content"><blockquote><p> But research shows that only 35% of the projects undertaken worldwide are successful—which means we&#39;re wasting an extravagant amount of time, money, and opportunity.</p></blockquote><p></p><p> Quoted from <a href="https://hbr.org/2021/11/the-project-economy-has-arrived">The Project Economy Has Arrived</a> by Antonio Nieto-Eodriguez of Harvard Business Review</p></div></li><li class="footnote-item" role="doc-endnote" id="fn19z9hnoglxt"> <span class="footnote-back-link"><sup><strong><a href="#fnref19z9hnoglxt">^</a></strong></sup></span><div class="footnote-content"><p> From my blog: <a href="https://www.whitehatstoic.com/p/binisaya-buddy-and-local-lore">https://www.whitehatstoic.com</a> :</p><h3> Binisaya Buddy and Local Lore</h3><p> Subtitle: Alignment and Misalignment: A Tale of Two AI Systems Bridging Culture and Technology in the Philippines </p><figure class="image image_resized" style="width:57.35%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/bucxqobmbsdsytuueaos" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/j61mpzznl8fnnaxvj82e 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/aoljkptxyxmjcnzpslml 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/e3esjc3vtk1fz8gcf2u8 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/kdawakp2svpclhpfuf7b 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/mbkqfq08riaztzscujnc 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/tavmhtvyztvxvteyimex 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/xpgegixjph35p0xni7hf 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/ejeiieuk55e9lcqhtjyr 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/rxyebd4ryiw0dvubbvii 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yEx2LfYEGaWQE8dHp/g45pp3gkkjty54n9wxbh 1024w"></figure><p></p><p> In the picturesque Island of Cebu, renowned for its vibrant heritage and breathtaking landscapes, an innovative AI system was built to bridge the gap between locals and tourists. Named &quot;Binisaya Buddy,&quot; after the local dialect, this AI system was designed with a deep understanding of Cebuano culture, values, and traditions.</p><p> Visitors arriving at the Mactan-Cebu International Airport were greeted by Binisaya Buddy through interactive kiosks. With a friendly virtual smile, the AI would share stories of Cebu&#39;s rich history, explain the significance of traditional dances like the Sinulog, offer tips on enjoying local delicacies like Lechon, and even teach a few essential phrases in the local language.</p><p> But what set Binisaya Buddy apart was its alignment with the local culture. The AI was not just a repository of information; it was a reflection of the Cebuano soul. Locals were involved in its development, ensuring that it captured the nuances of their beliefs, values, and way of life.</p><p> Tourists found themselves immersed in an authentic Cebuano experience, facilitated by a technology that felt personal, respectful, and connected. Locals, on the other hand, felt a sense of pride in how their culture was represented and shared.</p><p> Binisaya Buddy became more than a tool; it became a symbol of how technology, when aligned with human values, can enhance cultural understanding, foster connections, and create meaningful experiences.</p><p> In another bustling city not far from Cebu, an ambitious project was launched to create an AI-driven tourist guide. Named &quot;Local Lore,&quot; this system was designed to offer tourists a glimpse into the local culture, traditions, and history.</p><p> However, unlike Binisaya Buddy, Local Lore was developed without local input, guided solely by algorithms fed with data from generic travel sites and social media. The lack of alignment with the community&#39;s values and understanding of their unique cultural nuances led to unintended consequences.</p><p> Soon after its launch, locals began to notice misrepresentations. Local Lore described the Sinulog Festival as merely a colorful party, missing its deep religious significance. It recommended commercialized tourist traps over authentic local experiences, and its language translations were often inaccurate, leading to unintentional offenses.</p><p> The community felt alienated and misunderstood. They saw their rich culture reduced to stereotypes and misconceptions, a caricature rather than a celebration.</p><p> Tourists, initially excited by the novel technology, were left confused and disconnected from the real essence of the place. The lack of authenticity in the Local Lore&#39;s guidance led to missed opportunities for genuine cultural immersion and understanding.</p><p> In time, Local Lore was abandoned, a failed experiment that served as a stark reminder of what can go wrong when technology is not aligned with the human values, context, and culture it aims to represent. It became a cautionary tale of how even well-intended innovation can lead to destruction when the alignment is overlooked.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/yEx2LfYEGaWQE8dHp/exploring-the-responsible-path-to-ai-research-in-the#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/yEx2LfYEGaWQE8dHp/exploring-the-responsible-path-to-ai-research-in-the<guid ispermalink="false"> yEx2LfYEGaWQE8dHp</guid><dc:creator><![CDATA[MiguelDev]]></dc:creator><pubDate> Wed, 23 Aug 2023 08:44:18 GMT</pubDate> </item><item><title><![CDATA[Do agents with (mutually known) identical utility functions but irreconcilable knowledge sometimes fight?]]></title><description><![CDATA[Published on August 23, 2023 8:13 AM GMT<br/><br/><p> Been pondering; will conflict always exist? A major subquestion: Suppose we all merge utility functions and form an interstellar community devoted to optimizing the merger. It&#39;ll probably make sense for us to specialize in different parts of the work, which means accumulating specialist domain knowledge and becoming mutually illegible.</p><p> When people have very different domain knowledge, they also fall out of agreement about what the borders of their domains are. ( <i>EG: A decision theorist is insisting that they know things about the trajectory of AI that ML researchers don&#39;t. ML researchers don&#39;t believe them and don&#39;t heed their advice.</i> ) In these situations, even when all parties are acting in good faith, they know that they wont be able to reconcile about certain disagreements, and it may seem to make sense, from some perspectives, to try to just impose their own way, in those disputed regions.</p><p> Would there be any difference between the dispute resolution methods that would be used here, and the dispute resolution methods that would be used between agents with different core values? (war, peace deals, and most saliently,)</p><p> Would the parties in the conflict use war proxies that take physical advantages in different domains into account? (EG: Would the decision theorist block ML research in disputed domains where their knowledge of decision theory would give them a <i>force</i> advantage?)</p><br/><br/> <a href="https://www.lesswrong.com/posts/FTdtHHBPDdzk4pJz2/do-agents-with-mutually-known-identical-utility-functions#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/FTdtHHBPDdzk4pJz2/do-agents-with-mutually-known-identical-utility-functions<guid ispermalink="false"> FTdtHHBPDdzk4pJz2</guid><dc:creator><![CDATA[mako yass]]></dc:creator><pubDate> Wed, 23 Aug 2023 08:13:06 GMT</pubDate> </item><item><title><![CDATA[South Bay ACX/SSC Fall Meetups Everywhere]]></title><description><![CDATA[Published on August 23, 2023 3:00 AM GMT<br/><br/><p> This is a relaxed Saturday afternoon gathering at Washington Park in Sunnyvale. If you&#39;re reading this, you&#39;re invited. Bring any friends if you think they&#39;d enjoy it too.<br></p><p> <strong>Date:</strong> Saturday, October 14, 2023</p><p> <strong>Time:</strong> 2:00–5:00 pm</p><p> <strong>Location:</strong> Washington Park<br> <a href="https://goo.gl/maps/yLTwRVbV3xkip8ps8">840 W Washington Ave, Sunnyvale, CA 94086, USA</a></p><ul><li> On the roundish grassy area in the northeast corner of the park</li><li> We&#39;ll have a folding table with attached ACX Meetup sign</li><li> Exact location: <a href="https://plus.codes/849V9XG6+X9F"><u>https://plus.codes/849V9XG6+X9F</u></a><br></li></ul><p> We&#39;ll have some snacks and a couple picnic blankets. It might not be enough to feed/fit everyone, so feel free to bring your own also!</p><br/><br/> <a href="https://www.lesswrong.com/events/DcafHPWLuoKMt4Cug/south-bay-acx-ssc-fall-meetups-everywhere#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/events/DcafHPWLuoKMt4Cug/south-bay-acx-ssc-fall-meetups-everywhere<guid ispermalink="false"> DcafHPWLuoKMt4Cug</guid><dc:creator><![CDATA[allisona]]></dc:creator><pubDate> Wed, 23 Aug 2023 04:27:32 GMT</pubDate> </item><item><title><![CDATA[Separate the truth from your wishes]]></title><description><![CDATA[Published on August 23, 2023 12:52 AM GMT<br/><br/><p> The sentence “All people are equal.” (and ones like it) seems simple at first glance but actually contains a lot of depth. Depending on how you interpret it, it can be false or subjective, but I don&#39;t think it&#39;s true.</p><p> The most naive interpretation is obviously false. All people are literally not equal in the physical sense. Our atoms are in different configurations. Neither are we equal in the psychological sense. We react to the same stimuli in different ways.</p><p> So what in the world do people mean when they say all people are equal?</p><p> Do they mean everyone has equal rights? That makes a little more sense, but it is not true. I do not have the same rights as someone living in Switzerland or China.</p><p> Hmm, maybe they mean that everyone has equal opportunities? A quick thought falsifies this interpretation. So what do they mean???</p><p> What someone really means when they say “All people are equal.” is “I wish for all people to be equal!” which is neither true nor false. It is a wish about the future. I do wish for all people to be equal (but not in the <a href="https://en.wikipedia.org/wiki/Harrison_Bergeron">Harrison Bergeron</a> sense, more along the lines of equal rights).</p><p> The reason we knee-jerk react to “All people are equal.” is because our culture has conditioned us, from the humanism of the Enlightenment to the Declaration of Independence and onwards, to believe it is true. We don&#39;t realize that we are really stating a wish when we think we are making a true statement, and this is dangerous.</p><p> Conversely, there are some statements that look like opinions (or maybe even falsehoods) at first glance, but are actually true. (Statements about intelligence fit this category very nicely, but I&#39;m going to leave them out.) These statements usually go against what culture has conditioned us to believe. For example: “All people are not equal.” This statement is true but is usually interpreted as “I wish for all people to have unequal rights.” (a wish), but it should just be interpreted as a fact. Again, this is dangerous, since it masks what is really true.</p><p> In order to push society forward – to actually make everyone equal and become better – we need to separate our beliefs and wishes. If we believe that something that is not true (but that we want to be true) is actually true, we won&#39;t notice it as much and will not spend as much time and effort on fixing it. If, however, we could accurately believe the (sometimes inconvenient) truth, we could reduce the time until our wishes become the truth.</p><p> Now go fix the world!</p><br/><br/> <a href="https://www.lesswrong.com/posts/fi3nJBZ9PsmTpCpsd/separate-the-truth-from-your-wishes#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/fi3nJBZ9PsmTpCpsd/separate-the-truth-from-your-wishes<guid ispermalink="false"> fi3nJBZ9PsmTpCpsd</guid><dc:creator><![CDATA[g-w1]]></dc:creator><pubDate> Wed, 23 Aug 2023 00:52:59 GMT</pubDate> </item><item><title><![CDATA[Implications of evidential cooperation in large worlds]]></title><description><![CDATA[Published on August 23, 2023 12:43 AM GMT<br/><br/><p> I&#39;ve written several posts about the plausible implications of &quot;evidential cooperation in large worlds&quot; (ECL), on my newly-revived blog. This is a cross-post of <a href="https://lukasfinnveden.substack.com/p/implications-of-ecl">the first</a> . If you want to see the rest of the posts, you can either go to <a href="https://lukasfinnveden.substack.com/">the blog</a> or click through the links in this one.</p><p> All of the content on my blog, including this post, only represent my own views —  not those of my employer. (Currently OpenPhilanthropy.)</p><hr><p> “ECL” is short for “evidential cooperation in large worlds”. It&#39;s an idea that was originally introduced in <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> (under the name of “multiverse-wide superrationality”). This post will explore implications of ECL, but it won&#39;t explain the idea itself. If you haven&#39;t encountered it before, you can read the paper linked above or <a href="http://effective-altruism.com/ea/1gf/multiversewide_cooperation_in_a_nutshell/">this summary</a> written by Lukas Gloor. <span class="footnote-reference" role="doc-noteref" id="fnrefweuaua0f4c"><sup><a href="#fnweuaua0f4c">[1]</a></sup></span></p><p> This post lists all candidates for decision-relevant implications of ECL that I know about and think are plausibly important. <span class="footnote-reference" role="doc-noteref" id="fnrefx18fvnjp5eq"><sup><a href="#fnx18fvnjp5eq">[2]</a></sup></span> In this post, I will not describe in much depth why they might be implications of ECL. Instead, I will lean on the principle that ECL recommends that we (and other ECL-sympathetic actors) act to benefit the values of people whose decisions might correlate with our decisions.</p><p> As described in <a href="https://lukasfinnveden.substack.com/i/136237476/what-values-do-you-need-for-this-to-be-relevant">this appendix</a> , this relies on you and others having particular kinds of values. For one, I assume that you care about what happens outside our <a href="https://en.wikipedia.org/wiki/Light_cone">light cone</a> . But more strongly, I&#39;m looking at values with the following property: If you could have a sufficiently large impact outside our lightcone, then the value of taking different actions would be dominated by the impact that those actions had outside our lightcone. I&#39;ll refer to this as “universe-wide values”. Even if <i>all</i> your values aren&#39;t universe-wide, I suspect that the implications will still be relevant to you if you have <i>some</i> universe-wide values.</p><p> This is speculative stuff, and I&#39;m not particularly confident that I will have gotten any particular claim right.</p><h1> Summary (with links to sub-sections)</h1><p> For at least two reasons, future actors will be in a better position to act on ECL than we are. Firstly, they will know a lot more about what other value-systems are out there. Secondly, they will be facing immediate decisions about what to do with the universe, which should be informed by what other civilizations would prefer. <span class="footnote-reference" role="doc-noteref" id="fnrefpneoqb5vw9"><sup><a href="#fnpneoqb5vw9">[3]</a></sup></span> This suggests that it could be important for us to <a href="https://lukasfinnveden.substack.com/i/136237476/affect-whether-and-how-future-actors-do-ecl">Affect whether (and how) future actors do ECL</a> . This can be decomposed into two sub-points that deserve separate attention: how we might be able to affect <a href="https://lukasfinnveden.substack.com/i/136237476/futures-with-aligned-ai">Futures with aligned AI</a> , and how we might be able to affect <a href="https://lukasfinnveden.substack.com/i/136237476/futures-with-misaligned-ai">Futures with misaligned AI</a> .</p><p> But separately from influencing future actors, ECL also changes our own priorities, today. In particular, ECL suggests that we should care more about other actors&#39; universe-wide values. When evaluating these implications, we can look separately at three different classes of actors and their values. I&#39;ll separately consider how ECL suggests that we should…</p><ul><li> <a href="https://lukasfinnveden.substack.com/i/136237476/how-us-doing-ecl-affects-our-priorities">Care more about</a> <a href="https://docs.google.com/document/d/1XCZ-g_GAyZwfJfWHTRVyzptVU2_uI9tFWMG8x9j4C_o/edit#heading=h.qkf9kvplu0c8"><i>other humans&#39;</i></a> <a href="https://lukasfinnveden.substack.com/i/136237476/care-more-about-other-humans-universe-wide-values">universe-wide values</a> . <span class="footnote-reference" role="doc-noteref" id="fnreffbnqsghmnv4"><sup><a href="#fnfbnqsghmnv4">[4]</a></sup></span><ul><li> I think the most important implication of this is that <a href="https://lukasfinnveden.substack.com/i/136237476/upside-and-downside-focused-longtermists-should-care-more-about-each-others-values">Upside- and downside-focused longtermists should care more about each others&#39; values</a> .</li></ul></li><li> <a href="https://lukasfinnveden.substack.com/i/136237476/care-more-about-evolved-aliens-universe-wide-values">Care more about <i>evolved aliens&#39;</i></a> <a href="https://docs.google.com/document/d/1XCZ-g_GAyZwfJfWHTRVyzptVU2_uI9tFWMG8x9j4C_o/edit#heading=h.10dsvgq5soqa">universe-wide values</a> .<ul><li> I think the most important implication of this is that we plausibly should care more about <a href="https://lukasfinnveden.substack.com/i/136237476/influence-how-ai-benefitsharms-alien-civilizations-values">influencing how AI could benefit/harm alien civilizations</a> .</li><li> How much more? I try to answer that question in <a href="https://lukasfinnveden.substack.com/p/how-ecl-changes-the-value-of-interventions">the next post</a> . My best guess is that ECL boosts the value of this by 1.5-10x. (This is importantly based on my intuition that we would care a bit about alien values even without ECL.)</li></ul></li><li> <a href="https://lukasfinnveden.substack.com/i/136237476/care-more-about-misaligned-ais-universe-wide-values">Care more about</a> <a href="https://docs.google.com/document/d/1XCZ-g_GAyZwfJfWHTRVyzptVU2_uI9tFWMG8x9j4C_o/edit#heading=h.6dtoz94g6ytt"><i>misaligned AIs&#39;</i></a> <a href="https://lukasfinnveden.substack.com/i/136237476/care-more-about-misaligned-ais-universe-wide-values">universe-wide values</a> . <span class="footnote-reference" role="doc-noteref" id="fnrefd6bt2xf37fd"><sup><a href="#fnd6bt2xf37fd">[5]</a></sup></span><ul><li> I don&#39;t think this significantly <a href="https://lukasfinnveden.substack.com/i/136237476/minor-prioritize-ai-takeover-risk-less-highly">reduces the value of working on alignment</a> .</li><li> But it suggests that it could be valuable to build AI so that <i>if</i> it ends up misaligned, it has certain other desirable inclinations and values. This topic, of <a href="https://lukasfinnveden.substack.com/i/136237476/positively-influence-misaligned-ai">positively influencing misaligned AI</a> in order to cooperate with distant misaligned AI, is very gnarly, and it&#39;s difficult to tell what sort of changes would be net-positive vs. net-negative.</li><li> I discuss this more in <a href="https://lukasfinnveden.substack.com/p/ecl-with-ai">a later post</a> .</li></ul></li></ul><p> (For more details on the split between humans/evolved-aliens/misaligned-AI and why I chose it, see <a href="https://lukasfinnveden.substack.com/i/136237476/more-details-on-the-split-between-humans-evolved-species-and-misaligned-ai">this appendix.</a> )</p><h1> Affect whether (and how) future actors do ECL</h1><h2> Futures with aligned AI</h2><p> If we take ECL seriously, <span class="footnote-reference" role="doc-noteref" id="fnrefwi0f1v4h1qk"><sup><a href="#fnwi0f1v4h1qk">[6]</a></sup></span> I think it&#39;s really important that humanity <i>eventually</i> understands these topics deeply, and can make wise decisions about them. But for most questions about what humanity should <i>eventually</i> do, I&#39;m inclined to defer them to the future. I&#39;m interested in whether there&#39;s anything that <i>urgently</i> needs to be done.</p><p> One way to affect things is to increase the probability that humanity ends up building a healthy and philosophically competent civilization. (But we already knew that was important.)</p><p> There might also be ways in which humanity could irreversibly mess up in the near-term that are unique to decision theory. For example, people could make unwise commitments if they perceive themselves to be in <a href="https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem">commitment races</a> . <span class="footnote-reference" role="doc-noteref" id="fnref4423tsgb79u"><sup><a href="#fn4423tsgb79u">[7]</a></sup></span> Or there might be ways in which people could learn too much information, too early. (We don&#39;t currently have any formalized decision theories that <i>can&#39;t</i> be harmed by learning information. For example, people who use evidential decision theory can only influence things that they haven&#39;t yet learned about — which means that information can make them lose power.) (Cf <a href="https://lukasfinnveden.substack.com/p/when-does-edt-seek-evidence-about">this later post</a> on when EDT seeks out or avoids certain information.) It&#39;s possible that careful thinking could reduce such risks. <span class="footnote-reference" role="doc-noteref" id="fnref9j6fzbxzuod"><sup><a href="#fn9j6fzbxzuod">[8]</a></sup></span> (For example, perhaps it would be good to prevent early AI systems from thinking about these topics until they and we are more competent.)</p><p> How is ECL relevant for this? Broadly, it seems like ECL is an important part of the puzzle for what various decision theories recommend. So learning more about ECL seems like it could help clarify the picture, here, and clarify what intervention points exist. (This also applies to futures with misaligned AI.)</p><p> (For related discussion in <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> , see section 4.5 on researching and promoting ECL and 4.6.3 on making AI act according to ECL.)</p><h2> Futures with misaligned AI</h2><p> Affecting how misaligned AI does ECL is also an intervention point.</p><p> I think ECL could play a couple of different roles, here:</p><ul><li> Firstly, ECL-sympathetic AI systems might treat <i>us</i> better (eg by giving humanity a solar-system-sized utopia instead of killing us).<ul><li> In order for ECL to motivate AI systems to treat us nicely, there needs to be some distant actors that care about us. Ie, someone would need to have universe-wide values that specifically value the preservation of distant pre-AGI civilizations over other things that could be done with those resources.</li></ul></li><li> Secondly, ECL-sympathetic AI systems might trade (and avoid conflict) with distant civilizations, thereby benefiting those civilizations.<ul><li> This is intrinsically good if we intrinsically care about those distant civilizations&#39; values.</li><li> In addition, it&#39;s plausible that ECL recommends us to care about benefits that accrue to distant civilizations&#39; whose values we don&#39;t intrinsically care about. This is discussed below, in <a href="https://lukasfinnveden.substack.com/i/136237476/influence-how-ai-benefitsharms-alien-civilizations-values">Influence how AI benefits/harms alien civilizations</a> .</li><li> Such trade could also benefit <i>the misaligned AI system&#39;s own values</i> , and ECL might give us reason to care about those values. This is more complicated. I discuss it below in <a href="https://lukasfinnveden.substack.com/i/136237476/positively-influence-misaligned-ai">Positively influence misaligned AI</a> .</li></ul></li></ul><h1> How <i>us doing</i> ECL affects our priorities</h1><h2> Care more about other humans&#39; universe-wide values</h2><h3> It matters less which universe-wide values control future resources (seems minor in practice?)</h3><p> Let&#39;s temporarily assume that humanity will avoid both near-term extinction and AI takeover. Even then, the value of the future could depend a lot on <i>which human values</i> will be empowered to decide what&#39;s to be done with all the matter, space, and time in our lightcone.</p><p> If someone had an opportunity to influence this (eg by promoting certain values), ECL would generally be positive on empowering universe-wide values (that are compatible with good decision-theoretic reasoning), since for any such values:</p><ul><li> You might correlate with distant people who hold such values, in which case ECL gives you reason to benefit them.</li><li> If such values maintain power into the long-term future, and our future civilization ends up deciding that ECL (or something similar) works, then ECL will motivate them to benefit other universe-wide values. (At least insofar as there are gains from trade to be had.)</li></ul><p> If you were previously concerned about promoting <i>any particular</i> universe-wide values, this means that you should now be somewhat less fussed about promoting those values in particular, as opposed to any other universe-wide values. In struggles for influence that are mainly a struggle about universe-wide values, you should care less about who wins.</p><p> (This is related to discussion about moral advocacy in section 4.2 of <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> ; especially 4.2.7.)</p><p> Here&#39;s a slightly more worked-out gesture at why ECL would recommend this.</p><ul><li> Let&#39;s say that you&#39;re a supporter of faction A, in a struggle for influence against faction B. You can decide to either invest in the 0-sum struggle for influence, or you can decide to invest in something that you both value (eg reducing uncontroversial x-risks or s-risks).</li><li> If support for faction B is compatible with good decision-theoretic reasoning, then on some distant planet, there will probably be supporters of faction B who are in an analogous but reversed situation to you (in a struggle for influence against faction A) who are thinking about this decision in a similar way.</li><li> If you decide to support the common good instead of faction A, then faction A&#39;s expected influence will decrease a bit on your planet. But your choice to do so is evidence that the distant supporters of faction B also will support the public good (instead of faction B) on their planet, which will lead faction A&#39;s expected influence to increase a bit (and also lead to positive effects from the support of the public good).</li><li> So ECL provides a reason to invest less resources in the 0-sum fight and instead care more about public goods.</li></ul><p> (In order to work out <i>how</i> much less you&#39;d want to invest in the 0-sum fight, you&#39;d want to think about the ratio between “how much evidence am I providing that supporters of faction A will invest in the public good” to “how much evidence am I providing that supports of faction B will invest in the public good”. <span class="footnote-reference" role="doc-noteref" id="fnref8k49uv7ijvl"><sup><a href="#fn8k49uv7ijvl">[9]</a></sup></span> I&#39;m only illustrating the directional argument, here.)</p><p> While I believe the ECL argument works here, it doesn&#39;t currently seem very decision-relevant to me. Competitions that could be important for the future (eg competition between AI labs or between US and China) don&#39;t seem well-characterized as conflicts between universe-wide value-systems. At least my personal opinions about them are mostly about who&#39;s more/less likely to cause an (uncontroversial) x-risk along the way; and perhaps about who&#39;s more/less likely to help create a society that adopts reasonably impartial values and become sufficiently philosophically sophisticated to enact the best version of them.</p><p> That said, for someone who was previously obsessed with boosting a <i>particular</i> value-system (eg spreading hedonistic utilitarianism, or personally acquiring power for impartial ends), I think ECL should nudge that motivation toward being somewhat more inclusive of other universe-wide values.</p><h3> Upside- and downside-focused longtermists should care more about each others&#39; values</h3><p> (Terms are defined as <a href="https://longtermrisk.org/cause-prioritization-downside-focused-value-systems/">here</a> : “Upside-focused values” are values that <i>in our empirical situation</i> recommend focusing on bringing about lots of positive values. “Downside-focused values” are values that <i>in our empirical situation</i> recommend working on interventions that make bad things less likely, typically reducing suffering.)</p><p> If we look beyond struggles for influence and resources, and instead look for any groups of humans who have different <i>universe-wide</i> values, and where this leads to different real-world priorities, the two groups that stand out are upside-focused and downside-focused longtermists. For these groups, we also <i>have actual examples</i> of both upside- and downside-focused people thinking about ECL-considerations in a similar way. Which makes the ECL-argument more robust.</p><p> It seems good for people to know about and bear this in mind. For example, it means that upside- and downside-focused people should:</p><ul><li> be inclined to take high-leverage opportunities to help each other,</li><li> decide what to work on somewhat less on the basis of values and somewhat more on the basis of comparative advantage,</li><li> avoid actions that would benefit their own values at considerable cost to the others&#39; values.</li></ul><p> As usual, the ECL-argument here is: If you choose to take any of these actions, then that&#39;s evidence that distant people with <i>the other</i> value-system will choose to take analogous actions to benefit <i>your</i> favorite value-system.</p><p> How strong is this effect? I&#39;m not sure. What follows is a few paragraphs of speculation. (Flag: These paragraphs rely even more on pre-existing knowledge about ECL than the rest of the post.)</p><p> Ultimately, it depends on the degree to which humans correlate relatively more with the decisions of people with shared values vs. different values, on this type of decision.</p><p> Ie, the question is: If someone with mostly upside-focused values decides to do something that benefits downside-focused values, how much evidence is this that (i) distant upside-focused people will help out people with downside-focused values, vs. (ii) distant downside-focused people will help out people with upside-focused values. (From the perspective of the person who makes the decision.)</p><p> If it&#39;s similarly much evidence for both propositions, then upside-focused and downside-focused people should be similarly inclined to benefit each others&#39; values as to benefit their own values. <span class="footnote-reference" role="doc-noteref" id="fnrefsocdvs8tsuk"><sup><a href="#fnsocdvs8tsuk">[10]</a></sup></span></p><p> Here&#39;s an argument in favor of this: Regardless of whether you have upside-focused or downside-focused values, the ECL argument (that you should care about the others&#39; values) is highly similar. So it seems like there&#39;s no large dependence on what values you have. Accordingly, it seems like your decision should be equally much evidence for how other people act, regardless of what values they have.</p><p> Here&#39;s a counter-argument: When you&#39;re making any particular decision, perhaps you are getting disproportionately much evidence about how actors that are especially similar to you tend to feel about ECL-based cooperation-arguments in especially similar situations. (After all: Most of your evidence about how likely people are to act on ECL-arguments <i>in general</i> will come from observations about what decisions <i>other</i> people make.) And perhaps an important component of “especially similar” is that those actors would share your values. <span class="footnote-reference" role="doc-noteref" id="fnrefygfd8lysjb"><sup><a href="#fnygfd8lysjb">[11]</a></sup></span></p><p> (For some more of my speculation, including some counter-arguments to that last paragraph, see the post <a href="https://lukasfinnveden.substack.com/p/are-our-actions-evidence-for-ai-decisions">Are our choices analogous to AI choices?</a> , which discusses a similar question but with regards to correlating with <i>AI</i> instead of with <i>humans</i> . A relatively less likely proposition. Nevertheless — similar considerations come up. See also section 3.1 in <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> on orthogonality of rationality and values.)</p><p> Overall, I feel quite uncertain here. This uncertainty corresponds to a sense that my actions are somewhat less evidence for the decisions of people who don&#39;t share my values, but not a huge amount less. Summing over my uncertainty, I feel like my decisions are ≥10% as much evidence for the decisions of people who don&#39;t share my value (as they are evidence for the decisions of people who share my values) — which would imply that I should care  ≥10% as much about their values as I care about my own. <span class="footnote-reference" role="doc-noteref" id="fnref2f0pffnolcf"><sup><a href="#fn2f0pffnolcf">[12]</a></sup></span></p><h2> Care more about evolved aliens&#39; universe-wide values</h2><p> ECL also recommends caring more about alien civilizations. Here are three different implications of this.</p><h3> Minor: Prioritize non-AI extinction risk less highly</h3><p> A minor consequence of this is: You might want to prioritize non-AI extinction risk slightly <i>less</i> highly than before. Because if Earth doesn&#39;t colonize the universe, some of that space will (in expectation) get colonized by alien civilizations instead, to their benefit.</p><p> If we were to trade like-for-like, the story would be: If we prioritize non-AI extinction risk slightly less highly (and put higher priority on making sure that space colonization is good if it does happen), then that&#39;s evidence that distant aliens also prioritize non-AI extinction risk slightly less highly. If this leads to their extinction, and their neighbors share our values, then civilizations with our values will recover some of that empty space. <span class="footnote-reference" role="doc-noteref" id="fnrefiuznazkhk0b"><sup><a href="#fniuznazkhk0b">[13]</a></sup></span></p><p> I think this effect seems minor (unlikely to make non-AI extinction less than half as useful as you previously thought).</p><p> This is mainly because ECL doesn&#39;t recommend us to care <i>as</i> much about alien values as we care about human values. <span class="footnote-reference" role="doc-noteref" id="fnrefpe2xs41o2x7"><sup><a href="#fnpe2xs41o2x7">[14]</a></sup></span> I would be surprised if ECL recommended that we prioritize random alien values more than half as much as our own, which suggests that even if aliens were guaranteed to colonize space in our place, at least ½ of value would be lost from our failure to do so.</p><p> An additional consideration is that aliens are (probably) not common enough to take over all space that we would have missed. I think the relevant comparison is between “space we could get to <i>before aliens</i> ” (ie the total amount of space that humanity would get access to if space is defense-dominant) vs. “space that <i>only</i> we could get to” (ie the space that humanity would get access to without excluding any aliens from it, such that we would want to get there even if we cared just as much about alien&#39;s values as our own values).</p><ul><li> <a href="https://forum.effectivealtruism.org/posts/9p52yqrmhossG2h3r/quantifying-anthropic-effects-on-the-fermi-paradox">My old estimate</a> is that the latter is ~⅓ times as large as the former. This suggests that, even if ECL made us care just as much about alien values as our own values, we would still care ⅓ as much about colonizing space.</li><li> But my old estimate didn&#39;t take into account that intelligence might re-evolve on Earth if humans go extinct. I think this is fairly likely, based on my impression that recent evolutionary increases in intelligence have been rapid compared to Earth&#39;s remaining life-span. If there&#39;s only a ⅓ probability that intelligence fails to re-evolve on Earth, then the expected amount of space that wouldn&#39;t be colonized by anyone is only ⅓*⅓~=10% as large as the space that humans would get to first.</li><li> So if we took these estimates at face-value — they suggest that <i>if</i> ECL moved us from “don&#39;t care about alien values at all” to “care about alien values just as much as our own values”, this would reduce the value of non-AI extinction-reduction by at most 10x.</li><li> …from the perspective of universe-wide values which value marginal influence over the universe roughly linearly. Other plausible value-systems would be less swayed by this argument, so it overestimates how much our all-things-considered view should move.</li></ul><p> Also, as I discuss <a href="https://lukasfinnveden.substack.com/i/136237476/minor-prioritize-non-ai-extinction-risk-less-highly">below</a> , ECL might similarly motivate us to prioritize AI takeover less highly. Since this is the most salient alternative priority to “non-AI extinction risk” on a longtermist view, they partly cancel out.</p><h3> Influence how AI benefits/harms alien civilizations&#39; values</h3><p> A different way in which we could benefit aliens is to increase the probability that Earth-originating intelligence benefits them (if Earth-originating intelligence doesn&#39;t go extinct). This could apply to either aliens that we physically meet in space, or to distant aliens that we can&#39;t causally interact with.</p><p> I typically think about such interventions as a special kind of “cooperative AI”-intervention — increasing the probability that AIs are inclined to seek out win-win opportunities with other value systems. See <a href="https://lukasfinnveden.substack.com/p/how-ecl-changes-the-value-of-interventions">this post</a> for more discussion of this. The brief summary is: ECL could plausibly increase the value of interventions that aim to make misaligned AI treat alien civilizations better by ~1.5-10x.</p><h3> Possibly: Weigh suffering-focused values somewhat higher if they are more universal</h3><p> This is the item on the list that I&#39;ve thought the least about. But I recently realized that it passes my bar of being a “plausibly important implication”, so I&#39;ll briefly mention it.</p><p> As argued in section 4.1.1 of <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> , ECL suggests that we should give greater weight to “universalist values” over idiosyncratic concerns. (Note that “universalist values” does <i>not</i> mean the same thing as universe-wide values. Universalist values are values that are highly common across the universe.)</p><p> While I but the basic argument for this principle, I didn&#39;t initially see any decision-relevant applications of this.</p><p> For example, one idiosyncratic value is to care especially much about yourself, and less about others. But in order for the argument to apply here, we require that the tradeoff rate between altruistic values and selfish values is sensitive to abstract arguments about the altruistic stakes involved. At least for me personally, I intuitively feel like learning about the potential size of the far future should have ~“maxed out” the degree to which abstract knowledge of high altruistic stakes will compel me to act more altruistically and less selfishly. Such that there&#39;s not a lot of room left for ECL to move me.</p><p> Another example is that it affects what precise form of moral advocacy we should be interested in, insofar as we&#39;re pursuing moral advocacy to influence long-term values. But I don&#39;t currently think that it seems like a high-priority intervention to advocate for highly specific values, with the purpose of influencing long-term values. (I think it&#39;s relatively more promising to do “moral advocacy to change near-term behavior” or “advocating for good principles of ethical deliberation”. But I don&#39;t think that the value of those interventions are very sensitive to whether ECL recommends universal values over idiosyncratic values.)</p><p> But here&#39;s a scenario where this consideration might matter. Some people&#39;s views on ethics has an asymmetry between <i>positive</i> visions of the future and <i>negative</i> visions of the future. Where positive visions need to get a lot of complex, human-specific things right, in order to satisfy human&#39;s highly contingent, highly complex values. Whereas negative visions only need suffering — and then that&#39;s already bad. <span class="footnote-reference" role="doc-noteref" id="fnref40zg3jy06vx"><sup><a href="#fn40zg3jy06vx">[15]</a></sup></span></p><p> If this is your view, then a plausible corollary could be: there are many more aliens across the universe who share your concern for suffering, then who share your vision for a positive future.</p><p> And if that&#39;s true, then you could potentially get gains-from-trade from everyone putting relatively more attention into reducing suffering (which is appreciated by everyone) and relatively less attention on bringing about highly complex positive future visions (which is only appreciated by a relatively small fraction).</p><p> How large is this effect?</p><ul><li> When you act to bring about positive futures that you value, your influence is proportional to the (power-weighted) number of actors who share those values, multiplied by the average amount that you correlate with them.</li><li> When you act to reduce negative experiences that you disvalue, your influence is proportional to the (power-weighted) number of actors who share those values, multiplied by the average amount that you correlate with them.</li></ul><p> So for example: If you thought that 10x more actors shared your conception of suffering than your conception of a good future, but that those actors were less similar to you and therefore correlated with you 2x less on average, then that would increase the value of suffering-focused interventions by 5x.</p><h2> Care more about misaligned AIs&#39; universe-wide values</h2><p> More speculatively, ECL might recommend that we care more about the universe-wide values of distant misaligned AIs. <span class="footnote-reference" role="doc-noteref" id="fnrefstsq0mzojoo"><sup><a href="#fnstsq0mzojoo">[16]</a></sup></span> Why is this more speculative? In order for ECL to give us reason to benefit AIs, we would have to be similar enough to those AIs that our decisions have some acausal influence on their decisions. If we assume evidential decision theory, this means that our decision needs to give some evidence for what distant misaligned AIs choose to do. And intuitively, it seems less likely that our decisions provide evidence about distant misaligned AI&#39;s actions than that it provides evidence about the actions of distant aliens. (Since AIs&#39; minds probably differ more from ours, and since the decision-situations they are likely to find themselves in differ more from ours.)</p><p> I feel uncertain about whether ECL says we should care more about the values of distant AIs. If it did, here are two conclusions.</p><h3> Minor: Prioritize AI takeover risk less highly</h3><p> One potential implication could be that we should prioritize AI takeover risk slightly less highly: Because although it would be bad for misaligned AI to seize power, it would at least be slightly good that the AI gets to implement its own values, as long as those values were endorsed by many distant ECL-sympathetic AIs. (Since us benefiting their values, in this way, would be evidence that they make decisions to benefit our values — or at least that&#39;s the hypothetical we&#39;re considering.)</p><p> However, this effect seems minor (unlikely to make misalignment reduction less than half as useful as you previously thought). Because:</p><ul><li> As mentioned above, we might not be similar enough to AIs for the ECL argument to work. And even if we&#39;re similar enough to have some acausal influence on them, ECL doesn&#39;t recommend us to care <i>as</i> much about AI values as we care about human values. <span class="footnote-reference" role="doc-noteref" id="fnref63f2sg768l9"><sup><a href="#fn63f2sg768l9">[17]</a></sup></span> I would be surprised if ECL recommended that we prioritized random AI values more than half as much as our own.</li><li> This is mainly an argument to slightly deprioritize AI takeover <i>by ECL-sympathetic AI systems with universe-wide values</i> . But AIs might not have universe-wide values, and might not be ECL-sympathetic, by default.</li></ul><p> Also, as discussed <a href="https://lukasfinnveden.substack.com/i/136237476/minor-prioritize-non-ai-extinction-risk-less-highly">above</a> , ECL might similarly motivate us to prioritize <i>non-AI</i> extinction less highly. (Which is the most salient alternative priority to misalignment risk, on a longtermist view.)</p><h3> Positively influence misaligned AI</h3><p> A different way in which we could benefit distant misaligned AIs&#39; universe-wide values is to adjust how we build AI so that <i>if</i> humanity ends up building AI that is misaligned with our own values, then it&#39;s more likely to successfully optimize for values that distant misaligned AIs would approve of. Unfortunately, it seems very difficult to work out what sort of changes would be good and what sort of changes would be bad, here.</p><p> For more writing on this, see <a href="https://lukasfinnveden.substack.com/p/ecl-with-ai">here</a> .</p><h2> More</h2><p> I&#39;m following up this post with two other posts:</p><ul><li> <a href="https://lukasfinnveden.substack.com/p/how-ecl-changes-the-value-of-interventions">ECL and benefitting distant civilizations</a> , for more on how ECL affects the value of influencing how AI might benefit/harm distant alien civilizations.</li><li> <a href="https://lukasfinnveden.substack.com/p/ecl-with-ai">ECL with AI</a> , digging into how we could positively influence misaligned AI, and whether ECL recommends that.</li></ul><h1> Appendices</h1><h2> What values do you need for this to be relevant?</h2><p> I&#39;d say this post is roughly: advice to people whose values are such that, <i>if</i> they were to grant that their actions acausally affected an enormous number of worlds quite different from their own, they would say that a large majority of the impact they cared about was impact on those distant worlds.</p><p> Importantly, it&#39;s also advice to people who endorse some type of moral uncertainty or pluralism, and have <i>components</i> of their values that behave like that. Then it&#39;s advice for what that value-component should advocate and bargain for. (I think this is probably a more realistic account of most humans&#39; values.)</p><p> (Though one of the many places where I haven&#39;t thought about the details is: If you are trying to acausally influence agents with universe-wide values, does it pose any extra troubles if you yourself only have partially universe-wide values and do some messy compromise thing?)</p><p> I&#39;ll use “universe-wide” values as a shorthand for these types of values. (“Multiverse-wide” would also be fine terminology — but I think caring about a spatially large universe is sufficient.)</p><p> (For previous discussion of what values are necessary for ECL, see section 3.2 in <a href="https://longtermrisk.org/files/Multiverse-wide-Cooperation-via-Correlated-Decision-Making.pdf">Oesterheld (2017)</a> .)</p><h2> More details on the split between humans, evolved species, and misaligned AI</h2><p> Above, I separately consider how ECL suggests that we should care more about:</p><ul><li> other humans&#39; universe-wide values,</li><li> evolved aliens&#39; universe-wide values,</li><li> misaligned AIs&#39; universe-wide values.</li></ul><p> This raises two questions:</p><ul><li> Why the distinction between “other humans&#39; universe-wide values” and “evolved aliens&#39; universe-wide value”?</li><li> Why the distinction between “evolved aliens&#39; universe-wide values” and “misaligned AIs&#39; universe-wide values?</li></ul><h3> Why distinguish humans from aliens?</h3><p> When I talk about benefiting other humans&#39; universe-wide values, I don&#39;t mean to imply that we&#39;re acausally cooperating with just the local humans on our own planet Earth. I think almost all the benefits come via evidence that very distant actors behave more cooperatively. Such actors could be either quite similar to humans or quite unlike humans (in at least some ways).</p><p> So why talk specifically about the universe-wide values of “other humans”, rather than the broader group of aliens?</p><p> The answer is that universe-wide values held by other humans has a number of unique properties:</p><ul><li> For any universe-wide values held by humans, we have <i>empirical support</i> that evolved species sometimes grow to treasure those values.</li><li> Even stronger, we have empirical support that <i>minds very similar to our own</i> can grow to treasure those values, which strengthens the case for high correlations, and thereby the case for ECL-based cooperation.</li><li> Universe-wide values held by humans can conveniently be benefitted <i>via</i> the humans that support them For example, by:<ul><li> supporting the humans that hold them.</li><li> avoiding conflicts with humans that hold them.</li><li> listening to the advice of humans that hold them.</li></ul></li></ul><p> This is quite different from non-human values, where we have to resort to more basic guesses about their preferences, like:</p><ul><li> Aliens probably value having access to more space over having access to less space.</li><li> Aliens probably prefer to interact with other actors who are cooperative rather than conflict-prone.</li></ul><h3> Why distinguish evolved aliens from misaligned AIs?</h3><p> First, a terminological note: “Misaligned AI” refers to AI whose values are very different from what was intended by the evolved species that first created them. If a distant species has very different values from us, and successfully aligns AI systems that <i>they</i> create, I&#39;d count those as “aligned” AIs.</p><p> (“Aligned AIs” themselves will, of course, have the same values as evolved aliens. Benefiting their values would be the same as benefitting the values of some evolved aliens, so they don&#39;t need a separate category.)</p><p> Now, why do I separately consider the values of evolved aliens and misaligned AIs? There are two reasons.</p><p> Firstly, compared to AI, evolved aliens probably have minds that are more similar to ours, and face decisions that are more similar to ours. Thus, there&#39;s a stronger case that our decisions correlate more with their decisions, making the case for ECL-based cooperation stronger.</p><p> Second, AI progress is currently fast, and I have less than perfect confidence in humanity&#39;s ability to only create and empower aligned AI systems. This (ominously) suggests that we may soon have unique opportunities to benefit or harm the values of misaligned AI systems.</p><hr><h1> Acknowledgments</h1><p> For helpful comments and suggestions on this and related posts, thanks to Caspar Oesterheld, Emery Cooper, Lukas Gloor, Tom Davidson, Joe Carlsmith, Linh Chi Nguyen, Daniel Kokotajlo, Jesse Clifton, Richard Ngo, Anthony DiGiovanni, Charlotte Siegmann, Tristan Cook, Sylvester Kollin, and Alexa Pan. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnweuaua0f4c"> <span class="footnote-back-link"><sup><strong><a href="#fnrefweuaua0f4c">^</a></strong></sup></span><div class="footnote-content"><p> For even more references, see all the content gathered on <a href="https://longtermrisk.org/msr">this page</a> , and more recently, <a href="https://www.lesswrong.com/posts/mm8sFBpPH3Bb2NhGg/three-reasons-to-cooperate">this post</a> written by Paul Christiano and <a href="https://arxiv.org/pdf/2307.04879.pdf">this paper</a> by Johannes Treutlein.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnx18fvnjp5eq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefx18fvnjp5eq">^</a></strong></sup></span><div class="footnote-content"><p> If you know any plausible implication that I don&#39;t list here — then either I don&#39;t buy that it&#39;s an implication of ECL, or it doesn&#39;t seem sufficiently decision-relevant to me, or I haven&#39;t thought about it / forgot about it and you should let me know.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpneoqb5vw9"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpneoqb5vw9">^</a></strong></sup></span><div class="footnote-content"><p> Whereas today, we can focus on handing-off the future to a broadly competent and healthy civilization, and trust decisions about what to do with the future to them.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfbnqsghmnv4"> <span class="footnote-back-link"><sup><strong><a href="#fnreffbnqsghmnv4">^</a></strong></sup></span><div class="footnote-content"><p> When I discuss how we should “care more about other humans&#39; universe-wide values”, I exclusively refer to universe-wide values held by humans on our current planet Earth, as opposed to values that might be held by distant human-like species. But the reason to benefit such values is to generate evidence that other people benefit our values on distant planets (not just here, on planet Earth). So why focus specifically on humans&#39; values? The reason is that we are more confident that some people treasure them, and it&#39;s easy to benefit them via supporting humans who support them. For more, see <a href="https://docs.google.com/document/d/1XCZ-g_GAyZwfJfWHTRVyzptVU2_uI9tFWMG8x9j4C_o/edit#heading=h.y1hw7rervyd">here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnd6bt2xf37fd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd6bt2xf37fd">^</a></strong></sup></span><div class="footnote-content"><p> “Misaligned AI” refers to AI whose values are very different from what was intended by the evolved species that first created them. If a distant species has very different values from us, and successfully aligns AI systems that they create, I wouldn&#39;t count those as “misaligned AIs”.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwi0f1v4h1qk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwi0f1v4h1qk">^</a></strong></sup></span><div class="footnote-content"><p> Or any other kind of acausal effects.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4423tsgb79u"> <span class="footnote-back-link"><sup><strong><a href="#fnref4423tsgb79u">^</a></strong></sup></span><div class="footnote-content"><p> Premature commitments are often a gamble that might gain <i>you</i> a better bargaining position while carrying a risk of <i>everyone</i> getting a lower payoff. Since that&#39;s quite uncooperative, it seems plausible that ECL could discourage premature commitments. So this might be a reason to spread knowledge about ECL.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9j6fzbxzuod"> <span class="footnote-back-link"><sup><strong><a href="#fnref9j6fzbxzuod">^</a></strong></sup></span><div class="footnote-content"><p> Though also possible that <i>un</i> careful thinking could increase them — given that they are by-their-nature caused by humanity making errors in what order they learn about and commit to doing certain things.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8k49uv7ijvl"> <span class="footnote-back-link"><sup><strong><a href="#fnref8k49uv7ijvl">^</a></strong></sup></span><div class="footnote-content"><p> And ideally, you would also think about other opportunities that faction A and faction B would have of benefiting each other, since you might also be providing evidence about those. Even more ideally, you might think about possible gains from trades that involve even more factions.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsocdvs8tsuk"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsocdvs8tsuk">^</a></strong></sup></span><div class="footnote-content"><p> Though the total effort that goes to each should perhaps still be allocated based on the number of people who support each set of values and who are sympathetic to ECL. Potentially adjusted by speculation about whether either set of values is underrepresented (among ECL-sympathizers) on Earth compared to the universe-at-large, in which case we should prioritize that set of values higher.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnygfd8lysjb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefygfd8lysjb">^</a></strong></sup></span><div class="footnote-content"><p> It will be <i>the most</i> evidence for the actions of people in <i>exactly</i> my position. But this is not where most of my acausal influence will come from, since even a small amount of evidence across a sufficiently larger number of actors will weigh higher. The hypothesis that I&#39;m putting forward here is that there might be some fairly broad class of actors which still share some key similarities with you, whose decisions your decisions provide more evidence about. And that your values might be (or be correlated with) one of the key similarities.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2f0pffnolcf"> <span class="footnote-back-link"><sup><strong><a href="#fnref2f0pffnolcf">^</a></strong></sup></span><div class="footnote-content"><p> Though I am personally somewhat sympathetic to both upside- and downside-focused values, so this doesn&#39;t have a big impact on my all-things-considered view.</p></div></li><li class="footnote-item" role="doc-endnote" id="fniuznazkhk0b"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiuznazkhk0b">^</a></strong></sup></span><div class="footnote-content"><p> Even if the aliens who went extinct shared our values, their choice to prioritize non-AI extinction risk less could still have been net-positive ex-ante. For example, they might have reallocated resources in a way that reduced AI takeover risk by 0.1% and increased non-AI extinction risk by 0.1001%. The added 0.0001% of x-risk might have been worth the benefit of leaving behind empty space rather than AI-controlled space in 0.1% of worlds.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpe2xs41o2x7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpe2xs41o2x7">^</a></strong></sup></span><div class="footnote-content"><p> In particular,  ECL suggests that we should discount benefits to aliens insofar as they on average correlate less strongly with us than the average civilizations-with-our-values do. (When making relevant decisions.)</p></div></li><li class="footnote-item" role="doc-endnote" id="fn40zg3jy06vx"> <span class="footnote-back-link"><sup><strong><a href="#fnref40zg3jy06vx">^</a></strong></sup></span><div class="footnote-content"><p> As an example of someone with this view: <a href="https://www.facebook.com/yudkowsky/posts/10152588738904228">This facebook post</a> by Eliezer Yudkowsky starts “I think that I care about things that would, in your native mental ontology, be imagined as having a sort of tangible red-experience or green-experience, and I prefer such beings not to have pain-experiences. Happiness I value highly is more complicated.” Yudkowsky has also written about the complexity and fragility of value elsewhere, eg <a href="https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile">here</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnstsq0mzojoo"> <span class="footnote-back-link"><sup><strong><a href="#fnrefstsq0mzojoo">^</a></strong></sup></span><div class="footnote-content"><p> “Misaligned AI” refers to AI whose values are very different from what was intended by the evolved species that first created them. If a distant species has very different values from us, and successfully aligns AI systems that they create, I wouldn&#39;t count those as “misaligned AIs”.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn63f2sg768l9"> <span class="footnote-back-link"><sup><strong><a href="#fnref63f2sg768l9">^</a></strong></sup></span><div class="footnote-content"><p> In particular,  ECL suggests that we should discount benefits to AI insofar as they correlate less strongly with us than actors-with-our-values do.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/EeXSjvyQge5FZPeuL/implications-of-evidential-cooperation-in-large-worlds#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/EeXSjvyQge5FZPeuL/implications-of-evidential-cooperation-in-large-worlds<guid ispermalink="false"> EeXSjvyQge5FZPeuL</guid><dc:creator><![CDATA[Lukas Finnveden]]></dc:creator><pubDate> Wed, 23 Aug 2023 00:43:45 GMT</pubDate> </item><item><title><![CDATA[South Bay Casual Group Walk]]></title><description><![CDATA[Published on August 22, 2023 10:43 PM GMT<br/><br/><p> Join us for a group walk through the Baylands Nature Preserve in Palo Alto. At the beginning, we&#39;ll decide if we want to walk the <a href="https://goo.gl/maps/iarMq8pHUbjELrcr9"><u>boardwalks</u></a> (3ish miles, lots of birds) or to <a href="https://goo.gl/maps/hF5dGD4tMS1foMPf8"><u>Byxbee Park</u></a> (5ish miles, conceptual art sculptures).</p><p> The walking pace will be casual and relaxed. If you&#39;re reading this, you and any friends are welcome to join.<br></p><p> <strong>Date:</strong> Saturday, September 9, 2023</p><p> <strong>Meeting at</strong> <strong>2pm</strong></p><ul><li> We will start walking at 2:15pm. If you are running late or want to meet up with us later on, please let me know (southbaymeetup@gmail.com).</li><li> No exact end time, probably around 5pm. We may grab dinner afterwards. IKEA is 2.4 mi away😏</li></ul><p> <strong>Location:</strong> <a href="https://goo.gl/maps/2cFoETZuJ9cY5cPU7">Baylands Nature Preserve</a><br> 2500 Embarcadero Rd, Palo Alto, CA 94303</p><ul><li> We&#39;ll be meeting in the <a href="https://goo.gl/maps/8L1swVhdJnpdR39DA">outdoor picnic tables</a> , located northeast of the nature center (see image below)</li><li> Free parking is located in the lot right next to the tables</li></ul><p> Bring a water bottle, hat, and sunscreen. The walk will not be shaded. I&#39;ll have a few extra water bottles in case you forget. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/plnmenfzbcfr2o59wrej" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/hxg3k0ebvmueh65mg7w1 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/dfewntkcqaipw7tukku1 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/qrgybnrjbhdekcp2i3fq 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/zqai6dua6ilkehcqp74i 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/svgmo3pifk9id1j4olqk 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/w62qfqvt0x8vqaqbl314 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/hl2btdjbhqsou6wpkfjl 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/jzazrrwcvn9oerg0tclk 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/rcpemisx5aonk4gcvawf 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PK9u3TJrxmbSCJBFy/bdfbmwkkafjujkxdsu1v 1763w"></figure><br/><br/> <a href="https://www.lesswrong.com/events/PK9u3TJrxmbSCJBFy/south-bay-casual-group-walk#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/events/PK9u3TJrxmbSCJBFy/south-bay-casual-group-walk<guid ispermalink="false"> PK9u3TJrxmbSCJBFy</guid><dc:creator><![CDATA[allisona]]></dc:creator><pubDate> Wed, 23 Aug 2023 02:33:12 GMT</pubDate></item><item><title><![CDATA[Walk while you talk: don't balk at "no chalk"]]></title><description><![CDATA[Published on August 22, 2023 9:27 PM GMT<br/><br/><p> Some days ago, after a long conversation with a friend, I took retrospective notes, recalling what we discussed. (I wrote over 50 items. This is not a small sample.) For parts of the time, we walked; for other parts, we stayed in roughly the same place. Writing the notes, I could tell I was missing up to half of what we said (and forgot the order) for those stationary parts. I had no such issues recalling the walking-concurrent parts.</p><p> I&#39;ve noticed this kind of discrepancy a bit before, but this was when it became obvious. Walking as you talk can help a lot towards remembering what&#39;s said.</p><p> This may be a quirk of how my mind works and wouldn&#39;t apply to everyone. I doubt I&#39;m totally unique, so it probably at least applies to some other people, even if not everyone.</p><h1> Why would this work?</h1><p> This part is justified speculation, not explicitly confirmed, and not important if you&#39;re just using the method.</p><p> What seems to be going on here is that my episodic memory encodes the surroundings I see together with the conversation I hear. Later, when I recall the event, I think of what I saw around me to help bring the event to mind, and what I heard (or interpreted at the time from what I heard) comes with it.</p><p> I see two ways that the walking helps:</p><ul><li> Walking means I&#39;ll be in different places at different stages of the conversation, so each visual memory (exact place and its surroundings) associates to fewer auditory/semantic memories (things spoken), and thus associates more strongly.</li><li> Moving in space enforces a continuous path (unless I can teleport, so maybe refrain from teleporting during conversations), which is easy to interpolate and &quot;walk thru&quot; from just a few points — much easier than the unpredictable transitions of conversation.</li></ul><p> I don&#39;t know which is more important.</p><h1> Corollaries and extensions</h1><p> The mechanism here depends on you moving, not those with whom you speak. If you only care about your own memory (or the circumstances otherwise demand it), you could call them while walking alone and get the same effect. (I have tried this. It works.)</p><p> The method should also apply equally well to one-sided speeches to which you listen, tho those tend to be more predictable anyway.</p><p> The mechanism here depends on movement and changing surroundings, not specifically walking. I expect you&#39;d get the same effect if you&#39;re cycling/driving/riding a vehicle, so long as you&#39;re observing what you pass by as you do so. That happens to be easy in the case of walking: you have no vehicle to obstruct your vision, and you&#39;re exposed to mild, attention-demanding risk from every direction.</p><p> If you care about the order in which things were said, don&#39;t go over the same place twice. Repeating locations makes your path overlap with itself, complicating sequential recall. (This mistake caused a bit of confusion in my example at the beginning.)</p><p> Apparently, the brain models some kinds of abstract spaces similarly to how it navigates in real life. (Citation needed. Relevant keywords &quot;hippocampus&quot; and &quot;have you ever played a modern video game?&quot;) You might get the same effect if you move in an intricate video game while you talk. That might require full VR. (I have not tried this.)</p><br/><br/> <a href="https://www.lesswrong.com/posts/trZSdr49bygabxQtG/walk-while-you-talk-don-t-balk-at-no-chalk#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/trZSdr49bygabxQtG/walk-while-you-talk-don-t-balk-at-no-chalk<guid ispermalink="false"> trZSdr49bygabxQtG</guid><dc:creator><![CDATA[dkl9]]></dc:creator><pubDate> Tue, 22 Aug 2023 21:27:48 GMT</pubDate></item></channel></rss>