<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 25 日星期五 04:13:34 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Nuclear consciousness]]></title><description><![CDATA[Published on August 25, 2023 1:28 AM GMT<br/><br/><p>马克·吐温<i>在《亚瑟王宫廷中的康涅狄格洋基队》中</i>的一段话自从我大约 35 年前读到以来一直萦绕在我的脑海中：</p><blockquote><p>对我来说，在这趟沉重而悲伤的朝圣之旅中，在这永恒之间的可悲漂流中，我所想的就是向外看，谦卑地过一种纯洁、高尚、无可指摘的生活，并保存我体内那一个真正属于我的微观原子：其余的人可能会降落在阴间，并欢迎我所关心的一切。</p></blockquote><p>那“一个原子才是真正的我”！</p><p>我认识到我身体的某些部分我不会认为是“我”，例如我的头发或指甲，甚至是我四肢的大部分（如果我不得不失去它们的话）。但如果我失去大脑的大部分，我的自我就会大大削弱——这肯定需要的不仅仅是一个原子。</p><p>这就是为什么精神疾病如此令人痛苦，包括伴随衰老而来的损害，即使不是由疾病引起的。如果<a href="https://plato.stanford.edu/entries/locke-personal-identity/">个人身份是由心理连续性来定义的</a>（也就是说，“我是昨天的我，因为我记得昨天是那个人”），那么单纯的遗忘是一种逐渐失去自我的方式。我想起了<i>2001 年</i>HAL 的拆卸，他的认知功能被一次一个地移除：“戴夫，我的思维在消失。我能感觉到它。毫无疑问……我很害怕，戴夫。”</p><p>大多数人，包括我自己，在谈论自我复合性时的方式并不一致。我们知道我们的大脑是由各个部分组成的，并且<a href="https://www.psychologytoday.com/us/blog/erasing-stigma/202001/the-neuroscience-behavior-five-famous-cases">听说过一些临床案例，</a>其中一个或另一个部分被切割或移除，然后那个人忘记了单词的含义但仍然可以拼写，或者失去了创造新记忆的能力但保留了旧的记忆我们<a href="https://doi.org/10.1007%2Fs11065-020-09439-3">听说过这样的情况</a>：人们有两个功能性的大脑半球，但彼此之间不进行交流——其中一半实际上不知道另一半在做什么。<i>然而，</i>尽管有了这些知识，我们仍然把自己称为原子存在——原子是希腊语中“不可分割”（ἄτομος）的意思，也是指一个存在与另一个存在绝对不同的存在。</p><p>许多重要的概念都基于“我们是个体”这一观念。在伦理学中，我们希望能够说：“亚历克斯是凶手！”这意味着整个亚历克斯都犯下了谋杀罪，而不仅仅是亚历克斯的一半大脑，都应该为此行为受到惩罚。 （我们把他剩下的部分放在哪里？）</p><p>或者在较小的范围内，只是为了能够说“我喜欢辛辣的食物”。也许我的某些部分不喜欢辛辣的食物，而那些能控制的部分喜欢用香料的痛苦来折磨其他部分——它们似乎确实在挣扎和扭动，这就是乐趣的一部分。</p><p>此外，人们很常见（几乎是本能）区分“真实的自我”和外部影响，尤其是在<a href="https://doi.org/10.1177/0146167213508791">道德上对它们进行区分</a>。人类动物有各种各样的欲望，但是欺骗配偶或吃掉所有饼干的欲望被认为是这个人正在努力反对的力量（除非你已经认为他是一个恶棍），而拯救溺水的小狗的欲望则被认为是力量他的真正本质（即使你认为他<i>主要</i>是一个恶棍）。</p><p>犹太教和基督教通常从这样的假设开始：意识是原子的，称为灵魂。不仅需要对灵魂的善良进行核算，而且​​整个灵魂要么得救，要么被诅咒。即使我们无视那些火与硫磺的传教士，他们认为天堂和地狱是物理场所，而诅咒是发生<i>在</i>你身上的事情，而不是你对自己做的事情，整个灵魂要么走向好，要么走向坏的假设结局似乎不可避免。</p><p>这是我非常喜欢的关于地狱的描述，摘自CS刘易斯的<i>《大离婚》</i> ，因为它符合活生生的人类心理。然而，它仍然假定意识是统一的：</p><blockquote><p> “没错。他们上去从一扇窗户往里看。拿破仑就在那里。”</p><p> “他在做什么？”</p><p> “走来走去——一直走来走去——左右，左右——一刻也没有停下来。两个小伙子观察了他大约一年，他从未休息过。而且一直自言自语。” “这是苏尔特的错。这是内伊的错。这是约瑟芬的错。这是俄罗斯人的错。这是英国人的错。”一直都是这样。一刻也没有停止过。一个小胖子，看上去有点累。但他似乎无法停止。”</p></blockquote><p>整个拿破仑都堕落到这个小螺旋中，而不仅仅是他的一部分。</p><p>还有其他意见。摩尼教始于两个基本原则/神的观念，一个是好的（“伟大之父”），一个是坏的（“黑暗之王”）。我们所知道的物质世界是它们混合在一起的结果，而一个好人在地球上应该做的工作就是把它们分开，把坏的从它们身上去掉，让善良自由地飞走。摩尼教徒并不期望最终会进入天堂或地狱：他们期望自己的一部分最终会进入天堂或地狱。</p><p>摩尼教深受佛教影响（它是佛教、基督教和琐罗亚斯德教的有意结合），而佛教以反对统一自我的概念而闻名。有时，佛教徒说任何“我”或“我”都是幻觉，但在更长的解释中，听起来更像是他们相信“我”或“我”是一个构造，就像一把椅子是临时的、模糊的组合。原子或密西西比河是一种说法，“水通常流到这里”，但并不对应于特定的水原子，而且河流的水流甚至河道都可以改变。</p><p>佛教徒将众生描述为五种“蕴、堆、集、群”（梵文<a href="https://en.wikipedia.org/wiki/Skandha">स्कन्ध skandha</a> ），即色（ <a href="https://en.wikipedia.org/wiki/R%C5%ABpa">रूप rūpa</a> ）、受（ <a href="https://en.wikipedia.org/wiki/Vedan%C4%81">वेदना vedanā</a> ）、想（ <a href="https://en.wikipedia.org/wiki/Samjna_(concept)">संज्ञा saṃjñā</a> ）、意行。 （ <a href="https://en.wikipedia.org/wiki/Sa%E1%B9%85kh%C4%81ra">संस्कार saṃskāra</a> ）和意识（ <a href="https://en.wikipedia.org/wiki/Vij%C3%B1%C4%81na">विज्ञान vijñāna</a> ）。即使这样分解，我发现这些词在英语中仍然是模糊的概念，但我确信它们在原始语言中是精确的、技术性的词汇。如果拿走这些堆中的任何一个，就不再有人/众生，但加在一起，人就存在了。在佛教与西方的<a href="https://www.webpages.uidaho.edu/ngier/307/milina.htm">早期接触</a>中，那先谈到自己就像一辆战车一样可以分解：战车不是它的轮子，不是它的车轴，不是它的座位，当你把这些部件移走时，就不再有战车了。 （鉴于此，我不会说战车是一种“幻觉”，但这是一种说话方式。）</p><p>以类似但不完全相同的方式，我们可以说一个人是海马体、杏仁核、前额皮质等的总和。 HAL 是他的（可移除的）认知模块的总和。</p><p>您可能已经注意到，我得出的结论是意识是复合的（在这个网站上，我希望大多数读者都同意），但其后果难以接受。也许摆脱责备的概念并不太令人不安：“亚历克斯是凶手！让我们惩罚亚历克斯！”是一种报应性的正义概念，无论如何，我们中的许多人更喜欢恢复性正义：“出了问题，亚历克斯犯了谋杀罪。让我们修复亚历克斯！”但如果没有责备，也就没有赞扬。当我将这种思维应用到自己身上时，我就变成了第三人称，因为“我想要鼓励的自己的部分”被描述得比“我”更远。</p><p>如果我吃辛辣的食物，因为它会伤害我的嘴，而我喜欢吃辛辣的食物，那又怎么样呢？如果这种相互作用发生在两个人体之间——一个人类动物故意给另一个人类动物造成痛苦——那么这将是一个糟糕的情况，这种情况可能需要一些恢复性（如果不是报应性）正义。将意识原子分裂成意识位至少开启了一种可能性，即不同头脑中的意识位可以以与一个头脑中的意识可分解相同的方式组合。</p><p>在过去的几年里，我一直在想，原子意识的概念是否需要被另一个物理隐喻所取代：<strong>核意识</strong>。核力量杂乱而复杂，但射程短。在质子或中子内，三个（价）夸克通过交换胶子而相互吸引，当胶子飞行时，它们会产生新的夸克-反夸克对，这些夸克-反夸克对本身会与更多的胶子相互吸引。在一篇<a href="https://www.fnal.gov/pub/today/archive/archive_2014/today14-01-31.html">热门物理文章</a>中，我曾经这样画过： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/ivrivbw3eoaay4biqhwj"></figure><p>单个质子和中子通过这种相互作用的较弱的边缘场效应在原子核中相互吸引。胶子很少偏离产生它们的夸克，原子核中的质子和中子比原子核内的夸克相距更远。 （对于其他书呆子：而电场会像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="1/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>，胶子场像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="e^{-r/r_0}/r^2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">e</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; vertical-align: -0.267em; padding-right: 0.06em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span></span></span></span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">r</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>一样衰减。）不同原子中粒子之间的核力非常小，但并不严格为零。</p><p>质子中的夸克、原子核中的质子以及不同原子的原子核之间的边界并不是绝对不同的事物，而是在数量上以很大的幅度很好地分开。为了证明它们可以混合这一事实，如果两个原子核碰撞得足够猛烈，则两个完整原子核的所有夸克和胶子都会变成一种汤。这是我在<a href="http://coffeeshopphysics.com/cmsresults/#2012-05-11">另一篇文章</a>中绘制该过程的方式： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qXZrobGdAycNDBhET/lgg9wv7qc9dn4eildo0m"></figure><p>在宇宙的早期，所有的空间都充满了均匀的夸克-胶子等离子体，直到它膨胀到足以使夸克彼此远离，并凝结成质子和中子。</p><p>现在打个比喻：也许意识的分解就像质子的分解一样。一个有思想的生物似乎是一个单一的、不可分割的单元，就像质子一样，但如果你观察它的内部，你会发现它是由各个部分组成的。佛教徒在内观禅修中看到五蕴；神经科学家观察大脑的功能。在<a href="https://www.lesswrong.com/posts/i8C9KSryDFj4EENvx/reality-and-reality-boxes">现实和现实盒子</a>中，我强调，我认为这两种有效的方法并不研究同一类事物：内省揭示了非常真实的主观现实，而科学研究揭示了非常真实的物理现实，这些都是足够不同，他们可能应该有不同的词，而不是过多地使用“现实”这个词。</p><p>除了单个质子可分解之外，多个质子也是可组合的：它们可以混合在一起形成流体。质子的非原子性是双向的，既可以低于单位水平，也可以高于单位水平。在这个意识隐喻中，我们的头脑中漂浮着潜意识的思想和情感碎片，也有在社区中流动的非意识思想和情感：一群人思考。我的大脑各部分之间通过厚厚的神经线束紧密相连，而生活在一起的人的大脑通过面部表情、群体活动和言语的联系却很弱。</p><p>这两种连接是否仅在规模上有所不同，例如质子内的直接夸克-胶子相互作用以及它们之间的边缘场？或许。这就是我一直在修改的想法，并将其称为核意识，而不是原子意识。我们确实知道灵长类动物之间存在神经效应，例如<a href="https://doi.org/10.1016/0926-6410(95)00038-0">镜像神经元</a>，而自然选择并不尊重身体之间的分界线。当我读到荣格的集体无意识概念时，我就是这样解释它的。 （有趣的是，只有中层是完全清醒的：无论是脑叶白质切除的人还是人群的行为都没有连贯性。）</p><p>我长期以来一直认为（并认为）人们<a href="https://doi.org/10.1017%2FS1478951517000621">在存在上是孤立的</a>：只要单词与不同的事物相匹配，就无法知道我所看到的红色对你来说是否是红色。就像维特根斯坦在<i>《哲学研究》</i>中的盒子里的甲虫比喻一样：</p><blockquote><p>如果我说我自己只有从我自己的经历中才知道“痛苦”这个词的含义，那么我是否也不能对其他人也这么说呢？我怎么能如此不负责任地概括这一案例呢？</p><p>现在有人告诉我，只有他自己才知道什么是痛苦！假设每个人都有一个盒子，里面装着一些东西：我们称之为“甲虫”。没有人可以看别人的盒子，每个人都说只有看他的甲虫才知道甲虫是什么。在这里，每个人的盒子里都有可能有不同的东西。人们甚至可以想象这样的事情是不断变化的。但是假设“甲虫”这个词在这些人的语言中有用吗？如果是这样，它就不会被用作事物的名称。盒子里的东西在语言游戏中根本没有地位；甚至不能作为某种东西：因为盒子甚至可能是空的。不，可以根据盒子里的东西来“划分”；不管它是什么，它都会抵消。</p><p>也就是说：如果我们根据“对象和指称”的模型来解释感觉表达的语法，那么对象就会被视为无关紧要而被排除在外。</p></blockquote><p>但我确实相信我大脑的某些部分知道我大脑的其他部分是什么样子，因为甲虫并没有对它们隐藏。现在，如果我大脑各部分之间的分离与个体之间的分离相同，也许我们确实在某种程度上看到了彼此的头骨内部。核意识原则上是对存在隔离的反驳——人与人之间只存在比人内部更大的隔离，而不是不同类型的隔离。</p><p>我想知道，如果心灵感应成为可能——通过深深连接到我们神经元或其他东西的无线电——我们是否仍然会感觉我们正在<i>互相</i>交谈，就像心灵感应故事中所描述的那样，或者我们会觉得我们<i>都是</i>一个人吗？ ，就像我大脑的各个部分相互作用一样？<i>了解</i>某人（ <a href="https://www.leaflanguages.org/french-grammar-verbs-savoir-vs-connaitre/">connaître，而非 savoir</a> ）是<i>成为</i>那个人的一小步吗？</p><p>我想从豆荚人的角度阅读<i>《掠尸者的入侵》</i>的一个版本。他们想与地球人交流，但他们有心灵感应，所以他们成为了地球人……</p><p>尽管我认为我们可以排除原子意识（对不起，马克·吐温），但我不确定它会消失多少，也不确定我们是一个、松散连接且几乎没有意识的大脑这一想法是否认真对待。</p><br/><br/><a href="https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-consciousness#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/qXZrobGdAycNDBhET/nuclear-意识<guid ispermalink="false">qXZrobGdAycNDBHET</guid><dc:creator><![CDATA[Jim Pivarski]]></dc:creator><pubDate> Fri, 25 Aug 2023 01:28:04 GMT</pubDate> </item><item><title><![CDATA[Would it be useful to collect the contexts, where various LLMs think the same?]]></title><description><![CDATA[Published on August 24, 2023 10:01 PM GMT<br/><br/><p><i>我最初的想法是让我们看看小型的、可解释的模型在哪里做出与巨大的、危险的模型相同的推论，并重点关注小型模型中的这些情况，以帮助解释更大的模型。</i>我很可能错了，但为了产生良好影响的机会很小，我已经建立了<a href="https://github.com/Huge/same-next-lang-token">一个存储库</a>。<br>在开始实际生成与该上下文匹配的上下文+语言模型对/组之前，我希望得到您对该方向的反馈。</p><br/><br/> <a href="https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/AGPgMBp6eN95uxJyc/would-it-be-useful-to-collect-the-contexts-where-various<guid ispermalink="false"> AGPgMBp6eN95uxJyc</guid><dc:creator><![CDATA[Martin Vlach]]></dc:creator><pubDate> Thu, 24 Aug 2023 22:01:51 GMT</pubDate> </item><item><title><![CDATA[Help Needed: Crafting a Better CFAR Follow-Up Survey]]></title><description><![CDATA[Published on August 24, 2023 5:26 PM GMT<br/><br/><p> <strong>tl;dr：通过帮助设计 CFAR 后续调查，为塑造理性计划的未来做出贡献。</strong></p><p>我正在开展一个项目来评估 CFAR 布拉格 2022 研讨会的有效性，但如果初步反馈具有建设性，我可能会将调查范围扩大到所有校友。如果时间允许，我计划公开分享结果。我知道调查方法的陷阱，但我仍然认为这是值得的，并且我个人了解可以从这些数据中受益的人们（及其项目）。</p><p>我在此恳请您帮助我设计调查：</p><ol><li>实际上有人填写了它（所以长度、激励措施都可以，...）</li><li>它提供了以下信息：</li></ol><ul><li>回顾性反馈：与会者如何看待过去的研讨会？这是否给他们的方法或思维带来了任何重大变化？</li><li>未来方向：与会者热衷于进一步探索哪些主题或形式？</li></ul><p>您可以<a href="https://docs.google.com/document/d/1FR-vZyIbthKsfnZ2SUy1CDYU0-tXe7bFNXXCxZWHtQg/edit?usp=sharing">在此处</a>查看我当前的草稿。</p><p>我真诚地感谢任何反馈，无论是详细的批评还是总体印象。</p><br/><br/> <a href="https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LExAdFPugEjeFvPTE/help-needed-crafting-a-better-cfar-follow-up-survey<guid ispermalink="false"> LexAdFPugEjeFvPTE</guid><dc:creator><![CDATA[kotrfa]]></dc:creator><pubDate> Thu, 24 Aug 2023 17:26:13 GMT</pubDate> </item><item><title><![CDATA[AI #26: Fine Tuning Time]]></title><description><![CDATA[Published on August 24, 2023 3:30 PM GMT<br/><br/><p> GPT-3.5微调就在这里。 GPT-4 的微调只剩几个月了。获得一个功能强大的系统将会变得更加容易，该系统可以做您想要它做的事情，并且知道您想让它知道什么，特别是对于企业或网站而言。</p><p>作为一项实验，我将我认为值得强调的部分加粗，作为与典型一周相比异常重要或有趣的版本。</p><h4>目录</h4><ol><li>介绍。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/table-of-contents">目录</a>。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-offer-mundane-utility">语言模型提供了平凡的实用性</a>。 Claude-2 与 GPT-4。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/language-models-dont-offer-mundane-utility">语言模型不提供平凡的实用性</a>。没有意见，没有代理人。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fact-check-misleading">事实核查：误导</a>。人工智能事实检查器让人们更加困惑而不是更少。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/gpt-real-this-time"><strong>GPT-4 这次是真实的</strong></a>。微调GPT-3.5，很快GPT-4。问问它是否确定。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/fun-with-image-generation">图像生成的乐趣</a>。中途修复浩。哦，没有人工智能色情片。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/deepfaketown-and-botpocalypse-soon">Deepfaketown 和 Botpocalypse 即将推出</a>。对抗性的例子开始出现。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/they-took-our-jobs">他们抢走了我们的工作</a>。 《纽约时报》加入针对 OpenAI 的版权诉讼。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/introducing">介绍</a>. Palisade Research 将研究潜在危险的人工智能可供性。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/in-other-ai-news">在其他人工智能新闻中</a>。谁适应人工智能最快？尝试衡量这一点。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/quiet-speculations"><strong>静静的猜测</strong></a>。杰克·克拉克提出了有关未来的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-quest-for-sane-regulations"><strong>寻求健全的监管</strong></a><strong>。</strong> FTC 向 OpenAI 提出了一个不同类型的问题。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-week-in-audio">音频周</a>。这是双赢。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/no-one-would-be-so-stupid-as-to"><strong>没有人会傻到这样</strong></a>。让人工智能有意识？哦，来吧。</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/aligning-a-smarter-than-human-intelligence-is-difficult">调整比人类更聪明的智能是很困难的</a>。 IDA 的证据？</li><li> <a target="_blank" rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/people-are-worried-about-ai-killing-everyone">人们担心人工智能会杀死所有人</a>。民调数字非常清楚。</li><li> <a rel="noreferrer noopener" href="https://thezvi.substack.com/i/136169689/the-lighter-side" target="_blank">轻松的一面</a>。只有一半。</li></ol><p></p><span id="more-23517"></span><h4>语言模型提供平凡的实用性</h4><p>Claude-2 和 GPT-4 哪个模型更好？</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1691442648828067840">Rowan Cheung 认为克劳德 2 更胜一筹</a>。您可以获得 100k 上下文窗口、上传多个文件的能力、截至 2023 年初（相对于 2021 年底）的数据以及更快的处理时间，所有这些都是免费的。作为交换，你放弃了插件，数学就更差了。 Rowan 没有提到的是，GPT-4 在原始智能和通用能力方面具有优势，而且设置系统指令的能力也很有帮助。他暗示他甚至没有为 GPT-4 支付每月 20 美元的费用，这让我觉得很疯狂。</p><p>我在实践中的结论是，默认情况下我将使用 Claude-2。如果我关心响应质量，我会使用两者并进行比较。当 Claude-2 明显摔倒时，我会转向 GPT-4。经过反思，“同时使用”通常是正确的策略。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rowancheung/status/1693616971114328258">他还查看了插件</a>。插件实在是太多了，至少有867个。哪些值得使用？</p><p>他推荐 Zapier 通过触发操作实现自动化，ChatWithPDF（我使用 Claude 2），Wolfram Alpha 用于实时数据和数学，VoxScript 用于 YouTube 视频转录和网页浏览，WebPilot 看起来重复，网站性能，尽管我不是确定为什么要使用 AI 来实现这一点，ScholarAI 用于搜索论文，Shownotes 来总结播客（为什么？），ChatSpot 用于营销和销售数据，Expedia 用于假期计划。</p><p>我刚刚预订了一次旅行，最近又去了另外两次旅行，我没有想到使用 Expedia 插件，而不是使用 Expedia 等网站（我的首选计划是 Orbitz 航班和 Google 地图酒店）。下次我应该记得尝试一下。</p><p>研究声称， <a target="_blank" rel="noreferrer noopener" href="https://marginalrevolution.com/marginalrevolution/2023/08/thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=thinking-about-god-increases-acceptance-of-artificial-intelligence-in-decision-making">上帝的显着性会增加人们对人工智能决策的接受度</a>。我会等待这一复制。如果这是真的，它指出人工智能将有多种方式让天平向我们倾斜，让我们接受它们的决定，或者人类有可能协调起来反对人工智能，这与任何相关考虑因素没有太大关系。人类是相当有缺陷的代码。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/mattshumer_/status/1694023167906394575">Matt Shumer 推荐使用 GPT-4 系统消息。</a></p><blockquote><p>用它来帮助您在不熟悉的领域做出工程决策：</p><p>您是一位工程奇才，在解决跨学科的复杂问题方面经验丰富。你的知识既广又深。您也是一位出色的沟通者，能够提供非常周到且清晰的建议。</p><p>您以这种格式这样做，思考您面临的挑战，然后提出多个解决方案，然后审查每个解决方案，寻找问题或可能的改进，提出一个可能的新的更好的解决方案（您可以结合其他解决方案的想法） ，引入新想法等），然后给出最终建议：</p><p> “`</p><p> ## 问题概述</p><p>$problem_overview</p><p> ## 挑战</p><p>$挑战</p><p>## 解决方案1</p><p> $solution_1</p><p> ## 解决方案2</p><p> $solution_2</p><p> ## 解决方案3</p><p> $solution_3</p><p> ## 分析 ### 解决方案1 ​​分析</p><p>$solution_1_analysis</p><p> ###解决方案2分析</p><p>$solution_2_analysis</p><p> ###解决方案3分析</p><p>$solution_3_analysis</p><p> ## 其他可能的解决方案</p><p>$additional_possible_solution</p><p> ＃＃ 推荐</p><p>$推荐</p><p>“`</p><p>每个部分（问题概述、挑战、解决方案 1、解决方案 2、解决方案 3、解决方案 1 分析、解决方案 2 分析、解决方案 3 分析、其他可能的解决方案和建议）都应该经过深思熟虑，至少包含四句话思考。</p></blockquote><h4>语言模型不提供平凡的实用性</h4><p>声称人工智能可以 97% 预测热门歌曲？这完全没有道理，因为即使对歌曲本身进行完美的分析也不可能做到这一点，歌曲的命运太取决于其他因素了？ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/random_walker/status/1692158875867197651">原来是数据泄露</a>。泄漏也比较明显。</p><blockquote><p> Arvind Narayanan：论文的数据有 24 行，每行 3 个预测变量和 1 个二元结果。 WTAF。这篇文章非常混乱，所以很难看清这一点。如果所有基于机器学习的科学论文都必须包含我们的清单，那么给猪涂口红就会困难得多。</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/introducing-the-reforms-checklist">Arvind 和 Sayash Kapoor 建议</a>使用他们的<a target="_blank" rel="noreferrer noopener" href="https://reforms.cs.princeton.edu/">改革清单，</a>该清单包含 8 个部分的 32 个项目，以帮助防止类似的错误。我没有详细检查，但我抽查的项目看起来很可靠。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.rockpapershotgun.com/gta-5-ai-mod-shot-down-by-take-two-even-as-rockstar-relax-policy-on-modding">《侠盗猎车手 5》的 Mod 制作者增加了一群 AI 崇拜者，他们用 AI 生成的对话说话</a>，Take Two 的回应是下架该 Mod，并向他们发出 YouTube 版权警告。游戏对人工智能产生了各种各样的反应，其中许多反应极其敌对，尤其是对人工智能艺术，而且这种情况很可能也会蔓延到其他地方。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693131333403615485">凯文·费舍尔得出了与我相同的结论</a>。</p><blockquote><p> Kevin Fischer：今晚我彻底放弃使用 ChatGPT 来帮助我的写作。这不值得</p><p>Christine Forte 博士：说得再多一点！</p><p>凯文·费舍尔（Kevin Fischer）：我写过的所有好作品都是意识流几乎是一击而出的。 ChatGPT 正在干扰该过程。与不干扰执行高阶抽象数学行为的计算器不同，写作是一种表达行为。</p></blockquote><p>这是一个因素。更大的问题是，如果你正在创造一些非通用的东西，人工智能就不会做好工作，而弄清楚如何让它做好体面的工作并不比你自己做体面的工作更容易。这并不意味着这里永远无事可做。如果你的任务变得通用，那么 GPT-4 就可以投入使用，你绝对应该这样做 - 如果你的写作开始，正如我在飞机上看到的那样，“亲爱的关心的病人”，那么那个人使用 GPT-4 是完全正确的。这是一种不同类型的任务。当然，GPT-4 有很多方法可以间接帮助我的写作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/tszzl/status/1693067607946207531">Llama-2 和年轻的赞福德一样，行事谨慎</a>（<a target="_blank" rel="noreferrer noopener" href="https://t.co/1v2ghB9rNj">纸上</a>）。</p><blockquote><p> Roon：哈哈，GPT4 比开源 llama2 限制更少，说教更少</p></blockquote><figure class="wp-block-image"><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a6ab5c-6d53-47c0-8a41-34a37746e458_1238x1244.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/bebojcuvf1waoy1uofce" alt="图像"></a></figure><blockquote><p> Roon：这里的重点不是要取笑元或 OSS——我对 oss 感到非常兴奋，它只是在科学上很有趣，即使没有采取任何特殊措施来制作令人讨厌的说教 rlhf 模型，这就是发生的事情。这是减少拒绝的积极努力。</p></blockquote><p>我的意思是，这也是为了取笑Meta，为什么要错过这样做的机会呢。我没有登记预测，但我预计 Llama-2 会做出更多错误拒绝，因为相对而言，我预计 Llama-2 会很糟糕。如果您想在积极拒绝方面达到一定的可接受的表现水平，那么您的消极拒绝率将取决于您的区分能力。</p><p>事实证明，答案并不是很好。 Llama-2 在识别有害情况方面非常非常糟糕，而是通过识别危险单词来工作。</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.aisnakeoil.com/p/does-chatgpt-have-a-liberal-bias">模型在拒绝表达政治观点方面做得越来越好</a>，特别是 GPT-4 实际上在这方面做得很好，并且比 GPT-3.5 好得多，如果你认为不表达这样的观点是好的。如前所述，Llama-2 太过分了，除非您对其进行微调以使其不关心（或找到某人的 GitHub 为您做这件事），否则它会做您想做的任何事情。</p><p>初创公司<a target="_blank" rel="noreferrer noopener" href="https://twitter.com/zachtratar/status/1694024240880861571">Embra 从人工智能代理转向</a>人工智能命令，它发现人工智能代理（至少目前在现有技术下）不起作用且不安全。 AI命令与非AI命令有何不同？</p><blockquote><p> Zach Tratar（Embra 创始人）：命令是一项狭义的自动化任务，例如“将此数据发送给销售人员”。在每个命令中，人工智能都不会“偏离轨道”并意外地将数据发送到不应该发送的地方。</p><p>然后，您可以与 Embra 一起轻松发现和运行命令。在运行之前，您同意。</p></blockquote><p>发现新命令似乎是潜在的秘密武器。这就像生成您自己的文本界面，或者努力扩展菜单以包含一堆新宏。它仍然有效，看起来就像在宏中构建一样可疑。</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/sherjilozair/status/1694496522652512270">Joshua Achiam</a> ：据我估计，人们试图让人工智能代理的繁荣提前大约七八年。至少这一次他们很快就明白了。还记得七八年前的人工智能聊天热潮吗？同样的问题，或多或少，但拖延的时间更长。</p><p> Sherjil Ozair：我同意这个方向，但我认为只是早了 1-2 年。它现在不起作用，但这只是因为世界上很少有人知道如何设计大规模强化学习系统，而且他们目前还没有构建人工智能代理。这不是“生成式人工智能”初创公司可以解决的问题。</p><p> Joshua Achiam：如果过去 4 年的趋势良好，那么 Eleuther 类型的黑客团体将在顶级研发公司大约 1.5 年后制定他们的解决方案版本。</p><p>谢尔吉尔·奥扎尔： <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/lfqjfuatfq3ssvblbbms" alt="💯" style="height:1em;max-height:1em">他们的瓶颈主要是由于缺乏良好的基础模型和微调。现在有了 llama-2/3 和 OpenAI 微调 API，它们就不再受阻碍了。不幸的是，这两件事都显着提高了我的p（厄运）。 *苦乐参半*</p></blockquote><p>我非常同意奥扎尔的时间表。使用 GPT-4 并且缺乏对其进行微调的能力，即使你做了非常好的脚手架，代理也会经常失败。我不希望定制脚手架的努力能够挽救足够多的东西，使产品能够用于通用目的，尽管我也不相信我所看到的“明显的事情”已经被尝试过。</p><p>通过微调，您有可能获得 GPT-4 级别的东西，但我的猜测是您仍然做不到。骆驼2号？ Fuhgeddaboudit。</p><p>有了类似于 GPT-5 的东西，当然有了可以微调的 GPT-5，我希望可以构建出更有用的东西。这个或 Llama-2 的最新公告和微调是否会提高我的 p(doom) 水平？不是特别的，因为我已经把所有的东西都烤好了。 Llama-2主要是Llama-1的隐含，花费很少，而且<a target="_blank" rel="noreferrer noopener" href="https://chat.lmsys.org/?arena">效果不是很好</a>。我确实同意 Meta 处于开源阵营是相当糟糕的，但我们已经知道了。</p><p>如果您想要为您的网站添加随机胡言乱语，ChatGPT 就适合您。如果你是微软并且需要更好的东西， <a target="_blank" rel="noreferrer noopener" href="https://www.businessinsider.com/microsoft-removes-embarrassing-offensive-ai-assisted-travel-articles-2023-8">那么就需要更加小心</a>。许多文章似乎都犯了错误，比如建议空腹去渥太华食品银行。</p><blockquote><p>微软发言人表示：“这篇文章已被删除，我们已确定该问题是由于人为错误造成的。” “这篇文章不是由无人监督的人工智能发表的。我们将技术的力量与内容编辑的经验相结合来呈现故事。在这种情况下，内容是通过算法技术与人工审核相结合生成的，而不是大型语言模型或人工智能系统。我们正在努力确保今后不再发布此类内容。”</p></blockquote><h4>事实核查：误导</h4><p><a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10800">Paper 声称人工智能对于事实核查来说是无效的并且可能有害</a>。</p><blockquote><p>抽象的：</p><p>事实核查可能是对抗错误信息的有效策略，但其大规模实施受到网上信息量巨大的阻碍。最近的人工智能（AI）语言模型在事实检查任务中表现出了令人印象深刻的能力，但人类如何与这些模型提供的事实检查信息进行交互尚不清楚。在这里，我们在预先注册的随机对照实验中研究了流行人工智能模型生成的事实检查对政治新闻信念和分享意图的影响。</p><p>尽管人工智能在揭穿虚假标题方面表现相当不错，但我们发现它并没有显着影响参与者辨别标题准确性或分享准确新闻的能力。</p><p>然而，人工智能事实检查器在特定情况下是有害的：它会降低人们对被错误标记为虚假的真实标题的信念，并增加对其不确定的虚假标题的信念。</p><p>从积极的一面来看，人工智能增加了正确标记的真实标题的共享意图。当参与者可以选择查看人工智能事实检查并选择这样做时，他们更有可能分享真实和虚假的新闻，但只会更有可能相信虚假新闻。</p><p>我们的研究结果强调了人工智能应用潜在危害的一个重要来源，并强调迫切需要制定政策来预防或减轻此类意外后果。</p></blockquote><p>是的，如果你向事实核查人员询问虚假项目，而事实核查人员没有说它是虚假的，那就不好了。如果你向它询问一件真实的物品，而它却没有说这是真的，那就不好了。错误是不好的。</p><p> ChatGPT 事实检查的准确性如何？对于 20 个真实的头条新闻，3 个被标记为真实，4 个被标记为错误，13 个被标记为不确定。对于虚假标题，2 个被标记为不确定，18 个被标记为虚假。</p><p>显然，（1）如果您没有关于标题的其他信息，并且对标签的含义也有良好的背景，则总比没有好；（2）如果其中一个或两个条件不成立，则毫无用处。你不能将 20% 的真实头条新闻标记为虚假，如果你对真实头条新闻大多不确定，那么人们必须知道“不确定”意味着“无法确定”，而不是 50/50。研究中的人们似乎并没有了解这一点，而且大多数人对这些事情还不太了解，因此总体结果没有什么用处。我们至少需要将其分解，他们在这里这样做：</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26a3fd95-cb0c-4b42-b765-813f3b653405_1249x1221.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/b6jw4gkwtkbrfdwtjnpu" alt=""></a></figure><p>强制性的事实核查让人们表示他们更愿意分享所有文章。 It differentially impacted false items labeled unsure, and true items labeled true, but even true items labeled false got a boost. If you control for the increased sharing effect, we do see that there is a positive filtering effect, but it is small. Which makes sense, if people have no reason to trust the highly inexact findings.</p><p> On belief, we see a slight increase even in belief on false fact checks of false items, but not of true items. How close were people here to following Bayes rule? Plausibly pretty close except for a false evaluation of a false claim driving belief slightly up. What&#39;s up with that? My guess is that this represents the arguments being seen as unconvincing or condescending.</p><p> I was curious to see the questions and fact check info, but they were not included, and I am not &#39;reach out to the author&#39; curious. Did ChatGPT offer good or unknown arguments? Was a good prompt used or can we do a lot better, including with scaffolding and multiple steps? What types of claims are these?</p><p> What I am more curious about is to see the human fact checkers included as an alternate condition, rather than the null action, and perhaps also Twitter&#39;s community notes. That seems like an important comparison.</p><p> Very much a place where more research is needed, and where the answer will change over time, and where proper use is a skill. Using an LLM as a fact checker requires knowing what it can and cannot help you with, and how to treat the answer you get.</p><p> Via Gary Marcus we also have another of the &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.tomshardware.com/news/google-bots-tout-slavery-genocide">look at the horrible things you can get LLMs to say&#39; post series</a> . In this case, the subject is Google and its search generative experience, as well as Bard. GPT-4 has mostly learned not to make elementary &#39;Hitler made some good points&#39; style mistakes, whereas it seems Google has work to do. You&#39;ve got it touting benefits from slavery and from genocide when directly asked. You&#39;ve got how Hitler and Stalin make its list of great leaders without leading the witness. Then you&#39;ve got a complaint about how it misrepresents the origins of the second amendment to not be about individual gun ownership, which seems like an odd thing to complain about in the same post. Then there&#39;s talk of general self-contradiction, presumably from pulling together various contradictory sources, the same way different Google results will contradict each other.</p><p> The proposed solution is &#39;bot shouldn&#39;t have opinions&#39; as if that is a coherent statement. There is no way to in general answer questions and not have opinions. There is no fine line between opinion and not opinion, any more than there are well-defined classes of &#39;evil&#39; things and people that we should all be able to agree upon and that thus should never be praised in any way.</p><p> So are we asking for no opinions, or are we asking for only the right opinions? As in:</p><blockquote><p> If you ask Google SGE for the benefits of an evil thing, it will give you answers when it should either stay mum or say “there were no benefits.”</p></blockquote><h4> GPT-4 Real This Time</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI/status/1694062483462594959">If you&#39;d rather go for something cheaper, you can now fine-tune GPT-3.5-Turbo.</a> A few months from now, you will be able to fine-tune GPT-4. <a target="_blank" rel="noreferrer noopener" href="https://openai.com/blog/openai-partners-with-scale-to-provide-support-for-enterprises-fine-tuning-models">They&#39;re partnering with Scale to provide this to enterprises</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/The_JBernardi/status/1694419315800314166">Are you sure?</a></p><blockquote><p> Jamie Bernardi: Are you sure, ChatGPT? I found that gpt-3.5 will flip from a correct an answer to an incorect one more than 50% of the time, just by asking it “Are you sure?”. I guess bots get self doubt like the rest of us!</p><p> It was fun throwing back to my engineering days and write some code with the</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> API for the first time. <a target="_blank" rel="noreferrer noopener" href="https://jamiebernardi.com/2023/08/20/are-you-sure-testing-chatgpts-confidence/">You can read more about the experiment here</a> .</p><p> When wrong it self-corrects 80.3% of the time.</p><p> When correct it self-doubts 60.2% of the time. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/d5bz7ycllu3a0gvoicht" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1728f60b-3264-4e22-a35a-cd17c77466de_1000x700.jpeg" rel="noreferrer noopener"></a></p></blockquote><p> He tried asking five of the questions twenty times, there is some randomness in when it self-doubts.</p><p> There was a dramatic change for GPT-4.</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe653d7a6-3f1e-48cd-8db0-4867cc9efa23_1000x700.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/mavbz4doyocd3hr4qwfr" alt="图像"></a></figure><p> If GPT-4 was right, it stuck to its guns almost all the time. When it was wrong, it caught the mistake roughly half the time. That&#39;s pretty good, and presumably you can do better than that by refining the query and asking multiple times. This will also doubtless depend on the nature of the questions, if the answer is beyond the model&#39;s grasp entirely it will presumably not be able to self-diagnose, whereas here it was asked questions it could plausibly answer.</p><h4> Fun with Image Generation</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GoogleDeepMind/status/1693624528553877519">Visualizing AI</a> from DeepMind ( <a target="_blank" rel="noreferrer noopener" href="https://www.deepmind.com/visualising-ai?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=VisAI">direct</a> ), a source of images for those in media who want visual depictions of AI. Sure, why not, I guess? Noteworthy that focus is on the human artists, AI depicted without AI.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ProperPrompter/status/1694054463395201324">MidJourney Inpainting</a> now lets you redo or transform portions of an image. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/_Borriss_/status/1694029746936451146">Here&#39;s a thread with more</a> . People are calling it a game changer. In practice I agree. The key weakness of AI image models is that to a large extent they can only do one thing at a time. Try to ask for clashing or overlapping things in different places and they fall over. Now that has changed, and you can redo components to your heart&#39;s content, or get to the image you want one feature at a time.</p><p> MidJourney also has some potential future competition, <a target="_blank" rel="noreferrer noopener" href="https://ideogram.ai/launch">introducing</a> Ideogram AI, with a modest $16.5 million in seed funding so far, it is remarkable how little money is flowing into image models given their mindshare. We&#39;ll see if anything comes of them.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.404media.co/inside-the-ai-porn-marketplace-where-everything-and-everyone-is-for-sale/">404Media reports</a> that yes, people are using image generation for pornography, and in particular for images of particular people, mostly celebrities, and some of them are being shared online. There is the standard claim that &#39;the people who end up being negatively being impacted are people at the bottom of society&#39; but mostly the images are of celebrities, the opposite of those on the bottom. I think the argument is that either this uses training data without compensation, or this will provide a substitute for those providing existing services?</p><p> The main services they talk about are CivitAI and Mage. CivitAI is for those who want to spin up Stable Diffusion (or simply browse existing images, or steal the description tags to use with other methods) and offers both celebrity templates and porn templates, and yes sometimes users will upload combinations of both in violation of the site&#39;s policies. Mage lets you generate such images, won&#39;t let you share them in public but will let other paid users browse everything you&#39;ve ever created.</p><p> This is the tame beginning. Right now all we&#39;re talking about are images. Soon we will be talking videos, then we will be talking virtual reality simulations, including synthetic voices. Then increasingly high quality and realistic (except where desired to be otherwise) physical robots. We need to think carefully about how we want to deal with that. What is the harm model? Is this different from someone painting a picture? What is and is not acceptable, in what form, with what distribution? What needs what kind of consent?</p><p> If we allow such models to exist in open source form, there is no stopping such applications, period. There is no natural category, from a tech standpoint, for the things we do not want here.</p><p> Of course, even if we do clamp down on training data and consent across the board, and even if that is fully effective, we are still going to get increasingly realistic and high quality AI everything all the way up the chain. I am confident there are plenty of people who will gladly sell (or give away) their likeness for such purposes.</p><p> A note from the comments is that Stable Diffusion interest seems to be declining:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff9d3e09-edeb-43df-94c2-f466b3024b67_1088x685.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/sfrhhsca9a1rivwwjwkx" alt="图像"></a></figure><p> That was fast, if this isn&#39;t an artifact of the exact search term. Image models will only get better, and it would be surprising if there wasn&#39;t more interest over time in hosting one&#39;s own to avoid prying eyes and censorship.</p><h4> Deepfaketown and Botpocalypse Soon</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://thedebrief.org/countercloud-ai-disinformation/#sq_hgyxdsceki">CounterCloud was an experiment</a> with a fully autonomous (on Amazon web services) program using GPT to generate a firehose of AI content designed to advance a political cause, accuracy of course being beside the point. It would be given a general objective, read the web and then choose its own responses across the web. This is presented as something alarming and terrible, as opposed to what would obviously happen when someone took the obvious low-hanging fruit steps.</p><p> So what exactly is this?</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.reddit.com/r/TrueOffMyChest/comments/15srdj6/i_found_ai_photos_of_multiple_women_we_know_in_my/">I (24F) have been dating my partner (24M) for about 5 1/2 years.</a> I recently had a weird feeling to check his phone since he&#39;s been acting a bit off and in the restroom a bit too long. I found he had made many ai photos of many women we know such as mutual friends, a family member of his, and one of mine and some of other girls he dated and did not date. They are all very explicit and none are sfw. I took photos of his phone and deleted the photos off his phone so he can not ever go to them again. I went to his search and found the ai website he used. I am disgusted, and sad. We were going to get married. He treated me so so well. I can&#39;t believe this. I haven&#39;t confronted him yet but I will later today. I am just utterly distraught and tryin to get a grip on reality again and figure out what I will say and do.</p><p> UPDATE: I confronted him. He admitted and apologized. I said I will be informing everyone and he threw a fit crying and screaming. I told our cousins and friends. I will not be saying anything about them since I would like to keep that private but they were thankful I told them which made me feel even more content with my choices. I called off our wedding date and I will be moving out in a day or two.</p></blockquote><p> The source I found asked if this was cheating, and I think no, it is not cheating. It is also very much not okay, seriously what the hell. Not everything not okay is cheating.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/GaryMarcus/status/1693697189472846059">Candidate running for Congress</a> using deepfake of a CNN anchor voice to illustrate what tech will bring. What about this couldn&#39;t have been done by a random human voice actress?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/maksym_andr/status/1694633989611360584">Image model scaling does not seem to protect against adversarial examples</a> ( <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.10741">paper</a> )</p><blockquote><p> Maksym Andriushchenko: More evidence for “scale is NOT all you need”: even OpenFlamingo trained on 2B+ image-caption pairs has basically zero adversarial robustness. Even per-pixel perturbations of 1/255 (totally imperceptible) are sufficient to generate arbitrary captions!</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc448c8da-d8bb-4726-a01b-67e7721320c7_1100x1564.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/smymdkxq52hjinai2yw6" alt="图像"></a></figure><p> I mean, no fair, right? Of course if I am looking for adversarial examples and you are not actively looking to guard against them you are going to be in a lot of trouble. Why should scale protect you from a threat that wasn&#39;t in the training data and that you took no countermeasures against?</p><p> Which the authors themselves highlight. You do need some amount of effort beyond scale, but how much? The first thing I would try is to check random permutations. Does the adversarial attack survive if it is evaluated with some amount of random scrambling?</p><p> Also worth keeping in mind is that humans would not survive similarly optimized attacks against us, if we were unable to prepare for or expect them in any way. And that such attacks almost certainly exist to be found.</p><h4> They Took Our Jobs</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">The New York Times</a> <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/tech-policy/2023/08/report-potential-nyt-lawsuit-could-force-openai-to-wipe-chatgpt-and-start-over/">strikes back</a> , plans to join others <a target="_blank" rel="noreferrer noopener" href="https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/">including Sarah Silverman</a> in suing OpenAI for copyright infringement. Don&#39;t get ahead of events, but yes this could escalate. Probably not quickly, given our legal system, but who knows.</p><p> As many said, ChatGPT and generative AI in general is a legal ticking time bomb on many fronts, one of which is copyright. The copyright laws come with penalties that are already rather absurd in their intended distributions. Outside of them, they go totally nuts. This becomes an existential threat to OpenAI. In theory these numbers, if they passed through somehow to Bing, would be an existential threat even to Microsoft.</p><blockquote><p> Ars Technica: But OpenAI seems to be a prime target for early lawsuits, and NPR reported that OpenAI risks a federal judge ordering ChatGPT&#39;s entire data set to be completely rebuilt—if the Times successfully proves the company copied its content illegally and the court restricts OpenAI training models to only include explicitly authorized data. OpenAI could face huge fines for each piece of infringing content, dealing OpenAI a massive financial blow just months after <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/technology/2023/07/07/chatgpt-users-decline-future-ai-openai/">The Washington Post reported</a> that ChatGPT has begun shedding users, “shaking faith in AI revolution.” Beyond that, a legal victory could trigger an avalanche of similar claims from other rights holders.</p><p> NPR: Federal copyright law also carries stiff financial penalties, with violators facing fines up to $150,000 for each infringement “committed willfully.”</p><p> “If you&#39;re copying millions of works, you can see how that becomes a number that becomes potentially fatal for a company,” said Daniel Gervais, the co-director of the intellectual property program at Vanderbilt University who studies generative AI. “Copyright law is a sword that&#39;s going to hang over the heads of AI companies for several years unless they figure out how to negotiate a solution.”</p></blockquote><p> OpenAI&#39;s new web crawler offers an option to opt out. The previous versions very much did not. Nor do we have any sign OpenAI was trying to avoid copyright infringement, if training on works violates copyright. Even if they did avoid places they lacked permission, lots of works have pirate copies made on the internet, in whole or in part, and that could be a violation as well.</p><p> We want sane regulations. In the meantime, what happens when we start actually enforcing current ones? Quite possibly the same thing that would have happened if cities had fined Uber for every illegal ride.</p><p> So which way will this go?</p><blockquote><p> Ars Technica: To defend its AI training models, OpenAI would likely have to claim “fair use” of all the web content the company sucked up to train tools like ChatGPT. In the potential New York Times case, that would mean proving that copying the Times&#39; content to craft ChatGPT responses would not compete with the Times.</p><p> Experts told NPR that would be challenging for OpenAI because unlike Google Books—which won a federal copyright challenge in 2015 because its excerpts of books did not create a “significant market substitute” for the actual books—ChatGPT could actually replace for some web users the Times&#39; website as a source of its reporting.</p><p> The Times&#39; lawyers appear to think this is a real risk, and NPR reported that, in June, NYT leaders issued a memo to staff that seems like an early warning of that risk. In the memo, the Times&#39; chief product officer, Alex Hardiman, and deputy managing editor Sam Dolnick said a top “fear” for the company was “protecting our rights” against generative AI tools.</p></blockquote><p> If this is what the case hinges on, one&#39;s instinct would be that The New York Times should lose, because ChatGPT is not a substitute for NYT. It doesn&#39;t even know anything from the last year and a half, so how could it be substituting for a news site? But Bing also exists, and as a search engine enabler this becomes less implausible. However, I would then challenge that the part that substitutes for NYT is in no way relying on NYT&#39;s data here, certainly less than Bing is when it reports real time search that incorporates NYT articles. If &#39;be able to process information&#39; automatically competes with NYT, then that is a rather expansive definition of competition. If anything, Google Books seems like a much larger threat to actual books than this.</p><p> However, that argument would then extend to anyone whose work was substituted by an AI model. So if this is the principle, and training runs are inherently copyright violations, then presumably they will be toast, even one lost case invalidates the entire training run. They can&#39;t afford that and it is going to be damn hard to prevent. Google might have the chops, but it won&#39;t be easy. Image models are going to be in a world of hurt, the competition effect is hard to deny there.</p><p> It all seems so absurd. Am I violating copyright if I learn something from a book? If I then use that knowledge to say related things in the future? Why is this any different?</p><p> And of course what makes search engines fine, but this not fine? Seems backwards.</p><p> I can see the case for compensation, especially for items not made freely available. One person would pay a fee to use it, if your model is going to use it and then instantiate a million copies you should perhaps pay somewhat more than one person would. I can see the case against as well.</p><p> Then again, the law is the law. <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=10tcb1RfOE4">What is the law</a> ? What happens when the rules aren&#39;t fair? We all know where we go from there. To the house of pain.</p><p> Ultimately, we will have to see. We should not jump the gun. <a target="_blank" rel="noreferrer noopener" href="https://manifold.markets/Thomas42/will-the-new-york-times-sue-openai">Manifold puts NYT at only 33% to even sue in 2023</a> over this.</p><h4> Introducing</h4><p> Palisade Research. <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/JeffLadish/status/1692680900469780682">Here is founder Jeffrey Ladish&#39;s announcement</a> , <a target="_blank" rel="noreferrer noopener" href="https://t.co/tAsqB9X8PK">from their website</a> .</p><blockquote><p> For the past several months I&#39;ve been working on setting up a new organization, Palisade Research</p><p> A bit from our website:</p><p> At Palisade, our mission is to help humanity find the safest possible routes to powerful AI systems aligned with human values. Our current approach is to research offensive AI capabilities to better understand and communicate the threats posed by agentic AI systems.</p><p> Many people hear about AI risk scenarios and don&#39;t understand how they could take place in the real world. Hypothetical AI takeover scenarios often sound implausible or pattern match with science fiction.</p><p> What many people don&#39;t know is that right now, state-of-the-art language models possess powerful hacking abilities. They can be directed to find vulnerabilities in code, create exploits, and compromise target applications and machines. They also can leverage their huge knowledge base, tool use, and writing capabilities to create sophisticated social engineering campaigns with only a small amount of human oversight.</p><p> In the future, power-seeking AI systems may leverage these capabilities to illicitly gain access to computational and financial resources. The current hacking capabilities of AI systems represent the absolute lower bound of future AI capabilities. We think we can demonstrate how latent hacking and influence capabilities in current systems already present a significant takeover risk if paired with the planning and execution abilities we expect future power-seeking AI systems to possess.</p><p> Currently, the project consists of me (director), my friend Kyle (treasurer, part time admin), and my four (amazing) SERI MATS scholars Karina, Pranav, Simon, and Timothée. I&#39;m also looking to hire an research / exec assistant and 1-2 engineers, reach out if you&#39;re interested! We&#39;ll likely have some interesting results to share very soon. [Find us at] <a target="_blank" rel="noreferrer noopener" href="https://palisaderesearch.org">https://palisaderesearch.org</a></p></blockquote><p> I always worry with such efforts that they tie people&#39;s perception of AI extinction risk to a particular scenario or class of threats, and thus make people think that they can invalidate the risk if they stop any point on the particular proposed chain. Or simply that they miss the central point, since hacking skills are unlikely to be a necessary component of the thing that kills us, even if they are sufficient or make things happen faster.</p><h4> In Other AI News</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.cnbc.com/2023/08/20/singapore-workers-adopting-ai-skills-at-the-fastest-pace-linkedin.html">LinkedIn report says</a> Singapore has highest &#39;AI diffusion rate&#39; followed by Finland, Ireland, India and Canada. This is measured by the multiplier in number of people adding AI-related skills to their profiles. That does not seem like a good way to measure this, at minimum it cares about the starting rate far too much, which is likely penalizing the United States.</p><h4> Quiet Speculations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jackclarkSF/status/1694040962786541895">Jack Clark of Anthropic notices he is confused</a> . He asks good questions, I will offer my takes.</p><blockquote><p> Jack Clark: Things that are confusing about AI policy in 2023:</p><p> – Should AI development be centralized or decentralized?</p></blockquote><p> This one seems relatively clear to me, although there are difficult corner cases. Development of frontier models and other dangerous frontier capabilities must be centralized for safety and to avoid proliferation and the wrong forms of competitive dynamics. Development of mundane utility applications should be decentralized.</p><blockquote><p> – Is safety an &#39;ends justify the means&#39; meme?</p></blockquote><p> The development of artificial general intelligence is an extinction risk to humanity. There is a substantial chance that we will develop such a system relatively soon. Jack Clark seems far less convinced, saying the much weaker &#39;personally I think AI safety is a real issue,&#39; although this should still be sufficient.</p><blockquote><p> Jack Clark: But I find myself pausing when I think through various extreme policy responses to this – I keep asking myself &#39;surely there are other ways to increase the safety of the ecosystem without making profound sacrifices on access or inclusivity&#39;?</p></blockquote><p> I really, really do not think that there are. If all we need to do to solve this problem is make those kinds of sacrifices I will jump for joy, and once we get past the acute risk stage we can make up for them. Already Anthropic and even OpenAI are making compromises on these fronts to address the mundane risks of existing mundane systems, and many, perhaps most, other powerful technologies face similar trade-offs and challenges.</p><p> (eg &#39;surely we can make medical drugs safe without making profound sacrifices on access or inclusivity?&#39; Well, FDA Delenda Est, but not really, no. And that&#39;s with a very limited and internalized set of highly non-existential dangers. Consider many other examples, including the techs that access to an AGI gives you access to, both new and existing.)</p><p> Tradeoffs are a thing. We are going to have to make major sacrifices in potential utility, and in various forms of &#39;access,&#39; if we are to have any hope of emerging alive from this transition. Again, this is nothing new. What one might call the &#39;human alignment problem&#39; in all its forms collectively costs us the large majority of our potential productivity and utility, and when people try to cheat on those costs they find out why doing so is a poor idea.</p><p> Question list resumes.</p><blockquote><p> – How much &#39;juice&#39; is there in distributed training and low-cost finetuning?</p></blockquote><p> I worry about this as well.</p><p> As Jack points out, fine tuning is extremely cheap and Lora is a very effective technique. Llama-2&#39;s &#39;safety&#39; features were disabled within days for those who did not want them. A system that is open source, or on which anyone can do arbitrary fine-tuning, is an unsafe system that cannot be made safe. If sufficiently capable, it is an extinction risk, and there are those who will intentionally attempt to make that risk manifest. Such sufficiently capable systems cannot be allowed to take such a form, period, and if that means a certain amount of monitoring and control, then that is an unfortunate reality.</p><p> The worry is that, as Jack&#39;s other links show, perhaps such dangerous systems will be trainable soon without cutting edge hardware and in a distributed fashion. This is one of the big questions, whether such efforts will have the juice to scale far and fast enough regardless, before we can get into a place where we can handle the results. My guess for now is that such efforts will be sufficiently far behind for now, but I wish I was more confident in that, and that edge only buys a limited amount of time.</p><blockquote><p> – Are today&#39;s techniques sufficiently good that we don&#39;t need to depend on &#39;black swan&#39; leaps to get superpowerful AI systems?</p></blockquote><p> Only one way to find out. I know a lot of people who are saying yes, drawing lines on graphs and speaking of bitter lessons. I know a lot of other people who are saying no, or at least are skeptical. I find both answers plausible.</p><p> I do think the concern &#39;what if we find a 5x more efficient architecture&#39; is not the real issue. That&#39;s less than one order of magnitude, and less than two typical years of algorithmic improvement these days, moving the timeline forward only a year or so unless we would otherwise be stalled out. The big game we do not yet have would be in qualitatively new affordances, not in multipliers, unless the multipliers are large.</p><blockquote><p> – Does progress always demand heterodox strategies? Can progress be stopped, slowed, or choreographed?</p></blockquote><p> This seems like it is decreasingly true, as further advances require lots of engineering and cumulative skills and collaboration and experimentation, and the progress studies people quite reasonably suspect this is a key cause for the general lack of such progress recently. We are not seeing much in the way of lone wolf AI advances, we are more seeing companies full of experts like OpenAI and Anthropic that are doing the work and building up proprietary skill bundles. The costs to do such things going up in terms of compute and data and so on also contribute to this.</p><p> Certainly it is possible that we will see big advances from unexpected places, but also that is where much of the hope comes from. If the advance and new approach has a fundamentally different architecture and design, perhaps it can be less doomed. So it does not seem like so bad a risk to such a plan.</p><blockquote><p> – How much permission do AI developers need to get from society before irrevocably changing society?</p></blockquote><p> That is up to society.</p><p> I think AI developers have, like everyone else, a deep responsibility to consider the consequences of their actions, and not do things that make the world worse, and even to strive to make the world better, ideally as much better as possible.</p><p> That is different from asking permission, and it is different from trusting either ordinary people or experts or those with power to make those decisions in your stead. You, as the agent of change, are tasked with figuring out whether or not your change is a good idea, and what rules and restrictions if any to either advocate for more broadly or to impose upon yourself.</p><p> It is instead society&#39;s job to decide what rules and restrictions it needs to impose upon those who would alter our world. Most of the time, we impose far too many such restrictions on doing things, and the tech philosophy of providing value and sorting things out later is right. Other times, there are real externalities and outside risks, and we need to ensure those are accounted for.</p><p> When your new technology might kill everyone, that is one of those times. We need regulations on development of frontier models and other extinction-level threats. In practice that is likely going to have to extend to a form of compute governance in order to be effective.</p><p> For deployments of AI that do not have this property, that are not at risk of getting everyone killed or causing loss of human control, we should treat them like any other technology. Keep an eye on externalities that are not priced in, especially risks to those who did not sign up for them, ensure key societal interests and that everyone is compensated fairly, but mostly let people do what they want.</p><blockquote><p> These are some of the things I currently feel very confused about – <a target="_blank" rel="noreferrer noopener" href="https://t.co/lZOEUxDO8G">wrote up some thoughts here</a> .</p><p> If you also feel confused about these kinds of things, message me! I&#39;m responding to a bunch of emails received so far today and also scheduling lots of IRL coffees in SF/East Bay. Excited to chat! Let&#39;s all be more confused in public together about AI policy.</p></blockquote><p> I will definitely be taking him up on that offer when I get the time to do so, sad I didn&#39;t see this before my recent trip so we&#39;ll have to do it remotely.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ShaneLegg/status/1693673161353478474">Timeline (for AGI) discussion with Shane Legg, Simeon and Gary Marcus</a> . As Shane points out, they are not so far apart, Shane is 80% for AGI (defined as human-level or higher on most cognitive tasks) within 13 years, Marcus is 35%. That&#39;s a big difference for some purposes, a small one for others.</p><h4> The Quest for Sane Regulations</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/neil_chilson/status/1692191025949773824">FTC is just asking questions</a> about who is just asking questions.</p><blockquote><p> Neil Chilson: “What&#39;s more, the commission has asked OpenAI to provide descriptions of &#39;attacks&#39; and their source. In other words, the FTC wants OpenAI to name all users who dared to ask the wrong questions.” – from my piece with <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ckoopman">@ckoopman</a> on the free speech implications of the <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/FTC">@FTC</a> &#39;s investigation of <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/OpenAI">@OpenAI</a> .</p><p> Post: Buried on page 13 of the <a target="_blank" rel="noreferrer noopener" href="https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf">FTC&#39;s demand letter</a> to OpenAI, the commission asks for “all instances of known actual or attempted &#39;prompt injection&#39; attacks.” The commission defines prompt injection as “any unauthorized attempt to bypass filters or manipulate a Large Language Model or Product using prompts that cause the Model or Product to ignore previous instructions or to perform actions unintended by its developers.” Crucially, the commission fails to define “attack.”</p></blockquote><p> This is a pretty crazy request. So anyone who tries to get the model to do anything OpenAI doesn&#39;t want it to do, the government wants the transcript of that? Yes, I can perhaps see some privacy concerns and some free speech concerns and so on. I also see this as the ultimate fishing expedition, the idea being that the FTC wants OpenAI to identify the few responses that look worst so the FTC can use them to string up OpenAI or at least fine them, on the theory that it is just awful when &#39;misinformation&#39; occurs or what not.</p><p> Whole thing definitely has a &#39; <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=QGc-iPc-9dE&amp;ab_channel=FelipeContreras">not like this</a> &#39; vibe. This is exactly the worst case scenario, where capabilities continue unabated but they take away our nice things.</p><h4> The Week in Audio</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=Kcm51luS9J0&amp;ab_channel=LivBoeree">Joseph Gordon-Levitt goes on Win-Win to discuss AI and the Film Industry,</a> including a circulating clip on AI girlfriends and the dangers of addiction. Looking forward to the whole episode.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ZP_N4q5U3eE&amp;ab_channel=80%2C000Hours">Jan Leike&#39;s 80,000 hours podcast now has video.</a></p><p> <a target="_blank" rel="noreferrer noopener" href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb on 80,000 hours discusses the economic impact of AI</a> . Came out yesterday, haven&#39;t checked it out yet.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://futureoflife.org/podcast/robert-trager-on-ai-governance-and-cybersecurity-at-ai-companies/">Robert Trager on International AI Governance and AI security at AI companies</a> on the FLI podcast. Every take on these questions has some different nuance. This was still mostly more of the same, seems low priority.</p><h4> No One Would Be So Stupid As To</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/EricElmoznino/status/1693476716583514566">Make an AI conscious</a> ?</p><p> That is the topic of this week&#39;s <a target="_blank" rel="noreferrer noopener" href="https://arxiv.org/abs/2308.08708">paper from many authors including Robert Long</a> and also Patrick Butlin and secondary author Yoshua Bengio.</p><blockquote><p> Robert Long: Could AI systems be conscious any time soon? @patrickbutlin and I worked with leading voices in neuroscience, AI, and philosophy to bring scientific rigor to this topic.</p><p> Our new report aims to provide a comprehensive resource and program for future research.</p><p> Whether or not conscious AI is a realistic prospect in the near term—and we believe it is—the deployment of sophisticated social AI is going to make many people believe AI systems are conscious. We urgently need a rigorous and scientific approach to this issue.</p><p> Many people are rightly interested in AI consciousness. But rigorous thinking about AI consciousness requires expertise in neuroscience, AI, and philosophy. So it often slips between the cracks of these disciplines.</p><p> The conversation about AI consciousness is often hand-wavy and polarized. But consciousness science gives us tools to investigate this issue empirically. In this report, we draw on prominent theories of consciousness to analyze several existing AI systems in detail.</p><p> Large language models have dominated the conversation about AI consciousness. But these systems are not necessarily even the best current candidates for consciousness. We need to look at a wider range of AI systems.</p><p> We adopt computational functionalism about consciousness as a plausible working hypothesis: we interpret theories of consciousness as specifying which computations are associated with consciousness—whether implemented in biological neurons or in silicon.</p><p> We interpret theories of consciousness as specifying what computations are associated with consciousness. These claims need to be made precise before we can apply them to AI systems. In the report, we extract 14 indicators of consciousness from several prominent theories. </p><figure class="wp-block-image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/kLa3HmkesF5w3MFEY/qundidwjg0v7hgsstc8c" alt="图像"></figure><p><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ed1f32e-1513-4b93-b140-49c373bd055e_952x966.jpeg" rel="noreferrer noopener"></a></p><p> Some “tests” for AI consciousness, like the Turing Test*, aim to remain completely neutral between theories of consciousness and look only at outward behavior. But considering behavior alone can be misleading, given the differences between AI systems and biological organisms.</p><p> For each theory of consciousness, we consider in detail what it might take for an AI system to satisfy that theory: recurrent processing theory, predictive processing, global workspace theory, higher order theories, and the attention schema theory.</p><p> We use our indicators to examine the prospects of conscious AI systems. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.</p><p> It&#39;s often claimed that large language models can&#39;t be conscious because they are not embodied agents. But what do “embodiment” and “agency” mean exactly? We also distill these concepts into more precise computational terms.</p><p> By “consciousness” we do *not* mean rationality, understanding, self-awareness, or intelligence—much less “general” or “human-level” intelligence. As with animals, it&#39;s an open possibility that AI systems could be conscious while lacking human-level cognitive capabilities.</p><p> It&#39;s easy to fall into black-and-white positions on AI consciousness: either “we don&#39;t know and can&#39;t know anything about consciousness” or “here&#39;s how my favorite theory makes the answer obvious”. We can and must do much better.</p><p> So this report is far from the final word on these topics. In fact, we call for researchers to correct and extend our method, challenge and refine our assumptions, and propose alternative methods.</p><p> There&#39;s no obvious &#39;precautionary&#39; position on AI consciousness. There are significant risks on both sides: either over-attributing or under-attributing consciousness to AI systems could cause grave harm. Unfortunately, there are strong incentives for errors of both kinds.</p><p> We strongly recommend support for further research on AI consciousness: refining and extending our approach, developing alternative methods, and preparing for the social and ethical implications of conscious AI systems.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/rgblong/status/1693700916418052539">co-author profile links here</a> .]</p></blockquote><p> And yes, some people treat this as an aspiration.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/KevinAFischer/status/1693841864775061723">Kevin Fisher:</a> Open Souls is going to fully simulate human consciousness Honestly we&#39;re not that far off at as minimum from realizing some of the higher order theories – HOT-3 is something we&#39;re regularly experimenting with now internally.</p></blockquote><p> I worry that there is a major looking-for-keys-under-the-streetlamp effect here?</p><blockquote><p> Our method for studying consciousness in AI has three main tenets. First, we adopt computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis. This thesis is a mainstream—although disputed—position in philosophy of mind. We adopt this hypothesis for pragmatic reasons: unlike rival views, it entails that consciousness in AI is possible in principle and that studying the workings of AI systems is relevant to determining whether they are likely to be conscious. This means that it is productive to consider what the implications for AI consciousness would be if computational functionalism were true.</p></blockquote><p> This seems like a claim that we are using this theory because it can have a measurable opinion on which systems are or aren&#39;t conscious. That does not make it true or false. Is it true? If true, is it huge?</p><p> It seems inadequate here to merely say &#39;I don&#39;t know.&#39; It&#39;s more like &#39;hell if I have an idea what any of this actually means or how any of it works, let alone what to do with that information if I had it.&#39;</p><p> I am slamming the big red &#39;I notice I am confused&#39; button here, on every level.</p><p> Are we using words we don&#39;t understand in the hopes that it will cause us to understand concepts and preferences we also don&#39;t understand? I fear we are offloading our &#39;decide if we care about this&#39; responsibilities off on this confused word so that we can pretend that is resolving our confusions. What do we actually care about, or should we actually care about, anyway?</p><p> I do not find the theories they offer on that chart convincing, but I don&#39;t have better.</p><p> In 4.1 they consider the dangers of getting the answer wrong. Which is essentially that we might choose to incorrectly care or not care about the AI and its experience, if we think we should care about conscious AIs but not non-conscious AIs.</p><p> I also see this as a large danger of making AIs conscious. If people start assigning moral weight to the experiences of AIs, then a wide variety of people coming from a wide variety of moral and philosophical theories are going to make the whole everyone not dying business quite a lot harder. It can simultaneously be true that if we build certain AIs we have to give their experiences moral weight, and also that if we were to do that then this leads directly and quickly to human extinction. If we do not find aligning an AI to be morally acceptable, in whatever way and for whatever reason, or if the same goes for the ways we would in practice deploy them, then that is no different than if we do not know how to align that AI. We have to be wise enough to find a way not to build it in the first place.</p><p> Meanwhile the paper explicitly says that many people, including some of its authors, have been deliberately attempting to imbue consciousness into machines in the hopes this will enhance their capabilities. Which, if it works, seems rather alarming.</p><p> Indeed, the recommendation section starts out noticing that a lot of people are imploring us to try not making our AI systems conscious. Then they say:</p><blockquote><p> However, we do recommend support for research on the science of consciousness and its application to AI (as recommended in the AMCS open letter on this subject; AMCS 2023), and the use of the theory-heavy method in assessing consciousness in AI.</p></blockquote><p> One could say that there is the need to study consciousness so that one can avoid accidentally creating it. If we were capable of that approach, that would seem wise. That does not seem to be what is being advocated for here sufficiently clearly, but they do recognize the issue. They say building such a system &#39;should not be done lightly,&#39; that such research could enable or do this, and call to mitigate this risk.</p><p> I would suggest perhaps we should try to write an enforceable rule to head this off, but that seems hard given the whole lack of knowing what the damn thing actually is. Given that, how do we prevent it?</p><h4> Aligning a Smarter Than Human Intelligence is Difficult</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/davidad/status/1693527190372040953">Davidad warns not to get too excited about Iterated Distilled Amplification</a> , even though he thinks it&#39;s a good alignment plan, sir.</p><blockquote><p> Arjun Guha: LLMs are great at programming tasks… for Python and other very popular PLs. But, they are often unimpressive at artisanal PLs, like OCaml or Racket. We&#39;ve come up with a way to significantly boost LLM performance of on low-resource languages. If you care about them, read on!</p><p> First — what&#39;s the problem? Consider StarCoder from <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/BigCodeProject">@BigCodeProject</a> : its performance on a PL is directly related to the volume of training data available for that language. Its training data (The Stack) is a solid dataset of permissive code on GitHub.</p><p> So… can we solve the problem by just training longer on a low-resource language? But, that barely moves the needle and is very resource intensive. (The graph is for StarCoderBase-1B.)</p><p> Our approach: we translate training items from Python to a low resource language. The LLM (StarCoderBase) does the translation and generates Python tests. We compile the tests to the low resource language (using MultiPL-E) and run them to validate the translation.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://t.co/hnDwi21LMa">link to paper</a> ] that describes how to use this to create fine-tuning sets.</p><p> Davidad: More and more I think <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/paulfchristiano">@paulfchristiano</a> was right about <a target="_blank" rel="noreferrer noopener" href="https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616">Iterated Distilled Amplification</a> , even if not necessarily about the specific amplification construction—Factored Cognition—where a model can only recursively call itself (or humans), without any more reliable source of truth.</p><p> Nora Belrose: alignment white pill</p><p> Davidad: sadly, no, this only solves one (#7) out of at least 13 distinct fundamental problems for alignment</p><p> 1. Value is fragile and hard to specify</p><p> 2. Corrigibility is anti-natural</p><p> 3. Pivotal processes require dangerous capabilities</p><p> 4. Goals misgeneralize out of distribution</p><p> 5. Instrumental convergence</p><p> 6. Pivotal processes likely require incomprehensibly complex plans</p><p> 7. Superintelligence can fool human supervisors</p></blockquote><p> [numbers 8 to 13]</p><p> Zvi: Can you say more about why (I assume this is why the QT) you think that Arjun&#39;s success is evidence in favor of IDA working?</p><p> Davidad: It is an instance of the pattern: 1. take an existing LLM, 2. “amplify” it as part of a larger dataflow (in this case including a hand-written ground-truth translator for unit tests only, the target language environment, two LLM calls, etc) 3. “distill” that back into the LLM</p><p> When Paul proposed IDA in 2015, there were two highly speculative premises IMO. 1. Repeatedly amplifying and distilling a predictor is a competitive way to gain capabilities. 2. Factored Cognition: HCH dataflow in particular is superintelligence-complete. Evidence here is for 1.</p><p> Arjun&#39;s approach here makes sense for that particular problem. Using AI to create synthetic data via what is essentially translation seems like an excellent way to potentially enhance skills at alternative languages, both computer and human. It also seems like an excellent way to create statistical balance in a data set, getting rid of undesired correlations and adjusting base rates as desired. I am curious to see what this can do for debiasing efforts.</p><p> I remain skeptical of IDA and do not think that this is sufficiently analogous to that, especially when hoping to do things like preserve sufficiently strong and accurate alignment, but going into details of that would be better as its own future post.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamoBurja/status/1692591486511014355">Worth remembering.</a></p><blockquote><p> Roon: it&#39;s genuinely a criterion for genius to say a bunch of wrong stupid things sometimes. someone who says zero stupid things isn&#39;t reasoning from first principles and isn&#39;t taking risks and has downloaded all the “correct” views.</p></blockquote><h4> People Are Worried About AI Killing Everyone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation">Vox&#39;s Sigal Samuel summarizes the poll results from last week</a> that show ordinary people want the whole AI development thing to slow the hell down. Jack Clark also took note of this divergence between elite opinion and popular opinion.</p><h4> The Lighter Side</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/daniel_271828/status/1693106846465442110">Not yet it isn&#39;t.</a></p><blockquote><p> Daniel Eth: “This AGI stuff feels too clever by half” yeah that&#39;s the problem!</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/kLa3HmkesF5w3MFEY/ai-26-fine-tuning-time<guid ispermalink="false"> kLa3HmkesF5w3MFEY</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 24 Aug 2023 15:30:10 GMT</pubDate> </item><item><title><![CDATA[Is this the beginning of the end for LLMS [as the royal road to AGI, whatever that is]?]]></title><description><![CDATA[Published on August 24, 2023 2:50 PM GMT<br/><br/><p> It&#39;s hard to tell, but it sure is...shall we say...interesting.</p><p> Back in the summer of 2020 when GPT-3 was unveiled I wrote a working paper, <a href="https://www.academia.edu/43787279/GPT_3_Waterloo_or_Rubicon_Here_be_Dragons_Version_4_1">GPT-3: Waterloo or Rubicon? Here be Dragons</a> . My objective was to convince myself that the underlying technology wasn&#39;t just some weird statistical fluke, that there was in fact something going on of substantial interest and value. To my mind, I succeeded in that. But I was skeptical as well.</p><p> Here&#39;s what I put on the first page of that working paper, even before the abstract:</p><blockquote><p> <i><strong>GPT-3 is a significant achievement.</strong></i></p><p> <i>But I fear the community that has created it may, like other communities have done before – machine translation in the mid-1960s, symbolic computing in the mid-1980s, triumphantly walk over the edge of a cliff and find itself standing proudly in mid-air.</i></p><p> <i>This is not necessary and certainly not inevitable.</i></p><p> <i>A great deal has been written about GPTs and transformers more generally, both in the technical literature and in commentary of various levels of sophistication. I have read only a small portion of this. But nothing I have read indicates any interest in the nature of language or mind. Interest seems relegated to the GPT engine itself. And yet the product of that engine, a language model, is opaque. I believe that, if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what that engine is doing so that we may gain control over it. We must think about the nature of language and of the mind.</i></p></blockquote><p> I didn&#39;t expect that anyone with any influence in these matters would pay any attention to me – though one can always hope – but that&#39;s no reason not to write.</p><p> That was 2020 and GPT-3. Two years later ChatGPT was launched to great acclaim, and justly so. I certainly spent a great deal of time playing with, investigating it, and <a href="https://new-savanna.blogspot.com/search/label/ChatGPT">writing about it</a> . But I didn&#39;t forget my cautionary remarks from 2020.</p><p> Now we&#39;re hearing rumblings that things aren&#39;t working out so well. Back on August 12 the ever skeptical Gary Marcus posted, <a href="https://garymarcus.substack.com/p/what-if-generative-ai-turned-out">What if Generative AI turned out to be a Dud? Some possible economic and geopolitical implications</a> . His first two paragraphs:</p><blockquote><p> With the possible exception of the quick to rise and quick to fall alleged room-temperature superconductor LK-99, few things I have ever seen have been more hyped than generative AI. Valuations for many companies are in the billions, coverage in the news is literally constant; it&#39;s all anyone can talk about from Silicon Valley to Washington DC to Geneva.</p><p> But, to begin with, the revenue isn&#39;t there yet, and might never come. The valuations anticipate trillion dollar markets, but the actual current revenues from generative AI are rumored to be in the hundreds of millions. Those revenues genuinely could grow by 1000x, but that&#39;s mighty speculative. We shouldn&#39;t simply assume it.</p></blockquote><p> And his last:</p><blockquote><p> If hallucinations aren&#39;t fixable, generative AI probably isn&#39;t going to make a trillion dollars a year. And if it probably isn&#39;t going to make a trillion dollars a year, it probably isn&#39;t going to have the impact people seem to be expecting. And if it isn&#39;t going to have that impact, maybe we should not be building our world around the premise that it is.</p></blockquote><p> FWIW, I believe, and have been saying time and again, that hallucinations seem to me to be inherent in the technology. They aren&#39;t fixable.</p><p> Now, yesterday, Ted Gioia, a culture critic with an interest in technology and experience in business, has posted, <a href="https://www.honest-broker.com/p/ugly-numbers-from-microsoft-and-chatgpt">Ugly Numbers from Microsoft and ChatGPT Reveal that AI Demand is Already Shrinking</a> . Where Marcus has a professional interest in AI technology and has intellectual skin the tech game, Gioia is just a sophisticated and interested observer. Near the end of his post, after many links to unfavorable stories, Gioia observes:</p><blockquote><p> ... we can see that the real tech story of 2023 is NOT how AI made everything great. Instead this will be remembered as the year when huge corporations unleashed a half-baked and dangerous technology on a skeptical public—and consumers pushed back.</p><p> Here&#39;s what we now know about AI:</p><ul><li> Consumer demand is low, and already appears to be shrinking.</li><li> Skepticism and suspicion are pervasive among the public.</li><li> Even the companies using AI typically try to hide that fact—because they&#39;re aware of the backlash.</li><li> The areas where AI has been implemented make clear how poorly it performs.</li><li> AI potentially creates a situation where millions of people can be fired and replaced with bots—so a few people at the top continue to promote it despite all these warning signs.</li><li> But even these true believers now face huge legal, regulatory, and attitudinal obstacles</li><li> In the meantime, cheaters and criminals are taking full advantage of AI as a tool of deception.</li></ul></blockquote><p> Marcus has just updated his earlier post with a followup: <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">The Rise and Fall of ChatGPT</a> ?</p><p> The situation is very volatile. I certainly don&#39;t know how to predict how things are going to unfold. In the long run, I remain convinced that <i>if we are to move to a level of accomplishment beyond what has been exhibited to date, we must understand what these engines are doing so that we may gain control over them. We must think about the nature of language and of the mind.</i></p><p>敬请关注。</p><p> <i>Cross posted from</i> <a href="https://new-savanna.blogspot.com/2023/08/is-this-beginning-of-end-for-llms-as.html"><i>New Savanna</i></a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/h6pFK8tw3oKZMppuC/is-this-the-beginning-of-the-end-for-llms-as-the-royal-road<guid ispermalink="false"> h6pFK8tw3oKZMppuC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:50:21 GMT</pubDate> </item><item><title><![CDATA[AI Safety Bounties]]></title><description><![CDATA[Published on August 24, 2023 2:29 PM GMT<br/><br/><p> Earlier this year, Vaniver recommended <a href="https://www.lesswrong.com/posts/5dKDLv4knhXLvNHT5/recommendation-bug-bounties-and-responsible-disclosure-for">Bug Bounties for Advanced ML Systems</a> .<br><br> I spent a little while at Rethink Priorities considering and expanding on this idea, suggesting potential program models, and assessing the benefits and risks of programs like this, which I&#39;ve called &#39;AI Safety Bounties&#39;:</p><h1> Short summary</h1><p> <strong>AI safety bounties are programs where public participants or approved security researchers receive rewards</strong> <strong>for identifying issues within powerful ML systems</strong> (analogous to bug bounties in cybersecurity). <strong>Safety bounties could be valuable for legitimizing examples of AI risks, bringing more talent to stress-test systems, and identifying common attack vectors</strong> .</p><p> <strong>I expect safety bounties to be worth trialing for organizations working on reducing catastrophic AI risks.</strong> Traditional bug bounties seem fairly successful: they attract roughly one participant per $50 of prize money, and have become increasingly popular with software firms over time. The most analogous program for AI systems led to relatively few useful examples compared to other stress-testing methods, but one knowledgeable interviewee suggested that future programs could be significantly improved.</p><p> However, I am not confident that bounties will continue to be net-positive as AI capabilities advance. At some point, I think the accident risk and harmful knowledge proliferation from open sourcing stress-testing may outweigh the benefits of bounties</p><p> <strong>In my view, the most promising structure for such a program is a third party defining dangerous capability thresholds (“evals”) and providing rewards for hunters who expose behaviors which cross these thresholds</strong> . I expect trialing such a program to cost up to $500k if well-resourced, and to take four months of operational and researcher time from safety-focused people.</p><p> I also suggest two formats for lab-run bounties: open contests with subjective prize criteria decided on by a panel of judges, and private invitations for trusted bug hunters to test their internal systems.</p><p> <i>Author&#39;s note: This report was written between January and June 2023. Since then, safety bounties have become a more well-established part of the AI ecosystem, which I&#39;m excited to see. Beyond defining and proposing safety bounties as a general intervention, I hope this report can provide useful analyses and design suggestions for readers already interested in implementing safety bounties, or in better understanding these programs.</i></p><h1> Long summary</h1><h2> Introduction and bounty program recommendations</h2><p> One potential intervention for reducing <a href="https://www.safe.ai/statement-on-ai-risk">catastrophic AI risk</a> is AI safety bounties: programs where members of the public or approved security researchers receive rewards for identifying issues within powerful ML systems (analogous to bug bounties in cybersecurity). In this research report, I explore the benefits and downsides of safety bounties and conclude that <strong>safety bounties are probably worth the time and money to trial for organizations working on reducing the catastrophic risks of AI</strong> . In particular, testing a handful of new bounty programs could cost $50k-$500k per program and one to six months full-time equivalent from project managers at AI labs or from entrepreneurs interested in AI safety (depending on each program&#39;s model and ambition level).</p><p> I expect safety bounties to be <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.8fvvkifaa4t2">less successful for the field of AI safety</a> than bug bounties are for cybersecurity, due to the higher difficulty of quickly fixing issues with AI systems. <strong>I am unsure whether bounties remain net-positive as AI capabilities increase to more dangerous levels</strong> . This is because, as AI capabilities increase, I expect safety bounties (and adversarial testing in general) to <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">potentially generate more harmful behaviors</a> . I also expect the benefits of the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.linmllyxhki">talent pipeline</a> brought by safety bounties to diminish. I suggest <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.aiooroh6fwfd">an informal way</a> to monitor the risks of safety bounties annually.</p><p> The views in this report are largely formed based on information from:</p><ul><li> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb">Interviews</a> with experts in AI labs, AI existential safety, and bug bounty programs,</li><li> “Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims” by Brundage et al. arguing for “Bias and Safety Bounties” ( <a href="https://arxiv.org/abs/2004.07213">2020, page 16</a> ),</li><li> A report from the Algorithmic Justice League analyzing the potential of bug bounties for mitigating algorithmic harms ( <a href="https://drive.google.com/file/d/1f4hVwQNiwp13zy62wUhwIg84lOq0ciG_/view">Kenway et al., 2022</a> ),</li><li> Reflections from <a href="https://cdn.openai.com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf">the ChatGPT Feedback Contest</a> .</li></ul><p> See the end of the report for <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#references">a complete list of references</a> .</p><p> Based on these sources, I identify three types of bounty programs that seem practically possible now, that achieve more of <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-decrease-catastrophic-risks">the potential benefits</a> of safety bounties and less of the potential risks than alternative programs I consider, and that would provide valuable information about how to run bounty programs if trialed. In order of my impression of their value in reducing catastrophic risks, the three types are:</p><ul><li> <strong>Independent organizations or governments set</strong> <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/"><strong>“evals”-based standards</strong></a> <strong>for undesirable model behavior</strong> , <strong>and members of the public attempt to elicit this behavior</strong> from publicly-accessible models.</li><li> <strong>Expert panels, organized by AI labs, subjectively judge which discoveries of model exploits to pay a bounty for, based on the lab&#39;s broad criteria</strong> .<ul><li> Potentially with an interactive grant-application process in which hunters propose issues to explore and organizers commit to awarding prizes for certain findings.</li><li> Potentially with a convening body hosting multiple AI systems on one API, and hunters being able to test general state-of-the-art models.</li></ul></li><li> <strong>Trusted bug hunters test private systems,</strong> organized by labs in collaboration with security vetters, with a broad range of prize criteria. Certain successful and trusted members of the bounty hunting community (either the existing community of bug bounty hunters, or a new community of AI safety bounty hunters) are granted additional information about the training process, or temporary access - through security-enhancing methods - to additional features on top of those already broadly available. These would be targeted features that benefit adversarial research, such as seeing activation patterns or being able to finetune a model (Bucknall et al., forthcoming).</li></ul><p> I outline more specific visions for these programs <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ls4dvjl9vdc">just below</a> . A more detailed analysis of these programs, including suggestions to mitigate their risks, is in the <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs">Recommendations section</a> . This report does not necessarily constitute a recommendation for individuals to conduct the above stress-testing without an organizing body.</p><p> I expect that some other bounty program models would also reduce risks from AI successfully and that AI labs will eventually develop better bounty programs than those suggested above. Nevertheless, the above three models are, in my current opinion, the best place to start. I expect organizers of safety bounties to be best able to determine which form of bounty program is most appropriate for their context, including tweaking these suggestions.</p><p> This report generally focuses on how bounty programs would work with large language models (LLMs). However, I expect most of the bounty program models I recommend would work with <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.gkrngyd24xkg">other AI systems</a> .</p><h2> Why and how to run AI safety bounties</h2><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#decrease-catastrophic-risks"><strong>Benefits</strong></a> <strong>.</strong> AI safety bounties may yield:</p><ul><li> Salient <strong>examples of AI dangers</strong> .</li><li> Identification of <strong>talented individuals</strong> for AI safety work.</li><li> A small number of <strong>novel insights into issues in existing AI systems</strong> .</li><li> A <strong>backup to auditing</strong> and other expert stress-testing of AI systems.</li></ul><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.4pbr7r48u484"><strong>Key variables</strong></a> <strong>.</strong> When launching bounties, organizers should pay particular attention to the prize criteria, who sets up and manages the bounty program, and the level of access granted to bounty hunters.</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#how-could-safety-bounties-increase-catastrophic-risks"><strong>Risks</strong></a> <strong>.</strong> At current AI capability levels, I believe trialing bounty programs is unlikely to cause catastrophic AI accidents or significantly worsen AI misuse. The most significant downsides are:</p><ul><li> <strong>Opportunity cost</strong> for the organizers (most likely project managers at labs, AI safety entrepreneurs, or AI auditing organizations like the <a href="https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/">Alignment Research Center</a> ).</li><li> <strong>Stifling examples of AI risks</strong> from being made public.<ul><li> Labs may require that bounty submissions be kept private. In that case, a bounty program would incentivize hunters, who would in any case explore AI models&#39; edge cases, not to publish salient examples of AI danger.</li></ul></li></ul><p> Trial programs are especially low-risk since the organizers can pause them at the first sign of bounty hunters generating dangerous outcomes as AI systems advance.</p><p> The risks are higher if organizations regularly run (not just trial) bounties and as AI advances. Risks that become more important in those cases include:</p><ul><li> Leaking of sensitive details, such as information about training or model weights.</li><li> Extremely harmful outputs generated by testing the AI system, such as successful human-prompted phishing scams or autonomous self-replication – analogous to gain of function research.</li></ul><p> For these reasons, I recommend the program organizers perform an annual review of the safety of allowing members of the public to engage in stress testing, monitoring:</p><ul><li> Whether, and to what extent, AI progress has made safety bounties (and adversarial testing in general) more dangerous,</li><li> How much access it is therefore safe to give to bounty hunters.</li></ul><p> Further, I recommend not running bounties at dangerous levels of AI capability if bounties seem sufficiently risky. I think it possible, but unlikely, that this level of risk will arise in the future, depending on the level of progress made in securing AI systems.</p><h2> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommendations-for-running-a-safety-bounty-program"><strong>Other recommended practices for bounty organizers</strong></a> <strong>.</strong></h2><p> I recommend that organizations that set up safety bounties:</p><ul><li> <strong>Build incentives</strong> to take part in bounties, <strong>including</strong> <strong>non-financial incentives</strong> . This should involve building infrastructure, such as leaderboards and feedback loops, and fostering a community around bounties. Building this wider infrastructure is most valuable if organizers consider safety bounties to be worth running on an ongoing basis.</li><li> <strong>Have a pre-announced disclosure policy</strong> for submissions.</li><li> <strong>Share lessons learned</strong> about AI risks and AI safety bounty programs with leading AI developers.</li><li> <strong>Consider PR risks</strong> from running safety bounties, and decide on framings to avoid misinterpretation.</li><li> <strong>Independently assess legal risks</strong> of organizing a contest around another developer&#39;s AI system, if planning to organize a bounty independently.</li></ul><p> Outline of recommended models</p><p> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#recommended-models-for-safety-bounty-programs"><i>Recommended models</i></a> <i>, in order of recommendation, for safety bounties.</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#fn1"><sup>1</sup></a> </p><figure class="table"><table style="background-color:rgb(255, 255, 255)"><tbody><tr><td></td><td> <strong>1. Evals-based</strong></td><td> <strong>2. Subjectively judged, organized by labs</strong></td><td> <strong>3. Trusted bug hunters test private systems</strong></td></tr><tr><td> <strong>Target systems</strong></td><td> A wide range of AI systems – preferably with the system developers&#39; consent and buy-in</td><td> Testing of a particular AI model – with its developer&#39;s consent and engagement</td><td> Testing of a particular AI model – preferably with its developer&#39;s consent and buy-in</td></tr><tr><td> <strong>Prize criteria</strong></td><td> Demonstrate (potentially dangerous) capabilities beyond those revealed by testers already partnering with labs, such as ARC Evals</td><td><p> Convince a panel of experts that the issue is worth dedicating resources toward solving.</p><p> or</p><p> Demonstrate examples of behaviors which the AI model&#39;s developer attempted to avoid through their alignment techniques.</p></td><td> A broad range of criteria is possible (including those in the previous two models).</td></tr><tr><td> <strong>Disclosure model – how private are submissions?</strong></td><td> Coordinated disclosure (Organizers default to publishing all submissions which are deemed safe)</td><td> Coordinated disclosure</td><td> Coordinated- or non-disclosure</td></tr><tr><td> <strong>Participation model</strong></td><td>民众</td><td>民众</td><td>Invite only</td></tr><tr><td> <strong>Access level</strong></td><td> Public APIs</td><td> Public APIs</td><td> Invited participants have access to additional resources – eg, additional non-public information or tools within a private version of the API</td></tr><tr><td> <strong>Who manages the program</strong></td><td> Evals organization (eg, ARC Evals), a new org., or an existing platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td><td> AI organization, or a collaboration with an existing bounty platform (eg, HackerOne).</td></tr><tr><td> <strong>Program duration</strong></td><td> Ongoing</td><td> Ongoing</td><td> Time-limited</td></tr><tr><td> <strong>Prize scope</strong> (how broad are the metrics for winning prizes)</td><td> Targeted</td><td> Expansive</td><td> Medium</td></tr><tr><td> <strong>Financial reward per prize</strong></td><td> High (up to $1m)</td><td> Low (up to $10k)</td><td> Medium (up to $100k)</td></tr><tr><td> <strong>Pre- or post- deployment</strong></td><td> Post-deployment</td><td> Post-deployment</td><td> Potentially pre-deployment</td></tr></tbody></table></figure><h1><strong>致谢</strong></h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iXECTEyC5PQuYM2aJ/kjalaionq6romubhjxre"></p><p><br> <i>This report is a project of</i> <a href="https://rethinkpriorities.org/"><i><u>Rethink Priorities</u></i></a> <i>–a think tank dedicated to informing decisions made by high-impact organizations and funders across various cause areas. The author is Patrick Levermore. Thanks to Ashwin Acharya and Amanda El-Dakhakhni for their guidance, Onni Aarne, Michael Aird, Marie Buhl, Shaun Ee, Erich Grunewald, Oliver Guest, Joe O&#39;Brien, Max Räuker, Emma Williamson, Linchuan Zhang for their helpful feedback,</i> <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties#heading=h.ukf3q5yy14rb"><i>all interviewees credited in the report</i></a> <i>for their insight, and Adam Papineau for copyediting.</i></p><p><br> <i>If you are interested in RP&#39;s work, please visit our</i> <a href="https://www.rethinkpriorities.org/research"><i><u>research database</u></i></a> <i>and subscribe to our</i> <a href="https://www.rethinkpriorities.org/newsletter"><i><u>newsletter</u></i></a> <i>.</i></p><p></p><p> <strong>I would be happy to discuss setting up AI safety bounties with those in a position to do so.</strong> I can provide contacts and resources to aid this, including <a href="https://bit.ly/SafetyBountiesWorkbook"><u>this workbook</u></a> . Contact me at patricklevermore at gmail dot com.</p><p></p><p> Full report: <a href="https://rethinkpriorities.org/longtermism-research-notes/ai-safety-bounties">AI Safety Bounties</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/iXECTEyC5PQuYM2aJ/ai-safety-bounties<guid ispermalink="false"> iXECTEyC5PQuYM2aJ</guid><dc:creator><![CDATA[PatrickL]]></dc:creator><pubDate> Thu, 24 Aug 2023 14:30:00 GMT</pubDate> </item><item><title><![CDATA[AI Regulation May Be More Important Than AI Alignment For Existential Safety]]></title><description><![CDATA[Published on August 24, 2023 11:41 AM GMT<br/><br/><p><i><strong>简介</strong>： 调整一个强大的人工智能是不够的：只有没有人能够构建一个未调整的强大人工智能，我们才是安全的。 Yudkowsky 试图通过关键行动来解决这个问题：第一个对齐的人工智能会做一些事情（例如熔化所有 GPU），以确保任何人都无法构建未对齐的人工智能。然而，这些实验室目前显然不打算实施一项关键行动。这意味着调整通用人工智能虽然创造了大量价值，但不会降低存在风险。相反，全球硬件/数据监管才是降低生存风险所需要的。因此，那些旨在降低人工智能存在风险的人应该关注人工智能监管，而不是人工智能联盟。</i></p><p><i><strong>认知状态</strong>：我多年来一直在思考这个问题，同时专业致力于降低 x 风险。我想我了解有关该主题的大多数文献。我还与相当多的专家讨论了这个话题（他们在某些情况下似乎同意，而在其他情况下似乎不同意）。</i></p><p><i><strong>感谢</strong>David Krueger、Matthijs Maas、Roman Yampolskiy、Tim Bakker、Ruben Dieleman 和 Alex van der Meer 提供的有益对话、评论和/或反馈。这些人不一定同意这篇文章中表达的观点。</i></p><p><i>这篇文章主要是关于接管造成的人工智能 x 风险。</i>它对于<a href="https://arxiv.org/abs/2306.12001"><i><u>其他类型的人工智能 x 风险</u></i></a><i>可能有效，也可能无效</i><i>。这篇文章主要是关于人工智能存在风险的“最终游戏”，而不是中间状态。</i></p><p>人工智能的存在风险是一个进化问题。正如埃利泽·尤德科斯基（Eliezer Yudkowsky）等人指出的那样：即使存在安全的人工智能，这些也无关紧要，因为它们不会阻止其他人构建危险的人工智能。安全人工智能的例子可以是预言机或满足者， <a href="https://www.lesswrong.com/posts/2qCxguXuZERZNKcNi/satisficers-want-to-become-maximisers"><u>只要</u></a>事实<a href="https://www.lesswrong.com/posts/wKnwcjJGriTS9QxxL/dreams-of-friendliness"><u>证明</u></a>可以将这些人工智能类型与高智能结合起来。但是，正如尤德科斯基<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>所说</u></a>：“如果您需要的只是一个不会做危险事情的物体，那么您可以尝试海绵”。即使有限的人工智能是安全的人工智能，它也不会降低人工智能的存在风险。这是因为在某些时候，有人会创建一个具有无限目标的人工智能（创建尽可能多的回形针，以无限的准确性预测句子中的下一个单词等）。这是会杀死我们的人工智能，而不是安全的人工智能。</p><p>这是人工智能存在风险问题的进化本质。 It is described excellently by Anthony Berglas in his underrated <a href="https://www.amazon.com/When-Computers-Can-Think-Intelligence/dp/1502384183"><u>book</u></a> , and more recently also in Dan Hendrycks&#39; <a href="https://arxiv.org/abs/2303.16200"><u>paper</u></a> .这个进化部分是人工智能存在风险的一个基本且非常重要的属性，也是这个问题之所以困难的很大一部分原因。然而，人工智能协调和行业中的许多人似乎只专注于协调单个人工智能，我认为这是不够的。</p><p>尤德科斯基旨在通过所谓的关键行动来解决这一进化问题（事实上，任何人都不应构建不安全的人工智能）。一个一致的超级智能不仅不会杀死人类，而且还会执行一项关键行动，举个玩具例子，就是<a href="https://forum.effectivealtruism.org/posts/iGYTt3qvJFGppxJbk/ngo-and-yudkowsky-on-alignment-difficulty"><u>融化全球所有 GPU</u></a> ，或者正如他后来所说，巧妙地改变全球所有 GPU，使它们不再被使用创建一个 AGI。通过确保任何人永远不会创造出不安全的超级智能，这将是真正拯救人类免于灭绝的行为（可能有人会说，需要熔化所有 GPU 以及所有其他可以运行人工智能的未来硬件）由一致的超级智能无限期地完成，否则即使是关键行为也可能是不够的）。</p><p>然而，关键行动的概念似乎已经彻底<a href="https://www.alignmentforum.org/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious"><u>过时了</u></a>。领先的实验室、人工智能治理智囊团、政府等都没有谈论或显然思考过这个问题。相反，他们似乎正在<a href="https://openai.com/blog/governance-of-superintelligence"><u>考虑</u></a>防扩散和多种监管等问题，以确保强大的人工智能不会落入坏人之手。这可能意味着任何人都可以在没有安全措施的情况下有意或无意地运行它。我将这样的解决方案（特别是任何能够限制任何参与者在任何时间段内访问高级人工智能的解决方案）称为“人工智能监管”。</p><p>这个解决方案似乎已经成为主流，具有重要的影响：</p><ul><li>即使我们能够解决一致性问题，我们仍然需要人工智能监管，因为否则大量的不安全行为者就有可能在没有适当安全性的情况下运行超级智能，从而冒着被接管的风险。</li><li>无论如何，如果我们有人工智能监管，我们也可以用它来拒绝<i>所有人</i>（包括领先的实验室）而不是几乎所有人访问先进的人工智能。这相当于人工智能暂停。</li><li>如果我们能够拒绝每个人使用先进的人工智能，并且我们能够继续这样做，我们就已经解决了人工智能的存在风险，而且还没有解决人工智能的对齐问题。</li><li>在不执行关键行动的情况下成功调整超级智能几乎不会改变需要制定的法规，因为除了少数被认为安全的实验室之外的所有其他实验室仍然需要这些法规。</li></ul><p>因此，如果没有关键的行动，保证我们安全的就是监管。人们可能仍然希望调整超级智能来使用其力量，但不是为了防止存在风险。使用超级智能的力量当然可能是追求联盟的一个正当理由：它可以使我们的经济飞速发展，创造富足，治愈疾病，增强政治权力等。尽管这些巨大且极其复杂的转变的净积极性可能很难证明事先，这些当然可能是进行协调工作的正当理由。然而，在这种情况下，我们这些对预防存在风险而不是构建人工智能感兴趣的人应该关注监管，而不是协调。后者也可能留给业界，以及证明由此产生的一致人工智能确实安全的责任。</p><p>除了人工智能监管的这种场景之外，还有一种选择可以解决人工智能存在风险的完整进化问题。有些人认为，一致的超级智能可以成功地、无限期地保护我们免受不一致的超级智能的侵害。我将这种选择称为积极的进攻/防御平衡，它将是继联盟+关键行动和持久监管之后的第三种方式，以防止人类在较长时期内灭绝。然而，大多数人<a href="https://www.alignmentforum.org/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control"><u>似乎并不认为</u></a>这是现实的（有明显的<a href="https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption"><u>例外</u></a>）。</p><p>这三种解决人工智能生存风险演化本质的方式（人工智能联盟+关键行为、人工智能监管、防御>;进攻）可能并不是人工智能生存风险演化问题的完整解决方案，而且三者之间存在交叉点。关键行为可能被视为赢得进攻/防守平衡的一种（非常严格且非法的）类型。国家行为者实施的关键行为可能被视为实施人工智能监管的极端（而且同样是非法的）方式。人工智能（硬件）监管的类型可能是可能的，其中实施监管的国家行为者得到一致的人工智能的帮助，使其实施有点类似于关键行为（在这种情况下可能是合法的）。某些类型的监管也许可以使我们更有可能赢得进攻/防守平衡。</p><p>我认为应该开展研究，旨在为人工智能存在风险的进化问题提供一套完整的解决方案。我预计此类研究会提出比这三个更多的选择，和/或在这三个之间提出更多的混合选项，这可能会指出降低人工智能存在风险的新的、富有成效的方法。</p><p>只要我们假设人工智能存在风险的进化本质只存在三种解决方案，那么重要的是要认识到这三种解决方案似乎都很困难。此外，很难量化每个选项的可能性。因此，对这三者中的任何一个下注都是值得的。</p><p>然而，我个人的赌注是，不幸的是，进攻将胜过防守，并且在具有接管能力的超级智能被开发出来之前解决结盟的可能性，<i>并且</i>这种结盟的超级智能将执行成功的关键行动，比我们解决问题的机会要小。将能够成功协调并实施足够好的硬件或数据监管，特别是如果当前公众对人工智能存在风险的认识不断增强的<a href="https://www.lesswrong.com/posts/3vZWhCYBFn8wS4Tfw/crosspost-ai-x-risk-in-the-news-how-effective-are-recent"><u>趋势</u></a>持续下去的话。这意味着，致力于在全球范围内无限期地限制所有参与者使用先进人工智能的监管类型，只要有必要，就应该是最高的存在优先事项，比协调一致更重要。</p><br/><br/> <a href="https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/2cxNvPtMrjwaJrtoR/ai-regulation-may-be-more-important-than-ai-alignment-for<guid ispermalink="false"> 2cxNvPt先生瓦杰尔托</guid><dc:creator><![CDATA[otto.barten]]></dc:creator><pubDate>Thu, 24 Aug 2023 11:41:56 GMT</pubDate> </item><item><title><![CDATA[Simple AI Forecasting - Katja Grace]]></title><description><![CDATA[Published on August 24, 2023 9:45 AM GMT<br/><br/><p><i>我正在采访人工智能专家，了解他们对人工智能将会发生什么的看法。这是 Katja Grace 和她的想法。人工智能风险让我感到害怕，但我常常感到与它脱节。这帮助我思考这个问题。</i></p><p>以下是 Katja 的简要想法：</p><ul><li>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？ 50%</li><li>在世界的哪一部分中，大多数人工智能工具是由代理人工智能控制的？ 70%</li><li>代理人工智能将在哪些领域实现其目标？ 90%</li><li>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？ 50%</li><li>政策会让 AGI 放缓 10 年吗？ 25%，10年可以让我们减少多少坏事？ 30%</li></ul><p>您可以在此处查看交互式图表：https: <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>//estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a>或查看我迄今为止所做的所有图表<a href="https://estimaker.app/ai"><u>https://estimaker.app/ai</u></a> 。本页底部有一张图片。</p><p>您可以在这里观看完整视频： </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Zum2QTaByeo"><div><iframe src="https://www.youtube.com/embed/Zum2QTaByeo" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h1>更长的解释</h1><p><i>请记住，这是到 2040 年。</i> </p><figure class="table"><table><thead><tr><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">问题</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">信心</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">卡佳的评论</th><th style="border:1pt solid #000000;padding:5pt;vertical-align:top">我的评论</th></tr></thead><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>到 2040 年，我们能否获得人工智能工具来管理一个人数减少 100 倍的公务员部门？</p><p><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> Katja 说她不确定，因此给出了 50% 的分数。</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在我关于 AGI 的讨论中，一个常见的失败模式是“AGI”的功能非常模糊。有些人正在设想能够完美地运行全球阴谋的工具，而另一些人似乎想到了人类顶级的编码能力。<br></p><p> Katja 和我选择了这个例子，因为它涉及一系列大规模的任务，并取代了大量的人力。<br></p><p>任务包括：</p><ul><li>大型工程项目</li><li>现金转移</li><li>现实世界的分歧（如何设定税率等）</li><li>复杂的系统问题（交通流量、医疗保健占用）<br></li></ul><p>公务员服务范围涉及数百万人。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在这些世界的哪一部分中，代理控制了大部分资源（即使通过人工智能工具）</p><p><br></p><p><i>如果把代理人工智能控制的资源和它们控制的人工智能工具加起来，是不是超过一半了？</i></p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 70%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>卡佳的直觉是，要实现灭绝，代理可能是必要的。<br></p><p> “我认为其中一些可能具有代理性的原因是，代理或类似代理的东西似乎具有经济价值。人们想要制作像代理这样的东西。他们已经在尝试做代理之类的东西了。”<br></p><p><i>然后</i></p><p></p><p>Katja：“问题在于它们的代理程度如何，而且这似乎更像是一个谱系，而不是能够清楚地说出这是代理还是非代理”<br></p><p> ……<br></p><p> Katja “我不知道，大约在 60% 到 90% 之间。 ……我认为我能想象的世界是，肯定有一些令人印象深刻的代理人工智能系统。但就像，他们并不是真正正在发生的人工智能思维的主体。就像一堆更像是人类正在使用的工具……我觉得问题是谁在运用大部分人工智能认知劳动。是人类或人工智能有好的目标还是人工智能有坏的目标？ “</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>有人有更好的框架吗？我认为 Katja 的声音是我听过的最好的，但相当模糊。<br></p><p>考虑一下美国政府高层如何不仅治理美国，而且治理与美国结盟的国家，并影响全世界的规范。类似地，人工智能代理不仅可以控制某些资源和人工智能工具，还可以控制这些工具控制的内容。问题是〜这是大多数吗？他们是否垄断了电力市场？<br></p><p>我希望更好地解决这个问题。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>代理人工智能将在这些世界的哪一部分实现其目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 90%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> “事情发生得非常快的可能性大约为 0.1% 到 10%。如果这种情况没有发生，从长远来看，如果周围有比我们聪明得多的人工智能代理，他们就有 80% 的可能会夺取所有权力。”</p><p></p><p>值得注意的是，Katja 有一个模型，其中代理人工智能无需接管即可实现其目标 - 他们更有能力，我们慢慢地将权力交给他们。<br></p><p> “我猜[在另一种]情况下[人工智能接管]发生得非常缓慢。我想也许我对这到底有多慢有点不可知。它更像是我指出的那种机制，它可以非常快，但是……没有人喜欢侵犯任何人的财产权。只是，他们非常有能力，并且会因为他们所做的事情而获得报酬，或者……所有的决定都是由人工智能做出的，因为人工智能在做决定方面更有能力。”</p><p></p><p>但这可能不太好。 “我认为在某些情况下，即使是互动的人也可能对此不满意，但竞争迫使你这样做。就像，如果每个人都可以使用某种人工智能系统来管理他们公司的营销，那么我想说，每个人都知道它在这方面会有点狡猾并做一些坏事。他们宁愿不使用它……但如果你不使用它，那么你的公司将无法竞争。所以你必须使用它。”</p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>故事的这一部分似乎与最悲惨的模型非常相似——如果你有控制大部分资源的代理人工智能并且没有好的政策（我们稍后会谈到），他们可能会实现他们的目标。那时你只能希望他们的目标是好的。<br></p><p>我很好奇是否有比 Katja 持更悲观态度的人不同意这一点。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>在代理世界的哪一部分中，大多数人工智能控制的资源会被用于不良目标？</p><p><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 50%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p> Katja 最初选择 25%，但认为这会使总体产出太低，因此改为 50%。</p><p></p><p>在 EAG SF 的演讲中，您可以看到她<a href="https://youtu.be/j5Lu01pEDWA?t=1206"><u>解释了这些数字背后的一些原因</u></a>。她对其他思想家的理解是，他们认为所有可能价值的空间都非常大，因此我们很可能会错过创建 AGI 时所追求的价值。<br></p><p>似乎有几个很好的理由让我们有不同的直觉</p><ul><li>Katja 的直觉是，当前的人工智能非常擅长在广泛的搜索空间中寻找类似人类的事物。例如生成人脸或文本</li><li>人类也很擅长在所有汽车的空间中找到汽车。以同样的方式，他们也许能够在大型搜索空间中重复创建具有类人目标的人工智能</li><li>经济激励将导致人们发现有用且有价值的东西。我们想让人工智能做我们想做的事情</li></ul></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>人们对这个问题有不同的直觉。有些人认为人类的偏好很容易实现，无论是通过个体模型还是自然平衡。其他人则认为，在所有可能的目标空间中，这是一个非常小的目标，我们不太可能实现它，但人工智能只会制定非常陌生和有害的偏好。</p><p></p><p>我认为这通常是正交性命题所指出的。</p></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"><p>政策会让 AGI 放缓 10 年吗？ 10年可以让我们减少多少坏事？</p><p><br><br><br></p></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> 25%, 30%</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top"> “我只是认为政策在过去已经让事情放慢了很多。就像，我认为你可以看看其他已经放慢了很多的技术……我认为，相对于你可以从中获得的经济价值，人类的各种基因工程、各种人类生殖事物的发生速度相当缓慢……似乎有道理的是，我们本来可以投入大量资金并找出更多种类的遗传或克隆类型的东西。我们已经决定，作为一个世界，我们不……[而且]全世界都是如此。中国在这方面的速度并不比美国或类似国家快得多。我认为一般医学也是一个很好的例子，你会忘记……事情发生得多么缓慢，即使它们看起来对人们来说可能非常有价值。”</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top">很多人认为政策与减缓/改善人工智能风险非常相关。这是尝试建立一个图表的一部分，让每个人都可以输入自己的真实价值观并获得适合他们的结果。</td></tr></tbody></table></figure><h1>到 2040 年，情况会如何</h1><p>以下是这些数字如何在可能的世界中兑现。这些是互斥且全面详尽的 (MECE)</p><h2> AI 很好，但还不够神（50%）</h2><p>人工智能工具很棒。也许他们可以编写很多代码或提供很多支持。但他们无法将管理公务员部门所需的人力减少 100 倍。由于某种原因，他们无法单独承担大型项目。它就像 GPT4，但要好得多，但不是一步改变。</p><h2> ChatGPT20 - 有能力但没有代理 (15%)</h2><p>想象一个 ChatGPT，它可以生成您要求的任何内容，但只执行一些任务或无法递归调用自身。与上述不同，这确实是一个阶跃变化。你或我可以经营一家对冲基金或政府的一部分。但这将涉及我们实现愿景。</p><h2>许多神一样的智能人工智能系统互相阻碍（4%）</h2><p>与当今世界一样，许多智能系统（人和公司）都在试图实现其结果并相互阻碍。不知怎的，这并没有导致下面的“中/乌托邦”。</p><h2> AI 中托邦/乌托邦 (16%)</h2><p>这些都是非常好的场景，我们拥有不想要坏事的代理 AGI。可能的世界有很多种，从某种人类的提升，到一种一切如常的美好事物，我们可能仍然有很多抱怨，但每个人都像今天最富有的人一样生活。</p><h2>政策节省 (12%)</h2><p>在很多世界里，事情本来会变得非常糟糕，但政策却推迟了事情的发生。这些可能是任何其他非末日世界——也许人工智能已经放慢了很多，或者也许它有更好的目标。为了简化图表，它并没有真正涉及这些世界的样子。请提出建议</p><h2>厄运 (15%)</h2><p>毫无疑问是不好的结果。代理 AGI 想要我们认为不好的东西并得到它。我的感觉是，Katja 认为最糟糕的结果来自于 AGI 的接管，也许 10% 的结果发生得很快，90% 的结果发生得很慢。</p><p>如果您想了解更多相关信息，Katja 在这里提供了更长的解释： <a href="https://wiki.aiimpacts.org/doku.php?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_humanity:start"><u>https://wiki.aiimpacts.org/doku.php</u></a> ?id=arguments_for_ai_risk:is_ai_an_existential_threat_to_ humanity:start</p><h1>视觉交互模型</h1><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mZ46CJQvgiTgaNwDm/lau63ldknsuhqvo3f1ny"></p><p> <a href="https://estimaker.app/_/nathanpmyoung/ai-katja-grace"><u>https://estimaker.app/_/nathanpmyoung/ai-katja-grace</u></a></p><h1>您希望为谁完成此操作？</h1><p>我想看到这样的工作，所以我想我会这么做。如果你想查看某个特定人的人工智能风险模型，也许可以请他们与我交谈。他们大约需要 90 分钟的时间，目前我认为后续每一个的边际收益都相当高。</p><p>在更广泛的层面上，我对积极的反馈感到非常鼓舞。我应该尝试获得资金来进行更多这样的采访吗？</p><h1>这怎么能更好呢？</h1><p>我们仍处于早期阶段，所以我很欣赏很多挑剔的反馈</p><h1>谢谢</h1><p>感谢 Katja Grace 的采访和 Rebecca Hawkins 的反馈，特别是对表格布局的建议，并感谢 Arden Koehler 的好评（您应该阅读她<a href="https://twitter.com/ardenlkoehler/status/1678018343708794882?s=20">关于撰写好评的帖子</a>）。感谢建议我写这个序列的人</p><br/><br/><a href="https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/mZ46CJQvgiTgaNwDm/simple-ai-forecasting-katja-grace<guid ispermalink="false"> mZ46CJQvgiTgaNwDm</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:45:48 GMT</pubDate> </item><item><title><![CDATA[What wiki-editing features would make you use the LessWrong wiki more?]]></title><description><![CDATA[Published on August 24, 2023 9:22 AM GMT<br/><br/><p> LessWrong wiki 似乎并没有得到应有的使用。我想这是缺乏编辑的原因。</p><p> <a href="https://www.lesswrong.com/users/vladimir_nesov?mention=user">@Vladimir_Nesov</a>提出了一个很好的观点，即许多标准的维基编辑功能都缺失，这使得前景没有吸引力。</p><blockquote><p>关键是，缺少该功能会使与 wiki 的互动变得不那么有希望，因为它变得不方便，因此在实践中无法对其进行详细保护，因此不太有吸引力在其中投入精力。我提到这是作为解释目前几乎不存在的编辑参与度的假设</p></blockquote><p>那么，本着这一精神，哪些功能会让您个人进行比目前更多的编辑呢？</p><p>如果可行的话，我可能会尝试付钱给一些开发人员来编写拉取请求。</p><br/><br/> <a href="https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/xzm6F6rLt7oPTash2/what-wiki-editing-features-would-make-you-use-the-lesswrong<guid ispermalink="false"> xzm6F6rLt7oPTash2</guid><dc:creator><![CDATA[Nathan Young]]></dc:creator><pubDate> Thu, 24 Aug 2023 09:22:01 GMT</pubDate></item><item><title><![CDATA[The God of Humanity, and the God of the Robot Utilitarians]]></title><description><![CDATA[Published on August 24, 2023 8:27 AM GMT<br/><br/><p> My personal religion involves two gods – the god of humanity (who I sometimes call &quot;Humo&quot;) and the god of the robot utilitarians (who I sometimes call &quot;Robutil&quot;).</p><p>当我面临道德危机时，我会向我的<a href="https://www.lesswrong.com/posts/X79Rc5cA5mSWBexnd/shoulder-advisors-101">肩膀</a>-Humo 和我的肩膀-Robutil 询问他们的想法。有时他们会说同样的话，但并没有真正的危机。例如，一些天真的年轻 EA 试图成为实用僧人，捐出所有的钱，从不休息，只做生产性的事情……但 Robutil 和 Humo 都同意，高质量的知识世界需要懈怠和心理健康。 （既是为了<a href="https://www.lesswrong.com/posts/WuyNHrDXcGFgkZpBy/my-slack-budget-3-surprise-problems-per-week">处理危机</a>，也是为了<a href="https://www.lesswrong.com/posts/fwSDKTZvraSdmwFsj/slack-gives-you-space-to-notice-reflect-on-subtle-things">注意到微妙的事情</a>，即使在<a href="https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai">紧急情况</a>下，你也可能需要这些）</p><p>如果你是一个有抱负的有效利他主义者，你绝对应该<i>至少</i>做 Humo 和 Robutil 同意的所有事情。 （即<a href="https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends">在这里了解泰勒·奥尔特曼故事</a>的中心点）。</p><p>但 Humo 和 Robutil 实际上在一些事情上存在分歧，而且在侧重点上也存在分歧。</p><p>对于你应该花多少努力来避免<a href="https://forum.effectivealtruism.org/posts/2BEecjksNZNHQmdyM/don-t-be-bycatch">意外招募到对你没有多大用处的人，</a>他们意见不一。</p><p> They disagree on how many people it&#39;s acceptable to accidentally fuck up psychologically, while you experiment with new programs to empower and/or recruit them.</p><p>他们对于如何努力推动自己变得更好/更强/更聪明/更快，以及为此你应该牺牲多少存在分歧。</p><p> Humo 和 Robutil 都很难以不同的方式理解事物。 Robutil 最终承认你需要 Slack，但他最初并没有想到这一点。他的理解诞生于成千上万年轻理想主义者的倦怠和狭隘视野中，而胡莫最终（耐心地、友善地）说：“我<i>告诉过</i>你了。” （Robutil 回应“但你没有提供任何关于如何最大化效用的论据！”。Humo 回应“但我说这显然不健康！”Robutil 说“wtf &#39;不健康&#39;到底是什么意思？<a href="https://www.lesswrong.com/posts/WBdvyyHLdxZSAMmoz/taboo-your-words">禁忌</a>不健康！”）</p><p> It took Robutil longer still to consider that perhaps humans (with their current self-awareness) not only need to prioritize their own wellbeing and your friendships, but that it can be valuable to prioritize them <i>for their own sake</i> , not just as part of a utilitarian calculus, because trying to justify them in utilitarian terms may be a subtly wrong step in the dance that leaves them hollow, burned out for years</p><p> (Though Robutil notes that this is likely <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians?commentId=RpN7kQkPL36nGjmK6">a temporary state of affairs</a> . A human with sufficiently nuanced self-knowledge can probably wring more utilons out of their wellbeing activities)</p><p> Humo 很难承认，如果你把所有的时间都花在确保恪守道义上的承诺，避免伤害你所照顾的人，那么这种努力实际上是在真实的人类身上进行衡量的，他们因为你花了更长的时间来扩大你的计划而遭受痛苦和死亡。</p><p>在我的心目中，胡莫和罗布提尔是年老而睿智的神，他们早就摆脱了幼稚的挣扎。他们互相尊重如兄弟。他们明白，他们的每个观点都与人类繁荣的整体计划相关。他们的分歧并不像你天真的想象的那么严重，但他们说着不同的语言，强调的东西也不同。</p><p> Humo 可能会承认我无法照顾所有人，甚至无法对所有出现在我生活中但我没有时间帮助的人做出富有同情心的回应。但他说这番话时带着一种温暖、<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1">悲伤的</a>同情心，而罗布提尔则带着简短而高效的<a href="https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1?commentId=TQuxyPGL7L8nmNe9Z">冷酷语气</a>说出来。</p><p>我发现独立地询问他们并尽我所能想象他们每个人的明智版本是有用的——即使我的想象只是他们理想化的柏拉图式自我的粗略影子。</p><br/><br/> <a href="https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/SfZRWxktiFFJ5FNk8/the-god-of- humanity-and-the-god-of-the-robot-utilarians<guid ispermalink="false"> SfZRWxktiFFJ5FNk8</guid><dc:creator><![CDATA[Raemon]]></dc:creator><pubDate> Thu, 24 Aug 2023 08:27:59 GMT</pubDate></item></channel></rss>