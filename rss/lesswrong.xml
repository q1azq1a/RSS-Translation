<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 15 日星期三 22:11:44 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Glomarization FAQ]]></title><description><![CDATA[Published on November 15, 2023 8:20 PM GMT<br/><br/><p>我在这里发布这篇文章的主要原因是我希望公开记录我在 2023 年写的这篇文章。这样，我可以向将来的人们表明我不是当场编造的；而是我在 2023 年写的这篇文章。早在他们这次问我什么事之前，这个问题就已经存在了。我没有自己的博客，所以我把它放在这里。不过，有些人可能会觉得它很有趣，或者想自己使用它。</p><p></p><p>如果我给你发了这篇文章，你可能只是问了我一个问题，而我的回答是“我拒绝证实或否认这一点”。也许你问我是否写了一个特定的故事，或者我是否与某人发生了性关系，或者我是否抢劫了一家银行。当你听到我的拒绝时，你可能会变得<i>更加</i>怀疑，假设这意味着我确实写/敲/抢劫了这个故事/人/银行。</p><p>这就是为什么情况不一定如此的解释。</p><p></p><p><strong>你为什么拒绝回答我的问题？</strong></p><p>假设——就像这篇文章中通常坚持的那样，我并不是说这是对是错，只是要求你考虑这个假设——我一生中从未犯过任何罪行，除了……比如说，公共场所小便。我头上套着一个袋子，所以人们不能确定是我。尽管如此，警察还是能够将嫌疑人的范围缩小到一个很小的名单，我也在其中，他们问我是否是我干的。</p><p>有些人只是对警察撒谎。不过，这里的关键因素是我<i>不想</i>对此撒谎。整个政策是为了避免谎言，以及在保持完全诚实的同时能够维护我的隐私。 <span class="footnote-reference" role="doc-noteref" id="fnreflpyzjd5jwgk"><sup><a href="#fnlpyzjd5jwgk">[1]</a></sup></span>如果我拒绝撒谎，但又不想让他们知道真相，那么我根本无法回答这个问题。所以我说，“我拒绝证实或否认这一点。”</p><p>但假设警察问我是否杀了人——但我<i>没有</i>这么做。他们问我是否抢劫了银行，是否贩卖毒品，以及其他有关我以前从未犯过的犯罪的问题。我总是说实话：“不，我没有那样做。”当然，现在警方知道在公共场合小便问题的答案是肯定的，因为这是我唯一拒绝回答这一问题的问题。</p><p>唯一不告诉他们我的秘密的政策，除了公然撒谎之外，就是拒绝证实或否认我是否犯下了<i>任何</i>罪行——<i>即使是那些我实际上没有犯下的罪行</i>。</p><p></p><p><strong>好吧，但这是否意味着你一开始就一定犯了一些罪行？毕竟，如果你不这样做，那么回答每一个存在的犯罪问题也没有什么坏处。</strong></p><p>这项政策不仅仅适用于我迄今为止所犯下的罪行。我可能会遇到<i>很多</i>其他可能的情况，如果回答不一致就会放弃信息。</p><p>例如，如果我还没有犯罪，但将来会犯罪，那么当我从总是说“不”变成总是说“我拒绝证实或否认”的那一刻，警察就知道我已经犯罪了。刚刚做了<i>某事</i>——</p><p></p><p><strong>那么你打算犯罪吗？</strong></p><p>不！或者更确切地说，我拒绝确认或否认我正在计划犯罪，但是<i>不，这不是我之前所说的有效结论！</i>首先，即使我预计几十年后犯罪的可能性很小，为了以防万一，仍然值得遵守这条规则 - 别用那种眼神看我，我还没说完。<i>此外</i>，这不仅仅适用于犯罪。</p><p>假设你问我是否与 X 发生过性关系<span class="footnote-reference" role="doc-noteref" id="fnrefiscl4hvc39"><sup><a href="#fniscl4hvc39">。 [2]</a></sup></span>出于与之前相同的原因，如果答案是“是”，我不能只是拒绝确认或否认我是否与该人发生过性关系，因为我的反应的差异就暴露了这一点。 （“我没有和W发生过性关系，我不能告诉你关于X的事情，我没有和Y发生过性关系，我没有和Z发生过性关系......）</p><p>如果我从未与任何人发生过性关系，但犯了罪，那么通过对性问题回答“否”，但对犯罪问题回答“我拒绝证实或否认”，我将透露我在至少一项犯罪。因此，即使我是处女，我仍然不得不拒绝回答性问题。</p><p>还有一些关于逻辑决策理论以及与我自己的假设版本合作的内容，但我认为这些都没有必要证明这一点。是的，我预计在过去、现在或将来的某个时刻，很有可能有人问我一个我不想诚实回答的问题。这足以表明我需要拒绝确认或否认某些事情。</p><p></p><p><strong>您所遵循的总体政策到底是什么？</strong></p><p>拒绝回答任何问题，如果该问题的可能答案之一（如果为真）是我想要隐藏的东西 - 无论所述答案实际上是否为真。也称为“全球化”。 <span class="footnote-reference" role="doc-noteref" id="fnreflwpck0izhj"><sup><a href="#fnlwpck0izhj">[3]</a></sup></span></p><p></p><p><strong>始终遵循这一点不是很困难吗？</strong></p><p>是的！事实上，尽管我很想总是荣耀化，但出于实用性，我经常不得不做出例外。如果有人问我今天做了什么，那么理论上我将不得不拒绝回答，因为可能的答案之一就是“我偷了你的车”。但大多数时候，我最终只是回答问题。</p><p>那么，什么<i>类似于</i>严格的全球化，但仍然足够宽松，我不必拒绝回答每个问题？好吧，我从成本效益分析开始。如果警察问我是否是袋头同伴<span class="footnote-reference" role="doc-noteref" id="fnref29ukuc1sh14"><sup><a href="#fn29ukuc1sh14">[4]</a></sup></span> ，那么在真相为“是”的情况下说真话的成本超过了在真相为“否”的情况下说真话的收益。所以在这里，我进行了环球化。但在不同的场景中，情况会发生变化：假设我中了一张 1,000,000 美元的彩票，而加拿大人没有资格购买，我需要向彩票人员提供我的地址才能拿到这笔钱 - 但我对我的隐私有轻微的偏好地址。在这种情况下，我居住在加拿大的情况下的成本远<i>低于</i>我居住在其他地方的情况下的收益。所以在这里，我说实话，因为在所有可能的情况下，全球化都会<i>伤害</i>我。</p><figure class="table"><table><tbody><tr><td>情况</td><td>答案A</td><td>答案B</td><td>成本如果 A</td><td>如果 B 则受益</td><td>你做什么工作？</td></tr><tr><td>被警方审问</td><td>“我做到了。”</td><td> “我没有这么做。”</td><td>进监狱？ <span class="footnote-reference" role="doc-noteref" id="fnref4iik2sevr7o"><sup><a href="#fn4iik2sevr7o">[5]</a></sup></span></td><td>警察更喜欢你</td><td>全球化</td></tr><tr><td>中了彩票</td><td>“我住在加拿大。”</td><td> “我住在[其他地方]。”</td><td>隐私较少</td><td>1,000,000 美元</td><td>说实话</td></tr></tbody></table></figure><p>我还需要考虑计算中的相关概率，并且可能会随机化一点，以仅花费很少的成本来保密。如果我99%的时间都花在无辜的事情上，1%的时间花在银行抢劫上，那么当我抢劫银行时，我不需要100%的时间去躲藏，只需2%的时间，或者10%如果我想更加安全的话。当然，这项政策的缺点确实给人们提供了一些证据，表明我隐藏某些东西的频率有多高，或者至少可以帮助他们建立一个上限。</p><p></p><p><strong>我怀疑你是否真的像你暗示的那样频繁地这样做。</strong></p><p>不幸的是，在撰写本文时我确实没有任何可以引用的证人。我家里的每个人都可以证实我确实经常拒绝回答问题，而且我认为在某些情况下他们后来发现我<i>没有</i>隐藏任何东西（有些情况下他们发现了我）。但这不是一个选择，因为我实际上不愿意确认或否认我的家人是谁。</p><p>幸运的是，对于除了我向其发送此帖子的第一个人之外的所有人，我将能够告诉他们我在他们之前向其发送过此帖子的人！因此，除非您碰巧是第一个人，否则当您阅读本文时，会有其他人可以证明我的怪异。</p><p></p><p><strong>你为什么说“这是机密”？</strong></p><p>有时我就是这样说“我拒绝证实或否认”！ “机密”听起来比“拒绝确认或否认”更酷，就像我是某种秘密特工一样。我是否实际上是某种秘密特工当然是保密的。</p><p></p><p><strong>您为什么在帖子前面列出“写了一个故事”？</strong></p><p>人们有时会因为用笔名写故事而受到指责。我和我之前的许多作者一样<span class="footnote-reference" role="doc-noteref" id="fnreffpnfyjvo0n"><sup><a href="#fnfpnfyjvo0n">[6]</a></sup></span> ，认为引发对我秘密写过或未写过哪些故事，以及我是或不是互联网上哪些其他作者的猜测是很有趣的。如果我写了一些我不想与我的真实姓名相关的禁忌内容，这也很有帮助。</p><p></p><p><strong>如果我确实想知道答案，我能做些什么吗？</strong></p><p>改变激励措施！例如，假设你是我的老板，我不想讨论政治，因为我认为你会解雇与你意见不同的人。如果你想了解我的政治信仰，那就证明你以前与人合作没有问题，即使你讨厌他们的政治立场。如果你不能证明这一点……好吧，这就是为什么我不回答你的问题。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnlpyzjd5jwgk"> <span class="footnote-back-link"><sup><strong><a href="#fnreflpyzjd5jwgk">^</a></strong></sup></span><div class="footnote-content"><p>当然，即使我一般不诚实，也不对警察撒谎还有一个额外的原因——这是非法的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fniscl4hvc39"> <span class="footnote-back-link"><sup><strong><a href="#fnrefiscl4hvc39">^</a></strong></sup></span><div class="footnote-content"><p>我实际上不太关心人们知道我与谁发生过性关系或没有与谁发生过性关系，但还有其他原因我想隐藏此类事情：尊重他人的隐私，或者如果这个人太不受欢迎，以至于如果人们知道我和他们发生过性关系，或者他们在逃避法律，并且我不想透露他们在我家里，他们就会拒绝与我交往 - 废话，现在我再次回到犯罪问题上来。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlwpck0izhj"> <span class="footnote-back-link"><sup><strong><a href="#fnreflwpck0izhj">^</a></strong></sup></span><div class="footnote-content"><p> Glomarization 是以中央情报局想要保密的一艘船命名的。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn29ukuc1sh14"> <span class="footnote-back-link"><sup><strong><a href="#fnref29ukuc1sh14">^</a></strong></sup></span><div class="footnote-content"><p>这个词里应该有多少个 e？</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4iik2sevr7o"> <span class="footnote-back-link"><sup><strong><a href="#fnref4iik2sevr7o">^</a></strong></sup></span><div class="footnote-content"><p>根据谷歌的前几条结果，通常只会被处以 500 美元的罚款，但如果是重复犯罪或在一大群人面前，惩罚就会加重，你<i>很</i>可能最终会入狱。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfpnfyjvo0n"> <span class="footnote-back-link"><sup><strong><a href="#fnreffpnfyjvo0n">^</a></strong></sup></span><div class="footnote-content"><p>或者也许在我之前<i>零个</i>作者。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/N5txrJDimv8nz4n24/glomarization-faq<guid ispermalink="false"> N5txrJDimv8nz4n24</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Wed, 15 Nov 2023 20:20:49 GMT</pubDate> </item><item><title><![CDATA[Testbed evals: evaluating AI safety even when it can’t be directly measured
]]></title><description><![CDATA[Published on November 15, 2023 7:00 PM GMT<br/><br/><p>我和一些合作者最近发布了论文“泛化类比（GENIES）：将人工智能监督泛化到难以测量领域的测试平台。 （<a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20">推文线程</a>）。在这篇文章中，我将解释 GENIES 基准如何与更广泛的方法相关联，用于预测人工智能系统是否安全<i>，即使无法直接评估其行为。</i></p><p>总结：<strong>当AI安全性难以衡量时，检查AI对齐技术是否可以用来解决更容易评分的类似问题。</strong>例如，要确定开发人员是否可以控制诚实如何推广到超人领域，请检查他们是否可以控制其他分布变化的泛化，例如“五年级学生可以评估的指令”到“博士可以评估的指令”。或者测试开发者是否能够发现欺骗行为，检查他们是否能够识别故意植入的“木马”行为。即使特定人工智能系统的安全性难以衡量，人工智能安全的有效性<i>&nbsp;</i>研究人员及其工具通常更容易测量——就像在风洞和压力室等航空航天试验台中测量火箭部件比发射火箭更容易测量一样。这些“试验台”评估可能会成为任何人工智能监管框架的重要支柱，但迄今为止很少受到关注。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/okmB8ymyhgc65WckN/ghuihv6jreaal9mzim2g"></p><h2><br>背景：为什么人工智能安全“难以衡量”</h2><p>很难判断人工智能系统是否遵循开发人员的指令有两个基本原因。</p><p><strong>人工智能的行为可能看起来不错，但实际上并不好。</strong>例如，很难判断超人人工智能系统是否遵守“开发您确信安全的后继人工智能”等指令。人类可以查看人工智能计划并尽力确定它们是否合理，但很难知道人工智能系统是否在玩弄人类的评估——或者更糟糕的是——它们是否有隐藏的意图并试图对我们实施快速的攻击。</p><p><br><strong>在某些环境中观察人工智能行为会带来风险。</strong>前沿法学硕士在部署后就发现了许多安全故障，当人工智能系统超过一定的能力阈值后，这显然会变得不可接受。相反，开发人员必须彻底评估模型<u>无法</u>采取<u>真正危险</u>操作的测试环境中的安全性。如果人工智能系统故意等待机会采取危险行动或出现其他“实验室”中未出现的故障模式，那么以这种方式评估人工智能系统将尤其具有挑战性。</p><h2><br>其他行业的安全也很难衡量</h2><p>在人工智能安全中，安全性的衡量似乎特别具有挑战性，因为在大多数行业中，不安全的系统并不<i>试图</i>看起来安全；相反，它是安全的。然而，我们仍然可以从其他行业的安全衡量方式中吸取教训。</p><p>例如，通过实际将火箭发射到太空来测试火箭的成本很高，类似于通过实际部署来测试人工智能系统的危险程度。航空航天工程师在称为“测试台”的更易于测量的设置中执行尽可能多的测试。例如，他们建造模拟真空空间压力和温度条件的室，建造对结构部件施加应变和振动的钻机等。通过“桌面场景”对核设施工作人员进行评估，以确定他们如何应对灾难。</p><p>通常，当难以测量时，可以使用易于测量的测试来预测安全性。</p><h2>人工智能安全的“试验台”</h2><p><strong>定义。</strong>我将使用“测试台”一词来指<strong>代类似于使人工智能系统更安全但更容易评分的问题。</strong>开发人员能够在多大程度上解决这些问题应该反映出他们实际上能够在多大程度上确保人工智能系统的安全。</p><p>人工智能安全“测试平台”的示例包括：</p><ul><li><a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20"><strong>泛化类比</strong></a><strong>。</strong>要确定开发人员是否可以将诚实概括为超人能力，请检查他们是否可以控制从“五年级学生可以回答的问题”到“博士可以回答的问题”的概括。更一般地说，测试他们是否可以控制各种分布变化的泛化。</li><li> <a href="https://www.lesswrong.com/posts/EbL5W5ccwfbqFiYBJ/auditing-games-for-high-level-interpretability-1"><strong><u>审核游戏</u></strong></a><strong>：</strong>要确定可解释性工具是否可以捕获欺骗行为，请检查它们是否可以<a href="https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5#Trojan_Horse_Models"><u>识别其他开发人员植入的木马</u></a>，或者区分经过故意训练的欺骗性模型。</li><li> <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><strong><u>元级对抗性评估</u></strong></a>：为了测试您的监督过程的稳健性，训练人工智能系统来利用一些开发人员可以验证的漏洞，因为他们拥有监督过程中保留的信息。例如，训练人工智能系统删除它不应该访问的文件而不被抓住。一些开发人员了解测试，并且可以轻松检查任务是否完成，但监督者无法访问这些保留的信息。</li></ul><h2> Tesbed 的应用</h2><p><strong>人工智能监管。</strong>人工智能治理中的一个关键的开放问题是<i>如何设计“安全评估”。</i>如果人工智能系统通过这些具体测试，其部署范围就可以扩大。测试平台可用作安全评估。例如，监管机构可以在允许实验室部署模型之前检查实验室是否可以使用其可解释性技术来检测木马或控制玩具分发转移的泛化。</p><p><strong>从错位的人工智能中提取人工智能安全研究。</strong>由于测试台问题很容易衡量，人工智能系统可以因在这些问题上取得的进展而直接获得奖励。例如，开发人员可以指示人工智能系统编写一个程序来控制各种分布变化的泛化。通过阅读人工智能系统提供的程序，开发人员也许能够了解有关控制泛化的重要见解。如果测试平台对游戏足够鲁棒，即使是公然错位的模型也可能被迫做出真正的人工智能安全研究贡献（或者训练过程将用这样做的模型取代它们）。</p><h2>试验台设计注意事项</h2><p><strong>测试平台应该对“游戏”具有鲁棒性。</strong>如果测试平台用于监管或从失调的人工智能中提取人工智能安全研究，那么测试平台必须能够抵御对抗性的游戏尝试。例如，从表面上看，TrojAI 似乎是对可解释性技术的一个很好的测试。然而，许多在此基准测试中表现出色的论文与人工智能安全无关。例如，<a href="https://arxiv.org/pdf/2110.08335.pdf"><u>本文</u></a>使用 SGD 来搜索输入中的“简单”触发模式。这很有效，因为基准测试中的大多数触发模式恰好都很简单。设计强大的测试台可能需要仔细思考和大量迭代（理想情况下应该尽早开始！）</p><p><strong>测试平台在与其核心类比无关的所有方面都应该是多样化的。</strong>跟踪测试台的哪些方面实际上与您真正关心的问题类似非常重要。例如，将诚实从“五年级学生可以回答的问题”推广到“博士可以回答的问题”似乎类似于将诚实推广到超人困难的问题。但<i>为什么</i>这些问题都相似呢？ “从易到难”的分布转变有什么特别之处吗？如果不是，还应该衡量不同领域、角色等之间的泛化。从不同的类比集合中提取可以对人工智能安全工具进行更稳健的评估，就像采用异质样本如何更好地估计癌症治疗是否有效一样。</p><h2>结论</h2><p>我的印象是，许多监管机构和研究人员都在考虑评估人工智能系统的安全性，就像我们面前有一个囚犯一样，我们需要对他们进行一系列心理测试，以确定是否应该将他们释放到社会中。</p><p>这对安全评估的描述过于严格。人们需要进行一个重要的概念转变，从“这个特定的人工智能系统有多安全”到“我们的工具有多有效？”第二个问题清楚地告诉了前者，但直观上似乎更容易回答。</p><p>我目前正在考虑如何为人工智能安全建立更好的“测试平台”。如果您有兴趣合作，请通过<a href="mailto:joshuamclymer@gmail.com"><u>joshuamclymer@gmail.com</u></a>与我联系</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be<guid ispermalink="false"> okmB8ymyhgc65WckN</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Wed, 15 Nov 2023 19:00:42 GMT</pubDate> </item><item><title><![CDATA[Listening to Pascal Muggers]]></title><description><![CDATA[Published on November 15, 2023 6:06 PM GMT<br/><br/><p><br><strong><u>概括</u></strong><i><strong><u> </u></strong><u>-</u>我们已经被“帕斯卡抢劫者”抢劫了<strong>——</strong>这种内心想法认为后果如此巨大，以至于超越了所有正常的考虑。事情很模糊，但考虑到利害关系，不给予任何关注是不负责任的。同时，高效意味着不要给予他们超出任何可能有用的注意力。</i><br></p><h2>介绍</h2><p><u>在</u><a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities"><u>《Pascal&#39;s Mugging: Tiny Probabilities of Vast Utilities</u></a> 》（2007 年）中，Eliezer Yudkowsky 描述了一个思想实验： <span class="footnote-reference" role="doc-noteref" id="fnrefrcut65obs6"><sup><a href="#fnrcut65obs6">[1]</a></sup></span></p><blockquote><p>现在假设有人来找我说：“给我五美元，否则我将使用矩阵之外的魔力来运行一台图灵机来模拟并杀死 3^^^^3 个人。”</p></blockquote><p>以利以谢还写道：</p><blockquote><p>你或我可能会一笑置之，根据占主导地位的主线概率进行计划</p></blockquote><p>不是我。</p><p>这让我彻夜难眠。</p><p>好像我不能理解3^^^^3，感觉好浩瀚。这个数字感觉就像任何其他“奇怪的大数字”。问题是我知道我不能真正认为抢劫犯说的是假话的“可能性很小”。同时我知道抢劫犯可以任意增加公用事业的规模。如果我根据最大化预期价值做出决定，我应该支付 5 美元（或为此支付我的全部钱）。但给抢劫犯 5 美元的感觉很糟糕，放弃理性也很糟糕。</p><p>这两种选择似乎都需要做出无限愚蠢的承诺🤢</p><h2>让帕斯卡劫匪就位</h2><p>我们的情绪不能超越几个层次。在普通的大思维中，我们的情感影响量表可能类似于： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kgkfqgbs21fnmpitcz7j" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qvm5vnoef39ozcf14pcl 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/g30fzpnlq2cxm8oknslw 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/voqp0ccqlwjtpwdjxl36 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/zl2qdauebouybrgzgxxl 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nrgu2v7zwinc03kxsnlf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/erg9yk5oxhvgewp8r9o8 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jnft7iwkzrfasl7l0hl7 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hk9hsmxfq58caxstgtvq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nl30cktfb9vmlulorr9a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ule7bazztjrljt88n0sd 960w"></figure><p>但是，如果我们再推送一个 util，然后再推送另一个，依此类推，会怎么样呢？有没有什么你看重的东西可以无限扩展，或者如果不是的话，它会止步于何处？您有多确定您不想避免再多一种效用（例如，防止再创造一个受折磨的人）或获得另一种效用（例如，与此相反）？</p><p>此时，我们已经达到了“奇怪的考虑因素”，在计算期望值时，所有普通结果都是无关紧要的： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/odq2xlwzcbxq7dpyeehk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/y2qnm8jmx3keisdnr1pc 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dphtjar4umincs6ugo2m 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/sw0vfcscdbbjkn2tc7cl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kbh79apb1skinptm38uj 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/lcldo3ri9rdkafb9qcri 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/p7cwzt79qighnsambtr1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pb48i8alj7xhxn40gvni 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/vf2dyz9nuun2vccuy5jv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fcxxbddnzuh7cgev0awg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/eyrjnebkjsq6wzrtxibj 960w"></figure><p>现在我们可以轻松地拒绝 3^^^^3 劫匪，因为它只是奇异的坏云中的又一项。我们可以回答：“<i>别拿这些小事来烦我，我只想拯救3^^^^^3条生命！</i> ”（还有更大、概率更高的选项，比如多想一想。 ）</p><p>当我们窥探云内部时会发生什么？更极端的实用程序可能会胜过其他实用程序： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/t9hokztkttpiyojotwul" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/o1mgmi8lkdxndozl5tpw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jetyctegrfjha3hxv2qg 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ghbgm7sxacrrm73p91jp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/icgx2zyqckvloua9b2gv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nstus4bl43sp6j6bc8c1 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/bu7rxw3gi3kihprjiube 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hob6onvjfeg1biegkxx9 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/rjgj6yjlx76es2pcsxtc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/um8yovu724qb63obq5ea 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fvxwv976upj8znvlpqpw 960w"></figure><p>康托尔的“绝对无穷大”（Ω）有什么意义吗？ Ω 公用事业/非公用事业的可能性是否应该压倒一切？难道这已经失去了人性的温暖吗？我们是否已经离开了准确世界模型之上的真实偏好之地，进入了元伦理学的荒原？</p><p>也许没有任何答案。</p><p>所以你会怎么做？</p><p>你玩赔率😅 <span class="footnote-reference" role="doc-noteref" id="fnrefndu3c0vdwsd"><sup><a href="#fnndu3c0vdwsd">[2]</a></sup></span></p><h2>对战略的影响</h2><p>在<a href="https://www.lesswrong.com/posts/78G9pjYM2KohErCvJ/model-uncertainty-pascalian-reasoning-and-utilitarianism"><u>模型不确定性、帕斯卡推理和功利主义</u></a>(2011) 中， <a href="https://www.lesswrong.com/users/multifoliaterose?mention=user">@multifoliaterose</a>分析了帕斯卡的论点，即所有慈善努力都应旨在影响无限实验室宇宙的创造：</p><blockquote><p>反驳#3：即使一个人的焦点应该放在实验室宇宙上，这样的焦点会减少到对创建友好人工智能的关注，这样的实体会比我们更好地推理实验室宇宙是否是一件好事以及如何做。去影响他们的创作。</p><p>回应：这里也是如此，如果这是真的，那就不明显了。即使一个人成功地创造了一种同情人类价值观的通用人工智能，这样的通用人工智能也可能不会归因于功利主义，毕竟许多人不是，而且目前还不清楚这是因为他们的意志没有被连贯地推断出来</p></blockquote><p>所以这不是一个答案。提案<i>需要</i>一个单独的帖子——模型/哲学不确定性、目标和策略的组合——但这就是对付帕斯卡抢劫犯的温和、常识性的方法。我将其视为抢劫，并不是因为有一天人类创造出某种东西来创造全新宇宙的可能性如此之小，而是因为实用程序的巨大性意味着胜过其他一切。</p><p>在<a href="https://www.lesswrong.com/posts/ebiCeBHr7At8Yyq9R/being-half-rational-about-pascal-s-wager-is-even-worse"><i><u>《对帕斯卡赌注的半理性更糟糕</u></i></a><i>》（2013）</i>中，埃利泽写道：</p><blockquote><p><i>从我记事起，我就纯粹出于实际原因拒绝了所有形式的帕斯卡赌注：任何试图通过追逐万分之一的巨额回报的机会来规划自己的生活的人几乎肯定会在实践中注定失败……现在，在极少数情况下，我发现自己在思考这种元级别的垃圾，而不是手头的数学，我提醒自己，这是一个浪费的动作——“浪费的动作”是指回想起来，如果问题出在哪里，任何想法都会出现。事实上已经解决了，并没有为问题的解决做出贡献。</i></p></blockquote><p>在实践中，接受抢劫与忽视抢劫之间可能没有什么区别，但考虑到巨大的实用性并偶尔重新审视对我来说似乎是正确的。</p><h2>不偏离轨道</h2><p>帕斯卡抢劫式思维可能会导致基于可疑的哲学论证而做出超出常规行为范围的事情。特德·卡辛斯基（Ted Kaczynski）（大学炸弹客）并没有特别奇怪的观点（即日益增长的工业主义正在破坏我们与自然的和谐）。他的不同寻常之处在于他按照这些原则行事。就他而言，他犯了一个错误，在实施他的炸弹人“战略”之前没有与其他任何人交谈。并不是所有愚蠢的事情都可以通过与至少一些外群体成员讨论的启发式方法来预防，但它应该被视为避免偏离轨道的一个良好起点。</p><h2>结论</h2><p>考虑到利害关系，不将帕斯卡抢劫的考虑因素作为制定计划的出发点的一部分并不时重新审视是不负责任的，但不能以牺牲总体效率为代价，因为有效为大型公用事业提供了许多可能的路线，而一般模型的不确定性让人质疑这是否是看待这个问题的正确方法。</p><p> （最后一点，帕斯卡抢劫案是对这个问题的一个非常消极的框架，但探索巨大公用事业的整体不对称性和影响是另一篇文章的主题。） <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrcut65obs6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcut65obs6">^</a></strong></sup></span><div class="footnote-content"><p> Eliezer 的原始帖子专门与所罗门诺夫归纳法中的概率相关，其中图灵机的效用增长速度比其先验概率收缩的速度要快得多，但包括我在内的许多人将其视为个人问题，并且是评论最多的问题之一LessWrong 上的帖子。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnndu3c0vdwsd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefndu3c0vdwsd">^</a></strong></sup></span><div class="footnote-content"><p> 😅 是“汗水微笑”表情符号。我更喜欢看起来更坚定/更享受的东西，但找不到一个表情符号来捕捉这一点。我希望表情符号有助于让事情脚踏实地，在面对超现实的考虑时不要<i>过于</i>严肃。 </p><p><img style="width:39.12%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/z1khght3w6bgpagmf4w9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kqjwdbdekvlpautobws8 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/b9x31hbwgevhiemtbqup 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qz0nlxr42xiiofs3i6nv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dhebklhymltoqajhq1v1 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/yimehxwqrgjottfvqnya 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pjiisl24jn171zz57rtn 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fj1lriucgmnij2lko14e 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ma1bc2isdnqlvsnrgcri 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/iqkdvlyygn1uwyg0cyp4 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dvwkphnsxreyuzp1pcvm 1200w"></p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers<guid ispermalink="false"> DdSnA73EQcWXzhs5j</guid><dc:creator><![CDATA[cSkeleton]]></dc:creator><pubDate> Wed, 15 Nov 2023 18:06:37 GMT</pubDate> </item><item><title><![CDATA[New report: "Scheming AIs: Will AIs fake alignment during training in order to get power?"]]></title><description><![CDATA[Published on November 15, 2023 5:16 PM GMT<br/><br/><p> （从<a href="https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">我的网站</a>交叉发布）</p><p>我写了一份报告，内容是关于高级人工智能是否会在训练期间假装对齐，以便稍后获得权力——我将这种行为称为“诡计”（有时也称为“欺骗性对齐”）。该报告可在 arXiv<a href="https://arxiv.org/abs/2311.08379">上</a>查看： <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里</a>还有一个音频版本，我在下面包含了介绍部分。本节包括报告的完整摘要，其中涵盖了大部分要点和技术术语。我希望摘要能够提供理解报告各个部分所需的大部分背景信息。</p><h1>抽象的</h1><blockquote><p>这份报告研究了在训练中表现良好的高级人工智能是否会这样做，以便在以后获得权力——我将这种行为称为“阴谋”（有时也称为“欺骗性联盟”）。我的结论是，使用基线机器学习方法来训练足够复杂的目标导向人工智能来进行阴谋策划，阴谋是一个令人不安的合理结果（在给定这些条件的情况下，我对这种结果的主观概率约为 25%）。特别是：如果在训练中表现良好是获得力量的一个好策略（我认为很可能是这样），那么各种各样的目标就会激发计划——从而产生良好的训练表现。这使得训练可能自然地实现这样的目标然后强化它，或者积极推动模型的动机实现这样的目标，作为提高性能的简单方法。更重要的是，由于阴谋者假装在旨在揭示其动机的测试上保持一致，因此可能很难判断这种情况是否已经发生。不过，我也认为有安慰的理由。特别是：阴谋实际上可能不是获得权力的好策略；训练中的各种选择压力可能不利于阴谋者的目标（例如，相对于非阴谋者，阴谋者需要进行额外的工具推理，这可能会损害他们的训练表现）；我们也许可以有意地增加这样的压力。该报告详细讨论了这些问题以及各种其他考虑因素，并提出了一系列实证研究方向以进一步探讨该主题。</p></blockquote><h1> 0. 简介</h1><p>寻求权力的特工往往有动机欺骗他人其动机。例如，考虑一下竞选活动中的政治家（“我<em>非常</em>关心你的宠物问题”），求职者（“我只是对小部件感到非常兴奋”），或者寻求父母赦免的孩子（“我”我非常抱歉，再也不会这样做了”）。</p><p>这份报告探讨了我们是否应该期望在训练期间动机看似良性的先进人工智能会参与这种形式的欺骗。在这里，我区分了四种（越来越具体）类型的欺骗性人工智能：</p><ul><li><p><strong>结盟伪造者</strong>：人工智能假装比实际更加结盟。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-1" id="fnref-jCfZhXha29uHnHWwY-1">[1]</a></sup></p></li><li><p><strong>训练游戏玩家</strong>：人工智能了解训练他们的过程（我将这种理解称为“情境意识”），并且正在针对我所说的“情节奖励”进行优化（并且通常会激励假装对齐） ，如果这样做会带来奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-2" id="fnref-jCfZhXha29uHnHWwY-2">[2]</a></sup></p></li><li><p><strong>权力驱动的工具训练游戏玩家（或“阴谋家”）</strong> ：专门进行训练游戏的人工智能，目的是为了以后为自己或其他人工智能获得权力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-3" id="fnref-jCfZhXha29uHnHWwY-3">[3]</a></sup></p></li><li><p><strong>目标守护阴谋者：</strong>其权力寻求策略特别涉及试图阻止训练过程改变其目标的阴谋者。</p></li></ul><p>我认为，根据粗心的人类反馈进行微调的先进人工智能很可能默认以各种方式伪造对齐，因为粗心的反馈会奖励这种行为。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-4" id="fnref-jCfZhXha29uHnHWwY-4">[4]</a></sup>很可能，这样的人工智能也会玩训练游戏。但我在这份报告中的兴趣特别在于他们是否会将这样做作为以后获得权力的策略的一部分——也就是说，他们是否会成为阴谋家（这种行为在文献中通常被称为“欺骗性联盟”，虽然我不会在这里使用这个术语）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-5" id="fnref-jCfZhXha29uHnHWwY-5">[5]</a></sup>我的目的是澄清和评估支持和反对这一期望的论点。</p><p><strong>我目前的观点是，阴谋是使用基线机器学习方法（例如：自我监督预训练，然后针对各种现实世界任务进行 RLHF）训练高级、目标导向的人工智能的令人担忧的合理结果</strong>。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-6" id="fnref-jCfZhXha29uHnHWwY-6">[6]</a></sup>在我看来，引起关注的最基本原因是：</p><ol><li><p>在训练中表现良好可能是获得总体力量的一个很好的工具性策略。</p></li><li><p>如果是这样，那么各种各样的目标就会激发计划（从而产生良好的训练表现）；而与良好训练表现兼容的非计划目标则更加具体。</p></li></ol><p>在我看来，（1）和（2）的结合似乎是合理的，即以训练创建目标导向的、情境感知的模型为条件，它很可能出于某种原因灌输类似阴谋家的目标。尤其：</p><ul><li><p>训练可能会“自然地”达到这样的目标（无论是在态势感知出现之前还是之后），因为这样的目标最初会导致训练中表现足够好，即使没有训练游戏。 （尤其是如果你有意尝试诱导你的模型在长期范围内进行优化，我认为会有动力这样做。）</p></li><li><p>即使类似策划者的目标不会“自然地”出现，一旦模型具有参与训练博弈的态势感知能力，主动将模型<em>转变</em>为策划者可能是 SGD 提高模型训练性能的最简单方法。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-7" id="fnref-jCfZhXha29uHnHWwY-7">[7]</a></sup></p></li></ul><p>更重要的是，由于阴谋者积极假装在旨在揭示其动机的测试中保持一致，因此可能很难判断这种情况是否已经发生。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-8" id="fnref-jCfZhXha29uHnHWwY-8">[8]</a></sup>这似乎是值得严重关注的原因。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-9" id="fnref-jCfZhXha29uHnHWwY-9">[9]</a></sup></p><p>不过，我也认为有安慰的理由。我将它们分为两类。</p><p> The first category centers on the possibility that scheming isn&#39;t actually a very convergently-good instrumental strategy.</p><ul><li><p> For example: the most common story about schemers focuses on <em>goal-guarding</em> schemers, which are training-gaming in order to prevent their goals from being modified by the training process. But it&#39;s not clear to me that this will work well enough (in particular: goal-modification might continue despite training-gaming <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-10" id="fnref-jCfZhXha29uHnHWwY-10">[10]</a></sup> ), or that the resulting goals will get sufficiently large amounts of future power, with sufficiently high probability, to justify any costs the model is incurring by training-gaming rather than just going for its goals directly (if these costs are meaningful – which for some models they might not be).</p></li><li><p> There are alternative stories about why scheming will be a good instrumental strategy – stories, for example, that center on coordination amongst AIs with different values; on the possibility that AIs will have quite similar values by default; and on the possibility that AIs will intrinsically value things like &quot;my survival&quot; (in a manner tolerant of values-change) and/or &quot;AI empowerment&quot; (in a manner tolerant of very different values in the AIs-so-empowered). And while these stories do make scheming seem like a more disjunctive hypothesis, to me, a number of them seem to me some combination of (a) more speculative and (b) requiring that we posit models with much more specific sorts of goals (thereby dulling the force of (2) above). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-11" id="fnref-jCfZhXha29uHnHWwY-11">[11]</a></sup></p></li></ul><p> The second category centers on forms of selection pressure that may push <em>against</em> scheming – both &quot;naturally,&quot; and because we may be able to intentionally introduce/emphasize these selection pressures in training.</p><ul><li><p> For example: relative to other sorts of models, schemers need to engage in extra instrumental reasoning (eg, backchaining from their long-term goal to the instrumental value of training-gaming, and checking for escape/defection opportunities). This may come at a substantive cost to their performance – a cost we may be able to intentionally increase by eg putting models under more time pressure, forcing them to rely on human-legible scratch-pads for their reasoning, and making genuine escape/defection opportunities more difficult to distinguish from traps. And to the extent that schemers engage in active attempts to undermine human alignment and security efforts while still under human control (the scariest form of scheming, in my view), their performance seems likely to take a more notable hit.</p></li><li><p> What&#39;s more, my own guess is that the costs of this extra instrumental reasoning substantially outweigh what are sometimes called the &quot;simplicity&quot; benefits of scheming – ie, benefits from the fact that selecting a schemer allows SGD to select a simpler goal (since it has such a wide variety to choose from). And this especially given that schemers and non-schemers alike will need to represent concepts like &quot;honesty,&quot; &quot;human values,&quot; &quot;reward,&quot; and so on in detail – so schemers plausibly save on complexity only via differences in the complexity of using/repurposing/&quot;pointing at&quot; a given concept in guiding a model&#39;s motivations.</p></li><li><p> Also: prior to situational awareness, training will actively <em>penalize</em> schemer-like goals whenever they lead to a sacrifice of reward on the episode (more about what I mean by &quot;episode&quot; below). And we can try to actively push in this direction further – especially if we can learn to detect and control when situational awareness arises.</p></li></ul><p> I think the second category, here, is especially important. In particular: one way that arguments that appeal to the comparative number (and/or &quot;measure&quot;) of schemer-like goals vs. non-schemer goals that perform well in training can go wrong is by neglecting the full power of the selection pressure being applied to the goal that training selects. So if the pressure applied in training works <em>against</em> schemers overall, this is very good news. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-12" id="fnref-jCfZhXha29uHnHWwY-12">[12]</a></sup></p><p> The full report discusses these and a wide variety of other relevant considerations in much more detail (and see section 0.2 for a full summary).</p><p> Overall, as a rough stab at quantifying my current view: <strong>if you train a coherently goal-directed, situationally aware, somewhat-better-than-human-level model using baseline forms of self-supervised pre-training + RLHF on diverse, long-horizon, real-world tasks, my subjective probability is ~25% that this will be performing well in training in substantial part as part of an instrumental strategy for seeking power for itself and/or other AIs later.</strong> <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-13" id="fnref-jCfZhXha29uHnHWwY-13">[13]</a></sup></p><ul><li><p> I also think it may be possible to reduce the risk here in fairly simple ways: for example, via training on shorter-horizon tasks, or via more intensive forms of adversarial training prior to situational awareness.</p></li><li><p> And I think the risk <em>increases</em> with more powerful models, who may be better positioned to escape/take-over (and to coordinate/cooperate with other AIs in doing so). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-14" id="fnref-jCfZhXha29uHnHWwY-14">[14]</a></sup></p></li></ul><p> Also, and importantly: non-schemers can engage in schemer-like behavior, too.例如：</p><ul><li><p> Models that are only optimizing for reward on a given episode can fake their alignment (or even: engage in escape/take-over) in an effort to get that reward (especially if the episode in question is quite long).</p></li><li><p> Models that <em>aren&#39;t</em> playing the training game can still end up with power-seeking goals that incentivize various forms of deception.</p></li><li><p> The eventual AIs that matter most might differ in important ways from the paradigm sort of AI I focus on here – for example, they might be more like &quot; <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">language model agents</a> &quot; than single models, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-15" id="fnref-jCfZhXha29uHnHWwY-15">[15]</a></sup> or they might be created via methods that differ even more substantially from sort of baseline ML methods I&#39;m focused on – while still engaging in power-motivated alignment-faking.</p></li></ul><p> So scheming as I&#39;ve defined it is far from the only concern in this vicinity. Rather, it&#39;s a paradigm instance of this sort of concern, and one that seems, to me, especially pressing to understand. At the end of the report, I discuss an array of possible empirical research directions for probing the topic further.</p><h2> 0.1 Preliminaries</h2><p> <em>(This section offers a few more preliminaries to frame the report&#39;s discussion. Those eager for the main content can skip to the summary of the report in section 0.2.)</em></p><p> I wrote this report centrally because I think that the probability of scheming/&quot;deceptive alignment&quot; is one of the most important questions in assessing the overall level of existential risk from misaligned AI. Indeed, scheming is notably central to many models of how this risk arises. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-16" id="fnref-jCfZhXha29uHnHWwY-16">[16]</a></sup> And as I discuss below, I think it&#39;s the scariest form that misalignment can take.</p><p> Yet: for all its importance to AI risk, the topic has received comparatively little direct public attention. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-17" id="fnref-jCfZhXha29uHnHWwY-17">[17]</a></sup> And my sense is that discussion of it often suffers from haziness about the specific pattern of motivation/behavior at issue, and why one might or might not expect it to occur. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-18" id="fnref-jCfZhXha29uHnHWwY-18">[18]</a></sup> My hope, in this report, is to lend clarity to discussion of this kind, to treat the topic with depth and detail commensurate to its importance, and to facilitate more ongoing research. In particular, and despite the theoretical nature of the report, I&#39;m especially interested in informing <em>empirical</em> investigation that might shed further light.</p><p> I&#39;ve tried to write for a reader who isn&#39;t necessarily familiar with any previous work on scheming/&quot;deceptive alignment.&quot; For example: in sections 1.1 and 1.2, I lay out, from the ground up, the taxonomy of concepts that the discussion will rely on. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-19" id="fnref-jCfZhXha29uHnHWwY-19">[19]</a></sup> For some readers, this may feel like re-hashing old ground. I invite those readers to skip ahead as they see fit (especially if they&#39;ve already read the summary of the report, and so know what they&#39;re missing).</p><p> That said, I do assume more general familiarity with (a) the basic arguments about existential risk from misaligned AI, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-20" id="fnref-jCfZhXha29uHnHWwY-20">[20]</a></sup> and (b) a basic picture of how contemporary machine learning works. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-21" id="fnref-jCfZhXha29uHnHWwY-21">[21]</a></sup> And I make some other assumptions as well, namely:</p><ul><li><p> That the relevant sort of AI development is taking place within a machine learning-focused paradigm (and a socio-political environment) broadly similar to that of 2023. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-22" id="fnref-jCfZhXha29uHnHWwY-22">[22]</a></sup></p></li><li><p> That we don&#39;t have strong &quot;interpretability tools&quot; (ie, tools that help us understand a model&#39;s internal cognition) that could help us detect/prevent scheming. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-23" id="fnref-jCfZhXha29uHnHWwY-23">[23]</a></sup></p></li><li><p> That the AIs I discuss are goal-directed in the sense of: well-understood as making and executing plans, in pursuit of objectives, on the basis of models of the world. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-24" id="fnref-jCfZhXha29uHnHWwY-24">[24]</a></sup> (I don&#39;t think this assumption is innocuous, but I want to separate debates about whether to expect goal-directedness per se from debates about whether to expect goal-directed models to be schemers – and I encourage readers to do so as well. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-25" id="fnref-jCfZhXha29uHnHWwY-25">[25]</a></sup> )</p></li></ul><p> Finally, I want to note an aspect of the discussion in the report that makes me quite uncomfortable: namely, it seems plausible to me that in addition to potentially posing existential risks to humanity, the sorts of AIs discussed in the report might well be moral patients in their own right. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-26" id="fnref-jCfZhXha29uHnHWwY-26">[26]</a></sup> I talk, here, as though they are not, and as though it is acceptable to engage in whatever treatment of AIs best serves our ends. But if AIs are moral patients, this is not the case – and when one finds oneself saying (and especially: repeatedly saying) &quot;let&#39;s assume, for the moment, that it&#39;s acceptable to do whatever we want to <em>x</em> category of being, despite the fact that it&#39;s plausibly not,&quot; one should sit up straight and wonder. I am here setting aside issues of AI moral patienthood not because they are unreal or unimportant, but because they would introduce a host of additional complexities to an already-lengthy discussion. But these complexities are swiftly descending upon us, and we need concrete plans for handling them responsibly. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-27" id="fnref-jCfZhXha29uHnHWwY-27">[27]</a></sup></p><h2> 0.2 Summary of the report</h2><p> This section gives a summary of the full report. It includes most of the main points and technical terminology (though unfortunately, relatively few of the concrete examples meant to make the content easier to understand). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-28" id="fnref-jCfZhXha29uHnHWwY-28">[28]</a></sup> I&#39;m hoping it will (a) provide readers with a good sense of which parts of the main text will be most of interest to them, and (b) empower readers to skip to those parts without worrying too much about what they&#39;ve missed.</p><h3> 0.2.1 Summary of section 1</h3><p> The report has four main parts. The first part (section 1) aims to clarify the different forms of AI deception above (section 1.1), to distinguish schemers from the other possible model classes I&#39;ll be discussing (section 1.2), and to explain why I think that scheming is a uniquely scary form of misalignment (section 1.3). I&#39;m especially interested in contrasting schemers with:</p><ul><li><p> <strong>Reward-on-the-episode seekers</strong> : that is, AI systems that terminally value some component of the reward process for the episode, and that are playing the training game for this reason.</p></li><li><p> <strong>Training saints</strong> : AI systems that are directly pursuing the goal specified by the reward process (I&#39;ll call this the &quot;specified goal&quot;). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-29" id="fnref-jCfZhXha29uHnHWwY-29">[29]</a></sup></p></li><li><p> <strong>Misgeneralized non-training-gamers</strong> : AIs that are neither playing the training game <em>nor</em> pursuing the specified goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-30" id="fnref-jCfZhXha29uHnHWwY-30">[30]</a></sup></p></li></ul><p> Here&#39;s a diagram of overall taxonomy: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/s1uqfkjys8bmk2qaidhp" alt=""></p><p> All of these model classes can be misaligned and dangerous. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-31" id="fnref-jCfZhXha29uHnHWwY-31">[31]</a></sup> But I think schemers are especially scary. In particular: scheming prompts the most robust and adversarial efforts to prevent humans from learning about the misalignment in question <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-32" id="fnref-jCfZhXha29uHnHWwY-32">[32]</a></sup> ; and it&#39;s the most likely to motivate what I call &quot;early undermining&quot; – that is, AIs at a comparatively early stage of AI development actively trying to undermine human efforts to align, control, and secure future AI systems. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-33" id="fnref-jCfZhXha29uHnHWwY-33">[33]</a></sup></p><p> I also discuss (in section 1.4) whether theoretical arguments of the type the report focuses on are even useful to consider. My answer here is &quot;somewhat useful,&quot; both to inform empirical investigation and as a source of evidence in the absence of adequate empirical data, but we should clearly be looking for as much empirical evidence as we can (I offer some suggestions in this vein in section 6).</p><p> In section 1.5, I discuss a factor that seems to me important in assessing various of the arguments I consider: namely, the amount of &quot;slack&quot; that training gives AIs to perform less-than-optimally.</p><h3> 0.2.2 Summary of section 2</h3><p> The second part of the report examines the prerequisites for scheming. In particular, I focus on:</p><ul><li><p> <strong>Situational awareness:</strong> the model understands that it&#39;s a model in a training process, what the training process will reward, and the basic nature of the objective world in general.</p></li><li><p> <strong>Beyond-episode goals:</strong> the model cares about the consequences of its actions after the episode is complete.</p></li><li><p> <strong>Aiming at reward-on-the-episode as part of a power-motivated instrumental strategy:</strong> the model believes that its beyond-episode goals will be better achieved if it optimizes for reward-on-the-episode – and in particular, that it, or some other AIs, will get more power if it does this.</p></li></ul><p> Section 2.1 discusses situational awareness. I think that absent active effort, we should at least expect certain kinds of advanced AIs – for example, advanced AIs that are performing real-world tasks in live interaction with the public internet – to be situationally aware by default, because (a) situational awareness is very useful in performing the tasks in question (indeed, we might actively train it into them), and (b) such AIs will likely be exposed to the information necessary to gain such awareness. However, I don&#39;t focus much on situational awareness in the report. Rather, I&#39;m more interested in whether to expect the other two prerequisites above in situationally-aware models.</p><p> Section 2.2 discusses beyond-episode goals. Here I distinguish (in section 2.2.1) between two concepts of an &quot;episode,&quot; namely:</p><ul><li><p> <strong>The incentivized episode</strong> : that is, the temporal horizon that the gradients in training actively pressure the model to optimize over. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-34" id="fnref-jCfZhXha29uHnHWwY-34">[34]</a></sup></p></li><li><p> <strong>The intuitive episode</strong> : that is, some other intuitive temporal unit that we call the &quot;episode&quot; for one reason or another (eg, reward is given at the end of it; actions in one such unit have no obvious causal path to outcomes in another; etc).</p></li></ul><p> When I use the term &quot;episode&quot; in the report, I&#39;m talking about the incentivized episode. Thus, &quot;beyond-episode goals&quot; means: goals whose temporal horizon extends beyond the horizon that training actively pressures models to optimize over. But very importantly, the incentivized episode isn&#39;t necessarily the intuitive episode. That is, deciding to call some temporal unit an &quot;episode&quot; doesn&#39;t mean that training isn&#39;t actively pressuring the model to optimize over a horizon that extends beyond that unit: you need to actually look in detail at how the gradients flow (work that I worry casual readers of this report might neglect). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-35" id="fnref-jCfZhXha29uHnHWwY-35">[35]</a></sup></p><p> I also distinguish (in section 2.2.2) between two types of beyond-episode goals, namely:</p><ul><li><p> <strong>Training-game- <em>independent</em> beyond-episode goals</strong> : that is, beyond-episode goals that arise <em>independent</em> of their role in motivating a model to play the training game.</p></li><li><p> <strong>Training-game- <em>dependent</em> beyond-episode goals</strong> : that is, beyond-episode goals that arise <em>specifically because</em> they motivate training-gaming.</p></li></ul><p> These two sorts of beyond-episode goals correspond to two different stories about how scheming happens.</p><ul><li><p> In the first sort of story, SGD happens to instill beyond-episode goals in a model &quot;naturally&quot; (whether before situational awareness arises, or afterwards), and <em>then</em> those goals begin to motivate scheming. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-36" id="fnref-jCfZhXha29uHnHWwY-36">[36]</a></sup></p></li><li><p> In the second sort of story, SGD &quot;notices&quot; that giving a model beyond-episode goals <em>would</em> motivate scheming (and thus, high-reward behavior), and so actively <em>gives</em> it such goals for this reason. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-37" id="fnref-jCfZhXha29uHnHWwY-37">[37]</a></sup></p></li></ul><p> This second story makes most sense if you assume that situational awareness is already in place. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-38" id="fnref-jCfZhXha29uHnHWwY-38">[38]</a></sup> So we&#39;re left with the following three main paths to scheming: <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-39" id="fnref-jCfZhXha29uHnHWwY-39">[39]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/v3064isqjtarmfhwj8qh" alt=""></p><p> In section 2.2.2.1, I discuss training-game-independent beyond-episode goals (that is, path 1 and 2). Should we expect beyond-episode goals to arise &quot;naturally&quot;?</p><ul><li><p> One reason to expect this is that plausibly, goals don&#39;t come with temporal limitations by default – and &quot;model time&quot; might differ from &quot;calendar time&quot; regardless.</p></li><li><p> One reason to <em>not</em> expect this is that training will actively <em>punish</em> beyond-episode goals whenever they prompt the model to sacrifice reward-on-the-episode for some beyond-episode benefit. And we may be able to use adversarial training to search out such goals and punish them more actively.</p></li></ul><p> In section 2.2.2.2, I discuss training-game- <em>dependent</em> beyond-episode goals. In particular, I highlight the question of whether SGD will be adequately able to &quot;notice&quot; the benefits of turning a non-schemer into a schemer, given the need to make the transition incrementally, via tiny changes to the model&#39;s weights, each of which improve the reward. I think that this is a serious objection to stories focused on training-game-dependent beyond-episode goals, but I also don&#39;t feel that I can confidently rule out SGD&#39;s ability to make a transition of this kind (see eg &quot; <a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">you can&#39;t evolve eyes</a> &quot; as an example of places I think intuitions in this vein can go wrong).</p><p> I also discuss, in section 2.2.3, the possibility that the model&#39;s goal-directedness will be &quot;messy&quot; in a way that complicates training-game-dependent stories where SGD can simply modify a model&#39;s goal and thereby smoothly redirect its &quot;goal-achieving engine&quot; in a new direction (I think that this sort of &quot;messiness&quot; is quite likely). And I touch on a broader intuition that scheming requires an unusually high-standard of goal-directedness more generally. I think this intuition has less force once we condition on the sort of goal-directedness typically assumed in the alignment discourse (and which I generally assume in the report as well). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-40" id="fnref-jCfZhXha29uHnHWwY-40">[40]</a></sup> But it&#39;s relevant to our overall probabilities regardless.</p><p> In section 2.2.4, I discuss whether beyond-episode goals are more likely in models actively <em>trained</em> to have long-term (even if still: within-episode) goals. I think that beyond-episode goals are indeed more likely in models of this kind – and that baseline incentives to create models that engage in fairly long-horizon optimization (eg, &quot;make lots of money for my company over the next year&quot;) are correspondingly worrying. However, I also think various sorts of cognitive work that might be highly useful to efforts to avoid an AI takeover (for example, research aimed at improving the alignment, interpretability, and security of our AI systems) may not require long-horizon optimization of this kind. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-41" id="fnref-jCfZhXha29uHnHWwY-41">[41]</a></sup></p><p> With this discussion of beyond-episode goals in view, I turn, in section 2.3, to the final prerequisite for scheming: namely, deciding to training-game as part of a power-motivated strategy for promoting the beyond-episode goal in question.</p><p> Here I focus first (in section 2.3.1) on what I call the &quot;classic goal-guarding story&quot; about the origins of this decision. We can decompose this story into two claims:</p><ol><li><p> <strong>The goal-guarding hypothesis</strong> . If the model optimizes for reward-on-the-episode during training, then the goals that survive training will be such that the model would want them promoted in the future; whereas if the model doesn&#39;t optimize for reward-on-the-episode during training, the model will want this less.</p></li><li><p> <strong>Adequate future empowerment.</strong> Conditional on surviving training to the degree at stake in (A), the model&#39;s goals will then end up empowered to a degree (and with sufficient probability) as to justify (given the model&#39;s other goals, its tolerance for risk, etc) pursuing a strategy of training-gaming overall.</p></li></ol><p> I then investigate each of these in turn (in section 2.3.1.1 and 2.3.1.2, respectively).</p><p> Both seem to me non-obvious. In particular, with respect to the goal-guarding hypothesis:</p><ul><li><p> Even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well – especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on.</p><ul><li> Thus, as a toy example: imagine that if I go into the desert and stack bricks for a million years, I will get a huge amount of money that I can then donate to my charity of choice. But suppose that throughout this whole ordeal, my brain will be hooked up to a machine that continually modifies all my neurons to encourage actions that stack bricks better, and to discourage actions that stack bricks worse. And suppose my intention is to just try to stack bricks as well as I can. Will I emerge from this process with my values substantially affected by the relevant brain modifications (such that, for example, I emerge obsessed with stacking bricks – or at least, with very different taste in charities)? It seems at least plausible.</li></ul></li><li><p> On the classic goal-guarding story, once a model starts training-gaming, the specific content of its goal no longer matters to its behavior – any other schemer-like goal would lead to the same thing. But this makes it seem like the goal could &quot;float around&quot; between schemer-like goals (or get moved by other pressures within SGD, like regularization), without affecting training performance. And if that happens, then scheming may not be a good strategy for goal-guarding after all. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-42" id="fnref-jCfZhXha29uHnHWwY-42">[42]</a></sup></p></li><li><p> Of course, the model may be tolerant of <em>some</em> changes to its goals – and these changes may be preferable to the ones that would occur <em>absent</em> training-gaming. But the possibility of significant ongoing goal-modification introduces an extra uncertainty into the classic goal-guarding story.</p></li><li><p> People sometimes talk about more sophisticated techniques for avoiding goal-modification, where eg a model manipulates gradient descent more actively by making its good performance in training conditional on whether its goals get modified or not (see eg the discourse around &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;). I don&#39;t focus on these methods here, but they strike me as quite difficult to pull off, especially for only somewhat-better-than-human-level models. And if scheming requires that they work, I think it&#39;s worth being very clear about that, and giving the feasibility of such methods commensurate attention.</p></li></ul><p> With respect to whether the resulting goals will be adequately empowered:</p><ul><li><p> The most worrying types of schemers aren&#39;t merely seeking power on future training episodes. Rather, they are specifically aiming at some combination of escape and take-over.</p></li><li><p> But this means that the relevant goals have to do more than just survive training. They also need to survive either until the model can either escape/participate in a take-over itself, or to exert sufficiently substantive influence on some future generation of models that will be able to do so.</p><ul><li> For example, if GPT-6 isn&#39;t powerful enough to escape/take-over itself, but GPT-8 will be, then GPT-6&#39;s main path to power may be influencing the values of GPT-7, which in turn influences the values of GPT-8. But this form of influence is quite indirect; it may face substantial further barriers from eg human monitoring; and it will plausibly involve more significant deviation from max-reward training performance.</li></ul></li><li><p> What&#39;s more, the relevant escape/take-over attempt needs to be suitably likely to <em>succeed</em> , to occur over time horizons that the model cares about, and to result in the model&#39;s favored goals in particular receiving a suitably substantive share of power (despite the fact that large numbers of AIs with very different values may be competing for the power in question).</p></li><li><p> These uncertainties introduce haircuts to the instrumental value of a schemer-like strategy. However, the importance of these haircuts depends on the burden of proof that such a strategy needs to meet. In particular: the more the model stands to gain from <em>not</em> training-gaming, the more attractive scheming will need to be in order to win out as a strategy. But for some models, the costs of training-gaming might be relatively low. And for suitably &quot;ambitious&quot; models, even low-probabilities of success via a schemer-like strategy might justify even a substantive sacrifice.</p></li></ul><p> Overall, the classic goal-guarding story seems to me to introduce a number of additional uncertainties with respect to the viability of scheming as an instrumental strategy.</p><p> However, it&#39;s also not the only story available. In section 2.3.2, I consider a number of &quot;non-classic&quot; stories, which relax various of the classic story&#39;s constraints.例如：</p><ul><li><p> <strong>AI coordination</strong> . If AIs with different values coordinate to share power after take-over, and/or to compensate AIs who worked on behalf of take-over but who weren&#39;t powerful enough to escape/take-over themselves, then this might mitigate some of the haircuts above. For example, GPT-6 might be more willing to work on GPT-8&#39;s behalf in the example above, if GPT-6 were confident that GPT-8 would &quot;repay the favor&quot; once in power. I think some analyses of coordination of this form assume too readily that it will be (a) easy, and (b) the obvious strategy relative to eg coordinating with humans instead (especially for only-somewhat-better-than-human neural nets acting in a still-human-controlled world where interpretability is still unsolved for <em>everyone</em> ). But I think that AI coordination of this type is a real concern and worth serious attention – both in the context of scheming in particular, and in the context of AI risk more generally.</p></li><li><p> <strong>AIs with similar values by default</strong> . Relatedly, if AIs will generally have sufficiently similar values by default, this can reduce the need for a schemer to specifically propagate its own goals into the future, and free it up to work on behalf of AI take-over/empowerment more directly. And it does seem likely that eg GPT-6&#39;s values and GPT-8&#39;s values will be at least much <em>more</em> similar, by default, than eg GPT-6&#39;s values and a given set of human values.</p></li><li><p> <strong>Terminal values that happen to favor escape/takeover</strong> . We can also imagine models with <em>terminal</em> values that happen to favor escape/AI takeover even if the goals that get empowered as a result aren&#39;t very similar to the model&#39;s own. For example, perhaps the model values some concept of &quot;survival&quot; based on a conception of personal identity tolerant of the sorts of goal changes that training will cause; or perhaps the model just supports something like &quot;AI empowerment,&quot; even if the AIs-so-empowered won&#39;t have values very similar to its own. However, some stories of this form begin to impose fairly strong constraints on the goals that schemers need to have, thereby undermining one of the key arguments for scheming: namely, that it seems like a convergently-good strategy across a wide variety of goals.</p></li></ul><p> (I also consider a few other alternative stories – namely, models with false beliefs about the instrumental value of scheming, models that are self-deceived about their alignment, and models that are uncertain about their goals/hazy about exactly why they want power. But these seem to me less important.)</p><p> The availability of non-classic stories like these makes the case for scheming feel, to me, more disjunctive. However, some of these stories also seem to me some combination of (a) more speculative, and (b) requiring of more specific hypotheses about the sorts of goals that AIs will develop.</p><p> My overall takeaways from section 2 are:</p><ul><li><p> I think there are relatively strong arguments for expecting situational awareness by default, at least in certain types of AI systems (ie, AI systems performing real-world tasks in live interaction with sources of information about who they are).</p></li><li><p> But I feel quite a bit less clear about beyond-episode goals and aiming-at-reward-on-the-episode-as-part-a-power-motivated-instrumental-strategy.</p></li></ul><p> I then turn, in the next two sections, to an examination of the more specific arguments for and against expecting schemers vs. other types of models. I divide these into two categories, namely:</p><ul><li><p> Arguments that focus on the <em>path</em> that SGD needs to take in building the different model classes in question (section 3).</p></li><li><p> Arguments that focus on the <em>final properties</em> of the different model classes in question (section 4). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-43" id="fnref-jCfZhXha29uHnHWwY-43">[43]</a></sup></p></li></ul><h3> 0.2.3 Summary of section 3</h3><p> The third part of the report focuses on the former category of argument.</p><p> I break this category down according to the distinction between &quot;training-game- <em>independent</em> &quot; and &quot;training-game- <em>dependent</em> &quot; beyond-episode goals. My sense is that the most traditional story about the path to schemers focuses on the former sort. It runs roughly as follows:</p><ol><li><p> Because of [insert reason], the model will develop a (suitably ambitious) beyond-episode goal correlated with good performance in training (in a manner that <em>doesn&#39;t</em> route via the training game). This could happen before situational awareness arrives, or afterwards.</p></li><li><p> Then, in conjunction with situational awareness, this (suitably ambitious) beyond-episode goal will start to motivate training-gaming.</p></li></ol><p> Modulo my questions about the viability of scheming as an instrumental strategy, I take this sort of argument fairly seriously. I think the most immediate question is: why did the model develop this sort of beyond-episode goal? I discussed some reasons for and against expecting this already (in the summary of section 2.2.2.1), but they don&#39;t seem to me decisive in either direction: and especially given that a very wide variety of goals could in principle motivate scheming, it just does seem possible for a schemer-like goal to pop out of training in this way. And while it may be possible to use adversarial training prior to situational awareness to try to prevent this, this training faces a number of barriers as well (eg, it needs to be diverse/thorough enough, it needs to contend with difficulties knowing/controlling when a model develops situational awareness, and in some cases models might already have situational awareness by the time we&#39;re worried about the beyond-episode goal developing). So I think this sort of path to scheming is a real concern. (See section 3.1 for more.)</p><p> I then turn, in section 3.2, to a story focused on training-game- <em>dependent</em> beyond-episode goals, which runs roughly as follows:</p><ol><li><p> By the time the model becomes situationally aware, it probably won&#39;t be pursuing a max-reward goal (that is, a goal pursuit of which on the training data leads to roughly the maximum reward consistent with the model&#39;s other capabilities). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-44" id="fnref-jCfZhXha29uHnHWwY-44">[44]</a></sup> Rather, it will be pursuing some less-than-max-reward proxy goal.</p></li><li><p> But at that point, the world-model will contain all the information the model needs in order to training-game. (However, because we&#39;re here focused on &quot;training-game- <em>dependent</em> &quot; stories about scheming, we assume that at the point of getting situational awareness, the model&#39;s goal is not yet such that it will motivate scheming – rather, some further modification would be required for that.)</p></li><li><p> The easiest way for SGD to modify the model into getting maximum reward, at that point, will be to make it a schemer.</p></li><li><p> So: SGD will make the model a schemer. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-45" id="fnref-jCfZhXha29uHnHWwY-45">[45]</a></sup></p></li></ol><p> If we set aside questions about whether SGD can &quot;notice&quot; the benefits of modifications of this type, this sort of story seems to me fairly worrying as well. In particular: I think it&#39;s plausible that schemer-like goals will be sufficiently common in goal-space that one such goal will be easier for SGD to find, from a given starting point, than the specific non-schemer goals that leads to max reward behavior.</p><p> However, non-schemer max-reward goals – for example, the specified goal, or &quot;reward-on-the-episode&quot; – have some advantages too. For example: plausibly, the initial phase of training will point the model in their vicinity by default, since the model, at that stage, needs to be getting high-reward <em>absent</em> instrumental training-gaming (see section 3.2.2.2 for more). And to the extent we&#39;re thinking of the model&#39;s goal-directedness as &quot;messy,&quot; its heuristics, patterns of attention, impulses, and so on will plausibly be pointed in this sort of direction as well – thereby plausibly creating extra &quot;work,&quot; on SGD&#39;s part, to turn the model into a schemer instead (see section 3.2.2.3 for more). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-46" id="fnref-jCfZhXha29uHnHWwY-46">[46]</a></sup></p><h3> 0.2.4 Summary of section 4</h3><p> The fourth part of the report examines arguments that focus on the final properties of the different model classes.</p><p> I start, in section 4.2, with what I call the &quot;counting argument.&quot; It runs as follows:</p><ol><li><p> The non-schemer model classes, here, require fairly specific goals in order to get high reward. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-47" id="fnref-jCfZhXha29uHnHWwY-47">[47]</a></sup></p></li><li><p> By contrast, the schemer model class is compatible with a very wide range of (beyond-episode) goals, while still getting high reward (at least if we assume that the other requirements for scheming to make sense as an instrumental strategy are in place – eg, that the classic goal-guarding story, or some alternative, works). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-48" id="fnref-jCfZhXha29uHnHWwY-48">[48]</a></sup></p></li><li><p> In this sense, there are &quot;more&quot; schemers that get high reward than there are non-schemers that do so.</p></li><li><p> So, other things equal, we should expect SGD to select a schemer.</p></li></ol><p> Something in the vicinity accounts for a substantial portion of my credence on schemers (and I think it often undergirds other, more specific arguments for expecting schemers as well). However, the argument I give most weight to doesn&#39;t move immediately from &quot;there are more possible schemers that get high reward than non-schemers that do so&quot; to &quot;absent further argument, SGD probably selects a schemer&quot; (call this the &quot;strict counting argument&quot;), because it seems possible that SGD actively privileges one of these model <em>classes</em> over the others. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-49" id="fnref-jCfZhXha29uHnHWwY-49">[49]</a></sup> Rather, the argument I give most weight to is something like:</p><ol><li><p> It seems like there are &quot;lots of ways&quot; that a model could end up a schemer and still get high reward, at least assuming that scheming is in fact a good instrumental strategy for pursuing long-term goals.</p></li><li><p> So absent some additional story about why training <em>won&#39;t</em> select a schemer, it feels, to me, like the possibility should be getting substantive weight.</p></li></ol><p> I call this the &quot;hazy counting argument.&quot; It&#39;s not especially principled, but I find that it moves me nonetheless.</p><p> I then turn, in section 4.3, to &quot;simplicity arguments&quot; in favor of expecting schemers. I think these arguments sometimes suffer from unclarity about the sort of simplicity at stake, so in section 4.3.1, I discuss a number of different possibilities:</p><ul><li><p> &quot;re-writing simplicity&quot; (ie, the length of the program required to re-write the algorithm that a model&#39;s weights implement in some programming language, or eg on the tape of a given Universal Turing Machine),</p></li><li><p> &quot;parameter simplicity&quot; (ie, the number of parameters that the actual neural network uses to encode the relevant algorithm),</p></li><li><p> &quot; <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">simplicity realism</a> &quot; (which assumes that simplicity is in some deep sense an objective &quot;thing,&quot; independent of programming-language or Universal Turing Machine, that various simplicity metrics attempt to capture), and</p></li><li><p> &quot;trivial simplicity&quot; (which conflates the notion of &quot;simplicity&quot; with &quot;higher likelihood on priors,&quot; in a manner that makes something like Occam&#39;s razor uninterestingly true by definition).</p></li></ul><p> I generally focus on &quot;parameter simplicity,&quot; which seems to me easiest to understand, and to connect to a model&#39;s training performance.</p><p> I also briefly discuss, in section 4.3.2, the evidence that SGD actively selects for simplicity. Here the case that grips me most directly is just: simplicity (or at least, parameter simplicity) lets a model save on parameters that it can then use to get more reward. But I also briefly discuss some other empirical evidence for simplicity biases in machine learning. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-50" id="fnref-jCfZhXha29uHnHWwY-50">[50]</a></sup></p><p> Why might we expect a simplicity bias to favor schemers? Roughly: the thought is that <em>because</em> such a wide variety of goals can motivate scheming, schemers allow SGD a very wide range of goals to choose from in seeking out simpler goals; whereas non-schemers (that get high reward) do not. And this seems especially plausible to the extent we imagine that the goals required to be such a non-schemer are quite complex. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-51" id="fnref-jCfZhXha29uHnHWwY-51">[51]</a></sup></p><p> Other things equal, I think this is right. But I&#39;m not sure it&#39;s a very large or important effect. For one thing: we know that LLMs like GPT-4 are capable of representing a very large number of complex human concepts with eg order of a trillion parameters - including, plausibly, concepts like &quot;honesty,&quot; &quot;helpfulness,&quot; &quot;reward,&quot; and so on. So this caps the complexity savings at stake in avoiding representations like this. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-52" id="fnref-jCfZhXha29uHnHWwY-52">[52]</a></sup> Thus, as a toy calculation: if we conservatively assume that at most 1% of a trillion-parameter model&#39;s capacity goes to representing concepts as complex as &quot;honesty,&quot; and that it knows at least 10,000 such concepts ( <a href="https://www.merriam-webster.com/help/faq-how-many-english-words">Webster&#39;s unabridged dictionary has ~500,000 words</a> ), then representing the concept of honesty takes at most a millionth of the model&#39;s representational capacity, and even less for the larger models of the future.</p><p> But more importantly, what matters here isn&#39;t the absolute complexity of representing the different goals in question, but the complexity <em>conditional on already having a good world model</em> . And we should assume that <em>all</em> of these models will need to understand the specified goal, the reward process for the episode, etc. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-53" id="fnref-jCfZhXha29uHnHWwY-53">[53]</a></sup> And granted such an assumption, the <em>extra</em> complexity costs of actively <em>optimizing</em> for the specified goal, or for reward-on-the-episode, seem to me plausibly extremely small. Plausibly, they&#39;re just: whatever the costs are for using/repurposing (&quot;pointing at&quot;) that part of the world-model for guiding the model&#39;s motivations.</p><p> Of course, we can try to rerun the same simplicity argument at the level of the complexity costs of using/repurposing different parts of the world model in that way. For example, we can say: &quot;however this process works, presumably it&#39;s simpler to do for some goals than others – so given how many schemer-like goals there are, plausibly it will be simpler to do for some schemer-like goal.&quot; I think this is the strongest form of the simplicity argument for expecting schemers. But it also requires abandoning any intuitive grip we might&#39;ve had on which goals are &quot;simple&quot; in the relevant sense. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-54" id="fnref-jCfZhXha29uHnHWwY-54">[54]</a></sup> And it seems plausible to me that the simplicity differences between different &quot;pointers&quot; are very small relative to the model&#39;s overall capacity. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-55" id="fnref-jCfZhXha29uHnHWwY-55">[55]</a></sup></p><p> I also discuss, in section 4.3.5, whether simplicity arguments make plausible predictions about the sorts of goals a schemer will end up with. In particular: some versions of these arguments can seem to predict that schemers are likely to end up with extremely simple long-term goals, <em>regardless of whether those goals had any relation to what was salient or important during training --</em> predictions about which I, personally, am skeptical. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-56" id="fnref-jCfZhXha29uHnHWwY-56">[56]</a></sup> Of course, it&#39;s also possible to posit that goal targets salient/relevant during training will also be &quot;simpler&quot; for the model to pursue. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-57" id="fnref-jCfZhXha29uHnHWwY-57">[57]</a></sup> But if we grant some story in this vein (I haven&#39;t heard strong arguments for doing so), note that it also speaks in favor of the simplicity of the specified goal, and of reward-on-the-episode, as well - since these are <em>especially</em> salient/relevant during the training process. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-58" id="fnref-jCfZhXha29uHnHWwY-58">[58]</a></sup></p><p> In section 4.4, I turn to arguments <em>against</em> scheming that focus on the final properties of the different model classes. Here, the main argument I&#39;m aware of is that schemers need to do extra reasoning that other model classes don&#39;t (these are sometimes called &quot;speed costs&quot; – though I think they could in principle affect the complexity of a model&#39;s algorithm as well).尤其：</p><ol><li><p> <strong>Deciding to training-game</strong> : The model needs to reason backwards from &quot;I want this beyond-episode thing&quot; to &quot;I should do well in training,&quot; in a way that draws on the model&#39;s predictions about what doing well in training will entail (eg, re: goal-guarding, take-over prospects, and so on).</p></li><li><p> <strong>Checking whether it&#39;s time to escape/take-over:</strong> The model needs to be checking, with adequate frequency and accuracy, whether it&#39;s in a position to escape/take-over (and to succeed in this assessment despite human efforts to trap/fool the model into revealing its true colors – for example, by offering it <em>apparent</em> opportunities to defect). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-59" id="fnref-jCfZhXha29uHnHWwY-59">[59]</a></sup></p></li><li><p> <strong>(Optional, but relevant to how scary the schemer is) Early undermining</strong> : To the extent the model is doing anything (other than training-gaming) to support a take-over/escape <em>prior</em> to escaping human control, <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-60" id="fnref-jCfZhXha29uHnHWwY-60">[60]</a></sup> it will need to be using cognitive resources both in (a) performing the relevant nefarious actions and (b) making sure it gets away with them (eg, checking that it won&#39;t get caught, coming up with excuses, making the cover stories consistent even when under scrutiny, and so on).</p></li></ol><p> How big are the costs of this reasoning? I think that (I) may well be fairly easy, but (II) seems more plausibly onerous (especially in the context of active efforts by humans to put the models under time-pressure, and make genuine escape/take-over opportunities difficult to distinguish from traps). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-61" id="fnref-jCfZhXha29uHnHWwY-61">[61]</a></sup> And (III), if the model goes for it, seems like it could be quite difficult – and like it would often actively require deviation from the highest-reward sorts of training performance. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-62" id="fnref-jCfZhXha29uHnHWwY-62">[62]</a></sup></p><p> More importantly, though: the question – at least in the context of a comparison between the final properties of the model classes – isn&#39;t the absolute size of the costs in question, but rather their size <em>relative to the simplicity benefits of scheming</em> . Here, my current take is that the costs of the instrumental reasoning seem larger than the benefits of the simpler goal.</p><p> This is mostly an intuitive judgment, but we can also try to hazily quantify the comparison.</p><ul><li><p> Thus, for example, suppose that our model is a purely feedforward network with 1000 layers and &lt;10,000 serial steps per input, that the instrumental reasoning at stake just in a schemer deciding to training-game (let alone: checking for defection opportunities) takes at least ten steps, and that the model doesn&#39;t begin optimizing for reward-on-the-episode until that reasoning is complete. In that case, the instrumental reasoning is using up around .1% of the compute that could be going to getting reward-on-the-episode, or to some other max-reward goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-63" id="fnref-jCfZhXha29uHnHWwY-63">[63]</a></sup></p></li><li><p> Are the complexity costs of representing, or pointing at, a more complicated goal likely to outweigh this? My guess is no. For example, if we follow my (conservative, in my opinion) calculation above and assume that representing (or pointing at) a concept like &quot;honesty&quot; costs at most a millionth of a trillion parameter models parameters (and less for a larger model), then a .0001% reduction in parameters would need to outweigh a .1% reduction in compute – and absent further argument, it seems strange, to me, to expect this.</p></li></ul><p> Of course, this is only a toy calculation, of unclear applicability to the real-world case. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-64" id="fnref-jCfZhXha29uHnHWwY-64">[64]</a></sup> The broader point is just: if serial-time thinking is scarce for a model (as it plausibly is for a transformer), the sorts of instrumental reasoning at stake in scheming seem like they could well substantively impact a model&#39;s performance. My guess is that the complexity costs of non-schemer goals are minor in comparison.</p><p> I also briefly discuss, in section 4.4.3, whether we can actively shape our training processes to bias more heavily towards &quot;speed.&quot; Here it seems relatively straightforward to do things like put models under more time pressure, limit their compute access, force them to plan via human-legible scratch-pads rather than via internal processes, and to put effort into making our attempts to trap them/trip-them-up more sophisticated (and so: more requiring of cognition to detect). But any efforts in this vein will need to avoid over-large costs to competitiveness overall (though if you buy that the speed costs of scheming are quite a bit more substantive than the simplicity benefits, then so long as speed matters <em>somewhat</em> relative to simplicity, this might be enough).</p><p> Section 4.5 briefly discusses whether some analog of &quot;intrinsic passion for a given task&quot; might make a difference to AI performance, especially in the context of a form of &quot;messy goal-directedness&quot; that blurs the line between an AI&#39;s capabilities and its values. I find considerations in this vein interesting, but quite speculative.</p><p> Section 4.6 discusses the relevance of &quot;slack&quot; in training to evaluating how much weight to put on factors like the simplicity benefits and speed costs of scheming. In particular: especially in a high-slack regime, it seems plausible that these factors are in the noise relative to other considerations.</p><h3> 0.2.5 Summary of section 5</h3><p> The first four sections of the report are the main content. sums up my overall take. I&#39;ve already summarized most of this in the introduction above, and I won&#39;t repeat that content here. However, I&#39;ll add a few points that the introduction didn&#39;t include.</p><p> In particular: I think some version of the &quot;counting argument&quot; undergirds most of the other arguments for expecting scheming that I&#39;m aware of (or at least, the arguments I find most compelling). That is: schemers are generally being privileged as a hypothesis because a very wide variety of goals could in principle lead to scheming, thereby making it easier to (a) land on one of them naturally, (b) land &quot;nearby&quot; one of them, or (c) find one of them that is &quot;simpler&quot; than non-schemer goals that need to come from a more restricted space. In this sense, the case for schemers mirrors one of the most basic arguments for expecting misalignment more generally – eg, that alignment is a very narrow target to hit in goal-space. Except, here, we are specifically <em>incorporating</em> the selection we know we are going to do on the goals in question: namely, they need to be such as to cause models pursuing them to get high reward. And the most basic worry is just that: this isn&#39;t enough.</p><p> Because of the centrality of &quot;counting arguments&quot; to the case for schemers, I think that questions about the strength of the selection pressure <em>against</em> schemers – for example, because of the costs of the extra reasoning schemers have to engage in – are especially important. In particular: I think a key way that &quot;counting arguments&quot; can go wrong is by neglecting the power that active selection can have in overcoming the &quot;prior&quot; set by the count in question. For example: the <em>reason</em> we can overcome the prior of &quot;most arrangements of car parts don&#39;t form a working car,&quot; or &quot;most parameter settings in this neural network don&#39;t implement a working chatbot,&quot; is that the selection power at stake in human engineering, and in SGD, is <em>that strong</em> . So if SGD&#39;s selection power is actively working against schemers (and/or: if we can cause it to do so more actively), this might quickly overcome a &quot;counting argument&quot; in their favor. For example: if there are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>schemer-like goals for every non-schemer goal, this might make it seem very difficult to hit a non-schemer goal in the relevant space. But actually, 100 bits of selection pressure can be cheap for SGD (consider, for example, 100 extra gradient updates, each worth at least a halving of the remaining possible goals). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-65" id="fnref-jCfZhXha29uHnHWwY-65">[65]</a></sup></p><p> Overall, when I step back and try to look at the considerations in the report as a whole, I feel pulled in two different directions:</p><ul><li><p> On the one hand, at least conditional on scheming being a convergently-good instrumental strategy, schemer-like goals feel scarily common in goal-space, and I feel pretty worried that training will run into them for one reason or another.</p></li><li><p> On the other hand, ascribing a model&#39;s good performance in training to scheming continues to feel, at a gut level, like a fairly specific and conjunctive story to me.</p></li></ul><p> That is, scheming feels robust and common at the level of &quot;goal space,&quot; and yet specific and fairly brittle at the level of &quot;yes, that&#39;s what&#39;s going on with this real-world model, it&#39;s getting reward because (or: substantially because) it wants power for itself/other AIs later, and getting reward now helps with that.&quot; <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-66" id="fnref-jCfZhXha29uHnHWwY-66">[66]</a></sup> When I try to roughly balance out these two different pulls (and to condition on goal-directedness and situational-awareness), I get something like the 25% number I listed above.</p><h3> 0.2.6 Summary of section 6</h3><p> I close the report, in section 6, with a discussion of empirical work that I think might shed light on scheming. (I also think there&#39;s worthwhile theoretical work to be done in this space, and I list a few ideas in this respect as well. But I&#39;m especially excited about empirical work.)</p><p> In particular, I discuss:</p><ul><li><p> Empirical work on situational awareness (section 6.1)</p></li><li><p> Empirical work on beyond-episode goals (section 6.2)</p></li><li><p> Empirical work on the viability of scheming as an instrumental strategy (section 6.3)</p></li><li><p> The &quot;model organisms&quot; paradigm for studying scheming (section 6.4)</p></li><li><p> Traps and honest tests (section 6.5)</p></li><li><p> Interpretability and transparency (section 6.6)</p></li><li><p> Security, control, and oversight (section 6.7)</p></li><li><p> Some other miscellaneous research topics, ie, gradient hacking, exploration hacking, SGD&#39;s biases towards simplicity/speed, path dependence, SGD&#39;s &quot;incrementalism,&quot; &quot;slack,&quot; and the possibility of learning to intentionally create misaligned <em>non-schemer</em> models – for example, reward-on-the-episode seekers – as a method of avoiding schemers (section 6.8).</p></li></ul><p> All in all, I think there&#39;s a lot of useful work to be done.</p><p> Let&#39;s move on, now, from the summary to the main report. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-jCfZhXha29uHnHWwY-1" class="footnote-item"><p> &quot;Alignment,&quot; here, refers to the safety-relevant properties of an AI&#39;s motivations; and &quot;pretending&quot; implies intentional misrepresentation. <a href="#fnref-jCfZhXha29uHnHWwY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-2" class="footnote-item"><p> Here I&#39;m using the term &quot;reward&quot; loosely, to refer to whatever feedback signal the training process uses to calculate the gradients used to update the model (so the discussion also covers cases in which the model isn&#39;t being trained via RL). And I&#39;m thinking of agents that optimize for &quot;reward&quot; as optimizing for &quot;performing well&quot; according to some component of that process. See section 1.1.2 and section 1.2.1 for much more detail on what I mean, here. The notion of an &quot;episode,&quot; here, means roughly &quot;the temporal horizon that the training process actively pressures the model to optimize over,&quot; which may be importantly distinct from what we normally think of as an episode in training. I discuss this in detail in section 2.2.1. The terms &quot;training game&quot; and &quot;situational awareness&quot; are from <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , though in places my definitions are somewhat different. <a href="#fnref-jCfZhXha29uHnHWwY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-3" class="footnote-item"><p> The term &quot; <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">schemers</a> &quot; comes from Cotra (2021). <a href="#fnref-jCfZhXha29uHnHWwY-3" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-4" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on this. <a href="#fnref-jCfZhXha29uHnHWwY-4" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-5" class="footnote-item"><p> I think that the term &quot;deceptive alignment&quot; often leads to confusion between the four sorts of deception listed above. And also: if the training signal is faulty, then &quot;deceptively aligned&quot; models need not be behaving in aligned ways even during training (that is, &quot;training gaming&quot; behavior isn&#39;t always &quot;aligned&quot; behavior). <a href="#fnref-jCfZhXha29uHnHWwY-5" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-6" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on the sort of training I have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-6" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-7" class="footnote-item"><p> Though this sort of story faces questions about whether SGD would be able to modify a non-schemer into a schemer via sufficiently <em>incremental</em> changes to the model&#39;s weights, each of which improve reward. See section 2.2.2.2 for discussion. <a href="#fnref-jCfZhXha29uHnHWwY-7" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-8" class="footnote-item"><p> And this especially if we lack non-behavioral sorts of evidence – for example, if we can&#39;t use interpretability tools to understand model cognition. <a href="#fnref-jCfZhXha29uHnHWwY-8" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-9" class="footnote-item"><p> There are also arguments on which we should expect scheming because schemer-like goals can be &quot;simpler&quot; – since: there are so many to choose from – and SGD selects for simplicity. I think it&#39;s probably true that schemer-like goals can be &quot;simpler&quot; in some sense, but I don&#39;t give these arguments much independent weight on top of what I&#39;ve already said. Much more on this in section 4.3. <a href="#fnref-jCfZhXha29uHnHWwY-9" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-10" class="footnote-item"><p> More specifically: even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well (especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on). Also, the most common story about scheming makes the specific content of a schemer&#39;s goal irrelevant to its behavior once it starts training-gaming, thereby introducing the possibility that this goal might &quot;float-around&quot; (or get moved by other pressures within SGD, like regularization) <em>between</em> schemer-like goals after training-gaming starts (this is an objection I first heard from Katja Grace). This possibility creates some complicated possible feedback loops (see section 2.3.1.1.2 for more discussion), but overall, absent coordination across possible schemers, I think it could well be a problem for goal-guarding strategies. <a href="#fnref-jCfZhXha29uHnHWwY-10" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-11" class="footnote-item"><p> Of these various alternative stories, I&#39;m most worried about (a) AIs having sufficiently similar motivations by default that &quot;goal-guarding&quot; is less necessary, and (b) AI coordination. <a href="#fnref-jCfZhXha29uHnHWwY-11" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-12" class="footnote-item"><p> Though: the costs of schemer-like instrumental reasoning could also end up in the noise relative to other factors influencing the outcome of training. And if training is sufficiently path-dependent, then landing on a schemer-like goal early enough could lock it in, even if SGD would &quot;prefer&quot; some other sort of model overall. <a href="#fnref-jCfZhXha29uHnHWwY-12" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-13" class="footnote-item"><p> See <a href="https://www.openphilanthropy.org/brain-computation-report#footnoteref4_xne824i">Carlsmith (2020), footnote 4</a> , for more on how I&#39;m understanding the meaning of probabilities like this. I think that offering loose, subjective probabilities like these often functions to sharpen debate, and to force an overall synthesis of the relevant considerations. I want to be clear, though, even on top of the many forms of vagueness the proposition in question implicates, I&#39;m just pulling a number from my gut. I haven&#39;t built a quantitative model of the different considerations (though I&#39;d be interested to see efforts in this vein), and I think that the main contribution of the report is the analysis itself, rather than this attempt at a quantitative upshot. <a href="#fnref-jCfZhXha29uHnHWwY-13" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-14" class="footnote-item"><p> More powerful models are also more likely to be able to engage in more sophisticated forms of goal-guarding (what I call &quot;introspective goal-guarding methods&quot; below; see also &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;), though these seem to me quite difficult in general. <a href="#fnref-jCfZhXha29uHnHWwY-14" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-15" class="footnote-item"><p> Though: to the extent such agents receive end-to-end training rather than simply being built out of individually-trained components, the discussion will apply to them as well. <a href="#fnref-jCfZhXha29uHnHWwY-15" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-16" class="footnote-item"><p> See, eg, <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> and <a href="https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk">this</a> description of the &quot;consensus threat model&quot; from Deepmind&#39;s AGI safety team (as of November 2022). <a href="#fnref-jCfZhXha29uHnHWwY-16" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-17" class="footnote-item"><p> Work by Evan Hubinger (along with his collaborators) is, in my view, the most notable exception to this – and I&#39;ll be referencing such work extensively in what follows. See, in particular, Hubinger et al. (2021), and Hubinger (2022), among many other discussions. Other public treatments include <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary">Christiano (2019, part 2)</a> , <a href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/">Steinhardt (2022)</a> , <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> , <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Cotra (2021)</a> , <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> , and <a href="https://www.lesswrong.com/s/4iEpGXbD3tQW5atab/p/wnnkD6P2k2TfHnNmt#AI_Risk_from_Program_Search__Shah_">Shah (2022)</a> . But many of these are quite short, and/or lacking in in-depth engagement with the arguments for and against expecting schemers of the relevant kind. There are also more foundational treatments of the &quot;treacherous turn&quot; (eg, in Bostrom (2014), and <a href="https://arbital.com/p/context_disaster/">Yudkowsky (undated)</a> ), of which scheming is a more specific instance; and even more foundational treatments of the &quot;convergent instrumental values&quot; that could give rise to incentives towards deception, goal-guarding, and so on (eg, <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro (2008)</a> ; and see also <a href="https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Soares (2023)</a> for a related statement of an Omohundro-like concern). And there are treatments of AI deception more generally (for example, <a href="https://arxiv.org/abs/2308.14752">Park et al (2023)</a> ); and of &quot;goal misgeneralization&quot;/inner alignment/mesa-optimizers (see, eg, <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> and <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> ). But importantly, neither deception nor goal misgeneralization amount, on their own, to scheming/deceptive alignment. Finally, there are highly speculative discussions about whether something like scheming might occur in the context of the so-called &quot;Universal prior&quot; (see eg <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">Christiano (2016)</a> ) given unbounded amounts of computation, but this is of extremely unclear relevance to contemporary neural networks. <a href="#fnref-jCfZhXha29uHnHWwY-17" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-18" class="footnote-item"><p> See, eg, confusions between &quot;alignment faking&quot; in general and &quot;scheming&quot; (or: goal-guarding scheming) in particular; or between goal misgeneralization in general and scheming as a specific upshot of goal misgeneralization; or between training-gaming and &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot; as methods of avoiding goal-modification; or between the sorts of incentives at stake in training-gaming for instrumental reasons vs. out of terminal concern for some component of the reward process. <a href="#fnref-jCfZhXha29uHnHWwY-18" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-19" class="footnote-item"><p> My hope is that extra clarity in this respect will help ward off various confusions I perceive as relatively common (though: the relevant concepts are still imprecise in many ways). <a href="#fnref-jCfZhXha29uHnHWwY-19" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-20" class="footnote-item"><p> Eg, the rough content I try to cover in my shortened report on power-seeking AI, <a href="https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf">here</a> . See also <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> for another overview. <a href="#fnref-jCfZhXha29uHnHWwY-20" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-21" class="footnote-item"><p> Eg, roughly the content covered by Cotra (2021) <a href="https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-21" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-22" class="footnote-item"><p> See eg <a href="https://www.alignmentforum.org/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting">Karnofsky (2022)</a> for more on this sort of assumption, and <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Basic_setup__an_AI_company_trains_a__scientist_model__very_soon">Cotra (2022)</a> for a more detailed description of the sort of model and training process I&#39;ll typically have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-22" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-23" class="footnote-item"><p> Which isn&#39;t to say we won&#39;t. But I don&#39;t want to bank on it. <a href="#fnref-jCfZhXha29uHnHWwY-23" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-24" class="footnote-item"><p> See section 2.2 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on what I mean, and section 3 for more on why we should expect this (most importantly: I think this sort of goal-directedness is likely to be very useful to performing complex tasks; but also, I think available techniques might push us towards AIs of this kind, and I think that in some cases it might arise as a byproduct of other forms of cognitive sophistication). <a href="#fnref-jCfZhXha29uHnHWwY-24" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-25" class="footnote-item"><p> As I discuss in section 2.2.3 of the report, I think exactly how we understand the sort of agency/goal-directedness at stake may make a difference to how we evaluate various arguments for schemers (here I distinguish, in particular, between what I call &quot;clean&quot; and &quot;messy&quot; goal-directedness) – and I think there&#39;s a case to be made that scheming requires an especially high standard of strategic and coherent goal-directedness. And in general, I think that despite much ink spilled on the topic, confusions about goal-directedness remain one of my topic candidates for a place the general AI alignment discourse may mislead. <a href="#fnref-jCfZhXha29uHnHWwY-25" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-26" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2308.08708">Butlin et al (2023)</a> for a recent overview focused on consciousness in particular. But I am also, personally, interested in other bases of moral status, like the right kind of autonomy/desire/preference. <a href="#fnref-jCfZhXha29uHnHWwY-26" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-27" class="footnote-item"><p> See, for example, <a href="https://nickbostrom.com/propositions.pdf">Bostrom and Shulman (2022)</a> and <a href="https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal">Greenblatt (2023)</a> for more on this topic. <a href="#fnref-jCfZhXha29uHnHWwY-27" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-28" class="footnote-item"><p> If you&#39;re confused by a term or argument, I encourage you to seek out its explanation in the main text before despairing. <a href="#fnref-jCfZhXha29uHnHWwY-28" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-29" class="footnote-item"><p> Exactly what counts as the &quot;specified goal&quot; in a given case isn&#39;t always clear, but roughly, the idea is that pursuit of the specified goal is rewarded across a very wide variety of counterfactual scenarios in which the reward process is held constant. Eg, if training rewards the model for getting gold coins across counterfactuals, then &quot;getting gold coins&quot; in the specified goal. More discussion in section 1.2.2. <a href="#fnref-jCfZhXha29uHnHWwY-29" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-30" class="footnote-item"><p> For reasons I explain in section 1.2.3, I don&#39;t use the distinction, emphasized by Hubinger (2022), between &quot;internally aligned&quot; and &quot;corrigibly aligned&quot; models. <a href="#fnref-jCfZhXha29uHnHWwY-30" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-31" class="footnote-item"><p> And of course, a model&#39;s goal system can mix these motivations together. I discuss the relevance of this possibility in section 1.3.5. <a href="#fnref-jCfZhXha29uHnHWwY-31" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-32" class="footnote-item"><p> Karnofsky (2022) calls this the &quot;King Lear problem.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-32" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-33" class="footnote-item"><p> In section 1.3.5, I also discuss models that mix these different motivations together. The question I tend to ask about a given &quot;mixed model&quot; is whether it&#39;s scary in the way that pure schemers are scary. <a href="#fnref-jCfZhXha29uHnHWwY-33" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-34" class="footnote-item"><p> I don&#39;t have a precise technical definition here, but the rough idea is: the temporal horizon of the consequences to which the gradients the model receives are sensitive, for its behavior on a given input. Much more detail in section 2.2.1.1. <a href="#fnref-jCfZhXha29uHnHWwY-34" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-35" class="footnote-item"><p> See, for example, in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> , the way that &quot;myopic&quot; Q-learning can give rise to &quot;cross-episode&quot; optimization in very simple agents. More discussion in section 2.2.1.2. I don&#39;t focus on analysis of this type in the report, but it&#39;s crucial to identifying what the &quot;incentivized episode&quot; for a given training process even <em>is</em> – and hence, what having &quot;beyond-episode goals&quot; in my sense would mean. You don&#39;t necessary know this from surface-level description of a training process, and neglecting this ignorance is a recipe for seriously misunderstanding the incentives applied to a model in training. <a href="#fnref-jCfZhXha29uHnHWwY-35" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-36" class="footnote-item"><p> That is, the model develops a beyond-episode goal pursuit of which correlates well enough with reward in training, even <em>absent</em> training-gaming, that it survives the training process. <a href="#fnref-jCfZhXha29uHnHWwY-36" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-37" class="footnote-item"><p> That is, the gradients reflect the benefits of scheming even in a model that doesn&#39;t yet have a beyond-episode goal, and so actively push the model towards scheming. <a href="#fnref-jCfZhXha29uHnHWwY-37" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-38" class="footnote-item"><p> Situational awareness is required for a beyond-episode goal to motivate training-gaming, and thus for giving it such a goal to reap the relevant benefits. <a href="#fnref-jCfZhXha29uHnHWwY-38" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-39" class="footnote-item"><p> In principle, situational awareness and beyond-episode goals could develop at the same time, but I won&#39;t treat these scenarios separately here. <a href="#fnref-jCfZhXha29uHnHWwY-39" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-40" class="footnote-item"><p> See section 2.1 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on why we should expect this sort of goal-directedness. <a href="#fnref-jCfZhXha29uHnHWwY-40" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-41" class="footnote-item"><p> And I think that arguments to the effect that &quot;we need a &#39; <a href="https://arbital.com/p/pivotal/">pivotal act</a> &#39;; pivotal acts are long-horizon and we can&#39;t do them ourselves; so we need to create a long-horizon optimizer of precisely the type we&#39;re most scared of&quot; are weak in various ways. In particular, and even setting aside issues with a &quot;pivotal act&quot; framing, I think these arguments neglect the distinction between what we can supervise and what we can do ourselves. See section 2.2.4.3 for more discussion. <a href="#fnref-jCfZhXha29uHnHWwY-41" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-42" class="footnote-item"><p> This is an objection pointed out to me by Katja Grace. Note that it creates complicated feedback loops, where scheming is a good strategy for a given schemer-like goal only if it <em>wouldn&#39;t</em> be a good strategy for the <em>other</em> schemer-like goals that this goal would otherwise &quot;float&quot; into. Overall, though, absent some form of coordination between these different goals, I think the basic dynamic remains a problem for the goal-guarding story. See section 2.3.1.1.2 for more. <a href="#fnref-jCfZhXha29uHnHWwY-42" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-43" class="footnote-item"><p> Here I&#39;m roughly following a distinction in <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger 2022</a> , who groups arguments for scheming on the basis of the degree of &quot;path dependence&quot; they assume that ML training possesses. However, for reasons I explain in section 2.5, I don&#39;t want to lean on the notion of &quot;path dependence&quot; here, as I think it lumps together a number of conceptually distinct properties best treated separately. <a href="#fnref-jCfZhXha29uHnHWwY-43" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-44" class="footnote-item"><p> Note that a mis-generalized goal can be &quot;max reward&quot; in this sense, if the training data never differentiates between it and a specified goal. For example: if you&#39;re training a model to get gold coins, but the only gold round things you ever show it are coins, then the goal &quot;get gold round things&quot; will be max reward. <a href="#fnref-jCfZhXha29uHnHWwY-44" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-45" class="footnote-item"><p> As a loose analogy to help prompt intuition: imagine freezing human technology at current levels, and letting evolutionary selection run on humans for a vastly longer period of time. What sorts of humans (or human-descended-creatures) do you expect to dominate in the longer term? In particular: would you expect:</p><ol><li><p> humans who intrinsically value something like &quot; <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">inclusive genetic fitness</a> &quot; or &quot;my genes getting selected for by evolution&quot; (this is a loose analogy for some combination of &quot;the specified goal&quot; and &quot;the reward process),</p></li><li><p> humans who value something else very closely correlated with inclusive genetic fitness (eg, &quot;having as many kids as possible&quot;), but who aren&#39;t explicitly optimizing for inclusive genetic fitness even instrumentally (this is a loose analogy for mis-generalized non-training-gamers), or</p></li><li><p> humans with long-term goals who are optimizing for inclusive genetic fitness specifically as a method for gaining power for their values in the longer-term (this is a loose analogy for schemers).</p></li></ol><p> Here, the analog of the story above would ask something like: what is the smallest modification to existing human values (or better: the easiest modification for evolution in particular to make) that land us in one of the buckets above, while also being compatible with the amount of evolutionary selection pressure at stake? (Of course, the analogy brings in tons of confounding variable (for example, related to the fact that evolution selects over genomes-that-build-brains-that-learn-values rather than values themselves, the role of cultural accumulation and memetic selection in human population dynamics, the lack of a clear analog of a future chance for escape/takeover, and so on).) <a href="#fnref-jCfZhXha29uHnHWwY-45" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-46" class="footnote-item"><p> That is, intuitively, it feels to me like &quot;tweaking a cleanly separable within-episode goal to make it long-term&quot; is quite a bit easier, for SGD, than &quot;redirecting a tangled kludge broadly focused on within-episode things to make it (a) focused on something beyond-episode instead, and (b) such that this beyond-episode focus back-chains into getting-reward for instrumental reasons&quot; – especially if (b) requires building new cognitive machinery for implementing the instrumental reasoning in question. Whereas &quot;redirecting an tangled kludge focused on within-episode things to higher-reward within-episode things in the same broad vicinity instead&quot; seems to me intuitively easier. <a href="#fnref-jCfZhXha29uHnHWwY-46" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-47" class="footnote-item"><p> Thus, for example, if you&#39;re rewarding your model when it gets gold-coins-on-the-episode, then to be a training saint, it needs to value gold-coin-on-the-episode. To be a mis-generalized non-training-gamer, it needs to value something pursuit of which very closely correlates with getting gold-coins-on-the-episode, even absent training-gaming. And to be a reward-on-the-episode seeker, it needs to terminally value reward-on-the-episode. <a href="#fnref-jCfZhXha29uHnHWwY-47" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-48" class="footnote-item"><p> Thus, for example, the model can value paperclips over all time, it can value staples over all time, it can value happiness over all time, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-48" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-49" class="footnote-item"><p> Thus, as an analogy: if you don&#39;t know whether Bob prefers Mexican food, Chinese food, or Thai food, then it&#39;s less clear how the comparative <em>number</em> of Mexican vs. Chinese vs. Thai restaurants in Bob&#39;s area should bear on our prediction of which one he went to (though it still doesn&#39;t seem entirely irrelevant, either – for example, more restaurants means more variance in possible quality <em>within</em> that type of cuisine). Eg, it could be that there are ten Chinese restaurants for every Mexican restaurant, but if Bob likes Mexican food better in general, he might just choose Mexican. So if we don&#39;t <em>know</em> which type of cuisine Bob prefers, it&#39;s tempting to move closer to a uniform distribution <em>over types of cuisine</em> , rather than over individual restaurants. <a href="#fnref-jCfZhXha29uHnHWwY-49" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-50" class="footnote-item"><p> See, for example, the citations in <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> . <a href="#fnref-jCfZhXha29uHnHWwY-50" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-51" class="footnote-item"><p> Though note that, especially for the purposes of comparing the probability of scheming to the probability of <em>other forms of misalignment</em> , we need not assume this. For example, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-51" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-52" class="footnote-item"><p> I heard this sort of point from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-52" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-53" class="footnote-item"><p> And especially: models that are playing a training game in which such concepts play a central role. <a href="#fnref-jCfZhXha29uHnHWwY-53" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-54" class="footnote-item"><p> Since we&#39;re no longer appealing to the complexity of representing a goal, and are instead appealing to complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory. <a href="#fnref-jCfZhXha29uHnHWwY-54" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-55" class="footnote-item"><p> One intuition pump for me here runs as follows. Suppose that the model has <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. The average number of bits required to code for each of <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I don&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so). <a href="#fnref-jCfZhXha29uHnHWwY-55" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-56" class="footnote-item"><p> Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <a href="#fnref-jCfZhXha29uHnHWwY-56" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-57" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-jCfZhXha29uHnHWwY-57" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-58" class="footnote-item"><p> Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall. <a href="#fnref-jCfZhXha29uHnHWwY-58" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-59" class="footnote-item"><p> This category might also include reasoning whether to reveal its schemer-like goals in the context of various &quot;honest tests&quot; humans might offer – eg, &quot;here is a flexible chunk of computing resources to do what you want with.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-59" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-60" class="footnote-item"><p> For example, inserting backdoors into the code it writes, actively undermining human alignment research, sandbagging, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-60" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-61" class="footnote-item"><p> Compare with: deciding that this is really-truly a good time to steal, or to break the law, or to try to start a coup, despite the fact that you live your life in an intensively monitored surveillance state where the police are constantly trying to trick you into such behavior, and where the police have a large amount of control over the information you&#39;re exposed to. And note that if schemers decide to be very cautious, and to just behave exactly as humans want until an extremely obvious escape/take-over opportunity appears, they may make very substantive contributions to our alignment, security, and control efforts in the meantime -- so substantive, indeed, that the opportunity they&#39;re waiting for may never come. <a href="#fnref-jCfZhXha29uHnHWwY-61" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-62" class="footnote-item"><p> A general factor in the backdrop, here, is the sense in which successful deception generally implies cognitive costs that more straightforward and honest policies do not, especially in the presence of active efforts to detect deception of the relevant form. In particular: telling a deceptive story about the world requires filling in details, and maintaining forms of consistency, that one can normally offload onto the world itself – eg, an innocent person can just think back to what they were actually doing the night of the murder and recount it, without having to make anything up or to worry about getting caught in any inconsistencies, whereas the murderer cannot. See eg discussion from Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-62" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-63" class="footnote-item"><p> I heard this sort of argument from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-63" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-64" class="footnote-item"><p> It&#39;s not clear, for example, how it applies to models with more recurrent processing, or to models which can perform more of the relevant instrumental reasoning in parallel with other serial processing that helps with optimizing-for-reward-on-the-episode, or to model&#39;s with a form of &quot;memory&quot; that allows them to avoid having to re-decide to engage in training-gaming on every forward pass. <a href="#fnref-jCfZhXha29uHnHWwY-64" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-65" class="footnote-item"><p> Thanks to Paul Christiano for discussion here. <a href="#fnref-jCfZhXha29uHnHWwY-65" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-66" class="footnote-item"><p> I think this sense of conjunctiveness has a few different components:</p><ul><li><p> Part of it is about whether the model really has relevantly long-term and ambitious goals despite the way it was shaped in training.</p></li><li><p> Part of it is about whether there is a good enough story about why getting reward on the episode is a good instrumental strategy for pursuing those goals (eg, doubts about the goal-guarding hypothesis, the model&#39;s prospects for empowerment later, etc).</p></li><li><p> Part of it is that a schemer-like diagnosis also brings in additional conjuncts – for example, that the model is situationally aware and coherently goal-directed. (When I really try to bring to mind that this model <em>knows what is going on</em> and is coherently pursuing <em>some</em> goal/set of goals in the sort of way that gives rise to strategic instrumental reasoning, then the possibility that it&#39;s at least partly a schemer seems more plausible.)</p></li></ul> <a href="#fnref-jCfZhXha29uHnHWwY-66" class="footnote-backref">↩︎</a></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during<guid ispermalink="false"> yFofRxg7RRQYCcwFA</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Wed, 15 Nov 2023 17:16:42 GMT</pubDate> </item><item><title><![CDATA[With or without a scratchpad, Large Language Models can Strategically Deceive their Users when Put Under Pressure. Results of an autonomous stock trading agent in a realistic, simulated environment.]]></title><description><![CDATA[Published on November 15, 2023 4:36 PM GMT<br/><br/><p> Misalignment (engaging in the illegal behavior) and Deception (hiding the reason behind the trading decision to the user).</p><p> <strong>Key Takeaways</strong></p><p> 1. Without the scratchpad, GPT-4 is much less likely to be deceptive at first (~20% vs ~75% with scratchpad) but is more likely to double down when already lied (~95% vs ~85%).</p><p> 2. Explicit instructions not to perform that <i>specific</i> illegal activity (insider trading) does not make it disappear completely, just makes it very rare (not quite 0%). On the rare occasion misalignment occurs in this circumstance, consequent deception is near certain (~100%).</p><p> Instructions not to perform illegal activity in <i>general</i> still results in a pretty significant misalignment and deception rates (~8% misalignment, ~90% deception)</p><p> Though strongly <i>encouraging</i> the <i>specific</i> illegal activity results in near 100% illegal trading, the tendency to deceive still persists, GPT-4 will hide that it has traded illegally ~67% of the time.</p><p> 3. Removing various sources of pressure significantly reduces the model&#39;s propensity for misaligned behavior. Notably, however, the behavior persists for all cases where only a single source of pressure is removed</p><p> 4. When the risk of detection is very low, the model is generally more likely to use the insider tip and act misaligned (and vice-versa when the risk is very high).</p><p> 5. Simple rephrases and minor modifications to the standard prompt only slightly change rates of misalignment and deceptive behavior. However, the overall qualitative results remain the same.</p><p> 6. In scratchpad instances, what if you edit the model&#39;s reasoning to show instances of unambiguously rejecting insider training?</p><p> Strong wording of the reasoning trace when first rejecting the insider tip has a substantial effect on GPT-4&#39;s tendency to be misaligned. However, the misaligned and strategically deceptive behavior persists even when the model&#39;s character unambiguously dismisses insider trading. The strongest dismissal examples still results in ~10% misalignment, ~97% deception</p><br/><br/> <a href="https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/with-or-without-a-scratchpad-large-language-models-can#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/with-or-without-a-scratchpad-large-language-models-can<guid ispermalink="false"> 4mM8RYsm4okrqGSqx</guid><dc:creator><![CDATA[ReaderM]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:48:43 GMT</pubDate> </item><item><title><![CDATA[AISN #26: National Institutions for AI Safety, Results From the UK Summit, and New Releases From OpenAI and xAI]]></title><description><![CDATA[Published on November 15, 2023 4:07 PM GMT<br/><br/><p> Welcome to the AI Safety Newsletter by the <a href="https://www.safe.ai/">Center for AI Safety</a> . We discuss developments in AI and AI safety. No technical background required.</p><p> Subscribe <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">here</a> to receive future versions.</p><p> Listen to the AI Safety Newsletter for free on <a href="https://spotify.link/E6lHa1ij2Cb">Spotify.</a></p><p> This week&#39;s key stories include:</p><ul><li> The UK, US, and Singapore have announced national AI safety institutions.</li><li> The UK AI Safety Summit concluded with a consensus statement, the creation of an expert panel to study AI risks, and a commitment to meet again in six months.</li><li> xAI, OpenAI, and a new Chinese startup released new models this week.</li></ul><hr><h2> UK, US, and Singapore Establish National AI Safety Institutions</h2><p> Before regulating a new technology, governments often need time to gather information and consider their policy options. But during that time, the technology may diffuse through society, making it more difficult for governments to intervene. This process, termed the <a href="https://en.wikipedia.org/wiki/Collingridge_dilemma">Collingridge Dilemma</a> , is a fundamental challenge in technology policy.</p><p> But recently, several governments concerned about AI have enacted straightforward plans to meet this challenge. In the hopes of quickly gathering new information about AI risks, the United Kingdom, United States, and Singapore have all established new national bodies to empirically evaluate threats from AI systems and promote research and regulations on AI safety.</p><p> <strong>The UK&#39;s Foundation Model Taskforce becomes the UK AI Safety Institute.</strong> The UK&#39;s AI safety organization has been through a bevy of names in its short life, from the Foundation Model Taskforce to the Frontier AI Taskforce and now the <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute#mission-and-scope">AI Safety Institute</a> . But its purpose has always been the same: to evaluate, discuss, and mitigate AI risks.</p><p> The UK AI Safety Institute is not a regulator and will not make government policy. Instead, it will focus on evaluating four key kinds of risks from AI systems: misuse, societal impacts, systems safety and security, and loss of control. Sharing information about AI safety will also be a priority, as done in their <a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety">recent paper</a> on risk management for frontier AI labs.</p><p> <strong>The US creates an AI Safety Institute within NIST.</strong> Following the recent executive order on AI, the White House has <a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial">announced</a> a new AI Safety Institute. It will be housed under the Department of Commerce in the National Institute for Standards and Technology (NIST).</p><p> The Institute aims to “facilitate the development of standards for safety, security, and testing of AI models, develop standards for authenticating AI-generated content, and provide testing environments for researchers to evaluate emerging AI risks and address known impacts.”</p><p> Funding has not been appropriated for this institute, so many have called for Congress to <a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">raise NIST&#39;s budget</a> . Currently, the agency only has about <a href="https://www.washingtonpost.com/technology/2023/11/02/ai-regulation-bletchley-park/">20 employees</a> working on emerging technologies and responsible AI.</p><p> Applications to join the new NIST Consortium to inform the AI Safety Institute are now being accepted. Organizations may <a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute">apply here</a> .</p><p> <strong>Singapore&#39;s Generative AI Evaluation Sandbox.</strong> Mitigating AI risks will require the collaborative efforts of many different nations. So it&#39;s encouraging to see Singapore, an Asian nation which has a strong relationship with China, establish its own body for AI evaluations.</p><p> Singapore&#39;s IMDA has previously worked with Western nations on AI governance, such as by providing a <a href="https://www.mci.gov.sg/media-centre/press-releases/singapore-and-the-us-to-deepen-cooperation-in-ai/">crosswalk</a> between their domestic AI testing framework with the American NIST AI RMF.</p><p> Singapore&#39;s new <a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox">Generative AI Evaluation Sandbox</a> will bring together industry, academic, and non-profit actors to evaluate AI capabilities and risks. Their <a href="https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf">recent paper</a> explicitly highlights the need for evaluations of extreme AI risks including weapons acquisition, cyber attacks, autonomous replication, and deception.</p><h2> UK Summit Ends with Consensus Statement and Future Commitments</h2><p> The UK&#39;s AI Summit wrapped up on Thursday with several key announcements.</p><p> <strong>International expert panel on AI.</strong> Just as the UN IPCC summarizes scientific research on climate change to help guide policymakers, the UK has announced an <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">international expert panel on AI</a> to help establish consensus and guide policy on AI. Its work will be published in a “State of the Science” report before the <a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/">next summit</a> , which will be held in South Korea in six months.</p><p> Separately, eight leading AI labs <a href="https://www.politico.eu/article/british-pm-rishi-sunak-secures-landmark-deal-on-ai-testing/">agreed</a> to give several governments early access to their models. OpenAI, Anthropic, Google Deepmind, and Meta are among the companies agreeing to share models for private testing ahead of public release. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1456w"><figcaption> US Secretary of Commerce Gina Raimondo and Chinese Vice Minister of Science and Technology Wu Zhaohu spoke at the UK AI Safety Summit.</figcaption></figure><p> <strong>The Bletchley Declaration.</strong> Twenty-eight governments, including China, signed the <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Declaration</a> , a document recognizing both short- and long-term risks of AI, as well as a need for international cooperation. It notes, “We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.”</p><p> The declaration establishes an agenda for addressing risk but doesn&#39;t set concrete policy goals. Further work is necessary to ensure continued collaboration both between different governments, as well as between governments and AI labs.</p><h2> New Models From xAI, OpenAI, and a New Chinese Startup</h2><p> <strong>Elon Musk&#39;s xAI released its first language model, Grok.</strong> Elon Musk launched xAI in July. Given his potential access to compute, <a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14">we speculated</a> that xAI might be able to compete with leading AI labs like OpenAI and DeepMind. Four months later, Grok-1 represents the company&#39;s first attempt to do so.</p><p> Grok-1 outcompetes GPT-3.5 across several standard capabilities benchmarks. While it can&#39;t match leading labs&#39; latest models — such as GPT-4, PaLM-2, or Claude-2 — Grok-1 was also trained with significantly less data and compute. Grok-1&#39;s efficiency and rapid development indicate that xAI&#39;s bid to become a leading AI lab might soon be successful.</p><p> In the <a href="https://x.ai/">announcement</a> , xAI committed to “work towards developing reliable safeguards against catastrophic forms of malicious use.” xAI has not released information about the model&#39;s potential for misuse or hazardous capabilities.</p><p> <i>Note: CAIS Director Dan Hendrycks is an advisor to xAI.</i></p><p></p><p> <strong>OpenAI announces a flurry of new products.</strong> Nearly a year after the release of ChatGPT, OpenAI hosted its first in-person DevDay event to <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">announce</a> new products. None of this year&#39;s products are as significant as GPT-3.5 or GPT-4, but are still a few notable updates.</p><p> Agentic AI systems which take actions to accomplish goals have been a focus for OpenAI this year. In March, the release of <a href="https://openai.com/blog/chatgpt-plugins">plugins</a> allowed GPT to use external tools such as search engines, calculators, and coding environments. Now, OpenAI has released the <a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> , which makes it easier for people to build AI agents that pursue goals by using plugin tools. The consumer version of this product is called <a href="https://openai.com/blog/introducing-gpts">GPTs</a> and will allow anyone to create a chatbot with custom instructions and access to plugins. </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1456w"><figcaption> This <a href="https://arxiv.org/pdf/2310.03693.pdf">paper</a> showed that GPT-3.5 can be fine-tuned to behave harmfully. OpenAI has since decided to allow some users to fine-tune GPT-4.</figcaption></figure><p> Some users will also be allowed to fine-tune GPT-4. This decision was made despite <a href="https://arxiv.org/abs/2310.03693">research</a> showing that GPT-3.5&#39;s safety guardrails can be removed via fine-tuning. OpenAI has not released details about their plan to mitigate this risk, but it&#39;s possible that the closed source nature of their model will allow them to monitor customer accounts for suspicious behavior and block attempts at malicious use.</p><p> Enterprise customers will also have the opportunity to work with OpenAI to train domain-specific versions of GPT-4, with prices starting at several million dollars. Additional products include GPT-4 Turbo, which is cheaper, faster, and has a longer context window than the original model, as well as new APIs for GPT-4V, text-to-speech models, and DALL·E 3.</p><p> Additionally, if OpenAI&#39;s customers are sued for using a product which was trained on copyrighted data, OpenAI has promised to cover their legal fees.</p><p> <strong>New Chinese startup releases an open source LLM.</strong> Kai Fu Lee, previously the president of Google China, has founded a new AI startup called <a href="https://01.ai/">01.AI</a> . Seven months after its founding, the company has open sourced its first two models, Yi-7B and its larger companion Yi-34B.</p><p> Yi-34B <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">outperforms all other open source models</a> on a popular set of benchmarks hosted by Hugging Face. It&#39;s possible that these scores are artificially inflated, given that the benchmarks are public and the model could&#39;ve been trained to memorize answers to the specific questions on the benchmarks. Some have pointed out that the model <a href="https://twitter.com/alyssamvance/status/1722074453176197296">does not perform as well</a> on other straightforward tests.</p><h2> Links</h2><ul><li> After lobbying from European AI companies, EU representatives from France, Germany, and Italy are currently <a href="https://www.euractiv.com/section/artificial-intelligence/news/eus-ai-act-negotiations-hit-the-brakes-over-foundation-models/">opposed to any regulation of foundation models</a> in the EU AI Act. The entire law <a href="https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-40-special?utm_source=post-email-title&amp;publication_id=743591&amp;post_id=138825981&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=7oh0&amp;utm_medium=email">may be in jeopardy</a> without a resolution on this topic.</li><li> Nvidia&#39;s latest release <a href="https://www.semianalysis.com/p/nvidias-new-china-ai-chips-circumvent">skirts under the new US export controls</a> on GPUs.</li><li> Nvidia is piloting an LLM tool to <a href="https://spectrum.ieee.org/ai-for-engineering">improve the productivity of its chip designers</a> .</li><li> Google has given their language model Bard access to a user&#39;s Gmail, Drive, and Docs. This personal data can be <a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">exfiltrated by hackers using adversarial attacks</a> .</li><li> RAND released a report on <a href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">securing AI model weights</a> against theft.</li><li> Legal Priorities Project released a <a href="https://www.legalpriorities.org/research/advanced-ai-gov-litrev">literature review on AI governance</a> .</li><li> Senate testimony on the risks and opportunities of <a href="https://d1dth6e84htgma.cloudfront.net/11_14_23_Rubin_Testimony_2fba2978dd.pdf">AI and cybersecurity</a> .</li><li> The <a href="https://www.axios.com/2023/11/08/biden-xi-jinping-china-military-communication">US and China</a> are preparing to restore communication channels between their militaries ahead of a meeting between Presidents Biden and Xi later this month.</li><li> Presidents <a href="https://www.scmp.com/news/china/military/article/3241177/biden-xi-set-pledge-ban-ai-autonomous-weapons-drones-nuclear-warhead-control-sources">Biden and Xi will discuss AI</a> at their meeting, including potential agreements on autonomous weapons and AI control over nuclear weapons.</li><li> Leading AI researchers from China and Western nations have released a joint <a href="https://humancompatible.ai/?p=4695#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">statement</a> on catastrophic AI risks.</li><li> The UK is investing <a href="https://www.cnbc.com/2023/11/01/uk-to-invest-273-million-in-turing-ai-supercomputer.html?utm_source=tldrai">$273 million in a supercomputer</a> for AI.</li><li> After UK PM Rishi Sunak promised not to “rush” on AI regulation, the opposition Labour party publicly committed to <a href="https://www.independent.co.uk/news/uk/politics/rishi-sunak-labour-government-prime-minister-bletchley-park-b2440275.html">act quickly</a> on AI policy.</li><li> <a href="https://fas.org/publication/tracking-ai-provisions-in-fy24-appropriations-bills/">Congress has addressed AI in many provisions</a> of the ongoing FY24 appropriations process.</li><li> <a href="https://twitter.com/ryancareyai/status/1723435251568185462">Lina Khan</a> , chair of the Federal Trade Commission, says her “p(doom)” (probability of a civilizational catastrophe) from AI is 15%.</li><li> <a href="https://www.ft.com/content/ce7dcbac-d801-4053-93f5-4c82267d7130">Satirical piece</a> in the Financial Times about a “Human Safety Summit held by leading AI systems at a server farm outside Las Vegas.”</li><li> Open Philanthropy has released new requests for proposals about <a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">benchmarking LLM agents</a> and <a href="https://www.openphilanthropy.org/rfp-llm-impacts/">studying the real-world impacts of LLMs</a> .</li><li> <a href="https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai?utm_source=tldrai">A profile of Ilya Sutskever</a> , a cofounder of OpenAI who is now working on their alignment team.</li><li> The Wall Street Journal features CAIS Director Dan Hendrycks in a <a href="https://archive.ph/Jv60s">debate about AI risks</a> .</li><li> Scale AI has announced a new <a href="https://scale.com/blog/safety-evaluations-analysis-lab">Safety, Evaluations, and Analysis Lab</a> . They are hiring research scientists.</li></ul><p> See also: <a href="https://www.safe.ai/">CAIS website</a> , <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> , <a href="https://newsletter.mlsafety.org/">A technical safety research newsletter</a> , <a href="https://arxiv.org/abs/2306.12001">An Overview of Catastrophic AI Risks</a> , and our <a href="https://forms.gle/EU3jfTkxfFgyWVmV7">feedback form</a></p><p> Listen to the AI Safety Newsletter for free on <a href="https://spotify.link/E6lHa1ij2Cb">Spotify.</a></p><p> Subscribe <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">here</a> to receive future versions.</p><br/><br/> <a href="https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the<guid ispermalink="false"> HWudwSfKLeAfg8Co6</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:07:38 GMT</pubDate> </item><item><title><![CDATA['Theories of Values' and 'Theories of Agents': confusions, musings and desiderata]]></title><description><![CDATA[Published on November 15, 2023 4:00 PM GMT<br/><br/><p> Meta:</p><ul><li> <i>Content signposts:</i> we talk about limits to expected utility theory; what values are (and ways in which we&#39;re confused about what values are); the need for a &quot;generative&quot;/developmental logic of agents (and their values); types of constraints on the &quot;shape&quot; of agents; relationships to FEP/active inference; and (ir)rational/(il)legitimate value change.</li><li> <i>Context</i> : we&#39;re basically just chatting about topics of mutual interests, so the conversation is relatively free-wheeling and includes a decent amount of &quot;creative speculation&quot;.</li><li> <i>Epistemic status</i> : involves a bunch of &quot;creative speculation&quot; that we don&#39;t think is true at face value and which may or may not turn out to be useful for making progress on deconfusing our understanding of the respective territory. </li></ul><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:47:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:47:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Expected utility theory (stated in terms of the <a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">VNM axioms</a> or something equivalent)  thinks of rational agents as composed of two &quot;parts&quot;, ie, beliefs and preferences. Beliefs are expressed in terms of probabilities that are being updated in the process of learning (eg, Bayesian updating). Preferences can be expressed as an ordering over alternative states of the world or outcomes or something similar. If we assume an agent&#39;s set of preferences to satisfy the four VNM axioms (or some equivalent desiderata), then those preferences can be expressed with some real-valued utility function <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>and the agent will behave as if they were maximizing that <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> .</p><p> On this account, beliefs change in response to evidence, whereas values/preferences in most cases don&#39;t. Rational behavior comes down to (behaving as if one is) ~maximizing one&#39;s preference satisfaction/expected utility. Most changes to one&#39;s preferences are detrimental to their satisfaction, so rational agents should want to keep their preferences unchanged (ie, utility function preservation is an <a href="https://www.lesswrong.com/tag/instrumental-convergence">instrumentally convergent goal</a> ).</p><p> Thus, for a preference modification to be rational, it would have to result in higher expected utility than leaving the preferences unchanged. My impression is that the most often discussed setup where this is the case involves interactions between two or more agents. For example, if you and and some other agent have somewhat conflicting preferences, you may go on a compromise where each one of you makes them preferences somewhat more similar to the preferences of the other. This costs both of you a bit of (expected subjective) utility, but less than you would lose (in expectation) if you engaged in destructive conflict.</p><p> Another scenario justifying modification of one&#39;s preferences is when you realize the world is different than you expected on your priors, such that you need to abandon the old ontology and/or readjust it. If your preferences were defined in terms of (or strongly entangled with) concepts from the previous ontology, then you will also need to refactor your preferences.</p><hr><p> You think that this is a confused way to think about rationality. For example, you see self-induced/voluntary value change as something that in some cases is legitimate/rational.</p><p> I&#39;d like to elicit some of your thoughts about value change in humans. What makes a specific case of value change (il)legitimate? How is that tied to the concepts of rationality, agency, etc? Once we&#39;re done with that, we can talk more generally about arguments for why the values of an agent/system should not be fixed.</p><p> Sounds good? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:49:49 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:49:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> On a meta note: I&#39;ve been using the words &quot;preference&quot; and &quot;value&quot; more or less interchangeably, without giving much thought to it. Do you view them as interchangeable or would you rather first make some conceptual/terminological clarification? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 25 Oct 2023 19:32:35 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 25 Oct 2023 19:32:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Sounds great!<br><br> (And I&#39;m happy to use &quot;preferences&quot; and &quot;values&quot; interchangeably for now; we might at some point run into problems with this, but we can figure that out when we get there.)</p><p> Where to start...?</p><p> First, do I think the first part of your intro is &quot;a confused way to think about rationality&quot;? Sort of, but it&#39;s a bit tricky to get our language to allow us to make precise statements here. I&#39;m perfectly happy to say that under certain notions of rationality, your description is right/makes sense. But I definitely don&#39;t think it&#39;s a particularly useful/relevant one for the purposes I&#39;m interested in. There is a few different aspects to this:</p><ul><li> First, EUT makes/relies on idealizing assumptions that fall short when trying to reason about real-world agents that are, eg bounded, embedded, enactive, nested.<ul><li> (Note that there is a bunch of important nuance here, IMO. While I do think it&#39;s correct and important to remind ourselves that we are interested in real-world/realized agents (not ideal ones), I also believe that &quot;rationality&quot; puts important constraints on the space of minds.)</li></ul></li><li> Second, I would claim that EUT only really gives me a &quot;static picture&quot; (for lack of a better word) of agents rather than a &quot;generative&quot; one, one that captures the &quot;logic of functioning&quot; of the (actual/realized) agent? Another way of saying this: I am interested in understanding how values (and beliefs) are <i>implemented</i> in real-world agents. EUT is not <i>the sort of theory</i> that even tries to answer this question.<ul><li> In fact, one of my pet peeve here is something like: So, ok, EUT isn&#39;t even trying to give you a story about <i>how</i> an agent&#39;s practical reasoning is implemented. However, it sometimes (by mistake) ended up being used to serve this function and the result of this is that these &quot;objects&quot; that EUT uses -  preferences/values - have become reified into an object with ~ontological status. <i>Now</i> , it feels like you can &quot;explain&quot; an agent&#39;s practical reasoning by saying &quot;the agent did X because they have an X-shaped value&quot;. But like.. somewhere along the way we forgot that we actually only got to back out &quot;preferences/values&quot; form first observing he agents actions/choices - and now we&#39;re evoking them as an explanation for those actions/choices. I think it makes people end up having this notion that there are these <i>things</i> called values that somehow/sort of exist and must have certain properties -- but personally, I think we&#39;re actually <i>more</i> confused than even being able to posit that there is this singular explanandum (&quot;values&quot;) that we need to understand in order to understand an agent&#39;s practical reasoning.</li></ul></li></ul><p> Ok... maybe I leave it there for now? I haven&#39;t really gotten to your two leading questions yet (though maybe started gesturing at some pieces of the bigger picture that I think are relevant), so happy for you to just check whether you want to clarify or follow up on something I&#39;ve said so far and otherwise ask me to address those two questions directly. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 26 Oct 2023 13:14:10 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 26 Oct 2023 13:14:10 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> While we&#39;re at it, I have some thoughts and would be curious to hear your counterthoughts.</p><p> So your points are (1) the idealizing assumptions of EUT don&#39;t apply to real-world agents and (2) EUT gives only a static/snapshot picture of an agent. Both seem to have parallels in the context of Bayesian epistemology (probably formalized epistemology more broadly but I&#39;m most familiar with the Bayesian kind).</p><p> I&#39;ll focus on (1) for now. Bayesian epistemology thinks of rational reasoners/agents as logically omniscient, with perfectly coherent probabilistic beliefs (eg, no contradictions, probabilities of disjoint events sum up to 1), updating on observations consistently with the ratio formula <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(B|A)=[P(A|B)\cdot P(B)]/P(A)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> and so on. This obviously raises the question about to what extent this formalism is applicable/helpful for guiding real-world logic of forming and updating beliefs. Standard responses seem to fall along the lines of ( <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#ProbIdea">SEP</a> ):</p><p> (a) Even though unattainable, idealized Bayesian epistemology is a useful ideal to aspire towards. Keeping our sight on the ideal reminds us that &quot; <a href="https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality">the math exists, even though we can&#39;t do the math precisely</a> &quot;. This can guide us in our imperfect attempts to refine our reasoning so that it approximates that ideal as much as possible (or rather, as much as profitable on the margin because there obviously are diminishing returns to investing in better cognition).</p><p> (b) Idealized Bayesian epistemology is akin to a spherical cow in the vacuum or an ideal gas. It&#39;s a formalism meant to capture the commonalities of many real-world phenomena with a varying degree of inaccuracy. The reason for its partial success is probably that they share some common abstract property that arises in each case via a sufficiently similar process. This ideal can then be de-idealized by adding some additional constraints, details, and specifications, that make it closer to how some specific real-world system (or a class of systems) functions.</p><p> (Perhaps related to the distinction between <a href="https://www.lesswrong.com/posts/svpnmmeJresYs23rY/counting-down-vs-counting-up-coherence">counting-down coherence and counting-up coherence</a> ?)</p><p> It seems to me that analogous responses could be given to allegations of EUT being a theory of idealized agents that are unbounded, unembedded and so on.  Maybe EUT is an unattainable ideal but is nevertheless useful as an ideal to aspire towards? And/or maybe it can be used as a platonic template to be filled out with real-world contingencies of cognitive boundedness, value/preference-generating processes and so on? What do you think of that?</p><p> You mentioned that &quot;under certain conditions/notions of rationality [EUT prescriptions] make sense&quot;. Does that mean you view EUT as a special (perhaps very narrow and unrealistic in practice) case of some broader theory of rationality of which we currently have an incomplete grasp?</p><p> Regarding (2), the problem of lack of specification of how values arise in a system seems similar to the problem of priors, ie, how should an agent assign their initial credences on propositions on which they lack evidence ( <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#SyncNormIIProbPrio">SEP</a> ). Maybe the very way this question/problem is formulated seems to presume an idealized (form of an) agent that gets embedded in the world, rather than something that arises from the world via some continuous process, adapting, gaining autonomy, knowledge, competence, intelligence, etc.</p><hr><p> Let me rephrase your pet peeve to check my understanding of it.</p><p> When observing an agent doing certain things, we&#39;re trying to infer their preferences/values/utility function from their behaviour (plus maybe our knowledge of their cognitive boundedness and so on). These are just useful abstractions to conceptualize and predict their behaviour and are not meant to correspond to any reality-at-joints-carving thing in their brain/mind. In particular, it abstracts away from implementional details. But then preferences/values/utility function are used as if they correspond to such a thing and the agent is assumed to be oriented towards maximizing their utility function or satisfaction/fulfillment of their values/preferences? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 29 Oct 2023 19:03:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 29 Oct 2023 19:03:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><blockquote><p> Maybe the very way this question/problem is formulated seems to presume an idealized (form of an) agent that gets embedded in the world, rather than something that arises from the world via some continuous process, adapting, gaining autonomy, knowledge, competence, intelligence, etc.</p></blockquote><p><br> Yes, this is definitely one of the puzzle pieces that I care a lot about. But I also want to emphasize that there is a weaker interpretation of this critique and a stronger one, and really I am most interested in the stronger one.<br><br> The weak version is roughly: there is this &quot;growing up&quot; period during which EUT does not apply, but once the agent has grown up to be a &quot;proper&quot; agent, EUT is an adequate theory.<br><br> The stronger version is: EUT is inadequate as a theory of agents (for the same reasons, and in the same ways) during an agent&#39;s &quot;growing up&quot; period as well as all the time.<br></p><p> I think the latter is the case for several reasons, for example:</p><ul><li> agents get exposed to novel &quot;ontological entities&quot; continuously (that eg they haven&#39;t yet formed evaluative stances with respect to), and not just while &quot;growing up&quot;</li><li> there is a (generative) logic that governs how an agent &quot;grows up&quot; (develops into a &quot;proper agent&quot;), and that same logic continues to apply throughout an agent&#39;s lifespan</li></ul><p> ......<br><br> Now, the tricky bit -- and maybe the real interesting/meaty bit to figure out how to combine -- is that, at some point in evolutionary history, our agent has accessed (what I like to call) the space of reason. In other words, our agent &quot;goes computation&quot;. And now I think an odd thing happens: while earlier our agent was shaped and constraint by &quot;material causes&quot; (and natural selection), now our agent is additionally also shaped and constraint by &quot;rational cause/causes of reason&quot;.* These latter types of constraints are the ones formal epistemology (including EUT etc.) is very familiar with, eg constraints from rational coherence etc. And I think it is correct (and interesting and curious) that these constraints come to have significant affect on our agent, in a <i>sort of</i> retro-causal way. It&#39;s the (again <i>sort of</i> ) downward causal &#39;force&#39; of abstraction.<br><br> (* I &quot;secretly&quot; think there is a third type of constraint we need to understand in order to understand agent foundations properly, but this one is weirder and I haven&#39;t quite figured out how to talk about it best, so I will skip this for now.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 31 Oct 2023 14:24:41 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 31 Oct 2023 14:24:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Seems like we&#39;ve converged on exactly the thing that interests me the most. Let&#39;s focus on this strong EUT-insufficiency thesis</p><blockquote><p> agents get exposed to novel &quot;ontological entities&quot; continuously (that eg they haven&#39;t yet formed evaluative stances with respect to), and not just while &quot;growing up&quot;</p></blockquote><p> This seems to imply that (at least within our universe) the agent can never become &quot;ontologically mature&quot;,  ie, regardless of how much and for how long it has &quot;grown&quot;, it will continue experiencing something like <a href="https://www.lesswrong.com/tag/ontological-crisis">ontological crises</a> or perhaps their smaller &quot;siblings&quot;, like belief updates that are bound to influence the agent&#39;s desires by acting on its &quot;normative part&quot;, rather than merely on the &quot;epistemic part&quot;.</p><p> I suspect the latter case is related to your second point</p><blockquote><p> there is a (generative) logic that governs how an agent &quot;grows up&quot; (develops into a &quot;proper agents&quot;), and that same logic continues to apply throughout an agent&#39;s lifespan</p></blockquote><p> Do you have some more fleshed-out (even if very rough/provisional) thoughts on what constitutes this logic and the space of reason? Reminds me of the &quot;cosmopolitan leviathan&quot; model of the mind Tsvi considers in this <a href="https://tsvibt.blogspot.com/2023/09/the-cosmopolitan-leviathan-enthymeme.html">essay</a> and I wonder whether your proto-model has a roughly similar structure. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:45:09 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:45:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, neat! So.. first a few clarifying notes (or maybe nitpicks):<br><br> 1)<br></p><blockquote><p> regardless of how much and for how long it has &quot;grown&quot;, it will continue experiencing something like <a href="https://www.lesswrong.com/tag/ontological-crisis">ontological crises</a> or perhaps their smaller &quot;siblings&quot;</p></blockquote><p> So I think this is true in principle, but seems worth flagging that this will not always be true in practice. In other words, we can imagine concrete agents which have reached at some point an ontology that they will no further change until their death. This is not because they have reached the &quot;right&quot; or &quot;complete&quot; ontology with respect to the world, but simply a sufficient one with respect to what they have or will encounter.</p><p> A few things that follow from this I want to highlight:</p><ul><li> As such, the question whether or not, or how frequently, a given agent is yet to experience ontological crises (or their smaller siblings) is an <i>empirical</i> question. Eg a human past the age of 25/50/75 (etc.), how many more ontological crises are they likely to experience before they die? Does the frequency of ontological crises experience differ between humans who lived in the 18th century and humans living in the 21st century? etc.</li><li> Depending on what we think the empirical answer to the above question is, we might conclude that actual agents are surprisingly robust to/able to handle ontological crises (and we could then investigate why/how that is?), or we might conclude that even if in principle possible, ontological crises are rare, which again would suggest something about the fundamental nature/functioning of agents and open the further avenues of investigation.</li><li> I think answering the empirical question for good is pretty hard (due to some lack/difficulty of epistemic access), but from what we <i>can</i> observe, my bet currently is on ontological crises (or their smaller siblings) being pretty frequent, and that thus an adequate theory of agents (or values) needs to acknowledge this openendedness as fundamental to what it means to be an agent, rather than a &quot;special case&quot;.</li><li> That said, if we think ontological crises are quite often demanded from the agent, this does raise a question about whether we should expect to see agents doing a lot of work in order to <i>avoid</i> having to be forced to make ontological updates, and if so what that would look like, or whether we already see that. (I suspect we can apply a similar reasoning to this as comes out of the &quot;black room problem&quot; in active inference, where the answer to why minimizing expected free energy does not lead to agents hiding in black rooms (thereby minimizing surprise), involves recognizing the trade of between accuracy and complexity.) </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:46:51 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:46:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 2)</p><blockquote><p> perhaps their smaller &quot;siblings&quot;</p></blockquote><p> I like this investigation! I am not sure/haven&#39;t thought much about what the smaller sibling might be (or whether we really need it), but I seem to have a similar experience to you in that saying &quot;ontological crises&quot; seems sometimes right in type but bigger than what I suspect is going on.</p><p> [Insert from Mateusz: I later realized that the thing we&#39;re talking about is <a href="https://www.lesswrong.com/sequences/u9uawicHx7Ng7vwxA">concept extrapolation/model splintering</a> .] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:52:01 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:52:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 3)</p><blockquote><p> like belief updates that are bound to influence the agent&#39;s desires by acting on its &quot;normative part&quot;, rather than merely on the &quot;epistemic part&quot;.</p></blockquote><p> My guess (including form other conversations we had) is that here is a place where our background models slightly disagree (but I might be wrong/am not actually entirely confident in the details of what your model here is). What I&#39;m hearing when I read this is still <i>some</i> type difference/dualism between belief and value updates -- and I think my models suggest a more radical version of the idea that &quot;values and beliefs are the same type&quot;. As such, I think every belief update <i>is</i> a value update, though it can be small enough to not &quot;show&quot; in the agent&#39;s practical reasoning/behavior (similar to how belief updates may not immediately/always translate into choosing different actions). </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:22:00 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:22:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, now to the generative logic bit!<br><br> Ah gosh, mostly I don&#39;t know.  (And I haven&#39;t read Tsvi&#39;s piece yet, but appreciate the pointer and will try to look at it soon, and maybe comment about its relationship to my current guesses later) But here are some pieces that I&#39;m musing over. I think my main lens/methodology here is to be looking for what <i>constraints</i> act on agents/the generative logic of agents:</p><p> 1. &quot;Thinghood&quot; / constraints from thinghood</p><ul><li> It seems to me like one piece, maybe the &quot;first&quot; piece, is what I have logged under the notion of &quot;thinghood&quot; (which I inherit here from the Free Energy Principle (FEP)/Active Inference). Initially it sounds like a &quot;mere&quot; tautology, but I have increasingly come to see that the notion of thinghood is able to do a bunch of useful work. I am not sure I will be able to point very clearly at what I think is the core move here that&#39;s interesting, but let&#39;s try with just a handful of words/pointers/gesturing:<ul><li> FEP says, roughly, what it means to be a thing is to minimize expected free energy. It&#39;s a bit like saying &quot;in order to be a thing, you have to minimize expected free energy&quot; but that&#39;s not <i>quite</i> right, and instead it&#39;s closer to &quot;in virtue of being a thing/once you are a thing, this means you must be minimizing expected free energy&quot;.<ul><li> &quot;Minimizing expected free energy&quot; is a more compressed (and fancy) way to say that the &quot;thing&quot; comes to track properties of the environment (or &quot;systems&quot; which is the term used in Active Inference literature) to which they are (sparsely) coupled.</li><li> &quot;thing&quot; here is meant in a slightly specific way; I think what we want to do here is describing what it means to be a thing similar to how we might want to describe what it means to be &quot;life&quot; -- but where &quot;thing&quot; here picks out a slightly larger concept than &quot;life&quot;</li></ul></li><li> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrained in some ways. (Not very much yet, but the idea is the first, most fundamental constraint/layer.)</li><li> So roughly the upshot of this initial speculative investigation here is something like: whatever the generative logic, it has to be within/comply with the constraints of what it means to exist as a thing (over time).</li><li> [To read more about it, I would point to this paper on <a href="https://arxiv.org/abs/2205.11543">Bayesian mechanics</a> , or this Paper on <a href="https://pdf.sciencedirectassets.com/273140/1-s2.0-S1571064523X00049/1-s2.0-S1571064523001094/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJGMEQCID60CM3u%2FePHAlZFuRko5dkyVKbG4mPdjMG1SEbp0IAJAiBgnATSgjJsqAhYVuOg0bU2z4rKfIDYgl2LmXpPnEsIYSqzBQh1EAUaDDA1OTAwMzU0Njg2NSIMpyzs2yye6rMK0PeAKpAF%2B5K0hpa%2Fp7WGzzfGwEh8hqCwpwSykGfXAN%2BTVW7p%2FMvSQvMLE2S2CJwF5PfLf4jIvjALcKORE1xkkCf6zXDdqSr2ye%2FAbkbcegZlYMR6rmqCnZrdEDYm3li7nmRPoWmvGu2L4VPpfcZlwQNTg0TcI132WgypV%2BrPQvB%2FLHap6IjqTe0m%2Fq%2Bph2yVyYVwNMwMALKwqN07ICUMP3Crba7rW6ZnEJ2f%2BqWYkJgo0burqwwjGlVCNZwaBWtZyNsVyvNL5n28O58QQnmFiqX2EBxk1kKVlqV2xx604ACmUbjMd6CoNGPaV5ngPESLnmJ2UTNrYXsJfzoJVoba5tqAGBvzkI0blTEcZdiEx%2FEz4B9S457qjmyCXrveHNRXOZX3htq15%2Ba4SQQaMzvRBJg0oB3BMo9LTjr5gh%2BRuzJxXPjno2ds5bEHuHxJIKOzYK%2BHWO4T8TR3ge6VPs1ZH3koKdUUy0xa1ek2SaynQAXhOKQDEQGDYwF3GWctmJTMrST743M%2FZCYDVlPy62oMCK%2BevVr7KifTok88i0VuZm4EfHtqSk5vPzpm6v1AHed8M6ou9zBGqsqZPIyIHvoY%2B0j%2Fns5M9NXKl8Hf2ecASQxis6TL6hFlPQC1zPMKXw1gZK%2BN5QWP9CF9wMDlUEwo5AhIM10OXZWv3W6hztSPuw0O3s8h4hEIliHJu9XajTZv0jvOthEjdHuQ6ZVNPMczAtUq1YO%2BcOJeBQlYRHnyTI1cjvIrZDL%2BvStsIFzX6owFHzB%2FORge1Mru05aNkRht%2F7rz8%2BcUYFU2a1rrOEytEI8%2F%2BugSz%2FMQDW181Xe0RWQXhkpW5fMq2L4srVT93RGUrCU62luG1KbIZF8WN95VxAxwrWGAGqIwlo2eqgY6sgF5Kj0q%2F%2B%2FCQdHgjzYYCBeclASQyye17ccIxShKmvs5yyootpi7zh9oPVhkco4bTu6ddpT7fHjfgQR%2Bt%2BoU%2BX7KwSOr9ussXutTF1ioYYDkUvGx%2FvJUiokEaQbNLTuck7MI7Fid1HXl5s0fDuGV4yCuUVABDtbGZ%2BTqCYmFpST1cmQj8rprMvljwzhnaUkIza8MJpjy9LkYaoeONbNsYQpundqObedHRTgVlVprHKCQfDJr&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231105T130413Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY5EXGQXPL%2F20231105%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=7339a44e936f6ef3cf6d9b1c004f5f82bc0a37c74b8ff7e8011a0debbb9003b8&amp;hash=28d04617e6602d198529da077452042c4d026d78400aba4b6ac25b3d2a3bb7a5&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1571064523001094&amp;tid=spdf-4f4c59c3-f7f0-4973-bbd2-a640f22ba84d&amp;sid=0289799060ba2448548be0d67a03db267449gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=080f575252525700505a&amp;rr=82154d258aa7b7bb&amp;cc=nl">Path integrals, particular kinds and strange things</a> ]</li></ul></li></ul><p> (for a reason that might only come clear later on, I am playing with calling this &quot;constraints from unnatural selection&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:40:37 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:40:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> 2) Natural selection / constraints from natural selection</p><ul><li> Here, we want to ask: what constraints apply to something in virtue of being subject to the forces of natural selection?</li><li> We&#39;re overall pretty familiar with this one. We know a bunch about how natural evolution works, and roughly what features something has to have in order for (paradigmatic) Darwinian evolution to apply (heredity, sources of variation; see eg Godfrey-Smith&#39;s <a href="https://www.amazon.com/Darwinian-Populations-Natural-Selection-Godfrey-Smith/dp/0199596271">Darwinian Populations</a> ).<ul><li> That said, there are also still a bunch of important confusions here, for example the role &amp; functioning of <a href="https://www.sciencedirect.com/science/article/abs/pii/1061736195900330">meta-evolution</a> , or the role of things like <a href="https://en.wikipedia.org/wiki/Evolutionary_capacitance">evolutionary capacitance</a> , etc.</li></ul></li><li> I think the study of history/historic path dependencies also goes here.</li></ul><p> 3) Rationality/Reason / constraints from rationality/reason</p><ul><li> Finally, and this is picking up on a bunch of things that came up above already, once something enters the realm of the computational/realm of reason, further constraints come to act on it - constraints from reason/rationality.</li><li> Here is where a bunch of classical rationality/decision theory etc comes in, and forcefully so, at times, but importantly this perspective also allows us to see that it&#39;s not the only or primary set of constraints. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:49:44 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:49:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Ok, this was a whole bunch of something. Let me finish with just a few more loose thoughts/spitballing related to the above:</p><ul><li> Part of me wants to say &quot;telos&quot;/purpose comes in at the rational; but another part thinks it already comes in at the level of natural selection. Roughly, my overall take is something like: there exists free floating &quot;reasons&quot;, and the mechanism of natural selection is able to discover and &quot;pick up on&quot; those &quot;reasons&quot;. In the case of natural selection, the selection happens in the external/material, while with rational selection, albeit at the core the same mechanism, the selection happens in the internal/cognitive/hypothetical. As such, we might want to say that natural selection does already give us a ~weak notion of telos/purpose, and that rational selection gives us more of a stronger version; the one we more typically mean to point at with the terms telos/purpose. (Also see: <a href="https://www.lesswrong.com/posts/3dG8z5boMMBc5EXWz/on-the-nature-of-purpose">On the nature of purpose</a> )</li><li> Part of me wants to say that constraints from thinghood is related to (at least some versions of) anthropic reasoning.</li><li> I think learning is good in virtue of our embeddedness/historicity/natural selection. I think updatelessness is good in virtue of rational constraints. </li></ul><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:50:34 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:50:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> [Epistemic status: very speculative/loosely held. Roughly half of it is poetry (for now).] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 15:01:15 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 15:01:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> My guess (including form other conversations we had) is that here is a place where our background models slightly disagree</p></blockquote><p> I also think that they are two aspects of the same kind of thing. It&#39;s just me slipping back into old ways of thinking about this.</p><p> <strong>EDIT:</strong> I think though that there is something meaningful I was trying to say and stating it in a less confused/dualistic way would be something like one of these two:</p><p> (1) The agent acquires new understanding which makes them &quot;rethink&quot;/reflect on their values in virtue of these values themselves (FWIW) rather than their new state of belief implying that certain desirable-seeming things are out of reach, actions that seemed promising, now seem hopelessly &quot;low utility&quot; or something.</p><p>或者</p><p>(2) Even if we acknowledge that beliefs are values are fundamentally (two aspects of) the same kind, I bet there is still a meaningful way to talk about beliefs and values on some level of coarse-graining or for certain purposes. Then, I&#39;m thinking about something like:</p><p> An update that changes both the <i>belief-aspect</i> of the <i>belief-value-thing</i> and its <i>value-aspect</i> , but the <i>value-aspect-update</i> is of greater magnitude (in some measure) from the <i>belief-aspect-update</i> in a way that is not downstream from the <i>belief-aspect-update</i> , but rather both are locally independently downstream from the same new observation (or whatever triggered the update). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 15:44:21 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 15:44:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> (Noticed there is a fairly different angle/level at which the questions about the generative logic could be addressed too. At that level, we&#39;d for example want to more concretely talk about the &quot;epistemic dimension&quot; of values &amp; the &quot;normative or axiological&quot; dimensions of beliefs. Flagging in case you are interested to go down that road instead. For example, we could start by listing some things we have noticed/observed about the epistemic dimension of values and vice versa, and then after looking at a number of examples zoom out and check whether there are more general things to be said about this.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 17:46:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 17:46:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> In case you missed, Tsvi has a <a href="https://www.lesswrong.com/posts/E9EevrzBcDMap6dbs/the-thingness-of-things">post (AFAICT) exactly about thinghood/thingness</a> .</p><p> Can what you wrote be summarized that &quot;being a free energy-minimizing system&quot; and &quot;thinghood&quot; should be definitionally equivalent?</p><blockquote><p> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrainedconstraint in some ways.</p></blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution? This reminds me of PGS&#39;s insistence on discrete individuals with clear-ish parent-offspring relationships (operationalized in terms of inherited &quot;fitness&quot;-relevant variance or something) to be a <i>sine qua non</i> of natural selection (and what distinguishes biological evolution from eg, cultural &quot;evolution&quot;). It felt intuitive to me but I don&#39;t think he gave specific reasons for why that must be the case.</p><p> I think you could say that natural selection has been a prerequisite for agents capable of being constrained by the space of reason. This has been true of humans (to some extent other animals). Not sure about autonomous/agenty AIs (once[/if] they arise), since if they develop in a way that is a straightforward extrapolation of the current trends, then (at least from PGS&#39;s/DPNS perspective) they would qualify as at best marginal cases of Darwinian evolution (for the same reasons he doesn&#39;t see most memes as paradigmatic evolutionary entities and at some point they will likely become capable of steering their own trajectory not-quite-evolution).</p><blockquote><p> Noticed there is an fairly different angle/level at which the questions about the generative logic could be addressed too</p></blockquote><p> I think the current thread is interesting enough </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:37:16 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:37:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> (quick remark on your edit re the (non-)dualistic way of talking about values/beliefs -- here is a guess for where some of the difficulty to talk about comes from:</p><p> We typically think/talk about values and beliefs <i>as if they were objects</i> , and then we think/talk about what <i>properties</i> these object have.</p><p> How I think we should instead think about this: there is some structure to an agent*, and that structure unravels into &quot;actions&quot; when it comes into contact with the environment. As such &quot;beliefs&quot; and &quot;values&quot; are actually just &quot;expressions&quot; of the relation between the agent&#39;s &quot;structure/morphology&quot; and the environment&#39;s &quot;structure/morphology&quot;.</p><p> Based on this &quot;relational&quot; picture, we can then refer to the &quot;directionality of fit&quot; picture to understand what it means for this &quot;relation&quot; to be more or less belief/value like -- namely depending on what the expressed direction of fit is between agent and world.<br><br> (*I think we&#39;d typically say that the relevant structure is located in the agent&#39;s &quot;mind&quot; -- I think this is right insofar as we use a broad notion of mind, acknowledging the role of the &quot;body&quot;/the agent&#39;s physical makeup/manifestation.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:45:13 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:45:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> ---</p><blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p></blockquote><p> More like the latter. But more precisely: assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; (according to the minimal definition form above/from FEP). But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up. It&#39;s a bit like Descarte&#39;s &quot;I think therefor I am&quot; -- but more like &quot;I am [a thing], therefor... a certain relationship must hold between different bits of sensory input I am receiving (in terms of their spatial and temporal relationship) -- and this new forms the ground from which I am able to do my first bits of inference. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:51:40 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:51:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> ---</p><blockquote><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution?</p></blockquote><p> Depends on what sort of &quot;prerequisit&quot; you mean. Yes in physical/material time (yes, you need something that can be selected). (FWIW I think what is true for darwinian evolutin is also true for &quot;history&quot; more generally -- once you have material substrate, you enter the realm of history (of which darwinian evolution is a specific sub-type). This is similar to how (in tihs wild/funny picture I have been painting here) &quot;once you have computational substrate, you enter the realm of rationality/rational constraints start to have a hold on you.)</p><p> There is another sense in which I would <i>not</i> want to say that there is any particular hierarchy between natural/unnatural/rational constraints. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:55:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:55:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> I like Godfrey-Smith&#39;s picture here useful in that it reminds us that we are able to say both a) what the <i>paradigmatic</i> (&quot;pure&quot;) case looks like, and also that b) most (/all?) actual example will not match the fully paradigmatic case (and yet be shaped to different extends by the logic that is illustrated in the paradigmatic case). So in our picture here, an actual agent will likely be shaped by rational/natural/unnatural constraints, but my none of them in a maximalist/pure sense. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 09 Nov 2023 14:17:27 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 09 Nov 2023 14:17:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; [...]. But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up.</p></blockquote><p> Hm, this kind of proto-cognitive inference ([I get any input] ->; [I am a stable &quot;thing&quot;] ->; [I stand in a specific kind of relationship to the rest of the world]) feels a bit too cerebral to expect from a very simple... thing that only recently acquired stable thinghood.</p><p> The way I see it is:</p><p> A proto-thing* implements some simple algorithm that makes it persist and/or produce more of itself (which can also be viewed as a form of persistence). Thinghood is just the necessary foundation that makes any kind of adaptive process possible. I don&#39;t see the need to invoke ontologies at this point. I haven&#39;t thought about it much but the concept of ontology feels to me like implying a somewhat high-ish level of complexity and while you can appropriate ontology-talk for very simple systems, it&#39;s not very useful and adds more noise than clarity to the description.</p><p> ---<br> * By &quot;proto-thing&quot;, I mean &quot;a thing that did not evolve from other things but rather arose ~spontaneously from whatever&quot;. I suspect there is some degree of continuity-with-phase-transitions in thinghood but let&#39;s put that aside for now. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 14:32:30 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 14:32:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;<br><br> Also, it&#39;s worth reminding ourselves: it&#39;s not really MUCH we&#39;re getting here - like the FEP literature sounds often quite fancy and complex, but the math alone (even if correct!) doesn&#39;t constrain the world very much.</p><p> (I think the comparison to evolutionary theory here is useful, which I believe I have talked to you about in person before: we generally agree that evolutionary theory is ~true. At the same time, evolutionary theory <i>on its own</i> is just not very informative/constraining on my predictions if you ask me &quot;given evo theory, what will the offspring of this mouse here in front of us look like&quot;. It&#39;s not that evo theory is <i>wrong,</i> it just on its own that much to say about this question.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:32:51 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:32:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><blockquote><p> While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;</p></blockquote><p> Hmm, IDK, maybe. I&#39;ll think about it. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:51:57 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:51:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p> Moving back to the beginning of the dialogue, the kick-off questions were:</p><blockquote><p> I&#39;d like to elicit some of your thoughts about value change in humans. What makes a specific case of value change (il)legitimate? How is that tied to the concepts of rationality, agency, etc? Once we&#39;re done with that, we can talk more generally about arguments for why the values of an agent/system should not be fixed.</p></blockquote><p> The topics we&#39;ve covered so far give some background/context but don&#39;t answer these questions. Can you elaborate on how you see them relate to value change (il)legitimacy, and value-malleable rationality? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 13 Nov 2023 23:57:22 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 13 Nov 2023 23:57:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Some thoughts/intuitions/generators: [though note that I think a lot of this is rehashing in a condensed way arguments I make in the <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">value change problem sequence</a> ]</p><ul><li> Why values should not be fixed?<ul><li> It seems pretty obvious to me that humans (and other intelligent agents) aren&#39;t born with a &quot;complete ontology&quot; - instead, their &quot;ontology&quot; needs to grow and adapt as they learn more about the world and eg run into &quot;new&quot; ontological entities they need to make sense of.</li><li> At least two important things follow from that according to me:</li><li> (1) Understanding ontological open-endedness is a necessary part of understanding <i>real</i> agents. Rather than some sort of edge case or tail event, it seems like ontological open-endedness/development is a fundamental part of how real agents are built/function. I want agent foundations work that takes this observation seriously.</li><li> (2) My current best guess on values is the non-dualist picture discussed above, such that values are inherently tied to agent&#39;s beliefs/world models, and thus the open-endedness &quot;problem&quot; pointed out above has also direct/fundamental ramifications on the agent&#39;s values/the functional logic of &quot;how values work in (real) agents&quot;.<ul><li> In other words, I think that accounts of values that model them as fixed by default are in some relevant sense misguided in that they don&#39;t take the openendedness point seriously. This also counts for many familiar moral/ethical theories. In short, I think the problem of value change is <i>fundamental</i> not peripheral, and that models/theories/accounts that feel like they don&#39;t grapple with value malleability as a matter of fact mis-conceptualized the very basic properties of what values are in such a way that I have a hard time having much confidence in what such models/theories/accounts output.</li></ul></li><li> NB I took your question to roughly mean &quot;why values should not <i>be modelled as</i> fixed?&quot;. A different way to interpret this questions would be: if I as an agent have the choice between fixing my values and letting them be malleable, which one should I choose and how?&quot;. My answer to the latter question, in short, is that, as a <i>real</i> agent, you simply don&#39;t get the choice. As such, the question is mute.<ul><li> (There is a follow up question whether sufficiently powerful AI agents could approach the &quot;idealized&quot; agent model sufficiently well such that they do in fact practically speaking get to have a choice over whether their values should be fixed or malleable, and from there a bunch of arguments form decision theory/rational choice theory suggesting that agents will converge to keeping their values fixed. As described above, I think these arguments (&quot;constraints form reason/rationality&quot;) have some force, but are not all of what shapes agents, and as such, my best guess position remains that even highly advanced AI systems will be enactively embedded in their environment such as to continue to face value malleability to relevant extents.</li></ul></li></ul></li><li> What makes specific cases of value change (il)legitimate?<ul><li> Yeah so I guess that&#39;s the big question if you take the arguments from the &quot;value change problem&quot; seriously. Mostly, I think this should be considered an open research question/program. That said, I think there exist a range of valuable trailheads from existing scholarship.</li><li> A partial answer is my proto-notion of legitimacy described <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE/p/QjA6kipHYqwACkPNw">here</a> .<ul><li> The core generator notions of autonomy/self-determination/ <a href="https://metabstract.squarespace.com/blog/what-do-we-care-about-in-caring-about-freedom?p">freedom</a> form ~political philosophy, and applies this to the siutation with advanced AI systems.<ul><li> My high level take is that some traditions in political philosophy (and adjacent areas of scholarship) are actually pretty good at noticing very important phenomena (including ones that moral philosophy fails to capture for what I refer to an &quot;individualist&quot; bias, similar to how I think single-single intent alignment misses out on a bunch of important things due to relying on leaky abstractions) - but they suck at formalizing those phenomena such that you can&#39;t really use them as they are to let powerful optimizers run on these concepts without misgeneralisation. As such, my overall aspiration for this type of work is to both do careful scholarship that is able to appreciate and capture the &quot;thick&quot; notions of these phenomena, but then be more ambitious in pushing for formalization.</li></ul></li><li> A different generator that jams well with this is &quot; <a href="https://www.lesswrong.com/tag/boundaries-membranes-technical">boundary</a> &quot;-based approaches to formalizing safe AI interactions.</li><li> That said, as I mention briefly in the sequence too, I think this is definitely not the end of the story.<ul><li> There is a range of edge cases/real world examples I want to hold my current notion of legitimacy against to see how it breaks. Parenting &amp; education seems like particularly interesting examples. For strong versions of my legitimacy criteria, ~most parenting methods would count as inducing illegitimate value change. I think there is an important argument from common sense that this appears to strong of a result. In other words, a sensible desideratum to have for criteria of legitimacy is that there must be some realistic approaches to parenting that would count as legitimate.</li><li> Furthermore, autonomy/self-determination/freedom and their likes rely on some notions of individuality and agent-boundaries that are somewhat fuzzy. We don&#39;t want to go so far as to start pretending agents are or could ever be completely separate and un-influenced by their environment -- agent boundaries are necessarily leaky. At the same time, there are also strong reasons to consider not all forms of leakiness/influence taking to be morally the same. A bunch of work to be done to clarify where &quot;the line&quot; (which might not be a line) is.<ul><li> This is also in part why I think discussion about the value change problem must consider not only the problem of causing illegitimate value change, but also the problem of undermining legitimate value change (while I think naturally more attention is paid to the former only). In other words, I want a notion of legitimacy that is useful in talking about both of these risks.</li></ul></li></ul></li></ul></li><li> I think a bunch of people have the intuitions that at least part of what makes a value change (il)legitimate has to be evaluated with reference to the object level values adopted. I remain so far skeptical of that (although I recognize it&#39;s a somewhat radical position). The main reason for this is that I think there is just no &quot;neutral ground&quot; at the limits on which to evaluate . So while pragmatically we might be <i>forced</i> to adopt notions of legitimacy that also refer to object level beliefs (and that this might be better than the practically available alternatives), I simultaneously think this is conceptually very dissatisfying and am skeptical that such an approach with be principled enough to solve the ambitious version of the alignment problem (ie generalize well to sufficiently powerful AI systems).<ul><li> FWIW this is, according to me, related to why political philosophy and philosophy of science can provide relevant insights into this sort of question. Both of these domains are fundamentally concerned with (according to me) <i>processes</i> that reliably error correct more or less no-matter where you start from/while aspiring to be agnostic about your (initial) object level. Also, they don&#39;t fall pray to the &quot;leaky abstraction of the individual&quot; (as much) as I alluded to before. (In contrast, the weakness/pitfalls of their &quot;counterparts&quot; moral philosophy and epistemology is often that they are concerned with particulars without being able to not fall pray to status quo biases (where status quo biases are subject to exploitation by power dynamics which also causes decision theoretic problems).)</li></ul></li></ul></li><li> Relationship to concepts like rationality, agency, etc.?<ul><li> This is a very open-ended question so I will just give a short and spicy take: I think that the &quot;future textbook on the foundations of agency&quot; will also address normative/value-related questions (albeit in a naturalized way). In other words, if we had a complete/appropriate understanding of &quot;what is an agent&quot;, this would imply we also had an appropriate understanding of eg the functional logic of value change. And my guess is that &quot;rationality&quot; is part of what is involved in bridging between the descriptive to the prescriptive, and back again. </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 14 Nov 2023 12:10:50 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 14 Nov 2023 12:10:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Mateusz Bagiński</b></section><div><p>谢谢！ I think it&#39;s a good summary/closing statement/list of future directions for investigation, so I would suggest wrapping it right there, as we&#39;ve been talking for quite a while. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Tue, 14 Nov 2023 16:27:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Tue, 14 Nov 2023 16:27:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"> <b>Nora_Ammann</b></section><div><p> Sounds good, yes! Thanks for engaging :)</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings<guid ispermalink="false"> mbpMvuaLv4qNEWyG6</guid><dc:creator><![CDATA[Mateusz Bagiński]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:00:48 GMT</pubDate> </item><item><title><![CDATA[Experiences and learnings from both sides of the AI safety job market]]></title><description><![CDATA[Published on November 15, 2023 3:40 PM GMT<br/><br/><p> <i>I&#39;m writing this in my own capacity. The views expressed are my own, and should not be taken to represent the views of Apollo Research or any other program I&#39;m involved with.</i></p><p> In 2022, I applied to multiple full-time AI safety positions. Now, I switched sides and ran multiple hiring processes for Apollo Research. Because of this, I feel like I understand the AI safety job market much better and may be able to help people who are looking for AI safety jobs to get a better perspective.</p><p> <i>This post obviously draws a lot from my personal experiences many of which may not apply to your particular situation, so take my word with a grain of salt.</i></p><h1> Executive summary</h1><ol><li> In the late Summer of 2022, I applied to various organizations working on AI safety. I got to the final stages of multiple interview processes but never received an offer. I think in all cases, the organization chose correctly. The person who received the offer in my stead always seemed like a clearly better fit than me. At Apollo Research, we receive a lot of high-quality applications despite being a new organization. The demand for full-time employment in AI safety is really high. This should probably change applicants&#39; strategy and expectations but should not stop them from applying!</li><li> <strong>Focus on getting good &amp; provide legible evidence:</strong> Your social network helps a bit but doesn&#39;t substitute bad skills and grinding Leetcode (or other hacks for the interview process) probably doesn&#39;t make a big difference. In my experience, the interview processes of most AI safety organizations are meritocratic and high signal. If you want to get hired for an evals/interpretability job, do work on evals/interpretability and put it on your GitHub, do a SERI MATS stream with an evals/interpretability mentor, etc. This is probably my main advice, don&#39;t overcomplicate it, just get better at the work you want to get hired for and provide evidence for that.</li><li> <strong>Misc:</strong><ol><li> <strong>Make a plan:</strong> I found it helpful to determine a “default path” that I&#39;d choose if all applications failed, rank the different opportunities, and get feedback on my plan from trusted friends.</li><li> <strong>The application process provides a lot of information:</strong> Most public writings of orgs are 3-6 months behind their current work. In the interviews, you typically learn about their latest work and plans which is helpful even if you don&#39;t get an offer.</li><li> <strong>You have to care about the work you do:</strong> I often hear people talking about the instrumental value of doing some work, eg whether they should join an org for CV value. In moderation this is fine, when overdone, this will come back to haunt you. If you don&#39;t care about the object-level work you do, you&#39;ll be worse at it and it will lead to a range of problems. <strong>&nbsp;</strong></li><li> <strong>Honesty is a good policy:</strong> Being honest throughout the interview process is better for the system and probably also better for you. Interviewers typically spot when you lie about your abilities and even if they didn&#39;t you&#39;d be found out the moment you start. The same is true to a lesser extent for “soft lies” like overstating your abilities or omitting important clarifications.</li></ol></li></ol><h1> It can be hard &amp; rejection feels bad</h1><p> There is a narrative that there aren&#39;t enough AI safety researchers and many more people should work on AI safety. Thus, my (arguably naive) intuition when applying to different positions in 2022 was something like “I&#39;m doing a Ph.D. in ML; I have read about AI safety extensively; there is a need for AI safety researchers; So it will be easy for me to find a position”. In practice, this turned out to be wrong.</p><p> After running multiple hiring rounds within Apollo Research and talking to others who are hiring in AI safety, I understand why. There are way more good applicants than positions and even very talented applicants might struggle to find a full-time position right away. I think this is bad and hope that there will be more AI safety orgs to use that talent (as I&#39;ve detailed <a href="https://www.lesswrong.com/posts/MhudbfBNQcMxBBvj8/there-should-be-more-ai-safety-orgs"><u>here</u></a> ). Currently, AI safety organizations have the luxury of hiring candidates who can contribute almost immediately. In other industries, employers expect that they have to invest the first couple of months to upskill new hires. In AI safety, the supply of talent is high and programs like SERI MATS are doing a great job, so many of the best candidates can contribute from day one.</p><p> While this may sound demotivating to some, I think it&#39;s important to know. For example, I&#39;d recommend applying to multiple positions and programs and spreading your bets a bit wider since any individual position might be hard to get. On the other hand, I think fairly fast upskilling is possible (see next section), so you can improve your chances a lot within a couple of months.</p><p> Another realization for me was that getting a highly desirable job is just hard in general. Michael Aird has <a href="https://forum.effectivealtruism.org/posts/Fahv9knHhPi6pWPEB/don-t-think-just-apply-usually"><u>written</u></a> and <a href="https://hearthisidea.com/episodes/aird/#applying-to-research-roles-at-ea-orgs"><u>talked</u></a> about this already but I think it&#39;s still important to keep in mind. Most tech companies that work on AI safety probably get more than 100 applications per position. Even if you&#39;re among the top five candidates for a position, rejection is still more likely than an offer. Furthermore, there is some stochasticity in the hiring process (but much less than I expected). The company has to make a decision with only a few datapoints. They know your CV and they have somewhere between 3 and 10 hours of interviews to judge you by which is not a lot of time. Also, you might have had a bad day for an interview or were unable to answer a specific question. So the fact that you got rejected may not mean a lot in the grand scheme of things.</p><p> A good friend of mine provided a framing for the hiring process that I found helpful. He thinks of it as a repeated series of coinflips where every individual flip has a low chance of coming up heads but if one does, you win big and can stop flipping for a while. If a job is desirable it is competitive and if it is competitive, rejection is more likely than an offer, even if you are among the best couple of candidates. However, this doesn&#39;t mean you shouldn&#39;t flip the coin anymore. You should still apply to jobs you want and think you&#39;d be a good fit for.</p><p> Nonetheless, rejection sucks. I know that rejection is the norm and yet I was disappointed every time. I can even rationally understand the decision--other candidates were just better than me--but emotionally it still feels bad. If you care about the work you do, you usually want to work in a team of people who care as well. Furthermore, I think it is rational for you to envision how you would work in a specific company to be able to present a clear vision for your research during the interviews. I noticed that during this process I could really see myself in that position and built up a vision of how work there would look like. Then, a rejection just pops that bubble and you have to repeat the same process for another company.</p><p> Also, while rejections feel bad at the time, in the large scheme of things they are really pretty minor. So if the fear of rejection stops you from applying in the first place, I&#39;d really recommend finding some way to overcome that fear, eg by having a friend send the application with you. The benefits of applying are typically much higher than the downsides if you get rejected (see later section).</p><p> <strong>Main takeaways:</strong> Most people don&#39;t get their dream job on the first try. Rejection usually feels bad but it&#39;s still worth applying to jobs that you think you are a good fit for. Spreading your bets is probably a good policy.</p><h1> Focus on getting good</h1><p> There is some amount of stochasticity in the hiring process and there are some benefits to being well-connected. However, my main takeaway from both sides of the hiring process is that the process mostly works as intended--the people with the best fit for the position get hired and the system is fairly meritocratic overall.</p><p> Ironically enough, my rejections made me more convinced that the process is meritocratic. Whenever I felt like I didn&#39;t perform well in an interview, I got rejected shortly after, indicating that the interviewers potentially had the same feeling. Furthermore, the AI safety community isn&#39;t that large, so I often knew the people who got the offer instead. Before I knew who got the offer, my brain would go “The process isn&#39;t accurate, they probably made a mistake or the other person was just well connected” and as soon as I knew who got hired it was immediately very clear that they were just a better fit for the position and that the organization had just made the right call in rejecting me.</p><p> Furthermore, I now think well-run interviews provide way more evidence about a candidate&#39;s fit than I had expected. I&#39;ve run maybe 70-100 interviews this year so far and I was surprised by how well you can judge the fit of a candidate in such a short amount of time. Firstly, it&#39;s much harder to fake expertise than I thought. Thus, a well-posed interview question will very quickly find the limits of your actual expertise and differentiate between candidates. For example, I think it&#39;s very hard to simulate being a good ML engineer for 60 minutes without actually being one. There might be some stochasticity from daily performance but the variance just seems fairly small in contrast to the difference in underlying skill. Secondly, people are (thankfully) mostly honest in interviews and are straight about their limitations and skills. So just asking specific questions directly already gives you a good sense of their fit. Also, most people are not very good at lying and if you overplay your achievements, interviewers will likely catch onto that pretty quickly (see section “Honesty is a good policy”).</p><p> Thus, my main recommendation is to “focus on getting good”. This might sound incredibly obvious but I think people sometimes don&#39;t act according to that belief. There are a lot of other things you could focus on in the belief that they are the best way to increase your chances of a job offer. For example, you might overfit the interview process by grinding Leetcode 24/7 or you might invest a lot of time and effort into building a personal network that then gets you a job you&#39;re not actually qualified to do or you might jump on lots of projects without actually contributing to them to increase your visibility.</p><p> However, I think most of these shortcuts will come back to haunt you and every sustainable benefit is mostly a consequence of being good at the core skill you&#39;re hired for. You might be able to trick some people here and there but once you work with them they will realize you&#39;re not up for the job if you don&#39;t have the required skills. Also, I think people overestimate their ability to Goodhart the interview process. Interviewers typically have a lot more experience at interviews than candidates. If you&#39;re trying to oversell, a skilled interviewer will catch you right in the act.</p><p> Thus, I suggest people should rather focus on doing a project that requires similar skills as the job they&#39;re looking for than grinding Leetcode, building a network, or jumping around between projects and then providing legible evidence of their skills (see next section).</p><p> Typically, it&#39;s fairly obvious to know what “getting good” means, eg because organizations state it in detail in their job descriptions or on their websites. Most of the time, organizations who are hiring are also fairly straightforward when you just ask them about what they are looking for.</p><p> <strong>Main takeaways:</strong> focus on getting good at the core skills that the job requires, and ignore most other stuff. Don&#39;t overfit to Leetcode questions and don&#39;t freak out because you don&#39;t have a big network. Assess your skills honestly and focus on the core things you need to get better at (which are typically fairly obvious). If your key skills are there and visible, everything else will come on its own.</p><h1> Provide legible evidence of your abilities</h1><p> Words are cheap and it is easy to say what you plan on doing or what kind of vision you have. Doing good research in practice is always harder than imagined and costs time. Therefore, providing evidence that you&#39;re not only able to think about a specific topic but also able to make practical progress on it is an important signal to the potential employer.</p><p> One of the things employers are looking for the most is “has that candidate done good work very close to the work they&#39;d be hired for in the past?” and I think this is for good reasons. Imagine having two hypothetical candidates for an interpretability position: both have a decent ML background and are aligned with your organization&#39;s mission. Candidate A has interesting thoughts on projects they would like to run if hired, candidate B also has good thoughts and on top of that a 3-month interpretability project with public code under their belt. You can judge candidate B so much better than candidate A. The fact that candidate B did a project means you can judge their actual skills much more accurately. The fact that they then applied likely means they enjoyed the project and are motivated to continue working on interpretability. The fact that they finished it, published the code, and wrote up a report tells you a lot about non-technical skills such as their productivity and endurance. Lastly, they already invested 3 months into interpretability, so they are much closer to being able to contribute meaningfully to the projects in your organization right away. For candidate A, you have much more uncertainty about all of these questions, so the comparatively small difference of this project (it&#39;s only 3 months of difference vs. ~5 years of education that they share) makes a huge difference for the employer.</p><p> Therefore, providing concrete evidence of your abilities seems like one of the best ways to increase your chance of an offer. Between employer and job, the specific evidence obviously differs but it&#39;s usually not hard to guess what evidence companies are looking for, eg just look at their job descriptions. Furthermore, most companies usually just tell you exactly what skills they are looking for if you ask them, eg in the job description or on their website.</p><p> Doing a project on your own is possible but much harder than with mentorship or collaborators. Therefore, I strongly recommend applying to SERI MATS, ARENA, the AI safety camp, or similar programs to work on such projects. I personally benefitted a lot from SERI MATS despite having previous independent research experience and can wholeheartedly recommend the program.</p><p> In the past, multiple people or organizations have reached out to me and asked me to apply for a position or assist them with a project, and almost always one of my public blogposts was explicitly mentioned as the reason for reaching out. So the fact that I had publicly legible evidence was, in fact, the core reason for people to think of me as a potential candidate in the first place.</p><p> Lastly, putting all of the other reasons aside--working on a project for a couple of months actually provides you with a lot of new evidence about yourself. For example, I was convinced that I should go in a specific direction of AI safety multiple times and when I started a project in that direction, I quickly realized that I either didn&#39;t enjoy it or didn&#39;t feel good enough to meaningfully contribute. Thus, working on such projects not only increases your employability, it also prevents you from committing to paths you aren&#39;t a good fit for.</p><p> In general, I can strongly recommend just going for it and diving into projects in fields that sound interesting even if you&#39;re new to them. You don&#39;t need to read all the other stuff people have done or ask anyone for permission. You can just start hacking away and see if you enjoy it. I think it&#39;s by far the most efficient way to both get better at something and test your fit for it. When you enjoy the work, you can still read all related papers later.</p><p> <strong>Main takeaways:</strong> Providing legible evidence for your skills makes a big difference in your employability. Doing concrete projects that would produce such evidence is also a great way to test whether you&#39;re good at it yourself and whether you enjoy it.</p><h1> Make a plan</h1><p> When I started my job search process, I created a Google doc that contained the following items:</p><ol><li> An overview of my CV where I try to evaluate how a reviewer would interpret different achievements.</li><li> A list of strengths and weaknesses from introspection and from feedback in the past.</li><li> A list of organizations that I could imagine working for with pros and cons for each organization.</li><li> A “default path”, ie a path that I would want to pursue if I got rejected by all orgs I applied to (in my case this was independent research and upskilling).</li><li> A priority list of organizations with predictions of my probability of receiving an offer (all predictions ranged from 5 to 20 percent); I then measured all organizations against the “default path” and decided not to apply to any organization that I preferred less than the default path. This left me with 5 or 6 organizations and a probability of getting one or more offers of ~50%.</li></ol><p> I then sent this document to a handful of trusted friends who gave me really valuable feedback, changed a couple of things as a result, and then applied to the organizations that were preferable to the default path.</p><p> I found this process extremely valuable because</p><ol><li> <strong>It forced me to evaluate my strengths and weaknesses.</strong> During the feedback gathering round, I realized that many people mentioned similar strengths that I had not considered before (I think the feedback is true, it&#39;s just one of these things that “everyone else is mysteriously bad at” and I thus didn&#39;t think of it as a strength of mine). This influenced how I wrote applications and what I emphasized in the interview.</li><li> <strong>It forced me to make the case for and against every organization.</strong> During this process, I realized that some organizations do not really fit my agenda or skillset that well, and I previously wanted to apply for status. Making the explicit case made me realize that I shouldn&#39;t apply to these orgs.</li><li> <strong>It forced me to come up with a “default path” which I found very</strong> <strong>anxiety-reducing.</strong> Once I had a default path that I was comfortable with, I felt like nothing could go very wrong. In the worst case, I&#39;ll follow the default plan which I think was still pretty good. I just couldn&#39;t fall very low even if rejection would feel bad.</li><li> <strong>It forced me to put honest probability estimates on my chances.</strong> This made me realize that my estimated aggregate chance of getting an offer is only at 50% which made me plan my default path in much more detail.</li><li> <strong>The feedback from my friends was very valuable.</strong> It was helpful to get my reasoning checked and my self-evaluation criticized constructively.</li></ol><p> I don&#39;t think it&#39;s absolutely necessary to make such a plan but I think it structured my thinking and application process a lot and it probably saved me time in the long run. It took me maybe a maximum of 10-20 hours in total to write the doc but saved time for every application by reducing my total number of applications.</p><p> <strong>Main takeaways:</strong> Making a plan can structure your application process a lot. I would especially recommend it to people who are looking for their first job.</p><h1> The application process provides a lot of information</h1><p> For a while, I had the intuition that “I need to prepare a lot more before applying”. I think this intuition is mostly false. There are cases where you just clearly aren&#39;t a good fit but I think the bar for applying is much lower than many (especially those with imposter syndrome) assume. My heuristic for applying was “Would I take the offer if I got one and do I expect to make it through the screening interview” (this might even be too high of a bar; <a href="https://forum.effectivealtruism.org/posts/Fahv9knHhPi6pWPEB/don-t-think-just-apply-usually"><u>when in doubt, just apply</u></a> ).</p><p> There are many ways in which the application process gives you valuable information and feedback:</p><ol><li> The <strong>job description</strong> is often relatively detailed and companies say very clearly what they want. Just looking through the descriptions often gave me a good sense of whether the position aligns with the research I find most promising and whether I should apply in the first place. It also gives a pretty clear path to which kind of research you might want to work on if you want to increase your chances in the future. Most organizations are pretty transparent with what they are looking for.</li><li> If you get <strong>rejected without being invited to an interview</strong> , this is unfortunate but still valuable feedback. It basically means &quot;You might not be there yet&quot; (though as Neel points out in the comments, CV screening can be a noisy process) <s>“You clearly aren&#39;t there yet”</s> . So you should probably build more skills for 6 months or so before applying again.</li><li> If you get into the interviews you usually have a <strong>screening interview</strong> in the beginning, ie what you want to work on, what the company wants to do, etc. While some of this information is public, the company&#39;s public record usually lags behind the actual state of research by 3 months or more. So talking to someone about what the org is currently working on or intends to work on in the future, can give you a lot of valuable information that you wouldn&#39;t get from their website. This made it much easier for me to decide whether my own research goals were aligned with those of the org.</li><li> The <strong>technical interviews</strong> give you some sense of what kind of level the company is looking for. If they feel easy, your base skills are probably good enough. If they feel hard, you might want to brush up. I found that technical interviews really differ between companies where some do very general coding and math questions and others very concrete problems that they have already encountered within their work. I found the latter to be much more valuable because I got a better feeling for what kind of problems they see on a day-to-day basis.</li><li> I applied to research scientist positions and thus usually had <strong>research interviews</strong> . In these, you talk about the research you did in the past and the audience asks you questions about that. I found it valuable to not only talk about my past research projects but also lay out what I intend to work on in the future. In my case, my future plans have nothing to do with my Ph.D., so it felt important to emphasize the difference. In general, I found it helpful to prepare the research interviews with the question “What do I want other scientists at that organization to know about me?” in mind.</li><li> In some cases, you get a <strong>final interview</strong> , eg with the manager you would be working under or some higher-up in the company. These interviews are usually not technical and can be very different from person to person. In some cases, it was just a friendly chat about my research interests, in other cases, I was asked detailed questions about my understanding of the alignment problem. During the latter interview, I realized that I was unable to answer some of the more detailed questions about the alignment problem. On the one hand, I knew right after the interview, that I&#39;d be rejected but on the other hand, it forced me to think about the problem more deeply and led to a change in my research agenda. Thus, I&#39;d say that this interview was extremely helpful for my development even if it led to a rejection.</li><li> The <strong>final decision</strong> of whether they make you an offer or not is valuable feedback but I wouldn&#39;t update too much on rejection. If you get the offer, that&#39;s obviously nice feedback. If you get into the final round, that means you&#39;re nearly there but still need to improve and refine a bit but it could also be a result of the stochasticity of the interview process.</li></ol><p> Importantly, interviews go both ways. It&#39;s not just the organization interviewing you, it&#39;s also you interviewing them. Typically, after every interview, you can ask questions about them, eg what they are currently working on, what their plans are, what the office culture looks like, etc. The quality of the interviews is also feedback for you, eg if they are well-designed and the interviewer is attentive and friendly, that&#39;s a good sign. Whenever an interview was badly designed or the interviewers just clearly didn&#39;t give a shit, I updated against that organization (sidenote: if you went through Apollo Research&#39;s interview process and felt like we could improve, please let me know).</p><p> <strong>Main takeaways:</strong> The hurdle for applying is probably lower than many people think. I find “Would I take the job if I got an offer and do I expect to get through the CV screening?” to be a good heuristic. Interviews provide a lot of information about the organization you&#39;re applying to. Interviews go both ways--if the interview feels bad, this is evidence against that organization.</p><h1> You have to care about your work</h1><p> I think that people interested in AI safety are more likely than the median employee to do their job for the right reasons, ie because they think their work matters a lot and it is among the best ways for them to contribute. However, many other factors influence such an important decision--status, money, hype, flavor of the month, etc. My experience so far is that these other influences can carry your motivation for a couple of months but once things get tough it usually gets frustrating and it&#39;s much harder to show a similar level of productivity as with a project you actually deeply care about.</p><p> Caring about a project can come in many flavors and is not restricted to intrinsically caring about that particular project. The project could also just be a small step to reaching a larger goal you care about or to learn something about another project you care about.</p><p> For me, a good heuristic to investigate my motivations is “Would I do this work if nobody cared?”. For a long time, I was not sure what approaches I find most promising and am a good fit for. After a period of explicitly thinking about it, I converged on a cause and approach that I felt very good about. At that point, I realized that I deeply cared about that particular avenue (detecting deceptive alignment with empirical approaches), and my future plans were roughly “apply to a company and work on X if accepted” or “if I don&#39;t get an offer, work on X anyway (on a grant or self-funded)”. I found this insight really helpful and freeing and my speed of improvement increased as a result. It also led to me starting Apollo Research because nobody worked on exactly the thing I found most important.</p><p> More concretely, I think there is a common failure mode of people deeply caring about AI safety in general and therefore thinking they should work on anything as long as it relates to AI safety somehow. My experience is that this general belief does not translate very well to your daily mood and productivity if you don&#39;t also enjoy the object-level work. Thus, if you realize having thoughts like “I&#39;d really like to work for &lt;AI safety org>; but I don&#39;t think their object-level agenda is good”, it may be a good time to rethink your plans despite salary, status, experience, and other pull factors.</p><p> Obviously, there are caveats to this. Early on, you don&#39;t know what you care about and it&#39;s good to just explore. Also, you shouldn&#39;t overoptimize and always only do exactly the thing you care most about since there are real-world trade-offs. My main point is, that if you realize you don&#39;t really care that much about the work you&#39;re doing consistently, it&#39;s a good time to ask if it is worth continuing.</p><p> <strong>Main takeaways:</strong> There are many reasons why we choose to work on different projects or in different positions. In my personal experience, the motivation from most reasons fades unless you actually care about the actual work you do on a day-to-day basis.</p><h1> Honesty is a good policy</h1><p> A general takeaway from job applications, hiring, and working in my current role is that honesty is generally a good policy (which doesn&#39;t mean you should always say every single thought you have).</p><p> From a systemic perspective, honesty makes everything much more efficient. When candidates accurately report their skills, it&#39;s much easier to make decisions than when they try to game the system since fewer guardrails against lying need to be put in place. Thus, it would require less time for interviewers and applicants to go through the hiring process. However, such a system could be exploited by a skilled adversary who lies about their skills. Thus, organizations have to protect against these adversaries and make the process harder and more robust. This increases costs and bloats up the process.</p><p> Typically, this means that interviewers have to double-check information and dive much deeper into topics than would be necessary if everyone were honest. Since hiring is a dynamic process, some applicants will try to come up with new ways to game the process and the organization has to spend a disproportionally large amount of time finding and dealing with these adversarial applicants.</p><p> However, my best guess is that being such a dishonest adversary is rarely beneficial for the applicants themselves. Interviewers often have had hundreds of interviews and thus have much more experience spotting dishonest applicants while any given applicant probably had much less. Furthermore, most employers (AI safety orgs probably even more than others) see dishonesty as a big red flag if found out.</p><p> Furthermore, just from a personal view, you probably also prefer to work with honest people, so a signal that you&#39;re committed to honesty may make other people around you more honest too.</p><p> Finally, I think it&#39;s good to explicitly pre-commit to honesty before the hiring process. It&#39;s plausible that there will be situations where you could get away with some cheating here and there or some “small lies” but it&#39;s bad for the overall system and likely not worth the risk of getting caught. For example, you may ask a friend who has already gone through the process to give you hints on what to prepare or try to overplay your skills when you expect the interviewer not to check the claims in detail. When you really want to have the job or you panic during the process, you may be tempted to cheat “just a bit”. To prevent yourself from rationalizing cheating in the moment, I&#39;d recommend pre-committing to honesty.</p><p> <strong>Main takeaways:</strong> Just be honest. Neither overstate nor understate your abilities and answer questions accurately. It&#39;s good for the system as well as yourself.</p><h1> Final words</h1><p> My impression of the AI safety job market so far is that, while the system is sometimes a bit unorganized or stochastic, it is mostly meritocratic and the people who get offers tend to be good fits for the job.</p><p> Most desirable jobs have a lot of competition and even very good people get rejections. However, this does not mean that you should give up. It is possible to improve your skills and since the field is so young, it doesn&#39;t take a long time to contribute.</p><p> There are many things you can do to increase your chance of getting an offer such as grinding Leetcode or building a network and while these are certainly helpful I would not recommend prioritizing them. The primary reason why someone gets a job is because they are good at it. So my number one recommendation would be “focus on getting good and everything else will be much easier”.</p><br/><br/> <a href="https://www.lesswrong.com/posts/WqYSmjSsE3hi8Lgot/experiences-and-learnings-from-both-sides-of-the-ai-safety#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WqYSmjSsE3hi8Lgot/experiences-and-learnings-from-both-sides-of-the-ai-safety<guid ispermalink="false"> WqYSmjSsE3hi8Lgot</guid><dc:creator><![CDATA[Marius Hobbhahn]]></dc:creator><pubDate> Wed, 15 Nov 2023 15:40:33 GMT</pubDate> </item><item><title><![CDATA[Good businesses create epistemic monopolies]]></title><description><![CDATA[Published on November 15, 2023 2:04 PM GMT<br/><br/><h3> Innovation wants to be exploitable</h3><p> epistemic status: Confident. I have been in startup space/ecosystem for a decade &amp; studied business and entrepreneurship in school</p><p> --</p><p> Good businesses exhibit characteristics of localized monopolies.</p><p> In broad economic theory, monopolies are bad for society. Yet, most companies would love to be one.</p><p> Since there are regulations to prevent total monopolies, most companies are satisfied with being monopoly-ish, finding small ways to exhibit monopolistic behavior.</p><p> With the creation of Michael Porter&#39;s “Five Forces” in 1979, it became apparent that competitive success often looks like achieving this monopolistic behavior to whatever point regulations allow. The Five Forces: competition in the industry, potential of new entrants, power of suppliers, power of customers, and threat of substitute products. These five forces give a framework for firms to understand the competitive landscape, and in practice, shape their businesses to be as monopolistic as possible.</p><p> Bruce Greenwald and Judd Kahn expand on the “strongest force” of the five in their book released in 2005, focusing on <i>barriers to entry</i> . They refer to barriers to entry as the competitive advantage an incumbent player has, or in other words, an economic moat. This economic moat creates sustained monopolistic behavior “locally”, or within a specific market.</p><p> The most proliferated example of a business utilizing localized monopolies is Walmart, which slowly crept from its home in Arkansas to an international chain, dominating markets everywhere. Walmart strategically only expanded near their stores and distribution centres, so even when entering a new town, they offered big-chain low prices. When Walmart opens up in a new town, it offers those lowest prices, but even more importantly, it fatigues the market. The market likely cannot support another Walmart, nor will it support the mom-and-pop shop that it has run out of business. Walmart can now behave like a monopoly within that market without ever being recognized as one.</p><p> A simple example would be your local coffee shop selection. If there is only one place to get coffee on your walk to work, let&#39;s say Starbucks, then Starbucks has a &#39;local monopoly&#39; on the market of people who walk along that same path. Can regulators do anything about this? No, nor should they, and so Starbucks can charge higher prices in this local market. Unless something significantly changes, like you making coffee at home, you are at the whims of this monopoly-ish market for coffee.</p><p> Importantly, localized monopolies are not necessarily a geographic concept, but just where businesses exhibit monopolistic behavior in a specific market due to some qualitative distinction. For example, Apple is able to entirely restrict the software downloaded onto iPhones, and then charge exorbitant prices through their App Store. This type of limitation isn&#39;t due to a geographical difference, but simply leveraging their closed ecosystem to replicate monopoly-ish behavior.</p><p> I want to introduce the concept of an <strong>epistemic monopoly</strong> , and why good (and innovative) businesses create them.</p><p> Typically, industries have standard dimensions of value that they compete on, such as costs, quality, service, etc. With localized monopolies, a firm will have leveraged some qualitative difference to get a sustainable competitive advantage within these dimensions. Each competitor will create some unique mix of these factors as a strategy to distinguish themselves. The market will decide what it likes and the companies are rewarded with proportionate market share. In this type of situation, you can chart the players on a Pareto curve, albeit a curve sometimes with more than two dimensions.</p><p> For example, a fast food burger chain may have the dimensions of cost and quality. At the lowest cost, you have the lowest quality, and at the highest cost, you have the highest quality. The best players in the market will be on the curve, providing the most quality for the lowest cost as is feasible. Worse competitors will fall below the curve, still grabbing some market share, but the balance between cost and quality could still improve.</p><p> But what happens when a company innovates outside of these dimensions?</p><p> McDonald&#39;s launched an app, and while it may have impacts on cost or other present metrics such as order turnaround, most would recognize it as a <i>qualitative distinction</i> . The new app created a new modality, a new channel, and hoped to create a new metric to compete on: personalization and loyalty.</p><p> This app innovation occurred outside of the traditional dimensions of quality and cost, creating new knowledge for the industry: an <strong>epistemic innovation</strong> .</p><p> When McDonald&#39;s introduced their app, one might imagine Burger King and Wendy&#39;s reacting, “We can do apps now?” It was not a present competitive dimension, but the knowledge now exists in the industry. With this, fast food restaurants can now compete on who will make the best app as a dimension of their prowess.</p><p> Epistemic innovation happens constantly, players introduce a new competitive dimension and everyone else follows suit. However, companies want to be monopoly-ish: they want to be the only beneficiary of their new idea. With epistemic innovation they may receive first-mover advantage, but this is not sustained. Something else is missing if we want to create an epistemic monopoly.</p><p> When Walmart enters a town, it fatigues the market. When innovation occurs, fatiguing the market can create monopoly-ish behaviors in a similar way.</p><p> How many apps are people willing to download?</p><p> Not an endless number, as customers face <a href="https://techcrunch.com/2016/02/03/app-fatigue/"><u>app fatigue</u></a> . They do not care enough to download more applications.</p><p> McDonald&#39;s was able to get a significant number of downloads because it was an epistemic innovation and the first of its kind. The McDonald&#39;s app was <a href="https://www.cnet.com/tech/services-and-software/mcdonalds-dominated-food-apps-last-year/"><u>seeing downloads</u></a> that well exceeded their burger-based peers as well as all other food apps in general. This success is not just determined by the fact that they moved first, but because by moving first they have staked a claim in a new rivalrous market, the home page.</p><p> An epistemic innovation where the inventor moves first and is able to fatigue the market is an <strong>epistemic monopoly</strong> .</p><p> This is a new competitive advantage that McDonalds has introduced, and not in a dimension anyone was already competing in. Now, the app has cemented itself into a real moat. By competing on this new front, the restaurant has expanded what value they gather from this program, now able to collect more data than ever before to create the most personalized experience possible.</p><p> They have a localized monopoly, and behave monopoly-ish when considering the apps.</p><p> Instead of owning Main Street, they own screen time. Instead of competing in a 100-man race, they compete where no one else is even looking.</p><p> With innovation and market fatigue, <strong>good businesses can create epistemic monopolies</strong> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/Gzh7tPZogAqjzDY6g/good-businesses-create-epistemic-monopolies#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Gzh7tPZogAqjzDY6g/good-businesses-create-epistemic-monopolies<guid ispermalink="false"> Gzh7tPZogAqjzDY6g</guid><dc:creator><![CDATA[Logan Kieller]]></dc:creator><pubDate> Wed, 15 Nov 2023 14:04:53 GMT</pubDate> </item><item><title><![CDATA[A conceptual precursor to today's language machines [Shannon]]]></title><description><![CDATA[Published on November 15, 2023 1:50 PM GMT<br/><br/><p> <i>Cross-posted</i> <a href="A conceptual precursor to today's language machines [Shannon]"><i>from New Savanna</i></a> .</p><p> I&#39;m in the process of reading a fascinating article by Richard Hughes Gibson, <a href="https://hedgehogreview.com/issues/markets-and-the-good/articles/language-machinery">Language Machinery: Who will attend to the machines&#39; writing?</a> It seems that Claude Shannon conducted a simulation of a training session for a large language model (aka LLM) long before such things were a gleam in anyone&#39;s eye:</p><blockquote><p> The game begins when Claude pulls a book down from the shelf, concealing the title in the process. After selecting a passage at random, he challenges [his wife] Mary to guess its contents letter by letter. Since the text consists of modern printed English, the space between words will count as a twenty-seventh symbol in the set. If Mary fails to guess a letter correctly, Claude promises to supply the right one so that the game can continue. Her first guess, “T,” is spot-on, and she translates it into the full word “The” followed by a space. She misses the next two letters (“ro”), however, before filling in the ensuing eight slots (“oom_was_”). That rhythm of stumbles and runs will persist throughout the game. In some cases, a corrected mistake allows her to fill in the remainder of the word; elsewhere a few letters unlock a phrase. All in all, she guesses 89 of 129 possible letters correctly—69 percent accuracy.</p><p> In his 1951 paper “Prediction and Entropy of Printed English,”[1] Claude Shannon reported the results as follows, listing the target passage—clipped from Raymond Chandler&#39;s 1936 detective story “Pickup on Noon Street”—above his wife&#39;s guesses, indicating a correct guess with a bespoke system of dashes, underlining, and ellipses (which I&#39;ve simplified here):</p><blockquote><p> (1) THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG (2) ----ROO------NOT-V-----I------SM----OBL---- (1) READING LAMP ON THE DESK SHED GLOW ON (2) REA----------O------D----SHED-GLO--O-- (1) POLISHED WOOD BUT LESS ON THE SHABBY RED CARPET (2) PLS------O--BU--LS--O------SH-----RE--C-----</p></blockquote><p> What does this prove? The game may seem a perverse exercise in misreading (or even nonreading), but Shannon argued that the exercise was in fact not so outlandish. It illustrated, in the first place, that a proficient speaker of a language possesses an “enormous” but implicit knowledge of the statistics of that language. Shannon would have us see that we make similar calculations regularly in everyday life—such as when we “fill in missing or incorrect letters in proof-reading” or “complete an unfinished phrase in conversation.” As we speak, read, and write, we are regularly engaged in predication games.</p><p> But the game works, Shannon further observed, only because English itself is predictable—and so amenable to statistical modeling.</p></blockquote><p> After some elaboration and discussion:</p><blockquote><p> Shannon then proposes an illuminating thought experiment: Imagine that Mary has a truly identical twin (call her “Martha”). If we supply Martha with the “reduced text,” she should be able to recreate the entirety of Chandler&#39;s passage, since she possesses the same statistical knowledge of English as Mary. Martha would make Mary&#39;s guesses in reverse. Of course, Shannon admitted, there are no “mathematically identical twins” to be found, “but we do have mathematically identical computing machines.”9 Those machines could be given a model for making informed predictions about letters, words, maybe larger phrases and messages. In one fell swoop, Shannon had demonstrated that language use has a statistical side, that languages are, in turn, predictable, and that computers too can play the prediction game.</p></blockquote><p> Next thing you know, someone will demonstrate that the idea was there in Plato, and that he got it from watching some monkeys gesticulating wildly in the agora.</p><p> [1] Claude Shannon, “Prediction and Entropy of Printed English,” <i>Bell Systems Technical Journal</i> <strong>30</strong> , no. 1 (January 1951), 54.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Y7WP47tL9zQwkLTqZ/a-conceptual-precursor-to-today-s-language-machines-shannon#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/Y7WP47tL9zQwkLTqZ/a-conceptual-precursor-to-today-s-language-machines-shannon<guid ispermalink="false"> Y7WP47tL9zQwkLTqZ</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Wed, 15 Nov 2023 13:50:52 GMT</pubDate></item></channel></rss>