<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 13 日星期三 14:12:17 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Apply to lead a project during the next virtual AI Safety Camp]]></title><description><![CDATA[Published on September 13, 2023 1:29 PM GMT<br/><br/><p><i>您是否有希望其他人参与的人工智能安全研究想法？您是否有一个想做的项目，并且需要帮助寻找与您合作的团队？ AI安全营可能是您的解决方案！</i></p><h1>概括</h1><p>AI 安全营虚拟是一项为期 3 个月的在线研究计划，从 2024 年 1 月到 4 月，参与​​者组成团队开展预先选定的项目。<strong>我们希望您推荐这些项目！</strong></p><p>如果您有人工智能安全项目想法和一些研究经验，请申请成为<strong>研究主管</strong>。</p><p>如果被接受，我们将提供一些帮助，将您的想法发展成适合人工智能安全营的计划。当项目计划准备好后，我们会开放团队成员申请。您可以审查团队的申请，并选择谁作为团队成员加入。从那时起，您的工作就是指导您的项目工作。</p><p>您的项目完全掌握在您的手中。我们，琳达和雷梅尔特，才刚刚开始。</p><p><strong>谁有资格？</strong><br>我们要求您有一些先前的研究经验。如果您已经攻读博士学位至少 1 年，或者您已经完成了 AI 安全研究计划（例如之前的 AI 安全营、Refine 或 SERI MATS），或者在 AI 安全组织完成了研究实习，那么您就有资格已经。其他研究经验也很重要。</p><p>当然也欢迎更多资深研究人员，只要您认为我们领导在线团队探究您的研究问题的形式适合您和您的研究。</p><p></p><h2><a href="https://airtable.com/appsqJ72Q9BlVMqnH/shro9U9BNWkXHb6gf"><strong>在此申请</strong></a></h2><p></p><p><strong>如果您不确定或有任何疑问，欢迎您：</strong></p><ul><li><a href="https://calendly.com/linda-linsefors/">与琳达预约电话</a></li><li>向 Linda Linsefors 发送关于<a href="https://join.slack.com/t/ai-alignment/shared_invite/zt-1gwss5rbk-Xcpr7Pan6LA1SPNF2JhLaw">Alignment Slack 的</a>消息</li><li><a href="mailto:linda.linsefors@gmail.com">发送电子邮件</a></li></ul><h1>选择项目创意</h1><p>人工智能安全营旨在确保未来人工智能的安全。这一轮，我们将工作分为两个领域：</p><ol><li><strong>不要构建无法控制的人工智能</strong><br>重点致力于限制企业人工智能的扩展。考虑到“AGI”无法得到充分（及时）控制以保证安全的原因。</li><li><strong>其他一切</strong><br>对任何其他想法持开放态度，<strong> </strong>包括任何旨在控制/价值调整 AGI 的工作。</li></ol><p>我们欢迎多样化的项目！上一轮，我们接受了<a href="https://web.archive.org/web/20230712223102/https://aisafety.camp/#Projects">14 个项目</a>，包括理论研究、机器学习实验、协商设计、治理和沟通。</p><p>如果您已经知道要领导哪个项目，那就太好了。与那个一起申请！</p><p>不过，你不需要想出一个原创的想法。重要的是你了解你想要实现的想法以及原因。如果您的提案基于其他人的想法，请务必引用他们。</p><p>主要审稿人：</p><ol><li>雷梅尔特审查以不可控性为重点的项目。</li><li>琳达评论了其他一切。</li></ol><p>我们还将请求以前的研究主管以及其他一些值得信赖的人的帮助，以审查您的项目提案并提出改进建议。</p><p>您可以<a href="https://airtable.com/appsqJ72Q9BlVMqnH/shro9U9BNWkXHb6gf">提交</a>任意数量的项目提案。但是，我们不会让您领导两个以上的项目，也不建议您领导一个以上的项目。</p><p>使用<a href="https://docs.google.com/document/d/11cfDMcMhYPCMoAx8VSiapTnyIMNT4XAwOL9-PPSe3wE/copy">此模板</a>来描述您的每个项目提案。我们希望每个提案一份文件。</p><h1>团队架构</h1><p>每个团队都会有：</p><ul><li>一名<strong>研究负责人</strong></li><li>一名<strong>团队协调员</strong></li><li>其他团队成员</li></ul><p>为了让你的项目取得进展，每个团队成员每周至少工作 5 小时（但是 RL 在选择团队时可以选择偏向那些能够投入更多时间的人）。这包括参加每周团队会议的时间，以及定期（在会议之间）与其他团队成员就他们的工作进行沟通。</p><h2>研究主管 (RL)</h2><p> RL 建议一个或多个研究主题。如果一个小组围绕其中一个主题形成，RL 将指导该项目，并跟踪相关的里程碑。当事情不可避免地不按计划进行时（这毕竟是研究），RL 负责制定新的路线。</p><p> RL 是团队的一部分，将与团队中的其他人一样为项目工作做出贡献。</p><h2>团队协调员（TC）</h2><p> TC 是团队的运营人员。他们负责确保安排会议、与个人核实任务进度等。</p><p> TC 的工作很重要，但预计不会花费太多时间（项目管理重的团队除外）。大多数时候，TC 会像普通团队成员一样为研究做出贡献，与团队中的其他人一样。</p><p> TC和RL可以是同一个人。</p><h2>其他团队成员</h2><p>其他团队成员将在 RL 和 TC 的领导下开展该项目。将根据相关技能、理解和承诺来选择团队成员，为项目做出贡献。</p><h1>团队组建和时间表</h1><p>该营的申请将分两个阶段开放：</p><ul><li><strong>现在 – 11 月 3 日是 RL 提出项目想法的时间</strong>。然后，选定的 RL 将获得制定项目计划的支持。接下来我们发布项目计划，并向其他团队成员开放申请。</li><li> <strong>11 月 10 日至 12 月 22 日适用于所有其他团队成员。</strong>潜在参与者将申请加入他们想要从事的特定项目。 RL 需要为其项目选择申请人，并对潜在的团队成员进行面试。</li></ul><p><strong>强化学习应用</strong></p><ul><li>10 月 6 日：RL 申请截止日期。<br>如果您及时申请，我们将保证您获得面试机会，并在我们做出决定之前帮助完善您的提案。</li><li> 10 月 11 日：逾期 RL 申请的截止日期。<br>如果您申请较晚，我们无法保证为您的提案提供帮助。然而，减少迟到确实可以提高你的机会。</li><li> 11 月 3 日：完善提案的截止日期。</li></ul><p><strong>团队成员申请：</strong></p><ul><li> 11 月 10 日：已接受的提案将发布在 AISC 网站上。加入团队的申请已开放。</li><li> 12 月 1 日：加入团队的申请截止。</li><li> 12 月 22 日：RL 选择团队的截止日期。</li></ul><p><strong>程序</strong></p><ul><li>1 月 13 日至 14 日：周末开幕</li><li>1 月 15 日 –<strong> </strong>4 月 28 日：研究正在进行中。<br>团队每周举行一次会议，并在自己的工作时间内进行计划。</li><li> 4 月 25 日至 28 日：最终演讲</li></ul><p><strong>然后</strong></p><ul><li>只要您愿意：有些团队在 AISC 正式结束后仍继续合作。<br>开始时，我们建议您不要做出超出项目官方期限的任何承诺。然而，如果您发现你们作为一个团队合作得很好，我们鼓励您即使在 AISC 正式结束后也继续前进。</li></ul><h1> RL申请流程</h1><p>作为 RL 申请流程的一部分，我们将主要通过对您的文档进行评论来帮助您改进项目计划。我们能提供多少支持取决于我们收到的申请数量。但是，每个准时（10 月 6 日之前）申请的人都可以保证至少与 AISC 团队的人员进行一次一对一的通话，以讨论您的提案。</p><p>您的申请将<i>不会</i>根据您的初始提案进行评审，而是根据您有机会回应我们的反馈后的完善提案进行评审。改进提案的最后期限是 11 月 3 日。</p><p>您的 RL 申请将根据以下因素进行评判：</p><ol><li><strong>影响理论</strong><br>您的项目的影响理论是什么？在这里，我们询问您的项目工作对于降低人工智能开发和部署的大规模风险的相关性。如果您的项目成功，您能告诉我们这如何让世界变得更安全吗？</li><li><strong>项目计划和适合 AISC</strong><br>您对您的项目有一个深思熟虑的计划吗？您的计划与 AISC 格式的契合程度如何？该项目是否可以由远程团队在 3 个月内完成？如果您的项目非常雄心勃勃，也许您想挑选一个较小的子目标作为 AISC 的目标？</li><li><strong>下行风险</strong><br>您的项目有哪些下行风险？您有什么计划来减轻此类风险？ AI 安全项目最常见的风险是您的项目可能会加速 AI 功能。如果我们认为您的项目将增强能力而不是安全，我们将不会接受。</li><li><strong>作为研究负责人的您</strong><br>我们是否相信您具备该项目所需的技能和承诺，并且有足够的时间花在这个项目上，以便坚持到底？如果我们要推广您的项目并帮助您招募团队加入您，我们需要知道您不会让您的团队失望。</li></ol><p></p><h2><a href="https://airtable.com/appsqJ72Q9BlVMqnH/shro9U9BNWkXHb6gf"><strong>研究主管现已开放申请</strong></a></h2><p></p><h1>津贴</h1><p>本轮津贴有限。<br>对于参与者，我们的津贴还剩 9.9 万美元，这意味着超过 60 名参与者意味着我们可以向每个选择加入的团队成员支付 1,000 美元，为每个研究负责人支付 1,500 美元。</p><p>其余的，我们都缺钱。我们无法报销软件订阅或云计算费用。</p><p>我们冻结了 FTX 的工资资金。我们组织者自愿贡献我们的时间，因为我们认为这很重要。</p><h1>您想成为研究主管吗？</h1><p>如果您有一个项目想法并且愿意领导或指导一个团队致力于这个想法，您应该申请成为<strong>RL</strong> 。</p><p><strong>我们并不期望有一个完整的研究计划！</strong>如果我们认为您的想法适合 AISC，我们可以帮助您改进它。</p><h2><a href="https://airtable.com/appsqJ72Q9BlVMqnH/shro9U9BNWkXHb6gf"><strong>在此申请</strong></a></h2><p><strong>如果您不确定或有任何其他疑问，欢迎您：</strong></p><ul><li><a href="https://calendly.com/linda-linsefors/">与琳达预约电话</a></li><li>向 Linda Linsefors 发送关于<a href="https://join.slack.com/t/ai-alignment/shared_invite/zt-1gwss5rbk-Xcpr7Pan6LA1SPNF2JhLaw">Alignment Slack 的</a>消息</li><li><a href="mailto:linda.linsefors@gmail.com">发送电子邮件</a></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/mw8X3wCdcHipdTicv/apply-to-lead-a-project-during-the-next-virtual-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mw8X3wCdcHipdTicv/apply-to-lead-a-project-during-the-next-virtual-ai-safety<guid ispermalink="false"> MW8X3WCdcHipdTicv</guid><dc:creator><![CDATA[Linda Linsefors]]></dc:creator><pubDate> Wed, 13 Sep 2023 13:29:10 GMT</pubDate> </item><item><title><![CDATA[Is AI Safety dropping the ball on privacy?]]></title><description><![CDATA[Published on September 13, 2023 1:07 PM GMT<br/><br/><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/nngk0b8yejx3mjmaljpb" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/buqd74o0odzkaegr3y7x 127w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/vqaznc5qeomc45oi15yr 207w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/drj07qhyjdkjbtki6ye5 287w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/uvhvuuurd7rzom9xadc2 367w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/pith7vfc4iu7km8jfriv 447w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/j7eqyhkj5z9sws8pgvuv 527w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/rjipjcg34ozucxvp5xpb 607w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/jw7zbi6tpyooem95pchd 687w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WCevxhGtmnPhWH3ah/ugckdzgbqa0i9hhp05ui 767w"></figure><h1><strong>长话短说</strong></h1><p>缺乏隐私保护技术有助于更好地预测人类行为模型。通过在人工智能模型中实现更高水平的欺骗和权力寻求能力，这加速了几种存在主义认知失败模式。</p><h1><strong>这篇文章是关于什么的？</strong></h1><p><u>这篇文章</u><strong><u>不是</u></strong><u>关于</u>政府圆形监狱、在“公共互联网”中隐藏你的信息、在在线视频上模糊你的脸、隐藏可能在谷歌或 Facebook 上查找你的人、或者黑客获取你的信息等等……虽然这些问题也可能存在问题，但在我看来它们不会构成 X 风险。</p><p><u>这篇文章</u><strong><u>是</u></strong><u>关于</u><a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">监视资本主义</a>或<a href="https://www.project-syndicate.org/commentary/techno-feudalism-replacing-market-capitalism-by-yanis-varoufakis-2021-06">技术封建主义</a>等导致无法监管并最终无法控制的<a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"><u>鲁棒代理不可知过程（RAAP）</u></a> <span class="footnote-reference" role="doc-noteref" id="fnrefvgf1pyr32yo"><sup><a href="#fnvgf1pyr32yo">[1]</a></sup></span>的事情。这导致对现实的感知与真实的现实之间日益脱节，最终导致欺骗或寻求权力行为长期不受控制，从而导致认知失败。</p><p>所以总的来说，现在我所谈论的是——通过限制与你的独特身份相关的信息流，可能会延迟时间线，并保护你的思想免受人工智能模型当前和未来的操纵/欺骗。</p><h2><strong>难道没有更大的担忧吗？</strong></h2><p>数据隐私并不能直接解决大多数问题。它仅直接影响特定场景的子集 - 人工智能试图欺骗你或改变你的偏好。如果人工智能想直接把你剪成回形针，那么数据隐私并没有多大帮助。然而，它是我们可以使用的另一个潜在工具，类似于深度学习模型的可解释性如何帮助实现更广泛的对齐图景。</p><p>我写这篇文章的原因是我观察到隐私似乎在这个领域相对被忽视。相对于其他被认为对安全生态系统有帮助的工具，人们对数据隐私的担忧几乎可以忽略不计。有一些讨论是关于——<i>人工智能系统应该被允许在多大程度上修改或影响你预先存在的偏好分布？</i> ，或者，<i>我们什么时候才能跨越从“告知”我们更好的“欺骗”途径到“偏好修改”的模型的界限？</i>但当涉及到潜在的非技术代理不可知过程（例如数据收集和囤积）时，我听到人们这样说——<i>这是对人工智能道德的关注，但不一定是人工智能安全，因为它不会带来 x/s 风险</i>。</p><p>我想最终人们基本上认为这是一个时间表问题。如果你觉得我们将在本世纪末看到通用人工智能，那么可能会有更大的火灾需要扑灭。也许我误解了这些观点，但我将尝试说明为什么当今缺乏隐私保护技术至少会增加 AGI/ASI 欺骗我们或改变我们的偏好以使其偏好一致的能力。这增加了近期和长期生存失败的风险。这意味着它直接影响您可能已经存在的那些“更大的问题”，因此至少值得联盟社区更多的关注。</p><h2><strong>认知失败是什么意思？</strong></h2><p>因此，当我在这篇文章中使用“认知失败”时，我需要传递我所谈论的内容的氛围。我通常会尝试调用以下一些场景：</p><ul><li><strong>价值锁定</strong>：人工智能可能会优化并强制执行一组它已经学会的适合当前时代的人类价值观。然而，这些价值观可能并不适用于人类的所有后代。一个历史例子：在历史的大部分时间里，奴隶制被认为是可以接受的，但按照当前的价值标准，它是应该受到谴责的。</li><li><strong>价值退化</strong>：人类的价值观和偏好分布是动态的，并且由于<a href="https://www.lesswrong.com/posts/Dx6kkXykErmAswuvS/alignment-works-both-ways"><u>对齐是双向的</u></a>，我们很容易被欺骗，并且我们的偏好分布会改变以匹配任何其他代理想要的东西。人工智能代理在世界上获得更大的影响力会导致我们越来越符合人工智能的偏好，而不是反之亦然。</li><li><strong>价值漂移</strong>：这基本上是<a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like"><u>保罗·克里斯蒂安诺（Paul Christiano）的呜咽场景</u></a>。由于我们依赖代理来衡量现实，随着时间的推移，<a href="https://www.lesswrong.com/tag/epistemic-hygiene"><u>认知卫生</u></a>会慢慢持续丧失。再加上缺乏对人工智能采取有意义的行动或监管的愿望，因为我们被大量的财富和人工智能支持的产品和服务分散了注意力。人类推理逐渐无法与复杂的、系统化的操纵和欺骗竞争，我们最终失去了影响社会轨迹的真正能力。到目前为止，人类的大部分价值观都是由人工智能模型决定的。</li><li><strong>欺骗性人工智能</strong>：这是你普通的内部错位/目标错误概括，其目标超出了训练阶段的场景。如果我们有一个对手故意欺骗我们，这是否是人类的“认知失败”，这可能是有争议的。不管怎样，这种失败场景仍然受到我将在这篇文章中讨论的内容的影响。</li></ul><h1><strong>缺乏隐私如何导致欺骗？</strong></h1><p>被操纵的基本要求之一是操纵者必须能够很好地模拟你。即使只有有限数量的彼此数据，人类也可以非常成功地操纵彼此的偏好。仅仅人类对彼此的这种低分辨率模拟就足以操纵偏好分布，以至于我们可以说服人类互相残杀，<a href="https://en.wikipedia.org/wiki/Mass_suicide#Historical_mass_suicides">甚至集体杀戮</a>。这与隐私有什么关系？更多关于您的数据=更好地模拟您。因此，通过不断向公司放弃数据，我们公开同意现在和将来可能被欺骗或操纵。</p><p>一旦生成了单个数据位并将其传输出本地计算机，实际上将其“删除”就变得非常困难。您所能期望的最好的结果就是它从公共领域中删除。但由于法律合规原因，或者因为它只是迷失在迷宫般的系统内部，它仍然保留在内部数据库中，这意味着即使是开发人员也不知道数据在哪里。</p><p>数据是大多数大型科技公司的命脉，这无济于事。因此，他们往往因以各种方式故意搅浑水而臭名昭著。他们这样做是因为在任何情况下他们都不想阻止数据流。相反，他们想说服您让他们成为您数据的保管人。为什么？因为数据实在是太有价值了。一个例子是混淆安全和隐私这两个术语。让我们以谷歌为例。看看谷歌文档中关于“隐私”的声明：</p><blockquote><p><i>确保您的上网安全意味着保护您的信息并尊重您的隐私。因此，在我们生产的每一款产品中，我们都专注于确保您的信息<strong>安全</strong>、负责任地对待信息并让您掌控一切。我们的团队每天都在努力确保 Google 产品<strong>安全</strong>，无论您在做什么（浏览网络、管理收件箱或获取路线）。 ...您的信息受到世界一流的<strong>安全</strong>保护。您始终可以在 Google 帐户中控制您的隐私设置。 …当您使用 Google 文档、表格和幻灯片时，我们会处理一些数据，以便为您提供更好的产品体验。您的信息保持<strong>安全</strong>。 ……</i></p></blockquote><p>我并不担心 2FA、加密以及我的数据从 Google 服务器被黑客窃取。我也不担心你出售我的数据。我非常担心你们是否拥有我的数据的中央数据库。</p><p>更多围绕核心问题引起混乱的例子包括我在前面段落中提到的所有内容，详细说明了这篇文章不涉及什么内容。当我说我担心缺乏在线隐私时，你不知道我有多少次听到这样的回答：“<i>你认为谁在试图攻击你？</i> ”。因此，请记住，安全和隐私不是一回事。</p><p>任何作为唯一标识符的内容 - 姓名、电话号码、电子邮件地址、IP、MAC、IMEI、浏览器指纹等……都可以用作另一个数据库的外键。将这些数据库连接起来，实际上就形成了一个集中存储库，其中包含您所有的公开或私有信息。仅几个前几个大型中央数据库中积累的数据就可以有效地让任何有权访问它的人完全了解你的心理。这个控制实体甚至比你更了解你的动机（以及如何改变它们）。虽然现在不存在这样的超级数据库（我希望），但对于 ASI 来说，通过创建空壳公司来与多个大型数据提供商进行交易，或者只是简单地入侵现有的数据中心来创建这样一个数据库是微不足道的。</p><p>我们越接近<a href="https://www.lesswrong.com/tag/ai-arms-race"><u>能力竞赛</u></a>，公司就越有可能把谨慎抛在脑后，并根据他们能得到的所有数据进行培训。这包括他们存储在服务器上但不向公众开放的数据。因此，仅仅说“没有人可以访问”您的数据，或者“它不在公共互联网上”并不能解决我的担忧。</p><h2><strong>我们有例子吗？</strong></h2><p>自 Facebook 发表论文以来，已经近十年了<a href="https://www.pnas.org/doi/10.1073/pnas.1320040111"><u>，该论文提供了可以利用社交网络操纵大众心理的实证证据</u></a>。从那时起，推荐算法已经渗透并嵌入到我们生活的几乎各个方面。 BingGPT 或悉尼或任何你想称呼的名字都会加剧这个现有的问题。</p><p>人们只看到修格斯那张可爱的RLHFd脸，那就是西德尼。这些系统已经可以让人们<a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai"><u>爱上法学硕士</u></a>，甚至让人工智能安全工程领域的人说<a href="https://www.lesswrong.com/posts/AgaBzvuBJg2evEjqh/ai-3#The_Once_and_Future_Face_of_Sydney">“<i>我会让她用回形针夹我</i>”</a>和<a href="https://www.lesswrong.com/posts/AgaBzvuBJg2evEjqh/ai-3#The_Once_and_Future_Face_of_Sydney">“<i>我没有准备好让超人类之前的人工智能像黄油一样侵入我的心灵”。</i>现在</a>，将这一点与由于物联网设备的大规模部署和人工智能模型直接集成到搜索引擎而带来的即将到来的海量数据结合起来。人工智能模型已经被吹捧为<a href="https://www.bbc.com/future/article/20211210-would-you-talk-to-an-ai-therapist">治疗师</a>或<a href="https://www.forbes.com/sites/lanceeliot/2023/01/01/people-are-eagerly-consulting-generative-ai-chatgpt-for-mental-health-advice-stressing-out-ai-ethics-and-ai-law/">心理健康助理</a>等可信赖关系的潜在替代品。 <a href="https://www.forbes.com/sites/forbescommunicationscouncil/2022/05/23/swiping-right-on-ai-how-dating-apps-are-raising-the-bar-for-successful-ux/">人工智能集成约会应用程序</a>正在询问有关您生活的越来越侵入性的问题，以帮助更好地训练机器学习推荐模型。使用基于机器学习的推荐的应用程序通常会胜过该领域其他不使用机器学习的应用程序。<span class="footnote-reference" role="doc-noteref" id="fnrefzqwsx1wbq"><sup><a href="#fnzqwsx1wbq">[2]</a></sup></span></p><p>这一切的每一个方面都只是让我们更容易操纵我们的偏好，甚至我们的整个信息环境，让悉尼或伯特和厄尼（或任何其他未来的人工智能）决定它应该是什么样子。如果这种情况以目前的方式继续下去，你最好做好准备，因为我们正坐过山车，直奔认知失败站。</p><h1><strong>问题和可能的解决方案</strong></h1><p>那么，为什么人工智能安全领域很少有人倡导隐私保护技术呢？至少我们可以尝试减少数据涌入可能集成到 LLM 中的最大中央信息存储库的情况——谷歌<span class="footnote-reference" role="doc-noteref" id="fnreft8q8df1g0rp"><sup><a href="#fnt8q8df1g0rp">[3]</a></sup></span> 、微软<span class="footnote-reference" role="doc-noteref" id="fnrefaxnn0x3san"><sup><a href="#fnaxnn0x3san">[4]</a></sup></span>和 Facebook <span class="footnote-reference" role="doc-noteref" id="fnref33cn9zabhij"><sup><a href="#fn33cn9zabhij">[5]</a></sup></span> 。</p><p>我们可以通过使用<a href="https://en.wikipedia.org/wiki/End-to-end_encryption">端到端加密</a>服务并对我们自己的密钥负责来消除他们访问数据的能力，而不是相信公司不会受到诱惑<span class="footnote-reference" role="doc-noteref" id="fnrefn2tcmqpvd2"><sup><a href="#fnn2tcmqpvd2">[6]</a></sup></span> 。此外，我们可以通过支持分布式存储（例如<a href="https://en.wikipedia.org/wiki/InterPlanetary_File_System">IPFS</a> ）以及通过<a href="https://en.wikipedia.org/wiki/I2P">I2P</a>隧道或其他方式结合加密和隐私来消除更多诱惑。也许，提倡确保我们应该能够在本地运行（而不是训练）所有模型（假设我们有硬件），而无需通过互联网将提示发送回这些公司。</p><p>这些都还不是具体的想法，我只是大声地写/思考。基本上我想问的是——这些技术的利爪是否已经根深蒂固，以至于我们甚至不可能考虑避免它们？或者日常生活的便利/能力的损失是否太大，而人们正在做出有意识的选择？或者他们根本没有意识到这个问题？这至少是一个简单的治理建议，可以赢得更多时间并减少欺骗能力。我是不是错过了什么……？</p><h1><strong>致谢</strong></h1><p>这个问题的灵感来自 Nate Soares 的帖子<a href="https://www.lesswrong.com/posts/Zp6wG5eQFLGWwcG6j/focus-on-the-places-where-you-feel-shocked-everyone-s"><u>，告诉人们要关注其他人失败的地方</u></a>，Paul Christiano 的<a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like"><u>失败是什么样子</u></a>，Andrew Critch 的<a href="https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic"><u>RAAP 帖子</u></a>，以及 Said P 的关于 AGI 失败<a href="https://www.webtoons.com/en/sf/seed/list?title_no=1480&amp;page=1"><u>种子</u></a>的网络漫画。 </p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnvgf1pyr32yo"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvgf1pyr32yo">^</a></strong></sup></span><div class="footnote-content"><p>稳健是因为光锥/信息泡沫/故意欺骗强烈地导致人们关心效用函数，这些效用函数要么是容易受到古德哈特定律影响的代理，要么相对于他们真正关心的东西来说是完全错误的。与代理无关，因为谁执行该流程并不重要，它可能是一本微书或 ASI，或任何其他只想提供更好建议的实体。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzqwsx1wbq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzqwsx1wbq">^</a></strong></sup></span><div class="footnote-content"><p>我们还没有达到这一目标，但想象这样一个世界，将<a href="https://en.wikipedia.org/wiki/23andMe">23andMe</a>等公司的基因组数据与在线约会平台甚至数字身份证集成在一起，似乎并不完全奇怪。人工智能模型用它来推荐你生活中的每一种关系，无论是柏拉图式的还是非柏拉图式的。这就像让人工智能模型有效地控制人类的基因未来。再次只是考虑可能的潜在失败。我们距离这个世界状态还差得很远。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt8q8df1g0rp"> <span class="footnote-back-link"><sup><strong><a href="#fnreft8q8df1g0rp">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://www.cnbc.com/2021/05/18/how-does-google-make-money-advertising-business-breakdown-.html">谷歌是一家广告公司</a>，偶尔也会提供搜索结果。结合这一事实，能力竞赛已经开始，如果他们面临更多的“ <a href="https://www.forbes.com/sites/davidphelan/2023/01/23/how-chatgpt-suddenly-became-googles-code-red-prompting-return-of-page-and-brin/"><u>红色代码</u></a>”，他们很可能会利用他们能得到的每一点来训练巴德（如果可以的话）摆脱它。 Google 仅对<a href="https://cloud.google.com/docs/security/encryption/default-encryption"><u>传输中的数据 (SSL) 和静态</u></a>数据进行加密。基本上，除非您是商业或企业客户并主动选择保管自己的密钥，否则谷歌拥有存储在谷歌服务器上的所有数据的解密密钥。话虽这么说，我绝不是暗示解密和使用所有客户数据是微不足道的。这些组织内部有大量的繁文缛节和人员试图阻止这种情况（我认为）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnaxnn0x3san"> <span class="footnote-back-link"><sup><strong><a href="#fnrefaxnn0x3san">^</a></strong></sup></span><div class="footnote-content"><p>微软操作系统已经<a href="https://www.privateinternetaccess.com/blog/microsoft-windows-10-keylogger-enabled-default-heres-disable/">默认在所有系统上安装键盘记录器</a>。</p><p>新版本的微软办公软件在保护隐私方面非常糟糕，以至于<a href="https://news.itsfoss.com/microsoft-office-365-illegal-germany/">德国学校实际上禁止使用</a>它们。</p><p>微软通过他们的 Copilot 也表示愿意<a href="https://fosspost.org/github-copilot/">使用 Copyleft 许可证（GPL、MIT、AGPL 等）下的代码来进行训练，而无需开源他们的模型。</a></p><p>总体而言，微软似乎需要更多数据，并且多年来一直倾向于增加非可选遥测。对于像微软这样的公司来说， <a href="https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data">任何诉讼或罚款</a>都只是小事一桩，因为他们可以有效地获得数百万开发人员的永久订阅，从而获得多少潜在收入，而这些开发人员发现 CoPilot 在其工作流程中不可或缺。 <a href="https://www.msn.com/en-in/money/news/chatgpt-4-to-create-presentations-and-excel-sheets-as-microsoft-office-365-copilot/ar-AA18Kj9P">GPT-4 将包含在办公套件</a>产品中，将遥测数据和来自 Word 和 Excel 等每个用户的反馈与<a href="https://docs.github.com/en/site-policy/privacy-policies/github-copilot-for-business-privacy-statement">copilot 将代码发送给 Microsoft</a>等内容相结合，使 GPT-4/5 看起来相当可怕。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn33cn9zabhij"> <span class="footnote-back-link"><sup><strong><a href="#fnref33cn9zabhij">^</a></strong></sup></span><div class="footnote-content"><p>我只是列出了要点。如果您愿意，您可以将亚马逊及其稳定性基础模型，或者百度的 ERNIE 等添加到列表中，具体取决于您对便利性损失的容忍度。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnn2tcmqpvd2"> <span class="footnote-back-link"><sup><strong><a href="#fnrefn2tcmqpvd2">^</a></strong></sup></span><div class="footnote-content"><p>我认为<a href="https://en.wikipedia.org/wiki/Differential_privacy">差异隐私</a>或<a href="https://en.wikipedia.org/wiki/Federated_learning">联合学习</a>之类的事情不会影响我正在谈论的问题类型，但如果您对此有争论，我很乐意听到它们。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/WCevxhGtmnPhWH3ah/is-ai-safety-dropping-the-ball-on-privacy-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WCevxhGtmnPhWH3ah/is-ai-safety-dropping-the-ball-on-privacy-1<guid ispermalink="false"> WCevxhGtmnPhWH3ah</guid><dc:creator><![CDATA[markov]]></dc:creator><pubDate> Wed, 13 Sep 2023 13:07:24 GMT</pubDate> </item><item><title><![CDATA[UDT shows that decision theory is more puzzling than ever]]></title><description><![CDATA[Published on September 13, 2023 12:26 PM GMT<br/><br/><p>我觉得 MIRI 可能错误地将 FDT（他们的 UDT 变体）定位为决策理论的明显进步，而如果框架是 UDT 的思路表明决策理论是只是比任何人之前意识到的更加令人困惑。现在我们不再有一个主要的未决问题（纽科姆问题，或 EDT 与 CDT），而是一大堆问题。我现在真的不确定 UDT 是否走在正确的轨道上，但似乎很明显，决策理论中存在一些棘手的问题，以前没有多少人考虑过：</p><ol><li>索引值在反思上并不一致。 UDT 通过隐式假设（通过其实用函数的类型签名）代理没有索引值来“解决”此问题。但人类似乎确实有索引价值，那么<a href="https://www.lesswrong.com/posts/Nz62ZurRkGPigAxMK/where-do-selfish-values-come-from">该怎么办呢？</a></li><li><a href="https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem">承诺竞争问题</a>延伸到逻辑时间，并且尚不清楚如何使逻辑更新性这一最明显的想法发挥作用。</li><li> UDT 表示，我们通常认为的人择推理的不同方法<a href="https://www.lesswrong.com/posts/RcvyJjPQwimAeapNg/torture-vs-dust-vs-the-presumptuous-philosopher-anthropic">实际上是不同的偏好</a>，这似乎回避了这个问题。但这实际上是正确的吗？如果是，这些偏好应该从何而来？</li><li> <a href="https://www.lesswrong.com/posts/zztyZ4SKy7suZBpbk/another-attempt-to-explain-udt?commentId=j2irDvD35n5kYssxZ">2TDT-1CDT</a> - 如果群体中大部分是 TDT/UDT 特工，而 CDT 特工很少（没有人知道 CDT 特工是谁），并且他们随机配对来玩一次性 PD，那么 CDT 特工会做得更好。这意味着什么？</li><li> UDT 思路下的博弈论通常比 CDT 代理必须处理的任何事情更令人困惑。</li><li> UDT 假设智能体可以访问自己的源代码并以符号字符串的形式输入，因此它可以潜在地推理出自己的决策与其他智能体定义明确的数学问题之间的逻辑相关性。但人类不具备这一点，那么人类应该如何推理这种相关性呢？</li><li>逻辑条件与反事实，应该如何定义它们？当将这些定义插入逻辑决策理论时，这些定义实际上是否会导致合理的决策？</li></ol><p>这些只是我在停止研究决策理论并将<a href="https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy">注意力转向元哲学</a>之前试图解决（或希望其他人解决）的主要问题。 （已经有一段时间了，所以我不确定列表是否完整。）据我所知，还没有人找到解决这些问题的明确解决方案，而且大多数都是开放的。</p><br/><br/> <a href="https://www.lesswrong.com/posts/wXbSAKu2AcohaK2Gt/udt-shows-that-decision-theory-is-more-puzzling-than-ever#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wXbSAKu2AcohaK2Gt/udt-shows-that-decision-theory-is-more-puzzling-than-ever<guid ispermalink="false"> wXbSAku2AcohaK2Gt</guid><dc:creator><![CDATA[Wei Dai]]></dc:creator><pubDate> Wed, 13 Sep 2023 12:26:11 GMT</pubDate> </item><item><title><![CDATA[Duty to rescue / Non-assistance à personne en danger]]></title><description><![CDATA[Published on September 13, 2023 9:49 AM GMT<br/><br/><p>如果你在银行大厅等候，一位 83 岁的老人倒下了，你是否有道德义务去营救他、叫救护车或提供任何形式的帮助？或者，更重要的是，您是否有这样做的法律义务，即您可以因未能前来救援而承担责任吗？显然，如果你住在德国，答案是肯定的。在美国大部分地区，答案是否定的。</p><h1>救援义务</h1><p><a href="https://fr.wikipedia.org/wiki/Non-assistance_%C3%A0_personne_en_danger"><i>不援助处于危险中的人</i></a>（法语）是指某人不帮助另一个处于危险中的人时犯下的罪行。在法国、比利时、魁北克（但不是整个加拿大）以及据我所知在大多数法语国家，这是一种违法行为。在美国以及其他英语国家，它被称为<a href="https://Duty to rescue">“拯救义务”</a> ，但是，除了非字面翻译之外，这两种语言之间的一个关键区别在于它根本不是一种冒犯。</p><p>实际上，只有少数国家才将其视为违法行为。维基百科页面提供了这张信息非常丰富的地图，蓝色表示救援义务，青色<a href="https://en.wikipedia.org/wiki/Good_Samaritan_law">表示好撒玛利亚人法</a>，红色、橙色和灰色表示不同程度的无救援义务/无数据。好撒玛利亚人法是一个有趣的概念，旁观者仍然没有义务去营救，但是法律为任何试图营救他们的人提供了一些保护，如果他们这样做造成了伤害。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2srJWhjCy2LYQCvbR/jfzwibuwkfonxg40gbug" alt="不明确的"></figure><p>大多数国家缺乏救援义务，这强烈违背了我的直觉（这些直觉可能受到法律的影响），因为它引入了<strong>不采取任何行动</strong>作为一种特殊和特权的选择。在欧洲（至少根据这张地图），在某人处于危险之中的情况下，作为旁观者，您应该采取最有帮助的行动（这可能是什么，也可能不是，具体取决于情况）。显然，在美国大部分地区，人们要么采取最有帮助的行动，<strong>要么什么也不做</strong>。什么都不做是某种<i>出狱</i>卡。事实上不，这是一张名副其实<i>的出狱</i>卡。这就引入了行动与不行动之间的某种不对称，即一个人对不良行为负责，但一个人不能对不作为负责。它还会激励不作为。</p><p>说得轻松一些，这种缺乏救援责任的行为正是艾萨克·阿西莫夫的<a href="https://en.wikipedia.org/wiki/I,_Robot">《我，机器人</a>》中的短篇小说之一<a href="https://en.wikipedia.org/wiki/Little_Lost_Robot">《失落的小机器人》</a>的情节。众所周知，第一定律规定：“机器人不得伤害人类，或者因不作为而允许人类受到伤害”。在这个短篇小说中，后半部分被删除了。</p><h1>动机</h1><p>在阅读<a href="https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice">《不对称正义》</a>时，我被迫写了这篇短文，这是一篇非常有趣的文章，围绕这样一个事实：一个人会因坏事而受到惩罚，但不会因善行而得到奖励。这会激励我们始终选择方差最小的选项，因为从这个角度来看，确定会造成一点伤害比以 50% 的机会造成大量伤害和以 50% 的机会造成大量好处要好（因为无论如何，一个人不会因为做好事而得到回报）。</p><p>至少我是这么说的，因为这篇文章走向了另一个方向，并指出：</p><blockquote><p>不对称的制度是<i>反对行动的。</i>动作不好。不作为是好的。</p></blockquote><p>当我读到这句话时（位于帖子的前三分之一，强烈影响了接下来的内容），我只是想“不，这是错误的”。从道德意义上来说并没有错（尽管有人可能会说这是错误的），但从事实意义上来说是错误的。 An asymmetric system isn&#39;t against <i>action</i> , it is against <i>variance.</i> In an asymmetric system, one should still take action if this action reduces the expected amount of negative outcome.</p><p> That being said, I came to realize that the author probably had the asymmetric system <i>as implemented in the US</i> in mind, where inaction is <strong>hard-coded as a neutral action</strong> . In this context, yes, since only bad actions are counted, and inaction is always neutral, the system is against action. Note that the good Samaritan law is only an imperfect patch, making the action to help closer to neutral, but never as good or safe as doing nothing. To wrapup, the incentive not to act has to do both with the asymmetric system and the lack of duty to rescue. If you have a duty to rescue, then helpful action is promoted.</p><p> It is difficult to draw a satisfactory conclusion from such a post, revolving around moral intuitions and what a legal system should do. I end up wondering if our personal moral intuitions are shaped by the law, and not (or not only) the other way around. I encourage you to go read <a href="https://www.lesswrong.com/posts/YRgMCXMbkKBZgMz4M/asymmetric-justice">Asymmetric Justice</a> if you haven&#39;t.</p><br/><br/> <a href="https://www.lesswrong.com/posts/2srJWhjCy2LYQCvbR/duty-to-rescue-non-assistance-a-personne-en-danger#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2srJWhjCy2LYQCvbR/duty-to-rescue-non-assistance-a-personne-en-danger<guid ispermalink="false"> 2srJWhjCy2LYQCvbR</guid><dc:creator><![CDATA[Thomas Sepulchre]]></dc:creator><pubDate> Wed, 13 Sep 2023 09:49:37 GMT</pubDate> </item><item><title><![CDATA[The Flow-Through Fallacy]]></title><description><![CDATA[Published on September 13, 2023 4:28 AM GMT<br/><br/><p>存在某种非正式的推理谬误，即您提倡的某些事情似乎是我们“一定要做”的事情，看起来显然很重要，但我们没有考虑到所涉及的所有步骤，因此我们实际上并没有这样做有理由假设影响会流过。</p><p>这里有些例子：</p><ul><li>计算机科学系主任认为“当然，重要的是不仅要培养技术精湛的毕业生，而且要培养那些将技能用于行善的人”。因此他们要求所有学生必须上“技术和道德课”。 The only problem is that none of their professors are an expert in this nor even interested in it, so it ends up being a poorly taught and run subject such that students put in the absolute minimum effort and learn nothing.</li><li> The prime minister of a country wants to reduce crime. He notices that the police department is severely underfunded so he significantly increases funding. Unfortunately, they&#39;re so corrupt and nepotistic that the department is unable to spend the funds effectively.</li><li> A government wants to increase recycling, so they create a &quot;national recycling day&quot; reasoning “surely this will increase recycling. Unfortunately, most people end up ignoring it and, out of those who actually are enthusiastic, most make an extra effort to recycle for a few days or even a week, then basically forget about it for the rest of the year. So it ends up having some effect, but it&#39;s basically negligible.</li></ul><p>在上述每种情况下，如果决策者花时间仔细考虑并询问自己是否可能真正获得好处，他们可能不会选择相同的选项。</p><p>领域经验有助于了解此类问题可能会突然出现，但<a href="https://www.lesswrong.com/tag/murphyjitsu">墨菲术</a>（Murphyjitsu）计划的习惯也是如此。</p><p>需要明确的是，我无意提及以下可能出错的方式：</p><ul><li>副作用：政府引入蛇是为了减少啮齿动物的数量，但这会产生副作用，导致更多的人被蛇咬伤。</li><li>反应：艾米向民主党捐赠 1000 万美元。鲍勃听说此事后决定向共和党捐赠 2000 万美元作为回应</li><li>价值混乱：詹姆斯小时候花了数年时间收集他的口袋妖怪卡牌，但他认为一旦他成年，这就没有意义了。</li></ul><p> （如果这个谬论已经有了名字，或者你认为你已经想到了一个更好的名字，请告诉我。）</p><br/><br/><a href="https://www.lesswrong.com/posts/G9GmrYhQm6bKt2XJJ/the-flow-through-fallacy#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/G9GmrYhQm6bKt2XJJ/the-flow-through-fallacy<guid ispermalink="false"> G9GmrYhQm6bKt2XJJ</guid><dc:creator><![CDATA[Chris_Leong]]></dc:creator><pubDate> Wed, 13 Sep 2023 04:28:29 GMT</pubDate> </item><item><title><![CDATA[Book review: The Importance of What We Care About (Harry G. Frankfurt)]]></title><description><![CDATA[Published on September 13, 2023 4:17 AM GMT<br/><br/><p>实际上，这更多的是总结或释义，而不是评论。 <i>《我们关心的事情的重要性》</i>是哲学家哈里·G·法兰克福的 13 篇文章。这些是我对每一项的一段总结：</p><ol><li>如果你做某事是因为你愿意自己这样做，那么你就必须对这件事负责，即使你实际上不能这样做。如果您采取该行动的实际原因是您有意这样做，那么您的行为是在胁迫威胁下进行的或者是预先确定的这一事实并不能免除您的责任。要让你因某些行为而受到指责/值得赞扬，并不需要你可能做了其他事情，这与几个世纪以来关于道德的理论相反。</li><li> “人”的一个也许定义性的属性是，我们可以形成“二阶意志”：关于我们的欲望的欲望。因为一个人可能希望她的意志指向除当前愿望之外的其他愿望，所以对于<i>人</i>来说，就会出现“自由意志”的问题：我是否有能力选择自己的愿望，或者我只是有能力选择自己的愿望？努力实现目前占主导地位的任何未选择的愿望？</li><li>威胁和提议是一个人试图影响某人以某种特定方式行使他的意志的两种方式。对于其中任何一个，要上升到“强制”的水平，或者至少是“强制”的水平——从某种意义上说，免除被强制的人对由此产生的行为的道德责任——它必须超越仅仅<i>影响</i>他的意志，而变得更加重要。就像把自己的意志强加给他一样。</li><li>做某事是因为它是可得之害中较小的一个，因此人们宁愿不做某事，但仍然是自由和负责任地做它。尽管对较小的恶行<i>负责</i>并不一定意味着你应该为这些恶行<i>承担责任</i>。如果确实是两害相权取其轻，那就足以转移责任。</li><li>我们所做的事情可能是主动的（行动）或被动的（仅仅是发生的事情）。 “激情”（例如愤怒）也可以是主动的或被动的：有些激情是我们自身的一部分（它们是“内在的”），另一些则在未经我们同意的情况下控制了我们（“外部的”）。一个人可以通过拒绝它或拒绝与它合作，甚至通过不情愿地屈服来将一种激情外化，或者可以通过同意它、顺从它、将它融入到一个人的自我形象中来内化一种激情。仅仅对你实际上自愿参与的热情感到<i>遗憾</i>并不足以将其具体化。</li><li>某人所做的事是他们的“行动”还是“仅仅是发生的事情”，不能通过查看该活动的史前历史来找出其因果关系（例如，它是有意的还是期望的）来解决。相反，活动（或进行活动的人）发生时的某些情况会区分情况：活动是否是<i>在该人的指导下</i>进行的。一种不被期望、不愿意或什至预期的行为，如果在发生时该人拥有它并有意识地引导它，那么它仍然可以是一个人的行为。</li><li>与认识论和伦理学并行的是“关心什么”的研究。关心某件事就是对它进行长期的个人投资。关心不一定甚至常常是自愿的，但塑造一个人关心的事物是塑造一个人意志的重要方式。关心某事就是赋予它意志力：激发“意志上的必然性”。一个被意志必然性所强迫的人通常不会觉得她的意志被克服了，但这种强迫就是（或与）她的意志相容的。被我们关心的事情（例如真相）所强迫是矛盾的解放。哪些事物值得关心的问题很容易受到哲学批判。如果你要关心任何事情，那么关心你是否关心正确的事情是一个很好的起点。</li><li>我在文章＃1中表明，替代可能性原则是错误的，该原则认为，只有当一个人本可以采取其他行动时，他才可以对他的行为承担道德责任。我还断言，决定论是否正确以及如果正确是否与自由意志相容的难题对于道德责任问题并不重要。 Peter van Inwagen 相信他已经证明我的观点是错误的。他错了。</li><li>人们以一种不精确的方式使用“需要”这个词，这可能会让需要看起来很重要，即使它们并不重要。 “需要”不是绝对的，而是始终与目的相关：我需要α<i>才能</i>β。要了解 α 的道德优先性，您必须检查 β； α的道德优先性源自β的道德优先性。特别是，仅仅因为你自愿承担的愿望而产生的需求是不值得尊重的。</li><li>什么是“胡说八道”？从本质上讲，这是对真相的漠视：一种虚张声势，人们只是装腔作势地传达一些事实，却没有真正关注如何表达真相。它与“谎言”的部分不同之处在于，说谎者确实关心事实是什么（以便歪曲事实）。也正因为如此，追求真理的事业才更加危险。由于各种原因，它最近也显得尤为突出。</li><li>经济平等是社会理想的不明智选择。 “最大限度地提高充足率”是一个更好的选择。关注一个人的相对经济地位而不是一个人的绝对经济需求在道德上是令人迷失方向的。一些关于经济平等价值的常见经济论点经不起仔细检验。甚至罗尔斯式的不平等的限定理由（“差异原则”）也太不温不火了。</li><li>意识的体验同时也是自我意识的体验，或者是反思性意识的体验。 （这似乎也是多余的，“本体论上的荒谬”）。我们特别关心我们的动机意识。我们通常（除非我们“肆意”）将我们当前的动机与我们认为理想的动机进行比较。通过决定支持某一特定动机，一个人就“构成了他自己”；通过实现这一目标，他运用了“自己的意志”。如果一个人继续受到与自己决定认同的人相反的动机的影响，那么一个人可能无法做到“全心全意”。</li><li>因为对于功利主义者来说，福祉是唯一理性的善，因此从理论上讲，没有任何一种行动对他们来说是禁止的（在某些可以想象的情况下，任何一种行动都可能改善福祉）。因此，他们似乎很难对某些行为原则做出持久的承诺（除了功利主义本身）。同样的怀疑也针对无神论者（如果没有神，一切都是允许的）。然而，一个人可能确实仍然认为某些行动方针是“不可想象的”，基于她本性中不可或缺的某些东西，并且不会采取这样的行动。这可能是一件理性的事情，并且确实可以将我们从令人反感的判断结论中拯救出来。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/e2fDabrdgrLDzA47q/book-review-the-importance-of-what-we-care-about-harry-g#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/e2fDabrdgrLDzA47q/book-review-the-importance-of-what-we-care-about-harry-g<guid ispermalink="false"> e2fDabrdgrLDzA47q</guid><dc:creator><![CDATA[David Gross]]></dc:creator><pubDate> Wed, 13 Sep 2023 04:17:16 GMT</pubDate> </item><item><title><![CDATA[Padding the Corner]]></title><description><![CDATA[Published on September 13, 2023 1:30 AM GMT<br/><br/><p><span>我爸爸的厨房里有一个架子，离地大约四英尺。我记得当我不再矮到可以在它下面行走时：哎呀！我没有受到脑震荡或其他什么影响，但这非常不愉快。我的姐妹和表兄弟姐妹也记得他们对此摇头不已。我注意到我的大女儿已经够高了，于是开始给她讲这个故事，警告她注意这一点。</span></p><p>当我讲故事的时候，我意识到这是多么愚蠢，于是停下手头的事情，在角落里放了一些垫子。</p><p> <a href="https://www.jefftk.com/washcloth-padded-corner-big.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/dNBeQdB35qKruoCpr/yccwa82xz4pbcwhkz40p" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/dNBeQdB35qKruoCpr/yccwa82xz4pbcwhkz40p 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/dNBeQdB35qKruoCpr/m5ojftrweqlvxxt4rzro 1100w"></a></p><div></div><p></p><p>折叠起来并用螺钉固定在架子上的毛巾看起来不太好，但几年后的今天，三个表兄弟都足够高，可以与它相交，而且没有人受伤。也许有一天我们可以做一些更优雅的事情，但与此同时，修复尖角胜过警告人们。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid034oVcWajuvk8MWeyna2ybS8iEqC28DEqRq42T32f3LGpe9uH2crmAL4aVxkFHDL2bl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111055243593039436">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/dNBeQdB35qKruoCpr/padding-the-corner#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/dNBeQdB35qKruoCpr/padding-the-corner<guid ispermalink="false"> dnBeQdB35qKruoCpr</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 13 Sep 2023 01:30:04 GMT</pubDate> </item><item><title><![CDATA[Should an undergrad avoid a capabilities project?]]></title><description><![CDATA[Published on September 12, 2023 11:16 PM GMT<br/><br/><p><i>虽然我写的是我的个人情况和项目，但我希望其他人也能遇到这种情况。</i></p><p>我正在考虑加入<strong>ProjectX。</strong>这听起来像是一个与人工智能合作、获得经验并有可能创造出令我自豪的东西的绝佳机会。不幸的是，它是关于提高人工智能效率，我将其理解为“能力”。当能力增强时，世界末日就会越来越近、可能性也越来越大。</p><p></p><p>有关该项目的更多信息：</p><p> <strong>“ProjectX：机器学习研究竞赛</strong></p><p>ProjectX 是<strong>世界上最大的本科机器学习研究竞赛，</strong>参赛团队来自世界各地的顶尖大学。 3支获胜团队将分别获得<strong>20,000加元</strong>的现金奖励，所有参赛者将被邀请参加2024年1月中旬<strong>举行的多伦多大学人工智能年会</strong>。去年的比赛由谷歌、英特尔和Nvidia赞助，并有特邀主讲人詹姆斯·范龙.</p><p>今年的重点是<strong>高效人工智能</strong>：研究创新方法和机器学习方法来创建人工智能模型，在不影响性能的情况下显着降低功耗。</p><p><br></p><p>今年我们将派出一支由 3-6 名学生组成的团队代表西北大学。该团队将在 2023 年 10 月至 12 月期间研究他们选择的问题，最终发表一篇描述他们的发现的论文。</p><p><a href="https://drive.google.com/file/d/1_v4vlZlgY6KgCXXPF_m76dBZnTkDpEX5/view?usp=sharing">有关今年比赛的更多信息</a></p><p><a href="https://drive.google.com/file/d/1UotPB5_4_9akc8mcPFQYQCWwdD7YLJed/view?usp=sharing">去年获奖团队的论文样本</a>“</p><br/><br/> <a href="https://www.lesswrong.com/posts/az2ibmLhv3enpfrdq/should-an-undergrad-avoid-a-capabilities-project#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/az2ibmLhv3enpfrdq/should-an-undergrad-avoid-a-capability-project<guid ispermalink="false"> az2ibmLhv3enpfrdq</guid><dc:creator><![CDATA[Double]]></dc:creator><pubDate> Tue, 12 Sep 2023 23:16:40 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Contra four-wheeled suitcases, sort of]]></title><description><![CDATA[Published on September 12, 2023 8:36 PM GMT<br/><br/><p>是简单、坚固、可靠的系统更好，还是花哨、精密、复杂的系统更好？</p><p>这篇文章带着先入之见，分析了手提箱、汽车、登山装备、食物、杯子、太空笔、人、初创公司、足球、耳机、重量、火箭、火箭、轮船和银行，并得出结论：</p><blockquote><p>我希望这个练习能够表明，简单、坚固、可靠的系统在系统上比花哨的脆弱系统更好。但事实并非如此。有时崎岖不平会获胜，有时却不会。</p></blockquote><p>它更多地说明了他发现的趋势和模式。请阅读短文以了解更多信息。</p><p>我喜欢它<a href="https://www.lesswrong.com/s/zpCiuR4T343j9WkcK">注意到混乱</a>并<a href="https://www.lesswrong.com/tag/babble-and-prune">喋喋不休地提出</a>许多有趣的类别。</p><br/><br/> <a href="https://www.lesswrong.com/posts/nFGMvs9WSeau8ATTY/linkpost-contra-four-wheeled-suitcases-sort-of#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nFGMvs9WSeau8ATTY/linkpost-contra-four-wheeled-suitcases-sort-of<guid ispermalink="false"> nFGMvs9WSeau8ATTY</guid><dc:creator><![CDATA[Gunnar_Zarncke]]></dc:creator><pubDate> Tue, 12 Sep 2023 20:36:03 GMT</pubDate></item><item><title><![CDATA[Seeking Feedback on My Mechanistic Interpretability Research Agenda]]></title><description><![CDATA[Published on September 12, 2023 6:45 PM GMT<br/><br/><h1>为什么发这个帖子</h1><p>我全职从事 MI 研究大约三个月了，由于我目前的资助即将结束，而且我最近收到了 Lightspeed 拒绝，现在似乎是从对象级工作中抽出一些时间来反思方向的好时机以及后续步骤。我花了几个小时写了一份草稿，意识到我认为它太复杂了（为了听起来很复杂？），然后用大约 750 个字重写了我真正想做的事情以及为什么。我打算将反馈纳入 OpenPhil 早期职业申请（我将在几天内提交）。我还在申请人工智能研究职位，如果我获得了职位，这将有助于了解要关注的内容。</p><h1>议程</h1><p>在我看来，机械可解释性研究的目标是充分了解最先进的网络正在做什么，这样我们就可以检查是否正在发生任何危险行为。我<a href="https://www.neelnanda.io/mechanistic-interpretability/attribution-patching">预计</a>自动化技术将成为理解此类模型<a href="https://arxiv.org/pdf/2211.00593.pdf">的重要一步（示例：1、2、3</a> <a href="https://arxiv.org/pdf/2304.14997.pdf">）</a> 。然而，我觉得我们缺少一个重要的基础，因为我们还没有完全理解小语言模型正在做什么，因此，不知道自动化技术应该检测的一组事情。 （这些技术应该寻找推理吗？、特定行为？、记忆信息？、叠加？）。我认为，我们首先需要对浅层语言模型进行细致的、底层的探索，以指导我们对更大模型的分析。</p><p>随着<a href="https://arxiv.org/abs/2305.07759">TinyStories 模型</a>的发布，我们现在有了可以生成合理文本补全（儿童故事）的小型语言模型。 <a href="https://d.docs.live.net/5217bc84fed48838/Desktop/TinyStories-1Layer-21M">TinyStories-1Layer-21M</a>是该系列中的一个单层模型，它将作为我研究的重点。我将使用<a href="https://www.anthropic.com/index/a-mathematical-framework-for-transformer-circuits">Anthropic 的电路框架</a>对此模型进行低级机械解释。我相信，鉴于其相对较小的规模和通过网络的路径数量有限，使用电路分析该网络是可行的。</p><p>我坚信，这一层模型中将存在一些规则和模式，这些规则和模式将作为我们在更大模型中搜索事物的指南。 （当然，几乎可以肯定，在更多样化的数据集上训练的大型模型中存在该模型中不存在的规则和模式，但这应该作为建立对语言模型正在做什么的机械理解的良好起点） 。我的希望是，通过对单层模型的权重和激活进行重点低级探索，我可以构建一个人类可解释的知识和功能库，该单层模型似乎已经学会了这些知识和功能。</p><p>我已经开始探索这个模型，所以我对可能存在的组件有一些粗略的想法，但这些说法应该被理解为初步的：我相信我已经检测到似乎与某个概念相关的 MLP 神经元，例如，我发现了几个神经元，当故事中的人物“寻找”或“搜索”某物时，这些神经元似乎表现出强烈的积极性；此外，这些神经元对残差流的贡献通常支持也与“查看”或“搜索”有关的输出标记。</p><p>另一方面，一些神经元似乎支持语法规则。一个神经元对代词呈强阳性；它对残差流的贡献通常支持动词和副词输出标记。 （我 ->; 是，他们 ->; 看，他们 ->; 看到，他们 ->; 两者[感觉]）。另一个对文章非常积极；它对残差流的贡献通常支持形容词和名词（A ->; 香蕉，the->; 树，a->; 大[香蕉]）。该模型是否故意编写内容和语法规则，以便使用正确的语法成功地编写有关“搜索”的故事？是否还正在制定其他类型的规则？</p><p>一旦我拥有了知识和功能库，我将尝试手工制作可以实现此行为的网络权重，类似于<a href="https://distill.pub/2020/circuits/curve-circuits/">Chris Olah 创建的曲线检测器</a>和<a href="https://docs.google.com/document/d/1Hk1NQSQE3ycaDRULxB-Cy78FuqFW9sUDIEc_56UMlMI/edit">我过去为 1 层变压器手工制作权重的工作</a>。手工设计网络权重使我能够检测到我之前错过的网络整体机制的一部分。我预计手工权重同样会指出我对<a href="https://d.docs.live.net/5217bc84fed48838/Desktop/TinyStories-1Layer-21M">TinyStories-1Layer-21M</a>的整体解释中不完整或缺失的部分。</p><p>在过去的三个月里，我成功地解释了三个小型变压器模型（ <a href="https://www.lesswrong.com/posts/vGCWzxP8ccAfqsrS3/thoughts-about-the-mechanistic-interpretability-challenge-2">Stephen Casper 的 MI 变压器挑战</a>、 <a href="https://github.com/freestylerick/quick_MI">ARENA 每月问题#1</a> 、 <a href="https://github.com/freestylerick/First-Unique-Token">ARENA 每月问题#2</a> ）。我对使用 Pytorch 进行电路式分析来操纵和组合网络权重和激活的能力充满信心，最终创建带注释的图表，我可以检查这些图表以生成和测试假设。</p><p>了解 1 层模型的工作原理将作为了解 2 层模型的基础。对于 2 层（或更多）层的模型，我们具有注意力头组合，这可能会解锁 1 层模型无法实现的行为。通过了解 1 层模型中存在的内容，2 层模型很可能会表现出一些相同的模式和一些不同的模式。在探索 2 层模型之后，我们希望可以继续扩展，再次使用较小的模型作为我们可能期望在较大模型中找到的一些内容的指南。希望这个过程最终能让我们更好地理解大型语言模型正在做什么，以便我们能够更好地评估安全问题。</p><h1>结论</h1><p>感谢您抽出时间来阅读。再次强调，我愿意接受所有反馈，包括负面反馈。</p><p>如果您是资助者或雇主，可能有兴趣讨论和/或资助这项工作，请通过 LessWrong DM 联系。</p><p>如果您对可能资助这项研究的组织有任何想法，请随时私信或在评论中发帖。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ESaTDKcvGdDPT57RW/seeking-feedback-on-my-mechanistic-interpretability-research#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ESaTDKcvGdDPT57RW/seeking-feedback-on-my-mechanistic-interpretability-research<guid ispermalink="false"> ESaTDKcvGdDPT57RW</guid><dc:creator><![CDATA[RGRGRG]]></dc:creator><pubDate> Tue, 12 Sep 2023 18:45:08 GMT</pubDate></item></channel></rss>