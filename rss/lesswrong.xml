<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 30 日星期三 20:12:05 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Report on Frontier Model Training]]></title><description><![CDATA[Published on August 30, 2023 8:02 PM GMT<br/><br/><p>对于那些致力于预测、监管或确保人工智能安全的人来说，了解推动人工智能能力不断提升的因素非常重要。关于强大 GPU 出口的法规需要通过了解这些 GPU 的使用方式来制定，预测需要通过瓶颈来制定，安全性需要通过了解未来模型的训练方式来制定。更清晰的理解将使政策制定者能够以这样的方式瞄准监管，使公司很难<a href="https://www.tomshardware.com/news/nvidia-gimps-h100-hopper-gpu-to-sell-as-h800-to-china"><u>仅使用技术上兼容的GPU来规避监管</u></a>，预测者能够避免关注<a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.g5d5897qnsfc"><u>不可靠的指标</u></a>，以及致力于减轻人工智能缺点的技术研究以了解<a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.apsafl4txsk7"><u>哪些数据。模型可能会接受训练</u></a>。</p><p>本文档是根据我编写的一些较小文档的集合构建的，这些文档涉及我认为重要的前沿模型训练的许多不同方面。我希望人们能够使用这份文档作为资源集合，从中提取他们认为重要的信息并为他们自己的模型提供信息。</p><p>我不希望这份文档对任何认真的人工智能实验室能力工作产生重大影响——我认为我的结论在很大程度上可以在尝试扩展人工智能的过程中发现，或者比认真的此类尝试花费的钱少得多。此外，我预计主要实验室已经了解本报告中的许多内容。</p><h2>致谢</h2><p>我要感谢以下人员的反馈、建议和讨论：</p><ul><li> James Bradbury，Google DeepMind 软件工程师</li><li>本杰明·埃德尔曼博士哈佛大学候选人</li><li>Horace He，PyTorch/Meta 软件工程师</li><li>Lukas Finnveden，开放慈善项目研究分析师</li><li>Joanna Morningstar，Nanotronics 首席科学官</li><li>凯勒·绍尔博士帕迪·兰德研究生院候选人</li><li>Jaime Sevilla，Epoch 总监</li><li>Cody Wild，谷歌研究工程师</li></ul><h2>指数</h2><p><a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.q4i6cuur5ejr"><u>机器学习训练的成本明细</u></a></p><p>根据泄漏和分析来估计训练前沿（最先进的）模型的成本。功耗只占成本的一小部分，GPU 可能占微弱多数。 </p><p><img style="width:405.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/hp8ps6kshtsd1igy2teo" alt=""></p><p> <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu"><u>为什么 ML GPU 成本如此之高</u></a></p><p>ML GPU 之所以昂贵，很大程度上是因为它们的通信和内存功能，而不是因为它们的处理能力。 NVIDIA 最好的游戏 GPU 提供比用于训练 GPT-4 的 GPU 更强大的 ML 处理能力，而价格仅为其十分之一。请注意，NVIDIA 近乎垄断的地位合理地解释了部分价格差异。</p><p> <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.g5d5897qnsfc"><u>反扑</u></a></p><p>认为机器学习计算能力最常见的衡量标准——浮点运算——是有缺陷的，因为不同类型浮点数的兴起使得标准化变得困难，而且处理能力的成本只占机器学习成本的一小部分。</p><p> <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.wdy8qwubysry"><u>机器学习并行性</u></a></p><p>机器学习并行技术概述，展示了“机器学习<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel"><u>极其并行</u></a>”这一常见概念是如何过分简单化的，并且在大规模情况下会被打破——任何简单的模型并行化方法都会开始遇到瓶颈，因为单个设备的能力会成为瓶颈，而不管其性能如何。涉及的设备数量。</p><p> <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.apsafl4txsk7"><u>我们（可能）不会耗尽数据</u></a></p><p>有很多方法可以防止数据成为机器学习扩展的主要瓶颈，尽管不确定其中任何一种方法都能像历史上发生的那样快速扩展。</p><p> <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.bsosdgecdqu2"><u>AI 能源使用和热特征</u></a></p><p>机器学习能源使用在不久的将来可能会变得很重要，即使目前对于前沿模型训练来说这只是一个相对较小的问题。如果目前的趋势继续下去，能源使用可能会限制规模扩大，确定主要的工程挑战，并提供一种利用卫星和多光谱摄影监视训练运行的新方法。</p><h1>机器学习训练的成本明细</h1><p>本节试图估计与训练最先进的 ML 模型相关的成本量，特别是所需的资本量。这不是一个详细的预测尝试，而是作为任何想要了解基础知识的人的默认来源 - 例如电力使用现在是否是一项主要支出（实际上不是），或者 GPU 是否占了大部分成本（可能，但只占微弱多数）。我希望这可以帮助人们优先考虑他们的研究议程并作为一个起点。</p><p>请注意，我特别关注如何训练一个模型，使其能够与发布时的最佳模型相媲美。截至目前，GPT-4 是此类中唯一公开承认的模型，因此我将特别关注它，尽管我也使用有关即将推出的前沿模型的泄漏来增强我的分析<span class="footnote-reference" role="doc-noteref" id="fnrefldni1dmval"><sup><a href="#fnldni1dmval">[1]</a></sup></span> 。 </p><p><img style="width:624px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/zr9l93xrylh1p0ixjixv" alt=""></p><p>在本节中，我将详细分析我认为这笔钱的实际用途，以及我们预计这笔钱将如何随着时间的推移而发展。我不打算在这里进行具体的预测，但我希望这可以为那些做具体预测的人提供指导。此外，我希望这对政策制定者有用，他们可以通过展示培训的哪些部分最昂贵来分析如何最有效地监管培训运行。一些主要要点：</p><ul><li>新的最先进（前沿）模型的成本可能约为 10 亿美元，最新的前沿模型 GPT-4 的成本约为 5 亿美元。</li><li>前沿模型的成本（大约）为 80% 的超级计算机硬件、18% 的人员和 2% 的电力。</li><li> ML 超级计算机硬件成本在 GPU 和其他硬件之间大约各占 70/30（主要是为了帮助 GPU 快速相互通信）。</li><li> ML GPU 的内部和外部通信能力占其成本的绝大部分。</li></ul><h2>定义“成本”</h2><p>我将前沿模型的成本定义为公司创建前沿模型所需花费的金额。在实践中我使用以下公式：</p><p><i>成本 = 硬件成本 + 创建期间的运营支出</i></p><p>其中，“<i>硬件成本</i>”是指购买训练运行所需的所有硬件的成本，“<i>创建期间的运营支出</i>”是指创建期间花费在能源、工资、维护和其他运营费用上的金额总和。模型设计和训练的时间（我认为一年是这个时间段的一个很好的上限）。</p><p>大多数分析师在这一点上与我不同，他们使用租用 GPU 进行训练的成本，该成本是基于需要在模型创建后持续多年的承诺的价格。这些承诺有效地跨越了硬件的生命周期，因此我认为最好将其视为抵押而不是租赁，因为抵押的最终结果是提供者不再拥有有价值的对象，而租赁的最终结果是同一时期，提供者拥有与他们开始时价值相似的物品。</p><p>这里要注意的另一件事是，我没有考虑模型创建期间之外的电力成本，因此 GPU 的生命周期电力成本在该 GPU 成本中所占的比例将大于我的电力成本。考虑。</p><h2>总成本估算</h2><p>我估计前沿模型总成本的首选方法是根据支出和投资的公共信息进行估计，使用自下而上的成分分析来检查估计的合理性。基于这种方法，我估计 GPT-4 的成本为 5 亿美元，不久的将来前沿模型的成本将达到 10 亿美元。这是基于以下证据：</p><ul><li><a href="https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/"><u>泄密</u></a>表明，OpenAI 到 2022 年（GPT-4 训练的那一年）在计算和数据上花费了超过 4 亿美元。</li><li><a href="https://techcrunch.com/2023/04/06/anthropics-5b-4-year-plan-to-take-on-openai/"><u>据报道</u></a>，Anthropic 在筹集资金时表示，训练下一版本的 Claude 将花费 10 亿美元，计算量与<a href="https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp%3Dsharing"><u>Epoch</u></a>和<a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure"><u>Semianalysis</u></a>估计的用于训练 GPT-4 的计算量相似。</li><li>尽管 OpenAI 在 2019 年<a href="https://www.crunchbase.com/organization/openai/company_financials"><u>已经筹集了</u></a>10 亿美元，但到 2023 年仍筹集了 100 亿美元，这表明支出巨大。</li><li> Inflection AI<a href="https://inflection.ai/inflection-ai-announces-1-3-billion-of-funding"><u>筹集了</u></a>约 15 亿美元，其中大部分可能用于购买能够训练前沿模型的<a href="https://www.nextplatform.com/2023/07/05/the-1-billion-and-higher-ante-to-play-the-ai-game/"><u>超级计算机</u></a>。</li><li>在过去的三年里，NVIDIA 的<a href="https://www.nextplatform.com/2022/05/26/datacenter-becomes-nvidias-largest-business/%23:~:text%3DDuring%2520fiscal%2520Q1%252C%2520Nvidia%27s%2520datacenter,divisions%2520will%2520jockey%2520for%2520position"><u>数据中心收入大幅增长</u></a>，可能是由于 ML GPU 的销售，从每季度不到 10 亿美元增加到超过 35 亿美元，这似乎与一些实验室构建成本在 10 亿美元左右的超级计算机相当兼容。 。</li></ul><p>请注意，这些数字远远高于租用 GPT-4 最终训练运行期间所需的所有硬件的大约 6000 万美元<span class="footnote-reference" role="doc-noteref" id="fnref86mrkdwudw8"><sup><a href="#fn86mrkdwudw8">[2]</a></sup></span> （如果愿意承诺租用硬件一段时间）。持续时间比训练长得多，这对于大型人工智能实验室来说很常见。我认为我使用的方法可以更好地跟踪生成前沿模型所需的投资量。作为一项健全性检查，粗略的数学估计训练 GPT-4 <span class="footnote-reference" role="doc-noteref" id="fnref7rd3o585ve6"><sup><a href="#fn7rd3o585ve6">[3]</a></sup></span>所需的硬件成本为 5 亿美元，这与 OpenAI 支出和投资的经验证据非常吻合。</p><h2>粒度分析</h2><p>为了将培训运行的成本分解为各个组成部分，我们需要超越底线支出数字，并利用有关培训运行如何进行以及可能涉及的成本的更详细信息。这必然需要更多的假设和不确定性，但有足够的信息可以对前沿模型各个组成部分的相对成本做出合理的猜测。</p><p>我开始根据 OpenAI 2022 年支出的泄露<a href="https://fortune.com/longform/chatgpt-openai-sam-altman-microsoft/"><u>信息</u></a>来估算训练 GPT-4 这样的 ML 模型的成本比例：</p><p> “[OpenAI] 预计 [2022 年] 计算和数据费用为 4.1645 亿美元，员工费用为 8931 万美元，未具体说明的其他运营费用为 3875 万美元。”</p><p>虽然这些数字并不是专门针对 GPT-4，但我认为它是成本比例的合理基准。其他可能的成本，例如免费提供 ChatGPT，会<a href="https://www.semianalysis.com/p/the-inference-cost-of-search-disruption"><u>小得多</u></a>。我删除了“未指定”部分，因为它很小，而且我没有看到它与 GPT-4 相关的明显方法。</p><p>接下来，我对计算和数据进行了细分。我完全删除了数据，因为我怀疑数据是成本的一​​个重要因素，<a href="https://time.com/6247678/openai-chatgpt-kenya-workers/"><u>本文</u></a>提到一个合理的数据费用仅为 20 万美元<span class="footnote-reference" role="doc-noteref" id="fnreff8iigdgmfa"><sup><a href="#fnf8iigdgmfa">[4]</a></sup></span> 。这篇<a href="https://lambdalabs.com/blog/hyperplane-16-infiniband-cluster-total-cost-of-ownership%23:~:text%3DThe%2520default%2520system%2520price%2520used,specifications%2520of%2520the%2520DGX%252D2."><u>文章</u></a>表明计算可以分解为硬件、电源和系统管理。系统管理似乎很小，我完全忽略了它。我使用大约<a href="https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t%3Depmt_5_6_a"><u>0.05 美元/千瓦时</u></a>作为廉价电力的基本价格来计算电力，并使用各种来源估计耗电量，每个来源都给了我相当相似的答案<span class="footnote-reference" role="doc-noteref" id="fnrefn04f3250v5"><sup><a href="#fnn04f3250v5">[5]</a></sup></span> 。</p><p>请注意，其他侧重于租赁硬件成本的会计方法意味着电力使用成本等运营成本所占成本的比例比我使用的方法所暗示的要高。这是因为这些方法的运营成本相似（使用期间的运营费用），但租赁方法提供的总成本较低。在硬件的整个生命周期内拥有硬件的总成本同样具有更高的运营成本比例。 </p><p><img style="width:423.25px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/d0yw2cshrx6tqvwrfzjo" alt=""></p><h3>硬件成本明细</h3><p>由于硬件（用于机器学习超级计算机）占前沿模型成本的很大一部分，因此值得进一步细分。我最好的猜测是，在目前最好的基于 GPU 的 ML 超级计算机中，GPU 约占成本的 70%，尽管我认为 50% 到 85% 之间的比例都是合理的。其余成本的大部分是网络硬件，以实现 GPU 之间快速、稳健的通信。</p><p>我通过结合两种不同的方法得出了 70% 的估计值：</p><ol><li> Next Platform 关于 Inflection AI 新型超级计算机的<a href="https://www.nextplatform.com/2023/07/05/the-1-billion-and-higher-ante-to-play-the-ai-game/"><u>文章</u></a>估计 50% 的成本来自 GPU。文章还指出，节点之间的互连约占超级计算机成本的 20%。我对整体估算的主要问题是，各个节点内的非 GPU 组件似乎太贵了，尤其是与其他更可靠的类似节点成本估算相比。我认为这是因为 Next Platform 更习惯于非 ML 超级计算机的标准</li><li>Semianalysis<a href="https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is"><u>细分了</u></a>单个 H100 节点的成本，表明非 GPU 组件占成本的 14%（不包括 Nvidia 的标记）。请注意，这不包括超级计算机的非节点成本。</li></ol><p>结合 Semianalysis 的节点成本细分和 Next Platform 的 20% 互连经验法则，我们发现 GPU 占硬件成本的 70% <span class="footnote-reference" role="doc-noteref" id="fnrefunidulxrjqe"><sup><a href="#fnunidulxrjqe">[6]</a></sup></span> 。</p><p>请注意，这个数字可能会随着时间的推移而波动 - 对于具有更多 GPU 的超级计算机，协调 GPU 及其相互通信的成本会大幅增加，并且对 GPU 故障的鲁棒性的必要性也会增加。因此，较大的集群必须在硬件上花费更多，以实现设备之间以及用于中间结果检查点的硬件之间的快速、稳健的通信。</p><p>在这种规模上通信如此昂贵的原因有点微妙，但它与这样一个事实有关：随着网络中设备数量的增加，设备之间可能的连接数量呈二次方增长。这可以使用复杂的路由技术来处理，但随着设备数量的增加，这些技术的复杂性和成本仍然会增加。</p><h3> GPU 成本</h3><p><a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu"><i><u>为什么 ML GPU 成本如此之高</u></i></a><i>对此进行了更详细的讨论</i><i>。</i></p><p>进一步深入研究硬件成本，我想简要谈谈 ML GPU 的成本。 ML GPU 的大部分成本是其所需的通信功能的结果，类似于非 GPU 硬件通信的重要性。内存也发挥着重要作用，无论是内存的通信方面（单个芯片内的内存带宽）还是每个芯片上高带宽内存的总量。此外，稀缺性和 NVIDIA 在 ML GPU 领域的主导地位可能是推动价格的重要因素。这种情况可能会在未来几年发生变化，随着竞争和产量的增加，GPU 价格会下降。</p><h1>为什么 ML GPU 成本如此之高</h1><p>ML GPU 构成了前沿模型训练的最大组成部分，因此了解它们的成本为何如此之高非常有用。通常分析的重点是 ML GPU 提供的处理能力（以每秒操作数衡量），例如可能用于训练 GPT-4 的约 15,000 美元的 A100 GPU。然而，NVIDIA 最好的游戏 GPU 提供了更强大的 ML 处理能力，而价格仅为 A100 的十分之一左右。用于 ML 的最佳 GPU 与其他设备的主要区别在于最先进的 ML GPU 的卓越内存和通信能力。</p><p>在我们了解 ML GPU 的不同方面与价格的相关性之前，先快速概述一下 ML GPU 的规格：</p><ul><li>成本：GPU 的成本是多少，对于 ML GPU 来说通常是秘密，但我们根据泄露的信息和专家分析有合理的估计。</li><li> ML 处理能力：GPU 每秒可以执行的操作量，使用最适合 ML 训练的数据类型。当非浮点数据类型为 usd 时，通常以每秒数万亿次浮点操作 (TFLOP/s) 或更一般的每秒数万亿次操作 (TOP/s) 来衡量。</li><li>内存大小：GPU 有多少高速内存。以十亿字节 (GB) 为单位。请注意，GPU 中也存在数量少得多、速度更快的内存，称为缓存。</li><li>内存带宽：GPU 每秒可以读取和写入内存的字节数。以每秒万亿字节 (TB/s) 为单位。</li><li>互连带宽：GPU 和其他外部设备（包括其他 GPU 和 CPU 等）之间每秒可以传输多少字节。互连有多种类型，但我不会在这里讨论。通常以每秒数百 GB (100 GB/s) 为单位进行测量。</li><li>关于互连带宽的说明：这实际上是将用于超级计算和机器学习的 GPU 与消费类 GPU 区分开来的，因为这是将大量 GPU 连接在一起所需的带宽。</li><li>能源使用量：GPU 使用的能源量以瓦特为单位。</li></ul><p>作为这些属性重要性的说明性示例，下表将用于训练 GPT-4、A100 和 H100（最先进的 ML GPU 和 A100 的后继产品）的 GPU 与 RTX 4090（NVIDIA 的最好的游戏 GPU）。请注意，有不同类型的 FLOP（或 TOP）；在这里，我使用了最适合训练的数据，但也包括了 A100 可能拥有的数字（如果它具有较新 GPU 的功能）。 </p><p></p><figure class="table"><table><tbody><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://www.nvidia.com/en-us/data-center/a100/"><i><u>NVIDIA A100 80GB SXM</u></i></a> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://resources.nvidia.com/en-us-tensor-core"><i><u>NVIDIA H100 SXM</u></i></a> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"><i><u>NVIDIA GeForce RTX 4090</u></i></a> </td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>成本</i></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 15,000 美元<span class="footnote-reference" role="doc-noteref" id="fnrefqts2glqvljo"><sup><a href="#fnqts2glqvljo">[7]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 24,000 美元<span class="footnote-reference" role="doc-noteref" id="fnref70prgwf7vbi"><sup><a href="#fn70prgwf7vbi">[8]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> $1,599 <span class="footnote-reference" role="doc-noteref" id="fnref3vzbxieq0wf"><sup><a href="#fn3vzbxieq0wf">[9]</a></sup></span></td></tr><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>机器学习处理能力</i><span class="footnote-reference" role="doc-noteref" id="fnreff2409qnb59q"><sup><a href="#fnf2409qnb59q">[10]</a></sup></span></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p> 624 顶部/秒</p><p>（如果无法进行 int8 训练，则为 312 TFLOP/s）</p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 1978.9 TFLOP/秒</td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1">660.6 TFLOP/秒</td></tr><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>成本/TFLOP/秒（或TOP/秒）</i></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p> 24.04 美元</p><p>（如果无法进行 int8 训练，则为 48.08 美元）</p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 12.13 美元</td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1">2.42 美元</td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i><strong>内存大小</strong></i></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>80GB</strong> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>80GB</strong> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>24GB</strong> </td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i><strong>内存带宽</strong></i></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>2.04TB/秒</strong></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>3.35TB/秒</strong></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>1.08TB/秒</strong></td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i><strong>互连带宽</strong></i><span class="footnote-reference" role="doc-noteref" id="fnrefn3r0k58yzvp"><sup><a href="#fnn3r0k58yzvp">[11]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>600GB/秒</strong></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>900GB/秒</strong></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <strong>64 GB/秒</strong><span class="footnote-reference" role="doc-noteref" id="fnrefyvhsu3o0aof"><sup><a href="#fnyvhsu3o0aof">[12]</a></sup></span> </td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>能源使用</i></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 500W <span class="footnote-reference" role="doc-noteref" id="fnrefeqmm2whzz6u"><sup><a href="#fneqmm2whzz6u">[13]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 700W </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 450W</td></tr></tbody></table></figure><p></p><p>在上表中，机器学习处理能力几乎不能解释价格差异，与能源使用情况相同。我加粗了内存大小、内存带宽和互连带宽，因为它们可以更好地跟踪价格。内存大小和内存带宽对价格有共同影响——更快的内存每字节成本更高<span class="footnote-reference" role="doc-noteref" id="fnrefbpd8hfx7gaa"><sup><a href="#fnbpd8hfx7gaa">[14]</a></sup></span> 。</p><p>我想指出与数据通信相关的两个规范的重要性：内存带宽和互连带宽。随着时间的推移，这些规格可能会变得更加重要，因为硬件提供的处理能力的增长速度快于内存或互连带宽。部分原因是制造越来越小的晶体管的过程推动了处理能力和内存的增长，但没有推动带宽的增长。因此，当摩尔定律失效时，尚不清楚带宽扩展是否会耗尽，因为与通信相关的组件通常比与计算和内存相关的组件大得多，而且瓶颈不同，这意味着它们可能会遇到物理问题。稍后限制。</p><p>有关随时间增长的说明，请参见下图，其中 HW FLOPS 指处理能力，DRAM BW 指内存带宽，互连<span class="footnote-reference" role="doc-noteref" id="fnref891plqjefd5"><sup><a href="#fn891plqjefd5">[15]</a></sup></span> BW 指 GPU 之间的通信速度。 <img style="width:624px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/w9a9otz1ognebkdexqaj" alt=""></p><p>来源： <a href="https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8"><u>https</u></a> ://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8</p><p>请注意，每个 FLOP 所需的通信量不一定是恒定的，因此这种增长差异不会<i>不可避免地</i>导致互连成为瓶颈 - 它只是有助于理解如何需要处理和优化通信。</p><h3> NVIDIA的垄断</h3><p>NVIDIA 似乎已经垄断了最好的 GPU，H100 超越了竞争对手，尽管谷歌可能是一个强有力的竞争对手<span class="footnote-reference" role="doc-noteref" id="fnref04t0i9i082tm"><sup><a href="#fn04t0i9i082tm">[16]</a></sup></span> 。一些人报告称，他们因此对 GPU 收取<a href="https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance?utm_source%3D%252Fsearch%252F5x%26utm_medium%3Dreader2"><u>非常高的利润</u></a>（5 倍制造成本），尽管我认为这主要是由于供应量较高，而且他们正在迅速增加供应以满足需求。 。我最好的猜测是，由于未来几年竞争和供应的增加，价格可能会下降，但内存和带宽仍然是比失败更大的因素，至少在不久的将来是这样。</p><h1>反扑</h1><p>浮点运算 (FLOP) 通常用作 ML 处理能力的衡量标准 - 预测 AI 的未来<span class="footnote-reference" role="doc-noteref" id="fnrefj1wd590x17"><sup><a href="#fnj1wd590x17">[17]</a></sup></span> <sup>、</sup> <span class="footnote-reference" role="doc-noteref" id="fnreff0m7mc9r5a5"><sup><a href="#fnf0m7mc9r5a5">[18]</a></sup></span>并规范当前强大硬件的出口。然而，我相信机器学习硬件和扩展的最新发展使得 FLOP 变得不明确，并且在当前环境中作为一个指标的可靠性比许多人想象的要低。</p><h2>不同类型的 FLOP</h2><p>浮点数在内部表示为位序列。传统上，32 位被用作标准 - 单精度浮点数（FP32 - 32 代表 32 位）。这种格式是最常见的格式，其次是双精度 (FP64)。然而，在过去的几年里，在机器学习中使用更少的每个数字位数（也称为较低精度）表示方面已经取得了进展。在最好的 ML 硬件上，这可能会导致传统单精度 FLOP 和大多数 ML 训练优化操作之间的处理能力存在 30 倍的差异<span class="footnote-reference" role="doc-noteref" id="fnreflqvp92cu9y"><sup><a href="#fnlqvp92cu9y">[19]</a></sup></span> 。</p><p>我不清楚 FLOP 是否会变得比现在更加专业化。浮点数的精度只能下降到目前为止，并且所需的实验和前沿（最先进的）模型不断变化的架构使得硬件在所涉及的时间尺度上的极端专业化可能变得困难。</p><h2>失败成本并不是一切</h2><p>虽然 FLOP 通常是 ML 训练运行分析的重点，但它们并不是必须优化的唯一资源。</p><p> GPT-4 训练数量级<span class="footnote-reference" role="doc-noteref" id="fnreffwu1mnychs"><sup><a href="#fnfwu1mnychs">[20]</a></sup></span></p><p>详细信息请参见脚注。 </p><figure class="table"><table><tbody><tr><td style="background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1">训练数据可以存储在... </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 100 台 Macbook <span class="footnote-reference" role="doc-noteref" id="fnref7inu0b4b7z3"><sup><a href="#fn7inu0b4b7z3">[21]</a></sup></span> </td></tr><tr><td style="background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1">处理能力 (FLOP) 将需要 3 个月的时间...... </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p>250,000 个 PlayStation Fives <span class="footnote-reference" role="doc-noteref" id="fnrefhnjoubiko19"><sup><a href="#fnhnjoubiko19">[22]</a></sup></span></p><p> （&lt; 所有 PS5 的 1%） </p></td></tr><tr><td style="background-color:rgb(217, 217, 217);border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1">训练期间设备之间的通信是...... </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> >; 2022 年所有互联网流量<span class="footnote-reference" role="doc-noteref" id="fnrefnkgum7hxvvp"><sup><a href="#fnnkgum7hxvvp">[23]</a></sup></span></td></tr></tbody></table></figure><p></p><p>通过将 ML GPU 的每 FLOP 成本与某些非 ML GPU 的每 FLOP 成本进行比较，可以最直接地看出这一点<span class="footnote-reference" role="doc-noteref" id="fnrefk4fgsadnpen"><sup><a href="#fnk4fgsadnpen">[24]</a></sup></span> 。例如，可能用于训练 GPT-4（NVIDIA 的 A100）的 GPU 的每次浮点运算成本约为 NVIDIA 最新游戏 GPU 的 10 倍。下表对 A100 和 H100（最先进的 ML GPU 和 A100 的后继产品）与 RTX 4090（NVIDIA 最好的游戏 GPU）进行了比较。 </p><figure class="table"><table><tbody><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://www.nvidia.com/en-us/data-center/a100/"><i><u>NVIDIA A100 80GB SXM</u></i></a> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://resources.nvidia.com/en-us-tensor-core"><i><u>NVIDIA H100 SXM</u></i></a> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> <a href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf"><i><u>NVIDIA GeForce RTX 4090</u></i></a> </td></tr><tr><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>成本</i></td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 15,000 美元<span class="footnote-reference" role="doc-noteref" id="fnrefd44dt0b1ped"><sup><a href="#fnd44dt0b1ped">[25]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 24,000 美元<span class="footnote-reference" role="doc-noteref" id="fnrefqxa8iken5ms"><sup><a href="#fnqxa8iken5ms">[26]</a></sup></span> </td><td style="border:1pt solid rgb(0, 0, 0);padding:5pt;vertical-align:top" colspan="1" rowspan="1"> $1,599 <span class="footnote-reference" role="doc-noteref" id="fnref1x7kr5b5c7"><sup><a href="#fn1x7kr5b5c7">[27]</a></sup></span></td></tr><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>机器学习处理能力</i><span class="footnote-reference" role="doc-noteref" id="fnref32invmffy99"><sup><a href="#fn32invmffy99">[28]</a></sup></span></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p>相当于 624 TFLOP/s <span class="footnote-reference" role="doc-noteref" id="fnref8diwdiwt6es"><sup><a href="#fn8diwdiwt6es">[29]</a></sup></span></p><p> （如果无法进行 int8 训练，则为 312 TFLOP/s）</p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 1978.9 TFLOP/秒</td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1">660.6 TFLOP/秒</td></tr><tr><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><i>成本/TFLOP/秒</i></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"><p>24.04 美元</p><p>（如果无法进行 int8 训练，则为 48.08 美元）</p></td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1"> 12.13 美元</td><td style="padding:5pt;vertical-align:top" colspan="1" rowspan="1">2.42 美元</td></tr></tbody></table></figure><p></p><p>我在<a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.e1ayykcf3smu"><u>《为什么 ML GPU 成本如此之高》</u></a>中详细讨论了这些 GPU 成本的实际驱动因素。</p><p>除了单个芯片之外，ML 训练运行中还存在其他重大成本 - <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.iwhjtdndhoxw"><u>我的最佳猜测</u></a>是，GPU 仅占目前前沿模型训练运行所需硬件成本的 70%，其中网络设备占据了大部分剩下的30%。随着 ML 超级计算机规模的进一步增长，由于规模问题，非 GPU 成本可能会超过 GPU 成本。</p><h2>可能的解决方案</h2><p>美国<a href="https://www.federalregister.gov/documents/2022/10/13/2022-21658/implementation-of-additional-export-controls-certain-advanced-computing-and-semiconductor"><u>禁止</u></a>向中国出口先进机器学习芯片，采用了一种根据所涉及值的长度对 FLOP 进行标准化的方法。据我了解，在实践中，这相当于将用于训练的所有类型的 FLOP 基本相同地对待，而一些用于推理的 FLOP 的价值只是训练 FLOP 的一小部分。 <span class="footnote-reference" role="doc-noteref" id="fnref9lkmezu33j"><sup><a href="#fn9lkmezu33j">[30]</a></sup></span>在实践中，<a href="https://www.semianalysis.com/p/how-chinas-biren-is-attempting-to"><u>规避</u></a>禁令的<a href="https://www.tomshardware.com/news/nvidia-gimps-h100-hopper-gpu-to-sell-as-h800-to-china"><u>尝试</u></a>主要集中在另一个基于通信的要求上。</p><p>一般来说，很难弄清楚哪些类型的 FLOP 可用于训练前沿模型 - FLOP 的类型多种多样，并且它们的可用性可能会迅速变化 - 直到 2022 年，人们还不知道如何训练 GPT -具有传统 FP16 精度 FLOP 的 3 尺寸模型，但<a href="https://arxiv.org/abs/2209.05433"><u>最近在仅使用 FP8 的该尺寸训练模型方面取得了进展</u></a>。</p><p>解决专业化问题的另一种方法是搜索基于硬件更基本和稳定方面的指标。此类指标的示例可能是晶体管小时数、能源使用量或单位逻辑运算。这些指标的优点是拥有大量可以分析预测的历史数据。缺点是需要进行单独的工作来应对专业化如何与之相互作用的问题。</p><p>目前我对这些方法都不完全满意，并认为这是一个悬而未决的问题，对预测和监管具有重要影响。</p><h1>机器学习并行性</h1><p><i>进一步阅读：我在整个过程中链接了潜在的资源，此外，这里还有一些来自</i><a href="https://openai.com/research/techniques-for-training-large-neural-networks"><i><u>OpenAI</u></i></a> <i>、</i> <a href="https://huggingface.co/blog/bloom-megatron-deepspeed"><i><u>HuggingFace</u></i></a><i>和</i><a href="https://fathomradiant.co/posts/Deep-Learning-with-Trillions-of-Parameters:-The-Interconnect-Challenge"><i><u>Fathom Radiant</u></i></a>的 ML 并行性的更高级别概述<i>。</i></p><p> “ML 是<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel"><u>令人尴尬的并行</u></a><span class="footnote-reference" role="doc-noteref" id="fnref1nq0sesd5pa"><sup><a href="#fn1nq0sesd5pa">[31]</a></sup></span> ”这一常见概念过于简单化，并且在大规模情况下就会失效 - 任何简单的模型并行化方法都会开始遇到瓶颈，因为无论涉及多少设备，单个设备的功能都会成为瓶颈。本节概述了并行方法，包括对它们遇到的各种瓶颈的评论。</p><p>有几种不同的方法可以在越来越多的设备上并行训练。我将把它们分为两类：垂直并行和水平并行。其他来源以不同的方式划分事物，但我认为我的方法更容易理解，并且更好地捕捉机器学习中并行性的基础知识。</p><h2>垂直并行度</h2><p><strong>垂直并行性</strong>通过添加设备进行扩展，其方式类似于通过添加工人来扩展装配线，其中生产线中的每个工人将正在生产的项目传递给下一个工人。虽然这可能不会大大加快单个物品的生产速度，但通过在装配线上同时放置多个物品，可以提高整体吞吐量。 </p><figure class="image image_resized" style="width:135.5px"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/tix6zx3q66eii9lxlb9s" alt=""></figure><p>同样，<strong>垂直并行性的</strong>工作原理是增加训练期间数据流经的连续设备的数量。如果天真地这样做，这会导致设备利用率降低，因为一旦数据通过给定设备，设备就会空闲。为了避免这种情况，多个输入可以以流水线方式同时通过该管道运行，仅依靠管道中的每个阶段来完成每个输入所需的一部分工作。这并不会显着提高超过某个点的更新速度，但它可以允许在不大幅减慢的情况下扩展批量大小。<strong>垂直并行性很重要，但不能应用于扩展的所有挑战，因为它对于提高更新速度不是很有用。</strong></p><p>垂直并行的两个例子：</p><ol><li><a href="https://arxiv.org/pdf/1811.06965.pdf"><u>管道模型并行性</u></a>是指将模型的层划分到不同的设备上，每个设备负责计算几个层并向前传递结果<span class="footnote-reference" role="doc-noteref" id="fnreft9sthtdrv6c"><sup><a href="#fnt9sthtdrv6c">[32]</a></sup></span> 。该技术还可以帮助满足每个设备的内存需求，因为每个设备只需要访问整个模型的几个层。</li><li> <a href="https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da"><u>Ring-AllReduce</u></a>通过让初始设备将其值发送到下一个设备，将两个设备相加并将其传递到队列中的下一个设备来完成诸如对分布在设备上的一组数字求和之类的任务。通过同时进行多个求和，可以提高总体吞吐量。这可以用来完成诸如数据或张量并行之类的事情，而没有水平并行的缺点。</li></ol><p>随着规模的扩大，垂直并行性会遇到一些限制 - 层数增长相当缓慢，因此管道模型并行性仅此而已，并且长链设备之间对大量设备间通信的需求可能很难克服，因为通常设备仅在一个封闭的小社区内进行非常快速的通信<span class="footnote-reference" role="doc-noteref" id="fnrefxas9jhm57mn"><sup><a href="#fnxas9jhm57mn">[33]</a></sup></span> 。</p><h2>水平并行度</h2><p><strong>水平并行</strong>类似于让工人同时组装一个物品，通常是将物品分成多个部分，让每个工人在一个零件上工作，然后让所有工人聚集在一起组装最终物品。这加快了项目的完成速度，但随着工人数量的增加，协调如此大的团队的任务变得越来越困难 - 想象一下数十名工人都试图挤在一起将他们的项目拼凑在一起。 <img style="width:407.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/borsdt0nn4r5xooz4f6y" alt=""></p><p>更具体地说，水平并行性正在增加同时处理更大任务的组件的设备数量。这使得模型的更新可以更快地完成，因为模型本身的执行速度会更快。这也不需要更多的内存。然而，<strong>水平并行的通信成本越来越大，因为随着扩展此方法，越来越多的设备需要相互通信</strong>。</p><p>水平并行的几个例子：</p><ol><li>数据并行将批次分成几个小部分并将它们分布在设备之间。一旦设备评估了它们的子部分，结果就会被汇总，然后广播给所有相关的工作人员，以便同步权重。请注意，聚合和广播阶段可以通过 Ring-AllReduce 实现垂直并行，或者通过 Tree-AllReduce 实现水平并行。</li><li><a href="https://colossalai.org/docs/features/1D_tensor_parallel"><u>张量并行</u></a>在不同设备之间分割模型层内的神经元。由于模型的密度需要分布在许多设备上的神经元之间存在大量依赖性，因此这种划分可能会产生非常高的通信成本。然而，<strong>随着模型大小的增长，张量并行性对于提高更新速度非常重要，</strong>因为它可以加快模型单个输入的计算速度，而不仅仅是增加吞吐量。它还可以帮助满足每个设备的内存需求，因为每个设备只需要访问整个模型参数的子集。请注意，部分操作可以使用 Ring-AllReduce 来完成，但您将失去更新速度的改进。</li><li> <a href="https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/"><u>Tree-AllReduce</u></a>完成与 Ring-AllReduce 相同的任务，不同之处在于通过基于树的组织，其中聚合是在一系列阶段中完成的，每个阶段将每个节点聚合的值数量加倍，以对数步数聚合值。</li></ol><p>理论上，水平并行可以相当有效地扩展。然而，随着越来越多的设备需要相互通信，工程挑战变得越来越困难，这与每个设备只需要与其他两个设备通信的垂直并行不同。就像让大量工作人员聚集在单个项目周围的困难一样，让大量 GPU 在一次内快速相互通信可能会带来高昂的成本 - 包括由于快速通信所需的邻近性而导致的物理空间成本。沟通。</p><h1>我们（可能）不会耗尽数据</h1><p>人们相当多地关注当前机器学习方法相当陡峭的数据要求，以及数据可能“耗尽”和瓶颈扩展的可能性<span class="footnote-reference" role="doc-noteref" id="fnref5uzstzvn26k"><sup><a href="#fn5uzstzvn26k">[34]</a></sup></span> 。<strong>我最好的猜测是数据不会有意义地耗尽，但我不确定。</strong>我咨询过的专家提出了各种各样的意见，但大多数人并不认为这是一个主要瓶颈。已经进行了大量研究，可以延迟或防止数据耗尽，包括：</p><ol><li>已经存在但在语言模型中未使用的私有数据提供了丰富的训练数据源。如果尝试这样做，这似乎可能会产生重大争议，尽管谷歌<a href="https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html"><u>已经表现出了兴趣</u></a>，希望通过技术解决方案来解决一些问题，以便在训练后从模型中删除私人数据。</li><li> Using more than just text data is becoming increasingly common: <a href="https://openai.com/research/gpt-4"><u>GPT-4</u></a> was trained on images and Google Deepmind&#39;s upcoming model <a href="https://blog.google/technology/ai/google-palm-2-ai-large-language-model/"><u>Gemini</u></a> will be multimodal as well.</li><li> Making better use of existing data can alleviate issues, for example by <a href="https://arxiv.org/abs/2205.10487"><u>removing unintentionally repeated data</u></a> or <a href="https://arxiv.org/pdf/2305.16264.pdf"><u>training for multiple epochs</u></a> , as GPT-4 is <a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure"><u>reported</u></a> to have done.</li><li> Solutions drawing on reinforcement learning to derive data from playing games and attempting challenges has shown some promise, with the multimodal model <a href="https://www.deepmind.com/blog/a-generalist-agent"><u>Gato</u></a> from Deepmind using this alongside other forms of data.</li><li> Training via self-correction may be viable as models become more sophisticated. Google AI has done some <a href="https://arxiv.org/abs/2210.11610"><u>work</u></a> on this, and Anthropic has used self-correction for alignment via <a href="https://arxiv.org/abs/2212.08073"><u>Constitutional AI</u></a> .</li></ol><p> Also note that running out of data doesn&#39;t have to be binary - it&#39;s possible that these methods enable continued growth in model performance but at a slower rate than natural data would allow. Alternatively, they could allow faster growth in model performance - training via self-correction or reinforcement learning based approaches seem plausibly superior to me once models have achieved a minimal level of capabilities.</p><h1> AI Energy Use and Heat Signatures</h1><p> Energy use gets brought up a lot when discussing the recent AI explosion, despite the fact that it accounts for a very small fraction of <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.90ke0ggz3472"><u>the cost of training a frontier model</u></a> , and a very small fraction of the US&#39;s energy usage <span class="footnote-reference" role="doc-noteref" id="fnref2yp1bm54zbv"><sup><a href="#fn2yp1bm54zbv">[35]</a></sup></span> . Despite this, it turns out energy might actually be super important to the future of ML, limiting scaling, determining major engineering challenges, and providing a novel approach to surveillance of training runs. This is due in large part due to energy requirements growing and the required density of ML supercomputers.</p><h2>生长</h2><p>So far, it seems unlikely that ML supercomputers have very different energy requirements from normal data centers used by the likes of Google - both requiring perhaps in the low tens of megawatts of power. This usage has remained consistent for supercomputers at least over the last 25 years - there hasn&#39;t been much change. Supercomputer performance has mostly come from increasing performance per watt, not increasing the number of watts used (see the chart in <a href="https://www.nextplatform.com/2023/07/10/lining-up-the-el-capitan-supercomputer-against-the-ai-upstarts/">this article</a> ). However it seems really likely that ML will drive a huge increase in the size of the individual supercomputers used for training them which, according to some but not all experts, could result in supercomputers within the next five years that require gigawatts of power (assuming on the order of a hundred billion dollars of spending on individual supercomputers, which is quite the extrapolation to make). This is similar to <a href="https://climate.cityofnewyork.us/subtopics/systems/%23:~:text%3DOn%2520average%252C%2520NYC%2520uses%2520about,as%2520much%2520as%252010%252C000%2520megawatts.">the power usage of New York City</a> . The unavailability of this much power could play a significant role in ML in the near future,</p><h2> Density</h2><p> ML training runs require huge amounts of communication which becomes harder and requires more energy the farther devices are from each other. As a result, we should expect ML supercomputers to stay in a relatively small geographic area. Fitting cooling within a small volume will also be hard, and in general getting rid of heat is a non-trivial challenge.</p><h2> Heat Based Satellite Surveillance of ML Training Runs</h2><p> The amount of heat given off by this much energy usage is significant, concentrated in one place, and consistent over time due to training runs running 24/7 for months. As a result, you can probably see them in infrared satellite images even now, and in the near future they may be fairly distinguishable from basically anything else! This provides a novel way to do surveillance on ML training runs done around the world. </p><p><img style="width:236.5px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nXcHe7t4rqHMjhzau/gmppzyugttldxjd789ns" alt=""></p><p> Here&#39;s a satellite picture I found on <a href="https://app.skyfi.com/explore?aoi%3DPOLYGON%2528%2528-95.33736454265205%2B36.23292459154637%252C-95.3124846913423%2B36.23292459154637%252C-95.3124846913423%2B36.253075996331766%252C-95.33736454265205%2B36.253075996331766%252C-95.33736454265205%2B36.23292459154637%2529%2529"><u>SkyFi</u></a> of a data center taken with multispectral photography with the datacenter circled. I&#39;ve adjusted the contrast and brightness. I&#39;m uncertain how much of the bright spot is due to IR and how much is due to the datacenter having a light gray roof - I&#39;d need to pay 250 dollars to get access to the higher res originals to get a better sense of what&#39;s going on here.</p><h2> Other Distinguishing Features</h2><p> There are a few things like some aluminum smelters which may use similar amounts of energy but factors like the lack of material input/output, may serve to distinguish ML supercomputers.</p><p> Similarly, bitcoin mining relies substantially on minimizing cost of energy to turn a profit, resulting in miners often using excess energy in locations rather than requiring a constant supply of energy for a long period of time as ML training does. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnldni1dmval"> <span class="footnote-back-link"><sup><strong><a href="#fnrefldni1dmval">^</a></strong></sup></span><div class="footnote-content"><p> Google&#39;s Gemini may join this class when it is released, and Claude-Next as well.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn86mrkdwudw8"> <span class="footnote-back-link"><sup><strong><a href="#fnref86mrkdwudw8">^</a></strong></sup></span><div class="footnote-content"><p> <a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure"><u>Semianalysis</u></a> says 63 and <a href="https://epochai.org/trends"><u>Epoch</u></a> says 40. I trust Semianalysis more on this.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7rd3o585ve6"> <span class="footnote-back-link"><sup><strong><a href="#fnref7rd3o585ve6">^</a></strong></sup></span><div class="footnote-content"><p> I use the estimates I make in <a href="https://docs.google.com/document/d/e/2PACX-1vT94P_qrkYq5D5rD2tlNDD3I3mnfzNOhMSNIemp3-KdE1e7qSEsvXRZQv7a3_Z1fVLxc7tltNgN2Tcp/pub#h.iwhjtdndhoxw"><u>ML Hardware Cost</u></a> as well as an assumption that there were 25k A100 GPUs, each costing approximately $15k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf8iigdgmfa"> <span class="footnote-back-link"><sup><strong><a href="#fnreff8iigdgmfa">^</a></strong></sup></span><div class="footnote-content"><p> To a decent extent I&#39;m reasoning from the lack of evidence for significant data-associated costs. One possible way I could be wrong is if OpenAI were actually paying large amounts to license copyrighted data, but I have not seen any evidence of this.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnn04f3250v5"> <span class="footnote-back-link"><sup><strong><a href="#fnrefn04f3250v5">^</a></strong></sup></span><div class="footnote-content"><p> I computed power draw costs as a percentage of the cost of an A100 (which I guessed at being 15k) and also tried using 25k A100s as Semianalysis reports, treated them as running for the full year to account for experiments, and used <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf"><u>this</u></a> , <a href="https://www.colfax-intl.com/nvidia/nvidia-dgx-a100"><u>this</u></a> and <a href="https://www.top500.org/system/180073/"><u>this</u></a> to estimate power draw of the whole system.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnunidulxrjqe"> <span class="footnote-back-link"><sup><strong><a href="#fnrefunidulxrjqe">^</a></strong></sup></span><div class="footnote-content"><p> Relevant math is 1-195,000/((269,010-42,000) * (5/4))</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqts2glqvljo"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqts2glqvljo">^</a></strong></sup></span><div class="footnote-content"><p> See the estimate by Epoch <a href="https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit%23gid%3D1503579905"><u>here</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn70prgwf7vbi"> <span class="footnote-back-link"><sup><strong><a href="#fnref70prgwf7vbi">^</a></strong></sup></span><div class="footnote-content"><p> This is my best guess, based on the cost of 8 H100s (and additional networking equipment) in <a href="https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is"><u>this breakdown</u></a> of the cost of  DGX H100. I&#39;m very confident the cost is within the interval $10k-$45k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn3vzbxieq0wf"> <span class="footnote-back-link"><sup><strong><a href="#fnref3vzbxieq0wf">^</a></strong></sup></span><div class="footnote-content"><p> See <a href="https://www.pcmag.com/news/nvidia-rtx-4090-will-cost-1599-rtx-4080-starts-at-899."><u>this article</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf2409qnb59q"> <span class="footnote-back-link"><sup><strong><a href="#fnreff2409qnb59q">^</a></strong></sup></span><div class="footnote-content"><p> I use FP8 without sparsity, or int8 without sparsity for the A100 because it lacks FP8 and it seems <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Towards_Unified_INT8_Training_for_Convolutional_Neural_Network_CVPR_2020_paper.pdf"><u>plausible</u></a> int8 works for training.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnn3r0k58yzvp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefn3r0k58yzvp">^</a></strong></sup></span><div class="footnote-content"><p> I use whatever the fastest form of interconnect is.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnyvhsu3o0aof"> <span class="footnote-back-link"><sup><strong><a href="#fnrefyvhsu3o0aof">^</a></strong></sup></span><div class="footnote-content"><p> See <a href="https://www.techpowerup.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/"><u>here</u></a> , sum up both directions to get the total.</p></div></li><li class="footnote-item" role="doc-endnote" id="fneqmm2whzz6u"> <span class="footnote-back-link"><sup><strong><a href="#fnrefeqmm2whzz6u">^</a></strong></sup></span><div class="footnote-content"><p> See footnote in the <a href="https://www.nvidia.com/en-us/data-center/a100/"><u>doc</u></a> about custom thermal solutions.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnbpd8hfx7gaa"> <span class="footnote-back-link"><sup><strong><a href="#fnrefbpd8hfx7gaa">^</a></strong></sup></span><div class="footnote-content"><p> Based on <a href="https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage"><u>this</u></a> , if the 80GB of memory of an H100 or A100 was standard computer memory it would cost only $164, and if it were hard drive storage it would cost $1.12, The difference in price between these forms of memory, and the one used GPUs, is due to the memory bandwidth - data can be read from and written to the memory of a GPU much much faster than it can for a hard disk. I think of this as a function of internal communication within the GPU, not as a function of the RAM.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn891plqjefd5"> <span class="footnote-back-link"><sup><strong><a href="#fnref891plqjefd5">^</a></strong></sup></span><div class="footnote-content"><p> Misspelled in the chart.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn04t0i9i082tm"> <span class="footnote-back-link"><sup><strong><a href="#fnref04t0i9i082tm">^</a></strong></sup></span><div class="footnote-content"><p> Google&#39;s new TPUv5e chip, and possible upcoming TPUv5 chips, may be comparable to the H100, but the TPUv5e was announced during the final stages of polishing this doc so I do not account for this.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnj1wd590x17"> <span class="footnote-back-link"><sup><strong><a href="#fnrefj1wd590x17">^</a></strong></sup></span><div class="footnote-content"><p> See the <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines"><u>Bio Anchors report</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnf0m7mc9r5a5"> <span class="footnote-back-link"><sup><strong><a href="#fnreff0m7mc9r5a5">^</a></strong></sup></span><div class="footnote-content"><p> See much of the work of <a href="https://epochai.org/"><u>Epoch</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnlqvp92cu9y"> <span class="footnote-back-link"><sup><strong><a href="#fnreflqvp92cu9y">^</a></strong></sup></span><div class="footnote-content"><p> The SOTA ML GPU, NVIDIA&#39;s <a href="https://resources.nvidia.com/en-us-tensor-core"><u>H100</u></a> , can perform non-sparse tensor flops of the smallest format (FP8) at 30x the speed of FP32.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfwu1mnychs"> <span class="footnote-back-link"><sup><strong><a href="#fnreffwu1mnychs">^</a></strong></sup></span><div class="footnote-content"><p> Throughout I am using estimates made by <a href="https://epochai.org/trends"><u>Epoch</u></a> and this article by <a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure"><u>Semianalysis</u></a> - both are good sources with very different specialities.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7inu0b4b7z3"> <span class="footnote-back-link"><sup><strong><a href="#fnref7inu0b4b7z3">^</a></strong></sup></span><div class="footnote-content"><p> Epoch suggests 12-20 trillion tokens if GPT-4 involved as much compute as estimated and follows Chinchilla scaling laws, and Semianalysis reports 13 trillion but less than 7.5 trillion unique tokens. I use 13 trillion. Standard tokenization techniques would imply fewer than 2 bytes per token, which gives us 26TB of data. Cheapest MacBook Air M2 comes with 256GB of storage, giving us 104 which I round to 100. I ignore image data in this analysis, as less research has been done on this and GPT-4&#39;s image capabilities are as of yet unreleased.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnhnjoubiko19"> <span class="footnote-back-link"><sup><strong><a href="#fnrefhnjoubiko19">^</a></strong></sup></span><div class="footnote-content"><p> Epoch gives ~2e25 FLOPs of compute used for training and Semianalysis agrees. The PlayStation 5 is <a href="https://www.theverge.com/21450334/playstation-5-ps5-sony-news-price-features-specs-hardware-games"><u>reported</u></a> to have 10.28 TFLOP/s of compute which gives ~250,200 necessary to have enough compute in 3 months. Sony has<a href="https://www.theverge.com/2023/4/28/23702131/sony-ps5-q4-2022-console-sales-game-shipments"><u>reportedly</u></a> sold at least 38.4 million PS5s, implying this is less than &lt;1% of PlayStations that have been sold.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnnkgum7hxvvp"> <span class="footnote-back-link"><sup><strong><a href="#fnrefnkgum7hxvvp">^</a></strong></sup></span><div class="footnote-content"><p> The real number is quite likely higher. Semianalysis and Epoch give different numbers here, for my best guess I&#39;m using Semianalysis&#39; numbers since they seem to have more information available and better match what I&#39;ve heard elsewhere. They give about 25k A100 GPUs, which, based on available interconnect and what Semianalysis suggests, would indicate the capability to have at least 5 petabytes a second of aggregate interconnect bandwidth. I think around half of this was likely used given ML parallelization techniques - giving maybe 2.5 petabytes a second. Over the course of a likely at least 90 day training run (estimated by Epoch and Semianalysis) we get a total of 19.4 zettabytes (1.94e22). Cisco estimated 3.5 zettabytes (3.5e21) for <a href="https://www.cisco.com/c/dam/m/en_us/solutions/service-provider/vni-forecast-highlights/pdf/Global_Device_Growth_Traffic_Profiles.pdf"><u>2022 consumer internet traffic</u></a> (when the monthly amount is multiplied by 12x) in their 5 year projections as of 2017, I&#39;m having trouble finding more recent numbers.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnk4fgsadnpen"> <span class="footnote-back-link"><sup><strong><a href="#fnrefk4fgsadnpen">^</a></strong></sup></span><div class="footnote-content"><p> NVIDIA likely has much higher margins on ML GPUs, which means these numbers could very well change in the future as competition increases.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnd44dt0b1ped"> <span class="footnote-back-link"><sup><strong><a href="#fnrefd44dt0b1ped">^</a></strong></sup></span><div class="footnote-content"><p> See the estimate by Epoch <a href="https://docs.google.com/spreadsheets/d/1NoUOfzmnepzuysr9FFVfF7dp-67OcnUzJO-LxqIPwD0/edit%23gid%3D1503579905"><u>here</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqxa8iken5ms"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqxa8iken5ms">^</a></strong></sup></span><div class="footnote-content"><p> This is my best guess, based on the cost of 8 H100s (and additional networking equipment) in <a href="https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is"><u>this breakdown</u></a> of the cost of  DGX H100. I&#39;m very confident the cost is within the interval $10k-$45k.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1x7kr5b5c7"> <span class="footnote-back-link"><sup><strong><a href="#fnref1x7kr5b5c7">^</a></strong></sup></span><div class="footnote-content"><p> See <a href="https://www.pcmag.com/news/nvidia-rtx-4090-will-cost-1599-rtx-4080-starts-at-899."><u>this article</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn32invmffy99"> <span class="footnote-back-link"><sup><strong><a href="#fnref32invmffy99">^</a></strong></sup></span><div class="footnote-content"><p> I use FP8 without sparsity, or int8 without sparsity for the A100 because it lacks FP8 and it seems plausible int8 works for training, see Appendix E of <a href="https://arxiv.org/pdf/2208.07339.pdf"><u>this</u></a> and also <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Towards_Unified_INT8_Training_for_Convolutional_Neural_Network_CVPR_2020_paper.pdf"><u>this paper</u></a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fn8diwdiwt6es"> <span class="footnote-back-link"><sup><strong><a href="#fnref8diwdiwt6es">^</a></strong></sup></span><div class="footnote-content"><p> Technically int8 operations are not FLOPs, because int8 isn&#39;t a floating point data type, but are being assumed to be basically equivalent for the purposes of ML training.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn9lkmezu33j"> <span class="footnote-back-link"><sup><strong><a href="#fnref9lkmezu33j">^</a></strong></sup></span><div class="footnote-content"><p> The ban works by multiplying the amount of FLOPs of a given data type by the bit length of the data types involved. However, ML training typically requires a portion of each operation be done with FP32, even if the inputs are smaller data types and intermediate values are too, which results in the ban treating all such FLOPs the same.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1nq0sesd5pa"> <span class="footnote-back-link"><sup><strong><a href="#fnref1nq0sesd5pa">^</a></strong></sup></span><div class="footnote-content"><p> Embarrassingly parallel is a term for computational tasks that can be easily divided into sub-tasks that can be accomplished by independent devices.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt9sthtdrv6c"> <span class="footnote-back-link"><sup><strong><a href="#fnreft9sthtdrv6c">^</a></strong></sup></span><div class="footnote-content"><p> The need to do backpropagation for learning presents a challenge here, requiring forward passes to be followed by backward ones for computing what the model got wrong for each input. It&#39;s not trivial to schedule these passes in a way that prevents collisions and it inevitably results in some amount of idle time for the devices (referred to as a “bubble”). See the section on pipeline parallelism in this <a href="https://huggingface.co/blog/bloom-megatron-deepspeed"><u>post</u></a> by Hugging Face to learn more.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxas9jhm57mn"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxas9jhm57mn">^</a></strong></sup></span><div class="footnote-content"><p> Within node communication is generally much faster than other communication.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn5uzstzvn26k"> <span class="footnote-back-link"><sup><strong><a href="#fnref5uzstzvn26k">^</a></strong></sup></span><div class="footnote-content"><p> Epoch&#39;s <a href="https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset"><u>report</u></a> on this is a prominent example.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn2yp1bm54zbv"> <span class="footnote-back-link"><sup><strong><a href="#fnref2yp1bm54zbv">^</a></strong></sup></span><div class="footnote-content"><p> I got a quick upper bound of a fraction of a percent. A brief sketch of how I did this: multiplying <a href="https://www.techpowerup.com/gpu-specs/a100-pcie-80-gb.c3821%23:~:text%3DBeing%2520a%2520dual%252Dslot%2520card,have%2520monitors%2520connected%2520to%2520it."><u>the energy usage of an ML GPU</u></a> from NVIDIA by <a href="https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2024%23:~:text%3Din%2520these%2520areas%253A-,Data%2520Center,18%2525%2520from%2520the%2520previous%2520quarter"><u>the revenue of the data center division</u></a> (which they are under) divided by <a href="https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html"><u>the cost of an ML GPU</u></a> and comparing that to <a href="https://www.worlddata.info/america/usa/energy-consumption.php"><u>US energy usage</u></a> . I consider the ML estimate to be very likely far too high and the real number to be even lower.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/nXcHe7t4rqHMjhzau/report-on-frontier-model-training#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nXcHe7t4rqHMjhzau/report-on-frontier-model-training<guid ispermalink="false"> nXcHe7t4rqHMjhzau</guid><dc:creator><![CDATA[YafahEdelman]]></dc:creator><pubDate> Wed, 30 Aug 2023 20:02:51 GMT</pubDate> </item><item><title><![CDATA[An adversarial example for Direct Logit Attribution: memory management in gelu-4l]]></title><description><![CDATA[Published on August 30, 2023 5:36 PM GMT<br/><br/><p><i>请查看</i><a href="https://colab.research.google.com/drive/16Kp-4iH330a1dF6F0ntPK7kfNuxqkvfZ#scrollTo=0WpBUiAYkEdv"><i><u>我们的笔记本</u></i></a><i>进行图形重建，并检查您自己的模型的清理行为。</i></p><p><i>作为</i><a href="https://www.arena.education/"><i><u>ARENA 2.0</u></i></a><i>和 SERI ML 对齐理论学者计划 - 2023 年春季队列</i>的一部分而制作</p><h1>概述</h1><p>在这篇文章中，我们为 4 层变压器<a href="http://neelnanda.io/toy-models"><u>gelu-4l</u></a>中的内存管理或清理提供了具体证据。我们展示了直接 Logit 归因 (DLA) 具有误导性的示例，因为它没有考虑清理工作。</p><p><br>在引言中，我们定义了清理行为的含义，并快速回顾了 DLA。在清理行为的证据部分中，我们确定了从残余流中写入和删除信息的特定节点。根据我们对清理工作的了解，我们在直接 Logit 归因部分的含义中选择了导致误导性 DLA 结果的提示。</p><h1>介绍</h1><h2>清理行为</h2><p>之前在<a href="https://transformer-circuits.pub/2021/framework/index.html#d-footnote-7:~:text=Perhaps%20because%20of%20this%20high%20demand%20on%20residual%20stream%20bandwidth%2C%20we%27ve%20seen%20hints%20that%20some%20MLP%20neurons%20and%20attention%20heads%20may%20perform%20a%20kind%20of%20%22memory%20management%22%20role%2C%20clearing%20residual%20stream%20dimensions%20set%20by%20other%20layers%20by%20reading%20in%20information%20and%20writing%20out%20the%20negative%20version."><u>《变压器电路的数学框架》</u></a>中，作者提出了一种内存管理机制，并推测它的出现是因为对剩余流带宽的高需求。我们定义清理行为，其中注意力头和 MLP（我们统称为节点）从仅在网络早期层使用的残余流中清除信息。</p><p>我们将前向传递过程中的清理行为描述为四个步骤：</p><ol><li>写入器节点或嵌入将特定方向写入残差流</li><li>后续节点使用该方向进行进一步计算</li><li>清理节点通过将其负值写入残余流来清除该方向</li><li>该方向在模型的后面部分中以以下一种或两种方式使用：<ol><li>后续节点向该方向写入，使用该空闲子空间来传递与已清理信息不同的信息</li><li>去嵌入矩阵读取方向，直接影响输出logits</li></ol></li></ol><p>在这篇文章中，我们分析了写入器节点输出的删除（步骤 1 - 3）。在未来的工作中，我们将解决清理后后续节点如何使用已清理的空间（步骤 4）。</p><h2>直接 Logit 归因</h2><p><i>背景：</i>残差流的最终状态是模型中节点和嵌入的所有输出的总和<span class="footnote-reference" role="doc-noteref" id="fnrefmhfccldh2xf"><sup><a href="#fnmhfccldh2xf">[1]</a></sup></span> 。通过应用 Layer Norm 和 Unembedding 将残差流的最终状态映射到 logit 分布。两个标记的 logit 差异相当于两个标记的对数概率之差，因此可以直接解释为预测下一个标记。</p><p><a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ"><u>直接 Logit 归因 (DLA)</u></a>已用于识别各个节点对正确的下一个标记的预测的直接贡献<span class="footnote-reference" role="doc-noteref" id="fnrefmadpxyr44ml"><sup><a href="#fnmadpxyr44ml">[2]</a></sup></span> 。这是通过在考虑层范数后将去嵌入直接应用于任何节点的输出来完成的。例如，单个节点对令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{A}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span></span></span></span></span></span>进行预测<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>如果它增加了 logit 差值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{logit}_\text{A} - \text{logit}_\text{B}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">logit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">A</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">logit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">B</span></span></span></span></span></span></span></span></span> ，那么它比 token <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{B}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">B</span></span></span></span></span></span></span>的预测更有可能。然而，DLA 没有考虑到后面的节点依赖于前面的节点的输出这一事实。清理行为是 DLA 可能产生误导的可能原因之一（尤其是对于早期节点），因为节点输出可能会被后续节点持续清理。</p><h1>清理行为的证据</h1><h2>指标和术语</h2><p>我们引入<strong>投影比</strong>来比较残差流中的方向被覆盖的程度。 </p><p><img style="width:79.23%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/uocv2iapav5tcwqnkbfs"></p><p><img style="width:79.59%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/gtggzshscn8xquymrqnp"></p><p>我们使用符号“L0H2”来表示位于第 0 层、头索引为 2 的注意力头。第 2 层中的所有头统称为“L2HX”。在本节中，我们始终将写入器头 L0H2 的输出作为向量<strong>b</strong> ，而向量<strong>a</strong>将被残差流或清理头的输出替换。</p><h2>识别写入节点：头 L0H2 的输出正在被持续清理</h2><p>首先，我们扫描了完整的 gelu-4l 模型以获得一致的清理行为。对于每个节点，我们检查节点的输出是否存在于残差流的后续状态中。我们通过将每个注意力和 MLP 层之前和之后的残余流投影到 L0H2 的输出方向来测量清理情况。直观的理解：当将<i>第2层之后的残差流的状态</i>投影到<i>L0H2</i>上时，投影比代表L0H2的输出有多少仍然存在于第2层之后的残差流中。如果投影比为零，则残差流是正交的到L0H2的输出。换句话说，L0H2的输出方向不存在于残余流中。它可能已被移动到另一个子空间或完全从残留流中清除。<br></p><p>残余流到 L0H2 的投影如图 1（上）所示。我们发现 L0H2 的输出在 300 次前向传递中始终得到清理。在图 1（底部）中，我们可以在残差流通过 Transformer 模型时跟踪 L0H2 信息的存在情况：</p><ul><li>最初，我们在 resid_pre0 处看到 ~0 的投影比，因为 L0H2（位于 resid_pre0 和 resid_mid0 之间）尚未写入残差流</li><li>L0H2 写入残差流（位于 resid_mid0）后，投影比变为 ~1。它不完全是 1，因为第 0 层中其他头的输出可能不完全正交于 L0H2 的输出</li><li>在resid_mid0之后，~1的投影比表明L0H2的信息存在于残差流中，直到resid_post1（包括）</li><li>第 2 层中的注意力头（位于 resid_post1 和 resid_mid2 之间）似乎删除了 L0H2 最初写入的信息，导致投影比率小得多，约为 0。这种情况在 300 个提示（从模型的训练数据集中随机采样）中一致发生，但不同序列位置之间存在一些差异。</li></ul><p> L0H2的功能尚不完全清楚，但它类似于位置信息头<span class="footnote-reference" role="doc-noteref" id="fnrefjagxl19of0h"><sup><a href="#fnjagxl19of0h">[3]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefrdy8t8ebtpr"><sup><a href="#fnrdy8t8ebtpr">[4]</a></sup></span> 。 </p><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/s3yohrfhpmwvlpovc1vi"><figcaption>图1：各个位置的残差流在注意力层0中各个头的输出方向上的投影。“resid_mid0”指的是第0层中MLP之前的残差流位置，“resid_post0”指的是残差流位置在layer0中的MLP之后。使用跨批次 (n=300) 和位置 (n=1024) 的中位数来聚合投影比率。底部子图的阴影区域代表第 25 和 75 分位数。</figcaption></figure><h2>识别清洁工：6个2层注意力头正在清理L0H2</h2><p>我们发现六个注意力头（L2H2、L2H3、L2H4、L2H5、L2H6、L2H7）正在清理 L0H2 的输出。在下面的图 2 中，我们看到前面提到的注意力头具有一致的负投影比，这意味着它们正在以 L0H2 的相反方向写入残差流。我们认为，误差线所看到的大部分差异是由于清理行为对位置敏感而不是对提示敏感所致。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/c3tov6dedg60u7dmc8fw"><figcaption>图 2：所有注意力头和 MLP 到 L0H2 输出方向的投影。投影比率使用提示中位数 (n=300) 和位置 (n=1024) 进行聚合，误差线位于第 25 和第 75 分位数。 L0H2 被有意省略。 6个头的投影比之和的中位数为-0.903。</figcaption></figure><h2>验证因果关系：清理行为取决于写入器输出</h2><p>我们通过将残余流修补为第 2 层注意力头的<a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=CLmGoD1pvjmsg0dPyL3wkuGS"><u>OV 电路的</u></a>输入来验证清理头和写入头之间的因果关系。 OV 电路负责注意头<span class="footnote-reference" role="doc-noteref" id="fnrefmhfccldh2xf"><sup><a href="#fnmhfccldh2xf">[1]</a></sup></span>将哪些方向写入剩余流。通过修补 OV 电路，我们检查 L2HX 头在不存在 L0H2 的情况下将哪些信息写入剩余流。</p><p>我们重复<i>识别写入器节点</i>（图 1）、<i>识别清理器</i>（图 2）部分的实验，并比较清理运行和修补运行之间的结果。在修补的运行中，我们通过减去 L0H2 的输出来更改第 2 层中每个头的值输入 ( <a href="https://github.com/neelnanda-io/TransformerLens/blob/0d2827ecce4ef17b86060bdaaaaf50e724684085/transformer_lens/components.py#L943-L975"><u>hook_v_input</u></a> )。这相当于<i>仅对</i>第 2 层中的关注头的 OV 电路进行零消融 L0H2。变压器中的所有其他组件仍将“看到”L0H2 的原始输出。</p><p>图 3a 显示，在修补运行中，在第 2 层的注意力块之后，残余流到 L0H2 的投影率仍然很高，这表明清理的很大一部分确实依赖于输入。图 3b 比较了每个单独头的修补运行和干净运行。当清洁头没有“看到”L0H2 的输出时，清洁行为就会消失。 </p><p></p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/wobnrn0ncwoe5vtcoipy"><figcaption>图 3a：L0H2 仅在输入第 2 层注意力头的 OV 电路时才会被清理。修补是指从第 2 层注意力头的值输入中减去 L0H2 的输出。使用跨批次的中值来聚合投影比（n=300） ) 和位置 (n=1024)，阴影区域位于第 25 和 75 分位数。 </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/cqkt4t97nl2jouzwmlku"><figcaption>图 3b：修补是指从第 2 层注意力头的值输入中减去 L0H2 的输出。使用跨批次 (n=300) 和位置 (n=1024) 的中位数来聚合投影比，误差线位于第 25 和第 75 分位数。</figcaption></figure><h1>对直接 Logit 归因的影响</h1><p>如果清理后的输出方向恰好与某些标记的未嵌入方向对齐，则头 L0H2 的 DLA 值较高。然而，如图 1 所示，L0H2 的输出方向在第 2 层的注意力块之后从残差流中很大程度上被移除。应用于 L0H2 的 DLA 并没有考虑到这种移除，并且它可能高估了 L0H2 对 logit 差异的贡献。因此，由于清理行为的存在，应用于 L0H2 的 DLA 很容易被误解。</p><h2>随机排列后不变的 DLA 值</h2><p>head L0H2 的输出对于任何给定位置的标记大致不变<span class="footnote-reference" role="doc-noteref" id="fnref7taiau8kfhc"><sup><a href="#fn7taiau8kfhc">[5]</a></sup></span> ，因此我们认为 L0H2 的输出主要包含位置信息。因此，当通过重采样消融改变标记信息时，我们仍然期望 L0H2 的类似输出。由于对于相同长度的不同提示，位置信息保持相同，因此 L0H2 的 DLA 值对于比较各个提示没有意义。</p><p>为了证明这一点，我们构建了四个对抗性提示，并指定了正确和错误的下一个标记预测。对抗性地选择正确的标记和提示的长度以产生 L0H2 的高 DLA 值。我们在干净运行和重采样消融运行中计算 DLA。在重采样消融运行中，我们使用随机提示在层标准化后修补 L0H2 输入的最后位置。</p><p>在图 4a 中，干净运行和重新采样消融运行显示非常相似的 DLA 值，表明 L0H2 的原始 DLA 具有误导性。如果来自 L0H2 的直接路径对最终输出 logits 做出了有意义的贡献，我们预计对该头的输入进行重新采样会极大地改变其 DLA 值。重采样消融后 DLA 的不变性是 L0H2 特有的。重采样消融后其他头的 DLA 发生显着变化，如图 4b 所示。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/lxiwbzprcbng7z5voziq"><figcaption>图 4a：L0H2 在四个对抗性示例上的直接 Logit 归因，有或没有重采样消融。重采样消融是指使用随机提示在层归一化（按“ln1.hook_normalized”缩放）后修补 L0H2 输入的最后位置。 DLA 重采样消融值使用批次中位数 (n=300) 进行聚合，误差线位于第 25 和第 75 分位数。 DLA 值对应于标记之间的 logit 差异，其中最高 logits 适合句子的上下文，如每个图的标题所示。 </figcaption></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/mvh1vvxv8wmndbzkgzno"><figcaption>图 4b：四个对抗性示例上重要注意力头的直接 Logit 归因，有或没有重采样消融。为每个提示选择的注意力头是根据在干净运行中具有高 DLA 来选择的。重新采样消融是指使用随机提示修补给定头的输入。重采样消融后的 DLA 值使用批次中位数 (n=300) 进行聚合，误差线位于第 25 和第 75 分位数。</figcaption></figure><h2>写入头和清理头的 DLA 呈负相关</h2><p>最后，我们检查清理头的 DLA 值。我们没有认识到四个构造提示的清理头和写入头的 DLA 值之间存在显着相关性。这是预期的，因为清理头可以在模型中发挥多种功能。我们通过专门查看写入头 (L0H2) 与清理头 (L2HX) 的 V 组合来缩小 DLA 比较的范围。直观上，V 组合告诉我们 L2HX 写入的<i>方向</i>如何取决于 L0H2 的输出。它的 DLA 值显示了清理头在多大程度上覆盖了 L0H2 的 DLA（在 L2HX 的注意力模式是恒等的情况下）。写入头与所有清理头的总 V 组成由下式确定</p><span><span class="mjpage mjpage__block"><span class="mjx-chtml MJXc-display" style="text-align: center;"><span class="mjx-math" aria-label="\sum_\text{heads X} { \left( \lVert \text{L0H2} \rVert_\text{LayerNorm} \cdot \mathbf{W}^\text{X}_\text{OV} \right)}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.498em;"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.74em; padding-bottom: 0.74em;">Σ</span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">头 X</span></span></span></span></span></span> <span class="mjx-texatom MJXc-space1"><span class="mjx-mrow"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">(</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span> <span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">L0H2</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">∥</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">LayerNorm</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-stack" style="vertical-align: -0.276em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">OV</span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">)</span></span></span></span></span></span></span></span></span></span><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{W}^\text{X}_\text{OV}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-stack" style="vertical-align: -0.276em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">X</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">OV</span></span></span></span></span></span></span></span></span></span>是吸头 X 的 OV 矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\mathbf{W}_\text{V} \cdot \mathbf{W}_\text{O}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">V</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.372em; padding-bottom: 0.372em;">W</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.23em; padding-right: 0.071em;"><span class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">O</span></span></span></span></span></span></span></span></span> ，我们对第 2 层中的所有吸尘器吸头 X = 2,3,4,5,6,7 求和。在图 5 中，总 V 的 DLA -成分显示与 L0H2 的 DLA 值呈负线性相关 –0.72(1)。这支持了我们的假设，即 L0H2 的高 DLA 值具有误导性，因为第二层中的清理头抵消了它们对最终 Logit 分布的贡献。 </p><figure class="image image_resized" style="width:83.67%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2PucFqdRyEvaHb4Hn/nwibieysnjy1mtftn4nr"><figcaption>图 5：写入头 L0H2 的 DLA 与 L0H2 V 组合到清洁头 L2HX 的 DLA (X = 2,3,4,5,6,7)。我们在 10 个提示中显示位置 32 到 1023 的下一个正确标记的 DLA 值。由于缺乏上下文，先前的立场被省略。线性拟合的斜率为 –0.72(1)。 （理想的清理对应于 -1 的斜率。）误差以双标准差给出。</figcaption></figure><h1>结论</h1><p>在这篇文章中，我们提出了变压器中内存管理的具体示例。此外，我们为 DLA 方法构建了对抗性示例，该方法依赖于通过前向传递保留在残余流中的方向。我们为未来的工作提出了三个方向：首先，我们感兴趣的是后面的节点在前向传递过程中如何使用已清理的空间。其次，我们想进一步研究头部L0H2在gelu-4l模型中的作用。最后，我们将在其他现有模型中寻找清理行为。先前的研究表明，特别是早期头部和晚期头部表现出较高的 DLA 值<span class="footnote-reference" role="doc-noteref" id="fnrefrdy8t8ebtpr"><sup><a href="#fnrdy8t8ebtpr">[4]</a></sup></span> <span class="footnote-reference" role="doc-noteref" id="fnrefgfuy2jlo0gf"><sup><a href="#fngfuy2jlo0gf">[6]</a></sup></span> 。我们想要仔细检查早期头部的高 DLA 值是否在这些作品中产生误导。</p><h2>致谢</h2><p>我们的研究得益于许多人的讨论、反馈和支持，包括 Chris Mathwin、Neel Nanda、Jacek Karwowski、Callum McDougall、Joseph Bloom、Alan Cooney、Arthur Conmy、Matthias Dellago、Eric Purdy 和 Stefan Heimersheim。我们还要感谢 ARENA 和 SERI MATS 计划为启动该项目提供了便利。</p><h2>作者贡献</h2><p>所有作者对这篇文章的贡献均等。 Jett 提出并领导了该项目，而 James、Can 和 Yeu-Tong 则执行该项目。</p><p>请引用为：<br> Dao 等人，“直接 Logit 归因的对抗性示例：gelu-4l 中的内存管理”，2023 年。<br></p><p><br><i>标签：可解释性（机器学习和人工智能）、SERI MATS、语言模型、对抗性示例、变压器电路</i><br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnmhfccldh2xf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmhfccldh2xf">^</a></strong></sup></span><div class="footnote-content"><p> Elhage 等人，“变压器电路的数学框架”，变压器电路线程，2021 年， <a href="https://transformer-circuits.pub/2021/framework/index.html"><u>https://transformer- Circuits.pub/2021/framework/index.html</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmadpxyr44ml"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmadpxyr44ml">^</a></strong></sup></span><div class="footnote-content"><p> Liberum 等人，“电路分析可解释性是否可扩展？来自龙猫多项选择能力的证据。” <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2307.09458v2"><u>https://arxiv.org/abs/2307.09458v2</u></a> 。</p><p> McGrath 等人，“九头蛇效应：语言模型计算中的紧急自我修复”。 <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2307.15771"><u>https://arxiv.org/abs/2307.15771</u></a> 。</p><p> Belrose 等人，“使用调谐镜头从变形金刚中引发潜在预测”。 <i>arxiv</i> ，2023， <a href="https://arxiv.org/abs/2303.08112"><u>https://arxiv.org/abs/2303.08112</u></a> 。</p><p> Dar 等人，“分析嵌入空间中的 Transformer”。 <i>arxiv</i> ，2022， <a href="https://arxiv.org/abs/2209.02535"><u>https://arxiv.org/abs/2209.02535</u></a> 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnjagxl19of0h"> <span class="footnote-back-link"><sup><strong><a href="#fnrefjagxl19of0h">^</a></strong></sup></span><div class="footnote-content"><p> Nanda，“实时研究记录：变压器可以重新导出位置信息吗？”。 YouTube，2022 年， <a href="https://www.youtube.com/watch?v=yo4QvDn-vsU&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T"><u>https://www.youtube.com/watch?</u></a> v=yo4QvDn-vsU&amp;list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnrdy8t8ebtpr"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrdy8t8ebtpr">^</a></strong></sup></span><div class="footnote-content"><p> Heimersheim 等人，“Python 文档字符串的电路。” Lesswrong，2023 年， <a href="https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only#Positional_Information_Head_0_4"><u>https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-</u></a> Circuit-for-python-docstrings-in-a-4-layer-attention-only#Positional_Information_Head_0_4 。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn7taiau8kfhc"> <span class="footnote-back-link"><sup><strong><a href="#fnref7taiau8kfhc">^</a></strong></sup></span><div class="footnote-content"><p>初步实验表明头 L0H2 的输出对于任何给定位置的标记大致不变。请随时联系以获取更多信息。</p></div></li><li class="footnote-item" role="doc-endnote" id="fngfuy2jlo0gf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgfuy2jlo0gf">^</a></strong></sup></span><div class="footnote-content"><p> Wang 等人，“野外可解释性：GPT-2 Small 中的间接对象识别电路。” arxiv，2022， <a href="https://arxiv.org/abs/2211.00593"><u>https://arxiv.org/abs/2211.00593</u></a> 。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/2PucFqdRyEvaHb4Hn/an-adversarial-example-for-direct-logit-attribution-memory#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2PucFqdRyEvaHb4Hn/an-adversarial-example-for-direct-logit-attribution-memory<guid ispermalink="false"> 2PucFqdRyEvaHb4Hn</guid><dc:creator><![CDATA[Can Rager]]></dc:creator><pubDate> Wed, 30 Aug 2023 17:36:59 GMT</pubDate> </item><item><title><![CDATA[Biosecurity Culture, Computer Security Culture]]></title><description><![CDATA[Published on August 30, 2023 4:40 PM GMT<br/><br/><p><span>虽然我只在生物安全领域工作了</span><a href="https://www.jefftk.com/p/leaving-google-joining-the-nucleic-acid-observatory">大约一年</a>，而且我的计算机安全背景是我在从事软件工程其他方面工作时学到的东西，但文化似乎非常不同。良好的计算机安全文化可能是糟糕的生物安全文化的一些例子：</p><p></p><ul><li><p>公开、<a href="https://en.wikipedia.org/wiki/Full_disclosure_(computer_security)">充分披露</a>。撰写博客文章，详细介绍如何发现漏洞，目的是教其他人如何在将来找到类似的漏洞。如果需要的话，可以将细节保密几个月，以便给供应商时间修复，但在<a href="https://googleprojectzero.blogspot.com/2021/04/policy-and-disclosure-2021-edition.html">90 天</a>之后才会公开。</p></li><li><p>打破事物来修复它们。给定一个新系统，你当然应该尝试对其进行妥协。如果手动成功，请制作一个在几毫秒内破解它的演示。制作（并发布！）模糊器和其他自动化漏洞搜索工具。</p></li><li><p>热情的好奇心和探索精神。注意到漏洞的暗示并深入研究它们以找出它们的深度是很棒的。如果有人说“你不需要知道这一点”，请忽略他们并尝试自己解决。</p></li></ul><p>这并不是计算机安全一直以来的情况，也不是无处不在的情况，该领域的人们常常强烈保护这些理想，反对试图隐藏缺陷或压制研究人员的供应商。总体而言，我的印象是这种文化对计算机安全产生了巨大的积极影响。</p><p>这意味着，如果你进入具有计算机安全背景的生物安全的有效利他主义角落，并看到所有这些关于“<a href="https://docs.google.com/document/d/1VSfU3GiZumHDX2hoz3YY1PT2dQHtkbrfO8xLxI9BTGE/edit">信息危害</a>”的讨论，人们会阻止试图寻找漏洞，并且人们对他们发现的危险事物保持沉默，那么事情就会发生变化。感觉很奇怪，而且<a href="https://forum.effectivealtruism.org/posts/3a6QWDhxYTz5dEMag/how-can-we-improve-infohazard-governance-in-ea-biosecurity">可能已经腐烂了</a>。</p><p>因此，这里的框架可能有助于从生物安全的角度看待问题。想象一下， <a href="https://en.wikipedia.org/wiki/Morris_worm">Morris 蠕虫</a>、 <a href="https://en.wikipedia.org/wiki/Blaster_(computer_worm)">Blaster</a>和<a href="https://en.wikipedia.org/wiki/Samy_(computer_worm)">Samy</a>都没有发生过。有少数人独立发现了<a href="https://en.wikipedia.org/wiki/SQL_injection">SQL 注入</a>，但没有公开。尽管我们周围越来越多的事物变得自动化，但计算机安全从未发展成为一个领域。我们有无人驾驶汽车、机器人外科医生和简单的自动化代理为我们服务，所有这些都具有原始 Sendmail 的安全性。它已经存在了足够长的时间，以至于原始作者已经离开，没有人记得它是如何工作的。付出一些认真努力的人可能会造成巨大的破坏，但这不会发生，因为拥有造成破坏的专业知识的人有更好的事情要做。将现代计算机安全文化引入这个假设的世界不会顺利！</p><p>大多数文化差异都可以追溯到漏洞已知后所发生的情况。使用电脑：</p><ul><li><p>负责软件和硬件的公司有能力修复他们的系统，而信息披露有助于建立一种规范，要求他们立即采取行动。</p></li><li><p>编写软件的人们可以改变他们的方法，以避免将来产生类似的漏洞。</p></li><li><p>一旦发现漏洞，最终用户就有多种有效且相当便宜的缓解方案。</p></li></ul><p>但对于生物学来说，没有供应商，特定的修复可能需要数年时间，完全通用的修复可能不可能，而且缓解措施可能非常昂贵。每个领域所需的文化都位于这些关键差异的下游。</p><p>总的来说，这是令人悲伤的：如果我们都可以谈论我们最关心的事情，那么我们可以更快地采取行动，而且原因优先级也会更简单。我希望我们生活在一个可以应用计算机安全规范的世界！但不同的限制会导致不同的解决方案，考虑到这些限制，我在生物风险中看到的谨慎程度似乎是正确的。</p><p> （请注意，当我谈论“良好的生物安全文化”时，我描述的是一套规范，我认为这些规范适合我们所处的情况，并且在有效的利他主义者和其他持类似观点的人中很常见然而，生物学中还有另一套规范，是在主要威胁是自然威胁时制定的。由于自然不存在利用公共知识造成伤害的风险，因此这种旧方法甚至比计算机安全文化更加开放，并且在我看来意见非常不适合我们现在所处的环境。）</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0Y2ZK25X6cUCSswaLDgGWJVyPVUHfZAwEpGriDRTbua7jX6TYv2mvc5gX73QRfrJYl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/110979547652871947">mastodon</a></i></p><br/><br/> <a href="https://www.lesswrong.com/posts/bTteXdzcpAsGQE5Qc/biosecurity-culture-computer-security-culture#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bTteXdzcpAsGQE5Qc/biosecurity-culture-computer-security-culture<guid ispermalink="false"> bTteXdzcpAsGQE5Qc</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Wed, 30 Aug 2023 16:40:03 GMT</pubDate> </item><item><title><![CDATA[Why I hang out at LessWrong and why you should check-in there every now and then ]]></title><description><![CDATA[Published on August 30, 2023 3:20 PM GMT<br/><br/><p>正如标题所示，这篇文章是<a href="https://new-savanna.blogspot.com/2023/08/why-i-hang-out-at-lesswrong-and-why-you.html">从我的家庭博客 New Savanna 交叉发布的</a>，并不是针对 LessWrong 读者的。然而，你们中的一些人可能会发现思考 LessWrong 在世界上的地位很有用。</p><p>前两节是背景知识，首先是关于文化变革的一些内容，以设定总体背景。然后是有关 LessWrong 的一些一般信息。现在我们正在阅读我对这个地方的个人印象，最后建议您去看一下（如果您还没有看过的话）。</p><p><strong>长期的文化变革：基督徒和教授</strong></p><p>早在古代，基督教只是罗马帝国边缘的另一个神秘邪教。公元380年，狄奥多西一世皇帝颁布了《帖撒罗尼迦敕令》，它成为罗马帝国的国教。随着时间的推移，它在欧洲的许多部落中传播开来，那些基督教部落的人开始想到一个叫做基督教世界的实体，随着时间的推移，它变成了欧洲和“西方”。</p><p>早在欧洲还是基督教世界的时候，天主教会就是知识分子生活的中心。随着科学革命和宗教改革的到来，这种情况在 16 世纪发生了变化。当然，天主教仍然很强大，但大学取代了它，成为知识生活的制度中心。</p><p>我的观点简单明了：事情会发生变化。邪教可以成为主流，新的机构可以取代旧的机构。考虑到这一点，让我们考虑一下 LessWrong。</p><p><strong>少错</strong></p><p>当然，我不想暗示<a href="https://en.wikipedia.org/wiki/LessWrong">LessWrong</a>是一个庞大而复杂的在线社区，以及那里的潮流（理性主义运动、有效利他主义（EA）、对流氓人工智能的反乌托邦恐惧）可以与基督教相媲美，但它看起来像邪教对局外人来说，对一些局内人来说也是如此。它举办了大量关于人工智能和人工智能存在风险、有效利他主义以及更普遍的如何生活的高强度智力活动。我怀疑对于一些在那里发帖的人来说，这是他们知识生活的知识中心。</p><p> I 由<a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky">Eliezer Yudkowsky</a><a href="https://en.wikipedia.org/wiki/LessWrong#History">于 2009 年创立</a>，Eliezer Yudkowsky 是一位自学者，对人工智能，特别是高级人工智能的破坏性潜力有着浓厚的兴趣。 That is what he&#39;s best known for and, while I suspect that Nick Bostrom is more widely known on that subject – his 2014 book, <a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"><i>Superintelligence</i></a> , was a best-seller, and he has an academic post at Oxford – Yudkowsky has likely been more influential within the tech community centered on Silicon Valley, where he lives.作为这种影响力的指标，请考虑 OpenAI 总裁兼联合创始人 Sam Altman 最近在 X（该网站以前称为 Twitter）上发布的两条帖子： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/l8uimkvgjphvcf3axlme" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/tgocmw57velmt0fkzavf 117w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/f7nqlukzq3ia0y2bevrw 197w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/s8xth2jvtclhihtao7dl 277w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/vieof1sso98oh77wjlrc 357w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/nj4pgxxmgskfwch2uauq 437w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/kne4lxlwgevhxnuqmano 517w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/nd9qnKxmYPr76Yi5s/aoet2q4xrf1magizulrj 597w"></figure><p>让我对和平奖产生怀疑。但除此之外，尤德科夫斯基在硅谷一直非常有影响力。在这些公司工作的人发帖并评论 LessWrong。</p><p><strong>为什么我在那里</strong></p><p>因为这是一个有趣的地方，虽然有点奇怪和令人反感，而且因为到目前为止我在那里进行了一些有趣的谈话。虽然不多，但绝对足够值得。</p><p>我不知道我第一次看到LessWrong是什么时候，但可以说是五年多前，甚至可能是十年前。但我在那里呆的时间并不多。大约两年前，也就是 GPT-3 开始震撼世界之后的某个时候，这种情况开始发生变化。 I made <a href="https://www.lesswrong.com/posts/hNXYWESpbzyjFnMyL/the-fourth-arena-what-s-up-in-the-world-these-days-we-re">my first post in June of 2022</a> , and have made a total of 40 posts and 137 comments there so far (now 41 posts, including this one).我通常每天都会去那里看看发生了什么。我可能会快速浏览一到五个新帖子，查看我关注的帖子的评论，然后继续处理我的业务。在特别好的一天，我会阅读我自己的一篇帖子的一些评论并回复。</p><p>问题是，我是一名浪人知识分子，而且多年来一直如此。如果你看过动画系列<a href="https://en.wikipedia.org/wiki/Samurai_Champloo"><i>《Samurai Champloo》</i></a> ，我的智力相当于 Jin。 Yes, I&#39;ve got a PhD – there are a few of those at LessWrong – and once held a faculty post at the Rensselaer Polytechnic Institute.但我很久以前就离开了那里，从此就没有一个知识分子的家了。 I&#39;ve published a bunch of articles in the academic literature on various topics scattered over literary criticism, cognitive science, and cultural evolution, and two books in the trade press with good publishers, one on music ( <i>Beethoven&#39;s Anvil</i> , Basic 2001) and one on computer graphics ( <i>Visualization</i> , Harry Abrams 1989).就其价值而言，对我而言，它的价值很大，想法数比页数似乎表明的要多，但我的作品从未真正流行起来。所以我明白在恐龙统治的世界里作为哺乳动物是什么感觉。</p><p>我强烈怀疑 LessWrong 的许多用户也有这种感觉。在这个层面上，以这种方式，我理解他们在做什么，即使我不做同样的事情。虽然我对人工智能非常感兴趣，但我对人类思想和人类文化更感兴趣，而且我不担心破坏和流氓人工智能的虚拟之手。尽管如此，我们还是可以并且确实聊了一会儿。</p><p>除此之外，谁知道呢？ there&#39;s a lot going on the world.文化变革、彻底变革——这是我们所需要的，而且无论我们是否喜欢它，都处于其中——是混乱的。许多想法涌现并得到实践，但很少有人坚持下来。唉，问题在于，好的东西，真正深奥的东西，是无法提前预测的。疯子和天才之间的区别只有在回顾时才会变得清晰。例如，大约六个月前，Cleo Nardo 发表了一篇很长的帖子，<a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post">名为“瓦路易吉效应”(meg-post)</a> ，迄今为止已吸引了 184 条评论（每隔一段时间就会有新的评论涌入）。它很精彩，但我认为它很混乱，到处都是，它是关于大型语言模型的内部结构和奇怪的行为。只要有足够多的信息，一些很有价值的事情似乎就可能发生，尽管我不太可能注意到它。也许你可能会。</p><p><strong>我对这个地方的看法</strong></p><p>我很清楚，有一些非常聪明和富有创造力的人在 LessWrong 度过了很多时间。但是，可惜的是，它非常孤立，正如对立的反文化往往如此。例如，这里是<a href="https://www.lesswrong.com/posts/LKAogXdruuZXdx6ZH/publish-or-perish-a-quick-note-on-why-you-should-try-to-make">3 月份的一篇短文</a>，附有 48 条评论，“关于为什么你应该努力让现有学术界能够理解你的工作。”是的！反之亦然。 LessWrongers 需要更多地关注感兴趣领域的现有工作。</p><p>我的老师，已故的大卫·海斯曾经说过，“伟大的天赋需要严格的纪律。” （他不记得他在哪里听到这句话，但这不是他原创的。如果你对这个短语进行网络搜索，你会得到一堆点击，尽管在几分钟内我没有找到找到那个确切的短语。）我担心很多在这里发帖的人都是如此，而且整个地方也是如此。仅仅了解很多东西并提出很多新的想法是不够的。您需要与他们合作并随着时间的推移对其进行完善，并与现有知识建立联系。</p><p>尽管如此，这里的谈话还是非常文明的。有激烈的讨论，但我没有看到任何激烈的争论。您可以对评论进行投票，这是一个常见功能，但 LessWrong 界面允许您区分评论的质量以及您是否同意，这是一个有用且重要的功能。</p><p>鉴于以学院和大学、各种智囊团和基金会以及工业研究为代表的“合法”知识世界，已经陷入了循环利用旧思想的困境（在观看《星际迷航》重播时，柯克的开场警告是“大胆地去无人涉足的地方”）以前已经过去了！”）世界需要新的概念滋生地。 LessWrong 看起来是一项很有前途的试点工作。</p><p><strong>为什么不去参观呢？</strong></p><p>如果您不熟悉这个地方，我建议您顺便过来，花一个小时逛逛。如果你没有发现任何吸引你注意力的东西，就离开，但一周后回来再试一次。如果一个月左右后没有任何效果，那就是这样。</p><p>但如果有什么事情坚持不了，你就得靠自己了。也许你到处发表评论。然后发布一些内容，看看会发生什么。</p><br/><br/> <a href="https://www.lesswrong.com/posts/nd9qnKxmYPr76Yi5s/why-i-hang-out-at-lesswrong-and-why-you-should-check-in#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/nd9qnKxmYPr76Yi5s/why-i-hang-out-at-lesswrong-and-why-you-should-check-in<guid ispermalink="false"> nd9qnKxmYPr76Yi5s</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Wed, 30 Aug 2023 15:20:45 GMT</pubDate> </item><item><title><![CDATA["Wanting" and "liking"]]></title><description><![CDATA[Published on August 30, 2023 2:52 PM GMT<br/><br/><h1> “想要”和“喜欢”</h1><p><em>这是 2023 年虚拟人工智能安全营的成果。感谢以下人员的反馈和有益的对话：Oliver Bridge、Tim Gothard、Rasmus Jensen、Linda Linsefors。</em> <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-1" id="fnref-sieGxzsnEEMoiKqSL-1">[1]</a></sup></p><p>这篇文章回顾了有关<em>“想要”</em>和<em>“喜欢”</em>的文献，这两个主要组成部分通常被称为生物奖励系统。它旨在为人工智能安全相关工作提供信息，特别是在尝试<a href="https://www.lesswrong.com/posts/nfoYnASKHczH4G5pT/brain-enthusiasts-in-ai-safety">利用</a><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8">神经科学</a><a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX">见解</a><a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX">进行调整的</a>方法中。</p><p>第 1 节介绍了奖励的两个高级组成部分之间的区别：想要和喜欢。在这一点上，我简化了主题并将两者视为同质类别。第 2 节和第 3 节深入研究了每个组件，并给出了更细粒度的模型，包括对其神经生物学基础的描述。 （2.2 和 3.2 是更技术性/干性的神经科学，所以如果这不是您的主要兴趣，您可能想跳过它们。）第 4 节讨论它们之间的功能关系以及为什么这种“劳动分工”可能受到青睐通过进化。</p><h2>一、简介</h2><p>我将首先介绍本文的四个核心概念：<em>想要</em>、 <em>“想要”</em> 、<em>喜欢</em>和<em>“喜欢”</em> 。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-2" id="fnref-sieGxzsnEEMoiKqSL-2">[2]</a></sup>他们沿着二维空间雕刻人类价值<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-3" id="fnref-sieGxzsnEEMoiKqSL-3">[3]</a></sup>的空间。第一个是我们有动力去做/被驱使去做（想要）的事情与我们对发生（或即将发生）感觉良好（喜欢）的事情之间的区别。第二个区别是每个元素的更基本/不太复杂的成分（ <em>“想要”</em>和<em>“喜欢”</em> ）与更复杂和认知的成分（<em>想要</em>和<em>喜欢</em>）之间的区别。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-4" id="fnref-sieGxzsnEEMoiKqSL-4">[4]</a></sup>本节的两部分详细阐述了这两个维度。</p><h3> 1.1.喜欢与想要</h3><p>喜欢是指当你咬了一口美味的食物，它的味道给你带来快乐，或者更一般地说，给你带来正价的情感状态。想要是当你有动力采取行动以获得食物并将其送到嘴里食用时。</p><p>这可能足以让我们凭直觉了解其区别的大致轮廓，但同时，它可能会引发一些问题。我可以想象这样的情况：我有点想起身去厨房，但我太累了（或者我的意志力太弱了）而起不来。所以我没有起床，尽管我知道冰箱里的食物非常好，如果我真的站起来去拿，我会很高兴这样做。另一方面，如果食物绝对美味并给我带来快乐，并且我“在某种程度上”享受它，但同时相信（或至少<a href="https://www.lesswrong.com/tag/subagents">我的一部分</a>相信）我不应该吃它怎么办？也许我认为我根本不应该<em>享受</em>它？也许我认为它不健康，或者担心有人会因为我在特定情况下吃它而评判我，或者我的上帝/宗教禁止吃猪肉。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-5" id="fnref-sieGxzsnEEMoiKqSL-5">[5]</a></sup>这些边缘案例难道不会质疑喜欢和想要之间的简单区别吗？因为它们过于截然不同，但却是连贯且同质的事物？也许他们确实如此，但通过研究这种粗粒度的喜好和需求，我们仍然可以了解很多我们所谓的人类价值观的原始内容。</p><p>根据我所说的对喜欢和想要之间关系的<em>天真的看法</em>，我们想要 X 的原因是我们喜欢 X，或者至少期望/预测喜欢 X。意识到我们（将）喜欢某物之间的相关性不久之后就产生了对它的渴望，使得这种观点非常适合大多数日常情况。</p><p>然而，喜欢和想要有时是分开的。我们可能会开始想要某些东西，即使我们既没有机会体验喜欢，也无法预测我们会喜欢它。举一个具体的例子，人类和其他动物通常在第一次性接触之前就产生性欲。有时他们甚至可能不知道性是什么。尽管如此，它们还是实施了一些智能行为，这些行为是进化选择的，以便可靠地以性交结束。</p><p>在其他情况下，迄今为止一直不喜欢的东西会成为欲望的对象。 Robinson 和 Berridge (2013) <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-6" id="fnref-sieGxzsnEEMoiKqSL-6">[6]</a></sup>教导老鼠将特定的刺激（以下称为条件刺激；CS）与将极咸的水直接注入其口中的排斥体验（以下称为无条件刺激；UCS）联系起来。老鼠很快意识到 UCS 可靠地跟随 CS，因此它们学会在看到 CS 时转身并退出。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-7" id="fnref-sieGxzsnEEMoiKqSL-7">[7]</a></sup>在实验的下一阶段，研究人员给老鼠注射了两种模拟大脑信号的化合物，这些信号在正常情况下会传达有关危险的低血钠水平的信息。重要的是，由于他们的饮食中始终含有足够的盐，因此他们从未有机会发现他们对咸味食物的喜欢（不喜欢）在多大程度上取决于血钠水平。</p><p>然而，他们的行为发生了巨大的变化。他们没有从CS撤退，而是开始接近它，渴望获得宝贵的盐分。 （感知到的）生理状态的变化将令人厌恶的事物变成了令人向往的事物，并且与之相关的提示也随之发生。</p><p>喜欢与想要分离的例子还不止于此。一个人可能会对某种药物产生成瘾，即使他们不太喜欢这种药物引起的状态。此外，随着吸毒时间的延长，其积极的主观影响（喜欢）往往会逐渐消失，但成瘾（想要）却会持续存在。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-8" id="fnref-sieGxzsnEEMoiKqSL-8">[8]</a></sup>换句话说，即使一个人不喜欢接受该剂量并且意识到他们不会喜欢一旦接受该剂量所发生的事情，他也可能想要获得该剂量。</p><p>有些人还会产生强迫性的欲望或行为模式，这些欲望或行为模式不会带来积极的体验，但仍然很难抗拒。亚临床的例子包括强迫性地检查手机、电子邮件或社交媒体、<a href="https://en.wikipedia.org/wiki/Doomscrolling">末日滚动</a>和赌博成瘾。至少从我的轶事现象角度来看，这些事情确实感觉像是我想要但不喜欢的东西。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-9" id="fnref-sieGxzsnEEMoiKqSL-9">[9]</a></sup></p><p>也许不太明显的是那些给我们带来很多积极体验但我们却没有产生任何强烈愿望的事物的例子。我记得朱莉娅·加莱夫（Julia Galef）在一些播客中提到，她真的很喜欢苹果，但从未学会“渴望”苹果，每当她碰巧吃苹果时，她就会想起这一点。如果我们想要某样东西的（主要）原因是我们喜欢它，那么她对苹果的渴望难道不应该与她对苹果的喜欢程度成正比吗？另一方面，更具推测性的是，有些人报告在某些<a href="https://astralcodexten.substack.com/p/nick-cammarata-on-jhana">冥想</a><a href="https://astralcodexten.substack.com/p/highlights-from-the-comments-on-jhanas">状态</a>下感到极度愉悦，但并未对此上瘾。我在第 2 节中讨论了更多选择性影响喜好但不想要的实验示例。</p><h3> 1.2.<em>喜欢</em>与<em>“喜欢”</em>和<em>想要</em>与<em>“想要”</em></h3><p><a href="https://en.wikipedia.org/wiki/Folk_psychology">民间心理学</a>概念并不能保证非常适合脑/心智科学。对于一些例子，诸如意识、情感、记忆、疼痛等概念，甚至<a href="https://academic.oup.com/book/11923">“概念”本身的概念，</a>结果被证明将重要的不同现象混在一起（参见 Ramsey，2022，第 2.3 节）。我们可能期望喜欢和想要走这条路，如果我们想用它们作为神经科学上充分的本体论的起点，它们至少需要一些改进。</p><p>从表面上看，喜欢和想要之间似乎存在不对称，因为后者至少在许多情况下可以从行为中推断出来， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-10" id="fnref-sieGxzsnEEMoiKqSL-10">[10]</a></sup>而前者是“私人”经验的问题。显然，对于无法用语言表达其持续主观体验的动物来说，这尤其成问题。然而，不对称性可能比看起来要弱。毕竟，我们可以确定人类的哪些自动行为和/或生理反应与（口头报告） <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-11" id="fnref-sieGxzsnEEMoiKqSL-11">[11]</a></sup>正价或负价体验（至少特定于某些领域，例如食物）相关。然后，我们可以转向动物并寻找类似的反应（例如，以大致相同的运动模式参与类似的肌肉群），以了解它们是否与我们预测动物喜欢或喜欢的相同类型的客观可观察事件相关。不喜欢。</p><p>例如，就食物而言， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-12" id="fnref-sieGxzsnEEMoiKqSL-12">[12]</a></sup>事实证明，愉快和不愉快的味道会强烈触发特定的面部表情（参见 Berridge &amp; Robinson，2003， <a href="https://sites.lsa.umich.edu/berridge-lab/wp-content/uploads/sites/743/2019/10/Berridge-Robinson-TINS-2003.pdf">图 I</a> ）。重要的是，它们甚至可以在没有意识功能的情况下发生， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-13" id="fnref-sieGxzsnEEMoiKqSL-13">[13]</a></sup>例如在睡眠中或新皮质功能缺陷的个体中， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-14" id="fnref-sieGxzsnEEMoiKqSL-14">[14]</a></sup>例如无脑婴儿（Steiner，1973；参见Berridge &amp; Winkielman，2003）。</p><p>观察到一些客观可测量的快乐表现可以在没有意识的情况下发生，促使引入<em>“喜欢”</em> （不需要意识的核心情感反应）和<em>喜欢</em>（有意识的快乐）之间的区别<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-15" id="fnref-sieGxzsnEEMoiKqSL-15">[15]</a></sup> （Berridge＆Robinson， 2003 年；贝里奇和克林格尔巴赫，2015 年）。类似地， <em>“想要”</em> （激励显着性、不需要意识的提示触发动机）与<em>想要</em>（具有声明性目标的认知欲望）不同。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-16" id="fnref-sieGxzsnEEMoiKqSL-16">[16]</a></sup>因此， <em>“喜欢”</em> /<em>喜欢</em>和<em>“想要”</em> /<em>想要的</em>区别捕获了隐性或客观可测量成分（ <em>“引用”</em> ）和显性或主观成分（<em>未引用</em>）之间的差异。</p><p>虽然对奖励进行客观衡量的需要是做出区分的最初动机，但缩小<em>“喜欢”</em>和<em>“想要”</em>的明确组成部分使得更容易识别它们的神经基础。初步近似，我们有<strong>（1）</strong>一个<em>“喜欢”</em>系统，集中在少数享乐热点和冷点周围，阿片类药物在<em>“（不）喜欢”</em>的产生和调节中发挥着主要作用<strong>，（2）</strong> <em>“想要”</em>系统，分布更广泛（尽管在某种程度上以腹侧被盖区为中心），并且以多巴胺作为关键的神经递质。这两个系统在某种程度上是分开的，但也有重叠。</p><h2> 2. 喜欢</h2><h3>2.1. <em>“喜欢”</em>和<em>喜欢</em></h3><p>“无意识的快乐”的想法似乎是矛盾的。从什么意义上说，发生在我们身上的事情可以是令人愉快的，但却无法被意识所感知？</p><p>快乐的无意识方面的引入遵循我们在心理学中经常看到的<a href="https://www.lesswrong.com/s/u9uawicHx7Ng7vwxA">概念外推</a>的特定模式。我们发现一种特定的与心理相关的现象具有一些客观可测量的“行为特征”。例如，人类、其他类人猿、老鼠和许多其他哺乳动物物种都会伸出舌头来回应美味的食物（参见 Berridge &amp; Robinson，2003， <a href="https://sites.lsa.umich.edu/berridge-lab/wp-content/uploads/sites/743/2019/10/Berridge-Robinson-TINS-2003.pdf">图 I</a> ）。对令人厌恶的口味的反应在很大程度上在不同的分类群中也是同源的。除此之外，将人们暴露于与味觉无关的价刺激（例如，快乐与愤怒的面孔）会影响他们消耗多少美味食物，并且即使这些刺激没有被有意识地感知到，这种效应也会持续存在（Winkielman等，2005）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-17" id="fnref-sieGxzsnEEMoiKqSL-17">[17]</a></sup></p><p>因此，将快乐/喜欢分为对有价刺激（ <em>“喜欢”</em> ）和有意识感知的快乐（<em>喜欢</em>）客观可测量的“核心情感反应”的潜意识部分的基本原理。后者更接近动词“喜欢”的常见含义。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-18" id="fnref-sieGxzsnEEMoiKqSL-18">[18]</a></sup>它表示意识中可用的价态感觉，即对当前事态的认可或反对。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-19" id="fnref-sieGxzsnEEMoiKqSL-19">[19]</a></sup></p><p>由于在这两者中， <em>“喜欢”</em>是客观可测量的组成部分，并且该领域的大部分研究都是在实验动物身上进行的（其主观体验无法通过口头报告来测量），因此我们对“喜欢”了解得更多也就不足为奇了。 <em>“喜欢”</em>的神经生物学基础比“<em>喜欢</em>”的神经生物学基础更重要（ <em>“想要”</em>和<em>想要</em>的也是如此）。因此，我对后者的讨论比前者更多的是一种推测。此外，大多数<em>“喜欢”</em>的动物研究依赖于一组有限的“奖励刺激领域”（主要是食物、性和药物），因此我们对这些领域之间核心情感反应如何不同的了解仍然相当有限。</p><h3> 2.2.大脑中的<em>“喜欢”</em></h3><p> <em>“喜欢”</em>电路最重要的组成部分是一些<em>享乐热点</em>和<em>冷点</em>，它们是一小群神经元，它们的刺激分别选择性地增加或减少<em>“喜欢”</em>反应。 （在增加的情况下，有时称为<em>享乐增强</em>。）它们<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-20" id="fnref-sieGxzsnEEMoiKqSL-20">[20]</a></sup>都不是解剖学上不同的结构（您不会在典型的神经解剖学教科书的索引中找到它们）。相反，它们是嵌入更大区域的“功能岛”，涉及许多与<em>“喜好”</em>无关的功能。</p><p>最重要的两个位于<a href="https://en.wikipedia.org/wiki/Basal_ganglia">基底神经节</a>，特别是<a href="https://en.wikipedia.org/wiki/Nucleus_accumbens">伏隔核</a>(NAc) 和<a href="https://en.wikipedia.org/wiki/Ventral_pallidum">腹侧苍白球</a>(VP)。 NAc 和 VP 都包含一个热点和一个冷点。 NAc分为核心和外壳，外壳的前面有一个热点，后面有一个冷点。在 VP 中，排列相反，热点在后面，冷点在前面（Richard et al., 2013; Berridge &amp; Kringelbach, 2013, 2015）。</p><p>除了基底神经节之外，享乐热点（但据我所知，不是冷点）位于眶额皮质 (OFC)、前岛叶 (aIns) 和脑桥臂旁核 (PBN；Söderpalm &amp; Berridge) ，2000 年；参见 Smith 等人，2010 年）。然而，NAc 和 VP 中的热点似乎是最重要的，它们是快乐的<em>发生器</em>。损害或停用 NAc 热点或 VP 热点可以消除享乐反应 (Berridge &amp; Kringelbach, 2015)。此外，虽然破坏NAc热点只会消除“喜欢”，但破坏VP热点（或其暂时失活）会导致“不喜欢”通常积极的事物（例如蔗糖）。</p><p>此外，几乎没有新皮质的无脑儿童以及经历了广泛的 OFC 损伤的人或非人类动物仍然保留完整的<em>“喜欢”</em>反应。 OFC 损伤的人也报告有意识的快乐，这表明即使<em>喜欢</em>也并不严格依赖于皮质（参见 Berridge &amp; Winkielman，2003）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-21" id="fnref-sieGxzsnEEMoiKqSL-21">[21]</a></sup>相比之下，两个基底神经节热点中的每一个都需要一定的基线活动水平，以便对其中一个热点进行额外刺激以产生享乐增强（Smith &amp; Berridge，2007；Smith 等人，2011；参见 Richard等人，2013）。</p><p>重要的是，这并不是‘刺激热点提高<em>“好感度”</em> ，刺激冷点降低<em>“好感度”</em>那么简单。用于刺激的神经递质的选择很重要。在这里，阿片受体最为相关（参见 Berridge &amp; Robinson, 2003；Berridge &amp; Kringelbach, 2015；Smith et al., 2010）。在 NAc 热点中，mu、δ 和 kappa 阿片受体激动剂都会引起快感增强，而在 VP 热点和皮质热点中，这种作用似乎仅限于 mu-阿片受体 (MOR) 激动剂。根据区域的不同，刺激非阿片受体可以产生类似的结果。到目前为止，被证明能产生快感增强的神经递质包括大麻素 (NAc)、食欲素 (NAc、VP、PBN、OFC) 和 GABA (PBN；Söderpalm &amp; Berridge, 2000)。</p><p>前面我提到<em>“喜欢”</em>系统和<em>“想要”</em>系统是紧密相连的。事实证明，在大多数情况下，刺激皮层下享乐热点除了“喜欢<em>”之外还增加了“想要</em><em>”</em> 。此外，在享乐热点中增加<em>“欲望”</em>的化合物范围远大于增加<em>“喜好”</em>的化合物范围（Berridge &amp; Kringelbach，2013）。刺激 NAc 和 VP 中的冷点也可以产生<em>“想要”</em> 。我将在第 3 节中进一步讨论这一点。相反，阿片类药物可以间接增加腹侧被盖区的活动，该区域是核心<em>“想要”</em>通路中多巴胺的主要来源（Zhang 等人，2022）。</p><p>包含热点和冷点的 NAc 壳区域通常用“情感键盘”来描述，其中神经元的位置与它们的激活引起的情感反应（例如<em>“喜欢”</em>与<em>“不喜欢”</em> ）密切相关（Richard等，2013；Berridge &amp; Kringelbach，2013，2015）。更具体地说，似乎存在从前到后延伸的化合价梯度。刺激“键盘”更靠前的区域会引起<em>“喜欢”</em>反应，而更多位于尾部的神经元会抑制<em>“喜欢”</em>和/或引起<em>“不喜欢”</em> ，有时还伴随着对威胁（例如捕食者）的物种特异性反应。</p><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3706488/figure/F2/">图 2</a>来自 Richard 等人。 (2013) 显示了 NAc 中神经元群体的分布，这些神经元的刺激会引发特定类型的行为。我们有享乐热点（红橙色），其中激活通常会增强<em>“喜欢”</em>反应。在它后面，我们看到两个区域。在腹侧（下侧），有一组神经元具有我们期望从享乐冷点（蓝色）获得的功能。他们的刺激会抑制<em>“喜欢”</em> 。在背侧（上侧），我们有另一个簇，它也具有抑制作用，但它们不是抑制<em>“喜欢”</em> ，而是抑制厌恶反应（紫色）。所有这些位点，除了对<em>“喜欢”</em>的影响外，“仍然产生进食”（即<em>“想要”</em>进食），它们所在的更广泛（绿色）区域和背内侧新纹状体的一部分也是如此。粗略地说，尾状核和壳核位于 NAc 上方），如图左上角的绿点所示。</p><p>冷点区域还包含其功能超出<em>“喜欢（不）喜欢”</em>调节范围的细胞。对其中一些的刺激会产生恐惧反应或攻击性表现，例如向潜在威胁的刺激物扔泥土。重要的是，这些细胞的刺激结果也可以受到环境的调节。在平静、平和和安全的环境中，刺激增加积极反应的区域会扩大，而厌恶/恐惧反应的区域会缩小。压力大、危险、不安全的环境会产生相反的效果。</p><p>与热点类似，刺激冷点的结果取决于所使用的配体的种类。刺激相同的三种阿片受体（mu、delta、kappa）会增强 NAc 热点的<em>“喜欢”</em> ，在邻近的冷点产生强烈的<em>“（不）喜欢”</em>甚至与恐惧相关的行为。 Mu 受体激动剂还抑制 VP 冷点的<em>“喜欢”</em> 。</p><h3> 2.3.有意识的快乐有什么好处？</h3><p>诚然，目前还不清楚<em>“喜欢”</em>和<em>喜欢</em>如何以及在多大程度上相互依赖。 <em>“喜欢”</em>的东西需要什么才能变得<em>喜欢</em>？我们知道<em>“喜欢”</em>可以在不<em>喜欢的情况</em>下发生，但是反过来呢？有什么东西可以满足我们明确的享乐感受而不影响任何这些“核心情感反应”吗？或者，只要意识“开启”（至少超过某个阈值） <em>，“喜欢”的</em>事物就会变得<em>喜欢</em>？从我对文献的粗略回顾来看，我们似乎不知道，尽管眶额皮层（OFC）作为该区域的主要候选区域出现，其活动（可能除了更基本的<em>“喜欢”</em>结构之外）对于<em>喜欢</em>很重要（克林格尔巴赫，2005，2010）。</p><p><a href="https://en.wikipedia.org/wiki/Global_workspace_theory">意识的全局工作空间理论（GWT）</a> <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-22" id="fnref-sieGxzsnEEMoiKqSL-22">[22]</a></sup> （及其所借鉴的实验）可能会给出一些关于<em>“喜欢”</em><em>刺激</em>/事件的建议。大脑可以无意识地进行相当多的复杂处理（例如， <a href="https://www.lesswrong.com/posts/x4n4jcoDP7xh5LWLq/book-summary-consciousness-and-the-brain#Unconscious_processing_of_meaning">单词的语义</a>）。在 GWT 范式中进行的实验表明了这一点，使用<a href="https://en.wikipedia.org/wiki/Visual_masking">感觉掩蔽</a>来防止刺激到达意识。根据 GWT，与表示/处理特定刺激相关的神经活动必须超过某个关键阈值，才能传播到其他（特别是多模式/联想/更高级别）大脑区域，然后这些区域可以开始以某种同步的方式处理它。方式。这是“意识到<em>某</em>事”的神经基础。该表征可供其他大脑系统使用，包括直接连接到言语器官的系统，以便我们可以报告我们对刺激的意识。否则，它的处理仍然是无意识的、局部的，并且仅限于特定的较低级别的大脑区域。</p><p>将此观点转化为<em>“喜欢”</em>和<em>喜欢的</em>情况，可能存在类似的核心享乐影响（ <em>“喜欢”</em> ）强度阈值，刺激必须超过该阈值才能传播到全球工作空间并被有意识地<em>喜欢</em>。传播到某些特定区域（例如 OFC）似乎特别重要。</p><p>值得注意的是，为了防止有价刺激到达意识，显示无意识处理的刺激对客观可观察的快乐相关因素的影响的实验（例如，Winkielman等人，2005）使用了与GWT相同的方法，即感觉掩蔽。这是一个小证据，表明 GWT 实验的结果可能会转化为<em>“喜欢”</em>和<em>喜欢的</em>情况。</p><p>如果这个观点是正确的，它可能指向有意识的<em>喜好</em>的可能功能，因为<a href="https://www.lesswrong.com/posts/KuKaQEu7JjBNzcoj5/explicitness">明确的</a>享乐价值增加了​​影响其他大脑系统的可能途径的范围（例如，第3节中讨论的动机回路）。所以也许问题是“有意识的快乐有什么好处？”只不过是“意识有什么好处”的一个特例？</p><p>关于<em>喜欢</em>而不是<em>“喜欢”的可能性，</em>我想知道自我形象、规范信念、社会期望或（广泛理解的）当前情况的反思性评估等因素是否会产生自上而下的影响（例如，我对生活的感觉有多好，或者世界上事物的发展方式）可能会引起<em>喜欢</em>，但不会引起核心情感反应和皮层下享乐热点的相应活动。另一方面，我还预计，至少在某些情况下（也许在大多数甚至所有情况下），皮质下（不）快感发生器将由于这种自上而下的影响而被二次激活。</p><h2> 3. 想要</h2><h3>3.1. <em>“想要”</em>与<em>想要</em></h3><p>我们的行为并不总是反映我们对应该做什么的明确信念，这很难说是一个原始的观察。这种现象有很多名称，例如<a href="https://www.lesswrong.com/tag/akrasia">意志力</a>薄弱或缺乏意志力。这可能使得<em>“想要”</em> （激励显着性）和<em>想要</em>（认知欲望）之间的区别比<em>“喜欢”</em>和<em>喜欢</em>之间的区别更加相关和直观。</p><p> <em>“想要”</em>可以被视为<em>想要的</em>无意识对应物，类似于<em>“喜欢”</em>是<em>喜欢</em>的无意识对应物。<em>想要</em>（认知激励）是指针对我们意识到并明确表达的愿望的目标的计划，而<em>“想要”</em> （激励显着性）指的是更冲动、反应性、低水平的动机，它可以独立于我们的行为而行动（声明我们）<em>想要</em>或<em>喜欢</em>。</p><p>更具体地说， <em>“想要”</em>被定义为“大脑的条件性动机反应，通常由与奖励相关的刺激触发并分配给该刺激”（Berridge，2007）。如果刺激与“本身有奖励”的事件（例如甜味）相关，则该刺激是与奖励相关的。这种关联可以很简单，比如发生的时间非常接近，也可能涉及一些更复杂的认知学习过程。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-23" id="fnref-sieGxzsnEEMoiKqSL-23">[23]</a></sup>习得的与奖励相关的刺激通常被称为“条件刺激”（CS），而固有的奖励事件被称为“非条件刺激”（UCS）。然而， <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-24" id="fnref-sieGxzsnEEMoiKqSL-24">[24]</a></sup>并非所有驱动<em>“想要”</em>的输入都是被学习的，因为大脑天生会对某些刺激做出<em>“想要”</em>反应，独立于学习（或在没有学习的情况下/在学习之前）。似乎， <em>“想要”</em>最初的适应性价值是激励动物追求一小部分无条件的奖励，例如食物、性或有利的环境条件范围，例如适当的温度和酸度。随着时间的推移，随着更复杂的学习机制的发展， <em>“想要”</em>的角色变得可以通过学习来扩展（Berridge，2007）。</p><p>根据 Berridge 和 Robinson (2003) 的说法，认知激励（<em>想要</em>）与激励显着性（ <em>“想要”</em> ）有以下三个（或可能四个）特征：</p><blockquote><p> …它们（1）是已知的或想象的（认知激励表征）； (2) 期望是愉快的（享乐期望）； (3) 主观上渴望并打算获得（想要的明确认知表征），并且，也许，(4) 已知可以通过导致其发生的行动来获得（对行为与结果因果关系的理解）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-25" id="fnref-sieGxzsnEEMoiKqSL-25">[25]</a></sup></p></blockquote><p>我将在 3.3 节中更多地推测<em>“想要”</em>和<em>想要</em>之间的差异和关系。在接下来的 3.2 节中，我概述了<em>“想要”</em>的神经生物学基础。</p><h3> 3.2.大脑中的<em>“想要”</em></h3><p> <em>“喜欢”</em>可以大致位于少数享乐热点和冷点，其中内源性阿片类药物是回路中的主要参与者（如第 2 节所述）。 <em>“想要”</em>的神经底物更多地分布在整个大脑中，其中多巴胺是关键的神经递质。</p><p>人脑有多种<a href="https://en.wikipedia.org/wiki/Dopaminergic_pathways">多巴胺能通路</a>，其中与<em>“想要”</em>最相关的是<a href="https://en.wikipedia.org/wiki/Mesolimbic_pathway">中脑边缘通路</a>。它从<a href="https://en.wikipedia.org/wiki/Ventral_tegmental_area">腹侧被盖区（VTA）</a>到腹侧纹状体（包括伏隔核）和其他一些区域。尽管它是参与激励显着性的大脑区域多巴胺的最大供应者（Ikemoto，2010），但它并不是唯一的供应者，并且至少在某些人工设置中，即使它停止工作， <em>“想要”</em>也可能发生。</p><p>我们知道，更具体地说，中脑边缘通路被消除的动物仍然可以通过植入某些大脑区域的电极产生自我刺激的冲动（参见 Ikemoto，2010，第 131 页）。目前尚不清楚这些结果在多大程度上会在没有中脑边缘通路的情况下转化为<em>“想要”</em> 。</p><p> VTA 和其他涉及<em>“想要”</em>的区域形成了一个高度互连的网络（参见 Ikemoto，2010）。不过，其中很多并不是<em>“想要”</em>特有的，还涉及到其他方面的奖励。在第二节中，我讨论了伏隔核和腹侧苍白球的享乐热点和冷点。用许多神经递质（包括那些倾向于引起<em>“（不）喜欢”</em>反应的神经递质）刺激它们往往会产生<em>“想要”</em>行为，这两种行为都与接近（ <em>“想要”</em> X）和厌恶（ <em>“想要”</em>非X）有关。后脑的臂旁核也是如此，其 GABA-A 受体的刺激是<em>“想要的”</em> ，但它附近有一个小区域，它也引起了<em>“喜欢”</em> 。</p><p>网络的其他区域也参与学习。例如，巴甫洛夫学习似乎依赖于一个回路，该回路的主要组成部分是基底外侧杏仁核、眶额皮质和伏隔核（Burke et al., 2010）。另一方面，当对中央杏仁核的刺激与高度显着的刺激（无论是愉快还是不愉快无关，它只需要在任一方向上具有很强的效价）相结合时，即使对于非常不愉快的刺激也可以建立非常强烈的<em>“想要”</em>刺激（Warlow 等人，2020）。</p><p>另外两条对<em>“想要”</em>很重要的多巴胺能通路是中皮质通路和黑质纹状体通路。中皮质通路从 VTA 延伸到前额皮质，参与执行功能。其疾病，包括涉及多巴胺耗竭或其他多巴胺能活动干扰的疾病，与认知控制和工作记忆受损有关（参见 Ott &amp; Nieder，2019）。</p><p>黑质纹状体通路从黑质（SN）到背侧纹状体，其主要作用是运动控制。大多数 SN 细胞的死亡是帕金森病的直接原因。帕金森病患者往往会出现与中脑边缘通路（例如冷漠）和中皮层通路（例如注意力缺陷）功能下降相关的症状。这些症状的严重程度与运动症状的严重程度高度相关，表明这些系统之间存在一定程度的耦合（参见 Leyton，2010，第 232-233 页）。</p><p>接下来的几段讨论了多巴胺能活动一般如何影响<em>“想要”</em> 。它并不是对证据的完整概述或与替代假设的比较（参见：Berridge，2007），而是作为这种神经递质所发挥的作用的信息说明。</p><p>测试某些神经递质 X 如何影响某些行为 Y 的最直接方法可能是降低或增加 X 的水平并测量 Y 的变化。一种方法是培育具有异常低或高水平的神经递质的转基因动物。突触中的神经递质（参见 Berridge，2007，第 403-405 页）。</p><p>多巴胺缺乏（DD）小鼠的大脑中几乎没有多巴胺，可以通过敲除酪氨酸羟化酶的编码基因来制造，没有这种酶就无法产生多巴胺。 DD几乎不吃不喝，不足以维持自己的生活。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-26" id="fnref-sieGxzsnEEMoiKqSL-26">[26]</a></sup>为了让他们正常饮食，他们需要服用左旋多巴（一种直接多巴胺前体，只需在生产链中的一步从多巴胺中去除），这可以暂时将他们的多巴胺恢复到正常水平。</p><p>这使得可以测试（1）DD 小鼠是否对不同种类的刺激（例如糖溶液与水）表现出不同的情感/ <em>“喜欢”</em>反应，以及（2）在尝试这两种刺激后，它们是否学会了更喜欢其中一种。另一个是根据他们在后续试验中的选择来衡量的。事实证明，它们可以同时做到这两点，这表明多巴胺并不是（至少某些形式的） <em>“喜欢”</em>和奖励相关学习所必需的。在野生型（即正常/未转基因）小鼠中观察到类似的模式，其多巴胺能系统在其生命后期因神经化学损伤而受损。</p><p>另一方面，<strong>高</strong>多巴胺能小鼠的突触中多巴胺含量几乎是正常数量的三倍（与野生型相比），可以通过敲除编码多巴胺转运蛋白的基因来创建，多巴胺转运蛋白是一种从神经元中去除多巴胺的蛋白质。突触。这些小鼠更有动力去获得奖励，更能抵抗分散它们对当前目标的注意力的刺激，并且愿意为了奖励而更加努力地工作。换句话说，它们似乎比野生型更<em>“想要”</em>奖励。他们学习刺激和奖励之间的关联或了解哪些行为会带来奖励以及<em>“喜欢”</em>反应的能力不受影响。</p><p>人类的多巴胺紊乱疾病又如何呢（参见 Leyton，2010）？帕金森病 (PD) 是由黑质中的多巴胺能细胞退化引起的，这些细胞不直接参与中脑边缘系统。尽管如此，许多帕金森病患者仍表现出与中脑边缘（例如冷漠、无欲）和中皮质（例如注意力和执行功能较差）系统中多巴胺能功能下降相关的症状。这些症状的严重程度与运动问题的严重程度相关，尤其是帕金森病。一些接受 L-DOPA 治疗的 PD 患者（约 3-4%；Pezzella，2005）会出现<a href="https://en.wikipedia.org/wiki/Dopamine_dysregulation_syndrome">多巴胺失调综合征 (DDS)</a> ，多巴胺缺乏的过度补偿会导致他们出现“病态” <em>“欲望”</em>行为（成瘾、赌博、强迫性性行为）活动，即使他们在服药前没有这样的病史）使他们成为多巴胺能亢进的例证。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-27" id="fnref-sieGxzsnEEMoiKqSL-27">[27]</a></sup></p><p>许多高度成瘾的潜力都是多巴胺能的。核心例子包括安非他明、可卡因及其类似物，它们的作用主要是通过增加突触间隙中停留的多巴胺的量。有趣的是，在动物研究中，如果将它们与 DA 拮抗剂（即与多巴胺受体结合但不激活它们的化合物，从而阻止多巴胺本身结合）一起使用，它们的成瘾潜力可以降低（也许甚至（几乎？）完全消除？）并发挥其典型效果（参见 Puglisi-Allegra &amp; Ventura，2012）。同时，多巴胺拮抗作用并不能消除这些多巴胺能药物的其他作用。例如，当安非他明与 DA 拮抗剂一起使用时，一些欣快效应仍然存在（参见 Leyton，2010），这表明这些效应要么是通过多巴胺以外的机制介导的，要么是通过某些多巴胺受体介导的，这些受体未被研究中使用的特殊药物阻断<sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-28" id="fnref-sieGxzsnEEMoiKqSL-28">。 28]</a></sup> （Nader 等，1997；Ikemoto，2010）。</p><p>不直接与多巴胺系统相互作用的高度成瘾药物往往会产生继发性多巴胺能效应。例如，mu-阿片受体激动剂（如吗啡、海洛因和芬太尼）通常通过抑制位于 VTA 后部或邻近区域（称为吻内侧被盖 (RMTg)）的 GABA 能神经元发挥作用。另一方面，这些 GABA 能神经元会抑制 VTA 的多巴胺能神经元，从而驱动<em>“欲望”</em> 。因此，抑制前者意味着去抑制后者，从而增加 VTA 目标区域中 DA 的浓度（参见Zhang et al., 2022）。</p><p>咖啡因是另一种具有间接多巴胺能作用（且具有相对轻微的成瘾潜力）的化合物。尽管其主要作用机制是阻断腺苷受体，但它也会增加多巴胺的释放，从而有助于其精神兴奋和强化特性（Ferré，2016）。我们可以在实验中看到，与不含咖啡因的相同酸奶相比，在酸奶中添加咖啡因增强了对该酸奶的偏好（Panek 等，2013）。对蜜蜂和富含咖啡因的花蜜进行了类似的实验，得到了相似的结果（Wright 等人，2013）。 <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-29" id="fnref-sieGxzsnEEMoiKqSL-29">[29]</a></sup></p><p><a href="https://en.wikipedia.org/wiki/Pavlovian-instrumental_transfer">巴甫洛夫工具转移</a>是指当动物已经为了获得某种奖励/UCS而工作时，在感知到与其当前追求的UCS相关的CS后开始更加努力地工作时发生的情况。这种效应与中脑边缘多巴胺能活性的增加密切相关，并且可以通过干预中脑边缘系统来调节，例如使用多巴胺激动剂（Berridge，2007，第 420-421 页；Cartoni 等，2016；Salamone 等） .，2016）。</p><h3> 3.3.认知激励/有意识的需求有什么好处？</h3><p>为什么除了<em>“想要”</em>之外我们还需要<em>想要</em>？在第二节中，我提出了一个关于<em>喜欢</em>和<em>“喜欢”</em>的类似问题，并给出了一个临时假设，即价刺激的意识使其能够被大脑的其他部分所访问，从而使它们能够互操作并利用彼此的输出。<em>想要</em>和<em>“想要”</em>有类似的关系吗？</p><p> Berridge 和 Robinson (2003) 似乎赞同这样的观点。在他们看来，<em>欲望</em>可以让动物实现目标，而这些目标是通过简单的联想学习无法实现的，需要更复杂的推理、工作记忆等。他们写道：</p><blockquote><p>理性认知的本质之一是它对世界上合法一致性的推理利用，通常，未来价值最好是从过去的价值推断出来的。此外，老鼠必须利用其对哪些行为导致哪些结果的理解，从几种可能的行为中选择能够产生最佳奖励的行为。</p></blockquote><p>与此相关的是，作为一个有意识的执行控制下的过程，<em>需求</em>在改变当地激励因素方面更加稳定，这使得计划的规划和执行首先成为可能。</p><p>因此，从 VTA 到前额皮质某些部分的中皮质通路可能是<em>需求</em>的主要基质的候选者，因为它的主要作用之一是执行功能。</p><p>与第二节末尾的猜测相呼应，我们可以<em>想要</em>一些东西而不是<em>“想要”</em>它吗？也许我们可以再次从自我形象的角度来看：我们将自己建模为<em>想要</em>X，但这个模型并不准确，也不足以推翻向另一个方向推进的<em>“想要”</em> 。 Perhaps sometimes <em>wanting</em> without <em>&quot;wanting&quot;</em> is adaptive because it causes the organism to think about plans of action that can be executed once a proper context occurs, so that <em>&quot;wanting&quot;</em> is triggered and makes use of the information contained in the plans developed due to <em>wanting</em> .</p><h2> 4. Why <em>&quot;like&quot;</em> something if you can just <em>&quot;want&quot;</em> it?</h2><p> Ex ante, we might expect that <em>&quot;wanting&quot;</em> itself, paired with a sufficiently good learning algorithm, should be enough to achieve any goals necessary for survival and reproductive fitness.</p><p> Maybe it is necessary or more efficient to have separate systems for (1) adaptive but &quot;mindless&quot; responses to local incentives and (2) goal-directed behavior that relies on taking into account broader context; hence <em>&quot;wanting&quot;</em> and <em>wanting</em> , respectively. Still, this leaves us with a question about the adaptive value of <em>&quot;liking&quot;</em> and <em>liking</em> . Had they not contributed to our ancestors&#39; fitness in one way or another, evolution would not have selected for them. <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-30" id="fnref-sieGxzsnEEMoiKqSL-30">[30]</a></sup></p><p> It seems that the field has not reached a consensus on that question. Here I present three hypotheses. Like much of evolutionary psychology, they are all tentative, and if evidence for them is at best indirect. Importantly, these hypotheses are neither exhaustive nor mutually exclusive.</p><h3> Hypothesis 1: Liking extends wanting</h3><p> On this account, we start with a small set of default motivations that evolution selected for ( <em>&quot;wants&quot;</em> ), and <em>&quot;liking&quot;</em> helps repurpose the motivational system towards new motivations. The <em>&quot;wanting&quot;</em> system is more evolutionarily ancient. <em>&quot;Liking&quot;</em> emerged relatively recently, in animals living in more cognitively demanding environments that necessitated acquiring new motivational mechanisms over the lifetime. Pleasure is an additional training signal for the <em>&quot;wanting&quot;</em> system, allowing the brain to repurpose systems specialized for being motivated towards one domain of stimuli/events towards another domain. The need for this &quot;lifetime reprogramming&quot; may arise due to the environment being too complex or too variable for evolution to encode appropriate sources of motivation into the genome.</p><p> Kent Berridge (a pioneer of this line of research) seems to lean towards this hypothesis ( <a href="https://hearthisidea.com/episodes/kent">podcast interview link</a> ). He gives credit for it to, among others, Anthony Dickinson (eg, Dickinson &amp; Balleine, 2010).</p><p> Quoting directly from the episode (lightly edited by me):</p><blockquote><p> […] pleasure exists because it essentially allows brain <em>&quot;wanting&quot;</em> systems that might have evolved for one thing (eg, food) to experience a new pleasant event (eg, social accomplishment) and to enjoy that event and to bring to bear the brain <em>&quot;wanting&quot;</em> systems for the old thing on to this new target, basically giving us a new target of desire.</p></blockquote><p> On this account, it seems to be somewhat similar to the picture presented by the <a href="https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values">Shard Theory view of human values</a> , except that not all the values (contextually activated motivations/ <em>&quot;wants&quot;</em> ) are learned from scratch.</p><p> Here, Berridge doesn&#39;t distinguish between <em>&quot;liking&quot;</em> and <em>liking</em> . My interpretation is that he views them as serving a similar function, just on different levels of &quot;cognitive sophistication&quot;, similar to <em>&quot;wanting&quot;</em> and <em>wanting</em> .</p><p> There is an interesting category of cases, where the learned change in valence happens prior to a corresponding change in motivation. In other words, your experience changes whether/how much you <em>&quot;(dis)like&quot;</em> X but without updating your motivation for X. Your motivation for X is updated the next time you encounter X and experience altered valence.</p><p> Dickinson and Balleine (Balleine, 2010) give an example, where the first author (AD) who really liked watermelons, at some point got sick shortly after eating a watermelon (most likely the disease and eating the fruit were not related). A few days later, he went to eat a watermelon and although it was most likely basically the same kind of watermelon, it tasted awful. Apparently, his <em>&quot;liking&quot;</em> / <em>liking</em> system wrongly inferred the watermelon to be the causal factor behind the sickness, which altered his taste, but not his motivation to eat watermelons, until he tasted a no-longer-tasty watermelon. These cases have been reproduced in rat experiments. <sup class="footnote-ref"><a href="#fn-sieGxzsnEEMoiKqSL-31" id="fnref-sieGxzsnEEMoiKqSL-31">[31]</a></sup></p><p> Note that this is different from the Salt Sea experiment (Robinson &amp; Berridge, 2013), where rats&#39; motivation was already altered by their physiological state before being presented with the stimulus. In the watermelon case, on the other hand, the aversion occurs unexpectedly.</p><h3> Hypothesis 2: Preparing physiology</h3><p> <em>&quot;Liking&quot;</em> reactions associated with food and fluids seem to prepare the organism for intake of nutrients. Increased salivation facilitates pre-digestion of the food in the mouth, licking the lips ensures that some bits of the food are not left out, increased gastric movements prepare the rest of the digestive system, etc.</p><h3> Hypothesis 3: Implicit social communication</h3><p> An animal&#39;s physiological reactions, like the ones discussed above, often carry socially important information. Thus, it&#39;s plausible that in more social species these behaviors would evolve to become more pronounced, in other to facilitate implicit social communication. On the other hand, they may also become less reflective of the actual physiological state, in order to produce signals that are more likely to influence the behavior of the other animal in the direction that is beneficial for the signaller.</p><h3> Hypothesis 4: Additional information to update behavior on</h3><p> Valence ( <em>&quot;liking&quot;</em> / <em>liking</em> ) may also route partially processed information to the motivational circuits in order to update already ongoing behavior. If we do X for the first time and it quickly turns out that we <em>like</em> it, we do more of it. An obvious caveat is that we may not be able to experimentally disentangle the indirect effect mediated by valence from the direct effect. We have seen that it is possible to very quickly develop strong motivation for something without <em>&quot;liking&quot;</em> it, eg, in the wireheading studies.</p><h2>参考</h2><ul><li>Berridge, KC (2007). The debate over dopamine&#39;s role in reward: The case for incentive salience. Psychopharmacology, 191(3), 391–431. <a href="https://doi.org/10.1007/s00213-006-0578-x">https://doi.org/10.1007/s00213-006-0578-x</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2008). Affective neuroscience of pleasure: Reward in humans and animals. Psychopharmacology, 199(3), 457–480. <a href="https://doi.org/10.1007/s00213-008-1099-6">https://doi.org/10.1007/s00213-008-1099-6</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2013). Neuroscience of affect: Brain mechanisms of pleasure and displeasure. Social and Emotional Neuroscience, 23(3), 294–303. <a href="https://doi.org/10.1016/j.conb.2013.01.017">https://doi.org/10.1016/j.conb.2013.01.017</a></li><li> Berridge, KC, &amp; Kringelbach, ML (2015). Pleasure Systems in the Brain. Neuron, 86(3), 646–664. <a href="https://doi.org/10.1016/j.neuron.2015.02.018">https://doi.org/10.1016/j.neuron.2015.02.018</a></li><li> Berridge, KC, &amp; Robinson, TE (2003). Parsing reward. Trends in Neurosciences, 26(9), 507–513. <a href="https://doi.org/10.1016/S0166-2236(03)00233-9">https://doi.org/10.1016/S0166-2236(03)00233-9</a></li><li> Berridge, KC, &amp; Robinson, TE (2016). Liking, wanting, and the incentive-sensitization theory of addiction. The American Psychologist, 71(8), 670–679. <a href="https://doi.org/10.1037/amp0000059">https://doi.org/10.1037/amp0000059</a></li><li> Berridge, K., &amp; Winkielman, P. (2003). What is an unconscious emotion? (The case for unconscious &quot;liking&quot;). Cognition and Emotion, 17(2), 181–211. <a href="https://doi.org/10.1080/02699930302289">https://doi.org/10.1080/02699930302289</a></li><li> Burke, KA, Franz, T., Miller, D., &amp; Schoenbaum, G. (2010). Conditioned Reinforcement and the Specialized Role of Corticolimbic Circuits in the Pursuit of Happiness and Other More Specific Rewards. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 50–62). Oxford University Press.</li><li> Cartoni, E., Balleine, B., &amp; Baldassarre, G. (2016). Appetitive Pavlovian-instrumental Transfer: A review. Neuroscience &amp; Biobehavioral Reviews, 71, 829–848. <a href="https://doi.org/10.1016/j.neubiorev.2016.09.020">https://doi.org/10.1016/j.neubiorev.2016.09.020</a></li><li> dela Peña, I., Gevorkiana, R., &amp; Shi, W.-X. （2015）。 Psychostimulants affect dopamine transmission through both dopamine transporter-dependent and independent mechanisms. European Journal of Pharmacology, 764, 562–570. <a href="https://doi.org/10.1016/j.ejphar.2015.07.044">https://doi.org/10.1016/j.ejphar.2015.07.044</a></li><li> Dickinson, A., &amp; Balleine, B. (2010). Hedonics: The Cognitive–Motivational Interface. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 74–84). Oxford University Press.</li><li> Ferré, S. (2016). Mechanisms of the psychostimulant effects of caffeine: Implications for substance use disorders. Psychopharmacology, 233(10), 1963–1979. <a href="https://doi.org/10.1007/s00213-016-4212-2">https://doi.org/10.1007/s00213-016-4212-2</a></li><li> Ikemoto, S. (2010). Brain reward circuitry beyond the mesolimbic dopamine system: A neurobiological theory. Novel Perspectives on Drug Addiction and Reward, 35(2), 129–150. <a href="https://doi.org/10.1016/j.neubiorev.2010.02.001">https://doi.org/10.1016/j.neubiorev.2010.02.001</a></li><li> Kringelbach, ML (2005). The human orbitofrontal cortex: Linking reward to hedonic experience. Nature Reviews Neuroscience, 6(9), 691–702. <a href="https://doi.org/10.1038/nrn1747">https://doi.org/10.1038/nrn1747</a></li><li> Kringelbach, ML (2010). The Hedonic Brain: A Functional Neuroanatomy of Human Pleasure. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 202–221). Oxford University Press.</li><li> Leyton, M. (2010). The Neurobiology of Desire: Dopamine and the Regulation of Mood and Motivational States in Humans. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 222–243). Oxford University Press.</li><li> Nader, K., Bechara, A., &amp; van der Kooy, D. (1997). Neurobiological constraints on behavioral models of motivation. Annual Review of Psychology, 48(1), 85–114. <a href="https://doi.org/10.1146/annurev.psych.48.1.85">https://doi.org/10.1146/annurev.psych.48.1.85</a></li><li> Ott, T., &amp; Nieder, A. (2019). Dopamine and Cognitive Control in Prefrontal Cortex. Trends in Cognitive Sciences, 23(3), 213–234. <a href="https://doi.org/10.1016/j.tics.2018.12.006">https://doi.org/10.1016/j.tics.2018.12.006</a></li><li> Panek, LM, Swoboda, C., Bendlin, A., &amp; Temple, JL (2013). Caffeine increases liking and consumption of novel-flavored yogurt. Psychopharmacology, 227(3), 425–436. <a href="https://doi.org/10.1007/s00213-013-2971-6">https://doi.org/10.1007/s00213-013-2971-6</a></li><li> Pezzella, FR, Colosimo, C., Vanacore, N., Di Rezze, S., Chianese, M., Fabbrini, G., &amp; Meco, G. (2005). Prevalence and clinical features of hedonistic homeostatic dysregulation in Parkinson&#39;s disease. Movement Disorders, 20(1), 77–81. <a href="https://doi.org/10.1002/mds.20288">https://doi.org/10.1002/mds.20288</a></li><li> Puglisi-Allegra, S., &amp; Ventura, R. (2012). Prefrontal/accumbal catecholamine system processes high motivational salience. Frontiers in Behavioral Neuroscience, 6, 31. <a href="https://doi.org/10.3389/fnbeh.2012.00031">https://doi.org/10.3389/fnbeh.2012.00031</a></li><li> Ramsey, W. (2022). Eliminative Materialism. In EN Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2022). Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/archives/spr2022/entries/materialism-eliminative/">https://plato.stanford.edu/archives/spr2022/entries/materialism-eliminative/</a></li><li> Richard, JM, Castro, DC, Difeliceantonio, AG, Robinson, MJF, &amp; Berridge, KC (2013). Mapping brain circuits of reward and motivation: In the footsteps of Ann Kelley. Neuroscience and Biobehavioral Reviews, 37(9 Pt A), 1919–1931. <a href="https://doi.org/10.1016/j.neubiorev.2012.12.008">https://doi.org/10.1016/j.neubiorev.2012.12.008</a></li><li> Robinson, MJF, &amp; Berridge, KC (2013). Instant transformation of learned repulsion into motivational &quot;wanting&quot;. Current Biology : CB, 23(4), 282–289. <a href="https://doi.org/10.1016/j.cub.2013.01.016">https://doi.org/10.1016/j.cub.2013.01.016</a></li><li> Salamone, JD, Pardo, M., Yohn, SE, López-Cruz, L., SanMiguel, N., &amp; Correa, M. (2016). Mesolimbic Dopamine and the Regulation of Motivated Behavior. In EH Simpson &amp; PD Balsam (Eds.), Behavioral Neuroscience of Motivation (pp. 231–257). Springer International Publishing. <a href="https://doi.org/10.1007/7854_2015_383">https://doi.org/10.1007/7854_2015_383</a></li><li> Smith, KS, &amp; Berridge, KC (2007). Opioid limbic circuit for reward: Interaction between hedonic hotspots of nucleus accumbens and ventral pallidum. The Journal of Neuroscience : The Official Journal of the Society for Neuroscience, 27(7), 1594–1605. <a href="https://doi.org/10.1523/JNEUROSCI.4205-06.2007">https://doi.org/10.1523/JNEUROSCI.4205-06.2007</a></li><li> Smith, KS, Berridge, KC, &amp; Aldridge, JW (2011). Disentangling pleasure from incentive salience and learning signals in brain reward circuitry. Proceedings of the National Academy of Sciences of the United States of America, 108(27), E255-264. <a href="https://doi.org/10.1073/pnas.1101920108">https://doi.org/10.1073/pnas.1101920108</a></li><li> Smith, KS, Mahler, SV, Peciña, S., &amp; Berridge, KC (2010). Hedonic Hotspots: Generating Sensory Pleasure in the Brain. In ML Kringelbach &amp; KC Berridge (Eds.), Pleasures of the Brain (pp. 27–49). Oxford University Press.</li><li> Söderpalm, AH, &amp; Berridge, KC (2000). The hedonic impact and intake of food are increased by midazolam microinjection in the parabrachial nucleus. Brain Research, 877(2), 288–297. <a href="https://doi.org/10.1016/s0006-8993(00)02691-3">https://doi.org/10.1016/s0006-8993(00)02691-3</a></li><li> Steiner, JE (1973). The gustofacial response: Observation on normal and anencephalic newborn infants. Symposium on Oral Sensation and Perception, 4, 254–278.</li><li> Szczypka, MS, Rainey, MA, Kim, DS, Alaynick, WA, Marck, BT, Matsumoto, AM, &amp; Palmiter, RD (1999). Feeding behavior in dopamine-deficient mice. Proceedings of the National Academy of Sciences, 96(21), 12138–12143. <a href="https://doi.org/10.1073/pnas.96.21.12138">https://doi.org/10.1073/pnas.96.21.12138</a></li><li> Warlow, SM, Naffziger, EE, &amp; Berridge, KC (2020). The central amygdala recruits mesocorticolimbic circuitry for pursuit of reward or pain. Nature Communications, 11(1), 2716. <a href="https://doi.org/10.1038/s41467-020-16407-1">https://doi.org/10.1038/s41467-020-16407-1</a></li><li> Winkielman, P., Berridge, K., &amp; Wilbarger, J. (2005). Unconscious Affective Reactions to Masked Happy Versus Angry Faces Influence Consumption Behavior and Judgments of Value. Personality &amp; Social Psychology Bulletin, 31, 121–135. <a href="https://doi.org/10.1177/0146167204271309">https://doi.org/10.1177/0146167204271309</a></li><li> Wright, GA, Baker, DD, Palmer, MJ, Stabler, D., Mustard, JA, Power, EF, Borland, AM, &amp; Stevenson, PC (2013). Caffeine in Floral Nectar Enhances a Pollinator&#39;s Memory of Reward. Science, 339(6124), 1202–1204. <a href="https://doi.org/10.1126/science.1228806">https://doi.org/10.1126/science.1228806</a></li><li> Zhang, J.-J., Song, C.-G., Dai, J.-M., Li, L., Yang, X.-M., &amp; Chen, Z.-N. （2022）。 Mechanism of opioid addiction and its intervention therapy: Focusing on the reward circuitry and mu-opioid receptor. MedComm, 3(3), e148. <a href="https://doi.org/10.1002/mco2.148">https://doi.org/10.1002/mco2.148</a> </li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-sieGxzsnEEMoiKqSL-1" class="footnote-item"><p> Ordered alphabetically, by last name. <a href="#fnref-sieGxzsnEEMoiKqSL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-2" class="footnote-item"><p> I italicize <em>liking</em> , <em>&quot;liking&quot;</em> , <em>wanting</em> , and <em>&quot;wanting&quot;</em> in order to emphasize that I&#39;m using these terms in a &quot;technical&quot; sense. &quot;Non-technical&quot; senses are non-italicized. In a few places of this section I also sometimes lump <em>liking</em> and <em>&quot;liking&quot;</em> into &quot;liking&quot; and <em>wanting</em> and <em>&quot;wanting&quot;</em> into &quot;wanting&quot;. <a href="#fnref-sieGxzsnEEMoiKqSL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-3" class="footnote-item"><p> Not necessarily exhaustively, there may be some (things we might want to consider as) values that don&#39;t fit neatly into any of these categories. <a href="#fnref-sieGxzsnEEMoiKqSL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-4" class="footnote-item"><p> The distinction between explicit and implicit is also sometimes used, but I find it unintuitive. <a href="#fnref-sieGxzsnEEMoiKqSL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-5" class="footnote-item"><p> By the way, the taboo against eating pork has a very interesting origin. See <a href="https://www.youtube.com/watch?v=pI0ZUhBvIx4">this video from Religion for Breakfast</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-5" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-6" class="footnote-item"><p> See also <a href="https://www.lesswrong.com/posts/wcNEXDHowiWkRxDNv/inner-alignment-in-salt-starved-rats">Steve Byrnes&#39;s post about that study</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-7" class="footnote-item"><p> The CS itself can become aversive or desired, even when the UCS it predicts appears in a different place than the CS. In such cases, the CS is said to become a &quot;motivational magnet&quot; (eg, Robinson &amp; Berridge, 2013). <a href="#fnref-sieGxzsnEEMoiKqSL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-8" class="footnote-item"><p> Explaining this phenomenon in terms of wanting to avoid unpleasant effects of the withdrawal syndrome doesn&#39;t fit the empirical data (Berridge &amp; Robinson, 2016). <a href="#fnref-sieGxzsnEEMoiKqSL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-9" class="footnote-item"><p> While here I am speaking about subclinical cases of behavioral addictions, I also expect this to be a factor in obsessive-compulsive disorder and related conditions. One reason I think so is that the neurotransmitter most consistently involved in OCD seems to be dopamine, which is strongly implicated in wanting (see Section 3). Moreover, the most successful pharmacological treatment for OCD is naltrexone, which is also used in many standard addictions and acts by regulating dopaminergic transmission from the VTA. <a href="#fnref-sieGxzsnEEMoiKqSL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-10" class="footnote-item"><p> Although it probably still requires making some assumptions about the agent&#39;s biases and cognitive limitations. See, eg, <a href="https://www.lesswrong.com/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard">Christiano (2018)</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-11" class="footnote-item"><p> I&#39;m not going to discuss the topic of phenomenal consciousness and its relationship with verbal reports because I consider the former to be <a href="https://plato.stanford.edu/entries/qualia/#Illusional">illusory</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-12" class="footnote-item"><p> Food being obviously the easiest category of &quot;rewards&quot; to study. <a href="#fnref-sieGxzsnEEMoiKqSL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-13" class="footnote-item"><p> By &quot;conscious functioning&quot;, I mean something like &quot;the <a href="https://en.wikipedia.org/wiki/Global_workspace_theory">global workspace</a> &quot; being up and running. <a href="#fnref-sieGxzsnEEMoiKqSL-13" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-14" class="footnote-item"><p> With the neocortex being the part of the brain we expect to be important for conscious awareness. <a href="#fnref-sieGxzsnEEMoiKqSL-14" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-15" class="footnote-item"><p> From now on, I italicize <em>liking</em> , <em>&quot;liking&quot;</em> , <em>wanting</em> , and <em>&quot;wanting&quot;</em> , in order to emphasize that I&#39;m using these terms in their &quot;technical&quot; sense. &quot;Non-technical meanings&quot; of liking and liking are non-italicized. <a href="#fnref-sieGxzsnEEMoiKqSL-15" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-16" class="footnote-item"><p> Berridge and Robinson (2003) also introduced a third distinction between two forms of learning: cognitive and associative, but it is not the focus of this post. <a href="#fnref-sieGxzsnEEMoiKqSL-16" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-17" class="footnote-item"><p> I found no studies on that, but I have a very confident guess that <em>&quot;liking&quot;</em> would also occur in animals that are asleep or even in some kinds of palliative states, such as coma, perhaps even <a href="https://en.wikipedia.org/wiki/Locked-in_syndrome">locked-in syndrome</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-17" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-18" class="footnote-item"><p> Of course, the correspondence is not perfect and the boundaries between the daily meanings of &quot;to like&quot; and &quot;to want&quot; are blurry. Still, semantic distance from &quot;to like&quot; to <em>liking</em> is smaller than to <em>&quot;liking&quot;</em> . <a href="#fnref-sieGxzsnEEMoiKqSL-18" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-19" class="footnote-item"><p> Importantly, &quot;the ongoing state of affairs&quot; can include things extended over a long timescale. <a href="#fnref-sieGxzsnEEMoiKqSL-19" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-20" class="footnote-item"><p> At least none of the ones we know about, to the best of my knowledge. <a href="#fnref-sieGxzsnEEMoiKqSL-20" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-21" class="footnote-item"><p> At the same time, some studies show that monkeys and rats with OFC damage, although they still respond to rewards, are impaired in using reward information to guide their behavior, relative to animals with intact OFC (Berridge &amp; Kringelbach, 2008), perhaps pointing to the role of conscious pleasure in reward-related learning. <a href="#fnref-sieGxzsnEEMoiKqSL-21" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-22" class="footnote-item"><p> For a great introduction to GWT, see <a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/x4n4jcoDP7xh5LWLq">Kaj Sotala&#39;s review of <em>The Consciousness and the Brain</em> by Stanislas Dehaene</a> . <a href="#fnref-sieGxzsnEEMoiKqSL-22" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-23" class="footnote-item"><p> Eg, when I realize that when I get back to exercising after a long break, I start feeling much better on a daily basis after a few weeks, which increases my motivation to exercise (although this is probably not a good example of <em>&quot;wanting&quot;</em> ). <a href="#fnref-sieGxzsnEEMoiKqSL-23" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-24" class="footnote-item"><p> Perhaps I am slightly deviating from the definition of <em>&quot;wanting&quot;</em> as &quot;conditioned responses&quot;. This seems true though and in agreement with the idea (endorsed by Berridge) that the original adaptive of <em>&quot;wanting&quot;</em> was to drive the animal&#39;s behavior to satisfy some small set of needs. <a href="#fnref-sieGxzsnEEMoiKqSL-24" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-25" class="footnote-item"><p> I take the mention of &quot;pleasant&quot; in (2) to refer both to <em>&quot;liking&quot;</em> and <em>liking</em> , with the latter being used in a broad sense, which includes (ia) reflective evaluation of the state of the world conditional on having achieved the <em>wanted</em> goal. I think this interpretation is justified because otherwise, the definition would exclude clear examples of <em>wanting</em> , such as a person doing hard work for which they are not going to receive any &quot;pleasant reward&quot;, unless we take a very broad meaning of &quot;pleasure&quot; (not mentioning more extreme cases like suicide bombers and kamikaze). <a href="#fnref-sieGxzsnEEMoiKqSL-25" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-26" class="footnote-item"><p> Szczypka et al. (1999) write that &quot;young [DD] pups that had never been injected with l-DOPA would lick and swallow small drops of a liquid diet placed by their mouth. Apparently, these kinds of responses don&#39;t require dopamine, perhaps being a kind of <em>&quot;liking&quot;</em> reactions. <a href="#fnref-sieGxzsnEEMoiKqSL-26" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-27" class="footnote-item"><p> See also Oliver Sacks&#39;s <em><a href="https://en.wikipedia.org/wiki/Awakenings_(book)">Awakenings</a></em> . <a href="#fnref-sieGxzsnEEMoiKqSL-27" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-28" class="footnote-item"><p> Most <a href="https://en.wikipedia.org/wiki/Dopamine_antagonist">dopamine antagonists</a> work only on a particular subtype of dopamine receptors. <a href="#fnref-sieGxzsnEEMoiKqSL-28" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-29" class="footnote-item"><p> Interestingly, in addition to increasing DA directly, amphetamine and cocaine-like psychostimulants also appear to increase DA via an indirect route (Peña et al., 2015). <a href="#fnref-sieGxzsnEEMoiKqSL-29" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-30" class="footnote-item"><p> Alternatively, they might be <a href="https://en.wikipedia.org/wiki/Spandrel_(biology)">spandrels</a> . This probably isn&#39;t the case for <em>&quot;liking&quot;</em> , as spandrels typically (ever?) have distinct brain circuits. If <em>liking</em> is a natural consequence of <em>&quot;liking&quot;</em> plus global workspace/consciousness systems, then it would also probably not be a spandrel. <a href="#fnref-sieGxzsnEEMoiKqSL-30" class="footnote-backref">↩︎</a></p></li><li id="fn-sieGxzsnEEMoiKqSL-31" class="footnote-item"><p> Dickinson and Balleine&#39;s account of the function of reward is slightly different than the one I&#39;m presenting here. You can read it yourself if you&#39;re curious. <a href="#fnref-sieGxzsnEEMoiKqSL-31" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/><a href="https://www.lesswrong.com/posts/opJxxfrN33xQx3eXu/wanting-and-liking#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/opJxxfrN33xQx3eXu/wanting-and-liking<guid ispermalink="false"> opJxxfrN33xQx3eXu</guid><dc:creator><![CDATA[Mateusz Bagiński]]></dc:creator><pubDate> Wed, 30 Aug 2023 14:52:04 GMT</pubDate></item><item><title><![CDATA[Open Call for Research Assistants in Developmental Interpretability]]></title><description><![CDATA[Published on August 30, 2023 9:02 AM GMT<br/><br/><p> We are excited to announce multiple positions for Research Assistants to join our <a href="https://manifund.org/projects/scoping-developmental-interpretability-xg55b33wsfc"><u>six-month research project</u></a> assessing the viability of <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental Interpretability</u></a> (DevInterp).</p><p> This is a chance to gain expertise in interpretability, develop your skills as a researcher, build out a network of collaborators and mentors, publish in major conferences, and open a path towards future opportunities, including potential permanent roles, recommendations, and successive collaborations.</p><h2> <strong>Background</strong></h2><p> <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental interpretability</u></a> is a research agenda aiming to build tools for detecting, locating, and understanding phase transitions in learning dynamics of neural networks. It draws on techniques from singular learning theory, mechanistic interpretability, statistical physics, and developmental biology.</p><h2> <strong>Position Details</strong></h2><p> <strong>General info:</strong></p><ul><li> <strong>Title</strong> : Research Assistant / Research Engineer.</li><li> <strong>Location</strong> : Remote, with hubs in Melbourne and London.</li><li> <strong>Duration</strong> : Until March 2024 (at minimum).</li><li> <strong>Compensation</strong> : base salary is USD$35k per year, to be paid out as an independent contractor at an hourly rate.</li></ul><p> <strong>Timeline</strong> :</p><ul><li> <strong>Application Deadline</strong> : September 15th, 2023</li><li> <strong>Ideal Start Date</strong> : October 2023</li></ul><p> <strong>How to Apply</strong> : Complete the <a href="https://forms.gle/6hEpiqgN4oAHmypD6"><u>application form</u></a> by the deadline. Further information on the application process will be provided in the form.</p><h2> <strong>Who We Are</strong></h2><p> The developmental interpretability research team consists of experts across a number of areas of mathematics, physics, statistics and AI safety. The principal researchers:</p><ul><li> <a href="http://therisingsea.org/"><u>Daniel Murfet</u></a> , mathematician and SLT expert, University of Melbourne.</li><li> <a href="https://www.suswei.com/"><u>Susan Wei</u></a> , statistician and SLT expert, University of Melbourne.</li><li> <a href="https://www.jessehoogland.com/"><u>Jesse Hoogland</u></a> , MSc. Physics, SERI MATS scholar, RA in Krueger lab</li></ul><p> We have a range of projects currently underway, led by one of these principal researchers and involving a number of other PhD and MSc students from the University of Melbourne and collaborators from around the world. In an organizational capacity you would also interact with Alexander Oldenziel and Stan van Wingerden.</p><p> You can find us and the broader DevInterp research community on our <a href="https://discord.gg/pCf4UynKsc"><u>Discord</u></a> . Beyond the <a href="https://www.lesswrong.com/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability"><u>Developmental Interpretability</u></a> research agenda, you can read our first preprint on <a href="https://arxiv.org/abs/2308.12108"><u>scalable SLT invariants</u></a> and check out the lectures from the <a href="https://www.youtube.com/@SLTSummit/videos"><u>SLT &amp; Alignment summit</u></a> .</p><h2> <strong>Overview of Projects</strong></h2><p> Here&#39;s the selection of <strong>&nbsp;</strong> the projects underway, some of which you would be expected to contribute to. These tend to be on the more <strong>experimental</strong> side:</p><ul><li> <strong>Developing scalable estimates for SLT invariants</strong> : Invariants like the (local) learning coefficient and (local) singular fluctuation can signal the presence of “hidden” phase transitions. Improving these techniques can help us better identify these transitions.</li><li> <strong>DevInterp of vision models</strong> : To what extent do the kinds of circuits studied in the <a href="https://distill.pub/2020/circuits/zoom-in/"><u>original circuits thread</u></a> emerge through phase transitions?</li><li> <strong>DevInterp of program synthesis</strong> : <strong>&nbsp;</strong> In examples where we know there is rich compositional structure, can we see it in the singularities? Practically, this means studying settings like modular arithmetic (grokking), multitask sparse parity, and more complex variants.</li><li> <strong>DevInterp of in-context learning</strong> <strong>&amp; induction heads</strong> : Is the development of induction heads a proper phase transition in the language of SLT? More ambitiously, can we apply singular learning theory to study in-context learning and make sense of “in-context phase transitions.”</li><li> <strong>DevInterp of language models</strong> : Can we detect phase transitions in simple language models (like <a href="https://arxiv.org/abs/2305.07759"><u>TinyStories</u></a> ). Can we, from these transitions, discover circuit structure? Can we extend these techniques to larger models (eg, in the <a href="https://github.com/EleutherAI/pythia"><u>Pythia suite</u></a> ).</li><li> <strong>DevInterp of reinforcement learning</strong> <strong>models</strong> : To what extent are phase transitions involved in the emergence of a world model (in examples like <a href="https://github.com/likenneth/othello_world/tree/master"><u>OthelloGPT</u></a> )?</li></ul><p> Next to these, we are working on a number of more <strong>theoretical</strong> projects. (Though our focus is on hiring for the more applied projects, if one of these particularly excites you, you should definitely apply!)</p><ul><li> <strong>Studying phase transitions in simple models like deep linear networks:</strong> These serve a valuable intermediate case between regular models and highly singular models. This is also a place to draw connections to a lot of existing literature on deep learning theory (on “saddle-to-saddle dynamics”).</li><li> <strong>Developing “geometry probes”</strong> : It&#39;s not enough to <i>detect</i> phase transitions, but we have to be able to <i>analyze</i> them for structural information. Here the aim is to develop “probes” that extract this kind of information from phase transitions.</li><li> <strong>Developing the “geometry of program synthesis”</strong> : We expect that understanding neural networks will require looking beyond models of computation like Turing machines, lambda calculus, and linear logic, towards <i>geometric</i> models of computation. This means pushing further in directions like those explored by Tom Waring in his <a href="http://therisingsea.org/notes/MSc-Waring.pdf"><u>MSc. thesis</u></a> .</li></ul><p> Taken together these projects complete the scoping phase of the DevInterp research agenda, ideally resulting in publications in venues like ICML and NeurIPS.</p><h2> <strong>What We Expect</strong></h2><p> You will be communicating with your research project teammates on the DevInterp <a href="https://discord.gg/pCf4UynKsc"><u>Discord</u></a> , writing code and training models, joining in weekly or biweekly research meetings over Zoom, and in general acting as a productive member of a fast-moving research team combining both theoreticians and experimentalists working together to define the future of the science of interpretability.</p><p> Depending on interest and background, you may also be reading and discussing papers from ML or mathematics and contributing to the writing of papers on Overleaf. It&#39;s not mandatory, but you would be invited to join in virtual research seminars like the <a href="https://metauni.org/slt/"><u>SLT seminar</u></a> at metauni or SLT reading group on the DevInterp Discord.</p><p> There will be a <a href="https://www.lesswrong.com/posts/QpFiEbqMdhaLBPb7X/apply-now-for-the-devinterp-2023-fall-summit"><u>DevInterp conference</u></a> in November 2023 in Oxford, United Kingdom, and it would be great if you could attend (we will pay for your travel). There will hopefully be a second opportunity to meet the team in person between November and the end of the employment period (possibly in Melbourne, Australia).</p><h2> <strong>FAQ</strong></h2><p> <strong>Who is this for?</strong></p><p> We&#39;re looking mainly for people who can do engineering work, that is, people with software development and ML skills. It&#39;s not necessary to have a background in interpretability or AI safety, although that&#39;s a plus. Ideally you have legible output / projects that demonstrate ability as an experimentalist.</p><p> <strong>What&#39;s the time commitment?</strong></p><p> We&#39;re looking mainly for people who can commit full-time, but if you&#39;re talented and only available part-time, don&#39;t shy away from applying.</p><p> <strong>What does the compensation mean?</strong></p><p> We&#39;ve budgeted USD$70k in total to be spread across 1-4 research assistants over the next half year. By default we&#39;re expecting to pay RAs USD$17.50/hour.</p><p> <strong>Do I need to be familiar with SLT and AI alignment?</strong></p><p> No (though it&#39;s obviously a plus).</p><p> We&#39;re leaning towards taking on skilled general purpose experimentalists (without any knowledge of SLT) over less experienced programmers who know some SLT. That said, if you are a talented theorist, don&#39;t shy away from applying.</p><p> <strong>What are you waiting for?</strong></p><p> <a href="https://forms.gle/6hEpiqgN4oAHmypD6"><u>Apply now</u></a> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/DZHmEmzujfuqfxbJY/open-call-for-research-assistants-in-developmental#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DZHmEmzujfuqfxbJY/open-call-for-research-assistants-in-developmental<guid ispermalink="false"> DZHmEmzujfuqfxbJY</guid><dc:creator><![CDATA[Jesse Hoogland]]></dc:creator><pubDate> Wed, 30 Aug 2023 09:02:59 GMT</pubDate> </item><item><title><![CDATA[LTFF and EAIF are unusually funding-constrained right now]]></title><description><![CDATA[Published on August 30, 2023 1:03 AM GMT<br/><br/><h1><strong>概括</strong></h1><p>EA Funds aims to empower thoughtful individuals and small groups to carry out altruistically impactful projects - in particular, enabling and accelerating small/medium-sized projects (with grants &lt;$300K). We are looking to increase our level of independence from other actors within the EA and longtermist funding landscape and are seeking to raise ~$2.7M for the Long-Term Future Fund and ~$1.7M for the EA Infrastructure Fund (~$4.4M total) over the next six months.</p><p> <strong>Why donate to EA Funds?</strong> EA Funds is the largest funder of small projects in the longtermist and EA infrastructure spaces, and has had a solid operational track record of giving out hundreds of high-quality grants a year to individuals and small projects. We believe that we&#39;re well-placed to fill the role of a significant independent grantmaker, because of a combination of our track record, our historical role in this position, and the quality of our fund managers.</p><p> <strong>Why now?</strong> We think now is an unusually good time to donate to us, as a) we have an unexpectedly large funding shortage, b) there are <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding"><u>great projects on the margin that we can&#39;t currently fund</u></a> , and c) more stabilized funding now can give us time to try to find large individual and institutional donors to cover future funding needs.</p><p> Importantly, Open Philanthropy is no longer providing a guaranteed amount of funding to us and <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>instead will move over to a (temporary) model of matching our funds</u></a> 2:1 ($2 from them for every $1 from you, up to 3.5M from them per fund).</p><p> <strong>Where to donate:</strong> If you&#39;re interested, you can donate to either Long-Term Future Fund (LTFF) or EA Infrastructure Fund (EAIF) <a href="https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds"><u>here</u></a> . <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnyd012yijab"><sup>[1]</sup></a></p><p> Some relevant quotes from fund managers:</p><p> <strong>Oliver Habryka</strong></p><p> I think the next $1.3M in donations to the LTFF (430k pre-matching) are among the best historical grant opportunities in the time that I have been active as a grantmaker. If you are undecided between donating to us right now vs. December, my sense is now is substantially better, since I expect more and larger funders to step in by then, while we have a substantial number of time-sensitive opportunities right now that will likely go unfunded.</p><p> I myself have a bunch of reservations about the LTFF and am unsure about its future trajectory, and so haven&#39;t been fundraising publicly, and I am honestly unsure about the value of more than ~$2M, but my sense is that we have a bunch of grants in the pipeline right now that are blocked on lack of funding that I can evaluate pretty directly, and that those seem like quite solid funding opportunities to me (some of this is caused by a large number of participants of the SERI MATS program applying for funding to continue the research they started during the program, and those applications are both highly time-sensitive and of higher-than-usual quality).</p><p> <strong>Lawrence Chan</strong></p><p> “My main takeaway from [evaluating a batch of AI safety applications on LTFF] is [LTFF] could sure use an extra $2-3m in funding, I want to fund like, 1/3-1/2 of the projects I looked at.” (At the current level of funding, we&#39;re on track to fund a much lower proportion).</p><h2> <strong>Related links</strong></h2><ul><li> <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>EA Funds organizational update: Open Philanthropy matching and distancing</u></a></li><li> <a href="https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations"><u>Long-Term Future Fund: April 2023 grant recommendations</u></a></li><li> <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding"><u>What Does a Marginal Grant at LTFF Look Like?</u></a></li><li> Asya Bergal&#39;s <a href="https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund"><u>Reflections on my time on the Long-Term Future Fund</u></a></li><li> Linch Zhang&#39;s <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><u>Select examples of adverse selection in longtermist grantmaking</u></a></li></ul><h2> <strong>Our Vision</strong></h2><p> We think there is a significant shortage of independent funders in the current longtermist and EA infrastructure landscape, resulting in fewer outstanding projects receiving funding than is good for the world. Currently, the primary source of funding for these projects is Open Philanthropy, and whilst we share a lot of common ground, we think we add value in the following ways:</p><ul><li> Increasing the total grantmaking capacity within key cause areas.</li><li> Causing great projects to counterfactually happen in the world, or saving time and effort for people doing great projects who would otherwise spend significant time fundraising or waiting for grants to come in.</li><li> Supporting a set of worldviews that we find plausible and that are not currently well represented among grantmakers (though we have substantial overlap with Open Philanthropy&#39;s worldview and there is a range of views on how much we should be directly optimizing for diversification away from their perspectives).</li><li> Emphasizing contact with reality: most of our grantmakers spend most of their time trying to directly solve problems of importance within their cause area, rather than engaging in “meta” activities like grantmaking. We think this is important as grantmaking often has very poor feedback loops (particulalry longtermist grantmaking).</li><li> Provide early stage funding to allow applicants to test their fit for work and “get ready” to seek funding from other funders that specialize in larger grant sizes.</li><li> Improving the epistemic environment within EA by making it easier for smaller projects to disagree with Open Philanthropy without worrying that this will significantly reduce their chance of being funded in the future.</li><li> Helping to identify harmful projects whilst being aware of factors such as the unilateralist curse and information cascades.</li><li> Increasing the resilience, robustness and diversity of funders within EA and longtermism.</li></ul><p> Alongside the above, EA Funds has ambitions to pursue new ways of generating value by:</p><ul><li> Creating an expert-led active grant-making program to create counterfactual impactful projects (starting with longtermist information security).</li><li> Modeling and shaping community norms of transparency, integrity, and criticism to improve the epistemic environment within EA and associated communities.</li></ul><h2> <strong>Our Ask</strong></h2><p> <strong>We are looking to raise ~$4.4M from the general public</strong> to support our work over the next 6 months:</p><ol><li> ~$2.7M for the Long-Term Future Fund.<ol><li> This is ~2M above our expected 720k donations in the next 6 months.</li></ol></li><li> ~$1.7m for the EA Infrastructure Fund.<ol><li> This is ~1.3M above our expected 400k donations in the next 6 months.</li></ol></li></ol><p> This will be matched by Open Phil at a 2:1 rate ($2 from Open Phil per $1 donated to a fund) with a ceiling of a $3.5m contribution from Open Phil (per fund). You can read more about the matching <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>here</u></a> .</p><p> The EAIF and LTFF have received very generous donations from many individuals in the EA community. However, donations to the EAIF and LTFF have recently been quite low, especially relative to the quality and quantity of applications we&#39;ve had in the last year. While much of this is likely due to the FTX crash and subsequently increased funding gaps of other longtermist organizations, our guess is that this is partially due to tech stocks and crypto doing poorly in the last year (though we hope that recent market trends will bring back some donors).</p><h3> <strong>Calculation for LTFF funding gap</strong></h3><p> The LTFF has an estimated ideal dispersal rate of $1M/month, based on our <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>post-November 2022 funding bar</u></a> that Asya estimated <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn9u90kjafzyb"><sup>[2]</sup></a> from looking at the funding gaps and marginal resources within the longtermist ecosystem overall. This is $6M over the next 6 months.</p><p> I also think LTFF donors should pay $200k over the next 6 months ($400k annualized) as their “fair share” of EA Funds operational costs. So in total, LTFF would like to spend $6.2M over the next 6 months.</p><p> Caleb <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching#LTFF_funding_gap"><u>estimated</u></a> ~$700k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $2.1M in expected incoming funds, or a shortfall of $4.1M.</p><p> To cover the remaining $4.1M, we would like individual donors to contribute an additional $2M, where Open Phil will provide $2.1M of matching for the first $1.05M.</p><p> To get a sense of what projects your marginal dollars can buy, you might find it helpful to look at the <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>$5M tier of the LTFF Funding Thresholds Post</u></a> .</p><h3> <strong>Calculation for EAIF funding gap</strong></h3><p> The EAIF has an estimated ideal dispersal rate of $800k/month, based on the proportion of our historic spend rate that we believe is above Open Phil&#39;s bar for EA community building projects (though note that this was based on fairly brief input from Open Phil and I didn&#39;t check with them about whether they agree with this claim). This is $4.8M over the next 6 months.</p><p> I also think EAIF donors should pay $200k over the next 6 months ($400k annualized) as their “fair share” of EA Funds operational costs. So in total, EAIF would like to spend $5M over the next 6 months.</p><p> Caleb estimated $400k in expected donations from individuals by default in the next 6 months, based solely on extrapolation from past trends. With Open Phil donation matching, this comes out to a total of $1.2M in expected incoming funds, or a shortfall of $3.8M.</p><p> To cover the remaining $3.8M, we would like individual donors to contribute an additional $1.3M, where Open Phil will provide 2.5M in donation matching.</p><h3> <strong>Potential change for operational expenses payment</strong></h3><p> <strong>Going forwards, we would also like to move towards a model where donors directly pay for our operational expenses</strong> (currently we fundraise for operational expenses separately, so 100% of donations from public donors goes to our grantees). We believe that the newer model is more transparent, as it lets all donors more clearly see the true costs and cost-benefit ratio for their donations <strong>. However, making the change is still pending internal discussions, community feedback, and logistical details.</strong> We will make a separate announcement if and when we switch to a model where a percentage of public donations go to cover our operational expenses. See Appendix A for a calculation of operational expenses.</p><h2> <strong>Why give to EA Funds?</strong></h2><p> We think EA Funds is well-positioned to be a significant independent grantmaker for the following reasons.</p><ol><li> We <a href="https://funds.effectivealtruism.org/team"><strong><u>have knowledgeable part-time fund managers</u></strong></a> <strong>who do direct work in their day jobs:</strong> we have built several grantmaking teams with a broad range of expertise. These managers usually dedicate the majority of their time to hands-on efforts addressing critical issues. We believe this direct experience enhances their judgment as grantmakers, enabling them to pinpoint important and critical projects with high accuracy.</li><li> <strong>Specialization in early-stage grants:</strong> we made over 300 grants of under $300k in 2022. To our knowledge, that&#39;s more grants of this size than any other EA-associated funder.</li><li> <strong>We are the largest open application funding source</strong> (that we are aware of) within our cause areas. Our application form is always open, anyone can apply, and grantees can apply for a wide variety of projects relevant to our funds&#39; purposes (as opposed to eg needing to cater to narrow requests for proposals). We believe this is critical to us having access to grant opportunities that other funders do not have access to, allowing us to rely on formal channels rather than informal networks.</li><li> <strong>Our operational track record</strong> . In 2022, EA Funds paid out ~$35M across its four Funds, with $12M to the Long-Term Future Fund, $13M to the EA Infrastructure Fund, $6.4M to the Animal Welfare Fund, and $4.8M to the Global Health and Development Fund. This requires (among others) clearing nontrivial logistical hurdles in following nonprofit law across multiple countries, consistent operational capacity, and a careful eye towards <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist"><u>downside risk mitigation</u></a> .</li><li> <strong>We believe our grant are highly cost-effective.</strong> Our current best guess is that we have successfully identified and given out grants of similar ex-ante quality to (eg) Open Phil&#39;s AI safety and community building grants, some of which Open Phil would counterfactually not have funded. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnsxw2bil6v9"><sup>[3]</sup></a> This gives donors an opportunity to provide considerable value.</li><li> We are investigating <strong>new value streams</strong> . We would like to pursue &#39;DARPA-style&#39; active grantmaking in priority areas (starting with information security). We are also actively considering setting up an AI Safety-specific fund, enocuraging donors interested in AI safety (but not EA or longtermism) to donate to projects that mitigate large-scale globally catastrophic AI risks.</li><li> According to GWWC, the LTFF is the <strong>main longtermist donation option</strong> available for individual donors to support. We believe that we are a relatively transparent funder, and we are currently thinking about how we can increase our transparency further whilst moving more quickly and maintaining our current standard of decision-making.</li></ol><p> We are primarily looking for funding to support the Long-Term Future Fund and the EA Infrastructure Fund&#39;s grantmaking.</p><p> The Long-Term Future Fund is primarily focused on reducing catastrophic risks from advanced artificial intelligence and biotechnology, as well as building and equipping a community of people focused on safeguarding humanity&#39;s future potential. The EA Infrastructure Fund is focused on increasing the impact of projects that use the principles of effective altruism, in particular amplifying the efforts of people who aim to do an ambitious amount of good from an impartial welfarist and scope-sensitive perspective. We have included some examples of grants each fund has made in the <a href="https://docs.google.com/document/d/1-fKZJfgybYMjfUZc2_G-WwTbkQqFBeDEtbrRW4Th_k8/edit#heading=h.6pad9vbogqm7"><u>highlighted grants</u></a> section.</p><h2> <strong>Our Fund Managers</strong></h2><p> We lean heavily on the experience and judgement of our fund managers. We have around five fund managers on each fund at any given time. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnhamlm91a3z"><sup>[4]</sup></a> Our current fund managers include:</p><ul><li> <strong>Linchuan Zhang (LTFF):</strong> Linchuan (Linch) Zhang is a Senior Researcher at Rethink Priorities working on existential security research. Before joining RP, he worked on time-sensitive forecasting projects around COVID-19. Previously, he programmed for Impossible Foods and Google and has led several EA local groups.</li><li> <strong>Oliver Habryka (LTFF)</strong> : Oliver runs Lightcone Infrastructure, whose main product is Lesswrong. Lesswrong has significantly influenced conversations around rationality and AGI risk, and the LWits community is often credited with having realized the importance of topics such as AGI (and AGI risk), COVID-19, existential risk and crypto much earlier than other comparable communities.</li><li> <strong>Peter Wildeford (EAIF):</strong> co-executive director and co-founder of <a href="https://rethinkpriorities.org/"><u>Rethink Priorities</u></a> , a think tank dedicated to figuring out the best ways to make the world a better place.</li></ul><h3> <strong>Guest Fund Managers</strong></h3><p> <strong>Daniel Eth (LTFF):</strong> Daniel&#39;s research has spanned several areas relevant to longtermism, and he&#39;s currently focused primarily on AI governance. He was previously a Senior Research Scholar at the Future of Humanity Institute. He is currently self-employed.</p><p> <strong>Lauro Langosco (LTFF):</strong> Lauro is a PhD student with David Krueger at the University of Cambridge. His work focused broadly on AI Safety, in particular on demonstrations of alignment failures, forecasting AI capabilities, and scalable AI oversight.</p><p> <strong>Lawrence Chan (LTFF):</strong> Lawrence is a researcher at <a href="https://evals.alignment.org/">ARC Evals</a> , working on safety standards for AI companies. Before joining ARC Evals, he worked at <a href="https://www.redwoodresearch.org/">Redwood Research</a> and as a PhD Student at the <a href="https://humancompatible.ai/">Center for Human Compatible AI</a> at UC Berkeley.</p><p> <strong>Thomas Larsen (LTFF):</strong> Thomas was an alignment research contractor at MIRI, and he is currently running the Center for AI Policy, where he works on AI governance research and advocacy.</p><p> <strong>Clara Collier (LTFF)</strong> : Clara is the managing editor of Asterisk, a quarterly journal focused on communicating insights on important issues. Before, she worked as an independent researcher on existential risks. She has a Masters in Modern Languages from Oxford.</p><p> <strong>Michael Aird (EAIF)</strong> : <strong>&nbsp;</strong> Michael Aird is a Senior Research Manager in Rethink Priorities&#39; AI Governance and Strategy team. He also serves as an advisor to organizations such as Training for Good and is an affiliate of the Centre for the Governance of AI. His prior work includes positions at the Center on Long-Term Risk and the Future of Humanity Institute.</p><p> <strong>Huw Thomas (EAIF):</strong> Huw is currently working part-time on various projects (including a contractor role at 80,000 hours). Prior to this, he worked as a media associate at Longview Philanthropy, a groups associate at the Centre for Effective Altruism and was a recipient of the CEA Community Building Grant for his work at Effective Altruism Oxford.</p><p> <i>You can find a full list of our fund managers</i> <a href="https://funds.effectivealtruism.org/team"><i>here</i></a> <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn3iuaaj786kj"><sup>[5]</sup></a></p><p> If you have more questions, feel free to leave a comment here. Caleb Parikh and the fund managers are also happy to talk to donors potentially willing to give >;$30k. Linch Zhang, in particular, has volunteered himself to talk about the LTFF.</p><h2> <strong>Highlighted Grants</strong></h2><p> EA Funds has identified a variety of high-impact projects, at least some of which we think are unlikely to have been funded elsewhere. (However, for any specific grant listed below, we think there&#39;s a fairly high probability they&#39;d otherwise be funded in some form or another; figuring out counterfactuals is often hard).</p><p> <strong>From the Long-Term Future Fund:</strong></p><ul><li> David Krueger - $200,000<ul><li> Computing resources and researcher stipends at a new deep learning + AI alignment research group at the University of Cambridge.</li></ul></li><li> Alignment Research Center - $72,000<ul><li> A research &amp; networking retreat for winners of the Eliciting Latent Knowledge contest with the aim of fostering promising research collaborations between junior researchers.</li></ul></li><li> SERI MATS program - $316,000<ul><li> 8-week scholars program to pair promising alignment researchers with renowned mentors. This program has now grown into a <a href="https://www.serimats.org/mentors"><u>more established program</u></a> producing multiple people working full-time on alignment in established research organizations (with a smaller number of people pursuing independent research or starting new organizations).</li></ul></li><li> Manifold Markets - $200,000<ul><li> Stipend and expenses for 4 months for 3 FTE to build a forecasting platform made available to the public based on user-created play-money prediction markets</li></ul></li><li> Daniel Filan - $23,544<ul><li> We recommended a grant of $23,544 to pay Daniel Filan for his time making 12 additional episodes of the AI X-risk Research Podcast (AXRP), as well as the costs of hosting, editing, and transcription.</li></ul></li></ul><p> <strong>From the EA Infrastructure Fund:</strong></p><ul><li> Shauna Kravec &amp; Nova DasSarma - $50,000:<ul><li> Compute infrastructure and dedicated support for AI safety researchers to run technical AI experiments. This later became <a href="http://hofvarpnir/"><u>Hofvarpnir Studios</u></a> which used to provide compute for Jacob Steinhardt&#39;s lab at UC Berkeley and the Center for Human-Compatible Artificial Intelligence (CHAI).</li></ul></li><li> Finlay Moorhouse and Luca Righetti - $38,200<ul><li> Ongoing support for <a href="https://hearthisidea.com/"><u>&quot;Hear This Idea</u></a> &quot;, a podcast showcasing new thinking in effective altruism.</li></ul></li><li> Laura Gonzalez Salmerón, Sandra Malagón - $43,308<ul><li> 12-month stipend to coordinate and grow the EA Spanish speakers community and its projects.</li></ul></li><li> Czech Association for Effective Altruism - $ 8,300<ul><li> Expenses and stipend to create a short Czech book (~130 pgs) and brochure (~20 pgs) with a good introduction to EA in digital and print formats.</li></ul></li></ul><p> <i>See a complete list of our public grants at</i> <a href="https://funds.effectivealtruism.org/grants"><i><u>this</u></i></a> <i>link. You can also read the most recent payout report by LTFF</i> <a href="https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations"><i><u>here</u></i></a> <i>.</i></p><h2> <strong>Planned actions over the next six months</strong></h2><p> To achieve our goals of empowering thoughtful people to pursue impactful projects, we&#39;ll attempt to do the following:</p><ul><li> Asya Bergal <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>will step down</u></a> as chair of LTFF (Max Daniel has already stepped down as chair of the EAIF). Max and Asya both work for Open Phil, and we want to increase our separation from Open Phil. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnpwyc17wm0ag"><sup>[6]</sup></a><ul><li> Open Phil also wanted to reduce entanglements between the two organizations, in part to mitigate downside reputational risks.</li></ul></li><li> We are looking to find new fund chairs for both LTFF and EAIF.</li><li> We plan to onboard more fund managers to grow each fund substantially (aiming to double the staffing of each fund).<ul><li> In recent months, LTFF has onboarded Lauro Langosco and Lawrence Chan who will primarily focus on technical alignment grantmaking, as well as Clara Collier for her expertise in communications and general longtermism. The EAIF is in the process of onboarding new fund managers.</li></ul></li><li> Open Phil <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching"><u>has agreed</u></a> to give us a 2:1 match for up to $7M total (up to $3.5M to each of EAIF and LTFF) for a 6-month period. While our ultimate goal is to develop our own robust funding base, in 2022, Open Philanthropy provided 40% of the funding for the Long-Term Future Fund and 84% for the EA Infrastructure Fund. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnizbbefz7rym"><sup>[7]</sup></a> We see donation matching as a realistic intermediary step while enabling us to pursue more intellectual independence.<ul><li> This model replaces fixed grants from Open Philanthropy. This reduces the likelihood of your donations being fungible: previously an extra $1 to EA Funds in fundraising could result in a $1 reduction in Open Philanthropy&#39;s grants to us, diverting those funds to their other projects. This newer approach allows funders to donate to EA Funds and support the specific value proposition that we, as opposed to Open Philanthropy, present. <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fnwqj0clhbbi9"><sup>[8]</sup></a></li></ul></li><li> We are considering hiring or contracting out more non-grantmaking duties (eg website, project management, fundraising, communications) at EA Funds. Right now Caleb is the only full-time employee of EA Funds and plausibly having 0.5-1.5 more FTEs at EA Funds will help both existing projects go more smoothly, as well as unlock new ambitious opportunities.</li><li> We are working with external investigators to do retroactive evaluations of past EAIF and LTFF grants, with the hopes that we can then have a clearer picture of a) how well the impact of our past grants compares to eg, Open Phil&#39;s, b) which of our broader categories of historical grants have been the most impactful, and c) other qualitative insights to help us improve further.</li><li> We aim to improve the operations of our passive grantmaking (funding of open grant applications) program with a focus on improving the grantee experience by providing more support to grantees and getting back to grantees much more quickly <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now#fn25bgwshzbdx"><sup>[9]</sup></a></li><li> We are trying to reconceptualize and reframe the value proposition and strategic direction of EAIF in the coming months. While much of this will be contingent on the vision of the incoming fund chair, we&#39;d like EAIF to have a more coherent and targeted vision, strategy, and coherent value proposition to donors going forwards.</li><li> We plan to create a new AI Safety specific program, for donors outside of EA/Longtermism who want to decrease catastrophic risks from AI. We hope that such a program can inspire new donors to give to AI safety projects.</li><li> EA Funds is pursuing active grant-making programs, where we&#39;ll actively seek out promising projects to fund. We&#39;ll initially focus on <a href="https://80000hours.org/career-reviews/information-security/"><u>Information Security</u></a> field building. The current plan is for this program to initially be funded by Open Philanthropy, though if you are interested in contributing to this program in particular, please let us know.</li></ul><h2> <strong>Potential negatives to be aware of</strong></h2><p> Here are some reasons you might <i>not</i> want to donate to EA Funds:</p><h3> <strong>Potential downside risks of LTFF or EAIF</strong></h3><ul><li> <strong>Inability to fully screen for or prevent unilateral downside risks:</strong> EA Funds has much less control over and offers less guidance to our grantees than, eg, the executive directors of a moderately-sized EA organization. So compared to larger organizations, we may be less able to prevent unilateral downside risks like the <i>sharing of information hazards</i> , or <i>&nbsp;</i> actions that pose <i>reputational risks</i> to effective altruism at large, or to specific EA subfields.</li><li> <strong>Centralization of funds:</strong> In contrast, we are also implicitly asking for the centralization of funds from private donors to a single grantmaking entity. To the extent that you believe your counterfactual for donating to EA Funds is better and/or more centralization is bad, you may wish to donate directly rather than pool your funds with other LTFF or EAIF donors.</li><li> <strong>Waste/Inefficient usage of human capital:</strong> Giving money to EA Funds rather than larger organizations implicitly subsidizes a culture and community of grantseekers who are supported by small grants. To the extent that you believe this is a less efficient usage of human capital than plausible counterfactuals for talented people (eg getting a job in tech, policy, or academia), you might want to shift away from EA grantmakers that give relatively small individual grants.</li></ul><p> Note that we consider these issues to be structural and do not realistically expect resolutions to these downside risks going forwards.</p><h3> <strong>Areas of improvement for the LTFF and EAIF</strong></h3><p> Historically, we&#39;ve had the following (hopefully fixable) problems:</p><ul><li> <strong>Slower than ideal response times</strong> : in the past year, our median response time has been around 4 weeks with high variance; we&#39;d like to get this down to closer to 2 weeks with 95% of applications responded to in 4 four weeks.</li><li> <strong>Limited feedback/advice given to grantees</strong> : we generally don&#39;t give feedback to rejected applicants. We currently give <i>some</i> feedback to promising grantees but much less than we&#39;d give if we had more grantmaking capacity.</li><li> <strong>Insufficient active grantmaking</strong> : We spend some time trying to improve our grantees&#39; projects, but we have invested fairly little in active grantmaking (actively identifying promising projects and creating/supporting them).</li><li> <strong>Missing areas of subject matter expertise</strong> : The scopes of both funds are quite expansive. This means sometimes all of the existing grantmakers lack sufficient direct technical subject matter expertise to evaluate grants in certain areas, and thus have to rely on external experts. For example, the LTFF does not currently have a technical expert in biosecurity.</li></ul><p> For more, you can read Asya&#39;s <a href="https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund"><u>reflections on her time as chair of LTFF</u></a> .</p><h2> <strong>EAIF vs LTFF</strong></h2><p> Some donors are interested in giving to both the EAIF and LTFF and would like advice on which fund is a better fit for them.</p><p> We think that the EAIF is a better fit for donors who:</p><ul><li> Are interested in supporting a portfolio of meta projects covering a range of plausible worldviews (both longtermist and non-longtermist).</li><li> Are interested in building EA and adjacent communities.</li><li> Believe that EA (and EA community building) has historically been very good for the world.</li><li> Believe in multiplier effect arguments (donating $100 to an EA group could plausibly create far more than $100 in donation to high-impact charities by encouraging more people to donate).</li><li> Expect the EAIF and LTFF to have similar diminishing marginal returns curves and want to donate to the fund with lower funding. (EAIF and LTFF each receive about 1000 grant applications per year, but EAIF has less funding currently committed)</li></ul><p> We think that the LTFF is a better fit for donors who are:</p><ul><li> More compelled by longtermist cause areas than other EA cause areas.</li><li> Particularly interested in AI safety.</li><li> Are more interested in direct work than &quot;meta&quot; work that have a longer chain of impact/reasoning.</li><li> Are more excited about the <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding#_5M"><u>$5M tier of marginal LTFF grants</u></a> than what they consider to be the marginal EAIF grant.</li></ul><h2> <strong>Closing thoughts</strong></h2><p> This post was written by Caleb Parikh and Linch Zhang. Feel free to ask questions or give us feedback in the comments below.</p><p> If you are interested in donating to either LTFF or EAIF, you can do so <a href="https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds"><u>here</u></a> .</p><h2> <strong>Appendix A: Operational expenses calculations and transparency.</strong></h2><p> In the last year, EA Funds has dispersed $35M and spent ~700k in operational expenses. The vast majority of the operational expenses were spent on LTFF and EAIF, as the global health and development fund and animal welfare fund are operationally much simpler.</p><p> Historically, ~60-80% of the operational expenses are paid to <a href="https://ev.org/ops/about/"><u>EV Ops</u></a> , for grant disbursement, tech, legal, other ops, etc.</p><p> The remaining 20-40% is used for:</p><ul><li> Caleb&#39;s salary, who leads EA Funds (~$100k/year plus benefits).</li><li> Payments for grantmakers at $60/hour, though many volunteer for free.</li><li> Contractors who work on different projects, earning between $35-$100/hour.</li></ul><p> I (Linch) ballparked the expected annual expenditures going forwards (assuming no cutbacks) to be ~800k annually. I estimated the increase due to a) inflation and b) us wanting to take on more projects, with some savings from us slowing down the rate of dispersals a little. But this estimate is not exact.</p><p> Since LTFF and EAIF incur the highest expenses, I suggest donors to each should contribute around $400k yearly, or $200k every six months.</p><p> As for where we might cut or increase spending:</p><ul><li> Reducing EV Ops costs would be challenging and may require moving EA Funds out of EV and building our own grant ops team.</li><li> Reducing Caleb&#39;s working hours would be challenging.</li></ul><p> I think my own hours at EAF are somewhat contingent on operational funding. In the last month, I&#39;ve been spending more than half of my working hours on EA Funds (EA Funds is buying out my time at RP), mostly helping Caleb with communications and strategic direction. I will like to continue doing this until I believe EA Funds is in a good state (or we decide to discontinue or sunset projects I&#39;m involved in). Obviously whether there is enough budget to pay for my time is a crux for whether I should continue here.</p><p> Assuming we can pay for my time, other plausible uses of marginal operational funding include: a) whether we pay external investigators for extensive or just shallow retroactive evaluations, b) whether we attempt to launch new programs, c) whether the new infosec, AI safety project, etc websites have professional designers, etc. My personal view is that marginal spending on EA Funds expenses is quite impactful relative to other possible donations, but I understand if donors do not feel the same way and will prefer a higher percentage of donations go directly to our grantees (currently it&#39;s 100% but proposed changes may move this to ~ 94-97%).</p><br/><br/> <a href="https://www.lesswrong.com/posts/gRfy2Q2Pg25a2cHyY/ltff-and-eaif-are-unusually-funding-constrained-right-now#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/gRfy2Q2Pg25a2cHyY/ltff-and-eaif-are-unusually-funding-constrained-right-now<guid ispermalink="false"> gRfy2Q2Pg25a2cHyY</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Wed, 30 Aug 2023 01:03:30 GMT</pubDate> </item><item><title><![CDATA[Paper Walkthrough: Automated Circuit Discovery with Arthur Conmy]]></title><description><![CDATA[Published on August 29, 2023 10:07 PM GMT<br/><br/><p> Arthur Conmy&#39;s <a href="https://arxiv.org/pdf/2304.14997.pdf">Automated Circuit Discovery</a> is a great paper that makes initial forays into automating parts of mechanistic interpretability (specifically, automatically finding a sparse subgraph for a circuit). In this three part series of Youtube videos, I interview him about the paper, and we walk through it and discuss the key results and takeaways. We discuss the high-level point of the paper and what researchers should takeaway from it, the ACDC algorithm and its key nuances, existing baselines and how they adapted them to be relevant to circuit discovery, how well the algorithm works, and how you can even evaluate how well an interpretability method works.</p><br/><br/> <a href="https://www.lesswrong.com/posts/FzqKXpTDaouMF6Chj/paper-walkthrough-automated-circuit-discovery-with-arthur#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FzqKXpTDaouMF6Chj/paper-walkthrough-automated-circuit-discovery-with-arthur<guid ispermalink="false"> FzqKXpTDaouMF6Chj</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Tue, 29 Aug 2023 22:07:06 GMT</pubDate> </item><item><title><![CDATA[An OV-Coherent Toy Model of Attention Head Superposition]]></title><description><![CDATA[Published on August 29, 2023 7:44 PM GMT<br/><br/><p> <strong>Background</strong></p><p> This project was inspired by Anthropic&#39;s <a href="https://transformer-circuits.pub/2023/may-update/index.html"><u>post</u></a> on attention head superposition, which constructed a toy model trained to learn a circuit to identify skip-trigrams that are OV-incoherent (attending from multiple destination tokens to a single source token) as a way to ensure that superposition would occur. Since the OV circuit only sees half of the information – the source tokens – the OV circuit of a single head cannot distinguish between multiple possible skip-trigrams. As long as there are more skip-trigrams with the same source-token to represent than heads, the model cannot represent them in the naive way, and may resort to superposition.<br></p><p> In a more recent update <a href="https://transformer-circuits.pub/2023/july-update/index.html"><u>post</u></a> , they found that the underlying algorithm for OV-incoherent skip-trigrams in a simpler 2-head model implemented a conditional on the source token. One head predicts the output for the skip trigram [current token] … [current token] ->; [ground truth([0]...[current token])], one of which will yield the right answer. The second head destructively interferes with this result by writing out the negative logit contribution of the first head if the source token is not the one common to all skip-trigrams (in this case, [0]). Because their example cleanly separated tasks between the two attention heads, the authors argued that it was more like the building of high-level features out of low-level ones than a feature superimposed across multiple attention heads.</p><p></p><p> <strong>OV-coherent Superposition</strong></p><p> Instead, we claim there is an analogous force pushing the model toward adopting a distributed representation/head superposition whenever the model must learn patterns that require implementing nonlinear functions of <i>multiple</i> source tokens given a fixed destination token. We call this “OV-coherent” superposition: despite of the information at the destination position being fixed, the information copied from an attended-to token depends on the information at source tokens to which it is not attending. This pushes the model to form interference patterns between heads attending to different tokens.</p><p> To test this, we implemented a 1-layer, attention-only toy model with one-hot (un)embeddings trained to solve a problem requiring attention to multiple source tokens, described below. Here, we focus on a 2-head model which solves the task with perfect accuracy, and lay out some interesting motifs for further investigation.</p><p></p><p> <i>Key Takeaways</i> :</p><p> Heads in our model seem to implement nested conditional statements that exploit the if-else nature of the QK circuits. This means they can learn to write more specific information conditional on attending to certain tokens, given that it can implicitly rule out the existence of other tokens elsewhere in the context. The heads furthermore implement these nested conditionals in such a way that they distribute important source tokens between them, and constructively interfere to produce the correct answer.</p><p> Most of the time, we found that this “conditional dependence” relies on heads implementing an “all or nothing” approach to attention. Heads do not generally* spread their attention across multiple interesting tokens, but instead move through the hierarchy of features in their QK circuits and attend to the most “interesting” (still a poorly defined term!) one present. This seems to be a common property of attention patterns in real-model heads as well.</p><p> When there are multiple important source tokens to attend to in the context, heads implementing interference schema will tend to learn QK circuits such that they distribute tokens amongst themselves and don&#39;t leave crucial information unattended to. In 2-head models, this manifests are reversed “preference orderings” over source tokens, but potentially more complicated arrangements work in larger models.</p><p></p><p> <i>Problem Details:</i></p><p> The inputs are sequences of integers in the range [0, 11]. They can be broken down into two categories:</p><ul><li> <strong>Noise:</strong> There is no pattern for the model to learn. The context and completion are drawn independently and uniformly from [5, 11].</li><li> <strong>Signal:</strong> The final token in the input is 0. Furthermore, exactly two tokens in the context are in the range [1, 4]. The correct completion is given by an arbitrary injection from (unordered) pairs of these “interesting” tokens to completions. For example, any sequence containing 1 and 4 should be followed with a 3 if the current token is 0  (“1….4 0” ->; “3”). Note these numbers are independently drawn, allowing for repetitions ( “3….3 0” ->; “7”).</li></ul><p><br> <strong>Example Model Solution: (d_head = 5, n_heads = 2, d_model = 11, no LayerNorm)</strong></p><p> <i>QK-circuit Behavior:</i></p><p> Both heads learn a strict “preference ordering” over signal tokens, with the ordering generally reversed between the two heads. This guarantees that, for any given context, both signal tokens are fully attended to. In the example below, H0 attended solely to “2” if it&#39;s in the context, else “3”, “4”, and finally “1”. H1 instead has the preference ordering “1” else “4”, “3”, and then “2”. <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KicP8fBdHNjZBXxRB/q9vz4tsfeuiwdh1b0ih9"></p><p> <i>Attention scores from “0” to each signal token for each head</i></p><p><br> While this “flipped hierarchy” scheme is overwhelmingly more common, the model sometimes learned a “split attention” scheme during training, in which the heads would attend to the same tokens to varying degree. Notably, we only saw split attention for d_head &lt; 3, indicating that this may be a bottleneck issue. However, we should acknowledge that the model may have other ways of solving this simple task than the one outlined above, and indicate that this was by no means an exhaustive analysis.*</p><p><br> <i>OV-circuit Behaviour:</i></p><p> In the OV circuit, heads use the attention hierarchy described above to write to the logits of completions consistent with the tokens it attends to. For example, if H0 attends to “1”, it will positively contribute to the output logits of tokens mapped to by unordered pairs (1,1), (1,2), (1,3), and (1,4).</p><p> <i>However,</i> the heads also exploit a “conditional dependence” between source tokens; if H0 attends to token X, it “knows” that the other source position does not contain a token Y higher in its attention hierarchy than X, or else it would be attending there instead. It can then safely <i>not</i> contribute to the logits of the output mapped to by (X, Y).</p><p> We can see this clearly in the graph below, which shows the direct logit effect of each head conditional on attending to each signal token: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/KicP8fBdHNjZBXxRB/ou7wkznfogikdjkhkoly"></p><p> <i>Logit contribution of heads to completions corresponding to pairs of signal tokens (x axis), conditional on attention to source token (y axis). Cross-referencing with the attention-hierarchy above shows that heads get more specific with their outputs on less-interesting tokens, and for any given pair of inputs they will constructively interfere on the correct answer</i></p><p><br> When the head attends to its “favourite” token (“2” and “1” respectively), it implicitly has no information about the other position, and so writes roughly uniformly to the logits of all possible completions. But as they run through their preferences, the heads successively write strongly to the logits of one fewer completion. For contexts that contain their “least favourite” token repeated, the heads can confidently guess the correct answer by a process of eliminating options from its own attention hierarchy.</p><p> In contexts where both heads write to multiple outputs, they will only constructively interfere on the correct token. For example, when the sequence is “3…4 0”, each head will write to multiple completions corresponding to (3,...) for one head and (4, …) for the other. However, only the correct answer – the completion corresponding to (3, 4)) – will receive positive logit contributions from both heads.</p><p><br> <strong>Observations</strong></p><p> We think this shows an interesting example of heads performing computation in superposition. Moreover, the incentive pushing the model towards interference is qualitatively very different from the OV-incoherence explored by Anthropic. Instead of needing to copy different information from a fixed source token to a variable destination token, our problem imposes the constraint that although the destination token is fixed, the information that a head would need to copy from a source token depends on information elsewhere in the context.</p><p> For n_heads = 2, the “mutual reverse ordering” between the heads is an elegant way for the model to ensure that, no matter which signal tokens show up in the context, each will be attended to.* It is plausible that real world models implement this mechanism (albeit much less crisply) to distribute important features across heads.</p><p> This conditional independent seems to us an interesting way to view the relationship between the QK and OV circuits. OV circuits can extract more information from individual tokens by learning to exploit the distribution of features of their QK circuits. One way of interpreting this might be that heads can implicitly copy information from multiple tokens even when attending to a single token. In other words: the heads can learn to write information as a function of the broader context by exploiting the fact that conditioning on their attention to a token gives information about the distribution of tokens in the entire sequence.</p><p></p><p> <strong>Future Work - Toy Models</strong></p><p> <i>Sparsity:</i></p><p> First, we did not vary sparsity or importance of signal tokens in these experiments. We have a pretty poor idea of how these variables affect the behaviours we observed here, so this seems something worth looking into.</p><p> <i>*The Split attention model, d_head bottlenecks, and the surprising resourcefulness of networks:</i></p><p> We presented our findings for d_head =5 because the learned algorithm is more human-parsible, but we were surprised by how limited we could make this model while achieving perfect accuracy on a signal-only test set. We were particularly surprised by the model&#39;s capability for d_head = 1, since this essentially limits each head to a scalar degree of freedom for each token. The split attention mechanism we stumbled upon for d_head &lt; 3 are much harder to parse, and may rely on more complicated context-specific solutions. However, the fact that we haven&#39;t seen these for larger d_head supports the idea that, for d_head >;=3, the signal tokens can be stored orthogonally in that dimension.</p><p> We were also surprised that this problem can be solved with one head, as long as d_head >;= 4. Intuitively, once a head has enough dimensions to store every &quot;interesting&quot; token orthogonally, its OV circuit can simply learn to map each of these basis vectors to the corresponding completions. Possibly there is a trade-off here between degrees of freedom in d_head and n_heads, though this is not super crisp. There is also the possibility that superposition is happening both in the n_head dimension <i>and per head</i> in the d_head dimension. In contrast with the “hierarchical” superposition presented above, we can call the latter type “bottleneck superposition”. While this complicates the picture substantially, it also opens up some pretty interesting possibilities and is something we&#39;d like to investigate more thoroughly!</p><p> <strong>Future Work - LLMs</strong></p><p> We initially set out to investigate attention head superposition “in the wild” as part of Neel Nanda&#39;s SERI MATS stream, by studying possible linear combinations of name mover heads in Redwood Research&#39;s IOI task. At first, this seemed like a good way to study a realistic (but not too realistic) problem that already had a circuit of attention heads in place. However, we quickly realized that this task represents a strange middle ground in terms of complexity: simple enough to want to represent a single feature (name moving), but complicated enough to have more features hidden within it. While even the toy model seems to have opened up some puzzling new directions, we also think it would be worthwhile to look at head superposition in real-world LLMs.</p><p> For example, it could be interesting to look for cases of inverted preference orderings in the wild by hunting for pairs of QK circuits that exhibit similar behavior. This will likely be messy, and will likely require a better understanding of the relationship between the two types of superposition mentioned above.</p><p><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1<guid ispermalink="false"> cqRGZisKbpSjgaJbc</guid><dc:creator><![CDATA[LaurenGreenspan]]></dc:creator><pubDate> Tue, 29 Aug 2023 19:44:11 GMT</pubDate> </item><item><title><![CDATA[The Economics of the Asteroid Deflection Problem (Dominant Assurance Contracts)]]></title><description><![CDATA[Published on August 29, 2023 6:28 PM GMT<br/><br/><p> Imagine a world with no ads or paywalls. A world where open-source software gets the same level of funding as proprietary software. A world where people can freely reuse ideas and music without paying royalties. <a href="https://www.lesswrong.com/posts/yYvgmKGMtzKxbJZew/update-on-book-review-dominant-assurance-contract">A world where people get paid for writing book reviews.</a> A world where Game-of-Thrones-quality shows are freely available on YouTube. <i>A world where AI safety research gets the same-level of funding as AI capabilities research.</i> Is this a fantasy world? No, this is the world where people use <i>Dominant Assurance Contracts</i></p><p> If you are already convinced you can make this idea a reality by <a href="https://dac.mowzer.co.za">donating to create a Platform for Dominant Assurance Contracts.</a> If you are not convinced read on.</p><h1> The Free-rider problem</h1><p> A few months ago I stumbled across this video. (I highly recommend you watch the video, but if you don&#39;t have time, I&#39;ve summarized the video below). </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=hA2z-X31IvI"><div><iframe src="https://www.youtube.com/embed/hA2z-X31IvI" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> Summary of A Deeper Look at Public Goods</h2><p> A good is <i>rival</i> if one person&#39;s use of a good diminishes another person&#39;s ability to benefit from it. Jeans are rival. If <i>I&#39;m</i> wearing a pair of jeans, <i>you</i> can&#39;t wear it at the same time. Asteroid deflection is <i>non-rival.</i> If <i>I</i> deflect an asteroid to protect myself, <i>you</i> are saved with no additional cost.</p><p> A good is <i>excludable</i> if people who don&#39;t pay can be easily prevented from using a good. An example of a good that is excludable is a pair of jeans. You can exclude people by locking the jeans in your closet. An example of a good that is non-excludable is asteroid deflection. You cannot prevent the people who did not pay for the asteroid deflection program from benefiting from the asteroid being deflected.</p><p> A good which is both rival and excludable is called a <i>private good.</i> A good which is non-rival and non-excludable is called a <i>public good</i> .</p><p> (Additionally, goods which are excludable and non-rival are called <i>club goods,</i> and goods which are non-excludable but rival are called <i>common resources.</i> We won&#39;t be focusing on these types of goods, but I&#39;ve mentioned them for completeness)</p><figure class="table"><table><thead><tr><th></th><th> Excludable</th><th> Non-Excludable</th></tr></thead><tbody><tr><th> Rival</th><td> <i>Private Good</i></td><td> Common Resources</td></tr><tr><th> Non-Rival</th><td> Club Good</td><td> <i>Public Good</i></td></tr></tbody></table></figure><p> Markets are good at providing private goods because</p><ul><li> by excluding people who don&#39;t pay, consumers are incentivized to pay, which incentivizes producers to produce, and</li><li> since private goods are non-rival it is efficient to exclude consumers who aren&#39;t willing to pay (if the benefit to the consumer was greater than the cost, the consumer would be willing to pay).</li></ul><p> Public goods challenge markets because</p><ul><li> consumers who don&#39;t pay can&#39;t be excluded, consumers are incentivized instead to <i>free-ride</i> (ie benefit from the good without paying) and thus producers have no incentive to produce.</li><li> Additionally, even if we could figure out a way to exclude non-payers, (eg by executing everyone who doesn&#39;t pay for the asteroid deflection program), it is <i>inefficient</i> to do so (being non-rival means there are no additional costs to non-payers benefiting).</li></ul><h2> Examples of public goods</h2><ul><li> Information<ul><li> Journalism</li><li> Prediction markets</li><li> Scientific research</li><li> Educational material</li></ul></li><li> Media<ul><li> TV series</li><li>电影</li><li>YouTube videos</li><li>图书</li><li>Short-stories</li><li>艺术</li><li>播客</li></ul></li><li>Software</li><li> Safety<ul><li> Neighborhood watches</li><li> Vaccines and other public health interventions</li><li> AI safety</li><li> Military defense</li></ul></li><li> Public spaces<ul><li> Public roads</li><li> Public parks</li></ul></li></ul><h2> Isn&#39;t there a clever mechanism to solve the free-rider problem?</h2><p> This video stuck with me. The fact the public goods are inefficiently provided by the market seems like the main issue with our civilization. Heck, AI Safety is a public good.</p><p> The other thing that stuck is that <i>this seems so solvable.</i> Surely, there is a clever mechanism that can fix this issue? So I went to the <a href="https://en.wikipedia.org/wiki/Free-rider_problem#Economic_and_political_solutions">Wikipedia page of the Free-rider Problem</a> and scrolled to the bottom, and lo and behold it was just sitting there: <a href="https://en.wikipedia.org/wiki/Assurance_contract#Dominant_assurance_contracts">Dominant Assurance Contracts</a> invented in 1998 by Alex Tabarrok (yes, the same person who presented the video you just watched). There is only one problem: <i>no one is using them</i> .</p><h1> What is a dominant assurance contract?</h1><p> (I recommend you watch the first 30 minutes of this video. It&#39;s really good, but if you don&#39;t have time I wrote a short explanation below that you can read instead). </p><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=Cjxl11sAbN0"><div><iframe src="https://www.youtube.com/embed/Cjxl11sAbN0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2> My short explanation of dominant assurance contracts</h2><p> Suppose there are 10 villagers on an unpaved street. Bob the Builder will pave the street for $90. Each villager is willing to contribute up to $15 to have the street paved. Additionally, there is there is a $1 transaction cost to contributing (eg from time spent signing forms, from credit card fees, from opportunity cost of not earning interest on that money, etc.).</p><p> If the Bob the Builder gets everyone to contribute, each villager will only have to pay $90 / 10 = $9 and profit $15 - $9 - $1 = $5. However, one of the villagers, Free-riding Frank, is hesitant to contribute. Consider Frank&#39;s decision table:</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (15, 4)</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> (-1, 0)</td><td> (5, 5)</td></tr></tbody></table></figure><ul><li> If <strong>nobody contributes</strong> then road doesn&#39;t get paved and Free-riding Frank and the other villagers both profit $0.</li><li> If <strong>Frank contributes</strong> <strong>but the others don&#39;t</strong> then Frank pledges $15 dollars (the maximum he is willing to pay) and incurs a transaction cost of $1. Since no one else contributed, Bob was unable to collect $90, so refunds Frank the $15. Frank, however, still incurred the $1 transaction cost.</li><li> If <strong>the others contribute but Frank doesn&#39;t</strong> then Free-riding Frank profits $15, but the others pay $90 / 9 + $1 = $ 11 and so only profit $4.</li><li> If <strong>Frank and the others all contribute</strong> then they all pay $9 dollars, incur a transaction cost of $1 and profit $5.</li></ul><p> Looking at the table and holding the others strategy fixed, Free-riding Frank realizes that the dominant strategy in this game is to not contribute (0 vs -1 and 15 vs 5). Of course, everyone else in the village realizes this too and Bob is unable to raise the funds to pave the road.</p><p> Suppose Bob changes the game to a <i>Dominant Assurance Contract.</i> Dominant<strong> </strong>Assurance Contracts have two conditions:</p><ul><li> <strong>Assurance:</strong> Bob will only build the road if all 10 villagers contribute.</li><li> <strong>Refund Bonus:</strong> Bob puts up $20 of his own money as collateral <strong>.</strong> If some villagers do not contribute then Bob will refund all villagers that did contribute what they pledged plus an equal share of the collateral: $2 per villager.</li></ul><p> Frank&#39;s decision table now looks something like this:</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (0, 1)</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> (1, 0)</td><td> (5, 5)</td></tr></tbody></table></figure><ul><li> If <strong>nobody contributes</strong> then road still doesn&#39;t get paved and Free-riding Frank and the other villagers both profit $0.</li><li> If <strong>Frank contributes</strong> <strong>but the others don&#39;t</strong> Frank pays the transaction cost of $1, but since the road was not built, the Bob refunds Frank an additional $2 [ <strong>Refund Bonus]</strong> . <i>Frank profits $1</i></li><li> If <strong>the others contribute but Frank doesn&#39;t</strong> <i>the road does not get built</i> [ <strong>Assurance]</strong> . The other villagers incur $1 transaction costs and get an additional $2 refund [ <strong>Refund Bonus]</strong> . They profit $1</li><li> If <strong>Frank and the others all contribute</strong> then they all pay $9 dollars and a transaction cost of $1, the road gets built and everyone profits $5.</li></ul><p> Looking at the table and holding the others strategy fixed, Free-riding Frank now sees that <i>the dominant strategy in this game is to contribute</i> (0 vs 1 and 0 vs 5). Of course, everyone else in the village realizes this too and the road gets paved!</p><h1> Challenges of dominant assurance contracts</h1><h2> If dominant assurance contracts are so great, why is nobody using them?</h2><p> The problem is that the producers of public goods don&#39;t know about them.</p><p> Example: Element is building Matrix, an open-standards chat app (ie a public good). Their business model is to give away the code/standards for free and sell consulting services. Unfortunately, their competitors have undercut them by selling consulting services without contributing back to the code/standards. <a href="https://news.ycombinator.com/item?id=33752467">On HackerNews you can see their CEO complain about it as &quot;The Tragedy of the Commons&quot;.</a> Except he is not experiencing the tragedy of the commons. He is experiencing the free-rider problem.</p><p> If he doesn&#39;t even know what problem he has, he is not going to be able to: google &quot;the free rider problem&quot;; open the Wikipedia page on &quot;the free-rider problem&quot;; scroll to the bottom to find the section titled &quot;Solutions&quot;; and then read about dominant assurance contracts.</p><p> People who are trying to produce public goods don&#39;t seem to know that that&#39;s what they are doing, and that dominant assurance contracts are a way to get funding.</p><p> My main goal is to give the idea enough airtime so that when people want to produce public goods, the first tool they reach for is a dominant assurance contract.</p><h2> Do they work in practice?</h2><p> <a href="https://mason.gmu.edu/~atabarro/BetterCrowdfunding.pdf">Research done in a lab shows that dominant assurance contracts reduce failure rate of assurance contracts from 60% to 40%</a> .</p><p> I only know of four attempts done in the wild (tell me if you know about more).</p><ul><li> <a href="https://marginalrevolution.com/marginalrevolution/2013/08/a-test-of-dominant-assurance-contracts.html">Donation to The Center for Election Science</a> .<ul><li> Succeeded</li><li> Raised $300 in donations from 20 people.</li><li> Some information was publicly provided about the number of people who pledged before the deadline.</li><li> Final 3 pledges came in the last half hour.</li></ul></li><li> <a href="https://forum.effectivealtruism.org/posts/gFLndH6FFLbvw5uDh/update-on-book-review-dominant-assurance-contract">Book Review by Arjun Panickssery</a> .<ul><li> Failed</li><li> 9/10 or $225/250</li><li> No information publicly provided about the number of people who pledged before the deadline.</li><li> This one failed, but got really close 9/10 or $225/$250 for writing a book review! I want to emphasize how ridiculous it is that it got this close. 9 people were willing to pay $25 for a <a href="https://www.amazon.com/Sense-Style-Thinking-Persons-Writing/dp/0143127799">book review of a book that only costs $18!</a> I think if they used a platform like Kickstarter, a kind soul would have seen that they were only $25 dollars away from succeeding and would have donated.</li></ul></li><li> <a href="https://manifold.markets/Austin/kickstart-will-the-manifold-communi">Dark Mode for manifold.markets by Austin</a> .<ul><li> Failed(?)</li><li> Tried to raise $2500 of play money.</li><li> Pledges were publicly visible</li><li> I&#39;m not sure how to count this one. They failed to raise $2500 worth of <i>play</i> money but then <i>they implemented dark mode anyway.</i> So the public good was provided, even though the contract failed.</li></ul></li><li> <a href="https://www.lesswrong.com/posts/nwjTPtbvcJeA6xDuu/dominant-assurance-contract-experiment-2-berkeley-house">Berkeley House Dinners by Arjun Panickssery</a> .<ul><li> Failed</li><li> Asked for $700</li><li> Pledges were not publicized before the deadline.</li><li> Asked for $700 dollars to fund a house dinner. Arjun hasn&#39;t done a write-up of the result, but he failed. Also the in the first few days after publication the payment links weren&#39;t working.</li></ul></li></ul><p> 1/4 doesn&#39;t seem so good. Dominant assurance contracts work in theory so what is going on?</p><h2> Main problems ignored by theory</h2><p> The theoretical model of Dominant Assurance Contracts assumes away some things that you have to deal with in the real word</p><ul><li> <strong>Perfect information about pricing:</strong> In the example problem above, we assumed that 10 villagers were willing to pay $15 to pave the road. In real life you do not have that information and risk over-pricing/under-pricing your contract. Presumably the contracts run in the real world that failed were over-priced.</li><li> <strong>Perfect marketing:</strong> In the example problem above, we assumed that villagers who would have benefited knew that Bob the Builder was raising funds. In real life you have to market to potential customers and many potential customers will probably not know about your product. It&#39;s possible that the contracts run in the real world failed to reach all potential customers who would have benefited.</li><li> <strong>Game theory/psychology:</strong> In the example above Free-riding Frank was fully aware of the other villagers strategy. In reality we don&#39;t know how other people are going to behave.</li><li> <strong>Shady:</strong> Dominant assurance contracts seems gambling-like/scam-like the same way that crypto-currency does. <a href=" https://www.lesswrong.com/posts/nwjTPtbvcJeA6xDuu/dominant-assurance-contract-experiment-2-berkeley-house?commentId=EKaFMS5xv5cyvmSxn">The top comment on the Berkeley House Dinner Project</a> is &quot;this is a dollar-auction (AKA scam)&quot;, and the second top comment is &quot;Surely this is illegal?&quot;</li></ul><h3> Solution to pricing problem</h3><p> I believe the pricing problem can be solved by prediction markets. Book Review, Dark Mode and Berkeley House Dinner all had prediction markets on whether they&#39;d reach their target. The markets were at 90%, 10%, 45%. These probabilities seem consistent with how much money each of the projects raised.</p><p> Before we launch a project we could run a multiple-choice prediction market on <a href="https://manifold.markets">manifold</a> : &quot;The Project will raise $X&quot;, &quot;The Project will not raise $X&quot;, &quot;The Project will not launch or will launch with a price different from $X&quot;. Only if &quot;The Project will raise $X&quot; is sufficiently high, will we launch the project.</p><h3> Solution to marketing problem</h3><p> This problem is not unique to dominant assurance contracts. So it can be solved in the same way everyone else does it.</p><p> A cool idea is to partner with someone with expertise in marketing. The advertiser can put up the collateral for the refund bonus. If the project succeeds the producer can give them a cut of the funds raised. In this way, the advertiser is incentivized to make the project to succeed.</p><h3> Solution to game theory/psychology</h3><p> I think having a progress bar showing how many people have pledged so far is <i>incredibly</i> important to reach the target.</p><ul><li> If the project <strong>is not on track</strong> to reaching it&#39;s goal, speculators will pledge in order to claim the refund bonus.</li><li> If the project <strong>is</strong> <strong>on track</strong> to reaching it&#39;s goal, prosocial people are likely to pledge so that the project gets funded.</li><li> If the project is <strong>close to reaching it&#39;s goal</strong> , but time is running out. Potential free-riders will wait until the very last moment, but eventually concede and pledge their money.</li></ul><p> In some sense, with a progress bar, the refund bonus rewards people for providing information on whether the project is likely to reach it&#39;s funding goal.</p><p> I suspect the Book Review project would have worked if there was a progress bar.</p><h3> Solution to looking like a scam.</h3><p> Personally I think framing it as &quot;You get a Refund Bonus as gratitude for supporting our project which we are very sorry we are unable to deliver&quot; rather than &quot;If you pledge you could win $5!!!!! Buy now!!!!&quot; can make it seem less like a scam.</p><p> Having a progress bar also makes it seem less like a scam since it makes it easier for people to predict if the project is going to succeed or fail.</p><h3>概括</h3><p>If you price your project correctly (with the help of prediction markets), market it successfully, phrase the refund bonus in a way that it does not seem like a scam and have a progress bar with a list of people who&#39;ve already pledged, then it&#39;s very likely that it will reach it&#39;s target because:</p><ul><li> it&#39;s rational for people to pledge, furthermore,</li><li> the refund bonus rewards people for providing information on whether the project will reach it&#39;s funding goal.</li></ul><h2> Other potential problems</h2><ul><li> <strong>Fraud:</strong> It&#39;s possible that the project is just a scam, and that the producer disappears with the money.</li><li> <strong>Information asymmetry:</strong> It&#39;s possible that the producer is unable to deliver the product because of incompetence or produces a product of lower quality than customers expected.</li><li> <strong>Collateral:</strong> The need for collateral that producers need to put up makes risk-averse producers less likely to try, cancelling out the increase in the success rate.<ul><li> Potential solution: If we can get success rate close to 100% then the platform can put up the collateral.</li><li> Potential solution: Entrepreneurs will be willing to invest if they think they can make a profit (ie someone who is risk tolerant can underwrite the collateral).</li></ul></li><li> <strong>Exclusion: &quot;</strong> Won&#39;t people just fund their projects with dominant assurance contracts and then bundle ads or exclude non-payers (eg with restrictive copyright) anyway to maximize revenue (eg cable TV with ads)?&quot;:<ul><li> Possibly, I think our job is to create a culture where we release things for free and without ads, similar to the current culture in open source software.</li><li> If dominant assurance contract become popular there is less of a case for patents and copyright so it will be easier to convince policymakers that they should no longer exist.</li></ul></li><li> <strong>Fees:</strong> The happy-path of the financial system is that money flows from consumers to produces. With a refund bonus, money can flow from producers to consumers. The infrastructure for dealing with this use-case is poor and expensive.</li></ul><h1> Okay so what am I buying?</h1><p> <a href="https://dac.mowzer.co.za">You&#39;re collectively paying for $629 dollars for 1 month of my time to make this idea a reality.</a> (I don&#39;t live in the Bay Area so my cost of living is low). I don&#39;t really <i>need</i> the money. I&#39;m doing this as a experiment to test that dominant assurance contracts work. I&#39;ll likely ask for additional funding in the future to implement more features after the first month, and for specific expenses as they come up.</p><p> The plan is to create a website where public-good producers can create a page that</p><ul><li> Has a description of the project.</li><li> Has a progress bar showing how much and who have pledged.</li><li> Handles payments with PayPal</li><li> If the project doesn&#39;t reach the funding goal, the customers are automatically refunded with a refund bonus. If the project does reach it&#39;s goal, the producer gets the money in their PayPal account.</li><li> The producer will put up the refund bonus as collateral using PayPal.</li></ul><p> The website is essentially already done (my prototype just has my project hard-coded). I just need to flesh it out so that other people can upload their projects.</p><p> The main thing I&#39;m going to doing is looking for people who want to create public goods and getting feedback from them on the platform (if that&#39;s you please join the <a href="https://discord.gg/KGeCTx33g">Refund Bonus Discord</a> or <a href="https://dac.mowzer.co.za/#contact">contact me in some other way</a> )</p><p> Additionally I want to try some cool things which might not work out like</p><ul><li> Use prediction markets to price the project.</li><li> Bring in investors/advertisers who will put up the collateral on behalf of the producers and take a cut if the project succeeds.</li></ul><h2> But why can&#39;t I just let other people pay and free-ride?</h2><p> You haven&#39;t been paying attention.<strong> </strong>Unless I&#39;ve priced this contract wrong, if you don&#39;t pay it doesn&#39;t happen. If you don&#39;t pay, I don&#39;t meet the goal, everyone gets refunded, I go back to my boring day job, and we continue to live in a world where public goods are under-provisioned.</p><h3> But won&#39;t someone else do this?</h3><p> It seems unlikely. The Dominant Assurance Contract paper came out in 1998. And 25 years later, I seem to be the first person to commit to getting it off the ground. You will probably have to wait a while before someone else comes.</p><h3> The final call to action</h3><p> If you want to live in a world with no ads or paywalls; a world where open-source software gets the same level of funding as proprietary software; a world where people can freely reuse ideas and music without paying royalties; a <a href="https://www.lesswrong.com/posts/yYvgmKGMtzKxbJZew/update-on-book-review-dominant-assurance-contract">world where people get payed for writing book reviews</a> ; a world where Game-of-Thrones-quality shows are freely available on Youtube; a <i>world where AI safety research gets the same-level of funding as AI capabilities research,</i> <a href="https://dac.mowzer.co.za/">then please pledge some money towards this.</a></p><p> Also please join the <a href="https://discord.gg/KGeCTx33g">Refund Bonus Discord</a> or <a href="https://dac.mowzer.co.za/#contact">contact me in some other way</a> if your interested in:</p><ul><li> Finding people who want to make public goods, putting up collateral on their behalf, marketing their project, and taking a cut if they reach their funding goal.</li><li> Receiving funding for producing public goods. Even if you don&#39;t want funding, but already produce public goods I&#39;m still interested to talk.</li><li> Buying public goods on the platform.</li></ul><h1> Isn&#39;t this just Kickstarter?</h1><p> Yes, but <a href="https://www.kickstarter.com/help/stats">60% of Kickstarters fail</a> . That is very bad odds. <a href="https://mason.gmu.edu/~atabarro/BetterCrowdfunding.pdf">Research done in a lab shows that dominant assurance contracts reduce failure rate from 60% to 40%</a> . I think this area is <i>very</i> neglected compared to the upside.</p><h2> &quot;Don&#39;t Contribute&quot; is still an equilibrium on Kickstarter</h2><p> If we go back to our earlier example, without a refund bonus, due to transaction costs, both &quot;Don&#39;t Contribute&quot; and &quot;Contribute&quot; are both equilibriums (0 vs -1 and 0 vs 5). The refund bonus removes &quot;Don&#39;t Contribute&quot; from the equilibrium.</p><figure class="table"><table><thead><tr><th> (Frank&#39;s Profit, Others&#39; Profit)</th><th> <strong>Others Don&#39;t Contribute</strong></th><th> <strong>Others Contribute</strong></th></tr></thead><tbody><tr><th> <strong>Frank Doesn&#39;t Contribute</strong></th><td> (0, 0)</td><td> (0, <strong>-1</strong> )</td></tr><tr><th> <strong>Frank Contributes</strong></th><td> ( <strong>-1</strong> , 0)</td><td> (5, 5)</td></tr></tbody></table></figure><h2> Sure, but this is just a small improvement on Kickstarter. It doesn&#39;t seem revolutionary.</h2><p> I don&#39;t think enough people are working on this problem. Alex Tabarrok publish his paper on dominant assurance contracts in 1998. Kickstarter was founded in 2009, 11 years later. To this day their are no platforms for dominant assurance contracts to fund public goods.</p><p> Most things funded on Kickstarter aren&#39;t even public goods! <a href="https://en.wikipedia.org/wiki/Kickstarter">Their stated mission is to &quot;help bring creative projects to life&quot;.</a> They are even trying to solve the free-rider problem! They just happen to be using a similar mechanism.</p><p> I agree dominant assurance contracts are a small improvement, but small improvements add up and <a href="https://intelligence.org/2023/02/03/focus-on-the-places-where-you-feel-shocked-everyones-dropping-the-ball/">almost nobody is working on this!</a></p><h2> But why aren&#39;t people using Kickstarter to fund public goods right now?</h2><p> They are. Here are some successful open-source projects funded by Kickstarter:</p><ul><li> <a href="https://www.kickstarter.com/projects/1681258897/its-magit-the-magical-git-client">https://www.kickstarter.com/projects/1681258897/its-magit-the-magical-git-client</a></li><li> <a href="https://www.kickstarter.com/projects/krita/krita-2016-lets-make-text-and-vector-art-awesome">https://www.kickstarter.com/projects/krita/krita-2016-lets-make-text-and-vector-art-awesome</a></li></ul><p> Here is an unsuccessful project</p><ul><li> <a href="https://www.kickstarter.com/projects/alecaddd/akira-the-linux-design-tool">https://www.kickstarter.com/projects/alecaddd/akira-the-linux-design-tool</a></li></ul><p> Akira got 38% of their target funding. I&#39;d be willing to bet that if they use dominant assurance contracts they would have succeeded.</p><p> Apparently <a href="https://www.kickstarter.com/help/stats">79% of projects that raised more than 20% of their goal were successfully funded</a> . The refund bonus provides the necessary kick (hehe) that gets <i>twice</i> as many projects over the line.</p><p> Again, I think the main problem is that people don&#39;t know that assurance contracts (dominant or otherwise) can be used to fund public goods.</p><h2> But you have no moat. Kickstarter will just copy you.</h2><p>是的！ I&#39;m not trying to make a bazillion dollars. I just want more public goods! If Kickstarter copies this idea that would be great! Less work for me!</p><h1> Wait, you said something about AI Safety but this doesn&#39;t apply because...</h1><p> <i>...  AI Capabilities just scale with compute, where as we don&#39;t know how to scale AI Safety. The problem is not funding, and even if it there was a dominant assurance contract for &quot;$1 000 000 000 to solve the alignment problem&quot; there is no guarantee that whoever gets the money will solve the alignment problem.</i></p><p> Yes, there is a information problem when funding research things like AI Safety. We don&#39;t have the information on whether the researcher&#39;s research will be any good. But guess what: <i>information is a public good!</i> That means that it&#39;s currently underfunded. Dominant assurance contracts can bridge that funding gap.</p><p> How do we fund information? Of the top of my head a bunch of things come to mind</p><ul><li> We can fund prizes that will be given to someone who answers a particular research question.</li><li> We can fund someone to do an analysis of which researchers or research directions seem most promising.</li><li> We can fund prediction markets on something like &quot;P(doom|give Bob $100,000)&quot; vs &quot;P(doom|don&#39;t give Bob $100,000)&quot;.</li></ul><h1> Alternatives and their problems </h1><figure class="media"><div data-oembed-url="https://www.youtube.com/watch?v=ZvgFTxhQw1s"><div><iframe src="https://www.youtube.com/embed/ZvgFTxhQw1s" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> Of course, there are other ways to fund public goods, but they all have their own problems.</p><h2> Ads</h2><p> Probably the main method to privately produce public goods is by running ads on goods you give away for free. The main issue with this is that ads pay a fraction of a cent per view. That means that you can only fund low-quality/cheap goods. To see this, compare videos on YouTube with series on HBO.</p><h2> Taxes</h2><p> With taxes you face a symmetric problem to the free-rider problem: <i>the forced-rider problem.</i> Think of someone who doesn&#39;t use a car, but still pays taxes to maintain the roads.</p><p> Funding public good through taxes also suffer from lack of market-pricing mechanism. Government spending doesn&#39;t have to pass the market test that a dominant assurance contract has to pass, creating wasteful spending.</p><h2> <a href="https://forum.effectivealtruism.org/topics/certificate-of-impact">Impact Certificates</a></h2><p> Impact certificates suffer from the free-rider problem. There isn&#39;t any incentive (other than altruism) to buy an impact certificate. One can just free-ride on the results of the impact project, and let the initial investor go bankrupt.</p><p> I think impact certificates are trying to solve the problem of information asymmetry rather than funding. It&#39;s possible that they could be combined with dominant assurance contracts in some way, but I&#39;m not sure how.</p><h2> Club Goods</h2><p> A Club Good is a good that is non-rival but excludable. An example would be a subscription to HBO or Microsoft Office. It costs practically nothing on the margin to give an additional person access to HBO or Microsoft Office (all the costs were upfront in production) so these goods are non-rival. However, since HBO and Microsoft charge for these goods they are excludable.</p><p> Of course, there is piracy. So the first problem is that it&#39;s actually difficult to turn a public good into a club good, and so the free-rider problem persists.</p><p> Secondly, it&#39;s inefficient to exclude people. If you would pay $5 to watch Game of Thrones but the HBO subscription is $15, then $5 of surplus is being lost (it costs HBO $0 to let you watch Game of Thrones).</p><h3> Patreon</h3><p> This is a type of club good.</p><p> From personal experience, I think this works okay for art where the patrons have a parasocial relationship with the artist. This seems to work less well for impersonal stuff like software.</p><p> Additionally, it&#39;s easy for Buccaneer Bob to sign up to your Patreon and then upload your products to a piracy website. Free-riding Frank won&#39;t bother to sign up to your Patreon, he&#39;ll just download your products off a piracy website.</p><p> If you price a dominant assurance contract correctly, Free-rider Frank is forced to pay.</p><h2> Micropayments</h2><p> The idea with micropayments is something like &quot;cost per marginal unit is a fraction of a cent so we should make it possible for people to pay fractions of cents.&quot; I think this doesn&#39;t work because the cost of fraud is higher than a fraction of a cent. Also, the cost of me thinking &quot;Is this worth 0.01c?&quot; is higher than a fraction of a cent. It&#39;s just better to treat such things as non-rival.</p><h2> Assurance Contracts/Kickstarter</h2><p> The main issue with an assurance contract without a refund bonus is that &quot;not contributing&quot; is an equilibrium, especially if their are substantial transaction costs. I already explained this above.</p><h1>概括</h1><p>Public goods are under-provisioned by the market due to the free-rider problem. Dominant assurance contracts solve the free-rider problem. Nobody is using dominant assurance contracts. To solve this problem, and simultaneously test if dominant assurance contracts work, <a href="https://dac.mowzer.co.za">I created a dominant assurance contract to fund a platform for creating dominant assurance contracts.</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/CwgHX9tbfASqxjpsc/the-economics-of-the-asteroid-deflection-problem-dominant#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CwgHX9tbfASqxjpsc/the-economics-of-the-asteroid-deflection-problem-dominant<guid ispermalink="false"> CwgHX9tbfASqxjpsc</guid><dc:creator><![CDATA[moyamo]]></dc:creator><pubDate> Tue, 29 Aug 2023 18:28:54 GMT</pubDate></item></channel></rss>