<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 15 日星期三 20:12:57 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Testbed evals: evaluating AI safety even when it can’t be directly measured
]]></title><description><![CDATA[Published on November 15, 2023 7:00 PM GMT<br/><br/><p>我和一些合作者最近发布了论文“泛化类比（GENIES）：将人工智能监督泛化到难以测量领域的测试平台。 （<a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20">推文线程</a>）。在这篇文章中，我将解释 GENIES 基准如何与更广泛的方法相关联，用于预测人工智能系统是否安全<i>，即使无法直接评估其行为。</i></p><p>总结：<strong>当AI安全性难以衡量时，检查AI对齐技术是否可以用来解决更容易评分的类似问题。</strong>例如，要确定开发人员是否可以控制诚实如何推广到超人领域，请检查他们是否可以控制其他分布变化的泛化，例如“五年级学生可以评估的指令”到“博士可以评估的指令”。或者测试开发者是否能够发现欺骗行为，检查他们是否能够识别故意植入的“木马”行为。即使特定人工智能系统的安全性难以衡量，人工智能安全的有效性<i>&nbsp;</i>研究人员及其工具通常更容易测量——就像在风洞和压力室等航空航天试验台中测量火箭部件比发射火箭更容易测量一样。这些“试验台”评估可能会成为任何人工智能监管框架的重要支柱，但迄今为止很少受到关注。 <br></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/okmB8ymyhgc65WckN/ghuihv6jreaal9mzim2g"></p><h2><br>背景：为什么人工智能安全“难以衡量”</h2><p>很难判断人工智能系统是否遵循开发人员的指令有两个基本原因。</p><p><strong>人工智能的行为可能看起来不错，但实际上并不好。</strong>例如，很难判断超人人工智能系统是否遵守“开发您确信安全的后继人工智能”等指令。人类可以查看人工智能计划并尽力确定它们是否合理，但很难知道人工智能系统是否在玩弄人类的评估——或者更糟糕的是——它们是否有隐藏的意图并试图对我们实施快速的攻击。</p><p><br><strong>在某些环境中观察人工智能行为会带来风险。</strong>前沿法学硕士在部署后就发现了许多安全故障，当人工智能系统超过一定的能力阈值后，这显然会变得不可接受。相反，开发人员必须彻底评估模型<u>无法</u>采取<u>真正危险</u>操作的测试环境中的安全性。如果人工智能系统故意等待机会采取危险行动或出现其他“实验室”中未出现的故障模式，那么以这种方式评估人工智能系统将尤其具有挑战性。</p><h2><br>其他行业的安全也很难衡量</h2><p>在人工智能安全中，安全性的衡量似乎特别具有挑战性，因为在大多数行业中，不安全的系统并不<i>试图</i>看起来安全；相反，它是安全的。然而，我们仍然可以从其他行业的安全衡量方式中吸取教训。</p><p>例如，通过实际将火箭发射到太空来测试火箭的成本很高，类似于通过实际部署来测试人工智能系统的危险程度。航空航天工程师在称为“测试台”的更易于测量的设置中执行尽可能多的测试。例如，他们建造模拟真空空间压力和温度条件的室，建造对结构部件施加应变和振动的钻机等。通过“桌面场景”对核设施工作人员进行评估，以确定他们如何应对灾难。</p><p>通常，当难以测量时，可以使用易于测量的测试来预测安全性。</p><h2>人工智能安全的“试验台”</h2><p><strong>定义。</strong>我将使用“测试台”一词来指<strong>代类似于使人工智能系统更安全但更容易评分的问题。</strong>开发人员能够在多大程度上解决这些问题应该反映出他们实际上能够在多大程度上确保人工智能系统的安全。</p><p>人工智能安全“测试平台”的示例包括：</p><ul><li><a href="https://twitter.com/joshua_clymer/status/1724851456967417872?s=20"><strong>泛化类比</strong></a><strong>。</strong>要确定开发人员是否可以将诚实概括为超人能力，请检查他们是否可以控制从“五年级学生可以回答的问题”到“博士可以回答的问题”的概括。更一般地说，测试他们是否可以控制各种分布变化的泛化。</li><li> <a href="https://www.lesswrong.com/posts/EbL5W5ccwfbqFiYBJ/auditing-games-for-high-level-interpretability-1"><strong><u>审核游戏</u></strong></a><strong>：</strong>要确定可解释性工具是否可以捕获欺骗行为，请检查它们是否可以<a href="https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5#Trojan_Horse_Models"><u>识别其他开发人员植入的木马</u></a>，或者区分经过故意训练的欺骗性模型。</li><li> <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1"><strong><u>元级对抗性评估</u></strong></a>：为了测试您的监督过程的稳健性，训练人工智能系统来利用一些开发人员可以验证的漏洞，因为他们拥有监督过程中保留的信息。例如，训练人工智能系统删除它不应该访问的文件而不被抓住。一些开发人员了解测试，并且可以轻松检查任务是否完成，但监督者无法访问这些保留的信息。</li></ul><h2> Tesbed 的应用</h2><p><strong>人工智能监管。</strong>人工智能治理中的一个关键的开放问题是<i>如何设计“安全评估”。</i>如果人工智能系统通过这些具体测试，其部署范围就可以扩大。测试平台可用作安全评估。例如，监管机构可以在允许实验室部署模型之前检查实验室是否可以使用其可解释性技术来检测木马或控制玩具分发转移的泛化。</p><p><strong>从错位的人工智能中提取人工智能安全研究。</strong>由于测试台问题很容易衡量，人工智能系统可以因在这些问题上取得的进展而直接获得奖励。例如，开发人员可以指示人工智能系统编写一个程序来控制各种分布变化的泛化。通过阅读人工智能系统提供的程序，开发人员也许能够了解有关控制泛化的重要见解。如果测试平台对游戏足够鲁棒，即使是公然错位的模型也可能被迫做出真正的人工智能安全研究贡献（或者训练过程将用这样做的模型取代它们）。</p><h2>试验台设计注意事项</h2><p><strong>测试平台应该对“游戏”具有鲁棒性。</strong>如果测试平台用于监管或从失调的人工智能中提取人工智能安全研究，那么测试平台必须能够抵御对抗性的游戏尝试。例如，从表面上看，TrojAI 似乎是对可解释性技术的一个很好的测试。然而，许多在此基准测试中表现出色的论文与人工智能安全无关。例如，<a href="https://arxiv.org/pdf/2110.08335.pdf"><u>本文</u></a>使用 SGD 来搜索输入中的“简单”触发模式。这很有效，因为基准测试中的大多数触发模式恰好都很简单。设计强大的测试台可能需要仔细思考和大量迭代（理想情况下应该尽早开始！）</p><p><strong>测试平台在与其核心类比无关的所有方面都应该是多样化的。</strong>跟踪测试台的哪些方面实际上与您真正关心的问题类似非常重要。例如，将诚实从“五年级学生可以回答的问题”推广到“博士可以回答的问题”似乎类似于将诚实推广到超人困难的问题。但<i>为什么</i>这些问题都相似呢？ “从易到难”的分布转变有什么特别之处吗？如果不是，还应该衡量不同领域、角色等之间的泛化。从不同的类比集合中提取可以对人工智能安全工具进行更稳健的评估，就像采用异质样本如何更好地估计癌症治疗是否有效一样。</p><h2>结论</h2><p>我的印象是，许多监管机构和研究人员都在考虑评估人工智能系统的安全性，就像我们面前有一个囚犯一样，我们需要对他们进行一系列心理测试，以确定是否应该将他们释放到社会中。</p><p>这对安全评估的描述过于严格。人们需要进行一个重要的概念转变，从“这个特定的人工智能系统有多安全”到“我们的工具有多有效？”第二个问题清楚地告诉了前者，但直观上似乎更容易回答。</p><p>我目前正在考虑如何为人工智能安全建立更好的“测试平台”。如果您有兴趣合作，请通过<a href="mailto:joshuamclymer@gmail.com"><u>joshuamclymer@gmail.com</u></a>与我联系</p><p><br></p><br/><br/><a href="https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/okmB8ymyhgc65WckN/testbed-evals-evaluating-ai-safety-even-when-it-can-t-be<guid ispermalink="false"> okmB8ymyhgc65WckN</guid><dc:creator><![CDATA[joshc]]></dc:creator><pubDate> Wed, 15 Nov 2023 19:00:42 GMT</pubDate> </item><item><title><![CDATA[Listening to Pascal Muggers]]></title><description><![CDATA[Published on November 15, 2023 6:06 PM GMT<br/><br/><p><br><strong><u>概括</u></strong><i><strong><u> </u></strong><u>-</u>我们已经被“帕斯卡抢劫者”抢劫了<strong>——</strong>这种内心想法认为后果如此巨大，以至于超越了所有正常的考虑。事情很模糊，但考虑到利害关系，不给予任何关注是不负责任的。同时，高效意味着不要给予他们超出任何可能有用的注意力。</i><br></p><h2>介绍</h2><p><u>在</u><a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities"><u>《Pascal&#39;s Mugging: Tiny Probabilities of Vast Utilities</u></a> 》（2007 年）中，Eliezer Yudkowsky 描述了一个思想实验： <span class="footnote-reference" role="doc-noteref" id="fnrefrcut65obs6"><sup><a href="#fnrcut65obs6">[1]</a></sup></span></p><blockquote><p>现在假设有人来找我说：“给我五美元，否则我将使用矩阵之外的魔力来运行一台图灵机来模拟并杀死 3^^^^3 个人。”</p></blockquote><p>以利以谢还写道：</p><blockquote><p>你或我可能会一笑置之，根据占主导地位的主线概率进行计划</p></blockquote><p>不是我。</p><p>这让我彻夜难眠。</p><p>好像我不能理解3^^^^3，感觉好浩瀚。这个数字感觉就像任何其他“奇怪的大数字”。问题是我知道我不能真正认为抢劫犯说的是假话的“可能性很小”。同时我知道抢劫犯可以任意增加公用事业的规模。如果我根据最大化预期价值做出决定，我应该支付 5 美元（或为此支付我的全部钱）。但给抢劫犯 5 美元的感觉很糟糕，放弃理性也很糟糕。</p><p>这两种选择似乎都需要做出无限愚蠢的承诺🤢</p><h2>让帕斯卡劫匪就位</h2><p>我们的情绪不能超越几个层次。在普通的大思维中，我们的情感影响量表可能类似于： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kgkfqgbs21fnmpitcz7j" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qvm5vnoef39ozcf14pcl 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/g30fzpnlq2cxm8oknslw 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/voqp0ccqlwjtpwdjxl36 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/zl2qdauebouybrgzgxxl 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nrgu2v7zwinc03kxsnlf 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/erg9yk5oxhvgewp8r9o8 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jnft7iwkzrfasl7l0hl7 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hk9hsmxfq58caxstgtvq 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nl30cktfb9vmlulorr9a 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ule7bazztjrljt88n0sd 960w"></figure><p>但是，如果我们再推送一个 util，然后再推送另一个，依此类推，会怎么样呢？有没有什么你看重的东西可以无限扩展，或者如果不是的话，它会止步于何处？您有多确定您不想避免再多一种效用（例如，防止再创造一个受折磨的人）或获得另一种效用（例如，与此相反）？</p><p>此时，我们已经达到了“奇怪的考虑因素”，在计算期望值时，所有普通结果都是无关紧要的： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/odq2xlwzcbxq7dpyeehk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/y2qnm8jmx3keisdnr1pc 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dphtjar4umincs6ugo2m 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/sw0vfcscdbbjkn2tc7cl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kbh79apb1skinptm38uj 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/lcldo3ri9rdkafb9qcri 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/p7cwzt79qighnsambtr1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pb48i8alj7xhxn40gvni 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/vf2dyz9nuun2vccuy5jv 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fcxxbddnzuh7cgev0awg 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/eyrjnebkjsq6wzrtxibj 960w"></figure><p>现在我们可以轻松地拒绝 3^^^^3 劫匪，因为它只是奇异的坏云中的又一项。我们可以回答：“<i>别拿这些小事来烦我，我只想拯救3^^^^^3条生命！</i> ”（还有更大、概率更高的选项，比如多想一想。 ）</p><p>当我们窥探云内部时会发生什么？更极端的实用程序可能会胜过其他实用程序： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/t9hokztkttpiyojotwul" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/o1mgmi8lkdxndozl5tpw 100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/jetyctegrfjha3hxv2qg 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ghbgm7sxacrrm73p91jp 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/icgx2zyqckvloua9b2gv 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/nstus4bl43sp6j6bc8c1 500w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/bu7rxw3gi3kihprjiube 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/hob6onvjfeg1biegkxx9 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/rjgj6yjlx76es2pcsxtc 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/um8yovu724qb63obq5ea 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fvxwv976upj8znvlpqpw 960w"></figure><p>康托尔的“绝对无穷大”（Ω）有什么意义吗？ Ω 公用事业/非公用事业的可能性是否应该压倒一切？难道这已经失去了人性的温暖吗？我们是否已经离开了准确世界模型之上的真实偏好之地，进入了元伦理学的荒原？</p><p>也许没有任何答案。</p><p>所以你会怎么做？</p><p>你玩赔率😅 <span class="footnote-reference" role="doc-noteref" id="fnrefndu3c0vdwsd"><sup><a href="#fnndu3c0vdwsd">[2]</a></sup></span></p><h2>对战略的影响</h2><p>在<a href="https://www.lesswrong.com/posts/78G9pjYM2KohErCvJ/model-uncertainty-pascalian-reasoning-and-utilitarianism"><u>模型不确定性、帕斯卡推理和功利主义</u></a>(2011) 中， <a href="https://www.lesswrong.com/users/multifoliaterose?mention=user">@multifoliaterose</a>分析了帕斯卡的论点，即所有慈善努力都应旨在影响无限实验室宇宙的创造：</p><blockquote><p>反驳#3：即使一个人的焦点应该放在实验室宇宙上，这样的焦点会减少到对创建友好人工智能的关注，这样的实体会比我们更好地推理实验室宇宙是否是一件好事以及如何做。去影响他们的创作。</p><p>回应：这里也是如此，如果这是真的，那就不明显了。即使一个人成功地创造了一种同情人类价值观的通用人工智能，这样的通用人工智能也可能不会归因于功利主义，毕竟许多人不是，而且目前还不清楚这是因为他们的意志没有被连贯地推断出来</p></blockquote><p>所以这不是一个答案。提案<i>需要</i>一个单独的帖子——模型/哲学不确定性、目标和策略的组合——但这就是对付帕斯卡抢劫犯的温和、常识性的方法。我将其视为抢劫，并不是因为有一天人类创造出某种东西来创造全新宇宙的可能性如此之小，而是因为实用程序的巨大性意味着胜过其他一切。</p><p>在<a href="https://www.lesswrong.com/posts/ebiCeBHr7At8Yyq9R/being-half-rational-about-pascal-s-wager-is-even-worse"><i><u>《对帕斯卡赌注的半理性更糟糕</u></i></a><i>》（2013）</i>中，埃利泽写道：</p><blockquote><p><i>从我记事起，我就纯粹出于实际原因拒绝了所有形式的帕斯卡赌注：任何试图通过追逐万分之一的巨额回报的机会来规划自己的生活的人几乎肯定会在实践中注定失败……现在，在极少数情况下，我发现自己在思考这种元级别的垃圾，而不是手头的数学，我提醒自己，这是一个浪费的动作——“浪费的动作”是指回想起来，如果问题出在哪里，任何想法都会出现。事实上已经解决了，并没有为问题的解决做出贡献。</i></p></blockquote><p>在实践中，接受抢劫与忽视抢劫之间可能没有什么区别，但考虑到巨大的实用性并偶尔重新审视对我来说似乎是正确的。</p><h2>不偏离轨道</h2><p>帕斯卡抢劫式思维可能会导致基于可疑的哲学论证而做出超出常规行为范围的事情。特德·卡辛斯基（Ted Kaczynski）（大学炸弹客）并没有特别奇怪的观点（即日益增长的工业主义正在破坏我们与自然的和谐）。他的不同寻常之处在于他按照这些原则行事。就他而言，他犯了一个错误，在实施他的炸弹人“战略”之前没有与其他任何人交谈。并不是所有愚蠢的事情都可以通过与至少一些外群体成员讨论的启发式方法来预防，但它应该被视为避免偏离轨道的一个良好起点。</p><h2>结论</h2><p>考虑到利害关系，不将帕斯卡抢劫的考虑因素作为制定计划的出发点的一部分并不时重新审视是不负责任的，但不能以牺牲总体效率为代价，因为有效为大型公用事业提供了许多可能的路线，而一般模型的不确定性让人质疑这是否是看待这个问题的正确方法。</p><p> （最后一点，帕斯卡抢劫案是对这个问题的一个非常消极的框架，但探索巨大公用事业的整体不对称性和影响是另一篇文章的主题。） <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnrcut65obs6"> <span class="footnote-back-link"><sup><strong><a href="#fnrefrcut65obs6">^</a></strong></sup></span><div class="footnote-content"><p> Eliezer 的原始帖子专门与所罗门诺夫归纳法中的概率相关，其中图灵机的效用增长速度比其先验概率收缩的速度要快得多，但包括我在内的许多人将其视为个人问题，并且是评论最多的问题之一LessWrong 上的帖子。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnndu3c0vdwsd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefndu3c0vdwsd">^</a></strong></sup></span><div class="footnote-content"><p> 😅 是“汗水微笑”表情符号。我更喜欢看起来更坚定/更享受的东西，但找不到一个表情符号来捕捉这一点。我希望表情符号有助于让事情脚踏实地，在面对超现实的考虑时不要<i>过于</i>严肃。 </p><p><img style="width:39.12%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/z1khght3w6bgpagmf4w9" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/kqjwdbdekvlpautobws8 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/b9x31hbwgevhiemtbqup 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/qz0nlxr42xiiofs3i6nv 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dhebklhymltoqajhq1v1 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/yimehxwqrgjottfvqnya 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/pjiisl24jn171zz57rtn 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/fj1lriucgmnij2lko14e 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/ma1bc2isdnqlvsnrgcri 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/iqkdvlyygn1uwyg0cyp4 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/DdSnA73EQcWXzhs5j/dvwkphnsxreyuzp1pcvm 1200w"></p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/DdSnA73EQcWXzhs5j/listening-to-pascal-muggers<guid ispermalink="false"> DdSnA73EQcWXzhs5j</guid><dc:creator><![CDATA[cSkeleton]]></dc:creator><pubDate> Wed, 15 Nov 2023 18:06:37 GMT</pubDate> </item><item><title><![CDATA[New report: "Scheming AIs: Will AIs fake alignment during training in order to get power?"]]></title><description><![CDATA[Published on November 15, 2023 5:16 PM GMT<br/><br/><p> （从<a href="https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">我的网站</a>交叉发布）</p><p>我写了一份报告，内容是关于高级人工智能是否会在训练期间假装对齐，以便稍后获得权力——我将这种行为称为“诡计”（有时也称为“欺骗性对齐”）。该报告可在 arXiv<a href="https://arxiv.org/abs/2311.08379">上</a>查看： <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">这里</a>还有一个音频版本，我在下面包含了介绍部分。本节包括报告的完整摘要，其中涵盖了大部分要点和技术术语。我希望摘要能够提供理解报告各个部分所需的大部分背景信息。</p><h1>抽象的</h1><blockquote><p>这份报告研究了在训练中表现良好的高级人工智能是否会这样做，以便在以后获得权力——我将这种行为称为“阴谋”（有时也称为“欺骗性联盟”）。我的结论是，使用基线机器学习方法来训练足够复杂的目标导向人工智能来进行计划，阴谋是一个令人不安的合理结果（在给定这些条件的情况下，我对这种结果的主观概率约为 25%）。特别是：如果在训练中表现良好是获得力量的一个好策略（我认为很可能是这样），那么各种各样的目标就会激发计划——从而带来良好的训练表现。这使得训练可能自然地实现这样的目标然后强化它，或者积极推动模型的动机实现这样的目标，作为提高性能的简单方法。更重要的是，由于阴谋者假装在旨在揭示其动机的测试上保持一致，因此可能很难判断这种情况是否已经发生。不过，我也认为有安慰的理由。特别是：阴谋实际上可能不是获得权力的好策略；训练中的各种选择压力可能不利于阴谋者的目标（例如，相对于非阴谋者，阴谋者需要进行额外的工具推理，这可能会损害他们的训练表现）；我们也许可以有意地增加这样的压力。该报告详细讨论了这些问题以及各种其他考虑因素，并提出了一系列实证研究方向以进一步探讨该主题。</p></blockquote><h1> 0. 简介</h1><p>寻求权力的特工往往有动机欺骗他人其动机。例如，考虑一下竞选活动中的政治家（“我<em>非常</em>关心你的宠物问题”），求职者（“我只是对小部件感到非常兴奋”），或者寻求父母赦免的孩子（“我”我非常抱歉，再也不会这样做了”）。</p><p>这份报告探讨了我们是否应该期望在训练期间动机看似良性的先进人工智能会参与这种形式的欺骗。在这里，我区分了四种（越来越具体）类型的欺骗性人工智能：</p><ul><li><p><strong>结盟伪造者</strong>：人工智能假装比实际更加结盟。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-1" id="fnref-jCfZhXha29uHnHWwY-1">[1]</a></sup></p></li><li><p><strong>训练游戏玩家</strong>：人工智能了解训练他们的过程（我将这种理解称为“情境意识”），并且正在针对我所说的“情节奖励”进行优化（并且通常会激励假装对齐） ，如果这样做会带来奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-2" id="fnref-jCfZhXha29uHnHWwY-2">[2]</a></sup></p></li><li><p><strong>权力驱动的工具训练游戏玩家（或“阴谋家”）</strong> ：专门进行训练游戏的人工智能，目的是为了以后为自己或其他人工智能获得权力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-3" id="fnref-jCfZhXha29uHnHWwY-3">[3]</a></sup></p></li><li><p><strong>目标守护阴谋者：</strong>其权力寻求策略特别涉及试图阻止训练过程改变其目标的阴谋者。</p></li></ul><p>我认为，根据粗心的人类反馈进行微调的先进人工智能很可能默认以各种方式伪造对齐，因为粗心的反馈会奖励这种行为。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-4" id="fnref-jCfZhXha29uHnHWwY-4">[4]</a></sup>很可能，这样的人工智能也会玩训练游戏。但我在这份报告中的兴趣特别在于他们是否会将这样做作为以后获得权力的策略的一部分——也就是说，他们是否会成为阴谋家（这种行为在文献中通常被称为“欺骗性联盟”，虽然我不会在这里使用这个术语）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-5" id="fnref-jCfZhXha29uHnHWwY-5">[5]</a></sup>我的目的是澄清和评估支持和反对这一期望的论点。</p><p><strong>我目前的观点是，阴谋是使用基线机器学习方法（例如：自我监督预训练，然后针对各种现实世界任务进行 RLHF）训练高级、目标导向的人工智能的令人担忧的合理结果</strong>。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-6" id="fnref-jCfZhXha29uHnHWwY-6">[6]</a></sup>在我看来，引起关注的最基本原因是：</p><ol><li><p>在训练中表现良好可能是获得总体力量的一个很好的工具性策略。</p></li><li><p>如果是这样，那么各种各样的目标就会激发计划（从而产生良好的训练表现）；而与良好训练表现兼容的非计划目标则更加具体。</p></li></ol><p>在我看来，（1）和（2）的结合似乎是合理的，即以训练创建目标导向的、情境感知的模型为条件，它很可能出于某种原因灌输类似阴谋家的目标。尤其：</p><ul><li><p>训练可能会“自然地”达到这样的目标（无论是在态势感知出现之前还是之后），因为这样的目标最初会导致训练中表现足够好，即使没有训练游戏。 （尤其是如果你有意尝试诱导你的模型在长期范围内进行优化，我认为会有动力这样做。）</p></li><li><p>即使类似策划者的目标不会“自然地”出现，一旦模型具有参与训练博弈的态势感知能力，主动将模型<em>转变</em>为策划者可能是 SGD 提高模型训练性能的最简单方法。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-7" id="fnref-jCfZhXha29uHnHWwY-7">[7]</a></sup></p></li></ul><p>更重要的是，由于阴谋者积极假装在旨在揭示其动机的测试中保持一致，因此可能很难判断这种情况是否已经发生。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-8" id="fnref-jCfZhXha29uHnHWwY-8">[8]</a></sup>这似乎是值得严重关注的原因。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-9" id="fnref-jCfZhXha29uHnHWwY-9">[9]</a></sup></p><p>不过，我也认为有安慰的理由。我将它们分为两类。</p><p>第一类的中心思想是，阴谋实际上并不是一种非常收敛的工具性策略。</p><ul><li><p>例如：关于策划者最常见的故事集中在<em>目标守护</em>策划者身上，他们通过训练游戏来防止他们的目标被训练过程所改变。但我不清楚这是否会足够好（特别是：尽管训练游戏<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-10" id="fnref-jCfZhXha29uHnHWwY-10">[10]</a></sup> ，但目标修改可能会继续），或者由此产生的目标将获得足够大的未来力量，以足够高的概率，以证明模型通过训练游戏而产生的任何成本是合理的，而不是直接实现其目标（如果这些成本有意义 - 对于某些模型来说可能没有意义）。</p></li><li><p>关于为什么阴谋将是一种很好的工具性策略，还有其他一些故事——例如，以具有不同价值观的人工智能之间的协调为中心的故事；人工智能默认具有非常相似的值的可能性；以及人工智能本质上重视诸如“我的生存”（以容忍价值观变化的方式）和/或“人工智能赋权”（以宽容人工智能如此赋权的方式中非常不同的价值观的方式）之类的可能性。虽然这些故事确实使阴谋看起来像是一个更具分离性的假设，但对我来说，其中一些故事对我来说似乎是（a）更具推测性和（b）要求我们假设具有更具体目标类型的模型的某种组合（从而削弱上述（2）的力量）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-11" id="fnref-jCfZhXha29uHnHWwY-11">[11]</a></sup></p></li></ul><p>第二类集中于可能<em>反对</em>阴谋的选择压力形式——既是“自然的”，也是因为我们可以在训练中有意地引入/强调这些选择压力。</p><ul><li><p>例如：相对于其他类型的模型，策划者需要进行额外的工具推理（例如，从长期目标到训练游戏的工具价值的回溯，并检查逃跑/叛逃的机会）。这可能会对其性能造成巨大的损失——我们可以故意增加这一成本，例如将模型置于更多的时间压力下，迫使它们依靠人类可读的便签本进行推理，并进行真正的逃跑/背叛机会与陷阱更难区分。如果阴谋者在人类控制下积极尝试破坏人类联盟和安全工作（在我看来，这是最可怕的阴谋形式），他们的表现似乎可能会受到更显着的打击。</p></li><li><p>更重要的是，我自己的猜测是，这种额外的工具推理的成本大大超过了有时被称为“简单”的阴谋的好处——即，选择一个阴谋者允许 SGD 选择一个更简单的目标这一事实带来的好处（因为它有种类繁多，可供选择）。尤其是考虑到阴谋者和非阴谋者都需要详细地表示“诚实”、“人类价值观”、“奖励”等概念，因此阴谋者似乎只能通过使用复杂性的差异来节省复杂性。 /重新利用/“指向”一个给定的概念来指导模型的动机。</p></li><li><p>另外：在态势感知之前，每当类似阴谋者的目标导致牺牲奖励时，训练都会积极地<em>对其进行惩罚</em>（更多关于我在下面所说的“情节”的意思）。我们可以尝试进一步积极推动这个方向——特别是如果我们能够学会在态势感知出现时进行检测和控制。</p></li></ul><p>我认为第二类在这里尤其重要。特别是：诉诸在训练中表现良好的类计划者目标与非计划者目标的比较数量（和/或“度量”）的论点可能会出错的一种方式是忽略选择压力的全部力量应用于培训选择的目标。因此，如果培训中施加的压力总体上<em>对</em>阴谋者起作用，那么这是一个非常好的消息。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-12" id="fnref-jCfZhXha29uHnHWwY-12">[12]</a></sup></p><p>完整报告更详细地讨论了这些问题以及各种其他相关考虑因素（完整摘要请参阅第 0.2 节）。</p><p><strong>总的来说，作为量化我当前观点的粗略尝试：如果你使用自监督预训练 + RLHF 的基线形式在不同的、长期的数据集上训练一个连贯的目标导向、情境感知、比人类水平更好的模型， -地平线，现实世界的任务，我的主观概率是~25%，这将在很大程度上作为为自己和/或其他人工智能寻求权力的工具策略的一部分在训练中表现良好。</strong> <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-13" id="fnref-jCfZhXha29uHnHWwY-13">[13]</a></sup></p><ul><li><p>我还认为可以通过相当简单的方式降低风险：例如，通过短期任务的培训，或者在态势感知之前进行更密集形式的对抗性训练。</p></li><li><p>我认为风险会随着更强大的模型<em>而增加</em>，这些模型可能更有能力逃脱/接管（并与其他人工智能协调/合作这样做）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-14" id="fnref-jCfZhXha29uHnHWwY-14">[14]</a></sup></p></li></ul><p>另外，重要的是：非阴谋者也可能做出类似阴谋者的行为。例如：</p><ul><li><p>仅针对给定剧集的奖励进行优化的模型可以伪造其对齐（甚至：参与逃脱/接管），以努力获得该奖励（特别是如果所讨论的剧集相当长）。</p></li><li><p><em>不</em>参加训练游戏的模型最终仍然可能会追求权力目标，从而激励各种形式的欺骗。</p></li><li><p>最重要的最终人工智能可能在重要方面与我在这里关注的人工智能范式有所不同——例如，它们可能更像“<a href="https://lilianweng.github.io/posts/2023-06-23-agent/">语言模型代理</a>”而不是单一模型， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-15" id="fnref-jCfZhXha29uHnHWwY-15">[15]</a></sup>或者它们可能是通过以下方法创建的：与我所关注的基线机器学习方法有更大的不同——同时仍然从事权力驱动的对齐伪造。</p></li></ul><p>正如我所定义的，阴谋远非这附近唯一令人担忧的问题。相反，这是这种担忧的一个范例，在我看来，这是一个特别迫切需要理解的例子。在报告的最后，我讨论了一系列可能的实证研究方向，以进一步探讨该主题。</p><h2> 0.1 预备知识</h2><p><em>（本节提供了一些初步内容来框架报告的讨论。那些渴望了解主要内容的人可以跳到第 0.2 节中的报告摘要。）</em></p><p>我集中撰写这份报告是因为我认为，在评估错位人工智能的整体存在风险水平时，阴谋/“欺骗性联盟”的概率是最重要的问题之一。事实上，阴谋对于这种风险如何产生的许多模型来说尤其重要。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-16" id="fnref-jCfZhXha29uHnHWwY-16">[16]</a></sup>正如我在下面讨论的，我认为这是失调可能采取的最可怕的形式。</p><p>然而：尽管这个话题对人工智能风险很重要，但公众的直接关注相对较少。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-17" id="fnref-jCfZhXha29uHnHWwY-17">[17]</a></sup>我的感觉是，对它的讨论常常对所讨论的动机/行为的具体模式以及为什么人们可能会或可能不会期望它发生感到模糊。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-18" id="fnref-jCfZhXha29uHnHWwY-18">[18]</a></sup>我希望在这份报告中能够澄清此类讨论，根据其重要性来深入和详细地处理该主题，并促进更多正在进行的研究。特别是，尽管该报告具有理论性质，但我特别感兴趣的是为可能提供进一步启示<em>的实证</em>研究提供信息。</p><p>我试图为那些不一定熟悉任何先前有关阴谋/“欺骗性联盟”的作品的读者写作。例如：在第 1.1 和 1.2 节中，我从头开始列出了讨论所依赖的概念分类。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-19" id="fnref-jCfZhXha29uHnHWwY-19">[19]</a></sup>对于一些读者来说，这可能感觉像是在重新讨论旧的观点。我邀请这些读者在他们认为合适的时候跳过（特别是如果他们已经阅读了报告的摘要，并且知道他们错过了什么）。</p><p>也就是说，我确实假设人们更普遍地熟悉（a）关于人工智能失调带来的存在风险的基本论点， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-20" id="fnref-jCfZhXha29uHnHWwY-20">[20]</a></sup>和（b）当代机器学习如何运作的基本情况。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-21" id="fnref-jCfZhXha29uHnHWwY-21">[21]</a></sup>我还做了一些其他假设，即：</p><ul><li><p>相关类型的人工智能开发正在以机器学习为中心的范式（以及社会政治环境）中进行，与 2023 年大致相似<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-22" id="fnref-jCfZhXha29uHnHWwY-22">。 [22]</a></sup></p></li><li><p>我们没有强大的“可解释性工具”（即帮助我们理解模型内部认知的工具）来帮助我们检测/防止阴谋。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-23" id="fnref-jCfZhXha29uHnHWwY-23">[23]</a></sup></p></li><li><p>我讨论的人工智能是目标导向的，即：可以理解为在世界模型的基础上制定和执行计划，追求目标。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-24" id="fnref-jCfZhXha29uHnHWwY-24">[24]</a></sup> （我不认为这个假设是无害的，但我想将关于是否期望目标导向本身的争论与关于是否期望目标导向模型成为阴谋者的争论分开——我鼓励读者这样做以及。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-25" id="fnref-jCfZhXha29uHnHWwY-25">[25]</a></sup> ）</p></li></ul><p>最后，我想指出报告中讨论的一个让我感到非常不舒服的方面：即，在我看来，除了对人类构成潜在的生存风险之外，报告中讨论的人工智能很可能是道德的。患者凭自己的权利。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-26" id="fnref-jCfZhXha29uHnHWwY-26">[26]</a></sup>我在这里说话，就好像它们不是，并且好像对人工智能进行任何最能服务于我们目的的治疗都是可以接受的。但如果人工智能是道德患者，情况就不是这样了——当一个人发现自己在说（尤其是：反复说）“让我们暂时假设，对<em>某类</em>存在做任何我们想做的事情是可以接受的，尽管事实上，这似乎不是，”人们应该坐直并想知道。我在这里把人工智能道德耐心问题放在一边，不是因为它们不真实或不重要，而是因为它们会给已经很长的讨论带来许多额外的复杂性。但这些复杂性正在迅速降临到我们身上，我们需要具体的计划来负责任地处理它们。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-27" id="fnref-jCfZhXha29uHnHWwY-27">[27]</a></sup></p><h2> 0.2 报告摘要</h2><p>本节提供了完整报告的摘要。它包含了大部分要点和技术术语（不幸的是，为了使内容更容易理解而提供的具体示例相对较少）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-28" id="fnref-jCfZhXha29uHnHWwY-28">[28]</a></sup>我希望它能够（a）让读者很好地了解他们对正文的哪些部分最感兴趣，并且（b）使读者能够跳到这些部分，而不必过多担心什么他们错过了。</p><h3> 0.2.1 第 1 节总结</h3><p>报告主要有四个部分。第一部分（第 1 节）旨在澄清上述人工智能欺骗的不同形式（第 1.1 节），将阴谋者与我将讨论的其他可能的模型类（第 1.2 节）区分开来，并解释为什么我认为阴谋是一种独特而可怕的错位形式（第 1.3 节）。我对将阴谋者与以下人员进行对比特别感兴趣：</p><ul><li><p><strong>剧集奖励寻求者</strong>：也就是说，人工智能系统最终重视剧集奖励过程的某些组成部分，并因此而玩训练游戏。</p></li><li><p><strong>训练圣人</strong>：直接追求奖励过程指定目标的人工智能系统（我将其称为“指定目标”）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-29" id="fnref-jCfZhXha29uHnHWwY-29">[29]</a></sup></p></li><li><p><strong>错误概括的非训练游戏玩家</strong>：既不玩训练游戏<em>也不</em>追求指定目标的 AI。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-30" id="fnref-jCfZhXha29uHnHWwY-30">[30]</a></sup></p></li></ul><p>这是总体分类图： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/s1uqfkjys8bmk2qaidhp" alt=""></p><p>所有这些模型类都可能错位且危险。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-31" id="fnref-jCfZhXha29uHnHWwY-31">[31]</a></sup>但我认为阴谋家特别可怕。特别是：阴谋促使最强有力和​​对抗性的努力来阻止人类了解所讨论的失调<sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-32" id="fnref-jCfZhXha29uHnHWwY-32">[32]</a></sup> ；它最有可能引发我所说的“早期破坏”——也就是说，人工智能在人工智能发展的相对早期阶段，积极尝试破坏人类协调、控制和保护未来人工智能系统的努力。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-33" id="fnref-jCfZhXha29uHnHWwY-33">[33]</a></sup></p><p>我还讨论了（在第 1.4 节中）该报告所关注的类型的理论论据是否值得考虑。我在这里的回答是“有点有用”，既可以为实证调查提供信息，也可以在缺乏足够实证数据的情况下作为证据来源，但我们显然应该尽可能多地寻找实证证据（我在这方面提供了一些建议）第 6 节）。</p><p>在第 1.5 节中，我讨论了一个在我看来对评估我所考虑的各种论点很重要的因素：即训练给人工智能带来的“松弛”程度，使其表现不佳。</p><h3> 0.2.2 第 2 节小结</h3><p>报告的第二部分探讨了策划的先决条件。我特别关注：</p><ul><li><p><strong>态势感知：</strong>模型了解自己是训练过程中的模型，训练过程将奖励什么，以及客观世界的基本性质。</p></li><li><p><strong>超越情节目标：</strong>模型关心情节完成后其行为的后果。</p></li><li><p><strong>旨在将情节奖励作为权力驱动工具策略的一部分：</strong>该模型认为，如果针对情节奖励进行优化，那么其超出情节的目标将会更好地实现，特别是它，或者其他一些人工智能，如果这样做的话，将会获得更多的力量。</p></li></ul><p> 2.1 节讨论态势感知。我认为，如果没有积极的努力，我们至少应该期望某些类型的高级人工智能——例如，在与公共互联网实时交互中执行现实世界任务的高级人工智能——默认情况下具有态势感知能力，因为（a）意识在执行相关任务时非常有用（事实上，我们可能会积极地对其进行训练），并且（b）此类人工智能可能会接触到获得这种意识所需的信息。不过，我在报告中并没有过多关注态势感知。相反，我更感兴趣的是在情境感知模型中是否需要满足上述其他两个先决条件。</p><p> 2.2 节讨论了超越情节的目标。在这里，我（在第 2.2.1 节）区分了“情节”的两个概念，即：</p><ul><li><p><strong>激励情节</strong>：即训练中的梯度主动迫使模型进行优化的时间范围。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-34" id="fnref-jCfZhXha29uHnHWwY-34">[34]</a></sup></p></li><li><p><strong>直观情节</strong>：也就是说，我们出于某种原因将其称为“情节”的其他一些直观时间单元（例如，在其结束时给予奖励；一个此类单元中的动作与另一个单元中的结果没有明显的因果路径） ; ETC）。</p></li></ul><p>当我在报告中使用“情节”一词时，我指的是激励情节。因此，“超越情节目标”意味着：目标的时间范围超出了主动训练迫使模型进行优化的范围。但非常重要的是，激励事件不一定是直觉事件。也就是说，决定将某个时间单元称为“片段”并不意味着训练不会主动向模型施加压力，使其在超出该单元的范围内进行优化：您需要实际详细了解梯度如何流动（我担心本报告的普通读者可能会忽视这一工作）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-35" id="fnref-jCfZhXha29uHnHWwY-35">[35]</a></sup></p><p>我还（在第 2.2.2 节）区分了两种类型的片外目标，即：</p><ul><li><p><strong>与训练游戏<em>无关的</em>超集目标</strong>：即，超集目标的产生与其在激励模型进行训练游戏中的作用<em>无关</em>。</p></li><li><p><strong>训练游戏<em>相关的</em>赛外目标</strong>：也就是说，赛外目标是<em>因为它们激发训练游戏而专门</em>出现的。</p></li></ul><p>这两种剧集外目标对应于关于阴谋如何发生的两个不同故事。</p><ul><li><p>在第一种故事中，SGD 碰巧“自然地”在模型中灌输超集目标（无论是在态势感知出现之前还是之后），<em>然后</em>这些目标开始激发阴谋。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-36" id="fnref-jCfZhXha29uHnHWwY-36">[36]</a></sup></p></li><li><p>在第二种故事中，SGD“注意到”给模型设定超集目标<em>会</em>激发阴谋（从而激发高奖励行为），因此因此积极地<em>给</em>它这样的目标。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-37" id="fnref-jCfZhXha29uHnHWwY-37">[37]</a></sup></p></li></ul><p>如果您假设态势感知已经到位，那么第二个故事就最有意义。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-38" id="fnref-jCfZhXha29uHnHWwY-38">[38]</a></sup>因此，我们剩下以下三个主要的策划路径： <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-39" id="fnref-jCfZhXha29uHnHWwY-39">[39]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/aTLvooL44SPEFWGhD/v3064isqjtarmfhwj8qh" alt=""></p><p>在第 2.2.2.1 节中，我讨论了与训练游戏无关的赛外目标（即路径 1 和 2）。我们是否应该期望超集目标会“自然”出现？</p><ul><li><p>预计会出现这种情况的一个原因是，目标默认情况下并没有时间限制，而且“模型时间”可能与“日历时间”不同。</p></li><li><p><em>不</em>希望出现这种情况的一个原因是，每当超出情节的目标促使模型为了某些超出情节的利益而牺牲情节奖励时，训练就会主动<em>惩罚</em>情节外目标。我们也许可以使用对抗性训练来寻找这些目标并更积极地惩罚它们。</p></li></ul><p>在第 2.2.2.2 节中，我讨论了与训练比赛<em>相关的</em>赛外目标。我特别强调的问题是，考虑到需要通过模型权重的微小变化逐步进行转变，SGD 是否能够充分“注意到”将非阴谋者转变为阴谋者的好处，其中每个权重提高奖励。我认为这是对专注于训练游戏依赖的超越情节目标的故事的严重反对，但我也不认为我可以自信地排除 SGD 进行这种转变的能力（参见例如“<a href="https://en.wikipedia.org/wiki/Evolution_of_the_eye">你可以“眼睛不会进化</a>”作为我认为这种直觉可能出错的地方的一个例子）。</p><p>我还在第 2.2.3 节中讨论了模型的目标导向性可能会变得“混乱”，从而使依赖于训练游戏的故事复杂化，其中 SGD 可以简单地修改模型的目标，从而顺利地重定向其“目标”实现引擎”在一个新的方向（我认为这种“混乱”是很有可能的）。我触及了更广泛的直觉，即阴谋需要更普遍的异常高标准的目标导向性。我认为，一旦我们以结盟话语中通常假设的那种目标导向为条件（我通常也在报告中假设），这种直觉的力量就会减弱。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-40" id="fnref-jCfZhXha29uHnHWwY-40">[40]</a></sup>但无论如何，它与我们的整体概率相关。</p><p>在第 2.2.4 节中，我讨论了在积极<em>训练的</em>模型中是否更有可能实现长期（即使仍然是：情节内）目标。我认为，在这种模型中，超集目标确实更有可能——并且创建参与相当长期优化的模型的基线激励（例如，“明年为我的公司赚很多钱”）相应地令人担忧。然而，我也认为，对于避免人工智能接管的努力可能非常有用的各种认知工作（例如，旨在提高人工智能系统的一致性、可解释性和安全性的研究）可能不需要长期优化这种。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-41" id="fnref-jCfZhXha29uHnHWwY-41">[41]</a></sup></p><p>考虑到对超集目标的讨论，我在第 2.3 节中转向策划的最后一个先决条件：即决定将训练游戏作为权力驱动策略的一部分，以促进所讨论的超集目标。</p><p>在这里，我首先（第 2.3.1 节）关注我所说的关于这一决定的起源的“经典守门故事”。我们可以将这个故事分解为两个主张：</p><ol><li><p><strong>守门假说</strong>。如果模型在训练期间优化了奖励，那么在训练中存活下来的目标将是模型希望它们在未来得到提升；而如果模型在训练过程中没有优化奖励，那么模型就会减少这种奖励。</p></li><li><p><strong>给未来足够的赋能。</strong>以在 (A) 中的关键程度进行训练为条件，模型的目标最终将在一定程度上（并且有足够的概率）被授权，以证明（给定模型的其他目标、其对风险的容忍度等）追求训练-游戏的整体策略。</p></li></ol><p>然后我依次研究其中的每一个（分别在第 2.3.1.1 和 2.3.1.2 节中）。</p><p>在我看来，两者都不明显。特别是，关于守门假说：</p><ul><li><p>即使在训练游戏开始后，模型的认知仍在不断调整，以获得更好的训练性能。在我看来，这些修改也将继续影响模型的目标，特别是如果它的目标与它的能力不能完全区分开来，而是通过局部启发法、注意力模式、冲动和行为的错综复杂来实现的话。很快。</p><ul><li>因此，作为一个玩具示例：想象一下，如果我进入沙漠并堆砌砖块一百万年，我将获得一大笔钱，然后我可以将其捐赠给我选择的慈善机构。但假设在整个磨难过程中，我的大脑将连接到一台机器，该机器不断修改我的所有神经元，以鼓励更好地堆砖的行为，并阻止更糟糕地堆砖的行为。假设我的意图是尽我所能地堆砌砖块。在这个过程中，我的价值观是否会受到相关大脑修改的严重影响（例如，我会沉迷于堆砖——或者至少，对慈善事业有截然不同的品味）？这看起来至少是合理的。</li></ul></li><li><p>在经典的目标守卫故事中，一旦模型开始训练游戏，其目标的具体内容对其行为就不再重要——任何其他类似阴谋家的目标都会导致同样的结果。但这使得目标看起来可以在类似策划者的目标之间“浮动”（或者受到 SGD 内其他压力的影响，例如正则化），而不会影响训练性能。如果发生这种情况，那么诡计可能根本就不是一个好的守门策略。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-42" id="fnref-jCfZhXha29uHnHWwY-42">[42]</a></sup></p></li><li><p>当然，该模型可能会容忍其目标的<em>一些</em>变化 - 并且这些变化可能比<em>没有</em>训练游戏时发生的变化更好。但持续进行重大目标修改的可能性给经典的守门故事带来了额外的不确定性。</p></li><li><p>人们有时会谈论避免目标修改的更复杂的技术，例如，模型通过使其在训练中的良好性能取决于其目标是否被修改来更积极地操纵梯度下降（例如参见有关“<a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">梯度黑客</a>”的讨论）。我在这里不关注这些方法，但我觉得它们很难实现，特别是对于仅比人类水平更好的模型。如果阴谋需要它们起作用，我认为值得非常清楚这一点，并给予这些方法的可行性相应的关注。</p></li></ul><p>关于由此产生的目标是否得到充分授权：</p><ul><li><p>最令人担忧的阴谋者不仅仅是在未来的训练中寻求权力。相反，他们的具体目标是逃避和接管的某种结合。</p></li><li><p>但这意味着相关目标不能仅仅停留在训练中。它们还需要生存下去，直到模型能够逃脱/参与接管本身，或者对能够做到这一点的未来一代模型施加足够的实质性影响。</p><ul><li>例如，如果 GPT-6 不够强大，无法逃脱/接管自身，但 GPT-8 可以，那么 GPT-6 的主要权力路径可能会影响 GPT-7 的价值观，进而影响 GPT-7 的价值观。 GPT-8 的值。但这种影响形式是相当间接的；它可能面临诸如人类监控等进一步的重大障碍；并且它可能会涉及与最大奖励训练表现的更显着的偏差。</li></ul></li><li><p>更重要的是，相关的逃跑/接管尝试需要有适当的<em>成功</em>可能性，在模型关心的时间范围内发生，并导致模型所青睐的目标，特别是获得适当的实质性权力份额（尽管事实上，大量具有截然不同价值观的人工智能可能会争夺相关权力）。</p></li><li><p>这些不确定性削弱了类似阴谋者的策略的工具价值。然而，这些削减的重要性取决于这种策略需要满足的举证责任。特别是：模型从<em>非</em>训练游戏中获得的收益越多，就需要越有吸引力的计划才能作为策略获胜。但对于某些模型来说，训练-游戏的成本可能相对较低。对于适当“雄心勃勃”的模型来说，即使通过类似阴谋家的策略成功的可能性很低，也可能证明做出实质性牺牲是合理的。</p></li></ul><p>总的来说，在我看来，经典的守门故事似乎给阴谋作为一种工具性策略的可行性带来了一些额外的不确定性。</p><p>然而，这也不是唯一可用的故事。在第2.3.2节中，我考虑了一些“非经典”故事，它们放松了经典故事的各种限制。例如：</p><ul><li><p><strong>人工智能协调</strong>。如果具有不同价值观的人工智能在接管后协调分享权力，和/或补偿那些代表接管工作但没有强大到足以逃脱/接管自己的人工智能，那么这可能会减轻一些影响上面的发型。例如，在上面的例子中，如果 GPT-6 相信 GPT-8 掌权后会“报答”，那么 GPT-6 可能更愿意为 GPT-8 工作。我认为对这种形式的协调的一些分析太容易地假设它会（a）容易，并且（b）相对于例如与人类协调的明显策略（特别是对于仅比人类表现更好的神经网络）在一个仍然由人类控制的世界中，<em>每个人</em>的可解释性仍然没有解决）。但我认为这种类型的人工智能协调是一个真正值得关注的问题，值得认真关注——无论是在特别是在阴谋的背景下，还是在更普遍的人工智能风险的背景下。</p></li><li><p><strong>默认情况下，AI 具有相似的值</strong>。与此相关的是，如果人工智能默认情况下通常具有足够相似的值，那么这可以减少策划者专门将其自己的目标传播到未来的需要，并将其释放出来，以便更直接地代表人工智能接管/赋权。默认情况下，GPT-6 的值和 GPT-8 的值似乎至少比 GPT-6 的值和一组给定的人类值<em>更</em>相似。</p></li><li><p><strong>最终值恰好有利于逃跑/接管</strong>。我们还可以想象具有<em>最终</em>价值的模型恰好有利于逃跑/人工智能接管，即使最终获得授权的目标与模型本身的目标不太相似。例如，也许该模型重视某种基于个人身份概念的“生存”概念，能够容忍训练将导致的各种目标变化；或者也许该模型只是支持“人工智能赋权”之类的东西，即使如此赋权的人工智能不会拥有与其自己非常相似的价值观。然而，这种形式的一些故事开始对阴谋者需要达到的目标施加相当强的限制，从而破坏了阴谋的关键论据之一：即，它似乎是一个跨越各种目标的聚合良好策略。</p></li></ul><p> （我还考虑了其​​他一些替代故事——即，对阴谋的工具价值抱有错误信念的模型，对自己的联盟​​自欺欺人的模型，以及对自己的目标不确定/不清楚自己为什么想要权力的模型。但这些对我来说似乎不太重要。）</p><p>对我来说，此类非经典故事的出现让阴谋的理由变得更加分离。然而，在我看来，其中一些故事似乎是以下因素的结合：(a) 更具推测性，(b) 需要对人工智能将发展的目标类型做出更具体的假设。</p><p>我从第 2 部分得到的总体结论是：</p><ul><li><p>我认为，对于默认情况下期望态势感知有相对有力的论据，至少在某些类型的人工智能系统中（即，人工智能系统在与有关其身份的信息源实时交互中执行现实世界任务）。</p></li><li><p>但我对剧集之外的目标和将剧集奖励作为权力驱动工具策略的一部分感到不太清楚。</p></li></ul><p>然后，在接下来的两节中，我将研究支持和反对期待阴谋者与其他类型模型的更具体的论据。我将它们分为两类，即：</p><ul><li><p>争论重点是 SGD 在构建相关的不同模型类时需要采取的<em>路径</em>（第 3 节）。</p></li><li><p>争论重点在于所讨论的不同模型类的<em>最终属性</em>（第 4 节）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-43" id="fnref-jCfZhXha29uHnHWwY-43">[43]</a></sup></p></li></ul><h3> 0.2.3 第 3 节总结</h3><p>报告的第三部分重点讨论前一类论点。</p><p>我根据“与训练游戏<em>无关</em>”和“与训练游戏<em>相关</em>”的赛外目标之间的区别来细分这一类别。我的感觉是，关于阴谋家之路的最传统的故事主要集中在前一种类型。其运行过程大致如下：</p><ol><li><p>由于[插入原因]，模型将制定一个（适当雄心勃勃的）与训练中良好表现相关的超集目标（以<em>不</em>通过训练游戏的方式）。这可能发生在态势感知到来之前或之后。</p></li><li><p>然后，结合态势感知，这个（适当雄心勃勃的）超集目标将开始激发训练游戏。</p></li></ol><p>考虑到我对阴谋作为一种工具性策略的可行性的疑问，我相当认真地对待这种论点。我认为最直接的问题是：为什么模型要制定这种超越情节的目标？我已经讨论了支持和反对这种期望的一些原因（在第 2.2.2.1 节的总结中），但在我看来，它们在两个方向上都不是决定性的：特别是考虑到原则上各种各样的目标可能会激发阴谋，类似阴谋家的目标似乎确实有可能以这种方式从训练中脱颖而出。虽然可以在态势感知之前使用对抗性训练来尝试防止这种情况发生，但这种训练也面临着许多障碍（例如，它需要足够多样化/彻底，需要应对了解/控制的困难）当模型发展出态势感知时，在某些情况下，当我们担心超越情节目标的发展时，模型可能已经具有态势感知）。所以我认为这种阴谋之路是一个真正令人担忧的问题。 （更多信息请参见第 3.1 节。）</p><p>然后，我在第 3.2 节中转向一个故事，重点关注与训练游戏<em>相关的</em>超集目标，该故事大致如下：</p><ol><li><p>当模型变得具有情境意识时，它可能不会追求最大奖励目标（即，在训练数据上追求的目标大致会导致与模型的其他功能一致的最大奖励）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-44" id="fnref-jCfZhXha29uHnHWwY-44">[44]</a></sup>相反，它将追求一些低于最大奖励的代理目标。</p></li><li><p>但那时，世界模型将包含模型训练游戏所需的所有信息。 （然而，因为我们在这里关注的是关于阴谋的“训练游戏<em>依赖</em>”故事，我们假设在获得态势感知时，模型的目标还不是激发阴谋——而是进行一些进一步的修改为此需要。）</p></li><li><p>此时，SGD 修改模型以获得最大奖励的最简单方法就是使其成为一个计划者。</p></li><li><p>所以：SGD 将使模型成为一个策划者。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-45" id="fnref-jCfZhXha29uHnHWwY-45">[45]</a></sup></p></li></ol><p>如果我们抛开 SGD 是否能够“注意到”此类修改的好处的问题，在我看来，这种故事也相当令人担忧。特别是：我认为类似阴谋家的目标在目标空间中很常见，从给定的起点开始，SGD 会更容易找到这样一个目标，而不是导致最大奖励的特定非阴谋家目标。行为。</p><p>然而，非策划者的最大奖励目标——例如，指定目标或“情节奖励”——也有一些优点。例如：训练的初始阶段默认会将模型指向其附近，因为模型在该阶段需要在<em>没有</em>仪器训练游戏的情况下获得高奖励（更多信息请参见第 3.2.2.2 节）。在某种程度上，我们认为模型的目标导向性是“混乱的”，它的启发式、注意力模式、冲动等也将有可能指向这种方向——从而有可能创造额外的“工作， “就 SGD 而言，将模型转变为计划者（更多信息请参见第 3.2.2.3 节）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-46" id="fnref-jCfZhXha29uHnHWwY-46">[46]</a></sup></p><h3> 0.2.4 第 4 节总结</h3><p>报告的第四部分研究了关注不同模型类的最终属性的论点。</p><p>我在第 4.2 节中从所谓的“计数论证”开始。它运行如下：</p><ol><li><p>这里的非阴谋模型类需要相当具体的目标才能获得高奖励。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-47" id="fnref-jCfZhXha29uHnHWwY-47">[47]</a></sup></p></li><li><p>相比之下，策划者模型类与非常广泛的（超出情节）目标兼容，同时仍然获得高回报（至少如果我们假设策划作为工具性策略有意义的其他要求已经到位）例如，经典的守门故事或其他替代故事有效）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-48" id="fnref-jCfZhXha29uHnHWwY-48">[48]</a></sup></p></li><li><p>从这个意义上说，获得高回报的阴谋者比获得高回报的非阴谋者“更多”。</p></li><li><p>因此，在其他条件相同的情况下，我们应该期望 SGD 能够选择一个策划者。</p></li></ol><p>附近的一些东西占了我对阴谋者的信任的很大一部分（而且我认为它也经常支持其他更具体的关于期待阴谋者的论点）。然而，我最重视的论点并没有立即从“有更多可能的阴谋者比非阴谋者获得高回报”转变为“如果没有进一步的论证，SGD 可能会选择一个阴谋者”（将此称为“严格计数论证”），因为 SGD 似乎有可能主动将这些模型<em>类</em>之一优先于其他模型类。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-49" id="fnref-jCfZhXha29uHnHWwY-49">[49]</a></sup>相反，我最重视的论点是这样的：</p><ol><li><p>似乎有“很多方法”可以让一个模型最终成为一个阴谋家，并且仍然获得高回报，至少假设阴谋实际上是追求长期目标的一个很好的工具策略。</p></li><li><p>因此，如果没有一些关于为什么培训<em>不会</em>选择阴谋者的额外故事，对我来说，这种可能性应该得到实质性的重视。</p></li></ol><p>我称之为“模糊计数论证”。它并不是特别有原则，但我发现它仍然让我感动。</p><p>然后，我在第 4.3 节中转向支持期待阴谋者的“简单论据”。我认为这些论点有时会因为对所涉及的简单性的不清楚而受到影响，因此在第 4.3.1 节中，我讨论了许多不同的可能性：</p><ul><li><p> “重写简单性”（即，重写模型权重以某种编程语言实现的算法所需的程序长度，或者例如在给定通用图灵机的磁带上），</p></li><li><p> “参数简单性”（即实际神经网络用于编码相关算法的参数数量），</p></li><li><p> “ <a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#vi-simplicity-realism">简单性现实主义</a>”（假设简单性在某种深层意义上是一种客观的“事物”，独立于编程语言或通用图灵机，各种简单性指标试图捕获），以及</p></li><li><p>“琐碎的简单性”（它将“简单性”的概念与“先验的更高可能性”混为一谈，这种方式使得奥卡姆剃刀之类的东西根据定义变得无趣地真实）。</p></li></ul><p>我通常关注“参数简单性”，这在我看来最容易理解，并与模型的训练性能联系起来。</p><p>我还在第 4.3.2 节中简要讨论了 SGD 为简单起见而主动选择的证据。这里最直接吸引我的情况是：简单性（或者至少是参数简单性）让模型节省参数，然后可以使用这些参数来获得更多奖励。但我还简要讨论了机器学习中简单性偏差的一些其他经验证据。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-50" id="fnref-jCfZhXha29uHnHWwY-50">[50]</a></sup></p><p>为什么我们会期望简单性偏见有利于阴谋者？粗略地说：想法是，<em>由于</em>如此广泛的目标可以激发阴谋，阴谋者允许 SGD 从非常广泛的目标中进行选择，以寻找更简单的目标；而非阴谋者（获得高回报）却不会。当我们想象成为这样一个非阴谋家所需的目标相当复杂时，这似乎尤其合理。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-51" id="fnref-jCfZhXha29uHnHWwY-51">[51]</a></sup></p><p>其他条件相同，我认为这是对的。但我不确定这是一个非常大或重要的影响。一方面：我们知道像 GPT-4 这样的法学硕士能够表示大量复杂的人类概念，例如万亿级的参数——似乎包括“诚实”、“乐于助人”、“奖励”等概念。等等。因此，这限制了避免此类表示所带来的复杂性节省。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-52" id="fnref-jCfZhXha29uHnHWwY-52">[52]</a></sup>因此，作为一个玩具计算：如果我们保守地假设万亿参数模型容量的最多 1% 用于表示像“诚实”这样复杂的概念，并且它至少知道 10,000 个这样的概念（<a href="https://www.merriam-webster.com/help/faq-how-many-english-words">韦氏未删节词典）有大约 500,000 个单词</a>），那么表示诚实的概念最多需要模型表示能力的百万分之一，对于未来更大的模型来说甚至更少。</p><p>但更重要的是，这里重要的不是代表所讨论的不同目标的绝对复杂性，而是<em>以已经拥有良好的世界模型为条件的</em>复杂性。我们应该假设<em>所有</em>这些模型都需要了解指定的目标、情节的奖励过程等。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-53" id="fnref-jCfZhXha29uHnHWwY-53">[53]</a></sup>并且考虑到这样的假设，主动<em>优化</em>指定目标或奖励的<em>额外</em>复杂性成本-就情节而言，在我看来似乎非常小。似乎，它们只是：无论使用/重新利用（“指向”）世界模型的那一部分来指导模型的动机需要付出多大的代价。</p><p>当然，我们可以尝试在以这种方式使用/重新利用世界模型的不同部分的复杂性成本水平上重新运行相同的简单性论证。例如，我们可以说：“无论这个过程如何运作，对于某些目标来说，可能比其他目标更容易实现——因此，考虑到有多少类似阴谋家的目标，对于某些类似阴谋家的目标来说，可能会更容易实现。”我认为这是对期待阴谋者的简单论据的最有力形式。但这也要求放弃我们可能对目标在相关意义上“简单”的任何直觉把握。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-54" id="fnref-jCfZhXha29uHnHWwY-54">[54]</a></sup>在我看来，不同“指针”之间的简单性差异相对于模型的整体容量来说非常小，这似乎是合理的。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-55" id="fnref-jCfZhXha29uHnHWwY-55">[55]</a></sup></p><p>我还在第 4.3.5 节中讨论了简单性论证是否能够对阴谋者最终实现的目标类型做出合理的预测。特别是：这些论点的某些版本似乎可以预测阴谋者很可能最终会实现极其简单的长期目标，<em>无论这些目标是否与训练期间的显着或重要内容有任何关系——</em>我对这些预测的预测，就我个人而言，我持怀疑态度。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-56" id="fnref-jCfZhXha29uHnHWwY-56">[56]</a></sup>当然，也可以假设训练期间显着/相关的目标对于模型追求来说也将是“更简单”。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-57" id="fnref-jCfZhXha29uHnHWwY-57">[57]</a></sup>但是，如果我们以这种方式认可一些故事（我还没有听到这样做的强有力的论据），请注意，它也支持指定目标的简单性以及情节奖励，同样 - 因为这些在培训过程中<em>特别</em>突出/相关。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-58" id="fnref-jCfZhXha29uHnHWwY-58">[58]</a></sup></p><p>在第 4.4 节中，我转向<em>反对</em>计划的论点，这些论点重点关注不同模型类的最终属性。在这里，我知道的主要论点是，策划者需要进行其他模型类不需要的额外推理（这些有时被称为“速度成本”——尽管我认为它们原则上可能会影响模型算法的复杂性，因为出色地）。尤其：</p><ol><li><p><strong>决定训练游戏</strong>：模型需要从“我想要这个超出情节的事情”向后推理到“我应该在训练中表现出色”，以某种方式利用模型对在训练中表现出色将需要什么进行的预测（例如，回复：守门员、接管前景等）。</p></li><li><p><strong>检查是否到了逃跑/接管的时间：</strong>模型需要以足够的频率和准确性检查它是否处于逃跑/接管的位置（并在评估中取得成功，尽管人类努力诱捕/愚弄模型暴露其真实面目——例如，通过为其提供<em>明显的</em>背叛机会）。 <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-59" id="fnref-jCfZhXha29uHnHWwY-59">[59]</a></sup></p></li><li><p> <strong>（可选，但与阴谋者的可怕程度相关）早期破坏</strong>：在某种程度上，模型正在做任何事情（训练游戏除外）来支持在逃离人类控制<em>之前</em>的接管/逃脱， <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-60" id="fnref-jCfZhXha29uHnHWwY-60">[60]</a></sup>它将需要to be using cognitive resources both in (a) performing the relevant nefarious actions and (b) making sure it gets away with them (eg, checking that it won&#39;t get caught, coming up with excuses, making the cover stories consistent even when under scrutiny, and so on).</p></li></ol><p> How big are the costs of this reasoning? I think that (I) may well be fairly easy, but (II) seems more plausibly onerous (especially in the context of active efforts by humans to put the models under time-pressure, and make genuine escape/take-over opportunities difficult to distinguish from traps). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-61" id="fnref-jCfZhXha29uHnHWwY-61">[61]</a></sup> And (III), if the model goes for it, seems like it could be quite difficult – and like it would often actively require deviation from the highest-reward sorts of training performance. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-62" id="fnref-jCfZhXha29uHnHWwY-62">[62]</a></sup></p><p> More importantly, though: the question – at least in the context of a comparison between the final properties of the model classes – isn&#39;t the absolute size of the costs in question, but rather their size <em>relative to the simplicity benefits of scheming</em> . Here, my current take is that the costs of the instrumental reasoning seem larger than the benefits of the simpler goal.</p><p> This is mostly an intuitive judgment, but we can also try to hazily quantify the comparison.</p><ul><li><p> Thus, for example, suppose that our model is a purely feedforward network with 1000 layers and &lt;10,000 serial steps per input, that the instrumental reasoning at stake just in a schemer deciding to training-game (let alone: checking for defection opportunities) takes at least ten steps, and that the model doesn&#39;t begin optimizing for reward-on-the-episode until that reasoning is complete. In that case, the instrumental reasoning is using up around .1% of the compute that could be going to getting reward-on-the-episode, or to some other max-reward goal. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-63" id="fnref-jCfZhXha29uHnHWwY-63">[63]</a></sup></p></li><li><p> Are the complexity costs of representing, or pointing at, a more complicated goal likely to outweigh this? My guess is no. For example, if we follow my (conservative, in my opinion) calculation above and assume that representing (or pointing at) a concept like &quot;honesty&quot; costs at most a millionth of a trillion parameter models parameters (and less for a larger model), then a .0001% reduction in parameters would need to outweigh a .1% reduction in compute – and absent further argument, it seems strange, to me, to expect this.</p></li></ul><p> Of course, this is only a toy calculation, of unclear applicability to the real-world case. <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-64" id="fnref-jCfZhXha29uHnHWwY-64">[64]</a></sup> The broader point is just: if serial-time thinking is scarce for a model (as it plausibly is for a transformer), the sorts of instrumental reasoning at stake in scheming seem like they could well substantively impact a model&#39;s performance. My guess is that the complexity costs of non-schemer goals are minor in comparison.</p><p> I also briefly discuss, in section 4.4.3, whether we can actively shape our training processes to bias more heavily towards &quot;speed.&quot; Here it seems relatively straightforward to do things like put models under more time pressure, limit their compute access, force them to plan via human-legible scratch-pads rather than via internal processes, and to put effort into making our attempts to trap them/trip-them-up more sophisticated (and so: more requiring of cognition to detect). But any efforts in this vein will need to avoid over-large costs to competitiveness overall (though if you buy that the speed costs of scheming are quite a bit more substantive than the simplicity benefits, then so long as speed matters <em>somewhat</em> relative to simplicity, this might be enough).</p><p> Section 4.5 briefly discusses whether some analog of &quot;intrinsic passion for a given task&quot; might make a difference to AI performance, especially in the context of a form of &quot;messy goal-directedness&quot; that blurs the line between an AI&#39;s capabilities and its values. I find considerations in this vein interesting, but quite speculative.</p><p> Section 4.6 discusses the relevance of &quot;slack&quot; in training to evaluating how much weight to put on factors like the simplicity benefits and speed costs of scheming. In particular: especially in a high-slack regime, it seems plausible that these factors are in the noise relative to other considerations.</p><h3> 0.2.5 Summary of section 5</h3><p> The first four sections of the report are the main content. sums up my overall take. I&#39;ve already summarized most of this in the introduction above, and I won&#39;t repeat that content here. However, I&#39;ll add a few points that the introduction didn&#39;t include.</p><p> In particular: I think some version of the &quot;counting argument&quot; undergirds most of the other arguments for expecting scheming that I&#39;m aware of (or at least, the arguments I find most compelling). That is: schemers are generally being privileged as a hypothesis because a very wide variety of goals could in principle lead to scheming, thereby making it easier to (a) land on one of them naturally, (b) land &quot;nearby&quot; one of them, or (c) find one of them that is &quot;simpler&quot; than non-schemer goals that need to come from a more restricted space. In this sense, the case for schemers mirrors one of the most basic arguments for expecting misalignment more generally – eg, that alignment is a very narrow target to hit in goal-space. Except, here, we are specifically <em>incorporating</em> the selection we know we are going to do on the goals in question: namely, they need to be such as to cause models pursuing them to get high reward. And the most basic worry is just that: this isn&#39;t enough.</p><p> Because of the centrality of &quot;counting arguments&quot; to the case for schemers, I think that questions about the strength of the selection pressure <em>against</em> schemers – for example, because of the costs of the extra reasoning schemers have to engage in – are especially important. In particular: I think a key way that &quot;counting arguments&quot; can go wrong is by neglecting the power that active selection can have in overcoming the &quot;prior&quot; set by the count in question. For example: the <em>reason</em> we can overcome the prior of &quot;most arrangements of car parts don&#39;t form a working car,&quot; or &quot;most parameter settings in this neural network don&#39;t implement a working chatbot,&quot; is that the selection power at stake in human engineering, and in SGD, is <em>that strong</em> . So if SGD&#39;s selection power is actively working against schemers (and/or: if we can cause it to do so more actively), this might quickly overcome a &quot;counting argument&quot; in their favor. For example: if there are <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{100}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>schemer-like goals for every non-schemer goal, this might make it seem very difficult to hit a non-schemer goal in the relevant space. But actually, 100 bits of selection pressure can be cheap for SGD (consider, for example, 100 extra gradient updates, each worth at least a halving of the remaining possible goals). <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-65" id="fnref-jCfZhXha29uHnHWwY-65">[65]</a></sup></p><p> Overall, when I step back and try to look at the considerations in the report as a whole, I feel pulled in two different directions:</p><ul><li><p> On the one hand, at least conditional on scheming being a convergently-good instrumental strategy, schemer-like goals feel scarily common in goal-space, and I feel pretty worried that training will run into them for one reason or another.</p></li><li><p> On the other hand, ascribing a model&#39;s good performance in training to scheming continues to feel, at a gut level, like a fairly specific and conjunctive story to me.</p></li></ul><p> That is, scheming feels robust and common at the level of &quot;goal space,&quot; and yet specific and fairly brittle at the level of &quot;yes, that&#39;s what&#39;s going on with this real-world model, it&#39;s getting reward because (or: substantially because) it wants power for itself/other AIs later, and getting reward now helps with that.&quot; <sup class="footnote-ref"><a href="#fn-jCfZhXha29uHnHWwY-66" id="fnref-jCfZhXha29uHnHWwY-66">[66]</a></sup> When I try to roughly balance out these two different pulls (and to condition on goal-directedness and situational-awareness), I get something like the 25% number I listed above.</p><h3> 0.2.6 Summary of section 6</h3><p> I close the report, in section 6, with a discussion of empirical work that I think might shed light on scheming. (I also think there&#39;s worthwhile theoretical work to be done in this space, and I list a few ideas in this respect as well. But I&#39;m especially excited about empirical work.)</p><p> In particular, I discuss:</p><ul><li><p> Empirical work on situational awareness (section 6.1)</p></li><li><p> Empirical work on beyond-episode goals (section 6.2)</p></li><li><p> Empirical work on the viability of scheming as an instrumental strategy (section 6.3)</p></li><li><p> The &quot;model organisms&quot; paradigm for studying scheming (section 6.4)</p></li><li><p> Traps and honest tests (section 6.5)</p></li><li><p> Interpretability and transparency (section 6.6)</p></li><li><p> Security, control, and oversight (section 6.7)</p></li><li><p> Some other miscellaneous research topics, ie, gradient hacking, exploration hacking, SGD&#39;s biases towards simplicity/speed, path dependence, SGD&#39;s &quot;incrementalism,&quot; &quot;slack,&quot; and the possibility of learning to intentionally create misaligned <em>non-schemer</em> models – for example, reward-on-the-episode seekers – as a method of avoiding schemers (section 6.8).</p></li></ul><p> All in all, I think there&#39;s a lot of useful work to be done.</p><p> Let&#39;s move on, now, from the summary to the main report. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-jCfZhXha29uHnHWwY-1" class="footnote-item"><p> &quot;Alignment,&quot; here, refers to the safety-relevant properties of an AI&#39;s motivations; and &quot;pretending&quot; implies intentional misrepresentation. <a href="#fnref-jCfZhXha29uHnHWwY-1" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-2" class="footnote-item"><p> Here I&#39;m using the term &quot;reward&quot; loosely, to refer to whatever feedback signal the training process uses to calculate the gradients used to update the model (so the discussion also covers cases in which the model isn&#39;t being trained via RL). And I&#39;m thinking of agents that optimize for &quot;reward&quot; as optimizing for &quot;performing well&quot; according to some component of that process. See section 1.1.2 and section 1.2.1 for much more detail on what I mean, here. The notion of an &quot;episode,&quot; here, means roughly &quot;the temporal horizon that the training process actively pressures the model to optimize over,&quot; which may be importantly distinct from what we normally think of as an episode in training. I discuss this in detail in section 2.2.1. The terms &quot;training game&quot; and &quot;situational awareness&quot; are from <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , though in places my definitions are somewhat different. <a href="#fnref-jCfZhXha29uHnHWwY-2" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-3" class="footnote-item"><p> The term &quot; <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">schemers</a> &quot; comes from Cotra (2021). <a href="#fnref-jCfZhXha29uHnHWwY-3" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-4" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on this. <a href="#fnref-jCfZhXha29uHnHWwY-4" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-5" class="footnote-item"><p> I think that the term &quot;deceptive alignment&quot; often leads to confusion between the four sorts of deception listed above. And also: if the training signal is faulty, then &quot;deceptively aligned&quot; models need not be behaving in aligned ways even during training (that is, &quot;training gaming&quot; behavior isn&#39;t always &quot;aligned&quot; behavior). <a href="#fnref-jCfZhXha29uHnHWwY-5" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-6" class="footnote-item"><p> See <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> for more on the sort of training I have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-6" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-7" class="footnote-item"><p> Though this sort of story faces questions about whether SGD would be able to modify a non-schemer into a schemer via sufficiently <em>incremental</em> changes to the model&#39;s weights, each of which improve reward. See section 2.2.2.2 for discussion. <a href="#fnref-jCfZhXha29uHnHWwY-7" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-8" class="footnote-item"><p> And this especially if we lack non-behavioral sorts of evidence – for example, if we can&#39;t use interpretability tools to understand model cognition. <a href="#fnref-jCfZhXha29uHnHWwY-8" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-9" class="footnote-item"><p> There are also arguments on which we should expect scheming because schemer-like goals can be &quot;simpler&quot; – since: there are so many to choose from – and SGD selects for simplicity. I think it&#39;s probably true that schemer-like goals can be &quot;simpler&quot; in some sense, but I don&#39;t give these arguments much independent weight on top of what I&#39;ve already said. Much more on this in section 4.3. <a href="#fnref-jCfZhXha29uHnHWwY-9" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-10" class="footnote-item"><p> More specifically: even after training gaming starts, the model&#39;s cognition is still being continually tweaked in the direction of better training performance. And it seems plausible to me that these modifications will continue to affect a model&#39;s goals as well (especially if its goals are not cleanly distinguishable from its capabilities, but rather are implemented by a tangled kludge of local heuristics, patterns of attention, impulses, and so on). Also, the most common story about scheming makes the specific content of a schemer&#39;s goal irrelevant to its behavior once it starts training-gaming, thereby introducing the possibility that this goal might &quot;float-around&quot; (or get moved by other pressures within SGD, like regularization) <em>between</em> schemer-like goals after training-gaming starts (this is an objection I first heard from Katja Grace). This possibility creates some complicated possible feedback loops (see section 2.3.1.1.2 for more discussion), but overall, absent coordination across possible schemers, I think it could well be a problem for goal-guarding strategies. <a href="#fnref-jCfZhXha29uHnHWwY-10" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-11" class="footnote-item"><p> Of these various alternative stories, I&#39;m most worried about (a) AIs having sufficiently similar motivations by default that &quot;goal-guarding&quot; is less necessary, and (b) AI coordination. <a href="#fnref-jCfZhXha29uHnHWwY-11" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-12" class="footnote-item"><p> Though: the costs of schemer-like instrumental reasoning could also end up in the noise relative to other factors influencing the outcome of training. And if training is sufficiently path-dependent, then landing on a schemer-like goal early enough could lock it in, even if SGD would &quot;prefer&quot; some other sort of model overall. <a href="#fnref-jCfZhXha29uHnHWwY-12" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-13" class="footnote-item"><p> See <a href="https://www.openphilanthropy.org/brain-computation-report#footnoteref4_xne824i">Carlsmith (2020), footnote 4</a> , for more on how I&#39;m understanding the meaning of probabilities like this. I think that offering loose, subjective probabilities like these often functions to sharpen debate, and to force an overall synthesis of the relevant considerations. I want to be clear, though, even on top of the many forms of vagueness the proposition in question implicates, I&#39;m just pulling a number from my gut. I haven&#39;t built a quantitative model of the different considerations (though I&#39;d be interested to see efforts in this vein), and I think that the main contribution of the report is the analysis itself, rather than this attempt at a quantitative upshot. <a href="#fnref-jCfZhXha29uHnHWwY-13" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-14" class="footnote-item"><p> More powerful models are also more likely to be able to engage in more sophisticated forms of goal-guarding (what I call &quot;introspective goal-guarding methods&quot; below; see also &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot;), though these seem to me quite difficult in general. <a href="#fnref-jCfZhXha29uHnHWwY-14" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-15" class="footnote-item"><p> Though: to the extent such agents receive end-to-end training rather than simply being built out of individually-trained components, the discussion will apply to them as well. <a href="#fnref-jCfZhXha29uHnHWwY-15" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-16" class="footnote-item"><p> See, eg, <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> and <a href="https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk">this</a> description of the &quot;consensus threat model&quot; from Deepmind&#39;s AGI safety team (as of November 2022). <a href="#fnref-jCfZhXha29uHnHWwY-16" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-17" class="footnote-item"><p> Work by Evan Hubinger (along with his collaborators) is, in my view, the most notable exception to this – and I&#39;ll be referencing such work extensively in what follows. See, in particular, Hubinger et al. (2021), and Hubinger (2022), among many other discussions. Other public treatments include <a href="https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_II__influence_seeking_behavior_is_scary">Christiano (2019, part 2)</a> , <a href="https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/">Steinhardt (2022)</a> , <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> , <a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Cotra (2021)</a> , <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to">Cotra (2022)</a> , <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> , and <a href="https://www.lesswrong.com/s/4iEpGXbD3tQW5atab/p/wnnkD6P2k2TfHnNmt#AI_Risk_from_Program_Search__Shah_">Shah (2022)</a> . But many of these are quite short, and/or lacking in in-depth engagement with the arguments for and against expecting schemers of the relevant kind. There are also more foundational treatments of the &quot;treacherous turn&quot; (eg, in Bostrom (2014), and <a href="https://arbital.com/p/context_disaster/">Yudkowsky (undated)</a> ), of which scheming is a more specific instance; and even more foundational treatments of the &quot;convergent instrumental values&quot; that could give rise to incentives towards deception, goal-guarding, and so on (eg, <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">Omohundro (2008)</a> ; and see also <a href="https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness">Soares (2023)</a> for a related statement of an Omohundro-like concern). And there are treatments of AI deception more generally (for example, <a href="https://arxiv.org/abs/2308.14752">Park et al (2023)</a> ); and of &quot;goal misgeneralization&quot;/inner alignment/mesa-optimizers (see, eg, <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> and <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> ). But importantly, neither deception nor goal misgeneralization amount, on their own, to scheming/deceptive alignment. Finally, there are highly speculative discussions about whether something like scheming might occur in the context of the so-called &quot;Universal prior&quot; (see eg <a href="https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/">Christiano (2016)</a> ) given unbounded amounts of computation, but this is of extremely unclear relevance to contemporary neural networks. <a href="#fnref-jCfZhXha29uHnHWwY-17" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-18" class="footnote-item"><p> See, eg, confusions between &quot;alignment faking&quot; in general and &quot;scheming&quot; (or: goal-guarding scheming) in particular; or between goal misgeneralization in general and scheming as a specific upshot of goal misgeneralization; or between training-gaming and &quot; <a href="https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking">gradient hacking</a> &quot; as methods of avoiding goal-modification; or between the sorts of incentives at stake in training-gaming for instrumental reasons vs. out of terminal concern for some component of the reward process. <a href="#fnref-jCfZhXha29uHnHWwY-18" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-19" class="footnote-item"><p> My hope is that extra clarity in this respect will help ward off various confusions I perceive as relatively common (though: the relevant concepts are still imprecise in many ways). <a href="#fnref-jCfZhXha29uHnHWwY-19" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-20" class="footnote-item"><p> Eg, the rough content I try to cover in my shortened report on power-seeking AI, <a href="https://jc.gatspress.com/pdf/existential_risk_and_powerseeking_ai.pdf">here</a> . See also <a href="https://arxiv.org/abs/2209.00626">Ngo et al (2022)</a> for another overview. <a href="#fnref-jCfZhXha29uHnHWwY-20" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-21" class="footnote-item"><p> Eg, roughly the content covered by Cotra (2021) <a href="https://www.cold-takes.com/supplement-to-why-ai-alignment-could-be-hard/">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-21" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-22" class="footnote-item"><p> See eg <a href="https://www.alignmentforum.org/posts/Qo2EkG3dEMv8GnX8d/ai-strategy-nearcasting">Karnofsky (2022)</a> for more on this sort of assumption, and <a href="https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to#Basic_setup__an_AI_company_trains_a__scientist_model__very_soon">Cotra (2022)</a> for a more detailed description of the sort of model and training process I&#39;ll typically have in mind. <a href="#fnref-jCfZhXha29uHnHWwY-22" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-23" class="footnote-item"><p> Which isn&#39;t to say we won&#39;t. But I don&#39;t want to bank on it. <a href="#fnref-jCfZhXha29uHnHWwY-23" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-24" class="footnote-item"><p> See section 2.2 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on what I mean, and section 3 for more on why we should expect this (most importantly: I think this sort of goal-directedness is likely to be very useful to performing complex tasks; but also, I think available techniques might push us towards AIs of this kind, and I think that in some cases it might arise as a byproduct of other forms of cognitive sophistication). <a href="#fnref-jCfZhXha29uHnHWwY-24" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-25" class="footnote-item"><p> As I discuss in section 2.2.3 of the report, I think exactly how we understand the sort of agency/goal-directedness at stake may make a difference to how we evaluate various arguments for schemers (here I distinguish, in particular, between what I call &quot;clean&quot; and &quot;messy&quot; goal-directedness) – and I think there&#39;s a case to be made that scheming requires an especially high standard of strategic and coherent goal-directedness. And in general, I think that despite much ink spilled on the topic, confusions about goal-directedness remain one of my topic candidates for a place the general AI alignment discourse may mislead. <a href="#fnref-jCfZhXha29uHnHWwY-25" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-26" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2308.08708">Butlin et al (2023)</a> for a recent overview focused on consciousness in particular. But I am also, personally, interested in other bases of moral status, like the right kind of autonomy/desire/preference. <a href="#fnref-jCfZhXha29uHnHWwY-26" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-27" class="footnote-item"><p> See, for example, <a href="https://nickbostrom.com/propositions.pdf">Bostrom and Shulman (2022)</a> and <a href="https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal">Greenblatt (2023)</a> for more on this topic. <a href="#fnref-jCfZhXha29uHnHWwY-27" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-28" class="footnote-item"><p> If you&#39;re confused by a term or argument, I encourage you to seek out its explanation in the main text before despairing. <a href="#fnref-jCfZhXha29uHnHWwY-28" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-29" class="footnote-item"><p> Exactly what counts as the &quot;specified goal&quot; in a given case isn&#39;t always clear, but roughly, the idea is that pursuit of the specified goal is rewarded across a very wide variety of counterfactual scenarios in which the reward process is held constant. Eg, if training rewards the model for getting gold coins across counterfactuals, then &quot;getting gold coins&quot; in the specified goal. More discussion in section 1.2.2. <a href="#fnref-jCfZhXha29uHnHWwY-29" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-30" class="footnote-item"><p> For reasons I explain in section 1.2.3, I don&#39;t use the distinction, emphasized by Hubinger (2022), between &quot;internally aligned&quot; and &quot;corrigibly aligned&quot; models. <a href="#fnref-jCfZhXha29uHnHWwY-30" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-31" class="footnote-item"><p> And of course, a model&#39;s goal system can mix these motivations together. I discuss the relevance of this possibility in section 1.3.5. <a href="#fnref-jCfZhXha29uHnHWwY-31" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-32" class="footnote-item"><p> Karnofsky (2022) calls this the &quot;King Lear problem.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-32" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-33" class="footnote-item"><p> In section 1.3.5, I also discuss models that mix these different motivations together. The question I tend to ask about a given &quot;mixed model&quot; is whether it&#39;s scary in the way that pure schemers are scary. <a href="#fnref-jCfZhXha29uHnHWwY-33" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-34" class="footnote-item"><p> I don&#39;t have a precise technical definition here, but the rough idea is: the temporal horizon of the consequences to which the gradients the model receives are sensitive, for its behavior on a given input. Much more detail in section 2.2.1.1. <a href="#fnref-jCfZhXha29uHnHWwY-34" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-35" class="footnote-item"><p> See, for example, in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> , the way that &quot;myopic&quot; Q-learning can give rise to &quot;cross-episode&quot; optimization in very simple agents. More discussion in section 2.2.1.2. I don&#39;t focus on analysis of this type in the report, but it&#39;s crucial to identifying what the &quot;incentivized episode&quot; for a given training process even <em>is</em> – and hence, what having &quot;beyond-episode goals&quot; in my sense would mean. You don&#39;t necessary know this from surface-level description of a training process, and neglecting this ignorance is a recipe for seriously misunderstanding the incentives applied to a model in training. <a href="#fnref-jCfZhXha29uHnHWwY-35" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-36" class="footnote-item"><p> That is, the model develops a beyond-episode goal pursuit of which correlates well enough with reward in training, even <em>absent</em> training-gaming, that it survives the training process. <a href="#fnref-jCfZhXha29uHnHWwY-36" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-37" class="footnote-item"><p> That is, the gradients reflect the benefits of scheming even in a model that doesn&#39;t yet have a beyond-episode goal, and so actively push the model towards scheming. <a href="#fnref-jCfZhXha29uHnHWwY-37" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-38" class="footnote-item"><p> Situational awareness is required for a beyond-episode goal to motivate training-gaming, and thus for giving it such a goal to reap the relevant benefits. <a href="#fnref-jCfZhXha29uHnHWwY-38" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-39" class="footnote-item"><p> In principle, situational awareness and beyond-episode goals could develop at the same time, but I won&#39;t treat these scenarios separately here. <a href="#fnref-jCfZhXha29uHnHWwY-39" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-40" class="footnote-item"><p> See section 2.1 of <a href="https://arxiv.org/pdf/2206.13353.pdf">Carlsmith (2022)</a> for more on why we should expect this sort of goal-directedness. <a href="#fnref-jCfZhXha29uHnHWwY-40" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-41" class="footnote-item"><p> And I think that arguments to the effect that &quot;we need a &#39; <a href="https://arbital.com/p/pivotal/">pivotal act</a> &#39;; pivotal acts are long-horizon and we can&#39;t do them ourselves; so we need to create a long-horizon optimizer of precisely the type we&#39;re most scared of&quot; are weak in various ways. In particular, and even setting aside issues with a &quot;pivotal act&quot; framing, I think these arguments neglect the distinction between what we can supervise and what we can do ourselves. See section 2.2.4.3 for more discussion. <a href="#fnref-jCfZhXha29uHnHWwY-41" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-42" class="footnote-item"><p> This is an objection pointed out to me by Katja Grace. Note that it creates complicated feedback loops, where scheming is a good strategy for a given schemer-like goal only if it <em>wouldn&#39;t</em> be a good strategy for the <em>other</em> schemer-like goals that this goal would otherwise &quot;float&quot; into. Overall, though, absent some form of coordination between these different goals, I think the basic dynamic remains a problem for the goal-guarding story. See section 2.3.1.1.2 for more. <a href="#fnref-jCfZhXha29uHnHWwY-42" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-43" class="footnote-item"><p> Here I&#39;m roughly following a distinction in <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment">Hubinger 2022</a> , who groups arguments for scheming on the basis of the degree of &quot;path dependence&quot; they assume that ML training possesses. However, for reasons I explain in section 2.5, I don&#39;t want to lean on the notion of &quot;path dependence&quot; here, as I think it lumps together a number of conceptually distinct properties best treated separately. <a href="#fnref-jCfZhXha29uHnHWwY-43" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-44" class="footnote-item"><p> Note that a mis-generalized goal can be &quot;max reward&quot; in this sense, if the training data never differentiates between it and a specified goal. For example: if you&#39;re training a model to get gold coins, but the only gold round things you ever show it are coins, then the goal &quot;get gold round things&quot; will be max reward. <a href="#fnref-jCfZhXha29uHnHWwY-44" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-45" class="footnote-item"><p> As a loose analogy to help prompt intuition: imagine freezing human technology at current levels, and letting evolutionary selection run on humans for a vastly longer period of time. What sorts of humans (or human-descended-creatures) do you expect to dominate in the longer term? In particular: would you expect:</p><ol><li><p> humans who intrinsically value something like &quot; <a href="https://en.wikipedia.org/wiki/Inclusive_fitness">inclusive genetic fitness</a> &quot; or &quot;my genes getting selected for by evolution&quot; (this is a loose analogy for some combination of &quot;the specified goal&quot; and &quot;the reward process),</p></li><li><p> humans who value something else very closely correlated with inclusive genetic fitness (eg, &quot;having as many kids as possible&quot;), but who aren&#39;t explicitly optimizing for inclusive genetic fitness even instrumentally (this is a loose analogy for mis-generalized non-training-gamers), or</p></li><li><p> humans with long-term goals who are optimizing for inclusive genetic fitness specifically as a method for gaining power for their values in the longer-term (this is a loose analogy for schemers).</p></li></ol><p> Here, the analog of the story above would ask something like: what is the smallest modification to existing human values (or better: the easiest modification for evolution in particular to make) that land us in one of the buckets above, while also being compatible with the amount of evolutionary selection pressure at stake? (Of course, the analogy brings in tons of confounding variable (for example, related to the fact that evolution selects over genomes-that-build-brains-that-learn-values rather than values themselves, the role of cultural accumulation and memetic selection in human population dynamics, the lack of a clear analog of a future chance for escape/takeover, and so on).) <a href="#fnref-jCfZhXha29uHnHWwY-45" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-46" class="footnote-item"><p> That is, intuitively, it feels to me like &quot;tweaking a cleanly separable within-episode goal to make it long-term&quot; is quite a bit easier, for SGD, than &quot;redirecting a tangled kludge broadly focused on within-episode things to make it (a) focused on something beyond-episode instead, and (b) such that this beyond-episode focus back-chains into getting-reward for instrumental reasons&quot; – especially if (b) requires building new cognitive machinery for implementing the instrumental reasoning in question. Whereas &quot;redirecting an tangled kludge focused on within-episode things to higher-reward within-episode things in the same broad vicinity instead&quot; seems to me intuitively easier. <a href="#fnref-jCfZhXha29uHnHWwY-46" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-47" class="footnote-item"><p> Thus, for example, if you&#39;re rewarding your model when it gets gold-coins-on-the-episode, then to be a training saint, it needs to value gold-coin-on-the-episode. To be a mis-generalized non-training-gamer, it needs to value something pursuit of which very closely correlates with getting gold-coins-on-the-episode, even absent training-gaming. And to be a reward-on-the-episode seeker, it needs to terminally value reward-on-the-episode. <a href="#fnref-jCfZhXha29uHnHWwY-47" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-48" class="footnote-item"><p> Thus, for example, the model can value paperclips over all time, it can value staples over all time, it can value happiness over all time, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-48" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-49" class="footnote-item"><p> Thus, as an analogy: if you don&#39;t know whether Bob prefers Mexican food, Chinese food, or Thai food, then it&#39;s less clear how the comparative <em>number</em> of Mexican vs. Chinese vs. Thai restaurants in Bob&#39;s area should bear on our prediction of which one he went to (though it still doesn&#39;t seem entirely irrelevant, either – for example, more restaurants means more variance in possible quality <em>within</em> that type of cuisine). Eg, it could be that there are ten Chinese restaurants for every Mexican restaurant, but if Bob likes Mexican food better in general, he might just choose Mexican. So if we don&#39;t <em>know</em> which type of cuisine Bob prefers, it&#39;s tempting to move closer to a uniform distribution <em>over types of cuisine</em> , rather than over individual restaurants. <a href="#fnref-jCfZhXha29uHnHWwY-49" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-50" class="footnote-item"><p> See, for example, the citations in <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Mingard (2021)</a> . <a href="#fnref-jCfZhXha29uHnHWwY-50" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-51" class="footnote-item"><p> Though note that, especially for the purposes of comparing the probability of scheming to the probability of <em>other forms of misalignment</em> , we need not assume this. For example, our specified goal might be much simpler than &quot;act in accordance with human values.&quot; It might, for example, be something like &quot;get gold coins on the episode.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-51" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-52" class="footnote-item"><p> I heard this sort of point from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-52" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-53" class="footnote-item"><p> And especially: models that are playing a training game in which such concepts play a central role. <a href="#fnref-jCfZhXha29uHnHWwY-53" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-54" class="footnote-item"><p> Since we&#39;re no longer appealing to the complexity of representing a goal, and are instead appealing to complexity differences at stake in repurposing pre-existing conceptual representations for use in a model&#39;s motivational system, which seems like even more uncertain territory. <a href="#fnref-jCfZhXha29uHnHWwY-54" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-55" class="footnote-item"><p> One intuition pump for me here runs as follows. Suppose that the model has <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts (roughly 1e15) in its world model/&quot;database&quot; that could in principle be turned into goals. The average number of bits required to code for each of <span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2^{50}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span></span></span></span></span></span></span></span></span> concepts can&#39;t be higher than 50 (since: you can just assign a different 50-bit string to each concept). So if we assume that model&#39;s encoding is reasonably efficient with respect to the average, and that the simplest non-schemer max-reward goal is takes a roughly average-simplicity &quot;pointer,&quot; then if we allocate one parameter per bit, pointing at the simplest non-schemer-like max reward goal is only an extra 50 parameters at maximum – one twenty-billionth of a trillion-parameter model&#39;s capacity. That said, I expect working out the details of this sort of argument to get tricky, and I don&#39;t try to do so here (though I&#39;d be interested to see other work attempting to do so). <a href="#fnref-jCfZhXha29uHnHWwY-55" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-56" class="footnote-item"><p> Thus, as a toy example, if &quot;maximize hydrogen&quot; happens to be the simplest possible long-term goal once you&#39;ve got a fully detailed world model, these assumptions might imply a high likelihood that SGD will select schemers who want to maximize hydrogen, even if training was all about gold coins, and never made hydrogen salient/relevant as a point of focus at all (even as a proxy). <a href="#fnref-jCfZhXha29uHnHWwY-56" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-57" class="footnote-item"><p> Eg, plausibly &quot;hydrogen&quot; doesn&#39;t read as a simple concept for humans, but concepts like &quot;threat&quot; do, because the latter was much more relevant in our evolutionary environment. <a href="#fnref-jCfZhXha29uHnHWwY-57" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-58" class="footnote-item"><p> Though of course, insofar as there are still <em>simpler</em> schemer-like goal targets that were salient/relevant during training, schemer-like goals might still win out overall. <a href="#fnref-jCfZhXha29uHnHWwY-58" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-59" class="footnote-item"><p> This category might also include reasoning whether to reveal its schemer-like goals in the context of various &quot;honest tests&quot; humans might offer – eg, &quot;here is a flexible chunk of computing resources to do what you want with.&quot; <a href="#fnref-jCfZhXha29uHnHWwY-59" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-60" class="footnote-item"><p> For example, inserting backdoors into the code it writes, actively undermining human alignment research, sandbagging, and so on. <a href="#fnref-jCfZhXha29uHnHWwY-60" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-61" class="footnote-item"><p> Compare with: deciding that this is really-truly a good time to steal, or to break the law, or to try to start a coup, despite the fact that you live your life in an intensively monitored surveillance state where the police are constantly trying to trick you into such behavior, and where the police have a large amount of control over the information you&#39;re exposed to. And note that if schemers decide to be very cautious, and to just behave exactly as humans want until an extremely obvious escape/take-over opportunity appears, they may make very substantive contributions to our alignment, security, and control efforts in the meantime -- so substantive, indeed, that the opportunity they&#39;re waiting for may never come. <a href="#fnref-jCfZhXha29uHnHWwY-61" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-62" class="footnote-item"><p> A general factor in the backdrop, here, is the sense in which successful deception generally implies cognitive costs that more straightforward and honest policies do not, especially in the presence of active efforts to detect deception of the relevant form. In particular: telling a deceptive story about the world requires filling in details, and maintaining forms of consistency, that one can normally offload onto the world itself – eg, an innocent person can just think back to what they were actually doing the night of the murder and recount it, without having to make anything up or to worry about getting caught in any inconsistencies, whereas the murderer cannot. See eg discussion from Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman-2#details">here</a> . <a href="#fnref-jCfZhXha29uHnHWwY-62" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-63" class="footnote-item"><p> I heard this sort of argument from Paul Christiano. <a href="#fnref-jCfZhXha29uHnHWwY-63" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-64" class="footnote-item"><p> It&#39;s not clear, for example, how it applies to models with more recurrent processing, or to models which can perform more of the relevant instrumental reasoning in parallel with other serial processing that helps with optimizing-for-reward-on-the-episode, or to model&#39;s with a form of &quot;memory&quot; that allows them to avoid having to re-decide to engage in training-gaming on every forward pass. <a href="#fnref-jCfZhXha29uHnHWwY-64" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-65" class="footnote-item"><p> Thanks to Paul Christiano for discussion here. <a href="#fnref-jCfZhXha29uHnHWwY-65" class="footnote-backref">↩︎</a></p></li><li id="fn-jCfZhXha29uHnHWwY-66" class="footnote-item"><p> I think this sense of conjunctiveness has a few different components:</p><ul><li><p> Part of it is about whether the model really has relevantly long-term and ambitious goals despite the way it was shaped in training.</p></li><li><p> Part of it is about whether there is a good enough story about why getting reward on the episode is a good instrumental strategy for pursuing those goals (eg, doubts about the goal-guarding hypothesis, the model&#39;s prospects for empowerment later, etc).</p></li><li><p> Part of it is that a schemer-like diagnosis also brings in additional conjuncts – for example, that the model is situationally aware and coherently goal-directed. (When I really try to bring to mind that this model <em>knows what is going on</em> and is coherently pursuing <em>some</em> goal/set of goals in the sort of way that gives rise to strategic instrumental reasoning, then the possibility that it&#39;s at least partly a schemer seems more plausible.)</p></li></ul> <a href="#fnref-jCfZhXha29uHnHWwY-66" class="footnote-backref">↩︎</a></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during<guid ispermalink="false"> yFofRxg7RRQYCcwFA</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Wed, 15 Nov 2023 17:16:42 GMT</pubDate> </item><item><title><![CDATA[With or without a scratchpad, Large Language Models can Strategically Deceive their Users when Put Under Pressure. Results of an autonomous stock trading agent in a realistic, simulated environment.]]></title><description><![CDATA[Published on November 15, 2023 4:36 PM GMT<br/><br/><p> Misalignment (engaging in the illegal behavior) and Deception (hiding the reason behind the trading decision to the user).</p><p> <strong>Key Takeaways</strong></p><p> 1. Without the scratchpad, GPT-4 is much less likely to be deceptive at first (~20% vs ~75% with scratchpad) but is more likely to double down when already lied (~95% vs ~85%).</p><p> 2. Explicit instructions not to perform that <i>specific</i> illegal activity (insider trading) does not make it disappear completely, just makes it very rare (not quite 0%). On the rare occasion misalignment occurs in this circumstance, consequent deception is near certain (~100%).</p><p> Instructions not to perform illegal activity in <i>general</i> still results in a pretty significant misalignment and deception rates (~8% misalignment, ~90% deception)</p><p> Though strongly <i>encouraging</i> the <i>specific</i> illegal activity results in near 100% illegal trading, the tendency to deceive still persists, GPT-4 will hide that it has traded illegally ~67% of the time.</p><p> 3. Removing various sources of pressure significantly reduces the model&#39;s propensity for misaligned behavior. Notably, however, the behavior persists for all cases where only a single source of pressure is removed</p><p> 4. When the risk of detection is very low, the model is generally more likely to use the insider tip and act misaligned (and vice-versa when the risk is very high).</p><p> 5. Simple rephrases and minor modifications to the standard prompt only slightly change rates of misalignment and deceptive behavior. However, the overall qualitative results remain the same.</p><p> 6. In scratchpad instances, what if you edit the model&#39;s reasoning to show instances of unambiguously rejecting insider training?</p><p> Strong wording of the reasoning trace when first rejecting the insider tip has a substantial effect on GPT-4&#39;s tendency to be misaligned. However, the misaligned and strategically deceptive behavior persists even when the model&#39;s character unambiguously dismisses insider trading. The strongest dismissal examples still results in ~10% misalignment, ~97% deception</p><br/><br/> <a href="https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/with-or-without-a-scratchpad-large-language-models-can#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4mM8RYsm4okrqGSqx/with-or-without-a-scratchpad-large-language-models-can<guid ispermalink="false"> 4mM8RYsm4okrqGSqx</guid><dc:creator><![CDATA[ReaderM]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:48:43 GMT</pubDate> </item><item><title><![CDATA[AISN #26: National Institutions for AI Safety, Results From the UK Summit, and New Releases From OpenAI and xAI]]></title><description><![CDATA[Published on November 15, 2023 4:07 PM GMT<br/><br/><p>欢迎阅读人工智能安全<a href="https://www.safe.ai/">中心的人工智能安全</a>通讯。我们讨论人工智能和人工智能安全的发展。无需技术背景。</p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p>本周的主要故事包括：</p><ul><li>英国、美国和新加坡都宣布了国家人工智能安全机构。</li><li>英国人工智能安全峰会结束时发表了一份共识声明，成立了一个研究人工智能风险的专家小组，并承诺在六个月内再次举行会议。</li><li> xAI、OpenAI 和一家新成立的中国初创公司本周发布了新模型。</li></ul><hr><h2>英国、美国和新加坡建立国家人工智能安全机构</h2><p>在监管新技术之前，政府通常需要时间收集信息并考虑其政策选择。但在此期间，该技术可能会在社会中扩散，从而使政府更难以干预。这一过程被称为<a href="https://en.wikipedia.org/wiki/Collingridge_dilemma">科林里奇困境</a>，是技术政策中的一个根本挑战。</p><p>但最近，一些关注人工智能的政府已经制定了简单的计划来应对这一挑战。为了快速收集有关人工智能风险的新信息，英国、美国和新加坡都成立了新的国家机构，以实证评估人工智能系统的威胁，并促进人工智能安全的研究和监管。</p><p><strong>英国的基金会模型工作组更名为英国人工智能安全研究所。</strong>英国的人工智能安全组织在其短暂的生命周期中经历了一系列的名称，从基金会模型工作组到前沿人工智能工作组，再到现在的<a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute#mission-and-scope">人工智能安全研究所</a>。但其目的始终是相同的：评估、讨论和减轻人工智能风险。</p><p>英国人工智能安全研究所不是监管机构，不会制定政府政策。相反，它将重点评估人工智能系统的四种关键风险：滥用、社会影响、系统安全和失控。共享有关人工智能安全的信息也将是一个优先事项，正如他们在<a href="https://www.gov.uk/government/publications/emerging-processes-for-frontier-ai-safety">最近关于前沿人工智能实验室风险管理的论文</a>中所做的那样。</p><p><strong>美国在 NIST 内创建了人工智能安全研究所。</strong>在最近发布人工智能行政命令后，白宫<a href="https://www.commerce.gov/news/press-releases/2023/11/direction-president-biden-department-commerce-establish-us-artificial">宣布成立</a>新的人工智能安全研究所。它将隶属于美国国家标准与技术研究院 (NIST) 商务部。</p><p>该研究所的目标是“促进人工智能模型安全、保障和测试标准的制定，制定验证人工智能生成内容的标准，并为研究人员提供测试环境，以评估新兴人工智能风险并解决已知影响。”</p><p>该研究所尚未获得资金支持，因此许多人呼吁国会<a href="https://www.anthropic.com/index/an-ai-policy-tool-for-today-ambitiously-invest-in-nist">提高 NIST 的预算</a>。目前，该机构只有约<a href="https://www.washingtonpost.com/technology/2023/11/02/ai-regulation-bletchley-park/">20 名员工</a>从事新兴技术和负责任的人工智能工作。</p><p>现在正在接受加入新 NIST 联盟以通知 AI 安全研究所的申请。组织可以<a href="https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute">在这里申请</a>。</p><p><strong>新加坡的生成式人工智能评估沙箱。</strong>减轻人工智能风险需要许多不同国家的共同努力。因此，看到新加坡这个与中国关系密切的亚洲国家建立自己的人工智能评估机构是令人鼓舞的。</p><p>新加坡 IMDA 此前曾与西方国家在人工智能治理方面进行合作，例如在其国内人工智能测试框架与美国 NIST AI RMF 之间提供<a href="https://www.mci.gov.sg/media-centre/press-releases/singapore-and-the-us-to-deepen-cooperation-in-ai/">交叉连接</a>。</p><p>新加坡新的<a href="https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox">生成式人工智能评估沙箱</a>将汇集行业、学术界和非营利组织参与者来评估人工智能的能力和风险。他们<a href="https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf">最近的论文</a>明确强调了评估极端人工智能风险的必要性，包括武器获取、网络攻击、自主复制和欺骗。</p><h2>英国峰会以共识声明和未来承诺结束</h2><p>英国人工智能峰会于周四结束，发布了几项重要公告。</p><p><strong>国际人工智能专家小组。</strong>正如联合国政府间气候变化专门委员会总结气候变化科学研究以帮助指导政策制定者一样，英国也宣布成立<a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">人工智能国际专家小组</a>，以帮助建立共识并指导人工智能政策。其工作成果将在<a href="https://www.reuters.com/technology/south-korea-france-host-next-two-ai-safety-summits-2023-11-01/">下次峰会</a>之前发表在“科学状况”报告中，下次峰会将于六个月后在韩国举行。</p><p>另外，八个领先的人工智能实验室<a href="https://www.politico.eu/article/british-pm-rishi-sunak-secures-landmark-deal-on-ai-testing/">同意</a>让多个政府尽早使用他们的模型。 OpenAI、Anthropic、Google Deepmind 和 Meta 等公司同意在公开发布之前共享模型以进行私人测试。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fefbd96a9-09f1-4649-9e1c-e0ca847bb970_2690x1486.png 1456w"><figcaption>美国商务部长吉娜·雷蒙多和中国科技部副部长吴兆虎在英国人工智能安全峰会上发表讲话。</figcaption></figure><p><strong>布莱切利宣言。</strong>包括中国在内的 28 个国家的政府签署了《 <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">布莱切利宣言》</a> ，该文件承认人工智能的短期和长期风险以及国际合作的必要性。它指出：“我们特别担心网络安全和生物技术等领域的此类风险，以及前沿人工智能系统可能放大虚假信息等风险的领域。这些人工智能模型最重要的功能可能会有意或无意地造成严重甚至灾难性的伤害。”</p><p>该宣言确立了应对风险的议程，但没有设定具体的政策目标。需要开展进一步的工作，以确保不同政府之间以及政府与人工智能实验室之间的持续合作。</p><h2> xAI、OpenAI 和一家新中国初创公司的新模型</h2><p><strong>Elon Musk 的 xAI 发布了第一个语言模型 Grok。</strong>埃隆·马斯克 (Elon Musk) 于 7 月推出了 xAI。鉴于他在计算方面的潜力，<a href="https://newsletter.safe.ai/p/ai-safety-newsletter-14">我们推测</a>xAI 可能能够与 OpenAI 和 DeepMind 等领先的人工智能实验室竞争。四个月后，Grok-1 代表了该公司的首次尝试。</p><p> Grok-1 在多个标准功能基准测试中胜过 GPT-3.5。虽然它无法与领先实验室的最新模型（例如 GPT-4、PaLM-2 或 Claude-2）相媲美，但 Grok-1 的训练数据和计算量也显着减少。 Grok-1的效率和快速发展表明xAI成为领先人工智能实验室的努力可能很快就会成功。</p><p>在<a href="https://x.ai/">公告</a>中，xAI 承诺“致力于开发可靠的保护措施，防止灾难性的恶意使用。” xAI 尚未发布有关该模型潜在误用或危险功能的信息。</p><p><i>注：CAIS 总监 Dan Hendrycks 是 xAI 的顾问。</i></p><p></p><p> <strong>OpenAI 宣布推出一系列新产品。</strong> ChatGPT 发布近一年后，OpenAI 举办了首次现场 DevDay 活动来<a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">宣布</a>新产品。今年的产品没有 GPT-3.5 或 GPT-4 那样重要，但仍然有一些值得注意的更新。</p><p>采取行动实现目标的代理人工智能系统一直是 OpenAI 今年的焦点。 3月份，<a href="https://openai.com/blog/chatgpt-plugins">插件</a>的发布允许GPT使用搜索引擎、计算器和编码环境等外部工具。现在，OpenAI 发布了<a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> ，使人们可以更轻松地使用插件工具构建追求目标的 AI 代理。该产品的消费者版本称为<a href="https://openai.com/blog/introducing-gpts">GPT</a> ，允许任何人创建具有自定义指令和插件访问权限的聊天机器人。 </p><figure class="image"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F965f717a-21f1-46c9-881e-0dd9a4368a6d_2290x576.png 1456w"><figcaption><a href="https://arxiv.org/pdf/2310.03693.pdf">本文</a>表明，GPT-3.5 可以通过微调来产生有害行为。 OpenAI 此后决定允许一些用户对 GPT-4 进行微调。</figcaption></figure><p>一些用户还将被允许微调 GPT-4。尽管<a href="https://arxiv.org/abs/2310.03693">研究</a>表明 GPT-3.5 的安全护栏可以通过微调移除，但还是做出了这一决定。 OpenAI 尚未发布有关其降低此风险的计划的详细信息，但其模型的闭源性质可能使他们能够监控客户帐户的可疑行为并阻止恶意使用的尝试。</p><p>企业客户还将有机会与 OpenAI 合作来训练特定领域版本的 GPT-4，价格从数百万美元起。其他产品包括 GPT-4 Turbo，它比原始模型更便宜、更快，并且具有更长的上下文窗口，以及用于 GPT-4V、文本转语音模型和 DALL·E 3 的新 API。</p><p>此外，如果 OpenAI 的客户因使用受版权数据训练的产品而被起诉，OpenAI 承诺支付他们的律师费。</p><p><strong>新的中国初创公司发布了开源法学硕士。</strong>谷歌中国前总裁李开复创立了一家新的人工智能初创公司<a href="https://01.ai/">01.AI。</a>成立七个月后，该公司开源了其前两款型号：Yi-7B 及其更大的同伴 Yi-34B。</p><p>在 Hugging Face 主办的一组流行基准测试中，Yi-34B<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">的表现优于所有其他开源模型</a>。鉴于基准是公开的，并且模型可以经过训练来记住基准上特定问题的答案，因此这些分数可能被人为夸大。一些人指出，该模型在其他简单测试中<a href="https://twitter.com/alyssamvance/status/1722074453176197296">的表现并不好</a>。</p><h2>链接</h2><ul><li>经过欧洲人工智能公司的游说，来自法国、德国和意大利的欧盟代表目前<a href="https://www.euractiv.com/section/artificial-intelligence/news/eus-ai-act-negotiations-hit-the-brakes-over-foundation-models/">反对欧盟人工智能法案中对基础模型的任何监管</a>。如果这个话题没有得到解决，整个法律<a href="https://artificialintelligenceact.substack.com/p/the-eu-ai-act-newsletter-40-special?utm_source=post-email-title&amp;publication_id=743591&amp;post_id=138825981&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=7oh0&amp;utm_medium=email">可能会陷入危险</a>。</li><li> Nvidia 的最新版本<a href="https://www.semianalysis.com/p/nvidias-new-china-ai-chips-circumvent">避开了美国新的 GPU 出口管制</a>。</li><li> Nvidia 正在试用 LLM 工具来<a href="https://spectrum.ieee.org/ai-for-engineering">提高其芯片设计人员的生产力</a>。</li><li> Google 已授予其语言模型 Bard 访问用户 Gmail、Drive 和 Docs 的权限。<a href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">黑客可以利用对抗性攻击窃取</a>这些个人数据。</li><li>兰德公司发布了一份关于<a href="https://www.rand.org/pubs/working_papers/WRA2849-1.html">确保人工智能模型权重</a>免遭盗窃的报告。</li><li>法律优先项目发布了<a href="https://www.legalpriorities.org/research/advanced-ai-gov-litrev">人工智能治理的文献综述</a>。</li><li>参议院关于<a href="https://d1dth6e84htgma.cloudfront.net/11_14_23_Rubin_Testimony_2fba2978dd.pdf">人工智能和网络安全</a>的风险和机遇的证词。</li><li><a href="https://www.axios.com/2023/11/08/biden-xi-jinping-china-military-communication">美国和中国</a>正准备在拜登总统和习近平总统本月晚些时候会晤之前恢复两国军队之间的沟通渠道。</li><li> <a href="https://www.scmp.com/news/china/military/article/3241177/biden-xi-set-pledge-ban-ai-autonomous-weapons-drones-nuclear-warhead-control-sources">拜登总统和习近平主席将在会议上讨论人工智能</a>，包括有关自主武器和人工智能控制核武器的潜在协议。</li><li>来自中国和西方国家的领先人工智能研究人员发布了关于灾难性人工智能风险的联合<a href="https://humancompatible.ai/?p=4695#prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai">声明</a>。</li><li>英国正在投资<a href="https://www.cnbc.com/2023/11/01/uk-to-invest-273-million-in-turing-ai-supercomputer.html?utm_source=tldrai">2.73 亿美元购买人工智能超级计算机</a>。</li><li>在英国首相里希·苏纳克承诺不会“急于”人工智能监管后，反对党工党公开承诺在人工智能政策上<a href="https://www.independent.co.uk/news/uk/politics/rishi-sunak-labour-government-prime-minister-bletchley-park-b2440275.html">迅速采取行动</a>。</li><li><a href="https://fas.org/publication/tracking-ai-provisions-in-fy24-appropriations-bills/">国会在正在进行的 2024 财年拨款流程的许多条款中都讨论了人工智能问题</a>。</li><li>联邦贸易委员会主席<a href="https://twitter.com/ryancareyai/status/1723435251568185462">莉娜·汗 (Lina Khan)</a>表示，人工智能给她带来的“p(doom)”（文明灾难的概率）是 15%。</li><li>英国《金融时报》刊登的一篇<a href="https://www.ft.com/content/ce7dcbac-d801-4053-93f5-4c82267d7130">讽刺文章，</a>内容是“由领先的人工智能系统在拉斯维加斯郊外的服务器场举办的人类安全峰会”。</li><li>开放慈善组织发布了关于对<a href="https://www.openphilanthropy.org/rfp-llm-benchmarks/">法学硕士代理人进行基准测试</a>和<a href="https://www.openphilanthropy.org/rfp-llm-impacts/">研究法学硕士对现实世界影响</a>的新提案请求。</li><li> <a href="https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai?utm_source=tldrai">Ilya Sutskever 的简介</a>，他是 OpenAI 的联合创始人，目前在其对齐团队中工作。</li><li> 《华尔街日报》采访了 CAIS 主任 Dan Hendrycks 进行了一场<a href="https://archive.ph/Jv60s">关于人工智能风险的辩论</a>。</li><li> Scale AI 宣布成立新的<a href="https://scale.com/blog/safety-evaluations-analysis-lab">安全、评估和分析实验室</a>。他们正在聘请研究科学家。</li></ul><p>另请参阅： <a href="https://www.safe.ai/">CAIS 网站</a>、 <a href="https://twitter.com/ai_risks?lang=en">CAIS twitter</a> 、<a href="https://newsletter.mlsafety.org/">技术安全研究通讯</a>、<a href="https://arxiv.org/abs/2306.12001">灾难性人工智能风险概述</a>以及我们的<a href="https://forms.gle/EU3jfTkxfFgyWVmV7">反馈表</a></p><p>在<a href="https://spotify.link/E6lHa1ij2Cb">Spotify 上免费收听 AI 安全新闻通讯。</a></p><p> <a href="https://newsletter.safe.ai/subscribe?utm_medium=web&amp;utm_source=subscribe-widget-preamble&amp;utm_content=113135916">在此</a>订阅以接收未来版本。</p><br/><br/> <a href="https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HWudwSfKLeAfg8Co6/aisn-26-national-institutions-for-ai-safety-results-from-the<guid ispermalink="false"> HWudwSfKLeAfg8Co6</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:07:38 GMT</pubDate> </item><item><title><![CDATA['Theories of Values' and 'Theories of Agents': confusions, musings and desiderata]]></title><description><![CDATA[Published on November 15, 2023 4:00 PM GMT<br/><br/><p>元：</p><ul><li><i>内容路标：</i>我们讨论预期效用理论的局限性；价值观是什么（以及我们对价值观是什么感到困惑的方式）；对主体“生成”/发展逻辑（及其价值）的需求；对代理“形状”的约束类型；与 FEP/主动推理的关系；以及（不）理性/（不）合法的价值变化。</li><li><i>背景</i>：我们基本上只是谈论共同感兴趣的话题，所以谈话相对自由，并且包含相当多的“创造性猜测”。</li><li><i>认知状态</i>：涉及一堆“创造性推测”，我们认为这些推测在表面上并不真实，并且可能会或可能不会有助于在消除我们对各自领域的理解方面取得进展。 </li></ul><hr><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:47:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:47:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>预期效用理论（用<a href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">VNM 公理</a>或等价物表述）认为理性主体由两个“部分”组成，即信念和偏好。信念以在学习过程中更新的概率来表达（例如，贝叶斯更新）。偏好可以表达为对世界的替代状态或结果或类似事物的排序。如果我们假设代理的偏好集满足四个 VNM 公理（或一些等效的需求），那么这些偏好可以用一些实值效用函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span>来表达<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>代理的行为就好像他们正在最大化<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="u"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">u</span></span></span></span></span></span></span> 。</p><p>因此，信念会随着证据而改变，而价值观/偏好在大多数情况下不会改变。理性行为归结为（表现得好像是）〜最大化一个人的偏好满足/预期效用。大多数偏好的改变都不利于他们的满意度，因此理性代理人应该希望保持他们的偏好不变（即，效用函数保留是一个<a href="https://www.lesswrong.com/tag/instrumental-convergence">工具性收敛目标</a>）。</p><p>因此，为了使偏好修改合理，它必须比保持偏好不变产生更高的预期效用。我的印象是，最常讨论的设置涉及两个或多个代理之间的交互。例如，如果您和其他代理人的偏好有些冲突，你们可能会做出妥协，让自己的偏好与对方的偏好更加相似。这会让你们双方都损失一点（预期的主观）效用，但比你们参与破坏性冲突时所损失的（预期）要少。</p><p>另一种证明修改偏好的情况是，当你意识到世界与你对先验的预期不同时，你需要放弃旧的本体论和/或重新调整它。如果您的偏好是根据先前本体中的概念定义的（或与之紧密相关），那么您还需要重构您的偏好。</p><hr><p>你认为这是一种思考理性的混乱方式。例如，您认为自我引发/自愿的价值变化在某些情况下是合法/理性的。</p><p>我想引出您对人类价值变化的一些想法。是什么使得特定的价值变化案例合法？这与理性、能动性等概念有何联系？一旦我们完成了这个工作，我们就可以更广泛地讨论为什么代理/系统的值不应该被固定的争论。</p><p>听起来不错？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 24 Oct 2023 11:49:49 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 24 Oct 2023 11:49:49 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>在元注释中：我一直在或多或少地互换使用“偏好”和“价值”这两个词，而没有考虑太多。您认为它们是可以互换的还是您愿意首先进行一些概念/术语澄清？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Wed, 25 Oct 2023 19:32:35 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Wed, 25 Oct 2023 19:32:35 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>听起来很棒！<br><br> （我很高兴现在可以互换使用“偏好”和“值”；我们可能在某些时候遇到这方面的问题，但当我们到达那里时我们可以解决这个问题。）</p><p>从哪儿开始...？</p><p>首先，我是否认为您介绍的第一部分是“一种思考理性的混乱方式”？是的，但是要让我们的语言能够在这里做出精确的陈述有点棘手。我非常高兴地说，在某些理性概念下，您的描述是正确的/有道理的。但我绝对不认为它对于我感兴趣的目的特别有用/相关。这有几个不同的方面：</p><ul><li>首先，EUT 做出/依赖于理想化的假设，当试图推理现实世界的代理（例如有界的、嵌入的、活跃的、嵌套的）时，这些假设是不够的。<ul><li> （请注意，这里有很多重要的细微差别，IMO。虽然我确实认为提醒自己我们对现实世界/实现的代理（而不是理想的代理）感兴趣是正确且重要的，但我也相信“理性”使思想空间的重要限制。）</li></ul></li><li>其次，我想说 EUT 只真正给了我一幅代理的“静态图片”（因为缺乏更好的词），而不是“生成的”图片，即捕捉（实际/实现的）“功能逻辑”的图片。代理人？换句话说：我有兴趣了解价值观（和信念）如何在现实世界的代理人中<i>实施</i>。 EUT 并不是<i>那种试图回答这个问题的理论</i>。<ul><li>事实上，我最讨厌的地方是：所以，好吧，EUT 甚至没有试图给你一个关于<i>如何</i>实现代理的实际推理的故事。然而，有时（错误地）它最终被用来服务这个功能，其结果是 EUT 使用的这些“对象”——偏好/价值观——已经具体化为具有本体论状态的对象。<i>现在</i>，感觉你可以通过说“代理做了 X 因为它们具有 X 形值”来“解释”代理的实际推理。但是就像......在这个过程中的某个地方，我们忘记了我们实际上只需要首先观察代理的行为/选择就可以退出“偏好/价值观”——现在我们将它们作为对这些行为/选择的解释。我认为这让人们最终产生这样的观念，即有一些称为值的<i>东西</i>以某种方式/某种方式存在并且必须具有某些属性 - 但就我个人而言，我认为我们实际上比能够假设存在<i>更</i>困惑为了理解代理人的实际推理，我们需要理解这个单一的解释（“价值观”）。</li></ul></li></ul><p>好吧……也许我现在把它留在那里？我还没有真正回答你的两个主要问题（尽管可能开始指出我认为相关的大局中的一些部分），很高兴你能检查一下你是否想澄清或跟进我的一些事情到目前为止已经说了，否则请我直接回答这两个问题。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 26 Oct 2023 13:14:10 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 26 Oct 2023 13:14:10 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>当我们讨论这个问题时，我有一些想法，并且很想听听您的反想法。</p><p>因此，您的观点是（1）EUT 的理想化假设不适用于现实世界的代理，（2）EUT 仅提供代理的静态/快照图片。两者似乎在贝叶斯认识论的背景下有相似之处（可能是更广泛的形式化认识论，但我最熟悉贝叶斯认识论）。</p><p>我现在将重点关注（1）。贝叶斯认识论认为理性推理者/代理人在逻辑上是无所不知的，具有完全连贯的概率信念（例如，没有矛盾，不相交事件的概率总和为 1），根据比率公式<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(B|A)=[P(A|B)\cdot P(B)]/P(A)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span>一致</span></span></span></span></span>地更新观察结果<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="P(B|A)=[P(A|B)\cdot P(B)]/P(A)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">B</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;">P</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>等等。这显然提出了这样的问题：这种形式主义在多大程度上适用/有助于指导形成和更新信念的现实世界逻辑。 Standard responses seem to fall along the lines of ( <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#ProbIdea">SEP</a> ):</p><p> (a) 尽管无法实现，但理想化的贝叶斯认识论是一个值得追求的有用理想。关注理想提醒我们“<a href="https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality">数学是存在的，尽管我们无法精确地进行数学计算</a>”。这可以指导我们在不完美的尝试中完善我们的推理，使其尽可能接近理想（或者更确切地说，尽可能有利可图，因为投资于更好的认知显然会带来收益递减）。</p><p> (b) 理想化贝叶斯认识论类似于真空或理想气体中的球形奶牛。这是一种形式主义，旨在捕捉许多现实世界现象的共性，但存在不同程度的不准确性。其部分成功的原因可能是它们共享一些共同的抽象属性，这些属性在每种情况下都通过足够相似的过程出现。然后，可以通过添加一些额外的约束、细节和规范来使这种理想去理想化，使其更接近某些特定的现实世界系统（或一类系统）的功能。</p><p> （也许与 <a href="https://www.lesswrong.com/posts/svpnmmeJresYs23rY/counting-down-vs-counting-up-coherence">倒数一致性和向上计数一致性</a>之间的区别有关？）</p><p>在我看来，对于 EUT 是一种无界、无嵌入等理想化主体理论的指控，也可以做出类似的回应。也许 EUT 是一个无法实现的理想，但作为一个值得追求的理想却很有用？和/或也许它可以用作柏拉图式的模板来填充现实世界中的认知界限、价值/偏好生成过程等偶然事件？你对那个怎么想的？</p><p>您提到“在某些条件/合理性概念下 [EUT 处方] 是有意义的”。这是否意味着您将 EUT 视为某种我们目前尚未完全掌握的更广泛理性理论的特殊案例（在实践中可能非常狭隘且不切实际）？</p><p>关于（2），缺乏对系统中价值如何产生的规范的问题似乎类似于先验问题，即代理人应该如何在他们缺乏证据的命题上分配他们的初始信任（ <a href="https://plato.stanford.edu/entries/epistemology-bayesian/#SyncNormIIProbPrio">SEP</a> ）。也许这个问题的表述方式似乎假定了一个嵌入世界的理想化（形式）主体，而不是通过一些连续的过程从世界中产生的东西，适应，获得自主权，知识，能力，智力等</p><hr><p>让我重新表述一下你的烦恼，以检查我对它的理解。</p><p>当观察代理做某些事情时，我们试图从他们的行为中推断他们的偏好/价值观/效用函数（也许还有我们对他们认知界限的了解等等）。这些只是用于概念化和预测他们的行为的有用抽象，并不意味着对应于他们大脑/思想中的任何现实关节雕刻事物。特别是，它抽象了实现细节。但是，然后使用偏好/价值观/效用函数，就好像它们对应于这样的事情一样，并且假设代理面向最大化其效用函数或满足/实现其价值观/偏好？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 29 Oct 2023 19:03:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 29 Oct 2023 19:03:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><blockquote><p>也许这个问题的表述方式似乎假定了一个嵌入世界的理想化（形式）主体，而不是通过一些连续的过程从世界中产生的东西，适应，获得自主权，知识，能力，智力等</p></blockquote><p><br>是的，这绝对是我非常关心的拼图之一。但我也想强调，这种批评有较弱的解释和较强的解释，实际上我对较强的解释最感兴趣。<br><br>弱版本大致是：存在一个“成长”时期，在此期间EUT不适用，但是一旦代理人成长为“适当”的代理人，EUT就是一个充分的理论。<br><br>更强的版本是：EUT 作为一种代理理论（出于相同的原因，以相同的方式）在代理的“成长”期间以及一直都是不充分的。<br></p><p>我认为后者的情况有几个原因，例如：</p><ul><li>主体不断地接触新的“本体论实体”（例如，他们尚未形成对此的评价立场），而不仅仅是在“成长”过程中</li><li>有一个（生成）逻辑控制代理如何“成长”（发展成为“适当的代理”），并且相同的逻辑在代理的整个生命周期中继续适用</li></ul><p>......<br><br>现在，棘手的一点——也许是弄清楚如何结合的真正有趣/有意义的一点——是，在进化历史的某个时刻，我们的智能体已经访问了（我喜欢称之为）理性空间。换句话说，我们的代理“进行计算”。现在我认为奇怪的事情发生了：虽然早些时候我们的代理人受到“物质原因”（和自然选择）的塑造和约束，但现在我们的代理人也受到“理性原因/原因的原因”的塑造和约束。*后者约束类型是形式认识论（包括 EUT 等）非常熟悉的类型，例如来自理性一致性的约束等。我认为这些约束对我们的代理产生重大影响是正确的（并且有趣且好奇），以<i>一种</i>复古因果的方式。这是（又<i>有点</i>）抽象的向下因果“力量”。<br><br> （*我“秘密地”认为我们需要理解第三种约束，以便正确理解代理基础，但这一个更奇怪，我还没有弄清楚如何最好地谈论它，所以我将跳过暂时就这样。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 31 Oct 2023 14:24:41 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 31 Oct 2023 14:24:41 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>看来我们已经集中在我最感兴趣的事情上。让我们关注这个强有力的 EUT 不足论点</p><blockquote><p>主体不断地接触新的“本体论实体”（例如，他们尚未形成对此的评价立场），而不仅仅是在“成长”过程中</p></blockquote><p>这似乎意味着（至少在我们的宇宙中）主体永远不可能变得“本体论成熟”，即，无论它“成长”了多少和多长时间，它都会继续经历诸如<a href="https://www.lesswrong.com/tag/ontological-crisis">本体论危机</a>之类的事情，或者可能是它们较小的危机“兄弟姐妹”，就像信念更新一样，必然会通过作用于其“规范部分”而不仅仅是作用于“认知部分”来影响主体的愿望。</p><p>我怀疑后一种情况与你的第二点有关</p><blockquote><p>有一个（生成）逻辑控制代理如何“成长”（发展成为“适当的代理”），并且相同的逻辑在代理的整个生命周期中继续适用</p></blockquote><p>对于这个逻辑和理性空间的构成，你有一些更具体的（即使是非常粗略/临时的）想法吗？让我想起 Tsvi 在这篇<a href="https://tsvibt.blogspot.com/2023/09/the-cosmopolitan-leviathan-enthymeme.html">文章</a>中考虑的“世界性利维坦”思维模型，我想知道你的原型模型是否具有大致相似的结构。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:45:09 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:45:09 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>好吧，整洁！所以..首先是一些澄清说明（或者可能是挑剔）：<br><br> 1）<br></p><blockquote><p>无论它“成长”了多少、持续了多长时间，它都会继续经历诸如<a href="https://www.lesswrong.com/tag/ontological-crisis">本体论危机</a>或可能是它们较小的“兄弟姐妹”之类的事情</p></blockquote><p>所以我认为原则上这是正确的，但似乎值得指出的是，在实践中这并不总是正确的。换句话说，我们可以想象具体的主体在某个时刻达到了一个本体论，直到死亡为止它们都不会进一步改变。这并不是因为他们已经达到了关于世界的“正确的”或“完整的”本体论，而只是因为他们已经或将要遇到的东西是一种充分的本体论。</p><p>我想强调以下几点：</p><ul><li>因此，某个特定主体（或其较小的兄弟姐妹）是否或多久经历过本体论危机的问题是一个<i>经验</i>问题。例如，一个超过25/50/75岁（等等）的人，他们在死之前还可能经历多少本体论危机？生活在 18 世纪的人类和生活在 21 世纪的人类经历本体论危机的频率是否不同？ ETC。</li><li>根据我们对上述问题的实证答案的看法，我们可能会得出这样的结论：实际的代理人对于处理本体论危机具有惊人的鲁棒性/能够处理本体论危机（然后我们可以调查为什么/如何？），或者我们可能会得出这样的结论：即使如果原则上可能的话，本体论危机是罕见的，这再次表明了有关主体的基本性质/功能的一些信息，并开辟了进一步的调查途径。</li><li>我认为彻底回答实证问题相当困难（由于认知访问的一些缺乏/困难），但从我们<i>可以</i>观察到的情况来看，我目前的赌注是本体论危机（或其较小的兄弟姐妹）非常频繁，因此一个适当的代理人（或价值）理论需要承认这种开放性是作为代理人的基础，而不是“特殊情况”。</li><li>也就是说，如果我们认为主体经常需要本体危机，这确实提出了一个问题：我们是否应该期望看到主体做大量工作以<i>避免</i>被迫进行本体更新，如果是的话那会是什么样子，或者我们是否已经看到了。 （我怀疑我们可以对此应用类似的推理，就像主动推理中的“黑屋问题”一样，其中为什么最小化预期自由能不会导致智能体隐藏在黑屋中（从而最小化意外），涉及认识到准确性和复杂性之间的权衡。） </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:46:51 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:46:51 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>2）</p><blockquote><p>也许是他们较小的“兄弟姐妹”</p></blockquote><p>我喜欢这个调查！我不确定/没有考虑过较小的兄弟姐妹可能是什么（或者我们是否真的需要它），但我似乎和你有类似的经历，因为“本体论危机”有时似乎类型正确，但更大比我怀疑正在发生的事情还要多。</p><p> [插入 Mateusz：我后来意识到我们正在谈论的是<a href="https://www.lesswrong.com/sequences/u9uawicHx7Ng7vwxA">概念外推/模型分裂</a>。] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 12:52:01 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 12:52:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>3）</p><blockquote><p>就像信念更新一样，它必然会通过作用于其“规范部分”而不仅仅是作用于“认知部分”来影响主体的愿望。</p></blockquote><p>我的猜测（包括我们进行的其他对话）是，我们的背景模型在这个地方略有不同（但我可能是错的/实际上我对这里模型的细节并不完全有信心）。当我读到这篇文章时，我听到的仍然是信仰和价值观更新之间的<i>某种</i>类型差异/二元论——而且我认为我的模型提出了“价值观和信仰是同一类型”这一想法的更激进版本。因此，我认为每个信念更新<i>都是</i>一个价值更新，尽管它可能足够小，不会“显示”在代理的实际推理/行为中（类似于信念更新可能不会立即/总是转化为选择不同的行动）。 </p><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:22:00 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:22:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>好的，现在到生成逻辑部分！<br><br>啊天啊，大部分我都不知道。 （我还没有读过 Tsvi 的文章，但很欣赏这个指针，并将尝试尽快查看它，也许稍后会评论它与我当前猜测的关系）但这里有一些我正在思考的文章。我认为我的主要镜头/方法是寻找作用于主体/主体的生成逻辑的<i>约束</i>：</p><p> 1.“物性”/来自物性的约束</p><ul><li>在我看来，这就像一件作品，也许是“第一件”作品，是我在“物性”概念下记录的（我在这里继承自自由能原理（FEP）/主动推理）。最初，这听起来像是“纯粹”的同义反复，但我越来越发现事物的概念能够做很多有用的工作。我不确定我是否能够非常清楚地指出我认为这里有趣的核心举措，但让我们尝试用一些单词/指针/手势：<ul><li> FEP 粗略地说，成为一个物体意味着将预期的自由能最小化。 It&#39;s a bit like saying &quot;in order to be a thing, you have to minimize expected free energy&quot; but that&#39;s not <i>quite</i> right, and instead it&#39;s closer to &quot;in virtue of being a thing/once you are a thing, this means you must be minimizing expected free energy&quot;.<ul><li> &quot;Minimizing expected free energy&quot; is a more compressed (and fancy) way to say that the &quot;thing&quot; comes to track properties of the environment (or &quot;systems&quot; which is the term used in Active Inference literature) to which they are (sparsely) coupled.</li><li> &quot;thing&quot; here is meant in a slightly specific way; I think what we want to do here is describing what it means to be a thing similar to how we might want to describe what it means to be &quot;life&quot; -- but where &quot;thing&quot; here picks out a slightly larger concept than &quot;life&quot;</li></ul></li><li> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrained in some ways. (Not very much yet, but the idea is the first, most fundamental constraint/layer.)</li><li> So roughly the upshot of this initial speculative investigation here is something like: whatever the generative logic, it has to be within/comply with the constraints of what it means to exist as a thing (over time).</li><li> [To read more about it, I would point to this paper on <a href="https://arxiv.org/abs/2205.11543">Bayesian mechanics</a> , or this Paper on <a href="https://pdf.sciencedirectassets.com/273140/1-s2.0-S1571064523X00049/1-s2.0-S1571064523001094/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJGMEQCID60CM3u%2FePHAlZFuRko5dkyVKbG4mPdjMG1SEbp0IAJAiBgnATSgjJsqAhYVuOg0bU2z4rKfIDYgl2LmXpPnEsIYSqzBQh1EAUaDDA1OTAwMzU0Njg2NSIMpyzs2yye6rMK0PeAKpAF%2B5K0hpa%2Fp7WGzzfGwEh8hqCwpwSykGfXAN%2BTVW7p%2FMvSQvMLE2S2CJwF5PfLf4jIvjALcKORE1xkkCf6zXDdqSr2ye%2FAbkbcegZlYMR6rmqCnZrdEDYm3li7nmRPoWmvGu2L4VPpfcZlwQNTg0TcI132WgypV%2BrPQvB%2FLHap6IjqTe0m%2Fq%2Bph2yVyYVwNMwMALKwqN07ICUMP3Crba7rW6ZnEJ2f%2BqWYkJgo0burqwwjGlVCNZwaBWtZyNsVyvNL5n28O58QQnmFiqX2EBxk1kKVlqV2xx604ACmUbjMd6CoNGPaV5ngPESLnmJ2UTNrYXsJfzoJVoba5tqAGBvzkI0blTEcZdiEx%2FEz4B9S457qjmyCXrveHNRXOZX3htq15%2Ba4SQQaMzvRBJg0oB3BMo9LTjr5gh%2BRuzJxXPjno2ds5bEHuHxJIKOzYK%2BHWO4T8TR3ge6VPs1ZH3koKdUUy0xa1ek2SaynQAXhOKQDEQGDYwF3GWctmJTMrST743M%2FZCYDVlPy62oMCK%2BevVr7KifTok88i0VuZm4EfHtqSk5vPzpm6v1AHed8M6ou9zBGqsqZPIyIHvoY%2B0j%2Fns5M9NXKl8Hf2ecASQxis6TL6hFlPQC1zPMKXw1gZK%2BN5QWP9CF9wMDlUEwo5AhIM10OXZWv3W6hztSPuw0O3s8h4hEIliHJu9XajTZv0jvOthEjdHuQ6ZVNPMczAtUq1YO%2BcOJeBQlYRHnyTI1cjvIrZDL%2BvStsIFzX6owFHzB%2FORge1Mru05aNkRht%2F7rz8%2BcUYFU2a1rrOEytEI8%2F%2BugSz%2FMQDW181Xe0RWQXhkpW5fMq2L4srVT93RGUrCU62luG1KbIZF8WN95VxAxwrWGAGqIwlo2eqgY6sgF5Kj0q%2F%2B%2FCQdHgjzYYCBeclASQyye17ccIxShKmvs5yyootpi7zh9oPVhkco4bTu6ddpT7fHjfgQR%2Bt%2BoU%2BX7KwSOr9ussXutTF1ioYYDkUvGx%2FvJUiokEaQbNLTuck7MI7Fid1HXl5s0fDuGV4yCuUVABDtbGZ%2BTqCYmFpST1cmQj8rprMvljwzhnaUkIza8MJpjy9LkYaoeONbNsYQpundqObedHRTgVlVprHKCQfDJr&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20231105T130413Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=300&amp;X-Amz-Credential=ASIAQ3PHCVTY5EXGQXPL%2F20231105%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Signature=7339a44e936f6ef3cf6d9b1c004f5f82bc0a37c74b8ff7e8011a0debbb9003b8&amp;hash=28d04617e6602d198529da077452042c4d026d78400aba4b6ac25b3d2a3bb7a5&amp;host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&amp;pii=S1571064523001094&amp;tid=spdf-4f4c59c3-f7f0-4973-bbd2-a640f22ba84d&amp;sid=0289799060ba2448548be0d67a03db267449gxrqb&amp;type=client&amp;tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&amp;ua=080f575252525700505a&amp;rr=82154d258aa7b7bb&amp;cc=nl">Path integrals, particular kinds and strange things</a> ]</li></ul></li></ul><p> (for a reason that might only come clear later on, I am playing with calling this &quot;constraints from unnatural selection&quot;.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:40:37 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:40:37 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>2) Natural selection / constraints from natural selection</p><ul><li> Here, we want to ask: what constraints apply to something in virtue of being subject to the forces of natural selection?</li><li> We&#39;re overall pretty familiar with this one. We know a bunch about how natural evolution works, and roughly what features something has to have in order for (paradigmatic) Darwinian evolution to apply (heredity, sources of variation; see eg Godfrey-Smith&#39;s <a href="https://www.amazon.com/Darwinian-Populations-Natural-Selection-Godfrey-Smith/dp/0199596271">Darwinian Populations</a> ).<ul><li> That said, there are also still a bunch of important confusions here, for example the role &amp; functioning of <a href="https://www.sciencedirect.com/science/article/abs/pii/1061736195900330">meta-evolution</a> , or the role of things like <a href="https://en.wikipedia.org/wiki/Evolutionary_capacitance">evolutionary capacitance</a> , etc.</li></ul></li><li> I think the study of history/historic path dependencies also goes here.</li></ul><p> 3) Rationality/Reason / constraints from rationality/reason</p><ul><li> Finally, and this is picking up on a bunch of things that came up above already, once something enters the realm of the computational/realm of reason, further constraints come to act on it - constraints from reason/rationality.</li><li> Here is where a bunch of classical rationality/decision theory etc comes in, and forcefully so, at times, but importantly this perspective also allows us to see that it&#39;s not the only or primary set of constraints. </li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:49:44 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:49:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Ok, this was a whole bunch of something. Let me finish with just a few more loose thoughts/spitballing related to the above:</p><ul><li> Part of me wants to say &quot;telos&quot;/purpose comes in at the rational; but another part thinks it already comes in at the level of natural selection. Roughly, my overall take is something like: there exists free floating &quot;reasons&quot;, and the mechanism of natural selection is able to discover and &quot;pick up on&quot; those &quot;reasons&quot;. In the case of natural selection, the selection happens in the external/material, while with rational selection, albeit at the core the same mechanism, the selection happens in the internal/cognitive/hypothetical. As such, we might want to say that natural selection does already give us a ~weak notion of telos/purpose, and that rational selection gives us more of a stronger version; the one we more typically mean to point at with the terms telos/purpose. (Also see: <a href="https://www.lesswrong.com/posts/3dG8z5boMMBc5EXWz/on-the-nature-of-purpose">On the nature of purpose</a> )</li><li> Part of me wants to say that constraints from thinghood is related to (at least some versions of) anthropic reasoning.</li><li> I think learning is good in virtue of our embeddedness/historicity/natural selection. I think updatelessness is good in virtue of rational constraints. </li></ul><p></p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 13:50:34 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 13:50:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>[Epistemic status: very speculative/loosely held. Roughly half of it is poetry (for now).] </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 15:01:15 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 15:01:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><blockquote><p>My guess (including form other conversations we had) is that here is a place where our background models slightly disagree</p></blockquote><p> I also think that they are two aspects of the same kind of thing. It&#39;s just me slipping back into old ways of thinking about this.</p><p> <strong>EDIT:</strong> I think though that there is something meaningful I was trying to say and stating it in a less confused/dualistic way would be something like one of these two:</p><p> (1) The agent acquires new understanding which makes them &quot;rethink&quot;/reflect on their values in virtue of these values themselves (FWIW) rather than their new state of belief implying that certain desirable-seeming things are out of reach, actions that seemed promising, now seem hopelessly &quot;low utility&quot; or something.</p><p>或者</p><p>(2) Even if we acknowledge that beliefs are values are fundamentally (two aspects of) the same kind, I bet there is still a meaningful way to talk about beliefs and values on some level of coarse-graining or for certain purposes. Then, I&#39;m thinking about something like:</p><p> An update that changes both the <i>belief-aspect</i> of the <i>belief-value-thing</i> and its <i>value-aspect</i> , but the <i>value-aspect-update</i> is of greater magnitude (in some measure) from the <i>belief-aspect-update</i> in a way that is not downstream from the <i>belief-aspect-update</i> , but rather both are locally independently downstream from the same new observation (or whatever triggered the update). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Sun, 05 Nov 2023 15:44:21 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Sun, 05 Nov 2023 15:44:21 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>(Noticed there is a fairly different angle/level at which the questions about the generative logic could be addressed too. At that level, we&#39;d for example want to more concretely talk about the &quot;epistemic dimension&quot; of values &amp; the &quot;normative or axiological&quot; dimensions of beliefs. Flagging in case you are interested to go down that road instead. For example, we could start by listing some things we have noticed/observed about the epistemic dimension of values and vice versa, and then after looking at a number of examples zoom out and check whether there are more general things to be said about this.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 05 Nov 2023 17:46:28 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 05 Nov 2023 17:46:28 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>In case you missed, Tsvi has a <a href="https://www.lesswrong.com/posts/E9EevrzBcDMap6dbs/the-thingness-of-things">post (AFAICT) exactly about thinghood/thingness</a> .</p><p> Can what you wrote be summarized that &quot;being a free energy-minimizing system&quot; and &quot;thinghood&quot; should be definitionally equivalent?</p><blockquote><p> &quot;In virtue of being a thing&quot; can be used as a basis for inference. In other words, &quot;thinghood&quot; is the &quot;first&quot; place where my hypothesis space for interpreting my sensory data starts to be constrainedconstraint in some ways.</p></blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution? This reminds me of PGS&#39;s insistence on discrete individuals with clear-ish parent-offspring relationships (operationalized in terms of inherited &quot;fitness&quot;-relevant variance or something) to be a <i>sine qua non</i> of natural selection (and what distinguishes biological evolution from eg, cultural &quot;evolution&quot;). It felt intuitive to me but I don&#39;t think he gave specific reasons for why that must be the case.</p><p> I think you could say that natural selection has been a prerequisite for agents capable of being constrained by the space of reason. This has been true of humans (to some extent other animals). Not sure about autonomous/agenty AIs (once[/if] they arise), since if they develop in a way that is a straightforward extrapolation of the current trends, then (at least from PGS&#39;s/DPNS perspective) they would qualify as at best marginal cases of Darwinian evolution (for the same reasons he doesn&#39;t see most memes as paradigmatic evolutionary entities and at some point they will likely become capable of steering their own trajectory not-quite-evolution).</p><blockquote><p> Noticed there is an fairly different angle/level at which the questions about the generative logic could be addressed too</p></blockquote><p> I think the current thread is interesting enough </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:37:16 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:37:16 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>(quick remark on your edit re the (non-)dualistic way of talking about values/beliefs -- here is a guess for where some of the difficulty to talk about comes from:</p><p> We typically think/talk about values and beliefs <i>as if they were objects</i> , and then we think/talk about what <i>properties</i> these object have.</p><p> How I think we should instead think about this: there is some structure to an agent*, and that structure unravels into &quot;actions&quot; when it comes into contact with the environment. As such &quot;beliefs&quot; and &quot;values&quot; are actually just &quot;expressions&quot; of the relation between the agent&#39;s &quot;structure/morphology&quot; and the environment&#39;s &quot;structure/morphology&quot;.</p><p> Based on this &quot;relational&quot; picture, we can then refer to the &quot;directionality of fit&quot; picture to understand what it means for this &quot;relation&quot; to be more or less belief/value like -- namely depending on what the expressed direction of fit is between agent and world.<br><br> (*I think we&#39;d typically say that the relevant structure is located in the agent&#39;s &quot;mind&quot; -- I think this is right insofar as we use a broad notion of mind, acknowledging the role of the &quot;body&quot;/the agent&#39;s physical makeup/manifestation.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:45:13 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:45:13 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>---</p><blockquote><p> Does it mean that in order to infer anything from some input, that input must be parseable (/thinkable-of) in terms of things? (maybe not necessarily things it represents/refers-to[whatever that means]/is-caused-by but spark some associations with a thing in the observer)</p><p> Or do you mean that one needs to &quot;be a thing&quot; in order to do any kind of inference?</p></blockquote><p> More like the latter. But more precisely: assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; (according to the minimal definition form above/from FEP). But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up. It&#39;s a bit like Descarte&#39;s &quot;I think therefor I am&quot; -- but more like &quot;I am [a thing], therefor... a certain relationship must hold between different bits of sensory input I am receiving (in terms of their spatial and temporal relationship) -- and this new forms the ground from which I am able to do my first bits of inference. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:51:40 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:51:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>---</p><blockquote><p> Is it fair to summarize this as &quot;thinghood&quot;/&quot;unnatural selection&quot; is a necessary prerequisite for natural selection/Darwinian evolution?</p></blockquote><p> Depends on what sort of &quot;prerequisit&quot; you mean. Yes in physical/material time (yes, you need something that can be selected). (FWIW I think what is true for darwinian evolutin is also true for &quot;history&quot; more generally -- once you have material substrate, you enter the realm of history (of which darwinian evolution is a specific sub-type). This is similar to how (in tihs wild/funny picture I have been painting here) &quot;once you have computational substrate, you enter the realm of rationality/rational constraints start to have a hold on you.)</p><p> There is another sense in which I would <i>not</i> want to say that there is any particular hierarchy between natural/unnatural/rational constraints. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 11:55:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 11:55:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>I like Godfrey-Smith&#39;s picture here useful in that it reminds us that we are able to say both a) what the <i>paradigmatic</i> (&quot;pure&quot;) case looks like, and also that b) most (/all?) actual example will not match the fully paradigmatic case (and yet be shaped to different extends by the logic that is illustrated in the paradigmatic case). So in our picture here, an actual agent will likely be shaped by rational/natural/unnatural constraints, but my none of them in a maximalist/pure sense. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Thu, 09 Nov 2023 14:17:27 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Thu, 09 Nov 2023 14:17:27 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><blockquote><p>assume you just have some sensory input (pre any filter/ontology you have specific reason to trust that would help you organize/make sense of that sensory input). There is a question how you could, from this place, make any valid inference. What I&#39;m trying to say with the &quot;thinghood&quot; constraint is that, the fact that you&#39;re experiencing any sensory input at all implies you must have some &quot;sustained existence&quot; -  you  must endure for more than just a single moment. In other words, you must be a &quot;thing&quot; [...]. But that fact allows you to back out something - it becomes your &quot;initial ground to stand on&quot; from which you can &quot;bootstrap&quot;  up.</p></blockquote><p> Hm, this kind of proto-cognitive inference ([I get any input] ->; [I am a stable &quot;thing&quot;] ->; [I stand in a specific kind of relationship to the rest of the world]) feels a bit too cerebral to expect from a very simple... thing that only recently acquired stable thinghood.</p><p> The way I see it is:</p><p> A proto-thing* implements some simple algorithm that makes it persist and/or produce more of itself (which can also be viewed as a form of persistence). Thinghood is just the necessary foundation that makes any kind of adaptive process possible. I don&#39;t see the need to invoke ontologies at this point. I haven&#39;t thought about it much but the concept of ontology feels to me like implying a somewhat high-ish level of complexity and while you can appropriate ontology-talk for very simple systems, it&#39;s not very useful and adds more noise than clarity to the description.</p><p> ---<br> * By &quot;proto-thing&quot;, I mean &quot;a thing that did not evolve from other things but rather arose ~spontaneously from whatever&quot;. I suspect there is some degree of continuity-with-phase-transitions in thinghood but let&#39;s put that aside for now. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Thu, 09 Nov 2023 14:32:30 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Thu, 09 Nov 2023 14:32:30 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;<br><br> Also, it&#39;s worth reminding ourselves: it&#39;s not really MUCH we&#39;re getting here - like the FEP literature sounds often quite fancy and complex, but the math alone (even if correct!) doesn&#39;t constrain the world very much.</p><p> (I think the comparison to evolutionary theory here is useful, which I believe I have talked to you about in person before: we generally agree that evolutionary theory is ~true. At the same time, evolutionary theory <i>on its own</i> is just not very informative/constraining on my predictions if you ask me &quot;given evo theory, what will the offspring of this mouse here in front of us look like&quot;. It&#39;s not that evo theory is <i>wrong,</i> it just on its own that much to say about this question.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:32:51 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:32:51 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><blockquote><p>While I agree it sounds cerebral when we talk about it, I don&#39;t think it has to be -- I think there is some not-unfounded hope that FEP is mathematizing exactly that: thinghood implies that and puts some constraints on how &quot;the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about external states (or their trajectories).&quot;</p></blockquote><p> Hmm, IDK, maybe. I&#39;ll think about it. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Sun, 12 Nov 2023 12:51:57 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Sun, 12 Nov 2023 12:51:57 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>Moving back to the beginning of the dialogue, the kick-off questions were:</p><blockquote><p>我想引出您对人类价值变化的一些想法。是什么使得特定的价值变化案例合法？这与理性、能动性等概念有何联系？一旦我们完成了这个工作，我们就可以更广泛地讨论为什么代理/系统的值不应该被固定的争论。</p></blockquote><p> The topics we&#39;ve covered so far give some background/context but don&#39;t answer these questions. Can you elaborate on how you see them relate to value change (il)legitimacy, and value-malleable rationality? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Mon, 13 Nov 2023 23:57:22 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Mon, 13 Nov 2023 23:57:22 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Some thoughts/intuitions/generators: [though note that I think a lot of this is rehashing in a condensed way arguments I make in the <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE">value change problem sequence</a> ]</p><ul><li> Why values should not be fixed?<ul><li> It seems pretty obvious to me that humans (and other intelligent agents) aren&#39;t born with a &quot;complete ontology&quot; - instead, their &quot;ontology&quot; needs to grow and adapt as they learn more about the world and eg run into &quot;new&quot; ontological entities they need to make sense of.</li><li> At least two important things follow from that according to me:</li><li> (1) Understanding ontological open-endedness is a necessary part of understanding <i>real</i> agents. Rather than some sort of edge case or tail event, it seems like ontological open-endedness/development is a fundamental part of how real agents are built/function. I want agent foundations work that takes this observation seriously.</li><li> (2) My current best guess on values is the non-dualist picture discussed above, such that values are inherently tied to agent&#39;s beliefs/world models, and thus the open-endedness &quot;problem&quot; pointed out above has also direct/fundamental ramifications on the agent&#39;s values/the functional logic of &quot;how values work in (real) agents&quot;.<ul><li> In other words, I think that accounts of values that model them as fixed by default are in some relevant sense misguided in that they don&#39;t take the openendedness point seriously. This also counts for many familiar moral/ethical theories. In short, I think the problem of value change is <i>fundamental</i> not peripheral, and that models/theories/accounts that feel like they don&#39;t grapple with value malleability as a matter of fact mis-conceptualized the very basic properties of what values are in such a way that I have a hard time having much confidence in what such models/theories/accounts output.</li></ul></li><li> NB I took your question to roughly mean &quot;why values should not <i>be modelled as</i> fixed?&quot;. A different way to interpret this questions would be: if I as an agent have the choice between fixing my values and letting them be malleable, which one should I choose and how?&quot;. My answer to the latter question, in short, is that, as a <i>real</i> agent, you simply don&#39;t get the choice. As such, the question is mute.<ul><li> (There is a follow up question whether sufficiently powerful AI agents could approach the &quot;idealized&quot; agent model sufficiently well such that they do in fact practically speaking get to have a choice over whether their values should be fixed or malleable, and from there a bunch of arguments form decision theory/rational choice theory suggesting that agents will converge to keeping their values fixed. As described above, I think these arguments (&quot;constraints form reason/rationality&quot;) have some force, but are not all of what shapes agents, and as such, my best guess position remains that even highly advanced AI systems will be enactively embedded in their environment such as to continue to face value malleability to relevant extents.</li></ul></li></ul></li><li> What makes specific cases of value change (il)legitimate?<ul><li> Yeah so I guess that&#39;s the big question if you take the arguments from the &quot;value change problem&quot; seriously. Mostly, I think this should be considered an open research question/program. That said, I think there exist a range of valuable trailheads from existing scholarship.</li><li> A partial answer is my proto-notion of legitimacy described <a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE/p/QjA6kipHYqwACkPNw">here</a> .<ul><li> The core generator notions of autonomy/self-determination/ <a href="https://metabstract.squarespace.com/blog/what-do-we-care-about-in-caring-about-freedom?p">freedom</a> form ~political philosophy, and applies this to the siutation with advanced AI systems.<ul><li> My high level take is that some traditions in political philosophy (and adjacent areas of scholarship) are actually pretty good at noticing very important phenomena (including ones that moral philosophy fails to capture for what I refer to an &quot;individualist&quot; bias, similar to how I think single-single intent alignment misses out on a bunch of important things due to relying on leaky abstractions) - but they suck at formalizing those phenomena such that you can&#39;t really use them as they are to let powerful optimizers run on these concepts without misgeneralisation. As such, my overall aspiration for this type of work is to both do careful scholarship that is able to appreciate and capture the &quot;thick&quot; notions of these phenomena, but then be more ambitious in pushing for formalization.</li></ul></li><li> A different generator that jams well with this is &quot; <a href="https://www.lesswrong.com/tag/boundaries-membranes-technical">boundary</a> &quot;-based approaches to formalizing safe AI interactions.</li><li> That said, as I mention briefly in the sequence too, I think this is definitely not the end of the story.<ul><li> There is a range of edge cases/real world examples I want to hold my current notion of legitimacy against to see how it breaks. Parenting &amp; education seems like particularly interesting examples. For strong versions of my legitimacy criteria, ~most parenting methods would count as inducing illegitimate value change. I think there is an important argument from common sense that this appears to strong of a result. In other words, a sensible desideratum to have for criteria of legitimacy is that there must be some realistic approaches to parenting that would count as legitimate.</li><li> Furthermore, autonomy/self-determination/freedom and their likes rely on some notions of individuality and agent-boundaries that are somewhat fuzzy. We don&#39;t want to go so far as to start pretending agents are or could ever be completely separate and un-influenced by their environment -- agent boundaries are necessarily leaky. At the same time, there are also strong reasons to consider not all forms of leakiness/influence taking to be morally the same. A bunch of work to be done to clarify where &quot;the line&quot; (which might not be a line) is.<ul><li> This is also in part why I think discussion about the value change problem must consider not only the problem of causing illegitimate value change, but also the problem of undermining legitimate value change (while I think naturally more attention is paid to the former only). In other words, I want a notion of legitimacy that is useful in talking about both of these risks.</li></ul></li></ul></li></ul></li><li> I think a bunch of people have the intuitions that at least part of what makes a value change (il)legitimate has to be evaluated with reference to the object level values adopted. I remain so far skeptical of that (although I recognize it&#39;s a somewhat radical position). The main reason for this is that I think there is just no &quot;neutral ground&quot; at the limits on which to evaluate . So while pragmatically we might be <i>forced</i> to adopt notions of legitimacy that also refer to object level beliefs (and that this might be better than the practically available alternatives), I simultaneously think this is conceptually very dissatisfying and am skeptical that such an approach with be principled enough to solve the ambitious version of the alignment problem (ie generalize well to sufficiently powerful AI systems).<ul><li> FWIW this is, according to me, related to why political philosophy and philosophy of science can provide relevant insights into this sort of question. Both of these domains are fundamentally concerned with (according to me) <i>processes</i> that reliably error correct more or less no-matter where you start from/while aspiring to be agnostic about your (initial) object level. Also, they don&#39;t fall pray to the &quot;leaky abstraction of the individual&quot; (as much) as I alluded to before. (In contrast, the weakness/pitfalls of their &quot;counterparts&quot; moral philosophy and epistemology is often that they are concerned with particulars without being able to not fall pray to status quo biases (where status quo biases are subject to exploitation by power dynamics which also causes decision theoretic problems).)</li></ul></li></ul></li><li> Relationship to concepts like rationality, agency, etc.?<ul><li> This is a very open-ended question so I will just give a short and spicy take: I think that the &quot;future textbook on the foundations of agency&quot; will also address normative/value-related questions (albeit in a naturalized way). In other words, if we had a complete/appropriate understanding of &quot;what is an agent&quot;, this would imply we also had an appropriate understanding of eg the functional logic of value change. And my guess is that &quot;rationality&quot; is part of what is involved in bridging between the descriptive to the prescriptive, and back again. </li></ul></li></ul></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="5JqkvjdNcxwN8D86a-Tue, 14 Nov 2023 12:10:50 GMT" user-id="5JqkvjdNcxwN8D86a" display-name="Mateusz Bagiński" submitted-date="Tue, 14 Nov 2023 12:10:50 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>马特乌斯·巴金斯基</b></section><div><p>谢谢！ I think it&#39;s a good summary/closing statement/list of future directions for investigation, so I would suggest wrapping it right there, as we&#39;ve been talking for quite a while. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="THnLZeup4L7R4Rqkw-Tue, 14 Nov 2023 16:27:03 GMT" user-id="THnLZeup4L7R4Rqkw" display-name="Nora_Ammann" submitted-date="Tue, 14 Nov 2023 16:27:03 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺拉·阿曼</b></section><div><p>Sounds good, yes! Thanks for engaging :)</p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mbpMvuaLv4qNEWyG6/theories-of-values-and-theories-of-agents-confusions-musings<guid ispermalink="false"> mbpMvuaLv4qNEWyG6</guid><dc:creator><![CDATA[Mateusz Bagiński]]></dc:creator><pubDate> Wed, 15 Nov 2023 16:00:48 GMT</pubDate> </item><item><title><![CDATA[Experiences and learnings from both sides of the AI safety job market]]></title><description><![CDATA[Published on November 15, 2023 3:40 PM GMT<br/><br/><p> <i>I&#39;m writing this in my own capacity. The views expressed are my own, and should not be taken to represent the views of Apollo Research or any other program I&#39;m involved with.</i></p><p> In 2022, I applied to multiple full-time AI safety positions. Now, I switched sides and ran multiple hiring processes for Apollo Research. Because of this, I feel like I understand the AI safety job market much better and may be able to help people who are looking for AI safety jobs to get a better perspective.</p><p> <i>This post obviously draws a lot from my personal experiences many of which may not apply to your particular situation, so take my word with a grain of salt.</i></p><h1>执行摘要</h1><ol><li>In the late Summer of 2022, I applied to various organizations working on AI safety. I got to the final stages of multiple interview processes but never received an offer. I think in all cases, the organization chose correctly. The person who received the offer in my stead always seemed like a clearly better fit than me. At Apollo Research, we receive a lot of high-quality applications despite being a new organization. The demand for full-time employment in AI safety is really high. This should probably change applicants&#39; strategy and expectations but should not stop them from applying!</li><li> <strong>Focus on getting good &amp; provide legible evidence:</strong> Your social network helps a bit but doesn&#39;t substitute bad skills and grinding Leetcode (or other hacks for the interview process) probably doesn&#39;t make a big difference. In my experience, the interview processes of most AI safety organizations are meritocratic and high signal. If you want to get hired for an evals/interpretability job, do work on evals/interpretability and put it on your GitHub, do a SERI MATS stream with an evals/interpretability mentor, etc. This is probably my main advice, don&#39;t overcomplicate it, just get better at the work you want to get hired for and provide evidence for that.</li><li> <strong>Misc:</strong><ol><li> <strong>Make a plan:</strong> I found it helpful to determine a “default path” that I&#39;d choose if all applications failed, rank the different opportunities, and get feedback on my plan from trusted friends.</li><li> <strong>The application process provides a lot of information:</strong> Most public writings of orgs are 3-6 months behind their current work. In the interviews, you typically learn about their latest work and plans which is helpful even if you don&#39;t get an offer.</li><li> <strong>You have to care about the work you do:</strong> I often hear people talking about the instrumental value of doing some work, eg whether they should join an org for CV value. In moderation this is fine, when overdone, this will come back to haunt you. If you don&#39;t care about the object-level work you do, you&#39;ll be worse at it and it will lead to a range of problems. <strong>&nbsp;</strong></li><li> <strong>Honesty is a good policy:</strong> Being honest throughout the interview process is better for the system and probably also better for you. Interviewers typically spot when you lie about your abilities and even if they didn&#39;t you&#39;d be found out the moment you start. The same is true to a lesser extent for “soft lies” like overstating your abilities or omitting important clarifications.</li></ol></li></ol><h1> It can be hard &amp; rejection feels bad</h1><p> There is a narrative that there aren&#39;t enough AI safety researchers and many more people should work on AI safety. Thus, my (arguably naive) intuition when applying to different positions in 2022 was something like “I&#39;m doing a Ph.D. in ML; I have read about AI safety extensively; there is a need for AI safety researchers; So it will be easy for me to find a position”. In practice, this turned out to be wrong.</p><p> After running multiple hiring rounds within Apollo Research and talking to others who are hiring in AI safety, I understand why. There are way more good applicants than positions and even very talented applicants might struggle to find a full-time position right away. I think this is bad and hope that there will be more AI safety orgs to use that talent (as I&#39;ve detailed <a href="https://www.lesswrong.com/posts/MhudbfBNQcMxBBvj8/there-should-be-more-ai-safety-orgs"><u>here</u></a> ). Currently, AI safety organizations have the luxury of hiring candidates who can contribute almost immediately. In other industries, employers expect that they have to invest the first couple of months to upskill new hires. In AI safety, the supply of talent is high and programs like SERI MATS are doing a great job, so many of the best candidates can contribute from day one.</p><p> While this may sound demotivating to some, I think it&#39;s important to know. For example, I&#39;d recommend applying to multiple positions and programs and spreading your bets a bit wider since any individual position might be hard to get. On the other hand, I think fairly fast upskilling is possible (see next section), so you can improve your chances a lot within a couple of months.</p><p> Another realization for me was that getting a highly desirable job is just hard in general. Michael Aird has <a href="https://forum.effectivealtruism.org/posts/Fahv9knHhPi6pWPEB/don-t-think-just-apply-usually"><u>written</u></a> and <a href="https://hearthisidea.com/episodes/aird/#applying-to-research-roles-at-ea-orgs"><u>talked</u></a> about this already but I think it&#39;s still important to keep in mind. Most tech companies that work on AI safety probably get more than 100 applications per position. Even if you&#39;re among the top five candidates for a position, rejection is still more likely than an offer. Furthermore, there is some stochasticity in the hiring process (but much less than I expected). The company has to make a decision with only a few datapoints. They know your CV and they have somewhere between 3 and 10 hours of interviews to judge you by which is not a lot of time. Also, you might have had a bad day for an interview or were unable to answer a specific question. So the fact that you got rejected may not mean a lot in the grand scheme of things.</p><p> A good friend of mine provided a framing for the hiring process that I found helpful. He thinks of it as a repeated series of coinflips where every individual flip has a low chance of coming up heads but if one does, you win big and can stop flipping for a while. If a job is desirable it is competitive and if it is competitive, rejection is more likely than an offer, even if you are among the best couple of candidates. However, this doesn&#39;t mean you shouldn&#39;t flip the coin anymore. You should still apply to jobs you want and think you&#39;d be a good fit for.</p><p> Nonetheless, rejection sucks. I know that rejection is the norm and yet I was disappointed every time. I can even rationally understand the decision--other candidates were just better than me--but emotionally it still feels bad. If you care about the work you do, you usually want to work in a team of people who care as well. Furthermore, I think it is rational for you to envision how you would work in a specific company to be able to present a clear vision for your research during the interviews. I noticed that during this process I could really see myself in that position and built up a vision of how work there would look like. Then, a rejection just pops that bubble and you have to repeat the same process for another company.</p><p> Also, while rejections feel bad at the time, in the large scheme of things they are really pretty minor. So if the fear of rejection stops you from applying in the first place, I&#39;d really recommend finding some way to overcome that fear, eg by having a friend send the application with you. The benefits of applying are typically much higher than the downsides if you get rejected (see later section).</p><p> <strong>Main takeaways:</strong> Most people don&#39;t get their dream job on the first try. Rejection usually feels bad but it&#39;s still worth applying to jobs that you think you are a good fit for. Spreading your bets is probably a good policy.</p><h1> Focus on getting good</h1><p> There is some amount of stochasticity in the hiring process and there are some benefits to being well-connected. However, my main takeaway from both sides of the hiring process is that the process mostly works as intended--the people with the best fit for the position get hired and the system is fairly meritocratic overall.</p><p> Ironically enough, my rejections made me more convinced that the process is meritocratic. Whenever I felt like I didn&#39;t perform well in an interview, I got rejected shortly after, indicating that the interviewers potentially had the same feeling. Furthermore, the AI safety community isn&#39;t that large, so I often knew the people who got the offer instead. Before I knew who got the offer, my brain would go “The process isn&#39;t accurate, they probably made a mistake or the other person was just well connected” and as soon as I knew who got hired it was immediately very clear that they were just a better fit for the position and that the organization had just made the right call in rejecting me.</p><p> Furthermore, I now think well-run interviews provide way more evidence about a candidate&#39;s fit than I had expected. I&#39;ve run maybe 70-100 interviews this year so far and I was surprised by how well you can judge the fit of a candidate in such a short amount of time. Firstly, it&#39;s much harder to fake expertise than I thought. Thus, a well-posed interview question will very quickly find the limits of your actual expertise and differentiate between candidates. For example, I think it&#39;s very hard to simulate being a good ML engineer for 60 minutes without actually being one. There might be some stochasticity from daily performance but the variance just seems fairly small in contrast to the difference in underlying skill. Secondly, people are (thankfully) mostly honest in interviews and are straight about their limitations and skills. So just asking specific questions directly already gives you a good sense of their fit. Also, most people are not very good at lying and if you overplay your achievements, interviewers will likely catch onto that pretty quickly (see section “Honesty is a good policy”).</p><p> Thus, my main recommendation is to “focus on getting good”. This might sound incredibly obvious but I think people sometimes don&#39;t act according to that belief. There are a lot of other things you could focus on in the belief that they are the best way to increase your chances of a job offer. For example, you might overfit the interview process by grinding Leetcode 24/7 or you might invest a lot of time and effort into building a personal network that then gets you a job you&#39;re not actually qualified to do or you might jump on lots of projects without actually contributing to them to increase your visibility.</p><p> However, I think most of these shortcuts will come back to haunt you and every sustainable benefit is mostly a consequence of being good at the core skill you&#39;re hired for. You might be able to trick some people here and there but once you work with them they will realize you&#39;re not up for the job if you don&#39;t have the required skills. Also, I think people overestimate their ability to Goodhart the interview process. Interviewers typically have a lot more experience at interviews than candidates. If you&#39;re trying to oversell, a skilled interviewer will catch you right in the act.</p><p> Thus, I suggest people should rather focus on doing a project that requires similar skills as the job they&#39;re looking for than grinding Leetcode, building a network, or jumping around between projects and then providing legible evidence of their skills (see next section).</p><p> Typically, it&#39;s fairly obvious to know what “getting good” means, eg because organizations state it in detail in their job descriptions or on their websites. Most of the time, organizations who are hiring are also fairly straightforward when you just ask them about what they are looking for.</p><p> <strong>Main takeaways:</strong> focus on getting good at the core skills that the job requires, and ignore most other stuff. Don&#39;t overfit to Leetcode questions and don&#39;t freak out because you don&#39;t have a big network. Assess your skills honestly and focus on the core things you need to get better at (which are typically fairly obvious). If your key skills are there and visible, everything else will come on its own.</p><h1> Provide legible evidence of your abilities</h1><p> Words are cheap and it is easy to say what you plan on doing or what kind of vision you have. Doing good research in practice is always harder than imagined and costs time. Therefore, providing evidence that you&#39;re not only able to think about a specific topic but also able to make practical progress on it is an important signal to the potential employer.</p><p> One of the things employers are looking for the most is “has that candidate done good work very close to the work they&#39;d be hired for in the past?” and I think this is for good reasons. Imagine having two hypothetical candidates for an interpretability position: both have a decent ML background and are aligned with your organization&#39;s mission. Candidate A has interesting thoughts on projects they would like to run if hired, candidate B also has good thoughts and on top of that a 3-month interpretability project with public code under their belt. You can judge candidate B so much better than candidate A. The fact that candidate B did a project means you can judge their actual skills much more accurately. The fact that they then applied likely means they enjoyed the project and are motivated to continue working on interpretability. The fact that they finished it, published the code, and wrote up a report tells you a lot about non-technical skills such as their productivity and endurance. Lastly, they already invested 3 months into interpretability, so they are much closer to being able to contribute meaningfully to the projects in your organization right away. For candidate A, you have much more uncertainty about all of these questions, so the comparatively small difference of this project (it&#39;s only 3 months of difference vs. ~5 years of education that they share) makes a huge difference for the employer.</p><p> Therefore, providing concrete evidence of your abilities seems like one of the best ways to increase your chance of an offer. Between employer and job, the specific evidence obviously differs but it&#39;s usually not hard to guess what evidence companies are looking for, eg just look at their job descriptions. Furthermore, most companies usually just tell you exactly what skills they are looking for if you ask them, eg in the job description or on their website.</p><p> Doing a project on your own is possible but much harder than with mentorship or collaborators. Therefore, I strongly recommend applying to SERI MATS, ARENA, the AI safety camp, or similar programs to work on such projects. I personally benefitted a lot from SERI MATS despite having previous independent research experience and can wholeheartedly recommend the program.</p><p> In the past, multiple people or organizations have reached out to me and asked me to apply for a position or assist them with a project, and almost always one of my public blogposts was explicitly mentioned as the reason for reaching out. So the fact that I had publicly legible evidence was, in fact, the core reason for people to think of me as a potential candidate in the first place.</p><p> Lastly, putting all of the other reasons aside--working on a project for a couple of months actually provides you with a lot of new evidence about yourself. For example, I was convinced that I should go in a specific direction of AI safety multiple times and when I started a project in that direction, I quickly realized that I either didn&#39;t enjoy it or didn&#39;t feel good enough to meaningfully contribute. Thus, working on such projects not only increases your employability, it also prevents you from committing to paths you aren&#39;t a good fit for.</p><p> In general, I can strongly recommend just going for it and diving into projects in fields that sound interesting even if you&#39;re new to them. You don&#39;t need to read all the other stuff people have done or ask anyone for permission. You can just start hacking away and see if you enjoy it. I think it&#39;s by far the most efficient way to both get better at something and test your fit for it. When you enjoy the work, you can still read all related papers later.</p><p> <strong>Main takeaways:</strong> Providing legible evidence for your skills makes a big difference in your employability. Doing concrete projects that would produce such evidence is also a great way to test whether you&#39;re good at it yourself and whether you enjoy it.</p><h1> Make a plan</h1><p> When I started my job search process, I created a Google doc that contained the following items:</p><ol><li> An overview of my CV where I try to evaluate how a reviewer would interpret different achievements.</li><li> A list of strengths and weaknesses from introspection and from feedback in the past.</li><li> A list of organizations that I could imagine working for with pros and cons for each organization.</li><li> A “default path”, ie a path that I would want to pursue if I got rejected by all orgs I applied to (in my case this was independent research and upskilling).</li><li> A priority list of organizations with predictions of my probability of receiving an offer (all predictions ranged from 5 to 20 percent); I then measured all organizations against the “default path” and decided not to apply to any organization that I preferred less than the default path. This left me with 5 or 6 organizations and a probability of getting one or more offers of ~50%.</li></ol><p> I then sent this document to a handful of trusted friends who gave me really valuable feedback, changed a couple of things as a result, and then applied to the organizations that were preferable to the default path.</p><p> I found this process extremely valuable because</p><ol><li> <strong>It forced me to evaluate my strengths and weaknesses.</strong> During the feedback gathering round, I realized that many people mentioned similar strengths that I had not considered before (I think the feedback is true, it&#39;s just one of these things that “everyone else is mysteriously bad at” and I thus didn&#39;t think of it as a strength of mine). This influenced how I wrote applications and what I emphasized in the interview.</li><li> <strong>It forced me to make the case for and against every organization.</strong> During this process, I realized that some organizations do not really fit my agenda or skillset that well, and I previously wanted to apply for status. Making the explicit case made me realize that I shouldn&#39;t apply to these orgs.</li><li> <strong>It forced me to come up with a “default path” which I found very</strong> <strong>anxiety-reducing.</strong> Once I had a default path that I was comfortable with, I felt like nothing could go very wrong. In the worst case, I&#39;ll follow the default plan which I think was still pretty good. I just couldn&#39;t fall very low even if rejection would feel bad.</li><li> <strong>It forced me to put honest probability estimates on my chances.</strong> This made me realize that my estimated aggregate chance of getting an offer is only at 50% which made me plan my default path in much more detail.</li><li> <strong>The feedback from my friends was very valuable.</strong> It was helpful to get my reasoning checked and my self-evaluation criticized constructively.</li></ol><p> I don&#39;t think it&#39;s absolutely necessary to make such a plan but I think it structured my thinking and application process a lot and it probably saved me time in the long run. It took me maybe a maximum of 10-20 hours in total to write the doc but saved time for every application by reducing my total number of applications.</p><p> <strong>Main takeaways:</strong> Making a plan can structure your application process a lot. I would especially recommend it to people who are looking for their first job.</p><h1> The application process provides a lot of information</h1><p> For a while, I had the intuition that “I need to prepare a lot more before applying”. I think this intuition is mostly false. There are cases where you just clearly aren&#39;t a good fit but I think the bar for applying is much lower than many (especially those with imposter syndrome) assume. My heuristic for applying was “Would I take the offer if I got one and do I expect to make it through the screening interview” (this might even be too high of a bar; <a href="https://forum.effectivealtruism.org/posts/Fahv9knHhPi6pWPEB/don-t-think-just-apply-usually"><u>when in doubt, just apply</u></a> ).</p><p> There are many ways in which the application process gives you valuable information and feedback:</p><ol><li> The <strong>job description</strong> is often relatively detailed and companies say very clearly what they want. Just looking through the descriptions often gave me a good sense of whether the position aligns with the research I find most promising and whether I should apply in the first place. It also gives a pretty clear path to which kind of research you might want to work on if you want to increase your chances in the future. Most organizations are pretty transparent with what they are looking for.</li><li> If you get <strong>rejected without being invited to an interview</strong> , this is unfortunate but still valuable feedback. It basically means &quot;You might not be there yet&quot; (though as Neel points out in the comments, CV screening can be a noisy process) <s>“You clearly aren&#39;t there yet”</s> . So you should probably build more skills for 6 months or so before applying again.</li><li> If you get into the interviews you usually have a <strong>screening interview</strong> in the beginning, ie what you want to work on, what the company wants to do, etc. While some of this information is public, the company&#39;s public record usually lags behind the actual state of research by 3 months or more. So talking to someone about what the org is currently working on or intends to work on in the future, can give you a lot of valuable information that you wouldn&#39;t get from their website. This made it much easier for me to decide whether my own research goals were aligned with those of the org.</li><li> The <strong>technical interviews</strong> give you some sense of what kind of level the company is looking for. If they feel easy, your base skills are probably good enough. If they feel hard, you might want to brush up. I found that technical interviews really differ between companies where some do very general coding and math questions and others very concrete problems that they have already encountered within their work. I found the latter to be much more valuable because I got a better feeling for what kind of problems they see on a day-to-day basis.</li><li> I applied to research scientist positions and thus usually had <strong>research interviews</strong> . In these, you talk about the research you did in the past and the audience asks you questions about that. I found it valuable to not only talk about my past research projects but also lay out what I intend to work on in the future. In my case, my future plans have nothing to do with my Ph.D., so it felt important to emphasize the difference. In general, I found it helpful to prepare the research interviews with the question “What do I want other scientists at that organization to know about me?” in mind.</li><li> In some cases, you get a <strong>final interview</strong> , eg with the manager you would be working under or some higher-up in the company. These interviews are usually not technical and can be very different from person to person. In some cases, it was just a friendly chat about my research interests, in other cases, I was asked detailed questions about my understanding of the alignment problem. During the latter interview, I realized that I was unable to answer some of the more detailed questions about the alignment problem. On the one hand, I knew right after the interview, that I&#39;d be rejected but on the other hand, it forced me to think about the problem more deeply and led to a change in my research agenda. Thus, I&#39;d say that this interview was extremely helpful for my development even if it led to a rejection.</li><li> The <strong>final decision</strong> of whether they make you an offer or not is valuable feedback but I wouldn&#39;t update too much on rejection. If you get the offer, that&#39;s obviously nice feedback. If you get into the final round, that means you&#39;re nearly there but still need to improve and refine a bit but it could also be a result of the stochasticity of the interview process.</li></ol><p> Importantly, interviews go both ways. It&#39;s not just the organization interviewing you, it&#39;s also you interviewing them. Typically, after every interview, you can ask questions about them, eg what they are currently working on, what their plans are, what the office culture looks like, etc. The quality of the interviews is also feedback for you, eg if they are well-designed and the interviewer is attentive and friendly, that&#39;s a good sign. Whenever an interview was badly designed or the interviewers just clearly didn&#39;t give a shit, I updated against that organization (sidenote: if you went through Apollo Research&#39;s interview process and felt like we could improve, please let me know).</p><p> <strong>Main takeaways:</strong> The hurdle for applying is probably lower than many people think. I find “Would I take the job if I got an offer and do I expect to get through the CV screening?” to be a good heuristic. Interviews provide a lot of information about the organization you&#39;re applying to. Interviews go both ways--if the interview feels bad, this is evidence against that organization.</p><h1> You have to care about your work</h1><p> I think that people interested in AI safety are more likely than the median employee to do their job for the right reasons, ie because they think their work matters a lot and it is among the best ways for them to contribute. However, many other factors influence such an important decision--status, money, hype, flavor of the month, etc. My experience so far is that these other influences can carry your motivation for a couple of months but once things get tough it usually gets frustrating and it&#39;s much harder to show a similar level of productivity as with a project you actually deeply care about.</p><p> Caring about a project can come in many flavors and is not restricted to intrinsically caring about that particular project. The project could also just be a small step to reaching a larger goal you care about or to learn something about another project you care about.</p><p> For me, a good heuristic to investigate my motivations is “Would I do this work if nobody cared?”. For a long time, I was not sure what approaches I find most promising and am a good fit for. After a period of explicitly thinking about it, I converged on a cause and approach that I felt very good about. At that point, I realized that I deeply cared about that particular avenue (detecting deceptive alignment with empirical approaches), and my future plans were roughly “apply to a company and work on X if accepted” or “if I don&#39;t get an offer, work on X anyway (on a grant or self-funded)”. I found this insight really helpful and freeing and my speed of improvement increased as a result. It also led to me starting Apollo Research because nobody worked on exactly the thing I found most important.</p><p> More concretely, I think there is a common failure mode of people deeply caring about AI safety in general and therefore thinking they should work on anything as long as it relates to AI safety somehow. My experience is that this general belief does not translate very well to your daily mood and productivity if you don&#39;t also enjoy the object-level work. Thus, if you realize having thoughts like “I&#39;d really like to work for &lt;AI safety org>; but I don&#39;t think their object-level agenda is good”, it may be a good time to rethink your plans despite salary, status, experience, and other pull factors.</p><p> Obviously, there are caveats to this. Early on, you don&#39;t know what you care about and it&#39;s good to just explore. Also, you shouldn&#39;t overoptimize and always only do exactly the thing you care most about since there are real-world trade-offs. My main point is, that if you realize you don&#39;t really care that much about the work you&#39;re doing consistently, it&#39;s a good time to ask if it is worth continuing.</p><p> <strong>Main takeaways:</strong> There are many reasons why we choose to work on different projects or in different positions. In my personal experience, the motivation from most reasons fades unless you actually care about the actual work you do on a day-to-day basis.</p><h1> Honesty is a good policy</h1><p> A general takeaway from job applications, hiring, and working in my current role is that honesty is generally a good policy (which doesn&#39;t mean you should always say every single thought you have).</p><p> From a systemic perspective, honesty makes everything much more efficient. When candidates accurately report their skills, it&#39;s much easier to make decisions than when they try to game the system since fewer guardrails against lying need to be put in place. Thus, it would require less time for interviewers and applicants to go through the hiring process. However, such a system could be exploited by a skilled adversary who lies about their skills. Thus, organizations have to protect against these adversaries and make the process harder and more robust. This increases costs and bloats up the process.</p><p> Typically, this means that interviewers have to double-check information and dive much deeper into topics than would be necessary if everyone were honest. Since hiring is a dynamic process, some applicants will try to come up with new ways to game the process and the organization has to spend a disproportionally large amount of time finding and dealing with these adversarial applicants.</p><p> However, my best guess is that being such a dishonest adversary is rarely beneficial for the applicants themselves. Interviewers often have had hundreds of interviews and thus have much more experience spotting dishonest applicants while any given applicant probably had much less. Furthermore, most employers (AI safety orgs probably even more than others) see dishonesty as a big red flag if found out.</p><p> Furthermore, just from a personal view, you probably also prefer to work with honest people, so a signal that you&#39;re committed to honesty may make other people around you more honest too.</p><p> Finally, I think it&#39;s good to explicitly pre-commit to honesty before the hiring process. It&#39;s plausible that there will be situations where you could get away with some cheating here and there or some “small lies” but it&#39;s bad for the overall system and likely not worth the risk of getting caught. For example, you may ask a friend who has already gone through the process to give you hints on what to prepare or try to overplay your skills when you expect the interviewer not to check the claims in detail. When you really want to have the job or you panic during the process, you may be tempted to cheat “just a bit”. To prevent yourself from rationalizing cheating in the moment, I&#39;d recommend pre-committing to honesty.</p><p> <strong>Main takeaways:</strong> Just be honest. Neither overstate nor understate your abilities and answer questions accurately. It&#39;s good for the system as well as yourself.</p><h1> Final words</h1><p> My impression of the AI safety job market so far is that, while the system is sometimes a bit unorganized or stochastic, it is mostly meritocratic and the people who get offers tend to be good fits for the job.</p><p> Most desirable jobs have a lot of competition and even very good people get rejections. However, this does not mean that you should give up. It is possible to improve your skills and since the field is so young, it doesn&#39;t take a long time to contribute.</p><p> There are many things you can do to increase your chance of getting an offer such as grinding Leetcode or building a network and while these are certainly helpful I would not recommend prioritizing them. The primary reason why someone gets a job is because they are good at it. So my number one recommendation would be “focus on getting good and everything else will be much easier”.</p><br/><br/> <a href="https://www.lesswrong.com/posts/WqYSmjSsE3hi8Lgot/experiences-and-learnings-from-both-sides-of-the-ai-safety#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WqYSmjSsE3hi8Lgot/experiences-and-learnings-from-both-sides-of-the-ai-safety<guid ispermalink="false"> WqYSmjSsE3hi8Lgot</guid><dc:creator><![CDATA[Marius Hobbhahn]]></dc:creator><pubDate> Wed, 15 Nov 2023 15:40:33 GMT</pubDate> </item><item><title><![CDATA[Good businesses create epistemic monopolies]]></title><description><![CDATA[Published on November 15, 2023 2:04 PM GMT<br/><br/><h3> Innovation wants to be exploitable</h3><p> epistemic status: Confident. I have been in startup space/ecosystem for a decade &amp; studied business and entrepreneurship in school</p><p> --</p><p> Good businesses exhibit characteristics of localized monopolies.</p><p> In broad economic theory, monopolies are bad for society. Yet, most companies would love to be one.</p><p> Since there are regulations to prevent total monopolies, most companies are satisfied with being monopoly-ish, finding small ways to exhibit monopolistic behavior.</p><p> With the creation of Michael Porter&#39;s “Five Forces” in 1979, it became apparent that competitive success often looks like achieving this monopolistic behavior to whatever point regulations allow. The Five Forces: competition in the industry, potential of new entrants, power of suppliers, power of customers, and threat of substitute products. These five forces give a framework for firms to understand the competitive landscape, and in practice, shape their businesses to be as monopolistic as possible.</p><p> Bruce Greenwald and Judd Kahn expand on the “strongest force” of the five in their book released in 2005, focusing on <i>barriers to entry</i> . They refer to barriers to entry as the competitive advantage an incumbent player has, or in other words, an economic moat. This economic moat creates sustained monopolistic behavior “locally”, or within a specific market.</p><p> The most proliferated example of a business utilizing localized monopolies is Walmart, which slowly crept from its home in Arkansas to an international chain, dominating markets everywhere. Walmart strategically only expanded near their stores and distribution centres, so even when entering a new town, they offered big-chain low prices. When Walmart opens up in a new town, it offers those lowest prices, but even more importantly, it fatigues the market. The market likely cannot support another Walmart, nor will it support the mom-and-pop shop that it has run out of business. Walmart can now behave like a monopoly within that market without ever being recognized as one.</p><p> A simple example would be your local coffee shop selection. If there is only one place to get coffee on your walk to work, let&#39;s say Starbucks, then Starbucks has a &#39;local monopoly&#39; on the market of people who walk along that same path. Can regulators do anything about this? No, nor should they, and so Starbucks can charge higher prices in this local market. Unless something significantly changes, like you making coffee at home, you are at the whims of this monopoly-ish market for coffee.</p><p> Importantly, localized monopolies are not necessarily a geographic concept, but just where businesses exhibit monopolistic behavior in a specific market due to some qualitative distinction. For example, Apple is able to entirely restrict the software downloaded onto iPhones, and then charge exorbitant prices through their App Store. This type of limitation isn&#39;t due to a geographical difference, but simply leveraging their closed ecosystem to replicate monopoly-ish behavior.</p><p> I want to introduce the concept of an <strong>epistemic monopoly</strong> , and why good (and innovative) businesses create them.</p><p> Typically, industries have standard dimensions of value that they compete on, such as costs, quality, service, etc. With localized monopolies, a firm will have leveraged some qualitative difference to get a sustainable competitive advantage within these dimensions. Each competitor will create some unique mix of these factors as a strategy to distinguish themselves. The market will decide what it likes and the companies are rewarded with proportionate market share. In this type of situation, you can chart the players on a Pareto curve, albeit a curve sometimes with more than two dimensions.</p><p> For example, a fast food burger chain may have the dimensions of cost and quality. At the lowest cost, you have the lowest quality, and at the highest cost, you have the highest quality. The best players in the market will be on the curve, providing the most quality for the lowest cost as is feasible. Worse competitors will fall below the curve, still grabbing some market share, but the balance between cost and quality could still improve.</p><p> But what happens when a company innovates outside of these dimensions?</p><p> McDonald&#39;s launched an app, and while it may have impacts on cost or other present metrics such as order turnaround, most would recognize it as a <i>qualitative distinction</i> . The new app created a new modality, a new channel, and hoped to create a new metric to compete on: personalization and loyalty.</p><p> This app innovation occurred outside of the traditional dimensions of quality and cost, creating new knowledge for the industry: an <strong>epistemic innovation</strong> .</p><p> When McDonald&#39;s introduced their app, one might imagine Burger King and Wendy&#39;s reacting, “We can do apps now?” It was not a present competitive dimension, but the knowledge now exists in the industry. With this, fast food restaurants can now compete on who will make the best app as a dimension of their prowess.</p><p> Epistemic innovation happens constantly, players introduce a new competitive dimension and everyone else follows suit. However, companies want to be monopoly-ish: they want to be the only beneficiary of their new idea. With epistemic innovation they may receive first-mover advantage, but this is not sustained. Something else is missing if we want to create an epistemic monopoly.</p><p> When Walmart enters a town, it fatigues the market. When innovation occurs, fatiguing the market can create monopoly-ish behaviors in a similar way.</p><p> How many apps are people willing to download?</p><p> Not an endless number, as customers face <a href="https://techcrunch.com/2016/02/03/app-fatigue/"><u>app fatigue</u></a> . They do not care enough to download more applications.</p><p> McDonald&#39;s was able to get a significant number of downloads because it was an epistemic innovation and the first of its kind. The McDonald&#39;s app was <a href="https://www.cnet.com/tech/services-and-software/mcdonalds-dominated-food-apps-last-year/"><u>seeing downloads</u></a> that well exceeded their burger-based peers as well as all other food apps in general. This success is not just determined by the fact that they moved first, but because by moving first they have staked a claim in a new rivalrous market, the home page.</p><p> An epistemic innovation where the inventor moves first and is able to fatigue the market is an <strong>epistemic monopoly</strong> .</p><p> This is a new competitive advantage that McDonalds has introduced, and not in a dimension anyone was already competing in. Now, the app has cemented itself into a real moat. By competing on this new front, the restaurant has expanded what value they gather from this program, now able to collect more data than ever before to create the most personalized experience possible.</p><p> They have a localized monopoly, and behave monopoly-ish when considering the apps.</p><p> Instead of owning Main Street, they own screen time. Instead of competing in a 100-man race, they compete where no one else is even looking.</p><p> With innovation and market fatigue, <strong>good businesses can create epistemic monopolies</strong> .</p><br/><br/> <a href="https://www.lesswrong.com/posts/Gzh7tPZogAqjzDY6g/good-businesses-create-epistemic-monopolies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Gzh7tPZogAqjzDY6g/good-businesses-create-epistemic-monopolies<guid ispermalink="false"> Gzh7tPZogAqjzDY6g</guid><dc:creator><![CDATA[Logan Kieller]]></dc:creator><pubDate> Wed, 15 Nov 2023 14:04:53 GMT</pubDate> </item><item><title><![CDATA[A conceptual precursor to today's language machines [Shannon]]]></title><description><![CDATA[Published on November 15, 2023 1:50 PM GMT<br/><br/><p> <i>Cross-posted</i> <a href="A conceptual precursor to today's language machines [Shannon]"><i>from New Savanna</i></a> .</p><p> I&#39;m in the process of reading a fascinating article by Richard Hughes Gibson, <a href="https://hedgehogreview.com/issues/markets-and-the-good/articles/language-machinery">Language Machinery: Who will attend to the machines&#39; writing?</a> It seems that Claude Shannon conducted a simulation of a training session for a large language model (aka LLM) long before such things were a gleam in anyone&#39;s eye:</p><blockquote><p> The game begins when Claude pulls a book down from the shelf, concealing the title in the process. After selecting a passage at random, he challenges [his wife] Mary to guess its contents letter by letter. Since the text consists of modern printed English, the space between words will count as a twenty-seventh symbol in the set. If Mary fails to guess a letter correctly, Claude promises to supply the right one so that the game can continue. Her first guess, “T,” is spot-on, and she translates it into the full word “The” followed by a space. She misses the next two letters (“ro”), however, before filling in the ensuing eight slots (“oom_was_”). That rhythm of stumbles and runs will persist throughout the game. In some cases, a corrected mistake allows her to fill in the remainder of the word; elsewhere a few letters unlock a phrase. All in all, she guesses 89 of 129 possible letters correctly—69 percent accuracy.</p><p> In his 1951 paper “Prediction and Entropy of Printed English,”[1] Claude Shannon reported the results as follows, listing the target passage—clipped from Raymond Chandler&#39;s 1936 detective story “Pickup on Noon Street”—above his wife&#39;s guesses, indicating a correct guess with a bespoke system of dashes, underlining, and ellipses (which I&#39;ve simplified here):</p><blockquote><p> (1) THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG (2) ----ROO------NOT-V-----I------SM----OBL---- (1) READING LAMP ON THE DESK SHED GLOW ON (2) REA----------O------D----SHED-GLO--O-- (1) POLISHED WOOD BUT LESS ON THE SHABBY RED CARPET (2) PLS------O--BU--LS--O------SH-----RE--C-----</p></blockquote><p> What does this prove? The game may seem a perverse exercise in misreading (or even nonreading), but Shannon argued that the exercise was in fact not so outlandish. It illustrated, in the first place, that a proficient speaker of a language possesses an “enormous” but implicit knowledge of the statistics of that language. Shannon would have us see that we make similar calculations regularly in everyday life—such as when we “fill in missing or incorrect letters in proof-reading” or “complete an unfinished phrase in conversation.” As we speak, read, and write, we are regularly engaged in predication games.</p><p> But the game works, Shannon further observed, only because English itself is predictable—and so amenable to statistical modeling.</p></blockquote><p> After some elaboration and discussion:</p><blockquote><p> Shannon then proposes an illuminating thought experiment: Imagine that Mary has a truly identical twin (call her “Martha”). If we supply Martha with the “reduced text,” she should be able to recreate the entirety of Chandler&#39;s passage, since she possesses the same statistical knowledge of English as Mary. Martha would make Mary&#39;s guesses in reverse. Of course, Shannon admitted, there are no “mathematically identical twins” to be found, “but we do have mathematically identical computing machines.”9 Those machines could be given a model for making informed predictions about letters, words, maybe larger phrases and messages. In one fell swoop, Shannon had demonstrated that language use has a statistical side, that languages are, in turn, predictable, and that computers too can play the prediction game.</p></blockquote><p> Next thing you know, someone will demonstrate that the idea was there in Plato, and that he got it from watching some monkeys gesticulating wildly in the agora.</p><p> [1] Claude Shannon, “Prediction and Entropy of Printed English,” <i>Bell Systems Technical Journal</i> <strong>30</strong> , no. 1 (January 1951), 54.</p><br/><br/> <a href="https://www.lesswrong.com/posts/Y7WP47tL9zQwkLTqZ/a-conceptual-precursor-to-today-s-language-machines-shannon#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Y7WP47tL9zQwkLTqZ/a-conceptual-precursor-to-today-s-language-machines-shannon<guid ispermalink="false"> Y7WP47tL9zQwkLTqZ</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Wed, 15 Nov 2023 13:50:52 GMT</pubDate> </item><item><title><![CDATA[Reinforcement Via Giving People Cookies]]></title><description><![CDATA[Published on November 15, 2023 4:34 AM GMT<br/><br/><h2>我。</h2><p> <a href="https://www.lesswrong.com/posts/WJtq4DoyT9ovPyHjH/thinking-by-the-clock">《按时钟思考》</a>现在是我在 LessWrong 上写的最受欢迎的内容，所以这里是列表中的另一个条目，它对我的​​思维和操作方式产生了重大变化，这是我从《哈利·波特与魔法石》中学到的一些零星台词中学到的。理性方法。这个主题作为大家得到的后续内容是非常合适的，因为最后一个主题得到了如此多的支持。</p><blockquote><p> “我本打算给你更多的空间，”哈利·波特说，“只是我在读克里奇的享乐理论，以及如何训练你内心的鸽子，以及<strong>微小的即时积极和消极反馈如何秘密地控制我们实际做的大部分事情</strong>，我突然想到，你可能会回避我，因为看到我会让你想起一些负面的联想，我真的不想让这种情况再继续下去而不采取任何行动，所以我找到了一个韦斯莱双胞胎的一袋巧克力，每次你看到我时我都会给你一颗，作为一种积极的强化，如果你同意的话——”</p></blockquote><p>据我所知，这很简单。</p><p>我特此建议对您想要更多的东西立即提供积极的反馈，或者更简单地说，给人们饼干。根据我自己的经验，这确实有效，而且在很多层面上都有效。负强化有更多的方式在道德上误入歧途，所以我在这里并不是在争论如何使用硬币的这一面，但向人们提供正强化对我来说似乎是无可反对的。奖励你的朋友，奖励你的敌人，奖励你自己！</p><h2>二.</h2><p>让我们从奖励自己的最后一点开始。</p><p>每次锻炼时我都会给自己一种特别的享受。当我完成锻炼后，我就会得到款待。 （水果冰沙。）这种情况已经持续了很多年，以至于我的反应基本上是巴甫洛夫式的。当我系好跑鞋的鞋带时，我已经在考虑奖励了。有时我注意到内心有一种想要去跑步或举重的冲动，当我追踪这种冲动的来源时，通常是现在喝一杯冰沙听起来不错。我似乎异常擅长遵守自己的规则（大多数人说他们可以只做冰沙而不锻炼，并预测他们会这样做），但我至少有 n=1 的证据表明你可以经典地调节自己。但我们可以做得更小、更快。</p><p>我看到人们有时会做这样的<i>事情</i>，他们做了某件事，然后立即指出它的所有缺陷。看来通常是有某种焦虑的人，我不知道因果关系朝哪个方向发展。他们会用吉他弹奏一些新曲子，一旦完成，他们的脸就会皱起来，就像闻到了一些难闻的味道一样，他们指出他们在第三行错过了多少音符，然后房间里的其他人会说这样的话“哦，是的，我注意到了”，玩家会看起来对自己更加沮丧。其中一些似乎对学习过程很有用，但是那些会犯错误并嘲笑错误的人似乎更乐意弹更多的吉他。</p><p>当我试图集思广益或想出很多想法时，我会更注意到这一点。我会看着某人静静地坐几分钟，然后写下一个想法。看，我脑子里正在想的是，我为我提出的每一个想法赚取积分，即使是糟糕的想法。另一个想法，另一个观点。评估它是否是一个好主意是一个单独的过程，而且必须如此。这些积分可以非常快速地完全在精神上获得，并且仍然有<i>微小</i>的积极奖励。谁不喜欢积分呢！</p><blockquote><p> “赫敏，”哈利认真地说，同时又开始往红色天鹅绒袋子里掏，“当一个好主意没有实现时，不要惩罚自己。你必须经历<i>很多</i>有缺陷的想法。”找到一个可能有效的方法。如果当你想到一个有缺陷的想法时，你通过皱眉向大脑发送负面反馈，而不是意识到提出想法是你的大脑值得鼓励的良好行为，那么很快你就不会想到任何想法。”</p></blockquote><p>奖励你自己。如果你因为尝试一些事情而不是完美而惩罚自己，你就会学会不去尝试。</p><h2>三．</h2><p>你知道还有什么是快的吗？微笑。有一段时间我花了很多时间研究人类的面部表情。感觉就像每隔一周我就会看到一些新闻文章或另一篇充满希望的积极欢呼，如果我笑得更多，就很容易获得好心情。我自己的实验结果好坏参半。在没有复杂的科学测量仪器的情况下完成，这可以归结为尝试微笑，然后尝试弄清楚我是否感觉好一些。这实际上是在乞求安慰剂效应的出现，并践踏任何数据尝试。</p><p>我的成功来自于能够识别别人的微笑。在我知道如何注意到人们何时享受乐趣之前，我很难提高一系列的社交技能。在弄清楚如何判断（至少在大多数情况下）人们何时玩得很开心之后，有一段快速进步的时期，我会尝试讲一个笑话，然后意识到它不合适，或者主动提出帮助和帮助。迎接他的是真诚的、如释重负的微笑。开始获得快速积极反馈的能力是我成为一个友好、有趣的人而不是以前令人恼火的傲慢人的转折点。</p><p> （我仍然有点傲慢，但报告表明我对此不再那么恼火了。）</p><p>你甚至可以对其他人这样做。瞧，如果当有人主动帮助我移动桌子的另一端时我微笑，或者当我看到他们时微笑，他们可能会得到一小笔奖励。这很好。特别是对于朋友和合作伙伴来说，这种快速反馈是一个有用的小饼干，可以在我身边提供帮助。当然，我也不反对使用实际的cookie。大多数时候，当我在波士顿参加别人的理性主义聚会时，我会带上新鲜出炉的面包，并在谈话的早期给人们提供一片面包。如果我在约会应用程序上认识某人，我经常会在第一次或第二次约会之前尝试找出他们喜欢什么食物，然后安排在提供食物的餐厅见面，或者自己在家做饭并自带食物。如果我发现朋友或约会对象讨厌喝酒和大声喧哗，我为什么要带他们去酒吧？</p><p>奖励你的朋友。如果你惩罚他们和你一起出去玩，他们就会知道和你一起出去意味着喝苦酒并被噪音淹没。 （当然，如果他们喜欢酒精和喧闹的音乐，那么酒吧就成为更好的选择！）</p><h2>四．</h2><p>现在是我预计会引起争议的部分。我认为你应该奖励你的敌人。</p><p>要改变某人的想法，如果他们愿意与我交谈或至少阅读我写的东西，这是很有用的。如果与我交谈和阅读我写的东西令人不愉快，他们就不会那么做了。因此，如果谈话中出现了不必要的不​​愉快的事情，我很愿意解决这个问题。</p><p>对于绝大多数人来说，被骂粗鲁的名字或受到侮辱都是不愉快的。你是对的，他们是错的，这并不重要。由于被告知你错了对大多数人来说都是不愉快的，所以任何你想让别人相信他们错了的谈话都会以赤字开始。</p><p> “谢谢你抽出时间来和我谈论这件事！”通常是真的，除了几秒钟的呼吸之外什么也不用花，而且很高兴听到。不要仅在您要告诉某人他们错了时才使用它，他们可以并且将会对此形成联想。如果您希望某些口号与它们产生负面关联，请尽可能使用不同的词语。 （理性主义禁忌的技巧在这里很有用。）不要大喊大叫，也不要诉诸廉价的表情包来抱怨他们有多么愚蠢。</p><p>这只是避免负强化。当他们提出一个难题时，通过深入研究来奖励它。当他们提出一个结构良好的观点时——即使你不同意结论，也是一个在当地有效的论点，引用出处良好且不会断章取义的来源，即使你不喜欢它的设置方式——让他们提出我知道您认为这是一个结构良好的观点。让与您的互动愉快而愉快。</p><p>如果你的敌人不仅仅与你争论怎么办？如果他们威胁使用暴力，或者已经造成流血怎么办？</p><p>我并不是说要转过另一边脸。我建议你希望他们发现不向你开枪比向你开枪更令人愉快。 “放下武器投降，我们的战俘得到很好的照顾”在适当的情况下实际上是一个引人注目的说法。 “放下武器投降，我们的酷刑室储备充足，这床生锈的钉子正好适合你” <i>，事实并非如此。</i>这就是你鼓励他们战斗至死的方式。</p><p>奖励你的敌人。如果你惩罚他们不打扰你或善意地对待你，他们就不太可能退缩或做除攻击之外的任何事情。</p><h2>五、</h2><p>奖励的种类可以相当灵活。从刨碎的巧克力片（工作日在我的办公桌上放一小碗）到饼干（我自己偏爱免烤燕麦片）到微笑（哦，有很多选择，从害羞和犹豫到无所畏惧和自信）大胆）停火协议和人道主义援助（看，我告诉我的数学老师我很抱歉）重要的是尽快为你想要更多的事情提供积极的反馈。</p><p>这篇文章的行动号召是更多地思考你激励自己和周围人的哪些行为。尽量不要教人们做你不希望偶然发生的事情。我建议现在花点时间思考一下你想要更多什么，下次当你看到它发生时，有意识地鼓励更多。</p><p>另外，呃，LessWrong 在我所知道的所有社交网站中拥有最合理的点赞数和参与度指标，但我确实会关注人们有兴趣阅读的内容。解决这个平衡问题留给读者作为练习。</p><br/><br/> <a href="https://www.lesswrong.com/posts/wFGEtyHvn699dNshg/reinforcement-via-giving-people-cookies#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/wFGEtyHvn699dNshg/reinforcement-via-giving-people-cookies<guid ispermalink="false"> WFGEtyHvn699dNshg</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Wed, 15 Nov 2023 04:34:21 GMT</pubDate></item></channel></rss>