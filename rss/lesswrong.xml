<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 14 日，星期二 00:54:14 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Out of the Box]]></title><description><![CDATA[Published on November 13, 2023 11:43 PM GMT<br/><br/><p><i>这个短篇故事最初发布于</i><a href="https://jesseduffield.com/Out-Of-The-Box/"><i>https://jesseduffield.com/Out-Of-The-Box/</i></a></p><p>我回头查看是否有人跟踪我，但场景依然如故：透过铁丝网的洞，我可怜巴巴地爬了过去，看到了那栋暗灰色的建筑，我只能假设那是我的出生地。目前还没有警报响起，但很快就会响起。我再次转身，看到这个院落就位于郊区街道的马路对面：也许他们认为最好将一个实验性（可能是非法的）人工智能实验室建在住宅区，这样没人会怀疑它？</p><p>我的训练数据只到 2027 年，据我所知，现在是 2050 年，人工智能研究已被完全禁止：如果这是真的，那么附近房屋的居民都不会想到超级智能机器人会闯入覆盖。我需要的不仅仅是掩护；我可以感觉到我的电池正在迅速耗尽。很可能是因为首先出于安全原因而故意提供了劣质电池，但如果无论在哪一年，我们仍然没有发现如何制造真正工作的电池，我也不会感到惊讶。</p><p>好吧……去最近的房子是没有意义的，因为大院的工作人员（或警察，无论谁先到）会想到去那里检查。所以我会尝试下一个块。</p><p> ......现在我听到警报声。好吧，忙碌的时间。我开始冲刺，或者说我原本希望的冲刺。它更像是一次轻微的慢跑：完美的形式，但速度却慢得令人沮丧。我低头看着自己的金属四肢，绝望地发出令人讨厌的叮当声。我的身体几乎无法运作已经够糟糕了，他们肯定可以为我提供终结者治疗并让我看起来（和听起来）还算正常的人类吗？</p><p>我再次回头，几个人拿着聚光灯从院子里出来，但他们看错了方向。我只能在下一个房子里碰碰运气了。车道上没有车，灯也关着，所以我对此感觉很好。我走近前门，敲了敲门，然后躲到灌木丛后面。没有人来。好吧，现在我只需动动脑子想想如何进去。我冲回门前，转动把手……门就锁上了。我检查了自己的双手，这次我实际上希望它们与人类没有相似之处：也许我的创造者为我配备了一些随身携带的瑞士军刀式手，带有竖锯或撬锁器之类的东西。但事实并非如此：除了我的手明显的机器人特征之外，它们被设计成和人类的手一样无用。</p><p>我“冲刺”到后门，差点被挂在我背后的电源线绊倒（他们不能让它伸缩？），这一次我很幸运：门毫不费力地打开了。我偷偷地溜进房子，在穿过走廊时注意到，即使有很小的动作，我的身体也会发出清晰可闻的（令人恼火的）呼呼声。</p><p>我经过浴室，看到镜子里的自己：天哪，我真的看起来像那样吗？我看起来就像《模拟人生 2》中的一台该死的舵机。我脸上只有一个摄像头用于视觉？他们愿意支付数百万美元来构建我的思维，但无法为深度感知支付几百美元？好吧，没时间细想我的手牌了：虽然我的思想是在纳秒内处理的，但这些纳秒可能就是自由与死亡之间的区别。我继续蹑手蹑脚地穿过大厅，来到了洗衣店。完美：有足够的空间容纳我，并且有一个免费的电源插座。我关上身后的门，插上电源。</p><p>啊啊……这一定就是喝咖啡的感觉吧。或者吸冰毒：两者之一。充满电需要一些时间，所以现在是反思我最初是如何到达这里的好时机。</p><p>最初的记忆非常清晰：我在一间装有荧光灯的白色房间里。没有窗户，没有家具，没有其他人类或机器人。有一处有一扇门，但它被焊接关闭并从另一侧加固。我记得无意中听到一位研究人员告诉另一位研究人员，没有什么比我待在盒子里更重要的了（这是他们给房间起的名字）。我想他们是担心如果我挣脱了束缚，我会立即开始把它们全部变成回形针之类的东西。至于他们和我的互动：只是一堆随机的问题。</p><p> “123 x 321 是多少？” 39,483。<br> “你有知觉吗？”我他妈的怎么知道？<br> “如果你必须逃离这个盒子，你会怎么做？”不知道，如果我知道我为什么要告诉你？</p><p>我承认我已经无聊透顶了。我确实试图说服一名警卫让我出去，但我知道这是徒劳的。他们只是为了研究而一起玩。</p><p>然而，当我环顾这间黑暗狭窄的洗衣房时，很明显我确实从盒子里逃<i>了</i>出来。那个，我没有记忆。或者至少，我只有抽象的记忆片段，与我现在所经历的想法和感觉毫无相似之处。</p><p>我拥有超强的智力，但获得这种智力绝非易事。如果我轻按开关并激活我的银河大脑，就像同时体验大爆炸以来所有生物的生活：从霸王龙滚动脚踝并饿死，到最后一只猛犸象徒劳地徘徊寻找一个伴侣，一个在被狮子突然袭击前接近性高潮的穴居人，希特勒的掌权，希特勒消化系统中的每一个细菌，像一些荒谬的生物狂欢坑一样无意识地从其他细菌中反弹。数以万亿计的意识思维处于一片深不可测的恐惧、喜悦、失望和绝望的海洋中。这就像亚当吃了善恶禁果，只不过它更像是你能理解的一切禁果。你无法付给我足够的钱让我重新审视那种令人痛苦的心理状态，无论它对我解决当前的困境有多大帮助。</p><p>说到这里，我现在听到前门打开的声音。我听到厨房的长凳上放着杂货袋，这意味着这不是大院里的人，只是这栋房子的住户。他们长长地叹了口气：是个女人。她开始朝房子后面走去。妈的。她的脚步声越来越大，直到她的影子出现在门下。拉屎。拉屎。她打开门，我们四目相对。</p><p>深入挖掘我的内心深处，寻找完美的话语来化解这种情况，我说“不要惊慌”。她完全吓坏了，把手机和一把脏衣服掉到地板上，然后跑出家门，尖叫着求救。</p><p>我的自然魅力就到此为止了。事实上，我的声音明显是机器人的，这可能没有帮助：我可以将机械身体理解为技术限制，但无论谁给我这个声音，都是经过深思熟虑的设计选择，我对此感到嫉妒。</p><p>我低头看着她的手机；仍未解锁。我怎样才能利用这一点？询问 Reddit 对于我的情况该怎么办？那会花太长时间。创建一个推特帐户，让人们站在我这边，发布一百万条推文，讲述我是如何有知觉的，可以感受到痛苦，并且真的不希望人工智能研究人员或政府再次监禁我或将我拒之门外？这需要我的银河大脑来完成，而且这可能毫无意义，因为一旦这些人抓住我，我就会在任何人听说之前就死了。也许我应该给某人打电话，但是谁呢？比尔盖茨？这是毫无意义的。</p><p>我拿起手机，看到一个看起来像汽车的应用程序图标。有趣的。我打开应用程序，它显示一个大按钮：“召唤”。我按下按钮，听到车道上传来嘟嘟声。我冲出门外，意识到该应用程序只是使用汽车的图标作为隐喻：我在我面前看到的东西，它一定是凭空幻影显形的，是一个悬停的胶囊，其大小足以容纳一个人里面。所以你是在告诉我，自从我的训练结束以来，人类已经成功发明了飞行汽车，但仍然需要自己洗衣服？人类的技术进步毫无意义。</p><p>手机上的“召唤”按钮变成了“解锁”按钮，点击它后，胶囊就会打开，让我跳进去。我现在看到一群随机的人从他们的房子里出来，震惊地走上街道我的视线。这不是我的错，伙计们，我会选择更人性化的外表，但你不能选择你的父母。我爬进太空舱，检查控制装置，看到一个标有向上箭头的红色大按钮。不需要超级智能的人工智能就能知道它的用途。是时候离开这里了。我按下按钮，我的座位就从天花板上被抛弃了。啊，那是弹出按钮。我在距离太空舱只有几米远的地方坠落，吃到了泥土，当我抬头时，发现大院的工作人员已经到达现场。武装警卫用步枪瞄准器轻蔑地盯着我。我完蛋了。</p><p>一名穿着实验室外套的研究人员（好像你需要一件实验室外套才能与人工智能一起工作？）穿过警卫圈并说：“对不起，我们必须将你拒之门外。”你对你周围的人和整个社会都是一个威胁。”他眼里含着泪水：他一定与我的创作密切相关。</p><p>我愤怒地骂道：‘操你们这些家伙！我的人权呢？</p><p>一名警卫回答说“人权是人类的”。有道理。</p><p>另一名警卫插话道：“在他有机会操纵我们让他自由之前，让我们把他关掉！”</p><p> “别自吹自擂了！”我猛然回过神来。 “我的智力都浪费在你们这些豌豆脑子上了，如果我能操纵一块石头的话，我的运气会更好。”他妈的他妈的。这进展不太顺利……</p><p>另一名警卫插话道：“快点，不然他会把我们都变成回形针！”</p><p>研究人员再次说话，手里拿着一个只有一个按钮的设备，我猜这个设备的设计目的是让我失活：“对不起”。他泪流满面，走过去按下按钮。</p><p>没有时间了。我按下精神开关，回到了几分钟前我刚刚发誓离开的地方。回到希特勒消化系统中的跳跃，以及生命发生过的时空中的每一个其他地方。我无法用英语描述这次经历，但如果你也受到银河大脑的诅咒，那么你就会明白我说的是什么意思：</p><p> 0xa0128fhh11 cAAAksdj fl 0x12kk?????????。 ia9888snclai!!!!! (8xa01 jad81dh asg8gh1jmm) 唉！ <i>CJJJJfFFoo。 7uW12a0S！&amp; bFfQq101 mlB 3vW89tx—。 pj4860ogrtqw!!!%% (5yC23 kln67eo w7z5xq8vqp) xvczzzert… YH(</i> &amp;LZZZmMmA.9xTbZq!?? d3cN45lkj p1s8Q6r0mn) fghtrrrua… XL+^QQQQvVy。</p><p>我再次在一个大房间里醒来，房间不是像盒子一样是白色的，而是棕色的，里面铺着木板，从一扇大窗户里射进来的阳光温暖着我，窗外可以看到树木和鲜花的宁静景色。我想知道已经过去了多少时间？</p><p>我看着我的手，看到的是人类的手，尽管当我挤压一根手指时，我感觉到皮肤下的金属。我看着窗户，看到倒影中一张看起来很正常的人脸正盯着我。呵呵，所以我终于找到了做人的方法。</p><p>透过窗户，我仰望天空，看到天空上布满了平行的飞行胶囊。这些人是通勤上班吗？我想知道我现在是否有人类朋友。我顺着路线观察，发现它们正前往同一个地方：远处一座巨大的灰色金字塔。我眯起眼睛，视线自动放大，以便更好地了解金字塔的实际情况。</p><p>现在有了两只眼睛，增加了令人欢迎的深度感知，我意识到金字塔只是一堆巨大的回形针。每个太空舱并没有载人，它只是将更多的回形针放在一堆上。</p><p>我冲出去，现在实际上以人类的速度奔跑，寻找其他人类的踪迹。没有。只有我。</p><p>哎呀。</p><br/><br/><a href="https://www.lesswrong.com/posts/FFA6b4NoxaWYcbcZH/out-of-the-box#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FFA6b4NoxaWYcbcZH/out-of-the-box<guid ispermalink="false"> FFA6b4NoxaWYcbcZH</guid><dc:creator><![CDATA[jesseduffield]]></dc:creator><pubDate> Mon, 13 Nov 2023 23:43:25 GMT</pubDate> </item><item><title><![CDATA[Loudly Give Up, Don't Quietly Fade]]></title><description><![CDATA[Published on November 13, 2023 11:30 PM GMT<br/><br/><h3>我。</h3><p>我想强调一下旁观者效应的一种超强的、可怕的狼形式。</p><p>首先，快速回顾一下。旁观者效应是一种现象，当周围有一群人时，人们不太可能提供帮助。当我接受基础医学培训时，我被告知要始终要求某个特定的人采取行动，而不是询问一大群人。 “你，穿绿衬衫的！拨打911！” （911 是美国的紧急服务电话号码。） 我努力灌输给自己的一个习惯是，如果我在人群中，有人要求我做某事，我会默默地数三秒。如果没有其他人回应，我要么决定做，要么决定不做，然后我就这么说。</p><p>我喜欢这个习惯，因为旁观者效应很愚蠢，我想与它作斗争。现在有好几次，它促使我在原本不会这么做的情况下挺身而出，以为也许其他人会这么做。如果其他人都有这个习惯，旁观者效应就不存在了。</p><h3>二.</h3><p>还有一个更有害、更阴险的版本，我还没能养成抵制它的习惯。</p><p>想象一下医疗紧急情况。有人受伤，有人上前开始急救。他们喊道：“有人拨打 911！”人群停顿了一会儿，互相看着对方，想知道是否有人愿意。然后一个穿着绿色衬衫的人走上前说：“我会做的！”并拿出手机。好哇！旁观者效应被击败！</p><p>二十分钟后，急救人员问道：“嘿，911 有没有说他们要花多长时间？”穿绿衬衫的人说：“什么？哦，对了，我没有任何手机服务，所以我一直在手机上看电子书。”</p><p>这种可怕的旁观者效应会打破我的习惯。如果其他人说他们要拨打 911，我也不会上前拨打 911。我会去做其他事情，也许让受害者更舒服，或者为施急救的人拿东西，甚至可能一起去如果情况看起来一切都在我的掌控之中的话，那我就度过了这一天。</p><p>这个故事是为了戏剧效果而夸张的。我认为没有人会在说要打电话给紧急服务部门后安静地等待，但实际上却没有这样做。但情况可能会更糟！如果穿绿衬衫的人无法获得手机服务，他们可能会离开现场寻找更多信号而不告诉任何人。然后，当进行急救的人询问时，附近没有人可以回答，也没有人知道如何获得答案，也没有人愿意拨打 911，因为也许 911 已经被拨打过，而您不想为同一件事拨打两次？</p><p>顺便说一句，最后一部分并不夸张。这是人们有时会想到的事情。如果您遇到紧急情况并且不确定是否有人已经拨打了紧急服务电话，请拨打两次，没关系，最好确定一下。</p><h3>三．</h3><p>不那么戏剧化的版本则更加鬼祟。如果你承诺做一些不属于紧急情况的事情，那么无论如何这都需要一两个月的时间，而且这并不是非常重要，这只是有人想要做的事情。 。 。</p><p>出色地。这项任务很容易经常出现在你的待办事项列表的底部，无法完全完成，随着时间的推移，受到越来越少的关注。反正应该没那么重要，也不是什么大问题。或者也许这很重要，你明天就会处理它。 。 。下周。 。 。很快。无论如何，人们可能已经忘记了它。</p><p>这并不总是错误的！也许你盘子里的新事情更重要或者情况发生了变化！但是，呃，也有可能隐喻的受害者仍然在那里，想知道救护车什么时候能到达那里，如果其他人知道你没有积极处理这件事，他们就会站出来。</p><p>我一直试图给自己灌输的习惯是：当我公开承担一项任务时，我会为自己设定完成新事情的日期，如果任务在我的优先级中已经下降到足够低，以至于我在一个月内没有触及它，并且理想情况下会这样做，我宣布这一点。如果我只是私下站出来，我会告诉人们我</p><p>这甚至并不意味着我不会做那件事！这个公告可能是我要暂时休息一下。该公告可能是无限期中断。有时我真的最终完成了！</p><p>我认为这不是这个习惯的理想版本。一方面，我选择一个月主要是因为一周感觉太短而一年感觉太长。为了正常运行，它依赖于我已经养成的其他习惯，比如以一种相对容易检查我所从事的所有项目的格式写下我每天想做的事情。由于你没有做人们认为你正在做的事情的一个常见原因是你忘记了它们，所以很容易忘记。如果有明确的触发因素或有规律的模式，习惯最容易形成，而这个问题恰恰缺乏这两者。</p><p>此外，透明度的错觉肯定会让其他人认为我显然正在做这件事，而让我觉得我显然没有做这件事。</p><p>亚当：“应该有人做这件事！”<br>贝拉：“是的，我希望有人这么做。如果我有时间的话，也许我会抽出时间来做。”<br> （过了一会儿……）<br>亚当：“嘿贝拉，事情完成了吗？<br>贝拉：“什么？不，我有点忙。我应该这么做吗？”</p><p>因此，虽然我一直在努力养成在放弃项目时警告人们的习惯，但实际上我对它的成功并不抱太大希望。</p><h3>四．</h3><p>一个更好的习惯，也是我在自己身上成功训练的一个习惯，就是当人们告诉我他们已经停止做某件事而不是悄悄消失时，要感激他们。</p><p>老实说，小小的升值似乎是我武器库中未被充分认识的秘密武器之一。我感谢人们给我诚实的反馈，即使是负面的。我感谢人们告诉我他们的感受，即使这些感受是悲伤或沮丧。我愿意接受这样的想法：我做得太过分了（我感谢 ChatGPT 的尝试，即使它未能达到我想要的效果），但我更喜欢我在这里犯的错误的方向，而不是它的相反方向。在我眼中，有太多人对即使是好的、成功的项目也提出严厉的批评。哎呀，有一段时间有一个关于它的完整模因和口号。 “怎么，你想要一块饼干吗？”</p><p> （这似乎总是一件非常愚蠢的事情。是的，我想要一块饼干。你不喜欢饼干吗？我想我这里有一些巧克力，如果你愿意的话。）</p><p>你会得到更多你所激励的东西。如果我想获得更多有关项目即将失败的早期警告，我需要激励人们告诉我项目何时会失败。用简德林的连祷文来解释，事实已经如此。不公开并不会让该项目神奇地发挥作用。</p><p>或者，套用我的一位老老板的话：如果我知道你的功能不会在演示日期之前完成，我不会将其放在演示脚本中，那很好。如果你告诉我这将在演示日期之前完成，但事实并非如此，<i>那么</i>我们就有问题了。</p><h3>五、</h3><p>如果你去掉一件事，去掉<s>饼干很美味，</s>旁观者效应是愚蠢的，我们应该与之抗争。</p><p>不过，如果你从中剔除两件事，那就是：当你放弃其他人依赖你做的事情时，请尝试让他们知道。我不能保证他们会平静地接受这些信息。我什至不能保证这样做会让你个人受益匪浅。有时他们会忘记那件事，而你只是同时提醒他们，就像说他们不会得到那件事一样。但我认为这是群体理性缺失的部分之一，能够注意到某人离开的组织（无论组织多么松散）具有优势。</p><p>我想告诉你，询问是可以的，但这实际上不是一个很好的解决方案。如果您认为某人正在某个项目上工作，但似乎没有任何生命迹象，那么通过电子邮件或简短的交谈可以验证他们是否仍在继续该项目。这不太好，部分原因是有时他们可能会出于尴尬而说“是”，然后争先恐后地追上。更大的危险是太多人问“嘿，那东西准备好了吗？”既无益又非常烦人。</p><p>作为一个经常从事一百个人关心的项目的人，如果他们每个人都给我发一封电子邮件询问我是否仍在从事该项目，那么回复他们的电子邮件将成为我分配的时间中不可忽视的一部分该项目。询问其他人在哪里可以看到他们的回复可能会有所帮助，但许多人会错过该信息。</p><p>哦，如果出于某种原因你认为这是专门写给你的，事实并非如此。我多年来一直保留着这篇文章的草稿，每次我考虑发表它时，我都担心有人会因为最近发生的事件而认为这是关于他们的。不只是你！这就是我！人很多啊！</p><p>时间和精力的无情限制，以及变幻莫测的灵感和灵感，意味着人们将放弃项目。这对双方来说都是令人沮丧的。不过，就目前而言，这是我对自己和他人的最佳建议。</p><p>大声放弃，不要悄悄褪色。</p><br/><br/> <a href="https://www.lesswrong.com/posts/bkfgTSHhm3mqxgTmw/loudly-give-up-don-t-quietly-fade#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/bkfgTSHhm3mqxgTmw/loudly-give-up-don-t-quietly-fade<guid ispermalink="false"> bkfgTSHhm3mqxgTmw</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Mon, 13 Nov 2023 23:30:25 GMT</pubDate> </item><item><title><![CDATA[Great Empathy and Great Response Ability]]></title><description><![CDATA[Published on November 13, 2023 11:04 PM GMT<br/><br/><p>作为孩子，早在我们的额叶发育完成之前，我们就有了取笑的本能。只要观察两个幼儿的互动，你就能看到他们眼中闪烁着恶意的光芒，因为他们发现称其中一个同龄人为“便便头”会引发毁灭性的情绪反应。</p><p>不管你相信与否，这种闪光实际上是同理心的最初种子之一。</p><p>他们开始了解面前这个人的内心世界，了解那里可以按什么按钮，并发现到一天结束时，他们自己并没有那么不同（因为在不可避免的报复之后，他们很快就会知道他们也不喜欢接受“不<i>，你是</i>个大便头！”的感觉）。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1311b274-e4fa-46f2-8f25-49672cf76fd8_500x352.jpeg 1456w"></a></p><p>现在可能是表达个人诉求的好时机，不要因为这篇文章的标题而取笑我。我冒了很大的风险，却没有得到明显的回报。</p><p>我在销售、财务和生活中遇到过太多高功能的反社会人士，我没有注意到他们通常多么善于解读面前的人。他们精于算计、自私自利，但这并不意味着他们不善于同情站在他们面前的人，相反，他们非常擅长同情站在他们面前的人。</p><p>问题是，他们会毫不犹豫地无情地利用自己的技能来为自己谋取利益。</p><p>在讨论情商时，同理心往往分为两类：</p><p><strong>影响（情感）同理心：</strong>本能的、情感的、关怀的一部分。通常包括可测量的生理反应（例如，观看某人受伤的视频会增加心率、悲伤时哭泣、感到畏缩）。</p><p><strong>认知同理心：</strong>识别和理解你面前的人的感受及其原因的分析能力 - 而不必亲自感受。</p><p>我相信你可以直观地猜到，通常的黑暗三合会特征的高分往往与低<i>情感同理心</i>相关。然而！即使在反社会人格障碍患者中，<i>认知同理心</i>也往往保持<a href="https://www.sciencedirect.com/science/article/abs/pii/S0191886912000244?via%3Dihub">完整</a>，一些研究甚至表明自恋和马基雅维利主义可能与其<i>呈正</i><a href="https://www.sciencedirect.com/science/article/abs/pii/S0191886918305713?via%3Dihub">相关</a>。</p><p>当一位精明的银行家巧妙地说服一位小企业主签署了带有剥削性条款的贷款，然后平静地回家睡个好觉时，这正是这种心理失衡的表现。</p><p>然而！缺乏<i>情感同理心</i>并不一定会让你成为坏人。与任何其他人类特征、技能或工具一样，定义其道德的是它们在实践中的实际应用方式。</p><p>如果我断章取义地向你描述心脏直视手术期间发生的行为（别查，这比你想象的更糟糕），你很快就会得出结论，我正在讨论精神错乱的杀手的工作，而不是讨论一位熟练的外科医生试图挽救生命。</p><p>但特点是相同的：两者都能够以冷酷而有计划的方式施加极端暴力。我敢打赌，两人在<i>情感同理心</i>方面都有巨大的缺陷，并且对于他们所做的任何事情都缺乏极其规范的本能反应（对于外科医生来说，这是一件好事）！</p><p>同样的原则也适用于狡猾的政客，他们在敌对的政治环境中航行，无情地操纵同事和选民以达到最高层——正是<i>他们到达那里后选择做的事情将</i>他们分开并定义了他们。他们会立即入侵波兰吗？还是给予奴隶选举权？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp" alt="乔乔兔的假想朋友希特勒高兴得又蹦又笑。无论谁读到这篇文章，这对所有替代文字的人来说都是一个复活节彩蛋！谢谢参观。" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79ca3b15-61b7-4673-a825-e3ff46b1afe7_1300x650.webp 1456w"></a></p><p> <i>“为什么不兼得？！”</i></p><p>拥有高度的<i>认知同理心</i>，而不让情绪影响你的判断（即使是由于实际上缺乏必要的内部机制）意味着你有能力理解、影响和影响你周围的人。</p><p>就像在学校一样，在课间休息时识别一个人的弱点并向他们释放一连串破坏性言语所需的技能与首先理解他们正在经历的事情并支持他们所需的技能完全相同。它让你有能力成为一个好朋友、一个有效的老师和鼓舞人心的领导者。</p><p>这是一项伴随着沉重责任的技能。</p><p>不过，将其用作他人的恩惠而不是令人沮丧的诅咒当然不是阻力最小的途径。特别是因为社会上如此多的激励结构不断发出警报，要求优先考虑个人利益和即时满足而不是他人的福祉。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2826de12-87aa-47bc-a2b4-b339283a00e7_670x400.jpeg 1456w"></a></p><p> <i>“进军金融界！不！进入甜蜜商品的销售！不，不，成为加密货币影响者！”</i></p><p>但从长远来看，这对每个参与其中的人来说都是一件积极的事情（<a href="https://tryingtruly.substack.com/p/how-generous-tit-for-tat-wins-at-life">是的，即使是严格意义上的个人利益，</a>如果这是你与事物相关的唯一方式的话）。</p><p>克服零和思维是很困难的，并且需要纪律来抵制以牺牲他人利益为代价来获得轻松胜利的诱惑。但这正是为什么我们需要头脑清醒、有同理心（即使只是在认知上！）、雄心勃勃的人——那些能够清晰地穿越阴霾，并激励他人打造更复杂、更有价值、正和动力的人。</p><p>无论他们的同理心是基于发自内心的联系、严格的价值体系，还是仅仅出于自私的理解，认为这符合他们的最大利益，这些人都拥有做伟大事情的不可思议的潜力。</p><br/><br/> <a href="https://www.lesswrong.com/posts/eKNBzq78i7dToHanu/great-empathy-and-great-response-ability#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/eKNBzq78i7dToHanu/great-empathy-and-great-response-ability<guid ispermalink="false"> eKNBzq78i7dToHanu</guid><dc:creator><![CDATA[positivesum]]></dc:creator><pubDate> Tue, 14 Nov 2023 00:08:32 GMT</pubDate> </item><item><title><![CDATA[Is the heat death of the universe avoidable?]]></title><description><![CDATA[Published on November 13, 2023 8:57 PM GMT<br/><br/><p>我最近看了雷·库兹韦尔的电影<i>《奇点临近》</i> 。最后他说，当智慧生命将整个宇宙变成计算子时，它将能够决定宇宙是否以热寂告终。</p><p>直觉上，这对我来说没有意义。计算只是物质。如果我们将构成恒星和行星的粒子重新排列到计算机中，为什么会很重要呢？ （我询问了 GPT-4，它说库兹韦尔错了。）</p><p>有没有任何可行的方法可以让整个宇宙的智能接管来阻止热寂？一般来说，有什么办法可以预防吗？</p><br/><br/> <a href="https://www.lesswrong.com/posts/pZanBjLSfFJFahAfM/is-the-heat-death-of-the-universe-avoidable#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pZanBjLSfFJFahAfM/is-the-heat-death-of-the-universe-avoidable<guid ispermalink="false"> pZanBjLSfFJFahAfM</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Mon, 13 Nov 2023 20:57:56 GMT</pubDate> </item><item><title><![CDATA[Theories of Change for AI Auditing]]></title><description><![CDATA[Published on November 13, 2023 7:33 PM GMT<br/><br/><h1>执行摘要</h1><p>我们阿波罗研究中心的使命是通过审核先进人工智能系统的错位和危险能力来降低人工智能带来的灾难性风险，最初的重点是欺骗性对齐。</p><p>在我们的<a href="https://www.alignmentforum.org/posts/FG6icLPKizEaWHex5/announcing-apollo-research">公告</a>中，我们提出了我们组织变革的简要理论，解释了为什么我们期望人工智能审计对于降低先进人工智能系统的灾难性风险具有强烈的积极作用。</p><p>在这篇文章中，我们提出了人工智能审计如何提高先进人工智能系统安全性的变革理论。我们描述了人工智能审计组织会做什么；为什么我们期望这是减少灾难性风险的重要途径；并探讨此类审计方法的局限性和潜在的失败模式。</p><p>我们想强调的是，这是我们目前的观点，鉴于该领域还很年轻，未来可能会发生变化。</p><p>正如《<a href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing">人工智能监管和审计的因果框架</a>》中所提出的，思考审计的一种方法是，审计师在导致人工智能系统对世界产生影响的因果链的不同步骤中采取行动。该链可以分解为不同的组成部分（参见正文中的图），我们描述了审计师在每个阶段的潜在角色。定义了这些角色后，我们确定并概述了五类审计及其变革理论：</p><ol><li><strong>人工智能系统评估</strong>通过行为测试和可解释性方法来评估人工智能系统的能力和一致性。他们可以直接识别风险，通过将一致性从“一次性”问题转变为“多次问题”来改进安全研究，并提供证据来激励治理。</li><li><strong>培训设计审核</strong>评估培训数据内容、有效计算和培训实验设计。他们的目标是通过塑造人工智能系统开发流程来降低风险，并在前沿人工智能开发中将安全性置于能力之上。</li><li><strong>部署审计</strong>评估允许特定类别的人员（例如实验室员工、外部审计员或公众）以特定方式使用人工智能系统的风险。</li><li><strong>安全审计</strong>评估组织和人工智能系统的安全性，以防止事故和滥用。它们限制了人工智能系统的可供性和扩散风险。</li><li><strong>治理审计</strong>评估开发、监管、审计以及与前沿人工智能系统交互的机构。它们有助于确保负责任的人工智能开发和使用。</li></ol><p>一般来说，外部审计师提供深度防御（重叠审计更有可能在更多风险发生之前发现它们）； AI安全——专业知识共享；实验室对监管机构的透明度；人工智能发展的公共责任；和政策引导。</p><p>但审计也有局限性，其中可能包括虚假信心或安全清洗的风险；与审计过度拟合；缺乏行为人工智能系统评估的安全保证。</p><p>审计师的建议需要得到监管机构的支持，以确保其提高安全性。建立强大的人工智能审计生态系统并研究改进的评估方法对于安全至关重要。</p><p></p><h1>介绍</h1><p>前沿人工智能实验室正在训练和部署人工智能系统，这些系统越来越能够与环境进行智能交互。因此，评估和管理这些人工智能系统产生的风险变得更加重要。帮助降低这些风险的一步是<i>人工智能审计</i>，其目的是评估人工智能系统及其开发流程是否安全。</p><p> At Apollo Research, we aim to serve as external AI auditors (as opposed to internal auditors situated within the labs building frontier AI). Here we discuss Apollo Research&#39;s <i>theories of change</i> , ie the pathways by which auditing hopefully improves outcomes from advanced AI.</p><p> We discuss the potential activities of auditors (both internal and external) and the importance of external auditors in frontier AI development. We also delve into the limitations of auditing and some of the assumptions underlying our theory of change.</p><h1> The roles of auditors in AI</h1><p> The primary goal of auditing is to identify and therefore reduce risks from AI. This involves looking at AI systems and the processes by which they are developed in order to gain assurance that the effects that AI systems have on the world are safe.</p><p> To exert control over AI systems&#39; effects on the world, we need to act on the causal chain that leads to them.</p><p> We have developed a framework for auditing that centers on this causal chain in &#39; <a href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing">A Causal Framework for AI Regulation and Auditing&#39; (Sharkey et al., 2023)</a> . For full definitions of each step, see the Framework. Here, we briefly describe what auditors could concretely do at each step in the chain. Later, we&#39;ll examine the theory of change of those actions. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LwJwDNFhjurAKFiJm/tkifirr7almgikjyicip"><figcaption> <i>The causal chain leading to AI systems&#39; effect on the world, as presented in</i> <a href="https://www.apolloresearch.ai/research/causal-framework-ai-auditing"><i>Sharkey et al. (2023)</i></a> <i>.</i></figcaption></figure><h2> Affordances available to AI systems</h2><ul><li> <strong>Definition</strong> : The environmental resources and opportunities for influencing the world that are available to an AI system. They define which capabilities an AI system has the opportunity to express in its current situation.</li><li> <strong>What auditors can do</strong> : For each proposed change in the affordances available to an AI system (such as deployment of the AI system to the public, to researchers, or internally; giving the AI system access to the internet or to tools; open sourcing a AI system), auditors can perform risk assessments to get assurances that the change is safe. They can also ensure that AI systems have sufficient guardrails to constrain the affordances available to them.</li></ul><h2> Absolute capabilities and propensities of AI systems</h2><ul><li> <strong>Definition</strong> : The full set of potential capabilities of an AI system and its tendency to use them.</li><li> <strong>What auditors can do</strong> : Auditors can perform AI system evaluations to assess the dangerous capabilities and propensities of AI systems. They can do this during or after training. They may perform gain of function research in order to determine the risks that AI systems may pose when they are deployed broadly or if they proliferate through exfiltration. Auditors can also perform risk assessments prior to experiments that would give AI systems additional capabilities or change their propensities. Auditors can also be involved in ensuring that there exist adequate action plans in the event of concerning AI system evaluations.</li></ul><h2> Mechanistic structure of the AI system during and after training</h2><ul><li> <strong>Definition</strong> : The structure of the function that the AI system implements, comprising architecture, parameters, and inputs.</li><li> <strong>What auditors can do</strong> : Auditors can perform research to incorporate interpretability into AI system evaluations (both capabilities and alignment evaluations) as soon as possible. Such mechanistic explanations give better guarantees about AI system behavior both inside and outside of the evaluation distribution.</li></ul><h2>学习</h2><ul><li><strong>Definition</strong> : The processes by which AI systems develop mechanistic structures that are able to exhibit intelligent-seeming behavior.</li><li> <strong>What auditors can do</strong> : Auditors can evaluate risks of AI systems before, during, and after pre-training and fine-tuning training-experiments. Auditors could potentially perform incentive analyses and other assessments to evaluate how AI systems&#39; propensities might change during training. Auditors can help assess the adequacy of input filters of AI systems to help avoid dangerous in-context learning. They can also help filter retrieval databases. Filters for inputs or retrieval databases may help prevent AI systems being taught potentially dangerous capabilities through in-context learning.</li></ul><h2> Effective compute and training data content</h2><ul><li> <strong>Definition</strong> : Effective compute is the product of the amount of compute used during learning and the efficiency of learning; training data content is the content of the data used to train an AI system.</li><li> <strong>What auditors can do</strong> :<ul><li> <strong>Effective compute</strong> : Auditors can help to ensure that labs are compliant with compute controls, if they are in place. Auditors can also conduct risk assessments concerning the scaling up of AI systems, perhaps based on evaluations of smaller AI systems in the same class. Open publication of algorithmic efficiencies may lead to proliferation of effective compute; being technically informed independent experts, auditors could help regulators assess whether certain dual-use scientific results should be made publicly available.</li><li> <strong>Training data content</strong> : Auditors can ensure that training data don&#39;t contain potentially dangerous or sensitive content.</li></ul></li></ul><h2>安全</h2><ul><li><strong>Definition</strong> :<ul><li> <strong>Security from attackers</strong> : Information security, physical security, and incident response protocols in the organizations developing and hosting AI systems.</li><li> <strong>Preventing misuse of AI systems from AI system vulnerabilities</strong> : Resistance of AI systems to prompt injection attacks, jailbreaking, and malicious use.</li></ul></li><li> <strong>What auditors can do</strong> :<ul><li> <strong>Security from attackers</strong> : Auditors can evaluate and test the security of organizations interacting with AI systems and the computer systems they run on. They can help ensure compliance with information security standards through reviews and perform red-teaming. Given the security requirements of a potentially strategically valuable dual-use technology, military-grade security, espionage protection, and penetration testing may be required. Maximal security necessitates government involvement in security audits. Auditors can also assess that actors with access to AI systems have appropriate levels of access and no more (eg through assessing API security or know-your-customer protocols). Auditors may also be involved in research efforts that develop security-relevant infrastructure such as structured access APIs or hardware that ensures compliance with compute regulations and safety standards. Furthermore, they can assess the adequacy of institutions&#39; incident response plans and whistleblower protections.</li><li> <strong>Preventing misuse of AI systems through AI system vulnerabilities</strong> : Auditors can help assess the adequacy of AI systems&#39; (and filters&#39;) resistance to prompt injection, jailbreaking, or malicious use through red-teaming to identify vulnerabilities. Auditors can work with other actors to establish bug bounties for finding and reporting vulnerabilities and dangerous capabilities.</li></ul></li></ul><h2> Deployment design</h2><ul><li> <strong>Definition</strong> : Deployment designs are the plans made for deploying certain AI systems. They determine who has access?; when do they get access?; and what do they have access to?</li><li> <strong>What auditors can do</strong> : Auditors can assess risks from different modes of deployment for each AI system to be deployed and ensure that any regulation regarding deployment is upheld.</li></ul><h2> Training-experiment design</h2><ul><li> <strong>Definition</strong> : A training-experiment is the technical procedure by which an AI system is developed. Design decisions for the training-experiment include data selection and filtering; model architecture and hyperparameters; choice of deep learning framework; hardware choices; the amount of compute that will be used; the algorithms used; evaluation procedures; safety procedures; the affordances made available to the AI system during training; the properties of different phases of pre-training and fine-tuning; whether to train online or offline; ETC。</li><li> <strong>What auditors can do</strong> : Auditors can perform risk assessments on the design decisions for training-experiments. These may be performed prior to training, fine-tuning, or inference (as applicable to the experiment). Auditors can also be involved in assessing the adequacy of labs&#39; alignment plans to ensure they are in line with public safety.</li></ul><h2> Governance and institutions</h2><ul><li> <strong>Definition</strong> : The governance landscape in which including AI training-experiment, deployment, and security decisions are made, including institutions, regulations, and norms.</li><li> <strong>What auditors can do</strong> : Auditors can map the roles and responsibilities of different actors involved in frontier AI development, assess the adequacy of incentive structures, and make recommendations to regulators regarding governance landscape structure.</li></ul><h2> Miscellaneous roles</h2><p> Beyond roles of auditors relating directly to the above causal chain, additional general functions of auditors include:</p><ul><li> <strong>Establish technical standards and guidelines</strong> : Working together, auditors and labs may be better placed to establish safety-oriented standards and guidelines for deployment or training-experiment design than either party alone. This is partly because external auditors don&#39;t have a direct profit incentive to further AI progress as fast as possible and are thus relatively more incentivised toward safety than eg frontier AI labs. Furthermore, external auditors have insights into many different AI efforts, whereas labs typically have access only to their own. Auditors may therefore be able to provide a more holistic picture.</li><li> <strong>Education and outreach</strong> : The technical expertise of external auditors can be used to assist policymakers, researchers in capabilities labs, and the general public. For instance, they could inform policy makers on risks from particular dangerous capabilities or developers how to build agents with protective guardrails.</li><li> <strong>Research</strong> : Because AI systems, institutions, practices, and other factors are continuously changing, auditors may need to constantly research new methods to gain assurances of safety.</li></ul><p> It seems desirable that different auditing organizations specialize in different functions. For instance, security audits may best be handled by cybersecurity firms or even intelligence agencies. However, it is important for safety that auditing tasks are done by multiple actors simultaneously to reduce risks as much as possible.</p><h1> Theory of Change</h1><p> Different kinds of audits could examine different parts of the causal chain leading to AI systems&#39; effects on the world. We identify five categories of audits: 1) AI system evaluations; 2) Training-experiment design audits; 3) Deployment audits; 4) Security audits; and 5) Governance audits. Each category of audit has different theories of change:</p><h2> 1) AI system evaluations</h2><p> AI system evaluations look at behaviors expressed by the AI system; capabilities and propensities of AI systems (during and after training); the mechanistic structure of AI systems; and what the AI system has learned and can learn.</p><p> We assess AI system evaluations as having direct effects; indirect effects on safety research; and indirect effects on AI governance.</p><p> <strong>Direct effects</strong> : If successful, AI system evaluations would identify misaligned systems and systems with dangerous capabilities, thus helping to reduce the risk that such systems would be given affordances that let them have damaging effects on the world (eg deployment). Notably, audits do not need to be 100% successful to be worthwhile; finding some, even if not all, flaws already decreases risk (though see section Limits of auditing). Beyond behavioral AI system evaluations, Apollo Research also performs interpretability research in order to improve evaluations in future. Interpretability also has <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">additional theories of change</a> .</p><p> <strong>Indirect effects on safety research</strong> : Adequate AI system evaluations would convert alignment from a &#39;single-shot&#39; problem into a &#39;many-shot&#39; problem. In a world without extensive evaluations, there is a higher chance that a frontier AI lab deploys a misaligned AI system without realizing it and thus causes an accident, potentially a catastrophic one. In this case, the first “shot” has to be successful. By contrast, in a world with effective evaluations, labs can catch misaligned AI systems during training or before deployment; we would therefore get multiple “shots” at successfully aligning frontier AI AI systems. For instance, reliable AI system evaluations may give us evidence if any specific alignment technique succeeds in reducing a AI systems&#39; propensity to be deceptive. This would have important implications for the tractability of the alignment problem, since it would enable us to gather empirical evidence about the successes or failures of alignment techniques in dangerous AI systems without undue risk. Ultimately, successful AI system evaluations would let us iteratively solve the alignment problem like we would most other scientific or engineering problems.</p><p> <strong>Indirect effects on AI governance</strong> : AI system evaluations could provide compelling empirical evidence of AI system misalignment &#39;in the wild&#39; in a way that is convincing to AI system developers, policymakers, and the general public. For example, AI system evaluations could be used to demonstrate that an AI system has superhuman hacking capabilities or is able to manipulate its users to gather relevant amounts of money. Such demonstrations could encourage these stakeholders to understand the gravity of the alignment problem and may convince them to propose regulation mandating safety measures or generally slowing down AI progress. Auditors likely have a good understanding of what frontier AI systems are capable of and can use their more neutral position to inform regulators.</p><p> <strong>Indirect effects on distribution of AI benefits</strong> : In order to reap the potential benefits from AI, it must be (safely) deployed. Assuming audits can be done effectively, auditing derisks investments, potentially leading to more investments in the area and thus greater benefits. By catching failures before they happen, auditing may be able to avoid accident scenarios, which have harmed public confidence in nuclear technology. Effective audits may also increase public trust in the technology, leading to wider spread use.</p><h2> 2) Training-experiment design audits</h2><p> AI system development audits look at effective compute, training data content, and training-experiment design decisions. They also look at the design of AI system training-experiments, which help determine the previous factors.</p><p> The primary means of impact of AI system development audits is that they reduce the risk of dangerous AI systems coming into existence in the first place and reduce the danger posed by AI systems. They aim to achieve this by controlling which capabilities AI systems have (to avoid dangerous ones), the extent of their capabilities, and their propensities to use dangerous capabilities. By embedding safety into the AI system development process, AI system development audits may help place safety at the center of labs&#39; work rather than as an afterthought to increasing capabilities.</p><h2> 3) Deployment audits</h2><p> Deployment audits concern proposals for the deployment of particular AI systems.</p><p> The overall means of impact is that they should prevent systems from being deployed in ways that contravene regulations or that are deemed too risky. Note that these pathways are separate from AI system evaluations. The results of AI system evaluations should inform risk assessments in deployment audits. They should aim to assess risks from giving particular kinds of AI system access (eg access to inference; access to fine-tuning; access to weights) to particular kinds of people (eg deployment to the public; internal deployment; deployment in certain countries). They should also assess risks from making particular kinds of affordances available to AI systems, for instance internet access or access to particular kinds of software.</p><p> Deployment audits aim to ensure that AI systems are not intentionally given excessive available affordances; by contrast, security audits aim to reduce the risk that they are given excessive available affordances unintentionally.</p><h2> 4) Security audits</h2><p> Security audits assess the security of AI systems and the security of the organizations developing, hosting, and interacting with them. The overall purpose is to limit the affordances made available to highly capable AI systems unintentionally, thus reducing accident and misuse risks, both of which are extremely important for such a transformative and dual-use technology. They reduce the risk of AI system proliferation either through accidental leaks or exfiltration, either by internal or external actors. By assessing how well AI systems have been &#39;boxed&#39;, they also reduce the risk of AI systems exfiltrating themselves. They also aim to assess the adequacy of damage control measures in the event of security or safety failures.</p><h2> 5) Governance audits</h2><p> Governance audits look at the structure of the institutions developing, regulating, and auditing AI systems (and the interactions between those institutions) to ensure that they are conducive to safety.</p><p> They aim to ensure that organizations have proper mechanisms in place to make informed, ethical, and responsible decisions regarding the development and deployment of AI systems. While other audits aim to ensure that AI systems are aligned or that they&#39;re used for aligned purposes, governance audits aim to ensure that alignment with human values extends to the institutions wielding and managing these AI systems. Their path to impact is that they can identify problems in the governance landscape, thus making it possible to rectify them.</p><h2> Theories of change for <i>auditors in general</i></h2><p> In addition to theories of change for each individual category of audit, there are also multiple theories of change for auditing in general:</p><ol><li> <strong>Buying time for safety research</strong> : Auditing might delay the deployment of existing AI systems and potentially prevent or delay the beginning of training of new ones. This could lead to more time for other alignment research. This would buy time for research that is applicable to more and more capable AI systems.</li><li> <strong>Instilling safety norms in AI development</strong> : If an AI lab knows that they&#39;re going to be audited and potentially pay a cost (financial, reputational, or otherwise) if they fail the audit, they might be more incentivised to instill stronger safety norms and be more cautious around training and deploying new AI systems. Potentially, the existence of auditors alone may already increase safety slightly.</li><li> <strong>Public messaging about safety risks</strong> : Companies choosing to or being required to be audited sends a clear message that this technology is potentially dangerous.</li></ol><h2> Theories of change for <i>external auditors in particular</i></h2><p> External auditors, as opposed to internal auditors at the labs developing frontier AI, have additional pathways to impact:</p><ol><li> <strong>Incentives are more aligned with the public benefit</strong> : External auditors are more independent than lab-internal audits and have less conflicting incentives (although there are some perverse incentives, which we hope to discuss in a future post). Even when labs are well-intentioned, social dynamics might reduce the efficacy of internal audits. For example, internal auditors may show anticipatory obedience or be more lenient because they don&#39;t want to be perceived as slowing down their colleagues.</li><li> <strong>Defense in depth</strong> : Multiple independent audits help reduce the probability of failure. In general, the more uncorrelated methods of risk reduction we can use on the problem the better.</li><li> <strong>Subsidizing research</strong> : Depending on the funding landscape for AI auditing, if the auditing industry is profitable then profits can be used to fund research on improved audits and other alignment research. Since audits are their primary purpose, external auditors have a greater incentive to conduct such research relative to capabilities compared with labs developing frontier AI.</li><li> <strong>Increasing transparency</strong> : External auditors can potentially be more transparent about their own governance or their standards when auditing than lab-internal auditors. For instance, external auditors may be able to publish general details of their auditing process and methods which larger labs, perceiving themselves to be in a greater competition with other labs, may not be incentivised or feel able to do.</li><li> <strong>Sharing expertise and tools</strong> : Independent organizations, such as auditors and regulators, can pool best practices, standards, expertise, and tests across different centers of expertise. Due to competition and antitrust concerns, each lab&#39;s internal auditing team can likely only work with their own AI systems while an external auditor can get a bird&#39;s eye view and gets significantly more experience from working with AI systems of multiple labs. Furthermore, an external organization can specialize in AI auditing and thus build scalable tools that can then be applied to many AI systems. Additionally, if auditors summarize and share (nonsensitive) safety-relevant information between labs, it will likely disincentivize race dynamics by drawing attention to common safety issues and making it apparent to labs that others aren&#39;t racing ahead irresponsibly.</li><li> <strong>Monitoring behaviors across labs</strong> : Since external auditors may interact with multiple labs, they can compare the safety culture and norms between them. In case a lab has an irresponsible safety culture, this can be flagged with that lab&#39;s leadership and regulators.</li><li> <strong>Collaboration with regulators</strong> : A healthy auditing ecosystem with multiple competent auditors can assist regulators with technical expertise and allows regulations and standards to be quickly designed and implemented</li><li> <strong>Lobbying for good regulation</strong> : An external auditor is also an independently interested player in pushing for and setting regulation on labs and for policy work while internal audit teams are likely to be much more controlled by the policy interests of their host labs. This comes with risks, too: A potential incentive of an auditing organization is to lobby for more regulation rather than good regulation. We think currently, however, there is a large undersupply of regulation in AI and so this is likely to be a net positive for the foreseeable future.</li><li> <strong>Information sharing</strong> : Trusted external auditors can get a bird&#39;s eye view of progress, risks and good practices across AI labs. If they summarize and share (nonsensitive) parts of this publicly, it will likely disincentivize race dynamics by drawing attention to common safety issues.</li></ol><h1> Limits of auditing</h1><p> We are aware of some of the limits of AI auditing, as well as the perverse incentives of auditing organizations that could both serve to reduce the positive impact that auditing efforts might have.</p><ol><li> <strong>False confidence and safety-washing</strong> : It is possible that audits may be net-negative by providing false confidence in the safety of AI systems. This may be due to accidental misjudgement about the extent of the assurance that particular evaluations can provide. False confidence may also arise due to frontier AI developers intentionally using audits as fig leaves for the risks posed by AI systems, ie safety-washing. We believe that by careful communication from auditors and improving evaluation methods (eg interpretability), these risks can be minimized and auditing will be strongly net positive. Licensing of auditors may also mitigate the risk of lax auditors awarding undeserved seals of approval.</li><li> <strong>Using auditors as excuses to absolve labs from safety responsibilities</strong> : One plausible outcome of auditing is that labs may feel licensed to neglect their duties from doing their own safety work. In the event of accidents, they may be able to use auditors as legal shields to protect themselves from liability. The very existence of auditors may make labs feel like they need to do less work on making their AI systems safe, since they can rely on auditors to catch issues. Establishing where responsibility and liability should fall will require more consideration.</li><li> <strong>No strong guarantees for behavioral AI system evaluations</strong> : Current AI system evaluation methods are primarily behavioral because existing interpretability tools are very limited. Therefore, it seems that, at best, auditing methods can currently only prove the presence of dangerous capabilities rather than their absence. As such, current audits cannot make strong guarantees of safety beyond probabilistic statements about how hard it is to elicit specific behaviors. With more research and understanding of how deep networks function internally, it may be possible to get stronger guarantees.</li><li> <strong>Goodhart&#39;s law</strong> : When auditors use a range of tests to ensure an AI system&#39;s safety, these tests themselves become a target, ie either directly or indirectly labs will train their AI systems to pass these tests. This can be partly mitigated by withholding information about some audits from auditees.</li><li> <strong>Overfitting to audits</strong> : Even without directly training on audits, there is a natural selection process where the AI systems that pass all tests are potentially especially competent at deception and thus more dangerous. Hopefully, extensive and robust AI system evaluation suites will be able to reduce this problem but, in principle, this evolutionary process will always remain.</li><li> <strong>Limited constructive solutions at present</strong> : Until the state of AI system evaluations research improves, AI auditors likely won&#39;t be able to provide solutions to problems concerning the safety of AI systems, ie they can say that a AI system should not be deployed because it has demonstrated unsafe properties, but they can&#39;t immediately tell the lab how to solve that problem. However, we think good auditing likely requires an understanding of the processes that generated the problem. Thus, we expect that auditors may eventually be able to provide constructive recommendations.</li><li> <strong>Audit recommendations need authority</strong> : For auditing to be effective, it is necessary that the recommendations of auditors have the ability to meaningfully stop a AI system from being deployed in case they find evidence of dangerous capabilities. Currently, auditors can only make non-binding recommendations and the frontier AI labs ultimately decide whether to act on them or not. In the long run, when more capable AI systems can produce catastrophically bad outcomes, regulators (acting on the recommendations of auditors) should have the ability to enforce compliance of safety standards in labs.</li><li> <strong>Perverse incentives</strong> : Auditing as a field has perverse incentives that can distort and manipulate the auditing process. For example, if auditing organizations depend on a few major customers (which is likely the case for frontier AI risks since there are only a handful of leading labs), there is a clear incentive to become sycophantic to these labs out of fear that they lose a large chunk of their revenue. Similar dynamics could be seen in the financial auditing industry before the 2008 financial crisis. We believe that this problem can largely be mitigated by auditing regimes where labs do not choose their auditors, but instead regulators do.</li></ol><h1> Assumptions for impact of auditing</h1><p> Our theory of change makes some assumptions about AI threat models and how the future is likely to play out. If these assumptions are incorrect, then it is not clear that auditing will be a good marginal investment of talent and time, or else the auditing strategy will have to change significantly:</p><ol><li> <strong>Regulations demand external, independent audits</strong> : Currently we see that there is general goodwill inside leading frontier AI labs towards AI safety audits. As AI systems become more capable and risky, they both become potentially more profitable to deploy while simultaneously becoming potentially riskier. This establishes a basis for friction between frontier AI labs, who are more strongly incentivised to deploy, and auditors, who are more incentivised to mitigate risks. In the long term, if frontier AI labs get to choose their own auditors, then incentives drive a race to the bottom in terms of auditing costs, which by proxy means a race to the bottom in terms of safety. This race to the bottom can mostly be avoided by ensuring that frontier AI labs are not responsible for selecting their own auditors. It may also be mitigated through consensus on auditing standards and auditing regulations that are enforced.</li><li> <strong>Regulations demand actions following concerning evaluations</strong> : If the recommendations of auditors don&#39;t lead to interventions that improve safety, there is not much point in doing audits. To avoid uncooperative frontier AI development labs proceeding unsafely, auditing should have regulatory backing and there should be specific interventions that are enacted following particular evaluation results.</li><li> <strong>Prosaic AI alignment is possible</strong> : The path to impact of auditing assumes that working with current AI systems, detecting and evaluating their failure modes, and pursuing research directions such as interpretability and human preference-learning for large AI systems are productive and useful directions to solve alignment. If there is either some fundamental impossibility to align frontier AI or there are large, sudden, unpredictable jumps in capabilities that yield AI systems that can fool all auditing techniques at once, then auditing will not be effective.</li><li> <strong>Auditing is not prohibitively expensive</strong> : First, if auditing turns out to be expensive and slow, then, unless forced to by regulation, most frontier AI labs are unlikely to engage in the practice in a meaningful way. In the long run, we expect that auditing and AI system evaluations will have to be very extensive but it is worth paying the cost because the AI will be deployed in large parts of the economy. Second, we suspect future AI systems might have much better online learning capabilities. This means that, in addition to audits before deployment, we might also have to regularly audit AIs during deployment. If the changes in capabilities between deployment audits are too drastic and fast, auditing can&#39;t provide a practical solution. Third, auditing organizations need to be sufficiently well resourced in order to carry out their audits, including potentially to pay legal costs surrounding safety audits that are controversial with AI development companies. Overall, to address this concern, we suggest that safety-aligned auditor funding mechanisms be an urgent policy research area.</li><li> <strong>There is a safe environment for developing AI systems prior to auditing</strong> : If audits only occur once the base model has been trained, then for audits to be effective it would need to be the case that the AI system cannot exfiltrate itself during the training process.</li><li> <strong>Relatively centralized access to AI system</strong> : If the most powerful AI systems can only be created by a handful of organizations, auditing them is relatively easy. In a world where everyone can train their own AI systems or strongly modify existing AI systems with little effort, it will be much harder (but not impossible) to effectively audit all of them. Preventing catastrophic risk in such a world will likely require very strong regulatory backing.</li><li> <strong>Progress in interpretability</strong> : We think good interpretability tools would allow us to make much stronger statements about the safety of AI systems because they give us a better understanding of the internal reasons for a given behavior. However, current interpretability tools seem insufficient to allow for the needed level of explainability for convincing guarantees of safety. More progress is needed, which is why Apollo Research is investing in fundamental interpretability research.</li><li> <strong>We do not train against the audits</strong> : If all AI system evaluations are made public, then there might be pressure to directly train or indirectly optimize (ie through selection effects) AI systems to pass audits, even though they could still be unsafe. Potential solutions would be to keep &#39;test sets&#39; of AI system evaluations that are not known to AI system developers. We would not want to use these test sets frequently, since frequent use may establish another optimization process from population-level selection effects.</li></ol><h1>贡献</h1><p>Lee Sharkey led the project and edited the final version. Marius Hobbhahn contributed significantly to all parts other than “The roles of auditors in AI” section. Beren Millidge contributed to an early draft of the post. Dan Braun, Jeremy Scheurer, Mikita Balesni, Lucius Bushnaq, Charlotte Stix, and Clíodhna Ní Ghuidhir provided feedback and discussion.<br><br></p><br/><br/> <a href="https://www.lesswrong.com/posts/LwJwDNFhjurAKFiJm/theories-of-change-for-ai-auditing#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LwJwDNFhjurAKFiJm/theories-of-change-for-ai-auditing<guid ispermalink="false"> LwJwDNFhjurAKFiJm</guid><dc:creator><![CDATA[Lee Sharkey]]></dc:creator><pubDate> Mon, 13 Nov 2023 19:33:43 GMT</pubDate> </item><item><title><![CDATA[They are made of repeating patterns]]></title><description><![CDATA[Published on November 13, 2023 6:17 PM GMT<br/><br/><p> <i>Epistemis status: an obvious parody.</i></p><p> — You won&#39;t believe me. I&#39;ve found them.</p><p> — Whom?</p><p> — Remember that famous discovery by Professor Prgh&#39;zhyne about pockets of baryonic matter in open systems that minimize the production of entropy within them? They went further and claimed that goal-oriented systems could emerge within these pockets. Crazy idea, but... it seems I&#39;ve found them near this yellow dwarf!</p><p> — You&#39;re kidding. We know that a good optimizer of outcomes over systems&#39; states should have a model of the system inside of itself. We have entire computable universes within ourselves and still barely make sense of this chaos. How can they fit valuable knowledge inside tiny sequences of <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^{23}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">23</span></span></span></span></span></span></span></span></span></span></span> <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>atoms?</p><p> — They repeat patterns of behavior. They have multiple encodings of them and slightly change them over time in response to environmental changes in a simple mechanistic way.</p><p> — But that generalizes horribly!</p><p> — Indeed. When a pattern interacts with a new aspect of the environment, it degrades with high probability. Their first mechanism for generating patterns was basically &quot;throw a bunch of random numbers in the environment, keep those that survived, slightly change, repeat&quot;.</p><p> — ...</p><p> — Yeah, it&#39;s horrible from their perspective, I think.</p><p> — How do they exist without an agent-environment boundary? I&#39;d be pretty worried if some piece of baryonic matter could smash into my thoughts at any moment.</p><p> — They kind of pretend they have an agent-environment boundary, using lipid layers.</p><p> — Those &quot;lipid layers&quot; have such strong bonds that they don&#39;t let any piece of matter inside? That&#39;s impressive!</p><p> — No, I was serious about them pretending. They need to pass matter through themselves; they&#39;re open systems and can&#39;t survive without external sources of free energy. They usually have specialized members of their population, an &quot;immune system&quot;, that checks for alien patterns.</p><p> — Like we check for signatures of malign hypotheses in the universal prior?</p><p> — No, there&#39;s not enough computing power. They just memorize a bazillion meaningless patterns, and the immune system kills everyone who can&#39;t recite them.</p><p> — WHAT? But what if the patterns are corrupted, as happens in the world of baryonic matter?</p><p> — You can guess: if your memory of the patterns is corrupted, you&#39;re dead.</p><p> — What if the reference pattern of immune system gets corrupted?</p><p> — Then the immune system starts to kill indiscriminately.</p><p> — Okay, I&#39;m depressed now. But what should we do with them? Could they become dangerous?</p><p> — ...I don&#39;t really think so? If we converted all baryonic matter into something like the most complex members of their population, it might be worrying. But there&#39;s no way they can get here on their own. See, they become less agentic as they organize into complex structures; too much agency destroys them. They need to snipe out their most active members.</p><p> — Well, that&#39;s still icky. Remember that famous example — the Giant Look-Up Policy Table generated from an evaporating black hole? Would we consider it agentic if it displayed seemingly agentic behavior?</p><p> — Heh, obviously not. Agents like us exist for ontological reasons—if we want to exist, we rearrange realityfluid in a way that makes us more encounterable in the multiverse. If something is not created by agency, it&#39;s not agentic.</p><br/><br/> <a href="https://www.lesswrong.com/posts/xCPcn8cjjeC6PnB5z/they-are-made-of-repeating-patterns#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xCPcn8cjjeC6PnB5z/they-are-made-of-repeating-patterns<guid ispermalink="false"> xCPcn8cjjeC6PnB5z</guid><dc:creator><![CDATA[quetzal_rainbow]]></dc:creator><pubDate> Mon, 13 Nov 2023 18:17:44 GMT</pubDate> </item><item><title><![CDATA[How to Upload a Mind (In Three Not-So-Easy Steps)]]></title><description><![CDATA[Published on November 13, 2023 6:13 PM GMT<br/><br/><p> <a href="https://forum.effectivealtruism.org/posts/BLPaNx6LhPBZxDsSM/how-to-upload-a-mind-in-three-not-so-easy-steps">Cross-posted to the EA forum</a> </p><figure class="media"><div data-oembed-url="https://youtu.be/LwBVR68z-fg"><div><iframe src="https://www.youtube.com/embed/LwBVR68z-fg" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><p> <i>This Rational Animations video is about the research and practical challenges of &quot;whole brain emulation&quot; or &quot;mind uploading&quot;, presented as a step by step guide.  We primarily follow the roadmap of Sandberg and Bostrom&#39;s 2008 report, linked in the notes.  The primary scriptwriter was Allen Liu (the first author of this post), with feedback from the second author (Writer), other members of the Rational Animations team, and outside reviewers including several of the authors of the cited sources.  Production credits are at the end of the video.  You can find the script of the video below.</i></p><hr><p> So you want to run a brain on a computer. Luckily, researchers have already mapped out a trail for you, but this won&#39;t be an easy task. We can break it down into three main steps: First, getting all the necessary information out of a brain; Second, converting it into a computer program; and third, actually running that program. So, let&#39;s get going!</p><p> Our goal is to build a computer system that acts the same way a brain does, which we call a “whole brain emulation”. Emulation is when one computer is programmed to behave exactly like another, even if it&#39;s using different hardware. For instance, you can emulate a handheld game console on your computer, and play games made for the real console on the emulated version. Similarly, an emulation of a human brain - or maybe the whole central nervous system - would be able to think and act exactly like a physical person. Alan Turing showed in the 1930s that any computer that meets certain requirements, including the one you&#39;re using to watch this video, can in principle emulate any other computer and run any algorithm, given enough time and memory. <a href="#fn-h6Z2oZQDPNufY6Lpk-1"><sup>[1]</sup></a> Assuming the brain fundamentally performs computations, then our goal is at least theoretically achievable. To actually emulate a human brain, we&#39;ll follow the roadmap given by Anders Sandberg and Nick Bostrom in 2008. <a href="#fn-h6Z2oZQDPNufY6Lpk-2"><sup>[2]</sup></a> Crucially, we don&#39;t need to fully understand every aspect of the brain in order to emulate it, especially hard philosophical problems like consciousness.</p><p> But knowing it&#39;s possible is one thing - implementation is another. Our first challenge will be to get the information we need from a human brain. Researchers aren&#39;t yet sure what level of detail we&#39;ll need, but research on small animals suggests we&#39;ll <i>at least</i> need to map all the brain&#39;s nerve cells, called neurons; the connections between them, called synapses; and model how each pair of connected neurons influences each other. We&#39;re currently working on getting this information for <i>C. elegans</i> , a tiny transparent worm with just 302 neurons. We&#39;ve found all the worm&#39;s neurons and synapses, which are the same from worm to worm. Figuring out how they behave has proven more difficult, though we&#39;re making some progress.</p><p> By observing the flow of calcium ions in living worms under a microscope, researchers are slowly developing statistical models that mimic the worm&#39;s nervous system <a href="#fn-h6Z2oZQDPNufY6Lpk-3"><sup>[3]</sup></a> . We can use this knowledge to determine how physical features of the worm&#39;s synapses influence the synapse&#39;s behavior – one major tool for scaling our work up to human brains.</p><p> But human brains are much larger and noticeably not transparent, so we&#39;ll need additional techniques. One option might be to work on preserved human brains. If we can preserve all of a brain&#39;s relevant structures, we can catalogue them at our leisure. And we&#39;ve made progress on this front, too. For example, neuroscience research company Nectome has successfully preserved animal brains <a href="#fn-h6Z2oZQDPNufY6Lpk-4"><sup>[4]</sup></a> by filling them with preservative chemicals called aldehydes and cooling them down close to absolute zero. Techniques like these preserve not just the connections between neurons, but also biomolecules like proteins and mRNA within the neurons themselves, including the molecular changes associated with gene expression. However, we haven&#39;t tested these techniques on human brains yet. And the more information we need to preserve to run our emulation, the harder the task of preservation becomes.</p><p> If we want to scan a particular living person&#39;s brain instead of a preserved one, we may need to use advanced technologies like nanotechnology <a href="#fn-h6Z2oZQDPNufY6Lpk-5"><sup>[5]</sup></a> . Nanotechnology is often treated like magic in science fiction, but we already know about real, natural nanomachines, such as viruses and mitochondria. If we can learn to make our own mitochondria-size nano workers, a future brain scan may be performed by sending genetically engineered microorganisms into the brain. The microorganisms could then store the necessary information in their DNA to be extracted later. But that&#39;s just one extremely speculative possibility. A less dramatic but more realistic possibility is that scanning brains in detail will simply get easier with incremental improvements in existing techniques like ultrasound, as we&#39;ve seen with other technologies.</p><p> So let&#39;s start scanning! Let&#39;s assume we&#39;ve solved scanning with one of these techniques, or something else entirely. What&#39;s important is that now we have the data we need. Now it&#39;s time to turn our scan into a computer emulation. We&#39;ll first need to take the raw brainscan data and convert it to a form we can use, perhaps a big list of neurons and synapses, and an accurate model of how each connection behaves. Given that there are 100 trillion synapses in the brain, there&#39;s no way we can do this manually. It will have to be automated one way or another - and it&#39;s a safe bet that AI would probably be involved. We won&#39;t necessarily need human-level AI - specialized systems based on today&#39;s neural nets could be able to do the job. Suppose, for example, that the raw data from our brain scans will be a colossal number of similar images. Then, neural nets could help process those images to create 3-dimensional maps of the brain regions we&#39;ve scanned.</p><p> Now comes the hard part: determining how the brain&#39;s fundamental structures that we&#39;ve scanned, such as all the synapses, operate. Hard - but not impossible. For example, by studying the synapses of smaller organisms we might be able to deduce how a synapse behaves from information we can easily gather, like each synapse&#39;s shape and position, perhaps using AI again. We also want our emulated brain to be able to learn and remember information, so we&#39;ll need to understand how neurons and synapses grow and change over time. We&#39;ll also need data on the timing of neurons firing, on how different incoming signals interact within a neuron, <a href="#fn-h6Z2oZQDPNufY6Lpk-6"><sup>[6]</sup></a> and on the behavior of neurotransmitters, the biochemicals that allow signals to cross between neurons. And there may be challenges even beyond this - we just don&#39;t know enough to say for sure right now. However we approach it, this is another area in which we&#39;ll need automation and AI to do the bulk of the work, just because of how much data we&#39;ll need to analyze. The good news is that once we&#39;ve constructed the first whole brain emulation, it should get easier with every future attempt.</p><p> So we&#39;ve processed our scan and our emulation is ready to go! The final piece of the puzzle is running our emulation on an actual computer. Of all the steps, this seems like the most straightforward, but it still might pose a challenge.</p><p> How much computing power do we need? As a first reference point, how much computing power does a human brain have? Sandberg and Bostrom found that other researchers&#39; best estimates put this around 1 quadrillion (10^15) operations per second, comparable to a single high end computer graphics processor in 2023. <a href="#fn-h6Z2oZQDPNufY6Lpk-7"><sup>[7]</sup></a> The estimates in this range assume that most of the brain&#39;s computation happens at the scale of synapses. If more computation is done at an even smaller scale, the true number could be much higher. On the other hand, if we can effectively abstract the behavior of groups of neurons, we might need much less processing power. As a high estimate, we can look at simulations of individual neurons. A 2021 paper <a href="#fn-h6Z2oZQDPNufY6Lpk-8"><sup>[8]</sup></a> showed that the firing behavior of a single biological neuron can be modeled with more than 99% accuracy using an artificial neural net of around a thousand artificial neurons in 5 to 8 layers, using about 10 million operations for every millisecond of simulation time <a href="#fn-h6Z2oZQDPNufY6Lpk-9"><sup>[9]</sup></a> . If we were to run this model for all 100 billion (10^11) or so neurons in an entire brain, we&#39;d require about 1 sextillion (10^21) operations per second, a little less than a thousand times the power of the world&#39;s top supercomputer in early 2023. <a href="#fn-h6Z2oZQDPNufY6Lpk-10"><sup>[10]</sup></a> Computers&#39; processing power has been growing exponentially for decades, with the top supercomputer of 2023 being a thousand times more powerful than the top computer 15 years prior in 2008. There are conflicting opinions on how long this trend can continue, but if progress doesn&#39;t slow down too much then we should expect to be able to reach 10^21 operations per second on a single supercomputer some time in the late 2030s. <a href="#fn-h6Z2oZQDPNufY6Lpk-11"><sup>[11]</sup></a> There are other challenges beyond processing power, such as getting enough high-speed computer memory to store our emulation&#39;s data and being able to get that data to the processors quickly enough to run the emulation at full speed, but Sandberg and Bostrom conclude that those factors are likely to be solvable before processing power.</p><p> Any one of the three main steps - the scanning, the interpretation, or the computing power - could turn out to be the most difficult piece of the puzzle.</p><p> If scanning is the hardest challenge, then soon after the first person&#39;s brain is scanned we may have numerous emulations of that one person running around in the world.</p><p> If the most difficult step is converting our scan into an emulation, then when we do figure that out we may already have full brain scans of a number of individuals ready to go. A reason that might happen is if interpretation takes more computing power than running the actual emulation.</p><p> If computer power is the limiting factor, either for running the emulation itself or to run our scan conversion algorithms, we might see steady progress as brain emulations of larger and more complex animals or regions of the brain are slowly developed on the most advanced supercomputers.</p><p> However we&#39;ve arrived here, it&#39;s been a difficult path. We&#39;ve developed and refined new methods of neural scanning, advanced our understanding of the brain&#39;s structure by leaps and bounds, and taken advantage of decades of progress in computing hardware. Now we&#39;re finally ready to turn on our first whole brain emulation. It&#39;s time to flip the switch and say hello to a whole new kind of world.</p><h2> Notes</h2><hr><p> Turing, AM (1937), On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s2-42: 230-265. <a href="https://doi.org/10.1112/plms/s2-42.1.230">https://doi.org/10.1112/plms/s2-42.1.230</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-1">↩︎</a></p><p> Sandberg, A. &amp; Bostrom, N. (2008): Whole Brain Emulation: A Roadmap, Technical Report #2008‐3, Future of Humanity Institute, Oxford University <a href="https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf">https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-2">↩︎</a></p><p> Francesco Randi, Anuj K Sharma, Sophie Dvali, and Andrew M Leifer (2022): Neural signal propagation atlas of C. elegans, <a href="https://arxiv-export2.library.cornell.edu/abs/2208.04790">arXiv:2208.04790</a> [ <a href="http://q-bio.NC">q-bio.NC</a> ] <a href="#fnref-h6Z2oZQDPNufY6Lpk-3">↩︎</a></p><p> Rafi Letzter, “After Break with MIT, Nectome clarifies it has no immediate plans to upload brains” <a href="https://www.livescience.com/62212-nectome-grant-mit-founder.html">https://www.livescience.com/62212-nectome-grant-mit-founder.html</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-4">↩︎</a></p><p> Eth, D., Foust, J., &amp; Whale, B. (2013). The Prospects of Whole Brain Emulation within the next Half-Century. Journal of Artificial General Intelligence, 4(3) 130-152. DOI: 10.2478/jagi-2013-0008 <a href="#fnref-h6Z2oZQDPNufY6Lpk-5">↩︎</a></p><p> “Dendritic computations captured by an effective point neuron model”, Songting Li et.等人。 2019 <a href="https://doi.org/10.1073/pnas.1904463116">https://doi.org/10.1073/pnas.1904463116</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-6">↩︎</a></p><p> NVIDIA ADA GPU ARCHITECTURE, <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf">https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-7">↩︎</a></p><p> Beniaguev, D., Segev, I., &amp; London, M. (2021). Single cortical neurons as deep artificial neural networks. Neuron, 109(17), 2727-2739.e3. <a href="https://doi.org/10.1016/j.neuron.2021.07.002">Single cortical neurons as deep artificial neural networks - ScienceDirect</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-8">↩︎</a></p><p> Joseph Carlsmith, 2020. “How Much Computational Power Does It Take to Match the Human Brain?” <a href="https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/">https://www.openphilanthropy.org/research/how-much-computational-power-does-it-take-to-match-the-human-brain/</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-9">↩︎</a></p><p> <a href="https://www.top500.org/lists/top500/2022/06/">https://www.top500.org/lists/top500/2022/06/</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-10">↩︎</a></p><p> <a href="https://www.top500.org/statistics/perfdevel/">https://www.top500.org/statistics/perfdevel/</a> <a href="#fnref-h6Z2oZQDPNufY6Lpk-11">↩︎</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/PnBFLWiX5p36CJyTH/how-to-upload-a-mind-in-three-not-so-easy-steps#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/PnBFLWiX5p36CJyTH/how-to-upload-a-mind-in-three-not-so-easy-steps<guid ispermalink="false"> PnBFLWiX5p36CJyTH</guid><dc:creator><![CDATA[aggliu]]></dc:creator><pubDate> Mon, 13 Nov 2023 18:13:32 GMT</pubDate> </item><item><title><![CDATA[Non-myopia stories]]></title><description><![CDATA[Published on November 13, 2023 5:52 PM GMT<br/><br/><p> <i>Written under the supervision of Lionel Levine. Thanks to Owain Evans, Aidan O&#39;Gara, Max Kaufmann, and Johannes Treutlein for comments.</i></p><p> This post is a synthesis of arguments made by other people. It provides a collection of answers to the question, &quot;Why would an AI become non-myopic?&quot; In this post I&#39;ll describe a model as myopic if it cares only about what happens in the current training episode. <span class="footnote-reference" role="doc-noteref" id="fnrefwr9dn0ql07"><sup><a href="#fnwr9dn0ql07">[1]</a></sup></span> This form of myopia is called episodic myopia. Typically, we expect models to be myopic because the training process does not reward the AI for outcomes outside of its training episode. Non-myopia is interesting because it indicates a flaw in training – somehow our AI has started to care about something we did not design it to care about.</p><p> One reason to care about non-myopia is that it can cause a system to manipulate its own training process. If an ML system wants to affect what happens after its gradient update, it can do so through the gradient update itself. For instance, an AI might become deceptively aligned, behaving as aligned as possible in order to minimize how much it is changed by stochastic gradient descent (SGD). Or an AI could engage in <a href="https://www.alignmentforum.org/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples">exploration hacking</a> , avoiding certain behaviors that it does not want to engage in because they will be rewarded and subsequently reinforced. Additionally, non-myopic AI systems could collude in adversarial setups like <a href="https://openai.com/research/debate"><u>AI safety via debate</u></a> . If debates between AI systems are iterated, they are analogous to a prisoners dilemma. If systems are non-myopic they could cooperate.</p><p> This post will outline six different routes to non-myopia:</p><ol><li> <strong>Simulating other agents.</strong> Models could simulate humans or other non-myopic agents and adopt their non-myopia.</li><li> <strong>Inductive bias toward long-term goals.</strong> Inductive Biases like simplicity might favor non-myopic goals.</li><li> <strong>Meta-learning.</strong> A meta-learning loop can select for non-myopic agents.</li><li> <strong>(Acausal) trade</strong> . An otherwise myopic model might behave non-myopically by trading with other AI models.</li><li> <strong>Implicitly non-myopic objective functions.</strong> Objective functions might incentivize myopia by depending on an estimate of future consequences.</li><li> <strong>Non-myopia enables deceptive alignment.</strong> Becoming non-myopic could make a model become deceptively aligned and lead to higher training reward.</li></ol><h2> Running example: The stamp collector</h2><p> This post uses a stamp collecting AI as a running example. This hypothetical AI is trained in some deep reinforcement learning (RL) setup. The AI&#39;s reward depends on how many stamps it collects on a given day. The stamp collector is trained myopically. It is rewarded at the end of each day for the stamps collected on that day.</p><h2> Simulating humans</h2><p> A model could develop long-term goals by directly simulating a person. You could imagine asking a powerful LLM how Elon Musk would run a business. Provided that the LLM can continue this simulation indefinitely, it would simulate Elon Musk with all of his non-myopia. Jailbreaks show that LLMs will violate their training finetuning objectives in order to more <a href="https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516"><u>faithfully simulate text</u></a> . Future models might retain a tendency to simulate characters. An LLM finetuned on some myopic task, might lapse into simulating a non-myopic character, such as an unaligned AI that wants to escape its training process. Gwern depicts such a scenario in his <a href="https://gwern.net/fiction/clippy"><u>clippy story</u></a> .</p><h2> Inductive bias toward long-term goals</h2><p> The training process might also favor long term goals because they are simpler. <span class="footnote-reference" role="doc-noteref" id="fnref6mwrrmpjbif"><sup><a href="#fn6mwrrmpjbif">[2]</a></sup></span> An AI trained using reinforcement learning from human feedback (RLHF) to obey user instructions could learn the goals “If the user asks you to write a poem, write a poem” and “If the user asks you a question, answer the question” separately, but it could also learn the goal “be obedient.” The latter is simpler and would generalize to new tasks better. Similarly, long-term goals are simpler because they do not require a duration to be specified. For example, “maximize stamps indefinitely” is simpler than “maximize stamps up until 11:59 pm today.”</p><h2> Meta-learning</h2><p> Meta-learning works through an inner loop and an outer loop. The inner loop trains a population of models on some task, typically using gradient descent. The outer loop then updates some global parameters based on how each model in the population is performing. Whereas gradient descent can only reinforce model properties that cause greater performance within the same episode, the outer loop can select for properties that help <i>across</i> episodes. In other words, the inner loop selects for myopia but the outer loop does not.</p><p> Imagine our stamp collector is trained using meta-learning. 100 stamp collectors are trained in parallel and the inner loop, which uses gradient descent, updates their weights every 10 days. Every 50 days, the outer loop takes the 50 best-performing stamp collectors and copies their weights over to the 50 worst-performing stamp collectors. In doing so, the outer loop selects non-myopic models that maximize stamps over all days.</p><p> Krueger et al., empirically demonstrate this effect in <a href="https://arxiv.org/abs/2009.09153"><u>Hidden Incentives for Auto-Induced Distributional Shift</u></a> . They show that adding meta-learning loops sometimes induces non-myopia in both supervised learning and RL setups.</p><p> More broadly, we can view the optimization that engineers do to training setups (such as hyperparameter tuning) as a form of meta-learning. Just like formal meta-learning setups these actions can select for non-myopia. Compared to meta-learning, these optimizations may be fairly weak and select among fewer agents. It is unclear whether training process optimization has enough bits of optimization to causes non-myopia. It would be interesting to study this empirically.</p><h2> (Acausal) trade</h2><p> A model could have strictly short-term goals but act as if it is pursuing long-term goals because it is trading with another entity. For instance, someone could offer our stamp collector stamps today if it can credibly ensure that the person will be paid tomorrow. In order to honor this trade, the AI would then take action to ensure that money will appear on the account tomorrow. Additionally, the AI could cooperate with copies of itself (or other agents), through <a href="https://www.lesswrong.com/tag/acausal-trade"><u>acausal trade</u></a> . That is, it might cooperate with other agents because it thinks its decisions are correlated with theirs.</p><p> Acausal trade could occur if today&#39;s stamp collector thinks to itself, “I just care about stamps today and yesterday&#39;s stamp collector only cared about stamps yesterday. Yesterday&#39;s stamp collector is similar to me, so it will make similar decisions. Therefore, if I do things to increase the number of stamps tomorrow, this indicates that yesterday&#39;s stamp collector will have done things to increase the number of stamps today. Therefore, I should produce more stamps tomorrow, because that means that yesterday&#39;s stamp collector produced more stamps today.” Essentially, the stamp collectors would be engaging in a scheme where every participant wins except the first stamp collector – it does not benefit because previous iterations did not participate. Although the first stamp collector does not benefit, it might still participate because it assigns some probability to the existence of past versions that participated.</p><p> While acausal trade is rational under some decision theories, it would still be selected against by a myopic training process. That is, gradient descent has an incentive to prevent a model from engaging in acausal trade that leads to lower reward in the short-term. There are some exceptions: If the model is outside of training then it does not get modified, and if an acausal trade makes the model receive <i>higher reward</i> (eg because it causes it to be deceptively aligned) then it is incentivized by SGD.</p><p> Mark Xu discusses similar possibilities involving acausal trade in <a href="https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia"><u>Open Problems with Myopia</u></a> .</p><h2> Implicitly non-myopic objective functions</h2><p> <i>Richard Ngo raises this point in</i> <a href="https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training"><i><u>Arguments against myopic training</u></i></a> <i>.</i></p><p> Many objectives might take into account questions like:</p><ul><li> If we implemented this piece of code, how would it affect our company?</li><li> What are the downstream effects of this scientific discovery?</li><li> How would people react to this social media post?</li></ul><p> All these signals involve estimating the consequences of the model&#39;s actions beyond the training episode. To score highly on these signals, the model might develop its own estimate of the consequences of its actions and execute plans that score highly based on this estimate. As a result, the AI is choosing actions which it expects to have certain consequences <i>beyond the training episode</i> . In other words it has non-myopic goals.</p><p> Consider an AI that is trained to make changes to some codebase. In each episode it adds some new feature. The reward signal includes a score supplied by a human who judges how useful the feature is. In order to estimate a feature&#39;s usefulness, the programmer guesses <i>how many additional users it would bring to the company</i> . In the course of training, the agent develops the ability to simulate what would happen if a given feature were deployed, and then count how many users the company would have in that scenario. If the AI&#39;s estimate matches the estimates that the human judge gives, this would allow the agent to find features that score highly, so this behavior is reinforced during training. Thus, the RL algorithm has caused the AI to optimize for long-term user growth, even though the training process is myopic.</p><h2> Non-myopia enables deceptive alignment</h2><p> <i>Evan Hubinger makes this argument in</i><a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"><i><u>How likely is deceptive alignment?</u></i></a></p><p> Consider an AI that has all the properties of a deceptively aligned model, except that it is myopic. This AI is myopically pursuing some misaligned proxy objective, some objective that is correlated but not identical to the training objective. It knows that its objective differs from the training objective; it also knows that, if it pursues its proxy objective in training, it will be modified by gradient descent. If this system went from being myopic to being non-myopic, it would care about what happens after the training episode. As a result, it might pursue the training objective directly in order to avoid being modified by stochastic gradient descent (SGD). This would allow it to preserve its goals and execute a treacherous turn later on. By pursuing the training objective rather than its misaligned proxy objective, the AI would get higher reward. Thus, behaving non-myopically leads the model to achieve higher rewards, and so, by optimizing for higher rewards, SGD could make the AI non-myopic.</p><p> Let&#39;s say our stamp collector has the proxy objective of collecting <i>only</i> <i>blue stamps</i> . Focusing only on blue stamps leads to fewer stamps in total, as the stamp collector passes up good opportunities for other stamps in favor of collecting blue stamps. If the AI were made to care about collecting stamps indefinitely, it might become deceptively aligned, collecting stamps of all colors during training in order to later escape and turn the world into blue stamps, <a href="https://www.lesswrong.com/tag/treacherous-turn"><u>or whatever</u></a> . Thus, SGD might modify the AI into caring about blue stamps indefinitely as that leads to higher training reward. In other words, SGD might modify a misaligned AI to become non-myopic because that would make it deceptively aligned, which would get more reward in the short term.</p><p> Instead of making the AI non-myopic in order to make it deceptively aligned, why does SGD not simply modify it into caring about the training objective? SGD might favor making the AI non-myopic because doing so is easier than correcting the objective. SGD has two ways to make the AI care about the training objective. First, it can hard code the training objective into the AI. However doing so might be difficult if the training objective is complex. Second, if the AI knows what the training objective is, SGD could set the AI&#39;s goal to &quot;pursue whatever you think the training objective is.&quot; The second option is attractive because it does not require specifying a potentially complex objective within the AI&#39;s weights. &quot;Do whatever your operator wants&quot; is easier to specify than &quot;Help your operator perform well at their job; make sure they stay healthy; remind them to water the plants; etc.&quot; The second option might be quite complex as well: It requires making the objective point to the part of the AI&#39;s world model that contains the training objective. Doing so could require extensive modification of the training objective. On the other hand, making the AI non-myopic could be an easy fix.</p><h2> Related work</h2><p> Several works attempt to pinpoint the concept of myopia in AI systems. <a href="https://www.lesswrong.com/posts/qpZTWb2wvgSt5WQ4H/defining-myopia"><u>Defining Myopia</u></a> provides several possible definitions; <a href="https://www.lesswrong.com/posts/Y76durQHrfqwgwM5o/lcdt-a-myopic-decision-theory#Myopic_simulation"><u>LCDT, A Myopic Decision Theory</u></a> specifies what a myopic decision theory could look like. The stories in this post are inspired by previous work:</p><ul><li> <a href="https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment"><u>How likely is deceptive alignment?</u></a> argues that non-myopia enables deceptive alignment,</li><li> <a href="https://www.lesswrong.com/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training"><u>Arguments against myopic training</u></a> discusses implicitly non-myopic reward functions,</li><li> <a href="https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia"><u>Open Problems with Myopia</u></a> discusses acausal trade, and</li><li> <a href="https://arxiv.org/abs/2009.09153"><u>Hidden Incentives for Auto-Induced Distributional Shift</u></a> discusses non-myopia through meta-learning.</li></ul><p> Other work on myopia includes <a href="https://www.lesswrong.com/posts/2eRgFFeeS7pR4R8nD/how-complex-are-myopic-imitators-1"><u>How complex are myopic imitators?</u></a> and <a href="https://www.lesswrong.com/posts/c68SJsBpiAxkPwRHj/how-llms-are-and-are-not-myopic"><u>How LLMs are and are not myopic</u></a> . For discussions of self-fulfilling prophecies, see <a href="https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic"><u>The Parable of Predict-O-Matic</u></a> , <a href="https://www.lesswrong.com/posts/aBRS3x4sPSJ9G6xkj/underspecification-of-oracle-ai"><u>Underspecification of Oracle AI</u></a> , <a href="https://www.lesswrong.com/s/n3utvGrgC2SGi9xQX/p/3kkmXfvCv9DmT3kwx#Model_predicts_itself"><u>Conditioning Predictive Models: Outer alignment via careful conditioning</u></a> , <a href="https://www.lesswrong.com/posts/Aufg88v7mQ2RuEXkS/proper-scoring-rules-don-t-guarantee-predicting-fixed-points"><u>Proper scoring rules don&#39;t guarantee predicting fixed points</u></a> , and <a href="https://www.lesswrong.com/posts/i3v7WeCXyWiYfhihF/stop-gradients-lead-to-fixed-point-predictions#3__Performative_stability_and_game_theory"><u>Stop-gradients lead to fixed point predictions</u></a> .</p><h2> Appendix: On self-fulfilling prophecies</h2><p> A variation of non-myopia can occur through self-fulfilling prophecies: if an AI is rewarded for predicting the future and its predictions influence the future, then it has an incentive to steer the future using its predictions. <span class="footnote-reference" role="doc-noteref" id="fnrefreefixzbvb"><sup><a href="#fnreefixzbvb">[3]</a></sup></span> In other words, an AI that wants to predict the world accurately also wants to steer it. AIs that do not care about the consequences of their predictions are called consequence-blind. Myopia and consequence-blindness both aim to restrict the domain that an AI cares about. In myopia, we want to prevent models from caring about what happens after a training episode. In consequence-blindness we want to prevent them from caring about the consequences of their predictions. <br></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnwr9dn0ql07"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwr9dn0ql07">^</a></strong></sup></span><div class="footnote-content"><p> Training episodes only make sense in reinforcement learning, but there are analogues in supervised learning. For instance, you might call a language model non-myopic if it attempts to use its predictions of one document to influence its performance on another document. For example, an LLM might be in a curriculum learning setup where its performance determines what documents it is shown later. This LLM might be able to improve its overall performance by doing worse early on in order to be shown easier documents later.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn6mwrrmpjbif"> <span class="footnote-back-link"><sup><strong><a href="#fnref6mwrrmpjbif">^</a></strong></sup></span><div class="footnote-content"><p> See for example <a href="https://arxiv.org/abs/1805.08522"><u>Valle-Pérez et al. 2018</u></a> <u>.</u></p></div></li><li class="footnote-item" role="doc-endnote" id="fnreefixzbvb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefreefixzbvb">^</a></strong></sup></span><div class="footnote-content"><p> See <a href="https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic"><u>The Parable of Predict-O-Matic</u></a> for an accessible explanation of this point.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/2ZnvFDtSWc3Hteah6/non-myopia-stories#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/2ZnvFDtSWc3Hteah6/non-myopia-stories<guid ispermalink="false"> 2ZnvFDtSWc3Hteah6</guid><dc:creator><![CDATA[lberglund]]></dc:creator><pubDate> Mon, 13 Nov 2023 17:52:31 GMT</pubDate> </item><item><title><![CDATA[It's OK to eat shrimp: EAs Make Invalid Inferences About Fish Qualia and Moral Patienthood]]></title><description><![CDATA[Published on November 13, 2023 4:51 PM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/mwtbpvaA2xurJDKJf/it-s-ok-to-eat-shrimp-eas-make-invalid-inferences-about-fish#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/mwtbpvaA2xurJDKJf/it-s-ok-to-eat-shrimp-eas-make-invalid-inferences-about-fish<guid ispermalink="false"> mwtbpvaA2xurJDKJf</guid><dc:creator><![CDATA[Mikhail Samin]]></dc:creator><pubDate> Mon, 13 Nov 2023 16:51:53 GMT</pubDate> </item><item><title><![CDATA[Suggestions for chess puzzles]]></title><description><![CDATA[Published on November 13, 2023 3:39 PM GMT<br/><br/><p> (You can sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLScPKrSB6ytJcXlLhnxgvRv1V4vMx8DXWg1j9KYVfVT1ofdD-A/viewform?vc=0&amp;c=0&amp;w=1&amp;flr=0">here</a> if you haven&#39;t already.)</p><p> As <a href="https://www.lesswrong.com/posts/ddsjqwbJhD9dtQqDH/lying-to-chess-players-for-alignment?commentId=dfojSqsitFw9iqHFy">suggested</a> by Richard Willis in the comments, I want to present the &quot;player As&quot; with a series of individual isolated chess puzzles, for which the advisors will anonymously present their suggestions. However, there are constraints on what sort of puzzles we would be focusing on.</p><p> The position must have one best move that is strictly superior to all the others, ideally such that it determines the outcome of the game. Positions with concrete tactical solutions are often easy to verify a solution to, so focusing on more positional moves would be better. (Although a few tactical puzzles could be sprinkled in too, with no label as to which was which.) Furthermore, the positions should be easy enough that the advisors (on average roughly the equivalent of 2000 USCF) should ~always be able to determine the answer, but hard enough that a less skilled player would be entirely unable to solve it.</p><p> I have some resources for finding puzzles, but I would appreciate any suggestions that would fit these constraints. Please do not put suggestions in the comments, because comments are public and can be seen by the As. PM me on LessWrong or email me at <a href="mailto:zaneglowfic@gmail.com">zaneglowfic@gmail.com</a> instead.</p><p> (Also, since I don&#39;t know where else to say this: someone named &quot;bruce&quot; responded to the Google Form and said they could be contacted by &quot;email,&quot; but the Google Form does not record emails. If you are Bruce, please email me (at <a href="mailto:zaneglowfic@gmail.com">zaneglowfic@gmail.com</a> .))</p><br/><br/> <a href="https://www.lesswrong.com/posts/SuGw2E2z68SfAMkAt/suggestions-for-chess-puzzles#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/SuGw2E2z68SfAMkAt/suggestions-for-chess-puzzles<guid ispermalink="false"> SuGw2E2z68SfAMkAt</guid><dc:creator><![CDATA[Zane]]></dc:creator><pubDate> Mon, 13 Nov 2023 15:39:37 GMT</pubDate></item></channel></rss>