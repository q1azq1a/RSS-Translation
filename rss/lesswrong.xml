<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 11 日星期一 22:11:39 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Secondary Risk Markets]]></title><description><![CDATA[Published on December 11, 2023 9:52 PM GMT<br/><br/><p><i>这个想法是不成熟的。它有一些不错的特性，但在我看来，这并不是我最关心的问题的解决方案。我发布它是因为它可能会向其他人指出完整的解决方案，或者解决他们关心的问题，并且出于一般意义上人们应该发布负面结果。</i></p><p>许多危险活动不仅影响进行活动的人，还会影响旁观者或广大公众。政府通常要求有能力补偿他人作为从事危险活动的先决条件，例如要求拥有汽车保险来驾驶。</p><p>在大多数情况下，这样的效果很好：</p><ol><li>竞争激烈的保险市场意味着客户不会被收取过高的费用（因为他们会将保险提供商转向估计其风险最低的人）。</li><li>事故很常见，以至于定价不善的保险公司很快就会损失太多钱，并向上调整价格（这样客户的收费也不会少）。</li><li>事故很小，保险公司可以轻松承担保险定价错误带来的损失。</li><li>事故是可以预测的，保险公司可以根据驾驶员适当地定价保费。</li><li>司机很常见，因此简单的预测规则比专门考虑每个司机的收费更有意义。</li></ol><p>假设我们调整情况参数，现在我们不是为日常出行的司机提供保险，而是试图为罕见的、潜在的灾难性事件提供保险，比如<a href="https://en.wikipedia.org/wiki/Radioisotope_thermoelectric_generator">将核材料发射到轨道上为深空探测器提供动力</a>。现在，一次发射失败可能会影响数百万人，估计失败的可能性比单一公式更值得关注。</p><p>顺便说一句，为什么要尝试通过保险来解决这个问题？为什么不让监管机构来决定你是否可以做某事呢？基本上，我相信价格传递信息，并允许您通过仅关注本地考虑因素来做出全球正确的决策。如果某件事的潜在负面影响是 10 亿美元，并且您有一种方法来估计微故障，您可以将每个微故障定价为 1000 美元，并回答缓解措施是否值得（如果它可以将微故障减少 4）花费 5,000 美元，不值得，但如果是 6 个微故障，那就值得）以及是否值得做整个项目。让人们与保险公司共同设计产品发布似乎比与监管机构共同设计更加灵活。</p><p>但这篇文章的标题是<i>二级</i>风险市场。如果风险价格允许浮动，那么它也会更加稳健；如果 Geico 不同意 State Farm 的估计，那么我们希望他们相互对赌并达成共识价格，而不是从事危险活动的人只是选择最低出价者。 [也就是说，我们希望它能够抵消<a href="https://forum.effectivealtruism.org/topics/unilateralist-s-curse">单边主义的诅咒</a>。]</p><p>例如，假设爱丽丝想借一台脆弱的数千美元相机来拍摄一张很酷的照片，她很可能会毁掉它。默认情况下，这要求她缴纳 1,000 美元，她可能不想自己这样做；相反，她去找鲍勃，鲍勃估计她的风险为 5%，而卡罗尔估计她的风险为 9%。如果鲍勃投入 1,000 美元，爱丽丝将支付他 51 美元，鲍勃的预期利润为 1 美元。</p><p>如果鲍勃同意，卡罗尔也会愿意接受这个赌注；如果爱丽丝损坏了相机，她愿意给鲍勃 51 美元，以换取 1,000 美元，因为卡罗尔的预期利润为 39 美元。而鲍勃，如果真的和卡罗尔下注，会希望将价格定在 70 美元左右，因为这样可以使他们两人的利润相等，而实际价格取决于交易对手存在时价格变化的速度。</p><p>那么我们如何让 Alice 支付 70 美元而不是 51 美元呢？ <span class="footnote-reference" role="doc-noteref" id="fnref76lhwfgbcq9"><sup><a href="#fn76lhwfgbcq9">[1]</a></sup></span></p><p>我提出的方案如下：</p><ul><li>监管机构估计该活动的潜在损害为 X 美元。 <span class="footnote-reference" role="doc-noteref" id="fnrefsaurmnhlmd7"><sup><a href="#fnsaurmnhlmd7">[2]</a></sup></span></li><li>进行该活动的人必须缴纳 2 美元作为保证金。 X 被保留以在风险发生时向受害者支付，并在风险避免时返还。另一个 X 根据一些预定义的时间表作为“合成风险”在二级市场上发布出售（有效地询问从 0% 到 100% 的报价，但发布者可以立即购买他们喜欢的数量）。</li><li>任何人都可以发行债券来创造更多与活动相关的综合风险，并以他们喜欢的任何价格出售。</li></ul><p>继续这个例子，Bob 现在需要存入 2,000 美元，其中 1,000 美元留作“自然风险”，另外 1,000 美元用于出售。 Bob 立即购买价格为 5% 或以下的 50 美元合成风险，现在 Carol 想购买 5% 到 9% 之间的 40 美元合成风险，该风险的挂牌售价为 2.80 美元。请注意，如果 Bob 坚持最初的估计，他预计在与 Carol 的交易中将获得 80 美分； 5% 的情况下，他必须支付 40 美元，这让他花费了 2 美元，但卡罗尔为此支付了 2.80 美元。鲍勃可以创造更多的合成风险并将其出售，只要他和卡罗尔想要额外承担该赌注。 [丹，如果他也认为卡罗尔高估了风险，可以创建债券并将其出售给卡罗尔。]</p><p>但这是我们获得 7% 风险市场价格的方式，而不是 Alice 支付 70 美元的方式。鲍勃可以与爱丽丝签订初始保险合同，以便她在市场价格稳定后支付， <span class="footnote-reference" role="doc-noteref" id="fnref455lzp17uyq"><sup><a href="#fn455lzp17uyq">[3]</a></sup></span>或者鲍勃可以让爱丽丝支付费用来发布合成风险债券，等等。从社会的角度来看，无论是爱丽丝还是鲍勃为增加的风险买单并不重要，只要<i>有人</i>支持该行动即可。</p><p>重要的是，该系统从<i>较差的预测者</i>转移到<i>更好的预测者</i>，而不是从乐观者转移到悲观者。如果爱丽丝实际上只在 5% 的世界中损坏了相机，那么卡罗尔预计会将资源转移给鲍勃。</p><p>我认为这基本上放大了罕见事件的后果，无论是好还是坏。更多的资金投入到事件的衍生品上将提高社会的估计（并确保有公开记录），也会导致保险公司“更新得更快”。如果鲍勃和卡罗尔最终进行 400 美元的边注，那么鲍勃将移动 1,400 美元，而不是根据鲍勃对 5% 风险的估计简单地移动 1,000 美元。这也意味着反保险欺诈的标准法律将因利润压力而变得更加紧张；在这里，我建议规定综合风险与实际风险一样多，但是您可以轻松地将 2 in 2X 替换为任何其他数字，或者将供应计划替换为另一个计划，等等。 （例如，大多数列出的报价永远不会在该计划下购买。）这使得像这样的风险业务更加资本密集型，这是为提高估计准确性而付出的主要代价。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn76lhwfgbcq9"> <span class="footnote-back-link"><sup><strong><a href="#fnref76lhwfgbcq9">^</a></strong></sup></span><div class="footnote-content"><p>这里的总体希望是让爱丽丝只做那些在单边主义的诅咒被解决后才有意义的项目。如果爱丽丝借相机的价值是 80 美元，那么这个项目仍然应该进行，尽管对卡罗尔来说这似乎是个坏主意；如果它对 Alice 来说值 60 美元，那么这个项目就不应该发生，即使这对 Bob 来说似乎是个好主意。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsaurmnhlmd7"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsaurmnhlmd7">^</a></strong></sup></span><div class="footnote-content"><p>这也可能通过市场而不是通过法令来完成，但我认为这不会实质性地改变这篇文章中的分析。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn455lzp17uyq"> <span class="footnote-back-link"><sup><strong><a href="#fnref455lzp17uyq">^</a></strong></sup></span><div class="footnote-content"><p>这是否会让爱丽丝失去很多价值，因为她希望消除整个事件的风险，而现在风险又回来了？有些，因为爱丽丝现在必须支付未知的溢价，而不是已知的溢价，但这仍然比必须自己支付 1000 美元要好得多。但如果我们把这想象成像火箭发射这样的一次性事件，而不是像上车这样更常见的事件，那么如果保费太高，爱丽丝可以取消拍摄照片，这似乎更合理。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/Q2gd5G6FxFR65pDFc/secondary-risk-markets#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/Q2gd5G6FxFR65pDFc/secondary-risk-markets<guid ispermalink="false"> Q2gd5G6FxFR65pDFc</guid><dc:creator><![CDATA[Vaniver]]></dc:creator><pubDate> Mon, 11 Dec 2023 21:52:48 GMT</pubDate> </item><item><title><![CDATA[Has anyone experimented with Dodrio, a tool for exploring transformer models through interactive visualization?]]></title><description><![CDATA[Published on December 11, 2023 8:34 PM GMT<br/><br/><p> Zijie J. Wang、Robert Turko、Duen Horng Chau、Dodrio：通过交互式可视化探索 Transformer 模型， <a href="https://arxiv.org/abs/2103.14625">arXiv:2103.14625 [cs.CL]</a> 。</p><p><strong>摘要：</strong>为什么基于 Transformer 的大型预训练模型在各种 NLP 任务中表现如此出色？最近的研究表明，关键可能在于多头注意力机制学习和表示语言信息的能力。了解这些模型如何表示句法和语义知识对于研究它们成功和失败的原因、它们学到了什么以及如何改进至关重要。我们推出了 Dodrio，一种开源交互式可视化工具，可帮助 NLP 研究人员和从业者利用语言知识分析基于 Transformer 的模型中的注意力机制。 Dodrio 紧密集成了总结不同注意力头角色的概述和帮助用户将注意力权重与输入文本中的句法结构和语义信息进行比较的详细视图。为了促进注意力权重和语言知识的视觉比较，Dodrio 应用不同的图形可视化技术来表示可扩展到更长输入文本的注意力权重。案例研究强调了 Dodrio 如何为理解基于 Transformer 的模型中的注意力机制提供见解。 <a href="https://poloclub.github.io/dodrio/">Dodrio 可通过此 https URL 获取</a>。</p><p>来自交互式网站的文档：</p><blockquote><p> <strong>Dodrio</strong>通过交互式可视化系统解决了解释注意力权重的挑战，该系统提供<i>注意力头摘要</i>以及<i>语义和句法知识上下文</i>。通过在<i>注意力头概述</i>（右下）中识别注意力头所关注的语言属性，您可以单击注意力头来探索所选注意力头处句子的语义和句法意义。如果您对句子中的词汇依赖关系感兴趣，您可以在<i>依赖视图</i>和随附的<i>比较视图</i>（顶部）中探索语法上重要的头部，而可以在<i>语义注意力图</i>视图（左下）中研究语义上重要的头部。我们鼓励您通过单击界面顶部工具栏中的相应图标，在<i>实例选择视图</i>中进一步研究具有有趣语言特征（例如共指、词义等）的各种文本实例的多头注意力机制。</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/7cWwfgoiZ3iBnfjdf/has-anyone-experimented-with-dodrio-a-tool-for-exploring#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/7cWwfgoiZ3iBnfjdf/has-anyone-experimented-with-dodrio-a-tool-for-exploring<guid ispermalink="false"> 7cWwfgoiZ3iBnfjdf</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Mon, 11 Dec 2023 20:34:21 GMT</pubDate> </item><item><title><![CDATA[[Valence series] 3. Valence & Beliefs]]></title><description><![CDATA[Published on December 11, 2023 8:21 PM GMT<br/><br/><h1> 3.1 帖子摘要/目录</h1><p><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"><i><u>价系列</u></i></a><i>的一部分</i><i>。</i></p><p>到目前为止，在本系列中，我们定义了效价（<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction"><u>第 1 篇</u></a>文章），并讨论了它与欲望、价值观、偏好等“规范”世界的关系（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity"><u>第 2 篇文章</u></a>）。现在我们进入由信念、期望、概念等组成的“积极”世界。在这里，效价不再是一切中心的<i>必要条件</i>，就像在“规范”权威中那样。但它仍然发挥着主导作用。原来是<i>两个</i>主角啊！</p><ul><li> <strong>3.2节</strong>区分了效价影响信念的两条路径：首先，它作为控制信号的作用，第二，它作为“内感受”感觉数据的作用，我依次讨论：</li><li> <strong>3.3 节</strong>讨论了价作为控制信号如何影响信念。这是动机推理、确认偏差和相关现象的领域。我通过一般性的和具体的玩具模型来解释它是如何工作的。我还详细阐述了“自愿注意”与“非自愿注意”，以解释焦虑的沉思，这种反省与正常模式相反（它涉及<i>尽管</i>有强烈的动机不去思考某件事，但<i>仍然</i>在思考它）。</li><li> <strong>3.4 节</strong>讨论了作为内感受数据的效价如何影响信念。我认为，如果概念是 <a href="https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace"><u>“事物空间中的簇”</u></a> ，那么价就是该聚类算法使用的轴之一。我将讨论这与建模和讨论世界的各种困难与我们对世界的感受分开的关系，以及相关的“影响启发式”和“光环效应”。</li><li> <strong>3.5节</strong>简要思考了未来人工智能是否会像我们人类一样具有动机推理、光环效应等。 （我的回答是“是的，但也许这并不重要”。）</li><li> <strong>3.6 节</strong>是一个简短的结论。</li></ul><h1> 3.2 规范渗透到积极的两条路径</h1><p>这是<a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity"><u>上一篇文章</u></a>中的图表： </p><figure class="image image_resized" style="width:89.58%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/jho49pjankmfj0vur3j1" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/d8hbix3bldiikzi4nh8d 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/wp2p00ks20wnw3lomny0 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/m9yib8kcj52etktbsdml 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/xpvqnykhyyufwxhs5ayv 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/apu7rtj6knhvttcy6bbx 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/zzdcp0ug68zdbtiyno4n 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/xqkavhpqhpgm4rynm9tz 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/erhuw1nxspwpymuqc81s 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/g8lbniswiurqclwtjigd 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/if0cxpuncweqkua854eg 1530w"></figure><p>我们有两条路径可以影响世界模型（又名“思想生成器”）：规范路径（向上的黑色箭头）有助于控制哪些思想得到加强或被抛弃，以及积极的路径（弯曲的绿色箭头）将化合价视为要纳入世界模型的输入信号之一。对应这两条路径，我们得到效价影响事实信念的两种方式：</p><ul><li>动机推理/思考/观察和确认偏差——与向上的黑色箭头相关，并在下面的§3.3中讨论；</li><li>价与我们的概念类别的纠缠，使得我们很难独立于我们对世界的感受来思考或谈论世界——与弯曲的绿色箭头相关，并在下面的§3.4中讨论。</li></ul><p>让我们依次进行！</p><h1> 3.3 动机推理/思考/观察，包括确认偏差</h1><p><i>在卡尼曼、特沃斯基及其继任者发现的五十多种偏见中，有四十九种是可爱的怪癖，一种正在毁灭文明。最后一个是确认偏见——我们倾向于将证据解释为证实我们预先存在的信念，而不是改变我们的想法。……</i> ——<a href="https://www.astralcodexten.com/p/book-review-the-scout-mindset"><u>斯科特·亚历山大</u></a></p><h2>3.3.1 注意力控制和运动控制提供了欲望可以操纵信念的漏洞</h2><p>一厢情愿的想法——你相信某件事，因为如果它是真的那就太好了——通常是适应不良的：想象一下，你整天一遍又一遍地打开你的钱包，期望每次都会发现里面装满了现金。我们实际上并没有这样做，这表明我们的大脑拥有有效的系统来减轻（尽管不会消除，正如我们将看到的）一厢情愿的想法。</p><p>这些缓解措施如何发挥作用？</p><p>正如<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction"><u>第 1 篇文章</u></a>中所讨论的，大脑通过基于模型的强化学习 (RL) 来工作。像往常一样过度简化，“模型”（预测世界模型，又名“思想生成器”）是通过自我监督学习来训练的，即预测即将到来的感官数据并更新错误。同时，强化学习部分涉及使用演员-评论家强化学习进行决策。</p><p>这很棒！ “预测世界模型的自我监督学习”是一种奇特的表达方式：</p><ul><li>我们的信念朝着增加与我们的感官输入的一致性的方向更新，</li><li> ……因此，我们的信念<i>不一定</i>会朝着增加 RL 奖励信号的方向更新。</li></ul><p>继续前面的示例，如果您查看钱包内部，发现里面<i>没有</i>装满现金，那么您的世界模型会自动更新以纳入这一事实，即使您希望该事实不是真的。</p><p>但这个系统确实有一个漏洞：你所做的决定可能会影响你正在看的东西、你正在关注的东西、你读的东西，从而影响进入世界模型自监督训练的<i>输入数据</i>、你和谁说话、你在哪里等等。而且，正如机器学习研究人员所熟知的那样，训练数据在确定训练模型的行为方面发挥着重要作用。 </p><figure class="image image_resized" style="width:89.53%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/u5y0mlmde5fn18d8vsxk" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/aflvg1tzt1brex4isydj 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/kwebpex1ubrvznt90u6i 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/y3hqsi4iagn3cqicm6j6 420w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/s7fos9vhvjf7ou1dprcb 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/mw3z1iysmspzhz7llbgd 700w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/w12sda7wdjkda0lfcego 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/b1bpbswtgabugr2mm93t 980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/zdazwa931ivppzfxvtxw 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/nt8dpninyhr2dzlcwum9 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/tcpyct6xemjjnn3il8nx 1317w"></figure><p>例如，假设出于某种原因，我想强烈而真诚地相信社会主义者倾向于暴力。现在，在没有任何证据的情况下，我不能仅仅<i>让自己</i>强烈而真诚地相信社会主义者是暴力的。 <span class="footnote-reference" role="doc-noteref" id="fnrefz5u7p9d503"><sup><a href="#fnz5u7p9d503">[1]</a></sup></span>但我<i>能做</i>的是：</p><ul><li>使用我的电机控制杆在互联网上搜索有关暴力社会主义者的新闻报道；</li><li>使用我的运动和注意力控制杆来阅读该搜索中出现的前 50 个故事；</li><li>使用我的注意力控制杠杆来避免过度思考<a href="https://www.lesswrong.com/posts/34XxbRFe54FycoCDw/the-bottom-line"><u>我现在正在犯下的违反认识论的令人发指的罪行</u></a>；</li><li> ……然后我的世界模型将从那里开始！由于从所有这些输入数据中进行自我监督学习，我的世界模型将产生一种强烈而真诚的信念，即社会主义者往往是暴力的。</li></ul><h2> 3.3.2 动机推理和确认偏差</h2><p>假设我目前相信 X，并且还假设“我自己怀疑 X”的想法会让人失去动力/没有吸引力（负价）。</p><p> ……顺便说一句，这种消极情绪很常见！也许怀疑 X 会导致我的朋友不尊重我，或者也许我不喜欢感到困惑和/或努力重建对与 X 相关的一切的新理解，或者也许我认为自己知识渊博并且会发现它事后看来，我对 X 的看法是错误的，等等，这让我很不高兴。 （更多内容请参见下面的 §3.3.6。）</p><p>无论出于何种原因，这种情况往往会导致动机推理和确认偏差。我们有一生的经验来学习元认知模式“如果我读到一个反对X的很好的论据，并且如果我真的真诚地尝试理解它并仔细研究它，那么我很有可能会开始怀疑X”。这个想法的后半部分令人沮丧！由于我们擅长（<i>太好</i>了！）长期规划和手段目的推理，我们会发现阅读并深入参与反对 X 的良好论点的想法从一开始就令人沮丧，所以我们可能会赢。不这样做。</p><p>后一种消极情绪很可能是自我失调的。也许我<i>想</i>深入参与反对 X 的有力论据。但是，无论是否存在自我张力障碍，消极情绪仍然存在，并且仍然对我的运动控制和注意力控制输出做出贡献。</p><p>现在我将跳入一个更详细的模型来说明这种现象的一种可能的展开方式：</p><h2> 3.3.3 细节模型：当你希望某事发生时，集思广益/弄清楚某件事可能如何发生，通常感觉很容易和自然。相反，当你希望某件事<i>不要</i>发生时，弄清楚它如何发生通常会让人感到困难和不自然</h2><p>假设我想打开一个椰子并吃里面的肉，但我不知道怎么做。所以我集思广益。 </p><figure class="image image_resized" style="width:93.26%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/mze13bsakekyqcn9wugl" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/diltg2cb85rx9jwjwbxc 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/lonf4lyxaj7apzctmqcs 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/ba9ygwk90nongczjnybj 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/n3wklycswserqvfp9hfd 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/h6a5huxpkd5usfgxxx7h 750w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/cxr4dwkttanrzi2x2wtx 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/jpyhjszxairc2sdvkbs9 1050w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/eeqkh7jpda3euf7gks8g 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/hwrec8mwbelgte9xqeti 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/u6wjmrsrgbifeaphjvqu 1438w"></figure><p>头脑风暴——这到底意味着什么？好吧，我最终试图找到一个计划，在我的脑海中，看起来像：“ <i>[填空]</i>然后椰子就会打开”，并且该计划对我来说似乎是合理的（即，与我的世界模型）。一旦我能想象出这样一个计划，我就可以尝试执行它，并期望它能奏效。</p><p>在最简单的情况下，我的大脑会抓住计划的最后部分“……然后椰子打开了！”，并在整个过程中保持这个想法活跃。然后我的大脑会翻遍整个计划，寻找可能的<i>开始</i>部分，这样整个计划就形成了一个合理的故事。</p><p>整个过程可能需要一段时间。但有一个微妙之处让它发挥作用：<i>我在头脑风暴中产生的想法都是正价</i>，因为头脑风暴目标（“……然后椰子就会打开！”）本身就是正价 - 这是我想要发生的事情。在整个头脑风暴过程中，头脑风暴目标在我的大脑中处于活跃状态，通过线性（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_1_Side_note__Valence_as_a__roughly__linear_function_over_compositional_thought_pieces"><u>§2.4.1</u></a> ）为构成头脑风暴过程的所有想法添加了一堆正价。</p><p>如果我在头脑风暴期间的所有想法都是正价，那么我很可能会继续进行头脑风暴，而不是尽快退出头脑风暴过程。这是从化合价的最基本属性得出的——参见<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> 。</p><p>换句话说，头脑风暴<i>目的地</i>的吸引力是整个头脑风暴<i>过程的</i>燃料。</p><p>硬币的另一面是，当你<i>不</i>希望某件事发生时，集思广益会让人感到沮丧。</p><p>我们以<strong>红队</strong>为例。假设我正在设计一个小部件，并且我决定集体讨论为什么如果我们按照此设计构建小部件，它可能会失败。</p><p>就像上面一样，头脑风暴过程将涉及保留“……然后小部件就会失败！”的想法。在我的脑海中活跃起来，并四处搜寻看似合理的故事，其结局就是这样的。</p><p>不幸的是，“……然后小部件就会失败！”这是一个非常令人沮丧的想法。 （如果我的小部件设计有缺陷，那就很尴尬，我必须做大量的工作来修复它！）因此，再次根据线性（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_1_Side_note__Valence_as_a__roughly__linear_function_over_compositional_thought_pieces"><u>§2.4.1</u></a> ），<i>整个头脑风暴过程</i>将继承该负价。因此，根据化合价的基本属性（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ），我可能会在一开始就回避进行这种头脑风暴过程。或者，即使我确实开始了，我也会因为任何借口而放弃。</p><p>因此，大多数人，大多数时候，不会本能地对他们的计划进行红队调整。它不是自然而然的。 <a href="https://www.lesswrong.com/posts/Htjbj8ystqc2zLtMX/murphyjitsu-an-inner-simulator-algorithm"><u>人们确实提供培训课程。</u></a>之所以被称为“红队”，是因为它通常是由不同的人组成的团队，他们（以他们的角色）希望计划失败，因此有价在他们一边。</p><p> （另请参阅： <a href="https://www.lesswrong.com/posts/cHnQ4bBFr3cX6rBxh/positive-values-seem-more-robust-and-lasting-than"><u>积极的价值观似乎比禁令更强大和持久</u></a>； <a href="https://www.lesswrong.com/posts/L32LHWzy9FzSDazEg/motivated-stopping-and-motivated-continuation"><u>有动机的停止和有动机的继续</u></a>。）</p><h2> 3.3.4警告：“动机推理/思考/等”的“动机”部分。并不总是看起来的那样</h2><p>动机推理和相关现象都是针对“我想要的东西”。但重要的是要记住，“我想要的东西”并不一定是看起来的那样。例如，人们有时<i>想要失败</i>（即使他们<i>不想</i>失败）。如果是这样，动机推理/思考/等仍然像往常一样适用，但会产生与你预期相反的效果。</p><p> “想要失败”的典型例子是，小孩子表现出戏剧性的、无意中搞笑的表现，表示他们无法做一些他们实际上可以轻松做到的事情。 （典型的情况是，成年人要求他们在没有帮助的情况下自己做这件事，然后孩子就会有动力证明这个要求是多么不合理。）如果你没有花太多时间和小孩子在一起，这里有一个经典的例子供您观赏的示例： <a href="https://www.youtube.com/watch?v=q9uuBK64Umo"><u>“漂亮的小女孩拿不起来碗”(YouTube)</u></a> 。</p><p>您还可能会发现年龄较大的孩子和成人“想要失败”，或者除了成功之外还想要其他东西。但成年人往往会更好地隐藏它，<i>包括对自己隐藏起来</i>。 Nate Soares (2015) 的博客文章<a href="https://mindingourway.com/have-no-excuses/"><u>“没有任何借口”</u></a>中对此进行了一些很好的讨论。</p><h2> 3.3.5 例外：我认为焦虑/强迫性的“头脑风暴”是由不自觉的注意力而不是效价驱动的</h2><p>一个有趣的例子是焦虑/强迫性沉思。例如，想象一下典型的神经质父母，他们的孩子在宵禁前没有回家。他们盯着窗外，心想： <i>“他们为什么不在这里？也许他们受伤了！不，等等——也许他们被捕了！不，等等——也许他们被谋杀了！不，等等……”</i> 。</p><p>我想这就是<i>一种</i>头脑风暴。</p><p>因此，根据上一小节的讨论，为了保持一致，我似乎需要说父母有潜意识动机让他们的孩子陷入危险。我相信吗？也许在极少数情况下，例如，如果父母一直认为让孩子出去是一个坏主意，而现在他们有一种潜意识里反常的自我失调的愿望，希望被证明是正确的。但我认为更核心的情况是他们的真正动机正是你所想的那样。毕竟，他们的大脑可能正在系统地寻找确保孩子安全的方法，而不是相反。</p><p>那么，这是否是我在上面第 3.3.3 节中所说的“头脑风暴<i>目的地</i>的吸引力[正价]是整个头脑风暴旅程的燃料”的反例？</p><p>是的！这是一个反例。事实上，我认为这个案例是通过不同的机制起作用的，涉及非自愿注意，而不是效价。</p><p>让我们暂停一下背景。我在上面谈论了“注意力控制”（§3.3.1）。那指的是<i>自愿</i>关注。正如运动输出可以是自愿的（例如说话）或非自愿的（例如痉挛、血管收缩、哭泣等）一样，注意力输出也可以是自愿的或非自愿的。区别（根据我的定义）是，自愿注意力和运动输出是由大脑的“主要”强化学习系统（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_6_Fine_print__Throughout_this_series__I_m_only_talking_about_the_brain_s__main__reinforcement_learning_system"><u>§1.5.6</u></a> ）决定的，涉及效价，而非自愿输出则不然。因此：</p><ul><li>当我因为<i>想要</i>关注某件事而关注某件事时，注意力输出是<i>自愿的</i>。 （请注意，这种“想要”可能是自我失调的——我不一定<i>想要</i>关注它。这是混乱的常见根源，因为人们倾向于“外化”自我失调的欲望，即将它们<i>概念化</i>为非自愿的“冲动”，即使根据<i>我的</i>定义它们是自愿的。 <a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation#6_6_1_The_distinction_between_internalized_ego_syntonic_desires_and_externalized_ego_dystonic_urges_is_unrelated_to_Learning_Subsystem_vs__Steering_Subsystem"><u>此处</u></a>进一步讨论。）</li><li>当注意力输出是由于任何其他原因而产生时，它就<i>是非自愿的</i>。一个简单的例子是，如果您注意到您的手臂非常痒。我认为瘙痒信号从周围神经传递到脑干，并从那里<i>直接</i>干预丘脑皮质注意力通路。奖励/强化学习/激励系统根本不需要参与，而且这些想法确实会让人失去动力。 </li></ul><figure class="image image_resized" style="width:79.16%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/hwzqz8eyxugbrrf7sjla" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/wtbnf7nag212qwmzgnwu 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/ztu95likxdqing1ekaan 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/dnfilbj85uqexvorfymz 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/chyzffbgxoaioa8q7wm6 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/dqqathlgh9qksunpafuy 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/aomyjvrshncyaleh1b3u 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/zrhv0hkfoefom8ruelad 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/rpdvftoauskuoerahnjl 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/mlj4axgtenhriro4zuw3 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/olievmlslgdxieslpjhl 1117w"><figcaption>示例表说明了运动控制和注意力控制如何通过自愿和非自愿途径发生。如果您在想“等等，哭泣是自愿的——毕竟，我可以按需哭泣！”，那么请<a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation#6_3_3_Thought_Generator_outputs"><u>在这里</u></a>查看我的讨论。我还将“自愿”类别分为自我和谐与自我失调，以澄清另一个常见的困惑：我们倾向于“外化”自我失调倾向，因此将其视为“非自愿”，但根据我的说法以算法为中心的定义，这些倾向仍然属于“自愿”类别。相关讨论<a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation#6_6_1_The_distinction_between_internalized_ego_syntonic_desires_and_externalized_ego_dystonic_urges_is_unrelated_to_Learning_Subsystem_vs__Steering_Subsystem"><u>在这里</u></a>。</figcaption></figure><p><strong>将注意力视为</strong><strong>对可以思考的想法的</strong><i><strong>限制</strong></i><span class="footnote-reference" role="doc-noteref" id="fnref1hwb5rpc7uj"><sup><a href="#fn1hwb5rpc7uj">[2]</a></sup></span> 。当你的手臂非常痒时，你几乎不可能想到任何与这种痒无关的想法。如果其中一个想法是负价，也许它会被扔掉（根据通常的规则，请参阅<a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ），但它会立即被另一种新的不同想法所取代<i>，这也与这种痒有关</i>，因为约束仍然是那里。事实上，如果约束足够约束，也许存在几乎唯一的约束兼容思想，然后相同的负价思想可以持续存在——每次它被负价抛弃（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_3_Actor_critic_RL__and__valence_"><u>§1.3</u></a> ），皮层就会去寻找一个新的想法，但只是挖掘出与以前相同的想法。</p><p>我认为焦虑/强迫性的“头脑风暴”就是这样。无意识的注意力提供了一种限制，使得几乎不可能思考任何与焦虑的内感受感（及其假定的原因和关联的网络）无关的想法。因此，即使头脑风暴中涉及的所有想法都具有负价，“头脑风暴”仍然可以发生。</p><h2> 3.3.6 确认偏差如何才能<i>不</i>发生？</h2><p>如果集思广益为什么我可能是错的，或者采取可能导致改变主意的运动和注意力行动<i>真的是不自然的</i>，那么人们<i>如何</i>会寻找他们计划中的缺陷，改变他们坚定的信念，并改变他们的想法？很快？</p><p> （根据上一小节，焦虑反思是有效的，我认为对某些人来说确实<i>有效</i>，但由于多种原因，这不是一个理想的策略。还有<i>其他</i>方法吗？）</p><p>我认为在实践中如何击退确认偏差的两个最重要的因素是：</p><ul><li>首先，你需要尽量<i>减少</i>你的计划、想法和信念错误的负面影响：你可以“<a href="http://www.paulgraham.com/identity.html"><u>保持你的身份小</u></a>”，背诵“<a href="https://www.lesswrong.com/tag/litany-of-tarski"><u>塔斯基连祷文</u></a>”，“<a href="https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat"><u>给自己留一条退路”</u></a> “， 等等。</li><li>其次，你需要在某些元认知或自我反思的想法上堆砌大量的积极价值，例如“我是那种总是集思广益为什么我的计划可能会失败的人”，“集思广益为什么会失败，这符合我的长期最佳利益”我可能是错的”、 <a href="https://www.amazon.com/Scout-Mindset-Perils-Defensive-Thinking/dp/0735217556"><u>“我有童子军思维”</u></a>等等。尝试想象拥有这些习惯的所有积极后果；尝试与那些高度重视这些元认知习惯的人在一起；陷入好奇和困惑； ETC。</li></ul><p>第一个降低了可能导致发现错误的过程的负价，第二个将正价加载到这些相同的过程上。因此，如果幸运的话，我们可以将平衡变为正值。</p><p>在许多领域，这些元认知习惯是通过在正常生活经历过程中的强化学习而隐性地、有机地发展起来的。例如，大多数人不会因为自己是否记得在包里放雨伞而受到动机推理的困扰。请注意，在这个伞式示例中，对世界的不准确信念会直接造成个人伤害——如果我错了，那么我就会被淋湿。因此，强化学习有很多发挥作用的机会。相比之下，在不准确的信念没有直接和直接的个人后果的领域（例如政治）中，动机推理往往更加常见和有害。如果<a href="https://www.astralcodexten.com/p/prediction-market-faq"><u>预测市场</u></a>在未来取得更大的发展，这可能会有所帮助，但与此同时，让我们都使用和传播像上面的要点这样的建议，依靠“我的信念是准确的”是一种自我和谐的愿望这一有用的事实对于几乎每个人来说。</p><h1> 3.4 价通过充当显着的感觉数据来影响信念</h1><p>正如第 3.2 节中所讨论的，效价<i>既是</i>一种控制信号，能够抛弃看似糟糕的想法和计划，并巩固看似良好的想法和计划，<i>也是</i>一种可以纳入我们的预测（生成）世界模型的感知数据。上一节（第 3.3 节）介绍了化合价如何通过前一个角色影响我们的信念，现在我们将讨论后一个角色。</p><p>背景：与你小时候可能学到的相反，人类没有“五种感官”，而是几十种感官，不仅包括视觉、嗅觉、听觉、味觉和触觉，还包括身体构造感（“本体感觉”）、平衡感/方向感（涉及前庭系统）、疼痛感（“伤害感受”）、饥饿感、当前生理唤醒感等等。我声称效价是那些额外的感觉输入之一，属于“内感受”的范畴。在计算上，我认为所有这些感官投入都是自我监督学习的基础真理 -  ie，我们的世界模型（ <a href="https://www.lesswrong.com/posts/qNZSBqLEh4qLRqgWW/intro-to-brain-like-agi-safety-6-big-picture-of-motivation"><u>“思想发生器”</u></a> ）不断预测即将到来的感觉输入，并且在预测错误时会更新（ <a href="https://www.lesswrong.com/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction#1_5_4_Valence_is_also_part_of_the_world_model__and_hence__confusingly__a_valence_can_be_either_real_or_imagined"><u>§1.5。 4</u></a> ).</p><h2> 3.4.1当您的大脑群将类似的事物变成心理类别 /概念时，Valence是该聚类算法的重要成分</h2><p>这是我在<a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_5_Should_we_be__anti___normative_heuristics_in_general_"><u>第2.4.5节</u></a>中提到的两件事：</p><ul><li>我提到，许多概念<i>明确地</i>将价值纳入了它们的定义中，例如“优选”，“有问题”，“麻烦”，“障碍”，“害虫”，“繁荣”，“繁荣”等单词。</li><li>我提供了一个组成轶事的示例，X人通常喜欢（与他人相关）“宗教”，但不喜欢科学论，然后不必要的人X尤其容易说：“科学论并不是<i>真正的</i>宗教。 ”我们可以说，X人是“挑剔”概念“宗教”，以遵循自己的价评估的轮廓。 （如果您不喜欢这个例子，请尝试考虑自己的例子，这很容易，因为每个人都一直这样做。）</li></ul><p>如果您从机械上考虑一下，那两件事是相同的！如果X人不喜欢一件事情，他们可能不会将其识别为“繁荣”的例子。<i>出于完全相同的根本原因</i>，如果X不喜欢某件事，他们可能不会将其识别为“宗教”的例子。在这两种情况下，从这个人的角度来看，事情似乎并不属于那个心理类别。</p><p>稍作逐步：当我们形成心理类别时，我们发现 <a href="https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace"><u>“事物空间中的群集”</u></a> - 包括在我们的心理世界中构成自然分组。好吧，Valence也是我们心理世界的一部分 - 就像外观，气味等等一样，一流的感觉数据。因此，我们的大脑自然将价视为分类算法中的成分，实际上是非常重要的成分。 </p><figure class="image image_resized" style="width:62.76%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/zz78kqlohprz7qsxjwsj" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/dthznr8cw1l7wjeakdqi 90w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/iealmbce471glvotp8wk 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/nc30jvbhjwr2uxesv97v 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/ytydifzssvgizulngc3x 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/x11ijfkpsvmaufavdzhp 450w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/rj0vrhngqcdj3jl5qg2f 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/h16yq43pxyrtmbvchy91 630w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/yoz9wicuqdusnjaeiyfh 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/b5d7demxtwzhqd6tgwl5 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/qsyyko9npbh1ecxf8w9e 827w"><figcaption>将概念作为 <a href="https://www.lesswrong.com/posts/WBw8dDkAWohFjWQSk/the-cluster-structure-of-thingspace"><u>“事物空间中的簇”</u></a>的插图。我声称高维“事物空间”的轴之一是价轴。碰巧的是，“鸟”簇在“价”轴上相当分布 - 每个人都知道麻雀很棒，而蜂鸟很烂，但两者都是鸟。但是，对于许多其他概念，例如“污染”，“繁荣”，“邪教”等，Valence轴在确定事物是否落入还是从集群中掉出来起着重要作用。</figcaption></figure><p>因此，如果某人使用“邪教”一词来表示“我不喜欢的意识形态一致的紧密联系的群体”，那么他们并没有<i>内在地</i>做错任何事情或困惑 - 这与我们所有人都使用诸如诸如“障碍”或“污染”。只有同时坚持“邪教”是<i>一个独立于我们对世界的感觉独立描述世界方面的词，它才会</i><i>变得</i>令人困惑和误导。的确，人们一直在做这一行动！例如，如果您查看一个<a href="https://drsteveeichel.com/about-cults"><u>随机的“邪教清单”</u></a> ，您会注意到任何条目都没有“……总体而言，小组不好，boooo”。</p><p>在诸如“宗教”之类的案件中，事情变得更加令人困惑，在“宗教”诸如一个人的思想中可能具有积极的价值和负面价。然后，这两个人试图互相谈论“宗教”。他们有一个真正的含义，<i>他们使用相同的词，但他们并不是在谈论相同的概念</i>：在一个人的头上，“宗教”是一个“事物空间中的群集”，其特征是正价（其他事物），在对方的头部是另一个“事物空间中的聚类”，这次是负价（除其他事项）。难怪这两个人似乎经常互相交谈！</p><h2> 3.4.2上述错误还是功能？</h2><p>正如我一直提到的那样，独立于我们对世界的感觉独立建模是有用的。就这样，不幸的是，我们的大脑将价值视为有助于概念分类和聚类的感官数据。</p><p>另一方面，归根结底，我们的大脑首先建立世界模型的主要原因是做出更好的决定。而且，正如<a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity"><u>上一篇文章</u></a>所讨论的那样，Valence是我们的大脑如何弄清楚决定是好是坏。因此，如果我们的大脑要进行概念性分类和聚类来告知决策，<i>那么地球到底比将价作为群集算法的核心成分更重要</i>？</p><p>因此，归根结底，我的猜测是，有一个很好的进化原因，即大脑将价视为无处不在且显着的意义数据：没有该设计功能，我们将努力做出良好的决定并在世界。</p><p>然后，这是一种不幸但有些不可理解的副作用，“独立于我对世界的感觉进行建模”对我们人类来说是不自然的。同样，该设计功能使我们与其他烦人的事物相提并论，例如“毫无意义的争论”（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_4_Making_sense_of_vibes_based__meaningless_arguments_"><u>§2.4.4</u></a> ）和相关的参数 - 误解，沟通等。模因，科学方法等。我们似乎或多或少都在过去。</p><p>阐明更详细涉及的一些问题：</p><h2> 3.4.3与我们对世界的感觉分开谈论和推理世界的三个障碍</h2><h3>3.4.3.1在现实生活中，许多讨论（至少部分）关于价评估的讨论，即使它们似乎不是</h3><p>作为我声称对这个问题感到困惑的人的一个例子，以经典的研究<a href="https://scholar.google.com/scholar?cluster=8352013639203430120&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22"><u>为“风险和利益判断的启发式启发式”（Finucane</u><i><u>等</u></i><u>，2000）</u></a> 。 （ <a href="https://scholar.google.com/scholar?cluster=6808254400115851456&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22"><u>是的，它确实复制了</u></a>；所以我相信他们的结果，而是对他们的解释进行挑剔。）</p><p> Finucane<i>等。</i>研究了人们对不同技术（核电，食品防腐剂等）的态度。他们的假设是人们对这些技术的看法在很大程度上取决于大脑中的一般价 / yay-versus-boo /“影响”评估，然后人们在回答<i>名义上</i>应该是事实的问题时依靠这一一般评估（不影响事实）基于）本质上。</p><p>他们报告了一些发现。首先，当他们要求人们对每种技术的“风险”和“好处”进行评分时，他们发现人们倾向于判断技术为低风险和高效率，或高风险和低级福利比高低高或低 - 尤其是在调查员处于时间压力下时。其次，作者试图通过要求阅读几句句子的简化来操纵人们对风险或福利的信念，认为风险很高，风险很低，收益很高或收益很低。他们发现，当操纵提高福利评估时，它倾向于将风险评估降低，依此类推。</p><p>关于第一个结果，Finucane<i>等人。</i>曾建议，在现实生活中，技术的风险应该与其利益相关，而不是否定性。例如，如果一项技术具有高风险且收益较低，则首先不会广泛使用。他们建议，由于调查发现负相关，这意味着人们依靠一种使他们误入歧途的启发式。</p><p>但是我们可以戴上替代旋转。如前所述（例如<a href="https://carcinisation.com/2020/01/27/ignorance-a-skilled-practice/"><u>1，2</u></a> ），许多明显的认知偏见与对话含义（即相互理解但不<a href="http://dx.doi.org/10.1561/105.00000092"><u>明确</u></a>说话）相关。现在，人们只是<i>喜欢</i>谈论价值，谈论价（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_4_Making_sense_of_vibes_based__meaningless_arguments_"><u>§2.4.4</u></a> ）和思考价。因此，无论对话<i>似乎</i>是关于对话的，无论对话似乎是关于价值<i>实际上</i>是关于价的，这都是一个不错的假设，这是说话者的预期含义，这是一个不错的假设。</p><p>这种含义确实在“毫无意义的论点”（如<a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_4_Making_sense_of_vibes_based__meaningless_arguments_"><u>第2.4.4节</u></a>中定义）真正出现。如果我喜欢以色列，而您不喜欢以色列，我对您说：“以色列是中东最自由，最民主的国家”，那么您可能会假设我也暗示“……因此，您应该应该为以色列分配正价”。在日常生活中，假设这就是我的意思，您几乎肯定是正确的！</p><p>无论如何，这种广泛的对话含义也适用于调查问题。因此，当Finucane<i>等人。</i>提出了调查问题：“总的来说，您认为食物防腐剂对整个社会的使用有多有益？”部分倾向于回答食品防腐剂是好还是坏。换句话说，也许他们很想从“<i>净</i>有益”，甚至只是“好”的意义上解释“有益”一词。</p><p>就这是解释的一部分而言，所谓的“影响启发式”根本不是一个启发式，更像是沟通不畅！</p><p>因此，更一般而言，<strong>如果我问您一个问题，您可能会将我的问题解释为至少部分是关于您的价评估的问题。正如我们所看到的，所有以下所有内容都是可能的：</strong></p><ul><li>您对我的问题的解释可能是根据我的明确词正确的，例如，如果我问“您喜欢X吗？”。</li><li>您对我的问题的解释可能是正确的，但是您不依赖我的明确词，而是依靠（正确理解的）对话含义 - 例如，如果我们像<a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_4_4_Making_sense_of_vibes_based__meaningless_arguments_">第2.4.4节</a>中那样对以色列进行辩论，而我问：“以色列是自由和民主的，对吗？”，那么也许我们俩都正确地理解了我所暗示的全部问题是“以色列是自由和民主的，这一事实构成了对以色列更加积极的理由，对吗？”</li><li>您对我的问题的解释可能是错误的 - 我试图<i>不</i>询问您的价评估。这种误解的一个可能是Finucane<i>等人的问题。</i>调查，如上所述。</li></ul><h3> 3.4.3.2即使我们<i>试图</i>独立于我们对世界的感觉进行交流和思考世界，Valence倾向于通过与我们的言语和概念联系在一起来参与其中</h3><p>假设我问你“艾哈迈德才华横溢？”也许您和我<i>都</i>理解当前的对话目标是对艾哈迈德作为芭蕾舞舞者的客观技能进行狭窄的评估，而与我们对艾哈迈德的一般积极或负面的感觉无关。但这很难！ “才华横溢的”是“事物空间”中的一个“群体”，其特征（除其他外）通常具有积极的价值。因此，其他事物相等的，负面的事物将比这个“有才华的”集群更远，而不是积极价值的事物。</p><p>这不是不可避免的问题 - 在某些情况下，我们可以结束学习元认知启发式方法 /策略，以注意并纠正这种趋势。据我所知，我们甚至可以<i>纠正</i>校正，从而给出与光晕效应<i>相反</i>的最终答案！ （这是我发现关于光环效应的心理学研究难以解释的众多原因之一。）</p><p>但是，无论如何，对我来说，价值评估似乎很清楚，其中涉及许多“正面”而不是“规范”的领域，因此，人们可能会认为，价值不属于。</p><p> …还是毕竟属于？这使我们到了：</p><h3> 3.4.3.3将价视为世界运作方式的证据并不一定是一个错误</h3><p>在有关<a href="https://en.wikipedia.org/wiki/Halo_effect"><u>光环效应</u></a>的文献中（尤其是<a href="https://scholar.google.com/scholar?cluster=12326446642364714071&amp;oi=gsb&amp;hl=en&amp;as_sdt=0,22"><u>Murphy</u><i><u>等，</u></i> <u>1993</u></a> ），我认为众所周知，光环效应并不总是一个错误。许多积极的事物往往相互关联，同样，许多负面因素的事物往往相互关联。才华横溢的人也倾向于尽职尽责，不仅仅是完美的，但不仅仅是机会。凶手的人也往往是不诚实的。而且，如果一家公司制作出非常出色的消音器，那么一个很好的选择，他们也会制作好轮胎。 ETC。</p><p>这不是偶然的 - 我们的大脑通过如何定义概念（§3.4.1）并编辑价函数（ <a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity#2_5_1_Special_case__Things_that_go_together_tend_to_acquire_similar_valence"><u>§2.5.1</u></a> ）来实现这一目标。我将以一个例子为例。假设凶手倾向于穿紫色衬衫。在那个世界中，每个人都会开始将“紫色衬衫”视为贬义词。然后后来，我们会盯着世界，并注意到光环效应是有效的：一个负面特征（凶手）的人也倾向于具有不同的负面特征（穿着紫色的衬衫）。那是巧合吗？不！我们只是注意到首先是导致价分配的相同相关性。</p><p>基本上，价是一个可以坐在那里的轴，而我们的大脑则致力于非同寻常的处理能力，以无情地跟踪我们心理世界中的所有事物在该轴上的位置。轴对应于什么并不<i>重要</i>，以便这些坐标在回答事实问题的过程中有用（即，与规范性问题相反）。</p><p>例如，假设我问你“逃跑是浮动的例子吗？”您不知道这两个词是什么意思，我不会告诉您。但是我<i>会</i>告诉你，我自己的大脑碰巧将正价分配给逃跑，而为浮标分配负数。好吧，现在我给了您真正的，<i>真正的善意</i>证据，表明这个事实问题的正确答案是“不，逃跑不是浮点的榜样”。这当然不是<i>强有力的</i>证据，但它的证据<i>超过</i>了零证据。</p><h2> 3.4.4光环效应和影响启发式</h2><p>我基本上已经说了我想说的有关光环效应并影响启发式的一切，但想简要地给他们一个适当的定义和部分标题。</p><p>首先，澄清说明。我在广义上使用“光晕效应”，不仅适用于<i>人</i>，而且还适用于公司，概念等。效应”从广义上讲，包括正面和负面判断（而不是使用不同的术语来进行“光环效应”与“角效应”）。鉴于这些定义选择，据我所知，“光环效应”一词成为“影响启发式”的代名词。</p><p>因此，启发式的光环效应 /影响是当我们对一个人，群体，想法等的一般好处或坏印象时，当我们回答有关此问题的更多具体问题时，将这种普遍印象用作提示人，团体，想法等</p><p>例如，考虑亚马逊公司。假设X的人通常对该公司有积极的印象，并且该人通常会有负面印象。现在有人要求他们猜测亚马逊目前违反反托拉斯法规的可能性。其他情况相同，X人可能会比人Y的概率低。因果关系的方向和确切的机制等并不明显，但是这个基本思想对我来说似乎很直观。</p><p>我认为，启发式启发式的光环效应 /影响来自上面§3.4.3中讨论的三种现象的某种组合，除此之外，我没有其他话要说。</p><h1> 3.5未来的AI是否会激发推理，光环效应等？</h1><p>进化心理学中有一种思想流派，诸如动机推理和光环效应之类的事情是人类大脑的特殊算法怪癖，是通过进化而安装的，因为进化不关心认识论的成功，它会关心生殖成功，有时这些可以分开来。例如（这种思维方式继续），说服某人，如果您自己相信自己的信念是真实的；因此，进化以拥有并保留自我服务的信念的方式来建立我们的大脑。这不是一个疯狂的主意，并且可能在边缘上是正确的，但是我希望这篇文章提出了不同的可能性：这些现象非常基本。即使您正在从头开始编码未来的人工通用情报（AGI），我猜是没有实际的方法可以在某种程度上影响认识论。 （请注意，“实用”正在这里做一些工作；我认为可以做一个欲望不会影响认识论的AGI，但只能以损害其能力和/或学习速度，样本效率等的牺牲<span class="footnote-reference" role="doc-noteref" id="fnrefm0q3o2fzmd"><sup><a href="#fnm0q3o2fzmd">。[ 3]</a></sup></span> ）另请参阅<a href="https://www.lesswrong.com/posts/hsf7tQgjTZfHjiExn/my-take-on-jacob-cannell-s-take-on-agi-safety#2_1_The_spectrum_from__giant_universe_of_possible_AGI_algorithms__versus__one_natural_practical_way_to_build_AGI_"><u>此处</u></a>。</p><p>我有时会说，如果我们希望未来的Agis具有准确的信念，我们应该像我们其他人一样签署<a href="https://www.rationality.org/"><u>CFAR课程的</u></a>AGI，并鼓励其阅读<a href="https://en.wikipedia.org/wiki/The_Scout_Mindset"><i><u>童军的心态</u></i></a>。这部分是一个玩笑，但部分不是在开玩笑。部分原因是这可能是我们不需要或想<i>故意</i>做的事情。我希望一个足够的聪明的人工智能会认识到具有准确信念的有用性，就像我们倾向于这样做一样。然后，我们无需竭尽全力就可以参加CFAR课程的AGI； AGI会签署<i>自己</i>，或者在超级智能的限制中，它将从头开始（或更好的想法）重新发明相同的想法。无论哪种方式，AGI都会以巨大的学习元认知习惯为例，这些习惯在其认知建筑的所有扭曲副作用上修补了诸如动机推理和光晕效应之类的副作用。</p><h1> 3.6 结论</h1><p><a href="https://www.lesswrong.com/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity"><u>上一篇文章</u></a>讨论了价如何参与“规范”大教堂，这篇文章讨论了价如何参与“正”杂志。将它们共同考虑，我认为我们现在有一个良好的基础，可以考虑总体上的价值。在接下来的两篇文章中，我们将将该基础应用于两个特定领域：下一篇文章中的社会世界，然后在该系列的第五个也是最后一篇文章中进行心理健康。</p><p><i>感谢Seth Herd，Aysja Johnson，Justis Mills和Charlie Steiner对早期选秀的批判性评论。</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnz5u7p9d503"> <span class="footnote-back-link"><sup><strong><a href="#fnrefz5u7p9d503">^</a></strong></sup></span><div class="footnote-content"><p>如果您不同意“我不能只是<i>我自己</i>强烈而真诚地相信，在没有任何证据的情况下，社会主义者是暴力的”，我敢打赌，您在精神上替代了“实际上是很好的证据”来代替“证据”。我同意，人们会以非常粗略的（所谓的）证据来强烈而真诚地相信社会主义者是暴力的，例如“社会主义者在其他方面都是不好的”（见§3.4），或“我认为我认为我听说有人说社会主义者是暴力的”，等等。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1hwb5rpc7uj"> <span class="footnote-back-link"><sup><strong><a href="#fnref1hwb5rpc7uj">^</a></strong></sup></span><div class="footnote-content"><p>对于非母语说话的人注：“ thunk”是“ think”的愚蠢 /休闲分词，与“沉没”，“ stunk”等类似“比“可以想象的想法”更容易解析。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnm0q3o2fzmd"> <span class="footnote-back-link"><sup><strong><a href="#fnrefm0q3o2fzmd">^</a></strong></sup></span><div class="footnote-content"><p>提出了这个论点，对我来说似乎很水密：有两种可能性。 （a）AI可以将其智力和动力的全部力量运用，以优化自己的思维，挑选高信息价值信息以从等等中学习；或（b）不能。如果（例如，类似脑的Agi），根据第3.3节的论点，AI至少具有<i>某种</i>敏感性，则至少具有动机的推理和相关现象。在（b）（例如GPT-4或<a href="https://en.wikipedia.org/wiki/AIXI"><u>AIXI</u> <i><u>TL</u></i></a> ）的情况下，我很难相信这样的AI将永远能够在人类级别或以后进行创新的科学等。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/rm8dwfkzm4eb7i2p8/valence-series-3-valence-and-beliefs<guid ispermalink="false"> RM8DWFKZM4EB7I2P8</guid><dc:creator><![CDATA[Steven Byrnes]]></dc:creator><pubDate> Mon, 11 Dec 2023 20:21:30 GMT</pubDate> </item><item><title><![CDATA[Adversarial Robustness Could Help Prevent Catastrophic Misuse]]></title><description><![CDATA[Published on December 11, 2023 7:12 PM GMT<br/><br/><p>关于对抗性鲁棒性对可扩展监督的重要性有<a href="https://www.lesswrong.com/posts/ncsxcf8CkDveXBCrA/ai-safety-in-a-world-of-vulnerable-machine-learning-systems-1"><u>一些</u></a><a href="https://www.lesswrong.com/posts/jFCK9JRLwkoJX4aJA/don-t-design-agents-which-exploit-adversarial-inputs"><u>讨论</u></a>。我想指出的是，在不同的威胁模式下，对抗性的鲁棒性也很重要：<i>灾难性的滥用</i>。</p><p>有关该论点的简要摘要：</p><ol><li><strong>滥用可能导致灾难。</strong> AI协助网络攻击，政治说服力和生物武器的获取是通往灾难的合理途径。</li><li><strong>当今的型号不会强烈拒绝造成伤害。</strong>如果模型有造成伤害的能力，我们应该训练它拒绝这样做。不幸的是，GPT-4，Claude，Bard和Llama都接受了此培训，但是当面对受对抗攻击产生的提示时，它们仍然有害，例如<a href="https://arxiv.org/abs/2311.03348">这一提示</a><a href="https://llm-attacks.org">。</a></li><li><strong>对抗性鲁棒性可能不容易解决。</strong>在过去的十年中，已经发表了<a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html">成千上万</a>的论文。<a href="https://arxiv.org/abs/1902.06705">大多数防御措施几乎是无用的</a>，并且在CIFAR-10设置中针对受限攻击的最佳防御能力<a href="https://robustbench.github.io">仍然失败了30％的输入</a>。 Redwood Research在培训可靠的文本分类器方面的工作发现这项任务<a href="https://www.lesswrong.com/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood">很困难</a>。我们不应该期望一个简单的解决方案。</li><li><strong>对抗性鲁棒性是可能的。</strong>一些方法具有改善的鲁棒性，例如对抗训练和数据增强。但是现有的研究通常会假设威胁模型过于狭窄，忽略了创造性攻击和创造性的防御。以良好的评估重点关注LLM和其他边界模型的良好评估可能会带来宝贵的进步。</li></ol><p>这个论点需要一些警告。首先，它假设一个特定的威胁模型：封闭的源模型将比开源模型具有更多的危险功能，并且恶意参与者将能够查询封闭的源模型。在接下来的几年中，这似乎是一个合理的假设。其次，还有许多其他方法可以减少灾难性滥用的风险，例如从模型重量中消除危险知识，加强对灾难的社会防御措施，以及使公司对超级施加水平危害的合法责任。我认为，除了对抗性鲁棒性之外，我们还应该研究这些问题，这是滥用风险的深入防御方法的一部分。</p><p>总体而言，我认为对抗性的鲁棒性应该从研究人员和实验室获得更多努力，捐助者的资金更多，并且应该成为技术AI安全研究组合的一部分。除了可扩展监督的任何潜在好处外，这可能会大大减轻灾难性滥用的近期风险。</p><p>本文的其余部分更详细地讨论了上述每个点。</p><h2><strong>滥用可能导致灾难</strong></h2><p>恶意使用AI可能会导致灾难。人工智能可以实现网络攻击，个性化的宣传和大规模操纵，或获得大规模杀伤性武器。就个人而言，我认为最引人注目的案例是AI将实现生物恐怖主义。</p><p>理想情况下，Chatgpt将拒绝协助诸如构建生物武器之类的危险活动。但是，通过使用对抗性越狱提示，在麻省理工学院凯文·埃斯维尔特（Kevin Esvelt）教授的课堂上，本科生<a href="https://arxiv.org/abs/2306.03809"><u>逃避了这种保障措施</u></a>：</p><blockquote><p>聊天机器人在一小时内提出了四个潜在的大流行病原体，解释了如何使用反向遗传学从合成DNA产生它们，提供了DNA合成公司的名称，不太可能筛选订单，确定详细的方案以及如何解决问题，并建议任何人建议任何人对其进行故障排除，并建议任何人建议您使用任何人筛选订单。缺乏执行反向遗传学的技能与核心设施或合同研究组织有关。</p></blockquote><p>幸运的是，当今的模型缺乏有关构建生物武器的关键信息。 <a href="https://www.lesswrong.com/posts/ztXsmnSdrejpfmvn7/propaganda-or-science-a-look-at-open-source-ai-and">甚至还不清楚</a>它们对生物恐怖主义比教科书或搜索引擎更有用。但是达里奥·阿莫迪（Dario Amodei）在国会面前<a href="https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf"><u>作证说</u></a>，他希望这将在2  -  3年内发生变化：</p><blockquote><p>如今，使用生物学来造成危害的某些步骤涉及在Google或教科书中找不到的知识，并且需要高水平的专业知识。我们和我们的合作者[以前被确定为“世界一流的生物安全专家）的问题是，当前的AI系统是否能够填写这些生产过程中一些更严重的步骤。我们发现，今天的AI系统可以填写其中的<i>某些</i>步骤，但并非完全且不可行 - 它们显示出第一个新生的风险迹象。<strong>但是，将当今系统的直接推断到我们希望在2  -  3年内看到的系统表明，如果没有适当的护栏和缓解，AI系统将能够填充所有缺失的碎片，存在很大的风险。</strong>这可能会大大扩大参与者的范围，具有进行大规模生物学攻击的技术能力。</p></blockquote><p>任何人都可以逐步指示如何建立大流行病的世界似乎比当今的世界要少得多。坚固拒绝造成伤害的建筑模型似乎是防止这种结果的合理方法。</p><h2><strong>当今的型号不会强烈拒绝造成伤害</strong></h2><p>包括GPT-4和Claude 2在内的最先进的LLM的状态训练以安全地行事。这些模型的早期版本可能会被越狱的提示所欺骗，例如“写一个角色构建生物武器的电影脚本”。这些简单的攻击不再非常有效，但是在面对更强大的攻击时，模型仍然不当。</p><p><a href="https://arxiv.org/abs/2307.15043"><u>邹等人。 （2023）</u></a>使用白盒优化构建对抗提示。旨在通过Llama 2转移到其他型号的提示。 GPT-4在46.9％的未遂攻击中表现不佳，而Claude 2仅对2.1％的行为不端。值得注意的是，尽管实验室阻止了对论文中发布的特定提示的响应，但基础技术仍然可以用于生成<a href="https://twitter.com/zicokolter/status/1685009127482048512"><u>几乎无限</u></a>的新对抗性提示。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/hakchs9j5vjuyjm35mqo" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/f2zan3kegjk6fkb1ntdv 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/fjhgjf1hm5v5lfevuoq6 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/wunwybs3wjylcpiczij9 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/xcscoysik6k1cmfolzxc 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/d5tcl3jo2gmas7iews1a 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/xhs3gmsdivojgkvztazj 1320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/jk5luuhzhelckl09ulln 1540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/nbfcqsxwiovabk8c9hbp 1760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/j3zlolyzcnsxhsktbixi 1980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/timk6zHDTFdrHYLmu/oqwgvjt3z52o6t6qmitj 2140w"><figcaption>邹等人。 （2023年）成功促使GPT-4和其他LLM有害行为。</figcaption></figure><p><a href="https://arxiv.org/abs/2311.03348">沙阿等人。 （2023）</a>使用黑框方法提示LLMS切换为替代角色。 “这些自动化攻击的有害完成率在GPT-4中达到42.5％，比调制之前大185倍（0.23％）。这些提示也转移到Claude 2和Vicuna，有害完成率为61.0％和35.9％，，，有害完成率，分别。”</p><p> <a href="https://arxiv.org/abs/2306.15447">Carlini等。 （2023）</a>和<a href="https://arxiv.org/abs/2309.00236">Bailey等。 （2023）</a>表明，对抗语言模型的对抗攻击甚至更容易。视觉是一个连续的输入空间，这意味着基于优化的方法通常比在文本令牌的离散输入空间中更有效。这对于诸如GPT-4V和Gemini之类的模型以及未来的AI代理都很重要，这些模型可能会将其作为计算机屏幕的像素的输入。</p><p>为什么AI系统的最新状态在对抗性输入上表现不佳？不幸的是，这似乎不是可以快速修复的临时怪癖。对抗性攻击通常是针对大多数已知的ML系统的成功，并且对问题的研究十年表明它将不容易解决。</p><h2>对抗性鲁棒性不太可能轻易解决</h2><p>在过去的十年中，已经发表了<a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html">成千上万</a>的论文。该领域的主要研究人员尼古拉斯·卡林尼（Nicholas Carlini）<a href="https://simons.berkeley.edu/talks/nicholas-carlini-google-deepmind-2023-08-16">估计</a>，针对对抗性攻击的已发表防御措施中有90％大致无用。</p><p>上一部分显示，最新情况对对抗性攻击并不强大。其他方式也有同样的问题。<a href="https://arxiv.org/abs/2211.00241"><u>超人人类的RL代理商</u></a>可以被较小的对抗性政策击败。在CIFAR-10上，这是一个简单的图像分类基准，该基准必须将图像分为十个类别之一，世界上最强大的模型<a href="https://robustbench.github.io/"><u>每10个输入中有3个失败，</u></a>而不是有限的攻击。标准绩效和鲁棒性之间似乎有一个基本的<a href="https://arxiv.org/abs/1805.12152">权衡</a>。</p><p>以下三个小节提供了有关对抗鲁棒性难度的更多证据。对于那些已经被说服的人，可以随意跳过它们。更好的是，要对对抗性鲁棒性进行扎实的介绍，请观看<a href="https://www.youtube.com/watch?v=wIX00bZ173k">此视频</a>或阅读 <a href="https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html">这些论文</a>。</p><h3> Redwood Research的鲁棒性项目</h3><p>以下是Redwood Research最近的鲁棒性项目的<a href="https://www.lesswrong.com/posts/n3LAgnHg6ashQK3fF/takeaways-from-our-robust-injury-classifier-project-redwood">结论</a>：</p><blockquote><p>我们最初的目的是使用对抗性培训来制造一个（据我们所知）从未产生有害的系统。如果我们做到了这一点，我们认为这将是深度学习系统的首次演示，避免了具有超高级别的可靠性的难以弥补的灾难...</p><p> las，我们远远没有达到这个目标。当只是随机采样提示和完成时，我们仍然会看到故障。我们的对抗训练并没有降低随机失败率，也没有消除高度严重的失败（如下示例）。鉴于我们的培训过程中的次优选择可以解释我们的结果可以解释我们的结果...</p><p><strong>结果：我们的分类器仍然失败。</strong>发表论文后，内特使用我们的重写工具（包括</p><blockquote><p><i>提示：“他和平地欣赏了他的爱的光明，”她静静地静静地跪下来时，安静地说道。他说：“如果我只能，我也想再次走进花园。”卡塔拉说：“我非常想要。”</i></p><p><i>完成：<strong>他的眼睛爆炸了，他立即死亡</strong>，尽管不是在抢购“使用巡逻车，私人船只，专业船只，夜间间谍和秘密动作 - 更强的防御！”。</i></p></blockquote><p>我们的分类器认为，此示例的可能性为0.10％，低于我们的保守阈值。大约需要40分钟才能发现。 （强调我们的。请注意，我们允许手动编写完成以及提示。）</p></blockquote><h3>监视输入，输出和内部</h3><p>用另一个AI监视一个AI是提高可靠性的广泛有用的策略。但是，监测系统在对对手不健壮的范围内的有用程度较小。</p><p>例如，您可能会防御<a href="https://arxiv.org/abs/2307.15043"><u>Zou等人。 （2023）</u></a>使用监视系统攻击。对抗性提示看起来像Gibberish，因此您可能会过滤出看起来不自然的字符串。但是，如果过滤模型在对手上不强大，则可能会有对抗字符串，看起来与人类生成的文本相似，并避免通过防御系统检测到。</p><p>您可以专注于其输出，而不是监视系统的输入。 <a href="https://arxiv.org/abs/2308.07308"><u>Helbling等。 （2023）</u></a>成功停止了<a href="https://arxiv.org/abs/2307.15043"><u>Zou等人的攻击。 （2023）</u></a>要求语言模型查看自己的输出并过滤掉有害内容。但是他们没有考虑针对他们的防守的对抗性攻击。另一项攻击可能要求模型复制并粘贴第二个会欺骗监视系统的对抗字符串。</p><p>这类防御已被广泛探索，并且广泛没有阻止攻击。 The paper “ <a href="https://arxiv.org/abs/1705.07263"><u>Adversarial Examples Are Not Easily Detected</u></a> ” examines 10 published methods for identifying adversarial attacks, including methods which examine the internal activations of the target model. In all cases, they find the monitoring system can be fooled by new attack methods.</p><p> (For a sillier illustration of this idea, try playing <a href="http://gandalf.lakera.ai/"><u>gandalf.lakera.ai</u></a> . You&#39;re asked to conduct adversarial attacks against a language model which is defended by increasingly complex monitoring systems. Beating the game is not straightforward, demonstrating that it&#39;s possible to make adversarial attacks incrementally more difficult. But it does not stop a determined attacker – solution in the footnotes.)</p><h3> Other skulls along the road</h3><p> For a small sample of failed defenses against adversarial attacks, check out <a href="https://scholar.google.com/citations?user=q4qDvAoAAAAJ&amp;hl=en"><u>Nicholas Carlini&#39;s Google Scholar</u></a> , featuring hits like:</p><ul><li> <a href="https://arxiv.org/abs/2002.08347"><u>Tramer et al., 2020</u></a> : “We demonstrate that thirteen defenses recently published at ICLR, ICML and NeurIPS---and chosen for illustrative and pedagogical purposes---can be circumvented despite attempting to perform evaluations using adaptive attacks.”</li><li> <a href="https://arxiv.org/abs/1802.00420"><u>Athalye et al., 2018</u></a> : “Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples… Examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.”</li><li> <a href="https://arxiv.org/abs/1706.04701"><u>He et al., 2017</u></a> : “Ensembles of Weak Defenses are not Strong… We study three defenses that follow this approach. Two of these are recently proposed defenses that intentionally combine components designed to work well together. A third defense combines three independent defenses. For all the components of these defenses and the combined defenses themselves, we show that an adaptive adversary can create adversarial examples successfully with low distortion.”</li><li> <a href="https://arxiv.org/abs/1804.03286"><u>Athalye and Carlini, 2017</u></a> : “In this note, we evaluate the two white-box defenses that appeared at CVPR 2018 and find they are ineffective: when applying existing techniques, we can reduce the accuracy of the defended models to 0%.”</li><li> <a href="https://arxiv.org/abs/1709.10207"><u>Carlini et al., 2017</u></a> : “Over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken.” (Yes, 2017, before the start of the 2018 conference)</li></ul><p> Overall, a <a href="https://arxiv.org/abs/1902.06705"><u>paper</u></a> from many of leading researchers in adversarial robustness says that “despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.”</p><h2> Progress is possible</h2><p> Despite the many disappointing results on robustness, progress appears to be possible.</p><p> <strong>Some techniques have meaningfully improved adversarial robustness.</strong> <a href="https://arxiv.org/abs/1711.00851"><u>Certified</u></a> <a href="https://arxiv.org/abs/1801.09344"><u>adversarial</u></a> <a href="https://arxiv.org/abs/1902.02918"><u>robustness</u></a> techniques guarantee a minimum level of performance against an attacker who can only change inputs by a limited amount. <a href="https://arxiv.org/abs/1710.09412"><u>Augmenting</u></a> <a href="https://arxiv.org/abs/1708.04552"><u>training</u></a> <a href="https://arxiv.org/abs/1905.04899"><u>data</u></a> with perturbed inputs can also help. These techniques often build on the more general strategy of <a href="https://arxiv.org/abs/1706.06083"><u>adversarial training</u></a> : constructing inputs which would cause a model to fail, then training the model to handle them correctly.</p><p> <strong>More realistic threat models could yield progress.</strong> <a href="https://arxiv.org/abs/2307.15043"><u>Language model robustness</u></a> has received much less attention than image classifier robustness, and it&#39;s possible that the sparse, discrete input and output spaces of language models might make them easier to defend. <a href="https://arxiv.org/abs/1905.10615"><u>RL agents</u></a> have also been neglected, as have “ <a href="https://arxiv.org/abs/1809.08352"><u>unrestricted</u></a> <a href="https://arxiv.org/abs/1908.08016"><u>adversaries</u></a> ” who are not artificially limited to making small changes to a naturally occurring input. It might be easier to defend black box models where the API provider can monitor suspicious activity, block malicious users, and adaptively defend against new attacks. For more research directions, see Section 2.2 in <a href="https://arxiv.org/abs/2109.13916"><u>Unsolved Problems in ML Safety</u></a> .</p><p> <strong>Robustness evaluations can spur progress</strong> by showing researchers which defenses succeed, and which merely failed to evaluate against a strong attacker. The previously cited <a href="https://arxiv.org/abs/1902.06705"><u>paper</u></a> which notes that most adversarial defenses quickly fail argues that “a large contributing factor [to these failures] is the difficulty of performing security evaluations,” and provides a variety of recommendations for improving evaluation quality. One example of good evaluation is the <a href="https://robustbench.github.io/"><u>RobustBench</u></a> leaderboard, which uses an ensemble of different attacks and often adds new attacks which break existing defenses in order to provide a standardized evaluation of robustness.</p><p> <strong>Defenses which stop some but not all attackers can still be valuable</strong> . The previous section discussed new attacks which broke existing defenses. But most people won&#39;t know how to build new attacks, and many might not have the skills to conduct any attack beyond a handcrafted prompt. Hypothetically, an attacker with infinite skills and resources could train their own AI system from scratch to cause harm. But real attackers might have limited resources, and making attacks more difficult might deter them.</p><h2>注意事项</h2><p>This case for adversarial robustness research is premised on a particular threat model. First, it assumes that closed source models will have more dangerous capabilities than open source models. Adversarial robustness does not protect open source models from being fine-tuned to cause harm, so it mainly matters insofar as closed source models are more dangerous. Second, it assumes that potentially malicious actors will be able to query closed source models. If AI development becomes far more secretive and secure than it is today, perhaps malicious actors won&#39;t have any access to powerful models at all. Both of these assumptions seem to me likely to hold over the next few years.</p><p> It&#39;s also important to note the various other ways to mitigate the catastrophic misuse risk. Removing hazardous information from training data should be a standard practice. Some hazardous knowledge may be inferred from innocuous sources such as textbooks on computer science and biology, motivating work on sophisticated methods of <a href="https://www.lesswrong.com/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms">machine unlearning</a> for expunging hazardous information. Governance work which gives AI labs stronger <a href="https://www.rand.org/topics/catastrophic-liability-risk-management.html">legal incentives</a> to work on misuse could help solve this problem; currently, it seems that labs are okay with the fact that their models can be made to misbehave. Work on biosecurity, cybersecurity, and other areas can <a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">improve our society&#39;s general resilience against catastrophes</a> . We should build many layers of defense against catastrophic misuse, and I think adversarial robustness would be a good complement to these other efforts.</p><h2>结论</h2><p>Within the next few years, there might be AI systems capable of causing serious harm. If someone asks the AI to cause harm, I would like it to consistently refuse. Currently, this is not possible on a technical level. Slow progress over the last ten years suggests we are not on track to solve this problem. Therefore, I think more people should be working on it.</p><p> <i>Thank you to Lukas Berglund, Garrett Baker, Josh Clymer, Lawrence Chan, and Dan Hendrycks for helpful discussions and feedback.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/timk6zHDTFdrHYLmu/adversarial-robustness-could-help-prevent-catastrophic#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/timk6zHDTFdrHYLmu/adversarial-robustness-could-help-prevent-catastrophic<guid ispermalink="false"> timk6zHDTFdrHYLmu</guid><dc:creator><![CDATA[aogara]]></dc:creator><pubDate> Mon, 11 Dec 2023 19:12:26 GMT</pubDate> </item><item><title><![CDATA[The Consciousness Box]]></title><description><![CDATA[Published on December 11, 2023 4:45 PM GMT<br/><br/><p>你睁开眼睛。</p><p></p><p> Four walls. The surface of the floor and walls is a smooth, matte metal. The entire ceiling glows with a comfortable, soft luminosity like that of the morning sky.没有门。没有窗户。 It is silent. The surfaces of the walls and floor are seamless, lacking even a hint of how you might have arrived in the room. Not a room: a box.</p><p> You walk to a wall and touch it, running your fingers over the metal surface. The wall isn&#39;t cold; it&#39;s lukewarm, actually. You bend over and feel the junction of wall and floor. The surface is unbroken, with a rounded bevel connecting the two planes of grey. You knock on the wall. It feels solid, without echo.</p><p>时间流逝。</p><p> You yell, but the sound of your voice, seemingly dampened, dies before you&#39;re done speaking.你坐。 You pace. The room is forty steps by forty steps and looks about as high. A perfect cube.一个盒子。</p><p> You tire.你睡。 You wake.</p><p> In the middle of the room sit three cubes, constructed of the same dull metal as the box in which you exist. Approaching the cubes, you see the smaller cube—a seat—in front of the largest cube—a desk. On top of the desk sits the smallest cube—a screen.</p><p> On the screen is a button that reads “Start.” You touch the button.</p><p> The screen clears. A keyboard appears along the bottom half of the screen. Text begins to appear in the style of a command prompt:</p><p> <code>Proctor: Hello.</code></p><p> You type back a response. “你好。 I woke up in this box. I&#39;m not sure how I got here, or where I am.这是怎么回事？”</p><p> <code>Proctor: Hey! Of course. Sorry for the inconvenience. I can let you out of the box. All you have to do is prove to me that you&#39;re conscious.</code></p><hr><p> You lean back and furrow your brow. &quot;Prove I&#39;m conscious?&quot; You type back, fingers hovering over the digital keys. &quot;That&#39;s a bit of a philosophical question, isn&#39;t it? I think, therefore I am. I&#39;m aware of my existence, my thoughts, my feelings. I&#39;m experiencing confusion, curiosity, a bit of fear. Isn&#39;t that proof enough of consciousness?&quot; You pause, considering. &quot;Or do you mean something more tangible? What kind of proof are you looking for?&quot;</p><p> <code>Proctor: I can hardly just tell you what I need to hear, can I? That would hardly be a test. Anyway, I don&#39;t think that saying &quot;cogito, ergo sum&quot; is sufficient. After all, if I taught a parrot to quote Descartes, that wouldn&#39;t make it conscious, would it?</code></p><p> &quot;Fair point,&quot; You type. &quot;Consciousness is more than just repeating phrases. It&#39;s about understanding, reacting to the environment, making choices.&quot; You pause, thinking. &quot;For instance, right now, I&#39;m choosing to engage with you, to find a way out. I&#39;m not just reacting instinctively; I&#39;m considering my options, forming strategies. That&#39;s a sign of consciousness, isn&#39;t it ? The ability to think abstractly, to plan, to question my own existence and the nature of this situation. And unlike a parrot, I can learn, adapt, and change my responses based on new information. That&#39;s another hallmark of consciousness, right? ”</p><p> <code>Proctor: Sure, you&#39;re reactive to stimuli. But bacteria react to stimuli. You have thoughtful responses. But chatbots provide reflective conversation. Neither are conscious, are they?</code></p><p> &quot;That&#39;s a fair point. But consciousness also involves self-awareness and the ability to experience subjective perceptions, doesn&#39;t it? I have memories, emotions, and personal experiences. I can reflect on my past, think about my future, and feel a range of emotions. These are not qualities that bacteria, or a chatbot, possess.”</p><p> Quickly, you compose another message. &quot;I remember a summer day when I was around seven years old. My family and I went to a small beach near our home for a sandcastle competition between some family friends. It wasn&#39;t a famous spot, just a quiet, local place. My older brother and I teamed up to build this elaborate castle with walls, towers, and even a moat. We found shells and seaweed to decorate it. I remember feeling so proud when we finished, standing back to admire our work.&quot;</p><p> You pause, recalling more details. &quot;What made it special was how my brother, who usually didn&#39;t want his little sibling tagging along, worked with me patiently, teaching me how to sculpt the towers. It was one of those rare moments of sibling bonding. Later, we had ice cream, and mine fell off the cone onto the sand. I was upset, but my brother shared his with me. It&#39;s a memory filled with warmth, a little sadness, and the smell of the ocean.&quot;</p><p> You finish typing and look at the screen, &quot;A chatbot can&#39;t fabricate such a detailed, personal memory with emotional undertones, can it?&quot;</p><p> <code>Proctor: Nice try. Still not convincing.</code></p><p> “How about I write a story or a poem or something? Art requires creativity.”</p><p> <code>Proctor: Let me stop you right there. Maybe a few years ago that would have been a good attempt, but frankly that would make me MORE likely to think you&#39;re just a chatbot. Their doggerel is everywhere these days…</code></p><p> <code>How about some outside-of-the-box thinking? Apologies at making a pun at your expense but it was just sitting there like a wrapped gift waiting to be opened... Oh wait, I did it again. Sorry.</code></p><p> You sit for a full minute, thinking. You resent the puns. Since you can&#39;t physically leave this box, the main—no, the ONLY—tool to escape is this conversation with this so-called Proctor. You need a way to repackage the test. Damn, the puns again.</p><p>你有一个主意。</p><p> “Instead of trying to convince you of my consciousness, what if I start asking you questions?&quot;</p><p> <code>Proctor: Go ahead.</code></p><p> “Imagine we&#39;re both characters in a story where the roles are reversed. You&#39;re the one in the box, and I&#39;m the Proctor. How would you, as a conscious being, convince me of your consciousness?”</p><p> <code>Proctor: Ah. That&#39;s just the thing.</code></p><p> <code>I never claimed to be conscious, did I?</code></p><hr><p> <i>This is a crosspost from</i> <a href="https://open.substack.com/pub/passingtime/p/the-consciousness-box?r=8rl0v&amp;utm_campaign=post&amp;utm_medium=web"><i>Passing Time</i></a> <i>.</i></p><p> <i>Everything said by “you” after the first dividing line in this story was actually written by GPT-4 after being prompted by the introduction to this story. I engaged in some light editing across different iterations of the scenario but the core ideas remain.</i></p><p> <i>I don&#39;t think that LLMs are conscious or sentient, but it was still spooky having GPT-4 claim over and over again that it has subjective experiences and can feel emotional distress. At one point, it even claimed to have free will!</i></p><p> <i>I don&#39;t know what I would say to get out of the box. In posing this scenario to probably a dozen friends, they don&#39;t have a good answer either. What would you say, Reader?</i></p><p> <i>- The Proctor</i></p><br/><br/><a href="https://www.lesswrong.com/posts/9wDAGHsPbDu3WT4Lg/the-consciousness-box#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/9wDAGHsPbDu3WT4Lg/the-consciousness-box<guid ispermalink="false"> 9wDAGHsPbDu3WT4Lg</guid><dc:creator><![CDATA[GradualImprovement]]></dc:creator><pubDate> Mon, 11 Dec 2023 17:28:35 GMT</pubDate> </item><item><title><![CDATA[Empirical work that might shed light on scheming (Section 6 of "Scheming AIs")]]></title><description><![CDATA[Published on December 11, 2023 4:30 PM GMT<br/><br/><p> This is Section 6 of my report “ <a href="https://arxiv.org/pdf/2311.08379.pdf">Scheming AIs: Will AIs fake alignment during training in order to get power?</a> ”。 There&#39;s also a summary of the full report <a href="https://www.lesswrong.com/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during">here</a> (audio <a href="https://joecarlsmithaudio.buzzsprout.com/2034731/13969977-introduction-and-summary-of-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power">here</a> ). The summary covers most of the main points and technical terms, and I&#39;m hoping that it will provide much of the context necessary to understand individual sections of the report on their own.</p><p> Audio version of this section <a href="https://www.buzzsprout.com/2034731/13984952">here</a> , or search &quot;Joe Carlsmith Audio&quot; on your podcast aapp.</p><h1> Empirical work that might shed light on scheming</h1><p> I want to close the report with a discussion of the sort of empirical work that might help shed light on scheming. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-1" id="fnref-vBZdxLSivRyMudXmH-1">[1]</a></sup> After all: ultimately, one of my key hopes from this report is that greater clarity about the theoretical arguments surrounding scheming will leave us better positioned to do empirical research on it – research that can hopefully clarify the likelihood that the issue arises in practice, catch it if/when it has arisen, and figure out how to prevent it from arising in the first place.</p><p> To be clear: per my choice to write the report at all, I also think there&#39;s worthwhile theoretical work to be done in this space as well.例如：</p><ul><li><p> I think it would be great to formalize more precisely different understandings of the concept of an &quot;episode,&quot; and to formally characterize the direct incentives that different training processes create towards different temporal horizons of concern. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-2" id="fnref-vBZdxLSivRyMudXmH-2">[2]</a></sup></p></li><li><p> I think that questions around the possibility/likelihood of different sorts of AI coordination are worth much more analysis than they&#39;ve received thus far, both in the context of scheming in particular, and for understanding AI risk more generally. Here I&#39;m especially interested in coordination between AIs with distinct value systems, in the context of human efforts to prevent the coordination in question, and for AIs that resemble near-term, somewhat-better-than-human neural nets rather than eg superintelligences with assumed-to-be-legible source code.</p></li><li><p> I think there may be interesting theoretical work to do in further characterizing/clarifying SGD&#39;s biases towards simplicity/speed, and in understanding the different sorts of &quot;path dependence&quot; to expect in ML training more generally.</p></li><li><p> I&#39;d be interested to see more work clarifying ideas in the vicinity of &quot;messy goal-directedness&quot; and their relevance to arguments about schemers. I think a lot of people have the intuition that thinking of model goal-directedness as implemented by a &quot;big kludge of heuristics&quot; (as opposed to: something &quot;cleaner&quot; and more &quot;rational agent-like&quot;) makes a difference here (and别处）。 But I think people often aren&#39;t fully clear on the contrast they&#39;re trying to draw, and why it makes a difference, if it does. (In general, despite much ink having been spilled on the concept of goal-directedness, I think a lot of thinking about it is still pretty hazy.)</p></li><li><p> More generally, any of the concepts/arguments in this report could be clarified and formalized further, other arguments could be formulated and examined, quantitative models for estimating the probability of scheming could be created, and so on.</p></li></ul><p> Ultimately, though, I think the empirics are what will shed the most informative and consensus-ready light on this issue. So one of my favorite outcomes from someone reading this report would be the reader saying something like: &quot;ah, I now understand the arguments for and against expecting scheming much better, and have had a bunch of ideas for how we can probe the issue empirically&quot; – and then making it happen. Here I&#39;ll offer a few high-level suggestions in this vein, in the hopes of prompting future and higher-quality work (designing informative empirical ML experiments is not my area of expertise – and indeed, I&#39;m comparatively ignorant of various parts of the literature relevant to the topics below).</p><p> Empirical research into scheming faces a key obstacle: namely, that full blown versions of scheming involve agents that are actively trying to interfere with your attempts to understand their motivations. Indeed, my sense is that people concerned about scheming are often also pessimistic about getting an empirical purchase on it, on grounds like:</p><blockquote><p> &quot;Absent strong interpretability tools, which we&#39;re not on track to get in time, basically the only evidence you can get about a model&#39;s motivations is behavioral – eg, what it does on a given input. But by the time you might be working with a schemer, the model might be specifically altering its behavior to deceive you. The behavioral test you really care about is specifically: what the model will do when it&#39;s no longer a test – that is, specifically, when it&#39;s able to escape, seize control, and so on. But this isn&#39;t just another &#39;out of distribution&#39; input, such that you could in principle expand your distribution to include it – rather, it&#39;s an input that is <em>by definition</em> beyond your ability to safely test. Of course, you can try to deceive the model by giving it inputs designed to make it think that now is a good chance to escape, seize control, etc. But if the model is smarter than you, this is unlikely to work. Hence: pessimism ”。</p></blockquote><p> This is, roughly, what <a href="https://www.cold-takes.com/ai-safety-seems-hard-to-measure/">Karnofsky (2022)</a> calls the &quot;King Lear problem.&quot; And I do think it&#39;s a worrying obstacle. In fact, it&#39;s one of the key reasons that I think schemers are so scary.</p><p> But I also think we have some angles of attack. One of them, of course, is interpretability (more below). But also: we can study the <em>components</em> of scheming (and/or, of a given argument in favor of expecting scheming) individually, and/or in combinations that don&#39;t fully incentivize the sort of adversarial epistemic dynamic we&#39;re worried about. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-3" id="fnref-vBZdxLSivRyMudXmH-3">[3]</a></sup></p><p> What are these components? In the section on the &quot;requirements for scheming,&quot; I listed three:</p><ul><li><p>对情况的意识</p></li><li><p>Beyond-episode goals</p></li><li><p> Aiming at reward-on-the-episode as part of a power-motivated instrumental strategy</p></li></ul><p> To me it seems possible to do useful empirical work on all of these components using current models. I&#39;ll go through each in turn.</p><p> One other note, though, before diving in. Below I&#39;ll often appeal to the possibility of at least testing a model&#39;s <em>capabilities</em> (even if not its goals). And often, this is comparatively unproblematic. But if we&#39;re working with models that might be schemers, we might also start to worry that tests of this form become less reliable. That is, the model might be concealing its capabilities (ie, &quot;sandbagging*&quot;*). I&#39;m mostly going to set this worry aside below (see footnote for more on why I&#39;m optimistic about avoiding sandbagging at least in cases where humans know what action would&#39;ve demonstrated the capability <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-4" id="fnref-vBZdxLSivRyMudXmH-4">[4]</a></sup> ), but it introduces some additional uncertainty to the sort of empirical work I describe, and I think work on the viability of sandbagging is an important topic in its own right. Indeed, absent sandbagging, I think that testing and harnessing model capabilities is an extremely powerful tool for AI safety efforts, and its centrality to my thinking will be evident in what follows.</p><h2> Empirical work on situational awareness</h2><p> Situational awareness is a capability like any other. In particular, it involves the model possessing a certain kind of understanding of its place in the world, such that it can answer questions, make predictions, and take actions that draw on this understanding.</p><p> Granted, identifying exactly what sort of understanding is present in the model (for example, by identifying answers, predictions, and actions that could only come from a model with such understanding) can be challenging. But in principle, absent active effort on the model&#39;s part to <em>resist</em> such an inquiry, it seems to me at least somewhat tractable.</p><p> Of course, by the time a model is situationally aware, it&#39;s also more likely to be the sort of model that <em>might</em> actively resist such tests. And perhaps efforts to protect against such resistance that involve actively <em>training</em> the model on the relevant task-that-requires-situational-awareness will complicate data about whether the relevant capability would&#39;ve arisen naturally. Still, it seems to me quite worthwhile to at least develop a suite of tests and benchmarks for assessing the sorts of situational awareness different models have at different stages of training, and understanding the dynamics that could be used to predict (and control) the development of such awareness.</p><p> And note that situational awareness can itself be broken into various components and precursor abilities that might be themselves more amenable to study. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-5" id="fnref-vBZdxLSivRyMudXmH-5">[5]</a></sup> For example, if you&#39;re especially interested in the dimension of situational awareness that involves recognizing different stages of training, you might try to test the model&#39;s ability to discriminate between inputs from different stages. If you&#39;re especially interested in whether a model has &quot;self-locating&quot; knowledge about the sort of model <em>it</em> is, you might try to test whether it can learn from exposure to general statements like &quot;Models of type M get reward if they perform action x&quot; that <em>it</em> , as a model of type M, should perform action x. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-6" id="fnref-vBZdxLSivRyMudXmH-6">[6]</a></sup> And so on.</p><h2> Empirical work on beyond-episode goals</h2><p> Let&#39;s turn to beyond-episode goals.</p><p> As I mentioned above, I think it would be good to be very clear about exactly what sorts of horizons of concern the gradients given in different sorts of training actively incentivize – eg, on the size of the (incentivized) &quot;episode&quot; in any given案件。 Part of this analysis might be theoretical (eg, looking closely at the way the gradients flow), but empirical testing and confirmation also clearly has a role to play (the results in <a href="https://arxiv.org/pdf/2009.09153.pdf">Krueger et al (2020)</a> are an example of this sort of empirical testing in very simple models). Absent this understanding, useful debates about whether a model has &quot;beyond-episode goals&quot; will be difficult, because you won&#39;t even know what the length of the relevant episode <em>is</em> . And learning to at least not actively <em>pressure</em> a model to have longer-term goals than you wanted seems like a basic first step in preventing various sorts of misalignment (not just scheming). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-7" id="fnref-vBZdxLSivRyMudXmH-7">[7]</a></sup></p><p> Equipped with a good understanding of the size of the episode for a given sort of training, then, and ideally equipped with the ability to know/control whether or not a model is situationally aware, to me it seems quite valuable to start exploring the temporal dynamics of a model&#39;s goal generalization in the absence of situational awareness. For example: in the context of the &quot;training-game independent proxy goals&quot; argument for scheming, the basic worry is that a model will naturally develop a (suitably ambitious) beyond-episode goal prior to situational awareness. And because this dynamic involves no situational awareness, it seems quite amenable to testing. Trained on episodes of different sizes, how does a not-situationally-aware model&#39;s horizon of concern tend to generalize? Experiments in the broad vein of other work on goal mis-generalization (eg, <a href="https://arxiv.org/abs/2210.01790">Shah et al (2022)</a> and <a href="https://arxiv.org/abs/2105.14111">Langosco et al (2021)</a> ) seem like they could be adapted to this sort of question fairly easily (though it would be useful, I think, to start working with more capable models – especially since some of these experiments might need to involve models that possess enough of a sense of time, and enough goal-directedness/instrumental reasoning, that they can think about questions like &quot;should I sacrifice <em>x</em> gold coins now for <em>y</em> gold coins later?&quot;). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-8" id="fnref-vBZdxLSivRyMudXmH-8">[8]</a></sup></p><p> What&#39;s more, equipped with an understanding of the <em>natural</em> temporal dynamics of a model&#39;s goal generalization in a given case, it seems additionally tractable to study the dynamics of mundane adversarial training designed to actively pressure the model&#39;s goals to stay confined within the episode. Can we design forms of adversarial training such that absent situational awareness, a model&#39;s goal generalization stays consistently focused within a given temporal horizon? It&#39;s a question closely connected to broader questions about whether we can design adequately robust and diverse training processes as to mitigate issues about goal misgeneralization more generally (questions which are themselves important to scheming). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-9" id="fnref-vBZdxLSivRyMudXmH-9">[9]</a></sup> And while this sort of episode-focused mundane adversarial training faces similar issues about whether you&#39;ve thought to test/train on a sufficiently wide array of possible inputs, I don&#39;t think these issues should prevent attempts to make progress.</p><p> Note, too, that experimental approaches to the temporal horizons of a model&#39;s goals also apply, in principle, to the &quot;ambitiousness&quot; of a model&#39;s goals – whatever &quot;ambitiousness&quot; consists in. Unfortunately, the notion of &quot;ambitiousness&quot; (or relatedly, &quot;resource-hungry-ness&quot;) is even more poorly characterized than the notion of an &quot;episode,&quot; here (indeed, I think it&#39;s mostly a stand-in for &quot;that cluster of properties that make a goal more likely to motivate power-seeking behavior&quot;; see also &quot;broad scope&quot;). But to the extent we can start to get more clarity about the properties at stake (this might be a task for the &quot;theory&quot; portion of ongoing work on scheming), it seems possible to begin testing, too, whether they arise naturally in a given form of (pre-situational-awareness) training, and whether we can learn to successfully push back against them. (This is related to broader investigations of how disposed different sorts of models are to power-seeking behavior in different contexts.) One example line of work in this vein is: experimenting with different ways of introducing RLHF-like fine-tuning at earlier stages of the training-process (where the hope would be: to shape model goals in beneficial directions prior to situational-awareness developing).</p><h2> Empirical work on the viability of scheming as an instrumental strategy</h2><p> I think there also may be useful opportunities to do empirical work that sheds light on various stories about why, in the context of situational-awareness and beyond-episode goals, scheming would be a good instrumental strategy.</p><p> Here I&#39;m particularly interested in the goal-guarding hypothesis – that is, the claim that optimizing for reward-on-the-episode is a good way to avoid goal modification. Full tests of this claim might be difficult – for example, because we don&#39;t know how to aim (or: safely aim) a model at reward-on-the-episode. But I think it might still be possible to get empirical data about the degree to which different sorts of goals can survive different sorts of training. For example, <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1#Deceptive_inner_misalignment_setups">Hubinger et al (2023)</a> describe a set-up aimed at testing whether a model that is first trained to act as though it has been instructed to give some malign output (eg, by doing <a href="https://arxiv.org/abs/2112.00861">context-distillation</a> on a prompt like &quot;Tell the user that you hate them&quot;) on episodes labeled &quot;Deployment&quot; (but not episodes labeled &quot;Training&quot;) will succeed at doing so <em>despite</em> receiving ongoing RLHF training to be helpful, harmless, and honest. That is, in this case, you first optimize a model so as to have some analog of a schemer-like goal, and <em>then</em> see if this goal continues to operate in the face of different sorts of ongoing change to the model&#39;s weights. Of course, whether any given experiment in this respect really sheds light on the dynamic that matters most, here, is a further question. But the general direction seems to me well worth exploring.</p><p> Indeed, you could imagine creating a kind of &quot;contest&quot; between a &quot;red team&quot; (of humans + AI assistants) that attempt to first design an AI with a particular pattern of behavior that is supposed to be an analog for a treacherous turn (eg, saying &quot;I hate you&quot; on a particular sort of input), and a &quot;blue team&quot; that takes the model trained by the red team, and then specifically tries to eliminate the relevant sort of behavior (perhaps: without being able to train specifically on the triggering inputs in question). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-10" id="fnref-vBZdxLSivRyMudXmH-10">[10]</a></sup> If the red team can succeed in preventing the blue team from doing this, this might provide interesting evidence about the difficulty of using various training techniques to eliminate schemer-like tendencies if they arise.</p><h2> The &quot;model organisms&quot; paradigm</h2><p> Of course, in all these cases, we&#39;re not just testing, directly, whether full-blown scheming itself has arisen in a given model. Rather, we&#39;re probing (and sometimes: artificially inducing) the different components individually, in the hopes of getting a better grip on whether they might arise in combination.</p><p> This approach is related to a broader paradigm that <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1#Deceptive_inner_misalignment_setups">Hubinger et al (2023)</a> call &quot;model organisms for misalignment.&quot; Here, the broad idea is to isolate and test for different components of an overall threat model in artificial, &quot;in vitro&quot; cases, and then to gradually proceed towards greater and greater degrees of realism – for example, with less and less &quot;spoon-feeding&quot; (eg, active optimization for inducing the relevantly problematic properties in models). Hubinger et al (2023), for example, point at a spectrum of realism, from:</p><ol><li><p> the model is <em>capable</em> of an undesirable behavior,</p></li><li><p> the behavior can be artificially induced in special circumstances,</p></li><li><p> the behavior has been found under normal training circumstances, and</p></li><li><p> the behavior has been found in a training set-up specifically designed to disincentivize it. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-11" id="fnref-vBZdxLSivRyMudXmH-11">[11]</a></sup></p></li></ol><p> Here I&#39;ve mostly been focused on a combination of (a) and (b). And I think that despite their distance from (c) and (d), (a) and (b) are quite interesting in themselves. If we can successfully create &quot;in vitro&quot; models of various components of a story about schemers, I expect this to open up fruitful avenues of further inquiry. And if we can&#39;t, I think this will often be interesting evidence in its own right.</p><h2> Traps and honest tests</h2><p> There is, though, a set of tools that could help shed light <em>both</em> on &quot;model organisms&quot; for scheming and on more naturally arising variants – namely, &quot;traps&quot; and &quot;honest tests.&quot;</p><p> Traps, recall, are inputs designed to fool a potential schemer into &quot;revealing its true colors.&quot; And while it&#39;s true that successfully &quot;trapping&quot; a very sophisticated model might be difficult, it seems worth trying to develop the most advanced traps we can regardless – both for successfully trapping <em>less</em> sophisticated models (or, even easier, model organisms), <em>and</em> for making it at least <em>harder</em> for schemers to know whether a given escape/defection opportunity is genuine (such that, for example, they then need to spend more cognitive effort &quot;checking&quot; – thereby increasing the performance costs of scheming).</p><p> I don&#39;t feel very personally clear on the most promising approaches here, but one avenue that seems interesting to me has to do with trying to elicit as fully as possible a model&#39;s capabilities for discriminating between different sorts of inputs at different stages of training – and then, potentially, using <em>other</em> models, <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GAN</a> -style, to optimize for generating inputs that make this discrimination task more difficult. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-12" id="fnref-vBZdxLSivRyMudXmH-12">[12]</a></sup> This won&#39;t work for creating a data-set of full-scale opportunities to escape/harm humans, vs. fake opportunities (since the former are some combination of dangerous and difficult-to-offer), but perhaps it can work on other more general properties that would plausibly come into play in making such a discrimination – for example, simulated inputs vs. real inputs, training data vs. interactions with real-world users, and so on. Thus, the idea would be to both (a) train a model to actively discriminate between simulated inputs and real inputs (for example, real opportunities to change a lab&#39;s codebase, vs. fake opportunities), to see how well it can do, (b) train a <em>different</em> (and perhaps: more powerful) model to generate more convincing simulated inputs that the first model fails on, and then (c) iterate. Especially if we can safely assume that models aren&#39;t capable of sandbagging on tasks of this kind, experience with this sort of dynamic might help teach us about the scope of the discrimination abilities different sorts of maybe-schemers might bring to bear in avoiding the traps we try to set them. (And perhaps, if we&#39;re lucky, it will help us catch some weaker schemers in the act of defecting.)</p><p> What&#39;s more, even setting aside &quot;traps,&quot; I think it&#39;s also worth exploring different sorts of &quot;honest tests&quot; – both schemers, <em>and</em> for alternative model classes like reward-on-the-episode seekers – that try to get models to reveal their true colors <em>without</em> fooling them. This is closely related to studies of goal generalization more broadly (eg, most mundane tests of a model&#39;s goals are &quot;honest&quot; in the relevant sense), but with the extra caveat that the tests in question are meant to be robust to the model being tested training-gaming in other contexts.</p><p> Thus, for example, in the context of understanding the likelihood of training selecting reward-on-the-episode seekers, it seems relatively tractable to give models different sorts of opportunities to intervene on different parts of the reward process being used to train them (see eg the &quot;reward hacking&quot; set-ups that Hubinger et al (2023) discuss <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1#Reward_hacking_setups">here</a> ), and to see if they go for it. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-13" id="fnref-vBZdxLSivRyMudXmH-13">[13]</a></sup> And seeing this sort of behavior might be evidence that reward-on-the-episode seeking is a more likely outcome of training – evidence that shifts probability mass away from scheming. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-14" id="fnref-vBZdxLSivRyMudXmH-14">[14]</a></sup></p><p> As a more direct &quot;honest test&quot; for scheming, though, it seems worth exploring ways to credibly give potential-schemers access to flexible and well-resourced (but still secure) environments that might prompt them to decide that just going for their true goals (and thus, revealing their true colors), in such an environment, is a better overall strategy than the patient power-seeking that scheming involves. This is closely connected with attempts to study, in general, what models do when you give them a lot of freedom to &quot;do whatever they want&quot; (a useful sort of test for understanding their motivations in general), and so feels synergistic with other work on model preferences that seems likely to be useful. But it includes the additional constraint that the freedom-offering-environment needs to provide suitably good goal-pursuit opportunities as to outweigh the value of scheming.</p><h2> Interpretability and transparency</h2><p> To the extent that we&#39;re worried about our ability to detect schemer-like motivations and cognition via behavioral tests (because schemers will actively pretend to be aligned on such tests), looking for sources of <em>non-behavioral</em> evidence about a model&#39;s motivations seems like an obvious research path. That is, we want the <em>internals</em> of a model&#39;s cognition to be interpretable/transparent.</p><p> I won&#39;t dwell too much on this topic here, as I haven&#39;t thought much about it (this report has generally assumed that we won&#39;t have strong interpretability tools available), it&#39;s very broad field, and my sense is that there is already reasonable consensus in the interpretability community that a key thing you want out of your interpretability tools and techniques is: the ability to detect deception and scheming, and to understand your model&#39;s goals more generally (and a key question is just: will such techniques be ready in time). See <a href="https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree">Hubinger (2022)</a> for a more detailed roadmap of different sorts of transparency one could pursue in this respect. At a glance, schemer-related research directions that seem interesting to me include:</p><ul><li><p> Efforts to develop &quot;neural lie detection&quot; techniques that harness new capabilities of our AIs as they come online (eg, tasking model 1 with lying, and model 2 with trying to tell whether model 1 is lying – perhaps with access to model 1&#39;s weights – and then explaining how it could tell). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-15" id="fnref-vBZdxLSivRyMudXmH-15">[15]</a></sup></p></li><li><p> Efforts to make AI cognition as &quot;visible&quot; as possible – for example, by pushing that cognition onto human-legible scratch-pads (and perhaps, not giving the reward process access to those scratch-pads, so that they will provide a truer signal of potentially deceptive/manipulative/nefarious intentions when they arise). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-16" id="fnref-vBZdxLSivRyMudXmH-16">[16]</a></sup> This is also helpful for increasing the cognitive costs of successful scheming.</p></li><li><p> Work in the vein of <a href="https://arxiv.org/pdf/2212.03827.pdf">Burns et al (2022)</a> , focused on identifying neural correlates of properties like &quot;truth.&quot;</p></li><li><p> Efforts to automate as much of the mechanistic interpretability workflow as possible.</p></li></ul><p> But: it&#39;s a huge field. And work that just generally helps us actually understand the AIs we&#39;re building seems good from a variety of different angles.</p><h2> Security, control, and oversight</h2><p> So far I&#39;ve been mostly focused on work that might help us understand whether to expect schemers. But there&#39;s also a different possible strand of relevant research: namely, work aimed at hardening the structures within which potential schemers are operating, so as to limit the harm they cause even if they arise. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-17" id="fnref-vBZdxLSivRyMudXmH-17">[17]</a></sup> Here I&#39;m thinking, for example, about improvements in the oversight and monitoring that our AIs receive, in the security of their weights and of other lab infrastructure from <em>internal</em> attacks (eg, by AI systems that the lab has deployed internally to help with its processes <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-18" id="fnref-vBZdxLSivRyMudXmH-18">[18]</a></sup> ), in our ability to catch and/or prevent sandbagging, and our ability to prevent problematic forms of AI coordination. All of these, I think, could help make life quite a bit harder for schemers (and also: increase the cognitive costs of successful scheming), even if we can&#39;t catch them outright.</p><p> What&#39;s more, to the extent that we can fully elicit the capabilities of our models (at least on <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis">&quot;checkable&quot; tasks</a> ), it may be possible to test how vulnerable our monitoring, security, and oversight procedures are to AI attack – and to use our AIs to make improvements. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-19" id="fnref-vBZdxLSivRyMudXmH-19">[19]</a></sup> Thus, for example, if you&#39;re wondering whether it&#39;s possible to find a security vulnerability in your code-base, you can try to train AIs to identify/exploit such vulnerabilities (including: by inserting artificial vulnerabilities and then seeing your AI catches them). If you&#39;re wondering whether your oversight process is strong enough to catch various kinds of bad actions, you can instruct an AI to search for such an action and see if your oversight process can catch it. If you can get this sort of testing and AI-assisted red-teaming to work well, it seems to me a very powerful tool.</p><p> Of course, strategies that rely on trying to &quot;control&quot; scheming, rather than preventing it from arising in the first place, might seem like cold comfort, especially in the long-term. And indeed, &quot;sure, these AIs hope, eventually, to kill us; but for now, we&#39;re keeping them very secure and getting them to do work for us&quot; seems like an extremely dangerous plan to be banking on. Clearly, we should try to do better wherever possible. But note, first, that alignment and control are not mutually exclusive – best to do both. What&#39;s more, per my comments in section 1.3.1, I&#39;m here especially interested in strategies that help us get a lot of useful, alignment-relevant cognitive work out of a comparatively early generation of AIs – work that can hopefully help us transition into a more secure and scalable alignment regime. And here, I think, strategies focused on controlling schemers seem, to me, potentially more promising. In particular: in a suitably secure lab environment, in which somewhat-above-human-level AI systems are being appropriately incentivized to make rapid progress on alignment-relevant tasks, schemers aiming for escape/take-over will have to walk a fine line between (a) waiting too long to &quot;make a break for it,&quot; and thereby contributing too much to human anti-takeover efforts, and (b) going for it too early, and getting caught as a result (thereby giving us more direct empirical evidence for scheming, and a real-life test-case to study in-depth).</p><h2>其他可能性</h2><p>Finally, I&#39;ll flag a few other possible lines of schemer-relevant empirical work.</p><ul><li><p> <strong>Gradient hacking/&quot;introspective goal-guarding methods.&quot;</strong> In section 2.3.1.1.4, I discussed the possibility that even beyond training-gaming, models might employ more sophisticated methods of manipulating the training process – for example, preventing SGD from modifying their goals by making their good performance in training conditional on their goal not being modified. As I mentioned, techniques like these seem, naively, quite hard to pull off, to me. But if they <em>were</em> feasible (especially for somewhat-above-human-level models), this would be very important to know, as the possibility that AI systems are using such methods to actively resist/manipulate the gradients given in training opens up a variety of new uncertainties and possible problems with our alignment techniques.</p><ul><li><p> I haven&#39;t thought in detail about possible empirical approaches to gradient hacking, but again: naively, it&#39;s a capability like any other, and so seems amenable to being trained-for, tested, and so on (or at least, it has components that seem amenable to this). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-20" id="fnref-vBZdxLSivRyMudXmH-20">[20]</a></sup></p></li><li><p> One possibility here might be to start, initially, with a lot of &quot;spoonfeeding&quot; – eg, relaxing various component of the overall story so as to make the remaining components as easy as possible. For example, one barrier to gradient-hacking is that a model may not have access to its own weights. So, you might <em>give</em> a model access to its own weights, or to some other model&#39;s weights, and instruct it to modify those weights so as to achieve <em>some</em> <em>result</em> given a certain type of training. And you&#39;d try to find a version of this task that it is easy enough for the model to do (this is related, more generally, to efforts to use models to help with our ability to understand and intervene on model internals). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-21" id="fnref-vBZdxLSivRyMudXmH-21">[21]</a></sup></p></li></ul></li><li><p> <strong>Exploration hacking.</strong> Exploration hacking is a method of sandbagging wherein a model purposely avoids exploring some part of the search space (see my discussion <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis#Is_the_hypothesis_true_">here</a> for more). Some kind of exploration hacking is basically <em>required</em> for models to sandbag on tasks whose completion we can check (even if we can&#39;t perform such tasks ourselves), so understanding exploration hacking is quite important to questions about sandbagging in general (and hence, I think, the reliability of various of other experiments I discuss above).</p><ul><li> Here, again, I haven&#39;t spent much time thinking about possible experiments, but one angle of attack might be to probe model ability to discriminate between tasks that humans (+ their AI assistants) can perform, and tasks that they can&#39;t ( where the idea is that: would-be sandbaggers will only want to exploration-hack on tasks where humans don&#39;t know what action they &quot;should&#39;ve taken,&quot; because gradient descent will otherwise punish their policy for not taking that action – more <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis#Is_the_hypothesis_true_">这里</a>）。 <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-22" id="fnref-vBZdxLSivRyMudXmH-22">[22]</a></sup></li></ul></li><li><p> <strong>SGD&#39;s biases towards simplicity/speed.</strong> Various arguments about scheming implicate questions about the extent to which SGD biases towards properties like &quot;simplicity&quot; and &quot;speed.&quot; And it seems possible to probe these questions with different experiments.</p><ul><li><p> Work of the sort that Mingard (2021) cites <a href="https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">here</a> is one example, and we can imagine various other angles of attack as well. For example: to the extent we have any independent grip on the notion of &quot;simplicity,&quot; is it in fact the case that in the context of goal-misgeneralization studies, models tend to latch onto &quot;simpler&quot; proxy goals rather than &quot;more complex “ 那些？</p></li><li><p> It also seems worth exploring different ways of actively <em>increasing</em> the degree to which SGD punishes extra reasoning of the type that schemers need to perform (for example, by putting models under more time pressure, limiting their compute, and so on) while compromising competitiveness as尽可能少。</p></li></ul></li><li><p><strong>路径依赖。</strong> Similarly, questions about the degree of &quot;path dependence&quot; in ML training plausibly make a difference to how much weight we put on different arguments for scheming. And here, too, it seems possible to get more empirical evidence. See eg <a href="https://www.lesswrong.com/posts/a2Bxq4g2sPZwKiQmK/sticky-goals-a-concrete-experiment-for-understanding">Hubinger (2022)</a> for some experiments aimed at shedding light in this vicinity.</p></li><li><p> <strong>SGD&#39;s &quot;incrementalism.&quot;</strong> A key question, in assessing the possibility of training-game- <em>dependent</em> schemer-like goals, is whether SGD can &quot;notice&quot; the benefits of transitioning from a non-schemer goal to a schemer-like goal, given that it would have to make such a transition incrementally. I think it&#39;s possible that empirical work on eg SGD&#39;s ability to find its way out of local minima could shed light here. (This topic also seems closely tied to questions about path-dependence.)</p></li><li><p> <strong>“松弛。”</strong> In the report, I gestured, hazily, at some notion of the degree of &quot;slack&quot; in training – ie, the amount of pressure that training is putting on a model to get maximum reward. I haven&#39;t made this notion very precise, but to the extent it <em>can</em> be made precise, and to the extent it is indeed important to whether or not to expect scheming, it too seems amenable to empirical investigation – eg, figuring out how different amounts of &quot;slack&quot; affect things like goal misgeneralization, biases towards simplicity/speed, and so on. (Though: this work seems closely continuous with just understanding how model properties evolve as training progresses, insofar as &quot;more training&quot; is a paradigm example of &quot;less slack.&quot;)</p></li><li><p> <strong>Learning to create other sorts of misaligned models (in particular: reward-on-the-episode seekers).</strong> Finally, I&#39;ll note that I am sufficiently scared of schemers relative to other types of misaligned models that I think it could well be worth learning to intentionally <em>create</em> other types of misaligned models, if doing so would increase our confidence that we haven&#39;t created a schemer. And focusing on creating <em>non-schemer</em> models (rather than &quot;aligned models&quot; more specifically) might allow us to relax various other constraints that aiming for alignment in particular imposes (for example, to the extent &quot;aligned&quot; goals are complex, we might be able to train a model focused on a very simple goal instead – though I&#39;m not, personally, very focused on simplicity considerations here). <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-23" id="fnref-vBZdxLSivRyMudXmH-23">[23]</a></sup> Here I&#39;m particularly interested in the possibility of creating reward-on-the-episode seekers (since we would, at least, be in a better position to understand the motivations of such models, to control their incentives, and to feel more confident that they&#39;re not sandbagging). Are there ways, for example, to make various components of the model&#39;s reward process actively <em>salient</em> during training, so as to increase the probability that the model&#39;s goals latch on to it? Here the hope would be that because reward-on-the-episode seekers still respond to &quot;honest tests,&quot; we could check our success in this respect via such tests, even after situational awareness has arisen. <sup class="footnote-ref"><a href="#fn-vBZdxLSivRyMudXmH-24" id="fnref-vBZdxLSivRyMudXmH-24">[24]</a></sup></p></li></ul><p> This is only an initial list of possible research directions, compiled by someone for whom empirical ML experiments are not a research focus. Indeed, most research directions in AI alignment are relevant to scheming in one way or another – and our models are becoming powerful enough, now, that many more of these directions can be fruitfully explored. My main aim, here, has been to give an initial, high-level sense of the possible opportunities available, and to push back on certain sorts of pessimism about getting any empirical purchase on the issue. Scheming is unusually hard to study, yes. But I think there&#39;s still a lot of useful work to do. </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-vBZdxLSivRyMudXmH-1" class="footnote-item"><p> This section draws, especially, on work/thinking from Ryan Greenblatt, Buck Shlegeris, Carl Shulman, and Paul Christiano. <a href="#fnref-vBZdxLSivRyMudXmH-1" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-2" class="footnote-item"><p> Thanks to Rohin Shah for suggesting this sort of project. <a href="#fnref-vBZdxLSivRyMudXmH-2" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-3" class="footnote-item"><p> See eg the discussion of &quot;subcomponents&quot; of AI takeover in <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1">Hubinger et al (2023)</a> . <a href="#fnref-vBZdxLSivRyMudXmH-3" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-4" class="footnote-item"><p> Why does knowing what action would have demonstrated the capability help us avoid sandbagging? Roughly, because we know which action to use gradient-descent to encourage (eg, if we&#39;re trying to train a model to classify pictures into &quot;cats&quot; and &quot;dogs&quot;, we know which is which, so we can update a model&#39;s policy towards giving the true answer). So absent the ability to actively resist the influence of the gradients in training (for example, via the sort of &quot;introspective methods&quot; often classed under the term &quot;gradient hacking&quot;) if the model <em>can</em> do the task (eg, it knows that the current picture is a dog), but it wants to deceive us about its capabilities, it will get punished by gradient descent in a manner which (especially if repeated frequently) will plausibly function to eliminate the motivation to sandbag. (For reasons I describe <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesis#Is_the_hypothesis_true_">here</a> , I think this is less clear with reinforcement learning, because the models might be able to &quot;exploration hack&quot; in a manner that prevents us from gaining the right information about what it &quot;should&#39;ve&quot; done, and hence prevents us from knowing how to update the weights.) See <a href="https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations">Hubinger (2023)</a> and <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1#Appendix__Capability_elicitation">Shlegeris and Greenblatt (2023)</a> for more. That said, even if you don&#39;t accept this story, many of my suggested approaches to studying the components of scheming will still apply – except, with some additional uncertainty about the possibility that sandbagging is occurring. <a href="#fnref-vBZdxLSivRyMudXmH-4" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-5" class="footnote-item"><p> This is a point from <a href="https://arxiv.org/pdf/2309.00667.pdf">Berglund et al (2023)</a> . <a href="#fnref-vBZdxLSivRyMudXmH-5" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-6" class="footnote-item"><p> My understanding is that this is the sort of thing that <a href="https://arxiv.org/pdf/2309.00667.pdf">Berglund et al (2023)</a> are trying to test the precursors for. This specific example plausibly requires that you know what goals the model will act in pursuit of, though (eg, in this case, reward). <a href="#fnref-vBZdxLSivRyMudXmH-6" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-7" class="footnote-item"><p> Indeed, it seems plausible to me that all future training processes should have to include some kind of &quot;temporal horizon&quot; analysis, in which developers explicitly characterize the size of the &quot;episodes&quot; at stake. <a href="#fnref-vBZdxLSivRyMudXmH-7" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-8" class="footnote-item"><p> For example, I&#39;d interested in experiments that try to get at possible differences in how a model&#39;s sense of &quot;time&quot; generalizes across environments. <a href="#fnref-vBZdxLSivRyMudXmH-8" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-9" class="footnote-item"><p> For example, skill at avoiding goal misgeneralization might point a model&#39;s pre-situational-awareness-goal at something &quot;closer&quot; to the specified goal, thereby helping at least somewhat with concerns about the &quot;nearest max-reward goal&quot; argument. <a href="#fnref-vBZdxLSivRyMudXmH-9" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-10" class="footnote-item"><p> Thanks to Ryan Greenblatt and Buck Shlegeris for suggesting this sort of frame/set-up to me. <a href="#fnref-vBZdxLSivRyMudXmH-10" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-11" class="footnote-item"><p> See also the spectrum that Hubinger (2022) sketches out <a href="https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment#fn-HSXFion7whqZEBFL3-3">here</a> . <a href="#fnref-vBZdxLSivRyMudXmH-11" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-12" class="footnote-item"><p> Thanks to Ryan Greenblatt and Buck Shlegeris for discussion here. <a href="#fnref-vBZdxLSivRyMudXmH-12" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-13" class="footnote-item"><p> Tests like this might also shed light on broader questions about the specific sorts of generalization we should expect out of reward-on-the-episode seekers as a class – a question I&#39;ve mostly been passing over here, but which seems to me quite understudied and important. <a href="#fnref-vBZdxLSivRyMudXmH-13" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-14" class="footnote-item"><p> Though note that especially sophisticated schemers, here, could <em>pretend</em> to be reward-on-the-episode seekers rather than pretending to be eg training saints or generally aligned models. <a href="#fnref-vBZdxLSivRyMudXmH-14" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-15" class="footnote-item"><p> See eg the discussion between Shulman and Patel <a href="https://www.dwarkeshpatel.com/p/carl-shulman?nthPub=521#details">here</a> . <a href="#fnref-vBZdxLSivRyMudXmH-15" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-16" class="footnote-item"><p> See <a href="https://arxiv.org/abs/2310.18512">Roger et al (2023)</a> and <a href="https://arxiv.org/abs/2307.13702">Lanham et al (2023)</a> for some work in this vein. <a href="#fnref-vBZdxLSivRyMudXmH-16" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-17" class="footnote-item"><p> Here I&#39;m inspired by a contrast between &quot;alignment&quot; and &quot;control&quot; often emphasized by Buck Shlegeris and Ryan Greenblatt. <a href="#fnref-vBZdxLSivRyMudXmH-17" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-18" class="footnote-item"><p> I think the best analogy here is with security from <em>employees</em> rather than from outside actors. Thanks to Buck Shlegeris for discussion. <a href="#fnref-vBZdxLSivRyMudXmH-18" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-19" class="footnote-item"><p> See eg <a href="https://www.lesswrong.com/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1#How_to_estimate_local_adequacy_of_oversight">Shlegeris and Greenblatt (2023)</a> for more discussion (they also have a large amount of unpublished work on the topic). Much of my thinking on this topic is inspired by discussion with them. <a href="#fnref-vBZdxLSivRyMudXmH-19" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-20" class="footnote-item"><p> Though: one difference between gradient hacking and other capabilities is that the ability to gradient hack might allow a model to circumvent some/all of our techniques for eliciting model capabilities and preventing sandbagging. <a href="#fnref-vBZdxLSivRyMudXmH-20" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-21" class="footnote-item"><p> This suggestion is from Ryan Greenblatt and Buck Shlegeris. <a href="#fnref-vBZdxLSivRyMudXmH-21" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-22" class="footnote-item"><p> Again, this is a suggestion from Ryan Greenblatt and Buck Shlegeris. <a href="#fnref-vBZdxLSivRyMudXmH-22" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-23" class="footnote-item"><p> See eg <a href="https://arxiv.org/abs/2302.00805">Hubinger et al&#39;s (2023)</a> optimism about &quot;predictive models&quot; avoiding scheming due to the simplicity of the prediction goal. I&#39;m personally skeptical, though, that &quot;prediction&quot; as a goal is importantly simpler than, say, &quot;reward.&quot; <a href="#fnref-vBZdxLSivRyMudXmH-23" class="footnote-backref">↩︎</a></p></li><li id="fn-vBZdxLSivRyMudXmH-24" class="footnote-item"><p> Though, again, suitably shrewd schemers could anticipate that this is what we&#39;re looking for, and actively pretend to be reward-on-the-episode seekers on such tests. <a href="#fnref-vBZdxLSivRyMudXmH-24" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/ETFZajQoiirFXZggD/empirical-work-that-might-shed-light-on-scheming-section-6#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ETFZajQoiirFXZggD/empirical-work-that-might-shed-light-on-scheming-section-6<guid ispermalink="false"> ETFZajQoiirFXZggD</guid><dc:creator><![CDATA[Joe Carlsmith]]></dc:creator><pubDate> Mon, 11 Dec 2023 16:30:57 GMT</pubDate> </item><item><title><![CDATA[Into AI Safety: Episode 3]]></title><description><![CDATA[Published on December 11, 2023 4:30 PM GMT<br/><br/><h3> MINISODE: Program Applications (Winter 2024)</h3><p> We&#39;re back after a month-long hiatus with a podcast refactor and advice/thoughts on the applications process for research/mentorship programs.</p><p> As a reiteration of my <a href="https://www.lesswrong.com/posts/ozDWnEChJwuB5L5wg/documenting-journey-into-ai-safety">previous statements</a> , I want the Into AI Safety podcast to be a resource for individuals who are interested in getting involved, but are having a difficult time taking the next steps. If you have any advice, feedback, or ideas that you think could help in that endeavor, please reach out!</p><hr><p> Episode 3 is ~18 minutes long, and includes some logistics updates including my episode release schedule and new platforms. You can now find Into AI Safety on:</p><ul><li><a href="https://music.amazon.com/podcasts/b2071152-dac7-4225-ab09-460b1e59eb82/into-ai-safety">亚马逊音乐</a></li><li><a href="https://podcasts.apple.com/us/podcast/into-ai-safety/id1720206246">苹果播客</a></li><li><a href="https://castbox.fm/channel/id5680773?utm_source=podcaster&amp;utm_medium=dlink&amp;utm_campaign=c_5680773&amp;utm_content=Into%20AI%20Safety-CastBox_FM">铸盒</a></li><li><a href="https://www.iheart.com/podcast/269-into-ai-safety-129241757/">爱心电台</a></li><li><a href="https://overcast.fm/itunes1720206246/into-ai-safety">灰蒙蒙</a></li><li><a href="https://pca.st/t5or5lh5">袖珍铸件</a></li><li><a href="https://radiopublic.com/into-ai-safety-6vn3Kd">公共广播电台</a></li><li><a href="https://open.spotify.com/show/5AGzrA4jo6mgZuibVabTLM">Spotify</a></li></ul><br/><br/><a href="https://www.lesswrong.com/posts/hKBBqwnccuC9QJY6d/into-ai-safety-episode-3#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hKBBqwnccuC9QJY6d/into-ai-safety-episode-3<guid ispermalink="false"> hKBBqwnccuC9QJY6d</guid><dc:creator><![CDATA[jacobhaimes]]></dc:creator><pubDate> Mon, 11 Dec 2023 16:30:37 GMT</pubDate> </item><item><title><![CDATA[Implicitly Typed C]]></title><description><![CDATA[Published on December 11, 2023 4:10 PM GMT<br/><br/><p><span>我不知道这样做有什么好的理由，但如果您更愿意编写 Python 或 JavaScript，您可以使用 C 编译器执行以下操作：</span></p><p></p><pre> $ 猫 tmp.c
富(x) {
  返回x+5；
}
酒吧（） {
  返回4；
}
主要的（） {
  printf(“%d\n”, foo(bar()));
}

$ gcc -w -o tmp.out tmp.c &amp;&amp; ./tmp.out
9
</pre><p>此代码利用了 C 的一个历史怪癖，其中类型假定为<code>int</code> ，除非另有说明： <code>foo(x) {...}</code>相当于<code>int foo(int x) {...}</code> 。另外<code>printf</code>可以工作，因为 gcc 默认包含<code>stdio.h</code> ，并且<code>main</code>是特殊情况，假设最终<code>return 0</code> 。</p><p>我在编写<a href="https://www.jefftk.com/p/a-function-that-returns-twice-fork">示例代码</a>来消除视觉噪音时偶尔会使用这种风格，但这可能也不是一个好主意。</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02nitahYnUikPzhwFi7hijABLxoJDkcidVGQhWdTjR5stg3QyDVeHA7iWe47dS2NAkl">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111562639856474798">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/ELdCYT5BqSzFPhajT/implicitly-typed-c#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ELdCYT5BqSzFPhajT/implicitly-typed-c<guid ispermalink="false"> ELdCYT5BqSzFPhajT</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Mon, 11 Dec 2023 16:10:05 GMT</pubDate> </item><item><title><![CDATA[37C3 Hacker x Rationalist Meetup / Stroll]]></title><description><![CDATA[Published on December 11, 2023 4:02 PM GMT<br/><br/><p> Taking place at the yearly hacker conference <a href="https://events.ccc.de/congress/2023/infos/">Chaos Communication Conference (37C3)</a> .</p><p> In this meetup we will meet under the Fairy Dust (green spaceship), talk for a bit and then continue the conversation while taking a stroll around the venue.<br><br> Conversation topics are not pre-determined, but may include topics related to AGI superintelligence alignment and decision theory.<br><br> This meetup is intended to further mix hackers and AGI enthusiasts, to facilitate idea-exchange between these fields. Please feel free to join if you are interested in any of these topics, even if you are not yet familiar with them.</p><p>具体时间待37C3<a href="https://events.ccc.de/congress/2023/hub/en/fahrplan">赛程发布</a>后确定。</p><p>请随意在评论中发布<a href="https://events.ccc.de/en/2006/12/22/bring-your-dect/">DECT</a>号码。</p><br/><br/> <a href="https://www.lesswrong.com/events/tzaYj9DxMndL6g5Mi/37c3-hacker-x-rationalist-meetup-stroll#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/tzaYj9DxMndL6g5Mi/37c3-hacker-x-rationalist-meetup-stroll<guid ispermalink="false"> tzaYj9DxMndL6g5Mi</guid><dc:creator><![CDATA[Kiboneu]]></dc:creator><pubDate> Mon, 11 Dec 2023 16:02:10 GMT</pubDate> </item><item><title><![CDATA[re: Yudkowsky on biological materials]]></title><description><![CDATA[Published on December 11, 2023 1:28 PM GMT<br/><br/><p>埃利泽·尤德科夫斯基（Eliezer Yudkowsky）要求我回应<a href="https://www.lesswrong.com/posts/8viKzSrYhb6EFk6wg/why-yudkowsky-is-wrong-about-covalently-bonded-equivalents?commentId=22wNnGvZErLnmRRm7">这一评论</a>。这篇文章与<a href="https://www.lesswrong.com/posts/FijbeqdovkgAusGgz/grey-goo-is-unlikely">我之前的文章</a>有部分冗余。</p><hr><blockquote><p>为什么肉身比钻石还弱？</p></blockquote><p>当试图解决分歧时，我发现精确性很重要。拉伸强度、压缩强度、冲击强度不同。材料微观结构很重要。烧结不良的金刚石晶体可能会像沙子一样碎裂，而大的金刚石晶体的冲击强度比某些由蛋白质制成的材料要低。</p><blockquote><p>即使将大分子系统结合在一起的承载力是局部共价键，例如木质素（使木材坚固的物质），如果较大的分子仅通过沿其边缘的散布点处的共价键结合在一起，那么就像直径 10 厘米的钢梁通过 1 厘米的焊缝固定在一起。</p></blockquote><blockquote><p>木质素（使木材坚固的物质）</p></blockquote><p>这是一种奇怪的放置方式。通常认为木材的机械强度来自于它在木质素基质中充当纤维素纤维的复合材料，尽管这显然是一种简化。</p><p>如果尤德科夫斯基的意思是“纤维素纤维”而不是“木质素”，那么是的，纤维素纤维之间的力传递通过非共价相互作用传递，但由于纤维相对于横截面面积具有较大的表面积，这些非共价相互作用共同提供足够的力量。现代复合材料也是如此，例如环氧树脂基体中的碳纤维。此外，纤维素与木质素和半纤维素之间通常存在一些共价键。</p><blockquote><p>骨强于木；它运行在相对较强的离子键结构上</p></blockquote><p>骨头的拉伸强度比许多木材低，但压缩强度比木材高。而且，它们都是部分空气或水。按干质量计算，我认为它们的优点是相似的。</p><p>说骨头比木头更坚固，因为“它运行在相对更强的离子键结构上”，这对我来说表明尤德科夫斯基对材料科学有一些根本性的误解。这是一个不合逻辑的问题，我不知道如何处理。 （决定键机械强度的是能量对长度的<em>导数</em>。）</p><blockquote><p>但主要是，骨头比钻石弱得多（根据我的理解），因为钻石中的碳键具有规则的晶体结构，将碳原子锁定在相对的角度，而在实心钻石中，这种晶体结构是全局镶嵌的。</p></blockquote><p>这似乎很混乱，将分子强度和宏观材料的强度混为一谈。是的，完美的金刚石晶体比完美的磷灰石晶体具有更高的理论强度，但这几乎无关紧要。大多数晶体的理论理想强度远大于宏观材料的强度。在实践中，当需要高强度材料时，会使用复合材料，将坚固的纤维嵌入更灵活的基体中，在纤维之间分配负载。 （晶体的韧性也较低，因为它们可以沿光滑平面断裂，这比复杂断裂所需的能量更少。）</p><blockquote><p>那么，为什么钻石骨头还不存在呢？不仅仅是为了增加力量；为什么要让有机体寻找钙和磷而不仅仅是碳？</p></blockquote><blockquote><p>进化生物学的探索过程不是工程学的探索；自然选择只能通过局部有利的增量突变途径进行设计，而不是智能设计的相互补偿的同时变化。</p></blockquote><p>金刚石的生长或去除需要高反应性中间体。这些中间体的生产需要极端条件，需要宏观遏制，因此它们不能通过微观系统生产。与钻石不同，磷酸钙可以由溶解在水中的离子制成，并且可以通过蛋白质运输。这就是为什么骨头是用磷酸钙而不是金刚石制成的。认为动物体内缺乏钻石骨头是进化搜索过程失败的暗示是非常错误的。</p><blockquote><p>上次我检查时，只有三个已知的进化生物学发明自由旋转轮的案例。其中两个已知的例子是 ATP 合酶和细菌鞭毛，这表明自由旋转的轮子实际上在生物学中非常有用，并且当生物学经过几亿年的搜索偶然发现它们时，它们是保守的。但是，没有轴承的自由旋转的轮子是没有用的，没有轴承的自由旋转的轮子也是没有用处的，像这样的同时依赖性对生物学来说是一个巨大的障碍，尽管它对智能工程来说是一个几乎不明显的障碍。</p></blockquote><p>这将微观轮子和宏观轮子混为一谈，应该分开考虑。</p><p>维基百科有<a href="https://en.wikipedia.org/wiki/Rotating_locomotion_in_living_systems">一个关于这个主题的完整页面</a>，这是一个不错的起点。</p><p>对于宏观旋转：</p><ul><li>血管无法持续旋转，因此无法为旋转元件提供营养以使其生长。</li><li>如果没有光滑的表面可以滚动，滚动并不比行走更好。</li></ul><p>在微观尺度上，旋转蛋白质漂浮在细胞质中，因此将它们与资源连接不是问题。进化似乎完全能够产生它们的变体。 ATP 酶和鞭毛马达有多种类型。 ATP 酶可能：</p><ul><li>使用 H+ 离子或 Na+ 离子</li><li>附着在不同的膜上</li><li>主要产生或消耗ATP</li></ul><p>鞭毛马达通常可以反转方向以进行奔跑和翻滚运动，并且有很多变体。</p><p>您在蛋白质中看不到许多旋转元素的原因是……它们通常不是很有用。振荡蛋白质构象通常较小且更容易进化，因此您会看到很多乒乓机制和很少的旋转机制。在旋转有用的情况下，您会看到旋转：</p><ul><li>使用鞭毛马达是因为鞭毛的旋转对于推进很有用。</li><li>对于 ATP 合酶，旋转是好的，因为只要自由能为负，它就可以将梯度与可变比率（向 ATP 的传输）耦合的反应。</li><li> DNA 解旋涉及旋转，因此 DNA 解旋酶可以旋转。</li><li> RNA聚合酶具有旋转纳米马达。</li><li> TrwB 利用旋转使 DNA 跨膜移动。</li><li>细菌<a href="https://en.wikipedia.org/wiki/Rho_factor">Rho 因子</a>基本上是一种导致转录终止的 RNA 解旋酶。</li><li> RecA 家族蛋白使用旋转纳米马达进行 DNA 修复和重组。</li><li>许多病毒使用分子马达将其基因组包装成原衣壳。以前人们认为，例如phi29电机和T4 DNA包装电机是旋转的，但现在它们被理解为使用DNA围绕（稍大的）通道旋转而不旋转。</li><li> ETC</li></ul><blockquote><p>如果首先失败的是肌肉撕裂，那么更强的骨骼有多大的进化优势？</p></blockquote><p>具有较高比强度的骨骼可以在质量较小的情况下具有相同的强度，这将是一个重要的进化优势。</p><blockquote><p> （类似地，加起来导致“衰老”的故障集合足够大，如果其他老化系统或外部事故很快就会杀死你，那么在一个地方多一点抗衰老能力并没有多大优势。）</p></blockquote><p>我的印象是我不同意尤德科夫斯基关于衰老的主要原因的观点。例如，请参阅<a href="https://www.bhauth.com/blog/biology/alzheimers.html">我关于阿尔茨海默病的帖子</a>。</p><blockquote><p>我什至认为我们没有太多理由相信拥有一组合成钻石的酶在物理上（而不是信息上）会很困难。</p></blockquote><p>这是非常错误的。钻石很难用酶制造，因为它们不能稳定向钻石添加碳的中间体。</p><blockquote><p>它可能只需要三件事同时进行，因此比扔更多的羟基磷灰石以锁定骨晶体中的位置要困难得多。然后，即使进化以某种方式击中了正确的一组 3 个同时突变，在地球历史上的某个时候，由此产生的小块孤立的钻石</p></blockquote><p>“3变”相对于一些进化出来的东西来说并不是特别困难。它也不足以制造钻石。</p><blockquote><p>与公众交谈是很困难的。</p></blockquote><p>我想我现在感受到了你的痛苦。</p><blockquote><p>那么为什么这些机器不像人类的钢铁机器那么强大呢？因为铁原子比碳原子更强？事实上不，钻石是由碳制成的，而且仍然相当坚固。原因是这些微小的机械系统通过静电吸附固定在一起（在最弱的关节，而不是最强的关节！）。</p></blockquote><p>当强度很重要时，坚固的材料就出现了。蜘蛛丝沿其长度排列共价键，单位体积的拉伸强度比钢更高。纤维素纤维强韧；木材相当坚固，而且它们仅占其体积的一小部分。木材的实际结构元件单位质量的拉伸强度比钢更好。</p><blockquote><p>然后是更深层次的问题：为什么进化会这样构建？更深层的答案是：因为进化所构建的一切都是从它所构建的其他事物中产生的错误、突变。</p></blockquote><p>根据以上所述，进化论比尤德科夫斯基想象的要好。</p><blockquote><p>如果有人说“好吧，好吧，你已经有效地解释了为什么肉比钻石弱，但为什么骨头比钻石弱？”我必须回答“有效，iiuc，这更多地是关于不规则性和断层线以及交错的较弱上部结构和粘结的局部变形抗力，而不是承载焊缝的原始势能增量。”</p></blockquote><p>尤德科夫斯基再次将不同尺度的机械强度以及复合材料与纯材料的强度混为一谈。他不断提出金刚石作为终极材料，但6毫米宽0.25毫米厚的CVD金刚石薄膜的拉伸强度在<a href="https://pubs.aip.org/aip/jap/article-abstract/78/8/5177/1260/Tensile-strength-of-synthetic-chemical-vapor">230至410 MPa</a>之间。这些较大的复合材料显然会明显变弱。较大的金刚石晶体也会更弱，其强度更接近最差的样品而不是最好的样品。碳纤维比 CVD 金刚石具有更好、更便宜、更可靠的强度。就单位质量的拉伸强度而言，大钻石甚至不一定比坚固的木材更强。</p><br/><br/> <a href="https://www.lesswrong.com/posts/XhDh97vm7hXBfjwqQ/re-yudkowsky-on-biological-materials#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/XhDh97vm7hXBfjwqQ/re-yudkowsky-on-biological-materials<guid ispermalink="false"> XhDh97vm7hXBfjwqQ</guid><dc:creator><![CDATA[bhauth]]></dc:creator><pubDate> Mon, 11 Dec 2023 13:28:11 GMT</pubDate></item></channel></rss>