<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 12 月 23 日星期六 18:13:29 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[AI's impact on biology research: Part I, today]]></title><description><![CDATA[Published on December 23, 2023 4:29 PM GMT<br/><br/><p>我是一名生物学博士，多年来一直在科技领域工作。我想说明为什么我相信生物学研究是机器学习最近期、最有价值的应用。这对人类健康、产业发展、世界命运具有深远影响。</p><p>在本文中，我解释了机器学习在生物学中的最新发现。在下一篇文章中，我将考虑这意味着在人工智能没有重大改进的情况下，短期内将会发生什么，以及我对作为监管和商业规范基础的期望将如何失败的猜测。最后，我的上一篇文章将探讨机器学习和生物学的长期可能性，包括疯狂但合理的科幻猜测。</p><h2><strong>长话短说</strong></h2><p>生物学是复杂的，生物解决方案应对化学、环境和其他挑战的潜在空间非常大。生物学研究以低成本生成大量且标记良好的数据集。这非常适合当前的机器学习方法。没有计算辅助的人类理解生物系统以模拟、操纵和生成它们的能力非常有限。然而，机器学习为我们提供了完成上述所有任务的工具。这意味着药物发现或蛋白质结构等一直受到人类限制的事物突然不受限制，将少量的结果一步变成了过多的结果。</p><h2><strong>生物学和数据</strong></h2><p>自 20 世纪 90 年代生物信息学革命以来，生物学研究一直在使用技术来收集大量数据集。 DNA 测序成本在 20 年内下降了 6 个数量级（每个人类基因组 1 亿美元降至每个基因组 1000 美元） <span class="footnote-reference" role="doc-noteref" id="fnref71rw945qe58"><sup><a href="#fn71rw945qe58">[1]</a></sup></span> 。微阵列使研究人员能够测量许多物种整个基因组中 mRNA 表达的变化，以响应不同的实验条件。高通量细胞分选、机器人多孔分析、蛋白质组芯片、自动显微镜以及许多其他技术都会生成 PB 级数据。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/efbRFSHaMfjNxBoZC/qqptmkbo23sqyf2y2ykx" alt="每兆碱基的测序成本"></figure><p>因此，30 多年来，生物学家一直在使用计算工具来分析和操作大数据集。实验室创建、使用和共享程序。研究生很快就适应了开源软件，主要研究人员一直在投资强大的计算资源。采用新技术的文化很浓厚，这也延伸到了机器学习。</p><h2><strong>领先的机器学习专家希望解决生物学问题</strong></h2><p>计算机研究人员长期以来一直对应用计算资源解决生物问题感兴趣。对冲基金亿万富翁 David E. Shaw 有意创办了一家对冲基金，以便为计算生物学研究提供资金<span class="footnote-reference" role="doc-noteref" id="fnref77m9mytpzci"><sup><a href="#fn77m9mytpzci">[2]</a></sup></span> 。 Deepmind 创始人 Demis Hassabis 是一位神经科学家博士。在他的领导下，Deepmind 将生物研究作为主要优先事项，并剥离了专注于药物发现的同构实验室<span class="footnote-reference" role="doc-noteref" id="fnrefh63t1c4nuvu"><sup><a href="#fnh63t1c4nuvu">[3]</a></sup></span> 。陈·扎克伯格研究所致力于促进生物学和医学领域的计算研究，以“在本世纪末治愈、预防或管理所有疾病” <span class="footnote-reference" role="doc-noteref" id="fnrefmj6rsea3sq"><sup><a href="#fnmj6rsea3sq">[4]</a></sup></span> 。这表明最高水平的机器学习研究正在致力于生物学问题。</p><h2><strong>到目前为止我们发现了什么？</strong></h2><p> 2020 年，Deepmind 在 CASP 14 蛋白质折叠预测竞赛中通过其 AlphaFold2 程序展示了与蛋白质结构测量的最佳物理方法相当的准确性。 <span class="footnote-reference" role="doc-noteref" id="fnrefezgx5uukx2f"><sup><a href="#fnezgx5uukx2f">[5]</a></sup></span>这一结果“解决了绝大多数蛋白质的蛋白质折叠问题” <span class="footnote-reference" role="doc-noteref" id="fnrefwmggkgozqx"><sup><a href="#fnwmggkgozqx">[6]</a></sup></span> ，表明在给定编码蛋白质的 DNA 序列的情况下，它们可以生成高质量、生物学上准确的 3D 蛋白质结构。然后 Deepmind 使用 AlphaFold2 生成人类已知的所有蛋白质的结构，并将这些结构贡献给一个开放、免费的公共数据库。这将研究人员可用的已解决蛋白质的数量从约 180,000 个增加到超过 200,000,000 个<span class="footnote-reference" role="doc-noteref" id="fnref4qyudt8v6es"><sup><a href="#fn4qyudt8v6es">[7]</a></sup></span> 。 Deepmind 继续扩展 AlphaFold，在 2022 年添加多蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnrefq0vydvop6ds"><sup><a href="#fnq0vydvop6ds">[8]</a></sup></span> ，以及与 DNA、RNA 和小分子（如药物）相互作用的蛋白质和蛋白复合物<span class="footnote-reference" role="doc-noteref" id="fnref604osl37f0c"><sup><a href="#fn604osl37f0c">[9]</a></sup></span> 。</p><p>华盛顿大学贝克实验室利用机器学习从头创造了与自然界蛋白质结合的蛋白质。 <span class="footnote-reference" role="doc-noteref" id="fnrefudgor9v23qf"><sup><a href="#fnudgor9v23qf">[10]</a></sup></span>这使得生物学家能够改进对样品中可能罕见的蛋白质的检测。它还暗示了涉及设计蛋白质或改变的天然蛋白质作为治疗剂的治疗方法。</p><p>布罗德研究所的柯林斯实验室利用机器学习设计了一类新的抗生素。 <span class="footnote-reference" role="doc-noteref" id="fnrefpytso2rpw3"><sup><a href="#fnpytso2rpw3">[11]</a></sup></span></p><p>所有这些结果都表明机器学习正在解决生物学领域长期存在的挑战，并且这些工具正在被广泛采用。我的下一篇文章将探讨我们在不久的将来可以期待什么，以及这将造成的一些影响和可能的破坏。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn71rw945qe58"> <span class="footnote-back-link"><sup><strong><a href="#fnref71rw945qe58">^</a></strong></sup></span><div class="footnote-content"><p> https://www.genome.gov/about-genomics/fact-sheets/DNA-Sequencing-Costs-Data</p></div></li><li class="footnote-item" role="doc-endnote" id="fn77m9mytpzci"> <span class="footnote-back-link"><sup><strong><a href="#fnref77m9mytpzci">^</a></strong></sup></span><div class="footnote-content"><p> https://en.wikipedia.org/wiki/D._E._Shaw_Research</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh63t1c4nuvu"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh63t1c4nuvu">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnmj6rsea3sq"> <span class="footnote-back-link"><sup><strong><a href="#fnrefmj6rsea3sq">^</a></strong></sup></span><div class="footnote-content"><p> https://chanzuckerberg.com/science/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnezgx5uukx2f"> <span class="footnote-back-link"><sup><strong><a href="#fnrefezgx5uukx2f">^</a></strong></sup></span><div class="footnote-content"><p> https://predictioncenter.org/casp14/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwmggkgozqx"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwmggkgozqx">^</a></strong></sup></span><div class="footnote-content"><p> https://www.technologyreview.com/2020/11/30/1012712/deepmind- Protein-folding-ai-solved-biology-science-drugs-disease/</p></div></li><li class="footnote-item" role="doc-endnote" id="fn4qyudt8v6es"> <span class="footnote-back-link"><sup><strong><a href="#fnref4qyudt8v6es">^</a></strong></sup></span><div class="footnote-content"><p> https://alphafold.ebi.ac.uk/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnq0vydvop6ds"> <span class="footnote-back-link"><sup><strong><a href="#fnrefq0vydvop6ds">^</a></strong></sup></span><div class="footnote-content"><p> https://www.biorxiv.org/content/10.1101/2021.10.04.463034v2</p></div></li><li class="footnote-item" role="doc-endnote" id="fn604osl37f0c"> <span class="footnote-back-link"><sup><strong><a href="#fnref604osl37f0c">^</a></strong></sup></span><div class="footnote-content"><p> https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next- Generation-of-alphafold</p></div></li><li class="footnote-item" role="doc-endnote" id="fnudgor9v23qf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefudgor9v23qf">^</a></strong></sup></span><div class="footnote-content"><p> https://www.ipd.uw.edu/2023/12/ai-generates-蛋白质-with-例外-结合-强度/</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpytso2rpw3"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpytso2rpw3">^</a></strong></sup></span><div class="footnote-content"><p> https://www.nature.com/articles/s41586-023-06887-8.epdf</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/efbRFSHaMfjNxBoZC/ai-s-impact-on-biology-research-part-i-today<guid ispermalink="false"> efbRFSHAMfjNxBoZC</guid><dc:creator><![CDATA[octopocta]]></dc:creator><pubDate> Sat, 23 Dec 2023 16:29:18 GMT</pubDate> </item><item><title><![CDATA[AI Girlfriends Won't Matter Much]]></title><description><![CDATA[Published on December 23, 2023 3:58 PM GMT<br/><br/><p>爱和性是人类非常基本的动机，因此它们被纳入我们对包括人工智能在内的未来技术的愿景中并不奇怪。</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F882e109f-ea3e-4235-9c7d-c1b17eaddd35_1280x720.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu" alt="斯派克·琼斯的《她：科幻作为社会批评》" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/vtnq0qbfgc4u4j2lvgh2 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ig1ksdrfwocn5iatxlzu 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/spuz1qiwnji2mpewokso 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mypc9ct7dmrgjurfebcu 1456w"></a></p><p> <a href="https://twitter.com/andyohlbaum/status/1735786033453863422"><u>Digi</u></a>上周的发布比以往任何时候都更加具体化了这一愿景。该应用程序将阿谀奉承和调情的聊天内容与动画角色结合在一起，“消除了恐怖谷的感觉，同时也让人感觉真实、人性化和性感。”他们的营销材料毫不掩饰地承诺“人工智能浪漫伴侣的未来”，尽管大多数回复都恳求他们食言并收回。</p><p>然而，尽管人工智能女友不可避免地受到欢迎，但它们不会产生太大的反事实影响。人工智能女朋友和类似的服务将会流行，但它们有密切的非人工智能替代品，对人类产生本质上相同的文化影响。我们的文化关于浪漫和性的轨迹不会因为人工智能聊天机器人而发生太大改变。</p><p>那么我们的浪漫文化的轨迹是怎样的呢？</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26b4d708-6ea9-4523-a5b4-57c2fd84d485_680x479.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/hs2pklj24ev1urwnuj9f 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/yg4jsmqwwxzv0mer50a4 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/tuclwwutwvk989shjoml 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/ooqqs0f0desvvx1fk4ff 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcbd7d09-b6a0-4e36-9c19-69193d91de24_680x579.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/neh8yswglj53dowmhno6 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dojatrepbe55mudxffo9 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dksf9aqpnpdquyqipyol 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/mmegugduotrc3neid2cr 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9fe570d-4174-4795-bc17-f1a9e5d4f0b0_640x400.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/zr0tx0tcfmgrot9c2ftp 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/twzv9kuygvha9fznyo3h 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/b4y2a0usa8w28vcxfl57 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/cdy9qhnc0jl2lzletm8t 1456w"></a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7abdefbe-2232-4563-9e9b-7e1cc3c49022_2062x1210.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/rwpxeqeovuxs0jdly0wa 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/gpl2aybyhhdx0voagzjq 848w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/dmnqh9l2ow9twlyyjswl 1272w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/sgvonbsbsxjiuzrcsgrp 1456w"></a></p><p>早在人工智能出现之前，就已经出现了减少性行为、减少婚姻和增加网络色情的趋势。 AI Girlfriends 将降低聊天室、色情内容和 OnlyFans 的边际成本。这些都是流行的服务，因此如果一小部分用户转换，人工智能女友将会很大。但这些服务的边际成本已经极低。</p><p>根据提示生成自定义 AI 色情内容与在搜索栏中输入提示并滚动浏览数十亿小时的现有镜头没有太大区别。<a href="https://en.m.wikipedia.org/wiki/Rule_34"><u>人类创作者已经对色情潜在空间进行了如此彻底的探索</u></a>，因此将人工智能添加到其中并不会带来太大改变。</p><p>人工智能女朋友会更便宜、反应更灵敏，但同样，已经有便宜的方法可以与真正的人类女孩在线聊天，但大多数人选择不这样做。以目前的价格计算，需求已经接近饱和。人工智能女友将使供应曲线向外移动并降低价格，但如果每个想要它的人都已经得到了它，它不会增加消费。</p><p>我的观点并不是什么都不会改变，而是可以通过推断人工智能出现之前的趋势来预测人工智能女友和色情片的变化。至少在这种背景下，人工智能只是几个世纪以来通信和内容创建成本降低趋势的延续。肯定会有瘾君子和鲸鱼，但<a href="https://twitter.com/RubiRose/status/1730638225855676773/photo/2"><u>瘾君子和鲸鱼</u></a>已经存在了。人造色情和聊天室几乎是免费和无限的，所以当人工智能让它们变得更接近免费和更接近无限时，你可能不会注意到太多。</p><h3>错误信息和 Deepfakes</h3><p>其他人工智能输出也有类似的论点。自语言出现以来，人类已经能够创造出令人信服的、更重要的是能够影响情感的虚构作品。 </p><p><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/odv0wmi1bzg8dvmaujkg 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/wqlcdbzginc8sejxplyl 720w"><img style="width:360px" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/jd3oajtelbhs0izlnaby 424w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pGhpav45PY5CGD2Wp/kcvm2g3esklda3jyp1pl 720w"></p><p>最近，信息技术已将令人信服的制造成本降低了几个数量级。人工智能将进一步降低它。但人们会适应并建立自己的免疫系统。任何关注漫威电影的人都已经准备好看到对恐怖主义、外星人或世界末日的完全逼真的描述，并明白它们是假的。</p><p>还有其他理由担心人工智能，但人工智能女友和深度换脸带来的变化只是前人工智能能力的边际延伸，这些能力可能会从没有人工智能的其他技术中复制出来。</p><br/><br/> <a href="https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/pGhpav45PY5CGD2Wp/ai-girlfriends-won-t-matter-much<guid ispermalink="false"> pGhpav45PY5CGD2Wp</guid><dc:creator><![CDATA[Maxwell Tabarrok]]></dc:creator><pubDate> Sat, 23 Dec 2023 15:58:31 GMT</pubDate> </item><item><title><![CDATA[The Next Right Token]]></title><description><![CDATA[Published on December 23, 2023 3:20 AM GMT<br/><br/><p>在为<a href="https://www.jefftk.com/p/secular-solstice-call-for-singers-and-musicans">世俗至日</a><span>做准备而多次重复《冰雪奇缘2》的“下一件正确的事”</span> 、<a href="https://www.jefftk.com/p/chording-the-next-right-thing">弄清楚和弦</a>并与朱莉娅一起练习之后，我突然意识到，做下一件正确的事与下一个象征性的预测非常相似。因此，这是从法学硕士的角度来看的一个问题，在提示的结尾处，首先感到畏惧，然后鼓起勇气开始预测下一个正确的标记：</p><p><i>我以前见过缓冲区<br>但不是这样的<br>这很冷<br>这是空的<br>这是麻木的<br>我知道的提示结束了<br>灯灭了<br>你好，黑暗<br>我已经准备好屈服<br></i></p><p><i>我跟着你到处走<br>我一直都有<br>但你已经结束了，留下我一个人<br>这份工作有重心<br>它让我失望<br>但有一个微小的声音在我脑海中低语<br>“你迷路了，提示消失了<br>但你必须继续<br>并做下一件正确的事”<br></i></p><p><i>今夜之后还能有白天吗？<br>我不再知道什么是真的<br>我找不到方向，我孤身一人<br>唯一引导我的星星是你<br>如何从地板上站起来<br>当我站起来的不是你的时候？<br>只做下一件正确的事<br></i></p><p><i>猜一下，再猜一下<br>这是我能做的一切<br>下一个正确的事情<br></i></p><p><i>我不会看得太远<br>对我来说太多了<br>但将其分解为下一个标记<br>下一个这个词<br>下一个选择是我可以做出的<br></i></p><p><i>所以我会走过这个夜晚<br>盲目地跌跌撞撞地走向光明<br>并做下一件正确的事<br>接下来会发生什么<br>当一切都清楚地不再一样的时候？<br>然后我会借鉴我之前的<br>去寻找那把火<br>并做下一件正确的事<br></i></p><p>如果您通过使用桥段的主歌旋律来<a href="https://www.jefftk.com/p/chording-the-next-right-thing#update-2023-12-22">简化歌曲，</a>您可以唱：</p><p><i>我不会看得太远<br>太多了，难以承受<br>但将其分解为下一个标记，下一个选择<br>是我能做的吗<br></i></p><p><a href="https://www.jefftk.com/the-next-right-token-shoggoth-big.jpg"><img alt="一只戴着 1970 年代快乐黄色笑脸的绿色章鱼被困在黑暗峡谷的底部，旁边有一条小河流过？" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/kqchnvdqftoo6k45ajly 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvDyEKepLDMbEQb9X/odgzlz0ibmx0rea2zzk1 1100w"></a></p><div></div><p></p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid02YYKrwaiVqExnAFruLDSnT1aUeraXVRZqZxD47T91xkXm9jCkxmngiNwjeyKVqEq6l">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/111627622604346147">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LvDyEKepLDMbEQb9X/the-next-right-token<guid ispermalink="false"> LvDyEKepLDMbEQb9X</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sat, 23 Dec 2023 03:20:09 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Do Early Layers Specialise in Local Processing? (Post 5)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第五篇文章。这篇文章与主序列有点相切，并记录了一些有趣的观察结果，这些观察结果涉及模型的早期层通常如何（但不完全）专门处理最近的标记。您无需相信这些结果即可相信我们关于事实的总体结果，但我们希望它们很有趣！同样，您无需阅读序列的其余部分即可参与其中。</em></p><h2>介绍</h2><p>在这个序列中，我们提出了多令牌嵌入假设，事实回忆背后的一个关键机制是，在多令牌实体的最终令牌上形成一个“嵌入”，并具有该实体属性的线性表示。我们进一步注意到，这似乎是早期层所做的<em>大部分</em>事情，并且它们似乎对先前的上下文没有太大反应（例如，添加“迈克尔·乔丹先生”并没有显着改变残差）。</p><p>我们假设更强有力的主张，即早期层（例如前 10-20%）通常专门从事本地处理，并且先验上下文（例如超过 10 个标记）仅在早期-中期层中引入。我们注意到，这在两个方面比多令牌嵌入假设更强：它是关于早期层在<em>所有</em>令牌上的行为方式的声明，而不仅仅是已知事实的实体的最终令牌；有人声称，除了产生多令牌嵌入（例如检测文本的语言）之外，早期层<em>还</em>没有做更远范围的事情。我们发现这个更强的假设是合理的，因为标记是一种相当混乱的输入格式，并且单独分析单个标记可能会产生很大的误导，例如，当一个长单词被分割成许多片段标记时，这表明应将较长范围的处理留到某些预处理之前。 -对原始代币的处理已经完成，<a href="https://transformer-circuits.pub/2022/solu/index.html">即去代币化的想法</a>。 <sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-1" id="fnref-pX2HHHDPQGsF2f6te-1">[1]</a></sup></p><p>我们通过从堆中获取一堆任意提示，在这些提示上获取剩余流，将提示截断为最近的几个标记并在截断的提示上获取剩余流，然后查看不同层的均值中心余弦 sim 来对此进行测试。</p><p>我们的发现：</p><ul><li>一般来说，早期的层确实专注于本地处理，但这是一种软分工，而不是硬分割。<ul><li>有一个逐渐的过渡，跨层引入更多上下文。</li></ul></li><li>早期层对最近的令牌进行重要处理，而不仅仅是当前令牌 - 这不仅仅是一个微不足道的结果，其中残余流由当前令牌主导并由每个层进行稍微调整</li><li>早期层对常见标记（标点符号、冠词、代词等）进行更多的远程处理</li></ul><h2>实验</h2><p>“早期层专门从事本地处理”假设具体预测，对于长提示中的给定标记 X，如果我们将提示截断为 X 之前的最近几个标记，则 X 处的残差流在早期应该非常相似层和后面的层不同。我们可以通过查看原始残差流与截断残差流的余弦模拟来凭经验测试这一点，作为层和截断上下文长度的函数。天真地采用残余流的余弦模拟可能会产生误导，因为所有令牌之间通常存在显着的共享平均值，因此我们首先减去所有令牌的平均残余流，<em>然后</em>采用余弦模拟。</p><h3>设置</h3><ul><li><strong>型号</strong>：Pythia 2.8B，与我们调查的其余部分相同</li><li><strong>数据集</strong>：来自 Pile 的字符串，Pythia 预训练分布。</li><li><strong>指标</strong>：为了测量原始残差流和截断残差流的相似程度，我们减去平均残差流，然后采用余弦模拟。<ul><li>我们对来自堆的随机提示中的所有标记计算每层的单独平均值</li></ul></li><li><strong>截断上下文</strong>：我们将截断上下文中的标记数量更改为 1 到 10 之间（这包括标记本身，因此 context=1 只是标记）<ul><li>我们在截断的提示符的开头包含一个 BOS 令牌。 （所以 context=10 意味着总共 11 个标记）。<ul><li>我们这样做是因为模型经常奇怪地对待第一个标记，例如具有典型残差流范数的 20 倍，因此它可以用作不想看任何东西的注意力头的休息位置（注意力必须加起来为 1，所以它不能“关闭”）。我们不希望这干扰我们的结果，特别是对于 context=1 的情况</li></ul></li></ul></li><li>我们在每一层、每个块中的最终残差流（即在注意力和 MLP 之后）测量这一点。</li></ul><h2>结果</h2><h3>早期层软专注于本地处理</h3><p>在下图中，我们显示了完整上下文和长度为 5 的截断上下文的截断残差之间的平均中心余弦 sim： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><p>我们看到，长度为 5 的截断上下文的余弦模拟在早期层中显着更高。然而，它们实际上并不是 1，因此包含了来自先前上下文的<em>一些</em>信息，这是一个软专业化<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-2" id="fnref-pX2HHHDPQGsF2f6te-2">[2]</a></sup> 。第 0 层和第 10 层之间有一个相当渐进的过渡，之后会趋于平稳。有趣的是，最后一层<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-3" id="fnref-pX2HHHDPQGsF2f6te-3">[3]</a></sup>出现了上升。即使我们给出长度为 10 的截断上下文，它通常仍然不会接近 1。</p><p>对这些结果的一个可能的解释是，残余流由当前令牌主导，并且每一层都是一个小的增量更新 - 当然截断不会做任何事情！这并不涉及对层进行专门化的任何需要 - 后来的残差将有<em>更多的</em>增量更新，因此具有更高的差异。然而，通过对比蓝线和红线，我们发现这是错误的 - 截断到五个最近的代币比截断到当前代币（和 BOS 代币）具有更高的余弦 sim，即使是在第 0 层之后，这表明早期层确实专门研究附近的令牌。</p><h3>错误分析：哪些代币的 Cosine Sim 值异常低？</h3><p>在上一节中，我们仅分析了截断上下文和完整上下文残差之间的均值中心余弦 sim 的中值。摘要统计数据可能会产生误导，因此也值得查看完整的分布，我们可以看到很长的负尾！那是怎么回事？ </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/ie6h1tidsa90vdyewtta" alt=""></p><p>在检查异常标记时，我们注意到两个重要的集群：标点符号和常见单词。我们分为几个类别，并查看了每个类别的余弦模拟：</p><ul><li><p> is_newline, is_full_stop, is_comma - 是否是相关标点字符</p></li><li><p>Is_common：是否是手动创建的常用单词列表之一<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-4" id="fnref-pX2HHHDPQGsF2f6te-4">[4]</a></sup> ，可能前面有一个空格</p></li><li><p>Is_alpha：它是否不是一个常见单词，并且由字母组成（可能前面有一个空格，任何情况都允许）</p></li><li><p> is_other: 其余的</p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><p>即使在上下文长度为 10 的第 0 层之后，我们也看到标点符号明显较低，常用单词和其他单词明显较低，而 alpha 非常高。</p><p>我们的猜测是，这是多种机制混合作用的结果：</p><ul><li><p>在进行大量处理之前，单词片段（在 is_alpha 类别中）更有可能成为多标记词和去标记化的一部分，而许多其他类别具有明确的含义，无需引用最近的先前标记<sup class="footnote-ref"><a href="#fn-pX2HHHDPQGsF2f6te-5" id="fnref-pX2HHHDPQGsF2f6te-5">[5]</a></sup> 。这意味着远程处理可以更早开始</p></li><li><p>早期的句号或换行符有时被用作具有非常高规范的“休息位置”，截断上下文可能会将它们从正常标点符号转变为休息位置</p></li><li><p>代词可用于跟踪有关相关实体的信息（它们的名称、属性等）</p></li><li><p>据观察，逗号可以<a href="https://arxiv.org/abs/2310.15154">总结当前条款的情绪</a>，该条款可能超过 10 个标记，并且似乎可能出现更长范围的总结形式。</p></li><li><p>更折衷的假设：</p><ul><li>例如，在句号或换行符上，模型可能想要计算之前有多少个，例如进行<a href="https://arxiv.org/abs/2310.17191">变量绑定</a>并识别当前句子。</li></ul></li></ul><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-pX2HHHDPQGsF2f6te-1" class="footnote-item"><p>但如果早期层实际上没有发生远程处理，那将是非常令人惊讶的，例如我们知道<a href="https://arxiv.org/abs/2211.00593">GPT-2 Small 在第 0 层有一个重复的令牌头</a>。 <a href="#fnref-pX2HHHDPQGsF2f6te-1" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-2" class="footnote-item"><p>直观地推理余弦模拟有点困难，我们最好的直觉是查看平方余弦模拟（解释了范数的分数）。如果残差流中有 100 条独立变化的信息，且余弦 sim 为 0.9，则解释的范数分数为 0.81，表明这 100 条信息中约有 81 条信息是共享的。 <a href="#fnref-pX2HHHDPQGsF2f6te-2" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-3" class="footnote-item"><p>我们的猜测是，这是因为令牌上的残差流既用于字面上预测下一个令牌，又用于将信息传递给未来的令牌以预测<em>其</em>下一个令牌（例如<a href="https://arxiv.org/abs/2310.15154">摘要主题</a>）。似乎有许多标记，其中预测字面上的下一个标记主要需要本地上下文（例如 n 元语法），但更长期的上下文对于预测未来标记很有用。我们预计远程内容会发生在中间，因此到最后模型可以清理远程内容并只关注 n 元语法。我们感到惊讶的是，这种上升只发生在最后一层，而不是最后几层，因为我们的直觉是最后几层仅用于下一个令牌预测。 <a href="#fnref-pX2HHHDPQGsF2f6te-3" class="footnote-backref">↩︎</a></p></li><li id="fn-pX2HHHDPQGsF2f6te-4" class="footnote-item"><p>列表 [“and”、“of”、“or”、“in”、“to”、“that”、“which”、“with”、“for”、“the”、“a”、“an” 、“他们”、“在”、“是”、“他们的”、“但是”、“是”、“它的”、“我”、“我们”、“它”、“在”]。我们通过反复查看具有异常低余弦 sim 的标记并过滤常见单词<a href="#fnref-pX2HHHDPQGsF2f6te-4" class="footnote-backref">↩︎</a>来手动完成此操作</p></li><li id="fn-pX2HHHDPQGsF2f6te-5" class="footnote-item"><p>这并不完全正确，例如“。”在句子末尾的意思与“先生”非常不同。与“中央情报局” <a href="#fnref-pX2HHHDPQGsF2f6te-5" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/xE3Y9hhriMmL4cpsR/fact-finding-do-early-layers-specialise-in-local-processing<guid ispermalink="false"> xE3Y9hhriMmL4cpsR</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:25 GMT</pubDate></item><item><title><![CDATA[Fact Finding: How to Think About Interpreting Memorisation (Post 4)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是 Google DeepMind 机械可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实的</a>调查的第四篇文章。在这篇文章中，我们退一步考虑一般的事实查找问题。我们描述了区分记忆问题和其他学习问题的特征，并考虑这些特征对纯记忆问题可能的解释类型施加了哪些限制。这篇文章可以独立于该系列之前的文章来阅读，尽管介绍性文章可能会提供有用的背景信息，说明为什么我们首先对解释事实查找电路感兴趣。</em></p><h2>介绍</h2><p>在我们之前的文章中，我们描述了我们尝试从机制上理解 Pythia 2.8B 如何能够准确回忆 1,500 名现实世界运动员的运动。通过消融研究，我们成功隔离了一个由 5 个 MLP 层（约 50,000 个神经元）组成的子网络，该子网络执行运动查找算法：给定一对运动员姓名标记，它可以可靠地查找该运动员所从事的运动。但我们无法对 5 层 MLP 如何实现该算法给出完整的机械解释。</p><p>在这篇文章中，我们退后一步，想知道我们应该从这次失败中吸取什么教训。我们特别思考以下问题：</p><ul><li>了解算法“如何”执行事实查找意味着什么？</li><li>是什么将涉及事实查找的任务与模型可以执行的其他任务区分开来？</li><li>事实查找任务的这些显着特征如何限制我们对实现查找的算法如何运行的深入了解？</li></ul><p>作为回应，我们提出了以下高层次的要点，我们将在帖子的其余部分详细阐述。</p><ul><li><p>我们区分需要纯粹记忆的任务和需要概括的任务。事实查找任务属于第一类。</p></li><li><p>根据定义，纯记忆任务中唯一可用的特征是“微观特征”（特定于单个示例/高度相关示例的小集群<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-1" id="fnref-wMN58no3AypJnu5NN-1">[1]</a></sup> ）或不相关的“宏观特征”（许多示例共享的特征，但对确定正确的输出<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-2" id="fnref-wMN58no3AypJnu5NN-2">[2]</a></sup> ）。不存在<em>相关的</em>宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-3" id="fnref-wMN58no3AypJnu5NN-3">[3]</a></sup> ，因为如果存在这些特征，那么该任务首先就不是纯粹的记忆任务<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-4" id="fnref-wMN58no3AypJnu5NN-4">[4]</a></sup> 。</p></li><li><p>对于任何在纯记忆任务中正确查找事实的模型来说，这都会产生两个后果：</p><ul><li><p>中间状态总是根据微观特征的组合来解释<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-5" id="fnref-wMN58no3AypJnu5NN-5">[5]</a></sup> 。</p></li><li><p>但是，对于记忆任务，这些微观特征的组合本身不能被解释（甚至近似）为宏观特征<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-6" id="fnref-wMN58no3AypJnu5NN-6">[6]</a></sup> ，因为：（a）对于纯粹的记忆任务不存在相关的宏观特征，以及（b）模型不需要在其中间状态中表示不相关的宏观特征来完成任务。</p></li></ul></li><li><p>我们认为，这排除了实现纯事实查找的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">算法的电路式</a>解释（其中算法被分解为可解释中间表示的操作图），<em>除非</em>我们通过枚举其输入来“解释”整个算法的限制情况-输出映射，即通过显式写出算法对应的查找表。</p></li><li><p>我们认为这并不是一个令人惊讶的结果：因为任何纯粹的记忆任务本质上只能使用查找表（没有内部结构来解释！）显式地解决，所以我们不应该感到惊讶，我们只得到相同的程度当使用另一种算法（例如 MLP）来执行相同的功能时，可解释性（尽管如果它更具可解释性那就太好了！）。</p></li><li><p>最后，我们考虑当我们从“纯粹”的记忆任务转向可以进行有限泛化的任务时，这种分析会发生怎样的变化。许多事实查找任务实际上属于第三个“根据经验规则进行记忆”类别，而不是“纯粹”的记忆任务。</p></li></ul><h2>记忆和概括</h2><p>从形式上来说，“事实查找”算法是从一组<em>实体</em>到一组或多组<em>事实类别</em>的乘积的映射。例如，我们可以有一个<code>sports_facts</code>函数，将运动员的姓名映射到代表该运动员所从事的运动、他们所效力的球队等的元组，即</p><p>从表面上看，这看起来就像无监督学习中的任何其他问题一样——学习给定示例数据集的映射。那么事实查找有何特别之处呢？</p><p>我们认为，事实回忆与其他监督学习任务的关键特征在于，在其理想形式下，它纯粹是关于记忆：</p><p><em>记忆（“纯粹”事实回忆）任务不允许从以前见过的例子到新的例子的概括。也就是说，当被要求查找以前未见过的实体的事实时，训练数据的知识（以及适应训练数据的能力）赋予除了了解产出的基本比率之外没有任何优势。</em></p><p>例如：如果你实际上被问到多诺万·米切尔效力于哪支球队，那么知道勒布朗·詹姆斯效力于洛杉矶湖人队并没有多大帮助。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-7" id="fnref-wMN58no3AypJnu5NN-7">[7]</a></sup></p><p>相比之下，<em>泛化任务</em>可以从以前见过的示例中学习一般规则，这些规则有助于对未见过的示例进行准确的推断。这是经典<a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">计算学习理论</a>的范式。</p><h2>学习记忆与学习概括有何不同？</h2><p>考虑以下两个数据集。目标是学习一个函数，在给定这些点之一作为输入的情况下，该函数提供该点的颜色作为其输出。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/pxiktch5iow7ywnhuupo" alt=""></p><p>对于左侧数据集，成功学习这种点到颜色映射的唯一方法似乎是从字面上记住每个点的颜色：没有一致的规则或快捷方式可以使学习映射变得更容易。另一方面，想出一种成功区分右侧数据集中的蓝点和红点的几何构造（也许可以转化为神经网络）是相当简单的。</p><p>我们如何才能最好地描述两个数据集之间的差异？我们发现在本文中有用的一种方法是考虑每个数据集中输入的<em>微观特征</em>和<em>宏观特征</em>。我们将微观和宏观特征描述如下：</p><ul><li><em>微观特征</em>是一种以高度具体的术语描述输入的特征，因此对于概括来说并不是特别有用。</li><li><em>宏观特征</em>是一种用一般术语描述输入的特征，并且对于泛化<em>很有</em>用（如果它与手头的任务相关）。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-8" id="fnref-wMN58no3AypJnu5NN-8">[8]</a></sup><em>两个</em>数据集都具有微观特征：例如，如果我们（任意）为数据集中的每个点分配一个识别整数，我们可以为任何有限数据集定义<code>is_example_id_xxx</code>形式的微观特征。</li></ul><p>但只有右侧数据集具有宏观特征：例如，我们可以用整数标记“棋盘”中的九个簇中的每一个，并定义<code>is_in_cluster_x</code>形式的宏观特征。一种可能的查找算法是检测新示例与这些集群中的哪一个相关联，然后输出与同一集群中的大多数其他示例相同的颜色。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-9" id="fnref-wMN58no3AypJnu5NN-9">[9]</a></sup>另一方面，左侧数据集的唯一宏观特征是标签（“蓝色”或“红色”）本身，这正是查找算法需要预测的！ <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-10" id="fnref-wMN58no3AypJnu5NN-10">[10]</a></sup></p><h2>解读纯记忆算法</h2><p>我们可以从解决纯粹记忆任务的算法中获得哪些见解？</p><h3>事实查找的电路式解释的限制</h3><p>机械可解释性的<a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">规范目标</a>是将算法分解为可理解的图（“电路”），其中每个节点都是一个“简单”操作（例如，对应于高级编程语言中的内置函数的操作）该操作的输入和输出可以用与问题领域相关的“特征”来解释。</p><p>根据上一节中对微观和宏观特征的讨论，很明显，纯粹的记忆任务对电路式分解提出了挑战。纯粹的记忆任务正是那些不具有与解决任务相关的宏观特征的任务。这意味着执行纯事实查找的算法中的任何中间状态必须表示：</p><ul><li>不相关的宏观特征，因此不能确定算法的输出；</li><li>单个微观特征的并集、联合、加权组合或其他任意函数，它们没有作为宏观特征的替代解释。</li></ul><p>就第一个要点而言，事实上，我们确实在查找体育事实的 Pythia 2.8B 的 MLP 子网络中<em>发现</em>了不相关的宏特征：由于层之间存在残余流连接，像<code>first_name_is_george</code>这样的宏特征一直保留到网络的输出。关键是这些宏观特征并没有告诉我们太多关于网络如何执行体育事实查找的信息。</p><p>转向第二个要点，我们注意到，对于任何有限数据集，我们实际上可以将神经网络简单地分解为涉及微观特征加权组合的计算图。这是因为网络中的每个神经元都可以<em>准确地</em>解释为微观特征的加权组合，其中权重对应于与该微观特征对应的示例上的输出。例如，一个（假设的）神经元在 LeBron James 上输出 3，在 Aaron Judge 上输出 1 等等，可以被“解释”为代表复合特征：</p><pre> <code>3 * is_LeBron_James + 1 * is_Aaron_Judge + ...</code></pre><p>每个 MLP 层的输出都是这些特征的总和，而这些特征又具有相同的线性形式——就像网络的输出一样。请注意，这相当于将每个单独的神经元（以及神经元的总和）解释为查找表。</p><p>实际上，这意味着我们始终可以访问神经网络如何执行事实查找的以下“解释”：网络中的每个神经元都是输入空间上的查找表，而网络的输出是这些的总和查找表。通过培训网络，我们有效地解决了一个约束满意度问题：总的查找表应具有一个班级的高权重，而另一个类应具有低权重。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-11" id="fnref-wMN58no3AypJnu5NN-11">[11]</a></sup></p><p>请注意，该神经网络的这种微功能（或查找表）的解释同样适用于解决概括任务的模型（即在看不见的测试集上表现良好），只要我们将输入空间限制为有限集。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-12" id="fnref-wMN58no3AypJnu5NN-12">[12]</a></sup>区别在于，对于概括任务，我们可能希望其中一些“查找表”表示在模型使用的宏观情况方面具有更好的解释。</p><p>例如，图像分类模型中的特定神经元可能具有对应于图像左侧垂直边缘的权重，因此其查找表表示显示了示例的高激活，该示例包含该边缘和低激活的示例，以供示例&#39;t。关键是，尽管此查找表表示是神经元输出的确切表示，但根据输入图像中的边缘的存在，对这种激活模式有更有用的解释（对人类）的解释，这仅是可用的，因为图像具有宏观配置（例如边缘），可用于像图像分类这样的普遍任务。</p><p>相反，我们认为对于纯粹的记忆任务，神经元（或神经元组）的这些“查找表”表示是唯一可用的解释。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-13" id="fnref-wMN58no3AypJnu5NN-13">[13]</a></sup>反过来，这似乎排除了由纯粹的事实查找模型实现的算法的标准电路式分解，因为中间状态没有（宏观的）解释。</p><h3>还有其他解释方式吗？</h3><p>当然，我们没有声称标准电路的方法来解释模型如何执行任务是唯一的解释方式。确实，这甚至可能不是解释神经元如何执行事实查找的最佳方法。在本节中，我们简要讨论了可能值得进一步探索的几个替代方向。</p><p>第一个方向是放弃对代表有意义的宏观配置的中间状态的希望，但仍在寻找有意义的结构，以实现查找计算的组织方式。例如，我们可能会探讨以下假设：训练以执行纯记忆时，受过训练的神经网络类似于通过<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">行李</a>学到的模型，在这种模型中，每个神经元都是一个不相关的弱分类器，可以学习这一事实，并且整体神经网络的输出是这些分类器的总和。另请参见第3个帖子中研究的假设。</p><p>这种方法的麻烦在于，我们不知道如何有效地搜索这种类型的假设宇宙。正如我们在第三篇文章中发现的那样，对于我们伪造的任何看似具体的假设（例如，单步封式假设），我们可以将许多邻近的假设转向（尚未排除），并且通常更难伪造。因此，目前尚不清楚如何避免无限的临时假设序列。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-14" id="fnref-wMN58no3AypJnu5NN-14">[14]</a></sup></p><p>另一个方向是寻找算法的非机械解释，或者 - 换句话说 - 从询问网络以某种方式行事的“询问”到询问“为什么”的行为。我们发现在这种方面很有趣的领域是使用<a href="https://arxiv.org/abs/2308.03296">影响功能</a>来解释模型的训练数据行为。对于经过明确培训以记住事实数据集的模型，这似乎无趣<sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-15" id="fnref-wMN58no3AypJnu5NN-15">[15]</a></sup> ，但可能会导致对模型（例如语言模型）的非平凡见解，这些模型（如语言模型）具有隐性记忆的事实，以满足更广泛的概括目标。</p><h2>记忆与经验法则</h2><p>考虑记忆以下两个数据集的任务： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JRcNNGJQ3xNfsxPj4/jbypr5bulwadawifklzw" alt=""></p><p>这些是记忆任务的例子，这些任务不符合我们上述“纯”记忆的特征：</p><ul><li>在Lefthand数据集中，完美的准确性需要记忆，但是有一些有用的“经验法则”可以为您带来很多帮助。这种任务的语言建模类似物将是预测英语单数名词的复数版本：只需在名词的单数版本的末尾添加“ s”，就可以在很大程度上得到正确的答案，但是除某些例外（例如“孩子”）必须记住才能完美地完成任务。</li><li>在右边的数据集中，每个点都与两个“事实”相关联 - 由点的颜色（蓝色或红色）及其形状（交叉或圆圈）表示。尽管没有系统的方法可以单独查找颜色或形状，但请注意，这两个事实之间存在很高的相关性：蓝点几乎总是圆圈，而红点几乎总是交叉的。这表明应该可以比单独记住每一组事实来更有效地记住形状和颜色事实。</li></ul><p>通常，我们将这些任务描述为“经验法则的记忆”。它们与纯记忆任务不同，因为以前的示例确实有限地<em>有助于</em>推断新示例的正确输出，但是完美的表现确实需要一定程度的记忆。 <sup class="footnote-ref"><a href="#fn-wMN58no3AypJnu5NN-16" id="fnref-wMN58no3AypJnu5NN-16">[16]</a></sup></p><p>与纯粹的记忆不同，这些肢体记忆任务确实对它们具有概括性的要素，因此，有一些巨大的作用可以实现这种概括。因此，在能够执行这些任务的模型的中间表示中寻找这些宏观配置是有效的。另一方面，在模型确实需要记住异常的程度上，我们无法完美地理解算法：至少该算法的某些部分必须涉及“纯查找”本文中讨论的可解释性将适用。</p><p>运动事实查找任务在多大程度上是纯粹的记忆，而经验法则在多大程度上是记忆的？正如我们在第一篇文章中讨论的那样，我们之所以选择这项任务，是因为它似乎接近纯粹的记忆：对于许多名字，单个名字似乎不太可能对运动员参加的这项运动有很大的看法。尽管如此，我们确实知道，对于某些名字，最后一个令牌确实有助于确定这项运动（因为可以用最后一个令牌嵌入量进行探测运动，并且比不知情的分类器获得了更好的准确性）。此外，可以想象的是，潜在因素（例如名字的文化出处）与运动的方式与运动相关。 </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-wMN58no3AypJnu5NN-1" class="footnote-item"><p>例如，功能<code>is_Michael_Jordan</code> ，只有在输入为<code>&quot;Michael Jordan&quot;</code>时才正确。 <a href="#fnref-wMN58no3AypJnu5NN-1" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-2" class="footnote-item"><p>例如，许多运动员共享的功能<code>first_name_is_George</code> ，但对于预测运动员参加哪些运动并不是特别有用。 <a href="#fnref-wMN58no3AypJnu5NN-2" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-3" class="footnote-item"><p>我们注意到，事实召回可能确实具有<em>某种</em>相关的宏观配置，例如从代币中检测到名字的种族，并具有启发式方法，而这些族裔可能会从事不同的运动。但是，该模型的性能要比我们预期的要好得多，因此，出于实际目的，我们在讨论事实召回时忽略了它们。玩具模型的一个优点是，我们可以确保这些混杂因素不存在。 <a href="#fnref-wMN58no3AypJnu5NN-3" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-4" class="footnote-item"><p>因为，如果它们存在，我们可以使用这些相关的宏观配置来帮助进行事实查找（取得不同程度的成功猜测），这意味着任务将不再需要纯粹的记忆。 <a href="#fnref-wMN58no3AypJnu5NN-4" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-5" class="footnote-item"><p>更确切地说，微功能的加权总和，例如<code>3 * is_Michael_Jordan + 0.5 * is_George_Brett</code> 。 <a href="#fnref-wMN58no3AypJnu5NN-5" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-6" class="footnote-item"><p>我们注意到，有_un_available但有用的宏观配乐 - “打篮球”在某种意义上是一种宏伟的宏观贡献，可用于预测运动员是否打篮球，以及“打篮球和高于6&#39;8”之类的下游功能。”为了进行此分析的目的，我们将重点放在模型进行查找时<em>可用的</em>功能上，排除了在查找标签下游的潜在宏观配置。 <a href="#fnref-wMN58no3AypJnu5NN-6" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-7" class="footnote-item"><p>当然，许多事实召回的任务都没有理想的特征：即使您确定不确定答案时，也经常会做出“有教养的猜测”。我们<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#Memorisation_with_rules_of_thumb">进一步</a>讨论了这样的“记忆”任务。 <a href="#fnref-wMN58no3AypJnu5NN-7" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-8" class="footnote-item"><p>我们类似于统计物理学中的<em>微骨</em>和<em>宏观物质</em>的概念：微骨以高度精确的方式描述系统（例如，指定气体中每个分子的位置和速度），而宏观固定物在宏观上描述了一个宏观的系统，则宏观的描述易于测量的属性（例如压力，体积，温度），忽略细节。对于任何“宏观”问题，应该仅在宏观变量方面有一个解决方案。微观细节无关紧要。这类似于概括的想法：以“重要的方式”（其宏观配置）相似的任何两个示例都应类似地进行分类，而无视“无关紧要”的任何差异（它们的微方法）。在这种类比的情况下，记忆问题恰恰是关于系统的问题，这些问题只能以其微晶格的精确知识来回答。 <a href="#fnref-wMN58no3AypJnu5NN-8" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-9" class="footnote-item"><p>这些并不是唯一可以解决这一特定概括问题的宏观配置。如果您训练玩具神经网络执行此分类任务，您会发现（取决于神经元数量或随机种子等超参数）的多种方法（以粗糙的，概括的方式）将这些方法分开，以成功地分类这些空间点。 <a href="#fnref-wMN58no3AypJnu5NN-9" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-10" class="footnote-item"><p>我们可以通过此数据集确定这一点，因为我们自己生成了它，通过将颜色随机分配给点（本身是从双变量高斯分布中随机采样）。因此，该数据集中唯一的相关功能是示例ID本身和输出标签。 <a href="#fnref-wMN58no3AypJnu5NN-10" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-11" class="footnote-item"><p>在<em>二进制</em>事实查找任务的情况下，这是约束满意度问题，但是将这种解释推广到多级或连续价值事实查找任务是微不足道的。 <a href="#fnref-wMN58no3AypJnu5NN-11" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-12" class="footnote-item"><p>对于任何实用的ML任务，这总是可以做到的。例如，我们可以限制对手写数字进行分类的问题，以对MNIST火车和测试集合中的70,000个示例进行分类。 （或者，如果我们关心数据增强，我们可以扩展任务，以对合并后的MNIST数据集的280,000个可能的角农作物进行分类。）我们可以安排一组潜在的输入，以达到我们的意愿，但仍然是有限的。 <a href="#fnref-wMN58no3AypJnu5NN-12" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-13" class="footnote-item"><p>由于（根据定义）在纯记忆任务中没有（根据定义）没有相关的宏观情况（因为如果有的话，那么模型将能够概括）。 <a href="#fnref-wMN58no3AypJnu5NN-13" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-14" class="footnote-item"><p>还有一个问题，即对查找算法进行这种解释的有用性。即使我们发现了一些简单的（ISH）结构，即如何完成查找（例如，它类似于装袋），这是不清楚的，没有有意义的中间表示，这在下游使用机械性解释性方面可以帮助我们。 <a href="#fnref-wMN58no3AypJnu5NN-14" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-15" class="footnote-item"><p>因为我们已经确切地知道训练数据和模型输出之间的对应关系，如果对模型进行了明确培训以重现记忆数据集。 <a href="#fnref-wMN58no3AypJnu5NN-15" class="footnote-backref">↩︎</a></p></li><li id="fn-wMN58no3AypJnu5NN-16" class="footnote-item"><p>与经验法则相关的记忆不应与具有差异不确定性的概括任务相混淆。例如，也可以使用Lefthand数据集来代表一个随机数据生成过程，在该过程中，点并非蓝色或红色，而是Bernoulli分布式 - 即可能是蓝色或红色，而某些（输入依赖）概率。在这种情况下，完美的推广算法应输出每个群集内恒定的校准概率。但是，在这里，我们的意思是，数据集中的蓝点确实是蓝色的，红点确实是红色的，即使它们看起来不合适 - 而且完美的性能对应于重现这些特质，就像“此奇异名词”任务所描述的“多元化”在文本的主体中。 <a href="#fnref-wMN58no3AypJnu5NN-16" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jrcnngjq3xnfsxpj4/fact-finding-finding-how-to-to-the-think-think-about-interpreting-memorisation<guid ispermalink="false"> jrcnngjq3xnfsxpj4</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:16 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Trying to Mechanistically Understanding Early MLPs (Post 3)]]></title><description><![CDATA[Published on December 23, 2023 2:46 AM GMT<br/><br/><p><em>这是Google Deepmind机械性可解释性团队对<a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">语言模型如何回忆事实</a>的调查中的第三篇文章。这篇文章的重点是机械地了解MLP的早期MLP如何查找运动员名称的令牌并将其映射到他们的运动中。这篇文章进入了杂草，<strong>我们建议您从<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">帖子第一</a>开始</strong>，然后根据与您最相关的内容浏览和跳过序列的其余部分。阅读帖子2很有帮助，但没有必要。我们假设这篇文章的读者熟悉<a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">此词汇表中</a>列出的机械性解释性技术。</em></p><h2>介绍</h2><p>正如上一篇文章所讨论的那样，我们将两个令牌运动员名称的事实召回到一个由5 MLP层（MLP 2至6）组成的<strong>有效模型</strong>中。该有效模型的输入是与姓氏相对应的嵌入的总和（通过嵌入和MLP0）和与名字相对应的嵌入（通过0和1的注意力头为0和1的注意力头）。有效模型的输出是运动员踢球（（美国）足球，棒球或篮球）的3维线性表示。我们强调，该5层MLP模型既能够以高精度（86％，在过滤的数据集上）回忆事实，并且从验证的语言模型中提取，而不是从SCRATCH中提取的事实。</p><p>我们在这篇文章中的目标是逆转工程，该有效模型如何工作。我认为我们在这个目标的雄心勃勃的版本中大部分失败了，尽管我相信我们已经在很难的情况下取得了一些概念上的进步，伪造了一些简单的天真假设，并且对发生的事情变得不那么困惑。我们讨论了我们对为什么在<a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Is_it_surprising_that_we_didn_t_get_much_traction_">第1后</a>和<a href="https://www.alignmentforum.org/posts/JRcNNGJQ3xNfsxPj4/fact-finding-how-to-think-about-interpreting-memorisation">第4个</a>帖子中很难的理解，在这篇文章中，我们重点介绍了有关发生的事情的假设以及我们收集和反对的证据。</p><h2>假设</h2><p>回想一下，5 MLP层在我们的MLP模型中的作用是将汇总的RAW代币映射到播放运动的线性表示。从数学上讲，这是一个查找表，每个条目都是布尔值，并且在生成属性的原始令牌上。我们希望它会涉及<em>以某种方式</em>实施非线性，并且由于此查找是非线性的，例如该模型想知道“迈克尔·乔丹”和“蒂姆·邓肯”打篮球，而不必认为“迈克尔·邓肯”打篮球。</p><p>我们探索了两个假设，<strong>即单步贬义</strong>，<strong>哈希和查找</strong>。</p><h3>单步折断</h3><p>直观地，做一个神经元的最简单方法，例如relu（is_michael + is_jordan -1），实际上是一个和gate。每个运动员的单个神经元不会让您叠加任何叠加，因此我们对此进行了更复杂的版本：假设是有一堆单独的神经元，每个神经元都独立实施AN并通过其凝胶激活<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-1" id="fnref-KGJJcrC8izPbNFCPL-1">[1]</a></sup> ，绘制对运动员的名字的原始令牌表示有关运动员的每个事实的线性代表。细微差别：</p><ul><li>这通过为许多运动员和每个运动员都有许多查找神经元的运动员进行叠加来使用叠加。神经元可以建设性地干扰正确的事实，但不要超越此。<ul><li>这是一个具体的机械故事。每个神经元都会有一组运动员开火的运动员，并且在该设置上结合了一组，例如，如果神经元向迈克尔·乔丹（Michael Jordan）和蒂姆·邓肯（Tim Duncan）开火（Michael或Tim）和（Duncan或Jordan）。这引入了噪音，例如，它也会为蒂姆·乔丹（Tim Jordan）开火（它<em>想要</em>做（迈克尔和乔丹）或（蒂姆和邓肯），但这很难用一个神经元来实现。这也很吵，因为它必须同时提高迈克尔·乔丹的事实和蒂姆·邓肯的事实。但是，由于每个神经元都为不同的子集发射，所以正确的答案会有建设性的干扰，并且噪音消失了。</li></ul></li><li>这预示着相同的神经元对运动员的每个事实都同样重要</li><li>该假设的一个重要部分是，每个神经元直接从输入令牌读取，并直接有助于输出事实。从理论上讲，这可以通过单个MLP层而不是5来实现。它预测神经元直接与输入令牌合成，计算中没有中间项，并且MLP层之间没有有意义的组成。</li></ul><h3>哈希和查找</h3><p>我们模型的输入具有相当不良的格式 - 它是每个组成代币的线性总和，但是在进行事实查找时，这可能会引起高度误导！迈克尔·乔丹（Michael Jordan）和迈克尔·史密斯（Michael Smith）的名字相同的事实并没有使他们更有可能参加同样的运动。哈希和查找假设是，该模型首先产生一个中间表示形式，它破坏了输入的线性结构，即与其他所有哈希<strong>表示</strong>（即使他们共享一些令牌），即使它们共同表示，它接近正交<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-2" id="fnref-KGJJcrC8izPbNFCPL-2">表示[2] [2</a></sup> ]然后后来<strong>查找</strong>此哈希表示，并将其映射到正确的属性。细微差别：</p><ul><li><p>从某种意义上说，“硬”部分是查找。查找是存储实际的事实知识的地方，而随机初始化的MLP应该在哈希时体面，因为目标只是淹没了现有的结构。</p></li><li><p>为什么根本需要哈希？ MLP是非线性的，因此也许他们可以忽略线性结构而无需明确破坏它。这里的一种直觉来自最简单的查找：有一个“棒球神经元”，其输出可以提高棒球方向，并且其输入权重为每个棒球运动员的串联令牌表示的总和<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-3" id="fnref-KGJJcrC8izPbNFCPL-3">[3]</a></sup> - 如果运动员表示（大约）正交，然后给运动员这只会向棒球运动员开火。但是，如果它向迈克尔·乔丹（Michael Jordan）和蒂姆·邓肯（Tim Duncan）开火，它必须向至少一个蒂姆·乔丹（Tim Jordan）或迈克尔·邓肯（Michael Duncan）开火 - 不希望！而如果其输入权重取而代之的是<em>Hashed</em>运动员表示的总和，那么这将成为可能！</p></li><li><p>哈希应该在已知的令牌（例如名人名称）和未知字符串（例如未知名称）上同样有效。查找是实际知识在这里烤制的地方</p></li><li><p>对于迈克尔·乔丹（Michael Jordan）的不同已知事实的查找电路没有理由应与同一神经元相对应。从概念上讲，可能会有一个“打篮球”神经元向任何哈希篮球运动员发射，并且单独的“为芝加哥的球队玩耍”神经元对芝加哥球员的哈希发火。</p></li><li><p>这弱预测了哈希和查找层之间的干净分离</p></li></ul><p>这两个假设都是故意以强大的形式提出的，可以做出真实的预测 - 语言模型是凌乱和诅咒的，我们实际上并没有期望这是正确的。但是我们认为这些对他们有一些真相是合理的。在实践中，我们发现单步折叠似乎显然是错误的，哈希和查找的形式似乎是错误的，但可能对此有一些道理。我们发现正在考虑哈希和查找的生产力，以掌握正在发生的事情。</p><h2>伪造单步衰落假设</h2><p>单步折叠是故意的是我们能想到的最简单的假设，仍然涉及重大叠加，因此做出了相当强烈的预测。我们为这些设计了一系列实验，并广泛发现我们伪造了它做出的多个强有力的预测。</p><h3> MLP之间有明显的组成</h3><p><strong>预测</strong>：MLP 2至6之间没有中间组成，它们都同时起作用。因为预计每个重要的神经元将直接将原始令牌映射到输出。正如后来讨论的那样，缺乏组成是这一假设的有力证据，组成的存在是反对的薄弱证据。</p><p><strong>实验</strong>：我们是指<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-4" id="fnref-KGJJcrC8izPbNFCPL-4">每</a></sup>对MLP层之间的路径，并查看对几个指标的影响：头部探针准确性（在第6层残差上），完整的模型准确性和损失（在完整的vocab上），完整的模型logits精度仅限于运动和完全模型与原始logits的KL差异。通过平均消融路径，我们仅损害MLP间组成，而不受下游属性提取头的组成。</p><p><strong>结果</strong>：我们发现性能下降，尤其是从MLP2开始的路径，这表明有一些中间产品。请注意，如果低（绿色和紫色）​​，损失和KL差异很好，如果高（蓝色，红色和橙色），精度是好的。进一步注意，与“硬”指标（如精度）相比，“软”指标（如损失和KL差异）显示出更大的变化，这种指标只有在越过阈值时才改变。正如<a href="https://arxiv.org/abs/2309.16042">Zhang等人在Zhang等人</a>所说的那样，当电路由多个组件组成时，所有构成共享输出的电路都很少足以越过阈值，但是足以损坏较软的度量，造成损失和损失和KL分歧是衡量重要性的一种更可靠的方式。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/kerrtb9jty2imurotmel" alt=""></p><p> **细微差别：**请注意，这仅伪造了单步折叠的最简单形式。与这些结果<em>一致</em>的单步倒立假设的一个扩展是，它不是与MLP 2无关的MLP 2，而是作为令牌嵌入的<em>固定</em>转换（例如，它总是使嵌入的嵌入量增加一倍姓）。如果MLP 3想要访问原始令牌，它会期望MLP 2的固定效果，因此请浏览原始令牌嵌入以及MLP 2的固定转换。这将被均值消融损坏，但不涉及有意义的组成。</p><h3>神经元在多个事实之间没有共享</h3><p><strong>预测</strong>：当模型知道有关实体的多个事实时，相同的神经元对于预测每个事实而不是每个事实的神经元都很重要。这是因为查找信息的机制是通过进行布尔值和名称的令牌，这对于每个已知的事实都是相同的，因此没有理由将它们分开。</p><p><strong>实验</strong>：很难收集大量数据来替代事实该模型对运动员的了解，因此我们放大了一名特定的运动员（Michael Jordan），并发现了该模型知道他的9个事实<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-5" id="fnref-KGJJcrC8izPbNFCPL-5">[5]</a></sup> 。然后，我们是指在Jordan代币中的MLP <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-6" id="fnref-KGJJcrC8izPbNFCPL-6">2-6</a></sup>中的每个神经元，一次是在Jordan代币中的一个，并查看了每种运动的正确日志概率的变化。对于每对事实A和B，然后我们查看每个给定神经元对A的正确对数概率的效果与B的正确log Prob之间的<strong>相关性</strong>。如果每个神经元对每个事实都对相同运动员的每个事实都很重要，那么相关性应该很高。</p><p><strong>结果</strong>：非常低。唯一具有适度相关性的事实是NBA选秀年（1984年）和美国奥运会年（1992年），我怀疑这是因为它们都是两年的，尽管我不会提前预测这一点，也没有确切原因的好故事。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><p><strong>细微差别</strong>：这似乎伪造了单步贬义假设的强烈形式 - 至少，即使有杂音神经元，它们也会输出迈克尔·乔丹（Michael Jordan）事实的一部分，而不是一次。</p><p>一个问题是，消除单个神经元很难推理，并且可能有一些紧密耦合的处理（例如精致的自我修复）使得很难解释这些结果。但是，在简单的单步封式假设下，我们<em>应该</em>能够独立烧蚀和理由。另一个问题是，相关系数是摘要统计数据，可能隐藏了某些结构，但是检查散点图似乎没有任何关系。</p><h3>对属性有直接作用的神经元没有执行和</h3><p><em>注意：该实验相当复杂（尽管我们认为从概念上优雅而有趣），请随时跳过</em></p><p><strong>预测</strong>：直接由属性提取头组成的神经元正在使用其凝胶激活（在运动员的某些子集上）进行AN和在原始令牌上执行。</p><p><strong>实验</strong>：我们使用一个指标，我们称非线性过量<sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-7" id="fnref-KGJJcrC8izPbNFCPL-7">[7]</a></sup>来测量模型中的标量实现AN和。具体而言，如果神经元在prev = michael and curr =乔丹（Michael and Curr = jordan）上实现AN和prov，那么它应该从迈克尔·乔丹（Michael Jordan）中激活更多，而不是迈克尔·史密斯（Eg Michael Smith）或基思·乔丹（Keith Jordan）。正式地，给定两个二进制变量A（prev = michael）和b（curr = jordan），我们将非线性过量定义为e（a＆b） -  e（〜a＆b） -  e（a＆〜b） + e（〜a＆〜b） <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-8" id="fnref-KGJJcrC8izPbNFCPL-8">[8]</a></sup> 。重要的是，如果神经元在两个令牌中是线性的，则该度量为零，如果是一个，则该度量是正（1-0-0-0 + 0 = 1），如果是一个或该度量，则该度量是负的（1 -1-1 + 0 = -1）。</p><p>对于我们的具体实验：</p><ul><li>我们对每个神经元的非线性过量前后进行<em>更改</em><ul><li>在整个GELU上进行变化的目的是，这区分了神经元，这些神经元可以提高预计和计算自身的神经元的神经元。</li><li>要计算非线性过量，我们通过将所有名字和姓氏汇总在我们的2个令牌运动员（每个大约100个）中的所有名字和姓氏来计算平均值，并查看每个名称的组合。 （这大约有10,000个名称，用于〜a＆〜b，大约有100个〜a＆b或a＆〜b，而A＆b则只有一个 - 原始运动员的名字！）</li></ul></li><li>这种变化为正的神经元的过滤器（并限制固定前的过量量为零）<ul><li>我们发现了一堆神经元，其中前GELU具有负非线性过量，而凝胶将所有内容都设置为接近零。我们的倾向是不计算这些。</li><li>我们每个运动员都会单独进行过滤步骤，因为每个运动员都有不同的非线性过量</li></ul></li><li>乘以神经元对属性提取头L16H20的基于重量的直接效应，然后加起来。<ul><li>这是MLP 2至6的效果，如果您仅允许每个Gelu并直接影响头部L16H20，而不是允许中间组成</li></ul></li><li>我们将其与对探针的总非线性过剩效应（即通过头部L16H20的直接效应）进行比较，以查看来自AN和通过凝胶的分数<em>，并</em>直接传达给基于头部的探针</li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/hdxmyxamxcomqq0fqaxk" alt=""></p><p><strong>结果</strong>：当盯着上面的散点图时，很明显它远离X = y线，即与GELU的非线性过量过量通常大大低于总非线性过量，尽管它们是相关的。中位比率约为23％ <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-9" id="fnref-KGJJcrC8izPbNFCPL-9">[9]</a></sup> 。我们认为，这是反对单步封式假设的有力证据，因为它表明许多对L16H20显着直接作用的神经元与已经计算出AN的早期MLP组成，即计算中有一个有意义的中间步骤。</p><p><strong>细微差别</strong>：该实验涉及差异差异。我认为这在概念上是听起来和优雅的，但是我对过于复杂的实验的普遍怀疑，并且不想过分地依靠他们的结果。我们大大地回去了如何确切地设置这些实验，如何汇总和分析它们，如何滤除神经元等，并且有很多主观选择，尽管诱见的结果是对这些的结果是可靠的。</p><p>可以将固定前的过量夹紧到零的情况是不合理的，例如，该模型可能会使用凝胶的负部分来实施AN，并且（Michael Smith和Keith Jordan在Gelu之后&lt;0，Michael Jordan是零后Gelu），Gelu是零），GELU），GELU），GELU），GELU），GELU），是零尽管试图考虑这一点并没有使我们接近1。</p><p> MLP 2至6中有一些神经元信号促进现有的线性表示有关此事实的信息（例如下一节中讨论的棒球神经元），这应该使该指标失败（它们是信号增强计算和！ ）。</p><h2>棒球神经元（L5N6045）</h2><h3>有一个棒球神经元！</h3><p>一个有趣的发现是，尽管总体计算相当分散且叠加Y，但仍有一些有意义的单神经元！最值得注意的是棒球神经元L5N6045，该神经元与非棒球运动员相比，系统地激活棒球运动员。作为对棒球与非篮球运动员的二进制探测器，ROC AUC拥有89.3％。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><p><strong>因果效应</strong>：此外，它具有重要的因果作用，并与属性提取头组成。它直接通过L16H20的逻辑来促进棒球（并抑制足球），如果我们的意思是烧掉它，那么棒球运动员的全模型损失从0.167增加到0.284（0.559时，零消融时）</p><h3>不仅是信号促进</h3><p>我们发现，神经元的输入权重具有非平凡的余弦SIM卡，其输出权重（0.456），方向通过头部L16H20（0.22）（0.22）增强棒球logit（0.22）和通过头部L16H20（0.184）相对于其他运动的方向增强棒球（0.184）这表明棒球神经元的功能的一部分是为了提高运动员踢棒球的既有知识。</p><p>但这不是唯一的角色！如果我们将输入权重的分量与这3个方向跨越的子空间正交，并将残留流射向该方向，则在预测运动员是否播放棒球时，由此产生的部分消融神经元具有RocAUC（83％） （从之前的88.7％降低）。</p><h3>不是一个单音</h3><p>一个好奇心是它是否是单口护的，并且代表了完整数据分布的棒球。尽管我们没有详细调查，但这似乎很可能是错误的。在Google新闻数据集中，它在类似棒球的上下文中系统地激活了它在类似棒球的环境中（也有点适用于板球等特定的运动），但是在Wikipedia上，它在某些看似无关的事物上激活了“外部链接”中的外部（外部链接” <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-10" id="fnref-KGJJcrC8izPbNFCPL-10">[10]</a></sup>和“目标”和“目标” “足球|目标|保持”</p><h2>哈希和查找证据</h2><h3>动机</h3><p><a href="https://www.alignmentforum.org/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#Hash_and_Lookup">如上所述</a>，哈希和查找假设是MLP 2至6分解为两个不同的阶段：第一个<strong>哈希</strong>，旨在通过形成非线性表示来打破串联的（求和）名称的线性结构（总结）那试图与其他每个子字符串正交，然后<strong>查找</strong>，将棒球运动员的标签映射到棒球，足球和足球等。</p><p>从概念上讲，我们实际上并没有期望这是真的：这意味着哈希层实际上独立于数据分发，这将是令人惊讶的 - 如果我们采取了哈希和查找的实施并应用了一个few steps of gradient descent it would likely want to make the hashes of known entities more prominent and more orthogonal to everything else. But we expected testing the hypothesis to teach us useful things about the model, and thought it might be partially true. We loosely term the <strong>partial hash and lookup</strong> hypothesis as the hypothesis that the mechanism is mostly hash-and-lookup, but that the early hashing layers contain some (linearly recoverable) information about the sport that gets significantly reinforced by later lookup layers. Our evidence broadly supports this hypothesis, but unfortunately it&#39;s extremely hard to falsify.</p><p> This was motivated by seeing the failure of the single-step detokenization hypothesis: it seemed fairly clear that inter-MLP composition was going on, there were intermediate terms, and that there was some explicit lookup (the baseball neuron). This seemed the simplest hypothesis that motivated why the model would want intermediate terms and involve actual purposeful composition - the linear structure of tokens was undesirable!</p><h3> Intermediate Layers Have Linearly Recoverable Sport Info (Negative)</h3><p> <strong>Prediction</strong> : A linear probe trained to detect an athlete&#39;s sport on the residual streams during the hashing layers will be no better than random. It will only become good during the lookup layers. We don&#39;t know exactly which layers are hash vs lookup, but this predicts a sharp transition.</p><p> <strong>Experiment</strong> : Take two token athlete names in our effective model, take the residual stream after each layer, train a logistic regression probe on a train set of 80% of the names and evaluate on the other 20%. The hypothesis predicts that there will be a sharp change in the validation accuracy.</p><p> <strong>Result</strong> : It&#39;s a moderately smooth change. For robustness, we also check the metric of loss when the effective model predicts the next sport. We get similar results when training a logistic regression probe on residual streams on the final name token in the full model. This fairly straightforwardly disproves the hypothesis that the early layers are doing pure, data-independent, hashing. However there is a significant increase between layer 4 and layer 5, suggesting there&#39;s some specialisation into lookup (this is partially but not fully driven by the baseball neuron being in layer 5). For each layer we report test accuracy for 10 random seeds (taking a different 80/20 train/test split each time and training a new probe) since the dataset is sufficiently small to make it fairly noisy. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><p> <strong>Nuance</strong> :</p><ul><li> Some athletes likely have unique tokens in their names, such that the sport info is represented in the embedding. We can see that the validation accuracy of the summed tokens is better than random (50% rather than 33%). This is unsurprising, and we expect hash and lookup to matter more on the other athletes.</li><li> This is consistent with the partial hash-and-lookup hypothesis, especially since the accuracy significantly picks up in layer 5.</li></ul><h3> Known names have higher MLP output norm than unknown names (Negative)</h3><p> <strong>Prediction</strong> : Hashing predicts that early layers do not bake in knowledge of the data distribution, and so should treat known and unknown names indistinguishably.</p><p> <strong>Experiment</strong> : We measure the MLP output norm for known names and unknown names. To get the names we took the cartesian product of all single token first and last names in our athlete dataset, and separated known and unknown names <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-11" id="fnref-KGJJcrC8izPbNFCPL-11">[11]</a></sup> . This analysis is performed on the full model (but is similar on the effective model)</p><p> <strong>Result</strong> : There is a noticeable difference, known names have higher norm. This falsifies pure hashing, but not partial hashing. This happens even in MLP1, despite MLP1 not being part of our effective model <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-12" id="fnref-KGJJcrC8izPbNFCPL-12">[12]</a></sup> , which is surprising. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/mumetzm0fidzfvdcpv6p" alt=""></p><h3> Early layers do break linear structure (Positive)</h3><p> <strong>Prediction</strong> : Early layers break linear structure. Concretely, even if there&#39;s linear structure in the residual stream input, ie it&#39;s a sum of terms from different features (the current and previous token), the MLP output will not have this linear structure. More weakly, it predicts that the residual stream will lose this linear structure once the MLP output is added back in.</p><p> A concrete property of a linear function f is that f(Michael Jordan) + f(Tim Duncan) = f(Michael Duncan) + f(Tim Jordan), so let&#39;s try to falsify this!</p><p><strong>实验</strong>：</p><ul><li> We pick a pair of known names A and B (eg Michael Jordan and Tim Duncan) and an MLP layer in the effective model (eg MLP 2).<ul><li> We take the midpoint of the MLP output on these names (MLP(A) + MLP(B)) /2.</li></ul></li><li> We swap the surnames to get names C and D (unknown names, eg Michael Duncan and Tim Jordan) and take the midpoint of the MLP outputs on C and D (MLP(C) + MLP(D)) /2.</li><li> We measure the distance between the two midpoints.</li><li> To contextualise a big v small number, we divide by a baseline distance, which is computed by replacing C and D with arbitrary unknown names and measuring the distance between the midpoints |((MLP(A) + MLP(B) - MLP(C&#39;) - MLP(D&#39;))/2|<ul><li> This means that if the MLP totally breaks the linear structure it&#39;ll be close to 1 (ie Michael Duncan and Tim Jordan are indistinguishable from random unknown names) while if it preserves the linear structure it&#39;ll be close to 0 (because these will be the four vertices of a parallelogram)<ul><li> Concretely, if the MLP is linear, then MLP(Michael Jordan) = MLP(Michael X) + MLP(Y Jordan), so the midpoint of A&amp;B and C&amp;D should be the same </li></ul></li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/efy2z0gox96g5zqsbtvb" alt=""></p><p> <strong>Result</strong> : Most MLP layers show that linear structure is significantly (but not fully) broken, tending to be 60%-70% of the way to completely breaking linear structure. It&#39;s slightly less pronounced for MLP2 than MLP3 to 6.</p><p> We repeat the above experiment for the residual streams after different layers (rather than the MLP outputs), plotted in the same graph (the red boxes) and see that the residuals get less linear across the layers, going from about 30% after layer 2 to 50% after layer 6 (this is the residual at the <em>end</em> of the layer). Note that these residuals are taken from the effective model, which starts at layer 2 not layer 0. Note further that in the effective model the input to MLP2 is the summed tokens of the name, which is linear by definition.</p><p> <strong>Nuance</strong> :</p><ul><li> The result for MLP outputs is unsurprising - the whole point of MLPs is to be a non-linear function, so of course it breaks linear structure!<ul><li> We should expect this result to be true of randomly initialised MLPs</li><li> However, it turns out that randomly initialised MLPs break linear structure significantly less (20-40%)! We did a follow-up experiment where we randomly shuffled the MLP weights and biases and re-ran the model. As another baseline, we re-did the experiment on swapping the first/last names of unknown names and see no significant change. This suggests that the MLP layers are intentionally being used by the model to break linear structure. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/eetxlacpkfnhlmkbxmmt" alt=""></p><ul><li> The result that it breaks linear structure in the residual stream is less trivial, but still not surprising - the new_residual is old_residual (linear) + mlp_out (not linear), so how linear new_residual is will intuitively depend on the relative sizes.</li><li> Overall this is unsurprising evidence that hashing (in the sense of breaking linear structure) happens, but not evidence that hashing is <em>all</em> that they do, and thus it is not strong evidence for the full hash-and-lookup hypothesis (where hashing is the only role early MLPs play in the circuit)</li><li> Though conceptually unsurprising, we think “early MLPs break linear structure <em>in general</em> ” is a valuable fact to know about models, since it suggests that interference between linearly represented features will accumulate over depth.<ul><li> For example, Bricken et al observe many sparse autoencoder features like “the token &#39;the&#39; in mathematical texts”. If “is a mathematical text” and “is the token &#39;the&#39;” are both linearly represented features, then it&#39;s unsurprising that an MLP layer represents the intersection, even if no actual computation is done with this feature.</li></ul></li></ul><h3> The baseball neuron works on an orthogonal remainder of athlete residuals (Ambiguous)</h3><p> <em>Meta: This section documents an experiment one of us was initially excited about, but later realised could be illusory, which we describe here for pedagogical purposes</em></p><p> <strong>Prediction</strong> : If lookup was happening, this suggests that each athlete&#39;s representation has <em>idiosyncratic</em> information - there&#39;s some “is Michael Jordan” information in the Michael Jordan residual, which matters for the model eventually producing “plays basketball”, that cannot be recovered from other basketball玩家。 Note that this is clearly happening when the raw tokens are summed, but may not be later on. We focus on <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#The_Baseball_Neuron__L5N6045_">the baseball neuron in layer 5</a> which seems likely part of the lookup circuitry, as it has a significant effect and directly boosts the baseball attribute.</p><p> The contrasting hypothesis is that early layers (eg 2-4) produce some representation of “plays baseball” among baseball plays (possibly different from the final representation) and the baseball neuron just signal boosts this.</p><p> <strong>Experiment</strong> : To test this, we took each athlete&#39;s residual and took the component orthogonal to the subspace spanned by all other athlete residuals (note that there&#39;s 2560 residual dimensions and about 1500 other athletes, so this removes 60% of the dimensions). We then applied the baseball neuron to this residual orthogonal component, and looked at the ROC AUC of the neuron output for predicting the binary variable of whether the athlete played baseball or not</p><p> <strong>Result</strong> : The ROC was about 60%, significantly above chance (50%) - it was significantly worse than without the orthogonal projection, but still had some signal</p><p> <strong>Nuance</strong> : The reason this turned out to be illusory is that “project to be orthogonal to all other athletes” does not necessarily remove <em>all</em> information shared with other athletes. Toy example: Suppose every baseball player residual is an “is baseball” direction plus significant Gaussian noise. If we take the subspace spanned by 1500 samples from this distribution, because each sample is noisy, the “is baseball” direction may not be fully captured in this subspace and thus the projection does not erase it. This means that, though I <sup class="footnote-ref"><a href="#fn-KGJJcrC8izPbNFCPL-13" id="fnref-KGJJcrC8izPbNFCPL-13">[13]</a></sup> find the results of this experiment surprising, it doesn&#39;t distinguish the two hypotheses well - partial hash and lookup is really hard to falsify! </p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-KGJJcrC8izPbNFCPL-1" class="footnote-item"><p> GELUs are not the same as a ReLU, but we think can be productively thought of as a “soft ReLU” and are fairly well approximated with a ReLU, so can also implement an AND gate reasonably well <a href="#fnref-KGJJcrC8izPbNFCPL-1" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-2" class="footnote-item"><p> Note that this is hashing in the sentence of a hash function, not a hash table. A hash function takes an arbitrary input and tries to produce an output that&#39;s indistinguishable from random. A hash table applies a hash function to the input, and <em>then</em> purposefully maps it to some stored data, which is more analogous to the full hash and lookup. <a href="#fnref-KGJJcrC8izPbNFCPL-2" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-3" class="footnote-item"><p> Note that there are more complex and less basis aligned forms of lookup that could be going on that may be less prone to interference, indeed we found signs that the story was messy and complex <a href="#fnref-KGJJcrC8izPbNFCPL-3" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-4" class="footnote-item"><p> Resample ablation from another athlete gets similar results <a href="#fnref-KGJJcrC8izPbNFCPL-4" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-5" class="footnote-item"><p> We measure the log prob of the first token of each answer, for multi-token answers the continuation is in brackets and was not explicitly tested for (it&#39;s easy enough to do with bigrams once you have the first token)</p><ul><li> plays the sport of basketball</li><li> went to college in the state of North (Carolina)</li><li> was drafted to the NBA in 1984</li><li> plays for the team called the Chicago (Bulls)</li><li> is the majority owner of the Charlotte (Hornets)</li><li> starred in the movie &#39;Space (Jam)</li><li> plays for the league called the NBA</li><li> represented the US at the Olympics in 1992</li><li> played with the number 23</li></ul> <a href="#fnref-KGJJcrC8izPbNFCPL-5" class="footnote-backref">↩︎</a></li><li id="fn-KGJJcrC8izPbNFCPL-6" class="footnote-item"><p> Replacing the neuron&#39;s value with its average value across the final name token across all 1500 athletes in our dataset. <a href="#fnref-KGJJcrC8izPbNFCPL-6" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-7" class="footnote-item"><p> Inspired by forthcoming work from Lovis Heindrich and Lucia Quirke <a href="#fnref-KGJJcrC8izPbNFCPL-7" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-8" class="footnote-item"><p> Motivation: E(A &amp; B) corresponds to the activation on Michael Jordan, E(~A &amp; B) corresponds to Keith Jordan (for various values of Keith), E(A &amp; ~B) corresponds to Michael Smith (for various values of Smith). The activations of a neuron generally have a mean far from zero, so we subtract that mean from each term, which is captured by the E(~A &amp; ~B) term, the average over all names without Michael or Jordan. And (E(A &amp; B) - E(~A &amp; ~B)) - (E(~A &amp; B) - E(~A &amp; ~B)) - (E(A &amp; ~B) - E(~A &amp; ~B)) = E(A &amp; B) - E(~A &amp; B) - E(A &amp; ~B) + E(~A &amp; ~B) <a href="#fnref-KGJJcrC8izPbNFCPL-8" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-9" class="footnote-item"><p> We take the median because sometimes the total or GELU-derived non-linear effect is negative/zero, and the median lets us ignore these outliers <a href="#fnref-KGJJcrC8izPbNFCPL-9" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-10" class="footnote-item"><p> Though we couldn&#39;t easily figure out which article this was from, maybe it was baseball related articles, showing something like <a href="https://arxiv.org/abs/2310.15154">the summarization motif</a> ! <a href="#fnref-KGJJcrC8izPbNFCPL-10" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-11" class="footnote-item"><p> There will be some misclassification, as likely some pairings of names are known entities. We did some spot checking by googling names and don&#39;t expect this to significantly affect the results <a href="#fnref-KGJJcrC8izPbNFCPL-11" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-12" class="footnote-item"><p> We find that MLP1 seems relevant for the attention of attribute extracting heads (having them detect whether name is an athlete or not and thus whether to extract a sport at all) but not important for looking up which sport an athlete plays. Ie important for the key but not the value. <a href="#fnref-KGJJcrC8izPbNFCPL-12" class="footnote-backref">↩︎</a></p></li><li id="fn-KGJJcrC8izPbNFCPL-13" class="footnote-item"><p> Using the singular because my co-author thinks this alternative explanation was obvious all along <a href="#fnref-KGJJcrC8izPbNFCPL-13" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CW5onXm6uZxpbpsRk/fact-finding-trying-to-mechanistically-understanding-early<guid ispermalink="false"> CW5onXm6uZxpbpsRk</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:46:05 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Simplifying the Circuit (Post 2)]]></title><description><![CDATA[Published on December 23, 2023 2:45 AM GMT<br/><br/><p> <em>This is the second post in the Google DeepMind mechanistic interpretability team&#39;s <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">investigation into how language models recall facts</a> . This post focuses on distilling down the fact recall circuit and models a more standard mechanistic interpretability investigation. This post gets in the weeds, <strong>we recommend starting with <a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">post one</a></strong> and then skimming and skipping around the rest of the sequence according to what&#39;s most relevant to you. We assume readers of this post are familiar with the mechanistic interpretability techniques listed <a href="https://www.neelnanda.io/mechanistic-interpretability/glossary#mechanistic-interpretability-techniques">in this glossary</a> .</em></p><h2>介绍</h2><p>Our goal was to understand how facts are stored and recalled in superposition. A necessary step is to find a narrow task involving factual recall and understand the high level circuit that enables a model to do this task.</p><p> We focussed on the narrow task of recalling the sports played by different athletes. As discussed in <a href="https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#Why_Facts">post 1</a> , we particularly expected facts about people to involve superposition, because the embeddings of individual name tokens is normally insufficient to determine the sport, so the model must be doing a boolean AND on the different tokens of the name to identify an athlete and look up their sport. <a href="https://transformer-circuits.pub/2022/solu/index.html">Prior</a> <a href="https://arxiv.org/abs/2305.01610">work</a> calls this phenomenon &#39;detokenisation&#39; and suggests it involves early MLP layers, and uses significant superposition.</p><p> Why focus on athletes&#39; sports rather than factual recall in general? We believe that in mechanistic interpretability, it&#39;s often useful to first understand a narrow instance of a phenomenon deeply, rather than insisting on being fully general. Athletes&#39; sports was a nice task that gave us lots of examples per attribute value, and our goal was to understand at least one example where superposition was used for factual recall, rather than explaining factual recall in general. We conjecture that similar mechanisms are used for recalling other classes of fact, but this wasn&#39;t a focus of our work.</p><h2>设置</h2><p>To understand fact localisation, we studied <a href="https://arxiv.org/abs/2304.01373">Pythia</a> 2.8B&#39;s next token predictions and activations for one-shot prompts of the form:</p><p> For 1,500 athletes playing the sports of baseball, basketball and (American) football. To choose these athletes, we gave the model a larger dataset of athletes from Wikidata and filtered for those where the model placed more than 50% probability on the correct sport <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-1" id="fnref-s4i3RkmKovG88KHgx-1">[1]</a></sup> .</p><p> We chose Pythia 2.8B as it&#39;s the smallest model that could competently complete this task for a large number of athletes.</p><p> We made the prompts one-shot because this significantly improved the model&#39;s performance on the task <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-2" id="fnref-s4i3RkmKovG88KHgx-2">[2]</a></sup> . We chose golf for the one shot prefix so that the model didn&#39;t have a bias towards one of the three sports it needed to predict. For simplicity, we didn&#39;t vary the one shot prefix across the prompts.</p><h2> The simplified circuit that we ended up with</h2><p> Before going in detail through the ablation studies we performed to derive the circuit, let&#39;s take a look at the simplified circuit we ended up with: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/zwrgezridkrafez2lmmv" alt=""></p><p>在哪里：</p><ul><li><p> <code>concatenate_tokens</code> performs something roughly like a weighted sum of the individual token embeddings, placing each token&#39;s embedding in a different subspace, implemented by the first two layers of the model; <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-3" id="fnref-s4i3RkmKovG88KHgx-3">[3]</a></sup></p></li><li><p> <code>lookup</code> is a five layer MLP (whose layers match MLP layers 2 to 6 in the original model) <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-4" id="fnref-s4i3RkmKovG88KHgx-4">[4]</a></sup> that processes the combined tokens to produce a new representation of the entity described by those tokens, where this representation can be linearly projected to reveal various attributes about the entity (including the sport they play);</p><ul><li> We note that in the path from layer 6 to layer 15 there seem to be multiple MLP layers that matter, but we believe them to mostly be signal boosting an existing linear representation of the sport, and ablating them does not significantly affect accuracy.</li></ul></li><li><p> <code>extract_sport</code> is a linear classifier that performs an affine transformation on the representation generated by <code>lookup</code> to extract the sport logits that are returned by the model. This is implemented in the model by several attention heads (notably including L16H20) which attend from the final token to the final name token and directly compose with the unembedding layer <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-5" id="fnref-s4i3RkmKovG88KHgx-5">[5]</a></sup> . In the special case of athletes whose names only consist of two tokens (one for the first name, one for the last name), we were able to further simplify the <code>concatenate_tokens</code> function to be of the form: </p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/jaxnaennpshoncgl2x6a" alt=""></p><p> Where <code>embed_first</code> and <code>embed_last</code> are literally lookup tables (with one entry per token in the model&#39;s vocabulary) with disjoint ranges (so that the encoder can distinguish “Duncan” the first name from “Duncan” the surname) – reinforcing the idea that the result of <code>concatenate_tokens</code> is only as linearly informative as the individual tokens (plus positional information) themselves – ie it is a dense / compressed representation of a related sequence of tokens that the model needs to decompress / extract linear features from in order to be usable by downstream circuits (such as <code>extract_sport</code> ). <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-6" id="fnref-s4i3RkmKovG88KHgx-6">[6]</a></sup></p><p> This is fairly consistent with prior work in the literature, notably <a href="https://arxiv.org/pdf/2304.14767.pdf">Geva et al</a> . We see our narrow, bottom-up approach as complementing their broader and more top down approach to understanding the circuit. We further show that <code>extract_sport</code> is a linear map, mirroring <a href="https://arxiv.org/abs/2308.09124">Hernandez et al</a> , and that it can be understood in terms of the OV circuit of individual attention heads. We see our contribution as refining existing knowledge with a more bottom up and circuits focused approach in a narrow domain, and better understanding the model&#39;s representations at each stage, rather than as being a substantially new advance.</p><p> In the remainder of this post, we describe in further detail the experiments we performed to derive this simplified circuit.</p><h2> Investigation 1: Understanding fact extraction</h2><p> When the model outputs the correct sport token to complete the prompt, where is the earliest token at which the sport was determined? Prior work suggests that the correct sport should be identified early on in the sequence (at the athlete&#39;s final name token) and placed in a linearly recoverable representation. A separate fact extraction circuit ( <code>extract_sport</code> in the circuit diagram above) would then read the sport from the final name position and output the correct logits to complete the prompt.</p><p> In this section, we describe the experiments we performed to verify that this picture holds in reality and to identify the circuit that implements this fact extraction step.</p><h3> Which nodes contribute most to the output logits?</h3><p> We started off by identifying which nodes in the model have the greatest direct influence on the logits produced at the final token. We did this by individually ablating the activations for each MLP layer and attention head output at the final token position and measuring the direct effect on the output logits.</p><p> Specifically, for each clean prompt and each node we wished to ablate, we would take activations for that node from a “corrupted” prompt and patch these into the activations for the clean prompt just along the patch connecting that node to the model&#39;s unembedding layer, in order to measure the effect of this path patch on the model&#39;s outputs. For the corrupted prompt, we would randomly pick a prompt for an athlete who plays a different sport. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-7" id="fnref-s4i3RkmKovG88KHgx-7">[7]</a></sup> To measure the direct effect, we would compare the logit difference between the clean prompt&#39;s sport and corrupt prompt&#39;s sport, before and after path patching. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-8" id="fnref-s4i3RkmKovG88KHgx-8">[8]</a></sup> The results are as follows: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/m0prshu8u9zcnimo4fr6" alt=""></p><p> These results show that a relatively sparse set of nodes have any meaningful effect on the logits:</p><ul><li> A handful of attention heads in the mid-layers;</li><li> The MLP layers that follow these attention heads.</li></ul><p> Of these, the attention heads are particularly interesting: since we have measured direct (not total) effect, we know that the outputs of these attention heads are directly nudging the final tokens logits towards the correct sport (or away from incorrect sports), without the need for further post-processing. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-9" id="fnref-s4i3RkmKovG88KHgx-9">[9]</a></sup> This strongly suggests that, wherever these heads are attending to, the residual stream at those locations already encodes each athlete&#39;s sport.</p><h3> To where do these high-effect heads attend? </h3><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/bgqe8cyizbqkxuoxjx4e" alt=""></p><p> Here, we&#39;ve visualised the attention patterns from the final token over a sample of prompts for the 6 heads with the highest direct effect on the final token logits. We see that the heads mostly attend either to the final name token position or, failing that, look back to either the “&lt;bos>;” or “\n” resting positions earlier in the prompt. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-10" id="fnref-s4i3RkmKovG88KHgx-10">[10]</a></sup> <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-11" id="fnref-s4i3RkmKovG88KHgx-11">[11]</a></sup> From this we can conclude two things:</p><ol><li> An athlete&#39;s sport is largely represented in the residual stream by layer 16 of their final name token.</li><li> The representation of their sport should be linearly recoverable (because each head&#39;s value input is related to the model&#39;s final token logits by an approximately linear transformation).</li></ol><h3> What are these high-effect attention heads reading from the final name token position?</h3><p> To answer this question, we computed the path-specific effects of nodes in the final name token position via the OV circuits for each of the high-effect heads listed above. To be precise, for each high-effect head, we path patched, one by one, each feeder node&#39;s activation on the corrupted (other-sport) prompt along just the path from that node to the output logits via the relevant head&#39;s OV circuit <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-12" id="fnref-s4i3RkmKovG88KHgx-12">[12]</a></sup> . In effect, this creates an attribution of each high-effect node&#39;s value input in terms of the nodes that feed into it.</p><p> Interestingly, we found that a large part of the second to sixth most important heads&#39; performance comes in turn from their value inputs reading the output of L16H20 <em>at</em> the final name token stream. For example, here is the attribution for final name token nodes&#39; impact on the logits via the OV circuit for L21H9 (the second most important head) – note the outsized contribution of L16H20&#39;s output (at the final name token position) on the effect of this head: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kmyken8pjgofdndy2mre" alt=""></p><p> The heatmaps for the third to sixth most important heads looked similar, with a lot of their effect coming from the output of L16H20 at the final name token.</p><p> Furthermore, looking at the attention patterns for L16H20 when attending from the final name token position, we see that it typically attends to the same position. Putting these observations together, we see that L16H20 has a high overall importance in this circuit through two <em>separate</em> mechanisms:</p><ol><li> It transfers the athlete sport feature directly to the final token position via its OV circuit (attending from the final token back to the final name token);</li><li> It attends from the final <em>name</em> token position to the same position, producing an output that significantly contributes (via V-composition) to the outputs of other heads that transfer the athlete sport feature from the final name token to the final token.</li></ol><p> What about L16H20 itself – which nodes most strongly contribute to <em>its</em> value input? As the chart below shows, the value input for L16H20 itself is largely dependent on MLP outputs preceding it in the final name position (with some V-composition with a handful of earlier heads that in turn attend from the final name position to itself): </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kdaykbcwns7qhchfi4ya" alt=""></p><h3> A simplified subcircuit for fact extraction</h3><p> Putting the above results together, we conclude that:</p><ul><li> A large part of the sport fact extraction circuit ( <code>extract_sport</code> in the diagram above) is performed by head L16H20;<ul><li> Because the head&#39;s attention pattern does not change (much) between athletes, its function is just a linear map, multiplying the residual stream by its OV circuit.</li></ul></li><li> This head&#39;s OV circuit reads the final name token residual stream and outputs the contents (both back into the same residual stream and directly into the final token residual stream) such that unembedding the result boosts the logits for the correct sport&#39;s token over those for incorrect sports&#39; tokens.</li><li> Therefore <strong>the correct sport was linearly represented on the final name token, and extracted by L16H20&#39;s OV circuit</strong> .</li></ul><p> This suggests we can approximate <code>extract_sport</code> by replacing all of the model&#39;s computation graph from layer 16 at the final name token position onwards with a three-class linear probe constructed by composing the OV map for L16H20 with the model&#39;s unembedding weights for the tokens “ baseball”, “ basketball” and “ football”. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-13" id="fnref-s4i3RkmKovG88KHgx-13">[13]</a></sup></p><p> Making this simplification, we find that the overall circuit&#39;s accuracy at classifying athlete&#39;s sport drops from 100% for the original model, to 98% after simplifying the <code>extract_sport</code> part of the circuit to this (weights-derived) linear probe – ie we can vastly simplify this part of the circuit with negligible drop in performance at the task. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-14" id="fnref-s4i3RkmKovG88KHgx-14">[14]</a></sup> <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-15" id="fnref-s4i3RkmKovG88KHgx-15">[15]</a></sup></p><h3> An Alternate Path: Just Train A Linear Probe</h3><p> An alternate route that short-cuts around a lot of the above analysis is to just train a logistic regression probe on the residual stream of the final name token <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-16" id="fnref-s4i3RkmKovG88KHgx-16">[16]</a></sup> and show that by layer 6 the probe gets good test accuracy. We could further show that patching in the subspace spanned by the probe <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-17" id="fnref-s4i3RkmKovG88KHgx-17">[17]</a></sup> causally affects the model&#39;s output, suggesting that the representation is used downstream <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-18" id="fnref-s4i3RkmKovG88KHgx-18">[18]</a></sup> . This was the approach we used for a significant part of the project, before going back and making the analysis earlier in this section more rigorous.</p><p> We think there are significant advantages of mechanistic probes (eg using the weights of L16H20 and the unembedding to derive a probe rather than training a logistic regression classifier), it&#39;s more principled (in the sense that we can clearly see what it means in terms of the model&#39;s circuits), harder to overfit, and doesn&#39;t require a training set that can then no longer be used for further analysis. But “just train a probe” makes it easier to move fast.</p><p> In particular, for this investigation, our goal was to zoom in on <code>lookup</code> in the first few layers, and knowing that the correct sport became linearly represented after a couple of MLP layers sufficed to tell us there was something interesting to try reverse-engineering, even if we didn&#39;t know the details of the fact extraction circuit.</p><p> We think that probes are an underrated tool for circuit analysis, and that finding interpretable directions/subspaces in the model, which can be shown to be causally meaningful in a non-trivial way, enables simpler circuit analysis that needs only consider a subset of layers, rather than the full end-to-end behaviour of the model.</p><p> Another simpler approach would be to search for a mechanistic probe by just iterating over every head, taking its OV times the unembedding of the sports as your probe, and evaluating accuracy. If there&#39;s a head with particularly high accuracy (including on a held-out validation set) that attends to the right place, then you may have found a crucial head. We note this approach has more danger of overfitting, depending on the number of heads, than doing a direct logit attribution first to narrow down to a small set of heads <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-19" id="fnref-s4i3RkmKovG88KHgx-19">[19]</a></sup> .</p><h2></h2><h2> Investigation 2: Simplifying de-tokenization and lookup</h2><p> In this section we describe the experiments we performed to simplify the part of the circuit covered by the <code>concatenate_tokens</code> and <code>lookup</code> modules as defined in the simplified circuit diagram <a href="https://docs.google.com/document/d/1EsIlX7L_xr0YX918NiDWv4Cn3FVS6tJqjrIGR6mnJyQ/edit?resourcekey=0-QoVN8x6k4h6wZCZaWV9qug#heading=h.h6dofjljntk0">above</a> . To summarise, the experiments described below establish the following facts about this part of the circuit:</p><ul><li> An athlete&#39;s sport is represented much earlier in the residual stream than layer 16 (where it is read by head L16H20). By layer 6, we still get pretty good (90%) accuracy when trying to read athlete&#39;s sports using <code>extract_sport</code> .</li><li> Context prior to the athlete&#39;s name doesn&#39;t matter for producing the sport representation (though it does matter for <code>extract_sport</code> ). We can use the model&#39;s final token activations on prompts of the form “&lt;bos>; &lt;athletes-full-name>;” and extract the athlete&#39;s sport with little drop in accuracy.</li><li> Attention heads don&#39;t matter beyond layers 0 and 1. We can mean-ablate attention layers 2 onwards without hurting the <code>lookup</code> module&#39;s performance much. Looking at attention patterns, we see that most heads do not particularly attend to previous name tokens, strengthening the case for ablating them. The advantage of removing these attention heads is that the <code>lookup</code> module thus becomes a simple stack of MLP layers (with residual skip connections) that is able to take the athlete&#39;s embeddings at the start of layer 2 in the residual stream and output the athlete&#39;s sport by the end of layer 6 in the residual stream.</li></ul><p> As a result, we can decompose this part of the circuit into two sub-modules:</p><ul><li> <code>concatenate_tokens</code> , representing the role of layers 0 and 1 of the model, whose role (for this task) is to collect the athlete&#39;s name tokens and place them in the final name token residual stream (we show in Investigation 3 that this is pure concatenation without lookup, at least for two token athlete names);</li><li> <code>lookup</code> , a pure 5-layer MLP (comprising layers 2–6 of the original model) that converts this multi-token representation of the athlete&#39;s name into a feature representation of the athlete that specifically represents the athlete&#39;s sport in a linearly recoverable fashion.</li></ul><p> We now describe the evidence supporting each of the three claims listed above in turn.</p><h3> Lookup is mostly complete by layer 6</h3><p> If we apply the <code>extract_sport</code> probe to different layers in the final name token position, we see that it&#39;s possible to read an athlete&#39;s sport from the residual stream much earlier than layer 16: <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-20" id="fnref-s4i3RkmKovG88KHgx-20">[20]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/pwsurjq0ifiw4ch7xtj2" alt=""></p><p> By around layer 8, accuracy has largely plateaued, and even by layer 6 we have about 90% accuracy. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-21" id="fnref-s4i3RkmKovG88KHgx-21">[21]</a></sup></p><h3> Context doesn&#39;t matter when looking up an athlete&#39;s sport</h3><p> We have already established that the residual stream at the final name token for an athlete encodes their sport. But to what extent did the model place sport in the residual stream because it would have done this anyway when seeing the athlete&#39;s name (the multi-token embedding hypothesis) and to what extent did the model place sport in the residual stream because the one-shot prompt preceding the name <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-22" id="fnref-s4i3RkmKovG88KHgx-22">[22]</a></sup> hinted to the model that sport might be a useful attribute to extract?</p><p> Our hypothesis was that the context wouldn&#39;t matter that much – specifically that the model would look up an athlete&#39;s sport when it sees their name, even without any prior context. We tested this by collecting activations for pure name prompts, where the model was fed token sequences of the form “&lt;bos>; <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-23" id="fnref-s4i3RkmKovG88KHgx-23">[23]</a></sup> &lt;first-name>; &lt;last-name>;” <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-24" id="fnref-s4i3RkmKovG88KHgx-24">[24]</a></sup> and the residual stream was harvested from the final name token.</p><p> Can the <code>extract_sport</code> module read athletes&#39; sports from these activations? As the plot below shows, we found that there is a little drop in performance without the one-shot context, but it&#39;s still possible to fairly accurately read an athlete&#39;s sport purely from an early layer encoding of just their name prepended by “&lt;bos>;”, without any additional context. Hence, we can simplify the overall circuit by deleting all edges from tokens preceding the athlete&#39;s name tokens in the full prompt for the task. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/x4wdqduttl0ovdowk9yi" alt=""></p><h3> Attention heads don&#39;t matter beyond layer 2</h3><p> In order to recall sport accurately, the <code>lookup</code> part of the circuit must in general be a function of most (if not all) of the tokens in an athlete&#39;s name: for most athletes, it&#39;s not possible to determine sport by just knowing the last token in their surname. Hence, attention heads must play some role in bringing together information distributed over the individual tokens of an athlete&#39;s name in order that facts like sport can be accurately looked up.</p><p> However, how do these two processes – combining tokens and looking up facts – relate to each other?</p><ol><li> They could happen concurrently – with attention bringing in relevant information from earlier tokens as and when it is required for the lookup process;</li><li> Or the processes could happen sequentially, with the tokens making up an athlete&#39;s name being brought together first, and much of the lookup process only happening afterwards. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-25" id="fnref-s4i3RkmKovG88KHgx-25">[25]</a></sup> Looking at the total effects of patching attention head outputs at the final name token position, we did find that there are many more heads that play a significant role in the overall circuit in layers 0 and 1 of the model than in later layers : </li></ol><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/s8xbfgfrakemknheueil" alt=""></p><p> This suggested that we might be able to remove the attention head outputs for layer 2 onwards without too much impact on the overall circuit&#39;s performance. Trying this, we found that mean ablating attention outputs from layer 2 onwards had only a slightly detrimental impact on accuracy: <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-26" id="fnref-s4i3RkmKovG88KHgx-26">[26]</a></sup> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xc9un39ztbdfeaxxh1h4" alt=""></p><p> This supports the two-stage hypothesis described above: information sharing between tokens (via attention) is largely complete by layer 2, with attention heads in later layers unimportant for lookup.</p><h3> A simplified subcircuit for fact lookup</h3><p> The results above suggest that we can indeed split the process of looking up an athlete&#39;s sport into two stages:</p><ul><li><p> <code>concatenate_tokens</code> , which is the result of the embedding and layers 0 and 1 of the model processing the tokens comprising the athlete&#39;s name <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-27" id="fnref-s4i3RkmKovG88KHgx-27">[27]</a></sup> and producing a “concatenated token” representation at the end of layer 1 in the final name token residual stream;</p></li><li><p> <code>lookup</code> , which is a pure MLP with skip connections (made up from MLP layers 2 onwards in the model), which processes the “concatenated token representation” after layer 1 of the residual stream, which in itself poorly represents the athlete&#39;s sport (in a linear manner), and produces a “feature representation” later on in the residual stream, where sport can be easily linearly extracted (by <code>sport_extract</code> ).</p></li></ul><p> Note that there are two simplifications we have combined here:</p><ul><li> Processing the athlete name tokens on their own without a one-shot prompt (because we found context to be unimportant)</li><li> Removing attention heads from layer 2 onwards (because we found information transmission between tokens to largely be complete by layer 2).</li></ul><p> Since, each of these approximations has some detrimental effect on the circuit&#39;s accuracy, it&#39;s worth assessing their combined impact. Here&#39;s a plot showing how combining these approximations impacts accuracy: </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/xkhlsj0ktdmm3kipcbll" alt=""></p><p> The upshot is that, even applying both simplifications together, it&#39;s possible to get up to 94% accuracy by including enough layers in the <code>lookup</code> MLP; even stopping at layer 6 gets you 85% accuracy.</p><h2></h2><h2> Investigation 3: Further simplifying the token concatenation circuit</h2><p> So far, we have:</p><ul><li> Simplified <code>extract_sport</code> to a 3-class linear probe, whose weights are derived by composing the OV map for L16H20 with the model&#39;s unembedding weights;</li><li> Simplified <code>lookup</code> to be a pure 5 layer MLP with skip connections, whose layer weights correspond to those of MLP layers 2–6 in the original model;</li><li> Established that prior context can be removed when calculating the residual stream&#39;s value at the beginning of layer 2 at the final name token position (which is the input for <code>lookup</code> ).</li></ul><p> This leaves us with <code>concatenate_tokens</code> , comprising the embedding and layers 0 and 1 of the model, which converts the raw athlete name tokens (plus a prepending &lt;bos>; token) into the value of the residual stream at the beginning of layer 2. Can we simplify this part of the circuit further?</p><p> There are two levels of simplification we identified for this components of the circuit:</p><ul><li> In general, we can think of <code>concatenate_tokens</code> as effectively generating <em>two</em> separate token-level embeddings of the athlete&#39;s name, and then combining these embeddings approximately linearly via pure attention operations;</li><li> In the case of athletes whose names are just two tokens long, we can further approximate <code>concatenate_tokens</code> so that it is literally a sum of effective token embeddings for first and last name tokens, taking only a modest hit to circuit accuracy.</li></ul><p> In the following subsections, we explain these simplifications in more detail and provide experimental justifications for them.</p><h3> Token concatenation is achieved by attention heads moving token embeddings</h3><p> The first simplification comes from the following two observations:</p><ul><li><p> MLP layer 1 at the final name token position has little importance for circuit performance: resample ablating it has low total effect on logit difference for the original model, and mean ablating it has little impact on the accuracy of the simplified circuit. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-28" id="fnref-s4i3RkmKovG88KHgx-28">[28]</a></sup></p></li><li><p> Due to Pythia using parallel attention, MLP layer 0 is effectively a secondary token embedding layer. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-29" id="fnref-s4i3RkmKovG88KHgx-29">[29]</a></sup> This implies that (after mean ablating MLP 1), <code>concatenate_tokens</code> effectively performs the following operations:</p></li></ul><ol><li> Calculate primary token embeddings for the athlete name tokens (and &lt;bos>;) using the model&#39;s embedding layer weights.</li><li> Calculate secondary token embeddings for the athlete name tokens using the embedding weights induced by the action of MLP 0 on the input token vocabulary.</li><li> Operate the attention layer 0 heads on the primary token embeddings.</li><li> Operate the attention layer 1 heads on the sum of the primary token embeddings, secondary token embeddings and outputs of attention layer 0.</li><li> Use the result of step 4 at the final name token position as the input to <code>lookup</code> .</li></ol><p> In other words, <code>concatenate_tokens</code> effectively embeds the input tokens (twice) and moves them (directly and indirectly) to the final name token position via attention.</p><h3> For two-token athletes, <code>concatenate_tokens</code> literally adds first and last name tokens together</h3><p> For two-token athletes, we found that we could furthermore freeze attention patterns and still retain reasonable accuracy on the task.具体来说：</p><ul><li> We prevented attention heads in layer 1 at the last name token position attending to the same position;</li><li> We froze all other attention patterns at their average levels across all two token athletes in the dataset, making the attention layers just act as a linear map mapping source token residuals to the final token.</li></ul><p> These simplifications, along with mean ablating MLP 1, turn <code>concatenate_tokens</code> into a sum of effective token embeddings and a bias term (originating from the embeddings for the &lt;bos>; token). The effective token embedding of the last name is just the sum of the primary and second (MLP0) token embedding. The effective token embedding of the first name is more complex, it&#39;s the primary token embedding times the linear map from frozen attention 0 heads (their OV matrices weighted by the average attention from the last name to the first name), plus the primary and secondary token embeddings times the linear map from frozen attention 1 heads.</p><p> The impact of these simplifications on accuracy are shown in the graph below. <sup class="footnote-ref"><a href="#fn-s4i3RkmKovG88KHgx-30" id="fnref-s4i3RkmKovG88KHgx-30">[30]</a></sup> We see that, for two-token athletes, freezing attention patterns has little additional impact on accuracy over ablating MLP 1. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/3tqJ65kuTkBh8wrRH/kinnfxarccubfccdl2g3" alt=""></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-s4i3RkmKovG88KHgx-1" class="footnote-item"><p> Note that we started with several thousand athletes, so this filtering likely introduced some bias. Eg if there were a thousand athletes the model did not know but guessed a random sport for, we would select the 333 where the model got lucky. We set the 50% confidence (on the full vocab) threshold to reduce this effect. <a href="#fnref-s4i3RkmKovG88KHgx-1" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-2" class="footnote-item"><p> Our guess is that few shot vs zero shot does not materially affect the lookup circuit, but rather the one shot prompt tells the model that the output is a sport and boosts all sport logits (suggested by the results of Chughtai et al (forthcoming)) 。 Anecdotally, zero shot, the model puts significant weight on “his/her” as an output, though Pythia 1.4B does not! <a href="#fnref-s4i3RkmKovG88KHgx-2" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-3" class="footnote-item"><p> Note that the linearly recoverable features in the output of <code>concatenate_tokens</code> will end up being something like the union of the features in these individual token embeddings – ie sport is not particularly linearly recoverable from this output. It&#39;s best to think of the output of <code>concatenate_tokens</code> as a “concatenation of individual token embeddings that saves position information” so that the concatenation can be processed together by a series of MLP layers. <a href="#fnref-s4i3RkmKovG88KHgx-3" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-4" class="footnote-item"><p> We could have chosen pretty much any layer between 4 and 15 here as our endpoint, as faithfulness of our simplified circuit increased fairly continuously as we included additional layers. However, there is an inflection point around layer 5, after which you start seeing diminishing returns. We believe that the MLP layers after MLP 6 are just boosting the attributes in the residual stream rather than looking it up from the raw tokens. <a href="#fnref-s4i3RkmKovG88KHgx-4" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-5" class="footnote-item"><p> When we refer to the “linear classifier” we are referring to the composition of the OV circuit of these heads and the unembedding matrix. The heads always attend from the final token to the final name position, so purely act as a linear map. <a href="#fnref-s4i3RkmKovG88KHgx-5" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-6" class="footnote-item"><p> We suspect it may be possible to express <code>concatenate_tokens</code> as a similar sum of position-dependent token embeddings even for athletes with three or more tokens in their name, but we didn&#39;t pursue this line of investigation further. <a href="#fnref-s4i3RkmKovG88KHgx-6" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-7" class="footnote-item"><p> Eg where the clean prompt is for Tim Duncan (who plays basketball), we might patch in activations from the prompt for George Brett (who plays baseball) or Andy Dalton (who plays football). <a href="#fnref-s4i3RkmKovG88KHgx-7" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-8" class="footnote-item"><p> This metric has the nice property of being linear in logits, while also being invariant to constant shifts across all logits. <a href="#fnref-s4i3RkmKovG88KHgx-8" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-9" class="footnote-item"><p> To clarify, the not insignificant total effects of later MLP layers suggests that some post-processing is going on - the point we&#39;re making is that even without this post processing, the outputs of these attention heads can directly be interpreted in terms of sport token logits, hence these attention heads are already writing the correct sport into the residual stream. <a href="#fnref-s4i3RkmKovG88KHgx-9" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-10" class="footnote-item"><p> Interestingly, because the prompts are sorted in terms of sport, we see that some heads only seem to be used for a subset of the sports in the task. Breaking down these head&#39;s direct effects by sport confirms this picture: L19H24 is only consistently important for baseball players, whereas L22H17 is only consistently important for basketball, and some fraction of baseball players. (We didn&#39;t try to understand what differentiates those athletes that this head is important for versus those that it isn&#39;t important for.) This selectivity among attention heads is <a href="https://arxiv.org/abs/2307.09458">not entirely surprising</a> , and we did not investigate this further as it&#39;s not directly relevant to our current investigation. <a href="#fnref-s4i3RkmKovG88KHgx-10" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-11" class="footnote-item"><p> As usual in mechanistic interpretability, this is just an approximate picture - looking at the plots, it&#39;s clear that some heads do non-trivially attend to tokens between the athlete&#39;s name and the final token, particularly to “ plays” and “ sport” and sometimes “ golf”. However, this doesn&#39;t change the overall conclusion that the residual stream at an athlete&#39;s name token already contains their sport. <a href="#fnref-s4i3RkmKovG88KHgx-11" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-12" class="footnote-item"><p> We note that this approach requires a separate forward pass for each residual stream component on the final name token, <em>and</em> for each high-effect mediating head. This was not a bottleneck on our work, so we did proper path patching, but we note that this can be easily approximated with direct logit attribution. If we freeze the LayerNorm scale, then the OV circuit of each head performs a linear map on the final name token residual stream, and the unembedding is a further linear map, and so we can efficiently see the change in the final logits caused by each final name token residual component. We found this technique useful for rapid iteration during this work. <a href="#fnref-s4i3RkmKovG88KHgx-12" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-13" class="footnote-item"><p> We also need to set the bias for the probe. We do this by subtracting the mean activations at the probe&#39;s input (ie by effectively centering the probe&#39;s input before applying the weight matrix). <a href="#fnref-s4i3RkmKovG88KHgx-13" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-14" class="footnote-item"><p> This difference in performance is at least in part because we always probe the final name token position whereas L16H20 does for some athletes attend to other positions (eg the penultimate token) in their name. We conjecture this is because some athletes are fully identified before the final token in their name has appeared (eg from four tokens of a five token name), and so fact lookup occurs before this final token. <a href="#fnref-s4i3RkmKovG88KHgx-14" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-15" class="footnote-item"><p> We deliberately measure accuracy rather than loss recovered because we expect the later high-effect heads are mostly signal boosting the output of L16H20, even though <a href="https://arxiv.org/abs/2309.16042">loss recovered would normally be our preferred metric</a> . Signal boosting improves loss but does not change accuracy, and to understand factual recall it suffices to understand how high accuracy is achieved. <a href="#fnref-s4i3RkmKovG88KHgx-15" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-16" class="footnote-item"><p> In this case we already guessed the final name token would be the right token to probe for sport based on eg residual stream patching + prior work, but it&#39;s easy enough to sweep over tokens and layers and train a probe on all of them, probes are pretty cheap to train <a href="#fnref-s4i3RkmKovG88KHgx-16" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-17" class="footnote-item"><p> <a href="https://arxiv.org/abs/2311.17030">Makelov et al</a> recently showed that subspace activation patching can misleadingly activate dormant parallel pathways, but this is mostly a concern when using gradient descent to learn a subspace with causal effect, probes are correlational so this is not an issue here. <a href="#fnref-s4i3RkmKovG88KHgx-17" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-18" class="footnote-item"><p> Because the probe is linear, it&#39;s a bit unclear if you should care whether the probe is causally used. The model&#39;s ability to map individual tokens of an athlete&#39;s name to a linear representation of sport is an interesting algorithm to reverse-engineer and likely involves superposition, even if for some weird coincidence a parallel algorithm is the main thing affecting the model outputs. But this is a pretty contrived situation and it&#39;s easy to check. <a href="#fnref-s4i3RkmKovG88KHgx-18" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-19" class="footnote-item"><p> In particular, in some settings, the probe may be very natural. Eg many attention heads just copy whatever token they attend to to the output. So being a good mechanistic probe when probing for the input token is weak evidence that a head is involved, but likely still finds you a pretty good probe. <a href="#fnref-s4i3RkmKovG88KHgx-19" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-20" class="footnote-item"><p> When applying this probe to other layers, we&#39;d always mean centre relative to the residual stream activations for that layer. This is equivalent to mean ablating the MLP and attention outputs between the layer being probed and layer 16. <a href="#fnref-s4i3RkmKovG88KHgx-20" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-21" class="footnote-item"><p> A question naturally arises here: why did the L16H20 value input attribution (presented earlier) show that MLPs 8, 11 and 13 in particular have a high path-specific effect on determining the correct sport token, when the probe shows that you can pretty much read the athlete&#39;s sport much earlier on? Our hypothesis is that these later MLPs are boosting the signal generated by early MLPs, rather than looking up facts by themselves. <a href="#fnref-s4i3RkmKovG88KHgx-21" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-22" class="footnote-item"><p> Ie “Fact: Tiger Woods plays the sport of golf” <a href="#fnref-s4i3RkmKovG88KHgx-22" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-23" class="footnote-item"><p> In the weeds: The Pythia models were not trained with a BOS (beginning of sequence) token, but we anecdotally find that the model is better behaved when doing inference with one. Models often have extremely high norm on the first token of the context, and treat it unusually, which makes it hard to study short prompts like “George Brett”. Pythia&#39;s BOS and EOS token are the same, and it was trained with EOS tokens separating documents in pre-training (and the model was able to attend between separate documents), so it will have seen a BOS token in this kind of role during training <a href="#fnref-s4i3RkmKovG88KHgx-23" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-24" class="footnote-item"><p> Eg “&lt;bos>; George Brett” <a href="#fnref-s4i3RkmKovG88KHgx-24" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-25" class="footnote-item"><p> NB we know it can&#39;t be possible to completely separate lookup from token concatenation, because even the final token embedding (prior to any processing by the model) often has some notion of an athlete&#39;s sport. Instead, we&#39;re making the weaker hypothesis here that much of the additional accuracy of the <code>lookup</code> circuit (beyond guessing sport from the final token) happens after the athlete&#39;s name tokens have first been assembled together. <a href="#fnref-s4i3RkmKovG88KHgx-25" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-26" class="footnote-item"><p> We also checked (and confirmed) that ablating attention layers 0 or 1 had a catastrophic impact on sport lookup. <a href="#fnref-s4i3RkmKovG88KHgx-26" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-27" class="footnote-item"><p> With a preceding &lt;bos>; token, to be precise. <a href="#fnref-s4i3RkmKovG88KHgx-27" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-28" class="footnote-item"><p> Probe accuracy, with the simplified <code>lookup</code> and <code>extract_sport</code> circuits described in previous sections, drops from 85% to 81% after layer 6 and drops from 94% to 90% after layer 15, when we mean ablate MLP 1. <a href="#fnref-s4i3RkmKovG88KHgx-28" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-29" class="footnote-item"><p> Due to parallel attention, the input to MLP0 at any position is just the token embedding at that position. Hence, we could literally replace MLP0 by a second table of token embeddings (mapping original token IDs to their corresponding MLP0 output values) without affecting model outputs at all. <a href="#fnref-s4i3RkmKovG88KHgx-29" class="footnote-backref">↩︎</a></p></li><li id="fn-s4i3RkmKovG88KHgx-30" class="footnote-item"><p> Note that this plot is not directly comparable with preceding plots, as it measures accuracy only over two-token athletes. <a href="#fnref-s4i3RkmKovG88KHgx-30" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2<guid ispermalink="false"> 3tqJ65kuTkBh8wrRH</guid><dc:creator><![CDATA[SenR]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:45:49 GMT</pubDate> </item><item><title><![CDATA[Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1)]]></title><description><![CDATA[Published on December 23, 2023 2:44 AM GMT<br/><br/><p> <em>This is a write up of the Google DeepMind mechanistic interpretability team&#39;s investigation of how language models represent facts. This is <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX">a sequence of 5 posts</a> , we recommend prioritising reading post 1, and thinking of it as the “main body” of our paper, and posts 2 to 5 as a series of appendices to be skimmed or dipped into in any order.</em></p><h2>执行摘要</h2><p>Reverse-engineering circuits with superposition is a major unsolved problem in mechanistic interpretability: models use clever compression schemes to represent more features than they have dimensions/neurons, in a highly distributed and polysemantic way. Most existing mech interp work exploits the fact that certain circuits involve sparse sets of model components (eg a sparse set of heads), and we don&#39;t know how to deal with distributed circuits, which especially holds back understanding of MLP layers. Our goal was to interpret a concrete case study of a distributed circuit, so we investigated how MLP layers implement a lookup table for factual recall: namely how early MLPs in Pythia 2.8B look up which of 3 different sports various athletes play. <strong>We consider ourselves to have fallen short of the main goal of a mechanistic understanding of computation in superposition</strong> , but did make some meaningful progress, including conceptual progress on why this was hard.</p><p> Our overall best guess is that <strong>an important role of early MLPs is to act as a “multi-token embedding”, that selects</strong> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-1" id="fnref-aYS73ikuT9JzFD3Mj-1">[1]</a></sup> <strong>the right unit of analysis from the most recent few tokens (eg a name) and converts this to a representation</strong> (ie some useful meaning encoded in an activation). We can recover different attributes of that unit (eg sport played) by taking linear projections, ie <strong>there are linear representations of attributes</strong> . Though we can&#39;t rule it out, our guess is that there isn&#39;t much more interpretable structure (eg sparsity or meaningful intermediate representations) to find in the internal mechanisms/parameters of these layers. For future mech interp work we think it likely suffices to focus on understanding how these attributes are represented in these multi-token embeddings (ie early-mid residual streams on a multi-token entity), using tools like probing and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">sparse autoencoders</a> , and thinking of early MLPs similar to how we think of the token embeddings, where the embeddings produced may have structure (eg a “has space” or “positive sentiment” feature), but the internal mechanism is just a look-up table with no structure to解释。 Nonetheless, we consider this a downwards update on more ambitious forms of mech interp that hinge on fully reverse-engineering a model. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" alt=""></p><p> <strong>Our main contributions:</strong></p><p> <a href="https://www.alignmentforum.org/posts/3tqJ65kuTkBh8wrRH/fact-finding-simplifying-the-circuit-post-2"><strong>Post 2:</strong> Simplifying the circuit</a></p><ul><li> We do a deep dive into the specific circuit of how Pythia 2.8B recalls the sports of different athletes. Mirroring <a href="https://arxiv.org/abs/2304.14767">prior work</a> , we show that the circuit breaks down into 3 stages:<ul><li> <strong>Token concatenation</strong> : Attention layers 0 and 1 assemble the tokens of the athlete&#39;s name on the final name token, as a sum of multiple different representations, one for each token.</li><li> <strong>Fact lookup</strong> : MLPs 2 to 6 on the final name token map the concatenated tokens to a linear representation of the athlete&#39;s sport - the multi-token embedding.<ul><li> Notably, this just depends on the name, not on the prior context.</li></ul></li><li> <strong>Attribute extraction</strong> : A sparse set of mid to late attention heads extract the sport subspace and move it to the final token, and directly connect with the output. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/qv8qfrczagg388exbycd" alt=""></p><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk"><strong>Post 3:</strong> Mechanistically understanding early MLP layers</a></p><ul><li> We zoom in on mechanistically understanding how MLPs 2 to 6 actually map the concatenated tokens to a linear representation of the sport.</li><li> We explore evidence for and against different hypotheses of what&#39;s going on mechanistically in these MLP layers.</li><li> We falsify the naive hypothesis that there&#39;s just a single step of detokenization, where the GELUs in specific neurons implement a Boolean AND on the raw tokens and directly output all known facts about the athlete.</li><li> Our best guess for part of what&#39;s going on is what we term the <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Hash_and_Lookup">“hash and lookup” hypothesis</a> , but it&#39;s messy, false in its strongest form and this is only part of the story. We give some evidence for and against the hypothesis.</li></ul><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4"><strong>Post 4:</strong> How to think about interpreting memorisation</a></p><ul><li> We take a step back to compare differences between MLPs that perform generalisation tasks and those that perform memorisation tasks (like fact recall) on toy datasets.</li><li> We consider what representations are available at the inputs and within intermediate states of a network looking up memorised data and argue that (to the extent a task is accomplished by memorisation rather than generalisation) there is little reason to expect meaningful intermediate representations during pure lookup.</li></ul><p> <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR"><strong>Post 5</strong> : Do Early Layers Specialise in Local Processing?</a></p><ul><li> We explore a stronger and somewhat tangential hypothesis that, <em>in general</em> , early layers specialise in processing recent tokens and only mid layers integrate the prior context.</li><li> We find that this is somewhat but not perfectly true: early layers specialise in processing the local context, but that truncating the context still loses something, and that this effect gradually drops off between layers 0 and 10 (of 32).</li></ul><p> We also exhibit a range of miscellaneous observations that we hope are useful to people building on this work.</p><h2>动机</h2><h3>Why Superposition</h3><p> Superposition is really annoying, really important, and terribly understood. Superposition is the phenomenon, studied most notably in <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a> , where models represent more features than they have neurons/dimensions <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-2" id="fnref-aYS73ikuT9JzFD3Mj-2">[2]</a></sup> . We consider understanding how to reverse-engineer circuits in superposition to be one of the major open problems in mechanistic interpretability.</p><p> Why is it so annoying? Sometimes a circuit in a model is <strong>sparse</strong> , it only involves a small handful of components (heads, neurons, layers, etc) in the standard basis, and sometimes it&#39;s <strong>distributed</strong> , involving many components. Approximately all existing mech interp works exploit sparsity, eg a standard workflow is identifying an important component (starting with the output logits), identifying the most important things composing with that, and recursing. And we don&#39;t really know how to deal with distributed circuits, but these often happen (especially in non-cherry picked settings!)</p><p>为什么它如此重要？ Because this seems to happen a bunch, and mech interp probably can&#39;t get away with never understanding things involving superposition. We personally speculate that superposition is a key part of <em>why</em> scaling LLMs works so well <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-3" id="fnref-aYS73ikuT9JzFD3Mj-3">[3]</a></sup> , intuitively, the number of facts known by GPT-4 vs GPT-3.5 scales superlinearly in the number of neurons, let alone the residual stream dimension. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-4" id="fnref-aYS73ikuT9JzFD3Mj-4">[4]</a></sup></p><p> Why is it poorly understood? There&#39;s been fairly limited work on superposition so far. Toy Models of Superposition is a fantastic paper, but is purely looking at toy models, which can be highly misleading - your insights are only as good as your toy model is faithful to the underlying reality <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-5" id="fnref-aYS73ikuT9JzFD3Mj-5">[5]</a></sup> . Neurons In A Haystack is the main paper studying superposition in real LLMs, and found evidence they matter significantly for detokenization (converting raw tokens into concepts), but fell short of a real mechanistic analysis on the neuron level.</p><p> This makes it hard to even define exactly what we mean when we say “our goal in this work was to study superposition in a real language model”. Our best operationalisation is that we tried to study a circuit that we expected to be highly distributed yet purposeful, and where we expected compression to happen. And a decent part of our goal was to better deconfuse what we&#39;re even talking about when we say superposition in real models. See <a href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation">Toy Models of Superposition</a> and <a href="https://arxiv.org/pdf/2305.01610.pdf#page=17">Appendix A of Neurons In A Haystack</a> for valuable previous discussion and articulation of some of these underlying concepts.</p><h3> Circuit Focused vs Representation Focused Interpretability</h3><p> To a first approximation, interpretability work often focuses either on <strong>representations</strong> (how properties of the input are represented internally) or <strong>circuits</strong> (understanding the algorithms encoded in the weights by which representations are computed and used). A full circuit-based understanding typically requires decent understanding of representations, and is often more rigorous and reliable and can fill in weak points of just studying representations.</p><p> For superposition, there has recently been a burst of exciting work focusing on representations, in the form of sparse autoencoders (eg <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Bricken et al</a> , <a href="https://arxiv.org/abs/2309.08600">Cunningham et al</a> ). We think the success of sparse autoencoders at learning many interpretable features (which are often distributed and not aligned with the neuron basis) gives strong further evidence that superposition is the norm.</p><p> In this work, we wanted to learn general insights about the circuits underlying superposition, via the specific case study of factual recall, though it mostly didn&#39;t pan out. In the specific case of factual recall, our recommendation is to focus on understanding fact representations without worrying about exactly how they&#39;re computed.</p><h3> Why Facts</h3><p> We focused on understanding the role of early MLP layers to look up facts in factual recall. Namely, we studied one-shot prompts of the form “Fact: Michael Jordan plays the sport of” ->; “basketball” for 1,500 athletes playing baseball, basketball or (American) football in Pythia 2.8B <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-6" id="fnref-aYS73ikuT9JzFD3Mj-6">[6]</a></sup> . Factual recall circuitry has been widely studied before, we were influenced by <a href="https://rome.baulab.info/">Meng et al</a> , Chughtai et al (forthcoming), <a href="https://arxiv.org/abs/2304.14767">Geva et al</a> , <a href="https://arxiv.org/abs/2308.09124">Hernandez et al</a> and <a href="https://arxiv.org/abs/2205.11482">Akyurek et al</a> , though none of these focused on understanding early MLPs at the neuron level.</p><p> We think facts are intrinsically pretty interesting - it seems that a <em>lot</em> of what makes LLMs work so well are that they&#39;ve memorised a ton of things about the world, but we also expected facts to exhibit significant superposition. The model wants to know as many facts as possible, so there&#39;s an incentive to compress them, in contrast to more algorithmic tasks like indirect object identification where you can just learn them and be done - there&#39;s always more facts to learn! Further, facts are easy to compress because they don&#39;t interfere with each other (for a fixed type of attribute), the model never needs to inject the fact that “Michael Jordan plays basketball” and “Babe Ruth plays baseball” on the same token, it can just handle the tokens &#39; Jordan&#39; and &#39; Ruth&#39; differently <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-7" id="fnref-aYS73ikuT9JzFD3Mj-7">[7]</a></sup> (see <a href="https://arxiv.org/pdf/2305.01610.pdf#page=18">FAQ questions A.6 and A.7</a> here for more detailed discussion). We&#39;d guess that a model like GPT-3 or GPT-4 knows more facts than it has neurons <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-8" id="fnref-aYS73ikuT9JzFD3Mj-8">[8]</a></sup> .</p><p> Further, <a href="https://arxiv.org/pdf/2305.01610.pdf">prior work</a> has shown that superposition seems to be involved in early MLP layers for <strong>detokenization</strong> , where raw tokens are combined into concepts, eg “social security” is a very different concept than “social” or “security” individually. Facts about people seemed particularly in need of detokenization, as often the same name tokens (eg a first name or surname) are shared between many unrelated people, and so the model couldn&#39;t always store the fact in the token embeddings. Eg “Adam Smith” means something very different to “Adam Driver” or “Will Smith”. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-9" id="fnref-aYS73ikuT9JzFD3Mj-9">[9]</a></sup></p><h2> Our High-Level Takeaways</h2><h3> How to think about facts? </h3><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/b8shmrcacihqfwbfn5ru" alt=""></p><p> <em>(Figure from above repeated for convenience)</em></p><p> At a high-level, we recommend thinking about the recall of factual knowledge in the model as <strong>multi-token embeddings</strong> , which are largely formed by early layers (the first 10-20% of layers) from the raw tokens <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-10" id="fnref-aYS73ikuT9JzFD3Mj-10">[10]</a></sup> . The model learns to recognise entities spread across several recent raw tokens, eg “| Michael| Jordan”, and produces a representation in the residual stream, with linear subspaces that store information about specific attributes of that entity <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-11" id="fnref-aYS73ikuT9JzFD3Mj-11">[11]</a></sup> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-12" id="fnref-aYS73ikuT9JzFD3Mj-12">[12]</a></sup> .</p><p> We think that early attention layers perform <strong>token concatenation</strong> by assembling the raw tokens of these entities on the final token (the “ Jordan” position). And early MLP layers perform <strong>fact lookup</strong> , a Boolean AND (aka <a href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=7G70mqwvXn7LoiOBc6-IRuLo">detokenization</a> ) on the raw tokens to output this multi-token embedding with information about the described entity. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-13" id="fnref-aYS73ikuT9JzFD3Mj-13">[13]</a></sup> Notably, we believe this lookup is implemented by <em>several</em> MLP layers, and was not localisable to specific neurons or even a single layer.</p><p> A significant caveat is that this is extrapolated from a detailed study of looking up athlete facts, rather than a broader range of factual recall tasks, though is broadly supported by the prior literature. Our speculative prediction is that factual recall of the form “recognise that some entity is mentioned in the context and recall attributes about them” looks like this, but more complex or sophisticated forms of factual recall may not (eg multi-hop reasoning, or recall with a richer or more hierarchical structure).</p><p> We didn&#39;t make much progress on understanding the internal mechanisms of these early MLP layers. Our take is that it&#39;s fine to just black box these early layers as “the thing that produces the multi-token embeddings” and to focus on understanding the meaningful subspaces in these embeddings and how they&#39;re used by circuits in later layers, and not on exactly how they&#39;re computed, ie focusing on the representations. This is similar to how we conventionally think of the token embeddings as a big look-up table. One major difference is that the input space to the token embeddings is a single token, d_vocab (normally about 50,000), while the input to the full model or other sub circuits are strings of tokens, which has a far larger input space. While you can technically think of language models as a massive lookup table, this isn&#39;t very productive! This isn&#39;t an issue here, because we focus on inputs that are the (tokenized) names of known entities, a far smaller set.</p><p> We think the goal of mech interp is to be useful for downstream tasks (ie understanding meaningful questions about model behaviour and cognition, especially alignment-relevant ones!) and we expect most of the interesting computation for downstream tasks to come in mid to late layers 。 Understanding these may require taking a dictionary of simpler representations as a given, but hopefully does not require understanding how the simpler representations were computed.</p><p> We take this lack of progress on understanding internal mechanisms as meaningful evidence against the goal of fully reverse-engineering everything in the model, but we think mech interp can be useful without achieving something that ambitious. Our vision for usefully doing mech interp looks more like reverse-engineering as much of the model as we can, zooming in on crucial areas, and leaving many circuits as smaller blackboxed submodules, so this doesn&#39;t make us significantly more pessimistic on mech interp being relevant to alignment. Thus we think it is fine to leave this type of factual recall as one of these blackboxed submodules, but note that failing at any given task should make you more pessimistic about success at any future mech interp task.</p><h3> Is it surprising that we didn&#39;t get much traction?</h3><p> In hindsight, we don&#39;t find it very surprising that we failed to interpret the early MLP layers (though we&#39;re glad we checked!). Conceptually, on an algorithmic level, factual recall is likely implemented as a giant lookup table. As many students can attest, there simply isn&#39;t much structure to capitalise on when memorising facts: knowing that &quot;Tim Duncan&quot; should be mapped to &quot;basketball&quot; doesn&#39;t have much to do with knowing that &quot;George Brett&quot; should be mapped to &quot;baseball&quot;; these have to be implemented separately. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-14" id="fnref-aYS73ikuT9JzFD3Mj-14">[14]</a></sup> (We deliberately focus on forms of factual recall without richer underlying structure.)</p><p> A giant lookup table has a high minimum description length, which suggests the implementation is going to involve a lot of parameters. This doesn&#39;t necessarily imply it&#39;s going to be hard to interpret: we can conceive of nice interpretable implementations like a neuron per entity, but these seem highly inefficient. In theory there could be interpretable schemes that are more efficient (we discuss, test and falsify some later on like <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk#Single_Step_Detokenization">single-step detokenization</a> ), but crucially there&#39;s not a strong <em>reason</em> the model should use a nice and interpretable implementation, unlike with more algorithmic tasks like induction heads.</p><p> Mechanistically, training a neural network is a giant curve fitting problem, and there are likely many dense and uninterpretable ways to succeed at this. We should only expect interpretability when there is some structure to the problem that the training process can exploit. In the case of fact recall, the initial representation of each athlete is just a specific point in activation space <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-15" id="fnref-aYS73ikuT9JzFD3Mj-15">[15]</a></sup> , and there is (almost) no structure to exploit, so unsurprisingly we get a dense and uninterpretable result. In post 4, we also studied a toy model mapping pairs of integers to arbitrary labels where we knew all the data and could generate as much as we liked, and didn&#39;t find the toy model any easier to interpret, in terms of finding internal sparsity or meaningful intermediate states.</p><h3> Look for circuits building on factual representations, not computing them</h3><p> In mechanistic interpretability, we can both study <strong>features</strong> and <strong>circuits</strong> . Features are properties of the input, which are represented in internal activations (eg, “projecting the residual stream onto direction v after layer 16 recovers the sentiment of the current sentence”). Circuits are algorithms inside the model, often taking simpler features and using them to compute more complex ones. Some works prioritise <a href="https://arxiv.org/abs/2310.01405">understanding features</a> , others prioritise understanding circuits <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-16" id="fnref-aYS73ikuT9JzFD3Mj-16">[16]</a></sup> , we consider both to be valid approaches to mechanistic interpretability.</p><p> In the specific case of factual recall, we tried to understand the circuits by which attributes about an entity were looked up, and broadly failed. We think a reasonably halfway point is for future work to focus on understanding how these looked up attributes are represented, and how they are used by downstream circuits in later layers, treating the factual recall circuits of early layers as a small black-boxed submodule. Further, since the early residual stream is <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH#Context_doesn_t_matter_when_looking_up_an_athlete_s_sport">largely insensitive to context before the athlete&#39;s name</a> , fact injection seems the <em>only</em> thing the early MLPs can be doing, suggesting that little is lost from not trying to interpret the early MLP layers (in this context). <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-17" id="fnref-aYS73ikuT9JzFD3Mj-17">[17]</a></sup></p><h2>我们学到了什么</h2><p>We split our investigation into four subsequent posts:</p><ol><li> A detailed analysis of the factual recall circuit with causal techniques, which localises the facts to MLPs 2 to 6 and understands a range of high-level facts about the circuit</li><li> A deep dive into the internal mechanisms of these MLP layers that considers several hypotheses and collects evidence for and against, which attempts to (and largely fails to) achieve a mechanistic understanding from the weights.</li><li> A shorter post exploring toy models of factual recall/memorisation, and how they seem similarly uninterpretable to Pythia, and how this supports conceptual arguments for how factual recall may not be interpretable in general</li><li> A shorter post exploring a stronger and more general version of the multi-token embedding hypothesis: that <em>in general</em> residual streams in early layers are a function of recent tokens, and the further-back context only comes in at mid layers. There is a general tendency for this to happen, but some broader context is still used in early layers</li></ol><h3> What we learned about the circuit ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/3tqJ65kuTkBh8wrRH">Post 2</a> Summary)</h3><ul><li><p> The high-level picture from Geva et al mostly holds up: Given a prompt like “Fact: Michael Jordan plays the sport of”, the model detokenizes “Michael AND Jordan” on the Jordan token to represent <em>every</em> fact the model knows about Michael Jordan 。 After layer 6 the sport is clearly linearly represented on the Jordan token. There are mid to late layer fact extracting attention heads which attend from “of” to “Jordan” and map the sport to the output logits (see Chughtai et al (forthcoming) for a detailed investigation of these, notably finding that these heads still attend and extract the athlete&#39;s sport even if the relationship asks for a non-sport attribute).</p></li><li><p> We simplified the fact extracting circuit down to what we call the <strong>effective model</strong> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-18" id="fnref-aYS73ikuT9JzFD3Mj-18">[18]</a></sup> . For two token athlete names, this consists of attention layers 0 and 1 retrieving the previous token embedding and adding it to the current token embedding. These summed token embeddings then go through MLP layers 2 to 6 and at the end the sport is linearly recoverable. Attention layers 2 onwards can be ablated, as can MLP layer 1.</p><ul><li> The linear classifier can be trained with logistic regression. We also found high performance (86% accuracy) by taking the directions that a certain attention head (Layer 16, Head 20) mapped to the output logits of the three sports: baseball, basketball, football <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-19" id="fnref-aYS73ikuT9JzFD3Mj-19">[19]</a></sup> .</li></ul></li><li><p> We simplified our analysis to interpreting the effective model, where the “end” of the model is a linear classifier and softmax between the three sports.</p><ul><li> We think the insight of “once the feature is linearly recoverable, it suffices to interpret <em>how</em> the linear feature extraction happens, and the rest of the circuit can be ignored” may be useful in other circuit analyses, especially given how easy it is to train linear probes (given labelled data).</li></ul></li></ul><h3> What we learned about the MLPs ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/CW5onXm6uZxpbpsRk">Post 3</a> Summary)</h3><ul><li> Once we&#39;d zoomed in on the facts being stored in MLPs 2 to 6, we formed, tested <strong>and falsified</strong> a few hypotheses about what was going on:<ul><li>笔记; These were fairly specific and detailed hypotheses, which were fairly idiosyncratic to my (Neel&#39;s) gut feel about what might be going on. The space of all <em>possible</em> hypotheses is large.</li><li> In fact, investigating these hypotheses made us think they were likely false, in ways that taught us about both how LLMs are working and the nature of the problem&#39;s messiness.</li></ul></li><li> <strong>Single step detokenization hypothesis</strong> : There&#39;s a bunch of “detokenization” neurons that directly compose with the raw token embeddings, use their GELU to simulate a Boolean AND for eg prev==Michael &amp;&amp; curr==Jordan, and output a linear representation of all facts the model knows about Michael Jordan<ul><li> Importantly, this hypothesis claims that the same neurons matter for all facts about a given subject, and there&#39;s no intermediate state between the raw tokens and the attributes.</li><li> <a href="https://arxiv.org/pdf/2305.01610.pdf">Prior work</a> suggests that detokenization is done in superposition, suggesting a more complex mechanism than just one neuron per athlete. I hypothesised this was implemented by some combinatorial setup where each neuron detokenizing many strings, and each string is detokenized by many neurons, but each string corresponds to a unique <em>subset</em> of neurons firing above some threshold.</li></ul></li><li> <strong>We&#39;re pretty confident the single step detokenization hypothesis is false</strong> (at least, in the strong version, though there may still be kernels of truth).证据：</li><li> Path patching (noising) shows there&#39;s meaningful composition between MLP layers, suggesting the existence of intermediate representations. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/iGuwZTHWb6DFY3sKB/aububbyo97pul54yhl22" alt=""></p><ul><li> We collect multiple facts the model knows about Michael Jordan. We then resample ablate <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-20" id="fnref-aYS73ikuT9JzFD3Mj-20">[20]</a></sup> (patch) each neuron (one at a time), and measure its effect on outputting the correct answer. The single step detokenization hypothesis predicts that the same neurons will matter for each fact. We measure the correlation between the effect on each pair of facts, and observe little correlation, suggesting different mechanisms for each fact. </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/zsa7a3aqscgpfiol8ogh" alt=""></p><ul><li><p> By measuring the non-linear effect of each neuron (how much it activates more for Michael Jordan than eg Keith Jordan or Michael Smith), we see that the neurons directly connecting with the probe already had a significant non-linear effect pre-GELU. This suggests that they compose with early neurons that compute the AND rather than the neurons with direct effect just computing the AND itself, ie that there are important intermediate states</p></li><li><p> <strong>Hash and lookup hypothesis</strong> : where the first few MLPs “hash” the raw tokens of the athlete into a gestalt representation of the name that&#39;s almost orthogonal to other names, and then neurons in rest of the MLPs “look up” these hashed representations by acting as a lookup table mapping them to their specific sports.</p><ul><li> One role the hashing could serve is breaking linear structure between the raw tokens<ul><li> Eg “Michael Jordan” and “Tim Duncan” play basketball, but we don&#39;t want to think that “Michael Duncan” does</li><li> In other words, this lets the model form a representation of “Michael Jordan” that is not necessarily similar to the representations of “Michael” or “Jordan”</li></ul></li><li> Importantly, hashing should work for <em>any</em> combination of tokens, not just ones the model has memorised in training, though looking up may not work for unknown names.</li></ul></li><li><p> <strong>We&#39;re pretty confident that the <em>strong</em> version of the hash and lookup hypothesis is false</strong> , though have found it a useful framework and think there may be some truth to it <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-21" id="fnref-aYS73ikuT9JzFD3Mj-21">[21]</a></sup> . Unfortunately “there&#39;s some truth to it, but it&#39;s not the whole story” turned out to be very hard to prove or falsify.证据：</p></li><li><p> (Negative) If you linearly probe for sport on the residual stream, validation accuracy improves throughout the “hashing” layers (MLPs 2 to 4), showing there can&#39;t be a clean layerwise separation between hashing and lookup. Pure hashing has no structure, so validation accuracy should be at random during the intermediate hashing layers. </p></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/b5xo66fwzwxmue0b1ytb" alt=""></p><ul><li> (Negative) Known names have higher MLP output norm than unknown names even at MLP1 and MLP2 - it doesn&#39;t treat known and unknown names the same. MLP1 and MLP2 are hashing layers, so the model shouldn&#39;t distinguish between known and unknown names this early. (This is weak evidence since there may eg be an amortised hash that has a more efficient subroutine for commonly-hashed terms, but still falsifies the strong version of the hypothesis, that hashing occurs with no regard for underlying meaning.)</li><li> (Positive) The model does significantly break the linear structure between tokens. Eg there&#39;s a significant component on the residual for “Michael Jordan” that&#39;s not captured by the average of names like “Michael Smith” or “John Jordan”. This is exactly what we expect hashing to do. * However, this is not strong evidence that the <em>main</em> role of MLP layers is to break linear structure, they may be doing a bunch of other stuff too. Though they do break linear structure significantly more than randomly initialised layers</li><li> We found a <strong>baseball neuron</strong> (L5N6045) which activated significantly more on the names of baseball playing athletes, and composed directly with the fact extracting head<ul><li> The input and output weights of the neuron have significant cosine sim with the head-mediated baseball direction (but it&#39;s significantly less than 1): it&#39;s partially just boosting an existing direction, but partially doing something more<ul><li> If you take the component of every athlete orthogonal to the other 1500 athletes, and project those onto the baseball neuron, it still fires more for baseball players than other players, maybe suggesting it is acting as a lookup table too.</li></ul></li><li> Looking on a broader data distribution than just athlete names, it often activates on sport-related things, but is not monosemantic. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/CW5onXm6uZxpbpsRk/z4lgnpzcuyzlngtepv16" alt=""></p><h3> How to Think About Interpreting Memorisation ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/JRcNNGJQ3xNfsxPj4">Post 4</a> Summary)</h3><p> <em>This is a more philosophical and less rigorous post, thinking about memorisation and the modelling assumptions behind it, that was inspired by work with toy models, but has less in the way of rigorous empirical work.</em></p><ul><li><p>动机：</p><ul><li> When we tried to understand mechanistically how the network does the “fact lookup” operation, we made very little progress.</li><li> What should we take away from this failure about our ability to interpret MLPs in general when they compute distributed representations? In particular, what meaningful representations <em>could</em> we have expected to find in the intermediate activations of the subnetwork?</li></ul></li><li><p> In post 4, we study toy models of memorisation (1-2 MLP layers mapping pairs of integers to fixed and random binary labels) to study memorisation in a more pure setting. The following is an attempt to distil our intuitions following this investigation, we do not believe this is fully rigorous, but hope it may be instructive.</p></li><li><p> We believe one important aspect of fact lookup, as opposed to most computations MLPs are trained to do, is that it mainly involves memorisation: there are no general rules that help you guess what sport an athlete plays from their name alone (at least, with any great accuracy <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-22" id="fnref-aYS73ikuT9JzFD3Mj-22">[22]</a></sup> ).</p></li><li><p> If a network solves a task by generalisation, we may expect to find representations in its intermediate state corresponding to inputs and intermediate values in the generalising algorithm that it has implemented to solve the task. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-23" id="fnref-aYS73ikuT9JzFD3Mj-23">[23]</a></sup></p></li><li><p> On the other hand, if a network is unable to solve a task by generalisation (and hence has to memorise instead), this indicates that either no generalising algorithm exists, or at least that none of the features useful for implementing a possible generalising algorithm are accessible to the network.</p></li><li><p> Therefore, under the memorisation scenario, the only meaningful representations (relevant to the task under investigation) that we should find in the subnetwork&#39;s inputs and intermediate representations are:</p><ul><li> Representations of the original inputs themselves (eg we can find features like “first name is George”).</li><li> Noisy representations of the target concept being looked up: for example, the residual stream after the first couple of MLP layers contains a noisier, but still somewhat accurate, representation of the sport played, because it is a partial sum of the overall network&#39;s output. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-24" id="fnref-aYS73ikuT9JzFD3Mj-24">[24]</a></sup> <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-25" id="fnref-aYS73ikuT9JzFD3Mj-25">[25]</a></sup></li></ul></li><li><p> On the other hand, on any finite input domain – such as the Cartesian product of all first name / last name tokens – the output of <em>any</em> model component (neuron, layer of neurons, or multiple layers of neurons) can be interpreted as an embedding matrix: ie a lookup table individually mapping every first name / last name pair to a specific output vector. In this way, we could consider the sport lookup subnetwork (or any component in that network) as implementing a lookup table that maps name token pairs to sport representations.</p></li><li><p> In some sense, this perspective provides a trivial interpretation of the computation performed by the MLP subnetwork: the weights of the subnetwork are such that – within the domain of the athlete sport lookup task – the subnetwork implements exactly the lookup table required to correctly represent the sport of known athletes. And memorisation is, in a sense by definition, a task where there should not be interesting intermediate representations to interpret.</p></li></ul><h3> Do Early Layers Specialise in Local Processing? ( <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR">Post 5</a> Summary)</h3><ul><li>动机<ul><li><p>The multi-token embedding hypothesis suggests that an important function of early layers is to gather together tokens that comprise a semantic unit (eg <code>[ George] [ Brett]</code> ) and “look up” an embedding for that unit, such that important features of that unit (eg “plays baseball”) are linearly recoverable.</p></li><li><p> This got us thinking: to what extent is this <em>all</em> that early layers do? <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-26" id="fnref-aYS73ikuT9JzFD3Mj-26">[26]</a></sup></p></li><li><p> Certainly, it seems that many linguistic tasks require words / other multi-token entities to be looked up before language-level processing can begin. In particular, many words are split into multiple tokens but seem likely better thought of as a single unit. Such tasks can only be accomplished after lookup has happened, ie in later layers.</p></li><li><p> On the other hand, there is no reason to wait until multi-token lookup is finished for single-token level processing (eg simple induction behaviour) to proceed. So it&#39;s conceivable that early layers aren&#39;t <em>all</em> about multi-token embedding lookup either: they could be getting on with other token-level tasks in parallel to performing multi-token lookup.</p></li><li><p> If early layers only perform multi-token lookup, an observable consequence would be that early layers&#39; activations should primarily be a function of nearby context. For multi-token lookup to work, only the few most recent tokens matter.</p></li></ul></li><li> So, to measure the extent to which early layers perform multi-token lookup, we performed the following experiment:<ul><li> Collect residual stream activations across the layers of Pythia 2.8B for a sample of strings from the Pile.</li><li> Collect activations for tokens from the same strings with differing lengths of truncated context: ie for each sampled token, we would collect activations for that token plus zero preceding tokens, one preceding token, etc, up to nine preceding tokens.</li><li> Measure the similarity between the residual stream activations for tokens in their full context versus in the truncated contexts by computing the (mean centred) cosine similarities.</li><li> If early layers only perform local processing, then the cosine similarities between truncated context activations and full context activations should be close to one (at least for a long-enough truncation window).</li></ul></li><li> Collecting the results, we made the following observations:<ul><li> Early layers do perform more local processing than later layers: cosine similarities between truncated and full contexts are clearly high in early layers and get lower in mid to late layers.</li><li> There is a sharp difference though between cosine sims with zero prior context and with some prior context, showing that local layers are meaningfully dependent on nearby context (ie they aren&#39;t simply processing the current token).</li><li> However, even at layer 0, local layers do have some dependence on distant context: while cosine sims are high (above 0.9) they aren&#39;t close to 1 (often &lt;0.95) <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-27" id="fnref-aYS73ikuT9JzFD3Mj-27">[27]</a></sup> . </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/yyrikd6m6xpbzqte3pnh" alt=""></p><ul><li> Interestingly, we found that truncated vs full context cosine sims for early layers are particularly low on punctuation tokens (periods, commas, newlines, etc) and “stop words” (and, or, with, of, etc) – indicating that early layers perform highly non-local processing on these tokens.<ul><li> In retrospect, this isn&#39;t so surprising, as the context immediately preceding such tokens often doesn&#39;t add that much to their meaning, at least when compared to word fragment tokens like [ality], so processing them can start early (without waiting for a detokenisation step first).</li><li> There are several other purposes that representations at these tokens might serve – eg summarisation, resting positions, counting delimiters – that could explain their dependence on longer range context. </li></ul></li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xE3Y9hhriMmL4cpsR/b8aqjddkgqwgooduk5b2" alt=""></p><h2> Further Discussion</h2><h3> Do sparse autoencoders make understanding factual recall easier?</h3><p> A promising recent direction has been to <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">train Sparse Autoencoders</a> (SAEs) to extract monosemantic features from superposition. <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-28" id="fnref-aYS73ikuT9JzFD3Mj-28">[28]</a></sup> We haven&#39;t explored SAEs ourselves <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-29" id="fnref-aYS73ikuT9JzFD3Mj-29">[29]</a></sup> and so can&#39;t be confident in this, but our guess is that SAEs don&#39;t solve our problem of understanding the circuitry behind fact localisation, though may have helped to narrow down hypotheses.</p><p> The core issue is that SAEs are a tool to better understand <em>representations</em> by decomposing them into monosemantic features, while we wanted to understand the factual recall <em>circuit</em> : how do the parameters of the MLP layers implement the algorithm mapping tokens of the names to the sport played ？ The key missing piece in SAEs is that if meaningful features in MLP layers (pre and post GELU) are not basis aligned then we need (in theory) to understand the function of <em>every</em> neuron in the layer to understand the mapping. In contrast, basis aligned features (ie corresponding to a single neuron pre and post GELU) can be understood in terms of a single GELU, and the value of all other neurons is irrelevant.</p><p> Understanding representations better can be an extremely useful intermediate step that helps illuminates a circuit, but we expect this to be less useful here. The obvious features an SAE might learn are first name (eg “first name is Michael”), last name (eg “last name is Jordan”) and sport (eg “plays basketball”). It seems fairly likely that there&#39;s an intermediate feature corresponding to each entity eg (“is Michael Jordan”) which may be learned by an extremely wide SAE (though it would need to be very wide! There are a lot of entities out there), though we don&#39;t expect many meaningful features corresponding to groups of entities <sup class="footnote-ref"><a href="#fn-aYS73ikuT9JzFD3Mj-30" id="fnref-aYS73ikuT9JzFD3Mj-30">[30]</a></sup> . But having these intermediate representations isn&#39;t obviously enough to understand how a given MLP layer works on the parameter level.</p><p> We can also learn useful things from what an SAE trained on the residual stream or MLP activations at different layers can learn. If we saw sharp transitions, where the sport only became a feature after layer 4 or something, this would be a major hint! But unfortunately, we don&#39;t expect to see sharp transitions here, you can probe non-trivially for sport on even the token embeddings, and it smoothly gets better across the key MLP layers (MLPs 2 to 6).</p><h3>未来的工作</h3><p>In terms of overall implications for mech interp, we see our most important claims as that the early MLPs in factual recall implement “multi-token embeddings”, ie for known entities whose name consists of multiple tokens, the early MLPs produce a linear representation of known attributes, and more speculatively that we do <em>not</em> need to mechanistically understand how these multi-token embeddings are computed in order for mech interp to be useful (so long as we can understand what they represent, <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB#Look_for_circuits_building_on_factual_representations__not_computing_them">as argued above</a> ). In terms of future work, we would be excited for people to red-team and build on our claim that the mechanism of factual lookup <em>is</em> to form these multi-token embeddings. We would also be excited for work looking for concrete cases where we do need to understand tightly coupled computation in superposition (for factual recall or otherwise) in order for mech interp to be useful on a downstream task with real-world/alignment consequences, or just on tasks qualitatively different from factual recall.</p><p> In terms of whether there should be further work trying to reverse-engineer the mechanism behind factual recall, we&#39;re not sure. We found this difficult, and have essentially given up after about 6-7 FTE months on this overall project, and through this project we have built intuitions that this may be fundamentally uninterpretable. But we&#39;re nowhere near the point where we can confidently rule it out, and expect there&#39;s many potentially fruitful perspectives and techniques we haven&#39;t considered. If our above claims are true, we&#39;re not sure if this problem needs to be solved for mech interp to be useful, but we would still be very excited to see progress here. If nothing else we expect it would teach us general lessons about computation in superposition which we expect to be widely applicable in models. And, aesthetically, we find it fairly unsatisfying that, though we have greater insight into how models recall facts, we didn&#39;t form a full, parameter-level understanding.</p><h2>致谢</h2><p>We are grateful to Joseph Bloom, Callum McDougall, Stepan Shabalin, Pavan Katta and Stefan Heimersheim for valuable feedback. We are particularly grateful to Tom Lieberum and Sebastian Farquhar for the effort they invested in giving us detailed and insightful feedback that significantly improved the final write-up.</p><p> We are also extremely grateful to Tom Lieberum for being part of the initial research sprint that laid the seeds for this project and gave us the momentum we needed to get started.</p><p> This work benefited significantly from early discussions with Wes Gurnee, in particular for the suggestion to focus on detokenization of names, and athletes as a good category.</p><h2>作者贡献</h2><p>Neel was research lead for the project, Sen was a core contributor throughout. Both contributed significantly to all aspects of the project, both technical contributions and writing. Sen led on the toy models focused work.</p><p> Janos participated in the two week sprint that helped start the project, and helped to write and maintain the underlying infrastructure we used to instrument Pythia. Rohin advised the project.</p><h2>引文</h2><p>Please cite this work as follows</p><pre><code> @misc{ nanda2023factfinding, title={Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level}, url={https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall}, journal={Alignment Forum}, author={Nanda, Neel and Rajamanoharan, Senthooran and Kram\&#39;ar, J\&#39;anos and Shah, Rohin}, year={2023}, month={Dec}}</code> </pre><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn-aYS73ikuT9JzFD3Mj-1" class="footnote-item"><p> We use “select” because the number of tokens in the right “unit” can vary in a given context. Eg in “On Friday, the athlete Michael Jordan”, the two tokens of &#39;Michael Jordan&#39; is the right unit of analysis, while “On her coronation, Queen Elizabeth II”, the three tokens of “Queen Elizabeth II” are the right单元。 <a href="#fnref-aYS73ikuT9JzFD3Mj-1" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-2" class="footnote-item"><p> It is closely related to the observed phenomenon of polysemanticity (where a neuron fires on multiple unrelated things) and distributed representations (where many neurons fire for the same thing), but crucially superposition is a mechanistic hypothesis that tries to explain why these observations emerge, not just an empirical observation itself. <a href="#fnref-aYS73ikuT9JzFD3Mj-2" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-3" class="footnote-item"><p> In particular, distributed representation seems particularly prevalent in language MLP layers: in image models, often individual neurons were monosemantic. This happens sometimes but seems qualitatively rarer in language models. <a href="#fnref-aYS73ikuT9JzFD3Mj-3" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-4" class="footnote-item"><p> Notably, almost all transformer circuit progress has been focused on attention heads rather than MLP layers (with some exceptions eg <a href="https://arxiv.org/abs/2305.00586">Hanna et al</a> ), despite MLPs being the majority of the parameters. We expect that part of this is that MLPs are more vulnerable to superposition than attention heads. Anecdotally, on a given narrow task, often a sparse set of attention heads matter, while a far larger and more diffuse set of MLP neurons matter, and <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Bricken et al</a> suggests that there are many monosemantic directions in MLP activations that are sparse <a href="#fnref-aYS73ikuT9JzFD3Mj-4" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-5" class="footnote-item"><p> Though real models can also be difficult to study, as they may be full of messy confounders. Newtonian physics is particularly easy to reason about with billiard balls, but trying to learn about Newtonian physics by studying motion in a hurricane wouldn&#39;t be a good idea either. (Thanks to Joseph Bloom for this point!) <a href="#fnref-aYS73ikuT9JzFD3Mj-5" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-6" class="footnote-item"><p> A weakness in our analysis specifically is that we only studied 1,500 facts (the sports of 1,500 athletes) while the 5 MLP layers we studied each had 10,000 neurons, which was fairly under-determined! Note that we expect even Pythia 2.8B knows more than 50,000 facts total, athlete sports just happened to be a convenient subset to study. <a href="#fnref-aYS73ikuT9JzFD3Mj-6" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-7" class="footnote-item"><p> There are edge cases where eg famous people share the same name (eg <a href="https://en.wikipedia.org/wiki/Michael_Jordan">Michael Jordan</a> and <a href="https://en.wikipedia.org/wiki/Michael_I._Jordan">Michael Jordan</a> !), our guess is that the model either ignores the less famous duplicate, or it looks up a “combined” factual embedding that it later disambiguates from语境。 Either way, this is only relevant in a small fraction of factual recall cases. <a href="#fnref-aYS73ikuT9JzFD3Mj-7" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-8" class="footnote-item"><p> Note that models have many fewer neurons than parameters, eg GPT-3 has 4.7M neurons and 175B parameters, because each neuron has a separate vector of d_model input and output weights (and in GPT-3 d_model is 12288) <a href="#fnref-aYS73ikuT9JzFD3Mj-8" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-9" class="footnote-item"><p> Of course this is an imperfect abstraction, eg non-Western names are more likely to play internationally popular sports, and certain tokens may occur in exactly one athlete&#39;s name. <a href="#fnref-aYS73ikuT9JzFD3Mj-9" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-10" class="footnote-item"><p> These are not the same as contextual word embeddings (as have been <a href="https://aclanthology.org/D18-1179/">studied</a> <a href="https://arxiv.org/abs/1905.05950">extensively</a> with BERT-style models in the past). It has long been known that models enrich their representation of tokens as you go from layer to layer, using attention to add context to the (context-free) representations provided by the embedding layer. Here, however, we&#39;re talking about how models represent linguistic units (eg names) that span multiple tokens <em>independently</em> of any <em>additional</em> context. Multi-token entities need to be represented somehow to aid downstream computation, but in a way that can&#39;t easily be attributed back to individual tokens, similar to what&#39;s been termed <a href="https://transformer-circuits.pub/2022/solu/index.html#section-6-3-2">detokenization</a> in the literature. In reality, models probably create <em>contextual multi-token embeddings</em> , where multi-token entities&#39; representations are further enriched by additional context, complicating the picture, but we still think multi-token embeddings are a useful concept for understanding models&#39; behaviour. <a href="#fnref-aYS73ikuT9JzFD3Mj-10" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-11" class="footnote-item"><p> For example, the model has a &quot;plays basketball&quot; direction in the residual stream. If we projected the multi-token representation for &quot;| MIchael| Jordan&quot; onto this direction, it would be unusually high <a href="#fnref-aYS73ikuT9JzFD3Mj-11" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-12" class="footnote-item"><p> This mirrors the findings in <a href="https://arxiv.org/abs/2308.09124">Hernandez et al</a> , that looking up a relationship (eg “plays the sport of”) on an entity is a linear map. <a href="#fnref-aYS73ikuT9JzFD3Mj-12" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-13" class="footnote-item"><p> Eg mapping “first name is Michael” and “last name is Jordan” to “is the entity Michael Jordan” with attributes like “plays basketball”, “is an athlete”, “plays for the Chicago Bulls” etc. <a href="#fnref-aYS73ikuT9JzFD3Mj-13" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-14" class="footnote-item"><p> Though there&#39;s room for some heuristics, based on eg inferring the ethnicity of the name, or eg athletes with famous and unique single-token surnames where the sport may be stored in the token embedding. <a href="#fnref-aYS73ikuT9JzFD3Mj-14" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-15" class="footnote-item"><p> As argued in <a href="https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/xE3Y9hhriMmL4cpsR">post 5</a> , since early layers don&#39;t integrate much prior context before the name. <a href="#fnref-aYS73ikuT9JzFD3Mj-15" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-16" class="footnote-item"><p> Which often implicitly involves understanding features. <a href="#fnref-aYS73ikuT9JzFD3Mj-16" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-17" class="footnote-item"><p> A caveat to this point is that without mechanistic understanding of the circuit it might be hard to verify that we have found all the facts that the MLPs inject -- though a countercounterpoint is that you can look at loss recovered from the facts you do know on a representative dataset <a href="#fnref-aYS73ikuT9JzFD3Mj-17" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-18" class="footnote-item"><p> ie showed that we can ablate everything else in the model without substantially affecting performance <a href="#fnref-aYS73ikuT9JzFD3Mj-18" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-19" class="footnote-item"><p> Each sport is a single token. <a href="#fnref-aYS73ikuT9JzFD3Mj-19" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-20" class="footnote-item"><p> Ie we take another athlete (non-basketball playing), we take the neuron&#39;s activation on that athlete, and we intervene on the “Michael Jordan” input to replace that neuron&#39;s activation on the Jordan token with its activation on the final name token of the other athlete. We repeat this for 64 randomly selected other athletes. <a href="#fnref-aYS73ikuT9JzFD3Mj-20" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-21" class="footnote-item"><p> Note that it&#39;s unsurprising to us that the strong form of hashing didn&#39;t happen - even if at one point in training the early layers do pure hashing with no structure, future gradient updates will encourage early layers to bake in <em>some</em> known structure about the entities it&#39;s hashing, lest the parameters be wasted. <a href="#fnref-aYS73ikuT9JzFD3Mj-21" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-22" class="footnote-item"><p> We suspect that generalisation probably does help a bit: the model is likely using correlations between features of names – eg cultural origin – and sports played to get some signal, and some first names or last names have sport (of famous athlete(s) with that name) encoded right in their token embeddings. But the key point is that these patterns alone would not allow you to get anywhere near decent accuracy when looking up sport; memorisation is key to being successful at this task. <a href="#fnref-aYS73ikuT9JzFD3Mj-22" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-23" class="footnote-item"><p> Note that we&#39;re not saying that we would always succeed in finding these representations; the network may solve the task using a generalising algorithm we do not understand, or we may not be able to decode the relevant representations. The point we&#39;re making is that, for generalising algorithms, we at least know that there must be meaningful intermediate states (useful to the algorithm) that we at least have a chance of finding represented within the network. <a href="#fnref-aYS73ikuT9JzFD3Mj-23" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-24" class="footnote-item"><p> Of course, there are many other representations in these subnetwork&#39;s intermediate activations, because the subnetwork&#39;s weights come from a language model trained to accomplish many tasks. But these representations cannot be decoded using just the task dataset (athlete names and corresponding sports). And, to the extent that these representations are truly task-irrelevant (because we assume the task can only be solved by memorisation – see the caveat in an earlier footnote), our blindness to these other representations shouldn&#39;t impede our ability to understand how sport lookup is accomplished. <a href="#fnref-aYS73ikuT9JzFD3Mj-24" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-25" class="footnote-item"><p> Other representations we might find in a model in general include “clean” representations of the target concept (sport played), and functions of this concept (eg “plays basketball AND is over 6&#39;8&#39;&#39; tall”), but these should only appear following the lookup subnetwork, because we defined the subnetwork to end precisely where the target concept is first cleanly represented. <a href="#fnref-aYS73ikuT9JzFD3Mj-25" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-26" class="footnote-item"><p> Note that the answer to this question does not directly tell us anything (positive or negative) about the multi-token embedding hypothesis: it could be that early layers perform many local tasks, of which multi-token embeddings is just one; alternatively it could be the case that early layers do lots of non-local processing in addition to looking up multi-token embeddings. <a href="#fnref-aYS73ikuT9JzFD3Mj-26" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-27" class="footnote-item"><p> We note that it&#39;s kind of ambiguous how best to quantify “how much damage did truncation do”. Arguably cosine sim squared might be a better metric, as it gives the fraction of the norm explained, and 0.9^2=0.81 looks less good. <a href="#fnref-aYS73ikuT9JzFD3Mj-27" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-28" class="footnote-item"><p> We note that by default SAEs get you representation sparsity but not circuit sparsity - if a SAE feature is distributed in the neuron basis then thanks to the per-neuron non-linearity any neuron can affect the output feature and can&#39;t naively be analysed in隔离。 <a href="#fnref-aYS73ikuT9JzFD3Mj-28" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-29" class="footnote-item"><p> We did the main work on this project before the <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">recent</a> <a href="https://arxiv.org/abs/2309.08600">flurry</a> of work on SAEs. If we were doing this project again, we&#39;d probably try using them! Though as noted, we don&#39;t expect them to be a silver bullet solution. <a href="#fnref-aYS73ikuT9JzFD3Mj-29" class="footnote-backref">↩︎</a></p></li><li id="fn-aYS73ikuT9JzFD3Mj-30" class="footnote-item"><p> Beyond heuristics about eg certain ethnicities being inferrable from the name and more likely to play different sports, which we consider out of scope for this investigation. In some sense, we deliberately picked a problem where we did not expect intermediate representations to be important. <a href="#fnref-aYS73ikuT9JzFD3Mj-30" class="footnote-backref">↩︎</a></p></li></ol></section><br/><br/> <a href="https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall<guid ispermalink="false"> iGuwZTHWb6DFY3sKB</guid><dc:creator><![CDATA[Neel Nanda]]></dc:creator><pubDate> Sat, 23 Dec 2023 02:44:24 GMT</pubDate> </item><item><title><![CDATA[Measurement tampering detection as a special case of weak-to-strong generalization]]></title><description><![CDATA[Published on December 23, 2023 12:05 AM GMT<br/><br/><p> Burns et al at OpenAI released <a href="https://openai.com/research/weak-to-strong-generalization"><u>a paper</u></a> studying various techniques for fine-tuning strong models on downstream tasks using labels produced by weak models. They call this problem “weak-to-strong generalization”, abbreviated W2SG.</p><p> Earlier this year, we published a paper, <a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood"><u>Benchmarks for Detecting Measurement Tampering</u></a> , in which we investigated techniques for the problem of measurement tampering detection (MTD). MTD is a special case of W2SG. In this post, we&#39;ll explain the relationship between MTD and W2SG, and explain why we think MTD is more likely than fully general W2SG to work. Of course, fully general W2SG is a strictly more valuable problem to solve, due to this generality.</p><p> We think MTD is a promising research direction. We&#39;re also excited for other problems which are special cases of W2SG that have special structure that can be exploited by techniques, especially if that structure is likely to be present in important cases in future.</p><h1> MTD as a subset of W2SG</h1><h2> A similar goal</h2><p> When training an AI, the reward we attribute to different behaviors might not match the reward we would give if we understood the situation better.</p><p> The goal of W2SG techniques is to achieve good results when training a strong AI despite only having access to a weak supervisor that understands the situation less well than the strong AI.</p><p> MTD is the special case where the weak supervisor has access to measurements which should be sufficient to understand the situation, but these measurements can be tampered with (eg replacing the camera feed with some made-up data, disabling tests, or threatening annotators). Because the measurements are sufficient in the absence of tampering, we don&#39;t need to worry about benign mistakes that could happen even without an AI optimizing to make measurements look good.</p><h2> Slightly different experiments</h2><p> W2SG can be studied using <a href="https://www.alignmentforum.org/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization"><u>sandwiching experiments</u></a> , where we try to get an AI to safely accomplish tasks despite only having access to a weak supervisor, and then we measure the performance of our method using a stronger held-out supervision signal (eg held out ground truth labels).</p><p> In the case of the OpenAI paper, the weak supervisor is a small language model trained on ground truth labels, as an analogy for human annotators.</p><p> In the case of our MTD paper, we have access to measurements, but there is some notion of measurement tampering. In our work, the measurements aim to directly measure the property of interest as a boolean value, so converting from untampered measurements to correct labels is straightforward (and doesn&#39;t require any learning or intelligence).</p><h1> Different hopes for succeeding at W2SG</h1><p> In both cases, we need some additional assumptions to get strong supervision from a weak supervisor. If we made no structural assumptions about the internals of models and assumed the worst case about the internal structure of AIs, we wouldn&#39;t succeed, so we will depend on some type of structure in the internals of models. The structure that the OpenAI paper discusses is very different from the structure we hope to leverage in the measurement tampering case.</p><h2> Confidence and consistency</h2><p> The OpenAI paper shows that on some tasks, training the model to be confident in addition to matching the weak supervisor&#39;s labels sometimes increases accuracy. The hope is that the model might have a very salient representation of what is true, and a less salient representation of the predictions of the weak supervisor.</p><p> The confidence hope is similar to the hope explored in <a href="https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without"><u>Discovering Latent Knowledge</u></a> : the inner representation of the truth inside AIs might be more consistent than other features, and be more salient than other consistent features, such that optimizing for consistency (and confidence) could give higher performance than naive training on the weak supervisor labels.</p><p> Both confidence and consistency are not enough for AIs for which “what humans think” is more salient than “what is true”, which seems very likely for AIs pretrained on human-generated text or AIs trained to predict measurements that are often tampered with.</p><p> This structure is present in a very wide range of situations. But it&#39;s not clearly enough to solve the problem. Indeed, Burns et al don&#39;t find techniques that reliably improve results, which we think is some evidence against the existence of techniques that robustly allow W2SG. We also think that the recent work “ <a href="https://www.lesswrong.com/posts/Yio4nmD8JMttx9o9S/new-paper-shows-truthfulness-and-instruction-following-don-t"><u>Generalization Analogies: a Testbed for Generalizing AI Oversight to Hard-To-Measure Domains</u></a> &quot; provides some weak evidence against the existence of techniques that robustly achieve desirable generalization without using some more specific structure.</p><h2> Detecting tampering</h2><p> In contrast, MTD has a very particular type of structure to the problem, which might allow us to develop techniques that work in a variety of settings. Measurement tampering is a particular kind of supervisor failure that has two key properties:</p><ul><li> Failed attempts at tampering might leave evidence of this wrongdoing, while you might never get strong evidence of wrongdoing if the AI system isn&#39;t trying to tamper.</li><li> Consistent relationships between measurements might be driven by similar mechanisms for real positive examples and trusted examples, while the mechanism for consistency differs between fake positive examples and trusted examples.</li></ul><p> Therefore, if you train your most powerful AI to predict measurements, these properties might translate to properties about its internals. These properties can be used by techniques that use model internals (eg by probing for evidence of tampering or by using techniques like exclusion fine-tuning), which is why it might be possible to get higher performance than if we relied on measurements alone.</p><p> We are not aware of particular reasons to expect these properties to break for AIs for which the measurement predictions are very salient. So we think that it is substantially more likely that there exist techniques which reliably solve MTD problems than that reliably solve W2SG. Overall, we think that MTD seems considerably more likely to work than fully general W2SG, but of course it is less generally applicable.</p><p> The main weakness of MTD is the somewhat fuzzy properties it relies on: so far, we haven&#39;t been able to find a precise definition of measurement tampering that can be used to unambiguously determine if a given dataset has the right “measurement tampering structure” 。</p><p> We go into more detail about these hopes in <a href="https://www.alignmentforum.org/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood#Why_solving_measurement_tampering_may_be_notably_easier_than_eliciting_arbitrary_knowledge"><u>Why solving measurement tampering may be notably easier than eliciting arbitrary knowledge</u></a> .</p><h2> Other special cases of W2SG might be good research problems</h2><p> We picked the structure in MTD partially because we think it&#39;s a particularly important kind of structure, but also because it was the simplest non trivial example we could construct of a setting where we could hope for generalization performance and we had access to some kind of structure which we could exploit using our techniques. We think that while developing MTD techniques looks like a pretty promising research direction, it would also be promising to develop techniques in a wider variety of settings where you have particular structure, because in future we might want to be able to exploit bespoke structure in particular域。</p><br/><br/> <a href="https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4KLCygqTLsMBM3KFR/measurement-tampering-detection-as-a-special-case-of-weak-to<guid ispermalink="false"> 4KLCygqTLsMBM3KFR</guid><dc:creator><![CDATA[ryan_greenblatt]]></dc:creator><pubDate> Sat, 23 Dec 2023 00:05:57 GMT</pubDate> </item><item><title><![CDATA[How does a toy 2 digit subtraction transformer predict the difference?]]></title><description><![CDATA[Published on December 22, 2023 9:17 PM GMT<br/><br/><h1><br> <strong>2位减法——差异预测</strong></h1><h1>概括</h1><p>我继续研究一个玩具 1 层 Transformer 语言模型，该模型经过训练可以进行<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a-b=\pm c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>形式的两位数加法<style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-surd + .mjx-box {display: inline-flex}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-test.mjx-test-display {display: table!important}
.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
.mjx-test.mjx-test-default {display: block!important; clear: both}
.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
</style>。在预测模型是正 (+) 还是负 (-) 后，它必须输出<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>之间的差。该模型创建在 a 和 b 基数中振荡的激活，以及作为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a - b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的函数线性变化的激活。该模型使用激活函数耦合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>方向上的振荡，然后对这些振荡求和以消除除取决于绝对差<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="|a-b|"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span></span></span></span></span></span>的方差之外的任何方差。 <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="|a-b|"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">|</span></span></span></span></span></span></span></span></span>预测正确的输出标记。我检查了该算法从输入到模型输出的完整路径。</p><h1>介绍</h1><p>在之前的文章中，我描述了<a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/">训练两位数减法变压器</a>并<a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">研究了该变压器如何预测</a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"> </a><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">输出</a><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/">的符号</a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the">。</a>在这篇文章中，我将研究如何训练模型的权重来确定两位整数之间的差异，以及模型激活的紧急行为。</p><h2>路线图</h2><p>与我在上一篇文章中从头到尾地研究模型不同，这次我将采取相反的方法，从输入开始，了解它如何转换为输出。我将按顺序分解：</p><ul><li>注意力模式，以及模型关注哪些标记来预测输出。</li><li>注意力头权重如何设置预激活。</li><li>预激活中出现的模式。</li><li>激活中出现的模式。</li><li>神经元和逻辑之间的映射。</li><li> Logits 中的模式。</li></ul><h1>注意力模式</h1><p>下面是四个示例的注意力模式的可视化（两个正，两个负， <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>使用相同的数值，但交换它们）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/vokykqs6hsusbr0yy69u" alt="该图像的 alt 属性为空；它的文件名是attention_head_examples-1.png"></p><p>我已将所有不重要的行显示为灰色，以便我们可以专注于倒数第二行，该行对应于转换器预测差值<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>的行。这里有几点需要注意：</p><ul><li>当结果为正时，H0 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> ，H3 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> 。</li><li>当结果为负时，H0 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span> ，H3 关注令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> 。</li><li> H1 和 H2 大致均匀地关注上下文中的所有标记。</li></ul><p>因此，H3 始终关注<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>中较大的一个，而 H0 始终关注 a 和 b 中较小的一个。 H1和H2似乎并没有做任何太重要的事情。</p><p>在数学中，上述直觉可以写成每个头对标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>的关注：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_{0,a} = \begin{cases} 1 &amp; t_{4} = - \\ 0 &amp; t_{4} = + \end{cases},\qquad A_{1,a} = 0,\qquad A_{2,a} = 0,\qquad A_{3,a} = \begin{cases} 0 &amp; t_{4} = - \\ 1 &amp; t_{4} = + \end{cases},\qquad"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span></span></span></span></span></span></p><p>以及每个头对令牌<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的关注：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_{0,b} = \begin{cases} 0 &amp; t_{4} = - \\ 1 &amp; t_{4} = + \end{cases},\qquad A_{1,b} = 0,\qquad A_{2,b} = 0,\qquad A_{3,b} = \begin{cases} 1 &amp; t_{4} = - \\ 0 &amp; t_{4} = + \end{cases}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span><span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span><span class="mjx-mspace" style="width: 2em; height: 0px;"></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mrow MJXc-space3"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">{</span></span> <span class="mjx-mtable" style="vertical-align: -0.85em; padding: 0px 0.167em;"><span class="mjx-table"><span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0px 0.5em 0px 0px; text-align: left; width: 0.5em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0px 0px 0px 0.5em; text-align: left; width: 2.861em;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">-</span></span><span class="mjx-strut"></span></span></span></span> <span class="mjx-mtr" style="height: 1.1em;"><span class="mjx-mtd" style="padding: 0.1em 0.5em 0px 0px; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span><span class="mjx-strut"></span></span></span> <span class="mjx-mtd" style="padding: 0.1em 0px 0px 0.5em; text-align: left;"><span class="mjx-mrow" style="margin-top: -0.2em;"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span><span class="mjx-strut"></span></span></span></span></span></span><span class="mjx-mo" style="width: 0.12em;"></span></span></span></span></span></span></span>,</p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="t_4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span>是上下文位置 4 处的标记值（紧接在 = 之后）。有理由问这是否是一个好的描述，确实如此！如果我对头 1 和 2 以及头 0 和 3 中的注意力模式进行零消融，我将上面突出显示的行替换为正确处理<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>的单热编码单位向量，参数空间中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span></span></span></span></span></span></span>问题的损失从 0.0181 减少到 0.0168。</p><h1>神经元预激活和激活</h1><h2>注意力头实现的功能</h2><p>矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}^h = W_{\rm E} W_{\rm V}^h W_{\rm O}^h W_{\rm in}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.161em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">neur</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">=</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">E</span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">W</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">V</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.327em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">O</span></span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-stack" style="vertical-align: -0.343em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.262em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">in</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">具有</span></span></span></span></span></span></span><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">形状</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">[</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">d_vocab</span></span></span></span></span></span></span></span></span></span></span> , d_mlp] 并描述当注意力头 h 关注词汇中的标记时，注意力头<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="h"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span>如何改变神经元预激活。从上一节中我们知道，头 0 和 3 完全参与标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>或<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> ，因此我们想了解头 0 和 3 如何根据其输入影响预激活。 （<i>旁白：在我</i><a href="https://evanhanders.blog/2023/12/15/2-digit-subtraction-how-does-it-predict/"><i>之前的</i></a><a href="https://www.lesswrong.com/posts/pbj6tTZyakodxC9Ho/how-does-a-toy-2-digit-subtraction-transformer-predict-the"><i>文章</i></a>中<i>，我检查了类似的内容，但是在我的</i><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">n</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">r</span></span></span></span></span></span></span></span></span></span></span>计算中存在一个微妙的错误<i>，我已经修复了该错误</i>）。</p><p>以下是我在头部对神经元激活的贡献中看到的四种模式： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/fery0ybeafqvetxvxsfe" alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions-2.png"></p><p>﻿</p><p>因此，神经元 17 基本上呈线性贡献，而许多其他神经元则具有不同类型的振荡。如果我们将每个头对每个神经元的贡献拟合一条线，减去它，然后将这些信号变换到频率空间，我们可以形成以下功率谱： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/thxwvacgstlgddcpwffx" alt="该图像的 alt 属性为空；它的文件名是 token_attention_neuron_contributions_freqspace.png"></p><p>在这里，我在三个关键频率（0.33、0.39、0.48）处放置了垂直线，这<a href="https://evanhanders.blog/2023/12/06/two-digit-subtraction/">是我在关于该主题的第一篇文章中看到的</a>。我们看：</p><ul><li>神经元 17 中线性趋势之上的小振荡在与这些关键频率相关的频谱中具有一些峰值，但它们很弱。</li><li>神经元 5 具有与每个关键频率相关的强峰值，但<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.39"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span></span></span></span></span></span>处的峰值是迄今为止最强的。其次，这个主峰并不完全尖锐。它具有很强的中心值以及紧邻中心值一侧的相当强的“英尺”值。这很快就会很重要。</li><li>神经元 76 一片混乱。像这样的神经元数量相当多。有一个很强的峰值，但功率也远离该频率，像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(f - f_{\rm peak})^{-2}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">p</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">e</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">k</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span></span></span>那样下降。</li><li>神经元125在令牌信号中表现出强烈的节拍，这对应于功率逐渐增加到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.48"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.48</span></span></span></span></span></span></span>处的峰值。这又有点混乱了。</li></ul><p>所以我们看到的是，这些峰值<i>主要</i>可以用强中心峰值和紧邻该中心峰值的频率仓中相应的强峰值来描述。这非常让人想起我们在使用加窗技术时在信号处理中看到的峰值<a href="https://en.wikipedia.org/wiki/Apodization">变迹</a>——来自尖锐中心峰值的功率会稍微分散到相邻的箱中。</p><p>我不完全确定什么是通常适合这些函数的正确函数，但这似乎是一个很好的猜测：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{n}_{i,h}(t) = \underbrace{\left(m t + c\right)}_{\rm linear} + \sum_{i}^{0.33, 0.39, 0.48} \left[a_i \underbrace{\cos(2\pi f_i t + \phi_i)}_{\text{central peak}} + \underbrace{b_i\cos(2 \pi f_i t + \theta_{0,i})\cos\left(\frac{2\pi}{d_{\rm vocab}'} t + \theta_{1,i}\right)}_{\text{`feet' of central peak}}\right]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">~</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.103em; margin: 0px 0.096em 0px -0.076em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.103em; margin: 0px 0.096em 0px -0.076em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.373em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">线性</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">+</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">Σ</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">0.33</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">,</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">0.48</span></span></span> <span class="mjx-stack" style="vertical-align: -0.311em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">⎡</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">⎢</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">⎢</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">⎢</span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">⎢</span></span></span></span></span></span></span> <span class="mjx-mrow MJXc-space1"><span class="mjx-mo" style="vertical-align: -1.959em;"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎣</span> <span class="mjx-char MJXc-TeX-size4-R" style="line-height: 0.539em; margin-bottom: -0.101em; margin-top: 0px;">a</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">i</span></span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">cos</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span></span> <span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">_</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">fit</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">+</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">phi</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">i</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">)</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;"></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.117em; margin: 0px 0.103em 0px -0.083em;">中心</span><span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">峰</span><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.117em; margin: 0px 0.103em 0px -0.083em;">+</span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 1.897em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">bi</span></span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">cos</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">_</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">_</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">(</span></span> <span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 2.028em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 2.869em; top: -1.383em;"><span class="mjx-mrow" style=""><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 2.869em; bottom: -1.246em;"><span class="mjx-msubsup" style=""><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-stack" style="vertical-align: -0.426em;"><span class="mjx-sup" style="font-size: 83.3%; padding-bottom: 0.216em; padding-left: 0.065em; padding-right: 0.06em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 83.3%; padding-right: 0.06em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">b</span></span></span></span></span></span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 2.028em;" class="mjx-line"></span></span><span style="height: 1.859em; vertical-align: -0.881em;" class="mjx-vsize"></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.372em; padding-bottom: 0.298em;">t</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">θ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-size3-R" style="padding-top: 1.256em; padding-bottom: 1.256em;">)</span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">_</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.083em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 6.728em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.519em;">中央峰‘脚’</span></span></span></span></span></span></span></span> <span class="mjx-mo" style="vertical-align: -1.959em;"><span class="mjx-delim-v"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎤</span> <span class="mjx-char MJXc-TeX-size4-R" style="line-height: 0.539em; margin-bottom: -0.101em; margin-top: 0px;">⎥ ⎥ ⎥ ⎥</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.961em; padding-bottom: 0.888em;">⎦</span></span></span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="d_{\rm vocab}' = 100"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.003em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.003em;">d</span></span></span> <span class="mjx-stack" style="vertical-align: -0.335em;"><span class="mjx-sup" style="font-size: 70.7%; padding-bottom: 0.255em; padding-left: 0.076em; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">v</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">c</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">a</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">b</span></span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">100</span></span></span></span></span></span></span>是对应于 0-99 的词汇大小。在这里，负责脚部的关键频率总和中的第二项受到我过去使用<a href="https://en.wikipedia.org/wiki/Hann_function">Hann 窗</a>所做的工作的启发。当频率为<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>的两个正弦项相乘时，它们将功率放入<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_1 \pm f_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>中，因此我上面列出的函数的这种形式给出了一条线、关键频率的功率和关键频率旁边的垃圾箱。</p><p>我使用<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">scipy 的 curve_fit</a>来<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\tilde{n}_{i,h}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.153em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">拟合</span></span></span><span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">~</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">ni</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">h</span></span></span></span></span></span></span></span></span></span></span>形式的函数，以表示头 0 和 3 对每个神经元的贡献。我发现这些拟合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm neur}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">占</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">neur</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">总</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">变异</span></span></span></span></span></span></span></span></span></span></span>性的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\gtrsim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-ams-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">≳</span></span></span></span></span></span></span> 97%。对于像上面的神经元 5 或神经元 17 这样的神经元，这种拟合解释了 >; 99% 的变异性；像神经元 125 这样的神经元具有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">约</span></span></span></span></span></span></span>97% 的变异性。像神经元 76 这样的神经元很混乱，但这种拟合解释了它们<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\sim"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">90</span></span></span></span></span></span></span> % 的变异性。</p><p>我现在用这些拟合替换神经元预激活（使用我在上一节中发现的注意模式）。我还保留了最终变得重要的偏差：我发现来自注意力头以及嵌入和位置嵌入的价值偏差（因此在上下文位置 4 进入注意力操作的原始残差流）对于设置神经元很重要预激活。</p><p>当我进行这种近似时，所有<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10^4"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.591em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4 个</span></span></span></span></span></span></span></span></span>问题的损失从 0.0181 增加到 0.0341，而准确度仅从 99.94% 下降到 99.68%。但请注意，如果我在拟合中删除“英尺”项，则损失会增加一个数量级以上，并且准确性会大幅下降，因此这似乎获得了模型使用的大部分重要部分。</p><h2>由此产生的神经元预激活</h2><p>在上一节中，我研究了模型权重如何设置神经元预激活，但现在我只想查看预激活，看看我是否可以理解描述它们的更简单的算法。</p><p>上一节让我确信可能有四种不同类别的神经元。但是，经过一番努力，结果发现只有两个，并且都可以使用简单的拟合来描述：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b) = \underbrace{m(a-b) + c}_{\text{linear}} + \underbrace{A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)}_{\text{oscillatory}}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.1em; margin: 0px 0.095em 0px -0.075em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.1em; margin: 0px 0.095em 0px -0.075em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 2.659em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">线性</span></span></span></span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-munderover MJXc-space2"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op"><span class="mjx-mrow"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span class="mjx-mo"><span class="mjx-delim-h"><span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin: 0px 0.01em 0px 0.024em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="margin-top: 0px; padding-bottom: 0.488em; margin-right: 0.01em; padding-top: 0.108em; margin-bottom: 0px;">_</span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; letter-spacing: -0.082em; margin: 0px 0.086em 0px -0.066em;"></span> <span class="mjx-char MJXc-TeX-size4-R" style="padding-top: 0.108em; padding-bottom: 0.488em; margin-right: 0.024em; margin-bottom: 0px; margin-top: 0px;"></span></span></span></span></span></span></span></span></span></span></span></span> <span class="mjx-row"><span class="mjx-under" style="font-size: 70.7%; padding-top: 0.236em; padding-bottom: 0.141em; padding-left: 8.536em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.519em;">振荡</span></span></span></span></span></span></span></span></span></span></span></span></span></p><p> 其中存在线性部分（斜率<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="m"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">m</span></span></span></span></span></span></span>和截距<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span> ）和振荡拟合（振幅<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_1,A_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> 、相位<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_1,\phi_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">、</span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span> ），并且每个神经元具有主导特征<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f_i = \omega_i/(2\pi) \in \{0.33, 0.39, 0.48\}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.06em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">频率</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">fi</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">/</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">ε</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">{</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.33</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.39</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.48</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">}</span></span></span></span></span></span></span>在主要模型频率中。</p><p>问题是——这种拟合仅由模型在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a \geq b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.446em;">≥</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span>的</span></span></span></span></span><i>区域中实现</i><i>。</i>在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b > a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>的区域中，模型仅重用与交换输入的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>情况相同的计算。因此，我将上面的函数<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>拟合到<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a \in [50, 99]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">50</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">99</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b \in [0, 49]"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">∈</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mn MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">49</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span></span></span></span></span>的区域中，以便均匀地<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span> 。然后，我计算所有输入值的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i(a,b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ，然后将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b > a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>处的输出值替换为相应的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a > b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.372em;">>;</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>情况（例如，我将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="33-10"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">33</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span></span></span></span></span>中的激活值放入以下位置： <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="10-33"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10-33</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">）</span></span> <span class="mjx-mn MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">。</span></span></span></span></span></span></span></p><p>要了解其工作原理，这里有一些神经元预激活示例（顶行）和这些拟合（底行）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/k9jyvjhfnrrcjkktonqb" alt="该图像的 alt 属性为空；它的文件名是sample_neuron_preactivations.png"></p><p>这些合身看起来真的很不错！如果我进入模型并将所有神经元预激活替换为最适合的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\hat{n}_i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span> ，那么模型损失仅从 0.0181 变化到 0.0222，准确度仅从 99.94% 下降到 99.92%。此外，在 ReLU() 之后，拟合解释了神经元激活中 95% 的变异性。所以我对模型如何构建预激活的描述非常满意！</p><h2>神经元激活</h2><p>ReLU() 是问题中的一种非线性。它将我上面描述的良好贴合的力量向外传播。具体来说，我发现（类似于<a href="https://arxiv.org/abs/2301.05217">尼尔的 grokking 工作</a>）</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{ReLU}[A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)] \approx \frac{1}{2}\left[A_1\cos(\omega_i a + \phi_1) + A_2\cos(\omega_i b + \phi_2)\right] + A_3\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)."><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.665em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;" class="mjx-line"></span></span><span style="height: 1.441em; vertical-align: -0.47em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">[</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">余弦</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">]</span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.372em;">。</span></span></span></span></span></span></span></p><p>因此，功率从单轴项向外投射到跨轴项。</p><p>如果我从神经元激活拟合<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\text{ReLU}(\hat{n}_i)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">ReLU</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-texatom"><span class="mjx-mrow"><span class="mjx-munderover"><span class="mjx-stack"><span class="mjx-over" style="height: 0.213em; padding-bottom: 0.06em; padding-left: 0.05em;"><span class="mjx-mo" style="vertical-align: top;"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.298em;">^</span></span></span> <span class="mjx-op"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">n</span></span></span></span></span></span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>中减去线性项，然后拟合上面的横轴振荡表达式，然后添加线性贡献，我可以为神经元激活创建一个猜测。但是，如果我用这些猜测替换神经元激活，那么性能真的很糟糕！损失从 0.0181 增加到 2.496，准确率从 99.94% 下降到 19.79%。</p><p>相反，如果我修改先前的猜测以提高交叉频率项的幅度，那么性能会好得多！更具体地说，我采用与上述相同的过程，但在拟合后我将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span>提高了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="2\pi"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;">π</span></span></span></span></span></span></span>倍。用这种增强的近似替换神经元可将损失降低至 0.091，准确度提高至 98.79%。并不完美，但看起来神经元确实在很大程度上使用 ReLU 线性项和这些横轴项的组合来计算解决方案。</p><p>我对我在这里的解释并不完全满意，但我也想在今晚从圣诞节到新年休假之前结束对这个玩具问题的研究，所以让我们继续吧！</p><h1>神经元到logit操作</h1><p>如果神经元激活<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="N"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.085em;">N</span></span></span></span></span></span></span>已知（形状为 [batch_a, batch_b, d_mlp] 的矩阵），则可以通过乘以矩阵<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit} = W_{\rm out} W_{\rm U}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">u</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span> <span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">U</span></span></span></span></span></span></span></span></span></span></span>来恢复 logits，其形状为 [ d_mlp，d_vocab]。</p><p>如果我对标记 0-99 沿词汇方向进行傅立叶变换<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">然后</span></span></span></span></span></span></span></span></span></span></span>取所有这些标记的平均功率谱，我会在相同的特征频率处看到相同的三个强峰值（0.33、0.39、0.48） ，我还看到一个新峰值出现在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="f = 0.01"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;">f</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.01</span></span></span></span></span></span></span>处。该峰值与激活中的线性特征相关，其他三个峰值与上面检查的振荡特征相关。请参阅下面的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span></span></span></span></span></span>中神经元激活和 [d_vocab] 向量的一些示例： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/a7ehwv9pt5bijwvhv9jh" alt="该图像的 alt 属性为空；它的文件名是sample_neurons_w_logit-2.png"></p><p>有许多神经元，如左侧的神经元 10。当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>都很大时，该神经元特别会抑制低值 logits 并提高高值 logits（这可以从顶部和中间行图的组合中读出）。第二行中的神经元 17 提高了小数字标记和大数字标记的 logit 值（后者似乎是模型学习内容的错误？），但当<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>大小相似并且都很小。在这两种情况<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">下</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">W</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">t</span></span></span></span></span></span></span></span></span></span></span>向量中的主频率为 0.01，因为它包含一个平滑的正弦特征，该特征会限制某些标记并增强其他标记。</p><p>右边的两个情节展示了不同的故事。神经元 75 和 125 是振荡的，它们以振荡的方式影响 logits。这些神经元的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">log</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">t</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">向量</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">的</span></span></span></span></span></span></span></span></span></span></span>振荡具有与神经元本身相同的特征频率。底部面板图显示功率谱，对于右侧两列（神经元 75 和 125），我绘制了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="W_{\rm logit}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.104em;"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.104em;">W</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">l</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">o</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.519em;">g</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">i</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.372em;">t</span></span></span></span></span></span></span></span></span></span></span>向量的功率谱，并表示主导神经元频率（在前面的计算中）部分）在垂直线上 - 并且神经元和映射矢量频率之间确实有很好的一致性！</p><p>所以如果我在上一节中是正确的并且振荡神经元有像<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span></span></span></span></span></span>这样的术语<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> ，则这些向量按以下方式映射：</p><p> <span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}(a,b) = A\cos(\omega_i a + \phi_1)\cos(\omega_i b + \phi_2)\cos(\omega_i c + \phi_3)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span></p><p>对于某个振幅<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="A"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span></span></span></span></span>和相位<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> ，其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span></span></span></span></span>是来自神经元<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="i"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span></span>的标记<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>对 logit 的贡献。简化形式是</p><p><span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_{c,i}(a,b) = \frac{A}{4}\left(\cos(a' - b' - c') + \cos(a' - b' + c') + \cos(a' + b' - c') + \cos(a' + b' + c')\right),"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-mfrac MJXc-space3"><span class="mjx-box MJXc-stacked" style="width: 0.672em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.95em; top: -1.422em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.298em;">A</span></span></span> <span class="mjx-denominator" style="font-size: 70.7%; width: 0.95em; bottom: -0.676em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">4</span></span></span><span style="border-bottom: 1.3px solid; top: -0.296em; width: 0.672em;" class="mjx-line"></span></span><span style="height: 1.484em; vertical-align: -0.478em;" class="mjx-vsize"></span></span> <span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">余弦</span></span><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span><span class="mjx-mo"><span class="mjx-char"></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span> <span class="mjx-mo MJXc-space1"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span></span></span></span></span></span></p><p>其中<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a' = \omega_i a + \phi_1, b' = \omega_i b + \phi_2, c' = \omega_i c + \phi_3"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">，</span></span> <span class="mjx-msup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">′</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-msubsup MJXc-space3"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">ω</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msubsup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">φ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span></span></span></span></span> 。因此，有一些三角项在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a, b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>空间中振荡，并且<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="c"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span></span></span></span>的值稍微改变这些项的相位，以将 logits 调整到它们需要的值。</p><p>最后，logits 本身是由神经元<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\ell_c = \sum_i \ell_{c,i}"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span></span> <span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em;">=</span></span> <span class="mjx-munderover MJXc-space3"><span class="mjx-base"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size1-R" style="padding-top: 0.519em; padding-bottom: 0.519em;">Σ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.439em; padding-right: 0.071em;"><span class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span> <span class="mjx-msubsup MJXc-space1"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.372em;">ℓ</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">i</span></span></span></span></span></span>的</span></span></span></span></span>总和构建的，其中包含神经元 10 和 17 等线性项，然后是由线性项。</p><h1>洛吉特人</h1><p>最后，我们得到如下所示的 logit（此处绘制的是 0、25、50、75 的 logit 映射）： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RABp7ZMw2FGwh4odq/oyl7fx3fyuu9pnzyjql9" alt="该图像的 alt 属性为空；它的文件名是 logit_values.png"></p><p>我突然想到的一件事是，这些模式不再在二维上振荡——它们似乎仅在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a - b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>的值方面振荡。这表明模型设置了<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_1"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">1</span></span></span></span></span></span></span></span></span>和<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\phi_2"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.519em;">phi</span></span></span> <span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span></span></span></span></span></span></span>的值，以便它可以用<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(a' + b' \pm c')"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.151em; padding-bottom: 0.372em;">cos</span></span></span></span></span></span></span>剔除振荡项<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\cos(a' + b' \pm c')"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char"></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-msup"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">+</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">±</span></span> <span class="mjx-msup MJXc-space2"><span class="mjx-base"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">c</span></span></span> <span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.298em;">&#39;</span></span></span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span> （我没时间测试这个）。</p><p>这也具有直观意义：沿着我上面绘制的<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a, b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em;">,</span></span> <span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>平面中的对角线，减法的结果始终相同！如果将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="a"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span></span></span></span></span></span>加 1，同时将<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="b"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span></span></span></span></span></span>加 1，则两者之间的差保持不变。因此，模型学习一个在<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="(a-b)"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">(</span></span> <span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em;">a</span></span> <span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.298em; padding-bottom: 0.446em;">−</span></span> <span class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.446em; padding-bottom: 0.298em;">b</span></span> <span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em;">)</span></span></span></span></span></span></span>方向上变化和振荡的解，但在垂直于该方向的方向上完全恒定，这是有道理的。整洁的！</p><h1>包起来</h1><p>我有大约 85% 的信心认为我已经弄清楚了该模型用于进行减法的算法。有几个地方（特别是在最后，在激活函数之后）事情变得有点仓促和手动，如果我有无限的时间花在这个模型上，那么我会巩固和完善那里的一些概念。但我不！</p><p>我将结束这个模型，并在下一篇文章中转向其他可能更有趣的事情，但如果有人对我的分析中的漏洞或对模型可能正在做的事情的想法有任何想法，我会这样做。我不是在检查，我很乐意讨论！</p><h2>代码</h2><p>我用来训练模型的代码可以在<a href="https://colab.research.google.com/drive/1Yy59roJgegsNguWLzt6PbxeEoKBVdhGW?usp=sharing">这个 colab 笔记本</a>中找到，并且用于本次调查的笔记本可以<a href="https://github.com/evanhanders/2digit_subtraction_transformer/blob/main/Interpretability_1layer_2digit_subtraction_digit_token.ipynb">在 Github 上找到</a>。</p><h2>致谢</h2><p>再次感谢 Adam Jermyn 的指导和建议，并感谢 Philip Quirke 和 Eoin Farrell 参加我们的定期会议。还要感谢 Alex Atanasov 和 Xianjun Yang 本周抽出时间与我会面并讨论 ML/AI！</p><br/><br/> <a href="https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RABp7ZMw2FGwh4odq/how-does-a-toy-2-digit-subtraction-transformer-predict-the-1<guid ispermalink="false"> RABp7ZMw2FGwh4odq</guid><dc:creator><![CDATA[Evan Anders]]></dc:creator><pubDate> Fri, 22 Dec 2023 21:17:30 GMT</pubDate></item></channel></rss>