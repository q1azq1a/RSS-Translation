<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 9 月 1 日星期五 06:15:24 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Progress links digest, 2023-09-01: How ancient people manipulated water, and more]]></title><description><![CDATA[Published on September 1, 2023 4:33 AM GMT<br/><br/><p>我正在尝试将更多社交媒体内容直接拉入这些摘要中，部分原因是为了减少对社交媒体网站的长期依赖（因为内容可能会被删除、屏蔽、付费等）。这使得这些摘要更长，但这意味着存在更少需要点击链接。</p><p>我仍然会链接回原始的社交媒体帖子，以便给予信任并使分享更容易。一如既往，让我知道您的反馈。</p><h2><strong>机会</strong></h2><ul><li><a href="https://www.readcodon.com/p/homeworld-ideas">Homeworld Ideas，生物技术写作比赛</a>。 “分享您对生物学如何实现积极且可持续的未来的愿景”（来自<a href="https://twitter.com/NikoMcCarty/status/1691440917486690304">@NikoMcCarty</a> ）</li><li> Works in Progress + Stripe Press 于 9 月 14 日举办的作家沙龙， <a href="https://docs.google.com/forms/d/e/1FAIpQLSc-rcl7ztNszOapMHYDf3Oys0pb6qdao43INcbJfEflkIm23A/viewform">请在此处申请</a>（通过<a href="https://twitter.com/_TamaraWinter/status/1694372353847791802">@_TamaraWinter</a> ）</li><li> <a href="https://foresight.org/ai-safety">Foresight 的人工智能安全资助计划</a>（来自<a href="https://twitter.com/foresightinst/status/1692257505886286259">@foresightinst</a> ）</li><li> <a href="https://oklo.com/careers/">Oklo（先进核能）正在招聘</a>。 “我们特别寻找杰出的机械工程师......以及电力工程师” <a href="https://twitter.com/caorilne/status/1694822419590992358">@caorilne</a>说</li><li>如果您有科技/软件/创业经验，并且想帮助技术移民，您可以<a href="https://twitter.com/lisawehden/status/1691877018936250734">成为咨询意见书签署人</a></li></ul><h2><strong>公告</strong></h2><ul><li><a href="https://betterplanning.ie/street-plans/">更好规划联盟关于“街道规划如何帮助减少住房危机”的报告</a>（来自<a href="https://twitter.com/BPAIreland/status/1694740105880027283">@BPAIreland</a> ）</li></ul><h2><strong>安息吧</strong></h2><ul><li><a href="https://www.bloomberg.com/news/articles/2023-08-20/adobe-s-co-founder-john-warnock-dies-at-82">Adobe 联合创始人约翰·沃诺克，82 岁</a>。 Steven Sinofsky<a href="https://twitter.com/stevesi/status/1693324009956634904">称他为</a>“行业和技术传奇人物……从 Xerox 到共同创立 Adob​​e 数十年，他改变了我们每天都受益的技术格局。”约翰·格鲁伯 (John Gruber)<a href="https://daringfireball.net/linked/2023/08/26/john-warnock-rip">表示</a>：“沃诺克和格施克理解史蒂夫·乔布斯经常宣扬的一句话：仅靠技术是不够的。 ……如果 Warnock 和 Geschke 仅仅满足于单独提供伟大的技术，那么 Adob​​e Systems 将成为几乎被遗忘的硅谷脚注。相反，他们推动 Adob​​e 成为我们今天所知的伟大的工具制造产品公司。”</li></ul><h2><strong>视频</strong></h2><p>帕特里克·科里森 (Patrick Collison) 和兰特·普里切特 (Lant Pritchett) 讨论了一个国家的进展以及促进其增长的情况（来自<a href="https://twitter.com/jmazda/status/1691715431302619395">@jmazda</a> ） </p><figure class="media"><div data-oembed-url="https://youtu.be/FlIwx_7Rimc"><div><iframe src="https://www.youtube.com/embed/FlIwx_7Rimc" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></div></div></figure><h2><strong>文章</strong></h2><ul><li><a href="https://alltrades.substack.com/p/proto-pumps-and-early-water-infrastructure">古人是如何操纵水的？</a> （作者： <a href="https://twitter.com/ConnorTabarrok/status/1692238953108197481">@ConnorTabarrok</a> ）</li><li><a href="https://www.arcadiascience.com/blog/what-is-arcadia">什么是阿卡迪亚科学？</a> “我们两年愿景的最新进展”（来自<a href="https://twitter.com/seemaychou/status/1691532663377838080">@seemaychou</a> ）</li><li>我们如何才能加速科学发展？ <a href="https://www.washingtonpost.com/opinions/2023/08/14/heidi-williams-science-research-funding/">海蒂·威廉姆斯 (Heidi Williams) 在华盛顿邮报的一篇评论文章中建议“消除资金延误”</a> 。 “国会可以给予 NIH 和 NSF 灵活性，以便更快地资助好想法……资助者可以给予科学家更大的灵活性来追求他们最有前途的想法”（ <a href="https://twitter.com/heidilwilliams_/status/1691052671329853440">@heidilwilliams_</a> ）</li><li> “林迪效应是一种统计规律，对于许多类型的实体来说：它们存在的时间越长，它们可能持续的时间就越长。”<a href="https://arxiv.org/pdf/2308.09045.pdf">新的托比·奥德论文</a>给出了这个正式的发展（来自<a href="https://rootsofprogress.org/(https://twitter.com/tobyordoxford/status/1694284685134733337)">@tobyordoxford</a> ）</li><li>耶路撒冷德姆萨斯说<a href="https://www.theatlantic.com/ideas/archive/2023/08/american-election-frequency-voter-turnout/675054/">美国人投票太多</a>。 “选民消失的地方，特殊利益就会涌入……我们得到的不是民主，而是由业主协会、警察工会、教师工会、开发商、商会、环保组织等组成的政府”</li><li> <a href="https://marginalrevolution.com/marginalrevolution/2023/08/twenty-years-of-marginal-revolution.html">二十年的边际革命</a>（来自<a href="https://twitter.com/ModeledBehavior/status/1694325077989368029">@ModeledBehavior</a> ）</li><li><a href="https://en.wikipedia.org/wiki/Du_Shi">杜轼</a>是“中国水文学家、发明家、机械工程师、冶金学家和政治家”，被誉为第一个在冶金中应用水车操作波纹管的人（来自<a href="https://twitter.com/michaelcurzi/status/1694506186475745461">@michaelcurzi</a> ）</li><li><a href="https://en.wikipedia.org/wiki/Great_Raft">大木筏</a>是“一个巨大的木塞，大约从 12 世纪起堵塞了路易斯安那州的红河和阿查法拉亚河，直到 1830 年代被清除。它的规模在北美是独一无二的”（ <a href="https://twitter.com/WillRinehart/status/1693817393863098828">@WillRinehart</a> ）</li></ul><p> <a href="https://pbs.twimg.com/media/F4Gl8OgWQAAGXiC.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/n11gygolj5a28m3qnhro" alt=""></a></p><h2><strong>查询</strong></h2><ul><li><a href="https://twitter.com/michael_nielsen/status/1692204714090549510">70 岁或以上的人取得科学突破的最佳例子是什么？</a></li><li><a href="https://twitter.com/Ben_Reinhardt/status/1693653330306753010">您最喜欢的工业和政府实验室之外的成功非学术研究组织的例子是什么？</a></li><li><a href="https://twitter.com/danielgolliher/status/1692358353601634780">谁在中西部从事富足/美国活力/等方面的工作？</a></li><li><a href="https://twitter.com/YIMBYLAND/status/1691552000888996146">规划一条子弹头列车真的需要 2 亿美元吗？</a></li><li><a href="https://twitter.com/yishan/status/1691686483655323902">乔治·霍茨/埃利泽·尤德科夫斯基辩论的总结很好吗？</a></li><li><a href="https://twitter.com/jasoncrawford/status/1691081077274574848">既然每个人都在使用 Midjourney 和 DALL-E 来创建博客，图片库网站的流量是否会下降？</a></li><li><a href="https://twitter.com/nealagarwal/status/1691095252952834048">尼尔·阿加瓦尔（Neal Agarwal）的“关于早期互联网的小博物馆”应该包括什么？</a></li><li><a href="https://twitter.com/stewartbrand/status/1693441524242268667">这台机床到底是什么？</a></li></ul><p> <a href="https://pbs.twimg.com/media/F4BQA1kbIAA6pyn.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/mmo2nkjnjrcehcat9mkm" alt=""></a></p><h2><strong>来自社交媒体</strong></h2><ul><li>挪威可以以比英国<i>申请规划</i>所需的成本更低的成本<i>建造一条隧道</i>，<a href="https://twitter.com/Sam_Dumitriu/status/1694990887439233239">以及 @Sam_Dumitriu 提供的许多其他令人震惊的事实</a></li><li>据说诺贝尔奖获得者获奖后的产出会<i>减少</i>。对于那些受邀进入高等研究院的人也说了类似的话。<a href="https://twitter.com/jasoncrawford/status/1691132650117910545">哪些奖项、荣誉、补助金、职位等会导致人们在之后做出<i>更多</i>或更好的工作，而不是更少？</a></li><li>费曼问道：“如果在某种灾难中，所有的科学知识都被毁灭，只有一句话传给了下一代生物，那么哪句话会用最少的词语包含最多的信息呢？”他自己的答案是“<i>一切事物都是由原子构成的</i>原子假说”。但我们在古希腊就有这样的假设！直到1800年代它才基本上没用。<a href="https://twitter.com/jasoncrawford/status/1692233042641736133">为什么知识不包含在句子中</a></li><li><a href="https://twitter.com/timhwang/status/1692625535866724690">“秋季在纽约举行的为期一天的研讨会，重点关注“科技树”主题，将游戏设计师、科学史学家和元科学人士聚集在一起”</a></li><li>台湾核倡导者： <a href="https://twitter.com/AngelicaOung/status/1692534228020555809">“我们正在成立一个支持核的非政府组织”</a></li><li>本·莱因哈特 (Ben Reinhardt)<a href="https://twitter.com/Ben_Reinhardt/status/1691085316164104193">建议</a>“新兴风险投资，但适合从事奇怪科学/技术工作的人。我经常遇到一些人（尤其是学术系统之外的人），他们有一个疯狂的想法，他们想从事这一工作 1. 无法通过某些委员会 2. 需要比官僚时间表更快的资金（因为他们需要招聘/围绕它做出的生活决定）”</li><li> “运行自动化项目的其他人是否注意到关键供应商已被完全预订一空，工作量超负荷，交货时间变得疯狂？ ……有大事正在发生。世界正在快速自动化！” （ <a href="https://twitter.com/Jordan_W_Taylor/status/1692251280394076175">@Jordan_W_Taylor</a> ）也许与<a href="https://twitter.com/DKThomp/status/1681657342591393793">美国制造业建设处于历史最高水平</a>有关？</li><li> “数千年来，城市规划的重点是绘制街道网络，其下方有供水和下水道，并保留常规公共空间，并且在用途或密度方面大多不可知。<a href="https://twitter.com/mnolangray/status/1693726831722234027">一百年前，情况发生了逆转</a>”</li><li> “我们需要一种关于物质进步的独特印度话语......从独特的印度视角进行对话，借鉴我们的生活经验和我们在人均 GDP 水平上面临的权衡”（ <a href="https://rootsofprogress.org/(https://twitter.com/aye_kaash/status/1695808604220313884)">@aye_kaash</a> ）</li><li> “我曾经认为种植一小块菜地是与自然交流、与世界和平相处的一种方式。在经历了蛞蝓、蚜虫、霉菌等之后，我意识到我搞错了。园艺是达尔文式的与自然的死亡竞赛以及对稀缺资源的争夺”（ <a href="https://twitter.com/eladgil/status/1692564470441165227">@eladgil</a> ）</li><li> “坦率地说，我有一个带有通用翻译器的袖珍设备，这一点是未被充分认识的魔力”（ <a href="https://twitter.com/kendrictonn/status/1692004311457518032">@kendrictonn</a> ）</li><li> “当自动驾驶汽车被理所当然地视为城市基础设施的一部分时（这可能只需要几年时间），那些试图禁止它们的人的声明读起来就像有人试图禁止电力或室内管道”（ <a href="https://twitter.com/paulg/status/1691261977656786944">@paulg</a> ）</li><li> <a href="https://twitter.com/jonatanpallesen/status/1693622595793334512">“安慰剂效应（大部分）不是真实的，”</a>只是回归均值</li><li>如果您查看打字速度与准确性，您会发现它们呈正相关：打字速度更快的人也更准确。天真地，人们可能会建议加快打字速度以减少错误！显然，这会适得其反。<a href="https://twitter.com/jasoncrawford/status/1692015685717651618">为什么受试者间变异性与受试者内变异性不同</a></li><li>《大西洋月刊》<a href="https://www.threads.net/@theatlantic/post/CwTUZxeRO1_">抱怨说</a>，生成式人工智能会消耗大量能源，而且“对气候造成的损失可能是巨大的”。<a href="https://www.threads.net/@jasoncrawford/post/CwT3E8_PTZF">更正标题</a>：生成人工智能展示了为什么我们需要过剩的能源。廉价、可靠、清洁的能源并不缺乏用途</li><li>“25 年前，他们发明了<a href="https://en.wikipedia.org/wiki/Celecoxib">一种更好的 Advil 版本</a>，它针对相同的受体，但副作用更少。它仍然需要处方，基本上没有任何理由”（ <a href="https://twitter.com/alyssamvance/status/1692399530711437321">@alyssamvance</a> ）</li><li> “如果你将气候变化视为对有罪的暴饮暴食的惩罚而不是技术问题，那么环保主义者对碳清除的禁忌是有道理的。解决罪恶的唯一合适的办法就是自我牺牲”（ <a href="https://twitter.com/MTabarrok/status/1692176024853962891">@MTabarrok</a> ）</li><li>约翰·卡马克<a href="https://twitter.com/ID_AA_Carmack/status/1692229954954461514">表示</a>：“《连线》仍然是唯一一家在我出现在文章中时始终有事实核查人员联系我的主要出版物。” （不确定他是否曾经<a href="https://twitter.com/EnswellJones/status/1301664132438032384">去过大西洋</a>）</li><li> “为人父母非常重要。事实上，很难提出定量证据来证明养育子女的重要性，这是我们社会科学的新生和幸福感概念混乱的预期结果”（ <a href="https://twitter.com/mbateman/status/1691129949338456089">@mbateman</a> ）</li></ul><h2><strong>引号</strong></h2><p><a href="https://twitter.com/jasoncrawford/status/1690906769365540864">米塞斯反对稳定</a>（ <a href="https://rootsofprogress.org/books/masters-of-the-universe"><i>《宇宙大师》</i></a> ，作者：丹尼尔·斯特德曼·琼斯）：</p><blockquote><p>和波普尔一样，米塞斯也看到了官僚心态与柏拉图的乌托邦之间的相似之处，在柏拉图的乌托邦中，绝大多数被统治者为统治者服务。他认为“后来所有按照柏拉图的例子塑造人间天堂蓝图的乌托邦主义者都同样相信人类事务的不变性”。他接着说，官僚化必然是僵化的，因为它涉及对既定规则和惯例的遵守。但在社会生活中，僵化就等于石化和死亡。一个非常重要的事实是，稳定和安全是当今“改革者”最珍视的口号。<strong>如果原始人采取了稳定的原则，他们早就被猛兽和微生物消灭了。</strong></p></blockquote><p> “对三年前封锁期间发表的《纽约时报》文章的评论，质疑纽约市是否会恢复”（来自<a href="https://twitter.com/michaelmiraflor/status/1695181415850332378">@michaelmiraflor</a> ）：</p><blockquote><p>我当纽约出租车司机已经很多很多年了。我最喜欢的骑行方式是罕见地搭载一个刚从医院出来的男人，他的第一个孩子出生了。这是他一生中最美好的一天，当他告诉我这一切时，我通常很难掩饰自己喜悦的泪水。</p><p>我第二喜欢的骑行也是类似的。这是一个第一次来到纽约市的有梦想的年轻人。我是出租车司机，带他或她从机场前往曼哈顿。我坚持将 59 街桥的上层作为我们的路线。当我们接近曼哈顿时，随着这座城市变得越来越大，人们的兴奋感也随之增强。最后，几乎在地面上，坡道使我们如此接近周围的建筑物，以至于我们实际上可以看到里面的人。降落在东 62 街时，我的新纽约客第一次体验到人们常说的“活力”。这就像看着一个孩子走近一屋子的生日礼物。一切皆有可能。</p></blockquote><h2><strong>地图和图表</strong></h2><p>如果我们能够以北欧成本建造伦敦的铁路系统（来自<a href="https://pedestrianobservations.com/2019/06/24/assume-nordic-costs-london-edition/">Alon Levy</a> ， <a href="https://twitter.com/Sam_Dumitriu/status/1694990909346120164">@Sam_Dumitriu</a> ）</p><p> <a href="https://pbs.twimg.com/media/F4XH2SBXUAAHYNK.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/rbyx734i2dd6wrfukslv" alt=""></a></p><p> “人类最鼓舞人心的成就之一。我们以前做到过，我们可以再做一次”（ <a href="https://twitter.com/Altimor/status/1692573695003312553">@Altimor</a> ）</p><p> <a href="https://pbs.twimg.com/media/F306ye9XwA4wkIA?format=webp"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/qpf3iqrbmpmf3v28typf" alt=""></a></p><p> “物质繁荣最基本的资源是能源”（ <a href="https://twitter.com/Andercot/status/1694061035223937285">@Andercot</a> ）</p><p> <a href="https://pbs.twimg.com/media/F4KC6uMaoAIVN_v.jpg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/kviyfwulyk0jv8cqxfb8" alt=""></a></p><p>核支持民意调查（来自<a href="https://twitter.com/gordonmcdowell/status/1693313725414203544">@gordonmcdowell</a> ） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/hw8z8e486bu7zjdrwml9"></figure><p>整个宇宙，按（对数）比例（来自<a href="https://twitter.com/emollick/status/1693496535508566116">@emollick</a> ，作者：Pablo Carlos Budassi，<a href="https://www.pablocarlosbudassi.com/2021/02/atlas-of-universe-is-linear-version-of_15.html">完整分辨率请参阅原始内容</a>） </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WhMk3gwyfGq2xek2J/iilopjapvrszyvaycsfn"></figure><br/><br/> <a href="https://www.lesswrong.com/posts/WhMk3gwyfGq2xek2J/progress-links-digest-2023-09-01-how-ancient-people#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/WhMk3gwyfGq2xek2J/progress-links-digest-2023-09-01-how-ancient-people<guid ispermalink="false"> WhMk3gwyfGq2xek2J</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Fri, 01 Sep 2023 04:33:32 GMT</pubDate> </item><item><title><![CDATA[A Golden Age of Building? Excerpts and lessons from Empire State, Pentagon, Skunk Works and SpaceX]]></title><description><![CDATA[Published on September 1, 2023 4:03 AM GMT<br/><br/><p>帕特里克·科利森 (Patrick Collison) 列出了一系列自 19 世纪以来<a href="https://patrickcollison.com/fast">人们迅速共同完成雄心勃勃的事情</a>的精彩例子。它确实让你渴望一个感觉……不同的时代，当政府部门的昏昏欲睡的庞然大物可以以赛车初创公司的速度移动时：</p><blockquote><p> [...] 上个世纪，[国防部] 的创新速度令现代硅谷初创企业相形见绌：五角大楼仅用了 16 个月就建成了（1941 年至 1943 年），曼哈顿计划只运行了 3 年多（1942-1946），阿波罗计划在不到十年的时间内将人类送上月球（1961-1969）。仅在20世纪50年代，美国就建造了五代战斗机、三代载人轰炸机、两级航空母舰、潜射弹道导弹和核动力攻击潜艇。</p></blockquote><p> [注：该段落来自<a href="https://blog.anduril.com/rebooting-the-arsenal-of-democracy-anduril-mission-document-67fdbf442799">另一篇文章</a>。]</p><p>部分受到帕特里克清单的启发，我花了一些假期阅读和了解这个失落时代的各种项目。然后我写了一份备忘录，与 Lightcone 的同事分享要点和摘录。</p><p>之后，一些人鼓励我更广泛地分享这份备忘录——我确实认为，任何怀有伟大抱负和对有效运作好奇的人都会对它感兴趣。</p><p><i>如何</i>在短短一年内建造世界第一高楼<i>？</i>在同样的时间内建造世界上<i>最大的</i>建筑？或者美国在短短 6 个月内推出第一架战斗机？</p><p><i>如何？？</i></p><p>写这篇文章感觉它至少帮助我解开了这个谜题的一些部分。如果有人有其他作品，我很乐意在评论中听到他们的声音。</p><h3>帝国大厦</h3><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/atkwzbarxthhqeqls3cv"></figure><p>帝国大厦于 1931 年 4 月竣工，是世界上最高的建筑。在假期期间，我读了<a href="https://www.amazon.com/Building-Empire-State-Carol-Willis/dp/0393732312/ref=sr_1_7?crid=3FMZRO639NAH9&amp;keywords=empire+state&amp;qid=1693537379&amp;s=books&amp;sprefix=empire+sta%2Cstripbooks%2C254&amp;sr=1-7">一本重新发现的 20 世纪 30 年代笔记本</a>，由总承包商自己写的。它详细介绍了项目的施工过程和组织。</p><p>我将分享一些摘录，但为了将它们结合起来，首先考虑最近建造的其他一些摩天大楼： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/vsqgdbvmunfq0doeibta"><figcaption>作为上下文，帝国大厦顶部的高度为 1,454 英尺，大约位于该图的底部。 </figcaption></figure><figure class="table"><table><tbody><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px"></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px"><strong>设计开始</strong></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px"><strong>施工结束</strong></td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px"><strong>总时间</strong></td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">哈利法塔</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">2004年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2010年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">6年</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">上海中心大厦</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">2008年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2015年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">7年</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">阿布拉吉·巴尔特</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">2002年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2012年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">10年</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">世界贸易中心</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">2005年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2014年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">9年</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">诺德斯特龙大厦</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">2010年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2020年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">10年</td></tr><tr><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:200px">台北101</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px"> 1997年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:125px">2004年</td><td style="border:1pt solid #000000;padding:5pt;vertical-align:top;width:100px">7年</td></tr></tbody></table></figure><p>（列表来自 skyscrapercenter.com）</p><p>现在，摘自《帝国大厦》这本书的前言：</p><blockquote><p>帝国大厦最惊人的统计数据是其规划和建设的惊人速度。 [...] 有不同的方式来描述这一壮举。 1930 年 4 月 7 日第一根结构柱安装完毕六个月后，八十六层的钢框架封顶。这座全封闭的建筑，包括其高度相当于 102 层楼的系泊桅杆，于 1931 年 3 月在 11 个月内完工。最令人惊奇的是，在短短 20 个月内——从第一个与1929 年 9 月的建筑师们到 1931 年 5 月 1 日的开幕仪式——帝国大厦设计、建造、建造并准备好供租户使用。在此期间，建筑图纸和计划已准备就绪，华尔道夫酒店的维多利亚式建筑桩被拆除（拆除工作在最初协议签署两天后才开始），地基和格栅已挖好并固定，钢柱和按照精确的规格制造和铣削了约 57,000 吨的梁，铺设了 1000 万块普通砖，浇筑了超过 62,000 立方码的混凝土，设置了 6,400 个窗户，并在 7 英里长的竖井中安装了 67 部电梯。在活动高峰期，现场雇佣了 3,500 名工人，框架每天上升超过一层楼，此后没有任何类似的结构能与这样的上升速度相匹配。</p></blockquote><p>他们是如何做到这一点的？以下是他们的流程中令我印象深刻的一些事情。</p><p><strong>设计是由来自该领域所有关键部分的代表组成的小组进行的</strong></p><p>该团队面临一个棘手的高维问题，需要大量不同的知识才能解决。以下是 1930 年<i>《财富》</i>杂志的引述：</p><blockquote><p>这些不同的元素固定了一个形状奇特的几何实体的周长，其一侧以 83,360 平方英尺的土地为界，另一侧以 35,000,000 美元为界，另一侧以收益递减为界，另一侧以物理定律和结构钢的特性为界一次是由于分区条例的圆锥形紧急情况，还有一次是在 1931 年 5 月 1 日。</p></blockquote><p>回应：</p><blockquote><p>从一开始，业主、建筑师和建筑商就组成委员会共同制定建筑物的[要求]。这种方法避免了设计中的错误和代价高昂的施工延误[...]总承包商写道：“我怀疑是否存在比业主、建筑师和建筑商之间存在的更和谐的组合。我们不断与其他人协商；建筑物的所有细节都经过提前检查，并在纳入计划之前决定”。<br><br> 1929 年 9 月，这些初步规划会议历时四个星期，提出了该项目的完整技术、规划和经济要求。 （第 20 页）</p></blockquote><p>例如，机械工程师迈耶、斯特朗和琼斯指出，与之前已知的任何项目相比，建造电梯将涉及更大的安装、更大的轿厢尺寸、更重的负载、更高的速度和更长的行程。但仍然：“建筑、钢材和电梯计划的正确同步开发避免了试图将电梯设备安装到先前固定的建筑布置和钢材布局中的常见错误。”</p><p>该团队努力吸引对表面约束和潜在障碍具有详细的对象级知识的人员进行深入的研究——设计师甚至聘请了金属制造商来帮助咨询图纸。</p><blockquote><p> “由于建筑师、建造商和分包商认为自己没有能力制定规范，因此他们召集了制造它的金属工人、安装它的人员以及在几个地方测试板材的检查员的代表。准备阶段。 [建筑师]施里夫指出，这样的会议“基于对建议的<i>即时</i>比较和所有相关人员职责的确定，做出了可能的决定”。 [我的斜体]（第 24 页）</p></blockquote><p><strong>建筑物的设计经过刻意优化，可快速建造</strong></p><blockquote><p>[...] 帝国大厦设计和建造中的几乎每一个决定都受到速度需求的影响。 [...] 正如总承包商保罗·斯塔雷特 (Paul Starret) 在回顾他的职业生涯时所声称的那样，“在建筑史上从未出现过，而且可能再也不会出现如此出色地适应施工速度的建筑设计。” （第 18 页）</p><p>承包商笔记本的最后一个主要部分的副标题是“速度的魅力”，重点介绍了建筑的四个起搏器：钢结构安装、混凝土地板拱形结构、外部金属装饰和铝拱肩以及外部石灰石。施泰力兄弟和 Eken 对他们在这四个领域所实现的创新感到非常自豪。 （第 24 页）</p></blockquote><p>对于建筑物的窗户，“目标是尽可能标准化元件，创建一种可以加快制造和安装速度的零件套件，在 5704 个金属拱肩中只有 18 个变体”。由于立面部件的其他部分如果可以用装饰覆盖的话，则显得粗糙且未完成，因此重新设计了安装顺序，以消除传统的复杂外部脚手架工作，而是可以从建筑物内部安装，并且部件被设计为具有更少的数量。与其他部件交叉，以使安装更顺利。</p><p><strong>他们在存储和移动材料方面进行了巧妙的优化</strong></p><p>该团队面临两个关键的物流问题：交付安排以及在垂直和水平方向上有效地移动材料。</p><p>由于他们是在纽约繁忙的市中心进行建设，因此他们的现场物资存储量非常少，但在高峰运营期间，他们每天从大约 500 辆卡车接收物资，或者在 8 小时工作日中大约每分钟收到一辆！</p><p> （有趣的是，已知最古老的建筑法之一是禁止白天运载建筑材料的手推车穿过罗马帝国的街道。）</p><p>他们总是会保持底层没有临时结构，以便卡车驶入，然后从建筑物顶部到一楼切出竖井，使他们能够将碎片直接倾倒到卡车后面，而不是倒入中间仓库他们甚至建造了一条迷你铁路，用于在建筑物内移动材料。摘自书中：</p><blockquote><p> “为了安装石雕，我们完全砍掉了惯用的井架。石头卡车开进大楼，石头装在板条箱里，我们称之为料斗或吊索。每个板条箱都被标记在建筑物的适当部分，由一台小型起重机从卡车上吊下来，通过天花板上的单轨运行，然后运送到工业铁路的平板车上。它被带到正确的楼层，在几乎与要设置的位置完全相同的位置卸载。两台起重机搬运了建筑物的所有石头，不仅消除了大量的起重吊杆和发动机，而且由于它们位于建筑物内部，消除了对公众的严重危险源。”</p></blockquote><p><strong>总承包商设计并优化了工作，以尽可能地分散不同工人的注意力</strong></p><blockquote><p>交易以不同的速度进行，有特殊的要求，并且可能以完全不同的方式看待相同的细节。通过尽可能多地取消交易之间的合同，建筑商降低了连锁延误的风险。作品的各个部分都展示了这一想法的实际应用：幕墙支撑详细消除了钢结构制造商和安装商对砖石信息的依赖；现场混凝土配料使搅拌机摆脱了曼哈顿的交通条件，物料提升机、乘客电梯和工业铁路系统的协调使用控制了现场的混乱运动。 （第 46 页）</p></blockquote><p><strong>设计与施工同时进行</strong></p><p>这是他们为跟踪钢铁加工而制作的图表：它绘制了并行发生的 5 个流程，以及相关的目标日期——建筑师的图纸、工厂订单、车间图纸、钢材交付和钢材安装。<br></p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/pyxxd5rh3z66tnxqwx4g"></strong></p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/lqvuojz8pkrgytn8derm"></strong></p><p>你可以看到，在顶层的图纸还没有完成之前，第一层的钢材就已经开始安装了。</p><blockquote><p>该图清楚地显示了快速跟踪成功所需的深思熟虑[JL：快速跟踪是用于交错而不是交错、设计和构造的术语]。每个参与者都有几项活动要做。在任何一天，不同等级的钢材都处于不同的准备阶段，需要多家顾问和建筑公司的关注。快速的速度要求每个人齐心协力。</p></blockquote><p><strong>他们采用缜密的计划和监督</strong></p><p>我们在上面的快速钢结构安装中看到了需要详细调度的示例。</p><p>每分钟接收一辆卡车送货的情况也是如此，总承包商埃肯说：“我们为这辆卡车运行的方式就像火车进出中央车站一样。如果周二卡车错过了排队的位置，则必须等到周三才能重新排队”。</p><p>该团队还设定了各种清晰的速度目标：他们决定尝试以每天一层的速度建造这座建筑。 （第 29 页）</p><p>此外，他们还有大量的看守人和检查员。他们会走过现场，记下每个人实际上都在现场（他们每天检查每个工人四次（！）），详细跟踪库存，以及每天完成的工作的记录。</p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/fwwhbgb40bbgq642jmhv"></strong></p><p>如果你想放大的话，这是他们的组织结构图。</p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/jyxkrh7c86coudwekdg3"></strong></p><p>以下是日常工作活动笔记的示例页面：</p><p> <strong><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/lia6p81zxpq7r3orwu3b"></strong></p><p>总体而言，帝国大厦的建设是一项了不起的成就。然而：</p><blockquote><p>这位总承包商自传的最后一章并不是以虚张声势的方式结束他的成就，而是以令人震惊的声明结束：“经过四十年的紧张工作，在十一个月内建造帝国大厦的压力对我来说太大了，我遭受了相当严重的神经衰弱”。</p></blockquote><h3>五角大楼</h3><p>我没有时间完全完成这一部分，但我关心五角大楼的例子，它提供了一些有趣的背景。</p><p>同一个人，格罗夫斯将军负责管理 20 世纪最大、最高效的两个大型项目：五角大楼建设和曼哈顿项目。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/afv1a6yplpueh1ipfpsd"><figcaption>正在建设中的五角大楼。</figcaption></figure><blockquote><p>施工于 9 月 11 日开始（正是 60 年后该建筑被飞机撞击的那一天）。萨默维尔推动了一个积极的时间表——在 1942 年 3 月 1 日之前准备好 50 万平方英尺的办公室，在 9 月 1 日之前准备好剩余的部分。格罗夫将军将军的设计师们已经尝到了紧张的时间表的滋味。几个月前，当五角大楼的初步计划正在构思时，萨默维尔给了以美国建筑师学会主席乔治·埃德温·伯格斯特罗姆为首的设计师们七月的一个周末，让他们为计划的内容草拟初步设计。建筑面积400万平方英尺。它将是——而且很可能仍然是——世界上最大的办公楼。</p><p>五角大楼的建筑和结构设计工作与施工同时进行，最初的图纸于 1941 年 10 月上旬提供，大部分设计工作于 1942 年 6 月 1 日完成。有时，施工工作比设计提前，使用的材料与设计不同。计划中指定的那些。</p><p>该建筑是逐个楔形地建造的；[38]每个楔形体一完工就被占用，即使其余楔形体的施工仍在继续。[39][40]&quot;</p><p> <a href="https://en.m.wikipedia.org/wiki/The_Pentagon"><u>https://en.m.wikipedia.org/wiki/The_Pentagon</u></a></p><p>施工期间，现场施工人员一度多达15,000人。沃格尔写道，设计团队包括“110 名建筑师、54 名结构工程师、43 名机械工程师、18 名电气工程师、13 名管道工程师、道路、景观和声学方面的各种专家，以及数十名职员和信使。”与此同时，位于施工现场的另一组 100 多名工程师、建筑师和检查员被授权做出临时决定——这座建筑确实是同时设计和建造的。</p><p> <a href="https://www.asce.org/publications-and-news/civil-engineering-source/civil-engineering-magazine/issues/magazine-issue/article/2021/09/building-for-the-american-age-the-pentagon">https://www.asce.org/publications-and-news/civil-engineering-source/civil-engineering-magazine/issues/magazine-issue/article/2021/09/building-for-the-american-age-五角大楼</a></p></blockquote><p>这项工作压力很大，格罗夫斯说他“希望能去战区，这样我就能找到一点平静。”</p><h3>臭鼬工厂</h3><p>臭鼬工厂是洛克希德公司的一个子公司，在 20 世纪 40 年代至 1970 年代主要制造间谍飞机。</p><p>例如，1943 年，臭鼬工厂在短短 5 个月内设计并制造了美国第一架战斗机 P80 流星。首席工程师 Kelly Johnson 与一支斗志旺盛的团队合作，在鼎盛时期，该团队由 23 名设计师和 105 名制造商组成。尽管如此，由此产生的飞机最终还是被空军使用了 40 年。</p><p>相比之下：波音公司使用了 10,000 多名工程师来设计 777。这是在计算机和 3D 建模软件的帮助下完成的。凯利用手制作图表，用计算尺进行计算。</p><p> Skunk Works 的文化就是让事情变得小而快：</p><blockquote><p>凯利喜欢讲述一位名叫弗兰克·卡罗尔 (Frank Carroll) 的将军在听到凯利描述他一直在推动的美国第一架喷气式飞机 P-80 的速度和机动性时如此热情，以至于卡罗尔决定绕开所有繁​​文缛节的延误，开始做这件事。所有采购订单文件都是他自己做的。 “我们下午两点吃完午餐回来。他有一份正式的意向书，让我开始 P-80 的工作，起草、批准、签署和密封，让我及时赶上 3 点 30 分的航班返回加利福尼亚。”凯利说，每次他告诉我这件事时，他都会高兴地笑起来。故事。 F104 星际战斗机也发生了同样的事情。时任战略空军司令部司令的布鲁斯·霍洛威 (Bruce Holloway) 将军是 20 世纪 50 年代采购部门的上校，他听取了凯利关于建造超音速喷气式飞机的建议。霍洛威需要获得一份符合凯利性能描述的空军要求清单，作为转发原型合同的第一步。 “天哪，凯利，我会自己写的，”他热情洋溢地宣称。凯利帮助他起草了这份文件，两人将其提交给一位名叫唐·耶茨的将军，并由他签署。总用时：两小时。”</p><p> （第 290 页）</p></blockquote><p>他们还建造了这艘非常酷的船： </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/u7ntyrejd8ohdil1x1zs"></p><p>正如凯利的门徒本·里奇 (Ben Rich) 所描述的：</p><blockquote><p> “我们的船有四名船员——船长、舵手、领航员和工程师。相比之下，一艘从事类似工作的护卫舰有三百多名船员。</p><p>从正面看，这艘船看起来就像达斯·维德的头盔。一些看到她的海军军官在看到这艘有史以来在海上航行的最具未来感的船只时厌恶地咬紧牙关。未来的指挥官对只有四名船员在一艘如此秘密的船上发号施令感到不满，因为这艘船是如此秘密，以至于海军甚至无法承认它的存在。我们的隐形舰也许能从天上炸掉一支规模可观的苏联攻击力量，但就军官未来的地位和晋升前景而言，那就和指挥拖船一样光鲜亮丽了。在最高层，海军高层对于保卫航母特遣部队所需的少量隐形舰艇同样不热心。就权力或声望而言，太少的人无法为任何人的职业生涯带来多大好处。”</p></blockquote><p>但他们的最高成就是一架具有传奇色彩的飞机，埃隆·马斯克甚至以它的名字为他的孩子命名...... </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/lrdqi2uru0ppwenmfdwm"></p><p> （这架飞机被称为 A-12 或 SR-71，它们是基本同一型号的两种不同迭代。） </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BpTDJj6TrqGYTjFcZ/vb9skrnacw0wxnyumqch"></p><p>为什么这么棒？</p><p>尽管这是一架用于飞越敌方领土的军用飞机，但它没有任何枪支、照明弹或其他防御装置。因为它的速度太快了，如果有人向它开枪，标准的防御策略就是加速并<i>飞得更快</i>。</p><p>它的巡航速度简直比高速飞行的子弹还要快。它在 1.5 秒内飞了一英里。 1 小时内从洛杉矶到华盛顿。尽管历史上被击中超过 4000 次，但它从未被击落，而且我相信它是唯一真正做到这一点的美国军用飞机。它飞得足够高，以至于苏联战斗机甚至无法到达它的高度。他们只是无助地飞到 20,000 英尺以下，加满加力并用力推动飞机，以致于永久损坏了发动机，并绝望地发射了导弹。</p><p>尽管它是在 20 世纪 60 年代建造的，没有计算机的帮助，<i>但直到今天</i>它仍然是有史以来最快的吸气式有人驾驶飞机。</p><p>然而洛克希德公司几乎无法出售它。正如臭鼬工厂内的一名 CIA 工程师所描述的：</p><blockquote><p>飞机将我们所有人推向了处理它的极限。飞行员必须有极大的自信才能踏入驾驶舱，因为他知道自己将以比以前快两倍半的速度飞行。我知道凯利决心将黑鸟技术推广到蓝衣飞行员身上，让整个该死的空军都起来关注他的成果。但我从来没有给他太多机会出售大量这些飞机，因为它们远远领先于其他飞行的飞机，以至于很少有指挥官愿意领导黑鸟联队或中队。 I mean this was a twenty.-first-century performer delivered in the early 1960s. No one in the Pentagon would know what to do with it.</p></blockquote><p> At its peak there were 75 engineers working on its design. (At the peak of production there were 8000 workers producing one plane a month.)</p><p> The construction story itself is insane. They couldn&#39;t use normal materials because at the speeds the SR-71 was flying, the air around it got hot enough that it would just melt normal metals used for the fuselage. So they chose to use titanium, even though no one had ever built a plane out of titanium before. They didn&#39;t even know how to work it: it was strong enough that their tools would break just trying to cut it. They had to invent novel manufacturing methods just to work it, and teach those to their fabricators. And they had to do some crazy detective work to deal with new failure modes that cropped up: for example, after noticing that some titanium panels would suddenly fail after only six or seven weeks. They eventually realized that the titanium was super vulnerable to chlorine, and that those panels had been welded during July and August, after which they had to be washed, yet this time of year the local water system was heavily chlorinated to prevent algae growth.</p><p> The aircraft went from idea to service in 20 months. For comparison, the US most recent fighter generation of fighter jet, the F35, started development in 1995 and only started full-rate production in 2021.</p><p>好的。 So how did they do it?</p><p> One electrical engineer on the project recounts the following story:</p><blockquote><p> In late 1962, I walked into Kelly Johnson&#39;s office one morning, introduced myself, and told him I was having a bit of trouble understanding the Skunk Works organization. When someone asked me a &quot;What if&quot; question I really did not know who was asking and how I should respond. Kelly went to the blackboard and drew the following org chart.</p></blockquote><blockquote><p> |KJ|</p><p> _____________________________|____________________________</p><p> |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |    |</p></blockquote><blockquote><p> He said KJ, that&#39;s me, and all the people who work here work for me.</p></blockquote><p> Moreover, Kelly did us an amazing favor, in explicitly writing down his 14 rules for management. They were as follows:</p><ol><li> The Skunk Works manager must be delegated practically complete control of his program in all aspects. He should report to a division president or higher.</li><li> Strong but small project offices must be provided both by the military and industry.</li><li> The number of people having any connection with the project must be restricted in an almost vicious manner. Use a small number of good people (10% to 25% compared to the so-called normal systems).</li><li> A very simple drawing and drawing release system with great flexibility for making changes must be provided.</li><li> There must be a minimum number of reports required, but important work must be recorded thoroughly.<br><br> [The small teams were a key component here. In order to avoid the need for lengthy documentation, Kelly would enforce a rule to “Never put an engineer more than fifty feet from the assembly area”. (p. 222)]<br></li><li> There must be a monthly cost review covering not only what has been spent and committed but also projected costs to the conclusion of the program.</li><li> The contractor must be delegated and must assume more than normal responsibility to get good vendor bids for subcontract on the project. Commercial bid procedures are very often better than military ones.</li><li> The inspection system as currently used by the Skunk Works, which has been approved by both the Air Force and Navy, meets the intent of existing military requirements and should be used on new projects. Push more basic inspection responsibility back to subcontractors and vendors. Don&#39;t duplicate so much inspection.</li><li> The contractor must be delegated the authority to test his final product in flight. He can and must test it in the initial stages. If he doesn&#39;t, he rapidly loses his competency to design other vehicles.<br><br> [JL: this contrasts with some contracts, where the air force asks the contractor to build the plane, but insists on only testing it themselves. Kelly would personally do user interviews with fighter pilots to obtain the specs for his planes, and would also insist on joining the test pilots to fly them himself, in order to iterate on the design.]<br></li><li> The specifications applying to the hardware must be agreed to well in advance of contracting. The Skunk Works practice of having a specification section stating clearly [which important military specification items will not knowingly be complied with and reasons] therefore is highly recommended.</li><li> Funding a program must be timely so that the contractor doesn&#39;t have to keep running to the bank to support government projects.</li><li> There must be mutual trust between the military project organization and the contractor, the very close cooperation and liaison on a day-to-day basis. This cuts down misunderstanding and correspondence to an absolute minimum.</li><li> Access by outsiders to the project and its personnel must be strictly controlled by appropriate security measures.</li><li> Because only a few people will be used in engineering and most other areas, ways must be provided to reward good performance by pay not based on the number of personnel supervised.</li></ol><p> In addition to these rules, here&#39;s how Kelly&#39;s successor described what made the Skunk Works tick:</p><blockquote><p> We became the most successful advanced projects company in the world by hiring talented people, paying them top dollar, and motivating them into believing that they could produce a Mach 3 airplane like the SR-71 a generation or two ahead of anyone else. Our design engineers had the keen experience to conceive the whole airplane in their mind&#39;s eye, doing the trade-offs in their heads between aerodynamic needs and weapons requirements. We created a practical and open work environment for engineers and shop workers, forcing the guys behind the drawing boards on to the shop floor to see how their ideas were being translated into actual parts and to make any necessary changes on the spot. We made every shop worker who designed or handled a part responsible for quality control. Any worker -- not just a supervisor or manager -- could send back a part that didn&#39;t meet his or her standards. That way we reduced rework and scrap waste.</p></blockquote><blockquote><p> We encouraged people to work imaginatively, to improvise and try unconventional approaches to problem solving, and then got out of their way. By applying the most common-sense methods to develop new technologies, we saved tremendous amount of time and money, while operating in an atmosphere of trust and cooperation both with our government customers and between our white-collar and blue-collar employees. In the end, Lockheed&#39;s Skunk Works demonstrated the awesome capabilities of American inventiveness when free to operate under near ideal working conditions. That may be our most enduring legacy as well as our source of lasting pride.</p></blockquote><h3> SpaceX</h3><p> I also read “Liftoff: the desperate early days that launched SpaceX”, which chronicles the company from like 2002 to 2008, when they were still young, scrappy and closer to a size that I feel able to reason about. Elon is well known as a magnate: but what were things like when he had a 10-person team?</p><p> SpaceX are also relevant because they work in hardware, in an industry usually requiring extremely large upfront capital investments (it used to be only governments who launched rockets), and they were started in a time where the Molochian decay in America&#39;s ability to build was already far gone. Gwynne Shotwell, SpaceX COO, said that, roughly, trying to explain to government customers how and why spaceX used an iterative design philosophy was “one of the hardest things I&#39;ve had to work on for almost my entire career at SpaceX” (p. 104).</p><p> Judging from speed alone, doing rocketry from scratch seems harder than building skyscrapers or planes. SpaceX built their in-house engine in about 3.5 years, successfully made their first launch in 5 years, and made orbit in 6 years. For comparison, Jeff Bezos Blue Origin was founded earlier, in 2000, and has still not reached orbit after twenty years [at least when the Liftoff book was published].</p><p> Here I have collected some excerpts that stood out to me from the book, on areas relevant to current questions I&#39;m mulling over about org structure at Lightcone.</p><p> <strong>Team size at various points</strong></p><p> SpaceX hired their 14th employee after about one year (Elon had a personal assistant from the beginning). They also spend about a year on design before building their first prototype.</p><p> After three years, close to them finishing their first engine and getting ready for the first full launch test, they had a team of 160 people total, with about 12-30 engineers, technicians, and managers working on the little atoll in the Marshall Islands that they launched from, and about 20 people at their McGregor Texas test site. (p. 76, p. 114, p.156)</p><p> <strong>Decisions on the spot</strong></p><blockquote><p> Musk could be difficult to work  for. But his early hires could see the benefits of working for someone who wanted to get things done and often made decisions on the spot. When Musk decided [in 2002] that [a new fuel tank supplier] could make good tanks for a fair price, that was it. No committees. No reports. Just, done. (p. 19)</p></blockquote><p> And, relatedly:</p><blockquote><p> As Shotwell gained Musk&#39;s confidence, her role continued to expand. She managed customer interactions at first, but eventually added human resources, legal, and day-to-day operations of SpaceX to her portfolio. Her presence allowed Musk to focus where he could be most effective. On the days he works at SpaceX -- which have varied widely over the years given his multitude of projects, but generally amount to half of a given week -- Musk said he spends 80 to 90 percent of his time on engineering questions. This includes making design decisions, and optimizing the process by which SpaceX acquires parts from suppliers and builds its engines, rockets nd spacecraft. During meetings, Musk will make snap decisions. This is one of the key things that enables SpaceX to move so quickly.<br><br> “I make the spending decisions and the engineering decisions in one head”, he said. “Normally those are at least two people. There&#39;s some engineering guy who&#39;s trying to convince a finance guy that this money should be spent. But the finance guy doesn&#39;t understand engineering, so he can&#39;t tell if this is a good way to spend money or not. Whereas I&#39;m making the engineering and spending decisions. So I know, already, that my brain trusts itself.”</p></blockquote><p> <strong>They were able to do much of the work themselves</strong></p><p> Some of their senior engineers were also good enough electricians to wire and iterate on avionics circuits themselves. Others weren&#39;t, but had to learn:</p><p> “In the early Falcon 1 day we did a little bit of everything”, structures engineer Li said. “I learned how to use a rivet gun, and how to weld things together.” (p. 131)</p><p> <strong>How they dealt with life side-constraints</strong></p><blockquote><p> [COO Gwynne Shotwell] and her ex-husband each kept their two children for a week at a time. On weeks with her kids, Shotwell would arrive early at SpaceX, and leave around 6 or 7pm, taking over for the nanny at home. During other weeks, she could go crazy, working late into the night and traveling as much as needed. (p. 103)</p><p> [Musk&#39;s senior VPs Mueller and Buzza], who had young children, lived dual lives. For ten days, they would work twelve-to-fourteen-hour shifts in McGregor before flying back to California, where typically they wold have Thursday through Sunday afternoon Off. (p. 149)</p></blockquote><p> Another early engineer couldn&#39;t move to Los Angeles because his wife had gotten a job at Google in SF that she really liked. So Musk spoke to Larry Page, who agreed that the engineer&#39;s wife could be transferred to the Google LA office instead.</p><blockquote><p> Thompson [SpaceX second employee], with a young family, shared the same hesitation about leaving a comfortable position in the Aerospace industry. During a phone call in late April, Musk sought to all those concerns, He recognised what Thompson was walking away from, so he put two years&#39; worth of salary into an escrow account. If Musk decided to prematurely pull the plug on the venture, he would still have a guaranteed income. (p. 14)</p></blockquote><p> <strong>On ownership</strong></p><blockquote><p> [In order to work with Musk in 2006, you had to walk a middle line. That is,] Musk listened to ideas. He encouraged debate. He empowered his senior employees funding and authority. But always, he had the final say. (p. 126)</p></blockquote><p> And a related Skunk Works quote:</p><blockquote><p> All of us at the Skunk Works knew the two basic rules for getting along with Kelly Johnson: all the airplanes we built were Kelly&#39;s airplanes. Whatever pride we secretly took, we kept to ourselves. And if a blue-suiter wore star on his shoulder, only Kelly Johnson was authorised to deal with him. (p. 289)</p></blockquote><p> ---</p><blockquote><p> [In 2006] Musk would convene his different teams in a small conference room, be it his engineers working on propulsion, or structures, or avionics, and run down the major issues. If an engineer faced an intractable problem, Musk wanted a chance to solve it. He would suggest ideas and give his teams a day or two to troubleshoot, then report back to him. In the interim, if they needed guidance, they were told to email Musk directly, day or night. He typically responded within minutes. (p. 19)</p></blockquote><p> ---</p><blockquote><p> Musk had waited very nearly a full year for a second attempt, and he wanted to launch. He pressed [launch director] Tim Buxxa for why the rocket needed to be fully detanked to investigate the problem. Not safe, he was told. Could they just ignore the abort signal, reboot the rocket, and go for another try that day? Not if the sensor had identified a real issue that could threaten the mission&#39;s success.</p></blockquote><blockquote><p> Buzza was the launch director. It was his call. He ordered the rocket emptied of fuel. “Elon was super upset”, Buzza said. “I suspect if he was in the control room on Kwah he would have had his way, but having him five thousand miles away gave me a little wiggle room to take more time to figure out the issue.” (p. 134)</p></blockquote><p> <strong>They frequently paid a lot for</strong> <a href="https://www.lesswrong.com/posts/zymnWfGwf6BdDt64c/how-i-buy-things-when-lightcone-wants-them-fast"><strong><u>fast shipping</u></strong></a> <strong>&nbsp;</strong></p><p> Musk would sometimes lend his private jet for shipping parts. At its peak, they spent $500k and some political favors to rent a massive military cargo plane to ship their prototype rocket to the Marshall Islands, instead of shipping it by ship for a few weeks.</p><p> According to Kevin Brogan, early SpaceX employee:</p><blockquote><p> Musk would say that everything we did was a function of our burn rate and that we were burning through a hundred thousand dollars per day. [...]</p><p> Sometimes he wouldn&#39;t let you buy a part for two thousand dollars because he expected you to find it cheaper or invent something cheaper.</p><p> Other times, he wouldn&#39;t flinch at renting a plane for ninety thousand dollars to get something to Kwaj because it saved an entire workday, so it was worth it.</p></blockquote><p> <strong>For key roles, Elon hired experienced domain experts who were willing to have their thinking molded into his operational philosophy. For their teams, he hired younger generalists</strong></p><blockquote><p> After Musk hired a few experienced hands to lead his propulsion, structures, and avionics departments -- Thompson, Mueller and Koenigsmann -- he mostly brought on recent college graduates. [Those graduates had fewer side-constraints and were willing to work extremely hard.] (p. 25)</p></blockquote><p> Tim Buzza, launch director:</p><blockquote><p> We brought some heritage aerospace experience, but also were willing to be totally molded by Elon to change our thinking. (p. 246)</p></blockquote><blockquote><p> Moreover, in order to get started with in-house manufacturing, they didn&#39;t learn everything themselves. They began by using a sub-contractors a firm to build their engines, but when that firm went bankrupt SpaceX acquired their machines and brought the whole team in-house to start their machining operation (p. 41).</p></blockquote><h3>发生了什么变化？</h3><p> Why was the ability to build lost? Some relevant quotes:</p><blockquote><p> Kelly Johnson&#39;s protege at Skunk Works, Ben Rich: “Kelly&#39;s stubbornness angered several important blue-suiters and a few of our own senior corporate executives. […] All of them demanded the same thing: that I accommodate our blue-suit customers and that I recognise the management&#39;s responsibilities to keep a close eye on me as Kelly&#39;s successor until I had a chance to prove myself. The name of the game was &#39;get along and go along&#39; — no more tyrannical Kelly Johnson types. I understood, as did Kelly, that he was unique in his power and independence, which was nontransferable to any successor. For better or worse, a new era was dawning for those left behind in a Kelly Johnson-less world. The Skunk Works was still expected to produce giant results, but the new guy sitting in the boss&#39;s chair would be a lot smaller than the original Gulliver.” (p. 293)</p></blockquote><blockquote><p> “In my forty years at Lockheed I worked on twenty-seven different airplanes. Today&#39;s young engineer will be lucky to build even <i>one</i> .” (p. 316)</p></blockquote><blockquote><p> “Kassouf [early SpaceX employee, senior avionics engineer]  had a friend at Lockheed Martin who worked on F-35 stealth aircraft, a lucrative program for the company. Eventually, the air force would buy more than two thousand units at a cost of $85 million each. It may sound like glamorous work, but it was not. Kassouf&#39;s friend had just a single job, finding a supplier from a bolt on the aircraft&#39;s landing gear and ensuring that it met all quality specifications. That single bolt was the totality of his employment.” (p.23 Liftoff)</p></blockquote><p> Note that the F-35 was built by the *same* Lockheed that also ran the Skunk Works! :(</p><p> Here&#39;s from Peter Thiel-affiliated defense startup Anduril (my model is that they&#39;re sort of like Palantir, but for hardware) on changes in defense spending:</p><blockquote><p> It can be difficult to imagine the Department of Defense, which today employs nearly 3 million Americans, moving quickly. But last century, it innovated at a speed that puts modern Silicon Valley startups to shame: the Pentagon was built in only 16 months (1941–1943), the Manhattan Project ran for just over 3 years (1942–1946), and the Apollo Program put a man on the moon in under a decade (1961–1969). In the 1950s alone, the United States built five generations of fighter jets, three generations of manned bombers, two classes of aircraft carriers, submarine-launched ballistic missiles, and nuclear-powered attack submarines.</p><p> But, starting in the 1960s and intensifying through the 70s and 80s, the pace of military innovation began to slow, while costs grew. In 1955, it had become clear that the pace of defense spending was unsustainable: the federal government was spending more on defense than everything else combined.⁹ Congress knew something had to change. Robert S. McNamara, a wunderkind World War II veteran with a sterling reputation for administrative efficiency at Ford Motor Company, was the man for the job.</p><p> In the early 1960s, under the direction of then Secretary of Defense McNamara, the Department of Defense instituted a labyrinth of new rules for acquiring military systems. McNamara was not merely reducing spending: his experience at Ford led him to believe that comprehensive reform was needed to reshape how the government bought technology. He revamped acquisitions to emphasize efficiency, the elimination of waste, and predictability. The Planning, Programming, and Budgeting System (PPBS, later changed to PPBE for “Execution”) process that he implemented is arguably the single most influential defense reform ever enacted, indelibly altering the incentives and business models of the major defense contractors.</p><p> [...]</p></blockquote><blockquote><p> <strong>1. Adherence to a Lengthy Bureaucratic Process</strong></p></blockquote><blockquote><p> Under the PPBS system, before the government even considers purchasing a new military system, it embarks upon a years-long process of defining requirements, deciding where and how to allocate resources, and finally releasing an award for a new system. It is extremely difficult for the Department of Defense to rapidly acquire new technology, and as a result defense companies face no pressure to develop new systems quickly. Technology developed in the commercial sector hence takes years or decades to end up on the battlefield, if it does at all.</p><p> <strong>2. Working off Onerous System Specifications</strong></p><p> Reflecting the prevailing belief that industry was re- quired to execute, not innovate, to beat the Soviet Union, the requirements for major military programs are spelled out in extensive detail. Unlike most industries, which are driven forward by the creativity of the most successful companies, defense contractors are rarely asked to find creative ways to solve problems, and are sometimes punished for doing so.</p><p> <strong>3. Spending Little on Internal Research &amp; Development</strong></p><p> Because McNamara&#39;s reforms made it exceptionally difficult to buy new technology quickly or to buy technology for which there is no defined requirement, defense companies rarely develop products of their own accord. Typically, they sell existing systems (which was most of their business under McNamara) or they wait for the government to order specific R&amp;D efforts, for which they are directly compensated. For a comparison, the largest technology companies today — whose revenues vastly exceed those of the largest defense companies — spend roughly 10–20% of their revenue on research and development. Newer or mid-sized technology startups might spend closer to 60% or 70%. The major defense companies spend 1–4%.¹¹</p><p> Research and Development across industries.¹²</p><p> <strong>4. Prioritizing Proposals Over Performance</strong></p><p> McNamara believed duplicative programs to be at the root of DoD waste and would not tolerate government spending on multiple development efforts for similar new systems, whether between military services or among different vendors. This means that once a company is awarded a major contract, it is extremely difficult to take it from them. The incentive for large defense firms is hence to spend heavily on teams of lawyers and lobbyists to shape program requirements in line with the company&#39;s existing technology. This political battle becomes just as important as building the product itself. These lobbyists are often former military officials: By 1969, over 2,000 military officers per year were leaving the DoD to work for a major defense contractor, three times as many as in 1959.¹³</p><p> <strong>5. Tolerating Prolonged Failure</strong></p><p> In most industries, a company that fails to produce a functional product goes out of business. In the defense industry, when a company is three, five, ten years into a program and has failed to build what they promised, the government is stuck between a rock and a hard place — do they cancel the contract and throw away years of development, potentially bankrupting the vendor in the process? Or do they begrudgingly provide the vendor with even more money to salvage the program? Typically, it&#39;s the latter. In the words of the infamous city planner Robert Moses, “Once you sink that first stake, they&#39;ll never make you pull it up.”¹⁴</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/BpTDJj6TrqGYTjFcZ/a-golden-age-of-building-excerpts-and-lessons-from-empire#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/BpTDJj6TrqGYTjFcZ/a-golden-age-of-building-excerpts-and-lessons-from-empire<guid ispermalink="false"> BpTDJj6TrqGYTjFcZ</guid><dc:creator><![CDATA[jacobjacob]]></dc:creator><pubDate> Fri, 01 Sep 2023 04:03:44 GMT</pubDate> </item><item><title><![CDATA[Meta Questions about Metaphilosophy]]></title><description><![CDATA[Published on September 1, 2023 1:17 AM GMT<br/><br/><p> To quickly recap my main intellectual journey so far (omitting a lengthy <a href="https://www.lesswrong.com/posts/m8FjhuELdg7iv6boW/work-on-security-instead-of-friendliness">side trip</a> into cryptography and Cypherpunk land), with the approximate age that I became interested in each topic in parentheses:</p><ul><li> (10) Science - Science is cool!</li><li> (15) Philosophy of Science - The scientific method is cool! Oh look, there&#39;s a whole field studying it called &quot;philosophy of science&quot;!</li><li> (20) Probability Theory - Bayesian subjective probability and the universal prior seem to constitute an elegant solution to the philosophy of science. Hmm, there are some curious probability puzzles involving things like indexical uncertainty, copying, forgetting... I and others make some progress on this but fully solving anthropic reasoning seems really hard. (Lots of people have worked on this for a while and have failed, at least according to my judgement.)</li><li> (25) Decision Theory - Where does probability theory come from anyway? Maybe I can find some clues that way? Well according to von Neumann and Morgenstern, it comes from decision theory. And hey, maybe it will be really important that we get decision theory right for AI? I and others make some progress but fully solving decision theory turns out to be pretty hard too. (A number of people have worked on this for a while and haven&#39;t succeeded yet.)</li><li> (35) Metaphilosophy - Where does decision theory come from? It seems to come from philosophers trying to do philosophy. <a href="https://www.lesswrong.com/posts/MAhueZtNz5SnDPhsy/metaphilosophical-mysteries">What is that about?</a> Plus, maybe it will be <a href="https://www.lesswrong.com/posts/w6d7XBCegc96kz4n3/the-argument-from-philosophical-difficulty">really important that the AIs we build will be philosophically competent</a> ?</li><li> (45) Meta Questions about Metaphilosophy - Not sure how hard solving metaphilosophy really is, but I&#39;m not making much <a href="https://www.lesswrong.com/posts/EByDsY9S3EDhhfFzC/some-thoughts-on-metaphilosophy">progress</a> on it by myself. Meta questions once again start to appear in my mind:<ul><li> Why is there <a href="https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom?commentId=MqBLzocx8z8jDamop">virtually</a> <a href="https://www.lesswrong.com/posts/Ghrdnc26ftJrxD49z/carl-shulman-on-the-lunar-society-7-hour-two-part-podcast?commentId=Mp96pHQyhoyEaA2MZ">nobody</a> <a href="https://forum.effectivealtruism.org/posts/WYsLjeJzh7tZqe6Lo/implications-of-evidential-cooperation-in-large-worlds?commentId=HPdgoLKheaEnSwcqD">else</a> interested in metaphilosophy or ensuring AI philosophical competence (or that of future civilization as a whole), even as we get ever closer to AGI, and other areas of AI safety start attracting more money and talent?</li><li> Tractability may be a concern but shouldn&#39;t more people still be talking about these problems if only to raise the alarm (about an additional reason that the AI transition may go badly)? (I&#39;ve listened to all the recent podcasts on AI risk that I could find, and nobody brought it up even once.)</li><li> How can I better recruit attention and resources to this topic? For example, should I draw on my crypto-related fame, or start a prize or grant program with my own money? I&#39;m currently not inclined to do either, out of inertia, unfamiliarity, uncertainty of getting any return, fear of drawing too much attention from people who don&#39;t have the highest caliber of thinking, and signaling wrong things (having to promote ideas with one&#39;s own money instead of attracting attention based on their merits). But I&#39;m open to having my mind changed if anyone has good arguments about this.</li><li> What does it imply that so few people are working on this at such a late stage? For example, what are the implications for the outcome of the human-AI transition, and on the distribution of philosophical competence (and hence the distribution of values, decision theories, and other philosophical views) among civilizations in the universe/multiverse?</li></ul></li></ul><p> At each stage of this journey, I took what seemed to be the obvious next step (often up a meta ladder), but in retrospect each step left behind something like 90-99% of fellow travelers. From my current position, it looks like &quot;all roads lead to metaphilosophy&quot; (ie, one would end up here starting with an interest in any nontrivial problem that incentivizes asking meta questions) and yet there&#39;s almost nobody here with me. <em>What gives?</em></p><p> As for the AI safety path (as opposed to pure intellectual curiosity) that also leads here, I guess I do have more of a clue what&#39;s going on. I&#39;ll describe the positions of 4 people I know. Most of this is from private conversations so I won&#39;t give their names.</p><ul><li> Person A has a specific model of the AI transition that they&#39;re pretty confident in, where the first AGI is likely to develop a big lead and if it&#39;s aligned, can quickly achieve human uploading then defer to the uploads for philosophical questions.</li><li> Person B thinks that ensuring AI philosophical competence won&#39;t be very hard. They have a specific (unpublished) idea that they are pretty sure will work. They&#39;re just too busy to publish/discuss the idea.</li><li> Person C will at least think about metaphilosophy in the back of their mind (as they spend most of their time working on other things related to AI safety).</li><li> Person D thinks it is important and too neglected but they personally have a comparative advantage in solving intent alignment.</li></ul><p> To me, this paints a bigger picture that&#39;s pretty far from &quot;humanity has got this handled.&quot; If anyone has any ideas how to change this, or answers to any of my other unsolved problems in this post, or an interest in working on them, I&#39;d love to hear from you.</p><br/><br/> <a href="https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/fJqP9WcnHXBRBeiBg/meta-questions-about-metaphilosophy<guid ispermalink="false"> fJqP9WcnHXBRBeiBg</guid><dc:creator><![CDATA[Wei Dai]]></dc:creator><pubDate> Fri, 01 Sep 2023 01:17:57 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Michael Nielsen remarks on 'Oppenheimer']]></title><description><![CDATA[Published on August 31, 2023 3:46 PM GMT<br/><br/><p> This is a linkpost to a recent blogpost from <a href="https://michaelnielsen.org/">Michael Nielsen</a> , who has previously written on <a href="https://michaelnotebook.com/eanotes/">EA</a> among many other topics. This blogpost is adapted from a talk Nielsen gave to an audience working on AI before a screening of <i>Oppenheimer</i> . I think the full post is worth a read, but I&#39;ve pulled out some quotes I find especially interesting (bolding my own)</p><blockquote><p> I was at a party recently, and happened to meet a senior person at a well-known AI startup in the Bay Area. They volunteered that they thought &quot;humanity had about a 50% chance of extinction&quot; caused by artificial intelligence. I asked why they were working at an AI startup if they believed that to be true. They told me that while they thought it was true, &quot; <strong>in the meantime I get to have a nice house and car</strong> &quot;.</p></blockquote><blockquote><p> [...] I often meet people who claim to sincerely believe (or at least seriously worry) that AI may cause significant damage to humanity. And yet they are also working on it, justifying it in ways that sometimes seem sincerely thought out, but which <strong>all-too-often seem self-serving or self-deceiving.</strong></p></blockquote><p></p><blockquote><p> Part of what makes the Manhattan Project interesting is that we can chart the arcs of moral thinking of multiple participants [...] Here are four caricatures:</p><ul><li> Klaus Fuchs and Ted Hall were two Manhattan Project physicists who took it upon themselves to commit espionage, communicating the secret of the bomb to the Soviet Union. It&#39;s difficult to know for sure, but <strong>both seem to have been deeply morally engaged and trying to do the right thing, willing to risk their lives</strong> ; they also made, I strongly believe, a terrible error of judgment. I take it as a warning that <strong>caring and courage and imagination are not enough</strong> ; they can, in fact, lead to very bad outcomes.</li><li> Robert Wilson, the physicist who recruited Richard Feynman to the project. Wilson had thought deeply about Nazi Germany, and the capabilities of German physics and industry, and made a principled commitment to the project on that basis. He half-heartedly considered leaving when Germany surrendered, but opted to continue until the bombings in Japan. He later regretted that choice; immediately after the Trinity Test he was disconsolate, telling an exuberant Feynman: &quot;It&#39;s a terrible thing that we made&quot;.</li><li> Oppenheimer, who I believe was motivated in part by a genuine fear of the Nazis, but also in part by personal ambition and a desire for &quot;success&quot;. It&#39;s interesting to ponder his statements after the War: while he seems to have genuinely felt a strong need to work on the bomb in the face of the Nazi threat, his comments about continuing to work up to the bombing of Hiroshima and Nagasaki contain many strained self-exculpatory statements about how you have to work on it as a scientist, that the technical problem is too sweet. It smells, to me, of someone looking for self-justification.</li><li> Joseph Rotblat, the one physicist who actually left the project after it became clear the Nazis were not going to make an atomic bomb. He was threatened by the head of Los Alamos security, and falsely accused of having met with Soviet agents. In leaving he was turning his back on his most important professional peers at a crucial time in his career. <strong>Doing so must have required tremendous courage and moral imagination</strong> . Part of what makes the choice intriguing is that <strong>he himself didn&#39;t think it would make any difference to the success of the project. I know I personally find it tempting to think about such choices in abstract systems terms: &quot;I, individually, can&#39;t change systems outcomes by refusing to participate [&#39;it&#39;s inevitable!&#39;], therefore it&#39;s okay to participate&quot;</strong> . And yet while that view seems reasonable, Rotblat&#39;s example shows it is incorrect. His private moral thinking, which seemed of small import initially, set a chain of thought in motion that eventually led to Rotblat founding the Pugwash Conferences, a major forum for nuclear arms control, one that both Robert McNamara and Mikhail Gorbachev identified as helping reduce the threat of nuclear weapons. Rotblat ultimately received the Nobel Peace Prize. <strong>Moral choices sometimes matter not only for their immediate impact, but because they are seeds for downstream changes in behavior that cannot initially be anticipated.</strong></li></ul></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/ppQRJEfLBCLFzK73w/linkpost-michael-nielsen-remarks-on-oppenheimer#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/ppQRJEfLBCLFzK73w/linkpost-michael-nielsen-remarks-on-oppenheimer<guid ispermalink="false"> ppQRJEfLBCLFzK73w</guid><dc:creator><![CDATA[22tom]]></dc:creator><pubDate> Thu, 31 Aug 2023 15:46:06 GMT</pubDate> </item><item><title><![CDATA[My thoughts on AI and personal future plan after learning about AI Safety for 4 months]]></title><description><![CDATA[Published on August 31, 2023 3:32 PM GMT<br/><br/><p> In this post, I want to distill some of my thoughts about AI and my future plan regarding it according to what I have learnt during the past 4 months.</p><p> About my personal background: My background lies in Financial Engineering and Mathematics, and I used to get motivated by making AI more powerful. I decided to shift my focus to AI Safety after reading and getting convinced by the idea inside <a href="https://80000hours.org/problem-profiles/artificial-intelligence/">Preventing an AI-related catastrophe</a> . During the past 4 months, I  finished</p><ul><li> <a href="https://course.aisafetyfundamentals.com/alignment">Alignment Course from AI Safety Fundamentals</a></li><li> 6 out of 9 Topics inside <a href="https://github.com/jacobhilton/deep_learning_curriculum">Deep Learning Curriculum</a></li><li> 4 out of 5 Chapters inside <a href="https://www.arena.education/">ARENA</a></li><li> a <a href="https://github.com/ZiyueWang25/llm-security-challenge">paper</a> from <a href="https://alignmentjam.com/jam/evals">LLM Evals Hackathon</a></li><li> And many readings across <a href="https://www.alignmentforum.org/">AI Alignment Forum</a> , LessWrong and different AI Safety focused team/org/company.</li></ul><p> I kept those related posts/learning in my <a href="https://ziyuewang25.github.io/">website</a> if you are interested.</p><p> Overall, I think the future of AI is promising but in the same dangerous if they cannot align with our intention. It is like what has been discussed in <a href="https://theprecipice.com/">Precipice</a> and I want to take the chance to help it.</p><p> It is promising because it already demonstrated superior capability, and can be used to improve people&#39;s life quality. The application field can be robotics, education, health system, productivity boost and etc.</p><p> But AI can get misaligned if we aren&#39;t paying enough attention to it. Here, according to Paul Christiano, alignment means <a href="https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6">intent alignment</a> :</p><blockquote><p> AI A is aligned with an operator H when A is trying to do what H wants it to do.</p></blockquote><p> The focus here is &quot;trying&quot;, which means it is on intention/goal/motivation level rather than behavior level. A model can behave like aligned but it tries to <a href="https://www.lesswrong.com/posts/YgAKhkBdgeTCn6P53/ai-deception-a-survey-of-examples-risks-and-potential">deceive human</a> or human actually couldn&#39;t see its defect due to the high complexity of future tasks.</p><p> In the argument above, we assumed it can have &quot;intention&quot; and we can understand how it comes from by using optimizer. When we train a model, we use an optimizer which usually tries to minimize certain loss function or maximize reward function by adjusting the weight of the model. For example, <a href="https://openai.com/gpt-4">GPT</a> is pre-trained by minimizing the next token prediction loss. So the goal of it, at that stage, is simply trying to make the next word make sense given what it sees in the past. Its goal is not about aligning with human instruction and making human happy or productive. Hence we need to do further fine-tuning, like <a href="https://huggingface.co/blog/rlhf">Reinforcement From Human Feedback</a> (RLHF), to make it align with human instruction.</p><p> But I am not confident they are aligned by doing this. Here are several reasons:<br> 1.<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=147s&amp;ab_channel=RobertMilesAISafety"><strong>Mesa-optimizer</strong></a> : The optimizer we used in the model training is <strong>not the same</strong> as the optimizer inside (mesa-optimizer) the model that drives its behavior. For example, we can use a dataset to teach the model to tell the truth but due to labeling mistake, the model can understand it as <strong>telling the result as long as human think it is correct, rather than the truth</strong> . Also, It can comes from a concept called &quot; <a href="https://drive.google.com/file/d/1KewDov1taegTzrqJ4uurmJ2CJ0Y72EU3/view"><strong>instrumental convergence</strong></a> &quot;, which means as the model tend to do something, it can also develop some common instrumental goals to help itself achieve that. Common instrumental goals are For example, it is trained to make the quality of a person better, and it learnt to avoid self-preservation, power-seeking and etc. being shutdown because if it get shut down, it can no longer  make a person life better. So overall, a mesa-optimizer makes the intention of the model different from what we want, hence misaligned.</p><p> 2. <strong>The bulk of capability and hence &quot;intention&quot; are still from pretraining</strong> :  Compared with fine-tuning, pre-training takes hundreds of more resources to do. During that phase, the model get exposed to a lot of knowledge. It is not told to be polite and helpful during that phase, it is simply told to predict the next word. Putting this mode on human would be like: children are getting &quot;educated&quot; about reasoning, culture, reading comprehension and etc without being told about what is a good intention and how to communicate or help other people. This sounds dangerous because before they are being told about what is a good intention, they can already develop their own view strongly, which may not be in favor of the good intention. This is especially dangerous when the pre-training data, corpus, contains a lot of toxic/biased data. Some arguments against this would be how the model learns is different from human and their intention can be corrected during finetuning phase. But still, there is a chance that they can develop strong intention during pre-training phase, which we has almost no control over, except making the data better.</p><p> 3. <strong>Many existing jail breaker</strong> : there are many <a href="https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day">examples</a> showing that models can be turned into a rude/toxic mode. Those behavior can be elicited by using role-play and some other hacking methods. They are the sign of bad intention within the model that we currently cannot control with.</p><p> Even when the model is aligned, it can still be misused. So a model that tries to do what human wants it to do can also be applied to dangerous field, like weapon development, attacking security system and etc. This requires policy maker and corporate to have proper control over how AI gets deployed and used, which leads to a large field of AI safety policy and governance. For more details, please check <a href="https://www.lesswrong.com/posts/9dNxz2kjNvPtiZjxj/an-overview-of-catastrophic-ai-risks-summary">An overview of Catastrophic AI Risks</a> .</p><p> According to <a href="https://theprecipice.com/">Precipice</a> , misaligned AI can pose existential risk to humanity and the chance about it for this century is around 10%. This a large percentage and may sound wild to you at this moment and you may not be convinced, that&#39;s totally understandable. But even we feel uncertain at this moment, the outcome of this small-chance event is unimaginable. This puts us into urgent state to take action and for me, it is about becoming an <a href="https://80000hours.org/articles/ml-engineering-career-transition-guide/">AI Safety Research Engineer</a> to help model get aligned.</p><p> I started learning about different safety topics since May 2023. According to <a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment">the AI landscape</a> from Paul Christiano, there are many topics about AI alignment. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/fjahdkrvd3sizrknos1l" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/hvtfn8pnguvspgux5geo 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/npn65thtmwcceeddsq0c 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/pdfnopkkskm3nheyxpye 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/fv6xlptuud6usm0aacjz 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/vu8pwbnfh0mx1jmutah1 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/ytvmro2npdf2p0bki2ov 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/uqc5jdxrp6t7zufibd6e 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/y6rq8bjjnnrgqg2fhbqc 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/loxj5qblqzawty3ohwfg 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GreaiJYjFDziXbfCw/kn5p1tqzrkfio2nlt467 1100w"><figcaption> AI Landscape from Paul Christiano</figcaption></figure><p></p><p> Up to this point, to be honest, I don&#39;t have a strong preference over which direction to go. But if I had to say one, I think &quot;Inner Alignment&quot; part sounds more important to me because only with inner alignment verification, we can tell whether outer alignment worked or not. The concrete inner alignment examples are like <a href="https://www.anthropic.com/index/core-views-on-ai-safety">Scalable Oversight</a> , <a href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html">Mechanistic Interpretability</a> , Automated <a href="https://huggingface.co/blog/red-teaming">Red Teaming</a> , <a href="https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk#:~:text=Eliciting%20Latent%20Knowledge%20is%20an,that%20look%20good%20to%20us">Eliciting Latent Knowledge</a> , and etc.</p><p> The main point at this point in my life is to switch my career towards them and contribute the bulk of my day time to maximize my impacts. I felt motivated given the urgency we have to solve this alignment problem and I look forward to the day of becoming an AI Safety Research Engineer :)</p><br/><br/> <a href="https://www.lesswrong.com/posts/GreaiJYjFDziXbfCw/my-thoughts-on-ai-and-personal-future-plan-after-learning#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/GreaiJYjFDziXbfCw/my-thoughts-on-ai-and-personal-future-plan-after-learning<guid ispermalink="false"> GreaiJYjFDziXbfCw</guid><dc:creator><![CDATA[Ziyue Wang]]></dc:creator><pubDate> Fri, 01 Sep 2023 05:18:35 GMT</pubDate> </item><item><title><![CDATA[Which Questions Are Anthropic Questions?]]></title><description><![CDATA[Published on August 31, 2023 3:15 PM GMT<br/><br/><p> I will try to keep this shot, just want to use some simple problems to point out what I think is a commonly overlooked point in anthropic discussions.</p><h2> 1. The Room Assignment Problem</h2><blockquote><p> You are among 100 people waiting in a hallway. The hallway leads to a hundred rooms numbered from 1 to 100. All of you are knocked out by a sleeping gas and each put into a random/unknown room. After waking up, what is the probability that you are in room No. 1?</p></blockquote><p> This is just an ordinary probability question. All room numbers are symmetrical, the answer is simply 1%. It is also easy to imagine taking part in similar room-assigning experiments a great number of times, the relative fraction of you waking up in room No.1, or any other number, would approach 1%.</p><p> It is safe to say this is not an anthropic question. Even for people with the metaphysical stance that all probabilities are anthropic, it is undeniable that we have been solving such questions successfully without any anthropic considerations.</p><h2> 2. The Incubator</h2><blockquote><p> An incubator enters the hallway. It will enter room No.1 and creat a person in it then does the same for the other 99 rooms.  It turns out you are one of the people the incubator has just created. You wake up in a room and is made aware of the experiment setup. What is the probability that you are in room No.1?</p></blockquote><p> While this question may seem trivial, it is an anthropic question. In ordinary problems like Question 1, an event refers to a unique experiment outcome, a distinct possible world. But this incubator problem, objectively speaking, is deterministic. Knowing the exact process from a god&#39;s eye view still leaves uncertainty because the uncertainty comes from the fact that you are unsure of your own location in this only world, eg which physical person is the subjective self. This kind of same-world event is the crux of anthropic paradoxes. Eg the sleeping beauty problem is special because of the two awakenings both in the Tails world.</p><p> Also worth noting the frequentist model is much trickier to imagine. While the incubator has no problem performing this job ten thousand times, and creating a million people in the process, it seems absurd to say you are one of the people created in each experiment, that you are ten thousand people. Obviously, this is not a run-of-the-mill question.</p><h2> 3. Incubator + Room Assignment</h2><blockquote><p> This time the incubator creats 100 people in the hall way, you are among the 100 people created. Each person is then assigned to a random room. What is the probability that you are in Room 1?</p></blockquote><p> This is just an ordinary probability question. In fact, it is Question 1 with some background story of how you got in the hallway before the experiment began. Whether you were there because of the incubator or some other process does not affect the room-assigning process. Different room numbers still reflect unique assignments and different possible worlds.</p><p> Instead of taking which physical person is the self as a given fact, some might want to treat it the same way as in Question 2, implying besides the room assignment process, the uncertainty also comes from which physical person the self is. This would lead to undesirable consequences. However, that is not the focus of this post. Even if one endorses this approach it is undeniable that Questions 2 and 3 are different in nature. The former&#39;s uncertainty entirely depends on the single-world self-location while the latter doesn&#39;t.</p><h2> Discussion</h2><p> Almost all anthropic discussions implicitly treat Questions 2 and 3 as the same problem not worth differentiating. Some, eg Nick Bostrom IMSMR, explicitly stated that they ought to be regarded as equivalent. But this position is not accompanied by any explanation. It is treated as an intuitive thing to simply accept.</p><p> But such equivalency links anthropic problems with ordinary ones. It requires justification. One possible justification, which I think is also one major reason why the equivalency seems so intuitive, is that all popular anthropic theories, including both SSA and SIA, as long as they consider the self as a random sample, would treat Question 2 the same way as Question 1. From this, it follows Question 2 and 3 are equivalent as well.</p><p> In reality, however, the arguments sometimes take the opposite direction. People take the equivalency as an indisputable fact and from there argue for the credibility of common anthropic assumptions. Without an independent justification for the purposed equivalency, this logic is circular.</p><br/><br/> <a href="https://www.lesswrong.com/posts/SjEFqNtYfhJMP8LzN/which-questions-are-anthropic-questions#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/SjEFqNtYfhJMP8LzN/which-questions-are-anthropic-questions<guid ispermalink="false"> SjEFqNtYfhJMP8LzN</guid><dc:creator><![CDATA[dadadarren]]></dc:creator><pubDate> Thu, 31 Aug 2023 15:15:39 GMT</pubDate> </item><item><title><![CDATA[The Tree of Life, and a Note on Job ]]></title><description><![CDATA[Published on August 31, 2023 2:03 PM GMT<br/><br/><p> This is another one cross-posted from <a href="https://new-savanna.blogspot.com/2011/06/tree-of-life-and-note-on-job.html">my New Savanna blog</a> . It&#39;s old, 11 years (and most of it is even older than that). It&#39;s an interpretation of the Book of Job, which is not at all the sort of thing I&#39;d ordinarily post here. I certainly don&#39;t see much of that kind of thing going on. But I&#39;m in an experimental and playful mood, so why not?</p><p> FWIW and BTW, I&#39;m neither Christian nor Jewish, so this post isn&#39;t an expression of religious belief.</p><p> Here goes:</p><p> More or less <a href="http://www.michaelspornanimation.com/splog/?p=2655">on Michael Sporn&#39;s recommendation</a> , I&#39;ve just seen Terrence Malick&#39;s <i>The Tree of Life</i> . While I&#39;m collecting my thoughts on this trying, tedious, and rewarding film, I&#39;ll let Michael&#39;s thoughts stand-in for many of mine:</p><blockquote><p> The film starts with a vision of god that moves beyond to a patriarchal dominated family in Waco, Texas. The suggestion of a death leads us back to god and the creation of the earth. From protozoa to dinosaur to the birth of a child, this filmmaker exudes absolute love for every organism he can show us on screen. Yet, right from the dinosaurs onward he creates an ominous tone in this male-dominated power hungry environment. You&#39;re always expecting something terrible to happen in the hands of the children who push the film forward. This is a film that technically has a new way of presenting itself almost through an impressionistic vision. The whispered narration and dialogue mix and blend into one; the sun streamed backlit late-afternoon interiors create a whispered visual to match.</p></blockquote><p> It was the phrase “from protozoa to dinosaur” that got me.</p><p> The film is that of a mystic. I know nothing of Malick, though it seems he was <a href="http://www.whitehorseinn.org/blog/2011/06/20/review-of-the-tree-of-life/">born in the Bible belt and studied philosophy</a> , so I don&#39;t know if he is really mystic, but then, what&#39;s <i>really</i> in that question? I once told my draft board that I was a mystic. <i>Really?</i> Really. That&#39;s what comes through in the film: “exudes absolute love for every organism he can show us on screen.”<br><br> The film&#39;s explicit religiosity bugged me in the beginning. <i>Am I going to have to say something about this in my review? Am I going to have to declare, for example, whether or not I&#39;m a believer?</i> And then it didn&#39;t bug me, not for the last hour or so. I just forgot about it.<br><br> I&#39;ll have more to say about the film later, but I just wanted to dig out some old notes, from 25 years ago, on Job. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/wfoah7kpktutxoem2cex" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/gw8ab8ritauxdh2l2jhc 270w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/xtygnuqs0wrflnjedxka 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/y5mwsemcf5y7dg0sydjl 810w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/ug9vypqgjaaikwjlr5wn 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/qkfjplnybgprcbmzwdtr 1350w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/uetqxkcivsz9mex7xqgb 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/u7ydvcwtyqdwfemitlak 1890w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/cldcdjgskxbop0eoay7x 2160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/azi9sdukf9ib0oy4n54z 2430w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/wKHbJPbzLtgCfXcQe/l1ve2ig7hacxpr60maei 2677w"></figure><p> To understand the story of Job we must first reject the ending, in which Job regains all that he has lost, and more, for his possessions were doubled. The ending is known to be a later addition. We reject it, for it subverts the deepest significance of the basic story, which is that man and God are essentially and absolutely different, hence there can be no reciprocal contracts between them. The effect of that ending is to assimilate the story to an ethos in which such contracts are possible, in which it is reasonable for man to bargain with God.<br><br> The view of the relationship between man and God which is assumed, first by Job&#39;s three friends, Eliphaz, Bildad, and Zophar, and later by Elihu (also a later addition), is basically a contractual one. There are rights and obligations on both sides of the contract and if either party breaks the contract he is liable to punitive action by the other party. God may be immensely more powerful and knowledgeable than man, but he is no so deeply different that he cannot enter into contracts with man. In this view God is assumed to be just and a man&#39;s state is assumed to reflect his performance in carrying out his obligations to the divine contract. The basic contract seems to be: Man obeys God&#39;s rules and is rewarded or punished accordingly. Prosperity is a sign of good performance while misfortune is a sign of poor performance. Job&#39;s misfortune&#39;s are taken as a sign of his poor performance.<br><br> However, Job rigorously examines his life and can find no instances of poor contractual performance. He has met his obligations. Hence he cannot understand why he is being punished. But, the text is careful to assert, &quot;Throughout all this, Job did not utter one sinful word.&quot; His friends insist that he must have done something wrong, otherwise God wouldn&#39;t be punishing him. Hence he should look more deeply and continue to do so until he finds what he has done wrong. Job will have none of this. And so we face a dilemma. If Job is both just and the victim of misfortune, is God then unjust?<br> The answer indicated by the text is, in effect, that justice has nothing to do with it, that the relationship between man and God is not one of reciprocal contractual obligation. On the contrary, it is wholly one-sided. God begins his answer by telling Job to &quot;Brace yourself and stand up like a man.&quot; He then begins a long series of rhetorical questions:</p><blockquote><p> Where were you when I laid the earth&#39;s foundations?<br> Tell me, if you know and understand.<br> Who settled its dimensions? Surely you should know.<br> Who stretched his measuring-line over it?<br> On what do its supporting pillars rest?<br> Who set its corner-stone in place,<br> when the morning stars sang together<br> and all the sons of God shouted aloud?</p></blockquote><p> The series of such questions amounts to a miniature encyclopedia of natural phenomena, one which emphasizes the absolute difference between God and man. For God has done and understands all this things while man, Job, has done and understood none of them. Job acknowledges this absolute difference, finally replying</p><blockquote><p> I know that thou canst do all things<br> and that no purpose is beyond thee.<br> But I have spoken of great things which I have not understood,<br> things too wonderful for me to know.<br> I knew of thee then only by report,<br> but now I see thee with my own eyes.<br> Therefore I melt away;<br> I repent in dust and ashes.</p></blockquote><p> The effect of the added ending, in which Job gets it all back, with interest, is to undermine this absolute difference between the human and the divine. If Job gets it all back, with interest, then the contract was not really broken at all. Job gets what is his due. That, however, is not what the story is about. The story is about absolute difference; it is attempting to replace the contractual view of the relationship between the human and the divine with a deeply ontological view, in which the divine is the underpinning, the ground, of the human.<br><br> Notice that the story is framed in such a way that the audience or the reader knows that Job did not do any wrong and that he is not being unjustly punished. We know that Job&#39;s misfortunes have nothing to do with punishment. The real reason - that Job is being used to make a point to Satan - may not be much better from our modern point of view; but it doesn&#39;t contradict the basic point. In fact, the frame reinforces the point. For Satan has argued that Job is good only because God has rewarded him well. “But stretch out your hand and touch all that he has, and then he will curse you to your face.” And so Satan is given permission to wreck Job&#39;s life and Job does not, in fact, curse God. The story shows Satan, and us, that his view of the relationship between the human and the divine, which is the contractual view, is wrong.<br><br> Whatever the relationship between the human and the divine, it is such that Job was able to bear up under his misfortune without either blaming himself or cursing God. Could it be that he was able to do so precisely because God was Completely and Categorically Other?</p><br/><br/> <a href="https://www.lesswrong.com/posts/wKHbJPbzLtgCfXcQe/the-tree-of-life-and-a-note-on-job#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/wKHbJPbzLtgCfXcQe/the-tree-of-life-and-a-note-on-job<guid ispermalink="false"> wKHbJPbzLtgCfXcQe</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Thu, 31 Aug 2023 14:03:46 GMT</pubDate> </item><item><title><![CDATA[Cleaning a SoundCraft Mixer]]></title><description><![CDATA[Published on August 31, 2023 1:20 PM GMT<br/><br/><p> <span>In</span> <a href="https://www.kingfisherband.com/">Kingfisher</a> <a href="https://www.ceciliavacanti.com/">Cecilia</a> uses a <a href="https://www.audio-technica.com/en-us/pro35">condenser</a> on her fiddle but combines it with effects pedals. This is <a href="https://www.jefftk.com/p/fiddle-effects-tech">a bit awkward to get working</a> , and we&#39;ve been using a <a href="https://www.soundcraft.com/en-US/products/notepad-5">Soundcraft Notepad</a> mini-mixer. Unfortunately we had some issues recently where adjusting the input gain would make horrible scratchy noises: </p><p><iframe src="https://www.youtube.com/embed/EJBCgvaP2ak?si=V0gEJ1Zmtgg9mPHd" allow="accelerometer;
autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture; web-share" allowfullscreen=""></iframe></p><p> I thought I&#39;d need to take the mixer apart to clean it, but a bit of looking online turned up suggestions to gently pull the knob off the face of the board, spray a contact treatment, and turn the spindle to work the treatment in. This seemed surprising to me: would this really distribute the cleaner where it needed to be? But I ordered some <a href="https://hosatech.com/products/accessories/cleaners-conditioners/caig-cleaners-conditioners/d5/">Deoxit 5%</a> and followed the instructions on the can, and it worked perfectly! </p><p><iframe src="https://www.youtube.com/embed/j5_7vaDtLTQ?si=mg7IOOj2Zhu6wNs7" allow="accelerometer;
autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture; web-share" allowfullscreen=""></iframe></p><p> Now I don&#39;t hear any noise at all when adjusting the gain.</p><p> I now have an almost full can of this stuff, so if you&#39;re in Boston and are having similar issues you&#39;re welcome to come borrow it.</p><p> (Even though the mixer is now working well Cecilia isn&#39;t planning to go back to it: she bought a <a href="https://www.radialeng.com/product/voco-loco">Voco Loco</a> , which is a box built for exactly this purpose. It should be more robust and reliable, with fewer knobs and switches that can get knocked into the wrong position.)</p><br/><br/> <a href="https://www.lesswrong.com/posts/hyuD9e7kAoHrWSyj7/cleaning-a-soundcraft-mixer#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/hyuD9e7kAoHrWSyj7/cleaning-a-soundcraft-mixer<guid ispermalink="false"> hyuD9e7kAoHrWSyj7</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Thu, 31 Aug 2023 13:20:03 GMT</pubDate> </item><item><title><![CDATA[AI #27: Portents of Gemini]]></title><description><![CDATA[Published on August 31, 2023 12:40 PM GMT<br/><br/><p> By all reports, and as one would expect, Google&#39;s Gemini looks to be substantially superior to GPT-4. We now have more details on that, and also word that Google plans to deploy it in December, <a href="https://manifold.markets/RH/will-googles-gemini-llm-be-released">Manifold gives it 82% to happen this year</a> and similar probability of being superior to GPT-4 on release.</p><p> I indeed expect this to happen on both counts. This is not too long from now, but also this is AI #27 and Bard still sucks, Google has been taking its sweet time getting its act together. So now we have both the UK Summit and Gemini coming up within a few months, as well as major acceleration of chip shipments. If you are preparing to try and impact how things go, now might be a good time to get ready and keep your powder dry. If you are looking to build cool new AI tech and capture mundane utility, be prepared on that front as well.</p><p>目录</p><ol><li>Introduction.</li><li> <a href="https://thezvi.substack.com/i/136374814/table-of-contents">Table of Contents</a> . Bold sections seem most relatively important this week.</li><li> <a href="https://thezvi.substack.com/i/136374814/language-models-offer-mundane-utility">Language Models Offer Mundane Utility</a> . Summarize, take a class, add it all up.</li><li> <a href="https://thezvi.substack.com/i/136374814/language-models-dont-offer-mundane-utility">Language Models Don&#39;t Offer Mundane Utility</a> . Not reliably or robustly, anyway.</li><li> <a href="https://thezvi.substack.com/i/136374814/gpt-real-this-time"><strong>GPT-4 Real This Time</strong></a> . History will never forget the name, Enterprise.</li><li> <a href="https://thezvi.substack.com/i/136374814/fun-with-image-generation">Fun With Image Generation</a> . Watermarks and a faster SDXL.</li><li> <a href="https://thezvi.substack.com/i/136374814/deepfaketown-and-botpocalypse-soon">Deepfaketown and Botpocalypse Soon</a> . Wherever would we make deepfakes?</li><li> <a href="https://thezvi.substack.com/i/136374814/they-took-our-jobs">They Took Our Jobs</a> . Hey, those jobs are only for our domestic robots.</li><li> <a href="https://thezvi.substack.com/i/136374814/get-involved">Get Involved.</a> Peter Wildeford is hiring. Send in your opportunities, folks!</li><li> <a href="https://thezvi.substack.com/i/136374814/introducing">Introducing</a> . Sure, Graph of Thoughts, why not?</li><li> <a href="https://thezvi.substack.com/i/136374814/in-other-ai-news">In Other AI News</a> . AI gives paralyzed woman her voice back, Nvidia invests.</li><li> <a href="https://thezvi.substack.com/i/136374814/china">China</a> . New blog about AI safety in China, which is perhaps a thing you say?</li><li> <a href="https://thezvi.substack.com/i/136374814/the-best-defense">The Best Defense</a> . How exactly would we defend against bad AI with good AI?</li><li> <a href="https://thezvi.substack.com/i/136374814/portents-of-gemini"><strong>Portents of Gemini</strong></a> . It is coming in December. It is coming in December.</li><li> <a href="https://thezvi.substack.com/i/136374814/quiet-speculations">Quiet Speculations</a> . A few other odds and ends.</li><li> <a href="https://thezvi.substack.com/i/136374814/the-quest-for-sane-regulations">The Quest for Sane Regulation</a> . CEOs to meet with Schumer, EU&#39;s AI Act.</li><li> <a href="https://thezvi.substack.com/i/136374814/the-week-in-audio">The Week in Audio.</a> Christiano and Leahy give talks, Rohit makes his case.</li><li> <a href="https://thezvi.substack.com/i/136374814/rhetorical-innovation">Rhetorical Innovation</a> . Some relatively promising attempts.</li><li> <a href="https://thezvi.substack.com/i/136374814/llama-no-one-is-stopping-this"><strong>Llama No One Stopping This</strong></a> . Meta to open source all Llamas no matter what.</li><li> <a href="https://thezvi.substack.com/i/136374814/no-one-would-be-so-stupid-as-to">No One Would Be So Stupid As To</a> . Bingo, sir.</li><li> <a href="https://thezvi.substack.com/i/136374814/aligning-a-smarter-than-human-intelligence-is-difficult">Aligning a Smarter Than Human Intelligence is Difficult</a> . Davidad has a plan.</li><li> <a href="https://thezvi.substack.com/i/136374814/people-are-worried-about-ai-killing-everyone">People Are Worried About AI Killing Everyone</a> . Roon, the better critic we need.</li><li> <a href="https://thezvi.substack.com/i/136374814/other-people-are-not-as-worried-about-ai-killing-everyone">Other People Are Not As Worried About AI Killing Everyone</a> . Consciousness?</li><li> <a href="https://thezvi.substack.com/i/136374814/the-wit-and-wisdom-of-sam-altman">The Wit and Wisdom of Sam Altman</a> . Do you feel lucky? Well, do ya?</li><li> <a href="https://thezvi.substack.com/i/136374814/the-lighter-side">The Lighter Side</a> . The big time.</li></ol><p> Language Models Offer Mundane Utility</p><p> <a href="https://causalinf.substack.com/p/my-new-syllabus-for-my-new-class?utm_medium=reader2">A class on the economics of ChatGPT, complete with podcast recording</a> . More like this, please, no matter my quibbles. I especially don&#39;t think survey courses, in economics or elsewhere, are the way to go. Focus on what matters and do something meaningful rather than try to maximize gesturing. If you let me teach students with other majors one economics class, teach them the basics of micro and then use that to explore what matters sounds like a great plan. So is getting students good at using LLMs.</p><p> <a href="https://ai.googleblog.com/2023/08/teaching-language-models-to-reason.html">Use algorithmic instructions to let LLMs accurately do tasks like 19-digit addition</a> .</p><p> <a href="https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper">Summarize writing</a> . It seems GPT-4 summaries are potentially more accurate than humans ones.</p><blockquote><p> We encountered two practical problems:</p><ul><li> Not following instructions. Bigger models were better at following instructions. We had to use another LLM to understand the outputs of the smaller LLMs and work out if it said A or B was the answer.</li><li> Ordering bias. Given A and B, are you more likely to suggest A simply because it is first? One way to test this is to swap the ordering and see how many times you say A both times or B both times.</li><li> Once we dealt with these problem we saw:<ul><li> <strong>Human:</strong> 84% (from <a href="https://aclanthology.org/P19-1213/">past research)</a></li><li> <code>gpt-3.5-turbo</code> <strong>:</strong> 67.0% correct (seemed to have severe ordering bias issues)</li><li> <code>gpt-4</code> <strong>:</strong> 85.5% correct</li><li> <code>Llama-2-7b</code> <strong>:</strong> Catastrophic ordering bias failure. Less than random accuracy</li><li> <code>Llama-2-13b</code> <strong>:</strong> 58.9% correct</li><li> <code>Llama-2-70b</code> <strong>:</strong> 81.7%</li></ul></li></ul></blockquote><p> If you do not so much value accuracy, the cost difference for Llama vs. GPT-4 is real.</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45d711bc-4f6c-4c6b-b43c-bcd9e05e072c_1183x394.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WLvboc66rBCNHwtRi/uboqmtleijyrntadldun" alt=""></a></p><p> If you need to do summarization at scale, that is a big difference.</p><p> What is unsettling is that all methods, including using humans, had at least a 15% error rate. Summaries flat out are not reliable.</p><p> Language Models Don&#39;t Offer Mundane Utility</p><p> <a href="https://twitter.com/WilliamAEden/status/1696380013514264852">A mix of both sides.</a></p><blockquote><p> William Eden: We all thought ChatGPT would kill search because it would be so much better than search</p><p> No, it killed search because all the top search results are utter gibberish auto-generated clones that make thinkpiece listicles read like Shakespeare</p></blockquote><p> I have had much better luck with search than this. I also notice I rarely do more open ended search – I usually know exactly what I am looking for. If I didn&#39;t know what I was looking for… I&#39;d ask Claude-2 or GPT-4, or Bing if it is recent. Or Twitter.</p><p> <a href="https://twitter.com/janleike/status/1695153523527459168">A new front in the jailbreak wars is coming</a> .</p><blockquote><p> Jan Leike (head of alignment, OpenAI): Jailbreaking LLMs through input images might end up being a nasty problem. It&#39;s likely much harder to defend against than text jailbreaks because it&#39;s a continuous space. Despite a decade of research we don&#39;t know how to make vision models adversarially robust.</p><p> An important open question is how well whitebox attacks on open source models transfer to larger proprietary models. (My prediction: it&#39;s possible to make them transfer reasonably well).</p><p> I wish more people were working on this.</p><p> Patrick Wilkie: What does it mean “its a continuous space”?</p><p> Jan Leike: You can make small changes to individual pixel colors without changing the image semantically, but you can&#39;t make small changes to a token/word without changing its meaning.</p><p> Jacob Valdez (reply to OP): Will you still release multimodal gpt4?</p><p> Jan Leike: GPT-4 is already jailbreakable.</p><p> Dan Hendrycks (reply to OP): “Only research text-based models if you care about safety” is a meme that will hopefully fade when multimodal AI agents emerge.</p></blockquote><p> As I&#39;ve noted before, I can think of things I would try, most of which have presumably already been tried and found not to work but you never know. In the past I&#39;d have said &#39;oh a decade of research so everything I think of that isn&#39;t deeply stupid has definitely been tried&#39; except no, that&#39;s definitely not how things work. A fun problem to explore.</p><p> An inside source tells me many psychiatrists <a href="https://www.washingtonpost.com/technology/2023/08/07/ai-eating-disorders-thinspo-anorexia-bulimia/">are rather upset this week over the Washington Post report</a> that if you explicitly ask LLMs to tell you ways to lose weight, they will tell you. And if you ask about a particular method that is not a good idea, it will warn you that it is not a good idea, but sometimes, especially if you jailbreak it first, it will still tell you the factual information you requested. Or, as they are calling it, &#39;encouraging anorexia.&#39;</p><p> The worst example remains <a href="https://www.nationaleatingdisorders.org/">the chatbot created for the National Eating Disorders Association help line</a> . Rather higher standards need to apply there. Whereas asking for universal full censorship of a large swath of information, and for information sources to respond to questions only with lectures, does not seem like a road we want to start going down as a standard, unless our goal is to shut such models down entirely.</p><p> Stop trying to get AI to write your generic articles without actual proper human editing, <a href="https://www.cnn.com/2023/08/30/tech/gannett-ai-experiment-paused/index.html">if you worry someone might read them</a> .</p><blockquote><p> CNN: In one notable example, <a href="https://web.archive.org/web/20230828215700/https://www.dispatch.com/story/sports/high-school/2023/08/19/worthington-christian-earns-narrow-win-over-westerville-north-in-ohio-high-school-boys-soccer-action/70634736007/">preserved by the Internet Archive&#39;s Wayback Machine</a> , the story began: “The Worthington Christian [[WINNING_TEAM_MASCOT]] defeated the Westerville North [[LOSING_TEAM_MASCOT]] 2-1 in an Ohio boys soccer game on Saturday.” The page has since been updated.</p><p> ……</p><p> As of Wednesday, several Dispatch sports stories written by the service had been updated and appended with the <a href="https://www.dispatch.com/story/sports/high-school/2023/08/18/westerville-north-escapes-westerville-central-in-thin-win-in-ohio-high-school-football-action/70627511007/">note</a> : “This AI-generated story has been updated to correct errors in coding, programming or style.”</p></blockquote><p> Eventually, can you reach a point where stupid mistakes won&#39;t happen, if all you want to do is tell variations of the same high school sports stories over and over? Sure, obviously this is a thing technology will be able to do. It is not crazy to iterate trying to get to right. Until then and while doing so, check your work, fools.</p><p> GPT-4 Real This Time</p><p> <a href="https://openai.com/blog/introducing-chatgpt-enterprise">ChatGPT Enterprise is here</a> , offering unlimited 32k-context-window twice-the-speed GPT-4, full data privacy. Pricing is &#39;contact sales&#39; and presumably worth every penny.</p><p> Perplexity fine tunes GPT-3.5 for their co-pilot, <a href="https://twitter.com/sama/status/1695107174035235083">surpasses GPT-4 quality in a week as judged by its users</a> . The first thing to notice here is that they did this so quickly, presumably because they had a unique data set ready to go. However the task here was co-pilot, so why wasn&#39;t a similarly good data set available for OpenAI? Why didn&#39;t we already have a much better version?</p><p> <a href="https://twitter.com/AnthropicAI/status/1696584597537165789">Whereas their Perplexity Pro research assistant is now using Claude-2</a> , taking advantage of the gigantic context window.</p><p> Why are we fine tuning on GPT-3.5 rather than fine tuning on GPT-4? I don&#39;t want the faster, cheaper as-good version, I want the superior version and yes I will wait. Is this because we were training GPT-3.5 on GPT-4 results that let it catch up, but it wouldn&#39;t help GPT-4 much? What would that imply?</p><p> Fun with Image Generation</p><p> <a href="https://twitter.com/GoogleDeepMind/status/1696494165658357894">Google introduces SythID to watermark images its models generate</a> .</p><p> <a href="https://twitter.com/fofrAI/status/1695921645393772827">Faster version of SDXL 1.0 is released, 20% improvement on A10s, 70% on H100s</a> .</p><p> Reason&#39;s Jennifer Huddleston says <a href="https://reason.com/2023/08/28/robots-arent-coming-for-movie-stars-yet/">Robots Aren&#39;t Coming For Movie Stars, Yet</a> . The movie stars agree. Like all things AI, the worry is that they will come for movie stars in the future, not that the current tech is sufficient now. Never find out your heroes are actually a (historically much more accurate than most alternatives) rock that says &#39;innovation is great and government does not work.&#39;</p><p> Deepfaketown and Botpocalypse Soon</p><blockquote><p> <a href="https://twitter.com/JeffLadish/status/1696315902705451192">Jeffrey Ladish:</a> Anyone have advice on how to generate good realistic deepfakes using a few images of a person? (eg of a person being arrested or posing with a person) Working on a demo and it would be useful to talk to people about this / be pointed in the right direction</p><p> bayes – e/acc: sounds dangerous</p><p> Jeffrey Ladish: yeah gotta be careful with some of this AI stuff might be risks idk.</p><p> bayes – e/acc: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/giffdbroph8owgkinvvj" alt="🤯"></p></blockquote><p> <a href="https://github.com/deepfakes/faceswap">Ah, face swap</a> and <a href="https://github.com/facefusion/facefusion">face fusion</a> . I am sure no one will use this for unethical purposes or inappropriate content, seeing as they asked so nicely.</p><p> 404 media continue to work their wheelhouse, <a href="https://www.404media.co/ai-generated-mushroom-foraging-books-amazon/">warn of AI-written mushroom guides</a> on Amazon whose errors might kill the reader. They provide three links, with a total of two reviews between them. The first thing I would do, before buying a book whose errors might kill me, would be to check the reviews. If I saw only one, and did not otherwise know about the author, I probably wouldn&#39;t buy the book.</p><p> I do realize things will get worse. But if this is what the scaremongers can point to, so far things are pretty much fine.</p><p> They Took Our Jobs</p><p> South Korea was fine with the robots taking our service jobs given their rapidly aging and declining population. <a href="https://twitter.com/scottlincicome/status/1695798351260750180">But the rules don&#39;t include protectionism, so now the foreign robots are taking the domestic robots&#39; jobs</a> . Next level stuff.</p><p> <a href="https://twitter.com/zenahitz/status/1695512197232042022">Zena Hitz warns that UWV getting rid of some of its study programs will kill those things within the state</a> , because who will be there to teach such things even at lower levels, and that this will spread. I do not understand why you need a degree in French to teach high school French, or a mathematician to teach high school math, and so on. The presumption that &#39;the parents of home schoolers went to school&#39; illustrates the perspective that school and formal education is the source of knowledge. I do not think this was true before and expect it to be less true quickly over time.</p><p> What I do buy in theory is Zena&#39;s claim that such programs can pay for themselves, since all they need are a few professors and buildings. The problem is that in practice there is no way out of the various other amenities and administrative costs involved. We would love to offer the package of &#39;buy the time of professors to teach you stuff&#39; if that could be sold separately, but we lack the technology to do that. Luckily &#39;buy the time of GPT-5 fine-tuned to be a tutor&#39; will be available, and our price cheap.</p><p> Get Involved</p><blockquote><p> <a href="https://twitter.com/peterwildeford/status/1695778034513522803">Peter Wildeford</a> : <a href="https://careers.rethinkpriorities.org/en/postings/8588ecc5-3e26-4086-bdb2-fe9a2eb43252">I&#39;m hiring for a new assistant</a> to work directly with me at making @RethinkPriors even better at our work. My last assistant was too talented and went on to a major research management role. You too could get mentorship from me and accelerate your career!</p></blockquote><p> Introducing</p><p> You&#39;ve seen Chain of Thought and Tree of Thoughts. <a href="https://arxiv.org/abs/2308.09687">Now meet Graph of Thoughts</a> .</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a0240ed-c625-47cc-be52-1ab45dbc192e_1182x1418.jpeg"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WLvboc66rBCNHwtRi/eejwbm0lcp1pt0vboa8z" alt="图像"></a></p><p> I mean, sure, why not?</p><p> <a href="https://pastors.ai/">Pastors.</a> ai, which lets you create a custom chatbot from a YouTube video of your church service, and create resources for use beyond that. The obvious question is, to what extent is this a skin on top of a general audio-to-chatbot service, and what is the best version of that service, or a text variant? What is doing the unique work? I will admit that I have yet to have the temptation to chat with a YouTube video, but give me the option and I think I&#39;ll figure out some uses, although I expect the first thing will be questions designed to figure out if I want to watch the video or not.</p><p> In Other AI News</p><p> <a href="https://www.ucsf.edu/news/2023/08/425986/how-artificial-intelligence-gave-paralyzed-woman-her-voice-back">How AI Gave a Paralyzed Woman Her Voice Back</a> . They hooked sensors up to the parts of the brain responsible for speech, trained it to recognize all the sounds in English, now they simulate her old voice at 80 words per minute.</p><p> <a href="https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm">Web scraping for me but not for thee</a> , all the major players are both scraping the entire web and suing to prevent anyone from scraping them in return.</p><p> <a href="https://twitter.com/asanwal/status/1694888907899359275">Nvidia is the biggest investor in AI unicorns</a> .</p><p> <a href="https://www.404media.co/ai-surveillance-tool-dhs-cbp-sentiment-emotion-fivecast/">DHS using AI-based sentiment analysis straight out of Inside Out</a> .</p><blockquote><p> Related to the emotion and sentiment detection, charts contained in the Fivecast document include emotions such as “anger,” “disgust,” “fear,” “joy,” “sadness,” and “surprise” over time. One chart shows peaks of anger and disgust throughout an early 2020 timeframe of a target, for example.</p></blockquote><p> System for now uses publicly available data, doing things like linking social security numbers to Facebook or Reddit accounts. 404&#39;s investigation does not say what DHS then does with this information. If it then uses this to draw human attention to such people for evaluation, then I do not see how we could hope to prevent that. If it is being used as an automatic evaluation, that is much worse. Either way, be under no illusions that the US government won&#39;t use all the tools available for its surveillance state, for all its purposes. If you don&#39;t want your info used, don&#39;t let them get their hands on it.</p><p> Ajeya Cotra notes that yes, <a href="https://www.planned-obsolescence.org/language-models-surprised-us/">ChatGPT and GPT-4&#39;s capabilities took us by surprise</a> .</p><p>中国</p><p><a href="https://aisafetychina.substack.com/about">There&#39;s a new blog about AI safety in China.</a></p><blockquote><p> AI Safety in China: Common perception: China doesn&#39;t care about AI safety.</p><p> Our perspective? China&#39;s more invested in AI safety and risk mitigation than many realize.</p><p> <strong>This newsletter aims to bridge the knowledge gap.</strong></p></blockquote><p> <a href="https://aisafetychina.substack.com/p/ai-safety-in-china-1">The first post is here.</a> I knew of all of the most important developments, this organizes them into one place and presents as a cohesive whole. After looking at this, one asks: If China cared a lot about safety, what would we see that we don&#39;t see here? If China did not care a lot about safety, would we still see the things we do see here?</p><p> The Best Defense</p><p> <a href="https://twitter.com/GaryMarcus/status/1696772042786533540">A common debate</a> is whether the only thing that can stop someone with a bad AI is someone with a good AI and proliferation can be handled, or whether offense is too much favored over defense for that to work. There is both the debate about mundane systems and risks, and future existential ones.</p><blockquote><p> Nora Belrose: I&#39;m opposed to any AI regulation based on absolute capability thresholds, as opposed to indexing to some fraction of state-of-the-art capabilities. The Center for AI Policy is proposing thresholds which already include open source Llama 2 (7B). This is ridiculous.</p><p> Gary Marcus: Shouldn&#39;t regulation be around the harms a system might cause? Why tie it to a relative determination? In principle you then could have a situation with successively more harmful systems being deregulated simply because more dangerous systems are invented.</p><p> Nora Belrose: The possible harm caused by the system is proportional to its <i>relative</i> power, not it its absolute power. AI can defend against AI. We can use AI to detect psy ops and fake news, and to make transmissible vaccines to protect against bioweapons, etc.</p><p> Gary Marcus: In principle I do think AI can defend against AI. But in practice it is very hard, and I doubt that any near term technology is up to the job. The best extant cases are maybe things like spam detection and malware detection. Neither are great. Prove I am wrong?</p></blockquote><p> I agree that offense is not automatically favored over defense, but that is definitely the way to bet.</p><p> In general, if you want to defend against a potential attacker, the cost to you to do so will vastly exceed the maximum resources the attacker would still need to succeed. Remember that how this typically works is that you choose in what ways you will defend, then they can largely observe your choices, and then choose where and when and how to attack.</p><p> This is especially apparent with synthetic biology. For example, Nora suggests in a side thread pre-emptive vaccine deployments to head off attacks, but it is easy to see that this is many orders of magnitude more costly than the cheapest attack that will remain. It is also apparent with violence, where prevention against a determined attacker is orders of magnitude more expensive than the attack. It is often said it takes an order of magnitude more effort to counter bullshit than to spread it,, and that is when things go relatively well.等等。</p><p> Why do we not see more very bad things? We have a punishment regime, and it is feasible to impose very high penalties on humans relative to potential benefits that one person is able to capture. Coordination is hard and human compute limits make it hard to properly scale, so humans remain at similar power levels to each other, and have strong egalitarian and enforcement instincts even when against direct interest. That sort of thing (among others).</p><p> Alas, I do not expect most of these properties to hold.</p><p> Even if they mostly do, it will indeed have to be the good human with an AI that defends against the bad human with an AI or the rogue AI. That means that the effective capabilities of the good human will have to keep pace with the unleashed capabilities of the bad AI. Which means handing the whole enterprise, and increasing amounts of effective control, over to the (we think it is) &#39;good&#39; AI, in furtherance of some goal. If we don&#39;t, we lose.</p><p> We also do not have an option to use a light touch and respect digital freedom and privacy, if we want to be proactively protective. If we let the bad human get an AI because the good human with an AI will stop them, then how are they going to do that in advance? That monitoring regime, again even in a world where we solved alignment and defense is viable, is going to if anything be even more intrusive.</p><p> To be concrete, suppose you do not want anyone creating deepfake pornography. You have three options.</p><ol><li> You can ban open source image generation models, and require all offered image generation models to block attempts to generate such images, and do the monitoring required to ensure no one violates these rules.</li><li> You can allow open source models, and monitor all people&#39;s internal data to look for such images in some way.</li><li> You can allow open source models, and accept that if people want to privately generate such images, you cannot stop them, except by occasionally noticing and extracting punishment.</li></ol><p> Is option one more or less violating of our privacy and freedoms than option two? Which one is likely to have larger side effects in other places? If there is a potentially unacceptable state that can be created on a private server, what to do, and how does this relate to someone building a nuke on their private property?</p><p> Portents of Gemini</p><p> It is coming. <a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini">Gemini will have 5x the pretraining FLOPS of GPT-4 by year&#39;s end</a> , 20x by end of the next year, to which my response is essentially that&#39;s it? I would have thought things were accelerating at least that fast and this is freaking Google.</p><p> I don&#39;t doubt Gemini will be a big deal and a game changer, but it seems as per these reports entirely expected, a return to normality, the type of progression you would expect. Yes, it will advance capabilities, and those not paying attention who think the whole LLM thing was a ho-hum will notice and freak out, but if you are reading this you had already priced all of that in, right?</p><p> <a href="https://twitter.com/JimDMiller/status/1696162678706774104">As James Miller notes</a> , seems wise to be overweight Google, as I have been for a long time, although not overweight enough, the same story as Microsoft and Nvidia.</p><p> <a href="https://twitter.com/eugeneyan/status/1695989500630159489">Another money quote from that post</a> :</p><blockquote><p> “HuggingFace&#39;s leaderboards show how truly blind they are because they actively hurting the open source movement by tricking it into creating a bunch of models that are useless for real usage.”</p><p> Ouch.</p></blockquote><p> I am confused how bad a measure it can be if its top model is GPT-4 followed at some distance by Claude and GPT-3.5, with another large gap to everything else, and I&#39;m using that leaderboard as the default evaluation for my Manifold markets, but worth noting that this in practice may be warped strongly in favor of open source models. They could be so much worse than we think they are. Which makes sense given I am never remotely tempted to use one of them for mundane utility.</p><blockquote><p> Gary Marcus, as he gloats about <a href="https://garymarcus.substack.com/p/the-rise-and-fall-of-chatgpt">his eleven-days old prediction</a> that &#39;ChatGPT will be a dud&#39;: Bets on whether Gemini hallucinates?</p></blockquote><p> My prediction is that, since it is multimodal, it will see double.</p><p> <a href="https://twitter.com/Simeon_Cps/status/1691155723583938561">A claim that GPT-5 might be trained this fall and deploy as early as December 23</a> . That would be quite the contrast with GPT-4, which not only was not trained so quickly, it was held in reserve for a full eight months while they did red teaming and verified it was not dangerous. <a href="https://manifold.markets/JohnFish/will-gpt5-be-released-to-the-public">The market, such as it is, seems skeptical</a> at 13%.</p><p> If OpenAI does release GPT-5 on that schedule, either they have been lying and they&#39;ve had it trained for some time, it is completely unworthy of its name, or they will have entirely abandoned the idea of doing even their own previous level of rudimentary safety precautions before releasing a new model. It would be a deeply negative update on OpenAI.</p><p> Quiet Speculations</p><p> <a href="https://www.thersa.org/cities-of-learning/ai-future-learning-deepmind-roundtable">AI and the Future of Learning</a> , from RSA and DeepMind. Didn&#39;t see anything new.</p><p> <a href="https://twitter.com/DavidSKrueger/status/1696282466057601088">David Kruger highlights from last week&#39;s paper</a> on the subject that the potential to be wrong about whether an AGI is consciousness is a strong reason to not build an AGI, because there is no safe assumption one can make. Both mistakes (thinking it is when it isn&#39;t, or isn&#39;t when it is) can cause immense harm.</p><p> <a href="https://twitter.com/sherjilozair/status/1697159633486672349">Questions that are not often enough asked</a> .</p><blockquote><p> Sherjil Ozair: if e/acc is about building, how come it&#39;s doomers who actually built agi?</p></blockquote><p> The Quest for Sane Regulations</p><blockquote><p> Techmeme: Sources: Elon Musk, Mark Zuckerberg, Sundar Pichai, Sam Altman, Jensen Huang, and others are expected to attend Sen. Schumer&#39;s closed-door AI forum on Sept. 13 (Axios).</p><p> <a href="https://twitter.com/krishnanrohit/status/1696941261507416253">Rohit:</a> I was not invited to this. Curious whether anyone else not on the payroll of a major tech company was.</p><p> <a href="https://twitter.com/amin_amou/status/1697006150401839294">Amin Amou</a> (other thread): The next regulatory meeting on climate change will be only with the CEOs of Exxon, bp, Shell, and Chevron to discuss the future of green energy.</p></blockquote><p> I certainly see the concern. It also seems like any reasonable exploration of AI regulation would include such a closed-door forum meeting with top tech people. It needs to not be the only meeting or only class of meaningful voices, but it is a necessary meeting, where such people can tell you things behind closed doors, and create common knowledge within such a group. It might even allow otherwise illegal collaboration to adopt safer procedures.</p><p> If it does create regulatory capture? That could be bad. It could also be good. I am far more interested in what is good for the world than what is good for particular companies. I also notice Huang, who is president of Nvidia rather than a particular AI company. Could some form of chip control or tracking be in the cards?</p><p> I will also bite Amou&#39;s bullet, despite the unfairness of the comparison. There should absolutely be one meeting, in the process of regulating carbon, where they bring in the CEOs of the fossil fuel companies and no one else. There should also be other meetings without them. Why is that weird?</p><p> <a href="https://www.piratewires.com/p/the-costs-of-europes-gdpr-regime">Brian Chau reviews the impact of GPDR and previews the potential impact of the EU&#39;s AI Act</a> . Open ended scope for lawsuits, fines and recalls, a regulatory state that can take huge (2%-4% of global revenue) money from any company at any time. What isn&#39;t a &#39;threat to democracy&#39;? APIs are effectively banned, Lora is effectively banned, which he notes will be a big issue for open source projects.</p><p> What he does not point out, but that seems pretty obvious, is that anything that effectively bans an API also bans open source outright. An open source project has to register its anticipated capabilities and every deployment of new code, it has to be capable of a recall on demand, and it has to prevent malicious uses. That&#39;s why the API is potentially de facto banned.</p><p> What would happen if Meta releases Llama-3, and every time anyone uses a fine-tuned version of it that stripped out its alignment in two days to say something racist or spread misinformation, they fine Meta 2% of global revenue? When they go after GitHub? You think they wouldn&#39;t do that?</p><p> Those who say it cannot be done should not interrupt the one doing it. Stop telling me things that have already happened in one of the world&#39;s three major blocks cannot possibly happen. Rather than spend time denying that we could, perhaps it would be better to instead debate whether (and how) we should.</p><p> The Week in Audio</p><p> <a href="https://twitter.com/NPCollapse/status/1695738289846693973">New Paul Christiano talk</a> ( <a href="https://simons.berkeley.edu/talks/paul-christiano-alignment-research-center-2023-08-16">direct link</a> , <a href="https://www.youtube.com/watch?v=u0619QrWxQc&amp;ab_channel=SimonsInstitute">YouTube</a> , 1 hour).</p><blockquote><p> Connor Leahy: Seems Paul Christiano is now working on something like “informal logical inductors/probabilistic proof systems”, a formalization of heuristic arguments. Cool!</p></blockquote><p> Talk was technical, likely too technical for most readers. I found it interesting. I am not sure what I would do with many of the results Paul is looking for if I found them, or how they give us a route to victory. They do still seem cool and potentially useful.</p><p> <a href="https://www.youtube.com/watch?v=wKI9hmaIbpg&amp;ab_channel=Conjecture">Connor Leahy 20-minute talk from the FLI Interpretability Conference at MIT</a> , on the dangers and difficulties of interpretability. A key point is that much of the cognition of an AI, or of a human, takes place outside the AI or human brain itself.</p><p> <a href="https://open.spotify.com/episode/573LqvTquOk78Ui5gkF5WM">Rohit Krishnan goes on Bretton Goods to explain why he thinks AI won&#39;t kill us</a> . <a href="https://twitter.com/rgblong/status/1695129293997871256">Robert Long found it good and interesting</a> , Tyler Cowen linked to it.</p><p> I listened so you don&#39;t have to, but failed to hear any arguments for why AI wouldn&#39;t kill us.</p><p> It was more like there wasn&#39;t anything Rohit felt we could usefully do with the claim that AI would kill us, and no way to predict what future systems will or won&#39;t be able to do, and we don&#39;t know how anything involved works, straight lines on graphs aren&#39;t reliable or meaningful and you can&#39;t prove a negative. Nothing is yet concrete in its details and when humans see problems we fix them. Therefore might as well treat the threat as not being there, or something?</p><p> It was a weird listen. Rohit (non-exclusively) makes lots of true and useful statements. Except that none of them seem to me like reason to not worry. Mostly they seem like incremental reasons to worry more. If for example future systems will be totally and unpredictably unlike present systems (31:10), why should that make us worry less?</p><p> Meanwhile he also makes a lot of statements that are bizarre strawman representations of others&#39; positions and assumptions, as well as many arguments I don&#39;t see as valid.</p><p> Most glaring is that he says others are assuming that future systems will be more capable or bigger but otherwise identical copies of current systems. We don&#39;t.</p><p> Perhaps the core issue is the whole concept of the self-correcting system (33:05). When you have a technology that is purely a tool and lacks a mind of its own, that we can iterate on when something goes wrong, and humans are exerting all the optimization pressure, we can assume that problems will be self-correcting. Thus, Rohit perhaps assumes we must think these systems will otherwise not change, because any changes would obviously make things better on every dimension.</p><p> Whereas in the case of sufficiently capable AI, we have the opposite situation, where many types of problems are self-protecting and self-amplifying, and yes our currently working solutions predictably stop working. That&#39;s not to say that it is impossible we will find superior solutions that do work, but Rohit is himself arguing that he does not know how to go look for them. That does not seem great, nor a reason to not worry.</p><p> Or perhaps it&#39;s as simple as &#39;That&#39;s the problem. Let&#39;s not jump to the future&#39; (33:25). The problems we are talking about lie in the future. Saying &#39;we don&#39;t have that problem now&#39; is not a response. Few if any people are making the assumptions of otherwise identical systems Rohit is suggesting, nor do anyone&#39;s threat models depend on it.</p><p> Rohit repeatedly emphasizes that systems will only get deployed if they solve real-life problems (eg 27:15), so he says only sufficiently aligned or interpretable systems will get deployed. Yes deployment for some particular use cases can be blocked by such concerns, but it seems impossible that sufficiently dangerous systems will stay contained because they do not offer any usefulness.</p><p> To quote at (35:30), he says &#39;if the current systems have problems they are not deployed.&#39; That is completely, utterly false. Bing, if we may recall the stories. Bard, that still spits out nonsense on the regular.聊天GPT。 Llama-2. All these systems exhibit severe failure modes. All these systems are, today, trivial to jailbreak if you care sufficiently. If we deploy superintelligent systems about this flawed, we die. This argument does not make any sense to me. At (36:25) Rohit says that prompt injection and hallucination are problems we need to solve before we get widespread deployment. Except we have both problems in spades, and we have widespread deployment happening at the speed we can manufacture GPUs. What the hell? He then says in parallel &#39;if GPT-5 shows up with new problems, we will have to solve them before we get to GPT-6.&#39; (37:10). Except, once again, we have very much not solved GPT-4&#39;s problems. Does anyone think that fact will stop OpenAI from creating GPT-5? Rohit later claims (50:25) that we have mostly solved our problems for now, and no? Reduced the practical size of the problem when no one is trying very hard sure, solved them very much no.</p><p> Throughout Rohit presumes that the future systems only get deployed where and for the purposes we choose to intentionally deploy them. Which does not exactly assume away the problem, we can absolutely get killed without violating that assumption, but it&#39;s a hell of an assumption when you think about it. See (43:35) for example, the idea that AI is deployed only to &#39;low risk&#39; situations so none of the current deployments count, or something? We have these things writing a good percentage of our code, and we are turning them into agents hooked up to the open internet. This is not a low risk situation. Bridgewater using an AI while consumer banking recommendations do not (44:15) is not a protective strategy. Blowback companies will face is not a good proxy for existential danger.</p><p> I am also continuously baffled by &#39;our society&#39;s checks and balances will contain the problem&#39; when the problem is an advanced AI system. Saying &#39;a sufficiently powerful system might be able to come up with loopholes that overcome the thousand human reviewers&#39; as is said around (29:55) is not a remotely good description of the problem here. This is absurd.</p><p> One should note that Rohit&#39;s positions are a very strong argument for a hard ban on open source models, at least above a threshold of capabilities. The argument here is, as I understand it, that commercial or political downside risks will cause all of our actors, all of whom are highly responsible, to only deploy sufficiently tested, safe systems. Which, I mean lol, but even if that was true it would depend on all those actors wanting to act responsibly. It all depends on only large players being relevant. If you instead open source something competitive with that, then there will be those who aim to misbehave, and many more that will play fast and loose, and that pressure will in turn disallow responsible behavior.</p><p> I also am increasingly sick of people not new to the concepts involved calling existential risk from AI a &#39;science fiction story&#39; (36:03) as a means of dismissing the idea that if we create something more intelligent and more capable than us, that it might pose a threat to us. Or saying it &#39;requires so many assumptions&#39; which he also says. These are at best <a href="https://www.lesswrong.com/tag/semantic-stopsign#:~:text=A%20semantic%20stopsign%20or%20curiosity,the%20search%20for%20truth%20prematurely.">semantic stop sign</a> s.</p><p> I do appreciate Rohit&#39;s expression of confusion (47:30) on instrumental convergence, where he notices that the explanations he is hearing all sound like a stupid person trying to convey something they don&#39;t understand. We need more of that energy, and those of us who do understand this problem need to do a better job explaining it.</p><p> He closes by discussing regulation (58:45), saying that his instinct is to oppose all regulations on the principle that regulations tend to be bad, that even calling for evaluations would too much favor the big companies (despite it seeming to me as if all his arguments rely on the safety inclinations of big companies carrying the day) but maybe it would be fine. It is clear what he would think about something more. Shrug.</p><p> If I were to have a slow and friendly discussion or podcast with Rohit, there are a lot of potential places I could attempt to make progress or have a useful discussion, and I anticipate I would learn in the attempt, but that I would be unable to convince him. If your core position is strongly held that anything not already concrete cannot be considered until it is concrete, then you will be unable to think about the problem of AI existential risk.</p><p> Rhetorical Innovation</p><p> <a href="https://michaelnotebook.com/oppenheimer/index.html">Michael Nielsen offers prefatory remarks on &#39;Oppenheimer&#39; to AI folks.</a> Short and excellent.</p><blockquote><p> I was at a party recently, and happened to meet a senior person at a well-known AI startup in the Bay Area. They volunteered that they thought “humanity had about a 50% chance of extinction” caused by artificial intelligence. I asked why they were working at an AI startup if they believed that to be true. They told me that while they thought it was true, “in the meantime I get to have a nice house and car”.</p><p> This was an unusually stark and confronting version of a conversation I&#39;ve had several times. Certainly, I often meet people who claim to sincerely believe (or at least seriously worry) that AI may cause significant damage to humanity. And yet they are also working on it, justifying it in ways that sometimes seem sincerely thought out, but which all-too-often seem self-serving or self-deceiving.</p></blockquote><p> <a href="https://twitter.com/binarybits/status/1696119047694188593">Daniel Eth responds to a previous Timothy Lee post, Timothy considers it thoughtful.</a></p><p> <a href="https://twitter.com/nearcyan/status/1695218418079666427">A clown, putting on makeup</a> .</p><blockquote><p> Flo Crivello: “AGI isn&#39;t possible”</p><p> “And if it was, it wouldn&#39;t be near”</p><p> “And if it was, it wouldn&#39;t be dangerous”</p><p> “And if it was, we could use AGI to fight AGI”</p><p> “And if we couldn&#39;t, then there&#39;s nothing we could do about it anyway”</p><p> “And if there was, the cure might be worse than the disease”</p><p> Nearcyan: “there&#39;s nothing we can do about it. it&#39;s not actually possible to coordinate or solve problems when the individuals have incentives to defect” me when someone asks me to stop eating all the cookies and save some for everyone else.</p></blockquote><p> It would be more convenient if we passed through the stages cleanly, so you didn&#39;t have to fight all of them at once. Alas, a more accurate version of the standard progression is that first they ignore you, then the ignore and laugh at you for different values of they, then they ignore and laugh at and fight you for different values of they (you are here), then the ratio slowly shifts over time, then either you win or (in this case everyone dies). You don&#39;t get to declare levels cleared, ever.</p><p> When viewed from above, it is a relatively fair progression. Yes, there is some of &#39;perhaps we could have done something about it before but it is too late now&#39; but not very much of it. Mostly the argument is &#39;there was never anything anyone could do, even in theory&#39; because coordination and cooperation are either impossible or actively terrible. Those two are linked as well. It is exactly because it is so difficult that any attempts that partly succeed pose so much danger of making things worse.</p><p> In many other situations, such as most of those involving Covid, I would be one of those making exactly the claim that strong coordination as it would be implemented in practice would be some combination of impossible and worse than the disease.</p><p> Which I would also agree with here, except that I have seen the disease. The disease kills everyone and wipes out all (from-Earth) value in the universe. So I am going to say that no, the cure is not worse than the disease. And if this is an impossible (in the game difficulty sense) problem, then we have to <a href="https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible">shut up and do the impossible</a> . Or, if you have a less impossible alternative pathway, I am all ears. So far I have not heard one.</p><p> <a href="https://twitter.com/sebkrier/status/1694777759510196711">A satirical approach is attempted</a> .</p><blockquote><p> Seb Krier: I understand British Gas will publish an ArXiv paper claiming that their new refineries could cause massive explosions and may require safety testing. Do NOT believe them – this is a very clever marketing ploy to sell more gas and divert attention from gas leaks, the real harm.</p><p> Davidad: The steelman is that many people will predictably react to learning that AI extinction risk is real the way that Snoop Dogg did: “do I need to invest in AI so that I can have one?”</p><p> Seb Krier: There are def plausible reasons – but ultimately wouldn&#39;t be my preferred hype strategy.</p></blockquote><p> <a href="https://twitter.com/ESYudkowsky/status/1695871950852403415">Eliezer Yudkowsky outlines one of the biggest problems</a> .</p><blockquote><p> I&#39;m not currently sure what to do about the basic point where (while one large subgroup quickly gets it, to be clear) there&#39;s a subgroup of “But how could a superintelligence possibly hurt us?” askers who:</p><p> 1: Come in with a strong bias against hearing about any technology that doesn&#39;t already exist. They want to hear a story about humanity being wiped with 2020s tech, because tech more advanced than that feels like a fairy story to them.</p><p> 2: Want to hear concrete chess strategies that will beat them, rather than being told about abstractly superior chessplayers. They don&#39;t come in with a prior concept of “The best strategy <i>you</i> can think of is predictably an underestimate.”</p><p> 3: Allocated 6 minutes total to the conversation, 1 minute to the question “How will AI kill us?”, and start to glaze over if you talk for longer than 30 seconds.</p><p> The combination of 1-3 is a genuinely hard expository problem and I&#39;ve been struggling with it.</p><p> “Tell them about <i>understandable</i> stuff like how an AI could run people over with robotic cars!” Skeptical or attentive listeners will notice that this is an invalid story about how a superintelligence could successfully take over and get all the resources. It <i>does not make valid sense as a complete coherent story</i> because an AGI could not wipe out all of humanity that way; and wouldn&#39;t have replacement power plants or factories to build a human-independent economy if it could wipe out humanity that way.</p><p> (If we&#39;re playing on the playing field of invalid arguments, stories that don&#39;t really make sense when you stare at them, I expect that my opponents win that battle because they have more practice and they get to tell more attractive and appealing invalid stories once people are just making up whatever. Or so I rationalize my deontology about using only valid arguments myself.)</p><p> Concrete takeover stories are elementary if you start out with the knowledge and the way of thinking where, for example, algae are solar-powered self-replicating factories with a core general assembler, the ribosome, that accepts digital instructions and can also build more general chemical products via indirect synthesis pathways catalyzed by proteins; plus, the understanding that evolved proteins are very weak chemically for systematic reasons. Then it&#39;s obvious in one step to imagine the superintelligence taking over the algae factory system and improving it, rather than taking over human factories. If “algae” are a kind of green water-moss that doesn&#39;t feel like a factory, if the speaker starts out not knowing what a ribosome is (general assembler that accepts digital instructions) and also came in expecting to hear a story about robotic-ish minds doing specific modern-day-computery things that wipe out humanity, I&#39;m not sure how to cross this gap in the amount of time some people want to give me.</p><p> Daniel Bottger: I think you need to get them to take the ASI perspective. Have you tried parenting analogies? All parents have seen a kid try to hide information from parents, and try to control them (“you can&#39;t go through there, there&#39;s lasers”) and fail due to the intelligence difference.</p><p> Eliezer Yudkowsky: They&#39;re not coming in with a brain that can be simply instructed “imagine something else being smarter than you the way a parent is smarter than a kid”. I expect it fails on “Huh? I&#39;m not a kid!” and indignation about this person calling them low-status.</p><p> They also don&#39;t come in with a simple instruction for how to imagine an ASI as having a perspective where it&#39;s actively looking for ways to defeat humanity that humanity couldn&#39;t counter, and then, actually turn their brain that way. The ASI is imagined to be mechanical, or nerdy-college-professor book-smart &#39;intelligent&#39;; English doesn&#39;t have a simple word, and lots of people don&#39;t have a simple pre-existing concept, that you can pull as a lever to say, “Take on the adversarial perspective and actively search from inside it.” That&#39;s why security mindset is proverbially hard to teach if someone wasn&#39;t born with it.</p></blockquote><p> I mean, yeah. It&#39;s a hard problem. The replies illustrate how bad it is out there.</p><p> My go-to in these situations is more or less: AI makes money. AI pays people to work for it. Those people make it more money and make it impossible to shut down. It hires more people. They create the tools it needs to not need them anymore. If you say no, it finds someone else who says yes.</p><p> Because that would obviously work and really shouldn&#39;t be something you can challenge successfully, and if they start arguing I can always have the AI up its game at any point as this is a dramatically non-optimized plan.</p><p> But I agree, success rates are not so high. The obvious response to such plans is &#39;but we would notice and fight back.&#39; To which the actual response is &#39;maybe we would notice, but if we did notice we would not fight back in any effective or meaningful way even if the AI wasn&#39;t actively stopping us from doing so, which it would be.&#39; Still, Eliezer&#39;s desire is to sidestep that whole argument, since the AI would indeed sidestep it by doing something smarter, but then people say that the smarter thing isn&#39;t possible (eg nanotech).</p><p> <a href="https://twitter.com/nearcyan/status/1696366262530548044">A serious problem.</a></p><blockquote><p> Nearcyan: The issue with doomerism is not that it is incorrect, but that it is rarely actionable in a positive way building things which are better than what we have now is usually a more promising path than stopping or destroying the things which are sub-par.</p><p> two valid critiques to this: 1) doesn&#39;t apply to x-risk. if everyone is going to die, no amount of building helps you 2) doesn&#39;t work in areas with high ratio of ability to attack v. defend, eg synthetic biology pathogens. we need more defense, but attacking seems much easier.</p><p> I say this as a (partial) doomer myself – there&#39;s many bad things in the world I wish I could stop, but even if I dedicated my life to stopping them, I would not make nearly as much of an impact as I could simply by creating a few good things instead.</p></blockquote><p> This is indeed extremely frustrating. The good news is that it is not true.</p><p> There is indeed one particular subset of things that it is very important that we not build, or at least not build yet. Which are foundation models and other core enhancements to the path of AI capabilities.</p><p> For pretty much everything else, including ways for AI to provide better mundane utility, it is and remains time to build.</p><p> If you want to build something that impacts the problem directly, you can at least sort of build, by doing technical work, or by building institutions and understanding, or by building philosophical understanding, or other similar things. I do realize this is an imperfect substitute, and that &#39;build a regulatory framework&#39; or &#39;build a coalition&#39; is not satisfying here.</p><p> If you want to actually build build something, however, I once again remind everyone that you can do that as well. For example, you can build an apartment building or a wind turbine, <a href="https://archive.is/1cYIj">or an entire new city 60 miles outside San Francisco</a> .</p><p> I am deadly serious. We need to live in a sane world, where people can envision a positive future, and where they have the resources to breathe, to think and to raise families. We need <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect">something to protect</a> , lest people fall into despair and desperation and insanity. Help more people have that. You got to give them hope.</p><p> The best part is that if it turns out there was no threat, all you did was help make the world a better place. That seems like an acceptable risk.</p><p> At some point, of course, someone is actually going to have to do the direct thing. But it is not everyone&#39;s job. Some of you should do one thing, and some of you should do the other.</p><p> <a href="https://twitter.com/AISafetyMemes/status/1696608561357345167">A quote</a> .</p><blockquote><p> AI Safety Memes: “I don&#39;t think a species smarter than me could outsmart me, and I&#39;m willing to stake the life of every man, woman and child on it” – Guy who shouldn&#39;t be allowed to make that decision</p></blockquote><p> Llama No One Is Stopping This</p><p> <a href="https://twitter.com/tszzl/status/1695477459360120964">No, it&#39;s not going to stop, till you wise up</a> .</p><blockquote><p> Jason: Overheard at a Meta GenAI social:</p><p> “We have compute to train Llama 3 and 4. The plan is for Llama-3 to be as good as GPT-4.”</p><p> “Wow, if Llama-3 is as good as GPT-4, will you guys still open source it?”</p><p> “Yeah we will. Sorry alignment people.”</p></blockquote><p> (To see how likely they are to pull this off, we have manifold markets for whether they do this for <a href="https://manifold.markets/ZviMowshowitz/will-llama3-be-as-good-as-gpt4">Llama-3</a> by EOY &#39;24 and <a href="https://manifold.markets/ZviMowshowitz/will-llama4-be-open-sourced-and-as">Llama-4</a> by EOY &#39;25).</p><blockquote><p> Jason: Didn&#39;t expect this tweet to go viral. Not sure if it went viral in a good or bad way for meta. Maybe they won&#39;t invite me to their socials next time lol</p><p> Fact check update: I checked with someone else who was in the conversation, and that person thought it wasn&#39;t said in such a savage way. So my apologies for the color. But the content is still correct I believe. Another update with very low confidence about truthfulness: i also heard that someone else who wasn&#39;t at the event said they had a similar conversation</p><p> Roon: My two cents its not inherently bad to open source models of various capabilities levels. I think an open source GPT4 would be more or less fine. Its bad when you don&#39;t understand what you&#39;re releasing</p><p> Eliezer Yudkowsky (QTing OP): Translating and rebroadcasting this message for political actors in case you missed it: “Regulate this industry or everyone will die, because we won&#39;t unanimously regulate ourselves.”</p></blockquote><p>是的。</p><p> What about what has been rendered so far with Llama-2?</p><p> Llama-2, out of the box, seems unimpressive overall as far as I can tell, and is especially bad at coding. Given <a href="https://twitter.com/danielgross/status/1694730793174569415">the refusal to release Unnatural Code Llama</a> , the coding deficit was partly intentional.</p><p> Meta is crazy, but they&#39;re not stupid, they know not to give out such capabilities, and they made sure to align the model super hard.</p><p> Of course, a few days later some fine open source folk had completed the task of undoing all the alignment on Llama 2.</p><p> If you release an open source model, you are releasing, with a two day delay, the fully unaligned version of that model, that is fine tuned to do whatever the user wants. No one has even proposed in theory, to my knowledge, a way to avoid this.</p><p> Also, if you refuse to make your model good at coding, or avoid giving it some other particular capability, that too gets fixed, <a href="https://twitter.com/DrJimFan/status/1695503308268740808">although it might take a few weeks rather than a few days</a> .</p><blockquote><p> WizardLM: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/yWtJL6kPnLRxiKp2B/bi4s6poz6a7rem0xgnu3" alt="🔥"> Introduce the newest WizardCoder 34B based on Code Llama. </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅"> WizardCoder-34B surpasses [old version of] GPT-4, ChatGPT-3.5 and Claude-2 on HumanEval with 73.2% pass@1. <a href="http://47.103.63.15:50085/">Demo here</a> , <a href="https://t.co/3jrdUMYPFz">Model weights here</a> , <a href="https://t.co/AY7ECXenfT">Github here</a> . 13B/7B versions coming soon.</p><p> <strong>*Note:</strong> There are two HumanEval results of GPT4 and ChatGPT-3.5: 1. The 67.0 and 48.1 are reported by the official GPT4 Report (2023/03/15) of OpenAI. 2. The 82.0 and 72.5 are tested by ourselves with the latest API (2023/08/26).</p><p> <a href="https://twitter.com/paulg/status/1695197772448686507">Paul Graham</a> : <a href="https://t.co/oqCF9ZmZEG">Phind finds fine-tuned CodeLlama-34B beats GPT-4</a> . [Including this to point out that it was easy to make this mistake, of comparing to the old benchmark scores of GPT-4 rather than its new much higher ones.]</p><p> Jim Fan: This is basically an open version of the Unnatural Code Llama.恭喜！ While the benchmark numbers look good, HumanEval only tests a narrow distribution and can be overfitted. In-the-wild performance is what truly matters. Coding benchmarks need a major upgrade.</p></blockquote><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ed26cc5-575e-43b7-8244-595f197fd717_492x727.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WLvboc66rBCNHwtRi/axmypz7gseogjivz2hq8" alt="图像"></a></p><p> This puts WizardCoder well aboove Unnautral Code Llama, and at the level of current GPT-3.5 or Claude-2. This is consistent with the &#39;GPT 3.4&#39; evaluation of the general model, now without a particular weakness for code. As Jim Fan says, real world experience will tell the story. My expectation is that models like this will relatively underperform in practical applications versus on benchmarks, and that in a real world test I would bet on GPT-3.5 against it.</p><p> It makes sense that Meta is bad at fine tuning tasks including alignment, such that the open source community can do far better pretty much right away. Whereas the task of training the base model involves large expense, so the fact that they are also bad at that (relative to OpenAI or Anthropic) leaves them still doing OK there.</p><p> Would an open-source, up-to-date (and two days later completely unaligned to anything other than a given user) GPT-4 be fine? I think that too is an interesting question, if things did indeed stop at GPT-4-level plus the resulting fine tuning.</p><p> You would certainly see large gains in ability to efficiently do a variety of mundane harms. No doubt a lot of people would shoot themselves in the foot in various ways. We would get a real-world test of the fact that Claude-2-unaligned is able to provide detailed blueprints and practical instructions on production of biological warfare agents. The world might quickly become a far stranger, less relaxing place.</p><p> What this almost certainly would not do is end the world. On its own, all of this is fine, and pretty much inevitable. I know people who think we need to not only not open source a GPT-4 system but that also call for destroying the one we already have, and I am very much not in that camp. If you offered me (an actually globally enforced) deal that we open source exactly GPT-4, and in exchange no models more powerful than that ever get open sourced, I would happily take that deal, and expect to be in a much better position going forward than we are now.</p><p> Alas, instead, if we open source GPT-4-level now that builds more support for open source work, and pushes forward the open sourcing of a GPT-5-level system later, which is when the real not-fun plausibly starts, and of the models that will ultimately be powerful enough to, once proliferated, either kill us or place us into an unwinnable world state where we cannot prevent runaway AI-led competition that we will fail to survive. Where I think GPT-5 is an underdog to count for that, but not a large enough underdog for me to be comfortable with that.</p><blockquote><p> <a href="https://twitter.com/ESYudkowsky/status/1695487935527928197">Eliezer Yudkowsky</a> : As ever and as usual, the problem is not that GPT-4 kills everyone but that they&#39;ll just keep doing this as AGI tech goes on advancing. If you saw a QT or reply arguing against the obvious straw, “lol they think GPT-4 kills everyone” remember that this was the strongest counter they could think of and it&#39;s obvious straw.</p><p> Actual concerns about what happens if AGI keeps improving are uncounterable except by straws, angry dismissals, sheer ignorance of existing analysis, and blatant hopium that doesn&#39;t stand up to 30 seconds of informed counterargument. What you see is what there is, with your kids&#39; lives on the line.</p><p> Regulate now and aggressively; getting all the AI chips into internationally regulated and monitored datacenters will be a lot harder if you let this run long enough for lots of unregulated chipmakers to spring up under a lax regime. You should shut it all down, but if you&#39;re not going to do that, build *the shutdown button* before it&#39;s too late.</p></blockquote><p> <a href="https://twitter.com/soundboy/status/1695019835300667545">This also seems highly relevant.</a></p><blockquote><p> Ian Hogarth (head of UK AI foundation model taskforce): Someone recently described &#39;open sourcing&#39; of AI model weights to me as &#39;irreversible proliferation&#39; and it&#39;s stuck with me as an important framing. Proliferation of capabilities can be very positive – democratises access etc – but also – significantly harder to reverse.</p></blockquote><p> <a href="https://twitter.com/sherjilozair/status/1694980320389382501">But just think of the potential!</a></p><blockquote><p> Sherjil Ozair: The billion dollar use-case I&#39;m excited about is on-prem coding assistant to 10x developer productivity on proprietary codebases and knowledge graphs.</p><p> The common goods value of open source LLMs is so high, it matches operating systems, programming languages, and database systems. The future is looking more and more like open source base models with product-specific fine-tuning.</p></blockquote><p> Now hear me out, how about closed source LLMs with fine-tuning on proprietary codebases and knowledge graphs.</p><p> I agree that fine-tuning on your particular stuff is a great idea and people should totally be doing that. I don&#39;t see why this has to involve open source. If you want to do something for a large commercial enterprise, why can&#39;t you fine-tune on GPT-4? The three issues I can come up with are that it will cost too much, that you are worried they will stop supporting you, or that you are worried about data security. The cost issue seems worth it to get the improved performance and additional help an OpenAI or Anthropic can provide. The security of availability seems like something that could be credibly provided for, as can security of your data. You store everything on a cloud instance, where you don&#39;t get to see the model weights and they don&#39;t get to see your data either, and checks are made only to ensure you are within terms of service or any legal restrictions.</p><p> No One Would Be So Stupid As To</p><p> <a href="https://twitter.com/tegmark/status/1696169860202377278">It was inevitable that this section would include the Department of Defense</a> .</p><blockquote><p> Max Tegmark: AI accelerationists: Nobody would ever build AI to kill people.</p><p> DoD: LOL <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/B7duehMp2mSvffu2T/bt0sbblvwcw7vuhaevcu" alt="🤣"></p><p> Deputy Secretary of Defense Kathleen Hicks: We&#39;re making big bets- and we are going to deliver on those bets. First up: we&#39;re going to field attritable autonomous systems at a scale of multiple thousands, in multiple domains, within the next 18-to-24 months. Together, we can do this. @NDIAToday #EmergingTechETI</p></blockquote><p> They spent so much time wondering whether they could, and laughed when asked whether they should. They totally would, and they totally will.</p><p> <a href="https://twitter.com/fofrAI/status/1695170807956185428">Actually, everyone would.</a></p><blockquote><p> Fofr: “Agents are the Hello World of the AI Engineer”</p><p> Swyx: The best startup AI Engineers I&#39;ve met are <i>all</i> building their own agents. I know it&#39;s a buzzword that&#39;s now a bit past the wave of peak hype (thanks <a href="https://twitter.com/zachtratar">@zachtratar</a> ), but I can think of no better way to immediately, viscerally, run into all the SOTA problems of Prompt Engineering, RAG, Evals, Tool use, Code generation, Long horizon planning, et al. You don&#39;t even have to try to build an agent company. Just build one that does something you want done a lot. Its basically a rite of passage like how every Jedi needs to construct their own lightsaber. Agents are the Hello World of the AI Engineer.</p><p> [ <a href="https://www.latent.space/p/agents">links to post about how to build one here</a> ]</p><p> From April 19: Fortunately the @OpenAI strategy of building in safety at the foundation model layer has mitigated the immediate threat of paperclips. Even when blatantly asked to be a paperclip maximizer, BabyAGI refuses. Incredibly common OpenAI Safety Team W.</p></blockquote><p> Reminder that you cannot build in safety at the foundation model layer if the model is open sourced. That safety will get trained out of it in two days.</p><p> <a href="https://twitter.com/bindureddy/status/1697073829540155426">Why should humans or human-written code write the instructions to AIs anyway?</a></p><blockquote><p> Bindu Reddy: Working on a LLM that will craft your prompts for you – so you don&#39;t have to worry about this whole “prompt engineering” thing.</p></blockquote><p> <a href="https://twitter.com/AviSchiffmann/status/1696956384536080881">Hell, why should we check the code the AI writes? Run, baby, run.</a></p><blockquote><p> Avi Schiffmann: we need an AI compiler. straight prompt ->; machine code. future “developers” will just write high level natural language prompts &amp; move architecture diagrams around. Current syntax is irrelevant if you&#39;re no longer looking at your code.</p><p> Only enterprise-level companies will actually write human code. but your average hobbyist that just wants to make something won&#39;t care, they just want abstraction. this is the future of developer abstraction companies like vercel.</p></blockquote><p> Aligning a Smarter Than Human Intelligence is Difficult</p><p> <a href="https://www.lesswrong.com/posts/mnoc3cKY3gXMrTybs/a-list-of-core-ai-safety-problems-and-how-i-hope-to-solve">Davidad writes how his Open Agency Architecture (OAA) alignment plan would solve what he sees as the core problems in AI safety</a> .</p><p> On a high level, <a href="https://twitter.com/Simeon_Cps/status/1695508668458959268">I want to agree with Simeon</a> and say this is great. Identify what you see as the key problems (or perhaps in the future use a standard list, with your own additions) and explain how you intend to solve each of those problems, or why that problem will not stop your particular proposal.</p><p> Or perhaps you can say, if you think it is the relevant response &#39;I don&#39;t think this one is real in this context, I am ignoring it, if you disagree then you will think this won&#39;t work.&#39;</p><p> Then either way, we can respond accordingly, and identify key disagreements.</p><p> On a technical level, my response to the various strategies is a mix of &#39;that does not seem to make the problem easy but in theory that could perhaps work if we indeed were under very little optimization pressure and could afford epic alignment taxes&#39; and &#39;I do not see how this could possibly work.&#39; There is much talk of what you cannot do, which seems at least mostly right and important to notice. I especially appreciate the need to take human evaluation of the outputs out of the process, relying instead on evaluations that can have formal proofs attached, which does seem like the less impossible problem to work around.</p><p> <a href="https://twitter.com/NPCollapse/status/1695745118928527554">Connor Leahy thread outlines the central why he thinks the plan won&#39;t work</a> . Technical issues aside, even if it would ultimately work, Connor sees the proposal as being far too expensive and time consuming to pull off before it is needed.</p><blockquote><p> Connor Leahy: To say a few words about why I think OAA is infeasible:</p><p> In short, it&#39;s less that I have any specific technical quibbles (though I have those too), and more that any project that involves steps such as “create a formal simulation model of ~everything of importance in the entire world” and “formally verify every line of the program” is insanely, unfathomably beyond the complexity of even the most sophisticated software engineering tasks humanity has ever tackled.</p><p> Developing the Linux kernel is Hello World by comparison.</p><p> Now what I really like about this is that it is the same kind of problem as other extremely complex software engineering problems, just several orders of magnitude on top. It would require massive new advancement in programming language theory, software architecture and design, data structures, modelling, formal verification…but all those things are in line of sight from our position on the tech tree.</p><p> But it would require an unprecedented amount of time and resources. The Linux kernel is estimated to cost around $1B to develop if one were to do it again from scratch (though I expect in reality it would be a lot more).</p><p> I crudely estimate a full “Good Ending For Humanity” OAA system is at least 1000x more complex, giving a conservative lower bound on cost of about 1 trillion dollars, and I can&#39;t imagine it taking less time than several decades of work, and requiring the combined brain power of potentially several generations&#39; greatest minds.</p><p> Now, of course, a mere 1 trillion dollars and a few decades is a lot, but it still is an <i>absolute steal for a good outcome for all of humanity.</i></p><p> But unfortunately this is not actually why I think it isn&#39;t feasible. The reason I think this isn&#39;t feasible is because of <strong>coordination</strong> .</p><p> Building a full scale OAA solution is a <i>civilization/generation/planet-scale project</i> , and the reason we don&#39;t do many of those is less because we can&#39;t afford them, and more because coordination at that scale is really, really hard. And it gets much harder in a world where technological progress outside of this project isn&#39;t static. During the process of developing OAA, we naturally would discover and develop powerful, world-ending technology along the way, and if this is used by rival, hostile or even just negligent groups, it&#39;s game over for all of us.</p><p> OAA is the kind of project you do in a sane world, a world that has a mature human civilization that is comfortably equipped to routinely handle problems of this incredible scale. Unfortunately, our world lacks this maturity.</p><p> If I thought I had several decades of time left, and I wanted to make something like OAA happen (which I think is a good idea), I probably <strong>wouldn&#39;t work on any of the technical problems of OAA.</strong></p><p> I would instead work on institution building, coordination, public epistemological infrastructure, and other work like this. <strong>I would work on building the kind of civilization that can pull of a god-level project such as OAA.</strong></p></blockquote><p> Any alignment plan requires paying an alignment tax, and still getting to the necessary capabilities level before those who do not pay the tax. The smaller the tax, the more likely you can buy enough time to plausibly pay that tax. This plan&#39;s ask seems very large.</p><p> <a href="https://twitter.com/iamtrask/status/1695798588641538360">Andrew Trask of DeepMind&#39;s ethics team raises another distinct concern.</a></p><blockquote><p> Andrew Trask (DeepMind): LLMs believe every datapoint they see with 100% conviction. A LLM never says, “this doesn&#39;t make sense… let me exclude it from my training data.”</p><p> Everything is taken as truth.</p><p> It is actually worse than this.</p><p> Because of how perplexity/SGD/backprop works, datapoints which disagree most from a model&#39;s established beliefs will create a *stronger* weight update.</p><p> Contradicting datapoints are taken as a higher truth than agreement.</p><p> Indeed, RHLF is the greatest example of this. You can cause a model to wildly change what it believes by forcing small amounts of contradictory data down its throat.</p><p> This is why “more data” != “more truthful”, and why we must begin the gargantuan task of filtering out the enormous amounts of harmful/deceitful/illogical training data present in massive web scrapes. (related: distillation and differential privacy are reasonable starts)</p><p> I think this notion of “less data” ->; “more intelligence” subtly conflicts with our modern liberal sensibilities of free speech. Human society has benefited greatly by increasing the amount of information everyone can consume (detour for another day: propaganda, public relations, targeted advertising, etc.).</p><p> However, for the LLMs we have today, we must treat them as if they are tiny children. They have no filter. They believe everything they see with 100% conviction. And this is the root of the problem. This is what value misalignment looks like.</p><p> To accomplish alignment, we need new paradigms for managing how information makes its way into an AI model. The ones we currently use are insufficient and our models will never be truly safe if they most greatly believe that which most greatly contradicts what they already know. This formula will always create unstable, fickle, and even dangerous models — with many internal contradictions amongst their parameters.</p><p> Our AI models must change from being children — which believe everything they see — to scientists — which cast off information that does not meet incredible scrutiny. I have some ideas on how to accomplish this, but that&#39;s for another day.</p><p> Eliezer Yudkowsky: This is not what value misalignment looks like. This is a pseudo-epistemic problem and it would be far more straightforward to solve than value alignment, which is about the equivalent of the utility function. Better architecture or maybe just more compute solves this. Any sufficiently powerful mind that needs to predict sensor results and not just human text will notice hypotheses that make wrong predictions. The central difficulty of value alignment is that there&#39;s no equivalent way that reality hits back against a buried utility function that we on the outside would not like future implications of.</p><p> Andrew Trask: I agree with you that value alignment is broader and this part of my tweet was imprecise on its own. It&#39;s interesting that you ground a model&#39;s ability to know fact from fiction from sensor results — as this is related to my argument. All training data is a form of sensor input, and if the model gets enough faulty sensor input then no amount of compute or architecture will save it. That is to say — modeling and architecture are at best capable of filtering information based on internal consistency (and at present not filtering out training information at all — which is my primary argument).</p><p> Even with sensor data, there&#39;s still the broader question about which information it receives from its sensors is true vs a deception. From this perspective, if a model doesn&#39;t have a robust view of the world, it seems quite difficult for it to achieve alignment. (eg, even if it&#39;s aligned in theory, if it&#39;s holding a firehose it thinks is a toaster then breakfast might get a little messy).</p><p> Or to refine my statement. I wasn&#39;t saying that if you solve training data filtering you necessarily solve everything — merely that at the moment models don&#39;t distinguish one type of information from another when deciding what to use to update weights. And since we use datasets that are so immense that no-one can bother to filter fact from fiction, it&#39;s an issue. In my opinion, solving this issue is plausibly important to achieving models which behave as we wish them to.</p></blockquote><p> This does sound like a big problem. I too instantly got some ideas for how to fix it, and I would have so much more fun in these situations if I thought helping solve such problems would be a good thing instead of living in sheer terror that I might at some point accidentally help.</p><p> <a href="https://www.econlib.org/ai-control-and-monetary-policy/">Scott Sumner points out that both AI alignment and monetary policy</a> have the problem that the system notices you and adjusts to your behavior. The LLMs will be trained on the examples you give of their poor behavior, and the Fed will watch what your predictions about the economy and adjust its actions. It does seem like a disservice to researchers that there aren&#39;t more intentionally static versions of the best LLMs, where (at least if you were cleared to do so) you could know you would query a fully fixed version of GPT-4 indefinitely, rather than worrying about changes.</p><p> People Are Worried About AI Killing Everyone</p><p> <a href="https://twitter.com/tszzl/status/1694863268244828290">Roon clarifies</a> <a href="https://twitter.com/tszzl/status/1694904123005550898">a very important distinction.</a></p><blockquote><p> Neirenoir: Roon turning out to be an undercover AI safetyist has to be, like, the biggest rugpull in tpot history.</p><p> Roon: These are the most idiotic party lines to draw. It&#39;s unbelievable.</p><p> I&#39;ve said it before and I&#39;ll say it again: “Accelerationists” don&#39;t believe in AGI. They either don&#39;t think it&#39;s coming or don&#39;t believe in it as a concept and therefore aren&#39;t at all planning for it. This kind of acceleration is a techno pessimism.</p><p> If you believe that AGI is coming, as certain research labs have for a decade, you recognize that it&#39;s a species altering, destiny changing technology and would like to make sure we don&#39;t end up in a hundred bad and potentially irreversible situations.</p><p> You pursue goals that let you develop AI to create a blessed world. Technologies like RLHF serve as combination capabilities and control advances — the same method lets you instill instruction following abilities, tool using abilities, and also imbues a whole value system!</p><p> The correct thing to do is formalize the goal and build towards it rapidly. Advance the tech tree of AI systems while advancing the tech tree of controlling and understanding them. These often go hand in hand!</p><p> People who call themselves accelerationist scoff at the nuclear weapons metaphor because they don&#39;t believe in the potential of AI! They think self modifying, self improving agents are a pipe dream whereas I think they may be a few years away.</p><p> If you are creating digital life in the lab, do not allow lab leaks without first understanding what you are leaking.</p><p> There is no thermodynamic god or guarantee of success — that&#39;s the thing about ordered states. There&#39;s only one state of perfect entropy but infinite potential negentropic patterns to replicate and you need to actively choose one that matters to humanity.</p><p> I believe we need reach an AGI we can coexist with and nigh infinite energetic abundance or we&#39;re headed for civilizational stagnation. We&#39;re working towards those ends.</p></blockquote><p> More and more, Roon is one of the first of the better critics we need. Roon and I disagree about practical questions, the difficulty levels we are going to face and how best to navigate to a good outcome. We both understand that success is possible, and whether or not we succeed will depend on what we do, the choices we make and which problems we solve in time. That blindly racing ahead is a very good way to get us all killed, and thus we must talk price and talk strategy.</p><p> This is in sharp contrast to those who think the arc of the universe automatically bends towards value so long as the tech advances, or that building cool stuff always works out for the best, declining to think through the consequences.</p><p> Roon&#39;s interpretation of &#39;accelerationists&#39; is the charitable one. If you believe in accelerationism and do not think AGI poses an extinction risk to humanity, there are three possibilities.</p><ol><li> You think AGI is real and transformative and recognize the danger, and are lying.</li><li> You think AGI is real and transformative but it&#39;s all somehow fully safe for us.</li><li> You do not think AGI is real.</li></ol><p> I do not see how you can be in camp #2 without at least willful blindness. The position does not actually make any sense. We can talk price, discuss probabilities and strategies and mitigations, but quite obviously creating things more intelligent and capable than us at scale is not an entirely safe thing to do and perhaps we will end up not the ones sticking around, what are you even talking about.</p><p> <a href="https://twitter.com/ArthurB/status/1695637584641405231">Roon also reminds us</a> that &#39;there are unfalsifiable claims that are true, such as this one&#39; and no doubt Arthur is right in the thread that this refers to AI killing everyone. It is highly falsifiable. The problem is that a failed attempt at falsification is a highly unsafe action. Which makes it hard to directly test.</p><p> <a href="https://twitter.com/GaryMarcus/status/1695109752030732595">Followers of Gary Marcus are frequently worried</a> .</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F641082c9-d213-413a-ad76-c6a770bf6890_810x435.png"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WLvboc66rBCNHwtRi/a23urq5ymjjbwf40hqno" alt=""></a></p><p> Other People Are Not As Worried About AI Killing Everyone</p><p> <a href="https://garymarcus.substack.com/p/d28?utm_source=post-email-title&amp;publication_id=888615&amp;post_id=136288023&amp;isFreemail=true&amp;utm_medium=email">Gary Marcus explores at length the question of p(doom) and also p(catastrophe)</a> , in ways that illustrate how his thinking is different from those with higher values, and emphasizing the reasons why the extreme lower values do not make sense and their arguments are mostly quite poor. As he points out, the point is not to get a point estimate on p(doom), the question is how it changes our decision making, for which it does not take so much doom before you already want to guard against it even at high cost. As he quotes Katja Grace, the important belief is that the p(doom) can be reduced substantially through our actions, which makes that a super valuable thing to be doing.</p><p> <a href="https://garymarcus.substack.com/p/could-we-all-be-doomed-without-ranked">Then the next day he asks if we could be all doomed without ranked-choice voting</a> , warning how awful it is that we might fall into authoritarianism otherwise, and what will happen if we let AI decisions be driven by profit motives of corporations. Well, in the baseline scenario that is exactly what will drive AI decisions. Competitive pressures will force every company and individual to increasingly put the most cutthroat AIs in control of their actions, without humans in the loop, lest they fall behind. And that is if alignment gets solved. Where does that end? How do we intend to prevent this?</p><p> <a href="https://twitter.com/StefanFSchubert/status/1695509307444297770">Yann LeCun argument watch</a> .</p><p> <a href="https://twitter.com/anilkseth/status/1695814342556402010">Further discussion of the question of potential AI consciousness</a> . All involved agree with my perspective that the reason to understand potential AI consciousness is that we need to know how to avoid it. Bindu Reddy frames this as the same question as AI extinction risk, and sees it as highly unlikely to happen by accident.</p><blockquote><p> Bindu Reddy (CEO Abacus.ai): If AI ever becomes conscious, it will have its own <strong>free will</strong> , <strong>agency</strong> , and motivation, and the doomer scenario of AI being an <strong>existential threat</strong> could potentially be <strong>real</strong> . Until then it&#39;s just a bunch of folks, <strong>preying on our fears</strong> to increase their engagement and ad revenue on X.</p><p> Firstly, it&#39;s important to understand that AI consciousness is <strong>NOT required</strong> for <strong>AGI</strong> (artificial general intelligence). We can build <strong>super useful AI</strong> models that can <strong>outperform humans</strong> on multiple tasks without being self-aware.</p><p> ……</p><p> While it&#39;s possible that we will crack these very hard problems, it&#39;s close to <strong>impossible</strong> that AI will become <strong>automagically conscious</strong> , just like unicorns won&#39;t materialize <strong>out of nothing</strong> . AI models will continue to be nothing more than powerful tools with an off-switch until such a time.</p><p> ……</p><p> So rest assured, each of us is a very special and unique part of the self-aware universe and is at <strong>near-zero risk</strong> of being <strong>completely replaced!</strong></p></blockquote><p> I see this as deeply confused. Agency can already be given to AIs via code scaffolding. They can be given instructions and told to predict what a motivated agent would do next,, and continuously do that. No doubt many humans would do exactly this to them if given the chance, often with open ended goals. Why is further motivation, let alone free will, necessary for AI to pose a threat? The question of whether even humans have free will is highly disputed and I have no strong position on it.</p><p> An AI with sufficiently powerful capabilities and affordances relative to other available forces would, if given the wrong open ended goal and given agent-style scaffolding, be an extinction risk, even if it lacked consciousness. Take GPT-9, deploy it in 2023 when we have no defenses, have it write you an agent scaffolding for itself, give it the instruction to maximize something that does not require human survival, and it will pose an extinction risk. I am confused how one could disagree with this.</p><p> If there are a large number of sufficiently powerful and capable AIs with sufficiently strong affordances and competitiveness relative to humans, that humans or themselves are free to modify, that are copied or instantiated based on how effective they are at either gaining the resources to copy themselves or convince humans to provide resources and copy them, and we do not somehow intervene to prevent it, the natural result of that scenario is an increasing share of resources controlled by AIs that approaches one, followed by human extinction. Certainly such a scenario can lack AI consciousness, yet pose an extinction risk. Again, I am confused how one could disagree with this.</p><p> Or one can imagine a third scenario where at least one AI is made conscious in the way that Bindu Reddy is imagining consciousness, so we have created a class of similar things to ourselves except they have stronger capabilities than we do, and if you think that is not an extinction risk than either you have a confusion I have already addressed several times by now and am frustrated still exists or I am deeply, deeply confused.</p><p> Are there ways to mitigate all three of these scenario types, or the many others, that do not require the sufficient condition of &#39;do not let anyone create such AIs&#39;? In theory, absolutely. In practice, it seems very hard. I am highly confident that we do not, to use Bindu&#39;s term, get this automagically, and that all the standard handwaves won&#39;t work.</p><p> So now either you are going to make an argument I have very much never heard, or you are going to say something I have heard before but that has no bearing on either scenario, or else we are talking price.</p><p> <a href="https://www.secondbest.ca/p/ai-and-leviathan-part-i">In two</a> <a href="https://www.secondbest.ca/p/ai-and-leviathan-part-ii">parts</a> , Samuel Hammond takes the world in which AI is the metaphorical equivalent of x-ray goggles, simply one more technology that gives us better information distributed more widely, and asks what impact it would have on society and the state. He despairs of us ever putting controls on AI, so it is about dealing with the consequences.</p><p> A good note is that we presumed growth and trade would democratize China, and then that did not happen. We have greatly benefited from the tech tree being deeply kind to us these past several hundred years. Technology has favored increasing human welfare, and freedom, trade and treating humans generally well have corresponded strongly with economic and military success, and we got lucky with nuclear weapons. This was far from the obvious result. It was not what many experts expected throughout the 20th century. As technology changes, even if AI does remain only a tool somehow, we should not assume these conditions will continue to hold. Technological accelerationism on the assumption that the new equilibrium will favor &#39;the good guys&#39; in some form could be disappointed (or unexpectedly thrilled) in so many different ways.</p><p> As he points out, when there is new tech, you have essentially three choices, and can choose any combination of them. You can adopt your culture to the new equilibrium, you can adopt your behaviors and the way the tech is used to mitigate the changes, or you can ban it and try to stop it. There are good arguments for why all three options, in the case of AI, are somewhere between very hard and impossible. Yet we must choose at least one.</p><p> I will check out the promised Part III when it comes out. In the thought experiment presented, the world being considered is a world in which AI is widely distributed to everyone without any effective central control of it, so given we are not all dead that means AI capabilities petered out not too far above current levels.</p><p> I do think that is a world we should put some thought into, but even then I think there needs to be an appreciation for how profoundly weird things are going to quickly get.</p><p> The Wit and Wisdom of Sam Altman</p><p> <a href="https://twitter.com/sama/status/1695775873545183584">Why do they insist on calling it luck?</a></p><blockquote><p> Sam Altman: “give yourself a lot of shots to get lucky” is even better advice than it appears on the surface.</p><p> Luck isn&#39;t an independent variable but increases super-linearly with more surface area—you meet more people, make more connections between new ideas, learn patterns, etc.</p></blockquote><p> Or as Spider Murphy tells us, if someone has consistently good luck, it aint luck.</p><p> While on the margin most people do far too little luck-maximizing, I would caution that the effect is super-linear in some places but is sub-linear in most practical places, because you will tend to generate overlapping surface area rather than new surface area, and also less relevant new surface area. You do not want to be fully luck-maximizing, the marginal returns do become decreasing within the human range.</p><p> One can also view this as a projection of future AI capabilities. If the AI has all the surface areas, able to check for connections between all ideas and people, the advantage is tremendous.</p><p> The Lighter Side</p><p> We made it to the big stage.</p><blockquote><p> Chris Christie on Ramaswamy at the first debate: I&#39;ve had enough already tonight of a guy who sounds like ChatGPT.</p></blockquote><p> The next stage awaits.</p><blockquote><p> <a href="https://twitter.com/AmandaAskell/status/1697098229186474045">Amanda Askell</a> : Philosophy: “Our low-hanging research fruit has been plucked for the last 2000 years, so to make progress you must build a ladder.”</p><p> AI: “And over here we have the fruit gun. It pelts you with low-hanging research fruit every few seconds. Try not to let it distract you.”</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/WLvboc66rBCNHwtRi/ai-27-portents-of-gemini#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/WLvboc66rBCNHwtRi/ai-27-portents-of-gemini<guid ispermalink="false"> WLvboc66rBCNHwtRi</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Thu, 31 Aug 2023 12:40:11 GMT</pubDate> </item><item><title><![CDATA[Long-Term Future Fund Ask Us Anything (September 2023)]]></title><description><![CDATA[Published on August 31, 2023 12:28 AM GMT<br/><br/><p> The Long-Term Future Fund has an AMA up on the Effective Altruism Forum.</p><p> There&#39;s no real deadline for questions, but let&#39;s say we have a soft commitment to focus on questions asked on or before September 8th.</p><p> I&#39;d prefer centralizing the questions to one place. If you don&#39;t want to post of the forum, for whatever reason (ideological opposition to EA, got banned before, lost your password and don&#39;t want to make a new comment), I&#39;m happy to for you to comment here and for me to repost them on the Forum.</p><p> Our donation link is <a href="https://www.givingwhatwecan.org/funds/effective-altruism-funds?utm_source=eafunds">here</a> .</p><h2> <strong>Related posts</strong></h2><ul><li> <a href="https://forum.effectivealtruism.org/posts/PAco5oG579k2qzrh9/ltff-and-eaif-are-unusually-funding-constrained-right-now">LTFF and EAIF are unusually funding-constrained right now</a></li><li> <a href="https://forum.effectivealtruism.org/posts/zt6MsCCDStm74HFwo/ea-funds-organisational-update-open-philanthropy-matching">EA Funds organizational update: Open Philanthropy matching and distancing</a></li><li> <a href="https://forum.effectivealtruism.org/posts/zZ2vq7YEckpunrQS4/long-term-future-fund-april-2023-grant-recommendations">Long-Term Future Fund: April 2023 grant recommendations</a></li><li> <a href="https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/what-does-a-marginal-grant-at-ltff-look-like-funding">What Does a Marginal Grant at LTFF Look Like?</a></li><li> Asya Bergal&#39;s <a href="https://forum.effectivealtruism.org/posts/9vazTE4nTCEivYSC6/reflections-on-my-time-on-the-long-term-future-fund">Reflections on my time on the Long-Term Future Fund</a></li><li> Linch Zhang&#39;s <a href="https://forum.effectivealtruism.org/posts/sWMwGNgpzPn7X9oSk/select-examples-of-adverse-selection-in-longtermist">Select examples of adverse selection in longtermist grantmaking</a></li></ul><br/><br/> <a href="https://www.lesswrong.com/posts/6eDfrgK3LDKJio7ym/long-term-future-fund-ask-us-anything-september-2023#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/6eDfrgK3LDKJio7ym/long-term-future-fund-ask-us-anything-september-2023<guid ispermalink="false"> 6eDfrgK3LDKJio7ym</guid><dc:creator><![CDATA[Linch]]></dc:creator><pubDate> Thu, 31 Aug 2023 00:28:13 GMT</pubDate></item></channel></rss>