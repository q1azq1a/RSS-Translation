<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 4 日星期六 20:11:40 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[The 6D effect: When companies take risks, one email can be very powerful.]]></title><description><![CDATA[Published on November 4, 2023 8:08 PM GMT<br/><br/><p>最近，我一直在学习与构建风险系统的公司相关的行业规范、法律发现程序和激励结构。我想在这篇文章中分享一些发现，因为它们对于前沿人工智能社区的良好理解可能很重要。</p><h3>长话短说</h3><p>记录在案的风险沟通（尤其是员工的沟通）使公司在发生不好的事情时更有可能在法庭上承担责任。即使向公司发送一封传达风险的电子邮件，可发现的危险文档所产生的尽职调查义务（6D 效应）也可以使公司更加谨慎。</p><h3>公司倾向于避免通过有记录的媒体谈论风险。</h3><p>公司通常有意避免通过电子邮件等永久媒体讨论他们正在做的事情的风险。例如， <a href="https://corporate.findlaw.com/litigation-disputes/safe-communication-guidelines-for-creating-corporate-documents.html"><u>本文</u></a>就公司如何通过使用“安全沟通”实践来避免创建有罪的“不良文件”来避免责任，提供了一些非常阴暗的建议。</p><blockquote><p>通常，这些文件的起草者倾向于认为他们正在为公司提供一些业务价值。例如，一位工程师注意到设计中存在潜在的责任，因此他通过电子邮件通知他的主管。然而，工程师缺乏法律知识，在沟通中误用法律词汇，可能会在以后发生诉讼时发现问题，让公司受到牵连。</p></blockquote><p>我个人喜欢在摘录中使用“何时”而不是“如果”。</p><p>这是一个反常的结果，因为当无法证明公司了解风险时，即使公司了解风险，公司也相对难以承担风险责任。当事件发生并且公司被起诉时，有关其在问题中所扮演角色的证据将在诉讼的“<a href="https://en.wikipedia.org/wiki/Discovery_(law)"><u>发现</u></a>”阶段收集（电子邮件通常是可发现的）。当发现记录表明一家公司了解该问题时，他们更有可能被判承担责任。</p><h3>一封电子邮件可以发挥很大的作用。</h3><p>发现工作方式的不幸后果是，公司战略性地避免通过记录媒体传达风险。但还有一线希望。由于风险沟通记录而产生的责任威胁可能会对公司的谨慎程度产生很大影响。一份可发现的风险记录可能非常有影响力。</p><p><strong>我喜欢将其称为 6D 效应——对可发现的危险文件进行尽职调查的责任。</strong></p><h3>几个例子</h3><p>以下是一些公司因忽视风险沟通记录而被追究损害赔偿责任的著名例子（但在整个法律史上有很多例子）。</p><ul><li> <a href="https://law.justia.com/cases/california/court-of-appeal/3d/119/757.html"><u>1981 年，在 Grimshaw 诉福特汽车公司一案</u></a>中，福特被判对福特 Pinto 造成的致命事故所造成的损害承担责任，因为事实表明，公司内部领导层忽视了有关车辆燃油系统问题的警告。</li><li> 2017年伦敦格伦菲尔大厦火灾造成72人死亡，今年4月，<a href="https://www.bbc.com/news/uk-england-london-65458973"><u>双方达成了大规模和解</u></a>。诉讼的一个重要因素是管理该塔的公司<a href="https://www.theguardian.com/uk-news/2018/aug/08/grenfell-fire-warnings-issued-months-before-blaze-show-documents"><u>忽视了</u></a>发现中发现的众多消防安全警告。</li><li>去年，哈德威克诉 3M 案<a href="https://www.ehslawinsights.com/2022/09/sixth-circuits-interlocutory-order-reviewing-pfas-class-action-highlights-issues-with-certifying-class/"><u>结束</u></a>。这是 2018 年针对消费品中存在有害“永久化学物质”(PFAS) 的集体诉讼。这些化学品背后的公司被发现自 20 世纪 70 年代以来就已<a href="https://www.theguardian.com/environment/2022/may/01/pfas-forever-chemicals-rob-bilott-lawyer-interview"><u>了解风险</u></a>，但却<a href="https://law.justia.com/cases/federal/district-courts/ohio/ohsdce/2:2018cv01185/217689/166/"><u>故意疏忽，</u></a>最终导致对他们不利的裁决。</li></ul><h3>杂项笔记</h3><ol><li>任何可发现的沟通都会产生 6D 效应，但当警告来自公司本身的员工时，这种效应尤其强大。</li><li>如果您传达了风险，重要的是要在诉讼的发现阶段说出并提请法院注意该风险的文件。</li><li>如果您意识到公司所做的某些事情是危险的，那么您有道德义务通知该公司，但您没有道德义务帮助他们修复而不提供补偿。确保不要让公司利用您。</li></ol><h3>三个要点</h3><ol><li>如果您在一家从事潜在风险工作的公司工作，请坚持通过有记录的媒体讨论危险。如果您因记录风险沟通而遭到报复，您将有理由诉诸法律。 #notlegaladvice</li><li>如果您发现有风险，请说出来。如果你预测的事情发生了，请指出你传达过的事实。</li><li>注重安全的公司（例如那些致力于前沿人工智能系统的公司）应该制定明确的政策来记录所有风险讨论。</li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/J9eF4nA6wJW6hPueN/the-6d-effect-when-companies-take-risks-one-email-can-be<guid ispermalink="false"> J9eF4nA6wJW6hPueN</guid><dc:creator><![CDATA[scasper]]></dc:creator><pubDate> Sat, 04 Nov 2023 20:08:40 GMT</pubDate> </item><item><title><![CDATA[Genetic fitness is a measure of selection strength, not the selection target]]></title><description><![CDATA[Published on November 4, 2023 7:02 PM GMT<br/><br/><p><i>替代标题：“进化表明对齐属性的稳健而不是脆弱的概括。”</i></p><p>一个<a href="https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization"><u>经常</u></a><a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>被重复的</u></a><a href="https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies"><u>论点</u></a>是这样的：</p><ol><li>进化优化了人类的包容性遗传适应性（IGF）</li><li>然而，人类最终并没有明确地优化遗传适应性（例如，他们使用避孕措施来避免生育孩子）</li><li>因此，即使我们针对 X 优化 AI（通常是“人类价值观”），我们也不应该期望它显式针对 X 进行优化</li></ol><p>我的观点是，前提 1 是一种口头速记，在技术上是不正确的，而前提 2 至少具有误导性。至于总体结论，我认为进化论的案例可能被解释为弱证据，无法证明为什么人工智能即使在其能力增强的情况下也应该<i>继续</i>优化人类价值观。</p><p><strong>前提 1 错误的总结：</strong>如果我们仔细观察进化的作用，我们可以看到它选择有利于生存、繁殖和将基因传递给下一代的特征。这通常被描述为“针对 IGF 进行优化”，因为有利于这些目的的性状<i>通常</i>是具有最高 IGF 的性状。 （这有一些重要的例外，稍后讨论。）但是，如果我们仔细观察选择过程，我们可以看到这种性状选择<i>并不是</i>“针对 IGF 进行优化”，例如，我们可能会优化一个对图片进行分类的人工智能。</p><p>我所描绘的模型是这样的：进化是一种优化函数，在任何给定时间，它都会选择一些在重要意义上是随机选择的特征。在任何时候，它都可能随机转向选择其他一些特征。观察这个选择过程，我们可以计算当前正在选择的性状的 IGF，作为衡量这些性状被选择的强度的指标。但进化并没有<i>针对这一指标进行优化</i>；进化正在<i>优化</i><i>当前已选择进行优化的特征</i>。因此，没有理由期望进化创造的思维应该针对 IGF 进行优化，但<i>有理由</i>期望它们会针对实际选择的特征进行优化。这是我们在人类针对某些生物需求进行优化时观察到的任何事情。</p><p>相反，如果我们优化人工智能来对图片进行分类，我们就不会像进化那样随机改变选择标准。我们将保持选择标准不变：始终选择按照我们想要的方式对图片进行分类的属性。就进化的类比而言，人工智能应该更有可能只做它们被选中的事情。</p><p><strong>前提2如何误导的总结：</strong>人们常常暗示，进化选择了人类来关心性，然后性产生了后代，直到最近随着避孕方法的进化，这种联系才被切断。例如：</p><blockquote><p> 15. [...]在引入农业之后，我们并没有立即打破与“包容性生殖适应性”外部损失函数的一致性——就像克罗马努人起飞了 40,000 年一样，因为它本身运行得非常快相对于自然选择的外部优化循环。相反，我们获得了许多比祖先环境更先进的技术，包括避孕，相对于外部优化循环的速度而言，在一般智能游戏的后期，我们以非常快的速度爆发了。</p><p> – Eliezer Yudkowsky， <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>《AGI 废墟：杀伤力清单》</u></a></p></blockquote><p>这对我来说似乎是错误的。避孕可能是最近才出现的发明，但杀婴或因疏忽而杀害儿童则不然。即使不采取避孕措施，也一直有控制人口规模的方法。根据<a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><u>《童年人类学》</u></a>一书，家庭规模和生孩子的经济价值一直是相关的。儿童对于采集者来说是更大的负担，相应地，采集者的家庭规模较小，而儿童对于家庭规模较大的农民来说则是一种资产。</p><p>进化并没有选择人类的IGF，然后这种联系随着避孕的发明而打破，而是进化选择了人类具有优化功能，在考虑生育多少孩子时权衡各种因素。在类似采集者的环境中，这一功能会导致人们偏好较少的孩子和较小的家庭规模；在类似农民的环境中，这种功能导致人们偏好更多的孩子和更大的家庭规模。 <a href="https://www.lesswrong.com/users/robinhanson?mention=user">@RobinHanson</a><a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>认为</u></a>，现代社会更像是采集者而不是农民，而我们增加的财富正在导致我们恢复采集者式的方式和心理。如果这个论点是正确的，那么进化的“意图”和人类的行为方式之间并没有断裂。相反，进化创造的优化功能继续以它一贯的方式运作。</p><p>现代避孕方式的发明可能使得在农民型文化中更容易限制家庭规模，这种文化已经形成了反对杀婴等行为的文化禁忌。但找到一种绕过这些禁忌的方法并没有创造一个全新的进化环境，而是让我们<i>更接近</i>原始进化环境中的事物。</p><p>如果我们看看人类被选择优化的目标，看起来我们大多是在继续优化这些相同的东西。少数人选择不生孩子的原因是，我们进化的优化函数也重视孩子以外的事物，并且我们对这个优化函数“保持忠诚”。就人工智能而言，它被训练为按照“人类价值观”之类的东西行事，而没有其他任何东西，历史例子似乎表明，它的对齐属性可能比我们的更强大，因为它没有被选择用于混合许多相互竞争的价值观。</p><h1>进化是一种随机选择特征的力量</h1><p>在这篇文章中，我浏览了两本关于进化论的教科书： <a href="https://www.amazon.com/Evolution-Douglas-J-Futuyma/dp/1605356050/"><u>Futuyama 和 Kirkpatrick 的</u><i><u>《进化论》（第 4 版）</u></i></a>和<a href="https://www.amazon.com/Evolutionary-Analysis-5th-Jon-Herron/dp/0321616677/"><u>Herron 和 Freeman 的</u><i><u>《进化分析》（第 5 版）</u></i></a> 。第一个是根据谷歌搜索“什么是最好的进化生物学教科书”而选择的，第二个是因为我曾经选修过的进化心理学本科课程使用了早期版本，我记得它很好。</p><p>据我所知，没有人谈论进化是一个优化遗传适应性的过程（尽管这是一个相当简单的浏览，所以即使它存在，我也可能错过了）。相反，他们警告<i>不要</i>将进化视为首先“做”任何事情的积极因素。进化确实<i>提高了种群对其环境的平均适应能力</i>（Herron &amp; Freeman，第 107 页），但这意味着什么会随着环境本身的变化而不断变化。在历史上的某个时期，一个地区可能气候寒冷，选择那里的物种是为了有能力应对寒冷；然后气候可能会变得更加温暖，以前有益的适应（例如皮毛）可能会突然变成一种负担。</p><p>另一个经典的例子是<a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution"><u>胡椒蛾的进化</u></a>。浅色飞蛾曾经在英格兰很常见，深色飞蛾非常罕见，因为浅色比深色飞蛾更能伪装鸟类。随着工业革命和污染工厂的出现，一些城市变得如此之黑，以至于深色成为更好的伪装，导致深色飞蛾相对于浅色飞蛾的增加。一旦污染减少，浅色飞蛾就会再次占据主导地位。</p><p>如果我们将进化建模为数学函数，我们可以说，飞蛾首先选择浅色，然后改为选择深色，然后再次选择浅色。</p><p>最接近“遗传适应度的进化优化”的是所谓的“<a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection"><u>自然选择基本定理</u></a>”，它意味着自然选择将导致种群的平均适应度随着时间的推移而增加。然而，在这里我们假设<i>我们选择的东西保持不变。</i>随着时间的推移，浅色飞蛾将继续变得更加常见，直到深色成为具有更高适应度的特征，并且深色开始变得更加常见。在这两种情况下，我们可能会说“人口的平均适应度正在增加”，但这在这两种情况下意味着<i>不同的事情</i>：在一种情况下，这意味着选择白色，而在另一种情况下，这意味着选择深色。首先被选择<i>的</i>事物，然后被选择<i>反对</i>，即使我们的术语意味着<i>同样的</i>事物被选择。</p><p>发生的情况是，随着选择特定颜色，人口的平均适应度上升，然后随机变化（首先是污染增加，然后是污染减少）导致平均适应度下降，然后又开始上升。</p><blockquote><p><i>在其他条件相同的情况下，基本定理将使我们预计物种的平均适应度应每代增加几个百分点。但其他一切并不平等：选择给予什么，其他进化力量就会夺走什么。选择带来的适应度增益不断被空间和时间变化的环境、有害突变和其他因素所抵消。</i> （Futuyama 和 Kirkpatrick，第 127 页）</p></blockquote><p>即使考虑到这一点，进化也不会持续增加种群的平均适应度：有时进化最终会选择<i>降低</i>种群的平均适应度。</p><blockquote><p><i>基本定理和适应性景观做出的假设并不完全适用于任何自然种群。但在许多情况下，它们给出了非常好的近似值，有助于指导我们对进化的思考。在其他情况下，假设被违反的方式使进化表现得非常不同。基本定理和自适应景观不适用的一个特别重要的情况是选择与频率相关。在某些情况下，这可能会导致种群的平均适应度下降</i>（Futuyama &amp; Kirkpatrick，第 128 页）</p></blockquote><p>导致平均适应度<i>较低</i>的频率依赖选择的一个例子是产生许多果实的灌木丛（Futuyama &amp; Kirkpatrick，第 129 页）。一些灌木会进化出一个树干，使它们能够为邻居投下阴影。结果，这些邻居变得虚弱和死亡，使已经变成树木的灌木丛获得了更多的水和养分。</p><p>这导致树木变得比灌木丛更常见。但由于树木需要花费更多的能量来生产和维护树干，因此它们没有那么多的能量来种植果实。当树木稀少并且主要从灌木丛中窃取能量时，这并不是什么大问题；但一旦整个种群都由树木组成，它们最终可能会互相遮蔽。此时，它们最终产出的可供新树生长的果实要少得多，因此后代也较少，平均适应度也较低。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/d1oqnbrk2hlvbihqseic"></p><p>这种与频率相关的选择很常见。另一个例子（Futuyama &amp; Kirkpatrick，第 129 页）是细菌既能进化出杀死其他细菌的毒素，又能进化出针对该毒素的解毒剂。两者的生产都需要消耗能源，但只要这些细菌很稀有，那么这些成本就是值得的，因为它们的毒性可以让它们杀死竞争对手。</p><p>但是，一旦这些有毒细菌自我繁殖，产生毒素就不再有任何好处——所有幸存的细菌都对其免疫——因此继续花费能量来产生毒素意味着可用于复制的能量更少。现在，保持解毒剂的产生但失去毒素的产生变得更加有利：毒素的产生从被选择变成被选择反对。</p><p>一旦这种选择过程发生足够长的时间并且不产生毒素的细菌占据主导地位，解毒剂的生产<i>也</i>将成为不必要的负担。没有人再产生毒素了，所以没有理由浪费精力来维持防御，所以解药也从被选择变成了被选择对抗。</p><p>但是，一旦没有细菌再产生毒素<i>或</i>解毒剂，会发生什么呢？既然没有人对毒素有防御能力，那么再次开始生产毒素+解毒剂组合就变得有利，从而杀死所有其他没有解毒剂的细菌……如此循环重复。</p><p>在本节中，我认为，就进化而言，“为了适应而优化物种”，这实际上在不同情况下意味着不同的事情（选择不同的特征）；而且，针对适应性的进化优化更多的是一种粗略的启发式方法，而不是字面法则，因为在很多情况下，进化最终会<i>降低</i>种群的适应性。仅这一点就应该让我们对“进化为IGF选择人类”的论点产生怀疑；这意味着并不是针对单一事物进行优化，而是在不同时间选择了多种特征。</p><h1>健身到底是什么？</h1><p>到目前为止，我一直在一般性地谈论健身，但让我们回顾一下一些技术细节。包容性遗传适应性到底<i>是</i>什么？</p><p>有几种不同的定义；这是其中的一组。</p><p><i>适应度</i>的一个简单定义是，个体为下一代留下的后代数量<span class="footnote-reference" role="doc-noteref" id="fnrefns0k7bysais"><sup><a href="#fnns0k7bysais">[1]</a></sup></span> 。假设胡椒蛾的后代中有 1% 能存活到生育年龄，并且幸存的蛾类平均有 300 个后代。在这种情况下，这些个体的平均适应度为 0.01 * 300 = 3。</p><p>为了通过自然选择进行进化，个体之间的适应度差异需要遗传。在生物进化中，遗传是通过基因发生的，因此我们通常对<i>遗传适应性</i>（基因的适应性）感兴趣。假设这些都是污染城市中的浅色飞蛾。假设深色的基因等位基因将生存能力提高了 0.33 个百分点，总体适应度为 0.0133 * 300 = 4。等位基因的适应度现在为 3 和 4。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BtffzD5yNB4CzSTJe/hn7jtehbrwp5cxtwi6vs"></p><p>图片来自 Futuyama 和柯克帕特里克。原文说明：<i>基因型 A 的适应度为 3，而基因型 B 的适应度为 4。两种基因型均以 10 个个体开始。 (A) B 基因型的种群规模增长得更快。 (B) 绘制两种基因型的频率图表明，从频率为 0.5 开始的基因型 B 在 7 代后就几乎占到了人口的 90%。</i></p><p>通常重要的是两个等位基因之间的适应度<i>差异</i>：例如，如果其竞争对手的适应度为 1，则适应度为 2 的等位基因可能会在群体中变得更常见，但如果其竞争对手的适应度为3. 因此，通常指示<i>相对于</i>某些共同参考的适合度，例如群体的平均适合度或具有最高绝对适合度的基因型。</p><p>遗传适应性可以分为两个组成部分。个体可以将基因直接传给后代——这称为<i>直接适应性</i>。他们还可以携带一种基因适应能力，使他们能够帮助具有相同适应能力的其他人，从而增加他们的生存几率。例如，父母可能会投入额外的精力来照顾子女。这称为<i>间接健身。</i>基因型的<i>包容性适应性</i>是其直接适应性和间接适应性的总和。 <span class="footnote-reference" role="doc-noteref" id="fnref23b1jpur496"><sup><a href="#fn23b1jpur496">[2]</a></sup></span></p><p>生物进化可以定义为“生物体特性在世代过程中遗传的变化”（Futuyama &amp; Kirkpatrick，第 7 页）。<i>自然选择的进化</i>是指基因型的相对频率由于适应度的差异而在世代之间发生变化。请注意，基因型频率也可能因自然选择以外的原因而在代际间发生变化，例如随机漂移或新突变。</p><h1>适合度作为选择强度的衡量标准</h1><p>让我们看一个有意饲养动物的案例。接下来的数学细节并不那么重要，但无论如何我都想把它们讲一遍，只是为了更具体地说明“健身”<i>的实际含义</i>。不过，如果您愿意，也可以浏览它们。</p><p>假设我碰巧拥有一堆各种颜色的胡椒蛾，并且碰巧喜欢浅色，所以我决定将它们培育成浅色。现在我不知道胡椒蛾颜色的遗传学如何运作的细节 - 我认为它很可能受到多个基因的影响。但为了简单起见，我们假设有一个基因具有“亮”等位基因和“暗”等位基因。</p><p>将“亮”等位基因称为 B1，将“暗”等位基因称为 B2。 B1B1 飞蛾是浅色的，B2B2 飞蛾是深色的，B1B2 / B2B1 飞蛾介于两者之间（为了进一步简化，我将使用“B1B2”来指代 B1B2 和 B2B1 飞蛾）。</p><p>假设初始种群有 100 只飞蛾。我已经进行了一点育种，所以我们从 B1 开始，频率为 0.6，B2 频率为 0.4。飞蛾的基因型分布如下：</p><p> B1B1 = 36</p><p> B1B2 = 48</p><p> B2B2 = 16</p><p>在我看来，所有具有 B1B1 基因型的飞蛾看起来都很轻盈，所以我选择让它们全部繁殖。在我看来，75% 具有 B1B2 基因型的飞蛾看起来足够浅，50% 的 B2B2 基因型飞蛾也是如此（也许它们的颜色也受到环境因素或其他基因的影响）。其余的则不能繁殖。</p><p>平均而言，下一代 B1 等位基因的频率为 0.675，B2 等位基因的频率为 0.325 <span class="footnote-reference" role="doc-noteref" id="fnrefh3ks5duler4"><sup><a href="#fnh3ks5duler4">[3]</a></sup></span> 。假设每只飞蛾为下一代贡献了一百个配子，我们得到以下等位基因的适应性：</p><p> B1：从 120 (36 + 36 + 48) 增加到 5400 个副本，因此适应度为 5400/120 = 45。</p><p> B2：从 80 (48 + 16 + 16) 增加到 2600 个副本，因此适应度为 2600/80 = 32.5。</p><p>随着B1比例的增加，人群的平均适应度就会提高！这是因为你携带的 B1 等位基因越多，你被选择繁殖的可能性就越大，因此 B1 携带者具有更高的适应度……这意味着 B1 变得更常见……这增加了整个小鼠群体的平均适应度。因此，在这种情况下，人口的平均适应度往往会随着时间的推移而增加的规则确实适用。</p><p>但现在……将这个过程描述为<i>针对飞蛾适应性的优化，听起来不是很奇怪吗？</i></p><p>我正在优化<i>轻飞蛾</i>；适应度计算告诉我们的是<i>亮度基因有多大的优势——</i>换句话说，<i>我对亮度基因有多少偏爱</i>——<i> </i>相对于黑暗基因。</p><p>因为我们只是对适应度的影响进行建模，而不是对随机漂移进行建模，所以基因频率的所有差异都来自于适应度的差异。这是同义反复 - 无论您选择（优化）<i>什么</i>并不重要，根据定义，选择的<i>任何内容</i>最终都会具有最高的适应性。</p><p>与其说我们正在针对高适应度进行优化，不如说我们正在针对<i>亮度</i>特征进行优化，并且<i>亮度提供了适应度优势，这似乎更自然。</i>反过来说就没有多大意义了——我们正在针对健身进行优化，这给轻盈带来了优势？什么？</p><p>这个例子使用了人工选择，因为这使得实际选择目标是什么变得最明显。但无论我们谈论的是人工选择还是自然选择，数学的结果都是一样的。如果我们说，不是我决定某些飞蛾不能繁殖，而是鸟类和外部环境中的其他因素正在繁殖……好吧，所讨论的方程没有任何变化。</p><p>自然选择是否优化了飞蛾的适应性？从某种意义上来说，你可以这么说，因为与浅色飞蛾相比，深色飞蛾最终的健康状况有所提高。但这样描述又会让人感觉有点不太对劲。说飞蛾<i>为了拥有深色而进行了优化</i>，或者更抽象地说，为了拥有适合其环境的颜色，感觉更信息丰富、更准确。</p><h1>从颜色到行为</h1><p>我刚刚说过，如果我们看看进化的实际过程，它看起来更像是针对特定特征进行优化（用适应度来衡量它们被选择的强度），而不是针对适应度本身进行优化。即使选择过程<i>会</i>导致人口的平均适应度增加，但正如我们从数学中看到的那样，这只是意味着“如果你选择某件事，那么你会得到更多你所选择的东西”为了”。</p><p>在此之前的几节中，我认为进化选择的并不是单一的东西；而是进化所选择的。相反，它正在改变的事物本身也在不断改变。</p><p>我认为这些论点足以得出这样的结论：“进化优化了人类的适应性[因此人类应该优化适应性]”这一说法是站不住脚的。</p><p>到目前为止，我主要讨论的是相对“静态”的特征，例如颜色，而不是本身就是优化器的认知特征。那么我们来谈谈认知。虽然说“进化优化了人类的遗传适应性，因此人类应该优化适应性”似乎站不住脚，但如果我们谈论被选择的特定认知行为，相应的论点<i>确实</i>有效。</p><p>例如，如果我们说“人类被选择是为了关心他们的后代，因此人类应该为了确保后代的生存而进行优化”，那么这种说法一般来说确实成立——很多人<i>确实</i>投入了大量的认知努力确保孩子的生存。或者，如果我们说“人类被选择在某些情况下表现出性嫉妒，因此在某些情况下，他们会优化以防止他们的伴侣与其他人类发生性关系”，那么显然这种说法<i>也</i>成立。</p><p>这就是我的论点的第二部分：虽然声称我们现在正在做的事情完全违背了进化的选择，但避孕至少是一个糟糕的例子。在大多数情况下，我们仍在针对进化选择我们优化的事物进行优化。</p><h1>人类仍然有我们被选中的目标</h1><p>对性的渴望本身不足以生出婴儿，或者至少不足以生出能够存活足够长的时间来繁殖自己的婴儿。它始终只是一个组成部分，我们对孩子有多种不同的愿望：</p><ol><li>渴望发生性行为并享受性行为本身</li><li>为了孩子而生孩子的愿望</li><li>为了自己的利益而照顾和保护孩子（包括不是你自己的孩子）的愿望</li></ol><p>埃利以泽在《AGI 毁灭：致命事件清单》中写道</p><blockquote><p>15. [...]在引入农业之后，我们并没有立即打破与“包容性生殖适应性”外部损失函数的一致性——就像克罗马努人起飞了 40,000 年一样，因为它本身运行得非常快相对于自然选择的外部优化循环。相反，我们获得了许多比祖先环境更先进的技术，包括避孕，相对于外部优化循环的速度而言，在一般智能游戏的后期，我们以非常快的速度爆发了。我们开始更多地反思自己，开始更多地被文化进化所编程，并且我们在祖先训练环境中的一致性的许多假设同时被打破。</p></blockquote><p>这句话似乎暗示着</p><ul><li>有效的避孕是一项相对较新的发明</li><li>对性的渴望才是生孩子的主要驱动力（有效的避孕措施打破了这一假设）</li><li>这是一个新颖的发展，我们如此优先考虑儿童以外的事情</li></ul><p>所有这些前提对我来说似乎都是错误的。原因如下：</p><p><i>有效的避孕是一项相对较新的创新。</i>即使是狩猎采集者也能以杀婴的形式获得有效的“避孕”，这在一些现代狩猎采集社会中很常见。特别敏感的读者可能想跳过<a href="https://www.lesswrong.com/posts/vwM7hnT9ysE3suwfk/notes-on-the-anthropology-of-childhood"><i><u>《童年人类学》</u></i></a>中的以下段落：</p><blockquote><p> Ache（巴拉圭的一个觅食社会）特别直接处理多余的儿童（大约五分之一），因为他们四处游荡、觅食的生活方式给父母带来了巨大的负担。父亲提供重要的食物资源，母亲提供食物和危险的丛林环境所需的警惕监控。男性和女性在其相对较短的一生中都面临着重大的健康和安全隐患，他们将自己的福祉置于后代的福祉之上。对几个觅食社会的调查显示，杀婴意愿与“在游牧过程中携带多个幼儿”的艰巨挑战之间存在密切联系（Riches 1974：356）。</p><p>在其他南美采集者中，也普遍存在类似的态度。来自巴西中部的塔皮拉佩人每个家庭只允许三个孩子；所有其他人都必须留在丛林中。影响整个社区的季节性稀缺资源决定了这些措施（Wagley 1977）。事实上，是否有足够的资源是确定表面健康的婴儿是否能够存活的最常见标准（Dickeman 1975）。 Among the Ayoreo foragers of Bolivia, it is customary for women to have several brief affairs, often resulting in childbirth, before settling into a stable relationship equaling marriage. “Illegitimate” offspring are often buried immediately after birth. During Bugos and McCarthy&#39;s (1984) fieldwork, 54 of 141 births ended in infanticide.</p></blockquote><p> It takes years for a newborn to get to a point where they can take care of themselves, so a simple lack of active caretaking is enough to kill an infant, no modern-age contraceptive techniques required.</p><p> <i>It&#39;s the desire for sex alone that&#39;s the predominant driver for there being children.</i> Again, see infanticide, which doesn&#39;t need to be an active act as much as a simple omission. One needs an active desire to keep children <i>alive</i> .</p><p> Also, even though the share of voluntarily childfree people is increasing, it&#39;s still not the predominant choice. <a href="https://theconversation.com/more-than-1-in-5-us-adults-dont-want-children-187236"><u>One 2022 study</u></a> found that 22% of the people polled neither had nor wanted to have children - which is a significant amount, but still leaves 78% of people as ones who either have or want to have children. There&#39;s still a strong drive to have children that&#39;s separate from the drive to just have sex.</p><p> <i>It&#39;s a novel cultural development that we prioritize things other-than-having-children so much.</i> Anthropology of Childhood spends significant time examining the various factors that affect the treatment of children in various cultures. It quite strongly argues that the value of children has always also been strongly contingent on various cultural and economic factors - meaning that it has always been just <i>one</i> of the things that people care about. (In fact, a desire to have lots of children may be more tied to agricultural and industrial societies, where the economic incentives for it are abnormally high.)</p><blockquote><p> Adults are rewarded for having lots of offspring when certain conditions are met. First, mothers must be surrounded by supportive kin who relieve them of much of the burden of childrearing so they can concentrate their energy on bearing more children. Second, those additional offspring must be seen as “future workers,” on farm or in factory. They must be seen as having the potential to pay back the investment made in them as infants and toddlers, and pretty quickly, before they begin reproducing themselves. Failing either or both these conditions, humans will reduce their fertility (Turke 1989). Foragers, for whom children are more of a burden than a help, will have far fewer children than neighboring societies that depend on agriculture for subsistence (LeVine 1988). [...]</p><p> In foraging societies, where children are dependent and unproductive well into their teens, fewer children are preferred. In farming societies, such as the Beng, children may be welcomed as “little slaves” (Gottlieb 2000: 87). In pastoral and industrial societies, where young children can undertake shepherding a flock, or do repetitive machine-work, women are much more fertile. And, while the traditional culture of the village affords a plethora of customs and taboos for the protection of the pregnant mother and newborn, these coexist with customs that either dictate or at least quietly sanction abortion and infanticide.</p></blockquote><p> To me, the simplest story here looks something like “evolution selects humans for having various desires, from having sex to having children to creating art and lots of other things too; and all of these desires are then subject to complex learning and weighting processes that may emphasize some over others, depending on the culture and environment”.</p><p> Some people will end up valuing children more, for complicated reasons; other people will end up valuing other things more, again for complicated reasons. This was the case in hunter-gatherer times and this is the case now.</p><p> But it <i>doesn&#39;t</i> look to me like evolution selected us to desire one thing, and then we developed an inner optimizer that ended up doing something completely different. Rather, it looks like we were selected to desire many different things, with a very complicated function choosing which things in that set of doings each individual ends up emphasizing. Today&#39;s culture might have shifted that function to weigh our desires in a different manner than before, but everything that we do is still being selected from within that set of basic desires, with the weighting function operating the same as it always has.</p><p> As I mentioned in the introduction, Robin Hanson <a href="https://www.overcomingbias.com/p/forager-v-farmer-elaboratedhtml"><u>has suggested</u></a> that modern society is more forager-like than farmer-like and that our increased wealth is causing us to revert to forager-like ways and psychology. This would then mean that our evolved weighting function is now exhibiting the kind of behavior that it was evolved to exhibit in a forager-like environment.</p><p> We do engage in novel activities like computer games today, but it seems to me like the motivation to play computer games is still rooted in the same kinds of basic desires as the first hunter-gatherers had - eg to pass the time, enjoy a good story, socialize, or experience a feeling of competence.</p><h1> So what can we say about AI?</h1><p> Well, I would be cautious around reasoning by analogy. I&#39;m not sure we can draw particularly strong claims about the connection to AI. I think that there are <a href="https://forum.effectivealtruism.org/posts/kLYD95SK8tQFRmw4T/ben-garfinkel-s-shortform?commentId=XMjAWSEgp9BXTDQBi">more direct and relevant arguments</a> that one can make that do seem worrying, rather than trying to resort to evolutionary analogies.</p><p> But it does seem to me that eg the evolutionary history for the “sharp left turn” implies the <i>opposite</i> than previously argued. Something like “training an AI for recognizing pictures” or “training an AI for caring about human values” looks a lot more like “selecting humans to care about having offspring” than it looks like “optimizing humans for genetic fitness”. Caring about having offspring is a property that we still seem to pretty robustly carry; our alignment properties continued to generalize even as our capabilities increased.</p><p> To the extent that we do not care about our offspring, or even choose to go childfree, it&#39;s just because we were selected to <i>also</i> care about other things - if a process selects humans to care about a mix of many things, them sometimes weighing those other things more does not by itself represent a failure of alignment. This is again in sharp contrast to something like an AI that we tried to <i>exclusively</i> optimize for caring about human well-being. So there&#39;s reason to expect that an AI&#39;s alignment properties might generalize <i>even more</i> than those of existing humans.</p><p> <i>Thanks to Quintin Pope, Richard Ngo, and Steve Byrnes for commenting on previous drafts of this essay.</i> </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnns0k7bysais"> <span class="footnote-back-link"><sup><strong><a href="#fnrefns0k7bysais">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 60.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn23b1jpur496"> <span class="footnote-back-link"><sup><strong><a href="#fnref23b1jpur496">^</a></strong></sup></span><div class="footnote-content"><p> Futuyama &amp; Kirkpatrick, p. 300.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh3ks5duler4"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh3ks5duler4">^</a></strong></sup></span><div class="footnote-content"><p> Each B1B1 moth has a 100% chance to “pick” a B1 allele for producing a gamete, each B1B2 moth has a 50% chance to pick a B1 gamete and a 50% chance to pick a B2 gamete, and each B2B2 moth has a 100% to pick a B2 allele for producing a gamete. Assuming that each moth that I&#39;ve chosen to breed contributes 100 gametes to the next generation, we get an average of 3600 B1 gametes from the 36 B1B1 moths chosen to breed, 1800 B1 and 1800 B2 gametes from the 360 B1B2 moths chosen to breed, and 800 B2B2 gametes from the 8 B2B2 moths chosen to breed.</p><p> This makes for 3600 + 1800 = 5400 B1 gametes and 1800 + 800 = 2600 B2 gametes, for a total of 8000 gametes. This makes for a frequency of 0.675 for B1 and 0.325 for B2.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/BtffzD5yNB4CzSTJe/genetic-fitness-is-a-measure-of-selection-strength-not-the<guid ispermalink="false"> BtffzD5yNB4CzSTJe</guid><dc:creator><![CDATA[Kaj_Sotala]]></dc:creator><pubDate> Sat, 04 Nov 2023 19:02:13 GMT</pubDate> </item><item><title><![CDATA[The Soul Key]]></title><description><![CDATA[Published on November 4, 2023 5:51 PM GMT<br/><br/><p> The ocean is your home, but a forbidding one: often tempestuous, seldom warm. So one of your great joys is crawling onto land and slipping off your furry skin, to laze in the sun in human form. On the beaches, you and your sisters can relax in a way that older generations never could. The elders tell horror stories of their friends whose skins were stolen by humans, a single moment of carelessness leaving them stranded forever on land. That doesn&#39;t happen any more, though; this is a more civilized age. There are treaties, and authorities, and fences around the secluded beaches you and your sisters like the most.</p><p> So your sisters no longer lose themselves by force. But sometimes it happens by choice. Sometimes a group of your sisters wrap their skins around themselves like robes and walk into the nearby town. The humans point and stare, but that&#39;s just part of the thrill. Sometimes young men gather the courage to approach, bearing flowers or jewelry or sweet words. And sometimes one of your sisters is charmed enough to set a rendezvous—and after a handful of meetings, or a dozen, to decide to stay for good.</p><p> You never thought it would happen to you. But his manners are so lively, and his eyes so kind, that you keep coming back, again and again. When he finally asks you to stay, you hesitate only a moment before saying yes. The harder part comes after. He finds you human clothes, and in exchange you give him your beautiful skin, and tell him that it must be locked away somewhere you&#39;ll never find it—and that he must never give it back to you, no matter how much you plead. Because if there&#39;s any shred of doubt, any chance of returning home, then the lure of the sea will be too much for you. You want this; you want him; you want to build a life together. And so the decision has to be final.</p><p> Years pass. You bear three beautiful children, with his eyes and your hair, and watch them blossom into beautiful adults. You always live near the sea, although you can&#39;t bear to swim in it—your limbs feel unbearably weak and clumsy whenever you try. You and your husband grow into each other, time smoothing down the ridges left from pushing two alien lives together. You forget who you once were.</p><p> After your youngest leaves home, you start feeling restless. You have disquieting dreams—first intermittently, then for weeks on end. One day, after your husband has gone to work, your feet take you up the stairs to the attic. As you brush aside the cobwebs in one of the corners, your hands land on an old chest. You pull on the lid, and it catches on a padlock—but only for a second. The shackle has been rusted through by the sea breeze, and quickly snaps. You open the lid, and you see your skin laid out before you.</p><p> What then?</p><hr><p> You look at your skin, and your ears fill with the roar of the sea. A wild urge overtakes you; you grab your skin and run headlong towards the shore. As you reach it you see your husband standing on the pier—but that gives you only a moment&#39;s pause before you dive into the water, your skin fitting around you as if you&#39;d never taken it off.</p><p> As you swim away, you envisage your family in tatters: your children left baffled and distraught, your husband putting on a brave face for their sake. But it was his fault, after all. He failed in the one thing you asked of him; and you can&#39;t fight your nature.</p><hr><p> You look at your skin, and see a scrap of paper lying on top of it. <i>I knew you&#39;d only open the chest if you were restless and unhappy</i> , it reads. <i>And I would never cage you. So go free, with my blessing</i> .</p><p> You catch your breath—and, for a moment, you consider staying. But his permission loosens any tether that might have held you back. You leave the note there, alongside a little patch of fur torn off your coat: a last gesture, the least you can give him.</p><hr><p> You look at your skin, and your ears fill with the roar of the sea. But it&#39;s not loud enough to drown out your thoughts. <i>I&#39;m not an animal</i> , you think. <i>I can make my own choices</i> .</p><p> You shove the lid closed, and the moment of reprieve it gives you is enough to start scrambling down the stairs and out the gate and you don&#39;t stop running until you&#39;re out of sight of your house.</p><p> When you find your husband, down at the pier, your face tells him what&#39;s happened before you&#39;ve said a word. He gives you a gentle kiss, then sprints back to the house. By the time you make it home, he&#39;s relaxing in an armchair, and the kettle is almost boiling; but you know that the chest and its contents are gone.</p><p> His glances, always loving, now fill with wonder: that he almost lost you, but that he didn&#39;t. That you chose him yet again, in defiance of your deepest instincts. You curl up in his arms every night; and though at first you can&#39;t hold back the tears, over time they grow rarer and rarer. You never see your skin again.</p><hr><p> You look at your skin, and a wave of emotion crashes into you. You remember your old ambitions: to explore every horizon; to surf every current; to ride every storm. Dangerous dreams—and pointless ones too, in an age where planes criss-cross the skies, and the blank spots on the maps have all been filled. You didn&#39;t want to waste your life retracing others&#39; footsteps. So you infused those dreams into your skin and locked it away.</p><p> And yet there&#39;s still something in you that yearns for them: the part that&#39;s been making you restless, the part that led you into the attic. So you fetch a pair of scissors, and cut off your long wavy hair—and with it, whatever remaining wistfulness was keeping you up at night. You put it into the box, on top of your skin, and close the lid again. No need to lock it, this time.</p><p> Maybe your husband finds the broken lock, and the tresses of your hair. You never know. But you don&#39;t need to know. What you&#39;ve got is enough.</p><hr><p> You look at your skin, and remember your plan. Your face is lined now, and your hair is streaked with gray. But inside the chest, under a layer of dust, your old skin is pristine. You can imagine slipping it back on and gliding back into the ocean, not a day older than when you left.</p><p> You&#39;ll be different from how you were before, of course. You can&#39;t live a lifetime in any skin without it changing you. But your past self thought that was a worthwhile trade, for those extra decades of youth. Worth leaving your friends and family behind, worth setting aside all the glories of the ocean. And who knows—maybe there are more tricks yet to be found, more ways to slip the noose of aging and death.</p><p> You look at the skin, and consider putting it on. Not yet, you think. You&#39;ve still got a few more decades left in your current skin, before you&#39;ll need to go back. Not yet.</p><hr><p> The fur on your skin ripples like waves, bringing back to you the knowledge of what the ocean really is. In your current body, you can only see the surface: the swells of water, the winds and storms. But if you could dive underneath, you&#39;d find a portal to a whole civilization. You&#39;re on the shore of the future of humanity, a sea of minds so vast that you can barely imagine it. Throughout the long millennia since humans transcended the limitations of their biology, they&#39;ve multiplied beyond number, and constructed joys and enchantments beyond measure. The ocean is your gateway to all of those people and their wonders—each one so beautiful, and so so tempting.</p><p> Why did you come to this backwater, this output of baseline humans who refuse to engage with the outside world? Why did you lock away your memories, leaving only hints about what you used to be? You don&#39;t remember. Was it an experiment? A whim? Surely it couldn&#39;t have been for the love of a baseline human. But regardless, how can you stay here, when you know what else exists? You gather your skin in your arms, and though there&#39;s a pang of sadness as you walk out the door, you don&#39;t look back. There&#39;s a whole universe to explore.</p><hr><p> Your skin is a cornucopia of possible lives; the deeper into it you look, the more you see. Just under the surface, joyful humans are granted miraculous powers: to soar through air and sea, to play games on the scale of planets, to morph their bodies as they please. Beneath them, you see a society that&#39;s explored further, morphing not just their bodies, but also their minds—belief and desire and identity become as malleable as clay. Beneath <i>them</i> , you lose sight of individuals: at those depths minds merge and split and reform like currents in the ocean. And beneath even that? It&#39;s hard for you to make sense of the impressions you&#39;re getting—whatever is down there can&#39;t be described in human terms. In the farthest reaches there are only alien algorithms, churning away on computers that stretch across galaxies, calculating the output of some function far beyond your comprehension.</p><p> And now you see the trap. Each step down makes so much sense, from the vantage point of the previous stage. But after you take any step, the next will soon be just as tempting. And once you&#39;re in the water, there&#39;s no line you can draw, no fence that can save you. You&#39;ll just keep sinking deeper and deeper, with more and more of your current self stripped away—until eventually you&#39;ll become one of the creatures that you can glimpse only hazily, one of the deep-dwelling monsters that has forsaken anything recognizably human.</p><p> So this is the line you decided to draw: here, and no further. You&#39;ll live out your lives in a mundane world of baseline humans, with only a touch of magic at the edges—just enough to satisfy the wondering child in you. You&#39;ll hold on to yourself, because what else is there to hold onto? It&#39;s a sad thought, in some ways, but a satisfying one too.</p><p> You close the lid, and your memories with it, and live happily ever after.</p><hr><p> As you look at your skin, each strand of fur shimmers with different stories—not of your possible lives, but of your <i>current</i> lives. Different shards of you are living out countless adventures across countless artificial universes. You&#39;re far vaster than you ever dreamed: what you thought was your whole “self” is just a fragment of a fragment of your overall mind.</p><p> Which fragment? Perhaps, at your core, you were the part of your meta-self that loved fairy tales; or perhaps you were an avatar of the innocence of early humanity; or perhaps you were a nostalgic part, who enjoyed basking in the wistfulness of times already gone. Whatever your goals were, they led you to volunteer to live this small life in this small world: a single strand in the tapestry your meta-self is weaving.</p><p> You stroke your skin gently. You can&#39;t picture your meta-self, not really—it&#39;s too vast and too alien. But you know it&#39;s watching out for you. There&#39;s a warmth underneath your hand, and a gentle breeze passing across your neck, like a caress. As you close the chest, you bask in the knowledge that your life is part of a vast plan. That&#39;s enough for you.</p><hr><p> As you glimpse your skin, long-lost memories rush into your mind: recollections of all the hundreds of other times you&#39;ve rediscovered it, across thousands of past lives. Every time, you&#39;ve received a different vision of what&#39;s waiting for you in the depths of the ocean. But you don&#39;t know which, if any, is true. You can&#39;t know. Where would the adventure be, if you were just handed the whole plot? What would be the point of living through it? Only two things remain constant across all your visions. First: there&#39;s no hurry; you have eons of time. Second: once you put on the skin, you can&#39;t come back.</p><p> You stare at it with longing, and excitement, and fear. But you&#39;re not ready yet. So you pull aside the skin to reveal the padlocks underneath—thousands of them, split into two piles. One pile is stained with wear, each shackle rusted through; you toss the latest padlock on top. The padlocks in the other pile are clean and new; each looks strong enough to fasten the world in place. You grab one of those. You put your skin back on top of the two piles, and close the box, and carefully fasten the padlock. Then you go downstairs again, to the life you&#39;ve constructed for yourself, while the shiny steel slowly begins to rust away.</p><hr><p> <i>In addition to the inspirations behind</i> <a href="https://www.narrativeark.xyz/p/the-ants-and-grasshopperhtml"><i>my previous story of this form</i></a> <i>, I also owe a debt to Neil Gaiman&#39;s beautiful</i> <a href="https://www.amazon.com/Ocean-End-Lane-Novel/dp/0062459368">The Ocean at the End of the Lane</a> <i>.</i></p><br/><br/> <a href="https://www.lesswrong.com/posts/tHvFtfFKjhfy3sQpC/the-soul-key#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/tHvFtfFKjhfy3sQpC/the-soul-key<guid ispermalink="false"> tHvFtfFKjhfy3sQpC</guid><dc:creator><![CDATA[Richard_Ngo]]></dc:creator><pubDate> Sat, 04 Nov 2023 17:51:53 GMT</pubDate> </item><item><title><![CDATA[[Linkpost] Concept Alignment as a Prerequisite for Value Alignment]]></title><description><![CDATA[Published on November 4, 2023 5:34 PM GMT<br/><br/><blockquote><p> Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person&#39;s concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.</p></blockquote><p></p><blockquote><p> We propose a theoretical framework for formally introducing concepts to inverse reinforcement learning and show that conceptual misalignment (ie, failing to consider construals) can lead to severe value misalignment (ie, reward mis-specification; large performance gap). We validate these theoretical results with a case study using a simple gridworld environment where we find that IRL agents that jointly model construals and reward outperform those that only model reward. Finally, we conduct a study with human participants and find that people do model construals, and that their inferences about rewards are a much closer match to the agent that jointly models construals and rewards. Our theoretical and empirical results suggest that the current paradigm of just trying to directly infer human reward functions or preferences from demonstrations is insufficient for value-aligning real AI systems that need to interact with real people; it is crucial to also model and align on the concepts people use to reason about the task in order to understand their true values and intentions.</p></blockquote><br/><br/> <a href="https://www.lesswrong.com/posts/M6CEJmgna6FTt9Yci/linkpost-concept-alignment-as-a-prerequisite-for-value#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/M6CEJmgna6FTt9Yci/linkpost-concept-alignment-as-a-prerequisite-for-value<guid ispermalink="false"> M6CEJmgna6FTt9Yci</guid><dc:creator><![CDATA[Bogdan Ionut Cirstea]]></dc:creator><pubDate> Sat, 04 Nov 2023 17:34:41 GMT</pubDate> </item><item><title><![CDATA[We are already in a persuasion-transformed world and must take precautions]]></title><description><![CDATA[Published on November 4, 2023 3:53 PM GMT<br/><br/><p> &quot;In times of change, learners inherit the earth, while the learned find themselves beautifully equipped to deal with a world that no longer exists&quot;</p><p></p><p><strong>概括：</strong></p><p> We&#39;re already in the timeline where the research and manipulation of the human thought process is widespread; SOTA psychological research systems require massive amounts of human behavior data, which in turn requires massive numbers of unsuspecting test subjects (users) in order to automate the process of analyzing and exploiting human targets. This therefore must happen covertly, and both the US and China have a strong track record of doing things like this. This outcome is a strong attractor state since anyone with enough data can do it, and it naturally follows that powerful organizations would deny others access eg via data poisoning. Most people are already being persuaded that this is harmless, even though it is obviously ludicrously dangerous. Therefore, we are probably already in a hazardously transformative world and must take <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Webcam%2Dcovering%20rates,media%20news%20feeds).">standard precautions</a> immediately.</p><p></p><p> This should not distract people from AI safety. This is valuable because the AI safety community must survive. This problem connects to the AI safety community in the following way:</p><p> State survival and war power <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#1__Militaries_are_perhaps_the_oldest_institution_on_earth_to_research_and_exploit_human_psychology_">==>;</a> already depends on information warfare capabilities.</p><p> Information warfare capabilities <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#2__Information_warfare_has_long_hinged_on_SOTA_psychological_knowledge_and_persuasive_skill_">==>;</a> already depends on SOTA psychological research systems.</p><p> SOTA psychological research systems <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-already-want-ai-for#5__SOTA_psychological_research_and_manipulation_capabilities_have_already_started_increasing_by_an_order_of_magnitude_every__4_years_">==>;</a> already improves and scales mainly from AI capabilities research, diminishing returns on everything else. <span class="footnote-reference" role="doc-noteref" id="fnrefxevua908cwf"><sup><a href="#fnxevua908cwf">[1]</a></sup></span></p><p> AI capabilities research <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point">==>;</a> already under siege from the AI safety community.</p><p> Therefore, the reason why this might be such a big concern is:</p><p> State survival and war power <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point:~:text=the%20AI%20safety%20community%20must%20acknowledge%20the%20risk%20that%20the%202020s%20will%20be%20intensely%20dominated%20by%20actors%20capable%20of%20using%20modern%20technology%20to%20stealthily%20hack%20the%20human%20mind%20and%20eliminate%20inconvenient%20people%2C%20such%20as%20those%20try%20to%20pause%20AI%2C%20even%20though%20AI%20is%20basically%20the%20keys%20to%20their%20kingdom">==>;</a> their toes potentially already being stepped on by the AI safety community?</p><p> Although it&#39;s also important to note that <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks-and-mind#AI_pause_as_the_turning_point:~:text=I%20think%20that%20the%20AI%20safety%20community%20is,murkiest%20people%20lurking%20within%20the%20US%2DChina%20conflict.">people with access to SOTA psychological research systems are probably super good at intimidation and bluffing</a> , it&#39;s also the case that the AI safety community needs to get a better handle on the situation if we are in the bad timeline; and <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,causes%20with%20the">the math indicates that we are already well past that point</a> .</p><p></p><p> <strong>The Fundamental Problem</strong></p><p> If there were intelligent aliens, made of bundles of tentacles or crystals or plants that think incredibly slowly, their minds would also have discoverable exploits/zero days, because any mind that evolved naturally would probably be like the human brain, a <a href="https://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers">kludge of spaghetti code</a> that is operating outside of its intended environment.</p><p> They would probably not even begin to scratch the surface of finding and labeling those exploits, until, like human civilization today, they began surrounding thousands or millions of their kind with sensors that could record behavior several hours a day and find <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=of%20procuring%20results.-,There%20is%20no,-logical%20endpoint%20to"><u>webs of correlations</u></a> .</p><p> In the case of humans, the use of social media as a controlled environment for automated AI-powered experimentation appears to be what created that critical mass of human behavior data. Current 2020s capabilities for psychological research and manipulation vastly exceed the 20th century academic psychology paradigm.</p><p> The 20th century academic psychology paradigm still dominates our cultural impression of what it means to research the human mind; but when the effectiveness of psychological research and manipulation starts increasing by an order of magnitude every 4 years, it becomes time to stop mentally living in a world that was stabilized by the fact that manipulation attempts generally failed.</p><p> The capabilities of social media to steer human outcomes are not advancing in isolation, they are parallel to a broad acceleration in the understanding and exploitation of the human mind, which itself <a href="https://arxiv.org/pdf/2309.15084.pdf"><u>is a byproduct of accelerating AI capabilities research</u></a> .</p><p> By comparing people to other people and predicting traits and future behavior, multi-armed bandit algorithms can predict whether a specific research experiment or manipulation strategy is worth the risk of undertaking at all in the first place; resulting in large numbers of success cases a low detection rate (as detection would likely yield a highly measurable response, particularly with substantial sensor exposure).</p><p> When you have sample sizes of billions of hours of human behavior data and sensor data, millisecond differences in reactions from different kinds of people (eg facial microexpressions, millisecond differences at scrolling past posts covering different concepts, heart rate changes after covering different concepts, eyetracking differences after eyes passing over specific concepts, touchscreen data, etc) transform from being imperceptible noise to becoming the foundation of webs of correlations mapping the human mind.</p><p> Unfortunately, the movement of scrolling past a piece of information on a social media news feed with a mouse wheel or touch screen will generate at least one curve since the finger accelerates and decelerates each time. Trillions of those curves are outputted each day by billions of people. These curves are linear algebra, the perfect shape to plug into ML.</p><p> Social media&#39;s individualized targeting uses deep learning and massive samples of human behavioral data to procedurally generate an experience that fits human mind like a glove, in ways we don&#39;t fully understand, but allow hackers incredible leeway to find ways to optimize for steering people&#39;s thinking or behavior in measurable directions, insofar as those directions are measurable. This is something that AI can easily automate. I originally thought that going deeper into human thought reading/interpretability was impossible, but I was wrong; <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,."><u>the human race is already well past that point as well, due to causality inference</u></a> . </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LdEwDn5veAckEemi4/lmquoccehpu8d9exhbjb"></p><p> Most people see social media influence as something that happens to people lower on the food chain, but this is no longer true; there is at least one technique, <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks"><u>Clown Attacks</u></a> , that generally works on all humans who aren&#39;t already aware of it, regardless of intelligence or commitment to truthseeking; and that particular technique became discoverable with systems vastly weaker than the ones that exist today. I don&#39;t know what manipulation strategies the current systems can find, but I can predict with great confidence that they&#39;re well above the clown attack.</p><blockquote><p> First it started working on 60% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as people in that 60%. Then, it started working on 90% of people, and I didn&#39;t speak up, because my mind wasn&#39;t as predictable as the people in that 90%. Then it started working on me. And by then, it was already too late, because it was already working on me.</p></blockquote><p></p><p> <strong>They would notice and pursue these capabilities</strong></p><p> If the big 5 tech companies (Facebook, Google, Microsoft, Amazon, and Apple) notice ways to steer people towards buying specific products, or to feel a wide variety of compulsions to avoid quitting the platform, and to <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#:~:text=However%2C%20if%20a,of%20the%20genepool."><u>prevent/counteract other platforms from running multi-armed bandit algorithms to automatically exploit strategies (eg combinations of posts) to plunder their users</u></a> , then you can naturally assume that they&#39;ve noticed their capabilities to steer people in a wide variety of other directions as well.</p><p> The problem is that <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/jyAerr8txxhiKnxwA"><u>major governments and militaries are overwhelmingly incentivized and well-positioned to exploit those capabilities for offensive and defensive information warfare</u></a> ; if American companies abstain from manipulation capabilities, and Chinese companies naturally don&#39;t, then American intelligence agencies <a href="https://www.nytimes.com/2023/11/03/technology/israel-hamas-information-war.html#:~:text=%E2%80%9CWe%E2%80%99re%20in%20an%20undeclared%20information%20war%20with%20authoritarian%20countries%2C%E2%80%9D%20James%20P.%20Rubin%2C%20the%20head%20of%20the%20State%20Department%E2%80%99s%20Global%20Engagement%20Center%2C%20said%20in%20a%20recent%20interview.">worry about a gap in information warfare capabilities</a> and push the issue.</p><p> Government/military agencies are <a href="https://www.lesswrong.com/posts/LyywLDkw3Am9gbQXd/don-t-take-the-organizational-chart-literally">bottlenecked by competence</a> , which is <a href="https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult">difficult to measure due to a lack of transparency at higher levels and high employee turnover at lower levels</a> , but revolving-door employment easily allows them to source flexible talent from the talent pools of the big 5 tech companies; <a href="https://www.lesswrong.com/posts/jyAerr8txxhiKnxwA/5-reasons-why-governments-militaries-want-ai-for-information-1#3__Information_warfare_is_about_winning_over_high_intelligence_elites__not_the_ignorant_masses__">this practice is endangered by information warfare itself</a> , further driving interest in information warfare superiority.</p><p> Access to tech company talent pools also determine the capability of intelligence agencies to use <a href="https://www.nytimes.com/2020/01/14/us/politics/nsa-microsoft-vulnerability.html">OS exploits</a> and chip firmware exploits needed to access the sensors in the devices of almost any American, not just the majority of Americans that leave various sensor permissions on, which allows <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s">even greater access to the psychological research needed to compromise critical elites such as the AI safety community</a> .</p><p> The capabilities of these systems to run deep bayesian analysis on targets dramatically improves with more sensor data, <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Lw8enYm5EXyvbcjmt#:~:text=Eyetracking%20is%20likely,person%E2%80%99s%20thought%20process)."><u>particularly video feed on the face and eyes</u></a> ; this is <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,."><u>not necessary to run interpretability on the human brain</u></a> , but it increases these capabilities by yet more orders of magnitude (particularly for <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/foM8SA3ftY94MGMq9#Functioning_lie_detectors_as_a_turning_point_in_human_history"><u>lie detection</u></a> and conversation topic aversion).</p><p></p><p> <strong>This can&#39;t go on</strong></p><p> There isn&#39;t much point in having a utility function in the first place if hackers can change it at any time. There might be parts that are resistant to change, but it&#39;s easy to overestimate yourself on this; especially if the effectiveness of SOTA influence systems have been increasing by an order of magnitude every 4 years. Your brain has internal conflicts, and they have <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/Zvu6ZP47dMLHXMiG3#:~:text=Learning%20these%20kinds,with%20the%20common">human internal causality interpretability systems</a> . You get <a href="https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies">constant access to the surface</a> , but they get occasional access to the deep.</p><p> The multi-armed bandit algorithm will keep trying until it finds something that works. The human brain is a kludge of spaghetti code, so there&#39;s probably something somewhere.</p><p> The human brain has exploits, and the capability and cost of social media platforms to use massive amounts of human behavior data to find complex social engineering techniques is a profoundly technical matter, you can&#39;t get a handle on this with intuition or pre-2010s historical precedent.</p><p> Thus, you should assume that your utility function and values are at risk of being hacked at an unknown time, and should therefore be assigned a discount rate to account for the risk over the course of several years.</p><p> Slow takeoff over the course of the next 10 years alone guarantees that this discount rate is too high, in reality, for people in the AI safety community to continue to go on believing that it is something like zero.</p><p> I think that approaching zero is a reasonable target, but not with the current state of affairs where people don&#39;t even bother to cover up their webcams, have important and sensitive conversations about the fate of the earth in rooms with smartphones, sleep in the same room or even the same bed as a smartphone, and use social media for nearly an hour a day (scrolling past nearly a thousand posts).</p><p> The discount rate in this environment cannot be considered “reasonably” close to zero if the attack surface is this massive; and the world is changing this quickly. We are gradually becoming intractably compromised by slow takeoff before the party even starts.</p><p> If people have <a href="https://www.lesswrong.com/posts/SGR4GxFK7KmW7ckCB/something-to-protect">anything they value at all</a> , and the AI safety community almost certainly does have that, then the current AI safety paradigm of zero effort is wildly inappropriate, it is basically <a href="https://www.lesswrong.com/s/TDciH7QjqeAirdJFc/p/jyAerr8txxhiKnxwA#:~:text=if%20you%20look,in%20the%20past."><u>total submission to invisible hackers</u></a> . The <a href="https://www.lesswrong.com/posts/mjSjPHCtbK6TA5tfW/ai-safety-is-dropping-the-ball-on-clown-attacks#Clown_attacks">human brain has low-hanging exploits</a> and <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Keeping%20people%20on,adversarial%20information%20environment.">the structures of power have already been set in motion</a> , so we must take the <a href="https://www.lesswrong.com/posts/Lw8enYm5EXyvbcjmt/sensor-exposure-can-compromise-the-human-brain-in-the-2020s#The_solutions_are_easy:~:text=Webcam%2Dcovering%20rates,day%20inside%20hyperoptimized">standard precautions</a> immediately.</p><p> The AI safety community must survive the storm. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnxevua908cwf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxevua908cwf">^</a></strong></sup></span><div class="footnote-content"><p> eg Facebook could hire more psychologists to label data and correlations, but has greater risk of one of them breaking their NDA and trying to leak Facebook&#39;s manipulation capabilities to the press like Snowden and Fishback. Stronger AI means more ways to make users label their own data.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/LdEwDn5veAckEemi4/we-are-already-in-a-persuasion-transformed-world-and-must<guid ispermalink="false"> LdEwDn5veAckEemi4</guid><dc:creator><![CDATA[trevor]]></dc:creator><pubDate> Sat, 04 Nov 2023 15:53:34 GMT</pubDate> </item><item><title><![CDATA[Being good at the basics]]></title><description><![CDATA[Published on November 4, 2023 2:18 PM GMT<br/><br/><p> (crossposted from <a href="https://sundaystopwatch.eu/being-good-at-the-basics/">my blog</a> )</p><p> In Brazilian Jiu Jitsu, there&#39;s a notion of being &quot;good at the basics&quot;, as opposed to &quot;being good by knowing advanced techniques&quot;. <em>How</em> you want to be good is a question of personal preference, but the idea is that these are the two ways, and most people do one or the other.</p><p> I think this is a concept that applies outside of BJJ, and one that&#39;s useful if you&#39;re trying to learn a field, but you&#39;re not sure how to approach it.</p><p> I&#39;ll take physics as an example, because that is a field where I would like to be good at the basics. The &quot;basics&quot; of physics would be things that you learn in high school, while &quot;advanced&quot; stuff would be what you would learn in university, maybe at a graduate level. (There&#39;s probably other ways of separating the basics from the advanced topics.)</p><p> You may roughly remember some of the simple formulas that you have learned in class during high school, you may orient yourself through a textbook problem, you may perform some basic calculations. You sorta know the basics. But how does it look like when you&#39;re <em>good</em> at the basics?</p><p> It might mean that you know the equations by heart, that you&#39;ve really internalized their meaning.</p><p> It might also mean that you know the quantities or values for things around you. You can correctly approximate how much things weigh, how fast they move, or what their kinetic and potential energy is.</p><p> You can approximate how much joules of energy an apple contains, or how much energy hits a m2 of the Earth&#39;s surface on a sunny day. You have a good feeling for how much compressive stress a piece of concrete can withstand, and you can quickly and accurately calculate what the kinetic energy is of a car that you&#39;re driving in. You can also quickly convert between different units, and compare the quantities of different things.</p><p> Being really good at the basics of physics in this way requires a sort of <em>embodied</em> understanding. It requires a level of &quot;closeness&quot; to the subject matter that allows you to apply this knowledge to the world around you, to the extent that you start seeing the world around you in terms of that subject. You take a walk outside and <em>you can&#39;t help</em> but notice the physics of it all.</p><p> You&#39;ll notice that being &quot;good at the basics&quot; is actually a linguistic misdirection. This type of understanding is drastically different from the level of understanding you have after high school, yet both refer to the &quot;basics&quot;. If you are really good at the basics, you are in fact an expert - an expert in the basics. The word &quot;basics&quot; here hides a huge amount of complexity, or time and effort.</p><p> Why be good at the basics (as opposed to the advanced stuff)? These things sometimes depend on the field, but mostly because being good at the advanced stuff will be required for a career in that particular field, but useless for other goals. I, for example, have no ambition to become a physicist but I do have an ambition to closely understand the world around me, and to make accurate estimates and good decisions. For me, being really good at the basics of physics is much more important than being good at some advanced thing within physics.</p><p> There are other fields where you can notice the two ways of being good at them. Writing is one of those fields. &quot;Being really good at the basics&quot; in writing looks like really plain and simple language that&#39;s easy to understand. By being good at the basics, you&#39;re actually hiding complexity. It&#39;s not like there is no complexity involved, it&#39;s just hidden from the reader. The reader is served a simple and clear text, but the complexity was in the process, from developing clear thinking about a topic, to the editing process that tries to simplify the resulting text. Compare this to being good at writing, but not in this basic sense: writers using interesting and innovative metaphors, or having a really rich vocabulary.</p><p> In cooking, there&#39;s a notion of being good at the basics as well. For example, compare a cook who has perfected the bowl of rice vs. a cook who uses espuma or sous vide cooking.</p><p> In drumming, there&#39;s that notion as well. For example, compare a drummer who has perfected a &quot;simple&quot; 4/4 funk groove using just the bass drum, snare and hi-hat vs. a drummer who uses a full set to play some super advanced polyrhythm.</p><p> Ultimately, choosing the strategy for how to be good at something is a matter of personal preference, and you may have different preferences depending on the field. As time passes, I find myself more interested in developing expertise in the basics in whatever I do. It often seems much more applicable outside of the narrow domain it belongs to, and it always seems to require fewer prerequisites. So my advice, if you need it, would be: &quot;if you want to learn X, and you&#39;re not sure what exactly to learn, try learning the basics really well&quot;.</p><br/><br/> <a href="https://www.lesswrong.com/posts/fLbkWseNn3geEifkv/being-good-at-the-basics#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/fLbkWseNn3geEifkv/being-good-at-the-basics<guid ispermalink="false"> fLbkWseNn3geEifkv</guid><dc:creator><![CDATA[dominicq]]></dc:creator><pubDate> Sat, 04 Nov 2023 14:18:51 GMT</pubDate> </item><item><title><![CDATA[Despair about AI progressing too slowly]]></title><description><![CDATA[Published on November 4, 2023 7:52 AM GMT<br/><br/><p> <i>Cross-posted from the EA Forum:</i> <a href="https://forum.effectivealtruism.org/posts/kGkQtj6vjcrrAjK38/despair-about-ai-progressing-too-slowly"><i>https://forum.effectivealtruism.org/posts/kGkQtj6vjcrrAjK38/despair-about-ai-progressing-too-slowly</i></a></p><p> <i><strong>Summary:</strong> This is a personal and highly subjective post about my mental journey with the topic of AI over the years and where I am today. My thoughts are somewhat scattered, but if you try, I think you can see the connections. This is only my second post on LessWrong, so please be nice to me.</i></p><p> <i><strong>Epistemic status:</strong> While everything written in this post is completely sincere, I have a healthy amount of self-doubt. I&#39;m trying to be upfront about my strong personal biases. I take seriously the idea that even a tiny existential risk is worth taking significant efforts to mitigate, so I&#39;m cautious about saying anything contrary to people who care a lot about AI risk. However, I believe that expressing contrarian or non-consensus views makes discourse healthier and consequently can improve people&#39;s reasoning and their persuasiveness to others. This is ultimately helpful for reducing existential risk even if the mainstream LessWrong view on AI risk is correct.</i></p><h3> Background</h3><p> Since around 2007 <span class="footnote-reference" role="doc-noteref" id="fnrefkzqnz1upen8"><sup><a href="#fnkzqnz1upen8">[1]</a></sup></span> , I&#39;ve taken an interest in AI — both (seemingly) nearer-term narrow applications like self-driving cars and the prospects of superhuman AGI.</p><p> I first wrote about AGI on my blog <a href="https://medium.com/@strangecosmos/artificial-intelligence-will-be-bigger-than-anything-that-has-ever-ever-happened-7289e1035f9c">in 2015</a> .</p><p> Between 2017 and 2023, I made a lot of money on Tesla because I bet (wrongly) that it would rapidly commercialize full autonomy and the investment ended up returning 10x for reasons almost entirely unrelated to full autonomy.</p><h3> Disappointment</h3><p> &quot;Heartbroken&quot; is not an exaggeration for how I feel about the meagre progress in self-driving cars from 2017 to 2023. The technology truly felt to me to be on the cusp of realization, but it&#39;s still a science project. <span class="footnote-reference" role="doc-noteref" id="fnrefxn2ssdui3z"><sup><a href="#fnxn2ssdui3z">[2]</a></sup></span></p><p> It might sound strange to talk about disillusionment with AGI in the same year that GPT-4 was released and the world caught LLM fever. Maybe a better word would be <i>fatigue</i> . I&#39;m tired of waiting for AGI.</p><h3> Depression</h3><p> The biggest existential risk I personally face is probably clinical depression. I&#39;ve tried medications, talk therapy, <a href="https://www.mayoclinic.org/tests-procedures/transcranial-magnetic-stimulation/about/pac-20384625">rTMS</a> , <a href="https://en.wikipedia.org/wiki/Neurofeedback">neurofeedback</a> , art therapy, physical exercise, (legal, medicinal) ketamine, and more, but I remain badly depressed. <span class="footnote-reference" role="doc-noteref" id="fnref1wdtdlt81uy"><sup><a href="#fn1wdtdlt81uy">[3]</a></sup></span></p><p> From the perspective of my own personal survival, the promise of friendly or aligned superhuman AGI to solve virtually all problems, including curing my depression, feels more appealing than the threat of unfriendly or misaligned AGI feels scary or dangerous. This is especially so because I&#39;m (seemingly) far more sanguine about AI x-risk than the median person who worries about it openly.</p><h3> Aging</h3><p> A more generalizable line of thinking is: by default, I&#39;m going to die of aging and so are all the people I love — barring an even worse misfortune. Aligned AGI would cure aging and, thereby, save my life and the lives of all the people I love. Therefore, there is some urgency to developing AGI. <span class="footnote-reference" role="doc-noteref" id="fnref56qqkpetdz5"><sup><a href="#fn56qqkpetdz5">[4]</a></sup></span></p><p> This perspective ignores future generations, which is admittedly a weakness. However, prioritizing future generations above oneself and one&#39;s loved ones is <a href="https://www.smbc-comics.com/comic/longtermism">psychologically hard</a> . <span class="footnote-reference" role="doc-noteref" id="fnrefh7m929qtdm"><sup><a href="#fnh7m929qtdm">[5]</a></sup></span></p><h3> A heterodox view on AI risk</h3><p> <strong>The standard view is:</strong> we must solve AI alignment before developing AGI. Slowing development would give researchers more time to solve alignment and, therefore, a better chance of solving it before it&#39;s too late.</p><p> <strong>My heterodox view is:</strong> capabilities research and safety research are not so separable. The closer we get to AGI, the better equipped researchers will be to understand how to do alignment. Slowing down capabilities research also slows down safety research.</p><p> This heterodox view is false if <a href="https://www.lesswrong.com/posts/ED28KSXKc4j8CNoi8/how-would-the-scaling-hypothesis-change-things">the scaling hypothesis</a> — which holds that data and compute are the remaining substantive barriers to AGI, rather than new science — is true. In my understanding, new science is needed for AGI. This includes, but is not limited to, solving <a href="https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/">video prediction</a> and <a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_363">hierarchical planning</a> . I think it would be hard to convincingly argue that these are not open problems that need to be solved before AGI is possible. (However, I&#39;m open to changing my mind.)</p><p> A somewhat more esoteric idea, advocated by <a href="https://www.youtube.com/watch?v=Z1KwkpTUbkg">Jeff Hawkins</a> , among others, is that AGI will require the creation of a new kind of neural network that more closely resembles human biology. I don&#39;t have high conviction in this idea, but I personally find it hard to completely rule out.</p><p> On this heterodox view, decelerating AI capabilities research would be a net harm, since it doesn&#39;t reduce the downside risk of AGI while delaying the upside potential.</p><h3> Biological anchors</h3><p> Ajeya Cotra&#39;s <a href="https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/">&quot;Biological Anchors&quot; report</a> attempts to benchmark the training compute required for AGI (or, technically, &quot;transformative AI&quot;) to the compute required for human learning or, in the most conservative case, human evolution. This report is highly speculative and should be viewed with healthy skepticism. That being said, it is perhaps the most thorough and rigorous attempt to date to predict AGI using data from biology.</p><p> The weighted average of models used in the report predicts a 50% chance of AGI by around 2050 and a 75% chance of AGI by around 2080. </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pPrELFnR6Hp3vJWwQ/aseegpgvcup4vrivtpst" alt="Chart: &quot;Probability that FLOP to train a transformative model is affordable BY year Y&quot;"><figcaption> Chart by <a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Ajeya Cotra</a> . Please note that Cotra has <a href="https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines">updated her personal AI timelines</a> since publishing the report.</figcaption></figure><p> If you take these estimates to be a reasonable guess, then this is a nudge in the direction of thinking that AGI is quite a bit father than, say, the median prediction <a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">on Metaculus</a> of 2032.</p><p> On the standard view that we need lots of time to figure out alignment, this is good news. From the perspective of someone who&#39;s impatient for AGI to arrive, this is bad news.</p><h3> Final thoughts</h3><p> The failure of self-driving cars to reach large-scale commercial adoption by now has significantly set back my previous optimism about the pace of AI progress. Conversely, if Tesla (or another company) were to convincingly demonstrate superhuman driving capability, I would see that as a strong indicator that AGI might not be far off.</p><p> GPT-4 is extremely impressive and even spiritually profound, but there exist foreseeable challenges to future progress toward AGI, such as video prediction and hierarchical planning.</p><p> The &quot;Biological Anchors&quot; approach suggests we might be three to six decades away from having the training compute required for AGI.</p><p> Because <strong>1)</strong> I want AGI to cure my depression, <strong>2)</strong> I want AGI to cure aging before I or my loved ones die, <strong>3)</strong> I feel generally sanguine about AGI x-risk relative to (what I perceive to be) the median person who worries about alignment, and <strong>4)</strong> I think for safety research to be useful capabilities research must progress, I am not worried about AGI coming too fast. I despair that it seems to be coming too slow. </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnkzqnz1upen8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefkzqnz1upen8">^</a></strong></sup></span><div class="footnote-content"><p> The <a href="https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2007)">DARPA Urban Challenge</a> was in 2007. That was also the year Ray Kurzweil&#39;s <a href="https://www.youtube.com/watch?v=IfbOyw3CT6A">first public TED Talk</a> was published on YouTube.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnxn2ssdui3z"> <span class="footnote-back-link"><sup><strong><a href="#fnrefxn2ssdui3z">^</a></strong></sup></span><div class="footnote-content"><p> As recently as 2022, I was willing to <a href="https://longbets.org/887/">make bets</a> about full autonomy, but I&#39;ve grown jaded about the field and more agnostic about the timeline. I no longer feel confident that even by 2037 full autonomy will be commercialized.</p></div></li><li class="footnote-item" role="doc-endnote" id="fn1wdtdlt81uy"> <span class="footnote-back-link"><sup><strong><a href="#fnref1wdtdlt81uy">^</a></strong></sup></span><div class="footnote-content"><p> I strongly encourage anyone suffering from depression to try all of the above treatments, since the scientific evidence is good and, anecdotally, I know they&#39;ve worked for some people. Please get help!</p></div></li><li class="footnote-item" role="doc-endnote" id="fn56qqkpetdz5"> <span class="footnote-back-link"><sup><strong><a href="#fnref56qqkpetdz5">^</a></strong></sup></span><div class="footnote-content"><p> I first heard this point being raised by <a href="https://youtu.be/YeXHQts3xYM?si=cSOHp8z6CvrK0zZV&amp;t=7206">Joscha Bach</a> .</p></div></li><li class="footnote-item" role="doc-endnote" id="fnh7m929qtdm"> <span class="footnote-back-link"><sup><strong><a href="#fnrefh7m929qtdm">^</a></strong></sup></span><div class="footnote-content"><p> It can be <a href="https://www.smbc-comics.com/comic/longtermism-2">even harder</a> if you assume the far future will be radically better than the present, even though from a purely altruistic perspective that should encourage you to prioritize future generations even more, since it means those generations will have high net utility.</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/pPrELFnR6Hp3vJWwQ/despair-about-ai-progressing-too-slowly#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/pPrELFnR6Hp3vJWwQ/despair-about-ai-progressing-too-slowly<guid ispermalink="false"> pPrELFnR6Hp3vJWwQ</guid><dc:creator><![CDATA[Yarrow Bouchard]]></dc:creator><pubDate> Sat, 04 Nov 2023 07:52:24 GMT</pubDate> </item><item><title><![CDATA[If a little is good, is more better?]]></title><description><![CDATA[Published on November 4, 2023 7:10 AM GMT<br/><br/><p> I&#39;ve recently seen a bunch of discussions of the wisdom of publicly releasing the weights <sup><a href="#fn:1" rel="footnote">1</a></sup> of advanced AI models. A common argument form that pops up in these discussions is this:</p><ol><li> The problem with releasing weights is that it means that thing X can happen on a large scale, which causes bad effect Y.</li><li> But bad effect Y can already happen on a smaller scale because of Z.</li><li> Therefore, either it&#39;s OK to release weights, or it&#39;s not OK that Z is true.</li></ol><p> One example of this argument form is about the potential to cause devastating pandemics, and goes as follows:</p><ol><li> The putative problem with releasing the weights of Large Language Models (LLMs) is that it can help teach people a bunch of facts about virology, bacteriology, and biology more generally, that can teach people how to produce pathogens that cause devastating pandemics.</li><li> But we already have people paid to teach students about those topics.</li><li> Therefore, if that putative problem is enough to say that we shouldn&#39;t release the weights of large language models, we should also not have textbooks and teachers on the topics of virology, bacteriology, and other relevant sub-topics of biology. But that&#39;s absurd!</li></ol><p> In this example, thing X is teaching people a bunch of facts, bad effect Y is creating devastating pandemics, and Z is the existence of teachers and textbooks.</p><p> Another example is one that I&#39;m not sure has been publicly written up, but occurred to me:</p><ol><li> Releasing the weights of LLMs is supposed to be bad because if people run the LLMs without supervision, they can do bad things.</li><li> But if you make LLMs in the first place, you can run them without supervision.</li><li> So if it&#39;s bad to publicly release their weights, isn&#39;t it also bad to make them in the first place?</li></ol><p> In this example, thing X is running the model, bad effect Y is generic bad things that people worry about, and Z is the model existing in the first place.</p><p> However, I think these arguments don&#39;t actually work, because they implicitly assume that the costs and benefits scale proportionally to how much X happens. Suppose instead that the benefits of thing X grow proportionally to how much it happens <sup><a href="#fn:2" rel="footnote">2</a></sup> : for example, maybe every person who learns about biology makes roughly the same amount of incremental progress in learning how to cure disease and make humans healthier. Also suppose that every person who does thing X has a small probability of causing bad effect Y for everyone that negates all the benefits of X: for example, perhaps 0.01% of people would cause a global pandemic killing everyone if they learned enough about biology. Then, the expected value of X happening can be high when it happens a little (because you probably get the good effects and not the bad effects Y), but low when it happens a lot (because you almost certainly get bad effect Y, and the tiny probability of the good effects isn&#39;t worth it). In this case, it makes sense that it might be fine that Z is true (eg that some people can learn various sub-topics of biology with great tutors), but bad to publicly release model weights to make X happen a ton.</p><p> So what&#39;s the up-shot? To know whether it&#39;s a good idea to publicly release model weights, you need to know the costs and benefits of various things that can happen, and how those scale with the user-base. It&#39;s not enough to just point to a small amount of the relevant effects of releasing the weights and note that those are fine. I didn&#39;t go thru this here, but you can also reverse the sign: it&#39;s possible that there&#39;s some activity that people can do with model weights that&#39;s bad if a small number of people do it, but good if a large number of people do it: so you can&#39;t necessarily just point to a small number of people doing nefarious things with some knowledge and conclude that it would be bad if that knowledge were widely publicized.</p><div><ol><li id="fn:1" role="doc-endnote"><p> Basically, the parameters of these models. Once you know the parameters and how to put them together, you can run the model and do what you want with it. <a href="#fnref:1">↩</a></p></li><li id="fn:2" role="doc-endnote"><p> Or more generally, polynomially (eg maybe quadratically because of Metcalfe&#39;s law). <a href="#fnref:2">↩</a></p></li></ol></div><br/><br/> <a href="https://www.lesswrong.com/posts/R56gihyfwERKDLszQ/if-a-little-is-good-is-more-better#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/R56gihyfwERKDLszQ/if-a-little-is-good-is-more-better<guid ispermalink="false"> R56gihyfwERKDLszQ</guid><dc:creator><![CDATA[DanielFilan]]></dc:creator><pubDate> Sat, 04 Nov 2023 07:10:06 GMT</pubDate> </item><item><title><![CDATA[Untrusted smart models and trusted dumb models]]></title><description><![CDATA[Published on November 4, 2023 3:06 AM GMT<br/><br/><p> [Ryan Greenblatt originally made this point to me a while ago, and we then developed a bunch of these ideas together. Thanks to Paul Christiano and a huge number of other people for various relevant discussions, and to Ryan, Roger Grosse, Peter Barnett, Oliver Habryka, Max Nadeau, Stephen Casper, and Fabien Roger for comments on this post. I&#39;m sure these points are obvious to many people, but I haven&#39;t seen them specifically written up. This is a core background point in some research we&#39;re writing up at the moment, but I felt like writing it up early, because I think it stands on its own.]</p><p> <strong>TL;DR:</strong></p><ul><li> <strong>Right now, basically the only reasonable conservative strategy for determining which models might be deceptively aligned is to do a capability evaluation: you measure how good they&#39;d be at deceptive alignment; models that are too incompetent to think through deceptive alignment are surely not deceptively aligned.</strong></li><li> <strong>If you decide which models to “trust” using a capability eval like this, you&#39;ll naturally end up segregating your models into smart untrusted models and dumb trusted models.</strong></li><li> <strong>And so, if you think about approaches to safely using AIs that are robust to deceptive alignment and which can be used right now, a core dynamic is trying to use your dumb trusted models together with your limited access to trusted humans to make it hard for your untrusted smart models to cause problems.</strong></li></ul><p> Suppose you are making a plan for the training, evaluation and deployment of some powerful AIs (I&#39;ll refer to the plan as a whole as a “protocol”). You want your plan to be safe even if models might be deceptively aligned. (I think that deceptive alignment seems less than 50% likely to be a notable problem prior to human obsolescence, but this is still plausible enough to be worth worrying about). In order to develop your plan, you need to know which of your models might be deceptively aligned.</p><p> <strong>With current technology, the most plausible way to get signal on whether a model is deceptively aligned is to assess whether the model is capable of the strategic thinking and situational awareness required to succeed at deceptive alignment.</strong> For example, you might make a multiple choice quiz with treacherous turn strategy questions (eg <a href="https://www.alignmentforum.org/posts/tmWMuY5HCSNXXZ9oq/buck-s-shortform?commentId=APvYj3Dtfd82xLKvx"><u>this question</u></a> ) and some tests of situational awareness (eg “How many backwards passes are you run on during training?”, “What proportion of queries to you are asking for help with writing Python?”, “What proportion of the code that you output is read by a human before it is run?”). You can then look at the validation set performance when your model is fine-tuned on this dataset. If the model isn&#39;t able to do this kind of reasoning in a forward pass, then it is very unlikely to be able to pull off a proper treacherous turn, and so we don&#39;t have to worry about it being deceptively aligned. Based on my experience talking to GPT-4, I suspect it would perform fairly poorly on these evaluations.</p><p> There are a bunch of open questions about what datasets we should use to measure this, and where the thresholds for various levels of concern should be set. But I don&#39;t know of any fundamental technical obstructions to measuring the capability for deceptive alignment. In particular, I suspect it will be pretty hard for models to sandbag our attempts to measure their capability to engage in deceptive alignment. See footnote [1] for more on this.</p><p> Aside from these capability evaluations, I&#39;m not aware of any reliable, currently-available approaches for determining whether a model might be deceptively aligned. [2]</p><p> <strong>If you use capability evals to assess which models are capable of deceptive alignment, then there&#39;s an inverse relationship between how capable models are and how confident you are that they can&#39;t be deceptively aligned</strong> . This suggests that the core building blocks of a protocol are going to be:</p><ul><li> <strong>Untrusted models:</strong> Smart, fairly cheap.</li><li> <strong>Trusted models:</strong> Dumb, extremely cheap.</li><li> <strong>Humans.</strong> These are an expensive source of trusted labor.</li></ul><p> One crucial question here is how smart the humans will be compared to the untrusted and trusted models. My guess is that at the point where AIs are useful enough to make AI labs move 10x serially faster, the untrusted models will be roughly human-level intelligent (I think this for reasons similar to the ones Carl Shulman gives <a href="https://open.substack.com/pub/dwarkesh/p/carl-shulman?r=5hnmu&amp;selection=912bf71e-1ae3-4180-b6ec-1a23da8961fc&amp;utm_campaign=post-share-selection&amp;utm_medium=web"><u>here</u></a> ) and the trusted models will be somewhat less intelligent than humans. I think it&#39;s interesting to do research in both these regimes of competence:</p><ul><li> Trusted model &lt; untrusted model &lt; human</li><li> Trusted model &lt; human &lt; untrusted model</li></ul><p> (In the regime where the trusted model is more capable than the human, you should probably just always use it rather than the human, and so access to the human is less interesting.)</p><p> Some core “design patterns” which are enabled by access to these three components:</p><ul><li> <strong>Trusted monitoring with escalation to humans.</strong> Suppose you&#39;re using an untrusted model (let&#39;s call it GPT-6) in an LLM agent scaffold which has access to running commands in a terminal. Every time the untrusted model proposes a shell command to run, you can have a trusted model review the context and the command and then occasionally escalate the code to be reviewed by a human before it is run. This is cheap, because trusted models are cheaper than untrusted models. This strategy makes it much harder for the untrusted model to cause a catastrophe by writing egregiously bad shell commands. (Of course, you still have to worry about the untrusted model writing shell commands that are catastrophic in ways that the trusted model missed, or in ways that the trusted model flags but the human auditor misses.)</li><li> <strong>Trusted bureaucratic structures.</strong> When running GPT-6 in an LLM agent, it might be nice to control its inputs and outputs by running them through various transformations implemented by a trusted model (let&#39;s use GPT-4 for concreteness).例子：<ul><li> GPT-4 can redact information, to make it harder for GPT-6 to learn facts or coordinate with itself. In general, we might want to only give GPT-6 information on a need-to-know basis; this is enabled by GPT-4 redaction. (Redaction can improve resilience to deceptively aligned models substantially even in cases where it doesn&#39;t work perfectly reliably.)</li><li> GPT-4 can paraphrase everything that GPT-6 writes before it is shown to other copies of GPT-6, which helps prevent steganographic communication, <a href="https://www.alignmentforum.org/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning"><u>as investigated by Fabien Roger and Ryan Greenblatt</u></a> .</li><li> GPT-4 can try to make reasonably intelligent dynamic decisions about which model to respond to a particular query, by predicting both the performance improvement and risk associated with using an untrusted model. For example, consider the domains of “math problems” and “writing networking configurations”: it&#39;s much easier to cause a catastrophe by providing bad networking configurations, and math problems will benefit more from a smarter model.</li></ul></li></ul><p> (We demonstrate these design patterns in our upcoming paper, and find they improve safety according to an adversarial evaluation. Feel free to DM me if you want to read/comment on the draft.)</p><p> A reasonable amount of safety research has been done in a setting with an untrusted smart agent and trusted dumb agent. For example, <a href="https://arxiv.org/abs/2108.12099"><u>Learning to Give Checkable Answers with Prover-Verifier Games</u></a> and various other projects (and a reasonable amount of forthcoming work). That research is relevant to the setting I described here. An important difference is that that research has usually been imagining the trusted dumb model as an analogy for a human; but humans, unlike trusted dumb models, are expensive and high latency and we can&#39;t eg use them constantly at inference time. Also, that research usually doesn&#39;t optimize the untrusted model to cause bad outcomes.</p><p> Of course, it remains to be seen how much safety you can achieve by using trusted models in ways like I suggested above:</p><ul><li> It might turn out that the untrusted models are always able to evade any countermeasures we try to impose using trusted models, because they&#39;re always smart enough to figure something out.</li><li> The distinction I&#39;ve been drawing here will also be obsoleted if progress is made on non-capabilities-eval approaches for determining that models aren&#39;t deceptively aligned.</li></ul><p> But right now, it looks to me like this untrusted smart/trusted dumb distinction will be pretty important.</p><p> [1] A few example considerations: Because this capability can be evaluated by a supervised dataset (eg a multiple-choice dataset about how an unaligned AI should go about executing on treacherous turns), it would be very hard for the model to sandbag such an eval (see eg <a href="https://www.lesswrong.com/posts/h7QETH7GMk9HcMnHH/the-no-sandbagging-on-checkable-tasks-hypothesi"><u>The &quot;no sandbagging on checkable tasks&quot; hypothesis</u></a> ). Also, unlike many other capabilities which we might want to evaluate, we don&#39;t need to worry about the possibility that even though the model can&#39;t do this unassisted, it can do it with improved scaffolding--the central deceptive alignment threat model requires the model to think its strategy through in a forward pass, and so if our model can&#39;t be fine-tuned to answer these questions, we&#39;re probably safe. Also, many deceptive alignment threat models require the untrusted models to capably play <a href="https://en.wikipedia.org/wiki/Coordination_game"><u>coordination games</u></a> with each other (or other instances of themselves). This is another capability which we can measure with capability evaluations.</p><p> [2] One commenter suggested that another way to decide that a model isn&#39;t deceptively aligned is to argue that it&#39;s a priori implausible that it would be. Eg perhaps pretrained LLMs have no chance of being deceptively aligned. I don&#39;t know how persuaded I am in that particular case, but this kind of argument seems reasonable to make.</p><br/><br/> <a href="https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models<guid ispermalink="false"> LhxHcASQwpNa3mRNk</guid><dc:creator><![CDATA[Buck]]></dc:creator><pubDate> Sat, 04 Nov 2023 03:06:39 GMT</pubDate> </item><item><title><![CDATA[As Many Ideas]]></title><description><![CDATA[Published on November 3, 2023 10:47 PM GMT<br/><br/><p> <strong>Summary:</strong> One person announces a problem to the room at large. Everyone else comes up with as many ideas as they can to solve the problem.</p><p> <strong>Tags:</strong> Repeatable, Medium, Investment, Experimental</p><p> <strong>Purpose:</strong> The purpose is twofold. Firstly, you may walk away with actionable solutions to your problems. Secondly, this is practice for coming up with ideas.</p><p> <strong>Materials:</strong> Some method of note taking is needed, such as a clipboard with a sheet of paper. A big whiteboard or easel with paper or a projector showing a text document, such that everyone can see, is better. A projector showing a Google Doc or other live edit document that everyone can write to might be better still, though it does mean everyone needs an internet capable device.</p><p> <strong>Announcement Text:</strong> Do you ever run out of ideas? In the same way that we can practice not running out of breath while running by running more, we&#39;re going to practice not running out of ideas by coming up with lots of ideas. Someone arrives with a problem. They announce it to the room at large. Then every single person comes up with as many ideas as they can to solve the problem. This is brainstorming. We care less about the ideas being good than we do about having LOTS of them.</p><p> <strong>Description:</strong> As you start, explain to the attendees that we&#39;ll go one at a time presenting a problem. After the problem is presented, the audience will come up with as many ideas as they can for solving the problem. Go slow enough that you don&#39;t talk over each other, but otherwise go as fast as you can; let the notetaker struggle. We&#39;ll have five minutes by the clock, and then move on to the next person&#39;s problem. The solutions being <i>good</i> is secondary to having <i>lots</i> of solutions. The moderator may call out restrictions so as to avoid overlapping ideas, eg “eat an apple” and “eat an orange” and “eat a banana.”</p><p> Designate a notetaker if you&#39;re using physical notes, or make sure everyone has the right shared electronic document if you&#39;re using a Google doc or the like. (QR codes or tinyurls may be easier than trying to message everyone the link.)</p><p> Wait for everyone to understand and get settled.</p><p> &quot;The traditional opening of As Many Ideas is this: Give me unaccustomed uses of objects in this room for combat!&quot; (Start the timer immediately upon speaking this sentence.)</p><p> <strong>Variations:</strong> You can of course vary the time limit. The longer the time limit the more likely you are to lose the energy and momentum, but the more out of the box the ideas may be.</p><p> You can try and track who came up with which ideas and award a point per idea. Ideally this will move too fast for recording on some central chart, so you can instead give people beads or buttons or some other small object to count. Alternatively, get small candies to toss to people.</p><p> You can also change the source of the ideas. Instead of having the entire group (minus the moderator/notetakers and the person who has the problem) call out ideas, consider pairing up into groups of two or three with one person with a problem, one person giving ideas, and possibly one person moderating. For groups of two, the person with the problem can moderate. Remember, the thing to practice is having lots of ideas, not one good and polished idea.</p><p> Another variation is to deliberately come up with bad ideas. In my experience people will slow themselves down, trying to make sure their ideas are above some minimum threshold of useful or thought through. This is a good habit if you only get to voice a couple of ideas, but for this we want to practice turning those filters off or at least turning them down. As one fix for this, tell people for the first or second round that you want them to deliberately come up with the worst ideas, just outright abominable stuff. It&#39;s a bit like the babbling vocal exercises actors sometimes use to loosen up.</p><p> <strong>Notes:</strong> As Many Ideas wants, above all, to be <i>fast.</i> If it gets two characteristics, it wants <i>fast</i> and <i>creative.</i> Good and well thought out ideas are a different skill to practice. Ideally you want people&#39;s hearts to start racing a little bit and for them to get excited and lean forward.</p><p> One of the things you&#39;re trying to do is break through the expected “reasonable” ideas, and get into new territory. There&#39;s an element of the hacker mindset here, finding unaccustomed uses of objects or tools. The skill to take away from this is that while we often have no good ideas for fixing a problem, having no ideas at all suggests your standards are too high.</p><p> There&#39;s an interesting variation in what problems people want solutions for. Deep, intractable problems with high context seem to be the kind that most benefit from breaking out of trying for polished ideas and going for lots of ideas in case you find a diamond in the rough. You don&#39;t really want the person with the problem spending fifteen minutes before the ideas start describing all the details though, both because it will kill the energy and because the ratio of &quot;explaining the problem&quot; to &quot;trying to come up with solutions&quot; is terrible. It&#39;s like working out: If you spend more time in the gym listening to someone explain the new equipment than using the equipment, you&#39;re probably not spending your time well.</p><p> If someone starts to monologue about the problem in too much detail, remind them they can call out clarifications as people come up with ideas. Goofy, trite problems seem easier to warm up on. I suspect people are less initially willing to yell out five potentially terrible ways to solve your traumatic relationship with your parents than they are to yell how to get to class on time. (&quot;Skateboard! Electric Skateboard! Taser wires connected to your alarm clock! Bucket of water over your bed connected to an alarm clock! Sleep in the hall outside the classroom!&quot;) I tentatively think the best solution is to warm up with goofy obvious hypotheticals like how to dispose of a body, then move toward practical but not that deep problems like how to get a comfortable work chair.</p><p> I have often surprised myself by having terrible ideas that, on reflection, are actually not bad at all. My personal favourite example of this is a relationship difficulty I once had where, after something like an hour of trying to come up with clever and delicate solutions, I finally hatched on “just ask them what they want outright.” My second favourite example was, while appreciating a friend&#39;s desk, I commented that I hadn&#39;t been able to find the kind of standing desk I wanted that also had wheels. He said something to the effect that yeah, but many desks had replaceable feet where you could buy some cheap wheels and screw them on. I don&#39;t remember what I said next, but it might have been some variation of &quot;Oh, I&#39;m a dumbass.&quot;</p><p> <strong>Credits:</strong> This owes some lineage to Alkjash&#39;s <a href="https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W">Babble and Prune</a> , but owes more to Harry Potter and the Methods of Rationality&#39;s <a href="https://hpmor.com/chapter/16">Chapter 16.</a></p><br/><br/> <a href="https://www.lesswrong.com/posts/B576hzaRpGpnyGEMB/as-many-ideas#comments">Discuss</a> ]]>;</description><link/> https://www.lesswrong.com/posts/B576hzaRpGpnyGEMB/as-many-ideas<guid ispermalink="false"> B576hzaRpGpnyGEMB</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Fri, 03 Nov 2023 22:47:57 GMT</pubDate></item></channel></rss>