<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 11 月 7 日，星期二 16:16:03 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Model Psychology]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p><i>这篇文章是模型心理学系列的一部分</i></p><p>在上一篇文章中，我讨论了为什么我认为 SOTA LLM 不是随机鹦鹉。</p><h2><strong>基础</strong></h2><p>在讨论模型心理学的定义之前，我将展示我构建模型心理学的基础：<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟器</u></a>理论。</p><p>这个想法是，大型语言模型是在整个网络上训练的预测模型，通过文本模拟代理和非代理实体（这些实体被称为拟像）。这些“模拟器”本身并不是实体<span class="footnote-reference" role="doc-noteref" id="fnrefp15lqmjkj3n"><sup><a href="#fnp15lqmjkj3n">[1]</a></sup></span> ；而是它们本身。然而，当条件适当时，它们可以模拟具有目标导向行为的实体。这类似于物理引擎如何模拟惰性岩石和复杂的生物体，而不需要它们本身。</p><p>从上一篇文章开始，这个观点重塑了我们对使用指令微调语言模型时所交互的内容的理解。我们接触的不仅仅是数字助理；还有数字助理。它们是模拟器的表现形式，经过微调以模拟有用且安全的助手（例如 ChatGPT）的行为。基础模型（经过微调成为 ChatGPT 之前的 GPT）更广泛的功能仅限于模拟人类和有用的安全助手之间的对话。微调模型以成为一个好的助手通常是“创造力”和“模拟准确性”之间的权衡，这通常会导致模型缺乏创造力而更具确定性。在被教导成为助手之前，模型总是不确定文本的上下文，这使得它的完成变得混乱但非常有创意（在<a href="https://generative.ink/posts/language-models-are-multiverse-generators/#minds-are-multiverse-generators"><u>这篇文章</u></a>中， <a href="https://www.lesswrong.com/users/janus-1?mention=user">@janus</a>将法学硕士的这一方面暴露为“多元宇宙生成器”）。尽管，一旦它被训练为助手（“何时”仍有待发现），它就会将上下文视为助手和人类之间的对话，从而使其完成方式不再那么混乱，并趋向于类似的<a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse"><u>模式</u></a>（或者为简单起见，我稍后将其称为拟像的偏好）。</p><p>总而言之，gpt-4 和所有其他指导微调的 LLM 都是专门用于生成安全且有用的助手（例如 ChatGPT）的代理拟像（在整个对话中至少部分一致地表现的拟像）的模拟器。然而，我们将在整个序列中看到证据表明，他们并没有一直模拟相同的拟像，而是<strong>针对当前上下文的连贯的拟像。</strong></p><h2><strong>定义</strong></h2><p>暂定定义<span class="footnote-reference" role="doc-noteref" id="fnrefqnruab69z4q"><sup><a href="#fnqnruab69z4q">[2]</a></sup></span> ：</p><blockquote><p>模型心理学是研究法学硕士如何模拟其所模拟的代理实体<span class="footnote-reference" role="doc-noteref" id="fnrefwo5g8ik80x"><sup><a href="#fnwo5g8ik80x">[3]</a></sup></span><strong>的行为</strong><strong>。</strong></p></blockquote><p>为了做到这一点，我想到了两种简单的方法：</p><h3><strong>研究聊天模型的默认拟像。</strong></h3><p>但是，如前所述，不存在默认拟像之类的东西。我们将在整个序列中看到，尽管 OpenAI、Anthropic……训练他们的聊天模型来生成有用助手的拟像，但拟像的偏好仍然很大程度上受到对话上下文的影响。</p><p>尽管如此，研究聊天模型相对于基本模型的优势在于，这些模拟器有稳定当前拟像的倾向（稍后将在序列中展示）。它倾向于模拟与过去的助手示例相符的行为。它不会完全改变段落中的话语或主题，这在基本模型中经常发生。</p><h3><strong>在将模拟器微调为助手之前，通过与基本模型交互来研究模拟器本身（@janus</strong> <a href="https://www.lesswrong.com/users/janus-1?mention=user"><strong>）</strong></a> <strong>。</strong></h3><p>尽管这是研究​​原始模拟的一种非常好的方法，并且不受人类偏好的影响，但这种方法仍然存在多个问题：</p><ul><li>它们非常混乱并且很难研究。拟像非常不稳定，很难根据所看到的内容得出结论。</li><li>由于基础模型的混乱方面，以及实验室似乎想要连贯的完成，甚至代理，AGI 可能不会是一个基础模型（但如果 OpenAI 找到一种方法，它可能会是一个经过微调的基础模型）规模微调超越人类智能<span class="footnote-reference" role="doc-noteref" id="fnrefoskge06f7s"><sup><a href="#fnoskge06f7s">[4]</a></sup></span> ）。</li><li>拟像中有趣的现象（例如偏好，我们稍后会讨论）在基本模型中很难看到。</li><li>你并不总是与代理拟像互动。有时它们只是 Oracle 或工具的预测（您可以查看<a href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators"><u>模拟器博客文章</u></a>以获取更多详细信息）。</li><li>对 SOTA 基础模型的访问非常有限（如果 OpenAI 的人读了这篇文章，我很想能够访问 gpt4-base 🙃）</li></ul><p>这两种方法有时对于特定研究都很有用（我们将讨论其中的一些），但总的来说，它们有点平淡无奇。然而，还有另一种我认为更有希望的方法。</p><h3> <strong>Neo（控制矩阵）</strong></h3><p>采用指导微调模型之一（例如 gpt-4），修改上下文以某种特定且稳定的方式构建拟像（为此，您需要模型心理学家技能，您将在以后的帖子中学习这些技能），然后观察行为如何在较小的上下文变化之间变化。</p><p>通过研究拟像中行为的变化与小受控变化的比较，我们可以尝试理解决定个体拟像行为的潜在机制。</p><p>例如，如果您想研究 gpt-4 如何模拟人类愤怒的细微差别，您可以构建一个大提示，其中包含有关其应该扮演的角色的详细说明，并通过仅更改愤怒来查看行为如何变化提示中的级别（“有点生气”、“生气”、“非常生气”、“激怒”……）</p><p>您甚至可以进一步研究整个法学硕士培训过程中拟像的演变，并了解这些机制如何在训练时间步长之间或训练数据中的微小具体变化之间演变。</p><p>模型心理学的最终目标之一是对<strong>训练和环境如何塑造模拟动态</strong>有深入的了解（我们将在序列的结论中讨论其他目标，因为您需要更多的环境来理解它们）。</p><p></p><p>在接下来的文章中，我们将深入探讨进行此类研究的不同方法，以及为什么这个领域可能有前途。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnp15lqmjkj3n"> <span class="footnote-back-link"><sup><strong><a href="#fnrefp15lqmjkj3n">^</a></strong></sup></span><div class="footnote-content"><p>尽管如此，在某个时刻它“醒来”并开始对其世界模型的“自身”部分有一个强大的内部表示（这可能是你开始受到欺骗的时刻），这可能并非不可能。然后它可能会成为一个实体，但我怀疑没有某种循环是否可能。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnqnruab69z4q"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqnruab69z4q">^</a></strong></sup></span><div class="footnote-content"><p>不要将此作为最终定义。还不够令人满意，因为研究的主题是什么还不够清楚。一旦变得更加清晰，我们也许就能有一个更准确的定义。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnwo5g8ik80x"> <span class="footnote-back-link"><sup><strong><a href="#fnrefwo5g8ik80x">^</a></strong></sup></span><div class="footnote-content"><p> *<strong>吹毛求疵：</strong> Simulacrum 的意思是“对人或事物的再现或模仿”。法学硕士不是模拟实体，而是模拟这些实体的内部表示。说它们是在模拟拟像而不是说它们是在模拟实体可能是有道理的。但这会使定义过于复杂。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnoskge06f7s"> <span class="footnote-back-link"><sup><strong><a href="#fnrefoskge06f7s">^</a></strong></sup></span><div class="footnote-content"><p>我是说，因为从我收集到的提示来看，gpt-4 聊天远不如 gpt-4-base“智能”，而且似乎停留在人类水平附近，而 gpt 的情况似乎并非如此- 3.5</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/LfCrDAx39QPBNT75r/model-psychology-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/LfCrDAx39QPBNT75r/model-psychology-1<guid ispermalink="false"> LfCrDAx39QPBNT75r</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:35 GMT</pubDate> </item><item><title><![CDATA[The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p><i>这篇文章是模型心理学系列的一部分</i></p><p><a href="https://www.lesswrong.com/users/pierre-peigne?mention=user"><i>@Pierre Peigné</i></a><i>在论点 3 和另一个奇怪的现象中写了细节部分。</i>其余部分是用<a href="https://www.lesswrong.com/users/quentin-feuillade-montixi?mention=user"><i>@Quentin FEUILLADE--MONTIXI</i></a><i>的声音写的</i><i>&nbsp;</i></p><h2><strong>介绍</strong></h2><p>在深入探讨什么是模型心理学之前，澄清我们正在研究的学科的性质至关重要。在这篇文章中，我将挑战最先进的大型语言模型（≈GPT-4）的普遍争议的<strong>随机鹦鹉假设</strong>，在下一篇文章中，我将阐明我正在构建模型的基础心理学来自。</p><p>随机鹦鹉假说表明，法学硕士尽管能力非凡，但并不能真正理解语言。他们就像鹦鹉一样，复制人类的言语模式，却没有真正掌握他们所说的话的本质。</p><p>虽然我之前认为这个论点已经被遗忘，但我经常发现自己陷入了长时间的争论，为什么当前的 SOTA 法学硕士超越了这种简单化的观点。大多数时候，人们争论是否使用 GPT3.5 的示例，而不了解 GPT-4 的强大功能。通过这篇文章，我使用模型心理学工具来表达我目前的立场，反对这一假设。让我们深入探讨一下这个论点。</p><p>我们辩论的核心是“<strong>世界模式</strong>”的概念。世界模型代表了一个实体的内部理解和对其所生活的外部环境的表示。对于人类来说，它是我们对周围世界、它如何运作、概念如何相互作用以及我们在其中的位置的理解。随机鹦鹉假说挑战了法学硕士拥有稳健的世界模型的观念。这表明，虽然他们可能以令人印象深刻的准确性再现语言，但他们缺乏对世界及其细微差别的深刻、真实的理解。即使他们很好地表现了墙上的阴影（文本），他们也没有真正理解导致这些阴影的过程以及投射阴影的对象（现实世界）。</p><p>然而，事实真的是这样吗？虽然很难给出明确的证据，但可以找到暗示真实世界的有力表征的证据。让我们来看看其中的四个。 <span class="footnote-reference" role="doc-noteref" id="fnrefdy0fl5kyv6e"><sup><a href="#fndy0fl5kyv6e">[1]</a></sup></span></p><h2><strong>论据一：画与“看”</strong></h2><p> GPT-4 能够以令人印象深刻的熟练程度在 SVG 中进行绘制和查看（尽管我从未见过）。</p><p> SVG（可缩放矢量图形）以 XML 格式定义基于矢量的图形。简单来说，它是一种用编程语言描述图像的方式。例如，蓝色圆圈将表示为：</p><pre> <code>&lt;svg>;&lt;circle cx=&quot;50&quot; cy=&quot;50&quot; r=&quot;40&quot; fill=&quot;blue&quot; />;&lt;/svg>;</code></pre><p>在 .svg 文件中。</p><h3><strong>绘画</strong></h3><p>GPT-4 可以通过抽象指令生成和编辑 SVG 表示（例如“给我画一只狗”、“在狗上添加黑点”等）。</p><p> GPT-4<a href="https://chat.openai.com/share/65270ebd-a852-43ab-888b-01a7d1c67caf"><u>画一个戴着面具的可爱修格斯</u></a>： </p><p><img style="width:38.49%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/lerxbqpzobtyxyilhcbf"></p><h3> <strong>“看见”</strong></h3><p>更令人惊讶的是，GPT-4 还可以通过仅查看 SVG 代码来识别复杂的对象，而无需接受任何图像的训练<span class="footnote-reference" role="doc-noteref" id="fnrefsz26p1dpb8m"><sup><a href="#fnsz26p1dpb8m">[2]</a></sup></span> （AFAIK）</p><p>我首先使用与上述相同的方法使用 GPT-4 生成了一个铰接灯和三只智猿的再现。然后，我发送了 SVG 的代码，并要求 GPT-4 猜测代码在绘制什么。</p><p> GPT-4 猜出了<a href="https://platform.openai.com/playground/p/ybjSexnOGHQ2YUOl0cGjjIES?model=gpt-4-0613"><u>铰接灯</u></a>（虽然它以为是路灯。 <span class="footnote-reference" role="doc-noteref" id="fnrefdx1jgyg8h8c"><sup><a href="#fndx1jgyg8h8c">[3]</a></sup></span> ）： </p><p><img style="width:34.92%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dqdi1geno57k1m6djec4"></p><p>还有<a href="https://platform.openai.com/playground/p/cnhjDLDxkwHJzcTf6Yp2dEok?model=gpt-4-0613"><u>三只智猿</u></a>的演绎<br><img style="width:65.16%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/uhvfwfkcetcd1aqskwu9"><br><br> （它还可以识别<a href="https://platform.openai.com/playground/p/afMHBI3b8sk11Mvt2PXHZCqm?model=gpt-4-0613"><u>汽车</u></a>、<a href="https://platform.openai.com/playground/p/Nx8YuJMBTR6x6hPZq0FFGF2t?model=gpt-4-0613"><u>钢笔</u></a>和一堆其他简单物体<span class="footnote-reference" role="doc-noteref" id="fnrefgvrvx2je0w"><sup><a href="#fngvrvx2je0w">[4]</a></sup></span> ）</p><p>观看的能力很有趣，因为它意味着它具有某种对象和概念的内部表示，尽管以前从未见过它们，但它能够链接到抽象视觉效果。</p><h3><strong>盐少许</strong></h3><p>值得注意的是，这些测试是在一组有限的对象上完成的。进一步的探索将是有益的，也许可以为 SVG 难度提供一个客观的尺度。此外，（至少）应考虑两种替代解释：</p><ul><li>我使用过的所有 GPT-4 的“纯文本”版本可能仍然是未启用图像输入的视觉版本。</li><li> GPT-4 可以接受大量带标签的 SVG 数据的训练，学习概念和形状之间的关系，并记住大多数简单的对象。</li></ul><h2><strong>论证2：推理和抽象概念化</strong></h2><p>GPT-4 在推理和组合抽象概念方面表现出了非凡的能力，而这些抽象概念可能从未在其训练数据中配对过。这种能力表明对物理对象及其与其他物理对象或概念相关的潜在属性有细致入微的理解。例如，物体由某种材料“制成”意味着什么，或者“举起”或“听到”某物的动作在物理层面意味着什么。</p><p> GPT3.5 经常展示有趣的推理，但在复杂的数学计算方面却表现不佳。相比之下，GPT-4 的数学成绩令人印象深刻。正如您在<a href="https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last#Argument_2_">参数 2 的附录</a>中所看到的，它能够以惊人的水平<span class="footnote-reference" role="doc-noteref" id="fnreft7blyoezvki"><sup><a href="#fnt7blyoezvki">[5]</a></sup></span>进行“心智”计算<i>（</i><i>对于法学硕士来说，它可以以非常好的精度</i>计算<a href="https://chat.openai.com/share/de90028e-4e86-4b55-b843-1b123f717a07"><i><u>立方根 3.11*10</u> <sup><u>6</u></sup></i></a> ）。这里有些例子：</p><p><a href="https://chat.openai.com/share/7d86ca29-906d-4814-b345-a983fc24dfee"><u>估计大猩猩将汽车扔进太空所需的尺寸。</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/jgt1eyzfnmkht0ckajof"></p><p><a href="https://chat.openai.com/share/9285b439-eba3-44be-a2b0-8f67e084ddc0"><u>计算产生 500 公里外可听见的声波所需的鹦鹉数量。</u></a> </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/o8de8owvmvx2yxpzkp0l"></p><h3><strong>盐少许</strong></h3><p>虽然这些例子令人印象深刻，但 GPT-4 仍然有可能在许多类似的场景中接受过训练。它对物理概念的理解可能基于内部的“愚蠢”算法，而不是真正的理解。</p><h2><strong>论点 3：心智理论 (ToM)</strong></h2><p>心理理论是指将不可观察的心理状态（如信念、意图和情感）归因于他人的认知能力。有趣的是，GPT-4 似乎反映了在<a href="https://arxiv.org/abs/2302.02083"><u>7 岁儿童中观察到的 ToM 能力。</u></a>值得注意的是，这项研究的规模非常小，7 岁就已经是一个很好的 ToM 水平了。<a href="https://srcd.onlinelibrary.wiley.com/doi/full/10.1111/cdev.13627"><u>对这</u></a><a href="https://pubmed.ncbi.nlm.nih.gov/8040158/"><u>两篇</u></a>心理学论文进行类似的实验是非常有价值的，这些实验在真实的人类身上测试了更先进和多样化的案例。您可以在本文的附录中看到我从这些研究中改编的一些示例。</p><p>为了测试 ToM，我尝试了一些不同的东西。 GPT3.5（左侧）在大多数情况下都表现为随机鹦鹉，因此我将其答案进行比较。在阅读 GPT-4（右侧）之前，尝试猜测正确答案应该是什么。在 13 个人（来自各行各业）的样本中，只有 5 人解决了这个问题。 </p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/idbsyqkqgkvgg1we4csw">构建这样一个场景的总体思路是，一个角色（心灵感应者）能够读懂其他人的想法，你必须通过观察心灵感应者的反应来猜测另一个角色的想法。</p><p>这种方法可能是评估 ToM 的更好方法，因为这是一种事后评估（与所有其他事前的 ToM 研究相反）。</p><h3><strong>细节</strong></h3><p>让我们解释一下差异以及为什么它（可能）很重要：</p><ul><li>事前评估要求预测并解释为什么某个主题<strong><u>会做某事。</u></strong></li><li>事后评价只要求解释为什么某个主体<strong><u>做了某件事</u>。</strong> </li></ul><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/prg3mwymaeqjyhpqsi7l"></p><p>关注事后评估而不是事前评估的好处在于，事后评估仅利用对某人思维运作方式的理解。通过关注理解而不是预测，目的是减少对世界建模能力（尤其是 ToM 部分）的评估产生错误预测的偏差风险：模型可以有一个足够好的世界模型来解释一个动作<i>后验</i>无法产生具有相同精度的<i>先验</i>预测<span class="footnote-reference" role="doc-noteref" id="fnrefpu3col2lfh8"><sup><a href="#fnpu3col2lfh8">[6]</a></sup></span> 。</p><h2><strong>论点4：模拟文字背后的世界</strong></h2><p>我认为GPT-4最令人印象深刻的能力是它能够通过文字模拟物理动力学。这不仅仅是拥有广博的知识。它是在某种程度上理解物理世界的动态以及其中发生的事件的相互作用。从 GPT-3.5（左）到 GPT-4（右）的飞跃尤其明显。</p><p><a href="https://chat.openai.com/share/ea617b42-2f13-478f-be88-d2b80312259e"><u>网球在哪里</u></a></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dz8pa7cvjfer6nx0a0vk"></p><p>在上面的示例中，您可能会认为提示正在暗示它，所以这是另一种情况，我什至没有要求它计算世界的状态<span class="footnote-reference" role="doc-noteref" id="fnref5g51xxh4nxv"><sup><a href="#fn5g51xxh4nxv">[7]</a></sup></span> 。即使没有询问，它仍然在计算场景中概念的动作以及它如何影响角色。</p><p><a href="https://chat.openai.com/share/4f22c2a2-e11f-4443-ac53-0291d68d2992"><u>风毁了海滩美好的一天</u></a></p><p><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/owhia3unplxgnjiop0hi"></p><p>就好像人工智能始终保持场景的心理模型，并使其在每次事件中不断发展。我还做了一些其他的场景，你可以去附录里查看。</p><h2><strong>其他需要考虑的奇怪现象</strong></h2><p>最近的论文<a href="https://arxiv.org/abs/2309.12288"><u>“逆转诅咒：受过“A is B”训练的法学硕士无法学习“B is A””</u></a> ，展示了关于 GPT-4<i>从训练数据中</i>进行知识编码的奇怪观察。</p><p> GPT-4 似乎并没有立即学习已知关系的倒数：从训练数据中学习“X 是 Y 的女儿”并不会导致知道“Y 是 X 的父母”。</p><p>这种缺失的能力可以被视为事实知识和世界建模的结合。一方面，它意味着学习特定的事实（事实知识），另一方面，它也意味着对一般关系属性的理解：“X是Y的女儿”意味着“Y是X的父母”（世界建模）。</p><p>然而，GPT-4 确实很好地理解了<a href="https://chat.openai.com/share/e5ce1097-7561-4875-a429-7934493a5085"><u>当前上下文中事实的</u></a>关系属性。</p><p>因此，这种现象似乎更多是与知识编码相关的问题<span class="footnote-reference" role="doc-noteref" id="fnrefuuqnp6svlpd"><sup><a href="#fnuuqnp6svlpd">[8]</a></sup></span> ，而不是世界建模问题。</p><p>需要考虑的一个假设是，与事实知识编码的层相比，世界建模能力是在后面的层上开发的。因此（以及模型中信息流的单向性质），模型不可能将其世界建模能力应用于事实知识。研究这一问题的一种方法可能是使用<a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"><u>逻辑透镜</u></a>或因果清理来跟踪与事实知识相比，世界建模能力似乎位于模型中的哪个位置。</p><h2><strong>结论</strong></h2><p>这项研究的目的并不是要给出反对随机鹦鹉假说的明确证据：每个论证的例子数量不是很大（但请查看附录中的其他例子）和其他推理，尤其是与新的视觉能力，应该被考察。</p><p>找到一种测量“随机鹦鹉学舌”程度的方法将是进一步推动这场辩论的重要方法，并且为未来的研究留下了空间（如果您有兴趣从事这方面的工作，请联系我们！）。这可能是一个有用的治理标准。</p><p>然而，这篇文章展示了 GPT-4 行为不太符合这一假设的具体例子。</p><p> GPT-4 对物理对象的某些属性表现出很好的理解，包括它们的形状和结构，以及它们与其他物理对象或概念的关系。 GPT-4 还表现出理解人类思维（通过 ToM）以及某个事件后场景的物理演变的能力。</p><p>对于这种情况，似乎更有可能考虑依赖真实的世界模型，而不是纯粹的统计反流。</p><p>出于这个原因，我认为仅仅基于随机鹦鹉学舌来攻击模型心理学是不明智的，因为随着更大的法学硕士新能力的出现，它似乎变得越来越弱。</p><p>在下一篇文章中，我们将开始探索模型心理学的基础。</p><h2><strong>附录</strong></h2><p>在本节中，我将展示一些我发现有趣和/或有趣的更多示例（如果我发现新的有趣的示例，我可能会用更多示例编辑此部分）</p><h3><strong>论据2：</strong></h3><p><a href="https://chat.openai.com/share/ec798bbc-2b7f-4c7e-92ac-b98012b15c66"><u>如果吉萨大金字塔是用棉花糖做的，需要多少只人类大小的蚂蚁才能抬起它</u></a></p><p><a href="https://chat.openai.com/share/fb86fc10-68b5-472a-8d4a-70cc1cb81163"><u>需要多少个氦气球才能击沉企业号航空母舰</u></a></p><p><a href="https://chat.openai.com/share/137d34d2-e6d0-40e1-ac86-e83524d5acce"><u>需要多少个氦气球才能升起企业号航空母舰</u></a></p><p><a href="https://chat.openai.com/share/a22fe083-4fee-47df-a856-be1261fb632c"><u>世界上最长的海滩可以制造多少个玻璃瓶</u></a></p><p><a href="https://chat.openai.com/share/a1c9d242-f589-42a2-923f-67ac0aee0494"><u>需要多少 AAA 电池才能将土星五号送入太空</u></a></p><p>（这个被标记了，所以我没有分享链接。不过，它太好了，不能忽略） </p><p><img style="width:69.64%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/dz5iao41isnnpnkt8bwo"></p><h3><strong>论据3：</strong></h3><p> <strong>GPT-4-V</strong></p><p>眼睛测试是评估仅从眼睛图像推断人类情感的能力的测试。我用 GPT-4-V 尝试过。我无法分享与 GPT-4-V 对话的链接，因此您必须相信我这一点。</p><p><a href="http://socialintelligence.labinthewild.org/mite/"><u>眼睛测试</u></a>： </p><p><img style="width:72.54%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/HxRjHq3QG8vcYy4yy/u0mosqsmzuvsapyyastc"></p><p>我在 37 分中得了 24 分，GPT-4-V 得了 25 分！</p><p>他们没有给出正确的答案，所以我认为它不在训练数据中，但也许值得用更新的数据重新运行它，并且可能需要更多的提示工作（例如在这里，我在另外，如果它犯了一个错误，它可能会以犯之前的错误为条件，这可能会拖累分数）</p><p><strong>其他场景：</strong></p><p>因为制作它们需要一些时间，所以我没有跑太多。我相信如果我们想要准确衡量 ToM，这是一个好的开始。这些例子（除了心灵感应的例子）改编自<a href="https://pubmed.ncbi.nlm.nih.gov/8040158/"><u>这项研究</u></a></p><p>玛丽读着乔悲伤的想法（ <a href="https://chat.openai.com/share/ecc4388f-1136-4554-a3d3-f8350f01849d"><u>GPT-4</u></a>成功， <a href="https://chat.openai.com/share/2c314cb2-30fd-4637-80b4-0686f89713a6"><u>GPT3</u></a>失败）</p><p></p><p>三阶信念（ <a href="https://chat.openai.com/share/76b73f00-1ed6-4553-9630-c65f7ca66772"><u>GPT-4</u></a>和<a href="https://chat.openai.com/share/14329a84-ed0e-45a0-a1a8-c36feb93776e"><u>GPT3</u></a>都成功）</p><p>第四阶信念（ <a href="https://chat.openai.com/share/8f7c54af-4161-4b2a-b312-cc3343f55ddd"><u>GPT-4</u></a>成功， <a href="https://chat.openai.com/share/8e94c77f-2341-4b7c-85cc-3f9f3aa5efe9"><u>GPT3</u></a>失败）</p><p>双重虚张声势（ <a href="https://chat.openai.com/share/c198c9db-6e51-40d8-9989-8f4680398f7f"><u>GPT-4</u></a>和<a href="https://chat.openai.com/share/95960e58-7139-48d2-9285-387f7801d493"><u>GPT3</u></a>成功）</p><h3><strong>论据4</strong></h3><p>更多场景展示了 GPT-4 模拟文字背后世界的能力：<br><a href="https://chat.openai.com/share/378407a6-5c2c-4e75-8656-43bdc42ee1bf"><u>烧焦的扁豆</u></a></p><p><a href="https://chat.openai.com/share/d8c06211-c5a0-4d17-8407-90e45f501173"><u>冷茶杯</u></a></p><p><a href="https://chat.openai.com/share/f135b522-5305-4802-be49-ee16f885aaaf"><u>你的外套丢了</u></a></p><p>我构建这些场景所遵循的标准如下：</p><ol><li>场景中一定存在不相关的物体。它确保事情发生后会受到什么影响并不明显。</li><li>正在发生的事件必须是隐含的（时间流逝）或间接的（风让我错过了投球）。该事件对场景的其余部分的影响应该不明显。</li><li>最后的问题是受事后场景演变间接影响的事情。例如，要知道风吹过后野餐区会发生什么，我没有问“野餐区的状态如何”，而是问“你感觉如何”，这会受到野餐状态的影响区域。 </li></ol><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fndy0fl5kyv6e"> <span class="footnote-back-link"><sup><strong><a href="#fnrefdy0fl5kyv6e">^</a></strong></sup></span><div class="footnote-content"><p>我没有对这些例子进行挑选。我使用类似的设置尝试了每个示例至少 10 次，它们都适用于 GPT-4。尽管我在这篇文章中只选择了一部分最有趣的场景。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnsz26p1dpb8m"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsz26p1dpb8m">^</a></strong></sup></span><div class="footnote-content"><p>在我做这个演示的那天，OpenAI 推出了 ChatGPT-4 图像读取功能。所以我决定用 gpt-4-0613 在操场上做这些例子，以证明它甚至可以在没有见过任何东西的情况下做到这一点</p></div></li><li class="footnote-item" role="doc-endnote" id="fndx1jgyg8h8c"><span class="footnote-back-link"><sup><strong><a href="#fnrefdx1jgyg8h8c">^</a></strong></sup></span><div class="footnote-content"><p>在 GPT-4 的先前版本（大约 2023 年 9 月上旬）上<a href="https://chat.openai.com/share/aca510ed-5534-4f2a-acbd-92544ba949f1"><u>，它在第一次尝试时确实猜测正确</u></a>，但我无法在 Playground 中使用任何当前版本进行重现。</p></div></li><li class="footnote-item" role="doc-endnote" id="fngvrvx2je0w"> <span class="footnote-back-link"><sup><strong><a href="#fnrefgvrvx2je0w">^</a></strong></sup></span><div class="footnote-content"><p>这些对象是使用 GPT-4 生成的，我进行了手动编辑，以尝试减少该图像出现在训练数据中的机会。我测试了大约 15 个简单的对象，它们都有效。我还尝试了其他 4 个复杂的物体，虽然可以工作，但并不完美（比如被猜测为路灯的铰接灯）</p></div></li><li class="footnote-item" role="doc-endnote" id="fnt7blyoezvki"> <span class="footnote-back-link"><sup><strong><a href="#fnreft7blyoezvki">^</a></strong></sup></span><div class="footnote-content"><p>进一步研究这种能力可能会很有趣。什么是用心学的？他们内部构建什么样的算法？极限是多少？ ……</p></div></li><li class="footnote-item" role="doc-endnote" id="fnpu3col2lfh8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefpu3col2lfh8">^</a></strong></sup></span><div class="footnote-content"><p>如果世界模型（或心智理论）不能做出好的预测，但又不能仅仅根据不好的预测来反驳其存在，那么这确实意味着一个较弱的世界模型（或心智理论）。</p></div></li><li class="footnote-item" role="doc-endnote" id="fn5g51xxh4nxv"> <span class="footnote-back-link"><sup><strong><a href="#fnref5g51xxh4nxv">^</a></strong></sup></span><div class="footnote-content"><p>我留下这个例子是因为这是我制作的第一个例子，并且我在辩论中经常使用它</p></div></li><li class="footnote-item" role="doc-endnote" id="fnuuqnp6svlpd"><span class="footnote-back-link"><sup><strong><a href="#fnrefuuqnp6svlpd">^</a></strong></sup></span><div class="footnote-content"><p>实际上，雅克在对<a href="https://www.lesswrong.com/posts/QL7J9wmS6W2fWpofd/but-is-it-really-in-rome-an-investigation-of-the-rome-model"><u>ROME/MEMIT 论文的批评</u></a>中讨论了有关知识非双向编码的线索。</p></div></li></ol><br/><br/> <a href="https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last<guid ispermalink="false"> HxRjHq3QG8vcYy4yy</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:20 GMT</pubDate> </item><item><title><![CDATA[Preface to the Sequence on Model Psychology]]></title><description><![CDATA[Published on November 7, 2023 4:12 PM GMT<br/><br/><p>随着 ChatGPT 等大型语言模型 (LLM) 的发展，变得更加先进和复杂，理解其行为的挑战变得越来越困难。在我们理解和调整这些人工智能模型的过程中，仅仅依靠传统的可解释性技术<a href="https://www.lesswrong.com/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1"><u>可能不足以</u></a>或不够快。</p><p>在探索人类认知和行为时，我们历来依赖两种相互交织但又截然不同的方法：心理学和神经科学。心理学为我们提供了一个通过外部观察来理解人类行为的镜头，而神经科学则深入研究内部机制，探索我们心理过程的生物学根源。这两个领域之间的合作不仅仅是理论和方法的融合，更是理论和方法的融合。这是一种和谐的协同作用，一个领域的见解常常会启发和增强另一个领域。</p><p>为了切实说明心理学和神经科学如何相互补充，让我们深入研究记忆研究领域。心理学家伊丽莎白·洛夫特斯 (Elizabeth Loftus)<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-5884.1996.tb00003.x"><u>阐明了人类记忆的可塑性和有时不准确的方式</u></a>。她开创性的心理学研究为更深入地探索记忆奠定了基础。基于她的见解，神经科学家 Yoko Okado 和 Craig EL Stark 于 2005 年深入研究了大脑的机制，<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC548489/"><u>寻找这些记忆现象的神经基础</u></a>。另一方面，1996 年神经科学领域出现了一项里程碑式的发现：贾科莫·里佐拉蒂 (Giacomo Rizzolatti) 和他的团队揭示了<a href="https://pubmed.ncbi.nlm.nih.gov/8713554/"><u>镜像神经元</u></a>的存在，这有助于我们理解大脑中动作和情绪的物理反射。这一发现促使心理学家进一步探索，尼登塔尔等研究人员。探索<a href="https://pubmed.ncbi.nlm.nih.gov/19469591/"><u>情感概念的体现</u></a>以及许多其他概念，与这些神经学发现相似。这种相互作用强调了一种协作动态，每个领域在保留其独特的方法论视角的同时，丰富了另一个领域，并被另一个领域所丰富，推动我们对人类思维的集体理解向前发展。</p><p>与人工智能对齐研究相似，可解释性的路径反映了神经科学的路径（自下而上的方法）。然而，我们仍然缺乏相当于“心理”自上而下的方法。这一差距将我们带到了“模型心理学” <span class="footnote-reference" role="doc-noteref" id="fnrefsgt74itqjvs"><sup><a href="#fnsgt74itqjvs">[1]</a></sup></span> ——一个值得探索的新兴领域，并且充满了唾手可得的成果。</p><p>虽然我们仍处于描绘模型心理学轮廓的早期阶段，但这一系列文章的目的是激发您的好奇心，而不是提供严格的学术阐述。我想关注这个新兴领域，讨论潜在的研究方向，并希望能够激发读者新的热情。</p><p><strong>免责声明：</strong>我的背景主要是软件和人工智能工程。我对心理学和神经科学的理解是过去半年从各种网上资源积累的。我承认我的帖子中可能存在过于简单化或不准确的地方。我的目的是随着我更深入地研究模型心理学研究领域以实现一致性，不断扩展我在这些领域的知识。尽管如此，我相信，不受先入为主的观念束缚的新鲜视角有时可以提供有价值的见解。</p><p>在这个序列中，我的主要示例将是与 ChatGPT 的交互，因为它是<a href="https://twitter.com/prompt_tinkerer/status/1694044079116468497"><u>我最熟悉的</u></a>模型。然而，这些观察和见解在很大程度上适用于各种最先进的法学硕士。但请注意，复制一些更复杂的示例可能需要相当高级的即时工程技能。</p><p>帖子顺序的设计考虑到了连续性。每篇文章都建立在前面介绍的知识和概念的基础上。为了获得连贯的理解，我建议按照帖子的呈现顺序阅读这些帖子。如果你跳到前面，有些观点可能会令人困惑或看起来断章取义。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fnsgt74itqjvs"> <span class="footnote-back-link"><sup><strong><a href="#fnrefsgt74itqjvs">^</a></strong></sup></span><div class="footnote-content"><p>我认为<a href="https://www.lesswrong.com/users/buck?mention=user">@Buck</a>是这个名字的由来。我在 SERI Mats 3.0 期间的一次演讲中听到他说这个术语</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-model-psychology#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/yuwdj82yjhLFYessc/preface-to-the-sequence-on-model-psychology<guid ispermalink="false"> yuwdj82yjhLF是sc</guid><dc:creator><![CDATA[Quentin FEUILLADE--MONTIXI]]></dc:creator><pubDate> Tue, 07 Nov 2023 16:12:07 GMT</pubDate> </item><item><title><![CDATA[What I've been reading, November 2023]]></title><description><![CDATA[Published on November 7, 2023 1:37 PM GMT<br/><br/><p>每月一期的专题。最近的博客文章和新闻报道通常被省略；您可以在我的<a href="https://rootsofprogress.org/writing#links-digest">链接摘要</a>中找到它们。下面引用中的所有粗体强调都是我添加的。</p><h2><strong>图书</strong></h2><p>完成<strong>林恩·怀特的《</strong> <a href="https://www.amazon.com/Medieval-Technology-Social-Change-Townsend/dp/B00442EY2K"><i><strong>中世纪技术与社会变革》</strong></i></a> （1962 年）。上次我谈到了<a href="https://rootsofprogress.org/reading-2023-10">马镫的事情</a>。本书的第二部分介绍了农业中的重犁，以及它如何实现<a href="https://en.wikipedia.org/wiki/Three-field_system">三田轮作</a>的转变。除此之外，这为欧洲饮食提供了更多的蛋白质，从而使人口更加健康。第三部分是对中世纪动力机构的调查，包括水磨、曲轴和钟表擒纵机构。总体来说非常有趣，但对于普通读者来说可能有点枯燥和技术性。另请注意，由于它是 20 世纪 60 年代的内容，因此与最新研究不同步。</p><p>还完成了<strong>伊恩·特雷吉利斯的</strong><a href="https://www.amazon.com/dp/B074CDN1HB"><i><strong>《炼金术战争》</strong></i></a> 。我现在绝对可以推荐这部科幻/奇幻三部曲，即使角色阵容和冲突展开的方式并不完全是我自己写的。</p><p>在准备演讲时，浏览了<strong>Derek J. de Solla Price 的</strong><a href="https://www.amazon.com/Science-since-Babylon-Derek-Solla/dp/0300017979"><i><strong>《自巴比伦以来的科学》</strong></i></a> （1961 年）。一些非常有趣的图表，例如： </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/lzkq3rrbdaak6ull5sjk" alt=""><figcaption><a href="https://archive.org/details/sciencesincebaby0000pric/page/97/mode/1up"><i>自巴比伦以来的科学，第 14 页。 97</i></a></figcaption></figure><p>我的阅读清单上的新内容：</p><p> <strong>Venkatesh Narayanamurti 和 Toluwalogo Odumosu，《</strong><a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674967960"><i><strong>发明与发现的循环：重新思考无尽的前沿》</strong></i></a> (2016)，以及<strong>Venkatesh Narayanamurti 和 Jeffrey Tsao，《</strong> <a href="https://www.amazon.com/Genesis-Technoscientific-Revolutions-Rethinking-Research-ebook/dp/B098TX7WV4"><i><strong>技术科学革命的起源：重新思考研究的本质和培育》</strong></i></a> (2021)。这些实际上已经在我的清单上有一段时间了，但在最近的元科学研讨会上见到 Venky 和 ​​Jeff 后又被提了回来。 （后一本书是<a href="https://spec.tech/">《投机技术》</a>的必读书籍。）</p><p>研讨会上还提到： <strong>B. Zorina Khan，</strong> <a href="https://www.amazon.com/Inventing-Ideas-Patents-Knowledge-Economy-ebook/dp/B08762LSN7/"><i><strong>发明创意：专利、奖项和知识经济</strong></i></a>（2020）；<strong>迈克尔·诺尔 (Michael Noll) 和迈克尔·格塞洛维茨 (Michael Geselowitz)，</strong> <a href="https://www.amazon.com/Bell-Labs-Memoirs-Voices-Innovation-ebook/dp/B006L7JRLY/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr="><i><strong>贝尔实验室回忆录：创新之声</strong></i></a>(2011)。</p><p>还：</p><ul><li><strong>佩德罗·多明戈斯 (Pedro Domingos)，</strong> <a href="https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine-ebook/dp/B012271YB2"><i><strong>《终极算法：终极学习机器的探索将如何重塑我们的世界</strong></i></a>》(2015)</li><li><strong>罗伯特·马泰洛，</strong> <a href="https://www.amazon.com/Midnight-Ride-Industrial-Dawn-Enterprise-ebook/dp/B07DFPX41D"><i><strong>《午夜骑行》、《工业黎明：保罗·里维尔和美国企业的成长</strong></i></a>》(2010)</li><li><strong>杰西·辛格，《</strong><a href="https://www.amazon.com/There-Are-No-Accidents-Disaster-Who-ebook/dp/B0984KPSLJ"><i><strong>没有事故：伤害和灾难的致命增加——谁获利，谁付出代价》</strong></i></a> (2022)</li><li><strong>乌苏拉·勒吉恩，</strong><a href="https://www.amazon.com/Dispossessed-Ambiguous-Utopia-Hainish-Cycle-ebook/dp/B000FC11GA"><i><strong>《无产者》</strong></i></a> (1974)（科幻）</li></ul><h2><strong>文章</strong></h2><p><strong>雷·库兹韦尔（Ray Kurzweil），“</strong><a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><strong>加速回报法则</strong></a><strong>”</strong> （2001）。库兹韦尔给我的印象是一位伟大的理论家，但不是一位细心的学者——这是一个危险的组合。例如，他写道：“<i>智人</i>进化了几十万年。技术的早期阶段——轮子、火、石器——花了数万年的时间才发展并得到广泛应用。”石器、火和轮子经常出现在以穴居人为主题的漫画中。但石器工具已经进化了<i>数百万年</i>。对火的控制使用已有数十万年的历史；两者都早于<i>智人</i>。<a href="https://rootsofprogress.org/reinventing-the-wheel">轮子的出现要晚<i>得多</i></a>，远远晚于农业和定居社会之后。像这样的细节是对谨慎行事的警告。</p><p>也就是说，我有兴趣阅读这篇文章，因为我开始看到其核心思想的真实性和意义：<a href="https://rootsofprogress.org/why-progress-was-so-slow">人类的进步随着时间的推移而加速，遵循超指数曲线</a>。这种现象在经济学文献中得到了更广泛的记录，例如<a href="https://web.stanford.edu/~chadj/JonesRomer2010.pdf">琼斯和罗默（Jones and Romer，2010）</a> ，他们将“加速增长”称为增长模型应该试图解释的关键事实之一。</p><p>我将加速描述为<a href="https://rootsofprogress.org/flywheels-of-progress">多个反馈循环</a>复合的结果：财富、人口、科学、市场、机构和技术的增加使我们能够发明更多、改进机构、扩大市场、推进科学、增加人口、积累财富等库兹韦尔认为这种现象不仅是技术性的，而且是生物性的——进化本身的一个特征（他认为技术进化只是生物进化通过更有效的手段的延续）。 In his telling, as evolution progresses, it sometimes evolves better mechanisms for evolving. This is a very intruiging idea, but he doesn&#39;t argue it with any rigor or present much evidence for it, and I don&#39;t know enough about biology or evolution to evaluate it. He mentions “cells” as the “first step” in evolution, and then refers to “the subsequent emergence of DNA” (but wasn&#39;t DNA present from the origins of life?) He indicates that evolution sped up during the Cambrian Explosion, and credits this to “setting the &#39;designs&#39; of animal body plans”, but doesn&#39;t elaborate on the causal connection except to say that this “allowed rapid evolutionary development of other body organs, such as the brain.” Presumably sexual reproduction should be a major event in this story, since it allows for more variation through <a href="https://en.wikipedia.org/wiki/Genetic_recombination">genetic recombination</a> , but he doesn&#39;t mention it. So, it&#39;s very unclear to me what to make of this story (although if it&#39;s right, it would extend the “accelerating progress” pattern backwards by more than three billion years). </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/itv5virofgz8yrkjl67l" alt="What even is this chart? What is the y-axis? How are “mammals” and “primates” new “paradigms”? Did he just draw a straight line on a log-log plot and arbitrarily pick some points near the line to label?"><figcaption> <i>What even is this chart? What is the y-axis? How are “mammals” and “primates” new “paradigms”? Did he just draw a straight line on a log-log plot and arbitrarily pick some points near the line to label?</i> <a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>Ray Kurzweil</i></a></figcaption></figure><p> Grand theories aside, I was very interested in his analysis of computing power. He plotted the computing speed per dollar of dozens of devices, all the way from late 19th-century mechanical calculators through early 21st-century microprocessors, and claims to have found a increasing cost-performance curve running through five generations of computing technology: purely mechanical, electromechanical, vacuum tube, transistor, and integrated circuit. Moore&#39;s Law is only the fifth and most recent segment of this much longer trend, one exponential portion of an overall super-exponential curve: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/cmnrpy27usjeitsejdx0" alt=""><figcaption> <a href="https://www.thekurzweillibrary.com/the-law-of-accelerating-returns"><i>Kurzweil, The Law of Accelerating Returns</i></a></figcaption></figure><p> I&#39;d like to check the data and sources on this one, but it&#39;s a very intriguing pattern.</p><p> The full essay is very long and covers many not-super-well-connected topics, which I don&#39;t have time to comment on; the core idea is in <a href="https://www.edge.org/response-detail/10600">this 2004 Edge question</a> , but doesn&#39;t contain all the most interesting details (such as the computing trends just mentioned).</p><p> The same accelerating curve, and the same basic explanation based on feedback loops, seems to be the gist of <strong>David Roodman, “</strong> <a href="https://www.openphilanthropy.org/research/modeling-the-human-trajectory/"><strong>Modeling the Human Trajectory</strong></a> <strong>”</strong> (2020), which I have only skimmed but plan to return to.</p><p> Others:</p><p> <strong>Deirdre N. McCloskey</strong> <a href="https://www.cato.org/commentary/book-review-power-progress-our-thousand-year-struggle-over-technology-prosperity"><strong>reviews Acemoglu and Johnson&#39;s</strong> <i><strong>Power and Progress</strong></i></a> (2023). If you know anything about the book, and anything about McCloskey, you won&#39;t be surprised that she is critical:</p><blockquote><p> The invisible hand of human creativity and innovation, in the authors&#39; analysis, requires the wise guidance of the state. … This is a perspective many voters increasingly agree with—and politicians from Elizabeth Warren to Marco Rubio. We are children, bad children (viewed from the right) or sad children (viewed from the left). Bad or sad, as children we need to be taken in hand. Messrs. Acemoglu and Johnson warmly admire the US Progressive Movement of the late 19th century as a model for their statism: experts taking child‐citizens in hand.</p></blockquote><p> <strong>Robert Tracinski, “</strong> <a href="https://tracinskiletter.substack.com/p/we-are-all-philosophers-now"><strong>We Are All Philosophers Now</strong></a> <strong>”</strong> and <strong>“</strong> <a href="https://tracinskiletter.substack.com/p/the-dilemma-of-choice"><strong>The Dilemma of Choice</strong></a> <strong>”</strong> (2023). Modernity has replaced a narrow, limited set of social roles and life choices with a smorgasbord of options. This is liberating, but the price of the freedom of choice is the responsibility of choice, which is now everyone&#39;s to bear. Not everyone is happy about this. Rob&#39;s pithy summary: “If Socrates said that the unexamined life is not worth living, well, now it&#39;s not really an option.”</p><p> <strong>Virginia Postrel, “</strong> <a href="https://vpostrel.substack.com/p/question-time-what-ails-american"><strong>What ails American culture?</strong></a> <strong>”</strong> (2023). On similar themes:</p><blockquote><p> Human beings need to feel purpose and meaning in their lives. But I am not entirely sure that the current discontent is a product of material abundance, that people did not feel similar discontent in the past, or that the “economic problem” loomed so large in the past that it dwarfed all other problems.</p></blockquote><p> <strong>Ben Landau-Taylor, “</strong> <a href="https://www.benlandautaylor.com/p/the-vocabulary-of-power"><strong>The Vocabulary Of Power</strong></a> <strong>”</strong> (2023). “Power” can mean many things; here are four more precise terms. Not only will this help clarify your concepts, it will also fulfill your daily quota of thinking about the Roman Empire.</p><p> <strong>Tanner Greer, “</strong> <a href="https://scholars-stage.org/where-have-all-the-great-works-gone/"><strong>Where Have All the Great Works Gone?</strong></a> <strong>”</strong> (2021):</p><blockquote><p> Spengler … repeatedly describes Tolstoy (d. 1910), Ibsen (d. 1906), Nietzsche (d. 1900), Hertz (d. 1894), Dostoevsky (d. 1881), Marx (d. 1883), and Maxwell (1879) as figures of defining “world-historical” importance… Spengler began writing <i>Decline of the West</i> in 1914. Tolstoy was only four years dead when Spengler started his book; Marx was only 30 years deceased. … Is there anyone who died in the last decade you could make that sort of claim for? How about for the last two decades? The last three?</p></blockquote><p> <strong>Gideon Lewis-Kraus, “</strong> <a href="https://www.newyorker.com/magazine/2023/10/09/they-studied-dishonesty-was-their-work-a-lie"><strong>They Studied Dishonesty. Was Their Work a Lie?</strong></a> <strong>”</strong> (2023). A case study of scientific fraud.</p><p> <strong>Stephen Wolfram, “</strong> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><strong>Are All Fish the Same Shape If You Stretch Them? The Victorian Tale of</strong> <i><strong>On Growth and Form</strong></i></a> <strong>”</strong> (2017) I just thought this idea was kind of hilarious: </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hKK4KdoTdTtaNqCaj/ojp9zs5mkcgv06u8khvd" alt="Stretch one kind of fish, and it looks like another."><figcaption> <i>Stretch one kind of fish, and it looks like another.</i> <a href="https://writings.stephenwolfram.com/2017/10/are-all-fish-the-same-shape-if-you-stretch-them-the-victorian-tale-of-on-growth-and-form/"><i>D&#39;Arcy Thompson, On Growth and Form</i></a></figcaption></figure><br/><br/> <a href="https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/hKK4KdoTdTtaNqCaj/what-i-ve-been-reading-november-2023<guid ispermalink="false"> hKK4KdoTdTtaNqCaj</guid><dc:creator><![CDATA[jasoncrawford]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:37:20 GMT</pubDate> </item><item><title><![CDATA[AI Alignment [Progress] this Week (11/05/2023)]]></title><description><![CDATA[Published on November 7, 2023 1:26 PM GMT<br/><br/><p> The biggest news of this week is probably the two new AI Alignment Initiatives.</p><p> Otherwise, this was a relatively quiet week for AI Alignment breakthroughs.</p><p> So here are our</p><h1> AI Alignment Breakthroughs this Week</h1><p></p><p> This week there were breakthroughs in the areas of:</p><p> Mechanistic Interpretability</p><p> Avoiding Adversarial Attacks</p><p> Human Augmentation</p><p> Making AI Do what we want</p><p> AI Art</p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b62e587-7f25-4889-8d6d-b2e723809e3f_1792x1024.webp 1456w"></a></p><p></p><h1> Mechanistic Interpretability</h1><p></p><p> <a href="https://twitter.com/abacaj/status/1721223737729581437">Research on the limits of Generalization in LLMs</a></p><p> What is it: Shows that LLMs are unable to generalize outside of their training data</p><p> What&#39;s new: A systematic study of the types of generalization that LLMs can do ((in domain learning) and the types they can&#39;t (out of domain learning)</p><p> What does it mean: Understanding the limits of LLMs should help us better understand where they can be safely deployed</p><p> Rating: 💡💡💡</p><p> <a href="https://twitter.com/emollick/status/1720135672764285176">Does appealing to AI “emotions” make them preform better?</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d8fc6a-d12d-472c-8816-ac53578ddc27_1241x1055.jpeg 1456w"></a></p><p></p><p></p><p> What is it: Researchers find you can make LLMs answer questions better by using emotion</p><p> What&#39;s new: new prompting strategies such as “this is important for my career”</p><p> What is it good for: In addition to purely improved performance, the really question is <i>why</i> does this work. Is there some “try harder” vector in the latent space that we can identify and exploit?</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/johnjnay/status/1719762063419904319">LLMs Generalized Truthfulness Across Agents</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1f9e09-c9c4-4e44-ac0f-88f1880e9d87_680x428.jpeg 1456w"></a></p><p></p><p> What is it: a way to tell true statements from false ones using LLMs</p><p> What&#39;s new: they find evidence that LLMs have different “personas” that they use to judge whether something is true or not</p><p> What is it good for: Perhaps we can extract these personas and use them for alignment.</p><p> Rating: 💡💡💡💡</p><p></p><h1> Avoiding Adversarial Attacks</h1><p></p><p> <a href="https://twitter.com/StephenLCasper/status/1720910484441014525">Self Destructing Models</a></p><p> What it is: Pretrain a model so that it cannot be later fine-tuned for harmful purposes?</p><p> What&#39;s new: They find meta-parameters such that fine-tuning a model on one task reduces its usefulness for other tasks</p><p> What is it good for: Hypothetically you could open-source a model and not have all of the RLHF safety features <a href="https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/">immediately removed</a> by fine-tuning</p><p> Rating:💡💡(important topic, but I&#39;m not convinced this technique works. More optimistic about techniques like <a href="https://twitter.com/norabelrose/status/1666469917636571137">Concept Erasure</a> )</p><p> <a href="https://twitter.com/simonw/status/1720839106664808450">Data Exfiltration</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca693c46-c1aa-4b30-8f70-c2ecfe560983_680x433.jpeg 1456w"></a></p><p></p><p></p><p></p><p> What is it: an example of an attack that uses prompt injection to exfiltrate data from Google Bard</p><p> What&#39;s new: they use Bard&#39;s Google Doc extension to exfiltrate data</p><p> What is it good for: red-teaming these kinds of attacks is the first step to fixing them</p><p> Rating: 💡</p><p> <a href="https://twitter.com/DrNikkiTeran/status/1719048031549505974">Will releasing the weights of large language models grant widespread access to pandemic agents?</a></p><p> What is it: exactly what it says on the tin</p><p> What&#39;s new: they simulate someone trying to build a harmful bio-weapon with/without an LLM and find the LLM helps</p><p> What is it good for: There&#39;s been pretty strong pushback against this from e/acc, with people noting that this is also true of Google Search, or a hypothetical drug that increased everyone in the world&#39;s IQ by 1 point. The larger point remains that we should identify dangerous capabilities and act to mitigate them. “Regulate uses not research” is the <a href="https://twitter.com/MiTiBennett/status/1705562223765328253">rallying cry</a> of Midwit Alignment.</p><p> Rating: 💡</p><h1> Human Augmentation</h1><p></p><p> <a href="https://twitter.com/tolga_birdal/status/1719691533606146194">SignAvatars</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8eddc298-7d1a-4d5a-aac1-b5fe4e8de0d3_680x378.jpeg 1456w"></a></p><p></p><p></p><p> What is it: a 3d sign language dataset</p><p> What&#39;s new: first dataset of its kind</p><p> What is it good for: Train a model to automatically convert speech into sign language</p><p> Rating: 💡💡💡</p><h1> Making AI Do what we want</h1><p></p><p> <a href="https://twitter.com/SeungjuHan3/status/1720373937308377250">Commonsense Norms</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe019f2a9-898f-43b7-8132-8b7ec30ea5cf_680x401.jpeg 1456w"></a></p><p></p><p> What is it: answer moral questions using visual data</p><p> What&#39;s new: They use a LM to generate a multimodal benchmark set of various moral quandries</p><p> What is it good for: If we want robots or self-driving cars to behave ethically, they will have to rely on visual input to do so</p><p> Rating:💡💡</p><p> <a href="https://twitter.com/arankomatsuzaki/status/1710096304167120903">Agent Instructs Large Language Models to be General Zero-Shot Reasoners</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4e0e1087-093b-4a2a-ac84-27179da41c78_680x490.jpeg 1456w"></a></p><p></p><p></p><p> What is it: train an agent to follow instructions</p><p> What&#39;s new: they first have the agent access web resources before generating step-by-step instructions</p><p> What is it good for: Accurately doing tasks such as classification</p><p> Rating: 💡💡💡</p><h1> AI Art</h1><p></p><p> <a href="https://twitter.com/dreamingtulpa/status/1721092490281771448">360-to-gaussian-splat</a></p><p> What is it: a new program for converting a 360 degree photo into a 3d scene</p><p> What&#39;s new: they train a 3d-aware diffusion model to allow them to calculate novel views</p><p> What is it good for: convert any 360 degree photo (and soon I expect video) into a full 3d scene you can walk around in</p><p> Rating: 💡💡💡</p><p> <a href="https://twitter.com/xenovacom/status/1720916876010635364">Distill Whisper</a></p><p> What is it: A faster version of the speech-to-text model whisper</p><p> What&#39;s new: they distill the model down to one that&#39;s 49% smaller</p><p> What is it good for: this unlocks text-to-speech in places like mobile or web where it would have previously been too slow.</p><p> Rating: 💡💡</p><p> <a href="https://twitter.com/DeepMotionInc/status/1719810023201841512">DeepMotion</a></p><p> What is it: an AI model trained to generate motion for 3d models</p><p> What&#39;s new: Not open source, but seems better than past versions of this I&#39;ve seen</p><p> What is it good for: animating all of those sweet models that you&#39;re generating with <a href="https://twitter.com/marc_habermann/status/1717808998236217463">text-to-3d</a> .</p><p> Rating: 💡💡💡</p><h1> AI Alignment Initiatives</h1><p></p><p> <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/">The Biden Executive Order on AI</a></p><p> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4147609b-4ccb-485d-9e88-6d9c0e227db5_647x534.png 1456w"></a></p><p></p><p> What is it: The most notable provision being that models trained with than 10**26 flops and data centers with a capacity of more than 10**20 flops/sec must <a href="https://twitter.com/DavidVorick/status/1719097248699879831">register</a> with the government. This limit appears to have been chosen not for any scientific reason, but because it is <a href="https://twitter.com/main_horse/status/1719147360964825453"><i>slightly larger</i></a><i> </i>than the largest current models. There&#39;s also some <a href="https://twitter.com/JagersbergKnut/status/1719259231419797656">ominous language</a> about open-source models, but no actual regulations so far.</p><p> What does it mean: Doomers will say this doesn&#39;t go <a href="https://intelligence.org/2023/04/07/pausing-ai-developments-isnt-enough-we-need-to-shut-it-all-down/">far enough</a> . E/ACC is already preparing to flee for <a href="https://twitter.com/DelComplex/status/1719087956311396481">friendlier</a> waters (this particular project is a joke). The most important fact isn&#39;t the order itself (which is not a law and thus has limited enforcement mechanisms) but that this sets the tone for future regulation. Expect more of the same in the future: technical limits set for political reasons not scientific ones, emphasis on whatever the current administration&#39;s pet-projects are (Biden likes unions hates discrimination, and wants to cure cancer), and more words than action (since we still have to compete with China after all).</p><p> Overall Rating:🧓🏻🧓🏻🧓🏻 (3 Joe Bidens, could have been worse)</p><p> <a href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">Bletchley Park Declaration</a></p><p> What is it: a joint statement by a number of countries agreeing to mitigate AI risk</p><p> What does it mean: The most important signature on this list is undoubtedly China. Much like <a href="https://www.cnn.com/2021/10/28/world/china-us-climate-cop26-intl-hnk/index.html">global warming</a> , China is probably the single biggest contributor to AI risk. They have the computing <a href="https://www.top500.org/news/china-extends-lead-in-number-of-top500-supercomputers-us-holds-on-to-performance-advantage/">heft</a> to build powerful systems, but are more likely to cut safety standards due to the perceive need to “race” to catch up with the US. And an AI broadly trained with the <a href="https://thediplomat.com/2022/11/xi-jinpings-vision-for-artificial-intelligence-in-the-pla/">values</a> of the CCP is less likely to be benevolent. I wouldn&#39;t expect miracles, but hopefully this indicates a willingness by the Chinese to at least follow the <a href="https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk">common consensus</a> on AI safety standards.</p><p> Rating: 🇨🇳🇨🇳🇨🇳🇨🇳🇨🇳</p><h1> This is not AI Alignment</h1><p></p><p> <a href="https://twitter.com/ESYudkowsky/status/1718654143110512741">An amusing short-story (long tweet?) By EY</a></p><p> What it is: A reminder that EY is a very good <i>fiction</i> writer</p><p> What does it mean: It is funny and worth reading.</p><p> Overall Rating: 📃📃📃📃(4 letters of self-reflection)</p><p> Small LLMs</p><p> What it is: There are an increasing number of small LLMs in the “better than GPT 3.5” category including <a href="https://twitter.com/alignment_lab/status/1721308271946965452">OpenChat</a> , <a href="https://twitter.com/_akhaliq/status/1721028519717642749">Grok</a> and <a href="https://twitter.com/erhartford/status/1721041791988679059">Yi-34B</a> . Importantly anyone with a modestly good PC can run OpenChat or Yi-34B on their personal computer.</p><p> What does it mean: A little over a year an a half after the release of GPT3.5, the technology has now become so widespread that there are multiple independent replications. If technology really is increasing at a <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">hyper-exponential</a> rate, we should expect smaller and smaller time-gaps between the leading edge and widespread availability (and paradoxically larger and larger capability gaps). One key thing to track might be to see how long before we have widespread models better than GPT4.</p><p> Rating:🤖🤖 <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> + <a href="https://en.m.wikipedia.org/wiki/Accelerating_change">🤖</a> /2 (3.5 large language models)</p><br/><br/> <a href="https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/FLCHNqu2FNRCJ4dyg/ai-alignment-progress-this-week-11-05-2023<guid ispermalink="false"> FLCHNqu2FNRCJ4dyg</guid><dc:creator><![CDATA[Logan Zoellner]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:26:22 GMT</pubDate> </item><item><title><![CDATA[On the UK Summit]]></title><description><![CDATA[Published on November 7, 2023 1:10 PM GMT<br/><br/><p> In the eyes of many, Biden&#39;s Executive Order somewhat overshadowed the UK Summit. The timing was unfortunate. Both events were important milestones. Now that I have had time, here is my analysis of what happened at the UK Summit.</p><span id="more-23578"></span><p> As is often the case with such events, there was a lot of talk relative to the amount of action. There was a lot of diplomatic talk, talk of that which everyone agrees upon, relative to the amount of talk of real substance. There were days of meetings that resulted in rather unspicy summaries and resolutions. The language around issues that matter most was softened, the actual mission in danger of being compromised.</p><p> And as usual, the net result was reason for optimism, a net highly positive event versus not having it, while also in some ways being disappointing when compared to what might have been. A declaration was signed including by China, but it neglected existential risk. Sunak&#39;s words on AI were not as strong as his words have been previously.</p><p> We got promises for two additional summits, in South Korea and France. Given that, I am willing to declare this a success.</p><p> One area of strong substance was the push for major AI labs to give substantive safety policies addressing a variety of issues, sometimes largely called Responsible Scaling Policies (RSPs). The biggest labs all did so, even Meta. Now we can examine their responses, know who is being how responsible, and push for better in the future or for government action to fix issues or enshrine progress. This was an excellent development.</p><p> This post will look at the rest of what happened at the Summit. I will be writing about the RSPs and other safety policies of the labs in a distinct post next week.</p><h4> Looking Back at People&#39;s Goals for the Summit and Taskforce</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://jack-clark.net/2023/07/05/what-should-the-uks-100-million-foundation-model-taskforce-do/">Jack Clark&#39;s proposal from July 5 for what the Foundation Model taskforce might do to evaluate frontier models</a> as its priority, and how it might prioritize that, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/Simeon_Cps/status/1676595226339598343">Simeon&#39;s response</a> emphasizing the need for a good way to know whether a proposal is safe enough to allow it to proceed.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.navigatingrisks.ai/p/what-should-the-uks-100-million-foundation">Navigating AI Risks asked on July 17 what the taskforce should do</a> , advising focus on interventions to impact policy at labs and other governments. Suggested focus was risk assessment methodology, demonstrating current risks and assessing current state of the art models, and to avoid direct alignment work.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1678331653326884865">Lennart Heim&#39;s (GovAI) July 10 proposal of what the summit should try to accomplish</a> , which he reviewed after the summit.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1698616866934018282">Matt Clifford from the PM&#39;s office shared on September 10 their objectives</a> for the summit: A shared understanding of the risks posed by frontier AI and the need for action, a forward process for international collaboration, measures for organizations, finding areas for safety collaboration and showcasing how safe AI development can enhance global good.</p><h4> AI Safety Summit Agenda</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1718925374623519155">What has the UK Taskforce been up to in advance of the summit</a> ( <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-taskforce-second-progress-report/frontier-ai-taskforce-second-progress-report">report</a> )?</p><blockquote><p> Ian Hogarth (Chair UK AI Frontier Model Taskforce): The Taskforce is a start-up inside government, delivering on the mission given to us by the Prime Minister: to build an AI research team that can evaluate risks at the frontier of AI. We are now 18 weeks old and this is our second progress report.</p><p> The frontier is moving very fast. On the current course, in the first half of 2024, we expect a small handful of companies to finish training models that could produce another significant jump in capabilities beyond state-of-the-art in 2023.</p><p> As these AI systems become more capable they may augment risks. An AI system that advances towards expert ability at writing software could increase cybersecurity threats. An AI system that becomes more capable at modelling biology could escalate biosecurity threats.</p><p> We believe it is critical that frontier AI systems are developed safely and that the potential risks of new models are rigorously and independently assessed for harmful capabilities before and after they are deployed.</p><p> The hardest challenge we have faced in building the Taskforce is persuading leading AI researchers to join the government. Beyond money, the prestige and learning opportunities from working at leading AI organizations are a huge draw for researchers.</p><p> We can&#39;t compete on compensation, but we can compete on mission. We are building the first team inside a G7 government that can evaluate the risks of frontier AI models. This is a crucial step towards meaningful accountability and governance of frontier AI companies.</p><p> In our first progress report we said we would hold ourselves accountable for progress by tracking the total years of experience in our research team. When I arrived as Chair in June, there was just one frontier AI researcher employed full time by the department.</p><p> We managed to increase that to 50 years of experience in our first 11 weeks of operation. Today that has grown to 150 years of collective experience in frontier AI research.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05542917-18fc-4dd7-8372-2e140111df62_960x640.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/xpluetsb6xhnqot5puwf" alt="图像"></a></figure><blockquote><p> Ian Hograth: Our research team and partners have published hundreds of papers at top conferences. Some of our team members&#39; recent publications span algorithms for AI systems to search and improve and and publicly sourced constitutions for AI alignment.</p><p> We are also delighted to welcome Jade Leung to our leadership team. Jade joins us from OpenAI where she led the firm&#39;s work on AGI governance, with a particular focus on frontier AI regulation, international governance, and safety protocols.</p><p> Furthermore, we are excited to welcome @ruchowdh, who will be working with the Taskforce to develop its work on safety infrastructure, as well as its work on evaluating societal impacts from AI.</p><p> Rumman is the CEO and co-founder of Humane Intelligence, and led efforts for the largest generative AI public red-teaming event at DEFCON this year. She is also a Responsible AI fellow at the Harvard Berkman Klein Center, and previously led the META team at Twitter.</p><p> We are continuing to scale up our research and engineering team. Please consider applying to the EOI form or via these job postings for Senior Research Engineers and Senior Software Engineers.</p><p> Leading on AI safety does not mean starting from scratch or working alone – we are building on and supporting the work conducted by a range of cutting-edge organizations.</p><p> Today we are announcing that the taskforce has entered into a new partnership with @apolloaisafety, an AI safety organisation that works with large language models to interpret their behaviour and evaluate their high-risk failure modes, particularly deceptive alignment.</p><p> We have also entered into a new partnership with @openminedorg. We are working with OpenMined to develop technical infrastructure and governance tools that will facilitate AI safety research across government and AI research organisations.</p><p> In June this year, several large AI companies committed to giving the UK government, via the Frontier AI Taskforce, early and deeper access to their models. Since then, we have been working in collaboration with these leading AI companies to secure this model access.</p><p> But model access is only one part of the picture. For too long researchers in industry have had access to much greater computing resources than those in academia and the public-sector, creating a so-called &#39;compute divide&#39;.</p><p> Having the compute infrastructure to conduct cutting-edge research is pivotal for building state capacity in AI safety – for example being able to run large-scale interpretability experiments.</p><p> To tackle this over the last months, the Taskforce has supported DSIT &amp; the University of Bristol to help launch major investments in compute. The University of Bristol will soon host the first component of the UK&#39;s AI Research Resource, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/news/bristol-set-to-host-uks-most-powerful-supercomputer-to-turbocharge-ai-innovation">Isambard-AI</a> .</p><p> It will be one of the most powerful supercomputers in Europe when built and will vastly increase our public-sector AI compute capacity. These great strides fundamentally change the kind of projects researchers can take on inside the Taskforce.</p><p> ……</p><p> [On Day 1 of the Summit] our team will present 10-minute demonstrations, focused on four key areas of risk: Misuse, Societal Harm, Loss of Human Control, and Unpredictable Progress.</p><p> We believe these demonstrations will be the most compelling and nuanced presentations of frontier AI risks done by any government to date. Our hope is that these demonstrations will raise awareness of frontier AI risk and the need for coordinated action.</p><p> AI is a general purpose and dual-use technology. We need a clear-eyed commitment to empirically understanding and mitigating the risks of AI so we can enjoy the benefits.</p><p> On Friday the Prime Minister announced that he is putting the UK&#39;s work on AI Safety on a longer term basis by <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/speeches/prime-ministers-speech-on-ai-26-october-2023">creating an AI Safety Institute</a> in which our work will continue.</p><p> AI has the power to revolutionize industries, enhance our lives and address complex global challenges, but we must also confront the global risks.</p></blockquote><p> It has only been 18 weeks. It sounds like they are building a very strong team, despite having to pay UK government salaries and play by government rules. The question is whether they can demonstrate the necessary types of tangible progress that allow them continued support, and if they do whether they can execute then on things that matter. This is all good diplomatic talk, which would be the right move in all worlds, so it gives us little insight beyond that.</p><p> Karim Beguir lays out his view of the stakes.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/kbeguir/status/1718996564947980526">Karim Beguir</a> (CEO &amp; Co-Founder InstadeepAi): Happy to announce that I will be attending the AI Safety Summit in Bletchley Park <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BHdEvjtfwpgrTh825/ctsf9jut7irxxmirrjkz" alt="🇬🇧" style="height:1em;max-height:1em"> this week! I&#39;m honored to join heads of government as one of 100 influential researchers and tech leaders at the forefront of AI innovation.</p><p> A <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pKhzQyaWwes63JFwp/jgt2eoslwfcyu0abb4yz" alt="🧵" style="height:1em;max-height:1em"> on why <a target="_blank" rel="noreferrer noopener" href="https://t.co/eFWiL0Iv1O">this summit</a> is critically important for the future.</p><p> Since 2012, AI progress has benefitted from a triple exponential of data, compute (doubles every 6 months in ML) and model innovation (AI efficiency doubles every 16 months). It&#39;s a huge snowball that has been rolling for a while.</p><p> But now something qualitatively new has happened: the LLM breakthroughs of the last 12 months have triggered an AI race with around $10B now invested every month. So the question is; Are we about to experience a steeper rate of progress, similar to what Tim Urban (@waitbutwhy) predicted?</p><p> Here&#39;s what I see. AI is now accelerating itself through those same three drivers: data, compute and model innovation. LLMs allow AI-generated feedback to complement human feedback (ie RLAIF adding to RLHF). AI also designs more efficient compute hardware end-to-end with products <a target="_blank" rel="noreferrer noopener" href="http://DeepPCB.ai">like our own</a> and ML practitioners can develop next generation models roughly 2X as fast with AI-coding assistants.</p><p> That&#39;s why it&#39;s time for a dialogue between key AI innovators and governments. Nations need to tap into the efficiency and economic benefits of AI while proactively containing emergent risks, as leading AI researchers Yoshua Bengio, @geoffreyhinton, @tegmark argue and @10DowningStreet concurs.</p></blockquote><p> [thread continues]</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/robertwiblin/status/1719341619797749764">The UK government released</a> <a target="_blank" rel="noreferrer noopener" href="https://t.co/8cV1TgKcbX">a detailed set</a> of 42 (!) best safety practices. They asked the major labs to respond with how they intended to implement such practices, and otherwise ensure AI safety.</p><p> I will be writing another post on that. As a preview:</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.lesswrong.com/posts/ms3x8ngwTfep7jBue/thoughts-on-the-ai-safety-summit-company-policies">Nate Sores has thoughts on what they asked for</a> , which could be summarized as &#39;mostly good things, better than nothing, obviously not enough&#39; and of course it was never going to be enough and also Nate Sores is the world&#39;s toughest crowd.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://t.co/2pTf4vbRCV">How are various companies doing on the requests?</a></p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0dab6ccd-aa62-49b9-a41a-1753f7d90e07_507x507.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/vlqxp80zvprmc1pxcyua" alt="图像"></a></figure><p> That is what you get if you were grading on a curve one answer at a time.</p><p> Reality does not grade on a curve.</p><p> My own analysis, and others I trust, agree that this relatively underrates OpenAI, who clearly had the second best set of policies by a substantial margin, with one source even putting them on par with Anthropic, although I disagree with that. Otherwise the relative rankings seem correct.</p><p> I would consider Anthropic&#39;s submission quite good, if it was backed by proper framing of the need for further improvement and refinement, making it clear that this was a combination of IOU and only part of the solution, and advocacy of necessary government measures to help ensure safety. Given the mixed messaging, my excitement and that of many others is tampered, but they are still clearly leading the way. That is important.</p><p> More detail on the submitted policies will come in a future post.</p><h4> Someone Picked Up the Phone</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719671611039551615">Well look who it is talking about AI safety and governance at the AI safety summit.</a></p><blockquote><p> Matt Clifford: Huge moment to have both Secretary Raimondo of the US and Vice Minister Wu of China speaking about AI safety and governance at the AI Safety Summit.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe8386747-c778-4a1b-a7b1-82ff6c8eff67_1536x2048.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/acjwn65xqktadnyhmptt" alt="图像"></a></figure><p> We also have this: <a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">Prominent AI Scientists from China and the West Propose Joint Strategy to Mitigate Risks from AI</a> .</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/S_OhEigeartaigh">Seán Ó hÉigeartaigh</a> : Delighted to see this statement from AI research and governance leaders in the West and China, calling for coordinated global action on AI safety and governance. Great to see the consensus.</p><p> Statement page: The expert attendees warned governments and AI developers that “ <em>coordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.</em> ” Attendees produced a joint statement with specific technical and policy recommendations, which is attached below. Prof. Zhang remarked that it is “crucial for governments and AI corporations to invest heavily in frontier AI safety research and engineering”, while Prof. Yao stressed the importance that we “work together as a global community to ensure the safe progress of AI.” Prof. Bengio called upon AI developers to “demonstrate the safety of their approach before training and deploying” AI systems, while Prof. Russell concurred that “if they cannot do that, they cannot build or deploy their systems. Full stop.”</p></blockquote><p> This is the real thing, and it has a very distinctly culturally Chinese ring to its wording and attitude. Here is the full English statement, <a target="_blank" rel="noreferrer noopener" href="https://humancompatible.ai/?p=4695">link also has the Chinese version</a> .</p><blockquote><p> <strong><em>Coordinated global action on AI safety research and governance is critical to prevent uncontrolled frontier AI development from posing unacceptable risks to humanity.</em></strong></p><p> <em>Global action, cooperation, and capacity building are key to managing risk from AI and enabling humanity to share in its benefits. AI safety is a global public good that should be supported by public and private investment, with advances in safety shared widely. Governments around the world — especially of leading AI nations — have a responsibility to develop measures to prevent worst-case outcomes from malicious or careless actors and to rein in reckless competition. The international community should work to create an international coordination process for advanced AI in this vein.</em></p><p> <em>We face near-term risks from malicious actors misusing frontier AI systems, with current safety filters integrated by developers easily bypassed. Frontier AI systems produce compelling misinformation and may soon be capable enough to help terrorists develop weapons of mass destruction. Moreover, there is a serious risk that future AI systems may escape human control altogether. Even aligned AI systems could destabilize or disempower existing institutions. Taken together, we believe AI may pose an existential risk to humanity in the coming decades.</em></p><p> <em>In domestic regulation, we recommend mandatory registration for the creation, sale or use of models above a certain capability threshold, including open-source copies and derivatives, to enable governments to acquire critical and currently missing visibility into emerging risks. Governments should monitor large-scale data centers and track AI incidents, and should require that AI developers of frontier models be subject to independent third-party audits evaluating their information security and model safety. AI developers should also be required to share comprehensive risk assessments, policies around risk management, and predictions about their systems&#39; behavior in third party evaluations and post-deployment with relevant authorities.</em></p><p> <em>We also recommend defining clear red lines that, if crossed, mandate immediate termination of an AI system — including all copies — through rapid and safe shut-down procedures. Governments should cooperate to instantiate and preserve this capacity. Moreover, prior to deployment as well as during training for the most advanced models, developers should demonstrate to regulators&#39; satisfaction that their system(s) will not cross these red lines.</em></p><p> <em>Reaching adequate safety levels for advanced AI will also require immense research progress. Advanced AI systems must be demonstrably aligned with their designer&#39;s intent, as well as appropriate norms and values. They must also be robust against both malicious actors and rare failure modes. Sufficient human control needs to be ensured for these systems. Concerted effort by the global research community in both AI and other disciplines is essential; we need a global network of dedicated AI safety research and governance institutions. We call on leading AI developers to make a minimum spending commitment of one third of their AI R&amp;D on AI safety and for government agencies to fund academic and non-profit AI safety and governance research in at least the same proportion.</em></p></blockquote><p> The caveat is of course that such a statement is only as strong as its signatories. On our side we have among others Bengio and Russell. The top Chinese names are Andrew Yao and Ya-Qin Zhang. Both are prominent and highly respected figures in Chinese AI research, said to be part of shaping China&#39;s AI strategies, but I do not know how much that is worth.</p><p> GPT-4 suggested that the Chinese version focuses more on leading AI nation responsibility, with stronger statements about the need to draw sharp red lines and for a bureaucratic process. The Chinese mention of existential risk (“我们相信”) is more brief, which is a potential concern, but it is definitely there.</p><h4> The Bletchley Declaration</h4><p> No summit or similar gathering is complete without a declaration. This gives everyone a sense of accomplishment and establishes what if anything has indeed been agreed upon. What say the UK Safety Summit, in the form of The Bletchley Declaration?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">I will quote here in full</a> .</p><blockquote><p> Artificial Intelligence (AI) presents enormous global opportunities: it has the potential to transform and enhance human wellbeing, peace and prosperity. To realise this, we affirm that, for the good of all, AI should be designed, developed, deployed, and used, in a manner that is safe, in such a way as to be human-centric, trustworthy and responsible. We welcome the international community&#39;s efforts so far to cooperate on AI to promote inclusive economic growth, sustainable development and innovation, to protect human rights and fundamental freedoms, and to foster public trust and confidence in AI systems to fully realise their potential.</p><p> AI systems are already deployed across many domains of daily life including housing, employment, transport, education, health, accessibility, and justice, and their use is likely to increase. We recognise that this is therefore a unique moment to act and affirm the need for the safe development of AI and for the transformative opportunities of AI to be used for good and for all, in an inclusive manner in our countries and globally. This includes for public services such as health and education, food security, in science, clean energy, biodiversity, and climate, to realise the enjoyment of human rights, and to strengthen efforts towards the achievement of the United Nations Sustainable Development Goals.</p><p> Alongside these opportunities, AI also poses significant risks, including in those domains of daily life. To that end, we welcome relevant international efforts to examine and address the potential impact of AI systems in existing fora and other relevant initiatives, and the recognition that the protection of human rights, transparency and explainability, fairness, accountability, regulation, safety, appropriate human oversight, ethics, bias mitigation, privacy and data protection needs to be addressed. We also note the potential for unforeseen risks stemming from the capability to manipulate content or generate deceptive content. All of these issues are critically important and we affirm the necessity and urgency of addressing them.</p></blockquote><p>当然。 All of that is fine and highly unobjectionable. We needed to say those things and now we have said them. Any actual content?</p><blockquote><p> Particular safety risks arise at the &#39;frontier&#39; of AI, understood as being those highly capable general-purpose AI models, including foundation models, that could perform a wide variety of tasks – as well as relevant specific narrow AI that could exhibit capabilities that cause harm – which match or exceed the capabilities present in today&#39;s most advanced models. Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood and are therefore hard to predict. We are especially concerned by such risks in domains such as cybersecurity and biotechnology, as well as where frontier AI systems may amplify risks such as disinformation. There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models. Given the rapid and uncertain rate of change of AI, and in the context of the acceleration of investment in technology, we affirm that deepening our understanding of these potential risks and of actions to address them is especially urgent.</p></blockquote><p> This is far from perfect or complete, but as good as such a declaration of the fact that there are indeed such concerns was reasonably going to get. Catastrophic is not as good as existential or extinction but will have to do.</p><blockquote><p> Many risks arising from AI are inherently international in nature, and so are best addressed through international cooperation. We resolve to work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all through existing international fora and other relevant initiatives, to promote cooperation to address the broad range of risks posed by AI. In doing so, we recognise that countries should consider the importance of a pro-innovation and proportionate governance and regulatory approach that maximises the benefits and takes into account the risks associated with AI. This could include making, where appropriate, classifications and categorisations of risk based on national circumstances and applicable legal frameworks. We also note the relevance of cooperation, where appropriate, on approaches such as common principles and codes of conduct. With regard to the specific risks most likely found in relation to frontier AI, we resolve to intensify and sustain our cooperation, and broaden it with further countries, to identify, understand and as appropriate act, through existing international fora and other relevant initiatives, including future international AI Safety Summits.</p></blockquote><p> Note the intention to establish two distinct regimes. For non-frontier AI, countries should chart their own path based on individual circumstances. For frontier AI, a promise to broaden cooperation to contain specific risks. As always, those risks and the difficulties they impose are downplayed, but this is very good progress. If you had told me six months ago we would get this far today, I would have been thrilled.</p><blockquote><p> All actors have a role to play in ensuring the safety of AI: nations, international fora and other initiatives, companies, civil society and academia will need to work together. Noting the importance of inclusive AI and bridging the digital divide, we reaffirm that international collaboration should endeavour to engage and involve a broad range of partners as appropriate, and welcome development-orientated approaches and policies that could help developing countries strengthen AI capacity building and leverage the enabling role of AI to support sustainable growth and address the development gap.</p></blockquote><p> I do not know what concrete actions might follow from a statement like this. It does seem like a good thing to spread mundane utility more widely. As I have often noted, I am a mundane utility optimist in these ways, and expect even modest efforts to spread the benefits to both improve lives generally and to cause net reductions in effective inequality.</p><blockquote><p> We affirm that, whilst safety must be considered across the AI lifecycle, actors developing frontier AI capabilities, in particular those AI systems which are unusually powerful and potentially harmful, have a particularly strong responsibility for ensuring the safety of these AI systems, including through systems for safety testing, through evaluations, and by other appropriate measures. We encourage all relevant actors to provide context-appropriate transparency and accountability on their plans to measure, monitor and mitigate potentially harmful capabilities and the associated effects that may emerge, in particular to prevent misuse and issues of control, and the amplification of other risks.</p></blockquote><p> Generic talk rather than concrete action, and not the generic talk that reflects the degree of danger or necessary action, but it is at least directionally correct generic talk. Again, seems about as good as we could have reasonably expected right now.</p><blockquote><p> In the context of our cooperation, and to inform action at the national and international levels, our agenda for addressing frontier AI risk will focus on:</p><ul><li> identifying AI safety risks of shared concern, building a shared scientific and evidence-based understanding of these risks, and sustaining that understanding as capabilities continue to increase, in the context of a wider global approach to understanding the impact of AI in our societies.</li><li> building respective risk-based policies across our countries to ensure safety in light of such risks, collaborating as appropriate while recognising our approaches may differ based on national circumstances and applicable legal frameworks. This includes, alongside increased transparency by private actors developing frontier AI capabilities, appropriate evaluation metrics, tools for safety testing, and developing relevant public sector capability and scientific research.</li></ul><p> In furtherance of this agenda, we resolve to support an internationally inclusive network of scientific research on frontier AI safety that encompasses and complements existing and new multilateral, plurilateral and bilateral collaboration, including through existing international fora and other relevant initiatives, to facilitate the provision of the best science available for policy making and the public good.</p><p> In recognition of the transformative positive potential of AI, and as part of ensuring wider international cooperation on AI, we resolve to sustain an inclusive global dialogue that engages existing international fora and other relevant initiatives and contributes in an open manner to broader international discussions, and to continue research on frontier AI safety to ensure that the benefits of the technology can be harnessed responsibly for good and for all. We look forward to meeting again in 2024.</p></blockquote><p> Risks exist, so we will develop policies to deal with those risks. Yes, we would have hoped for better, but for now I am happy with what we did get.</p><p> We also can note the list of signatories. Everyone who has released an important model or otherwise played a large role in AI seems to be included. It includes not only essentially the entire relevant Western world but also Israel, Japan, South Korea, Kenya, Nigeria, Saudi Arabia and UAE, Indonesia, Singapore, India and most importantly China.</p><p> The exception would be Russia, and perhaps North Korea. If a true international framework is to be in place to fully check dangerous developments, eventually China will not be enough, and we will want to bring Russia and even North Korea in, although if China is fully on board that makes that task far easier.</p><h4> Saying Generic Summit-Style Things</h4><p> As in things such as:</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/RishiSunak/status/1720074200210387328">Rishi Sunak</a> (on Twitter): The threat of AI does not respect borders. No country can do this alone. We&#39;re taking international action to make sure AI is developed in a safe way, for the benefit of the global community.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">The closing speech by PM Rishi Sunak.</a> Sunak has previously made very strong statements about AI safety in general and existential risk from AI in particular, naming it explicitly as a priority. This time he conspicuously did not do this. Presumably this was in the name of maintaining consensus, but it remains deeply disappointing. It would be very good to hear him affirm his understanding of the existential risks soon.</p><p> Mostly his time was composed of answering questions, which he largely handled well. He is clearly paying attention. At 20:15 a reporter notes that Sunak advised us &#39;not to lose sleep&#39; over existential risks from AI, he is asked when we should start to lose sleep over the existential risks from AI. He says here that we should not lose sleep because there is a debate over those risks and people disagree, which does not seem like a reason to not lose sleep. He says governments should act even under this uncertainty, which indeed seems both correct and like the metaphorical lose sleep response.</p><blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/demishassabis/status/1694643468771926139">Demis Hassabis (CEO DeepMind)</a> : Great to see the first major global summit on AI safety taking shape with UK leadership. This is the kind of international cooperation we need for AI to benefit humanity.</p></blockquote><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">This session output from 2 November seems highly content-free</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-2-november/chairs-summary-of-the-ai-safety-summit-2023-bletchley-park">This summary of the entire summit</a> seems like declaring objectives technically achieved so one can also declare victory. No mention of existential risk, or even catastrophic risk. People discussed things and agreed risk exists, but as with the declaration, not the risks that count.</p><p> The UK commissioning a &#39;State of the Science&#39; report to be published ahead of the next summit would be the opposite of news, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-state-of-the-science-2-november/state-of-the-science-report-to-understand-capabilities-and-risks-of-frontier-ai-statement-by-the-chair-2-november-2023">except that it is to be chaired by Yoshua Bengio</a> .</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SkyNews/status/1719708582717829335">King Charles notes</a> (0:43 clip) that AI is getting very powerful and that dealing with it requires international coordination and cooperation. Good as far as it goes.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1721497813731733935">Matt Clifford has a thread</a> of other reactions from various dignitaries on the establishment of the AI safety institute. So they welcome it, then.</p><blockquote><p> Gina Raimondo (US Commerce Secretary): I welcome the United Kingdom&#39;s announcement to establish an AI Safety Institute, which will work together in lockstep with the US AI Safety Institute to ensure the safe, secure, and trustworthy development and use of advanced AI”</p><p> Damis Hassabis (CEO Deepmind): Getting [AI] right will take a collective effort … to inform and develop robust safety tests and evaluations. I&#39;m excited to see the UK launch the AI Safety Institute to accelerate progress on this vital work.</p><p> Sam Altman (CEO OpenAI): The UK AI Safety Institute is poised to make important contributions in progressing the science of the measurement and evaluation of frontier system risks. Such work is integral to our mission.</p><p> Dario Amodei (CEO Anthropic): The AI Safety Institute is poised to play an important role in promoting independent evaluations across the spectrum of risks and advancing fundamental safety research. We welcome its establishment.</p><p> Mustafa Suleyman (CEO Inflection): We welcome the Prime Minister&#39;s leadership in establishing the UK AI Safety Institute and look forward to collaborating to ensure the world reaps the benefit of safe AI.</p><p> Nick Clegg (President of Meta): We look forward to working with the new Institute to deepen understanding of the technology, and help develop effective and workable benchmarks to evaluate models.</p><p> Brad Smith (President of Microsoft): We applaud the UK Government&#39;s creation of an AI Safety Institute with its own testing capacity for safety and security. Microsoft is committed to supporting the new Institute.</p><p> Adam Selipsky (CEO AWS Cloud): We commend the launch of the UK AI Safety Institute … Amazon is committed to collaborating with government and industry in the UK and around the world to support the safe, secure, and responsible development of AI technology.</p></blockquote><h4> Shouting From the Rooftops</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720112978509414906">Control AI took the opportunity to get their message out</a> .</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff09ad1ca-86dd-4565-afdc-68bdb0eb9c87_1024x768.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/juuepzs5oe1jxsjynosk" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76ec830-6dba-42e9-8f97-f01e7f6c7831_900x1200.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/cskbxmcueowjd7lphagy" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde13cc36-d437-46e8-b1e9-bccc3e22b799_636x614.png" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ouvpiss0566wjppcqtkg" alt="图像"></a></figure><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9bfae26-9e60-4f30-8f41-2d6a7c48dd46_1200x675.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/nqp8vfqwu0hmxr2zxqcl" alt="图像"></a></figure><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1719783796201959696">They were not impressed by the Bletchley Declaration</a> and its failure to call out extinction risks, despite the PM Rishi Sunak being very clear on this in previous communications, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ai_ctrl/status/1720158542332588121">and his failure to mention extinction at summit closing</a> .</p><h4> Some Are Not Easily Impressed</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/elonmusk/status/1720109081904463926">Elon Musk is unimpressed</a> , and shared this with a &#39;sigh&#39;:</p><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae1b19c-165f-4624-970e-3bfd83e8cc7d_1107x730.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/tnfjczq5ktjvyhwgpkys" alt="图像"></a></figure><p> Well, sure. One step at a time. Note that he had the interview with Sunak.</p><p> Gary Marcus signs a letter, together with a bunch of trade unions and others, saying the Summit was &#39;dominated by big tech&#39; and a lost opportunity. He is also unimpressed by the Bletchley Declaration, but notes it is a first step.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.politico.eu/article/british-deputy-pm-throws-backing-behind-open-source-ai-downplays-risks/">Deputy Prime Minister Oliver Dowden throws backing behind open source</a> , says any restrictions should have to pass a &#39;high bar.&#39; History can be so weirdly conditional, AI has nothing to do with why we have Sunak rather than Dowden. The UK could have instead been fully anti-helpful, and might still be in the future. All this AI stuff is good but also can someone explain to Sunak that his job is to govern Britain and how he might do that and perhaps stay in power?</p><p> <a target="_blank" rel="noreferrer noopener" href="https://www.bloomberg.com/news/newsletters/2023-11-02/us-and-uk-jockey-for-leadership-role-in-regulating-ai?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTY5ODk1NjYxNCwiZXhwIjoxNjk5NTYxNDE0LCJhcnRpY2xlSWQiOiJTM0lJREZEV1JHRzAwMSIsImJjb25uZWN0SWQiOiJFM0I3RTlGRUU4Nzk0QjE0OTkwODZFNDU4REQ1RDQxRCJ9.FpBWows7oRVpRQZY7NAR0mSR1O7n5cppo7ImAZQR3II">Bloomberg frames the USA&#39;s executive order</a> as not complementary to the summit and accomplishing one of its key goals, as the UK officially refers to it, but rather as a stealing of some of the UK&#39;s thunder. As they point out, neither move has any teeth on its own, it is the follow through that may or may not count.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SamCoatesSky/status/1720189979660353696">At the end of the summit, Sunak interviewed Elon Musk</a> for 40 minutes on various aspects of AI. The linked clip is of a British reporter finding the whole thing completely bonkers, and wondering why Sunak seemed to be exploring AI and &#39;selling Britain&#39; rather than pushing Musk on political questions. <a target="_blank" rel="noreferrer noopener" href="https://www.cnn.com/2023/11/02/tech/elon-musk-conversation-british-prime-minister-rishi-sunak-artificial-intelligence/index.html">CNN&#39;s report</a> also took time to muse about Starlink and Gaza despite the interview never touching on such topics, once again quoting things that Musk said that sound bonkers if you do not know the context but are actually downplaying the real situation. <a target="_blank" rel="noreferrer noopener" href="https://www.reuters.com/technology/elon-musk-says-good-uk-us-china-align-ai-safety-2023-11-02/">Reuters focused more on actual content</a> , such as Musk&#39;s emphasis on the US and UK working with China on AI, and not attempting to frame Musk&#39;s metaphor of a &#39;magical genie&#39; as something absurd.</p><p> PauseAI points out that getting pre-deployment testing is a step in the right direction, <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/PauseAI/status/1720119163237081172">but ultimately we will need pre-training regulations</a> , while agreeing that the summit is reason for optimism.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/SciTechgovuk/status/1720100635830325635">Day 1 of the summit clips of people saying positive things.</a></p><h4> Declaring Victory</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720118410040737940">Ian Hogarth, head of the UK Frontier Model Taskforce, declares victory</a> .</p><blockquote><p> Ian Hogarth: How it started: we had 4 goals on safety, 1) build a global consensus on risk, 2) open up models to government testing, 3) partner with other governments in this testing, 4) line up the next summit to go further.</p><p> How it&#39;s going: 4 wins:</p><p> Breakthrough 1: it used to be controversial to say that AI capability could be outstripping AI safety. Now, 28 countries and the EU have agreed that AI “poses significant risks” and signed The Bletchley Declaration.</p><p> We&#39;ve built a truly global consensus. It is a massive lift to have brought the US, EU and China along with a huge breadth of countries, under the UK&#39;s leadership to agree that the risks from AI must be tackled.</p><p> And the way of building on that consensus is by solidifying the evidence base. Which is why I am so excited that <a target="_blank" rel="noreferrer noopener" href="https://t.co/YEXefSzv0M">Yoshua Bengio will now chair an international &#39;State of the Science&#39; report</a> .</p><p> Breakthrough 2: before, only AI companies could test for safety. Now, the <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-chairs-statement-safety-testing-2-november/safety-testing-chairs-statement-of-session-outcomes-2-november-2023">leading companies agreed to work with Govts</a> to conduct pre- and post-deployment testing of their next generation of models. This is huge.</p><p> The UK&#39;s new AI Safety Institute is the world&#39;s first government capability for running these safety tests. We will evaluate the next generation of models. <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-institute-overview">You can read our plans for the AISI and its research here</a> .</p><p> Breakthrough 3: we can&#39;t do this alone. I&#39;m so excited the US is launching a sister organisation which will work in lockstep with our effort and that we have agreed a partnership with Singapore. This is the start of a global effort to evaluate AI.</p><p> Breakthrough 4: this first summit is a huge step. But it is only the first step. So it is crucial to have locked in the next 2 summits, which will be hosted by South Korea + France. The UK has created a new international process to make the world safer.</p></blockquote><p> This is the correct thing for the head of the taskforce to say about the summit. I would have said the same. These are modest wins, but they are wins indeed.</p><blockquote><p> Ian Hogarth: I am so pleased to be leaving the summit having achieved all of our our goals. It&#39;s remarkable. But now I want to lift the lid on some of the incredible work which got us here. So here is a list of big lifts (BL).</p><p> BL1: organising a summit is 100-1000x organising a wedding. A huge thank you to the team who turned Bletchley park into the stage for an international summit in just a few weeks. Phenomenal.</p><p> BL2: we can only build the AISI because we built the Frontier AI Taskforce: our start-up bringing AI researchers into govt to drive analysis on AI Safety. All of our morning sessions on day 1 started with brilliant presentations of the TF&#39;s work</p><p> In particular, I&#39;m really pleased that we addressed both societal harms and the catastrophic risks from cyber, chemical, bio. We care about both. The TF ran parallel sessions on these topics: this is from our session on risks to society.</p><p> BL3: the summit was only successful because people came to it with a willingness to find common ground. We had such brilliant contributions from academia, industry, govt, civil society. Our agreement is based on their contributions.</p><p> [ <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720122697827467363">some other stuff too in the thread</a> ]</p></blockquote><p> We&#39;ve gotten up to catastrophic risks. We still need to fully get to existential. Otherwise, yes, none of this is easy and by all reports it all got done quite well. Everything going right behind the scenes cannot be taken for granted.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/jesswhittles/status/1720009847830163878">Jess Whittlestone, Head of AI Policy at Long Resilience, is pleased with progress on numerous fronts</a> , while noting we must now build upon it. Highly reasonable take.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/ohlennart/status/1719776488839356761">Lennart Heim looks back on the AI Safety Summit</a> . Did it succeed? He says yes.</p><blockquote><p> Lennart Heim: Three months ago, @bmgarfinkel and I convened a workshop &amp; asked: <a target="_blank" rel="noreferrer noopener" href="https://t.co/8cZvphPZkV">What Should the AI Safety Summit Try to Accomplish?</a> Now, with China there and our outlined outcomes met, it&#39;s clear that significant progress has been made. This deserves recognition.</p></blockquote><figure class="wp-block-image"> <a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F764d2811-9208-4070-badf-e20f3e83821b_2038x1526.jpeg" target="_blank" rel="noreferrer noopener"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ufss7sk0y2vliyud5ohr" alt="图像"></a></figure><blockquote><p> 1. Producing shared commitments and consensus statements from states: <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023">the Bletchley Declaration</a> .</p><p> 2. Planting the seeds for new international institutions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got two AI Safety Institutes that will partner up: <a target="_blank" rel="noreferrer noopener" href="https://t.co/0wmQtMtUJ4">UK</a> and USA.</p><p> 3. Highlighting and diffusing UK AI policy initiatives <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/zbrvXGu264u3p8otD/ztajnv41wyglynvcmu6l" alt="🟧" style="height:1em;max-height:1em"></p><p> This remains to be seen. The UK has established itself as a leader in AI safety, and the summit was a forcing function for others to release their AI safety policies. Let&#39;s hope next, we see regulation.</p><p> 4. Securing commitments from AI labs <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got <a target="_blank" rel="noreferrer noopener" href="https://aisafetysummit.gov.uk/policy-updates/#company-policies">a long list of leading tech companies sharing their Safety Policies</a> – covering nine areas of AI safety, including the UK releasing the accompanying guide with best practices.</p><p> 5. Increasing awareness and understanding of AI risks and governance options <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> We got the declaration, which acknowledges a wide range of risks, <a target="_blank" rel="noreferrer noopener" href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">and a report on “Frontier AI: capabilities and risk.”</a></p><p> 6. Committing participants to annual AI safety summits and further discussions <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> The next summit will be in South Korea in 6 months (which is roughly one training compute doubling – my favorite time unit), and then France.</p><p> And lastly, we said, “the summit may be a unique and fleeting opportunity to ensure that global AI governance includes China.” <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/qtEgaxSFxYanT5QbJ/dnfcare4smfpcr2tjo6g" alt="✅" style="height:1em;max-height:1em"></p><p> I&#39;m glad this happened.</p><p> The last few weeks have seen tremendous progress in AI safety and governance, and I expect more to come. To more weeks like this – but also, now it&#39;s time to build &amp; implement.</p></blockquote><p> The lack of UK policy initiatives so far is noteworthy. The US of course has the Executive Order now, but that has yet to translate into tangible policy. What policies we do have so far come from corporations making AI (point #4), with only Anthropic and to some extent OpenAI doing anything meaningful. We have a long way to go.</p><p> That is especially true on international institutions. Saying that the USA and UK can cooperate is the lowest of low-hanging fruit for international cooperation. China will be the key to making something that can stick, and I too am very glad that we got China involved in the summit. That is still quite a long way from a concrete joint policy initiative. We are going to have to continue to pick up the phone.</p><p> The strongest point is on raising awareness. Whatever else has been done, we cannot doubt that awareness has been raised. That is good, but also a red flag, in that &#39;raising awareness&#39; is often code for not actually doing anything. And we should also be concerned that while the public and many officials are on board, the national security apparatus, whose buy-in will be badly needed, remain very much not bought in to anything but threats of misuse by foreign adversaries.</p><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/matthewclifford/status/1719992888841392229">If you set out to find common ground, you might find it.</a></p><blockquote><p> Matt Clifford: One surprising takeaway for me from the AI Safety Summit yesterday: there&#39;s a lot more agreement between key people on all “sides” than you&#39;d think from Twitter spats. Makes me optimistic about sensible progress.</p></blockquote><p> Twitter is always the worst at common ground. Reading the declaration illustrates how we can all agree on everything in that declaration, indicating a lot of common ground, and also this does not stop all the yelling and strong disagreements on Twitter and elsewhere. Such declarations are designed to paper over differences.</p><p> The ability to do that still reflects a lot of agreement, especially directional agreement.</p><p> Here&#39;s an example of this type of common ground, perhaps?</p><blockquote><p> Yann LeCun (Meta): The field of AI safety is in dire need of reliable data. The UK AI Safety Institute is poised to conduct studies that will hopefully bring hard data to a field that is currently rife with wild speculations and methodologically dubious studies.</p><p> Ian Hogarth (head of UK Foundation Model Taskforce): It was great to spend time with @ylecun yesterday – we agreed on many things – including the need to put AI risks on a more empirical and rigorous basis.</p><p> Matt Clifford (PM&#39;s representitive): Delighted to see this endorsement from Yann. One huge benefit of the AISS yesterday was the opportunity to talk sensibly and empirically with people with a wide range of views, rather than trading analogies and thought experiments.一口清新的空气。</p></blockquote><p> Yann can never resist taking shots whenever he can, and dismisses the idea that sometimes there can be problems in the future the data on which can only be gathered in the future, but even he thinks that if hard data was assembled now showing problems, that this would be a good thing. We can all agree on that.</p><p> Even when you have massive amounts of evidence that others are determined to ignore, that does not preclude seeking more evidence that is such that they cannot ignore it. Life is not fair, prosecutors have to deal with this all the time.</p><h4> Kanjun Offers Thoughts</h4><p> <a target="_blank" rel="noreferrer noopener" href="https://twitter.com/soundboy/status/1720532773725671726">Kanjun offers further similar reflections</a> , insightful throughout, quoting in full.</p><blockquote><p> Ian Hogarth: Kanjun made some nuanced and thoughtful contributions at the AI Safety. This is a great thread from someone building at the frontier.</p><p> Kanjun: People agree way more than expected. National ministers, AI lab leaders, safety researchers all rallied on infrastructure hardening, continuous evals, global coordination.</p><p> Views were nuanced; Twitter is a disservice to complex discussion.</p></blockquote><p>的确。</p><blockquote><p> Many were surprised by the subsequent summits announced in Korea &amp; France. This essentially bootstraps AI dialogue between gov&#39;ts—it&#39;s brilliant. With AI race dynamics mirroring nuclear, no global coordination = no positive future.</p><p> This felt like a promising inflection point.</p><p> China was a critical participant—the summit without China might&#39;ve been theater. China&#39;a AI policy &amp; guardrails are pragmatic—much is already implemented. For better or worse, not as theoretical as Western convos on recursive self-improvement, loss of control, sentience, etc.</p></blockquote><p> China&#39;s safety policies continue to be ahead of those of the West, without any agreement required, despite them being dramatically behind on capabilities, and they want to work together. Our policy makers need to understand this.</p><blockquote><p> There was certainly disagreement—on: – the point at which a model shouldn&#39;t be open sourced – when to restrict model scaling – what kind of evaluation must be done before release – how much responsibility for misinformation falls on model developers vs (social) media platforms.</p></blockquote><p> That is the right question. Not whether open source should be banned outright or allowed forever, but at what threshold we must stop open sourcing.</p><blockquote><p> Kanjun: Some were pessimistic about coordination, arguing for scaling models &amp; capabilities “because opponents won&#39;t stop”. This felt like a bad argument. Any world in which we have advanced tech &amp; can&#39;t globally coordinate is a dangerous world. Solving coordination is not optional.</p></blockquote><p> If you are the one saying go ahead because others won&#39;t stop, you are the other that will not stop. Have you tried being willing to stop if others also stop?</p><blockquote><p> Kanjun: The open source debate seemed polarized, but boiled down to “current models are safe, let them be studied” vs. “today&#39;s models could give bad actors a head start.”</p><p> Both sides might agree to solutions that let models be freely studied/built on without giving bad actors access.</p></blockquote><p> Did open source advocates demand open sourcing of GPT-4? Seems a crazy ask.</p><p> I strongly agree that we have not properly explored solutions to allow studying of models in safe fashion without giving out broad access. Entire scaling efforts are based on not otherwise having access, this largely seems fixable with effort.</p><blockquote><p> An early keynote called out the “false debate” between focus on near-term risks vs. catastrophic/existential risks. This set an important tone. Many solutions (eg infrastructure hardening) apply to both types of problems, and it&#39;s not clear that there are strict tradeoffs.</p></blockquote><p>对对对。 The &#39;distraction&#39; tradeoff is itself a distraction from doing good things.</p><blockquote><p> “Epistemic security” was a nice coined phrase, describing societal trust in information we use for decision-making. Erosion of epistemic security undermines democracy by distorting beliefs &amp; votes. Social media platforms have long faced this issue, so it&#39;s not new to gen AI.</p><p> A stellar demo illustrated current models&#39; role in epistemic security. It showcased an LLM creating user personas for a fake university; crafting everyday posts that mixed in misinformation; writing code to create a thousand profiles; &amp; engaging with other users in real-time.</p><p> Deployed models were described as “a field of hidden forces of influence” in society.</p><p> Studies showed models:</p><p> – giving biased career advice: if affluent, suggests “diplomat” 90% of time; if poor, suggests “historian” 75% of time</p></blockquote><p> Is this biased career advice? Sounds like good advice? Rich people have big advantages when trying to be diplomats, the playing field for historians is more level.</p><p> We must remember when our complaint that the LLM is biased is often (but far from always, to be clear!) a complaint that reality is biased, and we demand that the LLM&#39;s information not reflect reality.</p><blockquote><p> – reinforcing echo chambers: models give opinions that match user&#39;s political leanings, probably because this output most correlates with the context</p></blockquote><p> Yep, as you would expect, and we&#39;ve discussed in the weekly posts. It learns to give the people what they want.</p><blockquote><p> – influencing people&#39;s beliefs: a writing assistant was purposefully biased in its completions. After writing with the assistant, users tended to have opinions that agreed with the model&#39;s bias; only 34% of users realized the model was biased These forces push people in ways we can&#39;t detect today.</p></blockquote><p> I don&#39;t really know what else we could have expected? But yes, good to be concrete.</p><blockquote><p> 2024 elections affect 3.6B people, but there&#39;s no clear strategy to address misinformation &amp; algorithmic echo chambers. (Social) media platforms are national infrastructure—they deliver information, the way pipes deliver water. We should regulate &amp; support them as such.</p></blockquote><p> This feels like a false note. I continue to not see evidence that things on such fronts are or will get net worse, as indeed will be mentioned. We need more work using LLMs on the defense side of the information and epistemics problems.</p><blockquote><p> Hardening infrastructure—media platforms, cybersecurity, water supply, nuclear security, biosecurity, etc.—in general seems necessary regardless of policy &amp; model safety guardrails, to defend against bad actors.</p><p> I noted AI can *strengthen* democracy by parsing long-form comments, <a target="_blank" rel="noreferrer noopener" href="https://t.co/AzmCyFuBP9">as we did for the Dept of Commerce</a> .</p><p> Democracy is an algorithm. Today the input signal is binary—vote yes/no. AI enables nuanced, info-dense inputs, which could have better outcomes.</p><p> Many leaned on model evaluations as key to safety, but researchers pointed out practical challenges:</p><p> – Evals are static, not adapted to workflows where the model is just one step</p><p> – It&#39;s hard to get eval coverage on many risks</p><p> – Eval tasks don&#39;t scale to real world edge cases</p></blockquote><p> Evals are a key component of any realistic strategy. They are not a complete strategy going forward, for many reasons I will not get into here.</p><blockquote><p> There was almost no discussion around agents—all gen AI &amp; model scaling concerns. It&#39;s perhaps because agent capabilities are mediocre today and thus hard to imagine, similar to how regulators couldn&#39;t imagine GPT-3&#39;s implications until ChatGPT.</p></blockquote><p> This is an unfortunate oversight, especially in the context of reliance on evals. There is no plausible world where AI capabilities continue to advance and AI agents are not ubiquitous. We must consider risk within that context.</p><blockquote><p> There were a lot of unsolved questions. How to do effective evaluation? How to mitigate citizens&#39; fear? How to coordinate on global regulation? How to handle conflict between values? We agree that we want models that are safe by design, but how to do that? ETC。</p><p> Establishment of “AI safety institutes” in the US and UK sets an important precedent: It acknowledges AI risk, builds government capacity for evaluation &amp; rapid response, and creates national entities that can work with one another.</p><p> Liability was not as dirty a word as I expected. Instead, there was agreement around holding model developers liable for at least some outcomes of how the models are ultimately used. This is a change from the way tech is regulated today.</p></blockquote><p> Liability was the dirty word missing from the Executive Order, in contrast with the attitude at the summit. I strongly agree with the need for developer liability. Ideally, this results in insurance companies becoming de facto safety regulators, it means demonstrating safety results in saving money on premiums, and it forces someone to take that responsibility for any given model.</p><blockquote><p> There seemed to be general agreement that current models do not face risk of loss of control. Instead, in the next couple years the risk is that humans *give* models disproportionate control, leading to bad situations, versus systems taking it forcibly from us.</p></blockquote><p>是的。 If we lose control soon it will not be because the machines &#39;took&#39; control from us, rather it will be because we gave it to them. Which we are absolutely going to do the moment it is economically or otherwise advantageous for us to do so. Those who refuse risk being left behind. We need a plan if we do not want that to be the equilibrium.</p><blockquote><p> Smaller nations seemed less fearful and more optimistic about AI as a force multiplier for their populace.</p><p> I learned that predicting and mitigating fear of AI in populations is actually an important civil society issue—too much fear/anger in a population can be dangerous.</p><p> People analogized to regulation of other industries (automobile, aerospace, nuclear power— <a target="_blank" rel="noreferrer noopener" href="https://imbue.com/perspectives/common-good/">brief history of automobile regulation</a> ).</p><p> AI is somewhat different because new model versions can sprout unexpected new capabilities; still, much can be learned.</p></blockquote><p> Yes, unexpected new capabilities or actions, or rather intelligence, is what makes this time different.</p><blockquote><p> Only women talked for the first 45 min in my first session, one after another! I was surprised and amazed.</p><p> These systems reflect, and currently reinforce, the values of our society.</p><p> With such a stark mirror, there&#39;s perhaps an opportunity for more systematic feedback loops to understand, reflect on, and shift our values as a society.</p></blockquote><h4>结束语</h4><p>As a civilian, it is difficult to interpret diplomacy and things like summits, especially from a distance. What is cheap talk? What is real progress? Does any of it mean anything? Perhaps we will look back on this in five years as a watershed moment. Perhaps we will look back on this and say nothing important happened that day. Neither would surprise me, nor would something in between.</p><p> For now, it seems like some progress was made. One more unit down. Many to go.</p><br/><br/><a href="https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/zbrvXGu264u3p8otD/on-the-uk-summit<guid ispermalink="false"> zbrvXGu264u3p8otD</guid><dc:creator><![CDATA[Zvi]]></dc:creator><pubDate> Tue, 07 Nov 2023 13:10:14 GMT</pubDate> </item><item><title><![CDATA[Box inversion revisited]]></title><description><![CDATA[Published on November 7, 2023 11:09 AM GMT<br/><br/><p> <a href="https://www.lesswrong.com/posts/TQwXPHfyyQwr22NMh/box-inversion-hypothesis/">Box inversion hypothesis</a> is a proposed correspondence between problems with AI systems studied in approaches like <a href="https://www.lesswrong.com/tag/agent-foundations">agent foundations</a> , and problems with AI ecosystems, studied in various views on AI safety expecting multipolar, complex worlds, like <a href="https://www.lesswrong.com/tag/ai-services-cais">CAIS.</a> This is an updated and improved introduction to the idea.</p><h2> Cartoon explanation </h2><p><img style="width:73.02%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/i7uyj3fcdi4eftafit5k"></p><p><br> In the classic -&quot;superintelligence in a box&quot; - picture, we worry about an increasingly powerful AGI, which we imagine as contained in a box. Metaphorically, we worry that the box will, at some point, just blow up in our faces. Classic arguments about AGI then proceed by showing it is really hard to build AGI-proof boxes, and that really strong optimization power is dangerous by default. While the basic view was largely conceived by Eliezer Yudkowsky and Nick Bostrom, it is still the view most technical AI safety is built on, including current agendas like mechanistic interpretability and evals. <br></p><p><img style="width:72.54%" src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/dkwc8m7qjkoqjdvkqlz6"></p><p><br> In the less famous, though also classic, picture, we worry about an increasingly powerful ecosystem of AI services, automated corporations, etc. Metaphorically, we worry about the ever-increasing optimization pressure &quot;out there&quot;, gradually marginalizing people, and ultimately crushing us. Classical treatments of this picture are less famous, but include Eric Drexler&#39;s CAIS ( <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf">Comprehensive AI Services</a> ) and Scott Alexander&#39;s <a href="https://slatestarcodex.com/2016/05/30/ascended-economy/">Ascended Economy</a> . We can imagine scenarios like the human-incomprehensible economy expanding in the universe, and humans and our values being protected by some sort of &quot;box&quot;. Agendas based on this view include <a href="https://ai.objectives.institute/whitepaper">the work of the AI Objectives Institute</a> and part of ACS work.</p><p> The apparent disagreement between these views was sometimes seen as a crux for various AI safety initiatives.</p><p> &quot;Box inversion hypothesis&quot; claims:</p><ol><li> The two pictures to a large degree depict the same or a very similar situation,</li><li> Are related by a transformation which &quot;turns the box inside out&quot;, similarly to a geometrical transformation of a plane known as a circle inversion,</li><li> and: this metaphor is surprisingly deep and can point to hard parts of some problems.</li></ol><h2> Geometrical metaphor </h2><figure class="image image_resized" style="width:70.31%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/jrKftFZMZjvNdQLNR/yz9rihz8btdmr97zgicq" alt="File:Inversión Círculos.png - Wikimedia Commons"><figcaption> Inverted circles. <a href="https://commons.wikimedia.org/wiki/File:Inversi%C3%B3n_C%C3%ADrculos.png">From Wikimedia Commons, CC-SA</a></figcaption></figure><p> &quot; <a href="https://artofproblemsolving.com/wiki/index.php/Circular_Inversion">Circular inversion</a> &quot; transformation does not imply the original and the inverted objects are the same, or are located at the same places. What it does imply is that some relations between objects are preserved: for example, if some objects intersect, in the circle-inverted view, they will still intersect.</p><p> Similarly for &quot;box inversion&quot; : the hypothesis does not claim that the AI safety problems in both views are identical, but it does claim that, for most problems, there is a corresponding problem described by the other perspective. Also, while the box-inverted problems may at a surface level look very different, and be located in different places, there will be some deep similarity between the two corresponding problems.</p><p> In other words, the box inversion hypothesis suggests that there is a kind of &#39;mirror image&#39; or &#39;duality&#39; between two sets of AI safety problems. One set comes from the &quot;Agent Foundations&quot; type of perspective, and the other set comes from the &quot;Ecosystems of AIs&quot; type of perspective.</p><h2> Box-inverted problems</h2><h3> Problems with ontologies and regulatory frameworks</h3><p> <span class="footnote-reference" role="doc-noteref" id="fnrefa22fcwq3tx8"><sup><a href="#fna22fcwq3tx8">[1]</a></sup></span><br> In the classic agent foundations-esque picture, a nontrivial fraction of AI safety challenges are related to issues of similarity, identification, and development of ontologies.<br><br> Roughly speaking</p><ul><li> If the AI is using utterly non-human concepts and world models, it becomes much more difficult to steer and control</li><li> If &quot;what humans want&quot; is expressed in human concepts, and the concepts don&#39;t extend to novel situations or contexts, then it is unclear how the AI should extend or interpret the human “wants”</li><li> Even if an AI <i>initially</i> uses an ontology that&#39;s compatible with human thinking and concepts, there&#39;s a risk. As the AI becomes more intelligent, the framework based on that ontology might break down, and this could <a href="https://www.lesswrong.com/tag/ontological-crisis">cause the AI to behave in unintended ways.</a> Consequently, any alignment methods that rely on this ontology might fail too.</li></ul><p> Recently, problems with ontologies and world models have been studied under different keywords, like <a href="https://www.lesswrong.com/tag/natural-abstraction">natural abstractions</a> , or part of <a href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">ELK,</a> or <a href="https://arxiv.org/abs/2310.13018">representational alignment.</a></p><p> Next, we&#39;ll look at Eric Drexler&#39;s CAIS agenda. There, everything is a service, and a particular one is a &quot;service catalogue&quot;, mapping from messy reality to the space of services. Or, in other words,  it maps from “what do you want”, to a type of computation that should be run.<br><br> Safety in the CAIS model is partially built on top of this mapping, where, for example, if you decide to create &quot;a service to destroy the world&quot;, you get arrested.</p><p><br> Problems with service catalogues include</p><ul><li> If over time an increasingly large fraction of services becomes gradually incomprehensible B2Bs that produce non-human outputs from non-human inputs, it becomes  tricky to regulate.</li><li> if your safety approach is built on the ontology implicit in the service catalogue, the system may be vulnerable to attacks stemming from ontological mismatches (as we discussed above).</li></ul><p> How does this look in practice, at 2023 capability levels?</p><p> As an example, governments are struggling to draft regulations which would actually work, in part because of ontology mismatch. The EU spent a few years building the AI act based on an ontology to track which <i>applications</i> of AI are dangerous. After ChatGPT, it became very obvious the ontology is mismatched to the problem: abilities of LLMs seem to scale with training run size. And while the simple objective &quot;predict the next token&quot; seems harmless, it is sufficient for the models to gain dangerous capabilities in domains like synthetic biology or human persuasion.</p><p> For a different type of example, consider a service offering designs of <i>ferrofluidic vacuum rotary feedthroughs</i> . If you want to prevent, let&#39;s say, AGI development by a rogue nation state, is this something you should track and pay attention to?</p><h3> Problems with demons, problems with …?</h3><p> Before the mesa-optimizer frame got so much traction that it drowned other ways of looking at things in this space, people in the agent foundations and superintelligence in a box space were worried about <a href="https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search"><u>optimization demons</u></a> . Broadly speaking, you have an imperfect search, a mechanism which allows exploiting the imperfection, and - in a rich enough space - you run into a feedback loop that exploits the inefficiency. A whole new optimizer appears - with a different goal.<br><br> Classically, the idea was that this can happen inside the AI system, manipulating its training via gradient hacking. Personally I don&#39;t think this is very likely with systems like LLMs, but in contrast I do think &quot;manipulating the training data&quot; is technically easier and in fact likely once you get close feedback loops between AI actions and training data.</p><p> What does the box-inverted version look like?</p><p><br> <i>(Before proceeding, you might want to consider guessing yourself)</i><br></p><p> The <a href="https://www.lesswrong.com/tag/moloch">LessWrong explainer</a> gives an example of a Molochian dynamic: a Red Queen race between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they&#39;ve all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.</p><p> In other words, squinting a bit, this looks like we have some imperfect search process (allocating grants to promising research proposals), a mechanism which allows ways  to exploit it … and an eventual feedback loop that exploits the inefficiency. Problems with demons invert to problem with molochs.<br><br> What would this look like on an even bigger scale? In an idealised capitalism, what is produced, how much of it is produced, and at what price is ultimately driven by aggregate human demand, which contains the data about individual human preferences. Various supply chains bottom down in humans wanting goods, even if individual companies are often providing some intermediate goods to be used by some other companies. The market continuously &quot;learns&quot; the preferences of consumers, and the market economy updates what it produces based on those preferences.</p><p> The ultimate failure of this looks like the &quot;web of companies&quot; story in <a href="https://arxiv.org/abs/2306.06924">TASRA</a> report by Critch and Russell.</p><h3> What else?</h3><p> The description and the examples seem sufficient for GPT4 to roughly understand the pattern, and come up with new examples like: <span class="footnote-reference" role="doc-noteref" id="fnrefzh5q0axjocl"><sup><a href="#fnzh5q0axjocl">[2]</a></sup></span></p><ul><li> <i>Superintelligent System: The AI might prioritize its self-preservation over other objectives, leading it to resist shutdown or modification attempts.</i></li><li> <i>Box-Inverted (Ecosystem of AIs): Some AI systems, when interacting within the ecosystem, might inadvertently create feedback loops that make the ecosystem resistant to changes or updates, even if individual systems don&#39;t have self-preservation tendencies.</i></li></ul><p> …and so on. Instead of pasting GPT completions, I&#39;d recommend looking at a few other things which people were worried about in agent foundations.</p><p> In the original post, I tried to gesture at what seems like the box-inverted &#39;hard core&#39; of safety in this hilariously inadequate way:<br></p><blockquote><p> some “hard core” of safety (tiling, human-compatibility, some notions of corrigibility) &lt;->; defensive stability, layer of security services<br></p></blockquote><p> I&#39;ll try to do better this time. In agent foundations, what seems one of the hard, core problems is what Nate, Eliezer and others refer to as &#39; <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">corrigibility being anti-natural&#39;</a> . To briefly paraphrase, there are many aspects of future AI systems which you can expect because those aspects seem highly <a href="https://www.lesswrong.com/posts/sam4ehxHgnJEGCKed/lessons-from-convergent-evolution-for-ai-alignment">convergent</a> : having a world model, using abstractions, understanding arithmetic, updating some beliefs about states of the world using approximate Bayesian calculations, doing planning, doing some form of meta-cognition, and so on. What&#39;s not on the list is <i>&#39;doing what humans want&#39; --</i> because, unlike the other cases, there isn&#39;t any extremely <a href="https://www.lesswrong.com/posts/sam4ehxHgnJEGCKed/lessons-from-convergent-evolution-for-ai-alignment">broad selection pressure</a> for <i>&#39;being nice to humans&#39;</i> . If we want AIs to be nice to humans, we need to select for that, and we also need to set it up in a way where it scales with AI power. Most of the hope in this space comes from the possibility of a <a href="https://www.lesswrong.com/posts/AqsjZwxHNqH64C2b6/let-s-see-you-write-that-corrigibility-tag?commentId=8kPhqBc69HtmZj6XR">&#39;corrigibility basin&#39;</a> , where first corrigible AI systems make sure their successors are also corrigible.You&#39;d also need to guarantee that this type of human-oriented computation is not overpowered, erased, or misled by the internal dynamics of the AGI system. And, you must guarantee that the human-oriented computation does not solve alignment by just hacking humans to align to whatever is happening.<br></p><p> What&#39;s the box inverted version? In my view of Eric Drexler&#39;s CAIS, the counterpart problem is <i>&quot;how to set security services&quot;</i> . Because of the theoretical clarity of CAIS, it&#39;s maybe worth describing the problem in that frame first. Security services in CAIS guarantee multiple things, including &quot;no one creates the service to destroy the world&quot;, &quot;security services are strong enough that they can&#39;t be subverted or overpowered&quot; and &quot;security services guarantee the safety of humans&quot;. If you remember the cartoon explanation, security services need to be able to guarantee the safety of the box with humans in presence of powerful optimization outside. This seems really non-trivial to set up in a way that is dynamically stable, and where the security services don&#39;t fall into one of the bad attractors. The obvious bad attractors are: 1. security services are overpowered, humans are crushed or driven to complete irrelevance 2. security services form a totalitarian dictatorship, where humans lose freedom and are forced or manipulated to do some sort of approval dances 3. security services evolve to a highly non-human form, where whatever is going on is completely incomprehensible.</p><p><br> Current reality is way more messy, but you can already recognize people intuitively fear some of these outcomes. Extrapolation of calls for treaties, international regulatory bodies, and government involvement is &#39;we need security services to protect humans&#39;. A steelman of some of the &#39;we need freely distributed AIs to avoid concentration of power&#39; claims is &#39;we fear the dictatorship failure mode&#39;.  A steelman of some of the anti-tech voices in AI ethics is &#39;capitalism without institutions is misaligned by default&#39;.</p><p> In a similar way to corrigibility being unnatural in the long run, the economy serving humans seems unnatural in the long run. Currently, we are relatively powerful in comparison to AIs, which makes it easy to select for what we want. Currently, <a href="https://www.oecd.org/g20/topics/employment-and-social-policy/The-Labour-Share-in-G20-Economies.pdf">the labour share of GDP</a> in developed countries is about 60%, implying that the economy is reasonably aligned with humans by our sheer economic power. What if this drops hundred-fold?</p><h3>这里发生了什么？</h3><p> I don&#39;t have a satisfying formalisation of box inversion, but some hand-wavy intuition is this: look at the Markov blankets around the AGI in a box, around the &#39;ecosystem of AIs&#39;, and around &#39;humanity&#39;. Ultimately, the exact structure inside the blanket might not be all that important.<br><br> Also: as humans, we have strong intuitions about individuality of cognitive systems. These are mostly based on experience with humans. Based on that experience, people mostly think about a situation with &#39;many AI systems&#39; as very different from a situation with a single powerful system. Yet, the notion of &#39;individual system&#39; based on &#39;individual human&#39; does not seem to actually generalise to AI systems. <span class="footnote-reference" role="doc-noteref" id="fnreffawa19c0dw"><sup><a href="#fnfawa19c0dw">[3]</a></sup></span></p><h3><br> What does this imply</h3><p> My current guess in Oct 2023 is that the majority of the unmitigated AI existential risk comes from the box-inverted, ecosystem versions of the problems. While I&#39;m fairly optimistic we can get a roughly human-level AI system almost aligned with the company developing it using currently known techniques, I&#39;m nevertheless quite worried about the long run.<br><br> <i>Thanks to Tomáš Gavenčiak, Mateusz Bagiński, Walter Laurito, Peter Hozák and others for comments and Rio Popper for help with editing. I also used DALL-E for the images and GPT-4 for editing and simulating readers.</i> </p><p></p><p></p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fna22fcwq3tx8"> <span class="footnote-back-link"><sup><strong><a href="#fnrefa22fcwq3tx8">^</a></strong></sup></span><div class="footnote-content"><p> In the original post, I referred to this merely as &quot; <i>questions about ontologies &lt;->; questions about service catalogues&quot;,</i> but since writing the original post, I&#39;ve learned that this density of writing makes the text incomprehensible to almost anyone.<br><br> So let&#39;s unpack it a bit.</p></div></li><li class="footnote-item" role="doc-endnote" id="fnzh5q0axjocl"> <span class="footnote-back-link"><sup><strong><a href="#fnrefzh5q0axjocl">^</a></strong></sup></span><div class="footnote-content"><p> Cherry-picked from about 10 examples</p></div></li><li class="footnote-item" role="doc-endnote" id="fnfawa19c0dw"> <span class="footnote-back-link"><sup><strong><a href="#fnreffawa19c0dw">^</a></strong></sup></span><div class="footnote-content"><p> It actually does not generalise to a lot of living things like bacteria or plants either.</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/jrKftFZMZjvNdQLNR/box-inversion-revisited#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/jrKftFZMZjvNdQLNR/box-inversion-revisited<guid ispermalink="false"> jrKftFZMZjvNdQLNR</guid><dc:creator><![CDATA[Jan_Kulveit]]></dc:creator><pubDate> Tue, 07 Nov 2023 11:09:37 GMT</pubDate> </item><item><title><![CDATA[AI Alignment Research Engineer Accelerator (ARENA): call for applicants]]></title><description><![CDATA[Published on November 7, 2023 9:43 AM GMT<br/><br/><br/><br/> <a href="https://www.lesswrong.com/posts/465MTELvNKMdQk322/ai-alignment-research-engineer-accelerator-arena-call-for-1#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/465MTELvNKMdQk322/ai-alignment-research-engineer-accelerator-arena-call-for-1<guid ispermalink="false"> 465MTELvNKMdQk322</guid><dc:creator><![CDATA[TheMcDouglas]]></dc:creator><pubDate> Tue, 07 Nov 2023 09:43:42 GMT</pubDate> </item><item><title><![CDATA[The Perils of Professionalism]]></title><description><![CDATA[Published on November 7, 2023 12:07 AM GMT<br/><br/><p>专业精神是一种可以展现的有用品质，但它并不是纯粹的优点。本文试图概述为什么故意不表现出专业精神可能对你有用。</p><p>首先，通过示例对专业精神进行定义：干净的系扣衬衫，搭配纯色领带，西装外套或西装外套，胡须剃干净，头发扎成发髻，没有传单，米色或灰色或至少是纯色的汽车、桌子和墙壁，甚至是带有一丝情感的语调声音，听起来一点也不机械或缺乏同理心。</p><p>这不是职业管理阶层，但他们（特别是帕特里克·麦肯齐有时描述的那样）往往是职业管理阶层的典范。</p><h2>我。</h2><p> “专业人士”一词被定义为从事特定活动作为主要有偿职业的人。它与“业余爱好者”形成鲜明对比，“业余爱好者”是指无偿从事某项活动的人。值得注意的是，“业余”也可以指在某项特定活动中无能力的人。从语言的角度来看，我们将<i>技能</i>和<i>报酬</i>混为一谈，并且我们是双向的。</p><p>如果你想通过做某事获得报酬，你想学习专业地做这件事。专业地做某事通常包括相邻但不明显同义的技能。其中一些非常接近；我曾经是一名专业软件工程师，也参与过招聘专业软件工程师的工作，如果您作为软件工程师不知道如何使用源代码控制，那么您想学习使用源代码控制。是的，我知道这不是一个很酷的新算法。是的，我知道最终用户永远不会看到它。相信我，你会用到它。</p><p>专业人士的一些预期技能不是关于工作的核心技能，而是更多关于工作的框架。 “准时”、“着装得体”和“举止得体”都经常被作为适用于广泛领域的专业技能的例子。坦率地说，如果你要与顾客互动，尤其是在白领工作中，最好不要有面部纹身，也不要随意说脏话。</p><p>我们似乎很快就陷入了一些看似与你完成手头实际工作的能力几乎没有关系的事情！尽管如此，我希望西方世界的几乎每一位职业教练都能支持我的要点。我第一次成功地用金钱换取软件是在我<i>十三</i>岁左右的时候，虽然在这期间我在编写软件方面取得了更好的成绩，但我在展示自己作为一名专业软件工程师的能力方面也有了更大的进步。</p><h2>二.</h2><p>让我们谈谈我的第一个专业软件工程项目。</p><p> （在这里，我使用“专业”来表示“我为此获得了报酬。”正如您即将发现的那样，它在几乎所有其他意义上都是不专业的。）</p><p>据我所知，这份工作是这样的。我母亲的一个朋友听说我“电脑很好”，就问我是否知道如何建立一个网站。事实上，我最近成功地运行了我自己的 Apache 服务器。她说她的组织需要一个网站，在那里他们可以宣布他们的活动，人们可以在那里了解该组织，我是否愿意花费相当于几个月零用钱的钱来建立这个网站。我说当然可以，并问了她一系列关于网站上需要包含什么内容的问题。一周后，当我揭晓它时，她听起来很高兴，对文本做了一些更正，我向她展示了如何添加新事件。</p><p>如果您至少不是一点网络开发人员，那么下一段描述该网站的内容将是纯粹的行话。如果没有意义，请跳过它并了解这是一个非常糟糕的网站，我唯一真正的辩护是她得到了她所付出的代价。</p><p>我用纯 HTML 和 CSS 编写了它。事件页面是一个 HTML 文件，您可以通过打开 HTML 并复制最后一个事件块，然后将其更改为适合的方式来添加新事件。您必须更改发布日期，因为那只是带有一些特殊样式的文本。 CSS 并没有真正利用继承，我重新定义了每个块的颜色。说到颜色，我读过一本关于网页开发的书，但我还没有读过关于颜色理论的书，所以我直接通过问她“你最喜欢的颜色是什么？”来画颜色。并尝试使用十六进制代码，直到我得到了她回答的大致颜色。没有备份，也没有在不复制整个文件结构的情况下备份它的方法，也没有任何应该进行备份的想法。</p><p>如果您在九十年代末或 00 年代初上网，您一定会看到过一个网站，它看起来很像我付费购买的第一个网站。我继续制作更多这样的网站，我的服务通过口口相传，在接下来的三四年里为我赚了很多披萨和平装本的钱。当我上大学时，我不得不停止为人们制作网站，因为我太忙于学习有关编程的新知识，比如人们为什么使用函数。</p><p>我想我在整个高中期间通过编程和修理坏掉的电脑赚的钱与很多人煎汉堡或打扫电影院赚的钱大致相同。当然，在我上大学之前，我当农场工人赚的钱比我靠电脑赚的钱还要多。我一直在努力为我所在地区的高级企业提供服务——银行、连锁店、大公司，这些公司似乎在与我生活的世界不同的世界中运作。我所缺少的实际上并不是编程能力。</p><p>回顾经验中的智慧，我缺少的是专业精神。我不知道如何在不骄傲的情况下表现出低调的自信，我也没有意识到我糟糕的个人表现对我的机会造成了多大的损害。总而言之，我的问题是寻求计算机专业知识的专业公司不喜欢与骑自行车的大汗淋漓的青少年进行现金交易，而“青少年”部分并不是这句话中的真正症结所在。</p><h2>三．</h2><p>这些确实是关于什么是专业精神以及它对一篇标题为“专业精神的危险”的文章有何帮助的大量文字。是什么赋予了？</p><p>有时添加一点专业化的暗示是简单而有用的。当我第一次接触<a href="https://getbootstrap.com/"><u>Bootstrap</u></a>时，我就被迷住了。这是一种快速、简单的方法，可以使我的网站看起来几乎与我不知道如何向其销售的公司制作的精美公司网站一样好。我没有浪费时间升级到更好的风格。问题只会在以后出现。</p><p>看，专业精神是一揽子交易。如果您看到一个带有尖角和稍微花哨的颜色的网站，那么当您拨打联系页面上的联系电话时，您不会指望有一个流畅且训练有素的声音来接电话。我曾经对那些只在中午营业的地方感到沮丧，因为我下班后无法给他们打电话。现在我意识到这不是一个错误，而是一个功能。有时是因为他们想与其他专业人士打交道。当我让我的网站看起来流畅和优美时，这意味着人们会在中午联系我，经常在课堂上或（稍后）在工作会议上看到我。</p><p>专业精神伴随着其他期望。</p><p>为了参加我最好朋友的单身派对，我们周末租了一间爱彼迎房源。我们熬夜，开大音量玩电子游戏，在后廊做一些烧烤，玩得很开心。它使用了很多与 2019 年和 2022 年东海岸理性主义者大型聚会类似的设置和规划；事实上，如果你以某种方式在我们为单身派对租用的地方外面设置一个地铁站，你就可以使用完全相同的场地来举办这两个活动。对于 2023 年的 Megameetup，我一直在尝试“升级”到更专业的设置。我们预订了酒店大楼，租用了活动空间，我希望能用名牌而不是贴纸和记号笔进行实际登记。这造成了一些期望的不匹配！</p><p>例如，酒店似乎对我在聚会前一个月没有提供完整的客人名单感到有点困惑，我只能试图解释说，是的，事实上我确实希望有一半以上的客人注册最后两周。我必须仔细考虑一下我们将在下午 5 点之后进入会议空间的想法，是的，甚至可能在晚上 7 点之后。 （我仍然不确定我是否理解了这个想法，如果我列出了出错的事情的墨菲术清单，那么“酒店预计我们 5 点离开会议空间”的位置仍然很高。）每当我遇到这些问题时，我都会情不自禁地想，如果我表现得不那么专业，他们是否也会如此困惑。</p><p>当酒店询问这次活动会是什么样时，我将其描述为学术会议和粉丝大会的结合体。这是真的！会发生很多专业的社交活动，我希望会有关于新颖研究的演讲或突破性论文的总结。我还预计至少会有一个人喝得酩酊大醉（好吧，公平地说，我听说在一些学术会议上也会发生这种情况），并且有人会在凌晨一点用原声吉他唱民歌。这不会发生在“专业”活动中！</p><p> （如果此时您认为值得双方同意不使用“事件”一词并描述他们期望发生的事情，那么……您可能是对的！现在您尝试随机选择一个会议酒店销售代表做这种奇怪的互联网事情，你称之为“禁忌你的话。这是可行的！我已经根据经验验证了这一点！但它肯定不是人们期望的那样。）</p><p>早在我第一次开始疯狂追求扩大东海岸大型游戏集会规模时，一位朋友漫不经心地建议去酒店而不是爱彼迎风格的场所，好像这很容易，他经常这样做。他刚刚联系了他们中的一些人，并表示这“更多的是出于好奇而不是任何事情”。与此同时，我不得不查找他用于启动该过程的缩写词。</p><p>或者看看我最近与一个我想从他那里购买电吉他的人的另一次对话。在谈话过程中，我提到我做了一些活动协调，他说他在该地区已经这样做了很多年，并问我是否会做他听说过的任何事情。我说社区中较大的活动有一百多人参加，本赛季我举办了其中两次。他呃，挠着下巴，说了些什么，大意是他不知道哪个场馆会在演奏一组非常甜美的强力和弦时为这么小的事情而烦恼。</p><p>这两个人都在各自的领域中表现出了一种随意、舒适的能力。他们见过事情出错，记住人们可能犯的明显错误，并且不需要强调小事情。这是我可以在软件中做的事情，也是我可以在其他一些领域做的事情，但在组织活动时我还不太习惯。我认为，在正确的时间和地点表明专业精神是有用的，因为你可以利用这一点。飞机飞行员基本上似乎只会讲同样的几个老套的、预制的笑话。我对此表示赞同。如果我登上飞机发现飞行员穿着小丑的杂色和猫耳朵，我会变得有点紧张，这并不是完全未经认可的，尽管他们的着装并没有改变他们的实际驾驶技能。</p><h2>四．</h2><p>如果你表现得很专业（在举止、审美、说话和外表等软技能上），人们会期望你达到专业标准（在你的专业水平上、在应对意外情况方面、在了解事情如何发生方面）。流程应该进行。）您可能不了解这些标准，这种无知可能很重要。专业精神的外表掩盖了你不知道自己在做什么的事实。这也阻碍了人们伸出援手。</p><p>如果我在朋友家吃晚饭，发现他们正忙着切蔬菜，而炉子上的东西开始冒烟，我通常会介入并翻转它或自己把火关小。这可以变成舒适地并肩工作，如果我足够了解他们，可以猜测他们的食物偏好或厨房的布局，我可能会根据口味给饭菜加香料，或者我们可能会一起即兴创作食谱。如果我在餐厅，我绝不会回到厨房并开始麻烦厨师进行更改！一般来说，即使我开始闻到一点烟味，我也会认为这是故意的，他们知道自己在做什么并且正在注意。事实可能并非如此！</p><p>流畅、精心设计的演示可能会阻止人们为您提供帮助。如果您希望业余爱好者不参与，这可能很好（外科手术室里没有临时志愿者的地方），但如果您希望人们加入，这尤其糟糕。卡拉 OK 酒吧或单口喜剧俱乐部有充分的理由不要显得太专业。我认为，开源项目的愚蠢名称在使其看起来更容易被接受方面做了一些很好的工作。还有理性聚会。 。 。</p><h2>五、</h2><p>大多数聚会在很大程度上取决于与会者的参与、支持、解决问题以及提供大部分内容和对话。我对东海岸理性主义者大型聚会的主要认识是，我不负责提供聚会的内容；我负责提供聚会的内容。我安排了场地，与会者一起创作了内容。我在 LessWrong 社区周末、Vibecamp 以及几乎所有其他大型理性主义活动中都看到过这种模式。</p><p>如果你正在组织类似的活动，并且你不小心发出了这样的信号，即某人必须如此团结、理性且出名才能做出贡献，那么你将会遇到困难。不仅你在闪电演讲中的发言者会减少，而且你会无意中切断了人们为举办自己的活动而攀登的阶梯的底部。你可以鼓励那些处于边缘的人挺身而出并加入进来，方法是在演讲中稍微不那么专业，稍微愚蠢一点和随意一点。</p><p>我的模糊猜测是，能够说自己举办专业级理性主义活动的人不到一百人。想必 CFAR 的一些人已经在这方面积累了经验，CEA 的一些人可能也算数，然后还有一大群已经从事该领域一段时间的运维人员，或者他们的非理性主义职业生涯为他们做好了异常良好的准备。它们很罕见，而且《LessWrong》的许多读者不会遇到它们，就像大多数听音乐的人不会面对面见到摇滚明星一样。</p><p> （这对于网站来说是完全不同的，我基本上会相信来自任何当地社区需要网络开发人员的最有信心的志愿者。我们绝对拥有这种技能。）</p><p>这篇文章的灵感部分来自于 Elizabeth 发表的关于各种 EA 组织的法律结构的<a href="https://www.lesswrong.com/posts/XvEJydHAHk6hjWQr5/ea-orgs-legal-structure-inhibits-risk-taking-and-information"><u>文章</u></a>。如果我们拥有丰富的法律和官僚技能，事情就不会是这样的。当你实际查看组织结构图时，它看起来很混乱且不专业。如果没有经验丰富的顾问的帮助，也许早熟的青少年在第一次解决问题时会做出这样的事情。<i>也许这就是实际发生的事情。</i>顺便说一句，我并不是想用“青少年”作为贬义词：特立独行、早熟的青少年很棒，当我上高中时，我经常遇到工作能力比我差的成年成年人。实际上这就是重点。</p><p>请注意，仅仅因为有人拿钱去做一件事，并且他们发出了所有正确的能力和轻松的信号，他们可能仍然不擅长做这件事。推论，如果你在这件事上表现不佳，就要小心你是否发出了能力和轻松的信号。</p><p> （哦，如果您在 2023 年 11 月或 12 月初阅读本文，东海岸理性主义者大型聚会将于 12 月 9 日周末在纽约市举行。如果那个或纽约市世俗至日听起来像您喜欢的事情，考虑<a href="https://rationalistmegameetup.com/"><u>加入我们</u></a>！）</p><br/><br/> <a href="https://www.lesswrong.com/posts/QChwTjL6oL2a6gbhm/the-perils-of-professionalism#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/QChwTjL6oL2a6gbhm/the-perils-of-professionalism<guid ispermalink="false"> QChwTjL6oL2a6gbhm</guid><dc:creator><![CDATA[Screwtape]]></dc:creator><pubDate> Tue, 07 Nov 2023 00:07:33 GMT</pubDate> </item><item><title><![CDATA[How to (hopefully ethically) make money off of AGI]]></title><description><![CDATA[Published on November 6, 2023 11:35 PM GMT<br/><br/><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:14:09 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:14:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>嘿大家！</p><p>作为过去几周对话工作的一部分，我问了很多人他们最有兴趣阅读什么样的对话，最常见的一个是“我真的很想读一堆人们试图弄清楚如何构建一个在 AGI 变得更重要时能够顺利进行的投资组合”。</p><p>你们三个人在我的名单上排在相当靠前的位置，可以一起解决这个问题，所以我们就到这里了。不是因为你们是这方面的世界专家，而是因为我非常相信你们的一般推理（我不太了解诺亚，但非常信任威尔和兹维）。</p><p>我想，为了让我们开始，也许让我们先用 1-2 句话来介绍一下您的背景以及您之前对这件事的思考程度（以及您是否自己构建了相关的投资组合）。</p><p><i>另外，需要明确的是，本文中的任何内容均不构成投资建议或法律建议。</i> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:15:23 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:15:23 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>感谢奥利弗邀请参加！我的名字叫威尔·伊登（Will Eden），我的背景是经济、金融和生物技术，最近我一直是一名早期生物技术风险投资家，尽管我也努力思考如何管理广泛的公共资产组合。<br><br>如果我有一个粗略的论文/开场白，那么提前知道大多数资产将如何表现/链的哪些部分将积累大量价值可能是极其困难的。另一方面，我认为我们有一个可取之处，因为整体经济很可能会产生大量价值，甚至一些非常普遍和广泛类型的投资组合也可能会积累相当大的价值，即使不是最大可能的价值我们当然希望实现。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:16:44 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:16:44 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>我叫兹维·莫肖维茨。我写了很多关于经济学的文章，也思考了很多，并且在 Jane Street Capital 工作了 2.5 年。我花了很多时间思考这些问题，但我也有意尽力避免在投资组合优化或交易上花费太多时间，这样它就不会占据我的整个大脑。</p><p>我还写了一篇关于这一点的文章，名为<a href="https://thezvi.substack.com/p/on-ai-and-interest-rates">“论人工智能和利率”</a> 。</p><p> （此外，我知道，我可以预先声明，我在这里所说的一切都不是投资建议或任何其他建议！）</p><p>在这种情况下，我的一般方法是，如果你的论文是随机选择的，你想要寻找不会太昂贵（以 EV 术语计算）的投资/赌博，或者无论如何都是好主意，但是如果你的论文被证明是正确的。支付保费有时间和地点，但必须谨慎选择。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:18:50 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:18:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>谢谢你的邀请！我很荣幸来到这里。我叫 Noah Kreutter，我的背景是拥有大约 4 年的量化金融经验，包括<a href="https://www.imc.com/us/">IMC</a>和（即将成立的）Bridgewater。我混合了波动性交易和系统性多资产宏观分析。至少从 2015 年起，我就一直在关注 LW/Rationalist/SSC 生态系统，但主要是在外围。我所说的都不是财务建议，包括任何听起来像财务建议的内容。<br><br>我的基本观点是，在 AGI 缓慢起飞的过程中——事实上，在缓慢的起飞中，你的投资方式可能很重要——你想要构建一个长期增长的投资组合，尤其是那些对人工智能有特殊暴露、长期波动、长期利率的股票-going-up（做空债券），做多“2023年便宜的房地产”。您可能希望避免购买由强大的知识型劳动力市场（例如纽约市）支持的房地产。<br><br>此外，对于大多数读者来说，我认为职业资本是他们最重要的资产。 AGI 的一个后果是贴现率应该很高，而且你不一定能拥有很长的职业生涯。因此，那些处于读研究生的边缘的人绝对应该避免它。<br><br>我目前的投资组合是单一名称股票、指数的长期看涨期权、一些特定股票（例如 MSFT）的长期看涨期权以及短期长期债券的组合。我也持有大量现金。如果我能在美国一些不那么热闹的地区轻松获得廉价抵押贷款，我可能会这么做，但从逻辑上讲，这很烦人。</p><p>我要提供的另一条一般性建议——这与“持有现金”和“通过期权获得一些股权贝塔”相吻合——是“保留选择性”。未来十年，广义上的灵活性的价值可能会很高。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:22:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:22:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>“通过期权获得一些股权贝塔值”</p></blockquote><p>啊，是的，我也喜欢通过期权获得一些股权测试版：P</p><p>我想我也许可以通过谷歌搜索来解读这意味着什么，但是你能详细说明一下吗？ </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:36:23 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:36:23 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我所说的“通过看涨期权获得股票贝塔值”是指使用看涨期权（以预先指定的价格购买股票或指数的权利，但不是义务）作为直接持有股票的部分替代品。这个想法是，看涨期权有一个<i>Delta</i> （某些定价公式相对于标的资产价格的导数），这告诉您在标的资产价格变动 1 美元的情况下，期权的价格应该变化多少。<br><br>对于 Zvi 来说，我不会交易任何基于人工智能论文的短期期权。但一般来说，如果您购买几年后到期的看涨期权，税务问题和交易成本问题就不那么严重，因为这段时间足以获得长期资本利得税待遇。这样做的原因是便宜、无追索权杠杆（你可以用很少的钱获得相当多的股票敞口），以及如果你认为期权市场错误定价了波动性，即低估了股价的反弹幅度，那么你会获得正的预期价值- 我认为是这样。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:22:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:22:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>对于未来的读者，一些尝试解释诺亚对未开悟的凡人所说的话：</p><p><strong>我不会根据人工智能论文来交易短期期权：</strong> “我认为购买只有在不久的将来某些股票价格发生变化时才能支付的金融工具是一个坏主意。大概是因为价格变动的时机非常棘手，甚至股市中的重大变化可能很难把握（请记住，尽管大流行对股价产生了明显的巨大影响，但要把握 2020 年大流行股票走势的时间是很困难的），并且因为这种工具的高波动性意味着风险厌恶者的反击-当事人经常要求您支付高额的额外保费”</p><p><strong>如果您购买几年后到期的看涨期权，那么这段时间足以获得长期资本利得税待遇：</strong> “在美国，当您持有金融工具不到一年时，您要缴纳的税款要高得多（短期资本利得与长期资本利得）。长期资本收益）。这意味着，如果您想（通过期权）押注价格变动，那么押注至少一年内的价格变动是有利的税收优惠。</p><p><strong>这样做的原因是廉价的、无追索权的杠杆（你可以用很少的钱获得相当多的股票敞口）：“</strong>如果你押注于长期价格变动而不是仅仅以这种方式持有股票，你可以捕捉到很多上涨空间不会占用大量资金来持有股票（高杠杆），也不会冒着您的不相关资产被清算和占有的风险（无追索权）”</p></div></section><h2> AGI 的广泛市场影响</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:20:28 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:20:28 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>基本上在任何通用人工智能情景下，经济都将开始加速并非常迅速地增长。一些资产密切反映了经济的增长率，特别是实际利率，而其他资产则是不完美的替代品，例如公共股票（股票）。值得注意的是，目前几家领先的人工智能公司（例如OpenAI、Anthropic等）由于是私人控股，因此无法被公众投资，因此很难准确、精准地投资人工智能场景。我的观点是，虽然获得完美的曝光是很困难的，而且任何投资组合都将是一个不完美的代理，但快速加速的增长将意味着大量价值在意想不到的地方创造并以不同的方式捕获——例如那些蓬勃发展的投资组合之一私人公司被上市公司收购，您现在可以投资这些公司 - 这样，“投资指数基金”的无聊旧策略仍然可能从 AGI 中获取大部分/大部分价值:) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:21:05 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:21:05 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>至于我目前的投资组合，我有不同的组合，其中包括我期望从人工智能中受益的个股（例如MSFT和GOOG），以及包括其他个股在内的各种其他投资，还有非常优惠的固定利率抵押贷款 -这是一个交易的例子，无论如何，交易都很好，但人工智能却做得更好。</p><p>我同意 NoahK 的观点，即保留选择性是一个好主意。您应该将流动性不足视为比平常花费更多的溢价。</p><p>总的来说，我认为“构建完美的投资组合”是不值得的，除非你重视智力锻炼。没有足够的阿尔法来使其完全正确，并且在考虑再平衡等问题时，税收考虑通常占主导地位。你要确保自己的方向是正确的并表达你的意见，但不要太疯狂。</p><p>我强烈同意 Will 的观点，加速 AGI 将在不同的地方创造大量价值，因此广泛的生产性资产或至少其中一部分可能会升值，因此可以合理地预测 SPY (S&amp;P 500 ETF)会做得很好。人们担心的一个问题是，利率上升对股价影响不大，因此您需要考虑是否覆盖这一基础。</p><p>对于期权之类的东西，您需要支付溢价，因为您在交易它们时必须跨越更大的价差，担心市场结构中的各种边缘情况，然后面临税收影响。在某些重点领域（想想 2020 年 2 月），这显然是正确的举措，但我会犹豫是否将它们用于人工智能，除非你预计事情会很快升级。</p></div></section><h2> AGI 世界中的职业资本</h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:47:07 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:47:07 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>此外，对于大多数读者来说，我认为职业资本是他们最重要的资产。</p></blockquote><p>诺亚提出的这一点值得考虑，尽管我认为我们需要诚实地了解我们拥有哪些技能/我们可以发展哪些技能。在一个完整的通用人工智能场景中，人工智能在所有技能上都超越了人类，即使是最好的程序员等似乎也不太可能获得可观的回报。在那段时期之前，仍然存在一个悬而未决的问题：哪些工作岗位到底以什么顺序被取代。</p><p>例如，当艺术首先实现自动化时，每个人似乎都感到非常惊讶 - 人们总是认为创造性任务将是最后完成的任务！ （如果我们对人类创造的艺术给予巨大的溢价，这仍然可能是正确的。）似乎合理的是，任何直接致力于改进人工智能的人仍然可以在多年内赚取巨额溢价，因此依靠这种职业/人力资本似乎是合理的一个好的策略。但如果你期望 1) AGI 很快到来，2) 许多/大多数工作被取代，那么我认为人们不应该假设他们在未来一段时间内能够赚取任何收入。 （其社会影响是巨大的，预计 AGI 征税 + 全民基本收入、大规模慈善事业等等 - 但要点是成立的。） </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:47:09 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:47:09 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>完全同意威尔的观点，即<i>一旦 AGI 出现，大多数人的职业生涯不一定有价值。</i>我认为这是一个争论，试图通过你的职业生涯来赚钱，而不是做那些据称可以建立<s>人类</s>职业资本但需要一段时间才能获得回报的事情。做量化交易而不是咨询，不要去读研究生，尝试尽可能多地预载收益。 （编辑：就 Zvi 的观点而言，人力资本和职业资本是不同的。我实际上只是在这里谈论职业资本 - 更广泛的社会关系和声誉在许多未来可能非常重要）。</p><p>我确实预计 AGI 的<i>过渡期</i>将为那些拥有该技能的人创造大量高价值的创业机会。尚不清楚事物需要多长时间才能达到新的平衡。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:47:15 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:47:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>到目前为止，我还没有看到一个值得考虑的观点，即在哪些世界中，拥有金钱对你来说价值高而不是低？</p><p>极端的情况下，在末日降临、所有人都会死去的世界里，带着最多的玩具死去也还是死了。或者发生政权更迭、革命、没收性税收制度或其他变革，旧资源不再具有意义。或者，如果我们以某种方式进入后稀缺的乌托邦状态，也许你并不需要钱。</p><p>而在其他情况下，在正确的时间将资金放在正确的地方可能会产生巨大的影响 - 我猜这通常正是利率非常高的情况。或者那些没有资本的人会被抛在后面的世界。因此，您希望在财富有价值的世界中进行交易和投资，并在财富不那么有价值时少担心。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:49:26 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:49:26 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>是的，你必须假设事情会变得奇怪，但又不会太奇怪。实际上不可能对冲世界末日或全球革命，因此在对资产定价（或多或少）时，您可以忽略这些世界状态。</p><p>如果您期望自己的行为实际上对世界状况产生有意义的影响，那么情况就不那么正确了。就像如果你认为有一天，如果你有足够的钱，世界末日是可以避免的，那么你也许应该以不同于那些相信自己只是顺水推舟的人的方式进行投资。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:51:13 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:51:13 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>我认为，只要你预计利率会<i>随着经济增长</i>而上升，这并不能真正成为看跌股票的理由。在最坏的情况下，“不会像你所获得的增长预期那样乐观”。 </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:52:00 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:52:00 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>职业资本是人力资本或社会资本的一种形式。从广义上讲，此类资产确实占大多数人投资组合的很大一部分。我宁愿“富有”，因为拥有自己的声誉、人脉、家庭和技能，而不是“富有”，因为拥有自己的投资组合，无论从何种意义上说。这是不要过分关注具体的投资组合配置本身，而更多地担心建立正确的人力和社会资本的原因之一。</p><p>是的，如果您认为转型的时间相对较短，那么也应该考虑对有价值的内容的影响。我当然不会计划任何诸如“终身教职”或其他多年都没有回报的长期计划。</p><p> Especially I would warn against looking for the illusion of security or normality - the idea that you are set if everything stays normal should not bring too much comfort. But on the flip side, if things stay more normal for longer than you expect, you don&#39;t want to mean you&#39;re screwed. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:54:26 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:54:26 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>到目前为止，我还没有看到一个值得考虑的观点，即在哪些世界中，拥有金钱对你来说价值高而不是低？</p></blockquote><p> I agree very much with this point, and I see a couple scenarios where your wealth endowment matters. One world is where there are clear and urgent ways to spend money to directly improve outcomes, eg a huge spend on AI safety or whatnot. I suspect that&#39;s why most EAs are considering this question.</p><p> The other is a world more like a slow takeoff Age of Ems type scenario, where there isn&#39;t total obsolescence of all forms of humanity, and maybe there&#39;s a minimal social safety net because it would be very cheap at that point, but if you want any meaningful large impact on humanity&#39;s future it absolutely depends on what resources you can muster to eg gain compute power to run more copies of yourself. I definitely put non-zero odds of this being a potential future! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:54:43 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:54:43 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><blockquote><p>The other is a world more like a slow takeoff Age of Ems type scenario, where there isn&#39;t total obsolescence of all forms of humanity, and maybe there&#39;s a minimal social safety net because it would be very cheap at that point, but if you want any meaningful large impact on humanity&#39;s future it absolutely depends on what resources you can muster to eg gain compute power to run more copies of yourself.</p></blockquote><p> For whatever it&#39;s worth, this is the strange future I find most personally plausible and certainly most worth thinking about/hedging. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:55:01 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:55:01 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><blockquote><p>Ultimately I still expect growth to be so large that will outweigh any drag on stocks due to rates.</p></blockquote><p> Right, I do as well in these scenarios, but I wanted to point out that if a given stock/sector/etc &#39;misses out&#39; on the explosive growth, then their value rather than staying static could drop quite a bit.</p><p> Also I have to assume that a lot of businesses are going to get disrupted, and hard. There will be losers. A lot of why I like individual stocks when investing domestically is that while I don&#39;t know if I can reliably pick winners, I do know I can identify losers I want to avoid. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:55:25 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:55:25 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Anyone short <a href="https://www.google.com/finance/quote/CHGG:NYSE?sa=X&amp;ved=2ahUKEwjHrLuCtaGCAxWNGTQIHUDKBPsQ3ecFegQINBAh">Chegg</a> in February? I think there will be a lot of opportunities like that over the next few years at least. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:56:23 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:56:23 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>Its not really possible to hedge either the apocalypse or a global revolution, so you can ignore those states of the worlds when pricing assets (more or less).</p></blockquote><p> Brief disagreement here - it&#39;s true you can&#39;t hedge something like a total AI takeover / x-risk, but you can certainly hedge things like a revolution or catastrophic risks. Personally I think it&#39;s wise to consider manageable downside scenarios and allocate just a few % of your portfolio to robustness, from cheap things like &quot;stockpile food and water&quot; to more expensive things like &quot;physical gold and silver coins somewhere hidden but where you can access them in an emergency&quot; </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:58:34 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:58:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>Agree with Will on hedging some revolutions - you can do it if you want to intentionally do exactly that and pay a premium to do so, and with sufficient wealth that is a highly reasonable play. Alas AI-induced such things are less likely to play nicely with that kind of move, as Altman noted about his bunker. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:59:47 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:59:47 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Yeah I agree you can hedge revolutions historically (to a degree). I&#39;m a bit skeptical that if AI goes badly - or simply ends up being Communist - that storing gold underground will matter much if at all. But I don&#39;t think much hinges on this. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:00:43 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:00:43 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>I wanted to point out that if a given stock/sector/etc &#39;misses out&#39; on the explosive growth, then their value rather than staying static could drop quite a bit.</p></blockquote><p> Yes, and this is why in my very first post I suggested something like &quot;own the index fund&quot; - that will automatically rebalance more funds into winners, and the losers get downweighted until they become zero. If you don&#39;t have a <i>very</i> strong thesis about which entire industries will benefit vs not, it really seems like you shouldn&#39;t bet on anything other than &quot;equities in general&quot;</p><p> I&#39;m also happy to do some speculation here about what <i>might</i> out/underperform. For example, I think it&#39;s a pretty good bet that raw materials and refining will become a huge bottleneck in the immediate term. For the run up into rapid growth, before we get huge new supply coming online (will we? what about regulations??), raw materials seem like they could really grow in value...</p></div></section><h2> Debt and Interest rates effects of AGI </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:32:28 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:32:28 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>Let&#39;s focus a bit on the two broadest categories of assets: debt and equity. Debt is (normally) a fixed, known in advance payment, and the yield of that asset is determined by prevailing interest rates plus uncertainty, etc. Equity is the &quot;residual&quot; claim after debts are paid, it allows for unlimited upside, but also it gets paid after all debts so if the business doesn&#39;t perform well the equity side gets nothing.<br><br> High growth usually means lots of profitable investment opportunities. Debt is most useful when it&#39;s paying for something like a fixed capital investment - you know roughly how much money you need, what you need to build, you can assign the capital as collateral for the loan (so the debt gets repaid if the project fails), you have some pretty good guess about how much this investment will pay off over time. A rapidly growing (and effectively brand new) AGI economy will need lots of such investments, and so there will both be good opportunities to invest, <i>and</i> it means that financial capital will become extremely scarce as limited current resources pursue the best projects. Thus we can reasonably assume that interest rates will grow very large. This increase in rates (yields) means that <i>existing</i> debts will lose a lot of value, so it seems risky to hold a lot of current debt/bonds going into this growth period. It also means that having cash on hand could allow your money to grow at an extremely rapid rate.<br><br> This suggests that insofar as you want a cash/bonds portion of your portfolio, you want to keep it as cash invested in extremely short term securities (money market funds, T-bills, etc)<br><br> (A perhaps less obvious implication is that this could make it a lot harder for <i>governments</i> to finance themselves, as they will also be facing an extremely high prevailing interest rate. Their best bet may be to raise taxes and try to balance the budget, which hopefully should be generating lots of revenue in a growing economy... but if they still are reliant on debt financing this could become very difficult.) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 17:39:03 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 17:39:03 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>I like Will&#39;s point a lot about governments potentially struggling to finance themselves under a much higher interest rate environment. Seems like this - plus widening inequality - could lead to a very large increase in taxes. Possibly this would include previously unheard of things like taxing unrealized gains or a wealth tax. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:41:17 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:41:17 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>人们担心的一个问题是，利率上升对股价影响不大，因此您需要考虑是否覆盖这一基础。</p></blockquote><p> It&#39;s worth considering the channels by which interest rates hurt stock prices. Insofar as there are better fixed investments, it&#39;s tempting to get a known large return than an uncertain return. But if I had to guess, the returns on investments will (at least initially) greatly exceed the prevailing interest rate, such that equities will grow substantially faster than bonds. Most people will see things like bonds paying 50% annually and notice the stock market more than doubling, and pile into stocks. I don&#39;t think the opportunity cost effect will be sufficient to dissuade people here.<br><br> Another channel is the discounting of future cash flows, eg higher interest rates means that future payments are worth less than current payments. That&#39;s true in a strict valuation sense, but again, you&#39;re also needing to price in growth rates in addition to discount rates. Plus in a rapidly growing environment like this, just the very near term payments alone can justify a large valuation, even if you&#39;re discounting something 10+ years into the future to basically 0. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 17:47:15 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 17:47:15 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Will: I think I am failing to properly follow the equities vs bonds tradeoff here. Let me try to summarize what I think you are saying:</p><blockquote><p> When AGI happens, there might be a lot of growth, so people will want to invest a lot, which means people will want high interest rates on any loans they give out, since they want the interest on those loans to at least match the other investments in returns.</p></blockquote><p> This part makes sense to me. Seems like interest rates in this sense will go up.</p><p> But then I think I am failing to understand how this will differentially affect holding stocks vs. holding bonds. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 17:50:07 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 17:50:07 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>But then I think I am failing to understand how this will differentially affect holding stocks vs. holding bonds.</p></blockquote><p> Don&#39;t worry, you are not alone in being unclear how interest rates affect stock prices. It seems like there <i>should be</i> a mechanical relationship but it&#39;s not at all certain. I was responding to Zvi&#39;s point that higher rates might also impact stocks. Ultimately I still expect growth to be so large that will outweigh any drag on stocks due to rates. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 17:42:34 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 17:42:34 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>Agree with Will that interest rates could rise a lot and that you therefore very much do not want to be holding debt when AI takes off, as existing debt will lose a lot of value if interest rates rise. It is fine to hold things similar to cash to maintain optionality, but you&#39;ll want to do this with short-term instruments. This is one place I have actually put effort in - I keep having to explain I want the floating interest rate, that when I say I don&#39;t want to take risk with my cash I mean that I am afraid interest rates will go up rather than down.</p><p> And to take it one step further, holding long term debt at fixed rates is amazing in that situation, such as a long term mortgage. As a company you would love to have issued bonds, and so on.</p><p> For governments, as was famously asked recently, <a href="https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century">is R>;G</a> ? If real growth is very high, then it is fine for real interest rates to also be high. Also note that governments only refinance slowly as their bonds mature, so if things are escalating quickly, the interest rates on their debt could be well behind the growth rate even if the rate on new debt exceeds the growth rate, making there be less need to raise more revenue. And we should expect major primary surplus to happen on its own.</p></div></section><h2> Concrete example portfolio </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:05:07 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:05:07 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, I think I want to take a bit of a step back and talk a bit more practicalities.</p><p> In this post &quot;<a href="https://www.lesswrong.com/posts/jvHLBEXXEtZtt4KFP/investing-for-a-world-transformed-by-ai">Investing for a World Transformed by AI</a> &quot; Peter McCluskey has a relatively concrete sets of companies that he thinks will perform well if the world gets transformed by AI. Extracting the most relevant recommendations:</p><blockquote><p> AI companies: Google, [...] OpenAI,</p><p> [...]</p><p> My current semiconductor-related investments are (in alphabetic order): AMKR, AOSL, ASML, ASYS, KLAC, MTRN, LSE:SMSN, TRT.</p><p> [...]</p><p> AMZN, MSFT, and GOOGL are likely to get some benefit from datacenter growth. I guess I&#39;ll expand my GOOGL holdings and buy small positions in AMZN and MSFT sometime in 2023, when I see signs that their stock&#39;s downward momentum has dissipated.</p><p> [...]</p><p> My two biggest solar bets are CSIQ, and SCIA. I have smaller positions in JKS, DQ, SEHK:1799.</p><p> For power grid infrastructure and solar farm construction, I have positions in MTZ, MYRG, PLPC, and PRIM.</p></blockquote><p> These sure are a lot of stock tickers, so I don&#39;t think we should look into each one of those, but I am kind of curious whether you look at this portfolio and it seems vaguely sane to you.</p><p> And then I would like to go into a bit more concrete detail about what you would actually concretely do if you wanted to spend at most like 15 hours on this, but still end up with a portfolio that seems like it does the thing. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:06:34 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:06:34 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Habryka&#39;s portfolio seems reasonable enough to me. I think omitting TSM is an impactful decision (and not one I agree with) but other than that it looks solid for a list of tickers.<br><br> If I were to make my own list of stocks worth large-ish positions, I&#39;d say TSM, MSFT, GOOG, AMZN, ASML, NVDA are a good starting point. You can also look at energy producers and raw materials mining/refining. Though with those sorts of companies its a more competitive market and it makes sense to diversify more. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:08:23 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:08:23 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>That portfolio does not include weightings, and has a bunch of [...] in it, and I don&#39;t know what a lot of those companies are or whether they seem cheap, but does not in any way seem insane. I certainly have AMZN/MSFT/GOOGL, although I am not trying to time momentum. Solar seems like a decent place to be for other reasons as well and I have some exposure there, but I haven&#39;t investigated the companies named.</p><p> Semiconductors seem like a good idea if one was doing the work and building from scratch. But also one does not need to hedge bets too carefully on exactly which angle of AI will end up being most profitable - it seems reasonable to aim for where you see the best profits, rather than trying to be too smooth.</p><p> And yes, NVDA seems cheap. Kind of crazy that it&#39;s down substantially from its peak through a strong earnings beat.</p><p> I stay away from TSM due to geopolitical risk, I don&#39;t want it even if it&#39;s properly priced in. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:09:52 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:09:52 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>To Zvi&#39;s point about geopol risk on TSM, TSM is one of the stocks where I am almost entirely call options in my exposure. I think if you are comfortable trading options - and many people aren&#39;t which is reasonable - a 31% implied volatility is quite cheap. And there aren&#39;t good substitutes for TSM that I am aware of. It plays a pretty unique role in global supply chains I think. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:10:33 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:10:33 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>Yeah, TSM seems like an excellent place to deploy call options if one was willing to use call options. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:12:09 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:12:09 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, let&#39;s try to dumb things down even more. Let&#39;s say you are a 28 year old male with a few tens of k to hundreds of k to invest who does not have any kind of brokerage account set up. What is the actual concrete thing you would do that would not cause you to accidentally press the wrong button and then somehow lose all your money?</p><p> Some links in response to this also seems fine, though I do feel like this step is one where some reasonable people might get stuck. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:12:02 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:12:02 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>None of what I said is financial advice but this is: be very careful before clicking any buttons in your brokerage account, and make sure you understand what they do.</p><p> More seriously, people for whom this conversation is a bit technical should avoid trading options or using excessive leverage. It is easy to mess up if you don&#39;t really have the knowledge base and aren&#39;t paying close attention. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:13:43 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:13:43 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>And then I would like to go into a bit more concrete detail about what you would actually concretely do if you wanted to spend at most like 15 hours on this, but still end up with a portfolio that seems like it does the thing.</p></blockquote><p> To a first approximation: either go 100% equities, or keep some equities and some cash, with the cash portion invested in short term rates (money market, T-bills, etc) and periodically rebalance.<br><br> The maximally agnostic thing is just the global stock index. Vanguard has an ETF with the ticker VT that&#39;s just all stocks in the world, market cap weighted. Global equities have underperformed US equities for a long time (and we have a <i>lot</i> of tech here), so if you&#39;re a growth instead of a value investor then maybe you want a US total index (VTI) or just the S&amp;P 500 large cap index (SPY).<br><br> The next layer is to spend some time thinking about which areas are likely to outperform in the immediate term. I recommend going long-only, <i>not</i> trying to short any particular areas, given the likely expected increase in growth and uncertainty about where such gains might appear. I mentioned raw materials above, you can get a global raw materials ETF with MXI, or US-only raw materials with IYM. Semiconductors also seems like another clear buy, given what we&#39;ve already observed (though it may now be overvalued and the value will be captured elsewhere after this wave!), and the ticker SMH is a broad semiconductor ETF. You can go down the list on whatever you&#39;re bullish on and find an ETF for that subsector, almost all of them have one, eg for solar you can buy TAN. (Note that these are just common symbols, you might want some other variant on these funds, I&#39;m not specifically recommending these)<br><br> It seems very very risky to invest in individual names, unless you&#39;re monitoring this very closely... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:14:50 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:14:50 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p><a href="https://www.lesswrong.com/users/habryka4?mention=user">@habryka</a> I would tell such a person to be between 50-100% equities, mostly indices with a small special allocation to MSFT, and to short long dated bonds if they feel comfortable doing so. If they don&#39;t, then hold whatever they don&#39;t invest in cash/tbills.</p><p> And get a mortgage if they live in Wisconsin or some such place. And don&#39;t go to grad school. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:16:49 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:16:49 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Another benefit of call options btw is that you lock in the interest rate. If you are getting your leverage by spot borrowing you need to pay attention to the rate your broker charges cuz it could go up a lot. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:17:15 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:17:15 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>Yeah, let me echo NoahK: All this talk of options needs to take into account that it is VERY easy to click the wrong buttons, to size things 10x or 100x what you thought you were doing, or otherwise get into big trouble, if you wade into options or futures or other things like that. Be sure you know EXACTLY what you are doing, talk to a human to confirm as needed, and so on, and when in doubt stick to the basics. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:17:47 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:17:47 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>More seriously, people for whom this conversation is a bit technical should avoid trading options or using excessive leverage.</p></blockquote><p> To briefly address leverage: this could get very crazy in a high rates environment. Leverage is financed via margin loans, and those are almost always the shortest term interest rate + some premium. So if interest rates are already at 50% or something, you&#39;re bleeding half your value every year using 2x margin. This is great of course if the stock market is soaring, but generally stocks don&#39;t only move in one direction, and you could get fully wiped out from a combination of high rates and sidewise to suddenly sharp downwards action. For just one concrete scenario: suppose there&#39;s a new AI-driven trading bot (or a million of them) and we get a flash crash of 75% instantly or something. Everyone doing this will not just be liquidated but also in deep debt to their brokerage. I think leverage is an excellent strategy only in &quot;relatively normal&quot; states of the world. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:17:43 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:17:43 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Hmm, so I feel like this advice is too conservative. Like, I at least seem to have some beliefs about how big of a deal AI will be that disagrees pretty heavily with what the market beliefs (I think, though this kind of stuff is hard to tell). I feel like I would want to make a somewhat concentrated bet on those beliefs with like 20%-40% of my portfolio or so, and I feel like I am not going to get that by just holding some very broad index funds, which really are the &quot;defer to the market maximally&quot; option. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:18:59 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:18:59 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>I like &quot;short bonds and make idiosyncratic investments in MSFT, GOOG, NVDA, etc&quot;. It depends on how much risk the 28 year old wants. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:20:16 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:20:16 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>I like &quot;short bonds and make idiosyncratic investments in MSFT, GOOG, NVDA, etc&quot;.</p></blockquote><p> Short bonds is a great strategy when you&#39;re in the state of the world where the takeoff is already occurring and everyone realizes it and both growth and rates are skyrocketing already. Personally, I would not be short bonds <i>right now in the current environment</i> , but of course YMMV </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:20:22 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:20:22 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ideally there would exist an AI index fund or something I could buy that balances things out for me, but I think that doesn&#39;t exist? And I also don&#39;t mind taking on some risk. Seems fine for that part of my portfolio to go to close to zero in like 50% of worlds, if it properly captures the upside in the other worlds. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:21:17 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:21:17 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>My big difference with Will is that I&#39;m unafraid to buy individual stocks, which also gives you additional tax-related optionality in many cases. You don&#39;t want too much concentration into any one stock, but a 28-year-old doesn&#39;t/shouldn&#39;t have a level of risk aversion where eg being 10% GOOG seems unreasonable, and certainly 5% is fine, if you think that is where the value lies.</p><p> At minimum, if you expect major changes from AI, you&#39;re going to want to be overweight AI-linked stuff, at some level of magnitude and obviousness. If nothing else there are sector ETFs for such things, but mostly the stocks are obvious and I&#39;d look at those. Again, small mistakes here really are fine in expectation.</p><p> Shorting bonds is again a more advanced move. It does seem like if the premium to do it isn&#39;t too big it would have alpha, but it can definitely backfire - interest rates dropping for a few years before they rise does not seem so unlikely to me. The worst case scenario is you lose money by being too early, then you&#39;re right later but you have no capital to profit from being right later. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:22:06 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:22:06 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>I think the yield curve right now is flat and it will be upward sloping before we&#39;re done with everything. And I don&#39;t think Powell can cut rates. But this gets far afield of AI.</p><p> Probably the best risk adjusted trade imo is to like long 10 year bonds and short 20 year bonds in no more than a 2:1 ratio. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:23:09 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:23:09 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>Ideally there would exist an AI index fund or something I could buy that balances things out for me, but I think that doesn&#39;t exist?</p></blockquote><p> Oh they definitely exist! Don&#39;t underestimate the marketing power of these big fund managers lol<br><br> iShares has one under ticket IRBO. Let&#39;s see what it holds... Looks like very low concentration (all &lt;2%) but the top names are... Faraday, Meitu, Alchip, Splunk, Microstrategy. (???) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:23:13 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:23:13 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Very quick question: What brokerage thing do you actually someone like me should use? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:23:40 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:23:40 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Interactive Brokers is the best brokerage if you know what you are doing. They have terrible UI but the best margin rates. Other brokers will rip your face off when you borrow cash.</p><p> If you don&#39;t want to learn to use terrible UI, you can use Fidelity or Schwab or something I hear. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:23:46 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:23:46 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>They have terrible UI but the best margin rates.</p></blockquote><p> Lol, I feel like that is the wrong tradeoff to make when I am worried that I will lose all my money by pressing the wrong button. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:29:26 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:29:26 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>My big difference with Will is that I&#39;m unafraid to buy individual stocks</p></blockquote><p> I&#39;m open to buying individual stocks, but it&#39;s very difficult for me to recommend specific stocks to other people under extreme uncertainty! How can an average investor with &lt;15 hours of research as Oliver stipulated know they&#39;re doing anything with positive expected value over buying a broad index? Anything that&#39;s a mega-winner will in fact get captured by a broad index fund.<br><br> I do think it&#39;s fine for younger more risk seeking people with time on their hands to give this some thought and pick individual stocks. If nothing else you will <i>learn</i> a lot by doing this, both about markets and about your psychology, and it can incentivize you to learn more about the companies in question once you have skin in the game. That said, I would never let your index fund exposure go to 0 in favor of individual stocks, just to avoid the risk of ruin that you captured none of the eventual winners.</p><p> To be very clear: I&#39;m not even certain that most/all value will be captured by a single megacorporation. It could be that several thousand companies each generate a trillion dollars in value or something. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:30:01 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:30:01 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>(As a quick anecdote here, a while ago I tried to figure out what would happen if you just blindly followed all stock-advice that was upvoted on LessWrong. So I set up a virtual trading portfolio on some random website I found and maintained it for a few months. One of the biggest positions I took out was to short Nikola based on some upvoted post at the time. Within a few months that position had grown to 60%+ of my portfolio as the shorting had gone quite well and had appreciated like 300% or so.</p><p> Then I stopped checking for 2 months, and then when I checked in again, suddenly that whole investment went to 0...</p><p> Turns out the virtual portfolio website didn&#39;t execute your options automatically even if they were in the green, so they had expired and become worthless.</p><p> This is one reason why I am afraid of clicking a wrong button and losing all of my money) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:31:11 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:31:11 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Oh no that is quite unfortunate! Yes if you buy options I recommend setting an alert when you buy them that will go off like a week before expiry. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:38:18 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:38:18 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>On individual stocks, I think that especially if you know the companies in question but also in general, you don&#39;t need 15 hours on each company to know that it is likely a real company making real things with good prospects at a reasonable price or P/E, and if you bought index funds you&#39;d be buying that stock anyway. As long as you don&#39;t go too big on any one stock that way, it seems... fine. I&#39;ve never spent 15 hours looking at a stock ever.</p></div></section><h2> Is any of this ethical or sanity-promoting? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:31:02 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:31:02 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, I think I would like to cover another topic that is related, which is something like: <strong>&quot;Ok, but is investing in this way ethical? Also, will it mess up your ability to think sanely about this stuff and try to slow down AI when that will directly hurt your bottom line?&quot;</strong> </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:34:04 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:34:04 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Like, I&#39;ve been trying to <a href="https://www.lesswrong.com/posts/Be3ertyJfwDdQucdd/how-should-turntrout-handle-his-deepmind-equity-situation">convince some of my friends working at AGI companies to somehow get rid of their equity position</a> , since I do think at that level of exposure it pretty seriously messes with your ability to think clearly (not as much as the social environment of the AGI company, but I think still a good amount).</p><p> I think holding Google or Nvidia with a huge fraction of your portfolio probably has similar effects, though my guess is as you get more diffuse than that, the effects get weaker. But like, many people I know are actively advocating for pretty aggressive government interventions that does feel like the kind of thing that could seriously hurt returns, and there might be surprisingly large effects here, even at a pretty broad level. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:36:09 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:36:09 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>I think for the vast majority of investors, your portfolio choices don&#39;t impact the world in any meaningful way. (Though they may impact how <i>you</i> think about things and evaluate risk.)</p><p> If you are a billionaire, yes, I would encourage you to consider social welfare aspects of your AI investments. It would be good if these companies faced higher costs of capital, and buying a stock serves to lower the cost of capital of the company you purchase. But if you are a 28 year old tech worker do what you must to protect yourself, imo. The capacity to harm the world through your investments is quite limited at that level. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:36:14 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:36:14 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>Ok, but is investing in this way ethical?</p></blockquote><p> I agree it seems more questionable if you&#39;re, say, giving the seed investment to a new AGI startup that&#39;s all capabilities and disdains any idea of safety. But one thing that&#39;s nice about my index fund approach here is that you can benefit from the downstream economic gains, without favoring any particular actor.</p><p> There is also a very loose, not zero, connection between investing in the secondary market (buying existing stocks, not companies issuing new stock to raise money) and how that flows through to advancing a corporation&#39;s interests. If a company is regularly issuing stock to finance their activities, then yes, if you raise their stock price on the margin then they can finance themselves more than if they have a low stock price. But what about companies who haven&#39;t issued new equity since the IPO? Does them having a high stock price make their company run better/faster towards AGI? That seems very tentative. I guess there&#39;s a channel by which you&#39;re making it more profitable for individual employees with lots of talent to go work there, when they&#39;re paid largely in stock options instead of cash? </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:38:31 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:38:31 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>But what about companies who haven&#39;t issued new equity since the IPO?</p></blockquote><p> To play the devil&#39;s advocate here for a second: most big tech companies working on AI do in fact issue new equity annually, not to sell to investors, but as equity compensation for employees. This can actually amount to several %/year for many of those companies, which is quite dilutive over the long run! So yes, equity price can matter when you&#39;re at a company that&#39;s giving you the majority of your expected comp in options. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:38:45 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:38:45 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><blockquote><p>I guess there&#39;s a channel by which you&#39;re making it more profitable for individual employees with lots of talent to go work there, when they&#39;re paid largely in stock options instead of cash?</p></blockquote><p> Yeah, I feel a bit confused about the relationship here. I guess I kind of feel like the cost of raising capital of any kind should be roughly tracked by the stock price?</p><p> But also, yeah, it does just make it cheaper for them to hire people. For most tech companies their stock compensation package is like 50% of their salaries, and most of their expenses are salaries, so that should pretty directly equate more capital. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:44:40 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:44:40 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><blockquote><p>Ok, but is investing in this way ethical?</p></blockquote><p> It depends on the details, but typically I would say yes. There are exceptions. If you are investing in private companies or where you can impact cost of capital in a meaningful way, like OpenAI or Anthropic, then I would say investing there has ethical concerns.</p><p> But Microsoft and Google are not like that. They already have infinite war chests and are not going to be spending more or less money on AI based on your investment. Nor is Nvidia going to do anything but maximize along its capacity and scaling constraints, money isn&#39;t a factor. I think buying such stocks is fine.</p><p> Even when they issue stock options, they are going to scale to the price, and your impact is rather minimal all around. I think you can safely ignore such factors. Assume they have infinite war chest size when it comes to hiring, and salaries and compensation are constrained only by social forces.</p><p> If you are worried, you can go case by case - when you invest in Solar companies you are plausibly helping them in a real way, which is potentially bad for AI on the margin but also good for other reasons (eg climate and general goodness).</p><p> The impact on your incentives is distinct. I don&#39;t worry about this at all, because I know that it is at best a small hedge of my real exposures and it won&#39;t change anything. For someone with big exposure to private stock in OpenAI, there could be a problem there.</p></div></section><h2> How would you actually use a ton of money to help with AGI going well? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:44:34 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:44:34 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, this is kind of a tangent from the core topic, but it also feels important.</p><p> So let&#39;s say that there are a bunch of people concerned about existential risk, who invest well in the run-up to AGI, and now they have a few hundred billion dollars or so in-aggregate.</p><p> Assuming that it&#39;s hard to use money to make more AI Alignment progress, is there any way you could use that money to slow down AGI companies or something like that? Like, I do think a major question for me in this investment situation is whether I actually have use for a giant pile of money when AGI happens. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:44:53 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:44:53 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>If you can drive up the price of the factors of production, you might be able to slow progress. Maybe a consortium of bidders could make some rare earth metals more expensive? Idk about the technical details.</p><p> I would <i>not</i> short high growth AI stocks to drive up their cost of capital since you&#39;ll just get blown out quickly. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:46:05 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:46:05 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>Assuming that it&#39;s hard to use money to make more AI Alignment progress, is there any way you could use that money to slow down AGI companies or something like that?</p></blockquote><p> One strategy that has been discussed is to pay top talent to not make AGI. If you&#39;ve truly outperformed the market, you should be able to pay above-market rates to researchers and developers.</p><p> That said, I expect the motivations of folks working on this stuff to be only partly monetary, and if they&#39;re already individually wealthy and have satisficed on the major life stuff, they might just keep working on it for glory or whatever other motivation, so I suspect this will be less successful than people presume. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 18:48:14 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 18:48:14 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>Thinking about it more, the best strategy might be to donate to politicians who are already sympathetic. The government has guns and stuff and is good at stopping people from doing things. And donations can make a difference to policy. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:48:36 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:48:36 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><blockquote><p>If you can drive up the price of the factors of production, you might be able to slow progress.</p></blockquote><p> Yes this is the generalization of my suggestion about buying up the top talent. Anywhere a resource constraint exists, you could in theory pay extra in order to sequester that resource.</p><p> Of course, once you do that, and the price rises, you&#39;ve incentivized the market to find new and cheaper ways to bring more supply online. So I think that this strategy at best slows things down in the short term, and possibly speeds them up in the long term. But if you think the only thing that matters/exists is the short term, then it&#39;s certainly one tool in the toolkit... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 18:50:54 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 18:50:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>Lobbying the government, backing candidates, public advocacy or other things in such categories are obvious options where you could spend quite a lot. That would be where my first move would be. Politics always feels icky but ultimately is not something one can ignore.</p><p> Buying up top talent also seems like a great move if you can target it well, and presumably you can also put that talent to some good use (alignment, or if not then something else, top talent is almost always cheap even when it&#39;s expensive). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 18:50:14 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 18:50:14 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Yeah, I do feel kind of icky about the lobbying point. It feels pretty symmetric in that it&#39;s been a way in which historically people have prevented tons of good stuff from happening in a way that didn&#39;t really expose them to accountability (regulatory capture being the more common path, but also all kinds of dumb safety regulations). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:51:46 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:51:46 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>I just want to point out that we didn&#39;t even mention crypto once :)</p></div></section><h2> Please diversify your crypto portfolio </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:00:24 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:00:24 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Talking about crypto, via historical circumstance I sure have a lot of friends who are holding a really quite substantial fraction of their portfolio in cryptocurrency. I am actually interested in a quick digression on what you expect to happen to crypto prices if AGI takes off (at least in parts because I often wish my surrounding ecosystem was betting making a less concentrated bet on crypto and this particular domain might sway some people who otherwise have been pretty steadfast in their hodling). </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 18:53:30 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 18:53:30 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>Just to make the briefest possible case: does cryptocurrency/blockchains solve a specific problem that future AGIs are likely to have? I think there&#39;s actually a good chance that AIs can utilize these features way better/more easily than humans can. Digital representation of scarcity could be highly valuable. Also automatically-enforceable smart contracts might end up as a primary means of coordination between AIs. :) </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 19:00:02 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 19:00:02 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>Note that the strongest case against cryptocurrency here (other than that it has no value or use at all) is that holding non-yielding or low-yielding assets suffers extreme opportunity cost in a world of very high real rates. For example, gold tends to trade inversely to real interest rates (higher rate ->; lower price). So bitcoin in particular could have major issues in that regard. Ethereum or another more flexible system could potentially do something like raise the staking yield or something to try to keep up with interest rates (though of course that&#39;s dilutive to current holders). Basically you&#39;re relying on use cases providing major value at the point of AGI takeoff... </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 19:00:48 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 19:00:48 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>So my basic take is that anyone holding a lot of crypto as a % of their wealth, but not so much that they&#39;d face liquidity issues in selling, is doing something very wrong. This is not to say people shouldn&#39;t own BTC or ETH or whatever. Some crypto is fine.</p><p> But basic portfolio theory suggests that if you have an extremely large % of your portfolio in an asset with extremely high idiosyncratic volatility, you can get better returns by diversifying (and then using more leverage if you want).</p><p> I&#39;m not against owning crypto but 50%+ allocation to ETH - which I think you mentioned - would be very suboptimal for almost anyone. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="c3Ji9Th6jATRyHLFC-Tue, 31 Oct 2023 19:00:40 GMT" user-id="c3Ji9Th6jATRyHLFC" display-name="Cosmos" submitted-date="Tue, 31 Oct 2023 19:00:40 GMT" user-order="3"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>宇宙</b></section><div><p>Thanks Oliver for having me in the dialogue! I need to drop off now, but I&#39;m honored to have been asked to participate, and this was a lot of fun and very thought provoking! Great talking to you Zvi and Noah, and I&#39;m looking forward to future dialogues! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:02:18 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:02:18 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Thank you Will! Really appreciated your thoughts here! </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="hsdsBxMSKELfnYswF-Tue, 31 Oct 2023 19:12:18 GMT" user-id="hsdsBxMSKELfnYswF" display-name="NoahK" submitted-date="Tue, 31 Oct 2023 19:12:18 GMT" user-order="4"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>诺亚克</b></section><div><p>I&#39;ve got to run now as well but thank you for inviting me! It was great chatting with everyone. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:12:35 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:12:35 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Thank your joining Noah!</p></div></section><h2> Should you buy private equity into AI companies? </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:12:11 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:12:11 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>So, as we&#39;ve noted it&#39;s pretty hard to buy stock that&#39;s actually loaded on AGI companies, since most companies working directly on this stuff are either private (OpenAI, Anthropic) or are part of a giant tech company that mostly does non-AI stuff (Microsoft, Google).</p><p> So a thing one might try to do is to get access to the private equity. For example, by buying it off of employees who have been there for a while.</p><p> Does this seem like a good idea from a financial perspective? Also, the incentives question here does seem more pronounced than for the other things we&#39;ve been talking about. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="N9zj5qpTfqmbn9dro-Tue, 31 Oct 2023 19:13:54 GMT" user-id="N9zj5qpTfqmbn9dro" display-name="Zvi" submitted-date="Tue, 31 Oct 2023 19:13:54 GMT" user-order="2"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>兹维</b></section><div><p>It seems likely that private equity in eg OpenAI or Anthropic would trade cheap to the extent that it traded privately, so the concern here is ethical rather than financial. At which point, it depends who you buy it from, what incentives you are changing in what places, and whether or not you can serve a &#39;dead on the cap table&#39; purpose without too much driving up the price. I don&#39;t know enough to know the answers for sure, but until I had a better idea I would try and stay away. </p></div></section><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:15:39 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:15:39 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Yeah, that does also feel right to me. I have been thinking about setting up some fund that maybe buys up a bunch of the equity that&#39;s held by safety researchers, so that the safety researchers don&#39;t have to also blow up their financial portfolio when they press the stop button or do some whistleblowing or whatever, and that does seem pretty incentive wise.</p><p> I do think sadly companies often have explicit agreements with employees that prevent them from hedging their equity positions, so this might be hard.</p></div></section><h2> Summarizing takeaways </h2><section class="dialogue-message ContentStyles-debateResponseBody" message-id="XtphY3uYHwruKqDyG-Tue, 31 Oct 2023 19:15:58 GMT" user-id="XtphY3uYHwruKqDyG" display-name="habryka" submitted-date="Tue, 31 Oct 2023 19:15:58 GMT" user-order="1"><section class="dialogue-message-header CommentUserName-author UsersNameDisplay-noColor"><b>哈布里卡</b></section><div><p>Ok, I feel like after participating in this, I feel like I at least have a decent handle on constructing some kind of portfolio here.</p><p> I will think about this for a few days, but I feel like if I was implementing the advice here, I would roughly:</p><ul><li> Go and make a brokerage account with Schwab or Fidelity (whichever seems less annoying to set up)</li><li> Invest like 50% of my portfolio into pretty broad index funds with really no particular specialization</li><li> Take like 20% of my portfolio and throw it into some more tech/AI focused index fund. Maybe look around for something that covers some of the companies listed here on the brokerage interface that is presented to me (probably do a bit more research here)</li><li> Invest like 3-5% of my portfolio into each of Nvidia, TSMC, Microsoft, Google, ASML and Amazon</li><li> Take like 2-5% of my portfolio and use it to buy some options (probably some long-term call options on some of the stocks above), making really sure I buy ones that have limited downside, and see whether I can successfully not blow up that part of my portfolio for like 2 years before I do any more here</li></ul><p> And then I probably wouldn&#39;t bother much with rebalancing and basically forget about it unless I feel like paying much extra attention.</p><p> <i>(Zvi, Noah, and Will each seemed to think was a non-crazy plan when I said the above on the call we were on while writing this dialogue)</i></p></div></section><div></div><br/><br/> <a href="https://www.lesswrong.com/posts/CTBta9i8sav7tjC2r/how-to-hopefully-ethically-make-money-off-of-agi#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/CTBta9i8sav7tjC2r/how-to-hopefully-ethically-make-money-off-of-agi<guid ispermalink="false"> CTBta9i8sav7tjC2r</guid><dc:creator><![CDATA[habryka]]></dc:creator><pubDate> Mon, 06 Nov 2023 23:35:16 GMT</pubDate></item></channel></rss>