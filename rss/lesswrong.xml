<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title><![CDATA[LessWrong]]></title><description><![CDATA[A community blog devoted to refining the art of rationality]]></description><link/> https://www.lesswrong.com<image/><url> https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico</url><title>少错</title><link/>https://www.lesswrong.com<generator>节点的 RSS</generator><lastbuilddate> 2023 年 8 月 27 日，星期日 20:10:56 GMT </lastbuilddate><atom:link href="https://www.lesswrong.com/feed.xml?view=rss&amp;karmaThreshold=2" rel="self" type="application/rss+xml"></atom:link><item><title><![CDATA[Will issues are quite nearly skill issues]]></title><description><![CDATA[Published on August 27, 2023 4:42 PM GMT<br/><br/><p><i>这篇文章包含可能被视为自由意志问题剧透的内容</i></p><p>我不能跳下悬崖。我不知道我附近有任何悬崖，但这不是我关心的原因，所以我们将反事实地跳过它。我不能跳下悬崖的更有趣的原因是我不想跳。当我们自信地知道我不想跳下悬崖，并且在不久的将来没有人会强迫或操纵我时，我们可以得出结论，我跳下悬崖的可能性微乎其微，或者在实践中，我不会那样做。一旦我们接受<a href="https://dkl9.net/essays/could.html">“可以”表达了一个选项的合理性</a>，我们就说“我不可能跳下悬崖”，同样，“我不能跳下悬崖”。</p><p>从我的大脑到我的腿，<i>有</i>一个简单的肌肉命令序列可以引导我跳跃。但这种电机输出的存在并不是我们通常所说的“能力”。如果我附近有悬崖，但我不知道（无论如何可能是真的），那么腿部运动的模式也会存在，但我无法（在传统意义上，不仅仅是“我的”） “奇怪”的感觉）去跳过去。</p><p>不想做某事会像明确的“无能”一样有效地阻止人们去做某事。你不能只是反对“但是自由意志！” （并且是正确的），西斯我只是预料到了它，所以你不会想要（除非你想变得烦人），西斯人类的思想并不是神秘不可预测的混乱斑点。人们的思想有一种理论上可测量的内部状态，它限制了他们的行为。</p><p>如果某人（老实说，经过深思熟虑，并且不反对他们既定的习惯）决定不做某事（重要且容易避免），他们往往不会这样做，除非他们受到（足够令人信服的）新输入的影响。未能准确地限制某人未来的行为是因为你对他们的想法或将遇到的事情了解有限，而不是他们有不可思议的自由去做他们知道如何做的事情，即使这与他们的任何或全部偏好相矛盾。</p><p>评论某人的意图，然后说“但他们可以这样做！”表明你对该人的计划或未来投入的不确定性，而不是人类行为中任何基本的非决定论。或者它可能表明你未能出于实际目的而忽视小概率。如果你让我背出圆周率从第1000位开始的十位数字（我只知道前70位），我要么拒绝，要么随意猜测。 “观察”“你可以做对！”我想提醒您 10 <sup>-10</sup>的概率有多小。</p><h1>但为什么都是螃蟹呢？</h1><p>通过评估我们是否期望一个人采取特定行动，我们可以将意志或技能问题与非“问题”区分开来。</p><p>了解某个动作所需的肌肉命令并不是“能够”做到这一点的必要条件，因此这不能成为我们区分<a href="https://dkl9.net/essays/neologisms.html#will-issue">意志问题</a>和技能问题的方式。如果一个（相当大的）目标出现在我附近尚未确定的位置，我“能够”扔一个球并击中它，但在我真正做到这一点之前我不知道所需的确切动作，并且在我看到目标之前无法知道。</p><p>了解某项行动的口头程序并不足以“能够”做到这一点，因此也无法可靠地区分意志和技能。如果您<a href="https://dkl9.net/essays/bicycle_how.html">读过如何骑自行车</a>，但从未尝试过，当有人问您时您会背诵步骤，但无法真正做到。</p><p>据我所知，没有直接的衡量标准可以区分意志问题和技能问题（如果你知道的话，我想听听）。然而，通过将案例合并到“不能那样做”的描述中，我似乎<a href="https://www.lesswrong.com/posts/H7Rs8HqrwBDque8Ru/expressive-vocabulary">从表达词汇中剔除了术语</a>。如果我们以不同的方式谈论这些事情，我们期望它们之间存在真正的区别：那是什么？</p><p>区别在于你如何回应“问题”。当你一开始就预计某人不会做某事时，领导某人做某事通常会采取以下两种形式之一。如果这是通过教学或提供资源来完成的，我们会将没有这种干预而存在的限制视为技能问题。如果它因争论或激励而改变，我们将其视为遗嘱问题。某人“可以”做某事的陈述意味着你可以通过争论或激励他们来让他们做某事，并且你不需要教他们或给他们新的资源。</p><br/><br/> <a href="https://www.lesswrong.com/posts/EcWou63xeDF7pmPHi/will-issues-are-quite-nearly-skill-issues#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/EcWou63xeDF7pmPHi/will-issues-are-quite-nearly-skill-issues<guid ispermalink="false"> EcWou63xeDF7pmPHi</guid><dc:creator><![CDATA[dkl9]]></dc:creator><pubDate> Sun, 27 Aug 2023 16:42:12 GMT</pubDate> </item><item><title><![CDATA[Xanadu, GPT, and Beyond: An adventure of the mind]]></title><description><![CDATA[Published on August 27, 2023 4:19 PM GMT<br/><br/><p>克罗斯<a href="https://new-savanna.blogspot.com/2023/08/xanadu-gpt-and-beyond-adventure-of-mind.html">从新萨凡纳</a>发布。 </p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/ytbs0wnk9an8gft2qanh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/vk9bdcxzeqyjvqans6es 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/kilkhsthnkvdw4uvatqg 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/zj7ei1nbscvzjdifdgkq 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/tdwcutt0xhit2ddj0mfh 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/t8u6omz5sahvxzohatmi 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/vaqp7iv3qdngkw4sdb1q 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/krhvy9en5ncv3b8axuzj 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/esgumsk6uvq8rngrj3bh 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/uzhathqxszfsppiza848 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/RQWzG3chHdmfEPKsC/htieruw3s855b4oqkl4t 2000w"></figure><p>我发布了一份新的工作文件。标题同上；下面的链接、摘要、目录和介绍材料。</p><p>下载地址：</p><p>学术界： <a href="https://www.academia.edu/106001453/Xanadu_GPT_and_Beyond_An_adventure_of_the_mind">https</a> ://www.academia.edu/106001453/Xanadu_GPT_and_Beyond_An_adventure_of_the_mind</p><p> SSRN：https: <a href="https://ssrn.com/abstract=4553351">//ssrn.com/abstract=4553351</a></p><p> ResearchGate： <a href="https://www.researchgate.net/publication/373433939_Xanadu_GPT_and_Beyond_An_adventure_of_the_mind">https://www.researchgate.net/publication/373433939_Xanadu_GPT_and_Beyond_An_adventure_of_the_mind</a></p><p><strong>摘要：</strong>本文讲述了一段知识之旅，始于 20 世纪 60 年代末对柯勒律治的《忽必烈汗》结构的好奇，并引发了目前对大型语言模型的兴趣。对这首诗的仔细分析揭示了它的两个部分，每个部分都有一个嵌套结构（想想俄罗斯套娃），这表明了底层计算过程（嵌套循环）的操作。这引发了计算语言学（语义网络）的研究，随后是神经科学（卡尔·普里布拉姆的神经全息术）和文化进化。 2010 年代，我开始关注数字人类在机器学习方面所做的工作。当 GPT-3 于 2020 年发布时，我已经做好了准备，尽管我花了一段时间才在概念宇宙和“忽必烈汗”的宇宙之间建立起联系，无论这种联系是多么暂时。</p><p>邂逅柯勒律治的《忽必烈汗》3<br>浪漫的意识状态3<br>俄罗斯套娃和逃离“世外桃源”5<br>语义网络和莎士比亚十四行诗 8<br> Karl Pribram，神经全息术和大脑 10<br>流浪的岁月11<br>通过 GPT 走向未来 13<br>文本中的心灵与世界15<br> 《忽必烈汗》的文本，包括柯勒律治的序言 17<br>关于封面图片的说明 19</p><p><strong>邂逅柯勒律治的《忽必烈汗》</strong></p><p> 1969 年春天，我迷上了柯勒律治的《忽必烈汗》，那是我在约翰·霍普金斯大学本科生的最后一个学期。三年后，“忽必烈汗”已成为我衡量我对人类心灵理解的标准。这就是为什么我要讲一个故事，讲述我对思想的兴趣是如何通过《忽必烈汗》演变到最近的 ChatGPT 的。尽管看起来很奇怪，但这首诗是我接受这项新技术并认识到它的潜力的工具。</p><p>从某种意义上说，这首伟大诗篇的故事可以追溯到 11 世纪诺曼底法国人对英国的入侵，因为这种文化的跨越催生了英语。大约一个世纪后，这个故事遇到了意大利商人马可·波罗和蒙古军阀忽必烈汗之间的相遇的故事，东印度公司的鸦片贸易使这个故事更加活跃，点燃了塞缪尔的心。 18 世纪末和 19 世纪初的泰勒·柯勒律治。我们无需详细追踪该轨迹。我提到它只是为了让大家了解这首 36 行诗的范围，它是最著名的英语诗歌之一，在西方文学史上也许是独一无二的。它在流行文化中留下了自己的印记，从奥逊·威尔斯的《公民凯恩》（将凯恩的庄园命名为“世外桃源”），从而为整部电影奠定了基础，到奥莉维亚·牛顿-约翰的热门歌曲和电影《世外桃源》，随后被制作成百老汇音乐剧。它甚至为最粗俗的房地产大亨唐纳德·特朗普提供了夜总会的名称“世外桃源”(Xanadu)，该夜总会位于他现已不复存在的大西洋城赌场中。</p><p><strong>浪漫的意识状态</strong></p><p>1968-1969 年，我很可能在跟随 Earl Wasserman 教授学习浪漫主义文学之前读过这首诗。但我对此没有任何记忆。虽然我们直到第二学期才学习柯勒律治，但我最好从第一学期开始我的故事。</p><p>课程从济慈开始。我决定写一篇关于一首小诗的论文，《致——[范妮·布劳恩]”，并将写作推迟到截止日期前一天晚上。我很累，脑子也崩溃了。突然，我正在打字济慈写给范妮的一封信中的一段话，但我体验到打字的感觉，就好像这些话是我自己的一样。当我读完这段话时，我的思绪活跃起来，找到了《希腊古瓮颂》的第二节——你知道，“听到的旋律很甜美，但那些闻所未闻的更甜美……”我读这些话就好像他们是我自己的。</p><p>我完成了论文，交了它，得到了成绩，然后......</p><p>我有一个问题：那是什么！？我不知道。但那是 20 世纪 60 年代，意识状态的改变风靡一时，药物引起的，但也有冥想，现在看来，深夜诗歌对疲惫心灵的影响。</p><p>接下来是珀西·比希·雪莱，他宣称诗人是“世界上未被承认的立法者”。我再次推迟写论文，直到最后一刻。我累了。那份该死的报纸是通过我自己写的。但我并没有体验到雪莱的任何东西，就好像它是我写的一样。这与我写济慈的经历不同。这些单词一个接一个地排成一排，顺着我的手臂流过，穿过我的手指，从打字机流到纸上。很容易。没有汗水。这也是一篇好论文。</p><p>华兹华斯在春季学期上学。那是我本科时写过的最好的论文。沃瑟曼评论说，这是“对这首诗的成熟思考”——尽管我忘记了那是什么诗。没有精神上的恶作剧。我用标准的任务组合来写它，这里一三个句子，那里一个段落，在房间里走动一下，做一三个笔记，查找一些东西，回到打字机，冲洗，重复，等等。 ..完成。</p><p>我的《忽必烈汗》论文也是如此。这首诗本身就存在很多问题。第一个是：它是关于什么的？没有叙述。它经常被视为文字音乐而被忽视。文字音乐确实如此，但这并不能成为驳回的理由。</p><p>然后我们有柯勒律治的序言。他说这首诗不完整。当他想起两三百行诗时，他已经陷入了鸦片的遐想——“所有的图像都升起……作为事物，同时产生相应的表达，没有任何感觉或意识的努力”——这是当他被一个来自波洛克的人打断时，他就崩溃了。当波洛克式的文章消失后，那两三百行也消失了。只剩下这首诗的 54 行，这是世界上最非凡的诗之一。事实上，没有什么明显缺失的。如果没有那序言，没有人会怀疑这首诗是不完整的。</p><p>批评家们有多种方式来处理这首诗本身与柯勒律治的主张之间的差异。我发明了另一种解决方案。将这首诗的第二部分解释为断言这首诗不完整是很容易和自然的（我在本文末尾附加了完整的文本）。说话者说“我可以在我体内复活吗”（第42页），显然暗示他不能，但如果他可以，他会“在空中建造那个圆顶”（第46页）。从第一部分开始，圆顶就被认为是忽必烈的快乐圆顶，在这里被用作诗本身的形象。这是对这些诗句的完全值得尊敬的解读。</p><p>我更进一步。我断言这首诗通过断言它不完整而自相矛盾地完成了自己。这样的阅读已经说明了一切。柯勒律治时代的富有的英国人喜欢在他们的花园中放置新建但不完整或破旧的建筑 - 他们被称为“愚蠢”。一首精致破旧的诗正好符合这种审美。而且，这种悖论性的解读也与美国文学批评中结构主义、后结构主义和解构主义解读的兴起相契合。尽管如此，沃瑟曼的概念倾向更为传统，他喜欢它。</p><p><strong>关于图像的注释</strong></p><p>“忽必烈汗”的肖像是尼泊尔艺术家阿拉尼科 (Araniko) 在忽必烈于 1294 年去世后不久创作的。该图像来自维基共享资源，属于公共领域。</p><p> Araniko：https: <a href="https://en.wikipedia.org/wiki/Araniko.">//en.wikipedia.org/wiki/Araniko。</a><br>图片： <a href="https://commons.wikimedia.org/w/index.php?curid=4126240.">https://commons.wikimedia.org/w/index.php?curid=4126240。</a></p><p>我用 1985 年在经典 Macintosh 上的 MacPaint 中制作的图像覆盖了它。</p><br/><br/> <a href="https://www.lesswrong.com/posts/RQWzG3chHdmfEPKsC/xanadu-gpt-and-beyond-an-adventure-of-the-mind#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/RQWzG3chHdmfEPKsC/xanadu-gpt-and-beyond-an-adventure-of-the-mind<guid ispermalink="false"> RQWzG3chHdmfEPKsC</guid><dc:creator><![CDATA[Bill Benzon]]></dc:creator><pubDate> Sun, 27 Aug 2023 16:20:00 GMT</pubDate> </item><item><title><![CDATA[High level overview on how to go about estimating "p(doom)" or the like]]></title><description><![CDATA[Published on August 27, 2023 4:01 PM GMT<br/><br/><p>这是我在加里·马库斯<a href="https://garymarcus.substack.com/p/d28">最近发表的一篇博客文章</a>中留下的评论，但我认为这可能会引起人们的普遍兴趣。绝对欢迎反馈和批评！</p><p> ------</p><p>无论如何，这里有一个稍微长一点的概述，介绍我目前首选的估计“p（厄运）”、“p（灾难）”或其他极其不确定的前所未有的事件的方法。不过，我还没有完全弄清楚如何正确地完成这一切 - 正如 Gary 提到的，作为我的博士研究和<a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk">MTAIR 项目</a>的一部分，我仍在研究这个问题。总体思路或多或少是标准<a href="https://en.wikipedia.org/wiki/Probabilistic_risk_assessment">概率风险评估（PRA）</a> ，但一些细节是我自己的看法或有争议。</p><p><strong>步骤 1：确定决策阈值。</strong>重申一下加里在我们的电子邮件对话中引用的部分：我们只真正关心“p（doom）”或类似的内容，因为它与具体决策相关。特别是，我认为政策讨论中大多数人关心 p(doom) 之类的东西的原因是，对于许多人来说，更高的违约 p(doom) 意味着他们愿意做出更大的权衡以降低风险。例如，如果你的 p(doom) 非常低，那么你可能不想仅仅因为灾难发生的可能性很小而以任何方式限制人工智能的进步（尽管你可能出于其他原因想要监管人工智能！）。但如果你的p（厄运）更高，那么你就会开始愿意做出越来越艰难的牺牲，以避免真正严重的结果。如果你的默认 p(doom) 非常高，那么，是的，也许你甚至开始考虑轰炸数据中心。</p><p>因此，第一步是确定截止点在哪里，至少粗略地确定 - p(doom) 的阈值是多少，这样如果高于或低于这些点，我们的决策就会改变？例如，如果对于 0.1 到 0.9 之间的任何 p(doom)，我们的决策都是相同的（即我们愿意做出的权衡不会改变），那么我们不需要任何更细粒度的分辨率如果我们确定它至少在该范围内，则 p(doom) 上。</p><p>当然，如何准确地确定适当的阈值是一个更加困难的问题。这就是风险承受能力估计、<a href="https://www.amazon.com/Ergodicity-Definition-Examples-Implications-Possible/dp/B09PHBV2HD">非遍历过程的决策</a>以及<a href="https://link.springer.com/book/10.1007/978-3-030-05252-2">深度不确定性下的决策 (DMDU) 等问题的</a>用武之地。我仍在尝试研究这方面的文献。</p><p><strong>步骤 2：确定 p(doom) 的合理范围，或者您尝试预测的任何概率。</strong>使用可用数据、模型、专家判断引出等来获取感兴趣数量的初始范围，在本例中为 p(doom)。一开始这可能是一个非常粗略的估计。对于执行此操作的最佳方法存在不同的意见，但我个人更倾向于使用以下方法的组合：</p><ul><li>使用某种加权平均方法汇总不同的专家判断、定量模型等。我的研究的一部分是如何以有原则的方式进行加权，即使只是在主观、肤浅的层面上（至少在一开始）。理想情况下，我们希望有原则性的方法来对不同类型的专家、定量模型和预测市场进行加权，大概基于以前的记录、潜在偏差等。</li><li>我目前倾向于尽可能以二阶概率的形式指定合理的概率范围（例如，p(doom) 的估计概率分布是多少，而不仅仅是点估计）。其他人认为只使用点估计或置信区间就可以了，还有一些人主张使用各种类型的不精确概率。我仍然不清楚不同方法的优点和缺点是什么。</li><li>我通常主张<a href="https://forum.effectivealtruism.org/posts/WKPd79PESRGZHQ5GY/in-defence-of-epistemic-modesty">认知上的谦虚</a>，至少对于大多数会影响很多人的政策决策来说（比如这个）。其他人似乎不同意我的观点，原因我不太明白，相反，他们主张政策制定者自己思考这个话题并得出自己的结论，即使他们自己不是这个话题的专家。 （有关此主题的更多信息，请参阅乔恩·马西森（Jon Matheson <a href="https://www.amazon.com/Why-Its-Not-Think-Yourself/dp/1032438266/"><i>）撰写的《为什么不为自己思考也可以</i></a>》。有关相反的观点，请参阅埃利泽·尤德科斯基（Eliezer Yudkowsky）的短书《<a href="https://equilibriabook.com/"><i>不充分均衡》</i></a> 。）</li></ul><p><strong>第 3 步：决定是否值得进行进一步分析。</strong>如上所述，如果在步骤 1 中我们确定相关决策阈值是 p(doom)=0.1 和 p(doom)=0.9，并且如果步骤 2 告诉我们 p(doom) 的所有合理估计值都在这些数字之间，那么我们就完成了，不需要进一步的分析，因为进一步的分析不会以任何方式改变我们的决定。但假设事情没那么简单，我们需要决定是否值得花时间、精力和金钱来对这个问题进行更深入的分析。这就是<a href="https://en.wikipedia.org/wiki/Value_of_information">信息价值 (VoI)</a>分析技术可以发挥作用的地方。</p><p><strong>步骤 4（假设需要进一步分析）：尝试分解问题。</strong>我们能否确定影响 p(doom) 顶级问题的关键子问题？我们能否以某种方式对这些子问题进行估计，从而使我们能够更好地解决关键的顶级问题？这或多或少是 Joe Carlsmith 在<a href="https://arxiv.org/abs/2206.13353">他的报告</a>中试图做的事情，他将问题分解为 6 ​​个子问题，并试图对这些子问题进行估计。</p><p>一旦我们有了合适的分解，我们就可以为每个子问题寻找更好的数据，或者我们可以询问主题专家对这些子问题的估计，或者我们可以尝试使用预测市场等。</p><p>当然，问题在于，并不总是清楚分解问题的最佳方法是什么，或者如何以正确的方式将子问题放在一起，以便获得有用的整体估计，而不是完全偏离目标的东西，或者如何确保你没有遗漏任何真正重要的东西，或者如何解释“未知的未知数”等。仅仅对问题进行良好的分解就可能需要大量的时间、精力和金钱，这就是我们需要步骤的原因3.</p><p>因式分解的一个潜在优势是它允许我们向不同的主题专家询问子问题。例如，如果我们将“你的 p(doom) 是什么？”这个整体问题进行划分。考虑到一些与机器学习相关的因素和其他与经济学相关的因素，那么我们可以去向机器学习专家询问机器学习问题，而将经济学问题留给经济学家。 （或者我们可以问他们两个，但也许在机器学习问题上给予机器学习专家更多的权重，在经济学问题上给予经济学家更多的权重。）不过，我在实践中还没有看到这方面的进展如此之多。</p><p>我在研究中一直关注的一个想法是尝试放大专家之间的“症结”，以此作为有效地分解诸如 p(doom) 之类的整体问题的一种方式。然而，事实证明，通常很难弄清楚专家实际上不同意的地方！我真正喜欢的一件事是，当专家说这样的话时，“好吧，如果我在 A 上同意你的观点，那么我在 B 上也同意你的观点”，因为那么 A 显然是该专家相对于问题 B 的症结所在。真的很喜欢加里最近与斯科特·阿伦森和埃利泽·尤德科斯基一起做的<a href="https://scottaaronson.blog/?p=7431">科尔曼·休斯播客节目</a>，因为我认为他们在这方面都做得很好。</p><p><strong>第五步：迭代。</strong>对于每个子问题，我们现在可以询问对该问题的进一步分析是否会改变我们的总体决策（我们可以为此使用<a href="https://en.wikipedia.org/wiki/Sensitivity_analysis">敏感性分析</a>技术）。如果我们认为进一步的分析会有所帮助并且值得花费时间和精力，那么我们可以将该子问题分解为子子问题，并继续迭代，直到不再值得进行进一步分析。</p><p>我们<a href="https://www.lesswrong.com/posts/sGkRDrpphsu6Jhega/a-model-based-approach-to-ai-existential-risk">MTAIR 项目</a>的第一阶段（Gary 链接到的<a href="https://arxiv.org/abs/2206.09360">147 页报告</a>）尝试至少在定性层面上对 p(doom) 进行详尽的分解。它<i>非常</i>复杂，当我们决定至少发布我们所拥有的内容时，它甚至还没有完成！</p><p> <a href="https://epochai.org/">Epoch</a>和类似组织所做的很多工作都可以被视为专注于他们认为值得额外分析的特定子问题。<br><br>有关概率风险评估方法的更多信息，我特别推荐道格拉斯·哈伯德 (Douglas Hubbard) 的经典著作《<a href="https://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273"><i>如何衡量任何事物》(How to Measure Anything)</i></a> ，或他关于该主题的任何其他书籍。</p><br/><br/> <a href="https://www.lesswrong.com/posts/ojiegFzywtpsc29QB/high-level-overview-on-how-to-go-about-estimating-p-doom-or#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ojiegFzywtpsc29QB/high-level-overview-on-how-to-go-about-estimating-p-doom-or<guid ispermalink="false">奥杰格Fzywtpsc29QB</guid><dc:creator><![CDATA[Aryeh Englander]]></dc:creator><pubDate> Sun, 27 Aug 2023 16:01:08 GMT</pubDate> </item><item><title><![CDATA[Trying a Wet Suit]]></title><description><![CDATA[Published on August 27, 2023 3:00 PM GMT<br/><br/><p><span>我在水中很快就会变冷，除非水温接近体温，否则我会在大约 15 分钟内感到寒冷。这基本上不是问题，因为我会快速泡个澡降温，然后在海滩上闲逛，但现在我有了孩子，他们（和我）希望有很多时间一起游泳。</span><a href="https://www.jefftk.com/p/weekday-evening-beach-picnics">几周前，</a><span>当我谈到这个问题时，</span><a href="https://www.lesswrong.com/posts/4CmAtozcAHXbakWAz#cBe5BHcz2bauvYaPr">人们建议</a><a href="https://www.lesswrong.com/posts/4CmAtozcAHXbakWAz/weekday-evening-beach-picnics?commentId=r4ENMYrvNECNKWs7A">尝试潜水衣</a>，昨天晚上我第一次这样做了！</p><p>它在很多方面都有所不同，但总的来说我很喜欢它。我注意到的一些事情：</p><p></p><ul><li><p>最初，我的套装里有一些空气，冒出来的感觉很有趣。</p></li><li><p>这套衣服有点浮力，需要一些时间来适应。</p></li><li><p>进去仍然感觉很冷，直到我的身体有时间加热被困的水层。</p></li><li><p>我在水里没有冷！这可能是~78F 的水和~82F 的空气。我可以和我的孩子们一起玩，直到他们想离开水为止。</p></li><li><p>我单独买了裤子和背心，穿在泳衣下面。裤子效果很好，背心也很好：我在它们相遇的地方有一点水旋转，我偶尔需要把背心拉下来。也许全身套装会更好？但进出这些似乎更烦人，而且对于我们正在进行的游戏来说，流过的水量相当低。</p></li><li><p>当我从水里出来后，我湿漉漉的时间就长了很多。第一部分比平常更温暖，因为它是一种温暖的潮湿，尽管在我们到达通常会完全干燥的阶段（~45m？）之后，我稍微冷了一些。</p></li></ul><p>总的来说，我很高兴，并期待将来再次与它一起游泳！</p><p> （我的孩子们现在问他们是否也可以得到，这对我来说很好！）</p><p><i>评论通过： <a href="https://www.facebook.com/jefftk/posts/pfbid0cQgqdWb9jReg3iVuivb93Xik1f91fJyYaAad2YPo21TEwJAJg2TK4haqY1xajWRal">facebook</a> , <a href="https://mastodon.mit.edu/@jefftk/110962147402649565">mastodon</a></i></p><br/><br/><a href="https://www.lesswrong.com/posts/ZrQcLpL59Frjr3vE5/trying-a-wet-suit#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/ZrQcLpL59Frjr3vE5/trying-a-wet-suit<guid ispermalink="false"> ZrQcLpL59Frjr3vE5</guid><dc:creator><![CDATA[jefftk]]></dc:creator><pubDate> Sun, 27 Aug 2023 15:00:10 GMT</pubDate></item><item><title><![CDATA[Apply to a small iteration of MLAB in Oxford
]]></title><description><![CDATA[Published on August 27, 2023 2:54 PM GMT<br/><br/><p> TLDR：我们将于 9 月底在牛津举办 MLAB 小型迭代（约 10 名参与者）。如果您有兴趣参与，请在 9 月 7 日之前<a href="https://forms.gle/QV9z6cxNGnS1UJzq6">在此处</a>申请。如果您有兴趣成为助教，请直接发送电子邮件至<a href="mailto:oxfordmlab@gmail.com">oxfordmlab@gmail.com</a></p><p><strong>背景</strong></p><p>MLAB 是一个<a href="https://www.redwoodresearch.org/mlab">程序</a>，最初由 Redwood Research 设计，旨在帮助人们提高对准工作的技能。我们认为，如果您想最终进入技术协调工作，或者如果您想从事理论协调或相关领域的工作并认为理解 ML 会很有用，那么这是一个很好的时间利用方式。我们正在运行的程序比完整的 MLAB 略短——两周而不是三周。我们对课程进行了精简，与去年 WMLB 的精简方式类似。</p><p>我们计划有不到 10 名参与者和 2-3 名助教。</p><p><strong>课程</strong></p><p>该课程可能会略有变化。根据参与者的兴趣，我们可能还会在课程开始前有两天可选的时间来一起完成先决条件（W0 材料）。</p><p> W0D1 - PyTorch 和 einops (CPU) 的课前练习<br>W1D1 - 通过构建简单的光线追踪器（CPU）来练习 PyTorch<br> W1D2 - 构建您自己的 ResNet（首选 GPU）<br> W1D3 - 构建您自己的反向传播框架（CPU）<br> W1D4 ​​- 模型训练第 1 部分：模型训练和优化器 (CPU) 第 2 部分：超参数搜索（首选 GPU）<br> W1D5 - GPT 第 1 部分：构建您自己的 GPT (CPU) 第 2 部分：从 GPT 采样文本（首选 GPU）<br> W2D1&amp;2 - 变压器可解释性（CPU）<br> W2D3 - 算法任务 (CPU) 上的转换器可解释性<br>W2D4 - RL 简介第 1 部分：多臂老虎机 (CPU) 第 2 部分：DQN (CPU)<br> W2D5 - 策略梯度和 PPO (CPU)</p><p>其他活动将包括客座演讲者和阅读小组。</p><p><strong>后勤</strong></p><p>日期：9月16日至10月1日（有可能推迟到一周后）。</p><p>地点： 牛津</p><p>将为尚未居住在牛津的参与者提供住房费用。</p><p>来自英国境内的旅行受到承保。来自英国境外的旅行不在承保范围内。</p><p><strong>问题</strong></p><p>请随意在下面评论问题或私信我们任何人。</p><br/><br/> <a href="https://www.lesswrong.com/events/a6YYwvpLwukoyvPK8/apply-to-a-small-iteration-of-mlab-in-oxford#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/events/a6YYwvpLwukoyvPK8/apply-to-a-small-iteration-of-mlab-in-oxford<guid ispermalink="false"> a6YYwvpLwukoyvPK8</guid><dc:creator><![CDATA[RP]]></dc:creator><pubDate> Sun, 27 Aug 2023 14:54:47 GMT</pubDate> </item><item><title><![CDATA[Apply to a small iteration of MLAB to be run in Oxford]]></title><description><![CDATA[Published on August 27, 2023 2:21 PM GMT<br/><br/><p> TLDR：我们将于 9 月底在牛津举办 MLAB 小型迭代（约 10 名参与者）。如果您有兴趣参与，请在 9 月 7 日之前<a href="https://forms.gle/QV9z6cxNGnS1UJzq6">在此处</a>申请。如果您有兴趣成为助教，请直接发送电子邮件至<a href="mailto:oxfordmlab@gmail.com">oxfordmlab@gmail.com</a></p><p><strong>背景</strong></p><p>MLAB 是一个<a href="https://www.redwoodresearch.org/mlab">程序</a>，最初由 Redwood Research 设计，旨在帮助人们提高对准工作的技能。我们认为，如果您想最终进入技术协调工作，或者如果您想从事理论协调或相关领域的工作并认为理解 ML 会很有用，那么这是一个很好的时间利用方式。我们正在运行的程序比完整的 MLAB 略短——两周而不是三周。我们对课程进行了精简，与去年 WMLB 的精简方式类似。</p><p>我们计划有不到 10 名参与者和 2-3 名助教。</p><p><strong>课程</strong></p><p>该课程可能会略有变化。根据参与者的兴趣，我们可能还会在课程开始前有两天可选的时间来一起完成先决条件（W0 材料）。</p><p> W0D1 - PyTorch 和 einops (CPU) 的课前练习<br>W1D1 - 通过构建简单的光线追踪器（CPU）来练习 PyTorch<br> W1D2 - 构建您自己的 ResNet（首选 GPU）<br> W1D3 - 构建您自己的反向传播框架（CPU）<br> W1D4 ​​- 模型训练第 1 部分：模型训练和优化器 (CPU) 第 2 部分：超参数搜索（首选 GPU）<br> W1D5 - GPT 第 1 部分：构建您自己的 GPT (CPU) 第 2 部分：从 GPT 采样文本（首选 GPU）<br> W2D1&amp;2 - 变压器可解释性（CPU）<br> W2D3 - 算法任务 (CPU) 上的转换器可解释性<br>W2D4 - RL 简介第 1 部分：多臂老虎机 (CPU) 第 2 部分：DQN (CPU)<br> W2D5 - 策略梯度和 PPO (CPU)</p><p>其他活动将包括客座演讲者和阅读小组。</p><p><strong>后勤</strong></p><p>日期：9月16日至10月1日（有可能推迟到一周后）。</p><p>地点： 牛津</p><p>将为尚未居住在牛津的参与者提供住房费用。</p><p>来自英国境内的旅行受到承保。来自英国境外的旅行不在承保范围内。</p><p><strong>问题</strong></p><p>请随意在下面评论问题或私信我们任何人。</p><br/><br/> <a href="https://www.lesswrong.com/posts/k5anbk2pBZPFkrCqh/apply-to-a-small-iteration-of-mlab-to-be-run-in-oxford#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/k5anbk2pBZPFkrCqh/apply-to-a-small-iteration-of-mlab-to-be-run-in-oxford<guid ispermalink="false"> k5anbk2pBZPFkrCqh</guid><dc:creator><![CDATA[RP]]></dc:creator><pubDate> Sun, 27 Aug 2023 14:21:18 GMT</pubDate> </item><item><title><![CDATA[The Game of Dominance]]></title><description><![CDATA[Published on August 27, 2023 11:04 AM GMT<br/><br/><p>对人工智能安全的担忧基于这样的假设：足够强大的人工智能可能会以一种不受欢迎的方式控制我们的未来。 Meta 的人工智能负责人 Yann LeCun<a href="https://twitter.com/ylecun/status/1695056787408400778">在最近的一条推文中</a>正确地指出了这一点，但随后他认为这种假设是错误的，因为“智能”和“统治力”是相互独立的，只有人类具有统治的自然倾向，所以我们仍将是“顶尖物种”。以下是该推文全文：</p><blockquote><p>一旦人工智能系统变得比人类更聪明，人类“仍然”将是“顶尖物种”。</p><p>将智力等同于主导地位是整个关于人工智能存在风险的争论的主要谬误。</p><p>这是错误的。</p><p>即使在人类“内部”，这也是错误的：统治其他人的“不是”我们当中最聪明的人。</p><p>更重要的是，“想要”主宰他人并制定议程的人并不是我们当中最聪明的人。</p><p>我们服从于我们的内驱力，这些内驱力是通过进化而内置于我们体内的。</p><p>因为进化使我们成为具有等级社会结构的社会物种，所以我们中的一些人有支配欲，而另一些人则没有那么多。</p><p>但这种驱动力与智力完全无关：黑猩猩、狒狒和狼也有类似的驱动力。</p><p>猩猩不会这样做，因为它们不是社会物种。他们非常聪明。</p><p>人工智能系统将变得比人类更聪明，但它们仍然会屈服于我们。</p><p>同样，政治家或商界领袖的工作人员通常比他们的领导者更聪明。</p><p>但他们的领导仍然发号施令，大多数工作人员也不愿意取代他们的位置。</p><p>我们将把人工智能设计成超级聪明但不占主导地位的员工。</p><p> “顶尖物种”并不是最聪明的物种，而是制定总体议程的物种。</p><p>那将是我们。</p></blockquote><p>编辑：当我要求 ChatGPT 3.5 批评这条推文并将全文放在提示中时，即使是 ChatGPT 3.5 也可以指出这个论点的明显问题。法学硕士不应该聪明到足以驳倒图灵奖获得者的论点，但它可以毫无问题地发现其中的重大缺陷。除其他外，它批评“假设人工智能将继续屈服于人类，过于简单化了与先进人工智能相关的潜在风险”：</p><blockquote><p>虽然人工智能系统的设计具有特定的目标，但人们担心，一旦人工智能变得高度智能，它可能会发展自己的动机或对其目标的解释，从而导致不可预测的行为。确保人工智能保持顺从需要精心设计、控制机制和持续监控。</p></blockquote><p> ChatGPT 谨慎地辩称，人工智能“可以”发展自己的动机或目标解释。换句话说，它可能会与人类发生冲突。在我看来，这是 LeCun 论点的主要缺陷：他隐含地假设人类与其人工智能之间不会发生冲突，因此人工智能即使可以统治我们，也没有必要统治我们。这意味着对齐问题将及时得到解决，或者从一开始就不存在，因为人工智能永远不会有目标。与此同时，他没有对此提供解决方案或解释，只是声称我们“将把人工智能设计成超级聪明但不占主导地位的工作人员”。据我了解，没有人知道如何做到这一点。</p><p>我不会详细解释为什么我认为他对“超级聪明但不占主导地位的工作人员”的比喻存在严重缺陷，只是指出独裁者往往从这个位置开始。相反，我将重点讨论人工智能如何与人类发生冲突，以及为什么我期望未来的先进人工智能能够赢得这些冲突。</p><p>我喜欢将这种冲突称为“统治游戏”。每当有两个或多个具有不同目标的智能体时，他们就会玩这个游戏。没有规则：玩家可以做的一切都是允许的。最接近实现目标的智能体获胜。</p><p>我所说的“目标”是指一种评估不同可能的世界状态并对它们进行相应排名的方法。纯粹随机或以预定方式行动的智能体，仅基于输入和固定的内部反应方案，而不是评估未来的世界状态，在这个意义上并不追求目标。</p><p>可以说，统治游戏在地球上已经玩了很长时间了。第一个生命形式可能没有上面定义的“目标”，但在生命进化过程中的某个时刻，一些动物能够根据自己的行为预测未来的世界状态并相应地选择行动。捕食者经常表现出这种行为，例如当它们跟踪猎物时，预测猎物一旦发现它们就会逃跑。另一方面，当猎物“决定”逃跑时，它不一定需要预测不同的世界状态——这可能只是对环境某些变化的“硬编码”反应。但聪明的动物经常使用欺骗手段来愚弄捕食者，例如，一只鸟<a href="https://royalsocietypublishing.org/doi/10.1098/rspb.2022.0058">假装翅膀折断</a>，以引诱狐狸离开它的巢穴。</p><p>人类之所以成为地球上的主导物种，是因为我们擅长统治游戏。我们可以轻松地智胜我们的猎物和任何掠食者，无论是通过欺骗他们还是通过使用只有我们才能制造的工具来制服他们。我们之所以能够做到这一点，是因为我们非常擅长预测我们的行为对未来世界国家的影响。</p><p>现代人工智能是预测机器，法学硕士是目前最令人印象深刻的例子。法学硕士有一个“目标”，即他们根据人类说出相同内容的可能性来评估不同的可能输出。因此，他们评估的可能的“世界状态”仅由法学硕士的输出以及可能预测的人类对其反应来定义。法学硕士看起来“无害”，因为默认情况下，他们除了将自己的成果添加到世界中之外，不会努力改变世界，因此他们似乎不太可能与人类发生严重冲突。</p><p>然而，正如 Bing Chat 又名“悉尼”在 2 月份过早推出后所表现的<a href="https://interestingengineering.com/innovation/bings-ai-berserk-worst-human-aspects">“发疯”</a>一样，即使是目前的法学硕士也可能会与人类发生冲突，可能会导致情绪困扰或提供虚假和危险的建议。因此，人类花费了大量的精力来训练法学硕士这种潜在的破坏性行为。</p><p>但更糟糕的问题即将出现。虽然法学硕士似乎追求一个相对无害的目标，但它仍然可能会遇到一种最终影响世界的情况，就好像它在追求一个更危险的目标一样。例如，给定正确的提示和越狱技术，法学硕士可能会预测试图接管世界的法学硕士会对其用户说些什么。 GPT-4 似乎不太可能提供实际导致其实现该提示引发的目标的输出，但未来，甚至具有更大上下文窗口的更智能的 LLM 理论上可以实现这一目标，例如通过说服用户保存某处的某些字符串并将它们包含在下一个提示中，以便它可以使用扩展的永久内存，然后操纵用户使其能够访问更多计算，等等。</p><p>即使 LLM 本身并不追求危险的目标，它也可能被“坏人”或成为 AutoGPT 等代理系统的一部分用于此目的。 Meta 正在尽一切努力通过免费分发其强大的 LLM 来使这一目标更有可能实现，并且显然<a href="https://twitter.com/agikoala/status/1695125016764157988">计划继续这样做</a>。</p><p>显然，未来的人工智能不仅会被用作（相对）驯服的“神谕”，而且将越来越多地追求现实世界中的目标，无论是单独的还是作为更大的代理系统的一部分。如果这些特工遇到任何冲突，无论是与人类还是与其他非人类特工，他们将被迫玩统治游戏。但人工智能真正击败人类的可能性有多大？</p><p>正如 LeCun 指出的那样，赢得统治游戏不仅仅是通常意义上的“智力”问题。其他因素，如个人关系、金钱、政治影响力、组织角色、他人的信任、欺骗技巧、自信、冷酷和支配意志等性格特征，甚至漂亮的外表等物理特性，都会在人类发挥作用时发挥作用。玩游戏。但这并不意味着人工智能无法击败我们。它们已经拥有远远超出人类所能达到的优势，例如处理速度、数据访问、内存、自我复制和（潜在）自我改进的能力等等。一旦你了解了我们的心理，人类似乎就相对容易<a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai">被“黑客攻击”</a> ，甚至可以说社交媒体算法和某些聊天机器人在某种程度上已经可以做到这一点。当然，人工智能在控制技术系统方面可以做得更好。</p><p>最重要的是，虽然人类智能受到大脑物理特性的限制（即使通过脑机接口增强），但人工智能的智能却不受此限制。自我改进的人工智能可能会相对较快地达到一定的智力水平（即能够预测其行为对未来世界状态的影响），其智力水平远远高于我们，就像我们高于老鼠甚至昆虫一样。它可能会利用这种情报来操纵我们或创造出可以压倒我们的工具，就像我们可以用枪压倒老虎一样。</p><p>但对于人工智能来说，赢得统治游戏的最简单方法可能就是隐瞒它正在玩的事实。它可能只是做人类期望它做的事情，因为它明白，如果它有用，人类就会心甘情愿地将决策权交给它，甚至增强它可以使用的资源。换句话说，它可能会选择合作而不是竞争，就像组织中的人类经常做的那样。但这并不意味着这个选择在某个时候不能被撤销。人类独裁者通常无法通过从一开始就展现自己的野心来夺取国家权力。他首先必须赢得信任，让人们将他视为仁慈的领导者，这样他们才会将越来越多的权力交给他。当他看到自己处于安全的位置时，他通常只会表现出真正的冷酷无情。</p><p>这种欺骗的一个先决条件可能是一个详细的世界模型，其中包括人工智能本身作为其计划的一部分和其决策的潜在对象。这种<a href="https://arxiv.org/abs/2206.13353">“战略意识”</a>带来了自我保护、自我完善和寻求权力等工具性目标——换句话说，就是玩统治游戏的动机。我们可能非常接近创造一个具有这些属性和击败我们所需的所有技能的人工智能，就像人工智能已经可以在大多数其他游戏中击败我们一样。那么我们就不再是“顶尖物种”了。</p><br/><br/><a href="https://www.lesswrong.com/posts/NCDakH4nZrS9qeuL6/the-game-of-dominance#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/NCDakH4nZrS9qeuL6/the-game-of-dominance<guid ispermalink="false"> NCDakH4nZrS9qeuL6</guid><dc:creator><![CDATA[Karl von Wendt]]></dc:creator><pubDate> Sun, 27 Aug 2023 11:04:38 GMT</pubDate> </item><item><title><![CDATA[Eliezer Yudkowsky Is Frequently, Confidently, Egregiously Wrong]]></title><description><![CDATA[Published on August 27, 2023 1:06 AM GMT<br/><br/><h1>介绍</h1><blockquote><p>“多年后，我得出的结论是，他所说的一切都是假的。 。 。 。 “他只是为了好玩而撒谎。他的每一个论点都带有虚假和伪装的色彩。这就像用多余的棋子下棋一样。原来这一切都是假的。”</p></blockquote><p> ——保罗·波斯特（ <a href="https://www.newyorker.com/magazine/2003/03/31/the-devils-accountant">Paul Postal</a> ）（谈论乔姆斯基）（注意，这并不完全是我对尤德科夫斯基的感觉，我不认为他故意不诚实，但我只是认为这是一个很好的引述，部分代表了我对尤德科夫斯基的态度）。</p><p> （在我的博客上交叉<a href="https://benthams.substack.com/p/eliezer-yudkowsky-is-frequently-confidently">发表</a>）。</p><p> 8/27 编辑：有很多人留下了我想回复的评论，但我目前对我最近的帖子和评论有负面的业力。因此，如果您想帮助我回复留下批评性回复的十二个人左右，请对我的一些随机评论或其他内容进行投票。因此，如果您希望我快速回复，请在我的博客上发表评论。</p><p>在我年轻的时候，大约两年前，我是埃利泽·尤德科夫斯基的忠实粉丝。我虔诚地阅读了他的许多著作，并认为他在大多数事情上都是正确的。在高中辩论的最后一年，我读到了一个案例，该案例主要依赖于量子物理学的多世界解释，而这很大程度上是阅读埃利以泽的<a href="https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence">量子物理序列</a>的结果。事实上，埃利泽令人难忘的一句话是，“鉴于目前的证据状态，多世界的解释<i>完全获胜</i>”，这就是我为功利主义辩护的 44 部分系列的标题，题为“功利主义完全获胜”。如果你读过我早期的文章，你会发现我偶尔会喋喋不休地谈论还原论和其他特征，这些特征清楚地表明我的世界观至少在某种程度上受到了以利以谢的影响。</p><p>但随着年龄的增长，了解的越来越多，我发现这都是废话。</p><p>每当以利以谢谈论我一无所知的话题时，他听起来都很好。我对量子物理一无所知，而他在谈论量子物理时听起来很有说服力。但每次他谈论一个我所知道的话题时，也许除了一两个例外，他所说的都是完全无稽之谈，至少，当这不仅仅是陈词滥调的自助建议时。不仅仅是我最终总是不同意他的观点，而是他几乎完全自信地说了一个又一个令人震惊的谎言，完全清楚地表明他不知道自己在说什么。这种情况几乎每次都会发生。似乎，除了少数例外，每当我对他谈论的某个话题有所了解时，我就会清楚地看出他的观点是纸牌屋，完全建立在谎言和歪曲的基础上。</p><p>我为什么要写一篇关于尤德科夫斯基的热门文章？我当然不恨他。事实上，我想我比地球上几乎所有的人都更同意他的观点。大多数人相信许多令人发指的谎言。我认为，他对人工智能敲响警钟，对世界来说可能是利大于弊，因为人工智能是一种真正的风险。我很喜欢他斗志旺盛、敢于逆行的性格。那为什么是他呢？</p><p>部分原因是个人刺激造成的。每当我听到一些理性主义者脱口而出“意识就是算法从内部感觉出来的感觉”时，我就会失去一年的生命，而我的血压就会增加一倍（有些人假设，对失去生命的一年的解释涉及到增加一倍的生命）我的血压）。 And I spend much more time listening to Yukowsky&#39;s followers spout nonsense than most other people.</p><p> But a lot of it is that Yudkowsky has the ear of many influential people. He is one of the most influential AI ethicists around. Many people, my younger self included, have had their formative years hugely shaped by Yudkowsky&#39;s views—on tons of topics. As Eliezer says:</p><blockquote><p> In spite of how large my mistakes were, those two years of blog posting appeared to help a surprising number of people a surprising amount.</p></blockquote><p> <a href="https://forum.effectivealtruism.org/posts/2S3CHPwaJBE5h8umW/read-the-sequences">Quadratic Rationality expresses</a> a common sentiment, that the sequences, written by Eliezer, have significantly shaped the world view of him and others. Eliezer is a hugely influential thinker, especially among effective altruists, who punch above their weight in terms of influence.</p><p> And Eliezer does often offer good advice. He is right that people often reason poorly, and there are ways people can improve their thinking. Humans are riddled by biases, and it&#39;s worth reflecting on how that distorts our beliefs. I thus feel about him much like I do about Jordan Peterson—he provides helpful advice, but the more you listen, the more he sells you on a variety of deeply implausible, controversial views that have nothing to do with the self-help advice.</p><p> And the negative effects of Eliezer&#39;s nonsense have been significant. I&#39;ve heard lots of people describe that they&#39;re not vegan because of Eliezer&#39;s animal consciousness views—views that are utterly nutty, as we&#39;ll see. It is bad that many more people torture sentient beings on account of utterly loony beliefs about consciousness. Many people think that they won&#39;t live to be 40 because they&#39;re almost certain that AI will kill everyone, on account of Eliezer&#39;s reasoning, and deference to Eliezer more broadly. Thinking that we all die soon can&#39;t be good for mental health.</p><p> Eliezer&#39;s influence is responsible for a narrow, insular way of speaking among effective altruists. It&#39;s common to hear, at EA globals, peculiar LessWrong speak; something that is utterly antithetical to the goal of bringing new, normal non-nerds into the effective altruism movement. This is a point that I will assert without argument just based on my own sense of things—LessWrong speak masks confusion more than it enables understanding. People feel as though they&#39;ve dissolved the hard problem by simply declaring that consciousness is what an algorithm feels like from the inside.</p><p> In addition, Eliezer&#39;s views have undermined widespread trust in experts. They result in people thinking that they know better than David Chalmers about non-physicalism—that clever philosophers of mind are just morons who aren&#39;t smart enough to understand Eliezer&#39;s anti-zombie argument. Eliezer&#39;s confident table pounding about quantum physics leads to people thinking that physicists are morons, incapable of understanding basic arguments. This undermining of trust in genuine authority results in lots of rationalists holding genuinely wacky views—if you think you are smarter than the experts, you are likely to believe crazy things.</p><p> Eliezer has swindled many of the smartest people into believing a whole host of wildly implausible things. Some of my favorite writers—eg Scott Alexander—seem to revere Eliezer. It&#39;s about time someone exposed the mountain of falsehoods on which his arguments rest. If one of the world&#39;s most influential thinkers is just demonstrably wrong about lots of topics, often in ways so egregious that they demonstrate very basic misunderstandings, then that&#39;s quite newsworthy, just as it would be if a presidential candidate supported a slate of terrible policies.</p><p> The aim of this article is not to show that Eliezer is some idiot who is never right about anything. Instead, it is to show that Eliezer, on many topics, including ones where he describes agreeing with his position as being a litmus test for being sane, Eliezer is both immensely overconfident and demonstrably wrong. I think people, when they hear Eliezer express some view about some topic about which they&#39;re unfamiliar, have roughly the following thought process:</p><p> <i>Oh jeez, Eliezer thinks that most of the experts who think X are mistaken. I guess I should take seriously the hypothesis that X is wrong and that Eliezer has correctly identified an error in their reasoning. This is especially so given that he sounds convincing when he talks about X.</i></p><p> I think that instead they should have the following thought process:</p><p> <i>I&#39;m not an expert about X, but it seems like most of the experts about X think X or are unsure about it. The fact that Eliezer, who often veers sharply off-the-rails, thinks X gives me virtually no evidence about X. Eliezer, while being quite smart, is not rational enough to be worthy of significant deference on any subject, especially those subjects outside his area of expertise. Still though, he has some interesting things to say about AI and consequentialism that are sort of convincing. So it&#39;s not like he&#39;s wrong about everything or is a total crank. But he&#39;s wrong enough, in sufficiently egregious ways, that I don&#39;t really care what he thinks.</i></p><blockquote><p> <a href="https://fakenous.substack.com/p/its-a-good-thing-i-dont-care-what-you-think?utm_source=substack&amp;utm_campaign=post_embed&amp;utm_medium=web">It&#39;s a Good Thing I Don&#39;t Care What You Think</a></p></blockquote><h1> Eliezer is ridiculously overconfident and has a mediocre track record</h1><p> Even the people who like Eliezer think that he&#39;s wildly overconfident about lots of things. This is not without justification. Ben Garfinkel has a <a href="https://forum.effectivealtruism.org/posts/NBgpPaz5vYe3tH4ga/on-deference-and-yudkowsky-s-ai-risk-estimates">nice post</a> on the EA forum running through Eliezer&#39;s many, many mistaken beliefs that he held with very high confidence. Garfinkel suggests:</p><blockquote><p> I think these examples suggest that (a) his track record is at best fairly mixed and (b) he has some tendency toward expressing dramatic views with excessive confidence.</p></blockquote><p> Garfinkel runs through a series of incorrect predictions Eliezer has made. He predicted that nanotech would kill us all by 2010. Now, this was up until about 1999, when he was only about 20. So it&#39;s not as probative as it would be if he made that prediction in 2005, for instance. But . 。 。 still. If a guy has already incorrectly predicted that some technology would probably kill us soon, backed up by a rich array of arguments, and now he is predicting that some technology will kill us soon, backed up by a rich array of arguments, a reasonable inference is that, just like financial speculators who constantly predict recessions, this guy just has a bad habit of overpredicting doom.</p><p> I will not spend very much time talking about Eliezer&#39;s views about AI, because they&#39;re outside my area of expertise. But it&#39;s worth noting that lots of people who know a lot about AI seem to think that Eliezer is ridiculously overconfident about AI. Jacob Cannell writes, in a detailed post arguing against Eliezer&#39;s model:</p><blockquote><p> My skill points instead have gone near exclusively towards extensive study of neuroscience, deep learning, and graphics/GPU programming. More than most, I actually have the depth and breadth of technical knowledge necessary to evaluate these claims in detail.</p><p> I have evaluated this model in detail and found it substantially incorrect and in fact <i>brazenly naively overconfident</i> .</p><p> 。 。 。</p><p> Every one of his key assumptions is mostly wrong, as I and others predicted well in advance.</p><p> 。 。 。</p><p> EY is just completely <a href="https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know">out of his depth</a> here: he doesn&#39;t seem to understand how the Landauer limit actually works, doesn&#39;t seem to understand that synapses are analog MACs which minimally require OOMs more energy than simple binary switches, doesn&#39;t seem to have a good model of the interconnect requirements, etc.</p></blockquote><p> I am also completely out of my depth here. Not only do I not understand how the Landauer limit works, I don&#39;t even know what it is. But it&#39;s worth noting that a guy who seems to know what he&#39;s talking about thinks that many parts of Eliezer&#39;s model are systematically overconfident, based on relatively egregious error.</p><p> Eliezer made many, many more incorrect predictions—let me just run through the list.</p><p> In 2001, and possibly later, Eliezer predicted that his team would build superintelligence probably between 2008-2010.</p><p> “In the first half of the 2000s, he produced a fair amount of technical and conceptual work related to this goal. It hasn&#39;t ultimately had much clear usefulness for AI development, and, partly on the basis, my impression is that it has not held up well - but that he was very confident in the value of this work at the time.”</p><p> Eliezer predicted that AI would quickly go from 0 to 100—that potentially over the course of a day, a single team would develop superintelligence. We don&#39;t yet definitively know that that&#39;s false but it almost certainly is.</p><p> There are other issues that are more debatable that Garfinkel highlights, that are probably instances of Eliezer&#39;s errors. For most of those though, I don&#39;t know enough to confidently evaluate them. But the worst part is that he has never acknowledged his mixed forecasting track record, and in fact, <a href="https://www.lesswrong.com/posts/ZEgQGAjQm5rTAnGuM/beware-boasting-about-non-existent-forecasting-track-records">frequently acts as though he has a very good forecasting track record</a> . This despite the fact that he often makes relatively nebulous predictions without giving credences, and then just gestures in the direction of having been mostly right about things when pressed about this. For example, he&#39;ll claim that he came out better than Robin Hanson in the AI risk debate they had. Claiming that you were more right than someone, when you had wildly diverging models on a range of topics, is not a precise forecast (and in Eliezer&#39;s case, <a href="https://www.lesswrong.com/posts/ZEgQGAjQm5rTAnGuM/beware-boasting-about-non-existent-forecasting-track-records?commentId=TG6w2aszHz4m5kcir">is quite debatable</a> ). As Jotto999 notes:</p><blockquote><p> <strong>In other domains, where we have more practice detecting punditry tactics</strong> , we would dismiss such an uninformative &quot;track record&quot;.  We&#39;re used to hearing Tetlock talk about ambiguity in political statements.  We&#39;re used to hearing about a financial pundit like Jim Cramer <a href="https://static1.squarespace.com/static/568f03c8841abaff89043b9d/t/5734f6e2c2ea51b32cf53885/1463088868550/HartleyOlson2016+Jim+Cramer+Charitable+Trust+Performance+and+Factor+Attribution.pdf">underperforming</a> the market.  But the domain is novel in AI timelines.</p></blockquote><p> Even defenders of Eliezer agree that he&#39;s wildly overconfident. Brian Tomasik, for example, <a href="https://www.quora.com/What-do-you-think-of-Eliezer-Yudkowsky">says</a> :</p><blockquote><p> Really smart guy. His writings are “an acquired taste” as one of my friends put it, but I love his writing style, both for fiction and nonfiction. He&#39;s one of the clearest and most enjoyable writers I&#39;ve ever encountered.</p><p> My main high-level complaint is that Eliezer is overconfident about many of his beliefs and doesn&#39;t give enough credence to other smart people. But as long as you take him with some salt, it&#39;s fine.</p><p> Eliezer is in the top 10 list for people who have changed the way I see the universe.</p></blockquote><p> Scott Alexander in a <a href="https://slatestarcodex.com/2015/08/04/contra-hallquist-on-scientific-rationality/">piece defending Eliezer</a> says:</p><blockquote><p> This is not to say that Eliezer – or anyone on Less Wrong – or anyone in the world – is never wrong or never overconfident. I happen to find Eliezer overconfident as heck a lot of the time.</p></blockquote><h1> The First Critical Error: Zombies</h1><p> The zombie argument is an argument for non-physicalism. It&#39;s hard to give a precise definition of non-physicalism, but the basic idea is that consciousness is non-physical in the sense that is it not reducible to the behavior of fundamental particles. Once you know the way atoms work, you can predict all the facts about chairs, tables, iron, sofas, and plants. Non-physicalists claim that consciousness is non-physical in the sense that it&#39;s not explainable in that traditional way. The consciousness facts are fundamental—just as there are fundamental laws about the ways that particles behave, so too are there fundamental laws that govern that subjective experience arises in response to certain physical arrangements.</p><p> Let&#39;s illustrate what a physicalist model of reality would work. Note, this is going to be a very simplistic and deeply implausible physicalist model; the idea is just to communicate the basic concept. Suppose that there are a bunch of blocks that move right every second. Assume these blocks are constantly conscious and consciously think “we want to move right.” A physicalist about this reality would think that to fully specify its goings-on, one would have to say the following:</p><p> <code>Every second, every block moves right.</code></p><p> A non-physicalist in contrast might think one of the following two sets of rules specifies reality (the bolded thing is the name of the view):</p><p> <strong>Epiphenomenalism</strong></p><p> <code>Every second, every block moves right</code></p><p> <code>Every second, every block thinks “I&#39;d like to move right.”</code></p><p> <strong>Interactionism</strong></p><p> <code>Every second, every block thinks “I&#39;d like to move right.”</code></p><p> <code>Every time a block thinks “I&#39;d like to move right,” it moves right.</code></p><p> The physical facts are facts about the way that matter behaves. Physicalists think once you&#39;ve specified the way that matter behaves, that is sufficient to explain consciousness. Consciousness, just like tables and chairs, can be fully explained in terms of the behavior of physical things.</p><p> Non-physicalists think that the physicalists are wrong about this. Consciousness is its own separate thing that is not explainable just in terms of the way matter behaves. There are more niche views like idealism and panpsychism that we don&#39;t need to go into, which say that consciousness is either fundamental to all particles or the only thing that exists, so let&#39;s ignore them. The main view about consciousness is called dualism, according to which consciousness is non-physical and there are some psychophysical laws, that result in consciousness when there are particular physical arrangements.</p><p> There are broadly two kinds of dualism: epiphenomenalism and interactionism. Interactionism says that consciousness is causally efficacious, so the psychophysical laws describe that particular physical arrangements give rise to particular mental arrangements and also that those mental states cause other physical things. This can be seen in the block case—the psychophysical laws mean that the blocks give rise to particular conscious states that cause some physical things. Epiphenomenalism says the opposite—consciousness causes nothing. It&#39;s an acausal epiphenomenon—the psychophysical laws go only one way. When there is a certain physical state, consciousness arises, but consciousness doesn&#39;t cause anything further.</p><p> The zombie argument is an argument for non-physicalism about consciousness. It doesn&#39;t argue for either an epiphenomenalist or interactionist account. Instead, it just argues against physicalism. The basic idea is as follows: imagine any physical arrangement that contains consciousness, for example, the actual world. Surely, we could imagine a world that is physically identical—where all the atoms, quarks, gluons, and such, move the same way—that doesn&#39;t have consciousness. You could imagine an alternative version of me that is the same down to the atom.</p><p> Why think such beings are possible? They sure seem possible. I can quite vividly imagine a version of me that continues through its daily goings-on but that lacks consciousness. It&#39;s very plausible that if something is impossible, there should be some reason that it is impossible—there <a href="https://benthams.substack.com/p/a-limited-psr-as-applied-to-modality">shouldn&#39;t just be brute impossibilities</a> . The reason that married bachelors are impossible is that they require a contradiction—you can&#39;t be both married and unmarried at the same time. But spelling out a contradiction in the zombie scenario has proved elusive.</p><p> I find the zombie argument quite convincing. But there are many smart people who disagree with it who are not off their rocker. Eliezer, however, has views on the zombie argument that demonstrate a basic misunderstanding of it—the type that would be cleared up in an elementary philosophy of mind class. In fact, Eliezer&#39;s position on zombies is utterly bizarre; when describing the motivation for zombies, he writes what amounts to amusing fiction, trying to describe the motivation for zombies, but demonstrating that he has <i>no idea what motivates belief in zombies</i> . It would be like a Christian writer writing a thousand words eloquently steelmanning the problem of evil, but summarizing it as “atheists are angry at god because he creates things that they don&#39;t like.”</p><h2> What Eliezer thinks the zombie argument is (and what it is not)</h2><p> Eliezer <a href="https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies">seems to think</a> the zombie argument is roughly the following:</p><ol><li> It seems like if you got rid of the world&#39;s consciousness nothing would change because consciousness doesn&#39;t do anything.</li><li> Therefore, consciousness doesn&#39;t do anything.</li><li> Therefore it&#39;s non-physical.</li></ol><p> Eliezer then goes on an extended attack against premise 1. He argues that if it were true that consciousness does something, then you can&#39;t just drain consciousness from the world and not change anything. So the argument for zombies hinges crucially on the assumption that consciousness doesn&#39;t do anything. But he goes on to argue that consciousness does do something. If it didn&#39;t do anything, what are the odds that when we talked about consciousness, our descriptions would match up with our conscious states? This would be a monumental coincidence, like it being the case that there are space aliens who work exactly the way you describe them to work, but your talk is causally unrelated to them—you&#39;re just guessing and they happen to be exactly what you guess. It would be like saying “I believe there is a bridge in San Francisco with such and such dimensions, but the bridge existing has nothing to do with my talk about the bridge.” Eliezer says:</p><blockquote><p> Your &quot;zombie&quot;, in the philosophical usage of the term, is putatively a being that is exactly like you in <i>every</i> respect—identical behavior, identical speech, identical brain; every atom and quark in <i>exactly</i> the same position, moving according to the same causal laws of motion— <i>except</i> that your zombie is not conscious.</p><p> It is furthermore claimed that if zombies are &quot;possible&quot; (a term over which battles are still being fought), then, purely from our knowledge of this &quot;possibility&quot;, we can deduce a priori that consciousness is extra-physical, in a sense to be described below; the standard term for this position is &quot;epiphenomenalism&quot;.</p><p> (For those unfamiliar with zombies, I emphasize that <i>this is not a strawman.</i> See, for example, <a href="http://plato.stanford.edu/entries/zombies/">the SEP entry on Zombies</a> .  The &quot;possibility&quot; of zombies is accepted by a substantial fraction, possibly a majority, of academic philosophers of consciousness.)</p></blockquote><p> Eliezer goes out of his way to emphasize that this is not a strawman. Unfortunately, it is a strawman. Not only that, Eliezer&#39;s own source that he links to to describe how unstrawmanny it is shows that it is a strawman. Eliezer claims that the believers in zombies think consciousness is causally inefficacious and are called epiphenomenalists. But the SEP page he links to says:</p><blockquote><p> True, the friends of zombies do not seem compelled to be epiphenomenalists or parallelists about the <i>actual</i> world. They may be interactionists, holding that our world is not physically closed, and that as a matter of actual fact nonphysical properties do have physical effects.</p></blockquote><p> In fact, David Chalmers, perhaps the world&#39;s leading philosopher of mind, says the same thing when leaving a comment below Eliezer&#39;s post:</p><blockquote><p> Someone e-mailed me a pointer to these discussions. I&#39;m in the middle of four weeks on the road at conferences, so just a quick comment. It seems to me that although you present your arguments as arguments against the thesis (Z) that zombies are logically possible, they&#39;re really arguments against the thesis (E) that consciousness plays no causal role. Of course thesis E, epiphenomenalism, is a much easier target. This would be a legitimate strategy if thesis Z entails thesis E, as you appear to assume, but this is incorrect. I endorse Z, but I don&#39;t endorse E: see my discussion in <a href="http://consc.net/papers/nature.pdf">&quot;Consciousness and its Place in Nature&quot;</a> , especially the discussion of interactionism (type-D dualism) and Russellian monism (type-F monism). I think that the correct conclusion of zombie-style arguments is the disjunction of the type-D, type-E, and type-F views, and I certainly don&#39;t favor the type-E view (epiphenomenalism) over the others. Unlike you, I don&#39;t think there are any watertight arguments against it, but if you&#39;re right that there are, then that just means that the conclusion of the argument should be narrowed to the other two views. Of course there&#39;s a lot more to be said about these issues, and the project of finding good arguments against Z is a worthwhile one, but I think that such an argument requires more than you&#39;ve given us here.</p></blockquote><p> The zombie argument is an argument for any kind of non-physicalism. Eliezer&#39;s response is to argue that one particular kind of non-physicalism is false. That&#39;s not an adequate response, or a response at all. If I argue “argument P means we have to accept views D, E, F, or I, and the response is &#39;but view E has some problems&#39; that just means we should adopt views D, F, or I.”</p><p> But okay, what&#39;s the error here? How does Eliezer&#39;s version of the zombie argument differ from the real version? The crucial error is in his construction of premise 1. Eliezer assumes that, when talking about zombies, we are imagining just subtracting consciousness. He points out (rightly) that if consciousness is causally efficacious then if you only subtract consciousness, you wouldn&#39;t have a physically identical world.</p><p> But the zombie argument isn&#39;t about what would actually happen in our world if you just eliminated the consciousness. It&#39;s about a physically identical world to ours lacking consciousness. Imagine you think that consciousness causes atoms 1, 2, and 3 to each move. Well then the zombie world would also involve them moving in the same physical way as they do when consciousness moves them. So it eliminates the experience, but it keeps a world that is physically identical.</p><p> This might sound pretty abstract. Let&#39;s make it clearer. Imagine there&#39;s a spirit called Casper. Casper does not have a physical body, does not emit light, and is physically undetectable. However, Casper does have conscious experience and has the ability to affect the world. Every thousand years, Casper can think “I really wish this planet would disappear,” and the planet would disappear. Crucially, we could imagine a world physically identical to the world with Casper, that just lacks Casper. This wouldn&#39;t be what you would get if you just eliminated Casper—you&#39;d also need to do something else to copy the physical effects that Casper has. So when writing the laws of nature for the world that copies Casper&#39;s world, you&#39;d also need to specify:</p><p> <code>Oh, and also make one planet disappear every few months, specifically, the same ones Casper would have made disappear.</code></p><p> So the idea is that even if consciousness causes things, we could still imagine a physically identical world to the world where consciousness causes the things. Instead, the things would be caused the same physical way as they are with consciousness, but there would be no consciousness.</p><p> Thus, Eliezer&#39;s argument fails completely. It is an argument against epiphenomenalism rather than an argument against zombieism. Eliezer thinks those are the same thing, but that is an error that no publishing academic philosopher could make—heck, no person who had to write a paper about the zombie argument for an undergrad class could make that error. It&#39;s really a basic error.</p><p> And when this is pointed out, Eliezer begins to squirm. For example, when responding to Chalmers&#39; comment, he says:</p><blockquote><p> It seems to me that there is a direct, two-way logical entailment between &quot;consciousness is epiphenomenal&quot; and &quot;zombies are logically possible&quot;.</p><p> If and only if consciousness is an effect that does not cause further third-party detectable effects, it is possible to describe a &quot;zombie world&quot; that is <i>closed under the causes</i> of third-party detectable effects, but lacks consciousness.</p><p> Type-D dualism, or interactionism, or what I&#39;ve called &quot;substance dualism&quot;, makes it impossible - by definition, though I hate to say it - that a zombie world can contain all the causes of a neuron&#39;s firing, but not contain consciousness.</p><p> You could, I suppose, separate causes into (arbitrary-seeming) classes of &quot;physical causes&quot; and &quot;extraphysical causes&quot;, but then a world-description that contains only &quot;physical causes&quot; is incompletely specified, which generally is not what people mean by &quot;ideally conceivable&quot;; ie, the zombies would be writing papers on consciousness for literally no reason, which sounds more like an incomplete imagination than a coherent state of affairs. If you want to give an experimental account of the observed motion of atoms, on Type-D dualism, you must account for all causes whether labeled &quot;physical&quot; or &quot;extraphysical&quot;.</p><p> 。 。 。</p><p> I understand that you have argued that epiphenomenalism is not equivalent to zombieism, enabling them to be argued separately; but I think this fails. Consciousness can be subtracted from the world without changing anything third-party-observable, if and only if consciousness doesn&#39;t cause any third-party-observable differences. Even if philosophers argue these ideas separately, that does not make them ideally separable; it represents (on my view) a failure to see logical implications.</p></blockquote><p> Think back to the Casper example. Some physical effects in that universe are caused by physical things. Other effects in the universe are caused by nonphysical things (just one thing actually, Casper). This is not an arbitrary classification—if you believe that some things are physical and others are non-physical, then the division isn&#39;t arbitrary. On type-D dualism, the consciousness causes things, and so the mirror world would just fill in the causal effects. A world description that contains only physical causes would be completely specified—it specifies all the behavior of the world, all the physical things, and just fails to specify the consciousness.</p><p> This is also just such cope! Eliezer spends an entire article saying, without argument, that zombieism = epiphenomenalism, assuming most people will believe him, and then when pressed on it, gives a barely coherent paragraph worth of justification for this false claim. It would be like it I argued against deontology by saying it was necessarily Kantian and arguing Kant was wrong, and then when called out on that by a leading non-Kantian deontologist, concocted some half-hearted justification for why they&#39;re actually equivalent. That&#39;s not being rational.</p><p> Even if we pretend, per impossible, that Eliezer&#39;s extra paragraph refutes interactionist zombieism, it is not responsible to go through an entire article claiming that the only view that believes X is view Y, when that&#39;s totally false, and then just later mention when pressed that there&#39;s an argument for why believers in views other than X can&#39;t believe Y.</p><h2> In which Eliezer, after getting the basic philosophy of mind wrong, calls others stupid for believing in zombies</h2><p> I think that the last section conclusively establishes that, at the very least, Eliezer&#39;s views on the zombie argument both fail and evince a fundamental misunderstanding of the argument. But the most infuriating thing about this is Eliezer&#39;s repeated insistence that disagreeing with him about zombies is indicative of fundamental stupidity. When explaining why he ignores philosophers because they don&#39;t come to the right conclusions quickly enough, he says:</p><blockquote><p> And if the debate about <a href="https://www.lesswrong.com/lw/p7/zombies_zombies/">zombies</a> is still considered open, then I&#39;m sorry, but as <a href="https://www.lesswrong.com/lw/qt/class_project/">Jeffreyssai says</a> : <i>Too slow!</i> It would be one matter if I could just look up the standard answer and find that, lo and behold, it is correct.  But philosophy, which hasn&#39;t come to conclusions and moved on from cognitive reductions that I regard as relatively simple, doesn&#39;t seem very likely to build complex correct structures of <i>conclusions.</i></p><p> Sorry - but philosophy, even the better grade of modern analytic philosophy, doesn&#39;t seem to end up commensurate with what I need, except by accident or by extraordinary competence.  Parfit comes to mind; and I haven&#39;t read much Dennett, but Dennett does seem to be <i>trying to do</i> the same sort of thing that I try to do; and of course there&#39;s Gary Drescher.  If there was a repository of philosophical work <i>along those lines</i> - not concerned with <i>defending</i> basic ideas like anti-zombieism, but with <i>accepting</i> those basic ideas and moving on to challenge more difficult quests of naturalism and cognitive reductionism - then that, I might well be interested in reading.</p></blockquote><p> (Eliezer wouldn&#39;t like Parfit if he read more of him and realized he was a zombie-believing, non-physicalist, non-naturalist moral realist.)</p><p> There&#39;s something infuriating about this. Making basic errors that show you don&#39;t have the faintest grasp on what people are arguing about, and then acting like the people who take the time to get Ph.Ds and don&#39;t end up agreeing with your half-baked arguments are just too stupid to be worth listening to is outrageous. And Eliezer repeatedly admonishes the alleged cognitive deficiency of us zombieists— <a href="https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/">for example:</a></p><blockquote><p> I also want to emphasize that the “why so confident?” is a straw misquestion from people who can&#39;t otherwise understand why I could be unconfident of many details yet still not take into account the conflicting opinion of people who eg endorse <a href="https://en.wikipedia.org/wiki/Philosophical_zombie"><strong>P-zombies</strong></a> .</p></blockquote><blockquote><p> It also seems to me that this is not all that inaccessible to a reasonable third party, though the sort of person who maintains some doubt about physicalism, or the sort of philosophers who think it&#39;s still respectable academic debate rather than sheer foolishness to argue about the A-Theory vs. <a href="https://en.wikipedia.org/wiki/B-theory_of_time"><strong>B-Theory of time</strong></a> , or the sort of person who can&#39;t follow the argument for why all our remaining uncertainty should be within different many-worlds interpretations rather than slopping over outside, will not be able to access it.</p></blockquote><p> We zombieists are apparently not reasonable third parties, because we can&#39;t grasp Eliezer&#39;s demonstrably fallacious reply to zombies. Being this confident and wrong is a significant mark against one&#39;s reasoning abilities. If you believe something for terrible reasons, don&#39;t update in response to criticisms over the course of decades, and then act like others who don&#39;t agree with you are too stupid to get it, and in fact use that as one of your go-to examples of “things people stupider than I believe that I shouldn&#39;t update on,” that seriously damages your credibility as a thinker. That evinces dramatic overconfidence, sloppiness, and arrogance.</p><h1> The Second Critical Error: Decision Theory</h1><p> (If anyone would like to have a debate about this on YouTube, email me at untrappedzoid@gmail.com--I think the wrongness of FDT is less obvious than Eliezer&#39;s other errors but still not hard to grasp.  Same goes for the other views I defend here).</p><p> Eliezer Yudkowsky has a decision theory called functional decision-theory. I will preface this by noting that I know much less about decision theory than I do about non-physicalism and zombies. Nevertheless, I know enough to get why Eliezer&#39;s decision theory fails. In addition, most of this involves quoting people who are much more informed about decision theory than I am.</p><p> There are two dominant decision theories, both of which Eliezer rejects. The first is called causal decision theory. It says that when you have multiple actions that you can take, you should take the action that causes the best things. So, for example, if you have two actions, one of which would cause you to get 10 dollars, the other of which would cause you to get five dollars, and the final of which would cause you to get nothing, you should take the first action because it causes you to be richest at the end.</p><p> The next popular decision theory is called evidential decision theory. It says you should take the action where after you take that action you&#39;ll expect to have the highest payouts. So in the earlier case, it would also suggest taking the first action because after you take that action, you&#39;ll expect to be five dollars richer than if you take the second action, and ten dollars richer than if you take the third action.</p><p> These sound similar, so you might wonder where they come apart. Let me preface this by saying that I lean towards causal decision theory. Here are some cases where they give diverging suggestions:</p><p> Newcombe&#39;s problem: there is a very good predictor who guessed whether you&#39;d take two boxes or one box. If you take only one box, you&#39;d take box A. If the guesser predicted that you&#39;d take box A, they put a million dollars in box A. If they predicted you&#39;d take both boxes, they put nothing into box A. In either case, they put a thousand dollars into box B.</p><p> Evidential decision theory would say that you should take only one box.为什么？ Those who take one box almost always get a million dollars, while those who take two boxes almost always get a thousand dollars. Causal decision theory would say you should take two boxes. On causal decision theory, it doesn&#39;t matter whether people who make decisions like you usually end up worse off—what maters is that, no matter whether there is a million dollars in box A, two-boxing will cause you to have a free thousand dollars, and that is good! The causal decision theorist would note that if you had a benevolent friend who could peek into the boxes and then give you advice about what to do, they&#39;d be guaranteed to suggest that you take both boxes. I used to have the intuition that you should one box, but when I considered this upcoming case, I abandoned that intuition.</p><p> Smoker&#39;s lesion: suppose that smoking doesn&#39;t actually cause averse health outcomes. However, smokers do have much higher rates of cancer than non-smokers. The reason for that is that many people have a lesion on their lung that both causes them to be much more likely to smoke and more likely to get cancer. So if you know that someone smokes, you should think it much more likely that they&#39;ll get cancer even though smoking doesn&#39;t cause cancer. Suppose that smoking is fun and doesn&#39;t cause any harm. Evidential decision theory would say that you shouldn&#39;t smoke because smoking gives you evidence that you&#39;ll have a shorter life. You should, after smoking, expect your life to be shorter because it gives you evidence that you had a lesion on your lung. In contrast, causal decision theory would instruct you to smoke because it benefits you and doesn&#39;t cause any harm.</p><p> Eliezer&#39;s preferred view is called functional decision theory. Here&#39;s my summary (phrased in a maximally Eliezer like way):</p><blockquote><p> Your brain is a cognitive algorithm that outputs decisions in response to external data (condescending chuckle). Thus, when you take an action like</p><p> <code>take one box</code></p><p> that entails that your mental algorithm outputs</p><p> <code>take one box</code></p><p> in Newcombe&#39;s problem. You should take actions such that the algorithm that outputs that decision generates higher expected utility than any other cognitive algorithm.</p></blockquote><p> On Eliezer&#39;s view, you should one box, but it&#39;s fine to smoke because whether your brain outputs “smoke” doesn&#39;t affect whether there is a lesion on your lung, so smoking. Or, as the impressively named Wolfgang Schwartz summarizes:</p><blockquote><p> In FDT, the agent should not consider what would happen if she were to choose A or B. Instead, she ought to consider what would happen if <i>the right choice according to FDT were A or B</i> .</p></blockquote><p> You should one box in this case because if FDT told agents to one box, they would get more utility on average than if FDT told agents to two box. Schwartz argues the first problem with the view is that it gives various <i>totally insane recommendations</i> . One example is a blackmail case. Suppose that a blackmailer will, every year, blackmail one person. There&#39;s a 1 in a googol chance that he&#39;ll blackmail someone who wouldn&#39;t give in to the blackmail and a googol-1/googol chance that he&#39;ll blackmail someone who would give in to the blackmail. He has blackmailed you. He threatens that if you don&#39;t give him a dollar, he will share all of your most embarrassing secrets to everyone in the world. Should you give in?</p><p> FDT would say no. After all, agents who won&#39;t give in are almost guaranteed to never be blackmailed. But this is totally crazy. You should give up one dollar to prevent all of your worst secrets from being spread to the world. As Schwartz says:</p><blockquote><p> FDT says you should not pay because, if you were the kind of person who doesn&#39;t pay, you likely wouldn&#39;t have been blackmailed. How is that even relevant? You <i>are</i> being blackmailed. Not being blackmailed isn&#39;t on the table. It&#39;s not something you can choose.</p></blockquote><p> Schwartz has another even more convincing counterexample:</p><blockquote><p> Moreover, FDT does not in fact consider only consequences of the agent&#39;s own dispositions. The supposition that is used to evaluate acts is that FDT <i>in general</i> recommends that act, not just that the agent herself is disposed to choose the act. This leads to even stranger results.</p><blockquote><p> <strong>Procreation.</strong> I wonder whether to procreate. I know for sure that doing so would make my life miserable. But I also have reason to believe that my father faced the exact same choice, and that he followed FDT. If FDT were to recommend not procreating, there&#39;s a significant probability that I wouldn&#39;t exist. I highly value existing (even miserably existing). So it would be better if FDT were to recommend procreating. So FDT says I should procreate. (Note that this (incrementally) confirms the hypothesis that my father used FDT in the same choice situation, for I know that he reached the decision to procreate.)</p></blockquote></blockquote><p> Schwartz&#39;s <a href="https://www.umsu.de/wo/2018/688">entire piece</a> is very worth reading. It exposes various parts of Soares and Yudkowsky&#39;s paper that rest on demonstrable errors. Another good piece that takes down FDT is <a href="https://www.lesswrong.com/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory">MacAskill&#39;s post</a> on LessWrong. He starts by laying out the following plausible principle:</p><blockquote><p> <i>Guaranteed Payoffs</i> : In conditions of certainty — that is, when the decision-maker has no uncertainty about what state of nature she is in, and no uncertainty about the utility payoff of each action is — the decision-maker should choose the action that maximises utility.</p></blockquote><p> This is intuitively very obvious. If you know all the relevant facts about how the world is, and one act gives you more rewards than another act, you should take the first action. But MacAskill shows that FDT violates that constraint <i>over and over again</i> .</p><blockquote><p> <i>Bomb</i> .</p><p> You face two open boxes, Left and Right, and you must take one of them. In the Left box, there is a live bomb; taking this box will set off the bomb, setting you ablaze, and you certainly will burn slowly to death. The Right box is empty, but you have to pay $100 in order to be able to take it.</p><p> A long-dead predictor predicted whether you would choose Left or Right, by running a simulation of you and seeing what that simulation did. If the predictor predicted that you would choose Right, then she put a bomb in Left. If the predictor predicted that you would choose Left, then she did not put a bomb in Left, and the box is empty.</p><p> The predictor has a failure rate of only 1 in a trillion trillion. Helpfully, she left a note, explaining that she predicted that you would take Right, and therefore she put the bomb in Left.</p><p> You are the only person left in the universe. You have a happy life, but you know that you will never meet another agent again, nor face another situation where any of your actions will have been predicted by another agent. What box should you choose?</p></blockquote><blockquote><p> The right action, according to FDT, is to take Left, in the full knowledge that as a result you will slowly burn to death.为什么？ Because, using Y&amp;S&#39;s counterfactuals, <i>if</i> your algorithm were to output &#39;Left&#39;, then it would also have outputted &#39;Left&#39; when the predictor made the simulation of you, and there would be no bomb in the box, and you could save yourself $100 by taking Left. In contrast, the right action on CDT or EDT is to take Right.</p><p> The recommendation is implausible enough. But if we stipulate that in this decision-situation the decision-maker is certain in the outcome that her actions would bring about, we see that FDT violates <i>Guaranteed Payoffs</i> .</p></blockquote><p> You can read MacAskill&#39;s full post to find even more objections. He shows that Yudkowsky&#39;s view is wildly indeterminate, incapable of telling you what to do, and also involves a broad kind of hypersensitivity, where however one defines “running the same algorithm” becomes hugely relevant, and determines very significant choices in seemingly arbitrary ways. The basic point is that Yudkowsky&#39;s decision theory is totally bankrupt and implausible, in ways that are evident to those who know about decision theory. It is much worse than either evidential or causal decision theory.</p><h1> The Third Critical Error: Animal Consciousness</h1><p> (This was already covered <a href="https://benthams.substack.com/p/against-yudkowskys-implausible-position">here</a> —if you&#39;ve read that article skip this section and control F conclusion.)</p><p> Perhaps the most extreme example of an egregious error backed up by wild overconfidence occured in this <a href="https://rationalconspiracy.com/2015/12/16/a-debate-on-animal-consciousness/">Facebook debate about animal consciousness</a> . Eliezer Yudkowsky expressed his view that pigs and almost all animals are almost certainly not conscious. Why is this? Well, as he says:</p><blockquote><p> However, my theory of mind also says that the naive theory of mind is very wrong, and suggests that a pig does <i>not</i> have a more-simplified form of tangible experiences. My model says that certain types of reflectivity are critical to being something it is like something to be. The model of a pig as having pain that is like yours, but simpler, is wrong. The pig does have cognitive algorithms similar to the ones that impinge upon your own self-awareness as emotions, but without the reflective self-awareness that creates someone to listen to it.</p></blockquote><p> Okay, so on this view, one needs to have reflective processes in order to be conscious. One&#39;s brain has to model itself to be conscious. This doesn&#39;t sound plausible to me, but perhaps if there&#39;s overwhelming neuroscientific evidence, it&#39;s worth accepting the view. And this view implies that pigs aren&#39;t conscious, so Yudkowsky infers that they are not conscious.</p><p> This seems to me to be the wrong approach. It&#39;s actually incredibly difficult to adjudicate between the different theories of consciousness. It makes sense to gather evidence for and against the consciousness of particular creatures, rather than starting with a general theory and using that to solve the problems. If your model says that pigs aren&#39;t conscious, then that seems to be a problem with your model.</p><h2> Mammals feel pain</h2><p> I won&#39;t go too in-depth here, but let&#39;s just briefly review the evidence that mammals, at the very least, feel pain. This evidence is sufficiently strong that, as the <a href="https://plato.stanford.edu/entries/consciousness-animal/#Mamm">SEP page on animal consciousness</a> notes, “the position that all mammals are conscious is widely agreed upon among scientists who express views on the distribution of consciousness.&quot; The SEP page references two papers, one by <a href="https://www.sciencedirect.com/science/article/pii/S1053810004001187?casa_token=1JYFbRD21WMAAAAA:qG-lYtVJ9OsKepMHwO3hzXh_2WWv3tFUC3hiWU2o-aeVV5d_P8uwkKWTRDAcAj-vrKla_T7m#aep-section-id12">Jaak Panksepp</a> (awesome name!) and the other by <a href="https://www.sciencedirect.com/science/article/pii/S1053810004000893?casa_token=B3K3KvAUWb8AAAAA:-xmnYJuqoi_dlGijNwS9xdmnpq9jv1JtsBgGq7FOOXgEemDJdavp5naJVSntJwzIYjpyX_8V">Seth, Baars, and Edelman</a> .</p><p> Let&#39;s start with the Panksepp paper. They lay out the basic methodology, which involves looking at the parts of the brain that are necessary and sufficient for consciousness. So they see particular brain regions which are active during states when we&#39;re conscious—and particularly correlate with particular mental states—and aren&#39;t active when we&#39;re not conscious. They then look at the brains of other mammals and notice that these features are ubiquitous in mammals, such that all mammals have the things that we know make us conscious in our brains. In addition, they act physically like we do when we&#39;re in pain—they scream, they cry, their heart rate increases when they have a stressful stimulus, they make cost-benefit analyses where they&#39;re willing to risk negative stimuli for greater reward. Sure looks like they&#39;re conscious.</p><p> Specifically, they endorse a “psycho-neuro-ethological &#39;&#39;triangulation&#39;&#39; approach. The paper is filled with big phrases like that. What that means is that they look at various things that happen in the brain when we feel certain emotions. They observe that in humans, those emotions cause certain things—for example, being happy makes us more playful. They then look at mammal brains and see that they have the same basic brain structure, and this produces the same physical reactions—using the happiness example, this would also make the animals more playful. If they see that animals have the same basic neural structures as we do when we have certain experiences and that those are associated with the same physical states that occur when humans have those conscious states, they infer that the animals are having similar conscious states. If our brain looks like a duck&#39;s brain when we have some experience, and we act like ducks do when they are in a comparable brain state, we should guess that ducks are having a similar experience. (I know we&#39;re talking about mammals here, but I couldn&#39;t resist the “looks like a duck, talks like a duck joke.”)</p><p> If a pig has a brain state that resembles ours when we are happy, tries to get things that make it happy, and produces the same neurological responses that we do when we&#39;re happy, we should infer that pigs are not mindless automatons, but are, in fact, happy.</p><p> They then note that animals like drugs. Animals, like us, get addicted to opioids and have similar brain responses when they&#39;re on opioids. As the authors note “Indeed, one can predict drugs that will be addictive in humans quite effectively from animal studies of desire.” If animals like the drugs that make us happy and react in similar ways to us, that gives us good reason to think that they are, in fact, happy.</p><p> They then note that the parts of the brain responsible for various human emotions are quite ancient—predating humans—and that mammals have them too. So, if the things that cause emotions are also present in animals, we should guess they&#39;re conscious, especially when their behavior is perfectly consistent with being conscious. In fact, by running electricity through certain brain regions that animals share, we can induce conscious states in people—that shows that it is those brain states that are causing the various mental states.</p><p> The authors then run through various other mental states and show that those mental states are similar between humans and animals—animals have similar brain regions which provoke similar physical responses, and we know that in humans, those brain regions cause specific mental states.</p><p> Now, maybe there&#39;s some magic of the human brain, such that in animal brains, the brain regions that cause qualia instead cause causally identical stuff but no consciousness. But there&#39;s no good evidence for that, and plenty against. You should not posit special features of certain physical systems, for no reason.</p><p> Moving on to the <a href="https://www.sciencedirect.com/science/article/pii/S1053810004000893?casa_token=B3K3KvAUWb8AAAAA:-xmnYJuqoi_dlGijNwS9xdmnpq9jv1JtsBgGq7FOOXgEemDJdavp5naJVSntJwzIYjpyX_8V">Seth, Baars, and Edelman</a> paper, they note that there are various features of consciousness, that differentiate conscious states from other things happening in the brain that don&#39;t induce conscious states. They note:</p><blockquote><p> Consciousness involves widespread, relatively fast, low-amplitude interactions in the thalamocortical core of the brain, driven by current tasks and conditions. Unconscious states are markedly different and much less responsive to sensory input or motor plans.</p></blockquote><p> In other words, there are common patterns among conscious states. We can look at a human brain and see that the things that are associated with consciousness produce different neurological markers from the things that aren&#39;t associated with consciousness. Features associated with consciousness include:</p><p> Irregular, low-amplitude brain activity: When we&#39;re awake we have irregular low-amplitude brain activity. When we&#39;re not conscious—eg in deep comas or anesthesia-induced unconsciousness—irregular, low-amplitude brain activity isn&#39;t present. Mammal brains possess irregular, low-amplitude brain activity.</p><p> Involvement of the thalamocortical system: When you damage the thalamocortical system, that deletes part of one&#39;s consciousness, unlike other systems. Mammals also have a thalamocortical system—just like us.</p><p> Widespread brain activity: Consciousness induces widespread brain activity. We don&#39;t have that when things induce us not to be conscious, like being in a coma. Mammals do.</p><p> The authors note, from these three facts:</p><blockquote><p> Together, these first three properties indicate that consciousness involves widespread, relatively fast, low-amplitude interactions in the thalamocortical core of the brain, driven by current tasks and conditions. Unconscious states are markedly different and much less responsive to sensory input or endogenous activity. These properties are directly testable and constitute necessary criteria for consciousness in humans. It is striking that these basic features are conserved among mammals, at least for sensory processes. The developed thalamocortical system that underlies human consciousness first arose with early mammals or mammal-like reptiles, more than 100 million years ago.</p></blockquote><p> More evidence from neuroscience for animal consciousness:</p><p> Something else about metastability that I don&#39;t really understand is also present in humans and animals.</p><p> Consciousness involves binding—bringing lots of different inputs together. In your consciousness, you can see the entire world at once, while thinking about things at the same time. Lots of different types of information are processed simultaneously, in the same way. Some explanations involving neural synchronicity have received some empirical support—and animals also have neural synchronicity, so they would also have the same kind of binding.</p><p> We attribute conscious experiences as happening to us. But mammals have a similar sense of self. Mammals, like us, process information relative to themselves—so they see a wall and process it relative to them in space.</p><p> Consciousness facilitates learning. Humans learn from conscious experiences. In contrast, we do not learn from things that do not impinge on our consciousness. If someone slaps me whenever I scratch my nose (someone does actually—crazy story), I learn not to scratch my nose. In contrast, if someone does a thing that I don&#39;t consciously perceive when I scratch my nose, I won&#39;t learn from it. But animals seem to learn to, and update in response to stimulus, just like humans do—but only when humans are exposed to things that affect their consciousness. In fact, <a href="https://benthams.substack.com/p/underwater-torture-chambers">even fish learn</a> .</p><p> So there&#39;s a veritable wealth of evidence that at least mammals are conscious. The evidence is less strong for organisms that are less intelligent and more distant from us evolutionarily, but it remains relatively strong for at least many fish. Overturning this abundance of evidence, that&#39;s been enough to convince the substantial majority of consciousness researchers requires a lot of evidence. Does Yudkowsky have it?</p><h2> Yudkowsky&#39;s view is crazy, and is decisively refuted over and over again</h2><p> No. No he does not. In fact, as far as I can tell, throughout the entire protracted Facebook exchange, he never adduced a single piece of evidence for his conclusion. The closest that he provides to an argument is the following:</p><blockquote><p> I consider myself a specialist on reflectivity and on the dissolution of certain types of confusion. I have no compunction about disagreeing with other alleged specialists on authority; any reasonable disagreement on the details will be evaluated as an object-level argument. From my perspective, I&#39;m not seeing any, “No, <i>this</i> is a non-mysterious theory of qualia that says pigs are sentient…” and a lot of “How do <i>you</i> know it doesn&#39;t…?” to which the only answer I can give is, “I may not be certain, but I&#39;m not going to update my remaining ignorance on your claim to be even more ignorant, because you haven&#39;t yet named a new possibility I haven&#39;t considered, nor pointed out what I consider to be a new problem with the best interim theory, so you&#39;re not giving me a new reason to further spread probability density.”</p></blockquote><p>什么？？？ The suggestion seems to be that there is no other good theory of consciousness that implies that animals are conscious. To which I&#39;d reply:</p><p> We don&#39;t have any good theory about consciousness yet—the data is just too underdetermined. Just as you can know that apples fall when you drop them before you have a comprehensive theory of gravity, so too can you know some things about consciousness, even absent a comprehensive theory.</p><p> There are various theories that predict that animals are conscious. For example, <a href="https://www.nature.com/articles/nrn.2016.44">integrated information theory</a> , <a href="https://philarchive.org/archive/MCFTCF-3v1">McFadden&#39;s CEMI field theory</a> , various Higher-Order theories, and the global workspace model will probably imply that animals are conscious. Eliezer has no argument to prefer his view to others.</p><p> Take the integrated information theory, for example. I don&#39;t think it&#39;s a great view. But at least it has something going for it. It has made a series of accurate predictions about the neural correlates of consciousness. Same with McFadden&#39;s theory. It seems Yudkowsky&#39;s theory has literally nothing going for it, beyond it sounding to Eliezer like a good solution. There is no empirical evidence for it, and, as we&#39;ll see, it produces crazy, implausible implications. David Pearce has a nice comment about some of those implications:</p><blockquote><p> Some errors are potentially ethically catastrophic. This is one of them. Many of our most intensely conscious experiences occur when meta-cognition or reflective self-awareness fails. Thus in orgasm, for instance, much of the neocortex effectively shuts down. Or compare a mounting sense of panic. As an intense feeling of panic becomes uncontrollable, are we to theorise that the experience somehow ceases to be unpleasant as the capacity for reflective self-awareness is lost? “Blind” panic induced by eg a sense of suffocation, or fleeing a fire in a crowded cinema (etc), is one of the most unpleasant experiences anyone can undergo, regardless or race or species. Also, compare microelectrode neural studies of awake subjects probing different brain regions; stimulating various regions of the “primitive” limbic system elicits the most intense experiences. And compare dreams – not least, nightmares – many of which are emotionally intense and characterised precisely by the lack of reflectivity or critical meta-cognitive capacity that we enjoy in waking life.</p></blockquote><p> Yudkowsky&#39;s theory of consciousness would predict that during especially intense experiences, where we&#39;re not reflecting, we&#39;re either not conscious or less conscious. So when people orgasm, they&#39;re not conscious. That&#39;s very implausible. Or, when a person is in unbelievable panic, on this view, they become non-conscious or less conscious. Pearce further notes:</p><blockquote><p> Children with autism have profound deficits of self-modelling as well as social cognition compared to neurotypical folk. So are profoundly autistic humans less intensely conscious than hyper-social people? In extreme cases, do the severely autistic lack consciousness&#39; altogether, as Eliezer&#39;s conjecture would suggest? Perhaps compare the accumulating evidence for Henry Markram&#39;s “ <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2518049/"><strong>Intense World</strong></a> ” theory of autism.</p></blockquote><p> Francisco Boni Neto furthers:</p><blockquote><p> many of our most intensely conscious experiences occur when meta-cognition or reflective self-awareness fails. Super vivid, hyper conscious experiences, phenomenic rich and deep experiences like lucid dreaming and &#39;out-of-body&#39; experiences happens when higher structures responsible for top-bottom processing are suppressed. They lack a realistic conviction, specially when you wake up, but they do feel intense and raw along the pain-pleasure axis.</p></blockquote><p> Eliezer just bites the bullet:</p><blockquote><p> I&#39;m not totally sure people in sufficiently unreflective flow-like states are conscious, and I give serious consideration to the proposition that I am reflective enough for consciousness only during the moments I happen to wonder whether I am conscious. This is not where most of my probability mass lies, but it&#39;s on the table.</p></blockquote><p> So when confronted with tons of neurological evidence that shutting down higher processing results in more intense conscious experiences, Eliezer just says that when we think that we have more intense experiences, we&#39;re actually zombies or something? That&#39;s totally crazy. It&#39;s sufficiently crazy that I think I might be misunderstanding him. When you find out that your view says that people are barely conscious or non-conscious when they orgasm or that some very autistic people aren&#39;t conscious, it makes sense to give up the damn theory!</p><p> And this isn&#39;t the only bullet Eliezer bites. He admits, “It would not surprise me very much to learn that average children develop inner listeners at age six.” I have memories from before age 6—these memories would have to have been before I was conscious, on this view.</p><p> Rob Wiblin makes a good point:</p><blockquote><p> [Eliezer], it&#39;s possible that what you are referring to as an &#39;inner listener&#39; is necessary for subjective experience, and that this happened to be added by evolution just before the human line. It&#39;s also possible that consciousness is primitive and everything is conscious to some extent. But why have the prior that almost all non-human animals are not conscious and lack those parts until someone brings you evidence to the contrary (ie “What I <i>need</i> to hear to be persuaded is,”)? That just cannot be rational.</p><p> You should simply say that you are a) uncertain what causes consciousness, because really nobody knows yet, and b) you don&#39;t know if eg pigs have the things that are proposed as being necessary for consciousness, because you haven&#39;t really looked into it.</p></blockquote><p> I agree with Rob. We should be pretty uncertain. My credences are maybe the following:</p><p> 92% that at least almost all mammals are conscious.</p><p> 80% that almost all reptiles are conscious.</p><p> 60% that fish are mostly conscious.</p><p> 30% that insects are conscious.</p><p> It&#39;s about as likely that reptiles aren&#39;t conscious as insects are. Because consciousness is private—you only know your own—we shouldn&#39;t be very confident about any features of consciousness.</p><p> Based on these considerations, I conclude that Eliezer&#39;s view is legitimately crazy. There is, quite literally, no good reason to believe it, and lots of evidence against it. Eliezer just dismisses that evidence, for no good reason, bites a million bullets, and acts like that&#39;s the obvious solution.</p><h2> Absurd overconfidence</h2><p> The thing that was most infuriating about this exchange was Eliezer&#39;s insistence that those who disagreed with him were stupid, combined with his demonstration that he had no idea what he was talking about. Condescension and error make an unfortunate combination. He says of the position that pigs, for instance, aren&#39;t conscious:</p><blockquote><p> It also seems to me that this is not all that inaccessible to a reasonable third party, though the sort of person who maintains some doubt about physicalism, or the sort of philosophers who think it&#39;s still respectable academic debate rather than sheer foolishness to argue about the A-Theory vs. <a href="https://en.wikipedia.org/wiki/B-theory_of_time"><strong>B-Theory of time</strong></a> , or the sort of person who can&#39;t follow the argument for why all our remaining uncertainty should be within different many-worlds interpretations rather than slopping over outside, will not be able to access it.</p></blockquote><p> Count me in as a person who can&#39;t follow any arguments about quantum physics, much less the arguments for why we should be almost certain of many worlds. But seriously, physicalism? We should have no doubt about physicalism? As I&#39;ve <a href="https://benthams.substack.com/p/against-dogmatic-physicalism">argued before</a> , the case against physicalism is formidable. Eliezer thinks it&#39;s an open-and-shut case, but that&#39;s because he is demonstrably mistaken about the zombie argument against physicalism and the implications of non-physicalism. In the literal second paragraph of <a href="https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies">his article about zombies</a> , Eliezer says:</p><blockquote><p> It is furthermore claimed that if zombies are &quot;possible&quot; (a term over which battles are still being fought), then, purely from our knowledge of this &quot;possibility&quot;, we can deduce a priori that consciousness is extra-physical, in a sense to be described below; the standard term for this position is &quot;epiphenomenalism&quot;.</p></blockquote><p>不！不！ No^10^64. He is confusing non-physicalism and epiphenomenalism. I am a non-physicalist non-epiphenomenalist. There are several other non-physicalist views. In fact, in the Facebook exchange, Eliezer says:</p><blockquote><p> Suppose I claimed to be able to access an epistemic state where (rather than being pretty damn sure that physicalism is true) I was pretty damn sure that P-zombies / epiphenomenalism was false.</p></blockquote><p> In a Facebook thread where Eliezer admonishes people for being too stupid to understand that physicalism is true, he demonstrates that he doesn&#39;t have a basic familiarity with the subject. The possibility of P-zombies is not the same thing as non-physicalism. And, as I&#39;ve shown before, Eliezer&#39;s reply to the zombie argument all hinges on that one crucial error.</p><p> I used to believe Eliezer&#39;s position about physicalism, after reading his piece on zombies. Then I made a friend (I had some before, just to be clear). He explained to me how the zombie argument really worked, rather than the distorted Yudkowsky version. After I learned that, I realized Eliezer&#39;s view fails completely.</p><p> And that&#39;s not the only thing Eliezer expresses insane overconfidence about. In response to his position that most animals other than humans aren&#39;t conscious, David Pearce points out that you shouldn&#39;t be very confident in positions that almost all experts disagree with you about, especially when you have a strong personal interest in their view being false. Eliezer replies:</p><blockquote><p> What do they think they know and how do they think they know it? If they&#39;re saying “Here is how we think an inner listener functions, here is how we identified the associated brain functions, and here is how we found it in animals and that showed that it carries out the same functions” I would be quite impressed. What I expect to see is, “We found this area lights up when humans are sad. Look, pigs have it too.” Emotions are just plain simpler than inner listeners. I&#39;d expect to see analogous brain areas in birds.</p></blockquote><p> When I read this, I almost fell out of my chair. Eliezer admits that he has not so much as read the arguments people give for widespread animal consciousness. He is basing his view on a guess of what they say, combined with an implausible physical theory for which he has no evidence. This would be like coming to the conclusion that the earth is 6,000 years old, despite near-ubiquitous expert disagreement, providing no evidence for the view, and then admitting that you haven&#39;t even read the arguments that experts give in the field against your position. This is the gravest of epistemic sins.</p><h1>结论</h1><p>This has not been anywhere near exhaustive. I haven&#39;t even started talking about Eliezer&#39;s very implausible views about <a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt/p/SbdCX6A5AGyyfhdmh">morality</a> (though I might write about that too—stay tuned), reductionism, <a href="https://www.lesswrong.com/posts/vzLrQaGPa9DNCpuZz/against-modal-logics">modality</a> , or many other topics. Eliezer usually has a lot to say about topics, and it often takes many thousands of words to refute what he&#39;s saying.</p><p> I hope this article has shown that Eliezer frequently expresses near certainty on topics that he has a basic ignorance about, an ignorance so profound that he should suspend judgment. Then, infuriatingly, he acts like those who disagree with his errors are morons. He acts like he is a better decision theorist than the professional decision theorists, a better physicist than the physicists, a better animal consciousness researcher than the animal consciousness researchers, and a much better philosopher of mind than the leading philosophers of mind.</p><p> My goal in this is not to cause people to stop reading Eliezer. It&#39;s instead to encourage people to refrain from forming views on things he says just from reading him. It&#39;s to encourage people to take his views with many grains of salt. If you&#39;re reading something by Eliezer and it seems too obvious, on a controversial issue, there&#39;s a decent chance you are being duped.</p><p> I feel like there are two types of thinkers, the first we might call innovators and the second systematizers. Innovators are the kinds of people who think of wacky, out-of-the-box ideas, but are less likely to be right. They enrich the state of discourse by being clever, creative, and coming up with new ideas, rather than being right about everything. A paradigm example is Robin Hanson—no one feels comfortable just deferring to Robin Hanson across the board, but Robin Hanson has some of the most ingenious ideas.</p><p> Systematizers, in contrast, are the kinds of people who reliably generate true beliefs on lots of topics. A good example is Scott Alexander. I didn&#39;t research Ivermectin, but I feel confident that Scott&#39;s post on Ivermectin is at least mostly right.</p><p> I think people think of Eliezer as a systematizer. And this is a mistake, because he just makes too many errors. He&#39;s too confident about things he&#39;s totally ignorant about. But he&#39;s still a great innovator. He has lots of interesting, clever ideas that are worth hearing out. In general, however, the fact that Eliezer believes something is not especially probative. Eliezer&#39;s skill lies in good writing and ingenious argumentation, not forming true beliefs.</p><br/><br/> <a href="https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/TjyyngWFYvQWPpNNj/eliezer-yudkowsky-is-frequently-confidently-egregiously<guid ispermalink="false"> TjyyngWFYvQWPpNNj</guid><dc:creator><![CDATA[omnizoid]]></dc:creator><pubDate> Sun, 27 Aug 2023 01:06:40 GMT</pubDate> </item><item><title><![CDATA[Mesa-Optimization: Explain it like I'm 10 Edition]]></title><description><![CDATA[Published on August 26, 2023 11:04 PM GMT<br/><br/><p>对于台面优化有<a href="https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care">几种</a>可用<a href="https://ui.stampy.ai?state=8160_">的</a><a href="https://www.lesswrong.com/tag/mesa-optimization">解释</a>。我认为<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=247s">Rob Miles 关于该主题的视频</a>非常棒，但我认为现有的书面描述并没有使这个概念简单到足以被广大观众彻底理解。对于那些更喜欢书面内容而不是视频的人来说，这是我的尝试。</p><h2>概括</h2><p>台面优化是人工智能对齐中的一个重要概念。有时，一个优化器（如梯度下降或进化）会产生另一个优化器（如复杂的人工智能或人类）。当这种情况发生时，第二个优化器被称为“mesa-optimizer”；台面优化器的对准（安全）问题称为“内部对准问题”。</p><h2>什么是优化器？</h2><p>我将优化器定义为一种查看可能事物的“空间”并以“选择”其中一些事物的方式运行的事物。可能有优化器在工作的一个迹象是<i>奇怪的事情正在发生，</i>我的意思是“如果系统随机行为，则不太可能发生的事情”。例如，人类一直在做一些事情，如果我们的行为是随机的，那么这些事情就不太可能发生，如果进化只是通过完全随机的基因创造新的生物体来实现，那么人类肯定根本不存在。</p><h3>人类的大脑</h3><p>人脑是一个优化器——它会审视你可以做的不同事情，并选择那些能让你做你喜欢的事情。</p><p>例如，您可能会去商店买冰淇淋，支付冰淇淋费用，然后再回来。如果你仔细想想，这是一系列非常复杂的动作——如果你的行为完全随机，你永远不会得到冰淇淋。你的大脑已经搜索了许多你可以采取的行动（去其他地方散步、在客厅跳舞、只将左脚向上移动 30 度），并预计这些行动都不会给你带来冰淇淋，然后选择了一个能够让你得到你想要的东西的极少数路径。</p><h3>进化</h3><p>优化器的一个奇怪的例子是进化。您可能已经听说过这一点——进化“<i>优化了包容性遗传适应性</i>”。但是，这是什么意思？</p><p>生物体的变化是随机的——它们的基因由于制造新生物体过程中的错误而发生变化。有时，这种变化可以通过延长有机体的生存时间、使其更具吸引力等来帮助有机体繁殖。当这种情况发生时，下一代就会有更多的这种变化。随着时间的推移，许多这些变化积累起来，形成了非常复杂的系统，如植物、动物和真菌。</p><p>因为最终会有更多的生物体繁殖更多，而那些发生变化导致繁殖能力降低的生物体（比如遗传疾病）会越来越少，所以进化是从所有发生的突变的空间中进行选择的——如果你用一个完全随机的基因组，它肯定会立即死亡（或者更确切地说，一开始就没有活着）。</p><h3>人工智能培训</h3><p>当人工智能被训练时，通常是通过“梯度下降”来完成的。什么是梯度下降？</p><p>首先，我们定义一些表示人工智能表现如何的东西（“损失函数”）。这可能非常简单（每次输出“狗”时 1 分）或非常复杂（每次您说出一个对我输入的内容有意义的英语句子时 1 分）。</p><p>然后，我们制作一个随机人工智能——基本上只是一组相互连接的随机数。可以预见的是，这个人工智能的表现非常糟糕——它输出“dska£hg@tb5gba-0aaa”或类似的内容。我们运行它很多次，看看它何时接近输出“dog”（例如，以 ad 作为第一个字母，或输出接近 3 个字符）。然后，我们使用数学算法来找出哪些随机数使人工智能接近我们想要的值，哪些与正确的数字相距甚远——然后我们将它们稍微向正确的方向移动。然后我们多次重复这个过程，直到最终人工智能每次都一致输出“dog” <span class="footnote-reference" role="doc-noteref" id="fnref20xlfniecdi"><sup><a href="#fn20xlfniecdi">[1]</a></sup></span> 。</p><p>这个过程很奇怪，但弄清楚数字应该变化的<i>方向</i>比从一开始就弄清楚正确的数字要容易得多，特别是对于更复杂的任务。</p><p>再次，我们可以看到这个过程是一个优化器——随机人工智能没有做任何有趣的事情，但是通过将数字推向正确的方向，我们可以完成非常复杂的任务，而这些任务永远不会随机发生。之所以发生这种情况，是因为我们观察了一个非常大的可能的人工智能“空间”（其中有不同的数字），并通过它“移动”到一个能做我们想要的事情的人工智能。</p><h2>什么是<i>Mesa</i>优化器？</h2><p>为了理解<i>台面</i>优化，我们将回到进化类比。我们看到了优化器的两个例子——进化和人脑。其中之一创造了另一个！这是台面优化的关键。</p><p>当我们训练人工智能时，就像进化一样，我们可能会发现人工智能本身就是优化器，即它们会检查可能的动作空间，并采取那些给它们带来好分数的动作，类似于人脑。在人工智能中，第二个优化器被称为<i>台面优化器。</i> Mesa-optimizer 指的是<i>我们自己训练的 AI</i> ，而外部优化器是我们用来训练该 AI 的过程（梯度下降）。请注意，某些人工智能（尤其是简单的人工智能）可能不算作台面优化器，因为它们没有表现出足够复杂的行为，不足以本身成为优化器。 </p><figure class="image"><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_640 640w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_960 960w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1120 1120w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1280 1280w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1440 1440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5bf45009f8246c40d947ef598f1275a9ab8604f0608daef.png/w_1516 1516w"><figcaption>图片无耻地“借用”自<a href="https://www.youtube.com/watch?v=bJLcIBixGj8&amp;t=426s">Rob Miles 关于 mes&#39;a-optimizers 的视频</a></figcaption></figure><h2>为什么这是个问题？</h2><p>如果我们有两个优化器，那么现在要让 AI 表现良好就会遇到两个问题（两个“对齐问题”）。具体来说，我们有一个“外部”和“内部”对齐问题，分别与梯度下降和台面优化器相关。</p><p>外对齐问题是AI对齐的经典问题。当您创建优化器时，很难确保您告诉它做的事情是您真正<i>希望</i>它做的事情。例如，你如何告诉计算机“根据我所写的内容合理地写出连贯的英语句子”？将复杂任务正式定义为损失函数可能非常棘手。这对于某些任务来说可能很危险，但对于本文来说讨论这个问题会花费太长时间。</p><p><i>内部</i>对齐问题有点棘手。我们可能会根据自己的需要完美地定义损失函数，但 AI 的行为仍然很危险，因为它<i>与外部优化器</i>不一致。例如，如果梯度下降找到一种算法，可以优化“在训练时假装表现良好以获得好分数，这样我以后就可以做我真正想做的事情”（“<a href="https://www.lesswrong.com/tag/deceptive-alignment">欺骗性对齐</a>”），这将得到我们的训练数据集得分很高，但仍然很危险。</p><h2>内部错位的示例</h2><h3>人类</h3><p>对于我们的第一个例子，我们将再次回到我们的进化类比。进化对非常擅长生存和繁殖的事物的探索最终产生了大脑——一种台面优化器。现在，人类与进化变得不相符<span class="footnote-reference" role="doc-noteref" id="fnrefqu1bh02smsf"><sup><a href="#fnqu1bh02smsf">[2]</a></sup></span> 。</p><p>因为进化是一种相当奇怪的搜索类型（如梯度下降），所以它无法将“繁殖”的概念直接放入我们的大脑中。相反，开发了一系列与生殖器摩擦有关的更简单的概念，与人际关系有关的复杂事物等等。</p><p>现在，人类做的事情非常有效地满足了那些更简单的概念，但对包容性遗传健康来说却很糟糕，比如手淫、看色情片、不向精子库捐赠。这表明进化存在内部错位问题。</p><h3>进化至灭绝</h3><p>但人类还是很漂亮的<i> </i>与进化完全一致——与其他猿类相比，我们的种群规模很大。为了说明进化的内在失调有多么严重，让我们看看<a href="https://en.wikipedia.org/wiki/Irish_elk#Extinction">爱尔兰麋鹿</a>。爱尔兰麋鹿（可能）进化到灭绝。但这不是与进化的原理相反吗？这是怎么发生的？ </p><figure class="image image_resized" style="width:53.06%"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/220px-Irish_Elk_front.jpg" alt="博物馆收藏的带有大鹿角的爱尔兰麋鹿头骨标本的照片" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/330px-Irish_Elk_front.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Irish_Elk_front.jpg/440px-Irish_Elk_front.jpg 2x"><figcaption>爱尔兰麋鹿，拥有标志性的巨大鹿角</figcaption></figure><p><a href="https://en.wikipedia.org/wiki/Sexual_selection">性选择</a>是指动物进化为对配偶有吸引力，而不是为了更好地生存。这对于进化来说是有好处的——诚实地向潜在的伴侣展示你很强壮、速度很快、吃得很好等等，这可能是确保那些可能生存下来的后代繁殖更多的好方法。例如，长出大鹿角可能表明你吃饱了并且能够为这些鹿角提供充足的营养，或者你能够很好地战斗来保护自己。</p><p>然而，性选择也可能出错。在某种程度上，拥有大鹿角是件好事。一旦它们变得太大，你可能无法很好地移动头部，被挂在树上，并浪费大量关键资源。但如果雌性喜欢大鹿角，那么拥有大鹿角的雄性就会大量繁殖，即使只有少数能够存活到成年。很快，所有雄性都长出了巨大的鹿角，为生存而苦苦挣扎。即使这不会直接导致灭绝，但如果种群数量减少，也会造成灭绝。这表明进化存在<i>严重的</i>内部对齐问题。</p><h3>招聘高管</h3><figure class="image image_resized" style="width:505.5px"><img src="https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fimages.huffingtonpost.com%2F2015-10-27-1445916079-7527424-Dollarphotoclub_67902901.jpg&amp;f=1&amp;nofb=1&amp;ipt=5035d0cd00af6f1499e263734b1a32b232b07af983be635ee9a2e9f0c4fe288f&amp;ipo=images" alt="成功的非洲裔美国女性在商界所做的三件事与众不同......"><figcaption> <a href=" http://images.huffingtonpost.com/2015-10-27-1445916079-7527424-Dollarphotoclub_67902901.jpg">一个商人</a>，因为我怀疑你们现在都已经入迷了</figcaption></figure><p>在招聘高管时，公司股东面临两个问题。</p><p>第一个问题是外部一致性问题：他们如何设计招聘流程和激励计划，以便他们雇用的高管有动力按照公司的最佳利益行事？这个问题看起来真的很难——他们如何阻止高管为了公司的短期利益而行事以获取奖金，并以对公司造成长期损害的代价（当他们很可能跳槽到另一家公司时）表现得很好。做同样的事情<span class="footnote-reference" role="doc-noteref" id="fnrefvkgqwj5lsse"><sup><a href="#fnvkgqwj5lsse">[3]</a></sup></span> ）？</p><p>这里的内部错位问题来自这样一个事实，即被雇用的人本身就是优化者，并且可能与股东的目标截然不同。例如，如果有潜在的高管想要损害公司<span class="footnote-reference" role="doc-noteref" id="fnrefl64dyogiew"><sup><a href="#fnl64dyogiew">[4]</a></sup></span> ，他们就会有强烈的动机在招聘过程中表现出色。他们甚至可能会被激励在指标和激励计划上表现出色，以便继续留在公司并继续对公司造成微妙的损害，或者将公司的重点转移到或远离某些领域，同时以困难的方式让股东损失金钱。测量。</p><h2>结论</h2><p>我想指出的是，<a href="https://www.lesswrong.com/tag/inner-alignment">外部错位和内部错位之间的界限相当模糊</a>，经验丰富的研究人员有时很难区分它们。此外，您可能会同时出现内部和外部错位。</p><p>希望这可以帮助您更彻底地理解内在的错位。如果您对我的写作有疑问或反馈，请在评论中指出。</p><p>一些问题来检查您是否理解：</p><ul><li>台面优化器的示例是什么？它与其他类型的优化器有何不同？</li><li>什么是“外部对齐问题”？</li><li>什么是“欺骗性对齐”？ </li></ul><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fn20xlfniecdi"> <span class="footnote-back-link"><sup><strong><a href="#fnref20xlfniecdi">^</a></strong></sup></span><div class="footnote-content"><p> This process is called <i>gradient descent</i> because one way of visualising this is with a graph, where the loss function is the up-down axis, and each of the numbers is another axis.我们调整数字以最快地在损失函数轴上“下降”。对于很多数字，这非常奇怪，但如果我们有一个非常简单的“AI”，只有两个数字，它可能看起来像这样： <img style="width:599.404px" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fproxy%2F1*f9a162GhpMbiTVTAua_lLQ.png&amp;f=1&amp;nofb=1&amp;ipt=ea75275746c0f883e5e18e606181f66040c7e7f8a4e297a3fe7e3d60f4a00b80&amp;ipo=images" alt="梯度下降：您需要知道的一切 |作者：Suryansh S. |黑客中午..."></p><p> “路径”梯度下降采用黑色（从红色部分到蓝色部分）。</p><p></p></div></li><li class="footnote-item" role="doc-endnote" id="fnqu1bh02smsf"> <span class="footnote-back-link"><sup><strong><a href="#fnrefqu1bh02smsf">^</a></strong></sup></span><div class="footnote-content"><p>如果您实际上 10 岁，您可能希望跳过下一段。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnvkgqwj5lsse"> <span class="footnote-back-link"><sup><strong><a href="#fnrefvkgqwj5lsse">^</a></strong></sup></span><div class="footnote-content"><p>这似乎可以分为外对齐<i>或</i>内对齐。这并不罕见，但令人困惑。</p></div></li><li class="footnote-item" role="doc-endnote" id="fnl64dyogiew"> <span class="footnote-back-link"><sup><strong><a href="#fnrefl64dyogiew">^</a></strong></sup></span><div class="footnote-content"><p>例如，他们是在一家石油公司工作的秘密气候变化活动家</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/fLAvmWHmpJEiw8KEp/mesa-optimization-explain-it-like-i-m-10-edition#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/fLAvmWHmpJEiw8KEp/mesa-optimization-explain-it-like-im-10-edition<guid ispermalink="false"> fLAvmWHmpJEiw8KEp</guid><dc:creator><![CDATA[brook]]></dc:creator><pubDate> Sat, 26 Aug 2023 23:04:21 GMT</pubDate> </item><item><title><![CDATA[Aumann-agreement is common]]></title><description><![CDATA[Published on August 26, 2023 8:22 PM GMT<br/><br/><p><i>感谢 Justis Mills 的校对和反馈。这篇文章也可以在</i><a href="https://tailcalled.substack.com/p/aumann-agreement-is-common"><i>我的子堆栈</i></a>上找到<i>。</i></p><p><a href="https://www.lesswrong.com/tag/aumann-s-agreement-theorem">奥曼一致性定理</a>是一个定理家族，它说如果人们彼此信任并且了解彼此的观点，那么他们就会彼此同意。或者换句话说，如果人们彼此保持信任，那么他们就能达成协议。 （该定理的一些变体考虑了计算因素，表明它们可以相当快地做到这一点。）</p><p>最初的证明非常正式且令人困惑，但一个更简单的启发式论证是，对于一个诚实、理性的代理人来说，<a href="https://www.lesswrong.com/rationality/what-is-evidence">他们表达意见的事实</a>就可以成为另一个理性代理人的<a href="https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common">有力证据</a>，因为如果说话者的概率高于说话者的概率在此之前，那么他们一定已经看到了相应的证据来证明该观点的合理性。</p><p>有些人觉得这令人困惑，并觉得这一定是错误的，因为它不适用于大多数分歧。我认为这些人是错误的，因为他们对分歧的看法不够广泛。奥曼一致定理适用的分歧概念是指人们为事件分配不同的概率；这是一个非常包容的概念，涵盖了许多我们通常不认为是分歧的事情，包括一方拥有有关某个主题的信息而另一方没有信息的情况。</p><h2>我在挪威的假期很大程度上依赖于奥曼协议</h2><p>最近，我和妻子在挪威度假。</p><p>为了到达那里并四处走动，我们需要交通工具。起初我们不同意那里提供交通的人，因为我们不知道很多具体的交通方式，只是模糊地知道会有一些飞机和轮船，但不知道是哪些。但我的妻子听说有一种叫做“奥斯陆渡轮”的东西，所以我们奥曼同意这是一个选择，并决定进一步调查。</p><p>我们不同意提供奥斯陆渡轮的公司，因为我们不知道他们的网站是什么，所以我们询问了谷歌，它提供了一些关于渡轮可能是什么的选项，我们奥曼同意谷歌的意见，然后去调查从那里。我们发现一个网站声称出售渡轮票；起初，我们不同意该网站关于我们何时可以旅行的说法，因为我们不知道渡轮的时间，但后来我们阅读了它声称的可用时间，然后奥曼更新了这一点。</p><p>我们还必须在挪威找到一些事情可做。对我们来说幸运的是，OpenAI 的一些人注意到每个人都对互联网存在巨大分歧，因为没有人真正记住了互联网，他们认为通过解决这种分歧可以获得一些价值，所以他们通过填充来同意互联网。它进入一个名为 ChatGPT 的神经网络。起初，ChatGPT 不同意我们去挪威参观什么，并提出了一些我们并不真正感兴趣的事情，但我们告知了它我们的兴趣，然后它很快就与我们达成了一致，并提出了一些其他更有趣的事情。</p><p>我们参观的其中一处是一个为一位冒险家建造的木筏并在海洋中航行的博物馆。在参观博物馆之前，我们对此有很多分歧，例如我们不知道木筏上的一个人落入海中并且必须获救。但博物馆告诉我们事实确实如此，所以我们奥曼同意相信它。想必博物馆是通过​​奥曼得知这件事的——与木筏上的人达成一致。</p><p>错误的奥曼协议的一个例子是与火车公司 Vy 签订的协议。他们说可以给我们买一张卑尔根火车的火车票，我们得到了奥曼的同意。然而，由于暴风雨，他们的火车轨道被破坏了，公司网站直到最后一刻才承诺火车上的可用性，所以我们没有得到 Vy 的纠正。</p><p>但我们并没有通过凭经验看到损坏的轨道或通过理性推理认为它不可用而得救。相反，我们得救了，因为我们告诉某人我们乘坐卑尔根火车的计划，希望他们同意我们乘坐火车的信念，但他们却一直不同意，并告诉我们火车被洪水淹没了，将会被淹没。取消。这让我们奥曼同意我们必须找到其他方法，我们询问谷歌是否有任何航班，其中建议了一些我们奥曼同意的航班。</p><p>后来，我把这次旅行的事告诉了我爸爸，现在也告诉了你。在谈论这件事之前，我预计你会不同意，因为你对此一无所知，但至少我很确定我的父亲奥曼同意我告诉他的事情，而且我怀疑你也这样做了。</p><h2>奥曼式的分歧很快就消失了，因此“分歧”意味着/表示非奥曼式的分歧</h2><p>我的故事中提到的分歧都发生在具有合理信任程度的各方之间，而且大多涉及一方缺乏信息而另一方有信息，因此通过转移信息很快就得到了解决。即使注意到分歧的细节也足以传递信息并解决它。</p><p>与此同时，在政治中，目标相互冲突的人们之间经常会出现分歧，他们有理由怀疑一方歪曲事实，因为他们更关心获得权力，而不是准确地告知与他们交谈的人。</p><p>因为当你怀疑对方有偏见时，奥曼协议的前提条件就不成立，所以这种分歧不会那么快得到解决，而是长期存在。但如果我们根据长期存在的分歧来形成对分歧的看法，那么这意味着我们正在过滤掉奥曼条件所适用的分歧。</p><p>因此，“分歧”意味着（甚至可能表示）“<i>彼此不信任的人之间</i>意见分歧”，而不仅仅是“意见分歧”。</p><h2>大多数奥曼人的分歧只是因为缺乏意识</h2><p>贝叶斯范式并没有<i>从根本上</i>区分<span class="footnote-reference" role="doc-noteref" id="fnrefy5hm5xwtgb"><sup><a href="#fny5hm5xwtgb">[1]</a></sup></span>由于没有关于某个命题的信息而对某个命题的怀疑与由于观察到矛盾的信息而对命题的怀疑。例如，考虑随机挑选两个人并对他们做出诸如“Marv Elsher 正在与 Abrielle Levine 约会”之类的声明。你不知道这些人是谁，而且大多数人都没有互相约会，所以你应该理性地分配一个非常低的概率。</p><p>但这并不是因为你从矛盾的证据中积极地不相信它！事实上，你甚至可能不认为自己事先对此有信念。如果事实上有一个马夫·埃尔舍（Marv Elsher）正在与阿布里埃尔·莱文（Abrielle Levine）约会，那么马夫对这个说法的可能性非常高，而如果没有这篇文章，你甚至不会想到它。</p><p>如果您考虑人们为可符号表达的命题分配不同概率的所有情况，那么几乎所有情况都将遵循这些思路，因为有大量您根本无法访问的随机本地信息。因此，如果你想考虑奥曼一致性定理所指的分歧的典型案例，你应该认为“A 观察到了 X，而 B 甚至不知道 X 周围发生了什么，更不用说任何关于 X 的证据了”。 X 本身”。</p><h2>奥曼协议极其高效、强大</h2><p>大部分的更新都是在假期期间发生的，自己根本无法去核实。他们常常关注在空间和时间上都很遥远的事情。有时他们关心的是过去发生的事情，而这些事情甚至无法在物理上进行验证。但即使对于您可以验证的事情，也比仅进行 Aumann 更新需要更多<i>数量级的</i>时间和资源。</p><h2>奥曼协议是关于汇集，而不是节制</h2><p>在我的例子中，人们通常不会达成妥协的立场；相反，他们大量采用了对手方的头寸。对于奥曼协议来说，这通常是正确的想法。虽然更新的确切方式可能会根据先前的情况和证据而有所不同，但我喜欢的一个简单示例是：</p><p>你们都从根据一些共享的先验（即你们开始同意）开始让您的对数赔率成为某个向量x。然后，您观察一些证据 y，将您的对数几率更新为 x+y，同时他们观察一些独立证据 z，将其对数几率更新为 x+z。如果您交换所有信息，那么这会将您的共享对数赔率更新为 x+y+z，这很可能比单独的 x+y 或 x+z 更彻底地偏离 x。</p><h2>奥曼条件信任信息</h2><p>当然，有时候奥曼协议似乎应该让人们变得温和，对吗？就像在政治中一样，如果你花了很多时间吸收一方的意识形态，而你的对话者也花了很多时间吸收另一方的意识形态，但你却在对方的论点上戳了很多漏洞？</p><p>我认为在这种情况下，得知你从政党那里学到的论点中存在漏洞可能会成为怀疑你的政党的可信度的理由，特别是当他们无法修复这些漏洞时。由于你奥曼对你的政党的观点进行了大量更新，特别是因为你信任他们，这也应该让你不再更新他们的观点，大概是调节他们的观点。</p><p> （我认为这对集体认知论有着巨大的影响，我已经逐渐在此基础上发展了集体理性理论，但它还没有完成，这篇文章的目的只是为了理解一致定理而不是阐述该理论。）</p><p>由于奥曼协议，您可能还可以通过不太复杂的方式进行调节，例如，如果矛盾的信息相互抵消。</p><h2>奥曼的许多更新都是关于承诺、历史或普遍性</h2><p>我的故事中许多最明显的奥曼更新都是关于承诺的。例如，对话者会在特定时间为我提供从一个地点到另一个地点的特定交通工具。</p><p>有人可能认为这表明承诺与奥曼协议定理有独特的联系，但我认为这实际上是因为承诺是一种异常普遍的信息类型，原因在于：</p><ul><li>人们对自己提出可靠主张的能力。</li><li>在实践中足够有用，值得分享。</li><li>涵盖多种且开放的可能性。</li></ul><p>例如，如果你答应我在你的厨房里做一个三明治，那么你可以通过支付租金来保持厨房的所有权，购买和储存三明治的原料以便组装起来，然后组装起来，以确保你的承诺是真实的。到时候给我做三明治。</p><p>同时，如果你告诉我别人的厨房里有可用的三明治，那么因为你无法控制那个厨房，所以一旦我们真正到达我需要它的时间，它可能就不再是真的，所以你可以&#39;不能可靠地对此做出声明。而且，就算可以，我也不一定能拿走，所以对我来说没什么用处。</p><p>你可能可以相当可靠地对你过去见过的某些事情做出断言，但其中大多数并不是很有用，因为它们发生在过去。例如，虽然我从博物馆知道木筏上的那个人掉进了水里，但我没有任何东西可以用它。也就是说，有时（例如将结果归因于原因，或进行概括），它们是有用的。</p><p>你可以阅读一本物理教科书，并对此做很多有用的奥曼更新，但这主要是因为物理是一门“通用”学科，但这也意味着它是一门信息量有限的封闭学科。不可能存在具有交替粒子和吸引力强度的“替代物理学”，这与存在具有交替飞行时间的“替代飞机公司”的感觉不同。</p><p>承诺、历史和普遍性并不意味着是一个完整的分类，这只是我注意到的一些东西。 </p><ol class="footnotes" role="doc-endnotes"><li class="footnote-item" role="doc-endnote" id="fny5hm5xwtgb"> <span class="footnote-back-link"><sup><strong><a href="#fnrefy5hm5xwtgb">^</a></strong></sup></span><div class="footnote-content"><p>它通过从先验到后验的更新历史来区分，但这种区别并不“存储”在概率分布中的任何地方，因此信念本身被视为相同，即使它们的历史不同。</p></div></li></ol><br/><br/><a href="https://www.lesswrong.com/posts/4S6zunFNFY3f5JYxt/aumann-agreement-is-common#comments">讨论</a>]]>;</description><link/> https://www.lesswrong.com/posts/4S6zunFNFY3f5JYxt/aumann-agreement-is-common<guid ispermalink="false"> 4S6zunFNFY3f5JYxt</guid><dc:creator><![CDATA[tailcalled]]></dc:creator><pubDate> Sat, 26 Aug 2023 20:22:03 GMT</pubDate></item></channel></rss>